<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 03 Feb 2025 10:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Anthropic: "Applicants should not use AI assistants" (217 pts)]]></title>
            <link>https://simonwillison.net/2025/Feb/2/anthropic/</link>
            <guid>42915905</guid>
            <pubDate>Mon, 03 Feb 2025 07:46:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://simonwillison.net/2025/Feb/2/anthropic/">https://simonwillison.net/2025/Feb/2/anthropic/</a>, See on <a href="https://news.ycombinator.com/item?id=42915905">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<blockquote cite="https://boards.greenhouse.io/anthropic"><p><em>While we encourage people to use AI systems during their role to help them work faster and more effectively, please do not use AI assistants during the application process. We want to understand your personal interest in Anthropic without mediation through an AI system, and we also want to evaluate your non-AI-assisted communication skills. Please indicate 'Yes' if you have read and agree.</em></p>
<p>Why do you want to work at Anthropic? (We value this response highly - great answers are often 200-400 words.)</p></blockquote>
<p>— <a href="https://boards.greenhouse.io/anthropic">Anthropic</a>, <span>online job application form</span></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Dumbest Trade War Fallout Begins (104 pts)]]></title>
            <link>https://www.wsj.com/opinion/the-trump-tariff-fallout-begins-canada-mexico-vow-retaliation-economic-uncertainty-da522b44</link>
            <guid>42914478</guid>
            <pubDate>Mon, 03 Feb 2025 03:06:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/opinion/the-trump-tariff-fallout-begins-canada-mexico-vow-retaliation-economic-uncertainty-da522b44">https://www.wsj.com/opinion/the-trump-tariff-fallout-begins-canada-mexico-vow-retaliation-economic-uncertainty-da522b44</a>, See on <a href="https://news.ycombinator.com/item?id=42914478">Hacker News</a></p>
Couldn't get https://www.wsj.com/opinion/the-trump-tariff-fallout-begins-canada-mexico-vow-retaliation-economic-uncertainty-da522b44: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Deep Research (395 pts)]]></title>
            <link>https://openai.com/index/introducing-deep-research/</link>
            <guid>42913251</guid>
            <pubDate>Mon, 03 Feb 2025 00:06:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/index/introducing-deep-research/">https://openai.com/index/introducing-deep-research/</a>, See on <a href="https://news.ycombinator.com/item?id=42913251">Hacker News</a></p>
Couldn't get https://openai.com/index/introducing-deep-research/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[GarminDB (178 pts)]]></title>
            <link>https://github.com/tcgoetz/GarminDB</link>
            <guid>42912515</guid>
            <pubDate>Sun, 02 Feb 2025 22:27:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/tcgoetz/GarminDB">https://github.com/tcgoetz/GarminDB</a>, See on <a href="https://news.ycombinator.com/item?id=42912515">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a href="https://github.com/tcgoetz/GarminDB/wiki/Screenshots"><img src="https://raw.githubusercontent.com/tcgoetz/GarminDB/master/Screenshots/Screen_Shot_jupyter_daily_sm.jpg" alt="Screen shot of a daily graph"></a></p>
<hr>
<p dir="auto"><a href="https://github.com/tcgoetz/GarminDB/wiki/Screenshots"><img src="https://raw.githubusercontent.com/tcgoetz/GarminDB/master/Screenshots/Screen_Shot_activity_sm.jpg" alt="Screen shot of an activity display"></a></p>
<hr>
<p dir="auto"><a href="https://github.com/tcgoetz/GarminDB/wiki/Screenshots"><img src="https://github.com/tcgoetz/GarminDB/raw/master/Screenshots/Screen_Shot_daily_trend.png" alt="Screen shot of daily trend "></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">GarminDB</h2><a id="user-content-garmindb" aria-label="Permalink: GarminDB" href="#garmindb"></a></p>
<p dir="auto"><a href="https://www.python.org/" rel="nofollow">Python</a> scripts for parsing health data into and manipulating data in a <a href="http://sqlite.org/" rel="nofollow">SQLite</a> database. SQLite is a light weight database that doesn't require a server.</p>
<p dir="auto">What they can do:</p>
<ul dir="auto">
<li>Automatically download and import Garmin daily monitoring files (all day heart rate, activity, climb/descend, stress, and intensity minutes) from the user's Garmin Connect "Daily Summary" page.</li>
<li>Extract sleep, weight, and resting heart rate data from Garmin Connect, store it as JSON files, and import it into the DB.</li>
<li>Download and import activity files from Garmin Connect. A summary table for all activities and more detailed data for some activity types. Lap and record entries for activities.</li>
<li>Summarizing data into a DB with tables containing daily, weekly, monthly, and yearly summaries.</li>
<li>Graph your data from the commandline or with Jupyter notebooks.</li>
<li>Retain downloaded JSON and FIT files so that the DB can be regenerated without connecting to or redownloading data from Garmin Connect.</li>
<li>Export activities as TCX files.</li>
</ul>
<p dir="auto">Once you have your data in the DB, I recommend using a supplied Jupyter notebooks, third party Jupyter notebooks, and/or SQLite browser like <a href="http://sqlitestudio.pl/" rel="nofollow">SQLite Studio</a>, <a href="https://www.heidisql.com/" rel="nofollow">HeidiSQL</a>, or <a href="https://sqlitebrowser.org/" rel="nofollow">DB Browser for SQLite</a> for browsing and working with the data. The scripts create some default <a href="http://www.tutorialspoint.com/sqlite/sqlite_views.htm" rel="nofollow">views</a> in the DBs that make browsing the data easier.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Using It</h2><a id="user-content-using-it" aria-label="Permalink: Using It" href="#using-it"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Releases</h2><a id="user-content-releases" aria-label="Permalink: Releases" href="#releases"></a></p>
<p dir="auto">GarminDb releases are hosted on <a href="https://pypi.org/project/garmindb/" rel="nofollow">PyPI</a>. GarminDb requires <a href="https://www.python.org/" rel="nofollow">Python</a> 3.x. With Python installed, install the latest release with <a href="https://pypi.org/project/pip/" rel="nofollow">pip</a> by running <code>pip install garmindb</code> in a terminal.</p>
<ul dir="auto">
<li>Copy <a href="https://github.com/tcgoetz/GarminDB/raw/master/garmindb/GarminConnectConfig.json.example"><code>GarminConnectConfig.json.example</code></a> to <code>~/.GarminDb/GarminConnectConfig.json</code>, edit it, and add your Garmin Connect username and password and adjust the start dates to match the dates of your data in Garmin Connect.</li>
<li>Starting out: download all of your data and create your db by running <code>garmindb_cli.py --all --download --import --analyze</code> in a terminal.</li>
<li>Incrementally update your db by downloading the latest data and importing it by running <code>garmindb_cli.py --all --download --import --analyze --latest</code> in a terminal.</li>
<li>Ocassionally run <code>garmindb_cli.py --backup</code> to backup your DB files.</li>
</ul>
<p dir="auto">Update to the latest release with <code>pip install --upgrade garmindb</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">From Source</h2><a id="user-content-from-source" aria-label="Permalink: From Source" href="#from-source"></a></p>
<p dir="auto">The scripts are automated with <a href="https://www.gnu.org/software/make/manual/make.html" rel="nofollow">Make</a>. Run the Make commands in a terminal window.</p>
<ul dir="auto">
<li>Git clone GarminDB repo using the <a href="https://github.com/git-guides/git-clone#git-clone-with-ssh">SSH clone method</a>. The submodules require you to use SSH and not HTTPS. Get the command from the green button on the project home page.</li>
<li>Run <code>make setup</code> in the cloned tree to get the scripts ready to process data.</li>
<li>Copy <a href="https://github.com/tcgoetz/GarminDB/raw/master/garmindb/GarminConnectConfig.json.example"><code>GarminConnectConfig.json.example</code></a> to <code>~/.GarminDb/GarminConnectConfig.json</code>, edit it, and add your Garmin Connect username and password and adjust the start dates to match the dates of your data in Garmin Connect.</li>
<li>Run <code>make create_dbs</code> once to fetch and process for you data.</li>
<li>Keep all of your local data up to date by periodically running only one command: <code>make</code>.</li>
</ul>
<p dir="auto">There is more help on <a href="https://github.com/tcgoetz/GarminDB/wiki/Usage">using the program</a> in the wiki.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Jupyter Notebooks</h2><a id="user-content-jupyter-notebooks" aria-label="Permalink: Jupyter Notebooks" href="#jupyter-notebooks"></a></p>
<p dir="auto">Jupyter notebooks for analzing data from the database can be found in the 'Jupyter' directory in the source tree. <a href="https://github.com/tcgoetz/GarminDB/wiki/Related-Projects#jupyter-notebooks">Links</a> to user submitted notebooks can be found in the wiki.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Plugins</h2><a id="user-content-plugins" aria-label="Permalink: Plugins" href="#plugins"></a></p>
<p dir="auto">Plugins allow the user to expand the types of data that are processed and stored in the database. GarminDb already has a number of plugins for handling data from third-party Connect IQ apps and data fields. Read more about plugins <a href="https://github.com/tcgoetz/GarminDbPlugins">here</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Support This Project</h2><a id="user-content-support-this-project" aria-label="Permalink: Support This Project" href="#support-this-project"></a></p>
<p dir="auto">Do you find this project useful? <a href="https://www.buymeacoffee.com/tcgoetz" rel="nofollow"><img src="https://camo.githubusercontent.com/67f1b859c85032255c81e5f861a5f572fa25813c2ea5e7623b7efa91c395e37f/68747470733a2f2f63646e2e6275796d6561636f666665652e636f6d2f627574746f6e732f64656661756c742d6f72616e67652e706e67" alt="Buy Me A Coffee" height="41" width="174" data-canonical-src="https://cdn.buymeacoffee.com/buttons/default-orange.png"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Success Stories</h2><a id="user-content-success-stories" aria-label="Permalink: Success Stories" href="#success-stories"></a></p>
<p dir="auto">Find out who's using GarminDb on what platforms, OSes, and python versions <a href="https://github.com/tcgoetz/GarminDB/wiki/Success-Stories">here</a>. If you're using GarminDB and your scenario isn't listed send me a message or file an issue with your success case.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Notes</h2><a id="user-content-notes" aria-label="Permalink: Notes" href="#notes"></a></p>
<ul dir="auto">
<li>You may get a DB version exception after updating the code, this means that the DB schema was updated and you need to rebuild your DBs by running <code>garmindb_cli.py --rebuild_db</code>. Your DBs will be regenerated from the previously downloaded data files. All of your data will not be redownloaded from Garmin.</li>
<li>The scripts were developed on MacOS. Information or patches on using these scripts on other platforms are welcome.</li>
<li>When a database update finishes, a summary of the data in the DB will be saved to stats.txt. The output includes the date ranges included in the downloaded daily monitoring files and activities. It includes the number of records for daily monitoring, activities, sleep, resting heart rate, weight, etc. Use the summary information to determine if all of your data has been downloaded from Garmin Connect. If not, adjust the dates in GarminConnectConfig.json and runt he download again.</li>
<li>In <code>GarminConnectConfig.json</code> the "steps" element of the "course_views" is list of course ids that per course database views will be generated for. The database view allows you to compare all activities from that course.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Bugs and Debugging</h2><a id="user-content-bugs-and-debugging" aria-label="Permalink: Bugs and Debugging" href="#bugs-and-debugging"></a></p>
<ul dir="auto">
<li>If you have issues, file a bug here on the project. See the Issues tab at the top of the project page. Run <code>make bugreport</code> or <code>garmindb_bug_report.py</code> and include bugreport.txt in your bug report.</li>
<li>Besides errors that appear on the screen, one of the first places to look for more information is the log files (garmindb.log).</li>
<li>If you're having issues with a particular data files, please considering sharing so I can debug it and add support.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Please submit a pull request targeting the develop branch and add your self to the contributors file. Run <code>make flake8</code> at the top level and fix all errors before submitting your pull request.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[F-strings for C++26 proposal [pdf] (134 pts)]]></title>
            <link>https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2024/p3412r0.pdf</link>
            <guid>42912438</guid>
            <pubDate>Sun, 02 Feb 2025 22:19:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2024/p3412r0.pdf">https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2024/p3412r0.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=42912438">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Don't make fun of renowned author Dan Brown (2013) (199 pts)]]></title>
            <link>https://onehundredpages.wordpress.com/2013/06/12/dont-make-fun-of-renowned-dan-brown/</link>
            <guid>42912133</guid>
            <pubDate>Sun, 02 Feb 2025 21:42:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://onehundredpages.wordpress.com/2013/06/12/dont-make-fun-of-renowned-dan-brown/">https://onehundredpages.wordpress.com/2013/06/12/dont-make-fun-of-renowned-dan-brown/</a>, See on <a href="https://news.ycombinator.com/item?id=42912133">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
		<main id="main" role="main">

		
			
<article id="post-945">
	
	<!-- .entry-header -->

	<div>
		<div>
<p><a href="https://onehundredpages.wordpress.com/wp-content/uploads/2013/06/dan-brown.jpg"><img data-attachment-id="946" data-permalink="https://onehundredpages.wordpress.com/2013/06/12/dont-make-fun-of-renowned-dan-brown/dan-brown/" data-orig-file="https://onehundredpages.wordpress.com/wp-content/uploads/2013/06/dan-brown.jpg" data-orig-size="236,363" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}" data-image-title="dan brown" data-image-description="" data-image-caption="" data-medium-file="https://onehundredpages.wordpress.com/wp-content/uploads/2013/06/dan-brown.jpg?w=195" data-large-file="https://onehundredpages.wordpress.com/wp-content/uploads/2013/06/dan-brown.jpg?w=236" alt="dan brown" src="https://onehundredpages.wordpress.com/wp-content/uploads/2013/06/dan-brown.jpg?w=768" srcset="https://onehundredpages.wordpress.com/wp-content/uploads/2013/06/dan-brown.jpg 236w, https://onehundredpages.wordpress.com/wp-content/uploads/2013/06/dan-brown.jpg?w=98&amp;h=150 98w" sizes="(max-width: 236px) 100vw, 236px"></a></p>
<p>Renowned author Dan Brown woke up in his luxurious four-poster bed in his expensive $10 million house – and immediately he felt angry. Most people would have thought that the 48-year-old man had no reason to be angry. After all, the famous writer had a new book coming out. But that was the problem. A new book meant an inevitable attack on the rich novelist by the wealthy wordsmith’s fiercest foes. The critics.</p>
</div>
<p>Renowned author Dan Brown hated the critics. Ever since he had become one of the world’s top renowned authors they had made fun of him. They had mocked bestselling book&nbsp;<i>The Da Vinci Code</i>, successful novel&nbsp;<i>Digital Fortress</i>, popular tome&nbsp;<i>Deception Point</i>, money-spinning volume&nbsp;<i>Angels &amp; Demons</i>&nbsp;and chart-topping work of narrative fiction&nbsp;<i>The Lost Symbol</i>.</p>
<p>The critics said his writing was clumsy, ungrammatical, repetitive and repetitive. They said it was full of unnecessary tautology. They said his prose was swamped in a sea of mixed metaphors. For some reason they found something funny in sentences such as “His eyes went white, like a shark about to attack.”&nbsp;<i>They even say my books are packed with banal and superfluous description</i>, thought the 5ft 9in man. He particularly hated it when they said his imagery was nonsensical. It made his insect eyes flash like a rocket.</p>
<p>Renowned author Dan Brown got out of his luxurious four-poster bed in his expensive $10 million house and paced the bedroom, using the feet located at the ends of his two legs to propel him forwards. He knew he shouldn’t care what a few jealous critics thought. His new book Inferno was coming out on Tuesday, and the 480-page hardback published by Doubleday with a recommended US retail price of $29.95 was sure to be a hit. Wasn’t it?</p>
<div>
<p><i>I’ll call my agent</i>, pondered the prosperous scribe. He reached for the telephone using one of his two hands. “Hello, this is renowned author Dan Brown,” spoke renowned author Dan Brown. “I want to talk to literary agent John Unconvincingname.”</p>
<p>“Mr Unconvincingname, it’s renowned author Dan Brown,” told the voice at the other end of the line. Instantly the voice at the other end of the line was replaced by a different voice at the other end of the line. “Hello, it’s literary agent John Unconvincingname,” informed the new voice at the other end of the line.</p>
<p>“Hello agent John, it’s client Dan,” commented the pecunious scribbler. “I’m worried about new book Inferno. I think critics are going to say it’s badly written.”</p>
<p>The voice at the other end of the line gave a sigh, like a mighty oak toppling into a great river, or something else that didn’t sound like a sigh if you gave it a moment’s thought. “Who cares what the stupid critics say?” advised the literary agent. “They’re just snobs. You have millions of fans.”</p>
<p><i>That’s true</i>, mused the accomplished composer of thrillers that combined religion, high culture and conspiracy theories. His books were read by everyone from renowned politician President Obama to renowned musician Britney Spears. It was said that a copy of&nbsp;<i>The Da Vinci Code</i>&nbsp;had even found its way into the hands of renowned monarch the Queen. He was grateful for his good fortune, and gave thanks every night in his prayers to renowned deity God.</p>
<p>“Think of all the money you’ve made,” recommended the literary agent. That was true too. The thriving ink-slinger’s wealth had allowed him to indulge his passion for great art. Among his proudest purchases were a specially commissioned landscape by acclaimed painter Vincent van Gogh and a signed first edition by revered scriptwriter William Shakespeare.</p>
<p>Renowned author Dan Brown smiled, the ends of his mouth curving upwards in a physical expression of pleasure. He felt much better. If your books brought innocent delight to millions of readers, what did it matter whether you knew the difference between a transitive and an intransitive verb?</p>
<p>“Thanks, John,” he thanked. Then he put down the telephone and perambulated on foot to the desk behind which he habitually sat on a chair to write his famous books on an Apple iMac MD093B/A computer. New book Inferno, the latest in his celebrated series about fictional Harvard professor Robert Langdon, was inspired by top Italian poet Dante. It wouldn’t be the last in the lucrative sequence, either. He had all the sequels mapped out. The Mozart Acrostic. The Michelangelo Wordsearch. The Newton Sudoku.</p>
<p>The 190lb adult male human being nodded his head to indicate satisfaction and returned to his bedroom by walking there. Still asleep in the luxurious four-poster bed of the expensive $10 million house was beautiful wife Mrs Brown. Renowned author Dan Brown gazed admiringly at the pulchritudinous brunette’s blonde tresses, flowing from her head like a stream but made from hair instead of water and without any fish in. She was as majestic as the finest sculpture by Caravaggio or the most coveted portrait by Rodin.&nbsp;<i>I like the attractive woman</i>, thought the successful man.</p>
<p>Perhaps one day, inspired by beautiful wife Mrs Brown, he would move into romantic poetry, like market-leading British rhymester John Keats.<i>That would be good</i>, opined the talented person, and got back into the luxurious four-poster bed. He felt as happy as a man who has something to be happy about and is suitably happy about it.</p>
<p>Inferno by Dan Brown 470pp, Bantam Press, rrp £20</p>

</div>

			
						</div><!-- .entry-content -->

	
	<!-- .entry-footer -->
</article><!-- #post-## -->

			
<!-- #comments -->

				<!-- .navigation -->
	
		
		</main><!-- #main -->
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Costa rican supermarket wins trademark battle against Nintendo (229 pts)]]></title>
            <link>https://ticotimes.net/2025/01/30/david-vs-goliath-costa-rican-super-mario-defeats-nintendo-in-court</link>
            <guid>42911842</guid>
            <pubDate>Sun, 02 Feb 2025 21:07:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ticotimes.net/2025/01/30/david-vs-goliath-costa-rican-super-mario-defeats-nintendo-in-court">https://ticotimes.net/2025/01/30/david-vs-goliath-costa-rican-super-mario-defeats-nintendo-in-court</a>, See on <a href="https://news.ycombinator.com/item?id=42911842">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-td-block-uid="tdi_70">

<p>A small Costa Rican supermarket has emerged victorious from a legal battle against the renowned video game giant, Nintendo. José Mario Alfaro González, owner of “Super Mario” in San Ramón de Alajuela, found himself in an unexpected legal showdown when he attempted to formally register his store’s name. In Costa Rica, the term “super” is often used as shorthand for “supermarket.”</p>



<p>What began as a routine trademark registration process turned into a clash of the titans when Nintendo of America filed an appeal, claiming exclusive rights to the “Super Mario” name. The video game behemoth, creator of iconic titles like Super Mario Bros., asserted that the name was registered under its <a href="https://ticotimes.net/2005/08/26/trademark-use-registration" target="_blank" rel="noreferrer noopener">trademark</a> in various classes, including clothing, games, and accessories.</p>



<p>However, Alfaro intended to register “Super Mario” specifically for international class 35, which covers “supply services for third parties of products from the basic food basket.” “We knew these big companies have vast resources to fight, and we had to think carefully,” said Edgardo Jiménez, Alfaro’s legal representative. “We even considered changing the supermarket’s name to avoid a lawsuit.”</p>



<p>But Alfaro and Jiménez persevered, and on January 21st, the National Registry issued a final notification in favor of “Super Mario” de San Ramón. It was game over for Nintendo, and the small Costa Rican business had won the legal bout. “We refuted all of Nintendo’s arguments, demonstrating their errors and our rightful claim,” Jiménez explained. “Although Nintendo’s registration covers 45 categories, it doesn’t include the specific class for suppliers of basic food products.”</p>



<p>As of yet, <a href="https://www.nintendo.com/us/" target="_blank" rel="noreferrer noopener">Nintendo</a> has not issued an official statement or comment on the outcome. This legal victory is a testament to the determination of a small Costa Rican entrepreneur who stood his ground against a global corporation, proving that even the smallest of players can triumph in the face of seemingly insurmountable odds.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A loophole used by Shein/Temu to ship packages to US tax-free (2024) (122 pts)]]></title>
            <link>https://www.businessinsider.com/shein-temu-de-minimis-tax-loophole-scrutiny-2024-1</link>
            <guid>42911511</guid>
            <pubDate>Sun, 02 Feb 2025 20:31:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.businessinsider.com/shein-temu-de-minimis-tax-loophole-scrutiny-2024-1">https://www.businessinsider.com/shein-temu-de-minimis-tax-loophole-scrutiny-2024-1</a>, See on <a href="https://news.ycombinator.com/item?id=42911511">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component-type="content-lock" data-load-strategy="exclude" id="piano-inline-content-wrapper" data-piano-inline-content-wrapper="" data-user-status="anonymous" data-track-content="" data-post-type="post">
                                    <ul><li>A US tax rule called de minimis has gotten increased attention as Shein and Temu have grown.&nbsp;</li><li>Detractors say it creates unfair competition and allows companies to bypass trade laws.&nbsp;</li><li>Shein and Temu representatives said they support de minimis reform — as long as it's fair.&nbsp;</li></ul><p>As Shein and Temu <a target="_blank" href="https://www.businessinsider.com/temu-passes-shein-in-traffic-purchasing-visits-hasnt-surpassed-amazon-2023-9" data-analytics-product-module="body_link" rel="">grow their foothold in the US</a>, analysts and lawmakers are paying close attention to both companies' use of a special tax rule that some say should be eliminated completely.</p><!-- Excluded mobile ad on desktop --><p>The provision, known as "de minimis," allows importers to avoid paying duty and tax on shipments that are going to individual consumers and are worth less than $800 in total.</p><p>Shippers using de minimis also do not have to provide as much information to US Customs and Border Protection as shippers using more traditional methods would.<strong> </strong>Opponents of the rule argue it creates unfair competition, and the lack of in-depth screening could allow for the <a target="_blank" href="https://www.businessinsider.com/shein-and-temu-skirt-import-tariffs-us-lawmakers-fast-fashion-2023-6" data-analytics-product-module="body_link" rel="">import of goods</a> containing banned materials like <a target="_blank" href="https://www.businessinsider.com/temu-blamed-shoppers-for-buying-forced-labor-lawmakers-say-2023-6" data-analytics-product-module="body_link" rel="">cotton from Xinjiang</a>, where forced labor is common.</p><!-- Excluded mobile ad on desktop --><p>De minimis has been around since 1938 when Congress introduced the rule in order to speed up the processing of items that were so cheap that they would not generate significant tax revenue for the government. The limit for eligible items has been raised many times over the years, most recently going up to $800 from $200 in 2016.</p><p>Two bills aimed at reforming de minimis were introduced in Congress last June. One, introduced by Rep. Earl Blumenauer, aims to limit companies in "nonmarket" economies <strong>—</strong>&nbsp;or countries where prices do not follow market dynamics, such as China and Russia<strong> — </strong>and countries on priority watch lists from using de minimis shipments. The other bill, from Sen. Bill Cassidy and Sen. Tammy Baldwin, would ban de minimis shipments from China.</p><p>But some experts say it's unlikely the provision will go away anytime soon.</p><!-- Excluded mobile ad on desktop --><p>A US trade law and policy expert interviewed by Bank of America analysts put the odds of Blumenauer's bill passing the Senate at more than 50% but said it had a roughly 30% chance of passing overall "given issues in the House," according to a research note the bank published on January 5. The same expert said the bill introduced by Cassidy and Baldwin was unlikely to pass in Congress.</p><p>"While many retailers are likely in favor of these bills, groups like the US Chamber of Commerce or the Express Shippers Association may not be in favor of them since they generally are looking for fewer tariffs, not more," UBS analyst Jay Sole wrote in a research note in August. "Also, direct carriers, direct importers, and logistics companies may not be in favor of a change since their businesses may benefit from the $800 rule."</p><p>The provision is more likely to change — a prospect that both Shein and Temu have said they support.</p><!-- Excluded mobile ad on desktop --><h2>A 'paradigm shift'</h2><p>Shein and Temu are not the only companies to use the de minimis provision, but they have gained the attention of advocacy groups and lawmakers in part because of how quickly they have grown in the last year.</p>
                          
                          
                          
                                 
                          <p>Shein, which was founded in China in 2008 but moved its headquarters to Singapore in late 2021, <a target="_blank" href="https://www.businessinsider.com/what-is-shein-billion-dollar-fast-fashion-company-explained-2023-7" data-analytics-product-module="body_link" rel=""><u>filed confidentially</u></a> for a US IPO in November. It's reportedly looking for a valuation of $90 billion.</p><p>Temu, meanwhile, is owned by Chinese commerce giant Pinduoduo Holdings and headquartered in Boston. It has grown extremely fast since launching in the US in September 2022, outpacing many more established e-commerce companies in terms of app downloads and <a target="_blank" href="https://www.insiderintelligence.com/content/shoppers-spend-nearly-twice-long-on-temu-s-app-than-its-competitors" data-analytics-product-module="body_link" rel=" nofollow"><u>engagement</u></a>.</p><!-- Excluded mobile ad on desktop -->
                            <figure data-type="img" data-e2e-name="image-figure-image" data-media-container="image" itemscope="" itemtype="https://schema.org/ImageObject">
                            
                            
                            
                              <div>
                          
                            
                            <meta itemprop="contentUrl" content="https://i.insider.com/64da3c621f51cc001968df91">
                            <p><img src="data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg xmlns='http://www.w3.org/2000/svg' width='1' height='1'/%3E" data-content-type="image/jpeg" data-srcs="{&quot;https://i.insider.com/64da3c621f51cc001968df91&quot;:{&quot;contentType&quot;:&quot;image/jpeg&quot;,&quot;aspectRatioW&quot;:4032,&quot;aspectRatioH&quot;:3024}}" alt="flat lay image of Temu package, checkered pants, and pink heels">
                          </p></div>
                            
                            <span>
                                  <figcaption data-e2e-name="image-caption">
                                    Goods bought on Temu.
                                    
                            <span data-e2e-name="image-source" itemprop="creditText">
                            
                            Jennifer Ortakales Dawkins/Insider
                            
                            </span>
                                  </figcaption>
                                </span>
                            </figure>
                          <p>Shipping consultancy ShipMatrix estimates that Shein and Temu each ship more than a million packages to the US daily.</p><p>An interim 2023 report from the US House Select Committee on the Chinese Communist Party said that Shein and Temu "likely" account for more than 30% of all shipments made to the US under the de minimis provision. It added that almost 50% of all de minimis shipments to the US come from China.</p><p>Shipping directly from manufacturers to individual consumers helps Shein and Temu keep prices low. Wired reported in May that the <a target="_blank" href="https://www.wired.com/story/temu-is-losing-millions-of-dollars-to-send-you-cheap-socks/" data-analytics-product-module="body_link" rel=" nofollow"><u>average order on Temu was about $25</u></a>, while <a target="_blank" href="https://www.barrons.com/articles/shein-clothing-shopping-retail-challenger-16d512db" data-analytics-product-module="body_link" rel=" nofollow"><u>Shein's was about $70 as of April</u></a>, according to Barron's.</p><!-- Excluded mobile ad on desktop --><p>The Bank of America analysts wrote in their research note that if de minimis were to change or go away completely, it could be disruptive for these companies.</p><p>"Changes to the exemption could create a paradigm shift for retailers like Temu and Shein," Bank of America analysts wrote in the note. "The discount prices charged by both retailers are aided by the absence of duties and the lack of trade and customs infrastructure that would be required if this regulation did not exist."</p><p>The analysts added that they "wouldn't expect price increases to be well-accepted by the consumer."</p><!-- Excluded mobile ad on desktop --><p>Representatives for Shein and Temu pushed back on the notion that their businesses have relied on de minimis to grow and said that they support reforms to the provision if they are fair.</p><p>"Our success has come from our ability to leverage on-demand technology to bring the latest styles to customers efficiently and at an affordable price," a spokesperson for Shein said.</p><p>In a July letter to the American Apparel and Footwear Association, Shein's executive vice chairman, Donald Tang wrote that the company supports "responsible reform of the de minimis exemption."</p><!-- Excluded mobile ad on desktop --><p>"SHEIN believes the de minimis framework should be reformed to create a more level, transparent playing field — one where all retailers play by the same rules, and where the rules are applied evenly and equally, regardless of where a company is based or ships from," he wrote.</p><p>A spokesperson for Temu said its "supply-chain efficiencies and operational proficiencies" have been the primary drivers of its rapid growth.</p><p>"We are open to and supportive of any policy adjustments made by legislators that align with consumer interests. We believe that as long as these policies are fair, they won't influence the outcomes of competitive business dynamics," they said. "We also see such reforms as potential avenues to alleviate concerns among various stakeholders, fostering greater comprehension and emphasizing the significance of each player in the market ecosystem."</p><!-- Excluded mobile ad on desktop --><p>The two companies also detailed their practices for remaining compliant with import laws. The Shein spokesperson said that the company has a "zero-tolerance policy for forced labor" and requires its manufacturers to only source cotton from approved regions. The Temu spokesperson said it requires its sellers to sign an agreement that they will maintain compliant business practices, and that "the use of forced, penal, or child labor is strictly prohibited."</p><h2>'If you can't beat 'em, join 'em'</h2><p>Other <a target="_blank" href="https://www.nytimes.com/2023/11/04/business/dealbook/us-retailers-say-an-old-trade-law-puts-them-at-a-disadvantage.html#:~:text=The%20rule%2C%20known%20as%20de,are%20textile%20and%20apparel%20products." data-analytics-product-module="body_link" rel=" nofollow"><u>opposition to de minimis has come from US retailers and trade groups</u></a> concerned about their ability to compete with companies shipping cheaply from China. Many retailers manufacture their products abroad and then ship them to the US in large quantities, meaning they can't use the de minimis provision as easily as Shein and Temu have.</p><p>"The textile industry is probably the biggest proponent against de minimis because they've got all their mills, they're making shirts, pants," said Steve Story, executive vice president at Apex Logistics International, which specializes in de minimis. "I can order a shirt from China at a quarter of the price, or I can go to Macy's, Nordstrom's, or Walmart and buy it."</p><!-- Excluded mobile ad on desktop --><p>He said that companies like Shein and Temu do not have to serve as the importers of record and are therefore not responsible for certifying the origin of items on their platforms.</p><p>"However, they're facilitating the sale for export to the United States, so they do have a financial interest in the merchandise," Story said. He serves on several committees exploring possible changes to de minimis, including a customs bond that would allow customs to make a claim in the event of a policy violation.</p><p>While many apparel retailers have spoken out against de minimis, an expert interviewed by UBS' Sole said that many retailers outside clothing are taking an "if you can't beat 'em, join 'em" approach and exploring ways they, too, can use de minimis to lower their costs instead of changing the rule altogether.</p><!-- Excluded mobile ad on desktop --><p>Satish Jindel, founder and president of ShipMatrix, said companies could lean on de minimis to lower their own prices.</p><p>"The companies who think it is not helping them should be looking at it and saying, how do I utilize that opportunity to revise and refine my business model to lower the cost of operating," Jindel said.</p><p><em>Got a tip? Contact this reporter at mstone@insider.com, mlstone@protonmail.com, or on the secure messaging app Signal at (646) 889-2143 using a non-work phone.&nbsp;</em></p>
                        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Waydroid – Android in a Linux container (355 pts)]]></title>
            <link>https://waydro.id/</link>
            <guid>42911042</guid>
            <pubDate>Sun, 02 Feb 2025 19:29:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://waydro.id/">https://waydro.id/</a>, See on <a href="https://news.ycombinator.com/item?id=42911042">Hacker News</a></p>
<div id="readability-page-1" class="page">
  <!-- ====== Header Start ====== -->
  
  <!-- ====== Header End ====== -->
  <!-- ====== Hero Start ====== -->
  <div id="home">
          <div data-wow-delay=".2s">
            <p><img src="https://waydro.id/assets/images/hero/waydroid_white_tb.png"></p><p> A container-based approach to boot a full Android system on regular GNU/Linux systems running Wayland based desktop environments. </p>
            <ul>
              <li>
                <a href="#docs" rel="nofollow noopener"> Documentation</a>
              </li>
              <li>
                <a href="#about" rel="nofollow noopener"> Learn More <i></i>
                </a>
              </li>
            </ul>
          </div>
          <div data-wow-delay=".25s">
            <div id="carouselExampleCaptions" data-bs-ride="carousel">
                <div>
                  <p><img src="https://waydro.id/assets/images/hero/main_landing.jpeg"></p><div>
                    <h5>Full Integration Of Android on Linux</h5>
                    <p>Using Waydroid's Multi-Window Mode</p>
                  </div>
                </div>
                <div>
                  <p><img src="https://waydro.id/assets/images/hero/Group%201.png"></p><div>
                    <h5>Mobile Linux Integrations</h5>
                    <p>Brought To Life With Waydroid</p>
                  </div>
                </div>
                <div>
                  <p><img src="https://waydro.id/assets/images/hero/clean_desktop.jpeg"></p><div>
                    <h5>Bring Your Desktop To Life</h5>
                    <p>With Waydroids Fullscreen Mode For Desktops &amp; Kiosks</p>
                  </div>
                </div>
              </div>
            <p><img src="https://waydro.id/assets/images/hero/dotted-shape.svg" alt="shape">
            <img src="https://waydro.id/assets/images/hero/dotted-shape.svg" alt="shape">
          </p></div>
        </div>
  <!-- ====== Hero End ====== -->
  <!-- ====== Features Start ====== -->
  <div id="features">
      <div>
            <p><span>Features</span></p><h2>Main Features of Waydroid</h2>
            <p> Waydroid uses Linux namespaces (user, pid, uts, net, mount, ipc) to run a full Android system in a container and provide Android applications on any GNU/Linux-based platform (arm, arm64, x86, x86_64). The Android system inside the container has direct access to needed hardware through LXC and the binder interface. </p>
          </div>
      <div>
        <!-- Feature Start -->
        <div data-wow-delay=".1s">
              <h3>Free and Open-Source</h3>
              <p> The Project is completely free and open-source, currently our repo is hosted on <a target="BLANK" href="https://github.com/waydroid">Github.</a> </p>
            </div>
        <!-- Feature Start -->
        <div data-wow-delay=".15s">
              <h3>Full app integration</h3>
              <p> Waydroid integrated with Linux adding the Android apps to your linux applications folder. </p>
            </div>
        <!-- Feature Start -->
        <div data-wow-delay=".2s">
              <h3>Multi-window mode</h3>
              <p> Waydroid expands on Android freeform window definition, adding a number of features. </p>
            </div>
        <!-- Feature Start -->
        <div data-wow-delay=".25s">
              <h3>Full UI Mode</h3>
              <p> For gaming and full screen entertainment, Waydroid can also be run to show the full Android UI. </p>
            </div>
        <!-- Feature Start -->
        <div data-wow-delay=".1s">
              <h3>Near native performance</h3>
              <p> Get the best performance possible using wayland and AOSP mesa, taking things to the next level </p>
            </div>
        <!-- Feature Start -->
        <div data-wow-delay=".1s">
              <h3>Active community</h3>
              <p> Find out what all the buzz is about and explore all the possibilities Waydroid could bring </p>
            </div>
      </div>
    </div>
  <!-- ====== Features End ====== -->
  <!-- ====== About Start ====== -->
  <div data-wow-delay=".2s" id="about">
        <div>
            <p><span>About Us</span></p><h2>Get your favourite Android Apps on Linux.</h2>
            <p> Waydroid brings all the apps you love, right to your desktop, working side by side your Linux applications.<br> The Android inside the container has direct access to needed hardwares.<br> The Android runtime environment ships with a minimal customized Android system image based on <a href="https://lineageos.org/" target="blank">LineageOS</a>. The used image is currently based on Android 11 </p>
            <p><a href="#install"> Install Instructions <i></i>
            </a>
          </p></div>
        <p><img src="https://waydro.id/assets/images/hero/photo1628875295.jpeg" alt="about-image">
        </p>
      </div>
  <!-- ====== About End ====== -->
  <!-- ====== Docs Start ====== -->
  <div id="docs">
            <p><span>Docs</span></p><h2>Our Documentation</h2>
            <p> Our documentation site can be found at <a target="BLANK" href="https://docs.waydro.id/"> docs.waydro.id</a>
            </p>
            <h2>Bugs &amp; Reports</h2>
            <p> Bug Reports can be filed on our repo <a target="BLANK" href="https://github.com/waydroid/waydroid/issues"> Github Repo</a>
            </p>
            <h2>Project Development</h2>
            <p> Our development repositories are hosted on <a target="BLANK" href="https://github.com/waydroid/"> Github</a>
            </p>
            <h2>How to Install ?</h2>
            <p> Please refer to our <a target="BLANK" href="https://docs.waydro.id/usage/install-on-desktops"> installation docs</a> for complete installation guide. </p>
            <h2>Manual Image Download</h2>
            <p>You can also manually download our images from</p>
            <p><a target="BLANK" href="https://sourceforge.net/projects/waydroid">
              <img src="https://waydro.id/assets/images/logo/sf.png" alt="sourceforge logo"> SourceForge <i></i>
            </a>
          </p></div>
  <!-- ====== Docs End ====== -->
  <!-- ====== FAQ Start ====== -->
  <section id="install">
    <p><img src="https://waydro.id/assets/images/faq/shape.svg" alt="shape">
    </p>
    <div>
      <div>
            <p><span>Instructions</span></p><h2>Quick install reference</h2>
            <p>For systemd distributions</p>
          </div>
      <div>
        
        <form>
          <p>Waydroid supports most common architectures (ARM, ARM64, x86 &amp; x86_64 CPUs)</p>
          <p>Waydroid uses Android's mesa integration for passthrough, and that enables support to most ARM/ARM64 SOCs on the mobile side, and Intel/AMD GPUs for the PC side. For Nvidia GPUs (except tegra) and VMs, we recommend using <a href="https://docs.waydro.id/faq/get-waydroid-to-work-through-a-vm">software-rendering</a> </p>
        </form>
      </div>
      <div>
        <p>Follow the install instructions for your linux distribution. You can find a list in our <a href="https://docs.waydro.id/usage/install-on-desktops">docs</a>.
        </p>
      </div>
      <div>
        <p>After installing you should start the waydroid-container service, if it was not started automatically:</p>
        <div>
          <figure>
            <pre><code data-lang="">sudo systemctl enable --now waydroid-container
</code></pre>
          </figure>
        </div>
      </div>
      <div>
        <p>Then launch Waydroid from the applications menu and follow the first-launch wizard.</p>
      </div>
      <div>
        <p>If prompted, use the following links for System OTA and Vendor OTA:</p>
        <div>
          <pre><code data-lang=""><p>https://ota.waydro.id/system</p></code></pre>
        </div>
        <div>
          <pre><code data-lang=""><p>https://ota.waydro.id/vendor</p></code></pre>
        </div>
      </div>
      <div>
        <p>For further instructions, please visit the docs site <a target="BLANK" href="https://docs.waydro.id/">here</a>
          </p>
      </div>
    </div>
  </section>
  <!-- ====== FAQ End ====== -->
  <div id="wdlinux">
              <p><img src="https://waydro.id/assets/images/hero/Computer_wd.png">
              </p>
              <div>
                
                <h3><span>Latest Beta </span><span> 01.30.2023</span></h3>
                <p> We have started creating a few fully-integrated distros in order to demonstrate some of the possibilities that Waydroid can help achieve. <br> Each of the distros we produce will also showcase some of the work from our growing community of contributors. <br> Our initial alpha releases of this integration started with Ubuntu 20.04 (focal) and is now on Ubuntu 22.04 (jammy) as well as Debian 12 (bookworm), and includes many added tools and scripts to help open up what is possible. </p>
                <div>
                  
                  <form>
                    <p>Waydroid-Linux currently only supports x86_64 CPUs (Intel/AMD)</p>
                    <p>Waydroid-Linux uses Android's mesa integration for passthrough, and that restricts support to Intel and AMD GPUs<br> For Nvidia GPUs and VMs, we recommend using <a href="https://docs.waydro.id/faq/get-waydroid-to-work-through-a-vm">software-rendering</a> </p>
                    <div id="sources">
                      <p>We have been working with a number of devs on Waydroid-Linux, and have been creating or contributing to a number of projects for it. <br> Here are just a few of the projects we've been using:</p>
                      <ol>
                        <li>
                          <div>
                            <p>Waydroid-Linux Tools</p><p> A few scripts, configs, and themes for the Waydroid-Linux builds </p>
                          </div>
                        </li>
                        <li>
                          <div>
                            <p>Waydroid-Settings</p><p> A GTK app written in Python to control Waydroid settings and expand with scripts (shell/py) </p>
                          </div>
                        </li>
                        <li>
                          <div>
                            <p>Penguins-Eggs</p><p> A tool used for packaging and installation of various Linux distros </p>
                            
                          </div>
                        </li>
                        <li>
                          
                        </li>
                      </ol>
                    </div>
                  </form>
                </div>
                <!---------->
                <details>
                  <summary>Live Mode Info</summary><br> Due to how Waydroid uses LXC and kernel modules for the binder, it will not work while running in live mode and must be installed before working properly.&nbsp; <br>Make sure to check the readme for the .iso, as it contains the specifics needed for each build.&nbsp;
                </details>
                <br>
                <!-- Button trigger modal -->
                <!-- Modal -->
                
                <!---------->
              </div>
            </div>
  <!-- ====== Team Start ====== -->
  <div id="team">
      <div>
            <p><span>Our Team</span></p><h2>Meet The Team</h2>
            <p>Here are the members of our team</p>
          </div>
      <div>
        <div data-wow-delay=".1s">
            
            <p>
              <h5>Erfan Abdi</h5>
              <p4>@erfanoabdi</p4>
              <h6 contenteditable="true">Lead Developer</h6>
            </p>
            
          </div>
        <div data-wow-delay=".15s">
            
            <p>
              <h5>Alessandro Astone</h5>
              <p4>@aleasto</p4>
              <h6>Developer</h6>
            </p>
            
          </div>
        <div data-wow-delay=".15s">
            
            <p>
              <h5>Jon West</h5>
              <p4>@electrikjesus</p4>
              <h6>Developer</h6>
            </p>
            
          </div>
        <div data-wow-delay=".2s">
            
            <p>
              <h5>Radek Błędowski</h5>
              <p4>@RKBDI</p4>
              <h6>Designer</h6>
            </p>
            
          </div>
      </div>
    </div>
  <div id="donate">
          <h2>Help Us Grow</h2>
          <br>
          <h4> Waydroid is now on Open-Collective </h4>
          <h4> We are now accepting donations and sponsorships through Open-Collective. </h4>
          <div>
            <p><a href="https://opencollective.com/waydroid/donate" target="_blank">
                <img src="https://opencollective.com/webpack/donate/button@2x.png?color=blue">
              </a>
            </p>
            <!-- /.animated scroll -->
          </div>
          <!-- /.banner-child -->
          <!-- /.banner-child -->
        </div>
  <!-- ====== Team End ====== -->
  <!-- ====== Contact Start ====== -->
  <!-- ====== Contact End ====== -->
  <!-- ====== Footer Start ====== -->
  
  <!-- ====== Footer End ====== -->
  <!-- ====== Back To Top Start ====== -->
  <span>
    <i> </i>
  </span>
  <!-- ====== Back To Top End ====== -->
  <!-- ====== All Javascript Files ====== -->
  
  
  
  
  



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Young, Inexperienced Engineers Aiding Elon Musk’s Government Takeover (106 pts)]]></title>
            <link>https://www.wired.com/story/elon-musk-government-young-engineers/</link>
            <guid>42910910</guid>
            <pubDate>Sun, 02 Feb 2025 19:12:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/elon-musk-government-young-engineers/">https://www.wired.com/story/elon-musk-government-young-engineers/</a>, See on <a href="https://news.ycombinator.com/item?id=42910910">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Elon Musk’s <a href="https://www.wired.com/story/elon-musk-twitter-playbook-federal-government/">takeover</a> of federal government infrastructure is ongoing, and at the center of things is a coterie of engineers who are barely out of—and in at least one case, purportedly still in—college. Most have connections to Musk, and at least two have connections to Musk’s longtime associate Peter Thiel, a cofounder and chair of the analytics firm and government contractor Palantir who has long <a data-offer-url="https://www.cato-unbound.org/2009/04/13/peter-thiel/education-libertarian/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.cato-unbound.org/2009/04/13/peter-thiel/education-libertarian/&quot;}" href="https://www.cato-unbound.org/2009/04/13/peter-thiel/education-libertarian/" rel="nofollow noopener" target="_blank">expressed</a> opposition to democracy.</p><p>WIRED has identified six young men—all apparently between the ages of 19 and 24, according to public databases, their online presences, and other records—who have little to no government experience and are now playing critical roles in Musk’s so-called <a href="https://www.wired.com/story/doge-elon-musk/">Department of Government Efficiency</a> (DOGE) project, tasked by executive order with “modernizing Federal technology and software to maximize governmental efficiency and productivity.” The engineers all hold nebulous job titles within DOGE, and at least one appears to be working as a volunteer.</p><p>The engineers are Akash Bobba, Edward Coristine, Luke Farritor, Gautier Cole Killian, Gavin Kliger, and Ethan Shaotran. None have responded to requests for comment from WIRED. Representatives from OPM, GSA, and DOGE did not respond to requests for comment.</p><p>Already, Musk’s lackeys have taken control of the <a href="https://www.wired.com/story/elon-musk-lackeys-office-personnel-management-opm-neuralink-x-boring-stalin/">Office of Personnel Management</a> (OPM) and <a href="https://www.wired.com/story/elon-musk-lackeys-general-services-administration/">General Services Administration</a> (GSA), and have <a data-offer-url="https://www.nytimes.com/2025/02/01/us/politics/elon-musk-doge-federal-payments-system.html" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.nytimes.com/2025/02/01/us/politics/elon-musk-doge-federal-payments-system.html&quot;}" href="https://www.nytimes.com/2025/02/01/us/politics/elon-musk-doge-federal-payments-system.html" rel="nofollow noopener" target="_blank">gained access</a> to the Treasury Department’s payment system, potentially allowing him access to a vast range of sensitive information about tens of millions of citizens, businesses, and more. On Sunday, CNN <a data-offer-url="https://www.cnn.com/2025/02/02/politics/usaid-officials-leave-musk-doge/index.html" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.cnn.com/2025/02/02/politics/usaid-officials-leave-musk-doge/index.html&quot;}" href="https://www.cnn.com/2025/02/02/politics/usaid-officials-leave-musk-doge/index.html" rel="nofollow noopener" target="_blank">reported</a> that DOGE personnel attempted to improperly access classified information and security systems at the <a href="https://www.wired.com/story/us-government-websites-are-disappearing-in-real-time/">US Agency for International Development</a> and that top USAID security officials who thwarted the attempt were subsequently put on leave. The Associated Press <a href="https://apnews.com/article/doge-musk-trump-classified-information-usaid-security-35101dee28a766e0d9705e0d47958611">reported</a> that DOGE personnel had indeed accessed classified material.</p><p>“What we're seeing is unprecedented in that you have these actors who are not really public officials gaining access to the most sensitive data in government,” says Don Moynihan, a professor of public policy at the University of Michigan. “We really have very little eyes on what's going on. Congress has no ability to really intervene and monitor what's happening because these aren't really accountable public officials. So this feels like a hostile takeover of the machinery of governments by the richest man in the world.”</p><p>Bobba has attended UC Berkeley, where he was in the prestigious Management, Entrepreneurship, and Technology program. According to a copy of his now-deleted LinkedIn obtained by WIRED, Bobba was an investment engineering intern at the Bridgewater Associates hedge fund as of last spring and was previously an intern at both Meta and Palantir. He was a featured guest on a since-deleted podcast with Aman Manazir, an engineer who interviews engineers about how they landed their dream jobs, where he <a data-offer-url="https://podtail.com/podcast/lift-off-with-aman-manazir/-3-my-secret-system-for-landing-a-meta-internship-/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://podtail.com/podcast/lift-off-with-aman-manazir/-3-my-secret-system-for-landing-a-meta-internship-/&quot;}" href="https://podtail.com/podcast/lift-off-with-aman-manazir/-3-my-secret-system-for-landing-a-meta-internship-/" rel="nofollow noopener" target="_blank">talked about those experiences last June</a>.</p><p>Coristine, as WIRED <a href="https://www.wired.com/story/elon-musk-government-tech-workers-gsa-tts/">previously reported</a>, appears to have recently graduated from high school and to have been enrolled at Northeastern University. According to a copy of his résumé obtained by WIRED, he spent three months at Neuralink, Musk’s brain-computer interface company, last summer.e</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Both Bobba and Coristine are listed in internal OPM records reviewed by WIRED as “experts” at OPM, reporting directly to Amanda Scales, its new chief of staff. Scales previously worked on talent for xAI, Musk’s artificial intelligence company, and as part of Uber’s talent acquisition team, per LinkedIn. Employees at GSA tell WIRED that Coristine has appeared on calls where workers were made to go over code they had written and justify their jobs. WIRED <a href="https://www.wired.com/story/elon-musk-government-tech-workers-gsa-tts/">previously reported</a> that Coristine was added to a call with GSA staff members using a nongovernment Gmail address. Employees were not given an explanation as to who he was or why he was on the calls.</p><p>Farritor, who per sources has a working GSA email address, is a former intern at SpaceX, Musk’s space company, and <a data-offer-url="https://www.businesswire.com/news/home/20240320742346/en/Thiel-Foundation-Announces-Next-Thiel-Fellow-Class" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.businesswire.com/news/home/20240320742346/en/Thiel-Foundation-Announces-Next-Thiel-Fellow-Class&quot;}" href="https://www.businesswire.com/news/home/20240320742346/en/Thiel-Foundation-Announces-Next-Thiel-Fellow-Class" rel="nofollow noopener" target="_blank">currently</a> a <a data-offer-url="https://thielfellowship.org/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://thielfellowship.org/&quot;}" href="https://thielfellowship.org/" rel="nofollow noopener" target="_blank">Thiel Fellow</a> after, according to his LinkedIn, dropping out of the University of Nebraska—Lincoln. While in school, he was part of an award-winning team that <a data-offer-url="https://news.unl.edu/article-2" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://news.unl.edu/article-2&quot;}" href="https://news.unl.edu/article-2" rel="nofollow noopener" target="_blank">deciphered portions of an ancient Greek scroll</a>.</p><p>Kliger, whose LinkedIn lists him as a special adviser to the director of OPM and who is listed in internal records reviewed by WIRED as a special adviser to the director for information technology, attended UC Berkeley until 2020; most recently, according to his LinkedIn, he worked for the AI company Databricks. His <a data-offer-url="https://substack.com/@weeklybyte" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://substack.com/@weeklybyte&quot;}" href="https://substack.com/@weeklybyte" rel="nofollow noopener" target="_blank">Substack</a> includes a post titled “The Curious Case of Matt Gaetz: How the Deep State Destroys Its Enemies,” as well as another titled “Pete Hegseth as Secretary of Defense: The Warrior Washington Fears.”</p><p>Killian, also known as Cole Killian, has a working email associated with DOGE, where he is currently listed as a volunteer, according to internal records reviewed by WIRED. According to a copy of his now-deleted résumé obtained by WIRED, he attended McGill University through at least 2021 and graduated high school in 2019. An archived copy of his now-deleted personal website indicates that he worked as an engineer at Jump Trading, which specializes in algorithmic and high-frequency financial trades.</p><p>Shaotran told <a data-offer-url="https://www.businessinsider.com/harvard-senior-balances-college-ai-startup-productivity-enterprise-openai-coding-2024-8?utm_medium=social&amp;utm_source=linkedin&amp;utm_campaign=insider-sf" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.businessinsider.com/harvard-senior-balances-college-ai-startup-productivity-enterprise-openai-coding-2024-8?utm_medium=social&amp;utm_source=linkedin&amp;utm_campaign=insider-sf&quot;}" href="https://www.businessinsider.com/harvard-senior-balances-college-ai-startup-productivity-enterprise-openai-coding-2024-8?utm_medium=social&amp;utm_source=linkedin&amp;utm_campaign=insider-sf" rel="nofollow noopener" target="_blank">Business Insider</a> in September that he was a senior at Harvard studying computer science and also the founder of an OpenAI-backed startup, Energize AI. Shaotran was the <a data-offer-url="https://x.com/xai/status/1846989686549696900" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://x.com/xai/status/1846989686549696900&quot;}" href="https://x.com/xai/status/1846989686549696900" rel="nofollow noopener" target="_blank">runner-up in a hackathon held by xAI</a>, Musk’s AI company. In the Business Insider article, Shaotran says he received a $100,000 grant from OpenAI to build his scheduling assistant, Spark.</p><hr><p><strong>Got a Tip?</strong></p><p><em>Are you a current or former employee with the Office of Personnel Management or another government agency impacted by Elon Musk? We’d like to hear from you. Using a nonwork phone or computer, contact Vittoria Elliott at <a href="mailto:vittoria_elliott@wired.com" target="_blank">vittoria_elliott@wired.com</a> or securely at velliott88.18 on Signal.</em></p><hr><p>“To the extent these individuals are exercising what would otherwise be relatively significant managerial control over two very large agencies that deal with very complex topics,” says Nick Bednar, a professor at University of Minnesota’s school of law, “it is very unlikely they have the expertise to understand either the law or the administrative needs that surround these agencies.”</p><p>Sources tell WIRED that Bobba, Coristine, Farritor, and Shaotran all currently have working GSA emails and A-suite level clearance at the GSA, which means that they work out of the agency’s top floor and have access to all physical spaces and IT systems, according a source with knowledge of the GSA’s clearance protocols. The source, who spoke to WIRED on the condition of anonymity because they fear retaliation, says they worry that the new teams could bypass the regular security clearance protocols to access the agency’s sensitive compartmented information facility, as the <a data-offer-url="https://www.cnn.com/2025/01/21/politics/trump-temporary-security-clearances/index.html" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.cnn.com/2025/01/21/politics/trump-temporary-security-clearances/index.html&quot;}" href="https://www.cnn.com/2025/01/21/politics/trump-temporary-security-clearances/index.html" rel="nofollow noopener" target="_blank">Trump administration has already granted temporary security clearances</a> to unvetted people.</p><p>This is in addition to Coristine and Bobba being listed as “experts” working at OPM. Bednar says that while staff can be loaned out between agencies for special projects or to work on issues that might cross agency lines, it’s not exactly common practice.</p><p>“This is consistent with the pattern of a lot of tech executives who have taken certain roles of the administration,” says Bednar. “This raises concerns about regulatory capture and whether these individuals may have preferences that don’t serve the American public or the federal government.”</p><p><em>Additional reporting by Zoë Schiffer and Tim Marchman.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Legacy of Lies in Alzheimer's Science (185 pts)]]></title>
            <link>https://www.nytimes.com/2025/01/24/opinion/alzheimers-fraud-cure.html</link>
            <guid>42910829</guid>
            <pubDate>Sun, 02 Feb 2025 19:00:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2025/01/24/opinion/alzheimers-fraud-cure.html">https://www.nytimes.com/2025/01/24/opinion/alzheimers-fraud-cure.html</a>, See on <a href="https://news.ycombinator.com/item?id=42910829">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2025/01/24/opinion/alzheimers-fraud-cure.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Everyone knows your location: tracking myself down through in-app ads (1389 pts)]]></title>
            <link>https://timsh.org/tracking-myself-down-through-in-app-ads/</link>
            <guid>42909921</guid>
            <pubDate>Sun, 02 Feb 2025 17:07:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://timsh.org/tracking-myself-down-through-in-app-ads/">https://timsh.org/tracking-myself-down-through-in-app-ads/</a>, See on <a href="https://news.ycombinator.com/item?id=42909921">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
            <p>Recently I read about a <a href="https://www.404media.co/hackers-claim-massive-breach-of-location-data-giant-threaten-to-leak-data/?ref=timsh.org" rel="noreferrer">massive geolocation data leak from Gravy Analytics</a>, which exposed more than 2000 apps, both in AppStore and Google Play, that secretly collect geolocation data without user consent. Oftentimes, even without developers` knowledge. </p><p>I looked into the list (<a href="https://docs.google.com/spreadsheets/d/1Ukgd0gIWd9gpV6bOx2pcSHsVO6yIUqbjnlM4ewjO6Cs/edit?gid=1257088277&amp;ref=timsh.org#gid=1257088277" rel="noreferrer">link here</a>) and found at least 3 apps I have installed on my iPhone. Take a look for yourself! <br>This made me come up with an idea to track myself down externally, e.g. to buy my geolocation data leaked by some application. </p><h3 id="tldr">TL;DR</h3><p>After more than couple dozen hours of trying, here are the main takeaways: </p><ol><li>I found a couple requests sent by my phone with <strong>my precise location </strong>+ 5 requests that leak <strong>my IP address</strong>, which can be turned into geolocation using reverse DNS. </li><li>Learned a lot about the RTB (real-time bidding) auctions and OpenRTB protocol and was shocked by the amount and types of data sent with the bids to ad exchanges. </li><li>Gave up on the idea to buy my location data from a data broker or a tracking service, because I don't have a big enough company to take a trial or $10-50k to buy a huge database with the data of millions of people + me. <br>Well maybe I do, but such expense seems a bit irrational. <br>Turns out that EU-based peoples` data is almost the most expensive. </li></ol><p>But still, I know my location data was collected and I know where to buy it! </p><hr><h2 id="starting-point">Starting point</h2><p>My setup for this research included:</p><ul><li>My old iPhone 11 restored to factory defaults + new apple id. <br>Felt too uncomfortable to do all this on my current phone. </li><li>Charles Proxy to record all traffic coming in and out. <br>I set up the SSL certificate on the iPhone to decrypt all https traffic.</li><li>A simple game called Stack by KetchApp - I remember playing it at school 10-12 years ago. Choosing it as a lab rat felt nostalgic. <br>To my surprise, there were a lot of KetchApp games on the list. </li></ul><figure><img src="https://timsh.org/content/images/2025/01/image-1.png" alt="" loading="lazy" width="320" height="180"></figure><h3 id="huge-amount-of-requests">Huge amount of requests</h3><p>Ok, here we go: only 1 app installed without the default Apple ones, Charles on, launching Stack in 3, 2, 1.... </p><figure data-kg-thumbnail="https://timsh.org/content/media/2025/01/Screen-Recording-2025-01-19-at-00.02.51_thumb.jpg" data-kg-custom-thumbnail="">
            <div>
                <video src="https://timsh.org/content/media/2025/01/Screen-Recording-2025-01-19-at-00.02.51.mp4" poster="https://img.spacergif.org/v1/2410x2152/0a/spacer.png" width="2410" height="2152" loop="" autoplay="" muted="" playsinline="" preload="metadata"></video>
                
                <div>
                        <p>
                        
                        <span>0:00</span></p><p>
                            /<span>0:11</span>
                        </p>
                        </div>
            </div>
            
        <img src="https://timsh.org/content/media/2025/01/Screen-Recording-2025-01-19-at-00.02.51_thumb.jpg"></figure><p>These are the requests that the app sends in the first minute after launch. <br>Take a look at the timing of the requests - almost every split second. </p><p>Let's take a look at the contents of the requests. <br>I actually checked every single one of them - but I'll leave out only the interesting ones here. </p><h3 id="unity-ads">Unity [ads]</h3><p>Let's start with the juiciest request sent to <code>https://o.isx.unity3d.com</code> - the first one that included my geo, while I <strong>disabled Location Services</strong> on iPhone for all apps! <br>If you are as naive as I was before this, you might be surprised - what does Unity, the 3D engine, have to do with the in-app advertisement or location tracking? <br>Perhaps that's just some monitoring data to help improve the engine? </p><p>Turns out that Unity's main revenue stream (they made $2 bln+ in 2023) is Unity Ads - "Mobile Game Ad Network". Sounds quite interesting.</p><p>Below is the request body in json format sent to Unity Ads. I will only leave the  fields worth mentioning - the actual size is 200+ keys. </p><pre><code>{
  "ts": "2025-01-18T23:27:39Z", // Timestamp
  "c": "ES", // Country code,
  "d": "sports.bwin.es", // Domain; the app or website where the ad will be displayed.
  "bn": "molocoads-eu-banner", // WTF is moloco ads? We'll see!
  "cip": "181.41.[redacted]", // my IP !!
  "dm": "iPhone12,1", 
  "ct": "2", // Connection type; e.g., Wi-Fi
  "car": "Yoigo", // mobile network operator
  "ifv": "6B00D8E5-E37B-4EA0-BB58-[redacted]", // ID for Vendor. We'll get back to it!
  "lon": "2.[redacted]", // Longitude ... 
  "lat": "41.[redacted]", // Latitude ... 
  "sip": "34.227.224.225", // Server IP (Amazon AWS in US) 
  "uc": "1", // User consent for tracking = True; OK what ?!
}</code></pre><p>Ok, so my IP + location + timestamp + some <code>ifv</code> id are shared with Unity → Moloco Ads → Bwin, and then I see the actual Bwin ad in the game. <br>Wonderful! </p><div><p>As a quick note - location shared was not very precise (but still in the same postal index), I guess due to the fact that iPhone was connected to WiFi and had no SIM installed. <br>If it was LTE, I bet the lat/lon would be much more precise. </p></div><h3 id="hello-facebook-what-are-you-doing-here">Hello Facebook... What are you doing here?</h3><p>Next interesting request that leaks my IP + timestamp (= geo-datapoint) is Facebook.<br>What?!</p><ul><li>I don't have any Meta [Facebook] app installed on this iPhone</li><li>I didn't link the app nor my Apple ID to any Facebook account</li><li>I didn't consent to Facebook getting my IP address!</li></ul><p>And yet here we are:</p><pre><code>{ 
	"bundles": {
		"bidder_token_info": {
			"data": {
				"bt_extras": {
                  "ip":"181.41.[redacted], // nice Extras, bro
                  "ts":1737244649
			},
			"fingerprint": null
		},
        {
          "a lot of data: yes a loooooooot"
         }</code></pre><p>We'll talk more about this one in the next section. </p><h3 id="why-do-you-need-my-screen-brightness-level">Why do you need my screen brightness level? </h3><p>Last request I found interesting was sent to... Unity again: <br><a href="https://configv2.unityads.unity3d.com/webview/4.12.1/release/config.json?ref=timsh.org"><code>https://configv2.unityads.unity3d.com</code></a>. <br>Let's see what's in that config Unity needs so much: </p><pre><code>{
  "osVersion":"16.7.1",
  "connectionType":"wifi",
  "eventTimeStamp":1737244651,
  "vendorIdentifier":"6B00D8E5-E37B-[redacted]", // ifv once again 
  "wiredHeadset":false, // excuse me? 
  "volume":0.5,
  "cpuCount":6,
  "systemBootTime":1737215978,
  "batteryStatus":3,
  "screenBrightness":0.34999999403953552,
  "freeMemory":507888,
  "totalMemory":3550640, // is this RAM?
  "timeZone":"+0100",
  "deviceFreeSpace":112945148
  "networkOperator":"6553565535"
  "advertisingTrackingId":"00000000-0000....", // interesting ...
  }</code></pre><p>There's no "personal information" here, but honestly this amount of data shared with an arbitrary list of 3rd parties is scary. <br>Why do they need to know my screen brightness, memory amount, current volume and if I'm wearing headphones? </p><p>I know the "right" answer - to help companies target their audience better! <br>For example, if you're promoting a mobile app that is 1 GB of size, and the user only has 500 MB of space left - don't show him the ad, right?</p><p>But I also heard lots of controversies on this topic. <br>Like Uber dynamically adjusting taxi price based on your battery level - because you're not waiting for a cheaper option with 4% left while standing in the street. </p><p>I can't know if that or another one is true. <br>But the fact that this data is available and accessible by advertisers suggests that they should at least think of using it. <br>I would. </p><p>Ok, enough with the requests. <br>We can already see the examples of different ip and geolocation leaks. <br>One more "provider" that <strong>also got my IP</strong> + timestamp was adjust.com - but the request body was too boring to include. </p><hr><h2 id="lets-talk-ids">Let's talk IDs</h2><p>You might've already noticed <code>ifv</code> and <code>advertisingTrackingId</code> == <code>IDFA</code> in the requests above - what are those? </p><p>IFV, or IDFV, is "ID for Vendor". <br>This is my id unique for each vendor, a.k.a developer - in this case, KetchApp. <br>This checks out: I installed another KetchApp game to quickly record the requests, and the <code>ifv</code> value was the same for it. </p><p>Advertising Tracking ID, on the other hand, is the cross-vendor value, the one that is shared with an app if you choose "Allow app to track your activity across ...". <br>As you can see above, it was actually set to <code>000000-0000...</code> because I "Asked app not to track". </p><p>I checked this by manually disabling and enabling tracking option for the Stack app and comparing requests in both cases. </p><h3 id="and-thats-the-only-difference-between-allowing-and-disallowing-tracking"><strong>And that's the only difference between allowing and disallowing tracking</strong></h3><p><br>I understand there might be nothing shocking to you in it - this is not really kept secret, you can go and check the docs for Apple developers, for example. </p><p>But I believe this is <strong>not</strong> communicated correctly to the end users, you and me, in any adequate way, shape or form: the free apps you install and use <strong>collect your precise location</strong> with timestamp and send it to some 3rd-party companies. </p><p>The only thing that stops anyone with access to bid data (yet another ad buying agent, or ad exchange, or a dataset bought or rented from data broker, as you'll see later) from tracking you down with all trips you make daily is this <code>IDFA</code> that is not shared when you disallow apps to "track you across apps" to "enhance and personalise your ads experience". </p><p>By the way: if you're using 10 apps from the same vendor (Playrix, KetchApp or another 1000-app company) and allow <strong>a single app</strong> to track you – it would mean that the data collected in all 10 apps will be enriched with your IDFA which can later be exchanged to your personal data. </p><p>At the same time, there is so much data in the requests that I'd expect ad exchanges to find some loophole ID that would allow cross-app tracking without the need for IDFA. <br>I found at least 20 ids like <code>tid</code> and <code>sid</code>, <code>device_id</code> and <code>uid</code> (these 2 are shared with Facebook), and so on. </p><p>By the way, the fact that Facebook collected my IP + timestamp without any adequate consent / app connection from my end is crazy. <br>I think Facebook is more than capable of connecting the dots and my Meta Account to this hit as soon as I login to Instagram or Facebook app on the same IP address. </p><hr><h2 id="how-does-the-data-flow">How does the data flow?</h2><p>Let's get back to the request that leaked my location for a second and look at its trace. We'll focus on the parties in the middle:</p><p>stack<strong> →  o.isx.unity3d.com → molocoads →</strong> bwin (advertiser)</p><p>Unity [ads] is an SSP (supply-side platform) that acts as a collector of data from the app via SDK. <br>As an app developer, you don't need to worry about gathering the right data, registering as a publisher on an ad exchange or whatever - just install the SDK and receive the money. </p><p>All right, what about <a href="https://www.moloco.com/?ref=timsh.org" rel="noreferrer">Molocoads</a>? </p><figure><img src="https://timsh.org/content/images/2025/01/Screenshot-2025-01-29-at-23.06.38.png" alt="" loading="lazy" width="2000" height="786" srcset="https://timsh.org/content/images/size/w600/2025/01/Screenshot-2025-01-29-at-23.06.38.png 600w, https://timsh.org/content/images/size/w1000/2025/01/Screenshot-2025-01-29-at-23.06.38.png 1000w, https://timsh.org/content/images/size/w1600/2025/01/Screenshot-2025-01-29-at-23.06.38.png 1600w, https://timsh.org/content/images/2025/01/Screenshot-2025-01-29-at-23.06.38.png 2286w" sizes="(min-width: 720px) 720px"><figcaption><span>screenshot from Molocoads landing page</span></figcaption></figure><p>Moloco ads is a DSP network that resells data from multiple SSPs (like Unity, Applovin, Chartboost). Basically, from almost every one of the requested hosts I've seen pop up in Charles Proxy.<br>It then applies some "smart optimisation" and connects a vacant banner space on your phone screen with the advertiser.</p><p>Sounds like moloco aggregates a lot of data and basically anyone (<em>to be clear</em> <em>- any company that becomes an ad partner</em>) can access the data by bidding lower than others. <br>Or imagine a real ad exchange that bids normally and collects all of the data along the way "as a side gig". <br>Basically, this is how intelligence companies and data brokers get their data. </p><p>At this point I was looking for any mentions of Moloco on Telegram and Reddit, and I ran into this post that answered a lot of my questions:</p><figure><blockquote>
<a href="https://www.reddit.com/r/adops/comments/rqlr36/eli5_what_is_the_controversy_behind_bidstream/?ref=timsh.org">ELI5: What is the controversy behind “bidstream data”? Are there really no restraints on who gets this data and what they do with it?</a><br> by
<a href="https://www.reddit.com/user/Pubh12/?ref=timsh.org">u/Pubh12</a> in
<a href="https://www.reddit.com/r/adops/?ref=timsh.org">adops</a>
</blockquote>
</figure><p>Especially, <a href="https://www.reddit.com/r/adops/comments/rqlr36/comment/hqbwmbr/?ref=timsh.org" rel="noreferrer">this comment</a>. To quote a part of it:</p><blockquote>They access it if they integrate with the provider of bidstream, which would be the SSP. It's on the SSP to verify the vendor to whom they give access to bids. Usually, the requirement would be that you actually... bid. <br>SSPs want you to spend money, that's how their business makes revenue. They might open up only part of the traffic to specific vendors (i.e.. if you don't bid worldwide, you won't get the bidstream worldwide, only in the regions in which you operate).</blockquote><p>Just wonderful. </p><h3 id="data-brokers">Data Brokers</h3><p>Let's move further. When I found out how the data gets out, I started looking for any place where it's being sold. It was a quick search.</p><p>I found a data marketplace called <a href="https://datarade.ai/data-categories/device-graph-data?ref=timsh.org" rel="noreferrer">Datarade</a> which is a panel with all sorts of data. When I searched for MAID-specific data, hundreds of options showed up, like these two: </p><figure><img src="https://timsh.org/content/images/2025/01/Screenshot-2025-01-29-at-23.24.10.png" alt="" loading="lazy" width="1680" height="740" srcset="https://timsh.org/content/images/size/w600/2025/01/Screenshot-2025-01-29-at-23.24.10.png 600w, https://timsh.org/content/images/size/w1000/2025/01/Screenshot-2025-01-29-at-23.24.10.png 1000w, https://timsh.org/content/images/size/w1600/2025/01/Screenshot-2025-01-29-at-23.24.10.png 1600w, https://timsh.org/content/images/2025/01/Screenshot-2025-01-29-at-23.24.10.png 1680w" sizes="(min-width: 720px) 720px"></figure><p>The price of the Redmob dataset surprised me, - $120k a year... for what?<br>Let's now take a look at their promo:</p><figure><img src="https://timsh.org/content/images/2025/01/Screenshot-2025-01-29-at-23.26.05.png" alt="" loading="lazy" width="1626" height="838" srcset="https://timsh.org/content/images/size/w600/2025/01/Screenshot-2025-01-29-at-23.26.05.png 600w, https://timsh.org/content/images/size/w1000/2025/01/Screenshot-2025-01-29-at-23.26.05.png 1000w, https://timsh.org/content/images/size/w1600/2025/01/Screenshot-2025-01-29-at-23.26.05.png 1600w, https://timsh.org/content/images/2025/01/Screenshot-2025-01-29-at-23.26.05.png 1626w" sizes="(min-width: 720px) 720px"></figure><div><p>Check out the list of features on the right - do any of them look familiar? </p><p><strong>Quick note</strong>: "low latency" means they know your location from the last time any of the apps shared it. It can be as little as 5 seconds ago. <br>What's even better is that Redmob provides a <strong>free sample</strong> of the data. </p></div><p>I tried to request it from their website, but the sample never landed in my mailbox (surprise-surprise, timsh.org doesn't seem like a customer with high potential). <br>Thankfully, this sample is public on <a href="https://marketplace.databricks.com/details/caa4c07a-b27e-4876-9c9c-3f3c2bbbc11f/Redmob_SAMPLE-Redmob-MAID-Data-for-Identity-Graph-I-Global-I-15B-Users-RealTime?ref=timsh.org" rel="noreferrer">Databricks Marketplace</a> with this annotation:</p><blockquote>Enhance your products and services using our global location data covering over 1.5 billion devices. Using our extensive location dataset, you can unearth concealed patterns, conduct rapid analyses, and obtain profound knowledge.<p>We can also provide region-specific data (MENA, Africa, APAC, etc.) based on your specific requirements. Our pricing model includes an annual licensing option, and we provide free sample data so that you can evaluate the quality of our dataset for yourself. </p></blockquote><figure><img src="https://timsh.org/content/images/2025/01/Screenshot-2025-01-31-at-01.12.16.png" alt="" loading="lazy" width="2000" height="562" srcset="https://timsh.org/content/images/size/w600/2025/01/Screenshot-2025-01-31-at-01.12.16.png 600w, https://timsh.org/content/images/size/w1000/2025/01/Screenshot-2025-01-31-at-01.12.16.png 1000w, https://timsh.org/content/images/size/w1600/2025/01/Screenshot-2025-01-31-at-01.12.16.png 1600w, https://timsh.org/content/images/size/w2400/2025/01/Screenshot-2025-01-31-at-01.12.16.png 2400w" sizes="(min-width: 1200px) 1200px"><figcaption><span>Some sample data for better understanding</span></figcaption></figure><p>To me, the most absurd part is the <code>app</code> column - the source of the data can't be more obvious. I'm also quite interested in the <code>yod</code> column - if it's the birthyear, where did they get it from? Never mind, who cares about your birthyear.</p><h3 id="show-me-the-pii">Show me the PII!</h3><p>All right, imagine I bought the access to a huge stream of Redmob data. <br>But my goal is to track and stalk people like myself or anyone else, so I need some way to exchange MAIDs (<em>=</em><code>ifa</code>) for the actual personal info: name, address, phone number... </p><p>No problem! This kind of dataset is surprisingly also present on Datarade. <br>Take a look at a sample table with <code>MAID &lt;&gt; PII</code> type that is provided by "<a href="https://www.agrmarketingsolutions.com/data-nuggets/?ref=timsh.org" rel="noreferrer">AGR Marketing Solutions</a>":</p><figure><a href="https://docs.google.com/spreadsheets/d/1gbom_3YO-oFB6Yrg_MAhrJKGkYtNHR2S/edit?gid=2029445799&amp;ref=timsh.org#gid=2029445799"><div><p>AGR_Mobile_Intent_PII20240903.xlsx</p><p><img src="https://timsh.org/content/images/icon/spreadsheets_2023q4.ico" alt=""><span>Google Docs</span></p></div><p><img src="https://timsh.org/content/images/thumbnail/AHkbwyL0Y-RadP94i-3ntHlOyBNz5QtZLseHmdD1MAKSAFQi7l2fIzkQjGmDmSQRI2yTN7B1h5cNCpCeVlmlA0dsdek_xQhi6nazcP6xeg-w1200-h630-p" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>Inside - all personal info (full name, email, phone number, physical address, property ownership... and IDFAs. </p><p>Congrats, you have just reached the bottom of this rabbit hole. <br>Let's wrap it up and make a couple of bold statements.</p><hr><h2 id="how-to-track-yourself-down">How to track yourself down?</h2><p>Easy! Just follow this simple step-by-step guide:</p><ol><li>Use some free apps for a bit. <br>Move around and commute - this makes the geo data more valuable. </li><li>"Allow" or "ask not to track" - a combo of IP + location + User-agent + geolocation will still be leaked to hundreds of "3rd parties" regardless of your choice.</li><li>Wait for a few seconds until fake DSPs and data brokers receive your data.</li><li>Exchange your full name or phone number for an IDFA (if present), IP address and user-agent through the <code>MAID &lt;&gt; PII</code> data purchased somewhere.</li><li>Now, access the "Mobility data" consisting of geolocation history, and filter it using the values from the previous step. </li></ol><p>Congratulations! You found yourself. </p><p>I <a href="https://excalidraw.com/?ref=timsh.org#json=Ip5AaR-FPppPmtL3AcrBg,-woEvDuI7vER5B7skpT3zA" rel="noreferrer">created a flowchart</a> that includes almost all actors and data mentioned above - now you can see how it's all connected. </p><figure><img src="https://timsh.org/content/images/2025/02/dataflow_upd-1.png" alt="" loading="lazy" width="2000" height="747" srcset="https://timsh.org/content/images/size/w600/2025/02/dataflow_upd-1.png 600w, https://timsh.org/content/images/size/w1000/2025/02/dataflow_upd-1.png 1000w, https://timsh.org/content/images/size/w1600/2025/02/dataflow_upd-1.png 1600w, https://timsh.org/content/images/size/w2400/2025/02/dataflow_upd-1.png 2400w"></figure><p>This is the worst thing about these data trades that happen constantly around the world - each small part of it is (or seems) legit. It's the bigger picture that makes them look ugly. </p><hr><h2 id="afterwords">Afterwords</h2><p>Thanks for reading this story until the end!<br>My research was heavily influenced by these posts and investigations: </p><figure><a href="https://krebsonsecurity.com/2024/10/the-global-surveillance-free-for-all-in-mobile-ad-data/?ref=timsh.org"><div><p>The Global Surveillance Free-for-All in Mobile Ad Data</p><p>Not long ago, the ability to remotely track someone’s daily movements just by knowing their home address, employer, or place of worship was considered a powerful surveillance tool that should only be in the purview of nation states. But a…</p><p><img src="https://timsh.org/content/images/icon/favicon.ico" alt=""><span>Krebs on Security</span><span>Skip to content</span></p></div><p><img src="https://timsh.org/content/images/thumbnail/peoplephone.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><figure><a href="https://www.404media.co/candy-crush-tinder-myfitnesspal-see-the-thousands-of-apps-hijacked-to-spy-on-your-location/?ref=timsh.org"><div><p>Candy Crush, Tinder, MyFitnessPal: See the Thousands of Apps Hijacked to Spy on Your Location</p><p>A hack of location data company Gravy Analytics has revealed which apps are—knowingly or not—being used to collect your information behind the scenes.</p><p><img src="https://timsh.org/content/images/icon/favicon-3.svg" alt=""><span>404 Media</span><span>Joseph Cox</span></p></div><p><img src="https://timsh.org/content/images/thumbnail/andrew-guan-lTUyP3RaLpw-unsplash.jpg" alt="" onerror="this.style.display = 'none'"></p></a></figure><figure><a href="https://interaktiv.br.de/ausspioniert-mit-standortdaten/en/index.html?ref=timsh.org"><div><p>Under Surveillance</p><p>How Location Data Jeopardizes German Security</p><p><img src="https://timsh.org/content/images/icon/apple-touch-icon.png" alt=""><span>BR</span><span>BR Data</span></p></div><p><img src="https://timsh.org/content/images/thumbnail/teaser.png" alt="" onerror="this.style.display = 'none'"></p></a></figure>
        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sniffnet – monitor your Internet traffic (219 pts)]]></title>
            <link>https://github.com/GyulyVGC/sniffnet</link>
            <guid>42909530</guid>
            <pubDate>Sun, 02 Feb 2025 16:14:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/GyulyVGC/sniffnet">https://github.com/GyulyVGC/sniffnet</a>, See on <a href="https://news.ycombinator.com/item?id=42909530">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
<themed-picture data-catalyst-inline="true"><picture>
<img alt="" title="Sniffnet" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/header_repository.png" width="95%">
</picture></themed-picture>
<p dir="auto"><a href="#download"><img alt="" title="Download" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/badges/download.svg"></a>
<a href="https://github.com/GyulyVGC/sniffnet/blob/main/ROADMAP.md"><img alt="" title="Roadmap" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/badges/roadmap.svg"></a>
<a href="https://sniffnet.net/" rel="nofollow"><img alt="" title="Website" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/badges/website.svg"></a>
<a href="https://github.com/GyulyVGC/sniffnet/wiki"><img alt="" title="Wiki" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/badges/wiki.svg"></a></p>
<p dir="auto">Application to comfortably monitor your Internet traffic <br>
Cross-platform, Intuitive, Reliable</p>
<p dir="auto">Translated in:<br>
🇨🇳 🇩🇪 🇫🇷 🇷🇺 🇵🇹 🇪🇦 🇮🇹 🇵🇱 <a href="https://github.com/GyulyVGC/sniffnet/issues/60" data-hovercard-type="issue" data-hovercard-url="/GyulyVGC/sniffnet/issues/60/hovercard">+&nbsp;12&nbsp;more&nbsp;languages</a></p>
</div>
<p dir="auto">
<themed-picture data-catalyst-inline="true"><picture>
<img alt="" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/hr.png" width="100%">
</picture></themed-picture>
</p>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/pages/overview.png"><img alt="" title="Overview page" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/pages/overview.png" width="95%"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/pages/inspect.png"><img alt="" title="Inspect page" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/pages/inspect.png" width="47%"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/pages/notifications.png"><img alt="" title="Notifications page" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/pages/notifications.png" width="47%"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/pages/catppuccin.png"><img alt="" title="Custom theme" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/pages/catppuccin.png" width="47%"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/pages/thumbnail.png"><img alt="" title="Thumbnail mode" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/pages/thumbnail.png" width="47%"></a>
</p>
<p dir="auto">
<themed-picture data-catalyst-inline="true"><picture>
<img alt="" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/hr.png" width="100%">
</picture></themed-picture>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><em>Support Sniffnet's development</em> 💖</h2><a id="user-content-support-sniffnets-development-" aria-label="Permalink: Support Sniffnet's development 💖" href="#support-sniffnets-development-"></a></p>
<p dir="auto"><i>Sniffnet is completely free, open-source software which needs lots of effort and time to develop and maintain.</i></p>
<p dir="auto"><i>If you appreciate Sniffnet, <a href="https://github.com/sponsors/GyulyVGC">consider sponsoring</a>:
your support will allow me to dedicate more time to this project,
constantly expanding it including <a href="https://github.com/GyulyVGC/sniffnet/blob/main/ROADMAP.md">new features and functionalities</a>.</i></p>
<p dir="auto"><i>A special mention goes to these awesome organizations and folks who are sponsoring Sniffnet:</i></p>
<p dir="auto">
<a href="https://github.com/github" title="GitHub"><img src="https://avatars.githubusercontent.com/github?v=4" width="60px" alt="GitHub"></a>&nbsp;&nbsp;
<a href="https://nlnet.nl/" title="NLnet" rel="nofollow"><img src="https://camo.githubusercontent.com/9e7f127e3732d4322576aa2462ce899725954f576fa5c36543b38f684f53a6cd/68747470733a2f2f6e6c6e65742e6e6c2f6c6f676f2f6c6f676f2e737667" width="60px" alt="NLnet" data-canonical-src="https://nlnet.nl/logo/logo.svg"></a>&nbsp;&nbsp;
<a href="https://ipinfo.io/" title="IPinfo" rel="nofollow"><img src="https://avatars.githubusercontent.com/ipinfo?v=4" width="60px" alt="IPinfo"></a>&nbsp;&nbsp;
<a href="https://github.com/Cthulu201" title="Cthulu201"><img src="https://avatars.githubusercontent.com/Cthulu201?v=4" width="60px" alt="Cthulu201"></a>&nbsp;&nbsp;
<a href="https://github.com/0x0177b11f" title="Tiansheng Li"><img src="https://avatars.githubusercontent.com/0x0177b11f?v=4" width="60px" alt="Tiansheng Li"></a>&nbsp;&nbsp;
<a href="https://github.com/ZEROF" title="ZEROF"><img src="https://avatars.githubusercontent.com/ZEROF?v=4" width="60px" alt="ZEROF"></a>&nbsp;&nbsp;
<a href="https://www.janwalter.org/" title="Jan Walter" rel="nofollow"><img src="https://avatars.githubusercontent.com/wahn?v=4" width="60px" alt="Jan Walter"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Download</h2><a id="user-content-download" aria-label="Permalink: Download" href="#download"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th><a href="#download"><img alt="Windows" title="Windows" height="35px" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/badges/windows.svg"></a></th>
<th><a href="#download"><img alt="macOS" title="macOS" height="35px" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/badges/macos.svg"></a></th>
<th><a href="#download"><img alt="Linux (.deb)" title="Linux (.deb)" height="35px" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/badges/linux_deb.svg"></a></th>
<th><a href="#download"><img alt="Linux (.rpm)" title="Linux (.rpm)" height="35px" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/badges/linux_rpm.svg"></a></th>
</tr>
</thead>
<tbody>
<tr>
<td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/GyulyVGC/sniffnet/releases/latest/download/Sniffnet_Windows_64-bit.msi">64‑bit</a>&nbsp;|&nbsp;<a href="https://github.com/GyulyVGC/sniffnet/releases/latest/download/Sniffnet_Windows_32-bit.msi">32‑bit</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
<td><a href="https://github.com/GyulyVGC/sniffnet/releases/latest/download/Sniffnet_macOS_Intel.dmg">Intel</a>&nbsp;|&nbsp;<a href="https://github.com/GyulyVGC/sniffnet/releases/latest/download/Sniffnet_macOS_AppleSilicon.dmg">Apple&nbsp;silicon</a></td>
<td><a href="https://github.com/GyulyVGC/sniffnet/releases/latest/download/Sniffnet_LinuxDEB_amd64.deb">amd64</a>&nbsp;|&nbsp;<a href="https://github.com/GyulyVGC/sniffnet/releases/latest/download/Sniffnet_LinuxDEB_arm64.deb">arm64</a>&nbsp;|&nbsp;<a href="https://github.com/GyulyVGC/sniffnet/releases/latest/download/Sniffnet_LinuxDEB_i386.deb">i386</a>&nbsp;|&nbsp;<a href="https://github.com/GyulyVGC/sniffnet/releases/latest/download/Sniffnet_LinuxDEB_armhf.deb">armhf</a></td>
<td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/GyulyVGC/sniffnet/releases/latest/download/Sniffnet_LinuxRPM_x86_64.rpm">x86_64</a>&nbsp;|&nbsp;<a href="https://github.com/GyulyVGC/sniffnet/releases/latest/download/Sniffnet_LinuxRPM_aarch64.rpm">aarch64</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">Links in the table above will download the latest version of Sniffnet directly from <a href="https://github.com/GyulyVGC/sniffnet/releases">GitHub releases</a>. <br></p>

<p dir="auto"><strong>Alternative installation methods</strong> are reported in the following:</p>
<details>
  <summary>from Crates.io</summary>
<p dir="auto">Follow this method only if you have <a href="https://www.rust-lang.org/tools/install" rel="nofollow">Rust installed</a> on your machine. <br>
In this case, the application binary can be built and installed with:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cargo install sniffnet --locked"><pre>cargo install sniffnet --locked</pre></div>
</details>
<details>
  <summary>from Homebrew</summary>
<p dir="auto">You can install <a href="https://github.com/Homebrew/homebrew-core/pkgs/container/core%2Fsniffnet">Sniffnet Homebrew package</a> with:</p>

</details>
<details>
  <summary>from Nixpkgs</summary>
<p dir="auto">You can install <a href="https://search.nixos.org/packages?channel=23.05&amp;show=sniffnet&amp;from=0&amp;size=50&amp;sort=relevance&amp;type=packages&amp;query=sniffnet" rel="nofollow">Sniffnet Nix package</a> adding the following Nix code to your NixOS Configuration, usually located in <code>/etc/nixos/configuration.nix</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="environment.systemPackages = [
  pkgs.sniffnet
];"><pre><span>environment</span><span>.</span><span><span>systemPackages</span></span> <span>=</span> <span>[</span>
  <span>pkgs</span><span>.</span><span><span>sniffnet</span></span>
<span>]</span><span>;</span></pre></div>
<p dir="auto">Alternatively, you can install it in your home using <a href="https://github.com/nix-community/home-manager">Home Manager</a> with:</p>
<div dir="auto" data-snippet-clipboard-copy-content="home.packages = [
  pkgs.sniffnet
];"><pre><span>home</span><span>.</span><span><span>packages</span></span> <span>=</span> <span>[</span>
  <span>pkgs</span><span>.</span><span><span>sniffnet</span></span>
<span>]</span><span>;</span></pre></div>
<p dir="auto">Alternatively, you can try it in a shell with:</p>

</details>
<details>
  <summary>on Arch Linux</summary>
<p dir="auto">You can install Sniffnet community package via <a href="https://wiki.archlinux.org/title/Pacman" rel="nofollow">pacman</a>:</p>

</details>
<details>
  <summary>on FreeBSD</summary>
<p dir="auto">You can install Sniffnet port with:</p>

</details>
<details>
  <summary>on NetBSD</summary>
<p dir="auto">You can install Sniffnet from the official repositories via <a href="https://pkgin.net/" rel="nofollow">pkgin</a>:</p>

</details>
<details>
  <summary>on Tiny Core Linux</summary>
<p dir="auto">You can install Sniffnet from the official repository with:</p>

</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>💻 choose a <strong>network adapter</strong> of your PC to inspect</li>
<li>🏷️ select a set of <strong>filters</strong> to apply to the observed traffic</li>
<li>📖 view overall <strong>statistics</strong> about your Internet traffic</li>
<li>📈 view <strong>real-time charts</strong> about traffic intensity</li>
<li>📌 keep an eye on your network even when the application is <strong>minimized</strong></li>
<li>📁 <strong>export</strong> comprehensive capture reports as <strong>PCAP files</strong></li>
<li>🔎 identify <strong>6000+ upper layer services</strong>, protocols, trojans, and worms</li>
<li>🌐 find out <strong>domain name</strong> and <strong>ASN</strong> of the hosts you are exchanging traffic with</li>
<li>🏠 identify connections in your <strong>local network</strong></li>
<li>🌍 get information about the country of remote hosts (<strong>IP geolocation</strong>)</li>
<li>⭐ save your <strong>favorite</strong> network hosts</li>
<li>🕵️‍♂️ search and <strong>inspect</strong> each of your network connections in real time</li>
<li>🔉 set <strong>custom notifications</strong> to inform you when defined network events occur</li>
<li>🎨 choose the <strong>style</strong> that fits you the most, including custom themes support</li>
<li>...and more!</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">User manual</h2><a id="user-content-user-manual" aria-label="Permalink: User manual" href="#user-manual"></a></p>
<p dir="auto">Do you want to <strong>learn more</strong>? <br>
Check out the <a href="https://github.com/GyulyVGC/sniffnet/wiki"><strong>Sniffnet Wiki</strong></a>, a comprehensive manual to help you
thoroughly master the application from a basic setup to the most advanced functionalities. <br>
The Wiki includes step-by-step guides, tips, examples of usage, and answers to frequent questions.</p>
<p dir="auto">
<a href="https://github.com/GyulyVGC/sniffnet/wiki">
<img alt="" title="Sniffnet Wiki" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/logos/wiki/wikilogo.svg" width="300px">
</a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Troubleshooting</h2><a id="user-content-troubleshooting" aria-label="Permalink: Troubleshooting" href="#troubleshooting"></a></p>
<details>
  <summary>See details</summary>
<p dir="auto"><h3 tabindex="-1" dir="auto">Missing dependencies</h3><a id="user-content-missing-dependencies" aria-label="Permalink: Missing dependencies" href="#missing-dependencies"></a></p>
<p dir="auto">Most of the errors that may arise are likely due to your system missing dependencies
required to correctly analyze a network adapter. <br>
Check the <a href="https://github.com/GyulyVGC/sniffnet/wiki/Required-dependencies">required dependencies page</a>
for instructions on how to proceed depending on your operating system.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Rendering problems</h3><a id="user-content-rendering-problems" aria-label="Permalink: Rendering problems" href="#rendering-problems"></a></p>
<p dir="auto">In some circumstances, especially if you are running on an old architecture or your graphical drivers are not updated,
the <code>wgpu</code> default renderer used by <a href="https://github.com/iced-rs/iced">iced</a>
may manifest bugs (the interface glitches, color gradients are unsupported, or some icons are completely black). <br>
In these cases you can set an environment variable to switch to the <code>tiny-skia</code> renderer,
a CPU-only software renderer that should work properly on every environment:</p>

<p dir="auto"><h3 tabindex="-1" dir="auto"><em><strong>In any case, don't hesitate to <a href="https://github.com/GyulyVGC/sniffnet/issues/new/choose">open an issue</a>, and I will do my best to help you!</strong></em></h3><a id="user-content-in-any-case-dont-hesitate-to-open-an-issue-and-i-will-do-my-best-to-help-you" aria-label="Permalink: In any case, don't hesitate to open an issue, and I will do my best to help you!" href="#in-any-case-dont-hesitate-to-open-an-issue-and-i-will-do-my-best-to-help-you"></a></p>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgements</h2><a id="user-content-acknowledgements" aria-label="Permalink: Acknowledgements" href="#acknowledgements"></a></p>
<ul dir="auto">
<li>A big shout-out to <a href="https://github.com/GyulyVGC/sniffnet/blob/main/CONTRIBUTORS.md">all the contributors</a> of Sniffnet!</li>
<li>The graphical user interface has been realized with <a href="https://github.com/iced-rs/iced">iced</a>, a cross-platform GUI library for Rust focused on simplicity and type-safety</li>
<li>IP geolocation and ASN data are provided by <a href="https://www.maxmind.com/" rel="nofollow">MaxMind</a></li>
<li>Last but not least, thanks to <a href="https://github.com/GyulyVGC/sniffnet/stargazers">every single stargazer</a>: all forms of support made it possible to keep improving Sniffnet!</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: What is interviewing like now with everyone using AI? (342 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=42909166</link>
            <guid>42909166</guid>
            <pubDate>Sun, 02 Feb 2025 15:19:32 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=42909166">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="42911944"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42911944" href="https://news.ycombinator.com/vote?id=42911944&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I've let people use GPT in coding interviews, provided that they show me how they use it. At the end I'm interested in knowing how a person solves a problem, and thinks about it. Do they just accept whatever crap the gpt gives them, can they take a critical approach to it, etc.</p><p>So far, everyone that elected to use GPT did much worse. They did not know what to ask, how to ask, and did not "collaborate" with the AI. So far my opinion is if you have a good interview process, you can clearly see who are the good candidates with or without ai.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912237"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42912237" href="https://news.ycombinator.com/vote?id=42912237&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>We do the same thing. It's perfectly fine for candidates to use AI-assistive tooling provided that they can edit/maintain the code and not just sit in a prompt the whole time. The heavier a candidate relies on LLMs, the worse they often do. It really comes down to discipline.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912619"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42912619" href="https://news.ycombinator.com/vote?id=42912619&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>same thing here. Interview is basically a representative thing of what we do, but also depends on the level of seniority. I ask people just to share the screen with me and use whatever you want / fell comfortable with. Google, ChatGPT, call your mom, I don't care as long as you walk me through how you're approaching the thing at hand. We've all googled tar xvcxfgzxfzcsadc, what's that permission for .pem is it 400, etc.. no shame in anything and we all use all of the things through day. Let's simulate a small task at hand and see where we end up at. Similarly, there is a bias where people leaning more on LLMs doing worse than those just googling or, gasp, opening documentation.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912579"><td></td></tr>
                  <tr id="42912049"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42912049" href="https://news.ycombinator.com/vote?id=42912049&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>My company, a very very large company, is transitioning back to only in-person interviews due to the rampant amount of cheating happening during interviews.</p><p>As an interviewer, it's wild to me how many candidates think they can get away with it, when you can very obviously hear them typing, then watching their eyes move as they read an answer from another screen. And the majority of the time the answer is incorrect anyway. I'm happy that we won't have to waste our time on those candidates anymore.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912304"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42912304" href="https://news.ycombinator.com/vote?id=42912304&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>So depressing to hear that “because of rampant cheating”</p><p>As a person looking for a job, I’m really not sure what to do.  If people are lying on their resumes and cheating in interviews, it feels like there’s nothing I can do except do the same.  Otherwise I’ll remain jobless.</p><p>But to this day I haven’t done either.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912988"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912988" href="https://news.ycombinator.com/vote?id=42912988&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>&gt; it feels like there’s nothing I can do except do the same.</p><p>Why does it feel like that when you’re replying to someone who already points out that it doesn’t work? Cheating can prevent you from getting a job, and it can get you fired from the job too. It can also impede your ability to learn and level up your own skills. I’m glad you haven’t done it yet, just know that you can be a better candidate and increase your chances by not cheating.</p><p>Using an LLM isn’t cheating if the interviewer allows it. Whether they allow it or not, there’s still no substitute for putting in the work. Interviews are a skill that can (and should) be practiced. Candidates are rarely hired for technical skill alone. Attitude, communication, curiosity, and lots of other soft skills are severely underestimated by so many job seekers, especially those coming right out of school. A small amount of strengthening your non-code abilities can improve your odds much faster than leetcode ever will. And if you have time, why not do both?</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912752"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912752" href="https://news.ycombinator.com/vote?id=42912752&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Here's the thing: 95% of cheaters still suck, even when cheating. Its hard to imagine how people can perform so badly while cheating, yet they consistently do. All you need to do to stand out is not be utterly awful. Worrying about what other people are doing is more detrimental to your performance than anything else is. Just focus on yourself: being broadly competent, knowing your niche well, and being good at communicating how you learn when you hit the edges of your knowledge. Those are the skills that always stand out.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912987"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_42912987" href="https://news.ycombinator.com/vote?id=42912987&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Yeah, we found this when we started doing take-home exams: it turns out that a junior dev who spends twice as much time on the problem than what we asked them to doesn’t put out senior-level code - we could read the skill level in the code almost instantly. Same thing with cheating like that - it turns out knowing the answer isn’t the same thing as having experience, and it’s pretty obvious pretty quickly which one you’re dealing with.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="42912503"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912503" href="https://news.ycombinator.com/vote?id=42912503&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I don't know, I kind of feel like leetcode interviews are a situation where the employer is cheating. I mean, you're admittedly filtering out a great number of acceptable candidates knowing that if you just find 1 in a 1000, that'll be good enough. It is patently unfair to the individuals that are smart enough to do your work, but poor at some farcical representation of the work. That is cheating.</p><p>In my opinion, if a prospective employee is able to successfully use AI to trick me into hiring them, then that is a hell of a lot closer to the actual work they'll be hired to do (compared to leetcode).</p><p>I say, if you can cheat at an interview with AI, do it.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912641"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_42912641" href="https://news.ycombinator.com/vote?id=42912641&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>The employer sets the terms of the interview. If you don’t like them, don’t apply.</p><p>What you’re suggesting here isn’t any different than submitting a fraudulent resume because you disagree with the required qualifications.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912678"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_42912678" href="https://news.ycombinator.com/vote?id=42912678&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>Yea, exactly.</p><p>If a candidate were up front with me and asked if they could use AI, or said they learned an answer from AI and then wanted to discuss it with me, I'd be happy with that. But attempting to hide it and pretend they aren't using it when our interview rules specifically ask you not to do it is just being dishonest, which isn't a characteristic of someone I want to hire.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912662"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_42912662" href="https://news.ycombinator.com/vote?id=42912662&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>On principle, what you’re saying has merit. In practice, the market is currently rife with employers submitting job postings with inflated qualifications, for positions that may or may not exist. So there’s bad actors all around and it’s difficult to tell who actually is behaving with integrity.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912863"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_42912863" href="https://news.ycombinator.com/vote?id=42912863&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I would like to be paid though. What do I care about the terms of the interview as long as they hire me?</p><p>What is being suggested here is not participating in the mind numbing process that is called ‘applying for a job’.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912953"><td></td></tr>
                        <tr id="42912771"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_42912771" href="https://news.ycombinator.com/vote?id=42912771&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I dunno why there is always the assumption in these threads that leetcode is being used. My company has never used leetcode-style questions, and likely never will.</p><p>I work in security, and our questions are pretty basic stuff. "What is cross-site scripting, and how would you protect against it?", "You're tasked with parsing a log file to return the IP addresses that appear at least 10 times, how would you approach this?" Stuff like that. And then a follow-up or two customized to the candidate's response.</p><p>I really don't know how we could possibly make it easier for candidates to pass these interviews. We aren't trying to trick people, or weed people out. We're trying to find people that have the foundational experience required to do the job they're being hired for. Even when people do answer them incorrectly, we try to help them out and give them guidance, because it's really about trying to evaluate how a person thinks rather than making sure they get the right answer.</p><p>I mean hell, it's not like I'm spending hours interviewing people because I get my rocks off by asking people lame questions or rejecting people; I <i>want</i> to hire people! I will go out of my way to advocate for hiring someone that's honest and upfront about being incorrect or not knowing an answer, but wants to think through it with me.</p><p>But cheating? That's a show stopper. If you've been asked to not use ChatGPT, but you use it anyway, you're not getting the benefit of the doubt. You're getting rejected and blacklisted.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="42912415"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912415" href="https://news.ycombinator.com/vote?id=42912415&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>Note also "And the majority of the time the answer is incorrect anyway."</p><p>I haven't looked for development-related jobs this millennium, but it's unclear to me how effective a crutch AI is for interviews--at least for well-designed and run interviews. Maybe in some narrow domains for junior people.</p><p>As a few of us have written elsewhere, I consider not having in-person  interviews past an initial screen sheer laziness and companies generally deserve whoever they end up with.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="42912102"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42912102" href="https://news.ycombinator.com/vote?id=42912102&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>When I was interviewing entry level programmers at my last job, we gave them an assignment that should only take a few hours, but we basically didn't care about the code at all.</p><p>Instead, we were looking to see if they followed instructions, and if they left anything out.</p><p>I never had a chance to test it out, since we hadn't hired anyone new in so long, but ChatGPT/etc would almost always fail this exam because of how bad it is at making sure everything was included.</p><p>And bad programmers also failed it.  It always left us with a few candidates that paid attention, and from there we figure if they can do that, they can learn the rest.  It seemed to work quite well.</p><p>I was recently laid off from that company, and now I'm realizing that I really want to see what current-day candidates would turn in.  Oh well.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912362"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912362" href="https://news.ycombinator.com/vote?id=42912362&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>For those tests I never follow the rules, I just make something quick and dirty because I refuse to spend unpaid hours. In the interview the first question is why I didnt follow the instructions, and they think my reason is fair.</p><p>Companies seem to think that we program just for fun and ask to make a full blown app... also underestimating the time candidates actually spend making it.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912830"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_42912830" href="https://news.ycombinator.com/vote?id=42912830&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>If you’re spending the time applying and submitting <i>something</i> then you might as well spend the extra 30 minutes or so to do it right, no?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912885"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_42912885" href="https://news.ycombinator.com/vote?id=42912885&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Any time someone says ‘should only take a few hours’ they’re far underestimating the time it actually takes.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912939"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_42912939" href="https://news.ycombinator.com/vote?id=42912939&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Not if you’re applying to hundreds, or thousands of jobs. Unless you know someone, it’s a quantity game.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                              <tr id="42912207"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42912207" href="https://news.ycombinator.com/vote?id=42912207&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>The industry (all industries really) might want to reconsider online applications, or at least privilege in-person resume drop-offs because the escalating ai application/evaluation war that's happening doesn't seem to be helping anyone.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912916"><td></td></tr>
                  <tr id="42912810"><td></td></tr>
                <tr id="42912907"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912907" href="https://news.ycombinator.com/vote?id=42912907&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>For the goal of the interview - showing your knowledge and skills - you are failing miserably. People know what LLMs can do, the interview is about you.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912842"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912842" href="https://news.ycombinator.com/vote?id=42912842&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I guess its more of a question if you can solve the problem without AI.</p><p>In most interview tasks you are not solving the task “with” ai.</p><p>Its AI who solves the task while you watch it do it.</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="42911155"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42911155" href="https://news.ycombinator.com/vote?id=42911155&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>My startup got acquired last year so I haven't interviewed anyone in a while, but my technical interview has always been:</p><p>- share your screen</p><p>- download/open the coding challenge</p><p>- you can use any website, Stack Overflow, whatever, to answer my questions as long as it's on the screenshare</p><p>My goal is to determine if the candidate can be technically productive, so I allow any programming language, IDE, autocompleter, etc, that they want.  I would have no problem with them using GPT/Copilot in addition to all that, as long as it's clear how they're solving it.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912005"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42912005" href="https://news.ycombinator.com/vote?id=42912005&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I recently interviewed for my team and tried this same approach. I thought it made sense because I want to see how people can actually work and problem solve given all the tools at their disposal, just like on the job.</p><p>It proved to be awkward and clumsy very quickly. Some candidates resisted it since they clearly thought it would make them judged harsher. Some candidates were on the other extreme and basically tried asking ChatGPT the problem straight up, even though I clarified up front "You can even use ChatGPT as long as you're not just directly asking for the solution to the whole problem and just copy/pasting, obviously."</p><p>After just the initial batch of candidates it became clear it was muddying things too much, so I simply forbade using it for the rest of the candidates, and those interviews went much smoother.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912901"><td></td></tr>
                <tr id="42912925"><td></td></tr>
                  <tr id="42912631"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912631" href="https://news.ycombinator.com/vote?id=42912631&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Did you tell them that you “want to see how people can actually work and problem solve given all the tools at their disposal, just like on the job”? Just curious.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="42912263"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42912263" href="https://news.ycombinator.com/vote?id=42912263&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>If you really don't penalize them for this, you should clearly state it. Some people may still think they'll be penalized as that is the norm.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912103"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42912103" href="https://news.ycombinator.com/vote?id=42912103&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>I did this while hiring last year and the number of candidates who got stuff wrong because they were too proud to just look up the answer was shocking.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912228"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912228" href="https://news.ycombinator.com/vote?id=42912228&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Is it pride or is it hard to shake the (reasonable, I'd say) fear the reviewer will judge regardless of their claims?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912527"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_42912527" href="https://news.ycombinator.com/vote?id=42912527&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Exactly. You never know. Some interviewers will penalize you for not having something memorized and having to look it up, some will penalize you for guessing, some will penalize you for simply not knowing and asking for help. Some interviewers will penalize you for coming up with something quick and dirty and then refining it, some will penalize you for jumping right to the final product. There's no consistency.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="42911656"><td></td></tr>
            <tr id="42912070"><td></td></tr>
            <tr id="42911761"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42911761" href="https://news.ycombinator.com/vote?id=42911761&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>I'd be fine with the GPT side of things, as long as I could somehow inject poor answers, and see if the interviewee notices and corrects.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42911804"><td></td></tr>
            <tr id="42912058"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912058" href="https://news.ycombinator.com/vote?id=42912058&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>the trick is to phrase the problem in a way that GPT4 will always give the incorrect answer (due to vagueness of your problem) and that multiple rounds of guiding/correcting are needed to solve.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912144"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_42912144" href="https://news.ycombinator.com/vote?id=42912144&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>That's pretty good because it can exhaust the context window quickly and then it starts spiraling out of control, which would require the candidate to act.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912469"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_42912469" href="https://news.ycombinator.com/vote?id=42912469&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>If you only use ChatGPT to code, you are only able to copy paste the llm emitted code, then you ask for changes to the code (to reflect for example the evolution of the product)</p>
              </div></td></tr>
        </tbody></table></td></tr>
                              <tr id="42911909"><td></td></tr>
                <tr id="42912021"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912021" href="https://news.ycombinator.com/vote?id=42912021&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>It's pretty obvious when someone's input focus changes to nothing or when their mouse leaves the screen entirely, or you could just ask to see the display settings to begin. Doesn't solve for multiple computers but it's pretty obvious in real time when someone's actual attention drifts or they suddenly have abilities they didn't have before.</p><p>Either way, screen sharing beats whiteboards. Even if we throw our hands up and give up, we'll be firing frauds before the probationary period ends.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912603"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_42912603" href="https://news.ycombinator.com/vote?id=42912603&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>There is nothing fraudulent about using LLMs. If people can use them on the job, it's okay to use them on the interview. They're the calculators of tomorrow if not of today.</p><p>Interviewing just needs to adapt such as by assessing one's open source  projects and contributions. Not much more is needed. And if the candidate completely misrepresents their open source profile, this can be handled by an initial contract-to-hire period.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912848"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_42912848" href="https://news.ycombinator.com/vote?id=42912848&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>I agree that there's nothing fraudulent with using a tool you would use on the job when you are interviewing. But in no way are LLMs equivalent to calculators. Calculators actually <i>give the correct answer</i> reliably, unlike LLMs. A sporadically reliable tool is worse than no tool at all.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912656"><td></td></tr>
                                    <tr id="42912517"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42912517" href="https://news.ycombinator.com/vote?id=42912517&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I've always just tried to hold a conversation with the candidate, what they think their strengths are weaknesses are and a little probing.</p><p>This works especially well if <i>I</i> don't know the area they're strongest in, because then they get to explain it to me. If I don't understand it then it's a pretty clear signal that they either don't understand it well enough or are a poor communicator. Both are dealbreakers.</p><p>Otherwise, for me, the most important thing is gauging: Aptitude, Motivation and Trustworthiness. If you have these three attributes then I could not possibly give a shit that you don't know how kubernetes operators work, or if you can't invert a binary tree.</p><p>You'll learn when you need it; it's not like the knowledge is somehow esoteric or hidden.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42910568"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42910568" href="https://news.ycombinator.com/vote?id=42910568&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>Part of my resume review process is trying to decide if I can trust the person. If their resume seems too AI-generated, I feel less like I can trust that candidate and typically reject the candidate.</p><p>Once you get to the interview process, it's very clear if someone thinks they can use AI to help with the interview process. I'm not going to sit here while you type my question into OpenAI and try to BS a meaningful response to my question 30 seconds later.</p><p>AI-proof interviewing is easy if you know what you're talking about. Look at the candidates resume and ask them to describe some of their past projects. If they can have a meaningful conversation without delays, you can probably trust their resume. It's easy to spot BS whether AI is behind it or not.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42910796"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42910796" href="https://news.ycombinator.com/vote?id=42910796&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>This, and tbh this has always been the best way. Someone who has projects, whether personal or professional, and has the capability to discuss those projects in depth and with passion will usually be a better employee than a leet code specialist.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42911672"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42911672" href="https://news.ycombinator.com/vote?id=42911672&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Doesn't even have to be a project per se, if they can discuss some sort of technical topic in depth (i.e. the sort of discussion you might have when discussing potential solutions to a problem) then that's a great sign imo.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="42911754"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42911754" href="https://news.ycombinator.com/vote?id=42911754&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>Good interviews are a conversation, a dialog to uncover how the person thinks, how they listen, how they approach problems and discuss. Also a bit detail knowledge, but that's only a minor component in the end. Any interview where AI in its current form helps is not good anyway. Keep in mind that in our industry, the interview goes both ways. If the candidate thinks your process is bad then they are less inclined to join your company because they know that their coworkers will have been chosen by a subpar process.</p><p>That said, I'm waiting for an "interview assistant" product. It listens in to the conversation and silently provides concise extra information about the mentioned subjects that can be quickly glanced at without having to enter anything. Or does this already exist?</p><p>Such a product could be useful for coding to. Like watching me over the shoulder and seeing aha, you are working with so-and-so library, let me show you some key parts of the API in this window, or you are trying to do this-and-that, let me give you some hints. Not as intrusive as current assistants that try to write code for you, just some proactive lookup without having to actively seek out information. Anybody knows a product for that?</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912560"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912560" href="https://news.ycombinator.com/vote?id=42912560&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I'm pretty sure I've been in an interview with an 'interview assistant' and that it was another person.</p><p>This was 2-3 years ago in a remote interview. The candidate would hear the question, BS us a bit and then sometimes provide a good answer.</p><p>But then if we asked follow up questions they would blow those.</p><p>They also had odd 'AV issues' which were suspicious.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912178"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912178" href="https://news.ycombinator.com/vote?id=42912178&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>That might be good for newbie developers but for the rest of us it'll end up being the Clippy of AI assistants. If I want to know more about an API I'm using, I'll Google (or ask ChatGPT) for details; I don't need an assistant trying to be helpful and either treating me like a child, or giving me info that maybe right but which I don't need at the moment.</p><p>The only way I can see that working is if it spends hundreds of hours watching you to understand what you know and don't know, and even then it'll be a bit of a crap shoot.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="42911301"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42911301" href="https://news.ycombinator.com/vote?id=42911301&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Agreed. This is why - while I won't ding an applicant for not having a public Github, I'm always happy when they do because usually they'll have some passion projects on there that we can discuss.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42911949"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42911949" href="https://news.ycombinator.com/vote?id=42911949&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I have 23 years of experience and I am almost invisible on GitHub, and for all those years I've been fired from 4 contracts due to various disconnects (one culture mis-fit and two under-performances due to illness I wasn't aware of at the time, and one because the company literally restructured over the weekend and fired 80% of all engineers), and I have been contracting a lot in the last 10 years (we're talking 17-19 gigs).</p><p>If you look solely at my GitHub you'd likely reject me right away.</p><p>I wish I had the time and energy for passion projects in programming. I so wish it was so. But commercial work has all but destroyed my passion for programming, though I know it can be rekindled if I can ever afford to take a properly long sabbatical (at least 2 years).</p><p>I'll more agree with your parent / sibling comments: take a look at the resume and look for bad signs like too vanilla / AI language, too grandiose claims (though when you are experienced you might come across as such so 50/50), or almost no details, general tone etc.</p><p>And the best indicator is a video call conversation, I found as a candidate. I am confident in what I can do (and have done), I am energetic and love to go for the throat of the problems on my first day (provided the onboarding process allows for it) and it shows -- people have told me that and liked it.</p><p>If we're talking passion, I am more passionate about taking a walk with my wife and discussing the current book we're reading, or getting to know new people, or going to the sauna, or wondering what's the next meetup we should be going to, stuff like that. But passion + work, I stand apart by being casual and not afraid of any tech problems, and by prioritizing being a good teammate first and foremost (several GitHub-centric items come to mind: meaningful PR comments and no minutiae, good commit messages, proper textual comment updates in the PR when f.ex. requirements change a bit, editing and re-editing a list of tasks in the PR description).</p><p>I already do too much programming. Don't hold it against me if I don't live on the computer and thus have no good GitHub open projects. Talk to me. You'll get much better info.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912205"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912205" href="https://news.ycombinator.com/vote?id=42912205&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>To add to this, lots of senior people in the consultanting world are brought in under escalations.  They often have to hide the fact they are an external resource.</p><p>Also if you have a novel or disclosure sensitive passion project, GitHub may be avoided even as a very conservative bright line.</p><p>As stated above I think it can be good to find common points to enhance the interview process, but make sure to not use it as a filter.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42911799"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42911799" href="https://news.ycombinator.com/vote?id=42911799&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Also because most people are busy with actual work and don't have the time to have passion projects. Some people do, and that's great, but most people are simply not passionate about labor, regardless of what kind of labor it is.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="42910842"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42910842" href="https://news.ycombinator.com/vote?id=42910842&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>There are much more sophisticated methods than that now with AI, like speech to text to LLM. It's getting increasingly harder to detect interviewees cheating.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42911031"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42911031" href="https://news.ycombinator.com/vote?id=42911031&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I think GP's point is that this says as much about the interview design and interviewer skill as it does about the candidate's tools.</p><p>If you do a rote interview that's easy to game with AI, it will certainly be harder to detect them cheating.</p><p>If you have an effective and well designed open ended interview that's more collaborative, you get a lot more signal to filter the wheat from the chaff.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42911570"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_42911570" href="https://news.ycombinator.com/vote?id=42911570&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>&gt; <i>If you have an effective and well designed open ended interview that's more collaborative, you get a lot more signal to filter the wheat from the chaff.</i></p><p>I understood their point but my point is a direct opposition to theirs, that at some point with AI advances this will essentially become impossible. You can make it as open ended as you want but if AI continues to improve, the human interviewee can simply act as a ventriloquist dummy for the AI and get the job. Stated another way, what kind of "effective and well designed open ended interview" can you make that would not succumb to this problem?</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42911872"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_42911872" href="https://news.ycombinator.com/vote?id=42911872&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>&gt; at some point with AI advances this will essentially become impossible.</p><p>In-person interviews, second round comes with a plane ticket.  This used to be the norm.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42911739"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_42911739" href="https://news.ycombinator.com/vote?id=42911739&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>This is called fraud, and it is a crime.</p><p>People don't really call the police, nor sue over this.  But they can, and have in the past.</p><p>If it gets bad, look for people starting to seek legal recourse.</p><p>People aren't developers with 5 years experience, if all they can do is copy and paste.  Anyone fraudulently claiming so is a scam artist, a liar, and deserves jail time.</p><p>So you create an interview process that can only be passed by a skilled dev, including them signing a doc saying the code is entirely their work, only referencing a language manual/manpages.</p><p>And if they show up to work incapable of doing the same, it's time to call the cops.</p><p>That's probably the only way to deal with scam artists and scum, going forward.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42911871"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_42911871" href="https://news.ycombinator.com/vote?id=42911871&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Can you cite case law around where some one misrepresented their capabilities in a job interview and were criminally prosecuted? Like what criminal statute specifically was charged? You won’t find it, because at worst this would fall under a contract dispute and hence civil law. Screeching “fraud is a crime” hysterically serves no one.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912395"><td><table>  <tbody><tr>    <td indent="6"><img src="https://news.ycombinator.com/s.gif" height="1" width="240"></td><td>
      <center><a id="up_42912395" href="https://news.ycombinator.com/vote?id=42912395&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>Fraud can be described as deceit to profit in some way.  You may note the rigidity of the process above, where I indicated a defined set of conditions.</p><p>It costs employers money to on board someone, not just in pay, but in other employees training that person.  Obviously the case must be clear cut, but I've personally hired someone who clearly cheated during the remote phone interview, and literally couldn't even code a function in any language in person.</p><p>There are people with absolutely no background as a coder, applying to jobs with 5 years experience, then fraudulently misrepresenting the work of others at their own, to get the job.</p><p>That's fraud.</p><p>As I said, it's not being prosecuted as such now.  But if this keeps up?</p><p>You can bet it will be.</p><p>Because it is fraud.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912972"><td></td></tr>
                        <tr id="42912073"><td></td></tr>
                                          <tr id="42912709"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42912709" href="https://news.ycombinator.com/vote?id=42912709&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>With AI making traditional coding problems trivial, tech interviews are shifting toward practical, real-world challenges, system design, and debugging exercises rather than pure algorithm puzzles. Some companies are revisiting in-person whiteboarding to assess thought processes, while others embrace AI, evaluating how candidates integrate it into their workflow. There's also a greater focus on explaining decisions, trade-offs, and collaboration. Instead of banning AI, many employers now test how effectively candidates use it while ensuring they have foundational skills. The trend favors assessing problem-solving in real work scenarios rather than just coding ability under artificial constraints.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42911831"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42911831" href="https://news.ycombinator.com/vote?id=42911831&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>The traditional tech interview was always designed to optimize for reliably finding someone who was willing to do what they were told even if it feels like busywork. As a rule someone who has the time and the motivation to brush up on an essentially useless skill in order to pass your job interview will likely fit nicely as a cog in your machine.</p><p>AI doesn't just change the interviewing game by making it easy to cheat on these interviews, <i>it should be changing your hiring strategy altogether</i>. If you're still thinking in terms of optimizing for cogs, you're missing the boat—unless you're hiring for a very short term gig what you need now is someone with high creative potential and great teamwork skills.</p><p>And as far as I know there is no reliable template interview for recognizing someone who's good at thinking outside the box and who understands people. You just have to talk to them: talk about their past projects, their past teams, how they learn, how they collaborate. And then you have to get good at understanding what kinds of answers you need for the specific role you're trying to fill, which will likely be different from role to role.</p><p>The days of the interchangeable cog are over, and with them easy answers for interviewing.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42911932"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42911932" href="https://news.ycombinator.com/vote?id=42911932&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>Have you spent a lot of time trying to hire people? I guarantee you there is no shadow council trying to figure out how to hire "busywork" worker bees. This perspective smells completely like "If I were in charge, things would be so much better." Guess what? If you were to take your idea and try to lead this change across a 100 people engineering org, there would be "out of the box thinkers" who would go against your ideas and cause dissent. At that point, guess what? You're going to figure out how to hire compliant people who will execute on your strategy.</p><p>"talk about their past projects, their past teams, how they learn, how they collaborate"</p><p>You have now excluded amazing engineers who suck at talking about themselves in interviews. They may be great collaborators and communicators, but freeze up selling themselves in an interview.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912575"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912575" href="https://news.ycombinator.com/vote?id=42912575&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>&gt; You have now excluded amazing engineers who suck at talking about themselves in interviews. They may be great collaborators and communicators, but freeze up selling themselves in an interview.</p><p>This was the norm until perhaps for about the last 10-15 years of Software Engineering.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912089"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912089" href="https://news.ycombinator.com/vote?id=42912089&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>My take is:</p><p>- “big” tech companies like Google, Amazon, Microsoft came up with these types of tech interviews. And there it seems pretty clear that for most of their positions they are looking for cogs</p><p>- The vast majority of tech companies have just copied what “big” tech is doing, including tech interviews. These companies may not be looking for cogs, but they are using an interview process that’s not suitable for them</p><p>- Very few companies have their own interview process suitable for them. These are usually small companies and therefore  the number of engineers in such companies is negligible to be taken into account (most likely, less than 1% of the audience here work at such companies)</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912409"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912409" href="https://news.ycombinator.com/vote?id=42912409&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>&gt; I guarantee you there is no shadow council trying to figure out how to hire "busywork" worker bees.</p><p>The council itself is made of "busywork" worker bees. Slave hiring slaves - the vast majority of IT interviewers and candidates are idiot savants - they know very little outside of IT, or even realize  that there is more to life than IT.</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="42910806"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42910806" href="https://news.ycombinator.com/vote?id=42910806&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>The key is having interviewers that know what they are talking about so in-depth meandering discussions can be had regarding personal and work projects which usually makes it clear whether the applicant knows what they are talking about. Leetcode was only ever a temporary interview technique, and this 'AI' prominence in the public domain has simply sped up it's demise.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42911989"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42911989" href="https://news.ycombinator.com/vote?id=42911989&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>This completely..</p><p>You ask a rote question and you'll get a rote answer while the interviewee is busy looking at a fixed point on the screen.</p><p>You then ask a pointed question about something they know or care about, and suddenly their face lights up, they're animated, and they are looking around.</p><p>It's a huge tell.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912589"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912589" href="https://news.ycombinator.com/vote?id=42912589&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>You know, this makes me wonder if a viable remote interview technique, at least until real-time deepfaking gets better, would be to have people close their eyes while talking to them. For somebody who knows their stuff it'll have zero impact; for someone relying entirely on GPT, it will completely derail them.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="42910923"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42910923" href="https://news.ycombinator.com/vote?id=42910923&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>This is the way. We do an intro call, an engineering chat (exactly as you describe), a coding challenge and 2 team chat sessions in person. At the end of that, we usually have a good feeling about how sharp the candidate is, of they like to learn and discover new things, what their work ethic is. It's not bullet proof, but it removes a lot of noise from the signal.</p><p>The coding challenge is supposed to be solved with AI. We can no longer afford not to use LLMs for engineering, as it's that much of a productivity boost when used right, so candidates should show how they use LLMs. They need to be able to explain the code of course, and answer questions about it, but for us it's a negative mark of a candidate proclaims that they don't use LLMs.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42911061"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42911061" href="https://news.ycombinator.com/vote?id=42911061&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>&gt; <i>The coding challenge is supposed to be solved with AI. We can no longer afford not to use LLMs for engineering, as it's that much of a productivity boost when used right, so candidates should show how they use LLMs. They need to be able to explain the code of course, and answer questions about it, but for us it's a negative mark of a candidate proclaims that they don't use LLMs.</i></p><p>Do you state this upfront or is it some hidden requirement? Generally I'd expect an interview coding exercise to not be done with AI, but if it's a hidden requirement that the interviewer does not disclose, it is unfair to be penalized for not reading their minds.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42911225"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_42911225" href="https://news.ycombinator.com/vote?id=42911225&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I would say as long as it is stated you can complete the coding exercise using any tool available it is fine. I do agree, no task should be a trick.</p><p>I am personally of the view you should be able to use search engines, AI, anything you want, as the task should be representative of doing the task in person. The key focus has to be the programmer's knowledge and why they did what they did.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42911622"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_42911622" href="https://news.ycombinator.com/vote?id=42911622&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>One client of mine has a couple repositories for non-mission critical things like their fork of an open source project, decommissioned microservices, a SVG generator for their web front-end, etc.</p><p>They also take this approach of "whatever tool works," but their coding test is "here's some symptoms of the SVG generator misbehaving, figure out what happened and fix it," which requires digging into the commit history, issues, actually looking at the SVG output, etc.</p><p>Once you've figured out how the system architecture works, and the most likely component to be causing the problem, you have to convert part of the code to use a newer, undocumented API exposed by a RPC server that speaks a serialization format that no LLM has ever seen before.  Doing this is actually way faster and accurate using an AI, if you know how to centaur with it and make sure the output is tested to be correct.</p><p>This is a much more representative test of how someone's going to handle doing actual work knocking issues out.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42911707"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_42911707" href="https://news.ycombinator.com/vote?id=42911707&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Reminds me of the old joke/story where the Caltech student asks, "Can we use Feynman in this open-book exam?"</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="42911995"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_42911995" href="https://news.ycombinator.com/vote?id=42911995&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>Well, the challenge involves using a python LLM framework to build a simple RAG system for recipes.</p><p>It's not a hidden requirement per se to use LLM assistance, but the candidate should have a good answer ready why they didn't use an LLM to solve the challenge.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912221"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_42912221" href="https://news.ycombinator.com/vote?id=42912221&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>Why is it a negative that the candidate can solve the challenge without using an LLM? I don’t really understand this.</p><p>Also, what is a good answer for not using one? Will you provide access to one during the course of the interview? Or I am just expected to be paying for one?</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912665"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_42912665" href="https://news.ycombinator.com/vote?id=42912665&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>It's not negative that the candidate can solve it without an LLM, but it is positive if the candidate can use the LLM to speed up the solution. The code challenge is timeboxed.</p><p>We are providing an API key for LLM inference, as implementing the challenge requires this as well.</p><p>And I haven't heard a good answer yet for not using one, ideally the candidate knows how to mitigate the drawbacks of LLMs while benefiting from their utility regardless.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912695"><td><table>  <tbody><tr>    <td indent="6"><img src="https://news.ycombinator.com/s.gif" height="1" width="240"></td><td>
      <center><a id="up_42912695" href="https://news.ycombinator.com/vote?id=42912695&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>&gt;I haven’t heard a good answer for not using one</p><p>Again, what would be a good answer? Or are you just saying there isn’t one?</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="42912075"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_42912075" href="https://news.ycombinator.com/vote?id=42912075&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Ah so you expect mind readers who can divine something from your brain that goes against 99.99% of interviewers' practices and would get them instantly disqualified from an overwhelming majority of interviews. Nice work good luck finding candidates.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="42912651"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912651" href="https://news.ycombinator.com/vote?id=42912651&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>&gt; as it's that much of a productivity boost when used right</p><p>Frankly, if an interviewer told me this, I would genuinely wonder why what they're building is such a simple toy product that an LLM can understand it well enough to be productive.</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="42912328"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42912328" href="https://news.ycombinator.com/vote?id=42912328&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>On our side we've transitioned to only in person interviews.</p><p>The biggest thing I've noticed is take home challenges have lost all value. Since GPT can plausibly solve almost anything you throw at it, and it doesn't give you any indication of how the candidate thinks.</p><p>And to be fair, I want a candidate that uses GPT / Cursor / whatever tools get the job done. But reading the same AI solution to a coding challenge doesn't tell me anything about how they think or approach problems.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912498"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42912498" href="https://news.ycombinator.com/vote?id=42912498&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I'm not a fan of take-home challenges anyway (for the most part). Anything non-trivial is a big time suck and you <i>know</i> some people will spend all weekend on you two hour assignment.</p><p>Sometimes you have to. In my previous analyst stint a writing sample was pretty non-negotiable unless they could oint to publicly-published material--which was much preferred. ChatGPT isn't much use there except to save some time. It's very formulaic and wouldn't pass though, honestly, some people are worse on their own.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="42911710"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42911710" href="https://news.ycombinator.com/vote?id=42911710&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I don’t know the answer, but I’d like to share that I asked a simple question about scheduling a phone interview to learn more about a candidate.</p><p>The candidate’s first response? “Memory updated”. That led to some laughs internally and then a clear rejection email.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42911725"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42911725" href="https://news.ycombinator.com/vote?id=42911725&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>My first read of this was they made a joke (not wise when scheduling for interviews sure but maybe funny) by intentionally responding that way.</p><p>This is because my brain couldn't fathom what is likely the reality here -- that someone was just pumping your email thru AI and pumping the response back unedited and unsanitized, and so the first thing you got back was just the first "part" of the AI response.</p><p>...Christ.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42911928"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42911928" href="https://news.ycombinator.com/vote?id=42911928&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>I'm with you. Looking at the way people respond online to things now since LLMs and GenAI went mainstream is baffling. So many comments along the lines of "this is AI" when there are more ordinary explanations.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42911896"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42911896" href="https://news.ycombinator.com/vote?id=42911896&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Yeah I don't know about this specific situation, but as someone who is on the job market, is a good developer, but can come off as a little odd sometimes, I often wonder how often I roll a natural 1 on my Cha check and get perceived as an AI imposter.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912215"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_42912215" href="https://news.ycombinator.com/vote?id=42912215&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>If anything, coming across as “a little odd” can be a sign I’m actually talking to a human.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912524"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_42912524" href="https://news.ycombinator.com/vote?id=42912524&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>That's a good point. The major LLMs are all tilted so much towards a weird blend of corpo-speak with third-world underpaid English speaker influence (e.g. "delve", from common Nigerian usage) that having any quirks at all outside that is a good sign.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="42912183"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912183" href="https://news.ycombinator.com/vote?id=42912183&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>Your perception of the reality is spot on. For this round I was hiring for entry level technical support and we had limited time to properly vet candidates.</p><p>Unfortunately what we end up doing is have to make some assumptions. If something seems remotely fishy, like that “Memory updated” or typeface change (ChatGPT doesn’t follow your text formatting when pasting into your email compose window), it raises a lot of eyebrows and very quickly leads to a rejection. There’s other cases where your written English is flawless but your phone interview indicates you don’t understand the English language compared to when we correspond over email/Indeed/etc.</p><p>Mind you, this is all before we even get to the technical knowledge part of any interview.</p><p>On a related hire, I am also in the unfortunate position where we may have to let a new CS grad go because it seemed like every code change and task we gave him was fully copy/pasted through ChatGPT. When presented with a simple code performance and optimization bug, he was completely lost on general debugging practices which led our team to question his previous work while onboarding. Using AI isn’t against company policy (see: small team with limited resources), but personally I see over reliance on ChatGPT as much, much worse than blindly following Stack Overflow.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912326"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_42912326" href="https://news.ycombinator.com/vote?id=42912326&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>&gt; typeface change</p><p>Long live plain text email.</p><pre><code>    ()  ascii ribbon campaign - against HTML e-mail 
    /\  www.asciiribbon.org   - against proprietary attachments</code></pre></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912401"><td></td></tr>
                        <tr id="42912841"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912841" href="https://news.ycombinator.com/vote?id=42912841&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>A friend of mine works with industrial machines, and once was tasked with translating machine's user's manual, even though he doesn't speak English. I do, and I had some free time, so I helped him. As an example, I was given user manual for a different, but similar machine.</p><p>1. The manual was mostly a bunch of phrases that were grammatically correct, but didn't really convey much meaning</p><p>2. The second half of the manual talked about a different machine than the first half</p><p>3. It was full of exceptionally bad mistranslations, and to this day "trained signaturee of the employee" is our inside joke</p><p>Imagine asking ChatGPT to write a manual except ChatGPT has down syndrome and a heart attack so it gives you five pages of complete bullshit. That was real manual that got shipped a 100 000€ or so machine. And nobody bothered to proofread it even once.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912938"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_42912938" href="https://news.ycombinator.com/vote?id=42912938&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>I once worked in the US for a Japanese company that had their manuals "translated" into English and then sent on for polishing. Like the parent, it would be mostly "<i>a bunch of phrases that were grammatically correct, but didn't really convey much meaning</i>" .  I couldn't spend more than an hour a day on that kind of thing; more than that and it would start to make sense.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                              <tr id="42912647"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42912647" href="https://news.ycombinator.com/vote?id=42912647&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>We haven't seen major issues with AI with candidates on camera. The couple that have tried to cheat have done so rather obviously, and the problem we use is more about problem-solving than it is about reverse-a-linked-list.</p><p>This is borne out by results downstream with clients. No client we've sent more than a couple of people has ever had concerns about quality, so we're fairly confident that we are in fact detecting the cheating that is happening with reasonable consistency.</p><p>I actually just looked at our data a few days ago to see how candidates who listed LLMs or related terms on their resume did on our interview. On average, they did much worse (about half the pass rate, and double the hard-fail rate). I suspect this is a general "corporate BS factor" and not anything about LLMs specifically, but it's certainly relevant.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42910524"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42910524" href="https://news.ycombinator.com/vote?id=42910524&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>As someone currently job searching it hasn’t changed much, besides companies adding DO NOT USE AI warnings before every section. Even Anthropic forces you to write a little “why do you want to work here DO NOT USE AI” paragraph. The irony.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42911230"><td></td></tr>
                <tr id="42911728"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42911728" href="https://news.ycombinator.com/vote?id=42911728&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>Applying at Anthropic was a bad experience for me. I was invited to do a timed set of leetcode exercises on some website. I didn't feel like doing that, and focused on my other applications.</p><p>Then they emailed me a month later after my "invitation" expired. It looked like it was written by a human: "Hey, we're really interested in your profile, here's a new invite link, please complete this automated pre-screen thingie".</p><p>So I swallowed my pride and went through with that humiliating exercise. Ended up spending two hours doing algorithmic leetcode problems. This was for a product security position. Maybe we could have talked about vulnerabilities that I have found instead.</p><p>I was too slow to solve them and received some canned response.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912150"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912150" href="https://news.ycombinator.com/vote?id=42912150&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>fyi, that's because (from experience) the last job req I publicly posted generated almost 450 responses, and (quite generously) over a third were simply not relevant.  It was for a full-stack rails eng.  Here, I'm not even including people whose experience was django or even React; I mean people with no web experience at all, or were not in the time zone requested.  Another 20% or so were nowhere near the experience level (senior) requested either.</p><p>The price of people bulk applying with no thought is I have to bulk filter.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912198"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_42912198" href="https://news.ycombinator.com/vote?id=42912198&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>So you allow yourself to use AI in order to save time, but we have to put up with the shit[1] companies make up? That's good, it's for the best if I don't work for a company that thinks so lowly of its potential candidates.</p><p>[1]: Including but not limited to: having to manually fill a web form because the system couldn't correctly parse a CV; take-home coding challenges; studying for LeetCode interviews; sending a perfectly worded, boot-licking cover letter.</p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="42910879"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42910879" href="https://news.ycombinator.com/vote?id=42910879&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Changed enormously. Both resumes and interviews are effectively useless now. If our AI agents can't find a portfolio of original work nearly exactly what we want to hire you for then you aren't ever going to hear from us. If you are one of the 1 in 4000 applications who gets an interview then you're already 70% likely to get an offer and the interview is mostly a formality.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912000"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42912000" href="https://news.ycombinator.com/vote?id=42912000&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>What worked for me is just ignoring the job listing websites, and calling recruiters directly on the phone. Don’t bother hitting “easy apply” just scroll to the bottom and call the number.</p><p>I’ve also been asked for the first time in ages to come to the companies office to do interviews.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42911194"><td></td></tr>
            <tr id="42910904"><td></td></tr>
                <tr id="42911247"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42911247" href="https://news.ycombinator.com/vote?id=42911247&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I thought that meant what you typically write in the "Experience" section. GP, am I wrong?</p><p>Is everyone writing a "Projects" section by rewording what they wrote in "Experience"?! For me, "Projects" should strictly be personal projects. If not, maybe that's what I'm missing.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42911864"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_42911864" href="https://news.ycombinator.com/vote?id=42911864&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>Projects are personal projects, or at least projects in which you did a distinguishable effort.</p><p>They don't have to be public to the whole world, you can have links that are only in your resume.</p><p>But if they're on GitHub, they have to be public, since there aren't unlisted repositories.</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="42912608"><td></td></tr>
                  <tr id="42910984"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42910984" href="https://news.ycombinator.com/vote?id=42910984&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I recently reviewed a medium-complexity assignment—just questions, no coding—and out of six candidates, I only approved one. The others were disqualified because their answers were filled with easily identifiable ChatGPT-generated fluff.</p><p>And I had made it clear that they should use their own words.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912725"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42912725" href="https://news.ycombinator.com/vote?id=42912725&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>My company actually encourages the use of AI. My interview process was one relatively complex take home, an explanation of my solutions and thinking, then a live "onsite" (via zoom) where I had to code in front of a senior engineer while thinking aloud.</p><p>If I was incompetent, I could've shoved the problem into o1 on ChatGPT and probably solved the problems, but I wouldn't have been able to provide insight into why I made the design choices I made and how I optimized my solutions, and that would've ultimately gotten me thrown out of the candidate pool.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42909842"><td></td></tr>
                <tr id="42911641"><td></td></tr>
                <tr id="42911803"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42911803" href="https://news.ycombinator.com/vote?id=42911803&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>&gt; hiring market tightened up... that doesn't mean there isn't one</p><p>tightened market is one thing, the absolute insanity of the recruitment process in last couple years with now AI thrown into the mix is really something to behold, test these waters at your own peril</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="42911854"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42911854" href="https://news.ycombinator.com/vote?id=42911854&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I don't understand why an interviewer would ban the use of AI if they are allowed to use AI in the role.</p><p>The interview is a chance to see how a candidate performs in a work like environment. Let them use the tools they will use on the job and see how well they can perform.</p><p>Even for verbal interviews, if they are using ChatGPT on the side and can manage the conversation satisfactorily then more power to them.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912422"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42912422" href="https://news.ycombinator.com/vote?id=42912422&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Run competitions. If you're hiring fresh grads, this is probably the best way to filter by skill. If you can use AI to beat all the other candidates that's a skill by itself. In practice, those that use AI rarely ever make it into the top 10. Add a presentation/demo as part of the criteria to filter out those with bad communication skills.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912195"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42912195" href="https://news.ycombinator.com/vote?id=42912195&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>Shouldn't a portfolio of personal projects be enough ? In the past couple years I:</p><p>- adapted Java's Regex engine to work on streams of characters</p><p>- wrote a basic parametric geometry engine</p><p>- wrote a debugger for an async framework</p><p>- did innovative work with respect to whole-codebase transformation using macros</p><p>Among other things.</p><p>As for ChatGPT in the context of an interview, I'd only use it if I were asked to do changes on a codebase I don't know in limited time.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912273"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42912273" href="https://news.ycombinator.com/vote?id=42912273&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I think the arguments is there is no way to validate it was you that did the work. There's been too many instances of groups that do interviews for others or the work for take homes to help get people placed. There was a big deal about some H1Bs a while back where the people that showed up didn't look anything like the people that interviewed. So I understand both sides.</p><p>It's frustrating though when you've done a lot of work, as you've listed. I think in a good interview maybe going over that code and getting the chance to explain things you did, why you did, or issues you had, could also go a long way.</p><p>Interviewing is tough, more so at scale.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912305"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42912305" href="https://news.ycombinator.com/vote?id=42912305&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Bit annoying is that when companies ask for a portfolio, they often mean GitHub.  Lot of non-technical hiring people I discussed this with were confused by the fact that there are other ways to contribute, like mailing lists.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="42912344"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42912344" href="https://news.ycombinator.com/vote?id=42912344&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>For the time being, I’ve banned LLMs in my interviews.</p><p>I want to see how the candidate reasons about code. So I try to ask practical questions and treat them like pairing sessions.</p><p>- Given a broke piece of code, can you find the bug and get it working?</p><p>- Implement a basic password generator, similar to 1Password (with optional characters and symbols)</p><p>If you can reason about code without an LLM, then you’ll do even better with an LLM. At least, that’s my theory.</p><p>I never ask trick questions. I never pull from Leetcode. I hardly care about time complexity. Just show me you can reason about code. And if you make some mistakes, I won’t judge you.</p><p>I’m trying to be as fair as possible.</p><p>I do understand that LLMs are part of our lives now. So I’m trying to explore ways to integrate them into the interview. But I need more time to ponder.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912478"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42912478" href="https://news.ycombinator.com/vote?id=42912478&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>Thinking out loud, here’s one idea for an LLM-assisted interview:</p><p>- Spin up a Digital Ocean droplet</p><p>- Add the candidate’s SSH key</p><p>- Have them implement a basic API. It must be publicly accessible.</p><p>- Connect the API to a database. Add more features.</p><p>- Set up a basic deployment pipeline. Could be as simple  as script that copies the code from your local machine to the server.</p><p>Anything would be fair game. The goal would be to see how the candidate converses with the LLM, how they handle unexpected changes, and how they make decisions.</p><p>Just a thought.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="42911599"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42911599" href="https://news.ycombinator.com/vote?id=42911599&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>I've been on both sides recently, and it hasn't really changed significantly. If you're heming and hawing you're not getting the job.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42911360"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42911360" href="https://news.ycombinator.com/vote?id=42911360&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>I have a colleague that uses AI to comment on RFCs. It is so clearly machine generated, that I wonder if I am the only one to see it. 
He is a good colleague though, but as he is a bit junior, it is still not clear to me if AI is helping him to improve faster or if it is hindering his deep learning of stuff.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912443"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42912443" href="https://news.ycombinator.com/vote?id=42912443&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>I listened in to someone interviewing people since many people used AI. It's the same with googling the answer it is very obvious that someone is taking too long to get to the answer and or you can't see a separate screen. Mitigation is literally looking at the text window and seeing if they are not typing / taking too long to even make a bad implementation. There is now a problem if you allow for google since google will autogen a gemini query to solve it.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912551"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42912551" href="https://news.ycombinator.com/vote?id=42912551&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>I expect in person interviews are going to be the norm soon, assuming they’re not already.  For now, the challenge I give candidates causes ChatGPT to produce convoluted code that a human never would. I then ask the person to explain the code line-by-line and they’re almost never able to give a satisfactory answer</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912251"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42912251" href="https://news.ycombinator.com/vote?id=42912251&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>If your interview process is susceptible to AI then you don't need to hire for the job. Just use an AI and prompt it.</p><p>The job you are therefore hiring for is now trivial. If it weren't, no amount of AI could pass your interview process.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912666"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42912666" href="https://news.ycombinator.com/vote?id=42912666&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I belive this line of thinking mistakes the result with the process, similar to assuming that the reason companies ask people to reverse a linked list is because there's an unmet market demand for list-reversal algorithms.</p><p>An interview has to be hard enough to filter those that are unqualified but also easy enough that the right person can pass with some minor preparation. If an interviewer asked me for the equivalent of production-ready code to add support for custom hardware in the Linux kernel I'd either reply with my freelance hourly rate or I'd end the interview.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="42911743"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42911743" href="https://news.ycombinator.com/vote?id=42911743&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>In my most recent cycle, I didn’t ask to use AI and I was only warned once about using AI when I had the official language plugin for an IDE annotate some struct fields with json tags. I explained the plugin functionality and we moved on.</p><p>When I was part of interviews on the other side for my former employer, I encountered multiple candidates who appeared to be using AI assistance without notifying the interviewers ahead of time or at all.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42911615"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42911615" href="https://news.ycombinator.com/vote?id=42911615&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I've been very curious about this and about how we should modify our hiring. Its obvious that an individual should be able to use AI companions to build better, faster, higher quality things... But the skillsets are sooo uneven now that its unfair to those who are with and without.</p><p>I think it ultimately comes back to impact (like always) which has remained largely unchanged.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42911683"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42911683" href="https://news.ycombinator.com/vote?id=42911683&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I haven't done any hiring in a while, but my feelings on the matter:</p><p>If they can talk through the technology and code fluently, honestly, I don't care how they do the work. Honestly I feel like the ability to communicate is a far more important skill than the precise technology.</p><p>This is of course presumes you have a clue about the technology you're hiring for.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42911905"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42911905" href="https://news.ycombinator.com/vote?id=42911905&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>Buddy of mine recently got a position with the help of custom built model that was listening on the call and printed answers on another screen. The arms race is here and frankly, given that a lot of people are already using it at work, there is no way to stop it short of minute upon minute supervision and even biggest micromanagers won't be able to deal with it.</p><p>Honestly, if I could trust that companies won't try to evaluate my conversation through 20 different ridiculous filters, I would probably argue that my buddy is out of line.. As it stands, however, he is merely leveling out the playing field. But, just life with WFH, management class does not like that imposition one bit.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912256"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42912256" href="https://news.ycombinator.com/vote?id=42912256&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>People are sending us emails that are not just written with chatgpt, but I think they've automated the process as well, as parts of the prompt slip in.</p><p>You can see things in the emails like:</p><p>"I provided a concise, polite response from a candidate to a job rejection, expressing gratitude, a desire for feedback, and interest in future opportunities."</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912851"><td></td></tr>
                  <tr id="42911523"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42911523" href="https://news.ycombinator.com/vote?id=42911523&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Panel interviews seem to be more common.  Curious if others have seen the same?  I personally feel very uncomfortable coding in front of a group.  First one of these I tried had like 5 people watching and I lost my nerve and bailed. :|</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912281"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42912281" href="https://news.ycombinator.com/vote?id=42912281&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Panels and live programming assignments are such an awful idea. Is that what the workplace is like? Doo they want people who can work under those conditions? I've been a working professional for 18 years who gives public talks regularly and I can still see myself clamming up in that situation. Everyone knows it's hard to think and type when you are being watched.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="42912676"><td></td></tr>
            <tr id="42911525"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42911525" href="https://news.ycombinator.com/vote?id=42911525&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>It's still remote. I don't get how you could pass an interview using ChatGPT unless it's purely leetcode.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42911737"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42911737" href="https://news.ycombinator.com/vote?id=42911737&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I can't speak for job interviewing, but having recently completed 3rd-semester trade-school oral exams in Java programming:</p><p>It is really important to watch people code.</p><p>Anyone can fake an abstract overview.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42911801"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42911801" href="https://news.ycombinator.com/vote?id=42911801&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>It can be weird. Seen some decent resumes for people that in the actual interview the candidate obviously has zero demonstrable knowledge of.</p><p>Ask even the shallowest question and they are lost and just start regurgitating what feels like very bad prompt based responses.</p><p>At that point it's just about closing down the interview without being unprofessional.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42910395"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42910395" href="https://news.ycombinator.com/vote?id=42910395&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>Not sure why interviews would change.</p><p>Even if you're using ChatGPT heavily it's your job to ensure it's right. And you need to know what to ask it. So you still need all the same skills and conceptual understanding as before.</p><p>I mean, I didn't observe interviews change after powerful IDE's replaced basic text editors.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42910678"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42910678" href="https://news.ycombinator.com/vote?id=42910678&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Because interviews were always an attempt to discern a signal few hours interview into an accurate prediction of performance from months to years. AIs generate a lot of nosie to mask that. Interviewees can just pass the question to the AI, who will generate a reasonable sounding response.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42910571"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42910571" href="https://news.ycombinator.com/vote?id=42910571&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Because there's a format of interview that's basically a brainteaser that takes 45 minutes to think through and whiteboard some code for, but which is trivially solvable by copy and pasting a screenshot of the prompt into ChatGPT. This amounts to candidates being given the answer and then pretending to struggle with understanding your question and then figuring out a solution to it when really they're just stalling for time and then just copying the answer from one browser tab to the next.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="42912126"><td></td></tr>
            <tr id="42912698"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42912698" href="https://news.ycombinator.com/vote?id=42912698&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>This conversation feels bizarrely tone deaf. The skill of being able to recall specific knowledge on demand is going away.</p><p>How LLMs will evaluate a skill they are making obsolete is a question I am not sure I understand.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912736"><td></td></tr>
                  <tr id="42910403"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42910403" href="https://news.ycombinator.com/vote?id=42910403&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I mentioned this in a different related post but there seems to be a pretty sad lack of basic integrity in the tech world where it's become a point of pride to develop and use apps which allow an applicant to blatantly cheat during interviews using LLMs.</p><p>As a result, several of my friends who assist in hiring at their companies have already returned to "on-site" interviews. The funny thing about this is that these are 100% remote jobs - but the interviews are being conducted at shared workspaces. This is what happens when the level of trust goes down to zero due to bad actors.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912077"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42912077" href="https://news.ycombinator.com/vote?id=42912077&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>Largely past COVID it seems like sheer laziness or cheapness not to conduct in-person interviews for a professional job other than a short-term project after essentially an initial screen for all sorts of reasons that have little to do with cheating. I don’t care if the job is largely remote.</p><p>As someone else noted, this used to be utterly standard. And frankly I’d probably just pass on someone who balked. Plenty of fish in the sea.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="42911837"><td></td></tr>
            <tr id="42910634"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42910634" href="https://news.ycombinator.com/vote?id=42910634&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>There are some tools that read your screen and can provide hints and solutions for coding type questions. I honestly don't trust myself to not mess it up, plus the whole ethics side of it, but I'm sure that will always be a problem for online assessments</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42911686"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42911686" href="https://news.ycombinator.com/vote?id=42911686&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I tell everyone to share their entire screen, have their video on, and start coding. It's not that different. Even as an interviewer, I experimented with the usual cheating techniques so I know what to look out for. The best are the AI teleprompters. If you can do the work with your own AI then I see no need to care as the business will not care either.</p><p>The story is completely different for airgapped dark room jobs, but if you know you know.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912555"><td></td></tr>
                  <tr id="42912661"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42912661" href="https://news.ycombinator.com/vote?id=42912661&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>If a problem can be “trivially” solved by GPT. The problem is with your interview process, not the tool. It’s wild to me that interviewers still ask candidates for senior positions leetcode type questions. Yet the actual job is for some front end or devops position.</p><p>The gap between interview and actual on job duties is very wide at many — delusional - companies.</p></div></td></tr>
        </tbody></table></td></tr>
            </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Lume – OS lightweight CLI for MacOS & Linux VMs on Apple Silicon. (273 pts)]]></title>
            <link>https://github.com/trycua/lume</link>
            <guid>42908061</guid>
            <pubDate>Sun, 02 Feb 2025 11:46:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/trycua/lume">https://github.com/trycua/lume</a>, See on <a href="https://news.ycombinator.com/item?id=42908061">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><strong>lume</strong> is a lightweight Command Line Interface and local API server to create, run and manage macOS and Linux virtual machines (VMs) with near-native performance on Apple Silicon, using Apple's <code>Virtualization.Framework</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Run prebuilt macOS images in just 1 step</h3><a id="user-content-run-prebuilt-macos-images-in-just-1-step" aria-label="Permalink: Run prebuilt macOS images in just 1 step" href="#run-prebuilt-macos-images-in-just-1-step"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/trycua/lume/blob/main/img/cli.png"><img src="https://github.com/trycua/lume/raw/main/img/cli.png" alt="lume cli"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="lume run macos-sequoia-vanilla:latest"><pre>lume run macos-sequoia-vanilla:latest</pre></div>
<p dir="auto">For a python interface, check out <a href="https://github.com/trycua/pylume">pylume</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="lume <command>

Commands:
  lume create <name>            Create a new macOS or Linux VM
  lume run <name>               Run a VM
  lume ls                       List all VMs
  lume get <name>               Get detailed information about a VM
  lume set <name>               Modify VM configuration
  lume stop <name>              Stop a running VM
  lume delete <name>            Delete a VM
  lume pull <image>             Pull a macOS image from container registry
  lume clone <name> <new-name>  Clone an existing VM
  lume images                   List available macOS images in local cache
  lume ipsw                     Get the latest macOS restore image URL
  lume prune                    Remove cached images
  lume serve                    Start the API server

Options:
  --help     Show help [boolean]
  --version  Show version number [boolean]

Command Options:
  create:
    --os <os>            Operating system to install (macOS or linux, default: macOS)
    --cpu <cores>        Number of CPU cores (default: 4)
    --memory <size>      Memory size, e.g., 8GB (default: 4GB)
    --disk-size <size>   Disk size, e.g., 50GB (default: 40GB)
    --display <res>      Display resolution (default: 1024x768)
    --ipsw <path>        Path to IPSW file or 'latest' for macOS VMs

  run:
    --no-display         Do not start the VNC client app
    --shared-dir <dir>   Share directory with VM (format: path[:ro|rw])
    --mount <path>       For Linux VMs only, attach a read-only disk image

  set:
    --cpu <cores>        New number of CPU cores
    --memory <size>      New memory size
    --disk-size <size>   New disk size

  delete:
    --force              Force deletion without confirmation

  pull:
    --registry <url>     Container registry URL (default: ghcr.io)
    --organization <org> Organization to pull from (default: trycua)

  serve:
    --port <port>        Port to listen on (default: 3000)"><pre>lume <span>&lt;</span>command<span>&gt;</span>

Commands:
  lume create <span>&lt;</span>name<span>&gt;</span>            Create a new macOS or Linux VM
  lume run <span>&lt;</span>name<span>&gt;</span>               Run a VM
  lume ls                       List all VMs
  lume get <span>&lt;</span>name<span>&gt;</span>               Get detailed information about a VM
  lume <span>set</span> <span>&lt;</span>name<span>&gt;</span>               Modify VM configuration
  lume stop <span>&lt;</span>name<span>&gt;</span>              Stop a running VM
  lume delete <span>&lt;</span>name<span>&gt;</span>            Delete a VM
  lume pull <span>&lt;</span>image<span>&gt;</span>             Pull a macOS image from container registry
  lume clone <span>&lt;</span>name<span>&gt;</span> <span>&lt;</span>new-name<span>&gt;</span>  Clone an existing VM
  lume images                   List available macOS images <span>in</span> <span>local</span> cache
  lume ipsw                     Get the latest macOS restore image URL
  lume prune                    Remove cached images
  lume serve                    Start the API server

Options:
  --help     Show <span>help</span> [boolean]
  --version  Show version number [boolean]

Command Options:
  create:
    --os <span>&lt;</span>os<span>&gt;</span>            Operating system to install (macOS or linux, default: macOS)
    --cpu <span>&lt;</span>cores<span>&gt;</span>        Number of CPU cores (default: 4)
    --memory <span>&lt;</span>size<span>&gt;</span>      Memory size, e.g., 8GB (default: 4GB)
    --disk-size <span>&lt;</span>size<span>&gt;</span>   Disk size, e.g., 50GB (default: 40GB)
    --display <span>&lt;</span>res<span>&gt;</span>      Display resolution (default: 1024x768)
    --ipsw <span>&lt;</span>path<span>&gt;</span>        Path to IPSW file or <span><span>'</span>latest<span>'</span></span> <span>for</span> macOS VMs

  run:
    --no-display         Do not start the VNC client app
    --shared-dir <span>&lt;</span>dir<span>&gt;</span>   Share directory with VM (format: path[:ro<span>|</span>rw])
    --mount <span>&lt;</span>path<span>&gt;</span>       For Linux VMs only, attach a read-only disk image

  set:
    --cpu <span>&lt;</span>cores<span>&gt;</span>        New number of CPU cores
    --memory <span>&lt;</span>size<span>&gt;</span>      New memory size
    --disk-size <span>&lt;</span>size<span>&gt;</span>   New disk size

  delete:
    --force              Force deletion without confirmation

  pull:
    --registry <span>&lt;</span>url<span>&gt;</span>     Container registry URL (default: ghcr.io)
    --organization <span>&lt;</span>org<span>&gt;</span> Organization to pull from (default: trycua)

  serve:
    --port <span>&lt;</span>port<span>&gt;</span>        Port to listen on (default: 3000)</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Install</h2><a id="user-content-install" aria-label="Permalink: Install" href="#install"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="brew tap trycua/lume
brew install lume"><pre>brew tap trycua/lume
brew install lume</pre></div>
<p dir="auto">You can also download the <code>lume.pkg.tar.gz</code> archive from the <a href="https://github.com/trycua/lume/releases">latest release</a>, extract it, and install the package manually.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Prebuilt Images</h2><a id="user-content-prebuilt-images" aria-label="Permalink: Prebuilt Images" href="#prebuilt-images"></a></p>
<p dir="auto">Pre-built images are available on <a href="https://github.com/orgs/trycua/packages">ghcr.io/trycua</a>.
These images come with an SSH server pre-configured and auto-login enabled.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Image</th>
<th>Tag</th>
<th>Description</th>
<th>Size</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>macos-sequoia-vanilla</code></td>
<td><code>latest</code>, <code>15.2</code></td>
<td>macOS Sonoma 15.2</td>
<td>40GB</td>
</tr>
<tr>
<td><code>macos-sequoia-xcode</code></td>
<td><code>latest</code>, <code>15.2</code></td>
<td>macOS Sonoma 15.2 with Xcode command line tools</td>
<td>50GB</td>
</tr>
<tr>
<td><code>ubuntu-noble-vanilla</code></td>
<td><code>latest</code>, <code>24.04.1</code></td>
<td><a href="https://ubuntu.com/download/server/arm" rel="nofollow">Ubuntu Server for ARM 24.04.1 LTS</a> with Ubuntu Desktop</td>
<td>20GB</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">For additional disk space, resize the VM disk after pulling the image using the <code>lume set &lt;name&gt; --disk-size &lt;size&gt;</code> command.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Local API Server</h2><a id="user-content-local-api-server" aria-label="Permalink: Local API Server" href="#local-api-server"></a></p>
<p dir="auto"><code>lume</code> exposes a local HTTP API server that listens on <code>http://localhost:3000/lume</code>, enabling automated management of VMs.</p>

<p dir="auto">For detailed API documentation, please refer to <a href="https://github.com/trycua/lume/blob/main/docs/API-Reference.md">API Reference</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Docs</h2><a id="user-content-docs" aria-label="Permalink: Docs" href="#docs"></a></p>
<ul dir="auto">
<li><a href="https://github.com/trycua/lume/blob/main/docs/API-Reference.md">API Reference</a></li>
<li><a href="https://github.com/trycua/lume/blob/main/docs/Development.md">Development</a></li>
<li><a href="https://github.com/trycua/lume/blob/main/docs/FAQ.md">FAQ</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">We welcome and greatly appreciate contributions to lume! Whether you're improving documentation, adding new features, fixing bugs, or adding new VM images, your efforts help make lume better for everyone. For detailed instructions on how to contribute, please refer to our <a href="https://github.com/trycua/lume/blob/main/CONTRIBUTING.md">Contributing Guidelines</a>.</p>
<p dir="auto">Join our <a href="https://discord.gg/8p56E2KJ" rel="nofollow">Discord community</a> to discuss ideas or get assistance.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">lume is open-sourced under the MIT License - see the <a href="https://github.com/trycua/lume/blob/main/LICENSE">LICENSE</a> file for details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Trademarks</h2><a id="user-content-trademarks" aria-label="Permalink: Trademarks" href="#trademarks"></a></p>
<p dir="auto">Apple, macOS, and Apple Silicon are trademarks of Apple Inc. Ubuntu and Canonical are registered trademarks of Canonical Ltd. This project is not affiliated with, endorsed by, or sponsored by Apple Inc. or Canonical Ltd.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Stargazers over time</h2><a id="user-content-stargazers-over-time" aria-label="Permalink: Stargazers over time" href="#stargazers-over-time"></a></p>
<p dir="auto"><a href="https://starchart.cc/trycua/lume" rel="nofollow"><img src="https://camo.githubusercontent.com/561442573e1c2212a9bfb1ba7e7bb847cfbf6f205c18769dbb73f4ea9cd1dd24/68747470733a2f2f7374617263686172742e63632f7472796375612f6c756d652e7376673f76617269616e743d6164617074697665" alt="Stargazers over time" data-canonical-src="https://starchart.cc/trycua/lume.svg?variant=adaptive"></a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Spaced repetition can allow for infinite recall (2022) (178 pts)]]></title>
            <link>https://www.efavdb.com/memory%20recall</link>
            <guid>42908041</guid>
            <pubDate>Sun, 02 Feb 2025 11:42:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.efavdb.com/memory%20recall">https://www.efavdb.com/memory%20recall</a>, See on <a href="https://news.ycombinator.com/item?id=42908041">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            
            <p>
         <img src="https://www.efavdb.com/images/jeopardy.jpg">
</p>

<p>My friend Andrew is an advocate of the “spaced repetition” technique for
memorization of a great many facts [1].  The ideas behind this are&nbsp;two-fold:</p>
<ul>
<li>
<p>When one first “learns” a new fact, it needs to be reviewed frequently in
  order to not forget it.  However, with each additional review, the fact can
be retained longer before a refresher is needed to maintain it in&nbsp;recall.</p>
</li>
<li>
<p>Because of this, one can maintain a large, growing body of facts in recall
  through daily review:  Each day, one need only review for ten minutes or so,
covering a small number of facts. The facts included should be sampled from the
full library in a way that prefers newer entries, but that also sprinkles in
older facts often enough so that none are ever forgotten.  Apps have been
written to intelligently take care of the sampling process for&nbsp;us.</p>
</li>
</ul>
<p>Taking this framework as correct motivates questioning exactly how far it can
be pushed:  <em>Would an infinitely-long-lived, but forgetful person be able to
recall an infinite number of facts using this method? </em>  <span>\(\ldots\)</span> Below, we
show that the answer is: <em><span>YES</span>!</em></p>
<h5>Proof:</h5>
<p>We first posit that the number of days <span>\(T\)</span> that a fact can be retained before
it needs to be reviewed grows as a power-law in <span>\(s\)</span>, the number of times it’s
been reviewed so&nbsp;far,</p>
<p>\begin{eqnarray} \tag{1}\label{1}
T(s) \sim s^{\gamma},
\end{eqnarray}</p>
<p>
with <span>\(\gamma &gt; 0\)</span>. With this assumption, if <span>\(N(t)\)</span> facts are to be recalled
from <span>\(t\)</span> days ago, one can show that the amount of work needed today to retain
these will go like (see appendix for a proof of this&nbsp;line)</p>
<p>\begin{eqnarray}\tag{2}\label{2}
w(t) \sim \frac{N(t)}{t^{\gamma / (\gamma + 1)}}.
\end{eqnarray}</p>
<p>
The total work needed today is then the sum of work needed for each past day’s&nbsp;facts,</p>
<p>\begin{eqnarray} \tag{3} \label{3}
W(total) = \int_1^{\infty} \frac{N(t)}{t^{\gamma / (\gamma + 1)}} dt.
\end{eqnarray}</p>
<p>
Now, each day we only have a finite amount of time to study.  However, the
above total work integral will diverge at large <span>\(t\)</span> unless it decays faster
than <span>\(1/t\)</span>.  To ensure this, we can limit the number of facts retained from
from <span>\(t\)</span> days ago to go&nbsp;as</p>
<p>\begin{eqnarray} \tag{4} \label{4}
N(t) \sim \frac{1}{t^{\epsilon}} \times \frac{1}{t^{1 / (\gamma + 1)}},
\end{eqnarray}</p>
<p>
where <span>\(\epsilon\)</span> is some small, positive constant.  Plugging (\ref{4}) into
(\ref{3}) shows that we are guaranteed a finite required study time each day.
However, after <span>\(t\)</span> days of study, the total number of facts retained scales&nbsp;as</p>
<p>\begin{eqnarray}
N_{total}(t) &amp;\sim &amp; \int_1^{t} N(t) dt \\
&amp;\sim &amp; \int_0^{t} \frac{1}{t^{1 / (\gamma + 1)}} \\
&amp;\sim &amp; t^{ \gamma / (\gamma + 1)}. \tag{5} \label{5}
\end{eqnarray}</p>
<p>
Because we assume that <span>\(\gamma &gt; 0\)</span>, this grows without bound over time,
eventually allowing for an infinitely large&nbsp;library.</p>
<p>We conclude that — though we can’t remember a fixed number of facts from each
day in the past using spaced repetition — we can ultimately recall an infinite
number of facts using this method.  To do this only requires that we gradually
curate our previously-introduced facts so that the scaling (\ref{4}) holds at
all&nbsp;times.</p>
<h3>Appendix: Proof of&nbsp;(2)</h3>
<p>Recall that we assume <span>\(N(s)\)</span> facts have been reviewed exactly <span>\(s\)</span> times.  On a
given day, the number of these that need to be reviewed then goes&nbsp;like</p>
<p>\begin{eqnarray} \tag{A1}\label{A1}
W(s) \sim \frac{N(s)}{T(s)}.
\end{eqnarray}</p>
<p>
where <span>\(T(s)\)</span> is given in (\ref{1}).  This holds because each of the <span>\(N(s)\)</span>
facts that have been studied <span>\(s\)</span> times so far must be reviewed within <span>\(T(s)\)</span>
days, or one will be forgotten.  During these <span>\(T(s)\)</span> days, each will move to
having been reviewed <span>\(s+1\)</span> times.&nbsp;Therefore,</p>
<p>\begin{eqnarray} \tag{A2} \label{A2}
\frac{ds}{dt} &amp;\sim &amp; \frac{1}{T(s)}
\end{eqnarray}</p>
<p>
Integrating this gives <span>\(s\)</span> as a function of <span>\(t\)</span>,</p>
<p>\begin{eqnarray} \tag{A3} \label{A3}
s \sim t^{1 / (\gamma + 1)}
\end{eqnarray}</p>
<p>Plugging this last line and (1) into (A1), we get&nbsp;(2).</p>
<h2>References</h2>
<p>[1] See Andrew’s blog post on spaced repetition <a href="https://andrewjudson.com/spaced-repitition/2022/06/03/spaced-repitition.html">
here</a>.</p>



             
 
                

                <hr>
    <p><a href="" target="_blank" rel="nofollow noopener noreferrer">
            <img src="https://www.efavdb.com/wp-content/uploads/2014/12/JonathanLinkedIn.jpg" alt="Jonathan Landy Avatar" title="Jonathan Landy">
            
        </a>
        Jonathan grew up in the midwest and then went to school at Caltech and&nbsp;UCLA. Following this, he did two postdocs, one at UCSB and one at UC Berkeley. &nbsp;His academic research focused primarily on applications of statistical mechanics, but his professional passion has always been in the mastering, development, and practical application of slick&nbsp;math methods/tools. He currently works as a data-scientist at Stitch Fix.
    </p>

            






            <hr>
            
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Reverse-engineering and analysis of SanDisk High Endurance microSDXC card (2020) (249 pts)]]></title>
            <link>https://ripitapart.com/2020/07/16/reverse-engineering-and-analysis-of-sandisk-high-endurance-microsdxc-card/</link>
            <guid>42907766</guid>
            <pubDate>Sun, 02 Feb 2025 10:32:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ripitapart.com/2020/07/16/reverse-engineering-and-analysis-of-sandisk-high-endurance-microsdxc-card/">https://ripitapart.com/2020/07/16/reverse-engineering-and-analysis-of-sandisk-high-endurance-microsdxc-card/</a>, See on <a href="https://news.ycombinator.com/item?id=42907766">Hacker News</a></p>
<div id="readability-page-1" class="page"><p><strong>TL;DR – The SanDisk High Endurance cards use SanDisk/Toshiba <a href="https://www.tomshardware.com/news/wd-sandisk-bics3-64-layer-3d-nand,32328.html" target="_blank" rel="noopener">3D TLC Flash</a>. It took way, way more work than it should have to figure this out (thanks for nothing, SanDisk!).<br>
In contrast, the SanDisk MAX Endurance uses the same 3D TLC in pMLC (pseudo-multi-level cell) mode.</strong></p><p>In a <a href="https://ripitapart.com/2019/08/17/unboxing-and-review-of-sandisk-64gb-microsdxc-high-endurance-card/" target="_blank" rel="noopener">previous blog post</a>, I took a look at SanDisk’s microSD cards that were aimed for use in write-intensive applications like dash cameras. In that post I took a look at its performance metrics, and speculated about what sort of NAND Flash memory is used inside. SanDisk doesn’t publish any detailed specifications about the cards’ internal workings, so that means I have no choice but to reverse-engineer the <del>can of worms</del> card myself.</p><p>In the hopes of uncovering more information, I sent an email to SanDisk’s support team asking about what type of NAND Flash they are using in their High Endurance lineup of cards, alongside endurance metrics like P/E (Program/Erase) cycle counts and total terabytes written (TBW). Unfortunately, the SanDisk support rep couldn’t provide a satisfactory answer to my questions, as they’re not able to provide any information that’s not listed in their public spec sheets. They said that all of their cards use MLC Flash, which I guess is correct if you call TLC Flash 3-bit MLC (which Samsung does).</p><div>
<blockquote><p>Dear Jason,</p>
<p>Thank you for contacting SanDisk® Global customer care. We really appreciate you being a part of our SanDisk® family.</p>
<p>I understand that you wish to know more about the SanDisk® High Endurance video monitoring card, as such please allow me to inform you that all our SanDisk® memory cards uses Multi level cell technology (MLC) flash. However, the read/write cycles for the flash memory is not published nor documented only the read and write speed in published as such they are 100 MB/S &amp; 40 MB/s. The 64 GB card can record Full HD video up to 10,000 hours. To know more about the card you may refer to the link below:</p>
<p><a title="Click to follow link: https://www.sandisk.com/home/memory-cards/microsd-cards/high-endurance-microsd" href="https://www.sandisk.com/home/memory-cards/microsd-cards/high-endurance-microsd" target="_blank" rel="noopener">SANDISK HIGH ENDURANCE VIDEO MONITORING microSD CARD</a></p>
<p>Best regards,<br>
Allan B.<br>
SanDisk® Global Customer Care</p></blockquote>
<p>I’ll give them a silver star that says “You Tried” at least.</p>
<h2>Anatomy of an SD Card</h2>
<p>While (micro)SD cards feel like a solid monolithic piece of technology, they’re made up of multiple different chips, each performing a different role. A basic SD card will have a controller that manages the NAND Flash chips and communicates with the host (PC, camera, etc.), and the NAND Flash itself (made up of 1 or more Flash dies). Bunnie Huang’s blog, Bunnie Studios, has an excellent article on the internals of SD cards, including counterfeits and how they’re made – check it out <a href="https://www.bunniestudios.com/blog/?p=3554" target="_blank" rel="noopener">here</a>.</p>
<div data-shortcode="caption" id="attachment_2132"><p><a href="https://ripitapart.com/wp-content/uploads/2019/10/sd-card-anatomy-2.png"><img aria-describedby="caption-attachment-2132" data-attachment-id="2132" data-permalink="https://ripitapart.com/2020/07/16/reverse-engineering-and-analysis-of-sandisk-high-endurance-microsdxc-card/sd-card-anatomy/" data-orig-file="https://ripitapart.com/wp-content/uploads/2019/10/sd-card-anatomy-2.png" data-orig-size="268,476" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="SD Card Anatomy" data-image-description="<p>Block diagram of a typical SD card.</p>
" data-image-caption="<p>Block diagram of a typical SD card.</p>
" data-medium-file="https://ripitapart.com/wp-content/uploads/2019/10/sd-card-anatomy-2.png?w=169" data-large-file="https://ripitapart.com/wp-content/uploads/2019/10/sd-card-anatomy-2.png?w=268" src="https://ripitapart.com/wp-content/uploads/2019/10/sd-card-anatomy-2.png?w=584" alt="SD Card Anatomy" srcset="https://ripitapart.com/wp-content/uploads/2019/10/sd-card-anatomy-2.png 268w, https://ripitapart.com/wp-content/uploads/2019/10/sd-card-anatomy-2.png?w=84&amp;h=150 84w" sizes="(max-width: 268px) 100vw, 268px"></a></p><p id="caption-attachment-2132">Block diagram of a typical SD card.</p></div>
<p>MicroSD cards often (but not always!) include test pads, used to program/test the NAND Flash during manufacture. These can be exploited in the case of <a href="https://blog.acelaboratory.com/pc-3000-flash-monolith-pinout-research.html" target="_blank" rel="noopener">data recovery</a>, or to reuse microSD cards that have a defective controller or firmware by turning the card into a piece of raw NAND Flash – check out Gough Lui’s adventures <a href="https://goughlui.com/2015/04/05/teardown-optimization-comsol-8gb-usb-flash-stick-au6989sn-gt-sdtnrcama-008g/" target="_blank" rel="noopener">here</a>. Note that there is no set standard for these test pads (even for the same manufacturer!), but there are common patterns for some manufacturers like SanDisk that make reverse-engineering easier.</p>
<h2>Crouching Controller, Hidden Test Pads</h2>
<p>microSD cards fall into a category of “monolithic” Flash devices, as they combine a controller and raw NAND Flash memory into a single, inseparable package. Many manufacturers break out the Flash’s data bus onto hidden (and nearly completely undocumented) test pads, which some other memory card and USB drive manufacturers take advantage of to make cheap storage products using failed parts; the controller can simply be electrically disabled and the Flash is then used as if it were a regular chip.</p>
<p>In the case of SanDisk cards, there is very limited information on their cards’ test pad pinouts. Each generation has slight differences, but the layout is mostly the same. <del>These differences can be fatal, as the power and ground connections are sometimes reversed (this spells instant death for a chip if its power polarity is mixed up!).</del></p>
<p><strong>CORRECTION (July 22, 2020):</strong> <em>Upon further review, I might have accidentally created a discrepancy between the leaked pinouts online, versus my own documentation in terms of power polarity; see the “Test Pad Pinout” section.</em></p>
<p>My card (and many of their higher-end cards – that is, not their Ultra lineup) features test pads that aren’t covered by solder mask, but are instead covered by some sort of screen-printed epoxy with a laser-etched serial number on top. With a bit of heat and some scraping, I was able to remove the (very brittle) coating on top of the test pads; this also removed the serial number which I believe is an anti-tamper measure by SanDisk.</p>

		
		

<p>After cleaning off any last traces of the epoxy coating, I was greeted with the familiar SanDisk test pad layout, plus a few extra on the bottom.</p>
<h2>Building the Breakout Board</h2>
<p>The breakout board is relatively simple in concept: for each test pad, bring out a wire that goes to a bigger test point for easier access, and wire up the normal SD bus to an SD connector to let the controller do its work with twiddling the NAND Flash bus. Given how small each test pad is (and how many), things get a bit… messy.</p>

		
		

<p>I started by using double-side foam adhesive tape to secure the SD card to a piece of perfboard. I then tinned all of the pads and soldered a small 1uF ceramic capacitor across the card’s power (Vcc) and ground (GND) test pads. Using 40-gauge (0.1 mm, or 4-thousandths of an inch!) magnet wire, I mapped each test pad to its corresponding machine-pin socket on the perfboard. Including the extra test pads, that’s a total of 28 tiny wires!</p>
<p>For the SD connector side of things, I used a flex cable for the <a href="http://xtc2clip.org/how-it-works" target="_blank" rel="noopener">XTC 2 Clip</a> (a tool used to service HTC Android devices), as it acted as a flexible “remote SD card” and broke out the signals to a small ribbon cable. I encased the flex cable with copper tape to act as a shield against electrical noise and to provide physical reinforcement, and soldered the tape to the outer pads on the perfboard for reinforcement. The ribbon cable end was then tinned and each SD card’s pin was wired up with magnet wire. The power lines were then broken out to an LED and resistor to indicate when the card was receiving power.</p>
<h2>Bus Analysis</h2>
<p>With all of the test pads broken out to an array of test pins, it was time to make sense of what pins are responsible for accessing the NAND Flash inside the card.</p>
<h2>Test Pad Pinout</h2>
<div data-shortcode="caption" id="attachment_2165"><p><a href="https://ripitapart.com/wp-content/uploads/2020/07/sandisk-high-endurance-microsd-test-pad-pinout.png"><img aria-describedby="caption-attachment-2165" data-attachment-id="2165" data-permalink="https://ripitapart.com/sandisk-high-endurance-microsd-test-pad-pinout/" data-orig-file="https://ripitapart.com/wp-content/uploads/2020/07/sandisk-high-endurance-microsd-test-pad-pinout.png" data-orig-size="1666,935" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="SanDisk High Endurance microSD Test Pad Pinout" data-image-description="<p>Diagram of the test pads on SanDisk’s High Endurance microSD card.</p>
" data-image-caption="<p>Diagram of the test pads on SanDisk’s High Endurance microSD card.</p>
" data-medium-file="https://ripitapart.com/wp-content/uploads/2020/07/sandisk-high-endurance-microsd-test-pad-pinout.png?w=300" data-large-file="https://ripitapart.com/wp-content/uploads/2020/07/sandisk-high-endurance-microsd-test-pad-pinout.png?w=584" src="https://ripitapart.com/wp-content/uploads/2020/07/sandisk-high-endurance-microsd-test-pad-pinout.png?w=584&amp;h=328" alt="Diagram of the test pads on SanDisk's High Endurance microSD card." width="584" height="328" srcset="https://ripitapart.com/wp-content/uploads/2020/07/sandisk-high-endurance-microsd-test-pad-pinout.png?w=584 584w, https://ripitapart.com/wp-content/uploads/2020/07/sandisk-high-endurance-microsd-test-pad-pinout.png?w=1168 1168w, https://ripitapart.com/wp-content/uploads/2020/07/sandisk-high-endurance-microsd-test-pad-pinout.png?w=150 150w, https://ripitapart.com/wp-content/uploads/2020/07/sandisk-high-endurance-microsd-test-pad-pinout.png?w=300 300w, https://ripitapart.com/wp-content/uploads/2020/07/sandisk-high-endurance-microsd-test-pad-pinout.png?w=768 768w, https://ripitapart.com/wp-content/uploads/2020/07/sandisk-high-endurance-microsd-test-pad-pinout.png?w=1024 1024w" sizes="(max-width: 584px) 100vw, 584px"></a></p><p id="caption-attachment-2165">Diagram of the test pads on SanDisk’s High Endurance microSD card. (click to enlarge)</p></div>
<p>The overall test pad pinout was the same for other microSD cards from SanDisk<del>, but there were some differences, primarily regarding the layout of the power pads; notably, <strong>the </strong><strong>main power pins are backwards</strong>! This can destroy the card if you’re not careful when applying power.</del></p>
<p><strong>CORRECTION (July 22, 2020):</strong> <em>I might actually have just gotten my own documentation mixed up in terms of the power and ground test pads. Regardless, one should always be careful to ensure the correct power polarity is sent to a device.</em></p>
<p>I used my <a href="https://www.dreamsourcelab.com/shop/logic-analyzer/dslogic-plus/" target="_blank" rel="noopener">DSLogic Plus</a> logic analyzer to analyze the signals on all of the pins. Since the data pinout was previously discovered, the hard part of figuring out what each line represented (data bus, control, address, command, write-protect, ready/busy status) was already done for me. However, not all of the lines were immediately evident as the pinouts I found online only included the bare minimum of lines to make the NAND Flash accessible, with one exception being a control line that places the controller into a reset state and releases its control of the data lines (this will be important later on).</p>
<p>By sniffing the data bus at the DSLogic’s maximum speed (and using its 32 MB onboard buffer RAM), I was able to get a clear snapshot of the commands being sent to the NAND Flash from the controller during initialization.</p>
<h2>Bus Sniffing &amp; NAND I/O 101 (writing commands, address, reading data)</h2>
<p>In particular, I was looking for two commands: RESET (0xFF), and READ ID (0x90). When looking for a command sequence, it’s important to know how and when the data and control lines change. I will try to explain it step-by-step, but if you’re interested there is an <a href="https://user.eng.umd.edu/~blj/CS-590.26/micron-tn2919.pdf" target="_blank" rel="noopener">introductory white paper</a> by Micron that explains all of the fundamentals of NAND Flash with much more information about how NAND Flash works.</p>

		
		

<p>When a RESET command is sent to the NAND Flash, first the /CE (Chip Select, Active Low) line is pulled low. Then the CLE (Command Latch Enable) line is pulled high; the data bus is set to its intended value of 0xFF (all binary ones); then the /WE (Write Enable, Active Low) line is pulsed from high to low, then back to high again (the data bus’ contents are committed to the chip when the /WE line goes from low to high, known as a “rising edge”); the CLE line is pulled back low to return to its normal state. The Flash chip will then pull its R/B (Ready/Busy Status) line low to indicate it is busy resetting itself, then releases the line back to its high state when it’s finished.</p>
<p>The READ ID command works similarly, except after writing the command with 0x90 (binary 1001 0000) on the data bus, it then pulls the ALE (Address Latch Enable) line high instead of CLE, and writes 0x00 (all binary zeroes) by pulsing the /WE line low. The chip transfers its internally programmed NAND Flash ID into its internal read register, and the data is read out from the device on each rising edge of the /RE (Read Enable, Active Low) line; for most devices this is 4 to 8 bytes of data.</p>
<h2>NAND Flash ID</h2>
<p>For each NAND Flash device, it has a (mostly) unique ID that identifies the manufacturer, and other functional data that is defined by that manufacturer; in other words, only the manufacturer ID, assigned by the <a href="https://en.wikipedia.org/wiki/JEDEC" target="_blank" rel="noopener">JEDEC Technology Association</a>, is well-defined.</p>
<p>The first byte represents the Flash manufacturer, and the rest (2 to 6 bytes) define the device’s characteristics, as set out by the manufacturer themselves. Most NAND vendors are very tight-lipped when it comes to datasheets, and SanDisk (and by extension, Toshiba/Kioxia) maintain very strict control, save for some slightly outdated leaked Toshiba datasheets. Because the two aforementioned companies share their NAND fabrication facilities, we can reasonably presume the data structures in the vendor-defined bytes can be referenced against each other.</p>
<p>In the case of the SanDisk High Endurance 128GB card, it has a NAND Flash ID of 0x45 48 9A B3 7E 72 0D 0E. Some of these values can be compared against a <a href="http://www.datasheet.hk/view_download.php?id=2027929&amp;file=0515%5Ctc58teg5dcjtai0_7779332.pdf" target="_blank" rel="noopener">Toshiba datasheet</a>:</p>
<table>
<thead>
<tr>
<th>Byte Value (Hex)</th>
<th>Description/Interpretation</th>
</tr>
</thead>
<tbody>
<tr>
<td>45</td>
<td>
<ul>
<li>Manufacturer: SanDisk</li>
</ul>
</td>
</tr>
<tr>
<td>48</td>
<td>
<ul>
<li>I/O voltage: Presumed 1.8 volts (measured with multimeter)</li>
<li>Device capacity: Presumed 128 GB&nbsp;(unable to confirm against datasheet)</li>
</ul>
</td>
</tr>
<tr>
<td>9A</td>
<td>
<ul>
<li>NAND type: TLC (Triple-Level Cell / 3 bits per cell)</li>
<li>Flash dies per /CE: 4 (card uses four 32GB Flash chips internally)</li>
</ul>
</td>
</tr>
<tr>
<td>B3</td>
<td>
<ul>
<li>Block size: 12 MiB (768 pages per block) excluding spare area (determined outside datasheet)</li>
<li>Page size: 16,384 bytes / 16 kiB excluding spare area</li>
</ul>
</td>
</tr>
<tr>
<td>7E</td>
<td>
<ul>
<li>Planes per /CE: 8 (2 planes per die)</li>
</ul>
</td>
</tr>
<tr>
<td>72</td>
<td>
<ul>
<li>Interface type: Asynchronous</li>
<li>Process geometry: BiCS3 3D NAND (determined outside datasheet)</li>
</ul>
</td>
</tr>
<tr>
<td>0D</td>
<td>
<ul>
<li>Unknown (no information listed in datasheet)</li>
</ul>
</td>
</tr>
<tr>
<td>0E</td>
<td>
<ul>
<li>Unknown (no information listed in datasheet)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Although not all byte values could be conclusively determined, I was able to determine that the <strong>SanDisk High Endurance cards use <a href="https://www.tomshardware.com/news/wd-sandisk-bics3-64-layer-3d-nand,32328.html" target="_blank" rel="noopener">BiCS3 3D TLC NAND Flash</a></strong>, but at least it is <strong>3D NAND</strong> which improves endurance dramatically compared to traditional/planar NAND. Unfortunately, from this information alone, I cannot determine whether the card’s controller takes advantage of any SLC caching mechanisms for write operations.</p>
<p>The chip’s process geometry was determined by looking up the first four bytes of the Flash ID, and cross-referencing it to a line from a configuration file in Silicon Motion’s <a href="https://www.usbdev.ru/files/smi/" target="_blank" rel="noopener">mass production tools</a> for their <a href="http://www.siliconmotion.com/download.php?t=U0wyRnpjMlYwY3k4eU1ERTVMekV3THpBNUwzQnliMlIxWTNReE16Z3lNamd4TWpVMkxuQmtaajA5UFZOTk16STNNU0J3Y205a2RXTjBJR0p5YVdWbUM%3D" target="_blank" rel="noopener">SM3271</a> USB Flash drive controller, and their <a href="http://en.siliconmotion.com/download/product-brief/SM2258XT_Product_Brief_ENG_Q1109.pdf" target="_blank" rel="noopener">SM2258XT</a> DRAM-less SSD controller. These tools revealed supposed part numbers of SDTNAIAMA-256G and SDUNBIEMM-32G respectively, but I don’t think this is accurate for the specific Flash configuration in this card.</p>
<h2>External Control</h2>
<p>I wanted to make sure that I was getting the correct ID from the NAND Flash, so I rigged up a Texas Instruments <a href="https://www.ti.com/tool/MSP-EXP430FR2433" target="_blank" rel="noopener">MSP430FR2433 development board</a> and wrote some (very) rudimentary code to send the required RESET and READ ID commands, and attempt to extract any extra data from the chip’s hidden JEDEC Parameter Page along the way.</p>
<p>My first roadblock was that the MSP430 would reset every time it attempted to send the RESET command, suggesting that too much current was being drawn from the MSP430 board’s limited power supply. This can occur during <a href="https://en.wikipedia.org/wiki/Bus_contention" target="_blank" rel="noopener">bus contention</a>, where two devices “fight” each other when trying to set a certain digital line both high and low at the same time. I was unsure what was going on, since publicly-available information didn’t mention how to disable the card’s built-in controller (doing so would cause it to <a href="https://en.wikipedia.org/wiki/Three-state_logic#Tri-state_Buffer" target="_blank" rel="noopener">tri-state</a> the lines, effectively “letting go” of the NAND bus and allowing another device to take control).</p>
<p>I figured out that the A1 test pad (see diagram) was the controller’s reset line (pulsing this line low while the card was running forced my card reader to power-cycle it), and by holding the line low, the controller would release its control of the NAND Flash bus entirely. After this, my microcontroller code was able to read the Flash ID correctly and consistently.</p>
<div data-shortcode="caption" id="attachment_2122"><p><a href="https://ripitapart.com/wp-content/uploads/2019/10/sandisk-he-128gb-nand-flash-id-1.png"><img aria-describedby="caption-attachment-2122" data-attachment-id="2122" data-permalink="https://ripitapart.com/2020/07/16/reverse-engineering-and-analysis-of-sandisk-high-endurance-microsdxc-card/sandisk-he-128gb-nand-flash-id/" data-orig-file="https://ripitapart.com/wp-content/uploads/2019/10/sandisk-he-128gb-nand-flash-id-1.png" data-orig-size="877,375" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="SanDisk High Endurance SD Card NAND Flash ID" data-image-description="" data-image-caption="<p>Reading out the card’s Flash ID with my own microcontroller code.</p>
" data-medium-file="https://ripitapart.com/wp-content/uploads/2019/10/sandisk-he-128gb-nand-flash-id-1.png?w=300" data-large-file="https://ripitapart.com/wp-content/uploads/2019/10/sandisk-he-128gb-nand-flash-id-1.png?w=584" src="https://ripitapart.com/wp-content/uploads/2019/10/sandisk-he-128gb-nand-flash-id-1.png?w=584" alt="Reading out the card's Flash ID with my own microcontroller code." srcset="https://ripitapart.com/wp-content/uploads/2019/10/sandisk-he-128gb-nand-flash-id-1.png 877w, https://ripitapart.com/wp-content/uploads/2019/10/sandisk-he-128gb-nand-flash-id-1.png?w=150&amp;h=64 150w, https://ripitapart.com/wp-content/uploads/2019/10/sandisk-he-128gb-nand-flash-id-1.png?w=300&amp;h=128 300w, https://ripitapart.com/wp-content/uploads/2019/10/sandisk-he-128gb-nand-flash-id-1.png?w=768&amp;h=328 768w" sizes="(max-width: 877px) 100vw, 877px"></a></p><p id="caption-attachment-2122">Reading out the card’s Flash ID with my own microcontroller code.</p></div>
<h2>JEDEC Parameter Page… or at least what SanDisk made of it!</h2>
<p>The JEDEC Parameter Page, if present, contains detailed information on a Flash chip’s characteristics with far greater detail than the NAND Flash ID – and it’s well-standardized so parsing it would be far easier. However, it turns out that SanDisk decided to ignore the standard format, and decided to use their own proprietary Parameter Page format! Normally the page starts with the ASCII string “JEDEC”, but I got a repeating string of “SNDK” (corresponding with their <a href="https://www.marketbeat.com/stocks/NASDAQ/SNDK/" target="_blank" rel="noopener">stock symbol</a>) with other data that didn’t correspond to anything like the JEDEC specification! Oh well, it was worth a try.</p>
<p>I collected the data with the same Arduino sketch as shown above, and pulled 1,536 bytes’ worth of data; I wrote a quick <a href="https://ideone.com/eLclhy#stdin" target="_blank" rel="noopener">program on Ideone</a> to provide a nicely-formatted hex dump of the first 512 bytes of the Parameter Page data:</p>
<pre>Offset 00:01:02:03:04:05:06:07:08:09:0A:0B:0C:0D:0E:0F 0123456789ABCDEF
------ --+--+--+--+--+--+--+--+--+--+--+--+--+--+--+-- ----------------
0x0000 53 4E 44 4B 53 4E 44 4B 53 4E 44 4B 53 4E 44 4B SNDKSNDKSNDKSNDK
0x0010 53 4E 44 4B 53 4E 44 4B 53 4E 44 4B 53 4E 44 4B SNDKSNDKSNDKSNDK
0x0020 08 08 00 08 06 20 00 02 01 48 9A B3 00 05 08 41 ..... ...H.....A
0x0030 48 63 6A 08 08 00 08 06 20 00 02 01 48 9A B3 00 Hcj..... ...H...
0x0040 05 08 41 48 63 6A 08 08 00 08 06 20 00 02 01 48 ..AHcj..... ...H
0x0050 9A B3 00 05 08 41 48 63 6A 08 08 00 08 06 20 00 .....AHcj..... .
0x0060 02 01 48 9A B3 00 05 08 41 48 63 6A 08 08 00 08 ..H.....AHcj....
0x0070 06 20 00 02 01 48 9A B3 00 05 08 41 48 63 6A 08 . ...H.....AHcj.
0x0080 08 00 08 06 20 00 02 01 48 9A B3 00 05 08 41 48 .... ...H.....AH
0x0090 63 6A 08 08 00 08 06 20 00 02 01 48 9A B3 00 05 cj..... ...H....
0x00A0 08 41 48 63 6A 08 08 00 08 06 20 00 02 01 48 9A .AHcj..... ...H.
0x00B0 B3 00 05 08 41 48 63 6A 08 08 00 08 06 20 00 02 ....AHcj..... ..
0x00C0 01 48 9A B3 00 05 08 41 48 63 6A 08 08 00 08 06 .H.....AHcj.....
0x00D0 20 00 02 01 48 9A B3 00 05 08 41 48 63 6A 08 08  ...H.....AHcj..
0x00E0 00 08 06 20 00 02 01 48 9A B3 00 05 08 41 48 63 ... ...H.....AHc
0x00F0 6A 08 08 00 08 06 20 00 02 01 48 9A B3 00 05 08 j..... ...H.....
0x0100 41 48 63 6A 08 08 00 08 06 20 00 02 01 48 9A B3 AHcj..... ...H..
0x0110 00 05 08 41 48 63 6A 08 08 00 08 06 20 00 02 01 ...AHcj..... ...
0x0120 48 9A B3 00 05 08 41 48 63 6A 08 08 00 08 06 20 H.....AHcj..... 
0x0130 00 02 01 48 9A B3 00 05 08 41 48 63 6A 08 08 00 ...H.....AHcj...
0x0140 08 06 20 00 02 01 48 9A B3 00 05 08 41 48 63 6A .. ...H.....AHcj
0x0150 08 08 00 08 06 20 00 02 01 48 9A B3 00 05 08 41 ..... ...H.....A
0x0160 48 63 6A 08 08 00 08 06 20 00 02 01 48 9A B3 00 Hcj..... ...H...
0x0170 05 08 41 48 63 6A 08 08 00 08 06 20 00 02 01 48 ..AHcj..... ...H
0x0180 9A B3 00 05 08 41 48 63 6A 08 08 00 08 06 20 00 .....AHcj..... .
0x0190 02 01 48 9A B3 00 05 08 41 48 63 6A 08 08 00 08 ..H.....AHcj....
0x01A0 06 20 00 02 01 48 9A B3 00 05 08 41 48 63 6A 08 . ...H.....AHcj.
0x01B0 08 00 08 06 20 00 02 01 48 9A B3 00 05 08 41 48 .... ...H.....AH
0x01C0 63 6A 08 08 00 08 06 20 00 02 01 48 9A B3 00 05 cj..... ...H....
0x01D0 08 41 48 63 6A 08 08 00 08 06 20 00 02 01 48 9A .AHcj..... ...H.
0x01E0 B3 00 05 08 41 48 63 6A 08 08 00 08 06 20 00 02 ....AHcj..... ..
0x01F0 01 48 9A B3 00 05 08 41 48 63 6A 08 08 00 08 06 .H.....AHcj.....</pre>
<p>Further analysis with my DSLogic showed that the controller itself requests a total of 4,128 bytes (4 kiB + 32 bytes) of Parameter Page data, which is filled with the same repeating data as shown above.</p>
<h3>Reset Quirks</h3>
<p>When looking at the logic analyzer data, I noticed that the controller sends the READ ID command twice, but the first time it does so without resetting the Flash (which should normally be done as soon as the chip is powered up!). The data that the Flash returned was… strange to say the least.</p>
<table>
<thead>
<tr>
<th>Byte Value (Hex)</th>
<th>Interpreted Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>98</td>
<td>
<ul>
<li>Manufacturer: Toshiba</li>
</ul>
</td>
</tr>
<tr>
<td>00</td>
<td>
<ul>
<li>I/O voltage: Unknown (no data)</li>
<li>Device capacity: Unknown (no data)</li>
</ul>
</td>
</tr>
<tr>
<td>90</td>
<td>
<ul>
<li>NAND type: SLC (Single-Level Cell / 1 bit per cell)</li>
<li>Flash dies per /CE: 1</li>
</ul>
</td>
</tr>
<tr>
<td>93</td>
<td>
<ul>
<li>Block size: 4 MB excluding spare area</li>
<li>Page size: 16,384 bytes / 16 kiB excluding spare area</li>
</ul>
</td>
</tr>
<tr>
<td>76</td>
<td>
<ul>
<li>Planes per /CE: 2</li>
</ul>
</td>
</tr>
<tr>
<td>72</td>
<td>
<ul>
<li>Interface type: Asynchronous</li>
<li>Process geometry: 70 nm planar</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>This confused me initially when I was trying to find the ID from the logic capture alone; after talking to a contact who has experience in NAND Flash data recovery, they said this is expected for SanDisk devices, which make liberal use of vendor-proprietary commands and data structures. If the fourth byte is to be believed, it says the block size is 4 megabytes, which I think is plausible for a modern Flash device. The rest of the information doesn’t really make any sense to me apart from the first byte indicating the chip is made by Toshiba.</p>
<h2>Conclusion</h2>
<p>I shouldn’t have to go this far in hardware reverse-engineering to just ask a simple question of what Flash SanDisk used in their high-endurance card. You’d think they would be proud to say they use 3D NAND for higher endurance and reliability, but I guess not!</p>
<h2>Downloads</h2>
<p>For those that are interested, I’ve included the logic captures of the card’s activity shortly after power-up. I’ve also included the (very crude) Arduino sketch that I used to read the NAND ID and Parameter Page data manually:</p>
<ul>
<li><a href="https://www.dropbox.com/s/8yolkc756q37mnh/sandisk%20he%20128%20power%20on.dsl?dl=0" target="_blank" rel="noopener">Logic capture #1</a></li>
<li><a href="https://www.dropbox.com/s/hhkl3c0jyk366c3/sandisk%20he%20128%20power%20on%202.dsl?dl=0" target="_blank" rel="noopener">Logic capture #2</a></li>
<li><a href="https://github.com/ginbot86/2433_nandflashtest" target="_blank" rel="noopener">NAND I/O Arduino sketch</a></li>
</ul>
</div></div>]]></description>
        </item>
    </channel>
</rss>