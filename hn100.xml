<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 18 May 2025 06:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Tornado warnings delayed because of DOGE cuts (108 pts)]]></title>
            <link>https://www.mesoscalenews.com/p/tornado-warnings-delayed-because</link>
            <guid>44018247</guid>
            <pubDate>Sun, 18 May 2025 01:18:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mesoscalenews.com/p/tornado-warnings-delayed-because">https://www.mesoscalenews.com/p/tornado-warnings-delayed-because</a>, See on <a href="https://news.ycombinator.com/item?id=44018247">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p>Of all the disasters I’ve studied, tornadoes scare me the most.</p><p>They come with little warning and can erase entire communities in minutes — even seconds.</p><p>There’s no four-day lead-up to prepare like we often have with major hurricanes, and the winds of these storms can far exceed the most violent tropical cyclones.</p><p>In those few moments before one hits, especially if you’re sleeping, you’re at the mercy of your local weather station. </p><p>If someone is watching, they can issue a warning in those critical minutes before it’s too late.</p><p><span>Those few minutes after an emergency alert is issued are the </span><a href="https://www.cnn.com/2019/05/29/us/weather-alert-tech-saved-lives-trnd/index.html#:~:text=They%20also%20save%20lives&amp;text=He%20shepherded%20the%20flock%20to,Hennen%20contributed%20to%20this%20story." rel="">difference between life and death</a><span>. </span></p><p><span>That’s why experts were shocked and outraged by</span><a href="https://www.latimes.com/california/story/2025-04-17/californias-national-weather-service-offices-reduce-services-amid-trump-admin-cuts#:~:text=On%20Thursday%2C%20the%20department%20announced,staffing%20limitations%20or%20operational%20priorities.%E2%80%9D" rel=""> budget cuts </a><span>made to the National Weather Service earlier this year.</span></p><p><span>Some offices were forced to </span><a href="https://www.cnn.com/2025/05/02/weather/nws-forecasting-layoffs-trump#:~:text=One%20NWS%20forecast%20office%2C%20in,stretch%20into%20the%20Pacific%20Northwest." rel="">no longer operate 24 hours a day </a><span>back in April.</span></p><p>In the Jackson, Kentucky NWS office, one of the positions they were forced to cut was the full-time overnight forecaster.</p><p>The office's website even lists the "Meteorologist in Charge" position as vacant.</p><p><a href="https://www.nbcnews.com/science/environment/noaa-scrambling-fill-forecasting-jobs-cuts-national-weather-service-rcna207050" rel="">Overnight forecasters </a><span>are responsible for monitoring severe weather outbreaks and issuing warnings while one of the most tornado-prone areas in the countries is sound asleep.</span></p><p>“It’s only a matter of time before these cuts lead to tragedy,” I said back in February.</p><p>Just before midnight last night, tragedy struck.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf1204d0-dd21-493d-b55a-0394aebc6880_1800x2250.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf1204d0-dd21-493d-b55a-0394aebc6880_1800x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf1204d0-dd21-493d-b55a-0394aebc6880_1800x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf1204d0-dd21-493d-b55a-0394aebc6880_1800x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf1204d0-dd21-493d-b55a-0394aebc6880_1800x2250.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf1204d0-dd21-493d-b55a-0394aebc6880_1800x2250.jpeg" width="1456" height="1820" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bf1204d0-dd21-493d-b55a-0394aebc6880_1800x2250.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1820,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf1204d0-dd21-493d-b55a-0394aebc6880_1800x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf1204d0-dd21-493d-b55a-0394aebc6880_1800x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf1204d0-dd21-493d-b55a-0394aebc6880_1800x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf1204d0-dd21-493d-b55a-0394aebc6880_1800x2250.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Photo by Austin Anthony for the New York Times. </span><a href="https://www.nytimes.com/2025/05/17/weather/storms-tornadoes-missouri-kentucky.html" rel="">Link here.</a></figcaption></figure></div><p><strong><span>At least </span><a href="https://www.foxweather.com/weather-news/deadly-tornado-outbreak-ohio-valley-kentucky-missouri-indiana" rel="">27 people are dead</a><span>,</span></strong><span> with more still missing, across Missouri and Kentucky.</span></p><p>Tornado warnings were delayed because of reduced staff. Those critical moments — a midnight warning to your phone waking you up, giving you precious seconds to find shelter — came too late for some.</p><p>The risk of these cuts creating this exact problem was known before last night.</p><p><span>Just one day before the disaster, on May 15, the New York Times ran an </span><a href="https://www.nytimes.com/2025/05/15/us/politics/national-weather-service-cuts-trump.html" rel="">investigative piece</a><span> about how DOGE cuts were undermining weather forecasting improvements.</span></p><p><span>The piece </span><em><strong>specifically included the Jackson, Kentucky</strong></em><span> NWS office as one targeted by DOGE for layoffs.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F084c8936-1c9d-40d8-b331-551a34dfcefa_1782x756.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F084c8936-1c9d-40d8-b331-551a34dfcefa_1782x756.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F084c8936-1c9d-40d8-b331-551a34dfcefa_1782x756.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F084c8936-1c9d-40d8-b331-551a34dfcefa_1782x756.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F084c8936-1c9d-40d8-b331-551a34dfcefa_1782x756.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F084c8936-1c9d-40d8-b331-551a34dfcefa_1782x756.png" width="1456" height="618" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/084c8936-1c9d-40d8-b331-551a34dfcefa_1782x756.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:618,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:276785,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.mesoscalenews.com/i/163800038?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F084c8936-1c9d-40d8-b331-551a34dfcefa_1782x756.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F084c8936-1c9d-40d8-b331-551a34dfcefa_1782x756.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F084c8936-1c9d-40d8-b331-551a34dfcefa_1782x756.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F084c8936-1c9d-40d8-b331-551a34dfcefa_1782x756.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F084c8936-1c9d-40d8-b331-551a34dfcefa_1782x756.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Severe weather is expected to continue today and tomorrow, and NOAA’s new PR team, now run by Trump loyalists, is scrambling to deny and diffuse the situation.</p><p>We can’t ask those who died if they received the warning, so we might never know how many lives would have been saved by having minimal staffing standards in NWS offices. </p><p>As the MAGA-rampage against science continues unabated, how many more will pay for the ignorance of this administration?</p><p><span>With an </span><a href="https://www.mesoscalenews.com/p/2025-atlantic-hurricane-season-outlook" rel="">above-normal hurricane season </a><span>starting in two week, how far will Americans let these threats to public safety go?</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AniSora: Open-source anime video generation model (137 pts)]]></title>
            <link>https://komiko.app/video/AniSora</link>
            <guid>44017913</guid>
            <pubDate>Sat, 17 May 2025 23:59:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://komiko.app/video/AniSora">https://komiko.app/video/AniSora</a>, See on <a href="https://news.ycombinator.com/item?id=44017913">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>The most powerful open-source animated video generation model presented by Bilibili. AniSora enables one-click video generation across diverse anime styles including series episodes, Chinese animations, manga adaptations, VTuber content, anime PVs, and more.</p></div><div><div tabindex="-1"><div><h2>Input</h2></div><div><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M537.6 226.6c4.1-10.7 6.4-22.4 6.4-34.6 0-53-43-96-96-96-19.7 0-38.1 6-53.3 16.2C367 64.2 315.3 32 256 32c-88.4 0-160 71.6-160 160 0 2.7.1 5.4.2 8.1C40.2 219.8 0 273.2 0 336c0 79.5 64.5 144 144 144h368c70.7 0 128-57.3 128-128 0-61.9-44-113.6-102.4-125.4zM393.4 288H328v112c0 8.8-7.2 16-16 16h-48c-8.8 0-16-7.2-16-16V288h-65.4c-14.3 0-21.4-17.2-11.3-27.3l105.4-105.4c6.2-6.2 16.4-6.2 22.6 0l105.4 105.4c10.1 10.1 2.9 27.3-11.3 27.3z"></path></svg><p>Tap to upload or drag your image here</p></div><p><label>Prompt</label></p></div><div tabindex="-1"><h2><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M216 0h80c13.3 0 24 10.7 24 24v168h87.7c17.8 0 26.7 21.5 14.1 34.1L269.7 378.3c-7.5 7.5-19.8 7.5-27.3 0L90.1 226.1c-12.6-12.6-3.7-34.1 14.1-34.1H192V24c0-13.3 10.7-24 24-24zm296 376v112c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V376c0-13.3 10.7-24 24-24h146.7l49 49c20.1 20.1 52.5 20.1 72.6 0l49-49H488c13.3 0 24 10.7 24 24zm-124 88c0-11-9-20-20-20s-20 9-20 20 9 20 20 20 20-9 20-20zm64 0c0-11-9-20-20-20s-20 9-20 20 9 20 20 20 20-9 20-20z"></path></svg> <!-- -->Animation Results</h2><div><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" height="48" width="48" xmlns="http://www.w3.org/2000/svg"><path fill="none" d="M0 0h24v24H0z"></path><path d="M15 2c-2.71 0-5.05 1.54-6.22 3.78a7.062 7.062 0 0 0-3 3A7.014 7.014 0 0 0 2 15c0 3.87 3.13 7 7 7 2.71 0 5.05-1.54 6.22-3.78a7.062 7.062 0 0 0 3-3A7.014 7.014 0 0 0 22 9c0-3.87-3.13-7-7-7zM9 20a5.002 5.002 0 0 1-4-8c0 3.87 3.13 7 7 7-.84.63-1.88 1-3 1zm3-3a5.002 5.002 0 0 1-4-8c0 3.86 3.13 6.99 7 7-.84.63-1.88 1-3 1zm4.7-3.3c-.53.19-1.1.3-1.7.3-2.76 0-5-2.24-5-5 0-.6.11-1.17.3-1.7.53-.19 1.1-.3 1.7-.3 2.76 0 5 2.24 5 5 0 .6-.11 1.17-.3 1.7zM19 12c0-3.86-3.13-6.99-7-7a5.002 5.002 0 0 1 7 7z"></path></svg><p>Your animation results will appear here</p><p>Upload an image and provide a prompt to get started</p></div></div></div><div><h2>AniSora AI Anime Video Generation Examples</h2><p>Explore a variety of AI-generated videos created with Bilibili's AniSora. Witness its capability to animate still images into dynamic anime and manga scenes, bringing your favorite characters and stories to life with smooth, coherent animation and rich detail. Discover the power of open-source AI in anime video creation.</p></div><h2>What is AniSora?</h2><p>AniSora is the most powerful open-source animated video generation model developed by Bilibili. As part of Project Index-AniSora, it represents Bilibili's open-source gift to the anime world. AniSora enables one-click video creation across a wide range of anime styles, including series episodes, Chinese original animations, manga adaptations, VTuber content, anime PVs, and more. It is powered by IJCAI'25-accepted research: AniSora — Exploring the Frontiers of Animation Video Generation in the Sora Era.</p><p>🤗 <a href="https://huggingface.co/IndexTeam/Index-anisora" target="_blank" rel="noopener noreferrer nofollow">Hugging Face</a>&nbsp;&nbsp; | &nbsp;&nbsp; 🤖 <a href="https://www.modelscope.cn/organization/bilibili-index" target="_blank" rel="noopener noreferrer nofollow">Model Scope</a></p><div><h2>How to Use AniSora with Komiko</h2><div><div><p>1</p><div><h3>Upload Your Image</h3><p>Begin by uploading a high-quality reference image you wish to animate into a dynamic video using AniSora AI Video Generator.</p></div></div><div><p>2</p><div><h3>Select Your AI Model</h3><p>Choose from available AI video generation models tailored for various visual styles and video qualities to match your creative vision.</p></div></div><div><p>3</p><div><h3>Generate and Download Video</h3><p>Click the generate button to initiate AniSora's AI-powered video creation. Once processing is complete, preview and download your high-quality AI-generated video for instant use.</p></div></div></div></div><div><h2>Explore Popular AI Tools</h2><p>Expand your creative horizons with more related AI tools.</p></div><div><h2>Why use AniSora AI Video Generator?</h2><p>AniSora by Bilibili offers a specialized, open-source approach to AI video generation, meticulously tailored for anime, manga and comics content. This makes it the ideal tool for creators aiming to materialize their artistic visions with AI-powered animation, particularly for Japanese anime, Chinese animation, and manga adaptations.</p><div><div><div><p><span>🌸</span></p><h3>Specialized for Anime &amp; Manga Styles</h3></div><p>AniSora's AI models are expertly trained on vast datasets of anime and manga, ensuring high-quality animation that authentically captures the unique visual characteristics and artistic nuances of these beloved styles, including popular manga adaptations.</p></div><div><div><p><span>🎨</span></p><h3>Intuitive Interface</h3></div><p>AniSora provides an intuitive interface, making AI video generation accessible to everyone, regardless of technical expertise. Bring your anime, manga, and VTuber content to life effortlessly with our one-click generation.</p></div><div><div><p><span>🌟</span></p><h3>High-Quality Animated Video</h3></div><p>AniSora supports high-resolution video output, ensuring your AI-generated animated creations, from series episodes to promotional videos, look crisp and professional across all platforms. Share your AI-powered anime and manga videos with confidence.</p></div></div></div><div><h2>AniSora AI Video Generator FAQ</h2><p>Find answers to common questions about AniSora, the open-source AI anime video generator. Learn about its features for creating animations, usage tips, and how it can help you bring your stories to life, including for VTuber content and manga adaptations.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Lost Japanese ROM of the Macintosh Plus (125 pts)]]></title>
            <link>https://www.journaldulapin.com/2025/05/17/the-lost-japanese-rom-of-the-macintosh-plus-which-isnt-lost-anymore/</link>
            <guid>44017692</guid>
            <pubDate>Sat, 17 May 2025 23:12:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.journaldulapin.com/2025/05/17/the-lost-japanese-rom-of-the-macintosh-plus-which-isnt-lost-anymore/">https://www.journaldulapin.com/2025/05/17/the-lost-japanese-rom-of-the-macintosh-plus-which-isnt-lost-anymore/</a>, See on <a href="https://news.ycombinator.com/item?id=44017692">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
						<p>If you look for information about the Macintosh Plus and its ROM, you’ll usually find that the ROM has a capacity of 128 KB and that it exists in <a href="https://www.tech-insider.org/mac/research/acrobat/8803-f.pdf">three revisions</a>. But that’s incorrect: there’s a fourth ROM, 256 KB in size, which includes fonts for kanji (Japanese characters). And I found (and preserved) this ROM.<br>
<span id="more-112672"></span><br>
I’m Belgian, and the article was originally written in French before being translated automatically.</p>
<p>I had talked about this mysterious ROM <a href="https://www.journaldulapin.com/2019/05/04/macintosh-plus-rom/">a few years ago</a>. It’s documented by Apple in <a href="https://spinsidemacintosh.neocities.org/tn405#tn138">some old documents</a>, but without much detail. According to Apple, the ROM contains fonts for kanji in 12 and 18 points, and they are loaded at startup by <a href="https://www.journaldulapin.com/2018/05/29/kanjitalk-%E6%BC%A2%E5%AD%97talk/">KanjiTalk</a>. On a regular Macintosh Plus, you need a floppy disk with the files, which slows down startup and uses some RAM, whereas on a Japanese Macintosh Plus, the font is in ROM and doesn’t take up RAM. You also avoid loading files from a floppy disk, theoretically saving 6 seconds during startup. That’s a conservative estimate, as we’ll see—it assumes you’re not switching disks manually.</p>
<div id="attachment_112658"><p><a href="https://www.journaldulapin.com/wp-content/uploads/2025/05/01-4.jpg"><img fetchpriority="high" decoding="async" aria-describedby="caption-attachment-112658" src="https://www.journaldulapin.com/wp-content/uploads/2025/05/01-4-640x360.jpg" alt="" width="640" height="360" srcset="https://www.journaldulapin.com/wp-content/uploads/2025/05/01-4-640x360.jpg 640w, https://www.journaldulapin.com/wp-content/uploads/2025/05/01-4-850x478.jpg 850w, https://www.journaldulapin.com/wp-content/uploads/2025/05/01-4-320x180.jpg 320w, https://www.journaldulapin.com/wp-content/uploads/2025/05/01-4-768x432.jpg 768w, https://www.journaldulapin.com/wp-content/uploads/2025/05/01-4-1536x864.jpg 1536w, https://www.journaldulapin.com/wp-content/uploads/2025/05/01-4.jpg 1920w" sizes="(max-width: 640px) 100vw, 640px"></a></p><p id="caption-attachment-112658">When booting with a Western ROM, it asks for the floppy with the 12-point font</p></div>
<p>Now, one question remained: how to find the ROM? Getting a Japanese Macintosh Plus shipped was out of the question—it’s bulky and heavy, and there was no guarantee that it had the Japanese ROM… or even that it existed. Searching the internet brings up <a href="https://68kmla.org/bb/index.php?threads/unique-japanese-mac-plus-rom.33029/">this thread</a> questioning its existence, and <a href="https://tinkerdifferent.com/threads/the-mac-plus-kanji-rom.1088/">another one</a> explaining that according to failed tests (as we’ll see), the Japanese ROM has the same content as a v2 ROM. A Japanese author <a href="https://note.com/kaigian/n/n40f4c4248e96">tried</a> to find information with no success (he concluded that the ROM was 128 KB, not 256 KB). But <a href="http://tamaru.world.coocan.jp/Tips/ROM/rom.html">this other Japanese site</a> does reference the Japanese ROM (<code>342-0441-A</code> and <code>342-0442-A</code>), and <a href="http://web.kyoto-inet.or.jp/people/s-oga/mac80/index.html">this one</a> confirms a 256 KB capacity.</p>
<p>In short, no one really knew if the ROM existed, but there were quite a few clues. So I kept digging. Instead of looking for a full Macintosh Plus, I searched for a Macintosh Plus motherboard in Japan, hoping a seller would upload photos that showed the chip references. My first purchase failed: the reference wasn’t fully readable, but the yellow sticker misled me.</p>
<div id="attachment_112661"><p><a href="https://www.journaldulapin.com/wp-content/uploads/2025/05/04-3.jpeg"><img decoding="async" aria-describedby="caption-attachment-112661" src="https://www.journaldulapin.com/wp-content/uploads/2025/05/04-3-640x459.jpeg" alt="" width="640" height="459" srcset="https://www.journaldulapin.com/wp-content/uploads/2025/05/04-3-640x459.jpeg 640w, https://www.journaldulapin.com/wp-content/uploads/2025/05/04-3-850x609.jpeg 850w, https://www.journaldulapin.com/wp-content/uploads/2025/05/04-3-320x229.jpeg 320w, https://www.journaldulapin.com/wp-content/uploads/2025/05/04-3-768x550.jpeg 768w, https://www.journaldulapin.com/wp-content/uploads/2025/05/04-3-1536x1101.jpeg 1536w, https://www.journaldulapin.com/wp-content/uploads/2025/05/04-3-2048x1468.jpeg 2048w" sizes="(max-width: 640px) 100vw, 640px"></a></p><p id="caption-attachment-112661">This time, it’s the right one.</p></div>
<p>With the second one, I had the correct chip reference—and a yellow sticker again. I now had a Macintosh Plus motherboard and its ROM, but no actual Macintosh Plus to test it on. The first step was trying to dump the ROM (two chips), but it didn’t work at first. I used an old device for the dump and ended up with two 64 KB files, which wasn’t what I expected. But I didn’t give up, for a simple reason I noticed with the help <a href="https://x86.fr/">of Doc TB</a>: the markings on the chips indicated a 1,024 kilobit capacity, meaning 128 KB.</p>
<div id="attachment_112660"><p><a href="https://www.journaldulapin.com/wp-content/uploads/2025/05/03-3.jpeg"><img decoding="async" aria-describedby="caption-attachment-112660" src="https://www.journaldulapin.com/wp-content/uploads/2025/05/03-3-640x468.jpeg" alt="" width="640" height="468" srcset="https://www.journaldulapin.com/wp-content/uploads/2025/05/03-3-640x468.jpeg 640w, https://www.journaldulapin.com/wp-content/uploads/2025/05/03-3-850x622.jpeg 850w, https://www.journaldulapin.com/wp-content/uploads/2025/05/03-3-320x234.jpeg 320w, https://www.journaldulapin.com/wp-content/uploads/2025/05/03-3-768x562.jpeg 768w, https://www.journaldulapin.com/wp-content/uploads/2025/05/03-3-1536x1124.jpeg 1536w, https://www.journaldulapin.com/wp-content/uploads/2025/05/03-3-2048x1498.jpeg 2048w" sizes="(max-width: 640px) 100vw, 640px"></a></p><p id="caption-attachment-112660">The marking helped, with the 1024 in the reference.</p></div>
<p>I had followed the information on <a href="https://www.instructables.com/Create-Macintosh-Plus-ROMs/">this site</a>, which says the Macintosh Plus can use 27C512 chips (64 KB), and I didn’t understand how you could have a 128 KB ROM with that pinout. Until I found a page that explained it. On <a href="http://www.synack.net/~bbraun/plusrom/index.html">Rob Braun’s site</a>, it’s clearly stated that the Macintosh Plus has a slightly different pinout than a 27C512 chip. It includes one extra address line, allowing it to address 128 KB, placed where the programming voltage pin would normally be. Since the Macintosh Plus socket isn’t meant for writing to an EPROM, this is entirely transparent. Doc TB helped again: <a href="https://x86.fr/the-uca-now-supports-roms-eeproms/">he developed a device</a> that can read many ROM and EPROM types, and he was able to dump my two chips. With a standard adapter, it’s not really possible directly, since it’s a proprietary pinout. That was the earlier cited person’s mistake: the lower half of Apple’s ROM (128 KB) matches a standard ROM.</p>
<div id="attachment_112659"><p><a href="https://www.journaldulapin.com/wp-content/uploads/2025/05/02-3.jpeg"><img loading="lazy" decoding="async" aria-describedby="caption-attachment-112659" src="https://www.journaldulapin.com/wp-content/uploads/2025/05/02-3-640x529.jpeg" alt="" width="640" height="529" srcset="https://www.journaldulapin.com/wp-content/uploads/2025/05/02-3-640x529.jpeg 640w, https://www.journaldulapin.com/wp-content/uploads/2025/05/02-3-850x702.jpeg 850w, https://www.journaldulapin.com/wp-content/uploads/2025/05/02-3-320x264.jpeg 320w, https://www.journaldulapin.com/wp-content/uploads/2025/05/02-3-768x635.jpeg 768w, https://www.journaldulapin.com/wp-content/uploads/2025/05/02-3-1536x1269.jpeg 1536w, https://www.journaldulapin.com/wp-content/uploads/2025/05/02-3-2048x1692.jpeg 2048w" sizes="auto, (max-width: 640px) 100vw, 640px"></a></p><p id="caption-attachment-112659">The two ROMs with the yellow sticker</p></div>
<p>Before continuing, <a href="http://down.dandu.be/MacPlusROMv4.zip">here’s a link to the files</a>.</p>
<h3>Testing the Japanese ROM</h3>
<p>At this point, I had a 256 KB dump of the Japanese Macintosh Plus ROM… but nothing to test it with. I didn’t have a Macintosh Plus (just two motherboards), and my first emulator tests failed. The reason is simple: they expect a known ROM, and not necessarily a Macintosh Plus ROM. That’s important to remember when it comes to Mac emulation—there are many models, and emulators don’t always emulate a specific Mac. Sometimes it’s more like a clone, as with Basilisk or SheepShaver. Technically, they emulate something close to a compatible machine. Mini vMac is better suited for this, but it checks the ROM’s checksum, so I couldn’t boot with mine—it’s not in the recognized list.</p>
<p>That’s when I asked for help. And I’d like to thank <a href="https://x86.fr/">Samuel</a> (for the ROM dump), <a href="https://www.polysoft.fr/">Gilles</a>—for the Macintosh Plus—and <a href="https://www.downtowndougbrown.com/">Doug</a>, for the software part.</p>
<p>Let’s talk hardware first. I didn’t have a Macintosh Plus, just a <a href="https://www.journaldulapin.com/2017/04/10/usb-souris-m0100/">mouse</a>. Gilles sent me a Macintosh Plus and a keyboard (which didn’t work). But with a <a href="https://www.tindie.com/products/lazaj/ps2-keyboard-to-macintosh-plus/">PS/2 adapter</a>, I managed. I had originally considered using the <a href="https://www.journaldulapin.com/2017/02/04/floppy-emu-macintosh/">Floppy Emu</a>, but I got odd results, and the sound of the floppy drive is more satisfying anyway. So I pulled out two 800 KB 3.5″ floppy disks to copy KanjiTalk 1.0 onto them. A tip: KanjiTalk really doesn’t like running from an English system—every time I put the system disk into a newer Mac, I couldn’t boot again on the Macintosh Plus without recreating the disk.</p>
<p>Now for the software part. First, I had to find an old version of KanjiTalk. You can (for example) find it on <a href="https://macintoshgarden.org/apps/kanjitalk">Macintosh Garden</a> or on <a href="https://winworldpc.com/product/mac-os-0-6/system-3x">WinWorld</a>. KanjiTalk 1.0 is based on System 3.1 and comes on two floppies. The first is 800 KB and holds the system itself. The second contains the fonts and is 400 KB (I copied it to an 800 KB disk with no issue).</p>
<div id="attachment_112664"><p><a href="https://www.journaldulapin.com/wp-content/uploads/2025/05/07-6.png"><img loading="lazy" decoding="async" aria-describedby="caption-attachment-112664" src="https://www.journaldulapin.com/wp-content/uploads/2025/05/07-6.png" alt="" width="512" height="342" srcset="https://www.journaldulapin.com/wp-content/uploads/2025/05/07-6.png 512w, https://www.journaldulapin.com/wp-content/uploads/2025/05/07-6-320x214.png 320w" sizes="auto, (max-width: 512px) 100vw, 512px"></a></p><p id="caption-attachment-112664">KanjiTalk isn’t entirely in Japanese</p></div>
<p>This is where the Japanese ROM shines. The usual behavior on a Western Macintosh Plus is quite long. After around 10 seconds, the system asks for the font disk to load the 12-point font. After about 6 seconds of loading, it asks again for the system disk. Five seconds later, it loads the 18-point font (13 seconds), and finally it needs 15 more seconds with the system disk. Even if you swap floppies quickly, it still takes over a minute (about 1:14 in my video) and a few steps to get the Macintosh Plus started.</p>
<p><iframe loading="lazy" width="853" height="480" src="https://www.youtube.com/embed/Hf1H-CNeZAg?si=utw83nXQh00I8V8Z" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
<p>With the Japanese ROM, the system loads the 12-point font from ROM, and asks for the 18-point font disk after a little over 10 seconds. About 12 seconds later, it requests the system disk again, and the rest loads quickly (about 15 seconds). So it avoids a few swaps and the Mac is ready after around 52 seconds.</p>
<p><iframe loading="lazy" width="853" height="480" src="https://www.youtube.com/embed/CAqU1_ls6Qo?si=yn41aj8FmVAUX7PL" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
<p>You can also skip loading the 18-point font entirely, which reduces boot time further—to about 25 seconds. I only tested this with the Japanese ROM because swapping ROMs means disassembling the Macintosh Plus motherboard, and I don’t like getting close to a CRT.</p>
<div id="attachment_112662"><p><a href="https://www.journaldulapin.com/wp-content/uploads/2025/05/05-8.png"><img loading="lazy" decoding="async" aria-describedby="caption-attachment-112662" src="https://www.journaldulapin.com/wp-content/uploads/2025/05/05-8.png" alt="" width="512" height="342" srcset="https://www.journaldulapin.com/wp-content/uploads/2025/05/05-8.png 512w, https://www.journaldulapin.com/wp-content/uploads/2025/05/05-8-320x214.png 320w" sizes="auto, (max-width: 512px) 100vw, 512px"></a></p><p id="caption-attachment-112662">This checkbox (checked here) prevents loading the 18-point font</p></div>
<p>In any case, it provides a time gain even greater than Apple’s estimate. Booting with the Japanese ROM saves around 15 seconds—and also frees about 113 KB of RAM, which is significant given the Macintosh Plus could be limited to just 1 MB. Skipping the 18-point font saves even more RAM, since it uses slightly more memory.</p>
<div id="attachment_112663"><p><a href="https://www.journaldulapin.com/wp-content/uploads/2025/05/06-7.png"><img loading="lazy" decoding="async" aria-describedby="caption-attachment-112663" src="https://www.journaldulapin.com/wp-content/uploads/2025/05/06-7.png" alt="" width="512" height="342" srcset="https://www.journaldulapin.com/wp-content/uploads/2025/05/06-7.png 512w, https://www.journaldulapin.com/wp-content/uploads/2025/05/06-7-320x214.png 320w" sizes="auto, (max-width: 512px) 100vw, 512px"></a></p><p id="caption-attachment-112663">The developers of KanjiTalk</p></div>
<h3>The Emulation Case</h3>
<p>This part is a bit tricky. By default, emulators don’t recognize the 256 KB ROM and won’t run it. Doug, much more experienced than I am in development, modified <a href="https://www.mamedev.org/">MAME</a> to allow the Japanese ROM. It works the same as a regular Macintosh: with a standard ROM, you’ll need the font disk. With the Japanese ROM, you’ll only need it for the optional 18-point font. My tests with a manually compiled version were successful, and the necessary changes to use the Japanese ROM with MAME should be integrated into the software soon.</p>
<div id="attachment_112669"><p><a href="https://www.journaldulapin.com/wp-content/uploads/2025/05/Capture-decran-2025-05-11-a-18.04.05.png"><img loading="lazy" decoding="async" aria-describedby="caption-attachment-112669" src="https://www.journaldulapin.com/wp-content/uploads/2025/05/Capture-decran-2025-05-11-a-18.04.05-640x466.png" alt="" width="640" height="466" srcset="https://www.journaldulapin.com/wp-content/uploads/2025/05/Capture-decran-2025-05-11-a-18.04.05-640x466.png 640w, https://www.journaldulapin.com/wp-content/uploads/2025/05/Capture-decran-2025-05-11-a-18.04.05-850x619.png 850w, https://www.journaldulapin.com/wp-content/uploads/2025/05/Capture-decran-2025-05-11-a-18.04.05-320x233.png 320w, https://www.journaldulapin.com/wp-content/uploads/2025/05/Capture-decran-2025-05-11-a-18.04.05-768x559.png 768w, https://www.journaldulapin.com/wp-content/uploads/2025/05/Capture-decran-2025-05-11-a-18.04.05-1536x1119.png 1536w, https://www.journaldulapin.com/wp-content/uploads/2025/05/Capture-decran-2025-05-11-a-18.04.05-2048x1492.png 2048w" sizes="auto, (max-width: 640px) 100vw, 640px"></a></p><p id="caption-attachment-112669">The Japanese ROM</p></div><br>
<div id="attachment_112667"><p><a href="https://www.journaldulapin.com/wp-content/uploads/2025/05/Capture-decran-2025-05-11-a-18.02.51-1.png"><img loading="lazy" decoding="async" aria-describedby="caption-attachment-112667" src="https://www.journaldulapin.com/wp-content/uploads/2025/05/Capture-decran-2025-05-11-a-18.02.51-1-640x502.png" alt="" width="640" height="502" srcset="https://www.journaldulapin.com/wp-content/uploads/2025/05/Capture-decran-2025-05-11-a-18.02.51-1-640x502.png 640w, https://www.journaldulapin.com/wp-content/uploads/2025/05/Capture-decran-2025-05-11-a-18.02.51-1-850x666.png 850w, https://www.journaldulapin.com/wp-content/uploads/2025/05/Capture-decran-2025-05-11-a-18.02.51-1-320x251.png 320w, https://www.journaldulapin.com/wp-content/uploads/2025/05/Capture-decran-2025-05-11-a-18.02.51-1-768x602.png 768w, https://www.journaldulapin.com/wp-content/uploads/2025/05/Capture-decran-2025-05-11-a-18.02.51-1-1536x1204.png 1536w, https://www.journaldulapin.com/wp-content/uploads/2025/05/Capture-decran-2025-05-11-a-18.02.51-1.png 1822w" sizes="auto, (max-width: 640px) 100vw, 640px"></a></p><p id="caption-attachment-112667">It boots</p></div><br>
<div id="attachment_112668"><p><a href="https://www.journaldulapin.com/wp-content/uploads/2025/05/Capture-decran-2025-05-11-a-18.03.09-1.png"><img loading="lazy" decoding="async" aria-describedby="caption-attachment-112668" src="https://www.journaldulapin.com/wp-content/uploads/2025/05/Capture-decran-2025-05-11-a-18.03.09-1-640x502.png" alt="" width="640" height="502" srcset="https://www.journaldulapin.com/wp-content/uploads/2025/05/Capture-decran-2025-05-11-a-18.03.09-1-640x502.png 640w, https://www.journaldulapin.com/wp-content/uploads/2025/05/Capture-decran-2025-05-11-a-18.03.09-1-850x666.png 850w, https://www.journaldulapin.com/wp-content/uploads/2025/05/Capture-decran-2025-05-11-a-18.03.09-1-320x251.png 320w, https://www.journaldulapin.com/wp-content/uploads/2025/05/Capture-decran-2025-05-11-a-18.03.09-1-768x602.png 768w, https://www.journaldulapin.com/wp-content/uploads/2025/05/Capture-decran-2025-05-11-a-18.03.09-1-1536x1204.png 1536w, https://www.journaldulapin.com/wp-content/uploads/2025/05/Capture-decran-2025-05-11-a-18.03.09-1.png 1822w" sizes="auto, (max-width: 640px) 100vw, 640px"></a></p><p id="caption-attachment-112668">It does indeed ask for the 18-point font directly</p></div>
<p>In any case, this was a project that took time, required persistence, and is personally rewarding: I managed to find something rare, undocumented, and unusual.</p>
											</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LLMs are more persuasive than incentivized human persuaders (116 pts)]]></title>
            <link>https://arxiv.org/abs/2505.09662</link>
            <guid>44016621</guid>
            <pubDate>Sat, 17 May 2025 20:05:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2505.09662">https://arxiv.org/abs/2505.09662</a>, See on <a href="https://news.ycombinator.com/item?id=44016621">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
    <div><p><span>Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Schoenegger,+P" rel="nofollow">Philipp Schoenegger</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Salvi,+F" rel="nofollow">Francesco Salvi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+J" rel="nofollow">Jiacheng Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Nan,+X" rel="nofollow">Xiaoli Nan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Debnath,+R" rel="nofollow">Ramit Debnath</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fasolo,+B" rel="nofollow">Barbara Fasolo</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Leivada,+E" rel="nofollow">Evelina Leivada</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Recchia,+G" rel="nofollow">Gabriel Recchia</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=G%C3%BCnther,+F" rel="nofollow">Fritz Günther</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zarifhonarvar,+A" rel="nofollow">Ali Zarifhonarvar</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kwon,+J" rel="nofollow">Joe Kwon</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Islam,+Z+U" rel="nofollow">Zahoor Ul Islam</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dehnert,+M" rel="nofollow">Marco Dehnert</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lee,+D+Y+H" rel="nofollow">Daryl Y. H. Lee</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Reinecke,+M+G" rel="nofollow">Madeline G. Reinecke</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kamper,+D+G" rel="nofollow">David G. Kamper</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Koba%C5%9F,+M" rel="nofollow">Mert Kobaş</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sandford,+A" rel="nofollow">Adam Sandford</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kgomo,+J" rel="nofollow">Jonas Kgomo</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hewitt,+L" rel="nofollow">Luke Hewitt</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kapoor,+S" rel="nofollow">Shreya Kapoor</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Oktar,+K" rel="nofollow">Kerem Oktar</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kucuk,+E+E" rel="nofollow">Eyup Engin Kucuk</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Feng,+B" rel="nofollow">Bo Feng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jones,+C+R" rel="nofollow">Cameron R. Jones</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gainsburg,+I" rel="nofollow">Izzy Gainsburg</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Olschewski,+S" rel="nofollow">Sebastian Olschewski</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Heinzelmann,+N" rel="nofollow">Nora Heinzelmann</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cruz,+F" rel="nofollow">Francisco Cruz</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tappin,+B+M" rel="nofollow">Ben M. Tappin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ma,+T" rel="nofollow">Tao Ma</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Park,+P+S" rel="nofollow">Peter S. Park</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Onyonka,+R" rel="nofollow">Rayan Onyonka</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hjorth,+A" rel="nofollow">Arthur Hjorth</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Slattery,+P" rel="nofollow">Peter Slattery</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zeng,+Q" rel="nofollow">Qingcheng Zeng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Finke,+L" rel="nofollow">Lennart Finke</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Grossmann,+I" rel="nofollow">Igor Grossmann</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Salatiello,+A" rel="nofollow">Alessandro Salatiello</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Karger,+E" rel="nofollow">Ezra Karger</a></p></div>            
    <p><a href="https://arxiv.org/pdf/2505.09662">View PDF</a></p><blockquote>
            <span>Abstract:</span>We directly compare the persuasion capabilities of a frontier large language model (LLM; Claude Sonnet 3.5) against incentivized human persuaders in an interactive, real-time conversational quiz setting. In this preregistered, large-scale incentivized experiment, participants (quiz takers) completed an online quiz where persuaders (either humans or LLMs) attempted to persuade quiz takers toward correct or incorrect answers. We find that LLM persuaders achieved significantly higher compliance with their directional persuasion attempts than incentivized human persuaders, demonstrating superior persuasive capabilities in both truthful (toward correct answers) and deceptive (toward incorrect answers) contexts. We also find that LLM persuaders significantly increased quiz takers' accuracy, leading to higher earnings, when steering quiz takers toward correct answers, and significantly decreased their accuracy, leading to lower earnings, when steering them toward incorrect answers. Overall, our findings suggest that AI's persuasion capabilities already exceed those of humans that have real-money bonuses tied to performance. Our findings of increasingly capable AI persuaders thus underscore the urgency of emerging alignment and governance frameworks.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Philipp Schoenegger [<a href="https://arxiv.org/show-email/997461f0/2505.09662" rel="nofollow">view email</a>]      <br>    <strong>[v1]</strong>
        Wed, 14 May 2025 14:31:33 UTC (1,041 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Directory of MCP Servers (130 pts)]]></title>
            <link>https://github.com/chatmcp/mcpso</link>
            <guid>44016336</guid>
            <pubDate>Sat, 17 May 2025 19:14:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/chatmcp/mcpso">https://github.com/chatmcp/mcpso</a>, See on <a href="https://news.ycombinator.com/item?id=44016336">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">MCP Directory</h2><a id="user-content-mcp-directory" aria-label="Permalink: MCP Directory" href="#mcp-directory"></a></p>
<p dir="auto">a directory for Awesome MCP Servers.</p>
<p dir="auto">live preview: <a href="https://mcp.so/" rel="nofollow">https://mcp.so</a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/chatmcp/mcpso/blob/main/preview.png"><img src="https://github.com/chatmcp/mcpso/raw/main/preview.png" alt="preview"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick Start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick Start" href="#quick-start"></a></p>
<ol dir="auto">
<li>clone the repo</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/chatmcp/mcp-directory.git
cd mcp-directory"><pre>git clone https://github.com/chatmcp/mcp-directory.git
<span>cd</span> mcp-directory</pre></div>
<ol start="2" dir="auto">
<li>install dependencies</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="pnpm install"><pre>pnpm install</pre></div>
<ol start="3" dir="auto">
<li>prepare database</li>
</ol>
<p dir="auto">create a database with <a href="https://supabase.com/" rel="nofollow">Supabase</a></p>
<p dir="auto">run the sql file in <code>data/install.sql</code></p>
<ol start="4" dir="auto">
<li>set env variables</li>
</ol>
<p dir="auto">put a .env file in the root directory</p>
<p dir="auto">with env variables:</p>
<div dir="auto" data-snippet-clipboard-copy-content="SUPABASE_URL=&quot;&quot;
SUPABASE_ANON_KEY=&quot;&quot;

NEXT_PUBLIC_WEB_URL=&quot;http://localhost:3000&quot;"><pre><span>SUPABASE_URL</span><span>=</span><span><span>"</span><span>"</span></span>
<span>SUPABASE_ANON_KEY</span><span>=</span><span><span>"</span><span>"</span></span>

<span>NEXT_PUBLIC_WEB_URL</span><span>=</span><span><span>"</span>http://localhost:3000<span>"</span></span></pre></div>
<ol start="5" dir="auto">
<li>run the dev server</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="pnpm dev"><pre>pnpm dev</pre></div>
<ol start="6" dir="auto">
<li>preview the site</li>
</ol>
<p dir="auto">open <a href="http://localhost:3000/" rel="nofollow">http://localhost:3000</a> in your browser</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Community</h2><a id="user-content-community" aria-label="Permalink: Community" href="#community"></a></p>
<ul dir="auto">
<li><a href="https://t.me/+N0gv4O9SXio2YWU1" rel="nofollow">MCP Server Telegram</a></li>
<li><a href="https://discord.gg/RsYPRrnyqg" rel="nofollow">MCP Server Discord</a></li>
<li><a href="https://x.com/chatmcp" rel="nofollow">ChatMCP Official Twitter</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">About the author</h2><a id="user-content-about-the-author" aria-label="Permalink: About the author" href="#about-the-author"></a></p>
<ul dir="auto">
<li><a href="https://bento.me/idoubi" rel="nofollow">idoubi</a></li>
<li><a href="https://x.com/idoubicv" rel="nofollow">Follow me on Twitter</a></li>
<li><a href="https://www.buymeacoffee.com/idoubi" rel="nofollow">Buy me a coffee</a></li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mystical (223 pts)]]></title>
            <link>https://suberic.net/~dmm/projects/mystical/README.html</link>
            <guid>44016037</guid>
            <pubDate>Sat, 17 May 2025 18:21:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://suberic.net/~dmm/projects/mystical/README.html">https://suberic.net/~dmm/projects/mystical/README.html</a>, See on <a href="https://news.ycombinator.com/item?id=44016037">Hacker News</a></p>
<div id="readability-page-1" class="page">


<p><img src="https://suberic.net/~dmm/projects/mystical/images/quicksort_example.png" alt="quicksort example"></p>
<p>I wanted to make a programming language that resembled magical circles. This is more like a way to write PostScript that looks like a magical circle, but I will refer to it as Mystical in this document.</p>
<h2>Rings</h2>
<p>The structure of Mystical is based on rings. These are circular bands of text and sigils, with an inner and outer border. The content of the main ring of a program starts at the rightmost (3:00) point and flow continues widdershins (counter-clockwise) both to respect postscript's angles and to reflect the assumption that these rings should be written from the outside.  Subsidiary rings start from their attachment point to their caller.</p>
<p>There are three types of rings in Mystical:</p>
<ul>
<li>executable arrays, written in <code>{</code> <code>}</code> in PostScript, are represented with simple circular borders on the inside and outside of the ring, with a star of some sort inside. The start/end point is marked by a symbol based on the "work complete" symbol from alchemy.</li>
<li>non-executable arrays, written in <code>[</code> <code>]</code> in PostScript, are the same but without the star. The start/end point is marked with a simple triangle.</li>
<li>dictionaries, written in <code>&lt;&lt;</code> <code>&gt;&gt;</code> in PostScript, are polygons with a double outer border and a single inner border. The start/end point is marked the same as the array.</li>
</ul>
<table>
<thead>
<tr>
<th>xarray</th>
<th>array</th>
<th>dict</th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/xarray_example.png" alt="xarray example"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/array_example.png" alt="array example"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/dict_example.png" alt="dict example"></td>
</tr>
<tr>
<td><code>{ 0 0 currentlinewidth 1.5 mul 0 360 arc fill }</code></td>
<td><code>[ 0 1 2 1.5 40 360 (Hooray World) ]</code></td>
<td><code>&lt;&lt; /longname (Mystical) /w 45 /h 8 /x 23 &gt;&gt;</code></td>
</tr>
</tbody>
</table>
<p>(Note that the entries in the dict image are in a different order than the PostScript text since dict insertion order is not preserved in PostScript.)</p>
<p>When one of these structures appear inside a different structure, a small circle or dot at the inclusion point is connected to a line which leads to the subsidiary ring's start/end sigil.</p>
<p><img src="https://suberic.net/~dmm/projects/mystical/images/link_example.png" alt="link example"></p>
<pre><code>[
    0 1 2 1.5 40 360 &lt;&lt;
        /longname (Mystical) /w 45 /h 8 /x 23
    &gt;&gt;
]
</code></pre>
<p>It is theoretically possible to use <code>[ ]</code> and <code>&lt;&lt; &gt;&gt;</code> in PostScript in ways that Mystical can't handle:</p>
<pre><code>[ 1 2 3 split { ] /first exch def [ } if 4 5 6 ] /final exch def
</code></pre>
<p>so don't do that.</p>
<p>Other commands like <code>gsave/grestore</code> and <code>begin/end</code> are more likely to be used in non-balanced or loop-crossing ways so those are treated as normal sigils below.</p>
<h2>Text and Sigils</h2>
<p>The rings' rims contain text or sigils.  Sigils are symbols that stand in for operators, variables, or other keywords. Any name, written in PostScript as <code>/name</code>, is instead written with a triangle surrounding or superimposing the text of the name or its sigil.  Any strings, written in () in Postscript, are cartouche-like shapes containing the string text.</p>
<table>
<thead>
<tr>
<th>array</th>
<th>/array</th>
<th>(array)</th>
<th>foo</th>
<th>/foo</th>
<th>/foobar</th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/operator_array.png" alt="array sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/name_array.png" alt="/array sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/string_array.png" alt="array string"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/operator_foo.png" alt="foo"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/name_foo.png" alt="foo name"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/name_foobar.png" alt="foobar name"></td>
</tr>
</tbody>
</table>
<h3>Standard Sigils</h3>
<p>Many built-in operators have been given their own sigils.  These are used in place of the text of the operator if it appears as a name or operator (but not if it appears as a string).  I have generally made these sigils based on the initial of the command and an illustration of the concept, though in some cases I have taken a more fully illustrative route or created some standard visual language.  Some examples are below - see <a href="https://suberic.net/~dmm/projects/mystical/docs/operators.html">Standard Sigils</a> for a full list.</p>
<h4>Sample sigils</h4>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_dup.png" alt="dup sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_copy.png" alt="copy sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_add.png" alt="add sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_mul.png" alt="mul sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_neg.png" alt="neg sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_for.png" alt="for sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_forall.png" alt="forall sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_repeat.png" alt="repeat sigil"></td>
</tr>
<tr>
<td>dup</td>
<td>copy</td>
<td>add</td>
<td>mul</td>
<td>neg</td>
<td>for</td>
<td>forall</td>
<td>repeat</td>
</tr>
<tr>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_if.png" alt="if sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_ifelse.png" alt="ifelse sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_eq.png" alt="eq sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_ne.png" alt="ne sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_ge.png" alt="ge sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_gt.png" alt="gt sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_le.png" alt="le sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_lt.png" alt="lt sigil"></td>
</tr>
<tr>
<td>if</td>
<td>ifelse</td>
<td>eq</td>
<td>ne</td>
<td>ge</td>
<td>gt</td>
<td>le</td>
<td>lt</td>
</tr>
<tr>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_moveto.png" alt="moveto sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_lineto.png" alt="lineto sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_arc.png" alt="arc sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_arcn.png" alt="arcn sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_curveto.png" alt="curveto sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_closepath.png" alt="closepath sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_stroke.png" alt="stroke sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_fill.png" alt="fill sigil"></td>
</tr>
<tr>
<td>moveto</td>
<td>lineto</td>
<td>arc</td>
<td>arcn</td>
<td>curveto</td>
<td>closepath</td>
<td>stroke</td>
<td>fill</td>
</tr>
<tr>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_gsave.png" alt="gsave sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_grestore.png" alt="grestore sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_translate.png" alt="translate sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_scale.png" alt="scale sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_rotate.png" alt="rotate sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_setmatrix.png" alt="setmatrix sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_currentmatrix.png" alt="currentmatrix sigil"></td>
<td></td>
</tr>
<tr>
<td>gsave</td>
<td>grestore</td>
<td>translate</td>
<td>scale</td>
<td>rotate</td>
<td>setmatrix</td>
<td>currentmatrix</td>
<td></td>
</tr>
<tr>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_setrgbcolor.png" alt="setrgbcolor sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_currentrgbcolor.png" alt="currentrgbcolor sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_setcmykcolor.png" alt="setcmykcolor sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_currentcmykcolor.png" alt="currentcmykcolor sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_sethsbcolor.png" alt="sethsbcolor sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_currenthsbcolor.png" alt="currenthsbcolor sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_setgray.png" alt="setgray sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_currentgray.png" alt="currentgray sigil"></td>
</tr>
<tr>
<td>setrgbcolor</td>
<td>currentrgbcolor</td>
<td>setcmykcolor</td>
<td>currentcmykcolor</td>
<td>sethsbcolor</td>
<td>currenthsbcolor</td>
<td>setgray</td>
<td>currentgray</td>
</tr>
<tr>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_dict.png" alt="dict sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_begin.png" alt="begin sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_end.png" alt="end sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_def.png" alt="def sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_get.png" alt="get sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_put.png" alt="put sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_length.png" alt="length sigil"></td>
<td></td>
</tr>
<tr>
<td>dict</td>
<td>begin</td>
<td>end</td>
<td>def</td>
<td>get</td>
<td>put</td>
<td>length</td>
<td></td>
</tr>
</tbody>
</table>
<h3>User Sigils</h3>
<p>Sigils for new functions or names can be added to <code>sigil_bank</code> at runtime.  They should fit into the 1-unit square centered on the origin, so no coordinate should be more than 0.5 (of course, you can transform your coordinate system for convenience).  If you use <code>nstroke</code> instead of <code>stroke</code> you will get the same calligraphic effect as the standard sigils.</p>
<p>Sigils for user variables can be designed with any sigil system.  My examples mostly use letter collision, inspired by Spare's Chaos Magick system, but anything that turns a word into a symbol will work - kameas, wheels, Square Word Calligraphy, Circular Gallifreyan, sitelen sitelen, illustration, puns, etc.  New names based on official operators can incorporate the standard sigils for those operators.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_arg.png" alt="arg sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_dot.png" alt="dot sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_softscale.png" alt="softscale sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_nstroke.png" alt="nstroke sigil"></td>
</tr>
<tr>
<td>arg</td>
<td>dot</td>
<td>softscale</td>
<td>nstroke</td>
</tr>
</tbody>
</table>
<h2>Ligature for <code>/name { ring } def</code></h2>
<p>There is a sigil for <code>def</code> but a very common pattern is to push a name, push a function, and def the name to the function. To save space and to emphasize this definition, there is special syntax for this case consisting of the usual name triangle with the end of the link line directly below it, and the def sigil is omitted entirely. This is extended to the other two ring types for simplicity. Any other use of <code>def</code> will just use the def sigil as normal.</p>
<table>
<thead>
<tr>
<th><img src="https://suberic.net/~dmm/projects/mystical/images/ligature_example.png" alt="ligature example"></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>{ ...  /even { 2 mod 0 eq } def ...  }</code></td>
</tr>
</tbody>
</table>
<p>This only applies inside of executable arrays. I considered a similar ligature for /name { ring } in dictionaries but there's too much chance of getting it wrong.</p>
<h2>Sample Algorithms</h2>
<p>Quicksort is the illustration at the top of this page.</p>
<p>Euclid's GCD algorithm (using my <code>/arg {exch def} def</code> function from dmmlib):</p>
<p><img src="https://suberic.net/~dmm/projects/mystical/images/gcd_example.png" alt="gcd example"></p>
<h2>Functions to generate Mystical images</h2>
<p>All of these are defined in "mystical.ps".</p>
<p><code>mystical</code>: takes an array, xarray, or dict and renders it in mystical, descending into substructures as necessary.  The entire image will be scaled to fit into a unit circle.</p>
<p><code>mystical_evoke</code>: The same as <code>mystical</code> but it takes a name that is looked up in the current dictionary.</p>
<p><code>mystical_evoke_label</code>:  Like <code>mystical_evoke</code> but adds a name-def ligature with the name at the top and orients the image so that the name sigil is right-side-up.</p>
<p>All of these have versions with <code>_unscaled</code> appended to them that skip the scaling step.  The rings will be 1 unit thick so the image will be quite large.</p>
<h3>layout issues</h3>
<p>Currently the code figures out the layout of the subcircles so that nothing collides, but it's overly safe so most programs will be very spread out.  For the examples on this page I ran the parsing/layout functions (<code>mystical_get_spell</code> and <code>mystical_make_evocation_ligature</code>) and then adjusted the results before calling the draw functions <code>draw_sigil</code> and <code>draw_link</code>.  I'm intending to improve the default layout somewhat.</p>
<h2>Is this a programming language?</h2>
<p>At the moment it's a way to draw a PostScript program - there's no interpreter that will ingest a Mystical image and perform the appropriate computation.  It could be run and interpreted by a human, or (more likely) a human could read it and turn it into a PostScript program and run that.  I'll leave further philosophical arguments to other people for now.</p>
<h2>Could this work for other languages?</h2>
<p>This approach seems applicable to other language with just operators, such as Forth. Languages with more complicated statements might be more difficult, and I don't know if a new ring for every brace or indent will be overly busy.</p>

<hr><p><a href="https://github.com/denismm/mystical_ps">Download on github</a></p><hr><p><a href="https://codeberg.org/yomikoma/mystical_ps">Download on codeberg</a></p><hr>
<address>This page generated 2025-05-16 by <a href="https://suberic.net/~dmm/">Denis</a>.</address>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dead Stars Don't Radiate (190 pts)]]></title>
            <link>https://johncarlosbaez.wordpress.com/2025/05/17/dead-stars-dont-radiate-and-shrink/</link>
            <guid>44015872</guid>
            <pubDate>Sat, 17 May 2025 17:54:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://johncarlosbaez.wordpress.com/2025/05/17/dead-stars-dont-radiate-and-shrink/">https://johncarlosbaez.wordpress.com/2025/05/17/dead-stars-dont-radiate-and-shrink/</a>, See on <a href="https://news.ycombinator.com/item?id=44015872">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page">
				<p><a href="https://johncarlosbaez.wordpress.com/wp-content/uploads/2025/05/end_of_universe_coming_sooner_2.jpg"><img data-attachment-id="39886" data-permalink="https://johncarlosbaez.wordpress.com/2025/05/17/dead-stars-dont-radiate-and-shrink/end_of_universe_coming_sooner_2/" data-orig-file="https://johncarlosbaez.wordpress.com/wp-content/uploads/2025/05/end_of_universe_coming_sooner_2.jpg" data-orig-size="984,815" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="end_of_universe_coming_sooner_2" data-image-description="" data-image-caption="" data-medium-file="https://johncarlosbaez.wordpress.com/wp-content/uploads/2025/05/end_of_universe_coming_sooner_2.jpg?w=300" data-large-file="https://johncarlosbaez.wordpress.com/wp-content/uploads/2025/05/end_of_universe_coming_sooner_2.jpg?w=450" src="https://johncarlosbaez.wordpress.com/wp-content/uploads/2025/05/end_of_universe_coming_sooner_2.jpg?w=450" alt="" width="450" height="373" srcset="https://johncarlosbaez.wordpress.com/wp-content/uploads/2025/05/end_of_universe_coming_sooner_2.jpg?w=450 450w, https://johncarlosbaez.wordpress.com/wp-content/uploads/2025/05/end_of_universe_coming_sooner_2.jpg?w=900 900w, https://johncarlosbaez.wordpress.com/wp-content/uploads/2025/05/end_of_universe_coming_sooner_2.jpg?w=150 150w, https://johncarlosbaez.wordpress.com/wp-content/uploads/2025/05/end_of_universe_coming_sooner_2.jpg?w=300 300w, https://johncarlosbaez.wordpress.com/wp-content/uploads/2025/05/end_of_universe_coming_sooner_2.jpg?w=768 768w" sizes="(max-width: 450px) 100vw, 450px"></a></p>
<p>Three guys claim that any heavy chunk of matter emits Hawking radiation, even if it’s not a black hole:</p>
<p>• Michael F. Wondrak, Walter D. van Suijlekom and Heino Falcke, <a href="https://arxiv.org/abs/2305.18521">Gravitational pair production and black hole evaporation</a>, <i>Phys. Rev. Lett.</i> <b>130</b> (2023), 221502.</p>
<p>Now they’re getting more publicity by claiming this means that the universe will fizzle out sooner than we expected.  <a href="https://iopscience.iop.org/article/10.1088/1475-7516/2025/05/023">They’re claiming</a>, for example, that a dead, cold star will emit Hawking radiation, and thus slowly lose mass and eventually disappear!</p>
<p>They admit that this would violate baryon conservation: after all, the protons and neutrons in the star would have to go away somehow!  They admit they don’t know how this would work.  They just say that the gravitational field of the star will create particle-antiparticle pairs that will slowly radiate away, forcing the dead star to lose mass <em>somehow</em> to conserve energy.</p>
<p>If experts thought this had even a chance of being true, it would be the biggest thing since sliced bread—at least in the field of quantum gravity.  Everyone would be writing papers about it, because if true it would be revolutionary.   It would overturn calculations by experts which say that a stationary chunk of matter doesn’t emit Hawking radiation.  It would also mean that quantum field theory in curved spacetime can only be consistent if baryon number fails to be conserved!  This would be utterly shocking.</p>
<p>But in fact, these new papers have had almost zero effect on physics.  There’s a short rebuttal, here:</p>
<p>• Antonio Ferreiro José Navarro-Salas and Silvia Pla, <a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.133.229001">Comment on “Gravitational pair production and black hole evaporation”</a>, <i>Phys. Rev. Lett.</i> <b>133</b> (2024), 229001.</p>
<p>It explains that these guys used a crude approximation that gives wrong results even in a simpler problem.   Similar points are made here:</p>
<p>• E. T. Akhmedov, D. V. Diakonov and C. Schubert, <a href="https://link.aps.org/pdf/10.1103/PhysRevD.110.105011">Complex effective actions and gravitational pair creation</a>, <i>Phys. Rev. D.</i> <b>110</b>, 105011.</p>
<p>Unfortunately, it seems the real experts on quantum field theory in curved spacetime have not come out and mentioned the <i>correct</i> way to think about this issue, which has been known at least since 1975.  To them—or maybe I should dare to say “us”—it’s just <em>well known</em> that the gravitational field of a static mass does not cause the creation of particle-antiparticle pairs.</p>
<p>Of course, the referees should have rejected Wondrak, van Suijlekom and Falcke’s papers.  But apparently none of those referees were experts on the subject at hand.  So you can’t trust a paper just because it appears in a supposedly reputable physics journal.  You have to actually understand the subject and assess the paper yourself, or talk to some experts you trust.</p>
<p>If I were a science journalist writing an article about a supposedly shocking development like this, I would email some experts and check to see if it’s for real.  But plenty of science journalists don’t bother with that anymore: they just believe the press releases.  So now we’re being bombarded with lazy articles like these:</p>
<p>• <a href="https://www.cbsnews.com/news/universe-end-much-sooner-than-expected-researchers-say/">Universe will die “much sooner than expected,” new research says</a>, <i>CBS News</i>, May 13, 2025.</p>
<p>• Sharmila Kuthunur, <a href="https://www.space.com/astronomy/scientists-calculate-when-the-universe-will-end-its-sooner-than-expected">Scientists calculate when the universe will end—it’s sooner than expected</a>, <i>Space.com</i>, 15 May 2025.</p>
<p>• Jamie Carter, <a href="https://www.forbes.com/sites/jamiecartereurope/2025/05/16/the-universe-will-end-sooner-than-thought-scientists-say/">The universe will end sooner than thought, scientists say</a>, <i>Forbes</i>, 16 May 2025.</p>
<p>The list goes on; these are just three.   There’s no way what I say can have much effect against such a flood of misinformation.  As Mark Twain said, “A lie can travel around the world and back again while the truth is lacing up its boots.”  Actually he probably didn’t say that—but everyone keeps saying he did, illustrating the point perfectly.</p>
<p>Still, there might be a few people who both care and don’t already know this stuff.  Instead of trying to give a mini-course here, let me simply point to an explanation of how things really work:</p>
<p>• Abhay Ashtekar and Anne Magnon, <a href="https://www.researchgate.net/publication/243683929_Quantum_Fields_in_Curved_Space-Times">Quantum fields in curved space-times</a>, <i>Proceedings of the Royal Society</i>, <b>346</b> (1975), 375–394.</p>
<p>It’s technical, so it’s not easy reading if you haven’t studied quantum field theory and general relativity, but that’s unavoidable.  It shows that in a static spacetime there is a well-defined concept of ‘vacuum’, and the vacuum is stable.  Jorge Pullin pointed out the key sentence for present purposes:</p>
<blockquote><p>
  Thus, if the underlying space-time admits a everywhere time-like Killing field, the vacuum state is indeed stable and phenomena such as the spontaneous creation of particles do not occur.
</p></blockquote>
<p>This condition of having an “everywhere time-like Killing field” says that a spacetime has time translation symmetry.  Ashtekar and Magnon also assume that spacetime is globally hyperbolic and that the wave equation for a massive spin-zero particle has a smooth solution given smooth initial data.  All this lets us define a concept of energy for solutions of this equation.  It also lets us split solutions into positive-frequency solutions, which correspond to particles, and negative-frequency ones, which correspond to antiparticles.  We can thus set up quantum field theory in way we’re used to on Minkowski spacetime, where there’s a well-defined vacuum which does not decay into particle-antiparticle pairs.</p>
<p>The Schwarzschild solution, which describes a static black hole, also has a Killing field.  But this ceases to be timelike at the event horizon, so this result does not apply to that!</p>
<p>I could go into more detail if required, but you can find a more pedagogical treatment in this standard textbook:</p>
<p>• Robert Wald, <i>Quantum Field Theory in Curved Spacetime and Black Hole Thermodynamics</i>, University of Chicago Press, Chicago, 1994.</p>
<p>In particular, go to Section 4.3, which is on quantum field theory in stationary spacetimes.</p>
<p>I also can’t resist citing this thesis by a student of mine:</p>
<p>• Valeria Michelle Carrión Álvarez, <a href="https://www.arxiv.org/abs/math-ph/0412032"><i>Loop Quantization versus Fock Quantization of p-Form Electromagnetism on Static Spacetimes</i></a>, Ph.D. thesis, U. C. Riverside, 2004.</p>
<p>This thesis covers the case of electromagnetism, while Ashtekar and Magnon, and also Wald, focus on a massive scalar field for simplicity.</p>
<p>So: it’s been rigorously shown that the gravitational field of a static object does not create particle-antiparticle pairs.  This has been known for decades.  Now some people have done a crude approximate calculation that seems to show otherwise.  Some flaws in the approximation have been pointed out.  Of course the authors of the calculation <a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.133.229002">don’t believe their approximation is flawed</a>.    We could argue about that for a long time.  But it’s scarcely worth thinking about, because no approximations were required to settle this issue.  It was settled over 50 years ago, and the new work is not shedding new light on the issue: it’s much more hand-wavy than the old work.</p>

				
				<p>
					<small>
					This entry was posted  on Saturday, May 17th, 2025 at 2:44 pm and is filed under <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>.					You can follow any responses to this entry through the <a href="https://johncarlosbaez.wordpress.com/2025/05/17/dead-stars-dont-radiate-and-shrink/feed/">RSS 2.0</a> feed.
											You can <a href="#respond">leave a response</a>, or <a href="https://johncarlosbaez.wordpress.com/2025/05/17/dead-stars-dont-radiate-and-shrink/trackback/" rel="trackback">trackback</a> from your own site.
					
					</small>
				</p>

				<nav id="nav-below">
					<h3>Post navigation</h3>
					<span><a href="https://johncarlosbaez.wordpress.com/2025/05/16/meteor-burst-communications/" rel="prev">« Previous Post</a></span>
					<span></span>
				</nav><!-- #nav-below -->

			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I built a knife steel comparison tool (115 pts)]]></title>
            <link>https://new.knife.day/blog/knife-steel-comparisons/all</link>
            <guid>44015649</guid>
            <pubDate>Sat, 17 May 2025 17:13:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://new.knife.day/blog/knife-steel-comparisons/all">https://new.knife.day/blog/knife-steel-comparisons/all</a>, See on <a href="https://news.ycombinator.com/item?id=44015649">Hacker News</a></p>
<div id="readability-page-1" class="page"><div width="narrow"><div><h2>Knife Steel Performance Comparison</h2><p>No steels found matching "<!-- -->"</p><div><p>Select up to 5 steels to compare their performance</p></div><div><h6>Note: All properties are rated on a scale of 1-10, where 10 is the highest performance.</h6><ul><li><p><strong>Corrosion Resistance:</strong> Higher values indicate better resistance to rust and staining</p></li><li><p><strong>Toughness:</strong> Higher values indicate better resistance to chipping and breaking</p></li><li><p><strong>Edge Retention:</strong> Higher values indicate longer-lasting sharpness</p></li><li><p><strong>Ease of Sharpening:</strong> Higher values indicate easier maintenance</p></li></ul></div></div><p>Browse our comprehensive collection of knife steel comparisons. We currently have <!-- -->150<!-- --> detailed comparisons available, with <!-- -->147<!-- --> featuring detailed comparison charts.</p><div><p><span>147 with comparison charts</span></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to have the browser pick a contrasting color in CSS (173 pts)]]></title>
            <link>https://webkit.org/blog/16929/contrast-color/</link>
            <guid>44015367</guid>
            <pubDate>Sat, 17 May 2025 16:26:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://webkit.org/blog/16929/contrast-color/">https://webkit.org/blog/16929/contrast-color/</a>, See on <a href="https://news.ycombinator.com/item?id=44015367">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-16929">
            
            

            <div>
                                
                <p>Have you ever wished you could write simple CSS to declare a color, and then have the browser figure out whether black or white should be paired with that color? Well, now you can, with <code>contrast-color()</code>. Here’s how it works.</p>
<p>Imagine we’re building a website or a web app, and the design calls for a bunch of buttons with different background colors. We can create a variable named <code>--button-color</code>  to handle the background color. And then assign that variable different values from our design system in different situations.</p>
<p>Sometimes the button background will be a dark color, and the button text should be white to provide contrast. Other times, the background will be a lighter color, and the text should be black. Like this:</p>
<figure><picture><source src="https://webkit.org/wp-content/uploads/cc-button-1-dark-scaled.png" type="image/png" media="(prefers-color-scheme: dark)"><img fetchpriority="high" decoding="async" src="https://webkit.org/wp-content/uploads/cc-button-1-light-scaled.png" alt="Two buttons side by side. White text on dark purple for the first, black text on pink background for the second. " width="2560" height="546" srcset="https://webkit.org/wp-content/uploads/cc-button-1-light-scaled.png 2560w, https://webkit.org/wp-content/uploads/cc-button-1-light-300x64.png 300w, https://webkit.org/wp-content/uploads/cc-button-1-light-1024x218.png 1024w, https://webkit.org/wp-content/uploads/cc-button-1-light-768x164.png 768w, https://webkit.org/wp-content/uploads/cc-button-1-light-1536x327.png 1536w, https://webkit.org/wp-content/uploads/cc-button-1-light-2048x436.png 2048w" sizes="(max-width: 2560px) 100vw, 2560px"></picture></figure>
<p>Now, of course, we could use a second variable for the text color and carefully define the values for <code>--button-color</code> and <code>--button-text-color</code> at the same time, in pairs, to ensure the choice for the text color is the right one. But, on a large project, with a large team, carefully managing such details can become a really hard task to get right. Suddenly a dark button has unreadable black text, and users can’t figure out what to do.</p>
<p>It’d be easier if we could just tell our CSS to make the text black/white, and have the browser pick which to use — whichever one provides more contrast with a specific color. Then we could just manage our many background colors, and not worry about the text color.</p>
<p>That’s exactly what the <code>contrast-color()</code> function will let us do.</p>
<h2>contrast-color()</h2>
<p>We can write this in our CSS:</p>
<pre><code><span>color</span>: <span>contrast-color</span>(<span>purple</span>);
</code></pre>
<p>And the browser will set <code>color</code> to either black or white, whichever choice provides better contrast with <code>purple</code>.</p>
<p>Let’s style our button. We’ll set the button background color to our variable. And we’ll define the text color to be the contrasting black/white choice that pairs with that variable.</p>
<pre><code><span>button</span> {
  <span>background-color</span>: <span>var</span>(<span>--button-color</span>);
  <span>color</span>: <span>contrast-color</span>(<span>var</span>(<span>--button-color</span>));
}
</code></pre>
<p>Now we only need to define one color, and the other follows! When we change the button color, the browser will reconsider whether the text should be black or white, and choose fresh the option with more contrast.</p>
<p>For fun, let’s also define a hover color using <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_colors/Relative_colors">Relative Color Syntax</a>, and now one variable determines four colors — the default button color &amp; the text to go with it, plus the hover color &amp; the text to go with that.</p>
<pre><code><span>:root</span> {
  <span>--button-color</span>: <span>purple</span>;
  <span>--hover-color</span>: <span>oklch</span>(<span>from</span> <span>var</span>(<span>--button-color</span>) <span>calc</span>(<span>l</span> + <span>.2</span>) <span>c</span> <span>h</span>);
}
<span>button</span> {
  <span>background-color</span>: <span>var</span>(<span>--button-color</span>);
  <span>color</span>: <span>contrast-color</span>(<span>var</span>(<span>--button-color</span>));
  <span>text-box</span>: <span>cap</span> <span>alphabetic</span>; <span>/* vertically centers the text */</span>
}
<span>button</span><span>:hover</span> {
  <span>background-color</span>: <span>var</span>(<span>--hover-color</span>);
  <span>color</span>: <span>contrast-color</span>(<span>var</span>(<span>--hover-color</span>));
}
</code></pre>
<p><a href="https://codepen.io/jensimmons/pen/XJJjKMO?editors=1100">Here’s a demo of the result</a>. Try it in <a href="https://developer.apple.com/safari/technology-preview/">Safari Technology Preview</a>, where you can change the button color dynamically.</p>

<h2>Accessibility considerations and contrast algorithms</h2>
<p>Now, it might be tempting to believe that <code>contrast-color()</code> will magically solve all contrast accessibility concerns all by itself, and your team will never have to think about color contrast again. Nope, that’s not the case. At all.</p>
<p>Using the <code>contrast-color()</code> function does not guarantee that the resulting pair of colors will be accessible. It’s quite possible to pick a color (in this case a background color) that will not have enough contrast with either black or white. It’s still up to the humans involved — designers, developers, testers, and more — to ensure there’s enough contrast.</p>
<p>In fact, if you try out <a href="https://codepen.io/jensimmons/pen/XJJjKMO?editors=1100">our demo</a> in Safari Technology Preview now (as this article is published in May 2025), you’ll find many of the pairings with mid-tone background colors don’t result in enough contrast. It often seems like the wrong choice is being made. For example, this #317CFF blue returns a contrast-color of black.</p>
<figure><picture><source src="https://webkit.org/wp-content/uploads/cc-button-2-dark-scaled.png" type="image/png" media="(prefers-color-scheme: dark)"><img decoding="async" src="https://webkit.org/wp-content/uploads/cc-button-2-light-scaled.png" alt="Medium dark blue button with black text. The text is hard to see." width="2560" height="545" srcset="https://webkit.org/wp-content/uploads/cc-button-2-light-scaled.png 2560w, https://webkit.org/wp-content/uploads/cc-button-2-light-300x64.png 300w, https://webkit.org/wp-content/uploads/cc-button-2-light-1024x218.png 1024w, https://webkit.org/wp-content/uploads/cc-button-2-light-768x164.png 768w, https://webkit.org/wp-content/uploads/cc-button-2-light-1536x327.png 1536w, https://webkit.org/wp-content/uploads/cc-button-2-light-2048x436.png 2048w" sizes="(max-width: 2560px) 100vw, 2560px">
</picture></figure>
<p>When white is clearly the better choice for perceptual contrast.</p>
<figure><picture><source src="https://webkit.org/wp-content/uploads/cc-button-3-dark-scaled.png" type="image/png" media="(prefers-color-scheme: dark)"><img decoding="async" src="https://webkit.org/wp-content/uploads/cc-button-3-light-scaled.png" alt="Same dark medium blue button, now with white text. Much easier to see what it says." width="2560" height="545" srcset="https://webkit.org/wp-content/uploads/cc-button-3-light-scaled.png 2560w, https://webkit.org/wp-content/uploads/cc-button-3-light-300x64.png 300w, https://webkit.org/wp-content/uploads/cc-button-3-light-1024x218.png 1024w, https://webkit.org/wp-content/uploads/cc-button-3-light-768x164.png 768w, https://webkit.org/wp-content/uploads/cc-button-3-light-1536x327.png 1536w, https://webkit.org/wp-content/uploads/cc-button-3-light-2048x436.png 2048w" sizes="(max-width: 2560px) 100vw, 2560px"></picture></figure>
<p>What is happening here? Why is the less-contrasting choice being made?</p>
<p>Well, the current implementation in Safari Technology Preview is using the contrast algorithm officially defined in <a href="https://www.w3.org/WAI/standards-guidelines/wcag/">WCAG 2</a> (Web Content Accessibility Guidelines version 2). If we put this color blue through a well-respected <a href="https://webaim.org/resources/contrastchecker/">color contrast checker at WebAIM</a>, it does clearly recommend using black for the text color, not white. WCAG 2 is the current authoritative standard for accessibility on the web, required by law in many places.</p>
<p>The WCAG 2 algorithm calculates black-on-#317CFF as having a contrast ratio of 5.45:1, while white-on-#317CFF has 3.84:1. The <code>contrast-color()</code> function is simply choosing the option with the bigger number — and 5.45 is bigger than 3.84.</p>
<figure><img loading="lazy" decoding="async" src="https://webkit.org/wp-content/uploads/WCAG2-test-blue-scaled.png" alt="Screenshots of the WCAG 2 color contrast checker, showing results of white on blue and black on blue. Black passes. White fails. But black is hard to read while white is easy to read." width="2560" height="1361" srcset="https://webkit.org/wp-content/uploads/WCAG2-test-blue-scaled.png 2560w, https://webkit.org/wp-content/uploads/WCAG2-test-blue-300x160.png 300w, https://webkit.org/wp-content/uploads/WCAG2-test-blue-1024x545.png 1024w, https://webkit.org/wp-content/uploads/WCAG2-test-blue-768x408.png 768w, https://webkit.org/wp-content/uploads/WCAG2-test-blue-1536x817.png 1536w, https://webkit.org/wp-content/uploads/WCAG2-test-blue-2048x1089.png 2048w" sizes="auto, (max-width: 2560px) 100vw, 2560px"><figcaption>Testing black versus white on a medium-dark blue in the <a href="https://webaim.org/resources/contrastchecker/">WCAG 2 color contrast checker at Web AIM</a>.</figcaption></figure>
<p>When machines run the WCAG 2 algorithm, the black text has higher contrast mathematically. But when humans look at these combinations, the black text has lower contrast perceptually. If you find this odd, well, you aren’t the only one. The WCAG 2 color contrast algorithm has long been a subject of criticism. In fact, one of the major driving forces for updating WCAG to level 3 is a desire to improve the contrast algorithm.</p>
<p>The <a href="https://github.com/Myndex/SAPC-APCA">Accessible Perceptual Contrast Algorithm (APCA)</a> is one possible candidate for inclusion in <a href="https://www.w3.org/WAI/standards-guidelines/wcag/wcag3-intro/">WCAG 3</a>. You can try out this algorithm today by using the APCA Contrast Calculator at <a href="https://apcacontrast.com/">apcacontrast.com</a>. Let’s look at what it thinks about black vs white text on this particular shade of blue background.</p>
<figure><img loading="lazy" decoding="async" src="https://webkit.org/wp-content/uploads/APCA-test-blue.png" alt="Screenshot of APCA Contrast Calculator, showing the same tests of black on blue vs white on blue. White clearly wins." width="1924" height="2442" srcset="https://webkit.org/wp-content/uploads/APCA-test-blue.png 1924w, https://webkit.org/wp-content/uploads/APCA-test-blue-236x300.png 236w, https://webkit.org/wp-content/uploads/APCA-test-blue-807x1024.png 807w, https://webkit.org/wp-content/uploads/APCA-test-blue-768x975.png 768w, https://webkit.org/wp-content/uploads/APCA-test-blue-1210x1536.png 1210w, https://webkit.org/wp-content/uploads/APCA-test-blue-1614x2048.png 1614w" sizes="auto, (max-width: 1924px) 100vw, 1924px"><figcaption>Testing the same black versus white on a medium-dark blue in the <a href="https://apcacontrast.com/">APCA Contrast Calculator</a>.</figcaption></figure>
<p>This contrast algorithm evaluates black-on-blue as having a score of Lc 38.7, while white-on-blue scores Lc -70.9. To know which has more contrast, ignore the negative sign for a moment, and compare 38.7 to 70.9. The bigger the number, the more contrast. The APCA test results say that white text is clearly better than black. Which feels exactly right.</p>
<p>(In the APCA scoring system, the negative number simply signifies that the text is lighter than the background. Think light mode = positive numbers, dark mode = negative numbers.)</p>
<p>Why is APCA giving such better results than WCAG 2? Because its algorithm calculates contrast perceptually instead of with simple mathematics. This takes into consideration the fact humans do not perceive contrast linearly across hue and lightness. If you’ve learned about LCH vs HSL color models, you’ve probably heard about how newer approaches to color mathematics do a better job of understanding our perception of lightness, and knowing which colors seem to be the same luminance or tone. The “Lc” marking the APCA score stands for “Lightness contrast”, as in “Lc 75”.</p>
<p>Luckily, the algorithm behind the <code>contrast-color</code> function can be swapped out. Support for this feature first shipped in March 2021, in <a href="https://webkit.org/blog/11577/release-notes-for-safari-technology-preview-122/">Safari Technology Preview 122</a>. (Also, at that time it was named <code>color-contrast</code>.) Back then, it was too early to choose a better algorithm.</p>
<p>The CSS standard still calls for browsers to use the older algorithm, but contains a note about the future: “Currently only WCAG 2.1 is supported, however this algorithm is known to have problems, particularly on dark backgrounds. Future revisions of this module will likely introduce additional contrast algorithms.” Debates over which algorithm is best for WCAG 3 are still ongoing, including discussion of licensing of the algorithms under consideration.</p>
<p>Meanwhile, your team should still take great care in choosing color palettes, keeping accessibility in mind. If you are choosing clearly-light or clearly-dark colors for the contrasting color,  <code>contrast-color()</code> will work great even when backed by the WCAG 2 algorithm. It’s in evaluating contrast with mid-tones where the algorithms start to differ in their results.</p>
<p>Plus, the <code>contrast-color()</code> function alone will never guarantee accessibility, even when updated with a better algorithm. “This one has <em>more</em> contrast” is not the same thing as “this one has <em>enough</em> contrast”. There are plenty of colors that never have enough contrast with either black or white, especially at smaller text sizes or thinner font weights.</p>
<h2>Providing enough contrast in the real world</h2>
<p>While thinking about color contrast, we should remember another tool in our arsenal to ensure we provide good contrast for everyone — the<a href="https://developer.mozilla.org/en-US/docs/Web/CSS/@media/prefers-contrast"></a><a href="https://developer.mozilla.org/en-US/docs/Web/CSS/@media/prefers-color-scheme"><code>prefers-contrast</code></a> media query. It lets us offer alternative styling to those who want more contrast.</p>
<pre><code><span>@media</span> (<span>prefers-contrast</span>: <span>more</span>) {
  <span>/* styling with more contrast */</span>
}
</code></pre>
<p>Let’s think through how to use these tools in a real world situation. Imagine we are creating a website for a tree nursery whose main brand color is a particular shade of bright medium green. Our design team really wants to use #2DAD4E as the main button background.</p>
<p>To keep things simple, let’s also pretend we live in a future when the APCA algorithm has replaced the WCAG 2 algorithm in CSS. This change will mean <code>contrast-color()</code> will return white for our text color against this medium green, not black.</p>
<p>But looking up <a href="https://apcacontrast.com/?BG=2dad4e&amp;TXT=ffffff&amp;DEV=G4g&amp;BUF=A22">this color combination</a>, we see there might not be enough contrast for some users, especially if the text is small. This is where good design is important.</p>
<p><img loading="lazy" decoding="async" src="https://webkit.org/wp-content/uploads/APCA-text-green-scaled.png" alt="Testing white on medium green in the APCA contrast calculator. The interface has lots of options for adjusting the colors. And it's got a panel across the bottom with six sections of examples of white text on this color green, in various sizes and weights of fonts. " width="2560" height="1341" srcset="https://webkit.org/wp-content/uploads/APCA-text-green-scaled.png 2560w, https://webkit.org/wp-content/uploads/APCA-text-green-300x157.png 300w, https://webkit.org/wp-content/uploads/APCA-text-green-1024x536.png 1024w, https://webkit.org/wp-content/uploads/APCA-text-green-768x402.png 768w, https://webkit.org/wp-content/uploads/APCA-text-green-1536x805.png 1536w, https://webkit.org/wp-content/uploads/APCA-text-green-2048x1073.png 2048w" sizes="auto, (max-width: 2560px) 100vw, 2560px"></p>
<p>When using this shade of green as the background for white text, the APCA score is Lc -60.4.</p>
<p>You might remember that WCAG 2 evaluates contrast with a ratio (like “2.9:1”). However, APCA scores are a single number, ranging from Lc -108 to 106. Whether or not Lc -60.4 has enough contrast depends on how big the text is — and, new in APCA, how thick the font weight is.</p>
<p>There’s information about what’s considered a good target for Bronze, Silver, and Gold level conformance in the <a href="https://readtech.org/ARC/tests/visual-readability-contrast/?tn=criterion">APCA Readability Criterion</a>. These recommendations can really help guide designers to select the size and weight of text to ensure enough contrast, while allowing a range of beautiful color combinations. In fact, the WCAG 3 itself is being designed to provide flexible guidance to help you understand how to support all users, rather than binary judgments the way WCAG 2 does. Good accessibility isn’t about simply meeting a magical metric to check off a box on a list. It’s about understanding what works for real people, and designing for them. And what people need is complex, not binary.</p>
<p>You’ll notice that this particular <a href="https://apcacontrast.com/">APCA Contrast Calculator</a> not only provides a score, but also evaluates the success of dynamic examples showing combinations of font size and font weight. In our case, for “Usage” it says “fluent text okay”. (For the black on blue example above, it instead says “Usage: spot &amp; non text only”.) The Calculator is showing that white text on #2DAD4E works at 24px text if the font weight is 400 or bolder. If we want to use a font-weight of 300, then the text should be at least 41px. Of course, this will depend on which font-face we use, and we aren’t using the same font as that Contrast Calculator does, but there’s far more nuance in this guidance than tools for the WCAG 2 algorithm. And it helps our team come up with a plan for a beautiful design.</p>
<p>Our tree nursery website supports both light and dark mode, and our designers determined that #2DAD4E works as a button color for both light and dark mode for many users, as long as they carefully designed our buttons considering how font size and weight impacts contrast. But even with those considerations, Lc -60.4 is not quite enough contrast for all users, so for anyone who has set their accessibility preferences to ask for more contrast, we’ll replace the button background color with two options — a darker #3B873E green for light mode (with white text, scoring Lc -76.1), and a lighter #77e077 green for dark mode (with black text, scoring Lc 75.2).</p>
<p>Here’s the color palette our fictional design team wants us to accomplish in CSS:</p>
<p><img loading="lazy" decoding="async" src="https://webkit.org/wp-content/uploads/green-design-2-scaled.png" alt="A diagram of our color palette, explaining when to use which color combination. (All information is also articulated in the text of this article.)" width="2560" height="1436" srcset="https://webkit.org/wp-content/uploads/green-design-2-scaled.png 2560w, https://webkit.org/wp-content/uploads/green-design-2-300x168.png 300w, https://webkit.org/wp-content/uploads/green-design-2-1024x575.png 1024w, https://webkit.org/wp-content/uploads/green-design-2-768x431.png 768w, https://webkit.org/wp-content/uploads/green-design-2-1536x862.png 1536w, https://webkit.org/wp-content/uploads/green-design-2-2048x1149.png 2048w" sizes="auto, (max-width: 2560px) 100vw, 2560px"></p>
<p>When we define colors in variables, it’s incredibly easy to swap out color values for these various conditions. And by using <code>contrast-color()</code>, we only need to worry about the background colors, not the text color pairings. We’ll make the browser do the work, and get the paired colors for free.</p>
<p>To accomplish all of these things at once, we can just write this code (because, remember, we are pretending to live in a future when a better algorithm has replaced the WCAG 2 algorithm in CSS):</p>
<pre><code><span>--button-color</span>: #2<span>DAD4E</span>;  <span>/* brand green background */</span> 

<span>@media</span> (<span>prefers-contrast</span>: <span>more</span>) {
  @<span>media</span> (<span>prefers-color-scheme</span>: <span>light</span>) {
    <span>--button-color</span>: <span>#419543</span>;  <span>/* darker green background */</span>
  }
  <span>@media</span> (<span>prefers-color-scheme</span>: <span>dark</span>) {
    <span>--button-color</span>: <span>#77CA8B</span>;  <span>/* lighter green background */</span>
  }
}

<span>button</span> {
  <span>background-color</span>: <span>var</span>(<span>--button-color</span>);
  <span>color</span>: <span>contrast-color</span>(<span>var</span>(<span>--button-color</span>));
  <span>font-size</span>: <span>1.5</span><span>rem</span>;  <span>/* 1.5 * 16 = 24px at normal zoom */</span>
  <span>font-weight</span>: <span>500</span>;
}
</code></pre>
<p>In reality, since the WCAG 2 algorithm is the one driving <code>contrast-color()</code>, we probably couldn’t use it on this website. But if we had another project where the brand color was a darker green, and the choice between white/black was the correct one, it could be quite helpful today.</p>
<p>Using <code>contrast-color()</code> is especially helpful when defining colors for multiple states or options like enabled/disabled, light/dark mode, prefers-contrast, and more.</p>
<h2>Beyond black &amp; white</h2>
<p>You might be wondering, “but what if I want the browser to choose a color beyond just black/white?” If you read about or tried out our original implementation in Safari Technology Preview 122 four years ago, you might remember that the original feature did much more. The newer <code>contrast-color()</code> function is greatly simplified from the original <code>color-contrast()</code>.</p>
<p>Because a decision on which color-contrast algorithm to use for WCAG 3 is still being debated, the CSS Working Group decided to move forward with a tool that simply chooses black or white to contrast with the first color. Keeping it simple makes it possible to swap out the algorithm later. By hardcoding the list of options to be black/white, websites are far less likely to break when the WCAG 2 algorithm is replaced, giving the CSSWG the flexibility it needs to keep making needed changes, even as <code>contrast-color</code> ships into the hands of users.</p>
<p>In the future, more complex tools will come along to support more powerful options. Perhaps you’ll be able to list a set of custom color options and have the browser pick from those, instead of picking from black/white. Perhaps you’ll list a set of options, plus specify a contrast level that you want the browser to aim for, instead of having it picking the choice that yields maximum contrast.</p>
<p>In the meantime, often a simple choice between black and white is all you need. We wanted to get the simple version into your hands sooner, rather than waiting for a process that will take years.</p>
<p>And while all of the examples above show black/white text on a color background, <code>contrast-color</code> can be used for much more. You can use a custom color for your text, and make the background be black/white. Or not involve text at all, and define colors for borders, background — anything. There’s a lot you can do.</p>
<h2>Continue the conversation</h2>
<p>You can learn more about the APCA (Accessible Perceptual Contrast Algorithm) by reading documentation from the folks <a href="https://github.com/Myndex/SAPC-APCA/?tab=readme-ov-file">creating it</a>. Including:</p>
<ul>
<li><a href="https://git.apcacontrast.com/documentation/APCAeasyIntro">The Easy Intro to the APCA Contrast Method</a> — a plain-language introduction to perceptually uniform contrast</li>
<li><a href="https://readtech.org/ARC/tests/bronze-simple-mode/?tn=criterion">Bronze Simple Mode</a> — a “most basic” design guideline, intended for users migrating from WCAG 2 contrast</li>
</ul>
<p>We’d love to hear your thoughts about <code>contrast-color()</code>. Your feedback on this tool can help shape its future. You can find me, Jen Simmons, on <a href="https://bsky.app/profile/jensimmons.bsky.social">Bluesky</a> / <a href="https://front-end.social/@jensimmons">Mastodon</a>. Or follow our other web evangelists —  Saron Yitbarek on <a href="https://bsky.app/profile/saron.bsky.social">BlueSky</a>, and Jon Davis on <a href="https://bsky.app/profile/jondavis.bsky.social">Bluesky</a> / <a href="https://mastodon.social/@jondavis">Mastodon</a>. You can also follow WebKit <a href="https://www.linkedin.com/in/apple-webkit/">on LinkedIn</a>.</p>

                            </div>
        </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[If nothing is curated, how do we find things (204 pts)]]></title>
            <link>https://tadaima.bearblog.dev/if-nothing-is-curated-how-do-we-find-things/</link>
            <guid>44015144</guid>
            <pubDate>Sat, 17 May 2025 15:51:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tadaima.bearblog.dev/if-nothing-is-curated-how-do-we-find-things/">https://tadaima.bearblog.dev/if-nothing-is-curated-how-do-we-find-things/</a>, See on <a href="https://news.ycombinator.com/item?id=44015144">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    

    
        
    

    
        

        <p>
            <i>
                <time datetime="2025-05-15T16:55Z">
                    May 15, 2025
                </time>
            </i>
        </p>
    

    <p>Bjork is currently promoting a new concert film being released called <em>Cornucopia</em>. She's been releasing new photoshoots and interviews almost every day for the past two weeks. For a musician who normally goes into hiding and only emerges when it's time to promote something, it's been a pretty exciting time to be a fan. However, all the information being put out, promoted on social media and reposted on places like Reddit, have all been a little confusing.</p>
<p>I saw a post from someone on Reddit asking to "explain to me like I'm five" what exactly is being released and what is it. Someone did reply to them with information, but the comment thread turned into a short argument as another commenter disagreed that the film would have a documentary attached to it. "Where did you get that info from?" a commenter asked. "I think I saw it somewhere in another interview that I translated into English," they replied, "but I might have gotten that wrong."</p>
<p>While reading this I thought, "It's times like these where an old-fashioned website would come in handy." Because, quite frankly, it would.<sup id="fnref-1"><a href="#fn-1">1</a></sup> As convenient as social media is, it scatters the information like bread being fed to ducks. You then have to hunt around for the info or hope the magical algorithm gods read your mind and guide the information to you.</p>
<p>I always felt like social media creates an illusion of convenience. Think of how much time it takes to stay on top of things. To stay on top of music or film. Think of how much time it takes these days, how much hunting you have to do. Although technology has made information vast and reachable, it's also turned the entire internet into a sludge pile. And now, instead of relying on professional curators to sort through things for us, now <em>we</em> have to do the sorting.</p>
<p>Think of the old days. When I was a kid, I lived in a podunk, suburban town in the middle of nowhere. It wasn't a major city or even a major state, but I was able to quite easily stay on top of everything pop culture related, even things that weren't mainstream or even super popular in my country.</p>
<p>I discovered interesting music like Aphex Twin, Squarepusher, Portishead, Tricky, Orbital, Takako Minekawa, Hooverphonic, Poe, Veruca Salt all from sporadically listening to one college radio station in my hometown and, once a week, watching one music program on MTV (usually <em>120 Minutes</em> or <em>AMP</em>). Then, once a month, I would sometimes flip through a music magazine while at the hair salon (usually <em>Rolling Stone</em> or <em>Spin</em>). And that was literally it.</p>
<p>Same with movies. Once a week I would watch <em>Ebert and Roeper</em>, who would discuss and review all the releases of that week, including indie and foreign ones. I would also sometimes flip through film magazines or randomly stumble across something cool being aired on the IFC channel or Bravo<sup id="fnref-2"><a href="#fn-2">2</a></sup>. That was how I discovered indie films like <em>Welcome to the Dollhouse</em> and <em>Run, Lola, Run</em>. I was a "cool," knowledgeable cultural teen with next to no Internet access.</p>
<p>The rise of social media has killed the art of curation because, these days, things are rarely curated. Criticism is dead (with Fantano<sup id="fnref-3"><a href="#fn-3">3</a></sup> being the one exception) and Gen Alpha doesn't know how to find music through anything but TikTok. Relying on algorithms puts way too much power in technology's hands. And algorithms can only predict content that you've seen before. It'll never surprise you with something different. It keeps you in a little bubble. Oh, you like shoegaze? Well, that's all the algorithm is going to give you until you intentionally start listening to something else.</p>
<p>It makes art (music, film, tv, etc.) seem like one big sludge pile. It makes it feel vast and exhausting, like an endless list of things that you'll never get to the end of. I've been noticing this sentiment with society, this feeling of always being mentally exhausted. How many times have we had a discussion with a friend who was recommending a show and our response was, "Oh yeah, I'll have to see it, but my list of shows is so long!" The reality is we're not going to watch it because we feel like we have no time to get through everything and we don't fully trust other people's recommendations.</p>
<p>And that's where curation comes in. We need critics who devote their lives to browsing through the pile and telling us what is worth our time and what isn't.</p>
<p>There are still some critics out there (Vulture, Pitchfork), but these sites are dying. They publish dozens of articles a day, trying to get as many clicks and pageviews as possible, adding to the volume of content we view daily. Before, you could reach for a magazine once a month or a watch a show once a week, but now you have to browse Vulture every day and read all 20+ articles they publish, even on the weekends. Who has time to read all that? Who has the time for any of this? Technology is making our lives harder, not easier.</p>
<p>So I guess the next question is, "How do I fix this?" Like most people, I've been pulling back. Less time relying on algorithms to predict what I like and more time just making notes and lists in Obsidian. Any time I stumble across something that looks interesting or something I don't want to forget, I make a note of it so I can retrieve it later.</p>
<p>It's honestly not much of a solution as it still makes "staying on top of things" feel like a job. But I'm struggling to find a better way to wrap up this post. This might just be society's new normal. The ones who prioritize comfort will stay in their algorithmic bubbles, while those who care about broadening their horizons will prioritize finding things on their own. Search long enough and eventually you <em>will</em> find what you're looking for. Eventually.</p>
<section>
<ol>
<li id="fn-1"><p>There actually is a <a href="https://bjorkcornucopia.com/">website now</a>, but when I wrote this a month ago there wasn't. Oh well. 🤷‍♀️<a href="#fnref-1">↩</a></p></li>
<li id="fn-2"><p>Pre-<em>Real Housewives</em> Bravo was a <em>very</em> different channel.<a href="#fnref-2">↩</a></p></li>
<li id="fn-3"><p>Uproxx recently had an <a href="https://uproxx.com/music/anthony-fantano-interview-music-criticism-needle-drop/">interesting convo</a> with him about this exact topic.<a href="#fnref-3">↩</a></p></li>
</ol>
</section>


    

    
        
            <p>
                
                    <a href="https://tadaima.bearblog.dev/blog/?q=internet">#internet</a>
                
                    <a href="https://tadaima.bearblog.dev/blog/?q=music">#music</a>
                
            </p>
        

        
            


        
    


  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA["We would be less confidential than Google" Proton threatens to quit Switzerland (357 pts)]]></title>
            <link>https://www.techradar.com/vpn/vpn-privacy-security/we-would-be-less-confidential-than-google-proton-threatens-to-quit-switzerland-over-new-surveillance-law</link>
            <guid>44014808</guid>
            <pubDate>Sat, 17 May 2025 14:59:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.techradar.com/vpn/vpn-privacy-security/we-would-be-less-confidential-than-google-proton-threatens-to-quit-switzerland-over-new-surveillance-law">https://www.techradar.com/vpn/vpn-privacy-security/we-would-be-less-confidential-than-google-proton-threatens-to-quit-switzerland-over-new-surveillance-law</a>, See on <a href="https://news.ycombinator.com/item?id=44014808">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">
<section>
<div>
<div>
<picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-1920-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R.jpg" alt="Proton CEO and founder Andy Yen poses next to the Proton logo at the headquarters of the encrypted email and VPN services company in Geneva." srcset="https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-1920-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-320-80.jpg 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R.jpg" data-pin-nopin="true" fetchpriority="high">
</picture>
</div>
<figcaption>
<span>(Image credit: Photo by FABRICE COFFRINI/AFP via Getty Images)</span>
</figcaption>
</div>

<div id="article-body">
<hr><ul><li><strong>Proton CEO confirmed the company will leave Switzerland if new controversial surveillance rules pass</strong></li><li><strong>An amendment to the current surveillance law would require VPNs and messaging apps to identify and retain user data</strong></li><li><strong>Another Swiss company, NymVPN, is also ready to leave the country instead of undermining its privacy and security infrastructure</strong></li></ul><hr><p>Proton confirms the company will leave Switzerland if new controversial surveillance rules pass.</p><p>Switzerland is <a data-analytics-id="inline-link" href="https://www.techradar.com/vpn/vpn-privacy-security/secure-encryption-and-online-anonymity-are-now-at-risk-in-switzerland-heres-what-you-need-to-know" data-before-rewrite-localise="https://www.techradar.com/vpn/vpn-privacy-security/secure-encryption-and-online-anonymity-are-now-at-risk-in-switzerland-heres-what-you-need-to-know">considering amending its surveillance law</a>, with experts warning against the risk to secure encryption and online anonymity in the country. Specifically, the amendment could require all VPN services, messaging apps, and social networks to identify and retain user data – an obligation that is now limited to mobile networks and internet service providers.</p><p>The firm behind one of the <a data-analytics-id="inline-link" href="https://www.techradar.com/vpn/best-vpn" data-before-rewrite-localise="https://www.techradar.com/vpn/best-vpn">best VPN</a> and encrypted email services, Proton, is ready to fight back on behalf of the privacy of its over 100 million users. Other Swiss-based companies, like <a data-analytics-id="inline-link" href="https://www.techradar.com/pro/vpn/nymvpn" data-before-rewrite-localise="https://www.techradar.com/pro/vpn/nymvpn">NymVPN</a>, are also doing the same.</p><h2 id="no-choice-but-to-leave-3">No choice but to leave</h2><p>In an <a data-analytics-id="inline-link" href="https://www.rts.ch/info/suisse/2025/article/proton-menace-de-quitter-la-suisse-face-aux-nouvelles-regles-de-surveillance-28883036.html" target="_blank" data-url="https://www.rts.ch/info/suisse/2025/article/proton-menace-de-quitter-la-suisse-face-aux-nouvelles-regles-de-surveillance-28883036.html" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">interview with RTS</a> (Radio Télévision Suisse) on May 13, 2025, Proton CEO Andy Yen slammed the proposed amendment as a "major violation of the right to privacy" that will also harm the country's reputation and its ability to compete on an international level.</p><p>"This revision attempts to implement something that has been deemed illegal in the EU and the United States. The only country in Europe with a roughly equivalent law is Russia," said Yen.</p><p><a data-analytics-id="inline-link" href="https://www.news.admin.ch/fr/nsb?id=103968" target="_blank" data-url="https://www.news.admin.ch/fr/nsb?id=103968" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">The amendment</a> aims to expand the number of service providers targeted to include so-called "derived service providers". Crucially, the new provisions will introduce three new types of information and two types of monitoring.</p><p>If the changes pass, Proton will be forced to modify how <a data-analytics-id="inline-link" href="https://www.techradar.com/reviews/protonmail-secure-email" data-before-rewrite-localise="https://www.techradar.com/reviews/protonmail-secure-email">Proton Mail</a> and <a data-analytics-id="inline-link" href="https://www.techradar.com/reviews/protonvpn" data-before-rewrite-localise="https://www.techradar.com/reviews/protonvpn">Proton VPN</a> handle encryption, alongside its strict no-log policies – something the company isn't willing to do.</p><p>"I think we would have no choice but to leave Switzerland," said Yen. "The law would become almost identical to the one in force today in Russia. It's an untenable situation. We would be less confidential as a company in Switzerland than Google, based in the United States. So it's impossible for our business model."</p><div><blockquote data-lang="en"><p lang="en" dir="ltr">In Switzerland, the new version of the surveillance law aims to make it impossible for Proton, Threema and@nymproject to operate from Switzerland. We are in the consultation phase. We will fight. https://t.co/BcMBxzIPFC<a href="https://twitter.com/cantworkitout/status/1904483355812377045" data-url="https://twitter.com/cantworkitout/status/1904483355812377045" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">March 25, 2025</a></p></blockquote></div><p>Proton is not alone in feeling this way, though.</p><p>A new player in the VPN world, <a data-analytics-id="inline-link" href="https://www.techradar.com/vpn/vpn-services/nymvpn-is-now-live-heres-everything-you-need-to-know" data-before-rewrite-localise="https://www.techradar.com/vpn/vpn-services/nymvpn-is-now-live-heres-everything-you-need-to-know">NymVPN</a> has also been publicly fighting Swiss government plans since the beginning.</p><p>Talking to TechRadar, Nym's co-founder and COO, Alexis Roussel, confirmed that Nym will do the same and leave Switzerland if the new surveillance rules are enforced.</p><h2 id="what-s-next-3">What's next?</h2><p>As public consultations ended on May 6, 2025, we will now have to wait and see what the Swiss government decides.</p><p>Nonetheless, Roussel confirmed to TechRadar that there has been significant push-back from political parties and Swiss companies.</p><p>Some Cantons, <a data-analytics-id="inline-link" href="https://www.ge.ch/document/39174/telecharger" target="_blank" data-url="https://www.ge.ch/document/39174/telecharger" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">including Geneva</a>, have even called on the right to digital integrity as an argument against these rules. Roussel was the main originator of the initiative that <a data-analytics-id="inline-link" href="https://www.swissinfo.ch/eng/democracy/how-swiss-federalism-is-helping-the-rise-of-a-new-digital-right/89023201" target="_blank" data-url="https://www.swissinfo.ch/eng/democracy/how-swiss-federalism-is-helping-the-rise-of-a-new-digital-right/89023201" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">introduced this new right</a> to protect citizens' online privacy and data – in Geneva in 2023 and Neuchâtel in 2024 – with over 90% consensus.</p><p>Yen also told RTS to be more optimistic, despite pointing out how this matter shows the need for a more balanced approach when it comes to crafting new laws.</p><p>"If we can get Bern to adopt common-sense rules that allow companies like Proton to be competitive in Switzerland and around the world, I will stay, take my passport, and continue to invest in Switzerland," he added.</p><h3 id="section-you-might-also-like"><span>You might also like</span></h3><ul><li><a href="https://www.techradar.com/vpn/vpn-services/once-you-have-the-data-you-have-to-cooperate-windscribe-ceo-speak-out-against-global-threats-to-no-log-vpns" data-before-rewrite-localise="https://www.techradar.com/vpn/vpn-services/once-you-have-the-data-you-have-to-cooperate-windscribe-ceo-speak-out-against-global-threats-to-no-log-vpns">Windscribe CEO speaks out against global threats to no-log VPNs</a></li><li><a href="https://www.techradar.com/vpn/vpn-privacy-security/encryption-backdoors-privacy-can-be-misused-but-the-cost-of-a-world-without-is-so-much-higher" data-before-rewrite-localise="https://www.techradar.com/vpn/vpn-privacy-security/encryption-backdoors-privacy-can-be-misused-but-the-cost-of-a-world-without-is-so-much-higher">Encryption backdoors: privacy can be misused, "but the cost of a world without is so much higher"</a></li><li><a href="https://www.techradar.com/vpn/vpn-privacy-security/a-win-for-privacy-florida-rejects-the-encryption-backdoor-law-for-social-media" data-before-rewrite-localise="https://www.techradar.com/vpn/vpn-privacy-security/a-win-for-privacy-florida-rejects-the-encryption-backdoor-law-for-social-media">"A win for privacy" – Florida rejects the encryption backdoor law for social media</a></li></ul>
</div>

<div id="slice-container-authorBio-QWbBN83AJHY8NHNgbHUYZL"><p>Chiara is a multimedia journalist committed to covering stories to help promote the rights and denounce the abuses of the digital side of life – wherever cybersecurity, markets, and politics tangle up. She believes an open, uncensored, and private internet is a basic human need and wants to use her knowledge of VPNs to help readers take back control. She writes news, interviews, and analysis on data privacy, online censorship, digital rights, tech policies, and security software, with a special focus on VPNs, for TechRadar and TechRadar Pro. Got a story, tip-off, or something tech-interesting to say? Reach out to chiara.castro@futurenet.com</p></div>
</section>




</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Palette lighting tricks on the Nintendo 64 (187 pts)]]></title>
            <link>https://30fps.net/pages/palette-lighting-tricks-n64/</link>
            <guid>44014587</guid>
            <pubDate>Sat, 17 May 2025 14:28:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://30fps.net/pages/palette-lighting-tricks-n64/">https://30fps.net/pages/palette-lighting-tricks-n64/</a>, See on <a href="https://news.ycombinator.com/item?id=44014587">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="text">

<p><em>This article is a continuation to <a href="https://bsky.app/profile/pekkavaa.bsky.social/post/3lnnwax4vxk2v">my Bluesky thread</a> from April.</em></p>
<!-- ![](castello_screenshot.jpg) -->
<p>We made a Nintendo 64 demo for <a href="https://2025.revision-party.net/">Revision 2025</a>!</p>
<center>
<iframe width="100%" src="https://www.youtube.com/embed/v3wYV6gxJII" title="Real-time tech demo Castello (N64)" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
</iframe>
</center>
<p>It has baked lighting with normal mapping and real-time specular shading, ahem, well sort of.
More on that later.
The beautiful song was made by <a href="https://bsky.app/profile/did:plc:svoizwy5mp6q4ol6j2pu74we">noby</a> with guitar performed by Moloko (<a href="https://soundcloud.com/sou_andrade">https://soundcloud.com/sou_andrade</a>).</p>
<p>Below I have some notes on the directional ambient and normal mapping techniques I developed.
They are both pretty simple in the end but I haven’t seen them used elsewhere.</p>
<h2 id="but-wait-normal-mapping-on-the-n64">But wait, normal mapping on the N64?</h2>
<p>I knew normal mapping on the N64 was possible due to earlier experiments by fellow homebrew developers WadeTyhon and <a href="https://www.youtube.com/@SpookyIluha">Spooky Iluha</a>. I had also done <a href="https://www.youtube.com/watch?v=UOHdDllyqOU">some emboss bump mapping hacks</a> myself.</p>
<p>The approach explained in this article is not new: <strong>the renderer computes lighting directly to textures at runtime</strong>.
It’s great because no specialized hardware support is needed and you can run arbitrary shading code on the CPU.
Too bad it’s so slow…</p>
<!-- The Nintendo 64 supports no shaders but it has a bunch of registers you can program for different combinations of textures and interpolated vertex colors. This kind of graphics hardware is known as a "register combiner" and it's also how early GeForce chips worked. -->
<h2 id="shading-a-palette-instead">Shading a palette instead</h2>
<p>So the idea is to do texture-space shading on the CPU.
But what if it’s a palette texture we’re shading?
Those are very common on the N64 anyway.
In that case it’s enough to update <em>only the palette</em> and the texture will respond as if we computed lighting for each texel.
Instant savings!</p>
<!-- In this case it's optimized by fitting a 256-color palette to the normal map, and then computing lighting only for each entry in the palette for speed. -->
<figure>
<img src="https://30fps.net/pages/palette-lighting-tricks-n64/palettes_example.jpg" alt="A demonstration of “palette-space” shading. When the palettes update, the full textures update too. When mapped to an object, it looks like the shading changed.">

</figure>
<p>The original palette is replaced with a shaded palette and the palette texture is applied as a regular texture to an object.
With just a diffuse “dot(N,L)” lighting the results look pretty good:
<!-- The shading model is basically `color=basecolor*dot(normal, light)`, but since this is on the CPU side, you could use any formula. --></p>
<figure>
<img src="https://30fps.net/pages/palette-lighting-tricks-n64/potatorock.png" alt="Another view of the above potato-shaped rock mesh.">

</figure>
<!-- The result looks pretty convincing if you don't see any UV map seams. -->
<p>In the above example I also did shading in linear space by undoing the gamma correction of the color texture :) In the final demo it wasn’t possible because I split the ambient and direct light terms to be combined by N64’s RDP unit in hardware.</p>
<h3 id="object-space-normal-mapping">Object-space normal mapping</h3>
<p>Usually normal mapping is done in tangent space.
This is way you can use repeating textures and the fine normals can modulate smoothly varying vertex normals.
A tangent-space normal map of a single color represents a smooth surface.</p>
<p>An object-space normal is simpler but more constrained.
Now the normal map’s texels don’t represent deviations from the vertex normals but absolute surface normals instead.
The runtime math becomes simpler – just read a color from a texture – but all surface points now need a unique texel, like in lightmaps.</p>
<!-- ![An early object-space normal mapping experiment in Blender. I reduced the color count of a baked normal map in an image editor and reapplied it back on the object. The results validated that the approach might work.](2025-01-21-object-space-normal-map-compression-comparison.jpg) -->
<figure>
<img src="https://30fps.net/pages/palette-lighting-tricks-n64/baseline_vs_32_palette_objectspace_normalmap.png" alt="An early experiment to validate the approach on a high-res normal map. Left: The original object-space normal map. Right: Compressed to a 32-color palette.">

</figure>

<p>The objects have both a diffuse texture (basecolor * ao) and a normal map.
Both textures actually share the same palette indices that I generated with scikit-learn’s <a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html">K-means clustering</a>.
The images were interpreted as a single six-channel image for that to work.
<!-- It took a lot of tweaking to persuade k-means to behave and weight both textures fairly. --></p>
<p>Below is an example how the compression looks with a tangent-space normal map.</p>
<figure>
<img src="https://30fps.net/pages/palette-lighting-tricks-n64/compression_diagram.png" alt="A roof tile texture compression example. An RGB diffuse texture and a normal map are both compressed to a 16-color palette image in a way that palette indices are shared. Therefore the actual image data has to be stored just once at 4 bits per pixel.">

</figure>
<!-- The objects still need a varying surface color, even though they are shaded with a normal map. -->
<!-- I achieved this by combining a diffuse map and an object-space normal map to a single six-channel image, and fit a palette to *that*. -->
<!-- I had to tweak the colors-vs-normals weights for each texture though. -->
<p>At shading time, which can happen on load or on each frame, each palette color is processed in a for loop.
A single index is used to fetch a normal and a surface diffuse color.
The CPU-side shader code then produces a new RGB color for that index.
The result of the loop is a new palette but with shading applied.</p>
<p>Unfortunately this approach only really works with directional lights.
It’s also difficult to represent any kind of shadows with just the palette alone.
That’s why I started looking into how baked lighting could fit in the to the equation.</p>
<h2 id="baked-directional-ambient-and-sun-light">Baked directional ambient and sun light</h2>
<p>I wanted the demo to have a building with realistic lighting.
Perhaps it was a bit too ambitious😅
After a lot of deliberation, I put ambient and direct sun lighting in vertex color RGB and alpha channels, respectively.
The ambient term is further split into a directional intensity (a greyscale environment map) and color (vertex RGB with a saturation boost).
The sun is a directional light whose visibility is transmitted in vertex alpha.</p>
<p>The shading formula is therefore this:</p>
<pre><code>ambient = vertex_rgb      * grey_irradiance_map(N) 
direct  = vertex_alpha    * sun_color * dot(N, sun_dir)
color   = diffuse_texture * (ambient + direct)</code></pre>
<!-- Ambient color is stored in `vertex_rgb` and sun visibility in `vertex_alpha`. -->
<p>Here’s how the different terms look:</p>
<figure>
<img src="https://30fps.net/pages/palette-lighting-tricks-n64/lighting_comparison.jpg" alt="Lighting decomposition.">

</figure>
<p>Note how the messy “Sun visibility” vertex colors get neatly masked out by the sun (N.L) computation in the bottom right corner.
In the end the ambient and direct terms are summed to get the shaded result below.</p>
<figure>
<img src="https://30fps.net/pages/palette-lighting-tricks-n64/shaded.jpg" alt="Shaded result.">

</figure>
<p>The thing about directional ambient is that even the baked lighting is rough, the details in the textures still make it look pretty high end.
Consider this scene that has just a colored blurred environment map and per-vertex ambient occlusion:</p>
<figure>
<img src="https://30fps.net/pages/palette-lighting-tricks-n64/ibl.png" alt="Image-based ambient lighting. In this image only an ambient sky light is enabled. Also shows the palettes used (top left corner).">

</figure>
<p>It really pops!
I love image-based lighting.</p>
<p>For the blurred environment maps, I used an equirectangular projection for simplicity.
Polyhaven’s HDRIs already use the projection.
Since I precomputed the shading at load time, the complex sampling math wasn’t an issue.</p>
<!-- Consider these messy vertex colors:

![Direct light intensity only.](pillars_vertex_alpha.png)

When these vertex colors are modulated by the surface texture, the shaded normal map, and a directional ambient term, the shading looks pretty convincing:

![Combined ambient and direct light.](pillars_shaded.png)

Here's using a colored environment and per-vertex ambient occlusion: -->
<!-- Conceptually, on each texture texel the renderer samples the irradiance map with the surface normal, multiplies the resulting sky color with the surface color. In this case the texture is then modulated by per-vertex ambient occlusion but in the final demo I also had other bounce light in the vertex color RGB channels. -->
<figure>
<img src="https://30fps.net/pages/palette-lighting-tricks-n64/hdri_plot.png" alt="Visualization of an 64x32 environment map (right) before it gets blurred to an irradiance map. The dot sphere on the left shows the image pixels mapped to unit sphere directions.">

</figure>
<!-- Both ambient and direct light respect normal maps. Ambient uses image-based lighting and the direct light is just a single directional light. Environment lighting is provided by a greyscale irradiance map (think of a blurred cubemap) and is later modulated by vertex colors. -->
<!-- ![The original 3D reconstruction geometry was way too dense.](zumaglia_wireframe2.png)

![A cleaned-up model. I used Instant Meshes for this step. It's really good!](zumaglia_mesh2.png)

The castle model in the demo is based on [a 3D reconstruction by Sketchfab user andxet](https://sketchfab.com/3d-models/zumaglias-castle-bi-italy-8ef740a8ca31498c9e8f73b1c27a3298).
I retopo'd and textured it myself. I'm still very slow working in Blender so the model was left in a pretty rough state in the end.
[Instant Meshes](https://github.com/wjakob/instant-meshes) was a great help in this process. -->
<h2 id="shading-a-larger-model-with-repeating-textures">Shading a larger model with repeating textures</h2>
<p>I designed the original shading algorithm for single objects and only tested it with the <code>potato_rock.obj</code> you saw in the beginning.
For the demo, the castle mesh’s repeating textures posed a problem.
As a workaround, I split the large mesh into submeshes that each conceptually share the same object-space normal map.</p>
<p>The task was done primarily by yours truly manually in Blender, by grouping geometry by material and surface direction.
The computer did its part by calculating a world-to-model matrix based on polygon normals for each group.
That is a pretty much an approximate tangent space.
So I couldn’t escape them in the end!</p>
<p>Each of these groups shares a palette so as a whole their lighting will be correct only in the average sense.</p>
<figure>
<img src="https://30fps.net/pages/palette-lighting-tricks-n64/cube_tangents.png" alt="Tangent-space basis vector visualization for a simple cube. In the final model many polygons that point roughly in the same direction have to share the same tangent space.">

</figure>
<p>The tangent spaces are <em>not</em> interpolated at runtime, which shows up as faceted lighting.
This is perhaps the biggest downside of this technique.</p>
<figure>
<img src="https://30fps.net/pages/palette-lighting-tricks-n64/lighting_facet.png" alt="Lighting isn’t interpolated smoothly on this arch because the tangent spaces are constant over polygons, unlike in proper tangent-space normal mapping.">

</figure>
<!-- The skydome consists of a 32x64 texture that repeats horizontally and a cut sphere with some vertex coloring. I think it looked alright in the end, even though it is blurry.

![The skydome model.](skydome.jpg)

Regarding bloom, it's done on the CPU and composited back as a white quad with an alpha texture. A bit slow and buggy though🙂

![The sky dome with some bloom.](skydomebloom.jpg)

Finally, the white "egg" was supposed to be just a test model that I replace with something else. Well, that didn't happen!

![How the ending of the demo could have been.](cat_statue.jpg)

It's a perfect sphere but due to projection precision issues (!) it got stretched. Later I wanted to put a cat sit on top of it but sadly couldn't make it look right on time.😿 -->
<h2 id="specular-shading">Specular shading</h2>
<!-- Unfortunately many surface points now share a single shaded color. -->
<p>Since many surface points now share the same shaded color, computing point light or specular shading correctly is not possible.
The “palette-space” approach only really works for diffuse directional lights because the shading formulas don’t need a “to camera” vector <span><em>V</em></span>, which depends on the position of the shaded surface point.
Yet still I tried to hack it for the speculars :)</p>
<p>If we approximate the object to be shaded as a sphere, then the point <em>p</em> being shaded is simply <code>p=radius*normal</code>.
We must also accept that the result will look faceted since many surface points share the same palette index.</p>
<figure>
<img src="https://30fps.net/pages/palette-lighting-tricks-n64/lion.jpg" alt="Fresnel shading. The sculpt was approximated as a stretched sphere in lighting calculations.">

</figure>
<p>In the demo, the specular highlights looked a bit funny but still they seemed to fool most people. I count this as a success.</p>
<h2 id="is-this-the-future">Is this the future?</h2>
<p>In the demo I tried to hide the main limitations of the technique: shading discontinuities, only greyscale textures supported (!), no point lights.
So it really only works with elaborate preprocessing.
I’d love to see the shading discontinuity issue solved somehow (Spooky Iluha’s techniques don’t have it) without losing support for both ambient and direct lighting.
I don’t know if it’s possible but that’s what makes this hobby so fun :)</p>
<p>A <a href="https://files.scene.org/view/parties/2025/revision25/wild/castello.zip">PAL-compatible N64 ROM</a> is available but note that it crashes a lot.</p>
<hr>
<p><em>I’m also thinking of writing a book. <a href="https://30fps.net/book">Sign up here</a> if you’re interested.</em></p>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[O2 VoLTE: locating any customer with a phone call (210 pts)]]></title>
            <link>https://mastdatabase.co.uk/blog/2025/05/o2-expose-customer-location-call-4g/</link>
            <guid>44014046</guid>
            <pubDate>Sat, 17 May 2025 13:08:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mastdatabase.co.uk/blog/2025/05/o2-expose-customer-location-call-4g/">https://mastdatabase.co.uk/blog/2025/05/o2-expose-customer-location-call-4g/</a>, See on <a href="https://news.ycombinator.com/item?id=44014046">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article id="blog-article"><div><p role="doc-subtitle">Privacy is dead: For multiple months, any O2 customer has had their location exposed to call initiators without their knowledge.</p></div><div><nav aria-label="Breadcrumbs"><ol><li><a href="https://mastdatabase.co.uk/">Home</a></li><li><a href="https://mastdatabase.co.uk/blog/1/">Blog articles</a></li><li><a aria-current="page" href="https://mastdatabase.co.uk/blog/2025/05/o2-expose-customer-location-call-4g/">O2 VoLTE: locating any customer wit…</a></li></ol></nav></div><div id="blog-article-content"><nav><h2 id="table-of-contents">Contents</h2><ul><li><a href="https://mastdatabase.co.uk/blog/2025/05/o2-expose-customer-location-call-4g/#introduction">Introduction</a></li><li><a href="https://mastdatabase.co.uk/blog/2025/05/o2-expose-customer-location-call-4g/#o2-uk">O2 UK</a></li><li><a href="https://mastdatabase.co.uk/blog/2025/05/o2-expose-customer-location-call-4g/#signalling-messages">Signalling messages</a></li><li><a href="https://mastdatabase.co.uk/blog/2025/05/o2-expose-customer-location-call-4g/#what-id-like-to-see-change">What I'd like to see change</a></li><li><a href="https://mastdatabase.co.uk/blog/2025/05/o2-expose-customer-location-call-4g/#conclusion">Conclusion</a></li><li><a href="https://mastdatabase.co.uk/blog/2025/05/o2-expose-customer-location-call-4g/#notes">Notes</a></li></ul></nav>
<h2 id="introduction"><a aria-label="introduction permalink" href="https://mastdatabase.co.uk/blog/2025/05/o2-expose-customer-location-call-4g/#introduction"><span></span></a>Introduction</h2>
<p>Voice over LTE (VoLTE) is a way to make calls via an internet-based protocol on mobile networks using a standard called IP Multimedia Subsystem
(IMS). IMS implementations have historically caused trouble due to their increased complexity and device interdependence. This increase in
complexity has traditionally only externally manifested with device incompatibility problems. In the past, it wasn't uncommon to find devices
that required special firmware to utilise VoLTE and WiFi Calling.</p>
<p>However, I have always been interested in another risk to this increased complexity. <strong>Security.</strong></p>
<p>With an IMS implementation, it is up to the mobile network to choose how they want to implement the services, and what configurations they want
to use. Your phone then talks directly with these servers. Mobile networks have a responsibility to ensure that these servers are kept up to date
and secure, and that their configurations do not lead to unnecessary data exposure.</p>
<p>Unfortunately, today we will be looking at a great example of a mobile network that has validated my concerns.</p>
<h2 id="o2-uk"><a aria-label="o2 uk permalink" href="https://mastdatabase.co.uk/blog/2025/05/o2-expose-customer-location-call-4g/#o2-uk"><span></span></a>O2 UK</h2>
<p>On 27 March 2017, O2 UK launched their first IMS service.<sup id="note-1-link"><a href="https://mastdatabase.co.uk/blog/2025/05/o2-expose-customer-location-call-4g/#note-1"><span>[1]</span></a></sup> Dubbed "4G Calling" by the network, it
provided improved voice quality and a better in-call data experience as customers did not drop down to 3G when making a call.</p>
<p>As someone who had recently moved to O2, I was interested in the network's IMS implementation, particularly which voice codecs were supported by
the network for calls made on 4G/WiFi Calling.</p>
<p>Using an application known as Network Signal Guru (NSG) on my rooted Google Pixel 8, I called another O2 customer (with a 4G VoLTE compatible
device) to try and determine audio quality.</p>
<p>A bug within NSG on modern Google Pixel devices with Samsung Modems means that the VoLTE section of the the app doesn't automatically populate
the codec used for the current call, meaning that I instead had to look at the raw IMS signalling messages sent between my device and the network
to find this out.</p>
<h2 id="signalling-messages"><a aria-label="signalling messages permalink" href="https://mastdatabase.co.uk/blog/2025/05/o2-expose-customer-location-call-4g/#signalling-messages"><span></span></a>Signalling messages</h2>
<p>Quite quickly I realised something was wrong. The responses I got from the network were extremely detailed and long, and were unlike anything I
had seen before on other networks. The messages contained information such as the IMS/SIP server used by O2
(<a href="https://www.mavenir.com/portfolio/mavcore/cloud-native-ims/" rel="noopener">Mavenir UAG</a>) along with version numbers, occasional error messages raised by the
C++ services processing the call information when something went wrong, and other debugging information. However, most notable were a set of five
headers near the bottom of the message:</p>
<figure><pre><code>SIP Msg
...
  P-Mav-Extension-IMSI: 23410123456789
  P-Mav-Extension-IMSI: 23410987654321
  P-Mav-Extension-IMEI: 350266809828927
  P-Mav-Extension-IMEI: 350266806365261
  ...
  Cellular-Network-Info: 3GPP-E-UTRAN-FDD;utran-cell-id-3gpp=2341010037A60773;cell-info-age=26371
</code></pre><figcaption><p>Synthesised excerpt of IMS signalling message for demonstration; not a genuine IMEI/IMSI/cell ID.</p></figcaption></figure>
<p>Two sets of IMSIs, two sets of IMEIs, and a Cell ID header. How curious…</p>
<p>Sure enough, when comparing both the IMSIs and IMEIs in the message to those of my own devices, I had been given both the IMSI and IMEI of my
phone which initiated the call, <strong>but also the call recipient's</strong>.</p>
<p>Curious, I looked into the <code>Cellular-Network-Info</code> header. I had never seen this SIP header before but a quick bit of research led me to learn
how to decode it. The start of the value, <code>3GPP-E-UTRAN-FDD</code> indicates that the cell data is for 4G (officially known as E-UTRAN) FDD (frequency
division duplex). The folllowing section starting with <code>utran-cell-id-3gpp</code> is broken down into 3 parts:</p>
<figure><img draggable="false" src="https://mastdatabase.co.uk/static/cell-header-breakdown-ee1811a0608846ee73b1ff754a072b07.svg" alt="" loading="lazy"></figure>
<ul>
<li>the first 5–6 digits are the network PLMN <em>of the recipient</em></li>
<li>the following 4 characters are the <em>recipient's</em> Location Area Code (LAC) in hexadecimal</li>
<li>the final 7 characters are the <em>recipient's</em> Cell ID, again in hexadecimal</li>
</ul>
<p>The final section represents how old the data is in seconds. This is present when the device isn't currently connected to a network, such as when
you have no signal or are relying on WiFi Calling.</p>
<p>That means for the above example, we are able to work out that the recipient:</p>
<ul>
<li>is currently connected to the O2 network (234-10)</li>
<li>is within LAC 0x1003 (decimal: 4009) on Cell ID 0x7a60773 (decimal: 128321395)</li>
<li>uses a Google Pixel 9 (IMEI: 350266806365261)</li>
<li>has an O2 SIM (IMSI: 23410987654321)</li>
</ul>
<p><strong>This is bad.</strong></p>
<p>With all this information, we can make use of publicly crowdsourced data, collected by tools such as <a href="https://cellmapper.net/" rel="noopener">cellmapper.net</a>, to
cross-reference this information to work out a general location of the user.</p>
<p>First, we just throw the Cell ID into a tool that calculates what site ID this corresponds to:</p>
<figure><span><span>
      <a href="https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/2c964/cell-id-calculator.png" target="_blank" rel="noopener">
    <span></span>
  <picture>
          <source srcset="https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/d5f3f/cell-id-calculator.avif 144w,
https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/a1ffd/cell-id-calculator.avif 288w,
https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/9565c/cell-id-calculator.avif 575w,
https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/6cb26/cell-id-calculator.avif 863w,
https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/75fe3/cell-id-calculator.avif 1150w,
https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/23c81/cell-id-calculator.avif 1312w" sizes="(max-width: 575px) 100vw, 575px" type="image/avif"><source srcset="https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/3506a/cell-id-calculator.webp 144w,
https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/5f1c3/cell-id-calculator.webp 288w,
https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/d2a82/cell-id-calculator.webp 575w,
https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/754d3/cell-id-calculator.webp 863w,
https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/e0a21/cell-id-calculator.webp 1150w,
https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/a43fd/cell-id-calculator.webp 1312w" sizes="(max-width: 575px) 100vw, 575px" type="image/webp">
          <source srcset="https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/c4366/cell-id-calculator.png 144w,
https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/672f0/cell-id-calculator.png 288w,
https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/4c1b5/cell-id-calculator.png 575w,
https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/e3d9b/cell-id-calculator.png 863w,
https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/5c19b/cell-id-calculator.png 1150w,
https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/2c964/cell-id-calculator.png 1312w" sizes="(max-width: 575px) 100vw, 575px" type="image/png">
          <img src="https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/4c1b5/cell-id-calculator.png" alt="cell id calculator" title="" loading="lazy" decoding="async">
        </picture>
  </a>
    </span></span><figcaption><p><a href="https://www.cellmapper.net/enbid?net=LTE&amp;cellid=128321395" rel="noopener">Cellmapper's cell ID calculator</a></p></figcaption></figure>
<p>Then we just have to search for that site on Cellmapper's map:</p>
<figure><span><span>
      <a href="https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/28810/cellmapper-sector.png" target="_blank" rel="noopener">
    <span></span>
  <picture>
          <source srcset="https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/d5f3f/cellmapper-sector.avif 144w,
https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/a1ffd/cellmapper-sector.avif 288w,
https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/9565c/cellmapper-sector.avif 575w,
https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/6cb26/cellmapper-sector.avif 863w,
https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/75fe3/cellmapper-sector.avif 1150w,
https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/9e767/cellmapper-sector.avif 2502w" sizes="(max-width: 575px) 100vw, 575px" type="image/avif"><source srcset="https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/3506a/cellmapper-sector.webp 144w,
https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/5f1c3/cellmapper-sector.webp 288w,
https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/d2a82/cellmapper-sector.webp 575w,
https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/754d3/cellmapper-sector.webp 863w,
https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/e0a21/cellmapper-sector.webp 1150w,
https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/a9c27/cellmapper-sector.webp 2502w" sizes="(max-width: 575px) 100vw, 575px" type="image/webp">
          <source srcset="https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/c4366/cellmapper-sector.png 144w,
https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/672f0/cellmapper-sector.png 288w,
https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/4c1b5/cellmapper-sector.png 575w,
https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/e3d9b/cellmapper-sector.png 863w,
https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/5c19b/cellmapper-sector.png 1150w,
https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/28810/cellmapper-sector.png 2502w" sizes="(max-width: 575px) 100vw, 575px" type="image/png">
          <img src="https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/4c1b5/cellmapper-sector.png" alt="cellmapper sector" title="" loading="lazy" decoding="async">
        </picture>
  </a>
    </span></span><figcaption><p><a href="https://www.cellmapper.net/map?MCC=234&amp;MNC=10&amp;type=LTE&amp;latitude=51.52666039304819&amp;longitude=-0.6526597015225719&amp;zoom=16&amp;showTowers=true&amp;showIcons=true&amp;showTowerLabels=true&amp;clusterEnabled=true&amp;tilesEnabled=true&amp;showOrphans=false&amp;showNoFrequencyOnly=false&amp;showFrequencyOnly=false&amp;showBandwidthOnly=false&amp;DateFilterType=Last&amp;showHex=false&amp;showVerifiedOnly=false&amp;showUnverifiedOnly=false&amp;showLTECAOnly=false&amp;showENDCOnly=false&amp;showBand=0&amp;showSectorColours=true&amp;mapType=roadmap&amp;darkMode=false&amp;imperialUnits=false" rel="noopener">O2 UK eNB 501255 on Cellmapper</a></p></figcaption></figure>
<p>Here, you can see the macro cell the user was on at the time of the call. While this has a relatively sizeable coverage area, shown by the
selected polygon with the blue outline, dense urban areas will make use of very many sites (such as small cells, which are often fitted directly
to streetlamps) with small coverage areas. Each site in these areas can often cover areas as small as 100m<sup>2</sup>. In a city, this becomes
an <em>extremely</em> accurate measure of location.</p>
<p>I also tested the attack with another O2 customer who was roaming abroad, and the attack worked perfectly with me being able to pinpoint them to
the city centre of Copenhagen, Denmark.</p>
<figure><span><span>
      <a href="https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/e93db/dk-site.png" target="_blank" rel="noopener">
    <span></span>
  <picture>
          <source srcset="https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/d5f3f/dk-site.avif 144w,
https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/a1ffd/dk-site.avif 288w,
https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/9565c/dk-site.avif 575w,
https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/6cb26/dk-site.avif 863w,
https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/75fe3/dk-site.avif 1150w,
https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/905d8/dk-site.avif 1708w" sizes="(max-width: 575px) 100vw, 575px" type="image/avif"><source srcset="https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/3506a/dk-site.webp 144w,
https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/5f1c3/dk-site.webp 288w,
https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/d2a82/dk-site.webp 575w,
https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/754d3/dk-site.webp 863w,
https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/e0a21/dk-site.webp 1150w,
https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/69203/dk-site.webp 1708w" sizes="(max-width: 575px) 100vw, 575px" type="image/webp">
          <source srcset="https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/c4366/dk-site.png 144w,
https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/672f0/dk-site.png 288w,
https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/4c1b5/dk-site.png 575w,
https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/e3d9b/dk-site.png 863w,
https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/5c19b/dk-site.png 1150w,
https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/e93db/dk-site.png 1708w" sizes="(max-width: 575px) 100vw, 575px" type="image/png">
          <img src="https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/4c1b5/dk-site.png" alt="dk site" title="" loading="lazy" decoding="async">
        </picture>
  </a>
    </span></span><figcaption>Location of eNB 107258 on 3DK, near Vesterport St., Copenhagen</figcaption></figure>
<p>I think it's important to note here that my device is <strong>in no way special</strong>. It's not doing anything odd to the network and hasn't behaved any
differently. All it is doing is allowing me to see the information being sent to it. This effectively means that every O2 device that is making a
phone call on IMS (4G Calling / WiFi Calling) is receiving information that can be used to trivially geolocate the recipient of the call.</p>
<h2 id="what-id-like-to-see-change"><a aria-label="what id like to see change permalink" href="https://mastdatabase.co.uk/blog/2025/05/o2-expose-customer-location-call-4g/#what-id-like-to-see-change"><span></span></a>What I'd like to see change</h2>
<p>O2 must remove the highlighted headers from all IMS / SIP messages to protect the privacy and safety of customers. It would be logical to also
disable debug headers, as I imagine a scenario where those unintentionally leak further information could occur. There is no reason for any
device outside of the network core to see those headers.</p>
<p>I'm extremely disappointed as an O2 customer to see a lack of any escalation route to report these kind of potential vectors for attack. EE, a
rival network, have a clear and well defined escalation route (see <a href="https://www.bt.com/about/contact-bt/responsible-disclosure" rel="noopener">https://www.bt.com/about/contact-bt/responsible-disclosure</a>) that I imagine
would have prevented the need for this information to have been publicly shared. I don't want to be the enemy, I simply want to feel comfortable
using my phone.</p>
<h2 id="conclusion"><a aria-label="conclusion permalink" href="https://mastdatabase.co.uk/blog/2025/05/o2-expose-customer-location-call-4g/#conclusion"><span></span></a>Conclusion</h2>
<p>Any O2 customer can be trivially located by an attacker with even a basic understanding of mobile networking.</p>
<p>There is also <strong>no way to prevent this attack</strong> as an O2 customer. Disabling 4G Calling does <em>not</em> prevent these headers from being revealed, and
if your device is ever unreachable these internal headers will still reveal the last cell you were connected to and how long ago this was.</p>
<p>Attempts were made to reach out to O2 via email (to both Lutz Schüler, CEO and <a href="https://mastdatabase.co.uk/cdn-cgi/l/email-protection#5526303620273c212c3c3b363c31303b212615233c27323c3b3830313c347b363a7b203e" rel="noopener"><span data-cfemail="1360767066617a676a7a7d707a77767d676053657a61747a7d7e76777a723d707c3d6678">[email&nbsp;protected]</span></a>) on the 26 and 27 March 2025
reporting this behaviour and privacy risk, but I have yet to get any response or see any change in the behaviour.</p>
<hr>
<h2 id="notes"><a aria-label="notes permalink" href="https://mastdatabase.co.uk/blog/2025/05/o2-expose-customer-location-call-4g/#notes"><span></span></a>Notes</h2>
<p><sup id="note-1"><a href="https://mastdatabase.co.uk/blog/2025/05/o2-expose-customer-location-call-4g/#note-1-link"><span>[1]</span></a></sup> <a href="https://www.engadget.com/2017-03-29-o2-wifi-4g-calling.html" rel="noopener">https://www.engadget.com/2017-03-29-o2-wifi-4g-calling.html</a></p>
<p>Article edited by David Wheatley.</p></div><hr><nav><a href="https://mastdatabase.co.uk/blog/1/">Back to article list</a></nav></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pyrefly: A new type checker and IDE experience for Python (176 pts)]]></title>
            <link>https://engineering.fb.com/2025/05/15/developer-tools/introducing-pyrefly-a-new-type-checker-and-ide-experience-for-python/</link>
            <guid>44013913</guid>
            <pubDate>Sat, 17 May 2025 12:47:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://engineering.fb.com/2025/05/15/developer-tools/introducing-pyrefly-a-new-type-checker-and-ide-experience-for-python/">https://engineering.fb.com/2025/05/15/developer-tools/introducing-pyrefly-a-new-type-checker-and-ide-experience-for-python/</a>, See on <a href="https://news.ycombinator.com/item?id=44013913">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

		<p><span>Today we are announcing an alpha version of </span><a href="https://pyrefly.org/" target="_blank" rel="noopener"><span>Pyrefly</span></a><span>, an open source Python type checker and IDE extension crafted in <a href="https://engineering.fb.com/2021/04/29/developer-tools/rust/" target="_blank" rel="noopener">Rust</a>. Pyrefly is a static type checker that analyzes Python code to ensure type consistency and help you catch errors throughout your codebase before your code runs. It also supports IDE integration and CLI usage to give you flexibility in how you incorporate it into your workflow.&nbsp;</span></p>
<p><span>The open source community is the backbone of the Python language. We are eager to collaborate on Pyrefly with the community and improve Python’s type system and the many libraries that we all rely on.&nbsp;&nbsp;</span></p>
<h2><span>Get started</span></h2>
<p><span>Ready to dive in? </span><a href="https://pyrefly.org/" target="_blank" rel="noopener"><span>The official Pyrefly website</span></a><span> has all the details, but to quickly get started:</span></p>
<ul>
<li aria-level="1"><a href="https://pyrefly.org/en/docs/installation/" target="_blank" rel="noopener"><span>Install</span></a><span> Pyrefly on the command-line: </span><span>pip install pyrefly</span><span>.</span></li>
<li aria-level="1"><span><a href="https://pyrefly.org/en/docs/migrating-to-pyrefly/" target="_blank" rel="noopener">Migrate your existing type checker configuration to Pyrefly</a>.</span></li>
<li aria-level="1"><span>Enhance Your IDE: Download the </span><a href="https://marketplace.visualstudio.com/items?itemName=meta.pyrefly" target="_blank" rel="noopener"><span>Pyrefly extension for VSCode</span></a><span> and enjoy a lightning fast IDE experience from starter projects to monorepos.</span></li>
<li aria-level="1"><span>Leave feedback for us on </span><a href="https://github.com/facebook/pyrefly/issues" target="_blank" rel="noopener"><span>GitHub</span></a><span>.</span></li>
</ul>
<h2><span>Why we built Pyrefly</span></h2>
<p><span>Back in 2017, we embarked on a mission to create a type checker that could handle </span><a href="https://instagram-engineering.com/web-service-efficiency-at-instagram-with-python-4976d078e366" target="_blank" rel="noopener"><span>Instagram’s massive codebase</span></a><span> of typed Python. This mission led to the birth of the </span><a href="https://github.com/facebook/pyre-check" target="_blank" rel="noopener"><span>Pyre</span></a><span> type checker, inspired by the robust designs of </span><a href="https://hacklang.org/" target="_blank" rel="noopener"><span>Hack</span></a><span> and </span><a href="https://flow.org/"><span>Flow</span></a><span>, and written in OCaml to deliver scalable performance.&nbsp;</span></p>
<p><span>Over the years, Pyre served us well, but as the type system evolved and the need for typechecking to drive responsive IDE emerged, it was clear that we needed to take a new approach. We explored alternate solutions and leveraged community tools like </span><a href="https://github.com/Microsoft/pyright" target="_blank" rel="noopener"><span>Pyright</span></a><span> for code navigation. But the need for an extensible type checker that can bring code navigation, checking at scale, and exporting types to other services drove us to start over, creating Pyrefly.&nbsp;</span></p>
<h2><span>The principles behind Pyrefly</span></h2>
<p><span>Today, we’re excited to unveil Pyrefly, a project <a href="https://github.com/facebook/pyrefly" target="_blank" rel="noopener">we’ve been developing openly on </a></span><span>GitHub</span><span>. We invite you to explore our work and try it out on your own project. While a project like Pyrefly is the sum of thousands of technical choices, a few notable principles we’ve followed are:</span></p>
<h3>Performance</h3>
<p><span>We want to shift checks that used to happen later on CI to happening on every single keystroke. That requires checking code at speed (on large codebases we can check 1.8 million lines of code per second!) and careful thought to incrementality and updates. Pyrefly is implemented in Rust and designed for high performance on codebases of all sizes.</span></p>
<h3>IDE first</h3>
<p><span>We want the IDE and command line to share a consistent view of the world, which means crafting abstractions that capture the differences without incurring unnecessary costs. Designing these abstractions from the beginning is much easier than retrofitting them, which we tried with Pyre.</span></p>
<h3>Inference</h3>
<p><span>Some </span><a href="https://engineering.fb.com/2024/12/09/developer-tools/typed-python-2024-survey-meta/" target="_blank" rel="noopener"><span>Python programs are typed</span></a><span>, but many aren’t. We want users to benefit from types even if they haven’t annotated their code – so automatically infer types for returns and local variables and display them in the IDE. What’s more, in the IDE you can even double click to insert these inferred types if you think that would make the program better.</span></p>
<h3>Open source</h3>
<p><span>Python is open source, and hugely popular. The </span><a href="https://typing.python.org/en/latest/spec/" target="_blank" rel="noopener"><span>Python typing specification</span></a><span> is open source, which made Pyrefly vastly easier to develop. Many of the libraries Meta contributes to are open source,( e.g., </span><a href="https://pytorch.org/" target="_blank" rel="noopener"><span>PyTorch</span></a><span>).</span></p>
<p><span>Pyrefly is also open source, </span><a href="https://github.com/facebook/pyrefly/" target="_blank" rel="noopener"><span>available on GitHub</span></a><span> under the </span><a href="https://github.com/facebook/pyrefly/blob/main/LICENSE" target="_blank" rel="noopener"><span>MIT license</span></a><span>, and we encourage </span><a href="https://github.com/facebook/pyrefly/pulls" target="_blank" rel="noopener"><span>pull requests</span></a><span> and </span><a href="https://github.com/facebook/pyrefly/issues" target="_blank" rel="noopener"><span>issue reports</span></a><span>. We also have a </span><a href="https://discord.gg/Cf7mFQtW7W" target="_blank" rel="noopener"><span>Discord channel</span></a><span> for more free flowing discussions. We would love to build a community around Pyrefly.</span></p>
<h2><span>The future of Pyrefly</span></h2>
<p><span>We will work with the Python community to drive the language forward and improve the developer experience. Since the beginning of Pyre, we open sourced our code and contributed a number of PEPs alongside the community of type checker maintainers. We feel we can do more with Pyrefly to help Python developers leverage the benefits of types for developers, library authors, and folks just learning the language.&nbsp;</span></p>
<p><span>Meta has leveraged types in dynamic languages from the beginning and knows the significant benefits it brings to developer productivity and security. We plan to share more of our learnings and tooling with </span><a href="https://engineering.fb.com/2024/12/09/developer-tools/typed-python-2024-survey-meta/" target="_blank" rel="noopener"><span>blogs</span></a><span>, better types in the ecosystem and language enhancements.&nbsp;</span></p>
<p><span>Today we’re releasing Pyrefly as an alpha. At the same time, we’re busy burning down the long-tail of bugs and features aiming to remove the alpha label this Summer. Your feedback is invaluable to get there, so please give it a try and </span><a href="https://github.com/facebook/pyrefly/issues" target="_blank" rel="noopener"><span>report your bugs</span></a><span> or things you think can be improved. Even if Pyrefly isn’t right for your project, we would love to hear how you use types and what you would like to see improved in your editor.</span></p>
<p><span>Join us on the journey as we help illuminate your bugs with Pyrefly. Happy coding! 🐍✨</span></p>
<h2><span>Hear more about Pyrefly&nbsp;</span></h2>
<p><span>Check out the <a href="https://engineering.fb.com/2025/05/15/developer-tools/open-sourcing-pyrefly-a-faster-python-type-checker-written-in-rust" target="_blank" rel="noopener">episode of the Meta Tech Podcast</a> where several team members share their experience developing Pyrefly and technical details for how it works. We also just </span><a href="https://us.pycon.org/2025/schedule/presentation/118/" target="_blank" rel="noopener"><span>talked at PyCon US</span></a><span> about high-performance Python through faster type checking and free threaded execution.</span></p>
<p><span>To learn more about Meta Open Source, visit our </span><a href="https://opensource.fb.com/" target="_blank" rel="noopener"><span>open source site</span></a><span>, subscribe to our </span><a href="https://www.youtube.com/channel/UCCQY962PmHabTjaHv2wJzfQ" target="_blank" rel="noopener"><span>YouTube channel</span></a><span>, or follow us on </span><a href="https://www.facebook.com/MetaOpenSource" target="_blank" rel="noopener"><span>Facebook</span></a><span>, </span><a href="https://www.threads.net/@metaopensource" target="_blank" rel="noopener"><span>Threads</span></a><span>, </span><a href="https://x.com/MetaOpenSource" target="_blank" rel="noopener"><span>X</span></a>,<span> and </span><a href="https://www.linkedin.com/showcase/meta-open-source?fbclid=IwZXh0bgNhZW0CMTEAAR2fEOJNb7zOi8rJeRvQry5sRxARpdL3OpS4sYLdC1_npkEy60gBS1ynXwQ_aem_mJUK6jEUApFTW75Emhtpqw" target="_blank" rel="noopener"><span>LinkedIn</span></a><span>.</span></p>
<h2><span>Acknowledgements&nbsp;</span></h2>
<p><i><span>Pyrefly was created By Meta’s Python Language Tooling Team: Jia Chen, Rebecca Chen, Sam Goldman, David Luo, Kyle Into, Zeina Migeed, Neil Mitchell, Maggie Moss, Conner Nilsen, Aaron Pollack, Teddy Sudol, Steven Troxler, Lucian Wischik, Danny Yang, and Sam Zhou.</span></i></p>

		
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Push Ifs Up and Fors Down (421 pts)]]></title>
            <link>https://matklad.github.io/2023/11/15/push-ifs-up-and-fors-down.html</link>
            <guid>44013157</guid>
            <pubDate>Sat, 17 May 2025 09:31:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://matklad.github.io/2023/11/15/push-ifs-up-and-fors-down.html">https://matklad.github.io/2023/11/15/push-ifs-up-and-fors-down.html</a>, See on <a href="https://news.ycombinator.com/item?id=44013157">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <article>
        <h2>
          Push Ifs Up And Fors Down <time datetime="2023-11-15">Nov 15, 2023</time>
        </h2>
        <p>A short note on two related rules of thumb.</p>
        <section id="Push-Ifs-Up">
          
          <p>
            If there’s an <code>if</code> condition inside a function, consider
            if it could be moved to the caller instead:
          </p>

          <figure>
            <pre><code><span><span>// GOOD</span></span>
<span><span>fn</span> <span>frobnicate</span>(walrus: Walrus) {</span>
<span>    ...</span>
<span>}</span>
<span></span>
<span><span>// BAD</span></span>
<span><span>fn</span> <span>frobnicate</span>(walrus: <span>Option</span>&lt;Walrus&gt;) {</span>
<span>  <span>let</span> <span>walrus</span> = <span>match</span> walrus {</span>
<span>    <span>Some</span>(it) =&gt; it,</span>
<span>    <span>None</span> =&gt; <span>return</span>,</span>
<span>  };</span>
<span>  ...</span>
<span>}</span></code></pre>
          </figure>
          <p>
            As in the example above, this often comes up with preconditions: a
            function might check precondition inside and “do nothing” if it
            doesn’t hold, or it could push the task of precondition checking to
            its caller, and enforce via types (or an assert) that the
            precondition holds. With preconditions especially, “pushing up” can
            become viral, and result in fewer checks overall, which is one
            motivation for this rule of thumb.
          </p>
          <p>
            Another motivation is that control flow and <code>if</code>s are
            complicated, and are a source of bugs. By pushing <code>if</code>s
            up, you often end up centralizing control flow in a single function,
            which has a complex branching logic, but all the actual work is
            delegated to straight line subroutines.
          </p>
          <p>
            <em>If</em> you have complex control flow, better to fit it on a
            screen in a single function, rather than spread throughout the file.
            What’s more, with all the flow in one place it often is possible to
            notice redundancies and dead conditions. Compare:
          </p>

          <figure>
            <pre><code><span><span>fn</span> <span>f</span>() {</span>
<span>  <span>if</span> foo &amp;&amp; bar {</span>
<span>    <span>if</span> foo {</span>
<span></span>
<span>    } <span>else</span> {</span>
<span></span>
<span>    }</span>
<span>  }</span>
<span>}</span>
<span></span>
<span><span>fn</span> <span>g</span>() {</span>
<span>  <span>if</span> foo &amp;&amp; bar {</span>
<span>    <span>h</span>()</span>
<span>  }</span>
<span>}</span>
<span></span>
<span><span>fn</span> <span>h</span>() {</span>
<span>  <span>if</span> foo {</span>
<span></span>
<span>  } <span>else</span> {</span>
<span></span>
<span>  }</span>
<span>}</span></code></pre>
          </figure>
          <p>
            For <code>f</code>, it’s much easier to notice a dead branch than
            for a combination of <code>g</code> and <code>h</code>!
          </p>
          <p>
            A related pattern here is what I call “dissolving enum” refactor.
            Sometimes, the code ends up looking like this:
          </p>

          <figure>
            <pre><code><span><span>enum</span> <span>E</span> {</span>
<span>  <span>Foo</span>(<span>i32</span>),</span>
<span>  <span>Bar</span>(<span>String</span>),</span>
<span>}</span>
<span></span>
<span><span>fn</span> <span>main</span>() {</span>
<span>  <span>let</span> <span>e</span> = <span>f</span>();</span>
<span>  <span>g</span>(e)</span>
<span>}</span>
<span></span>
<span><span>fn</span> <span>f</span>() <span>-&gt;</span> E {</span>
<span>  <span>if</span> condition {</span>
<span>    E::<span>Foo</span>(x)</span>
<span>  } <span>else</span> {</span>
<span>    E::<span>Bar</span>(y)</span>
<span>  }</span>
<span>}</span>
<span></span>
<span><span>fn</span> <span>g</span>(e: E) {</span>
<span>  <span>match</span> e {</span>
<span>    E::<span>Foo</span>(x) =&gt; <span>foo</span>(x),</span>
<span>    E::<span>Bar</span>(y) =&gt; <span>bar</span>(y)</span>
<span>  }</span>
<span>}</span></code></pre>
          </figure>
          <p>
            There are two branching instructions here and, by pulling them up,
            it becomes apparent that it is the exact same condition, triplicated
            (the third time reified as a data structure):
          </p>

          <figure>
            <pre><code><span><span>fn</span> <span>main</span>() {</span>
<span>  <span>if</span> condition {</span>
<span>    <span>foo</span>(x)</span>
<span>  } <span>else</span> {</span>
<span>    <span>bar</span>(y)</span>
<span>  }</span>
<span>}</span></code></pre>
          </figure>
        </section>
        <section id="Push-Fors-Down">
          <h2>
            <a href="#Push-Fors-Down">Push Fors Down </a>
          </h2>
          <p>
            This comes from data oriented school of thought. Few things are few,
            many things are many. Programs usually operate with bunches of
            objects. Or at least the hot path usually involves handling many
            entities. It is the volume of entities that makes the path hot in
            the first place. So it often is prudent to introduce a concept of a
            “batch” of objects, and make operations on batches the base case,
            with a scalar version being a special case of a batched ones:
          </p>

          <figure>
            <pre><code><span><span>// GOOD</span></span>
<span><span>frobnicate_batch</span>(walruses)</span>
<span></span>
<span><span>// BAD</span></span>
<span><span>for</span> <span>walrus</span> <span>in</span> walruses {</span>
<span>  <span>frobnicate</span>(walrus)</span>
<span>}</span></code></pre>
          </figure>
          <p>
            The primary benefit here is performance. Plenty of performance, <a href="http://venge.net/graydon/talks/VectorizedInterpretersTalk-2023-05-12.pdf">in extreme cases</a>.
          </p>
          <p>
            If you have a whole batch of things to work with, you can amortize
            startup cost and be flexible about the order you process things. In
            fact, you don’t even need to process entities in any particular
            order, you can do vectorized/struct-of-array tricks to process one
            field of all entities first, before continuing with other fields.
          </p>
          <p>
            Perhaps the most fun example here is <a href="https://en.wikipedia.org/wiki/Sch%C3%B6nhage%E2%80%93Strassen_algorithm">FFT-based polynomial multiplication</a>: turns out, evaluating a
            polynomial at a bunch of points simultaneously could be done faster
            than a bunch of individual point evaluations!
          </p>
          <p>
            The two pieces of advice about <code>for</code>s and <code>if</code>s even compose!
          </p>

          <figure>
            <pre><code><span><span>// GOOD</span></span>
<span><span>if</span> condition {</span>
<span>  <span>for</span> <span>walrus</span> <span>in</span> walruses {</span>
<span>    walrus.<span>frobnicate</span>()</span>
<span>  }</span>
<span>} <span>else</span> {</span>
<span>  <span>for</span> <span>walrus</span> <span>in</span> walruses {</span>
<span>    walrus.<span>transmogrify</span>()</span>
<span>  }</span>
<span>}</span>
<span></span>
<span><span>// BAD</span></span>
<span><span>for</span> <span>walrus</span> <span>in</span> walruses {</span>
<span>  <span>if</span> condition {</span>
<span>    walrus.<span>frobnicate</span>()</span>
<span>  } <span>else</span> {</span>
<span>    walrus.<span>transmogrify</span>()</span>
<span>  }</span>
<span>}</span></code></pre>
          </figure>
          <p>
            The <code>GOOD</code> version is good, because it avoids repeatedly
            re-evaluating <code>condition</code>, removes a branch from the hot
            loop, and potentially unlocks vectorization. This pattern works on a
            micro level and on a macro level — the good version is the
            architecture of TigerBeetle, where in the data plane we operate on
            batches of objects at the same time, to amortize the cost of
            decision making in the control plane.
          </p>
          <p>
            While performance is perhaps the primary motivation for the <code>for</code> advice, sometimes it helps with expressiveness as well.
            <code>jQuery</code> was quite successful back in the day, and it
            operates on collections of elements. The language of abstract vector
            spaces is often a better tool for thought than bunches of
            coordinate-wise equations.
          </p>
          <p>
            To sum up, push the <code>if</code>s up and the <code>for</code>s
            down!
          </p>
        </section>
      </article>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Implementing a RISC-V Hypervisor (102 pts)]]></title>
            <link>https://seiya.me/blog/riscv-hypervisor</link>
            <guid>44012729</guid>
            <pubDate>Sat, 17 May 2025 07:46:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://seiya.me/blog/riscv-hypervisor">https://seiya.me/blog/riscv-hypervisor</a>, See on <a href="https://news.ycombinator.com/item?id=44012729">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>To implement a seamless Linux integration into <a href="https://github.com/starina-os/starina">Starina</a>, I decided to go with a Linux lightweight VM approach similar to WSL2. This means I need to implement a hypervisor that can run Linux.</p>
<p>I had <a href="https://seiya.me/blog/implementing-hypervisor-on-resea">implemented an Intel VT-x based hypervisor</a> before, but this time I wanted to try something different: RISC-V H-extension based hypervisor!</p>
<p>This post is a diary of my journey of writing a RISC-V hypervisor incrementally.</p>
<h2>RISC-V H-extension</h2>
<p>RISC-V H-extension introduces new CPU modes and some more CSRs (so-called control registers) to implement hardware-assisted virtualization. Its design is similar to Intel VT-x in the sense that both host and guest modes have their own kernel mode and user mode. This design makes it easy to run host OS along with guests, that is, guests behave as normal host processes (e.g. QEMU and Firecracker).</p>
<h2>How can I test a hypervisor on macOS?</h2>
<p>Unlike Linux KVM-based hypervisors (more specifically, virtual machine monitors), Starina is a new operating system and has been tested on QEMU.</p>
<p>In this case, you typically need to use nested virtualization where the hardware-assisted virtualization is emulated by the host OS. That's how I did it for Intel VT-x.</p>
<p>Here's great news: QEMU itself can emulate RISC-V H-extension! You just need to add <code>-cpu rv64,h=true</code> to the QEMU command line. I presume this is thanks to RISC-V's simplicity and designers' foresight (and of course QEMU developers' effort!).</p>
<p>Having a software emulation in QEMU is a key enabler when you're writing a new operating system from scratch because you can attach GDB to QEMU to debug the OS.</p>
<h2>Step 1: Entering the guest</h2>
<p>The first thing to do is to enter the guest state. In RISC-V, guest kernel mode is called VS-mode. In RISC-V, you just fill a few CSRs. Specifically, <code>hstatus.SPV</code> should be set to 1 before <code>sret</code> instruction:</p>
<p><img alt="first-inst-guest-page-fault" loading="lazy" width="2040" height="1188" decoding="async" data-nimg="1" srcset="https://seiya.me/_next/image?url=%2Fmedia%2Friscv-hypervisor%2Ffirst-inst-guest-page-fault.png&amp;w=2048&amp;q=75 1x, https://seiya.me/_next/image?url=%2Fmedia%2Friscv-hypervisor%2Ffirst-inst-guest-page-fault.png&amp;w=3840&amp;q=75 2x" src="https://seiya.me/_next/image?url=%2Fmedia%2Friscv-hypervisor%2Ffirst-inst-guest-page-fault.png&amp;w=3840&amp;q=75"></p>
<p>The kernel panicked with an interesting error name: <em>instruction <strong>guest</strong>-page fault</em>. Yes, CPU has entered the guest mode!</p>
<h2>Step 2: First <code>ecall</code></h2>
<p>The next step is to run something in the guest mode. Let's start with a simple <code>ecall</code>:</p>
<figure data-rehype-pretty-code-figure=""><pre><code><span data-line=""><span>const</span><span> BOOT_CODE</span><span>:</span><span> &amp;</span><span>[</span><span>u8</span><span>] </span><span>=</span><span> &amp;</span><span>[</span></span>
<span data-line=""><span>    0x73</span><span>, </span><span>0x00</span><span>, </span><span>0x00</span><span>, </span><span>0x00</span><span>, </span><span>// ecall</span></span>
<span data-line=""><span>];</span></span></code></pre></figure>
<p>To make it work, we need to prepare the guest's page table which maps the guest-physical address to the host-physical address so that the CPU can read the instructions in <code>BOOT_CODE</code>.</p>
<p>RISC-V defines another paging modes called Sv39x4/Sv48x4/Sv57x4, and they're mostly identical to Sv39/Sv48/Sv57. The only caveat is U bit needs to be set to 1 for kernel pages too.</p>
<p>Once <code>hgatp</code> is set, I got another trap reason:</p>
<p><img alt="first ecall" loading="lazy" width="2040" height="1188" decoding="async" data-nimg="1" srcset="https://seiya.me/_next/image?url=%2Fmedia%2Friscv-hypervisor%2Ffirst-ecall.png&amp;w=2048&amp;q=75 1x, https://seiya.me/_next/image?url=%2Fmedia%2Friscv-hypervisor%2Ffirst-ecall.png&amp;w=3840&amp;q=75 2x" src="https://seiya.me/_next/image?url=%2Fmedia%2Friscv-hypervisor%2Ffirst-ecall.png&amp;w=3840&amp;q=75"></p>
<h2>Step 3: Hello World from guest!</h2>
<p>Now we're ready to run a Hello World program. I wrote a simple program in assembly:</p>
<pre><code>.section .text
.global _start

_start:
    li a0, 'H'
    li a7, 1
    ecall

    li a0, 'i'
    li a7, 1
    ecall

    li a0, '!'
    li a7, 1
    ecall

    li a0, '\n'
    li a7, 1
    ecall

    unimp
</code></pre>
<p>This assumes hypervisor's ecall handler implements the SBI, a RISC-V's BIOS interface. This snippet calls so-called <code>putchar</code> API and finally calls an invalid instruction (<code>unimp</code>) to trigger a trap.</p>
<p>Building this tiny guest OS is easy:</p>
<pre><code>$ clang --target=riscv64 -march=rv64g -nostdlib -Wl,-Ttext=0x80200000 guest.S -o guest.elf
$ llvm-objcopy -O binary guest.elf guest.bin
</code></pre>
<p>And it works!</p>
<p><img alt="minimal hello world" loading="lazy" width="2040" height="1188" decoding="async" data-nimg="1" srcset="https://seiya.me/_next/image?url=%2Fmedia%2Friscv-hypervisor%2Fminimal-hello-world.png&amp;w=2048&amp;q=75 1x, https://seiya.me/_next/image?url=%2Fmedia%2Friscv-hypervisor%2Fminimal-hello-world.png&amp;w=3840&amp;q=75 2x" src="https://seiya.me/_next/image?url=%2Fmedia%2Friscv-hypervisor%2Fminimal-hello-world.png&amp;w=3840&amp;q=75"></p>
<h2>Step 4: Booting Linux</h2>
<p>Our minimal guest Hello World worked, so it's time to try with Linux.</p>
<p>Here are some kernel config options I enabled:</p>
<pre><code>CONFIG_SERIAL_EARLYCON_RISCV_SBI=y
CONFIG_RISCV_SBI_V01=y
CONFIG_HVC_RISCV_SBI=y
CONFIG_RISCV_TIMER=y
</code></pre>
<p>Linux kernel for RISC-V can be built with:</p>
<pre><code>make ARCH=riscv CROSS_COMPILE=riscv64-linux-gnu- Image
</code></pre>
<p>The boot image format is documented <a href="https://www.kernel.org/doc/html/v5.5/riscv/boot-image-header.html">here</a>, and it's basically raw binary. Just copy the image file into the guest memory, and jump to the beginning when the CPU enters the guest mode.</p>
<p>Looks like Linux booted, but it crashed with a null dereference:</p>
<p><img alt="null dereference in device tree" loading="lazy" width="2040" height="1188" decoding="async" data-nimg="1" srcset="https://seiya.me/_next/image?url=%2Fmedia%2Friscv-hypervisor%2Fnull-dereference-in-device-tree.png&amp;w=2048&amp;q=75 1x, https://seiya.me/_next/image?url=%2Fmedia%2Friscv-hypervisor%2Fnull-dereference-in-device-tree.png&amp;w=3840&amp;q=75 2x" src="https://seiya.me/_next/image?url=%2Fmedia%2Friscv-hypervisor%2Fnull-dereference-in-device-tree.png&amp;w=3840&amp;q=75"></p>
<p>According to the <code>sepc</code> value, the kernel panicked at <code>__pi_fdt32_ld</code>:</p>
<pre><code>$ gobjdump -d linux/vmlinux | grep -A 5 801cace8 | head
ffffffff801cace8 &lt;__pi_fdt32_ld&gt;:
ffffffff801cace8:	00054783          	lbu	a5,0(a0)  &lt;-- null deferecence here!
ffffffff801cacec:	00154703          	lbu	a4,1(a0)
ffffffff801cacf0:	0187979b          	slliw	a5,a5,0x18
</code></pre>
<h2>Step 5: Device Tree</h2>
<p>In RISC-V, we need to provide a device tree, a tree data structure defining what devices are available in the computer.</p>
<p>Fortunately, we can use a Rust crate for this: <a href="https://docs.rs/vm-fdt/latest/vm_fdt/">vm-fdt</a>, a device tree builder from RustVMM project (famous for Firecracker). The library supports no_std, so it was super easy to use it in Starina (thank you, thank you RustVMM folks!).</p>
<p>To make Linux work, the device tree should include at least: free memory areas (<code>reg</code> in <code>device_type = memory</code>), and CPUs with RISC-V ISA extensions (<code>riscv,isa-extensions</code>).</p>
<h2>Step 6: <code>rdtime</code> support</h2>
<p>After adding the device tree, I got another trap. Looks like Linux tried to read <code>rdtime</code> but failed because <code>hcounteren</code> was not set:</p>
<blockquote>
<p>hcounteren register is clear, attempts to read the cycle, time, instret, or hpmcounter n register while V=1 will cause a virtual-instruction exception</p>
</blockquote>
<p>The fix is to fill <code>hcounteren</code> CSR. Easy peasy.</p>
<h2>Step 7: Timer support</h2>
<p>While booting, Linux kernel tries to probe the abilities of the CPU and peripherals. That intialization step is mostly done by itself without any help from the hypervisor. However, there's a bit head-scratching part: the timer speed detection.</p>
<p>In the step, Linux kernel waits for <code>jiffies</code> to progress and it looks as if it's hanging if the hypervisor doesn't implement timer.</p>
<p>In RISC-V there are two approaches to implement a timer:</p>
<ul>
<li><strong><code>sbi_set_timer</code>:</strong> Call the hypervisor (SBI) to set the timer.</li>
<li><strong><code>sstc</code> extension:</strong> A CPU extension to trigger a timer interrupt without hypervisor's help.</li>
</ul>
<p>This step is also the first time I needed to inject interrupts into the guest. While RISC-V spec is clean, but I was totally lost with how to do it. Setting <code>hideleg</code> was what I needed to do, but I was confused by RISC-V Advanced Interrupt Architecture, which extends the RISC-V interrupt handling.</p>
<p>RISC-V extensions are more somtimes look like "patches" to the base spec. <a href="https://github.com/riscv-software-src/riscv-unified-db">RISC-V Unified Database</a> is a great resource to understand how the extensions extend the base spec.</p>
<p>Once I enabled <code>sstc</code> extension and implemented interrupt handling correctly, the kernel booted successfully:</p>
<p><img alt="linux-hello-world" loading="lazy" width="2040" height="1188" decoding="async" data-nimg="1" srcset="https://seiya.me/_next/image?url=%2Fmedia%2Friscv-hypervisor%2Flinux-hello-world.png&amp;w=2048&amp;q=75 1x, https://seiya.me/_next/image?url=%2Fmedia%2Friscv-hypervisor%2Flinux-hello-world.png&amp;w=3840&amp;q=75 2x" src="https://seiya.me/_next/image?url=%2Fmedia%2Friscv-hypervisor%2Flinux-hello-world.png&amp;w=3840&amp;q=75"></p>
<h2>Step 8: MMIO support</h2>
<p>Linux booted successfully on Starina's hypervisor. The next step is to provide devices. In RISC-V, as in other architectures, devices are mapped to physical addresses (memory-mapped I/O). Devices need to be emulated are:</p>
<ul>
<li><strong>An interrupt controller.</strong> The guest kernel needs it to read pending interrupts, and acknowledge them. PLIC in RISC-V.</li>
<li><strong>Disk device:</strong> Where the root filesystem is stored. Virtio-blk is a popular choice.</li>
<li><strong>Network device:</strong> For networking. Virtio-net is the most popular choice.</li>
</ul>
<p>In hypervisor, MMIO is implemented by <em>not</em> mapping anything at the address range. That is, everytime the guest tries to access the address range:</p>
<ol>
<li>The CPU tries to read/write a MMIO address, but it's not mapped. Trigger a guest page fault.</li>
<li>The hypervisor checks if the address range is a MMIO address.</li>
<li>Decode the instruction to determine destination/source registers, and the access width.</li>
<li>Emulate the MMIO operation.</li>
<li>If it's a read, write the value to the register.</li>
<li>Advance the program counter to the next instruction, and resume the guest.</li>
</ol>
<p>Doesn't it sound a bit hacky? In MMIO, hypervisor behaves like a CPU interpreter. Fortunately, RISC-V is very simple and gives us summary of the instruction in <code>htinst</code> CSR. In x86-64, ... good luck!</p>
<p>For future RISC-V hypervisor developers, here are few pitfalls I encountered:</p>
<ul>
<li>A guest physical address written to <code>htval</code> is shifted right by 2 bits. You'll see a weird address if you forget this.</li>
<li><code>htinst</code> could be compressed instruction. This means the instruction width is not always 4 bytes, but sometimes 2 bytes.</li>
</ul>
<h2>Step 9: virtio-fs</h2>
<p>This is the last step of this post. While virito-blk is a typical choise to provide the root filesystem, I chose something different: virtio-fs.</p>
<p>Why? Because it allows more seamless integration with Starina. I plan to write another post about the integration, but in short, Starina provides some virtual files to the guest.</p>
<p>Virtio-fs is a virtual filesystem over Virtio. More specifically, it uses FUSE (Filesystem in Userspace) protocol to communicate with the guest kernel.</p>
<p>I have nothing to say about this step because we've finished the hypervisor-specific steps already! Design a good virtio library, implement the virtio-fs emulation on top of it, and you're done:</p>
<p><img alt="virtio-fs" loading="lazy" width="1856" height="1364" decoding="async" data-nimg="1" srcset="https://seiya.me/_next/image?url=%2Fmedia%2Friscv-hypervisor%2Fhello-from-virtio-fs.png&amp;w=1920&amp;q=75 1x, https://seiya.me/_next/image?url=%2Fmedia%2Friscv-hypervisor%2Fhello-from-virtio-fs.png&amp;w=3840&amp;q=75 2x" src="https://seiya.me/_next/image?url=%2Fmedia%2Friscv-hypervisor%2Fhello-from-virtio-fs.png&amp;w=3840&amp;q=75"></p>
<p>Phew, that's it!</p>
<h2>Tip: Debugging both hypervisor/guest worlds</h2>
<p>Starina supports Unikernel-like mode, where the microkernel and apps are built into a single ELF executable. This is not only for performance, but also for debugging.</p>
<p>Here's a <code>gdbinit</code> script which enabled me to watch VMM, hypervisor in Starina kernel, and the Linux kernel in guest:</p>
<pre><code># Load Starina (hypervisor's) debug info
file build/kernel/debug/kernel

# Load Linux (guest's) debug info
add-symbol-file path/to/vmlinux
</code></pre>
<p>And look! You're seeing the guest's kernel stack trace!</p>
<p><img alt="gdb" loading="lazy" width="1566" height="834" decoding="async" data-nimg="1" srcset="https://seiya.me/_next/image?url=%2Fmedia%2Friscv-hypervisor%2Fyou-can-attach-gdb.png&amp;w=1920&amp;q=75 1x, https://seiya.me/_next/image?url=%2Fmedia%2Friscv-hypervisor%2Fyou-can-attach-gdb.png&amp;w=3840&amp;q=75 2x" src="https://seiya.me/_next/image?url=%2Fmedia%2Friscv-hypervisor%2Fyou-can-attach-gdb.png&amp;w=3840&amp;q=75"></p>
<p>This is why I haven't yet implemented stack traces in Starina: you just need to attach the GDB and type <code>bt</code>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Catalog of Novel Operating Systems (159 pts)]]></title>
            <link>https://github.com/prathyvsh/os-catalog</link>
            <guid>44012615</guid>
            <pubDate>Sat, 17 May 2025 07:19:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/prathyvsh/os-catalog">https://github.com/prathyvsh/os-catalog</a>, See on <a href="https://news.ycombinator.com/item?id=44012615">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Novel Operating Systems Catalog</h2><a id="user-content-novel-operating-systems-catalog" aria-label="Permalink: Novel Operating Systems Catalog" href="#novel-operating-systems-catalog"></a></p>
<p dir="auto">Catalogue of novel operating systems</p>
<p dir="auto">Somewhere after the fall in popularity of note-taking apps, perhaps recognizing that just note-taking is not enough and the deafening hype of LLMs, there was a sweet period of lull when a lot of people started boldly building new operating systems. This is a catalogue of such operating systems that I have come across. In the past, before the commercialization of computers, we had a plethora of operating systems with unique languages to interact with computers, like AmigaOS, Symbolics, SunOS, MULTICS, Burroughs, Meneut, BeOS PARC, Star, Oberon, Plan9, NeXTSTEP, OS/2, PL/8, Inferno, QNX, RISCOS etc. This spirit can only be glimpsed in pockets, and kudos to all those who keep the fire alive!</p>
<p dir="auto">A thread on it <a href="https://x.com/Prabros/status/1922915943631523899" rel="nofollow">here</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://100r.co/site/uxn.html" rel="nofollow">UXN</a></h2><a id="user-content-uxn" aria-label="Permalink: UXN" href="#uxn"></a></p>
<p dir="auto">Perhaps the best one to start off this catalog is the UXN/Varvara personal computing stack of <a href="https://100r.co/" rel="nofollow">100 Rabbits</a>. Such a great couple with such a radical vision!</p>
<p dir="auto"><a href="https://github.com/prathyvsh/os-catalog/blob/main/UXN%20logo"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/uxn-logo.jpg" alt="./img/uxn-logo.jpg"></a></p>
<p dir="auto"><a href="https://github.com/prathyvsh/os-catalog/blob/main/UXN%20screenshot"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/uxn-screenshot.jpg" alt="./img/uxn-screenshot.jpg"></a></p>
<p dir="auto">They have documented their rationale in these two documents:</p>
<ul dir="auto">
  <li><a href="https://100r.co/site/tools_ecosystem.html" rel="nofollow">Tools Ecosystem</a></li>
  <li><a href="https://100r.co/site/weathering_software_winter.html" rel="nofollow">Weathering Software Winter</a></li>
</ul>
<p dir="auto">Documents related to UXN can be obtained here: <a href="https://github.com/hundredrabbits/awesome-uxn?tab=readme-ov-file">https://github.com/hundredrabbits/awesome-uxn?tab=readme-ov-file</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://playb.it/" rel="nofollow">Playbit</a></h2><a id="user-content-playbit" aria-label="Permalink: Playbit" href="#playbit"></a></p>
<p dir="auto">Daring effort from <a href="https://github.com/rsms">Rasmus Andersson</a> and team to reinvent the computer stack.</p>
<p dir="auto">And alpha version available <a href="https://playb.it/alpha/" rel="nofollow">here</a>.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/prathyvsh/os-catalog/blob/main/img/playbit-screenshot.webp"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/playbit-screenshot.webp" alt="Screenshot of Playbit"></a></p>

<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://folk.computer/" rel="nofollow">Folk.computer</a></h2><a id="user-content-folkcomputer" aria-label="Permalink: Folk.computer" href="#folkcomputer"></a></p>
<p dir="auto">Omar Rizwan and Andreas Cuérvo</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/prathyvsh/os-catalog/blob/main/img/folk-computer.gif"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/folk-computer.gif" alt="Video of with Folk.computer" data-animated-image=""></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://nette.io/" rel="nofollow">Nette.io</a></h2><a id="user-content-netteio" aria-label="Permalink: Nette.io" href="#netteio"></a></p>
<p dir="auto">Nette.io by <a href="https://github.com/qazwsxpawel">Pawel Ceranka</a> positions itself as a research OS for the web.</p>
<p dir="auto"><a href="https://github.com/prathyvsh/os-catalog/blob/main/Nette%20website%20screenshot"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/nette.png" alt="./img/nette.png"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://github.com/mntmn/interim">Interim</a></h2><a id="user-content-interim" aria-label="Permalink: Interim" href="#interim"></a></p>
<p dir="auto"><a href="https://github.com/prathyvsh/os-catalog/blob/main/Interim%20Logo"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/interim-logo.png" alt="./img/interim-logo.png"></a></p>
<p dir="auto">Something about Lisp draws people into construct OSes from ground up. Perhaps it is the simplicity of the language that acts as the foundation. Here‘s Interim, one of our favourite minimal OSes constructed with Lisp.</p>
<p dir="auto"><a href="https://github.com/prathyvsh/os-catalog/blob/main/Interim%20Screenshot"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/interim-screenshot.jpg" alt="./img/interim-screenshot.jpg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://github.com/froggey/Mezzano">Mezzano</a></h2><a id="user-content-mezzano" aria-label="Permalink: Mezzano" href="#mezzano"></a></p>
<p dir="auto"><a href="https://github.com/prathyvsh/os-catalog/blob/main/Mezzano%20Screenshot"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/mezzano.png" alt="./img/mezzano.png"></a></p>
<p dir="auto">An OS written in CommonLisp</p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://github.com/vygr/ChrysaLisp">ChrysalLisp</a></h2><a id="user-content-chrysallisp" aria-label="Permalink: ChrysalLisp" href="#chrysallisp"></a></p>
<p dir="auto"><a href="https://github.com/prathyvsh/os-catalog/blob/main/ChrysaLisp%20screenshot"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/chrysalisp-screenshot.png" alt="./img/chrysalisp-screenshot.png"></a></p>
<p dir="auto">ChrysaLisp is amulti-threaded, multi-core, multi-user parallel OS with features such as a GUI, terminal, OO Assembler, class libraries, C-Script compiler, Lisp interpreter, debugger, profiler, vector font engine, and more.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://ravynos.com/" rel="nofollow">RayvnOS</a></h2><a id="user-content-rayvnos" aria-label="Permalink: RayvnOS" href="#rayvnos"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://www.redox-os.org/" rel="nofollow">RedoxOS</a></h2><a id="user-content-redoxos" aria-label="Permalink: RedoxOS" href="#redoxos"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Ideas</h2><a id="user-content-ideas" aria-label="Permalink: Ideas" href="#ideas"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://desktopneo.com/" rel="nofollow">DesktopNeo</a></h2><a id="user-content-desktopneo" aria-label="Permalink: DesktopNeo" href="#desktopneo"></a></p>
<p dir="auto">DesktopNeo, a rethinking of the desktop interface by <a href="https://www.lennartziburski.com/" rel="nofollow">Lennart Ziburski</a></p>
<p dir="auto"><a href="https://github.com/prathyvsh/os-catalog/blob/main/Screenshot%20of%20Desktop%20Neo"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/desktop-neo.jpg" alt="./img/desktop-neo.jpg"></a></p>
<p dir="auto"><a href="https://github.com/prathyvsh/os-catalog/blob/main/Another%20screenshot%20of%20Desktop%20Neo"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/desktop-neo-screenshot.png" alt="./img/desktop-neo-screenshot.png"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://www.mercuryos.com/" rel="nofollow">MercuryOS</a></h2><a id="user-content-mercuryos" aria-label="Permalink: MercuryOS" href="#mercuryos"></a></p>
<p dir="auto">MercuryOS by Jason Yuan is an interesting rethink of the OS based on intensions:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/prathyvsh/os-catalog/blob/main/img/mercury-screenshot.png"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/mercury-screenshot.png" alt="./img/mercury-screenshot.png"></a>
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/prathyvsh/os-catalog/blob/main/img/mercury-dark-mode.png"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/mercury-dark-mode.png" alt="./img/mercury-dark-mode.png"></a></p>
<p dir="auto">Prototype by Rauno Freiberg. <a href="https://github.com/prathyvsh/os-catalog/blob/main/Source">https://x.com/raunofreiberg/status/1666122499401166873</a></p>

<p dir="auto">The team seems to be working on MercuryOS → Makespace.fun → <a href="https://new.computer/" rel="nofollow">New.computer</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://x.com/getFreezeframe" rel="nofollow">Freeze.app</a></h2><a id="user-content-freezeapp" aria-label="Permalink: Freeze.app" href="#freezeapp"></a></p>
<p dir="auto">Freeze the desktop interface and then thaw it at will: <a href="https://x.com/getFreezeframe/status/1358805285393948673" rel="nofollow">https://x.com/getFreezeframe/status/1358805285393948673</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://azlen.me/stories/worm-os/" rel="nofollow">WormOS</a></h2><a id="user-content-wormos" aria-label="Permalink: WormOS" href="#wormos"></a></p>
<p dir="auto">Interesting article on partitioned rooms by mental space with little bubbles on the edges that act as wormholes into things you want to achieve.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/prathyvsh/os-catalog/blob/main/img/wormos.png"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/wormos.png" alt="./img/wormos.png"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Status Unknown</h2><a id="user-content-status-unknown" aria-label="Permalink: Status Unknown" href="#status-unknown"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://bedrock.computer/" rel="nofollow">Bedrock.computer</a></h2><a id="user-content-bedrockcomputer" aria-label="Permalink: Bedrock.computer" href="#bedrockcomputer"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Other lists</h2><a id="user-content-other-lists" aria-label="Permalink: Other lists" href="#other-lists"></a></p>
<ul dir="auto">
  <li><a href="https://github.com/jubalh/awesome-os">AwesomeOS by @jubalh</a></li>
  <li><a href="https://1.anagora.org/node/os" rel="nofollow">Anagora List</a></li>
</ul>
</article></div></div>]]></description>
        </item>
    </channel>
</rss>