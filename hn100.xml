<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 10 Aug 2025 20:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Fight Chat Control (413 pts)]]></title>
            <link>https://fightchatcontrol.eu/</link>
            <guid>44856426</guid>
            <pubDate>Sun, 10 Aug 2025 16:50:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fightchatcontrol.eu/">https://fightchatcontrol.eu/</a>, See on <a href="https://news.ycombinator.com/item?id=44856426">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <!-- Hero Section -->
        <div>
                    <h2>
                        The EU (still) wants to scan <br> your private messages and photos
                    </h2>
                    <p>
                        The "Chat Control" proposal would mandate scanning of <strong>all</strong> private digital communications,
                        including encrypted messages and photos. This threatens <strong>fundamental privacy rights</strong> and digital security
                        for all EU citizens.
                    </p>
                    <div>
                        
                        <div>
                            <h3 id="support-count">15</h3>
                            <p>Member States Supporting</p>
                        </div>
                        <div>
                            <h3 id="undecided-count">9</h3>
                            <p>Member States Undecided</p>
                        </div>
                    </div>
                    
                </div>

        <!-- Overview Section -->
        <div id="overview">
                <h2>You Will Be Impacted</h2>
                <p>
                    Every photo, every message, every file you send will be automatically scanned‚Äîwithout your consent or suspicion. This is not about catching criminals. It is mass surveillance imposed on all 450 million citizens of the European Union.
                </p>
                <div>
                    <div>
                        <p>üì±</p>
                        <h3>Mass Surveillance</h3>
                        <p>Every private message, photo, and file scanned automatically: no suspicion required, no exceptions*, even encrypted communications.</p>
                    </div>
                    <div>
                        <p>üîìÔ∏è</p>
                        <h3>Breaking Encryption</h3>
                        <p>Weakening or breaking end-to-end encryption exposes everyone‚Äôs communications‚Äîincluding sensitive financial, medical, and private data‚Äîto hackers, criminals, and hostile actors.</p>
                    </div>
                    <div>
                        <p>‚öñÔ∏è</p>
                        <h3>Fundamental Rights</h3>
                        <p>Undermines your fundamental rights to privacy and data protection, as guaranteed by Articles 7 and 8 of the EU Charter‚Äîrights considered core to European democratic values.</p>
                    </div>
                    <div>
                        <p>üéØ</p>
                        <h3>False Positives</h3>
                        <p>Automated scanners routinely misidentify innocent content, such as vacation photos or private jokes, as illegal, putting ordinary people at risk of false accusations and damaging investigations.</p>
                    </div>
                    <div>
                        <p>üë®‚Äçüë©‚Äçüëß‚Äçüë¶</p>
                        <h3>Ineffective Child Protection</h3>
                        <p>Child protection experts and organisations, including the UN, warn that mass surveillance fails to prevent abuse and actually makes children less safe‚Äîby weakening security for everyone and diverting resources from proven protective measures.</p>
                    </div>
                    <div>
                        <p>üåç</p>
                        <h3>Global Precedent</h3>
                        <p>Creates a dangerous global precedent enabling authoritarian governments, citing EU policy, to roll out intrusive surveillance at home, undermining privacy and free expression worldwide.</p>
                    </div>
                </div>
                
                <p>
                    *EU politicians exempt themselves from this surveillance under "professional secrecy" rules.
                    They get privacy.<br> You and your family do not. Demand fairness.
                </p>
            </div>

        <!-- Member States Section -->
        <div id="member-states">
                <h2>Member State Positions</h2>
                <br>
                
                
            </div>

        <!-- Delegates Section -->
        <div id="delegates">
                <h2>Find Your Representatives</h2>
                <br>
                
                
            </div>

        <!-- Contact Tool Section -->
        <div id="contact-tool">
                <h2>Take Action!<br> Contact Your MEPs</h2>
                <h3>
                    Your privacy and freedoms are at risk.
                    These policies will impact every European‚Äîyour messages, photos, and private conversations will be scanned without your consent.
                    But we have the power to stop this.
                    Contact your MEPs now with a clear message: NO to mass surveillance.
                    Your voice matters. Make it heard today.
                </h3>
                
            </div>

        <!-- Timeline Section -->
        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AOL closes its dial up internet service (144 pts)]]></title>
            <link>https://www.ispreview.co.uk/index.php/2025/08/after-34-years-aol-finally-closes-its-dial-up-internet-service.html</link>
            <guid>44856090</guid>
            <pubDate>Sun, 10 Aug 2025 16:02:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ispreview.co.uk/index.php/2025/08/after-34-years-aol-finally-closes-its-dial-up-internet-service.html">https://www.ispreview.co.uk/index.php/2025/08/after-34-years-aol-finally-closes-its-dial-up-internet-service.html</a>, See on <a href="https://news.ycombinator.com/item?id=44856090">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<div id="news_thumbpicture"><picture id="primaryimage"><img width="600" height="600" src="https://www.ispreview.co.uk/wp-content/uploads/2025/08/nggallery_import/aol-uk-homepage-from-the-past-e1754805650657-600x600.webp" alt="aol uk homepage from the past" decoding="async" fetchpriority="high" srcset="https://www.ispreview.co.uk/wp-content/uploads/2025/08/nggallery_import/aol-uk-homepage-from-the-past-e1754805650657-600x600.webp 600w, https://www.ispreview.co.uk/wp-content/uploads/2025/08/nggallery_import/aol-uk-homepage-from-the-past-e1754805650657-300x300.webp 300w, https://www.ispreview.co.uk/wp-content/uploads/2025/08/nggallery_import/aol-uk-homepage-from-the-past-e1754805650657-768x768.webp 768w, https://www.ispreview.co.uk/wp-content/uploads/2025/08/nggallery_import/aol-uk-homepage-from-the-past-e1754805650657-150x150.webp 150w, https://www.ispreview.co.uk/wp-content/uploads/2025/08/nggallery_import/aol-uk-homepage-from-the-past-e1754805650657.webp 1000w" sizes="(max-width: 600px) 100vw, 600px"></picture></div><!-- Article Start --><p>In a somewhat surprising development, mainly because almost everybody assumed it had died a long time ago, <a href="https://www.aol.co.uk/" target="_blank" rel="noopener">AOL</a> (America Online) ‚Äì one of the very first consumer ISPs in both the USA and UK ‚Äì recently caused a stir again by announcing that it had ‚Äú<em>decided to discontinue Dial-up Internet</em>‚Äù on 30th September 2025.<span id="more-42747"></span></p>
<p>According to <a href="https://help.aol.com/articles/dial-up-internet-to-be-discontinued" target="_blank" rel="noopener">AOL‚Äôs website</a>: ‚Äú<em>AOL routinely evaluates its products and services and has decided to discontinue Dial-up Internet. This service will no longer be available in AOL plans. As a result, on September 30, 2025 this service and the associated software, the AOL Dialer software and AOL Shield browser, which are optimized for older operating systems and dial-up internet connections, will be discontinued</em>.‚Äù But their email service will continue.</p>
<p><strong>NOTE:</strong> Many <a href="https://www.ispreview.co.uk/index.php/link/dialup" target="_blank" rel="" title="dialup" data-chref="https://www.ispreview.co.uk/broadband_dialup.php">dialup</a> ISPs in the UK during 1995 ‚Äì like AOL ‚Äì used expensive premium rate numbers, although this did soon gravitate to local call rates and then unmetered via FRIACO. The v90 <a href="https://www.ispreview.co.uk/index.php/link/dialup" target="_blank" rel="" title="dialup" data-chref="https://www.ispreview.co.uk/broadband_dialup.php">dialup</a> standard (56Kbps capable or 0.056Mbps) didn‚Äôt arrive until 1998 and by then <a href="https://www.ispreview.co.uk/index.php/link/adsl" target="_blank" rel="" title="digital subscriber line" data-chref="https://www.ispreview.co.uk/broadband_DSL.php">ADSL</a> and cable broadband were just around the corner ‚Äì ready to revolutionise the market.</p>
<p>The change appears to have been announced within the past few weeks, although it wasn‚Äôt picked up more widely until journalist <a href="https://tedium.co/" target="_blank" rel="noopener">Ernie Smith</a> noted it in a post on <a href="https://bsky.app/profile/ernie.tedium.co/post/3lvwjugziec2f" target="_blank" rel="noopener">Bluesky</a>. Just to be clear, the announcement above refers to the USA and Canada. However, we‚Äôre fairly confident that what remains of AOL UK (aka ‚Äì <a href="https://www.ispreview.co.uk/index.php/go/tt" target="_blank" rel="nofollow" title="talktalk">TalkTalk</a>) doesn‚Äôt have any legacy dial-up customers left, although we would have said the same about the USA and Canada too, until that announcement dropped (dial-up speeds in 2025 would be practically unusable). ISPreview is currently checking, just to be sure.</p>

<p>In case anybody has forgotten. The original AOL UK experience was somewhat of a walled-garden way of accessing the internet, which forced you to use the company‚Äôs own software and restricted your ability to access certain internet services. This had the benefit of simplifying the experience, but AOL later fell behind the curve and ended up being overtaken by rivals.</p>
<p>The <a title="carphone warehouse" href="https://www.ispreview.co.uk/index.php/go/cpw" target="_blank" rel="nofollow">Carphone Warehouse</a> (CPW) ultimately won the auction to buy AOL UK‚Äôs Internet access business in 2006 for ¬£370m (note: AOL‚Äôs content division became a separate business). At the time, AOL were the UK‚Äôs third-largest ISP with around <strong>2.1 million customers</strong> (600,000 on dial-up and 1.5 million with broadband) and were later re-branded to AOL Broadband.</p>
<p>A second big change occurred on 29th March 2010, when CPW and <a href="https://www.ispreview.co.uk/index.php/go/tt" target="_blank" rel="nofollow" title="talktalk">TalkTalk</a> separated (demerged) ‚Äì the latter became a separate business, which included customers from CPW‚Äôs prior acquisitions (e.g. AOL Broadband, Tiscali etc.). Several more years passed until May 2014, when TalkTalk confirmed that AOL Broadband (formerly AOL UK) had stopped taking on new internet and phone customers (<a href="https://www.ispreview.co.uk/index.php/2014/05/uk-isp-aol-broadband-longer-available-new-customers.html">here</a>), although no mention was made of the dial-up base.</p>
<p>We‚Äôre certain that plenty of our readers (those now of a certain age group) will have stories to share of the early AOL UK days. Yours truly only used the original service briefly, before promptly switching away as the UK‚Äôs then dialup (narrowband) internet market became more competitive, affordable and less restrictive. It‚Äôs a service I was glad to forget, but it played an important role.</p>

<p><iframe title="AOL (Sign On - Dial Up)" width="500" height="281" src="https://www.youtube.com/embed/D1UY7eDRXrs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
<!-- CONTENT END 1 -->
<!-- Article End -->
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Zig's Lovely Syntax (138 pts)]]></title>
            <link>https://matklad.github.io/2025/08/09/zigs-lovely-syntax.html</link>
            <guid>44855881</guid>
            <pubDate>Sun, 10 Aug 2025 15:33:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://matklad.github.io/2025/08/09/zigs-lovely-syntax.html">https://matklad.github.io/2025/08/09/zigs-lovely-syntax.html</a>, See on <a href="https://news.ycombinator.com/item?id=44855881">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <article>
        <h2>
          Zig‚Äôs Lovely Syntax <time datetime="2025-08-09">Aug 9, 2025</time>
        </h2>
        <p>
          It‚Äôs a bit of a silly post, because syntax is the least interesting
          detail about the language, but, still, I can‚Äôt stop thinking how Zig
          gets this detail just right for the class of curly-braced languages,
          and, well, now you‚Äôll have to think about that too.
        </p>
        <p>
          On the first glance, Zig looks almost exactly like Rust, because Zig
          borrows from Rust liberally. And I think that Rust has great syntax,
          considering all the semantics it needs to express (see
          <a href="https://matklad.github.io/2023/01/26/rusts-ugly-syntax.html">‚ÄúRust‚Äôs Ugly Syntax‚Äù</a>). But Zig improves on that, mostly by
          leveraging simpler language semantics, but also through some purely
          syntactical tasteful decisions.
        </p>
        <section id="Integer-Literals">
          <h2>
            <a href="#Integer-Literals">Integer Literals </a>
          </h2>
          <p>
            How do you spell a number ninety-two? Easy, <code>92</code>. But
            what type is that? Statically-typed languages often come with
            several flavors of integers: <code>u32</code>, <code>u64</code>,
            <code>u8</code>. And there‚Äôs often a syntax for literals of a
            particular types: <code>92u8</code>, <code>92l</code>, <code>92z</code>.
          </p>
          <p>
            Zig doesn‚Äôt have suffixes, because, in Zig, all integer literals
            have the same type: <code>comptime_int</code>:
          </p>

          <figure>
            <pre><code><span><span>const</span> an_integer = <span>92</span>;</span>
<span>assert(<span>@TypeOf</span>(an_integer) <span>==</span> <span>comptime_int</span>);</span></code></pre>
          </figure>
          <p>
            The value of an integer literal is known at compile time and is
            coerced to a specific type on assignment
            <span><code>const x: i32 = 92;</code></span>
            or ascription:
            <span><code>@as(i32, 92)</code></span>
          </p>
          <p>
            To emphasize, this is <em>not</em> type inference, this is implicit
            comptime coercion. This does mean that code like
            <span><code>var x = 92;</code></span>
            generally doesn‚Äôt work, and requires an explicit type.
          </p>
        </section>
        <section id="String-Literals">
          <h2>
            <a href="#String-Literals">String Literals </a>
          </h2>
          <p>Raw or multiline strings are spelled like this:</p>

          <figure>
            <pre><code><span><span>const</span> raw =</span>
<span>    <span>\\Roses are red</span></span>
<span>    <span>\\  Violets are blue,</span></span>
<span>    <span>\\Sugar is sweet</span></span>
<span>    <span>\\  And so are you.</span></span>
<span>    <span>\\</span></span>
<span>;</span></code></pre>
          </figure>
          <p>
            This syntax doesn‚Äôt require a special form for escaping <code>\\</code> itself:
          </p>

          <figure>
            <pre><code><span><span>const</span> still_raw =</span>
<span>    <span>\\const raw =</span></span>
<span>    <span>\\    <span>\\</span>Roses are red</span></span>
<span>    <span>\\    <span>\\</span>  Violets are blue,</span></span>
<span>    <span>\\    <span>\\</span>Sugar is sweet</span></span>
<span>    <span>\\    <span>\\</span>  And so are you.</span></span>
<span>    <span>\\    <span>\\</span></span></span>
<span>    <span>\\;</span></span>
<span>    <span>\\</span></span>
<span>;</span></code></pre>
          </figure>
          <p>
            It nicely dodges indentation problems that plague every other
            language with a similar feature. And, the best thing ever:
            lexically, each line is a separate token. As Zig has only
            line-comments, this means that <code>\n</code> is <em>always</em>
            whitespace. Unlike most other languages, Zig can be correctly lexed
            in a line-by-line manner.
          </p>
          <p>
            Raw strings is perhaps the biggest improvement of Zig over Rust.
            Rust brute-forces the problem with
            <code>r##""##</code> syntax, which does the required job,
            technically, but suffers from the mentioned problems: indentation is
            messy, nesting quotes requires adjusting hashes, unclosed raw
            literal breaks the following lexical structure completely, and
            rustfmt‚Äôs formatting of raw strings tends to be rather ugly. On the
            plus side, this syntax at least cannot be expressed by a
            context-free grammar!
          </p>
        </section>
        <section id="Record-Literals">
          <h2>
            <a href="#Record-Literals">Record Literals </a>
          </h2>
          <p>For the record, Zig takes C syntax (not that C would notice):</p>

          <figure>
            <pre><code><span><span>const</span> p: Point = .{</span>
<span>    .x = <span>1</span>,</span>
<span>    .y = <span>2</span>,</span>
<span>}</span></code></pre>
          </figure>
          <p>
            The <code>.{</code> feels weird! It will make sense by the end of
            the post. Here, I want only to note <code>.x = 1</code>
            part, which matches the assignment syntax <code>obj.x = 1</code>.
            This is great! This means that grepping for
            <code>".x ="</code> gives you <em>all</em> instances where a field
            is written to. This is hugely valuable: most of usages are reads,
            but, to understand the flow of data, you only need to consider
            writes. Ability to mechanically partition the entire set of usages
            into majority of boring reads and a few interesting writes does
            wonders for code comprehension.
          </p>
        </section>
        <section id="Prefix-Types">
          <h2>
            <a href="#Prefix-Types">Prefix Types </a>
          </h2>
          <p>
            Where Zig departs from C the most is the syntax for types. C uses a
            needlessly confusing spiral rule. In Zig, all types are prefix:
          </p>

          <figure>
            <pre><code><span><span>u32</span>      <span>// An integer</span></span>
<span>[<span>3</span>]<span>u32</span>   <span>// An array of three integers</span></span>
<span>?[<span>3</span>]<span>u32</span>  <span>// An array of three integers or null</span></span>
<span></span>
<span><span>// A pointer to...</span></span>
<span><span>*</span><span>const</span> ?[<span>3</span>]<span>u32</span></span></code></pre>
          </figure>
          <p>
            While pointer type is prefix, pointer dereference is postfix, which
            is a more natural subject-verb order to read: <span><code>ptr.* = 92;</code></span>
          </p>
        </section>
        <section id="Identifiers">
          <h2>
            <a href="#Identifiers">Identifiers </a>
          </h2>
          <p>
            Zig has general syntax for ‚Äúraw‚Äù identifiers:
            <span><code>@"a name which a space"</code></span>
            It is useful to avoid collisions with keywords, or for exporting a
            symbol whose name is otherwise not a valid Zig identifier. It is a
            bit more to type than Kotlin‚Äôs delightful
            <span><code>`a name with a space`</code>,</span> but
            manages to re-use Zig‚Äôs syntax for built-ins (<code>@TypeOf</code>)
            and strings.
          </p>
        </section>
        <section id="Functions">
          <h2>
            <a href="#Functions">Functions </a>
          </h2>
          <p>
            Like, Rust, Zig goes for <code>fn foo</code> function declaration
            syntax. This is such a massive improvement over C/Java style
            function declarations: it puts <code>fn</code> token (which is
            completely absent in traditional C family) and function name next to
            each other, which means that textual search for <code>fn name</code>
            allows you to quickly find the function. Then Zig adds a little
            twist. While in Rust we write
          </p>

          <figure>
            <pre><code><span><span>fn</span> <span>add</span>(x: <span>i32</span>, <span>i32</span>) <span>-&gt;</span> <span>i32</span></span></code></pre>
          </figure>
          <p>Zig is</p>

          <figure>
            <pre><code><span><span>fn</span><span> add</span>(x: <span>i32</span>, <span>i32</span>) <span>i32</span></span></code></pre>
          </figure>
          <p>
            The arrow is gone! Now that I‚Äôve used this for some time, I find
            arrow very annoying to type, and adding to the visual noise. Rust
            needs the arrow: Rust has lambdas with an inferred return type, and,
            in a lambda, the return type is optional. So you need some sort of
            an explicit syntax to tell the parser if there is return type:
          </p>

          <figure>
            <pre><code><span>|| expression;</span>
<span>|| <span>-&gt;</span> Type { }</span></code></pre>
          </figure>
          <p>
            And its understandable that lambdas and functions would want to use
            compatible syntax. But Zig doesn‚Äôt have lambdas, so it just makes
            the type mandatory. So the main is
          </p>

          <figure>
            <pre><code><span><span>pub</span> <span>fn</span><span> main</span>() <span>void</span> {}</span></code></pre>
          </figure>
          <p>
            Related small thing, but, as name of the type, I think I like <code>void</code> more than <code>()</code>.
          </p>
        </section>
        <section id="Locals">
          <h2>
            <a href="#Locals">Locals </a>
          </h2>
          <p>
            Zig is using <code>const</code> and <code>var</code> for binding
            values to names:
          </p>

          <figure>
            <pre><code><span><span>const</span> mid = lo <span>+</span> <span>@divFloor</span>(hi <span>-</span> lo, <span>2</span>);</span></code></pre>
          </figure>
          <p>
            This is ok, a bit weird after Rust‚Äôs, whose <code>const</code> would
            be <code>comptime</code> in Zig, but not really noticeable after
            some months. I do think this particular part is not great, because
            <code>const</code>, the more frequent one, is longer. I think Kotlin
            nails it: <code>val</code>, <code>var</code>, <code>fun</code>. Note
            all three are monosyllable, unlike <code>const</code> and <code>fn</code>! Number of syllables matters more than the number of
            letters!
          </p>
          <p>Like Rust, Zig uses</p>

          <figure>
            <pre><code><span><span>'name'</span> (<span>':'</span> Type)?</span></code></pre>
          </figure>
          <p>syntax for ascribing types, which is better than</p>

          <figure>
            <pre><code><span>Type <span>'name'</span></span></code></pre>
          </figure>
          <p>
            because optional suffixes are easier to parse visually and
            mechanically than optional prefixes.
          </p>
        </section>
        <section id="Conjunction-Is-Control-Flow">
          <h2>
            <a href="#Conjunction-Is-Control-Flow">Conjunction Is Control Flow
            </a>
          </h2>
          <p>
            Zig doesn‚Äôt use <code>&amp;&amp;</code> and <code>||</code> and
            spells the relevant operators as <code>and</code> and <code>or</code>:
          </p>

          <figure>
            <pre><code><span><span>while</span> (count &gt; <span>0</span> <span>and</span> ascii.isWhitespace(buffer[count <span>-</span> <span>1</span>])) {</span></code></pre>
          </figure>
          <p>
            This is easier to type and much easier to read, but there‚Äôs also a
            deeper reason why they are not sigils. Zig marks any control flow
            with a keyword. And, because boolean operators short-circuit, they
            <em>are</em> control flow! Treating them as normal binary operator
            leads to an entirely incorrect mental model. For bitwise operations,
            Zig of course uses <code>&amp;</code> and <code>|</code>.
          </p>
        </section>
        <section id="Explicit-return">
          <h2>
            <a href="#Explicit-return">Explicit return </a>
          </h2>
          <p>
            Both Zig and Rust have statements and expressions. Zig is a bit more
            statement oriented, and requires explicit returns:
          </p>

          <figure>
            <pre><code><span><span>fn</span><span> add</span>(x: <span>i32</span>, y: <span>i32</span>) <span>i32</span> {</span>
<span>  <span>return</span> x <span>+</span> y;</span>
<span>}</span></code></pre>
          </figure>
          <p>
            Furthermore, because there are no lambdas, scope of return is always
            clear.
          </p>
          <p>
            Relatedly, the value of a block expression is void. A block is a
            list of statements, and doesn‚Äôt have an optional expression at the
            end. This removes the semicolon problem ‚Äî while Rust rules around
            semicolons are sufficiently clear (until you get to macros), there‚Äôs
            some constant mental overhead to getting them right all the time.
            Zig is more uniform and mechanical here.
          </p>
          <p>
            If you need a block that yields a value, Zig supports a general
            syntax for breaking out of a labeled block:
          </p>

          <figure>
            <pre><code><span><span>const</span> header_oldest = blk: {</span>
<span>    <span>var</span> oldest: ?<span>usize</span> = <span>null</span>;</span>
<span>    <span>for</span> (headers.slice, <span>0</span>..) <span>|</span><span>*</span>header, i<span>|</span> {</span>
<span>        <span>switch</span> (Headers.dvc_header_type(header)) {</span>
<span>            .blank =&gt; assert(i &gt; <span>0</span>),</span>
<span>            .valid =&gt; oldest = i,</span>
<span>        }</span>
<span>    }</span>
<span>    <span>break</span> :blk <span>&amp;</span>headers.slice[oldest.?];</span>
<span>};</span></code></pre>
          </figure>
        </section>
        <section id="If">
          <h2>
            <a href="#If">If </a>
          </h2>
          <p>
            Rust makes pedantically correct choice regarding <code>if</code>s:
            braces are mandatory:
          </p>

          <figure>
            <pre><code><span><span>if</span> cond1 {</span>
<span>  case_a</span>
<span>} <span>else</span> {</span>
<span>  <span>if</span> cond2 {</span>
<span>    case_b</span>
<span>  } <span>else</span> {</span>
<span>    case_c</span>
<span>  }</span>
<span>}</span></code></pre>
          </figure>
          <p>
            This removes the dreaded ‚Äúdangling else‚Äù grammatical ambiguity.
            While theoretically nice, it makes
            <code>if</code>-expression one-line feel too heavy. It‚Äôs not the
            braces, it‚Äôs the whitespace around them:
          </p>

          <figure>
            <pre><code><span>if (a) b else c</span>
<span>if a { b } else { c }</span></code></pre>
          </figure>
          <p>
            But the ternary is important! Exploding a simple choice into
            multi-line condition <em>hurts</em>
            readability. Zig goes with traditional choice of making parentheses
            required and braces optional:
          </p>

          <figure>
            <pre><code><span>  .direction = <span>if</span> (prng.boolean()) .ascending <span>else</span> .descending,</span></code></pre>
          </figure>
          <p>
            By itself, this does create a risk of <code>goto: fail;</code> style
            bugs. But in Zig formatter (non-configurable, user-directed) is a
            part of the compiler, and formatting errors that can mask bugs are
            caught during compilation. For example, <code>1 -2</code> is an
            error due to inconsistent whitespace around the minus sign, which
            signals a plausible mixup of infix and binary minus. No such errors
            are currently produced for incorrect indentation (the value add
            there is relatively little, given <code>zig fmt</code>), but this is
            planned.
          </p>
          <p>
            NB: because Rust requires <code>if</code> branches to be blocks, it
            is forced to make <code>{ expr }</code> synonym with
            <code>(expr)</code>. Otherwise, the ternary <code>if</code> would be
            even more unusable! Syntax design is tricky! Whether you need <code>return</code>s and whether you make <code>()</code> or <code>{}</code> mandatory in ifs are not orthogonal!
          </p>
        </section>
        <section id="Loops">
          <h2>
            <a href="#Loops">Loops </a>
          </h2>
          <p>
            Like Python, Zig allows <code>else</code> on loops. Unlike Python,
            loops are expressions, which leads to a nicely readable imperative
            searches:
          </p>

          <figure>
            <pre><code><span><span>pub</span> <span>const</span> Word = <span>for</span> (.{ <span>u8</span>, <span>u16</span>, <span>u32</span>, <span>u64</span>, <span>u128</span>, <span>u256</span> }) <span>|</span>W<span>|</span> {</span>
<span>    <span>if</span> (<span>@bitSizeOf</span>(W) &gt;= bitset_capacity) <span>break</span> W;</span>
<span>} <span>else</span> <span>unreachable</span>;</span></code></pre>
          </figure>
          <p>
            Zig doesn‚Äôt have syntactically-infinite loop like Rust‚Äôs <code>loop
              {</code> or Go‚Äôs <code>for {</code>. Normally I‚Äôd consider that a
            drawback, because these loops produce different control flow,
            affecting reachability analysis in the compiler, and I don‚Äôt think
            it‚Äôs great to make reachability dependent on condition being visibly
            constant. But! As Zig places <code>comptime</code> semantics front
            and center, and the rules for what is and isn‚Äôt a comptime constant
            are a backbone of every feature, ‚Äúanything equivalent to
            <code>while (true)</code>‚Äù becomes sufficiently precise.
            Incidentally, these days I tend to write ‚Äúinfinite‚Äù loops as
          </p>

          <figure>
            <pre><code><span><span>for</span> (<span>0</span>..safety_bound) <span>|</span>_<span>|</span> {</span>
<span></span>
<span>} <span>else</span> <span>@panic</span>(<span>"loop safety counter exceeded"</span>);</span></code></pre>
          </figure>
          <p>
            Almost always there is an up-front bound for the number of
            iterations until the break, and its worth asserting this bound,
            because debugging crashes is easier than debugging hangs.
          </p>
          <p>
            <code>for</code>, <code>while</code>, <code>if</code>, <code>switch</code>, and <code>catch</code> all use the same Ruby/Rust
            inspired syntax for naming captured values:
          </p>

          <figure>
            <pre><code><span><span>for</span> (slice) <span>|</span>element<span>|</span> {</span>
<span>  use(element);</span>
<span>}</span>
<span></span>
<span><span>while</span> (iterator.next()) <span>|</span>element<span>|</span> {</span>
<span>  use(element);</span>
<span>}</span></code></pre>
          </figure>
          <p>
            I like how the iterator comes first, and then the name of an item
            follows, logically and syntactically.
          </p>
        </section>
        <section id="Clarity-of-Names">
          <h2>
            <a href="#Clarity-of-Names">Clarity of Names </a>
          </h2>
          <p>
            I have a very strong opinion about variable shadowing. It goes both
            ways: I spent hours debugging code which incorrectly tried to use a
            variable that was shadowed by something else, but I also spent hours
            debugging code that accidentally used a variable that should have
            been shadowed! I really don‚Äôt know whether on balance it is better
            to forbid or encourage shadowing!
          </p>
          <p>
            Zig of course forbids shadowing, but what‚Äôs curious is that it‚Äôs
            just one episode of the large crusade against any complexity in name
            resolution. There‚Äôs no ‚Äúprelude‚Äù, if you want to use anything from
            std, you need to import it:
          </p>

          <figure>
            <pre><code><span><span>const</span> std = <span>@import</span>(<span>"std"</span>);</span></code></pre>
          </figure>
          <p>
            There are no glob imports, if you want to use an item from std, you
            need to import it:
          </p>

          <figure>
            <pre><code><span><span>const</span> ArrayList = std.ArrayList;</span></code></pre>
          </figure>
          <p>
            Zig doesn‚Äôt have inheritance, mixins, argument-dependent lookup,
            extension functions, implicit or traits, so, if you see <code>x.foo()</code>, that <code>foo</code> is guaranteed to be a boring
            method declared on <code>x</code>
            type. Similarly, while Zig has powerful comptime capabilities, it
            <a href="https://matklad.github.io/2025/04/19/things-zig-comptime-wont-do.html">intentionally disallows</a>
            declaring methods at compile time.
          </p>
          <p>
            Like Rust, Zig used to allow a method and a field to share a name,
            because it actually is syntactically clear enough at the call site
            which is which. But then this feature got removed from Zig.
          </p>
          <p>
            More generally, Zig doesn‚Äôt have namespaces. There can be only one
            kind of <code>foo</code> in scope, while Rust allows things like
          </p>

          <figure>
            <pre><code><span><span>struct</span> <span>Point</span> { x: <span>i32</span>, y: <span>i32</span> }</span>
<span><span>fn</span> <span>Point</span>(x: <span>i32</span>, y: <span>i32</span>) <span>-&gt;</span> Point { Point { x, y } }</span></code></pre>
          </figure>
          <p>
            I am astonished at the relative lack of inconvenience in Zig‚Äôs
            approach. Turns out that <code>foo.bar.baz</code>
            is all the syntax you‚Äôll ever need for accessing things? For the
            historically inclined, see ‚ÄúThe module naming situation‚Äù thread in
            the
            <a href="https://github.com/brson/rust-dev-archives">rust mailing list archive</a>
            to learn the story of how rust got its <code>std::vec</code> syntax.
          </p>
        </section>
        <section id="Everything-Is-an-Expression">
          <h2>
            <a href="#Everything-Is-an-Expression">Everything Is an Expression
            </a>
          </h2>
          <p>
            The lack of namespaces touches on the most notable (by its absence)
            feature of Zig syntax, which deeply relates to the most profound
            aspect of Zig‚Äôs semantics. Everything is an expression. By which I
            mean, there‚Äôs no separate syntactic categories of values, types, and
            patterns. Values, types, and patterns are of course different
            things. And usually in the language grammar it is <em>syntactically</em>
            obvious whether a particular text fragment refers to a type or a
            value:
          </p>

          <figure>
            <pre><code><span><span>let</span> <span>PATTERN</span>: TYPE = VALUE;</span></code></pre>
          </figure>
          <p>
            So the standard way is to have separate syntax families for the
            three categories, which need to be internally unambiguous, but <em>can</em> be ambiguous across the categories because the place in
            the grammar dictates the category: when parsing <code>let</code>,
            everything until <code>:</code> is a pattern, stuff between
            <code>:</code> and <code>=</code> is a type, and after <code>=</code> we have a value.
          </p>
          <p>
            There are two problems here. First, there‚Äôs a combinatorial
            explosion of sorts in the syntax, because, while three categories
            describe different things, it turns out that they have the same
            general tree-ish shape.
          </p>
          <p>
            The second problem is that it might be hard to maintain category
            separation in the grammar. Rust
            <em>started</em> with the three categories separated by a bright
            line. But then, changes happen. Originally, Rust only allowed
            <span><code>VALUE = VALUE;</code></span>
            syntax for assignment. But today you can also write
            <span><code>PATTERN = VALUE;</code></span>
            to do unpacking like
            <span><code>(a, b) = (b, a);</code></span>
          </p>
          <p>
            Similarly, the turbofish used to move the parser from the value to
            the type mode, but now const parameters are values that can be found
            in the type position!
          </p>
          <p>
            The alternative is not to pick this fight at all. Rather than trying
            to keep the categories separately in the syntax, use the same
            surface syntax to express all three, and categorize later, during
            semantic analysis. In fact, this is already happens in the <span><code>VALUE = VALUE</code></span>
            example ‚Äî these are different things! One is a place (lvalue) and
            another is a ‚Äútrue‚Äù value (rvalue), but we use the same syntax for
            both.
          </p>
          <p>
            I don‚Äôt think such syntactic unification necessarily implies
            semantic unification, but Zig does treat everything uniformly, as a
            value with comptime and runtime behavior (for some values, runtime
            behavior may be missing, for others ‚Äî comptime):
          </p>

          <figure>
            <pre><code><span><span>const</span> E = <span>enum</span> { a, b };</span>
<span></span>
<span><span>pub</span> <span>fn</span><span> main</span>() <span>void</span> {</span>
<span>    <span>const</span> e: <span>if</span> (<span>true</span>) E <span>else</span> <span>void</span> = .a;</span>
<span>    _ = <span>switch</span> (e) {</span>
<span>        (<span>if</span> (<span>true</span>) .a <span>else</span> .b) =&gt; .a,</span>
<span>        (<span>if</span> (<span>true</span>) .b <span>else</span> .a) =&gt; .b,</span>
<span>    };</span>
<span>}</span></code></pre>
          </figure>
          <p>
            The fact that you can write an <code>if</code> where a type goes is
            occasionally useful. But the fact that simple types look like simple
            values syntactically consistently make the language feel
            significantly less busy.
          </p>
        </section>
        <section id="Generics">
          <h2>
            <a href="#Generics">Generics </a>
          </h2>
          <p>
            As a special case of everything being an expression, instances of
            generic types look like this:
            <span><code>ArrayList(u32)</code></span>
          </p>
          <p>
            Just a function call! Though, there‚Äôs some resistance to trickery
            involved to make this work. Usually, languages rely on type
            inference to allow eliding generic arguments. That in turn requires
            making argument <em>syntax</em> optional, and that in turn leads to
            separating generic and non-generic arguments into separate parameter
            lists and some introducer sigil for generics, like <code>::&lt;&gt;</code> or
            <code>!()</code>.
          </p>
          <p>
            Zig solves this syntactic challenge in the most brute-force way
            possible. Generic parameters are never inferred, if a function takes
            3 comptime arguments and 2 runtime arguments, it will always be
            called with 5 arguments syntactically. Like with the (absence of)
            importing flourishes, a reasonable reaction would be ‚Äúwait, does
            this mean that I‚Äôll have to specify the types all the time?‚Äù And,
            like with import, in practice this is a non-issue. The trick are
            comptime closures. Consider a generic
            <code>ArrayList</code>:
          </p>

          <figure>
            <pre><code><span><span>fn</span><span> ArrayListType</span>(<span>comptime</span> T: <span>type</span>) <span>type</span> {</span>
<span>    <span>return</span> <span>struct</span> {</span>
<span>        <span>const</span> ArrayList = <span>@This</span>();</span>
<span></span>
<span>        <span>fn</span><span> init</span>(gpa: Allocator) ArrayList {}</span>
<span>        <span>fn</span><span> deinit</span>(list: <span>*</span>ArrayList, gpa: Allocator) <span>void</span> {}</span>
<span>        <span>fn</span><span> push</span>(list: <span>*</span>ArrayList, item: T) <span>!</span><span>void</span> {}</span>
<span>    };</span>
<span>}</span>
<span></span>
<span><span>fn</span><span> usage</span>(gpa: Allocator) <span>!</span><span>void</span> {</span>
<span>    <span>var</span> xs: ArrayListType(<span>u32</span>) = .init(gpa);</span>
<span>    <span>defer</span> xs.deinit(gpa);</span>
<span></span>
<span>    <span>try</span> xs.push(<span>92</span>);</span>
<span>}</span></code></pre>
          </figure>
          <p>
            We have to specify type <code>T</code> when creating an instance of
            an <code>ArrayList</code>. But subsequently, when we are <em>using</em> the array list, we don‚Äôt have to specify the type
            parameter again, because the type of
            <code>xs</code> variable already closes over <code>T</code>. This is
            the major truth of object-orienting programming, the truth so
            profound that no one even notices it: in real code, 90% of functions
            are happiest as (non-virtual) methods. And, because of that, the
            annotation burden in real-world Zig programs is low.
          </p>
        </section>
        <section id="Declaration-Literals">
          <h2>
            <a href="#Declaration-Literals">Declaration Literals </a>
          </h2>
          <p>
            While Zig doesn‚Äôt have Hindley-Milner constraint-based type
            inference, it relies heavily on one specific way to propagate types.
            Let‚Äôs revisit the first <code>comptime_int</code> example:
          </p>

          <figure>
            <pre><code><span><span>const</span> x = <span>if</span> (condition()) <span>1</span> <span>else</span> <span>2</span>;</span></code></pre>
          </figure>
          <p>
            This doesn‚Äôt compile: <code>1</code> and <code>2</code> are
            different <code>comptime</code> values, we can‚Äôt select between two
            at runtime because they are different. We need to coerce the
            constants to a specific runtime type:
          </p>

          <figure>
            <pre><code><span><span>const</span> x: <span>u32</span> = <span>if</span> (condition()) <span>1</span> <span>else</span> <span>2</span>;</span>
<span></span>
<span><span>const</span> x = <span>@coerceTo</span>(</span>
<span>  <span>u32</span>,</span>
<span>  <span>if</span> (condition()) <span>1</span> <span>else</span> <span>2</span>,</span>
<span>);</span></code></pre>
          </figure>
          <p>
            But this doesn‚Äôt kick the can sufficiently far enough and
            essentially reproduces the <code>if</code> with two incompatible
            branches. We need to sink coercion down the branches:
          </p>

          <figure>
            <pre><code><span><span>const</span> x = <span>if</span> (condition())</span>
<span>    <span>@coerceTo</span>(<span>u32</span>, <span>1</span>)</span>
<span><span>else</span></span>
<span>    <span>@coerceTo</span>(<span>u32</span>, <span>2</span>);</span></code></pre>
          </figure>
          <p>
            And that‚Äôs exactly how Zig‚Äôs ‚ÄúResult Location Semantics‚Äù works. Type
            ‚Äúinference‚Äù runs a simple left-to-right tree-walking algorithm,
            which resembles interpreter‚Äôs <code>eval</code>. In fact, <code>eval</code> is
            <em>exactly</em> what happens. Zig is not a compiler, it is an
            interpreter. When <code>zig</code> evaluates an expression, it gets:
          </p>
          <ul>
            <li>
              expression‚Äôs type (as a Zig value),
            </li>
            <li>
              expression‚Äôs value (if it can be evaluated at comptime),
            </li>
            <li>
              code to compute expression‚Äôs value otherwise.
            </li>
          </ul>

          <figure>
            <pre><code><span>eval("1 + 2") =</span>
<span>  3</span>
<span></span>
<span>eval("f() + g()") =</span>
<span>  $1 = call 'f'</span>
<span>  $2 = call 'g'</span>
<span>  $3 = add $1, $2</span>
<span></span>
<span>eval("f() + 2") =</span>
<span>  $1 = call 'f'</span>
<span>  $2 = add $1,  imm 2</span></code></pre>
          </figure>
          <p>When interpreting code like</p>

          <figure>
            <pre><code><span>obj.field = if (condition()) 1 else 2;</span></code></pre>
          </figure>
          <p>
            the interpreter passes the result location (<code>obj.field</code>)
            and type down the tree of subexpressions. If branches store result
            directly into object field (there‚Äôs a <code>store</code> inside each
            branch, as opposed to one <code>store</code> after the <code>if</code>), and each coerces its comptime constant to the
            appropriate runtime type of the result.
          </p>
          <p>
            This mechanism enables concise <code>.variant</code> syntax for
            specifying enums:
          </p>

          <figure>
            <pre><code><span><span>const</span> E = <span>enum</span> { a, b };</span>
<span></span>
<span><span>fn</span><span> example</span>(e: E) <span>u32</span> {</span>
<span>    <span>return</span> <span>switch</span> (e) {</span>
<span>        .a =&gt; <span>1</span>,</span>
<span>        (<span>if</span> (<span>true</span>) .b <span>else</span> .a) =&gt; <span>2</span>,</span>
<span>    };</span>
<span>}</span></code></pre>
          </figure>
          <p>
            When <code>zig</code> evaluates the switch, it first evaluates the
            scrutinee, and realizes that it has type
            <code>E</code>. When evaluating <code>switch</code> arm, it sets
            result type to <code>E</code> for the condition, and a literal <code>.a</code>
            gets coerced to <code>E</code>. The same happens for the second arm,
            where result type further sinks down the
            <code>if</code>.
          </p>
          <p>
            Result type semantics also explains the leading dot in the record
            literal syntax:
          </p>

          <figure>
            <pre><code><span><span>const</span> p: Point = .{</span>
<span>    .x = <span>1</span>,</span>
<span>    .y = <span>2</span>,</span>
<span>};</span></code></pre>
          </figure>
          <p>
            Syntactically, we just want to disambiguate records from blocks.
            But, semantically, we want to coerce the literal to whatever type we
            want to get out of this expression. In Zig, <code>.whatever</code>
            is a shorthand for <code>@ResultType().whatever</code>.
          </p>
          <p>
            I must confess that <code>.{}</code> did weird me out a lot at first
            during <em>writing</em> code (I don‚Äôt mind reading the dot). It‚Äôs
            not the easiest thing to type! But that was fixed once I added <code>..</code> snippet, expanding to <code>.{$0}</code>.
          </p>
          <p>
            The benefits to lightweight record literal syntax are huge, as they
            allow for some pretty nice APIs. In particular, you get named and
            default arguments for free:
          </p>

          <figure>
            <pre><code><span><span>fn</span><span> exec</span>(argv: []<span>const</span> <span>u8</span>, options: <span>struct</span> {</span>
<span>    working_directory: ?[]<span>const</span> <span>u8</span> = <span>null</span></span>
<span>}) <span>!</span><span>void</span> {</span>
<span>    <span>// ...</span></span>
<span>}</span>
<span></span>
<span><span>fn</span><span> usage</span>() <span>!</span><span>void</span> {</span>
<span>    <span>try</span> exec(<span>&amp;</span>.{ <span>"git"</span>, <span>"status"</span>}, .{});</span>
<span></span>
<span>    <span>try</span> exec(<span>&amp;</span>.{ <span>"git"</span>, <span>"status"</span>}, .{</span>
<span>        .working_directory = <span>"./src"</span>,</span>
<span>    });</span>
<span>}</span></code></pre>
          </figure>
          <p>
            I don‚Äôt really miss the absence of named arguments in Rust, you can
            always design APIs without them. But they are free in Zig, so I use
            them liberally. Syntax wise, we get two features (calling functions
            and initializing objects) for the price of one!
          </p>
        </section>
        <section id="Built-ins">
          <h2>
            <a href="#Built-ins">Built-ins </a>
          </h2>
          <p>
            Finally, the thing that weirds out some people when they see Zig
            code, and makes others reconsider their choice GitHub handles, even
            when they haven‚Äôt seen any Zig: <code>@divExact</code> syntax for
            built-in functions.
          </p>
          <p>
            Every language needs to glue ‚Äúuserspace‚Äù code with primitive
            operations supported by the compiler. Usually, the gluing is
            achieved by making the standard library privileged and allowing it
            to define intrinsic functions without bodies, or by adding ad-hoc
            operators directly to the language (like Rust‚Äôs <code>as</code>).
            And Zig does have a fair amount of operators, like <code>+</code> or
            <code>orelse</code>. But the release valve for a lot of
            functionality are built-in functions in distinct syntactic
            namespace, so Zig separates out <code>@bitCast</code>, <code>@addrSpaceCast</code>, <code>@alignCast</code>, <code>@constCast</code>, <code>@ptrCast</code>, <code>@intCast</code>,
            <code>@floatCast</code>, <code>@volatileCast</code>, <code>@ptrFromInt</code>, and <code>@intFromPtr</code>. There‚Äôs no need
            to overload casting when you can give each variant a name.
          </p>
          <p>
            There‚Äôs also <span><code>@as(i32, 92)</code></span>
            for type ascription. The types goes first, because the mechanism
            here is result type semantics: <code>@as</code> evaluates the first
            argument as a type, and then uses that as the type for the second
            argument. Curiously, <code>@as</code> I think actually can be
            implemented in the userspace:
          </p>

          <figure>
            <pre><code><span><span>fn</span><span> as</span>(<span>comptime</span> T: <span>type</span>, value: T) T {</span>
<span>    <span>return</span> value;</span>
<span>}</span></code></pre>
          </figure>
          <p>
            In Zig, a type of function parameter may depend on values of
            preceding (comptime) ones!
          </p>
          <p>
            My favorite builtin is <code>@import()</code>. First, it‚Äôs the most
            obvious way to import code:
            <span><code>const foo =
                @import("./foo.zig")</code></span>
            Its crystal clear where the file comes from.
          </p>
          <p>
            But, second, it is an instance of reverse syntax sugar. You see,
            import isn‚Äôt really a function. You can‚Äôt do
          </p>

          <figure>
            <pre><code><span><span>const</span> name = <span>"./foo.zig"</span>;</span>
<span><span>const</span> foo = <span>@import</span>(name);</span></code></pre>
          </figure>
          <p>
            The argument of <code>@import</code> has to be a string,
            syntactically. It really is
            <span><code>import "./path.zig"</code></span>
            syntax, except that the function-call form is re-used, because it
            already has the right shape.
          </p>
          <hr>
          <p>
            So, this is it. Just a bunch of silly syntactical decisions, which
            add up to a language which is positively enjoyable to read. As for
            big lessons, obviously, the less features your language has, the
            less syntax you‚Äôll need. And less syntax is generally good, because
            varied syntactic constructs tend to step on each other toes.
            Languages are not combinations of orthogonal aspects. Features tug
            and pull the language in different directions and their combinations
            might turn to be miraculous features in their own right, or might
            drag the language down.
          </p>
          <p>
            Even with a small feature-set fixed, there‚Äôs still a lot of work to
            pick a good concrete syntax: unambiguous to parse, useful to grep,
            easy to read and not to painful to write. A smart thing is of course
            to steal and borrow solutions from other languages, not because of
            familiarity, but because the ruthless natural selection tends to
            weed out poor ideas. But there‚Äôs a lot of inertia in languages, so
            there‚Äôs no need to fear innovation. If an odd-looking syntax is
            actually good, people will take to it.
          </p>
          <p>
            Is there anything about Zig‚Äôs syntax I don‚Äôt like? I thought no,
            when starting this post. But in the process of writing it I did
            discover one form that annoys me. It is the while with the increment
            loop:
          </p>

          <figure>
            <pre><code><span><span>var</span> i: <span>u32</span> = <span>0</span>;</span>
<span><span>while</span> (i &lt; <span>10</span>) : (i<span>+=</span><span>1</span>) {</span>
<span>    print(<span>"{d}"</span>, .{i});</span>
<span>}</span></code></pre>
          </figure>
          <p>
            This is two-thirds of a C-style <code>for</code> loop (without the
            declarator), and it sucks for the same reason: control flow jumps
            all other the place and is unrelated to the source code order. We go
            from condition, to the body, to the increment. But in the source
            order the increment is between the condition and the body. In Zig,
            this loop sucks for one additional reason: that <code>:</code>
            separating the increment I think is the single example of control
            flow in Zig that is expressed by a sigil, rather than a keyword.
          </p>
          <p>
            This form used to be rather important, as Zig lacked a counting
            loop. It has
            <span><code>for(0..10) |i|</code></span>
            form now, so I am tempted to call the while-with-increment
            redundant.
          </p>
          <p>Annoyingly,</p>

          <figure>
            <pre><code><span><span>while</span> (condition) {</span>
<span>    <span>defer</span> increment;</span>
<span></span>
<span>    body</span>
<span>}</span></code></pre>
          </figure>
          <p>is <em>almost</em> equivalent to</p>

          <figure>
            <pre><code><span><span>while</span> (condition) : (increment) {</span>
<span>  body</span>
<span>}</span></code></pre>
          </figure>
          <p>
            But not exactly: if <code>body</code> contains a <code>return</code>, <code>break</code> or <code>try</code>, the <code>defer</code> version would run the
            <code>increment</code> one extra time, which is useless and might be
            outright buggy. Oh well.
          </p>
        </section>
      </article>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GPT-OSS vs. Qwen3 and a detailed look how things evolved since GPT-2 (175 pts)]]></title>
            <link>https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the</link>
            <guid>44855690</guid>
            <pubDate>Sun, 10 Aug 2025 15:06:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the">https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the</a>, See on <a href="https://news.ycombinator.com/item?id=44855690">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>OpenAI just released their new open-weight LLMs this week: gpt-oss-120b and gpt-oss-20b, their first open-weight models since GPT-2 in 2019. And yes, thanks to some clever optimizations, they can run locally (but more about this later).</p><p>This is the first time since GPT-2 that OpenAI has shared a large, fully open-weight model. Earlier GPT models showed how the transformer architecture scales. The 2022 ChatGPT release then made these models mainstream by demonstrating concrete usefulness for writing and knowledge (and later coding) tasks. Now they have shared some long-awaited weight model, and the architecture has some interesting details.</p><p>I spent the past few days reading through the code and technical reports to summarize the most interesting details. (Just days after, OpenAI also announced GPT-5, which I will briefly discuss in the context of the gpt-oss models at the end of this article.)</p><p>Below is a quick preview of what the article covers. For easier navigation, I recommend using the Table of Contents on the left of on the article page.</p><ul><li><p>Model architecture comparisons with GPT-2</p></li><li><p>MXFP4 optimization to fit gpt-oss models onto single GPUs</p></li><li><p>Width versus depth trade-offs (gpt-oss vs Qwen3)</p></li><li><p>Attention bias and sinks</p></li><li><p>Benchmarks and comparisons with GPT-5</p></li></ul><p>I hope you find it informative!</p><p>Before we discuss the architecture in more detail, let's start with an overview of the two models, gpt-oss-20b and gpt-oss-120b, shown in Figure 1 below.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!rlW0!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!rlW0!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 424w, https://substackcdn.com/image/fetch/$s_!rlW0!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 848w, https://substackcdn.com/image/fetch/$s_!rlW0!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 1272w, https://substackcdn.com/image/fetch/$s_!rlW0!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!rlW0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png" width="1456" height="681" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:681,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:243817,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!rlW0!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 424w, https://substackcdn.com/image/fetch/$s_!rlW0!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 848w, https://substackcdn.com/image/fetch/$s_!rlW0!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 1272w, https://substackcdn.com/image/fetch/$s_!rlW0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>Figure 1: The two gpt-oss models side by side.</figcaption></figure></div><p><span>If you have looked at recent LLM architecture diagrams before, or read my previous </span><a href="https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison" rel="">Big Architecture Comparison</a><span> article, you may notice that there is nothing novel or unusual at first glance. </span></p><div data-component-name="DigestPostEmbed"><a href="https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison" rel="noopener" target="_blank"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!83ox!,w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd72e5a99-1a11-42b7-8831-8f5785ed2bc1_1600x1116.png"><img src="https://substackcdn.com/image/fetch/$s_!83ox!,w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd72e5a99-1a11-42b7-8831-8f5785ed2bc1_1600x1116.png" sizes="100vw" alt="The Big LLM Architecture Comparison" width="140" height="140"></picture></div></a></div><p>This is not surprising, since leading LLM developers tend to use the same base architecture and then apply smaller tweaks. This is pure speculation on my part, but I think this is because</p><ul><li><p>There is significant rotation of employees between these labs.</p></li><li><p><span>We still have not found anything better than the transformer architecture. Even though state space models and text diffusion models exist, as far as I know no one has shown that they perform as well as transformers at this scale. (Most of the comparisons I found focus only on benchmark performance. It is still unclear how well the models handle real-world, multi-turn writing and coding tasks. At the time of writing, the highest-ranking non-purely-transformer-based model on the </span><a href="https://lmarena.ai/leaderboard/text" rel="">LM Arena</a><span> is Jamba, which is a transformer‚Äìstate space model hybrid, at rank 96.)</span></p></li><li><p>Most of the gains likely come from data and algorithm tweaks rather than from major architecture changes.</p></li></ul><p>That being said, there are still many interesting aspects of their design choices. Some are shown in the figure above (while others are not, but we will discuss them later as well). In the rest of this article, I will highlight these features and compare them to other architectures, one at a time.</p><p>I should also note that I am not affiliated with OpenAI in any way. My information comes from reviewing the released model code and reading their technical reports. If you want to learn how to use these models locally, the best place to start is OpenAI's official model hub pages:</p><ul><li><p><a href="https://huggingface.co/openai/gpt-oss-20b" rel="">https://huggingface.co/openai/gpt-oss-20b</a></p></li><li><p><a href="https://huggingface.co/openai/gpt-oss-120b" rel="">https://huggingface.co/openai/gpt-oss-120b</a></p></li></ul><p>The 20B model can run on a consumer GPU with up to 16 GB of RAM. The 120B model can run on a single H100 with 80 GB of RAM or newer hardware. I will return to this later, as there are some important caveats.</p><p>Before we jump into comparisons between gpt-oss and a more recent architecture, let's hop into the time machine and take a side-by-side look at GPT-2 (Figure 2) to see just how far things have come.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!AsnD!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!AsnD!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 424w, https://substackcdn.com/image/fetch/$s_!AsnD!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 848w, https://substackcdn.com/image/fetch/$s_!AsnD!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 1272w, https://substackcdn.com/image/fetch/$s_!AsnD!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!AsnD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png" width="1456" height="788" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:788,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:267271,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!AsnD!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 424w, https://substackcdn.com/image/fetch/$s_!AsnD!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 848w, https://substackcdn.com/image/fetch/$s_!AsnD!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 1272w, https://substackcdn.com/image/fetch/$s_!AsnD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 2: A side-by-side comparison between gpt-oss-20b and GPT-2 XL 1.5B.</figcaption></figure></div><p><span>Both gpt-oss and GPT-2 are decoder-only LLMs built on the transformer architecture introduced in the </span><a href="https://arxiv.org/abs/1706.03762" rel="">Attention Is All You Need (2017)</a><span> paper. Over the years, many details have evolved.</span></p><p><span>However, these changes are not unique to gpt-oss. And as we will see later, they appear in many other LLMs. Since I discussed many of these aspects in the previous </span><a href="https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison" rel="">Big Architecture Comparison</a><span> article, I will try to keep each subsection brief and focused.</span></p><p><a href="https://arxiv.org/abs/1207.0580" rel="">Dropout (2012)</a><span> is a traditional technique to prevent overfitting by randomly "dropping out" (i.e., setting to zero) a fraction of the layer activations or attention scores (Figure 3) during training. However, dropout is rarely used in modern LLMs, and most models after GPT-2 have dropped it (no pun intended).</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!BS-w!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!BS-w!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 424w, https://substackcdn.com/image/fetch/$s_!BS-w!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 848w, https://substackcdn.com/image/fetch/$s_!BS-w!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 1272w, https://substackcdn.com/image/fetch/$s_!BS-w!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!BS-w!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png" width="554" height="557.2781065088758" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:850,&quot;width&quot;:845,&quot;resizeWidth&quot;:554,&quot;bytes&quot;:130475,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!BS-w!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 424w, https://substackcdn.com/image/fetch/$s_!BS-w!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 848w, https://substackcdn.com/image/fetch/$s_!BS-w!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 1272w, https://substackcdn.com/image/fetch/$s_!BS-w!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 3: An illustration of dropout applied to the attention score matrix.</figcaption></figure></div><p>I assume that dropout was originally used in GPT-2 because it was inherited from the original transformer architecture. Researchers likely noticed that it does not really improve LLM performance (I observed the same in my small-scale GPT-2 replication runs). This is likely because LLMs are typically trained for only a single epoch over massive datasets, which is in contrast to the multi-hundred-epoch training regimes for which dropout was first introduced. So, since LLMs see each token only once during training, there is little risk of overfitting.</p><p><span>Interestingly, while Dropout is kind of ignored in LLM architecture design for many years, I found a </span><a href="https://arxiv.org/abs/2505.24788" rel="">2025 research paper</a><span> with small scale LLM experiments (Pythia 1.4B) that confirms that Dropout results in worse downstream performance in these single-epoch regimes.</span></p><p>In transformer-based LLMs, positional encoding is necessary because of the attention mechanism. By default, attention treats the input tokens as if they have no order. In the original GPT architecture, absolute positional embeddings addressed this by adding a learned embedding vector for each position in the sequence (Figure 4), which is then added to the token embeddings.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!YCov!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!YCov!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 424w, https://substackcdn.com/image/fetch/$s_!YCov!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 848w, https://substackcdn.com/image/fetch/$s_!YCov!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 1272w, https://substackcdn.com/image/fetch/$s_!YCov!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!YCov!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png" width="1195" height="533" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:533,&quot;width&quot;:1195,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:123823,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!YCov!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 424w, https://substackcdn.com/image/fetch/$s_!YCov!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 848w, https://substackcdn.com/image/fetch/$s_!YCov!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 1272w, https://substackcdn.com/image/fetch/$s_!YCov!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 4: Illustration of absolute positional embeddings.</figcaption></figure></div><p><span>RoPE (</span><a href="https://arxiv.org/abs/2104.09864" rel="">Rotary Position Embedding</a><span>) introduced a different approach: instead of adding position information as separate embeddings, it encodes position by rotating the query and key vectors in a way that depends on each token's position. (RoPE is an elegant idea but also a bit of a tricky topic to explain. I plan to cover separately in more detail one day.)</span></p><p>While first introduced in 2021, RoPE became widely adopted with the release of the original Llama model in 2023 and has since become a staple in modern LLMs.</p><p>Early GPT architectures used GELU. Why now use Swish over GELU? Swish is considered computationally slightly cheaper, and in my opinion, that all there is to it. Depending on which paper you look at, you will find that one is slightly better than the other in terms of modeling performance. In my opinion, these small differences are probably within a standard error, and your mileage will vary based on hyperparameter sensitivity.</p><p>Activation functions used to be a hot topic of debate until the deep learning community largely settled on ReLU more than a decade ago. Since then, researchers have proposed and tried many ReLU-like variants with smoother curves, and GELU and Swish (Figure 5) are the ones that stuck.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!WIz6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!WIz6!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 424w, https://substackcdn.com/image/fetch/$s_!WIz6!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 848w, https://substackcdn.com/image/fetch/$s_!WIz6!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 1272w, https://substackcdn.com/image/fetch/$s_!WIz6!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!WIz6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png" width="1407" height="775" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:775,&quot;width&quot;:1407,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:237022,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!WIz6!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 424w, https://substackcdn.com/image/fetch/$s_!WIz6!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 848w, https://substackcdn.com/image/fetch/$s_!WIz6!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 1272w, https://substackcdn.com/image/fetch/$s_!WIz6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 5: Comparison between Swish and GELU activations, which are both smoother versions or ReLU.</figcaption></figure></div><p><span>Early GPT architectures used GELU, which is defined as </span><code>0.5x * [1 + erf(x / sqrt(2))]</code><span>. Here, </span><code>erf</code><span> (short for error function) is the integral of a Gaussian and it is computed using polynomial approximations of the Gaussian integral, which makes it more computationally expensive than simpler functions like the sigmoid used in Swish, where Swish is simply </span><code>x * sigmoid(x)</code><span>.</span></p><p>In practice, Swish is computationally slightly cheaper than GELU, and that's probably the main reason it replaced GELU in most newer models. Depending on which paper we look at, one might be somewhat better in terms of modeling performance. But I'd say these gains are often within standard error, and the winner will depend heavily on hyperparameter tuning.</p><p>Swish is used in most architectures today. However, GELU is not entirely forgotten; for example, Google's Gemma models still use GELU.</p><p><span>What's more notable, though, is that the feed forward module (a small multi-layer perceptron) is replaced by a gated "GLU" counterpart, where GLU stands for gated linear unit and was proposed in a </span><a href="https://arxiv.org/pdf/2002.05202" rel="">2020 paper</a><span>. Concretely, the 2 fully connected layers are replaced by 3 fully connected layers that are used as shown in Figure 6 below.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!8gzt!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!8gzt!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 424w, https://substackcdn.com/image/fetch/$s_!8gzt!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 848w, https://substackcdn.com/image/fetch/$s_!8gzt!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 1272w, https://substackcdn.com/image/fetch/$s_!8gzt!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!8gzt!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png" width="655" height="550.0696517412936" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:844,&quot;width&quot;:1005,&quot;resizeWidth&quot;:655,&quot;bytes&quot;:190423,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!8gzt!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 424w, https://substackcdn.com/image/fetch/$s_!8gzt!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 848w, https://substackcdn.com/image/fetch/$s_!8gzt!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 1272w, https://substackcdn.com/image/fetch/$s_!8gzt!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 6: A comparison between Swish and GELU and their gated counterparts, SwiGLU and GEGLU.</figcaption></figure></div><p><span>At first glance, it may appear that the GEGLU/SwiGLU variants may be better than the regular feed forward layers because there are simply more parameters due to the extra layer. But this is deceiving because in practice, the </span><code>W</code><span> and </span><code>V</code><span> weight layers in SwiGLU/GEGLU are usually chosen to be half the size each of the </span><code>W_1</code><span> layer in a traditional feed forward layer.</span></p><p>To illustrate this better, consider the concrete code implementations of the regular and GLU variants:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!_JVz!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!_JVz!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 424w, https://substackcdn.com/image/fetch/$s_!_JVz!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 848w, https://substackcdn.com/image/fetch/$s_!_JVz!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 1272w, https://substackcdn.com/image/fetch/$s_!_JVz!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!_JVz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png" width="687" height="513.1010587102984" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:776,&quot;width&quot;:1039,&quot;resizeWidth&quot;:687,&quot;bytes&quot;:267148,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!_JVz!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 424w, https://substackcdn.com/image/fetch/$s_!_JVz!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 848w, https://substackcdn.com/image/fetch/$s_!_JVz!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 1272w, https://substackcdn.com/image/fetch/$s_!_JVz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 7: Regular feed forward module (top) and SwiGLU variant (bottom) next to each other.</figcaption></figure></div><p>So, suppose we have an embedding dimension of 1024. In the regular feed forward case, this would then be</p><ul><li><p>fc1: 1024 √ó 4096 = 4,194,304</p></li><li><p>fc2: 1024 √ó 4096 = 4,194,304</p></li></ul><p>That is fc1 + fc2 = 8,388,608 parameters.</p><p>For the GLU variant, we have</p><ul><li><p>fc1: 1024 √ó 2048 = 2,097,152</p></li><li><p>fc2: 1024 √ó 2048 = 2,097,152</p></li><li><p>fc3: 2048 √ó 1024 = 2,097,152</p></li></ul><p>I.e., 3 √ó 2,097,152 = 6,291,456 weight parameters.</p><p>So, overall, using the GLU variants results in fewer parameters, and they perform better as well. The reason for this better performance is that these GLU variants provide an additional multiplicative interaction, which improves expressivity (the same reason deep &amp; slim neural nets perform better than shallow &amp; wide neural nets, provided they are trained well).</p><p>In addition to upgrading the feed forward module to a SwiGLU, as discussed in the previous section, gpt-oss replaces the single feed forward module with multiple feed forward modules, using only a subset for each token generation step. This approach is known as a Mixture-of-Experts (MoE) and illustrated in Figure 8 below.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!SYqb!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!SYqb!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 424w, https://substackcdn.com/image/fetch/$s_!SYqb!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 848w, https://substackcdn.com/image/fetch/$s_!SYqb!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 1272w, https://substackcdn.com/image/fetch/$s_!SYqb!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!SYqb!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png" width="1307" height="640" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:640,&quot;width&quot;:1307,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:120915,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!SYqb!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 424w, https://substackcdn.com/image/fetch/$s_!SYqb!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 848w, https://substackcdn.com/image/fetch/$s_!SYqb!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 1272w, https://substackcdn.com/image/fetch/$s_!SYqb!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 8: The feed forward module is replaced by a Mixture-of-Expert (MoE) module.</figcaption></figure></div><p><span>So, replacing </span><em>a single</em><span> feed forward module with </span><em>multiple</em><span> feed forward modules (as done in a MoE setup) substantially increases the model's total parameter count. However, the key trick is that we don't use ("activate") all experts for every token. Instead, a router selects only a small subset of experts per token.</span></p><p><span>Because only a few experts are active at a time, MoE modules are often referred to as </span><em>sparse</em><span>, in contrast to </span><em>dense</em><span> modules that always use the full parameter set. However, the large total number of parameters via an MoE increases the capacity of the LLM, which means it can take up more knowledge during training. The sparsity keeps inference efficient, though, as we don't use all the parameters at the same time.</span></p><p>(Fun fact: In most MoE models, expert weights account for more than 90% of the total model parameters.)</p><p>As mentioned in my previous articles, Grouped Query Attention (GQA) has emerged in recent years as a more compute- and parameter-efficient alternative to Multi-Head Attention (MHA).</p><p>In MHA, each head has its own set of keys and values. GQA reduces memory usage by grouping multiple heads to share the same key and value projections.</p><p>For example, as shown in Figure 9, if there are 2 key‚Äìvalue groups and 4 attention heads, heads 1 and 2 might share one set of keys and values, while heads 3 and 4 share another. This grouping decreases the total number of key and value computations, leading to lower memory usage and improved efficiency ‚Äî without noticeably affecting modeling performance, according to ablation studies.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Kohq!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Kohq!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 424w, https://substackcdn.com/image/fetch/$s_!Kohq!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 848w, https://substackcdn.com/image/fetch/$s_!Kohq!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 1272w, https://substackcdn.com/image/fetch/$s_!Kohq!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Kohq!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png" width="637" height="302.7938561034762" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:588,&quot;width&quot;:1237,&quot;resizeWidth&quot;:637,&quot;bytes&quot;:83420,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Kohq!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 424w, https://substackcdn.com/image/fetch/$s_!Kohq!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 848w, https://substackcdn.com/image/fetch/$s_!Kohq!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 1272w, https://substackcdn.com/image/fetch/$s_!Kohq!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 9: A comparison between MHA and GQA. Here, the group size is 2, where a key and value pair is shared among 2 queries.</figcaption></figure></div><p>So, the core idea behind GQA is to reduce the number of key and value heads by sharing them across multiple query heads. This (1) lowers the model's parameter count and (2) reduces the memory bandwidth usage for key and value tensors during inference since fewer keys and values need to be stored and retrieved from the KV cache.</p><p><span>(If you are curious how GQA looks in code, see my</span><a href="https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/07_gpt_to_llama/converting-llama2-to-llama3.ipynb" rel=""> GPT-2 to Llama 3 conversion guide</a><span> for a version without KV cache and my KV-cache variant </span><a href="https://github.com/rasbt/LLMs-from-scratch/blob/main/pkg/llms_from_scratch/llama3.py" rel="">here</a><span>.)</span></p><p><span>While GQA is mainly a computational-efficiency workaround for MHA, ablation studies (such as those in the</span><a href="https://arxiv.org/abs/2305.13245" rel=""> original GQA paper</a><span> and the </span><a href="https://arxiv.org/abs/2307.09288" rel="">Llama 2 paper</a><span>) show it performs comparably to standard MHA in terms of LLM modeling performance.</span></p><p><span>Sliding-window attention (Figure 10 below) was first introduced in the </span><a href="https://arxiv.org/abs/2004.05150" rel="">LongFormer paper (2020)</a><span> and later popularized by Mistral. Interestingly, gpt-oss applies it in every second layer. You can think of it as a variation of multi-head attention, or in this case grouped query attention (GQA), where the attention context is restricted to a smaller window, reducing both memory usage and compute costs.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!wwFe!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!wwFe!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 424w, https://substackcdn.com/image/fetch/$s_!wwFe!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 848w, https://substackcdn.com/image/fetch/$s_!wwFe!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!wwFe!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!wwFe!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg" width="1456" height="721" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:721,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:225815,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!wwFe!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 424w, https://substackcdn.com/image/fetch/$s_!wwFe!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 848w, https://substackcdn.com/image/fetch/$s_!wwFe!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!wwFe!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 10: Comparison between regular attention (left) and sliding window attention (right).</figcaption></figure></div><p>Concretely, gpt-oss alternates between GQA layers that attend to the full context and GQA layers with a sliding window limited to 128 tokens.</p><p><span>As I discussed in my </span><a href="https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison" rel="">previous article</a><span>, </span><a href="https://arxiv.org/abs/2408.00118" rel="">Gemma 2 (2024)</a><span> used a similar 1:1 ratio. </span><a href="https://arxiv.org/abs/2503.19786" rel="">Gemma 3</a><span> earlier this year went much further and shifted to a 5:1 ratio, which means only one full-attention layer for every five sliding-window (local) attention layers.</span></p><p>According to the Gemma ablation studies, sliding-window attention has minimal impact on modeling performance, as shown in the figure below. Note that the window size in Gemma 2 was 4096 tokens, which Gemma 3 reduced to 1024. In gpt-oss, the window is just 128 tokens, which is remarkably small.</p><p><span>And as a fun fact, the </span><a href="https://openai.com/index/introducing-gpt-oss/" rel="">official announcement article</a><span> notes that sliding-window attention was apparently already used in GPT-3:</span></p><blockquote><p>The models use alternating dense and locally banded sparse attention patterns, similar to GPT-3</p></blockquote><p><span>Who knew!? I went back to the original </span><a href="https://arxiv.org/abs/2005.14165" rel="">GPT-3 paper</a><span>, and it was indeed mentioned there:</span></p><blockquote><p>We use the same model and architecture as GPT-2 [ RWC+19 ], including the modified initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer [ CGRS19 ]. </p></blockquote><p><span>Finally, the last small tweak, coming from GPT-2, is replacing </span><a href="https://arxiv.org/abs/1607.06450" rel="">LayerNorm (2016)</a><span> by </span><a href="https://arxiv.org/abs/1910.07467" rel="">RMSNorm (2019)</a><span>, which has been a common trend in recent years.</span></p><p>Akin to swapping GELU with Swish and SwiGLU, RMSNorm is one of these smaller but sensible efficiency improvements. RMSNorm is similar to LayerNorm in its purpose to normalize layer activations, as shown in Figure 11 below.</p><p>You might recall that not too long ago, BatchNorm was the go-to choice for this task. It has since fallen out of favor, largely because it is harder to parallelize efficiently (due to the mean and variance batch statistics) and performs poorly with small batch sizes.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!H32R!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!H32R!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 424w, https://substackcdn.com/image/fetch/$s_!H32R!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 848w, https://substackcdn.com/image/fetch/$s_!H32R!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 1272w, https://substackcdn.com/image/fetch/$s_!H32R!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!H32R!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png" width="1367" height="599" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:599,&quot;width&quot;:1367,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:274255,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!H32R!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 424w, https://substackcdn.com/image/fetch/$s_!H32R!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 848w, https://substackcdn.com/image/fetch/$s_!H32R!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 1272w, https://substackcdn.com/image/fetch/$s_!H32R!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 11: A comparison between LayerNorm (left) and RMSNorm (right) for a small linear layer.</figcaption></figure></div><p>As we can see in Figure 11 above, both LayerNorm and RMSNorm scale the layer outputs to be in a reasonable range.</p><p>LayerNorm subtracts the mean and divides by the standard deviation such that the layer outputs have a zero mean and unit variance (variance of 1 and standard deviation of one).</p><p>RMSNorm divides the inputs by the root-mean-square. This doesn't force zero mean and unit variance, but the mean and variance are in a reasonable range: -1 to 1 for the mean and 0 to 1 for the variance. In this particular example shown in Figure 11, the mean is 0.77 and the variance is 0.41.</p><p>Both LayerNorm and RMSNorm stabilize activation scales and improve optimization, but RMSNorm is often preferred in large-scale LLMs because it is cheaper to compute. Unlike LayerNorm, RMSNorm has no bias (shift) term and reduces the expensive mean and variance computations to a single root-mean-square operation. This reduces the number of cross-feature reductions from two to one, which lowers communication overhead on GPUs and improving training efficiency.</p><p>Figure 12 shows what this looks like in code:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!m5aM!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!m5aM!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 424w, https://substackcdn.com/image/fetch/$s_!m5aM!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 848w, https://substackcdn.com/image/fetch/$s_!m5aM!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 1272w, https://substackcdn.com/image/fetch/$s_!m5aM!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!m5aM!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png" width="589" height="442.23068552774754" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/de968991-5068-40d9-87bb-98b887f5f384_919x690.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:690,&quot;width&quot;:919,&quot;resizeWidth&quot;:589,&quot;bytes&quot;:259430,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!m5aM!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 424w, https://substackcdn.com/image/fetch/$s_!m5aM!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 848w, https://substackcdn.com/image/fetch/$s_!m5aM!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 1272w, https://substackcdn.com/image/fetch/$s_!m5aM!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 12: Code implementations of LayerNorm and RMSNorm showing that RMSNorm is computationally simpler.</figcaption></figure></div><p>I still think that GPT-2 is an excellent beginner architecture when learning about LLMs. It's simple enough to understand without getting lost in layers of optimization tricks, but still complex enough to give you a solid grasp of how modern transformer models work.</p><p>By starting with GPT-2, you can focus on the fundamentals (attention mechanisms, positional embeddings, normalization, and the overall training pipeline) without being overwhelmed by the extra features and tweaks found in newer architectures.</p><p>In fact, I think it's worth the time to learn about and even implement GPT-2 first before trying to stack newer changes on top. You will not only have an easier time understanding those changes, but you will likely also appreciate them more, because you will get a better understanding of what limitations or problems they try to solve.</p><p><span>For instance, starting with my GPT-2 code I recently implemented the </span><a href="https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/11_qwen3" rel="">Qwen3 architecture from scratch</a><span>, which is super similar to gpt-oss, which brings us to the next topic: Comparing gpt-oss to a more recent architecture.</span></p><p>Now that we have walked through the evolution from GPT-2 to GPT OSS, we can take the next step and compare GPT OSS to a more recent architecture, Qwen3, which was released three months earlier in May 2025.</p><p>The reason I am selecting Qwen3 here is that it is among the top open-weight models as of the time of writing. Additionally, one of the Qwen3 MoE models is more or less directly comparable to GPT OSS due to its relatively similar overall size in terms of trainable parameters.</p><p>Figure 13 below compares gpt-oss-20b to a Qwen3 model of comparable size.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!5K75!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!5K75!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 424w, https://substackcdn.com/image/fetch/$s_!5K75!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 848w, https://substackcdn.com/image/fetch/$s_!5K75!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 1272w, https://substackcdn.com/image/fetch/$s_!5K75!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!5K75!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png" width="1456" height="741" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:741,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:268927,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!5K75!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 424w, https://substackcdn.com/image/fetch/$s_!5K75!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 848w, https://substackcdn.com/image/fetch/$s_!5K75!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 1272w, https://substackcdn.com/image/fetch/$s_!5K75!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 13: A gpt-oss and Qwen3 model of comparable size side by side.</figcaption></figure></div><p>As we can see, gpt-oss 20B and Qwen3 30B-A3B are very similar in their architecture components. The primary difference here, aside from the dimensions, is that gpt-oss employs sliding window attention, as discussed earlier in section 1.6 (not shown in this figure), whereas Qwen3 does not.</p><p>Let's walk through the noteworthy details one by one in the following subsections.</p><p>If we look at the two models closely, we see that Qwen3 is a much deeper architecture with its 48 transformer blocks instead of 24 (Figure 14).</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!G1hj!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!G1hj!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 424w, https://substackcdn.com/image/fetch/$s_!G1hj!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 848w, https://substackcdn.com/image/fetch/$s_!G1hj!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 1272w, https://substackcdn.com/image/fetch/$s_!G1hj!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!G1hj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png" width="1456" height="696" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:696,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:307435,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!G1hj!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 424w, https://substackcdn.com/image/fetch/$s_!G1hj!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 848w, https://substackcdn.com/image/fetch/$s_!G1hj!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 1272w, https://substackcdn.com/image/fetch/$s_!G1hj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 14: Qwen3 has twice as many transformer blocks as gpt-oss-20b.</figcaption></figure></div><p>On the other hand, gpt-oss is a much wider architecture:</p><ul><li><p>An embedding dimension of 2880 instead of 2048</p></li><li><p>An intermediate expert (feed forward) projection dimension of 5760 instead of 768</p></li></ul><p>It's also worth noting that gpt-oss uses twice as many attention heads, but this doesn't directly increase the model's width. The width is determined by the embedding dimension.</p><p>Does one approach offer advantages over the other given a fixed number of parameters? As a rule of thumb, deeper models have more flexibility but can be harder to train due to instability issues, due to exploding and vanishing gradients (which RMSNorm and shortcut connections aim to mitigate).</p><p>Wider architectures have the advantage of being faster during inference (with a higher tokens/second throughput) due to better parallelization at a higher memory cost.</p><p><span>When it comes to modeling performance, there's unfortunately no good apples-to-apples comparison I am aware of (where parameter size and datasets are kept constant) except for an ablation study in the </span><a href="https://arxiv.org/abs/2408.00118" rel="">Gemma 2 paper (Table 9)</a><span>, which found that for a 9B parameter architecture, a wider setup is slightly better than a deeper setup. Across 4 benchmarks, the wider model achieved a 52.0 average score, and the deeper model achieved a 50.8 average score.</span></p><p>As shown in Figure 14 above, it's also noteworthy that gpt-oss has a surprisingly small number of experts (32 instead of 128), and only uses 4 instead of 8 active experts per token. However, each expert is much larger than the experts in Qwen3.</p><p>This is interesting because the recent trends and developments point towards more, smaller models as being beneficial. This change, at a constant total parameter size, is nicely illustrated in Figure 15 below from the DeepSeekMoE paper.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!qYc3!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!qYc3!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 424w, https://substackcdn.com/image/fetch/$s_!qYc3!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 848w, https://substackcdn.com/image/fetch/$s_!qYc3!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 1272w, https://substackcdn.com/image/fetch/$s_!qYc3!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!qYc3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png" width="1131" height="609" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:609,&quot;width&quot;:1131,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:219481,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!qYc3!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 424w, https://substackcdn.com/image/fetch/$s_!qYc3!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 848w, https://substackcdn.com/image/fetch/$s_!qYc3!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 1272w, https://substackcdn.com/image/fetch/$s_!qYc3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Figure 15: An annotated figure from "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models", </span><a href="https://arxiv.org/abs/2401.06066" rel="">https://arxiv.org/abs/2401.06066</a></figcaption></figure></div><p>Notably, unlike DeepSeek's models, neither gpt-oss nor Qwen3 uses shared experts, though.</p><p>To be fair, the small number of experts in gpt-oss could be a side effect of the 20B size. Looking at the 120B mode below, they indeed increased the number of experts (and transformer blocks) while keeping everything else fixed, as shown in Figure 16 below.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!w8-R!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!w8-R!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 424w, https://substackcdn.com/image/fetch/$s_!w8-R!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 848w, https://substackcdn.com/image/fetch/$s_!w8-R!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 1272w, https://substackcdn.com/image/fetch/$s_!w8-R!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!w8-R!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png" width="1456" height="726" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:726,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:291088,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!w8-R!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 424w, https://substackcdn.com/image/fetch/$s_!w8-R!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 848w, https://substackcdn.com/image/fetch/$s_!w8-R!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 1272w, https://substackcdn.com/image/fetch/$s_!w8-R!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 16: The two gpt-oss architectures side by side, where the larger 120B model only scales the number of transformer blocks and number of experts.</figcaption></figure></div><p>The boring explanation for the fact that the 20B and 120B models are so similar is probably that the 120B model was the main focus. And the easiest way to create a smaller model was to make it a bit shorter (fewer transformer blocks) and to reduce the number of experts, because that's where most of the parameters are. However, one might speculate whether they started training the 120B model, and then chopped some of the transformer blocks and experts for continued pre-training (instead of starting from random weights).</p><p>In any case, it's because it's quite unusual to only scale those two (transformer blocks and number of experts). For instance, when looking at Qwen3 MoE models of multiple sizes (Figure 17 below), they were scaled more proportionally to each other over many more aspects..</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!0h6T!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!0h6T!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 424w, https://substackcdn.com/image/fetch/$s_!0h6T!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 848w, https://substackcdn.com/image/fetch/$s_!0h6T!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 1272w, https://substackcdn.com/image/fetch/$s_!0h6T!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!0h6T!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png" width="1120" height="903" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:903,&quot;width&quot;:1120,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:210100,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!0h6T!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 424w, https://substackcdn.com/image/fetch/$s_!0h6T!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 848w, https://substackcdn.com/image/fetch/$s_!0h6T!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 1272w, https://substackcdn.com/image/fetch/$s_!0h6T!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 17: Architecture differences in the various Qwen3 models.</figcaption></figure></div><p>Both gpt-oss and Qwen3 use grouped query attention. The main difference is that gpt-oss restricts the context size via sliding window attention in each second layer, as mentioned earlier.</p><p>However, there's one interesting detail that caught my eye. It seems that gpt-oss uses bias units for the attention weights, as shown in the figure below.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!U3bl!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!U3bl!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 424w, https://substackcdn.com/image/fetch/$s_!U3bl!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 848w, https://substackcdn.com/image/fetch/$s_!U3bl!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 1272w, https://substackcdn.com/image/fetch/$s_!U3bl!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!U3bl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png" width="1456" height="441" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:441,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:176606,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!U3bl!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 424w, https://substackcdn.com/image/fetch/$s_!U3bl!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 848w, https://substackcdn.com/image/fetch/$s_!U3bl!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 1272w, https://substackcdn.com/image/fetch/$s_!U3bl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Figure 18: gpt-oss models use bias units in the attention layers. See code example </span><a href="https://github.com/huggingface/transformers/blob/369c99d0cea403b77bd0aef818527106453fd9fc/src/transformers/models/gpt_oss/modular_gpt_oss.py#L228-L243" rel="">here</a><span>.</span></figcaption></figure></div><p><span>I haven't seen these bias units being used since the GPT-2 days, and they are commonly regarded as redundant. Indeed, I found a </span><a href="https://arxiv.org/abs/2302.08626" rel="">recent paper</a><span> that shows mathematically that this is at least true for the key transformation (k_proj). Furthermore, the empirical results show that there is little difference between with and without bias units (see Figure 19 below).</span></p><p><span>Another detail you may have noticed is the definition of </span><code>sinks</code><span> in the code screenshot in Figure 18. In general models, attention sinks are special "always-attended" tokens placed at the start of the sequence to stabilize attention, which is especially useful in long-context scenarios. I.e., if the context gets very long, this special attended token at the beginning is still attended to, and it can learn to store some generally useful information about the entire sequence. (I think it was originally proposed in the </span><a href="https://arxiv.org/abs/2309.17453" rel="">Efficient Streaming Language Models with Attention Sinks</a><span> paper.)</span></p><p><span>In the gpt-oss implementation, </span><em>attention sinks</em><span> are not actual tokens in the input sequence. Instead, they are learned per-head bias logits that are appended to the attention scores (Figure 20). The goal is the same as with the above-mentioned attention sinks, but without modifying the tokenized inputs.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Qwo6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Qwo6!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 424w, https://substackcdn.com/image/fetch/$s_!Qwo6!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 848w, https://substackcdn.com/image/fetch/$s_!Qwo6!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 1272w, https://substackcdn.com/image/fetch/$s_!Qwo6!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Qwo6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png" width="988" height="684" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:684,&quot;width&quot;:988,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:202184,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Qwo6!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 424w, https://substackcdn.com/image/fetch/$s_!Qwo6!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 848w, https://substackcdn.com/image/fetch/$s_!Qwo6!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 1272w, https://substackcdn.com/image/fetch/$s_!Qwo6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Figure 20: The use of attention sinks in gpt-oss; based on the Hugging Face code </span><a href="https://github.com/huggingface/transformers/blame/369c99d0cea403b77bd0aef818527106453fd9fc/src/transformers/models/gpt_oss/modular_gpt_oss.py" rel="">here</a><span>.</span></figcaption></figure></div><p>Lastly, and similar to Qwen3, the gpt-oss models are Apache 2.0 open-source license, which is great (it's the same license that I prefer for my own open-source projects). This means that the models can be distilled into other models or used in commercial products without restriction.</p><p><strong>Open-weight vs. open-source LLMs.</strong><span> This distinction has been debated for years, but it is worth clarifying to avoid confusion about this release and its artifacts. Some model developers release only the model weights and inference code (for example, Llama, Gemma, gpt-oss), while others (for example, OLMo) release everything including training code, datasets, and weights as true open source.</span></p><p><span>By that stricter definition, gpt-oss is an </span><em>open-weight</em><span> model (just like Qwen3) because it includes the weights and inference code but not the training code or datasets. However, the terminology is used inconsistently across the industry.</span></p><p><span>I assume the "oss" in "gpt-oss" stands for </span><em>open source software</em><span>; however, I am positively surprised that OpenAI itself clearly describes gpt-oss as an open-weight model in their official </span><a href="https://openai.com/index/introducing-gpt-oss/" rel="">announcement article</a><span>.</span></p><p>While the previous sections described how the architecture has evolved since GPT-2 and discussed its similarities to Qwen3 (and most other recent models), there are still a few additional but noteworthy details I have not mentioned, yet. These are points that did not fit neatly into the earlier sections but are still worth mentioning.</p><p><span>Unfortunately, there is not much information about the training set sizes and algorithms available. I added the most interesting puzzle pieces from the </span><a href="https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf" rel="">model card report</a><span> (1) and </span><a href="https://openai.com/index/introducing-gpt-oss/" rel="">announcement post</a><span> (2) below:</span></p><blockquote><p>The gpt-oss models were trained using our most advanced pre-training and post-training techniques [...] (1)</p><p>[...] required 2.1million H100-hours to complete, with gpt-oss-20b needing almost 10x fewer. (1)</p><p>[...] including a supervised fine-tuning stage and a high-compute RL stage [...] (2)</p><p>We trained the models on a mostly English, text-only dataset, with a focus on STEM, coding, and general knowledge. (2)</p></blockquote><p><span>So, we know that the gpt-oss models are reasoning models. The training compute of 2.1 million H100 GPU hours is roughly on par with the 2.788 million H800 GPU hours that the ~5.6x larger </span><a href="https://arxiv.org/abs/2412.19437" rel="">DeepSeek V3</a><span> model was trained for. Unfortunately, there is no information about the Qwen3 training time available yet.</span></p><p>Interestingly, the GPT-oss training hour estimate includes both the supervised learning for instruction following and the reinforcement learning for reasoning, whereas DeepSeek V3 is just a pre-trained base model on top of which DeepSeek R1 was trained separately.</p><p>As mentioned in the previous section, the gpt-oss models are reasoning models. However, what's particularly interesting is that they were trained so that users can easily control the degree of reasoning via inference time scaling.</p><p>Concretely, gpt-oss models can receive "Reasoning effort: low/medium/high" instructions as part of their system prompt, which directly affects the response length and accuracy, as shown in Figure 21.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!LsLL!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!LsLL!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 424w, https://substackcdn.com/image/fetch/$s_!LsLL!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 848w, https://substackcdn.com/image/fetch/$s_!LsLL!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 1272w, https://substackcdn.com/image/fetch/$s_!LsLL!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!LsLL!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png" width="1219" height="548" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:548,&quot;width&quot;:1219,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:175317,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!LsLL!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 424w, https://substackcdn.com/image/fetch/$s_!LsLL!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 848w, https://substackcdn.com/image/fetch/$s_!LsLL!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 1272w, https://substackcdn.com/image/fetch/$s_!LsLL!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Figure 21: Response length and quality of gpt-oss models under different reasoning efforts (annotated figure from the </span><a href="https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf" rel="">model card</a><span>)</span></figcaption></figure></div><p>This level of adjustability is useful because it lets us balance cost, compute, and accuracy. For example, if the task is simple, such as answering a straightforward knowledge question or fixing a small typo, we can skip extended reasoning. This saves time and resources while avoiding unnecessarily long responses and verbose reasoning traces.</p><p>It is somewhat unfortunate that OpenAI did not release the base models prior to reinforcement learning-based reasoning training, unlike Qwen3 or OLMo. Base models are particularly valuable starting points for researchers working on reasoning methods (which is one reason I currently like working with Qwen3 Base). My guess is that OpenAI's decision was driven more by industry and production use cases than by research considerations.</p><p><span>Note that the original Qwen3 models also have a toggle for enabling/disabling thinking (reasoning) modes (via a </span><code>enable_thinking=True/False</code><span> setting in the tokenizer that simply adds &lt;think&gt;&lt;/think&gt; tags to disable the reasoning behavior). However, the Qwen3 team updated their models in the last few weeks and moved away from the hybrid model towards dedicated Instruct/Thinking/Coder variants.</span></p><p>The reason was that the hybrid mode resulted in lower performance compared to the individual models:</p><blockquote><p><span>After discussing with the community and reflecting on the matter, we have decided to abandon the hybrid thinking mode. We will now train the Instruct and Thinking models separately to achieve the best possible quality. </span><a href="https://www.actuia.com/en/news/alibaba-launches-qwen3-235b-a22b-instruct-2507-and-breaks-away-from-hybrid-reasoning/?utm_source=chatgpt.com" rel="">Source</a></p></blockquote><p>One interesting surprise is that OpenAI released the gpt-oss models with an MXFP4 quantization scheme for the MoE experts.</p><p>Quantization formats used to be a niche topic, mostly relevant to mobile or embedded AI, but that's changed with the push toward bigger models. In this case, the MXFP4 optimization allows the model to run on single GPU devices.</p><p>Here‚Äôs what that looks like in practice:</p><ul><li><p>The large model (think 120B) fits on a single 80GB H100 or newer GPU. Not consumer hardware, but hey, it's much cheaper to rent a 1-H100 machine than a multi-H100 machine. Plus, we don't have to worry about distributing the model across GPUs and adding communication overhead. It's really nice that AMD MI300X cards are supported from day 1 as well!</p></li><li><p>The smaller 20B model even fits into 16 GB of VRAM; the caveat is that it has to be a RTX 50-series GPU or newer to support MXFP4.</p></li></ul><p>Note that the models will also run on older hardware but without MXFP4 support and will thus consume more RAM. Without MXFP4 optimization, the models in bfloat16 will consume more like 48 GB (gpt-oss-20b) and 240 GB (gpt-oss-120b).</p><p>By the way, I can run the gpt-oss-20b model comfortably on my Mac Mini using ollama. It uses about 13.5 Gb or memory, which is really reasonable.</p><p><span>The models are still a bit too new for independent benchmarks. Checking the </span><a href="https://lmarena.ai/leaderboard" rel="">LM Arena leaderboard</a><span>, I found that gpt-oss is not listed, yet. So, Qwen3-Instruct remains the top open-weight model, according to users on the LM Arena, for now (Figure 22).</span></p><p>Looking at a reasoning benchmarks provide in the gpt-oss announcement post, we can see that the gpt-ossmodels are on par with OpenAI's proprietary models as well as Qwen3 (Figure 23).</p><p>However, this should be caveated by the fact that gpt-oss-120b is almost half the size of the Qwen3 A235B-A22B-Thinking-2507 model and can run on a single GPU.</p><p>Benchmark performance, however, does not always reflect real-world usability. In my limited use over the past few days, I have found gpt-oss to be quite capable. That said, as others have observed, it does seem to have a relatively high tendency to hallucinate (a point also mentioned in its model card).</p><p>This may stem from its heavy training focus on reasoning tasks such as math, puzzles, and code, which could have led to some "general knowledge forgetting." Still, because gpt-oss was designed with tool use in mind, this limitation may become less relevant over time. Tool integration in open-source LLMs is still in its early stages, but as it matures, I expect that we increasingly let models consult external sources (like search engines) when answering factual or knowledge-based queries.</p><p>If that happens, it could be sensible to prioritize reasoning capacity over memorization. This is much like in human learning in school (or in life in general), where problem-solving skills often matter more than memorizing facts.</p><p>OpenAI had a busy week and released the long-awaited GPT-5 model shortly after gpt-oss. The GPT-5 release was interesting. And if there's one thing I have to say here, it's that I am really surprised by how good their open-source models really are compared to their best product offering in terms of benchmark performance (Figure 24).</p><p>All in all, even though some people called the release overhyped, I am glad that we have a new set of really strong open weight models that are not too far behind the best proprietary ones. Of course, benchmarks often do not accurately reflect real-world use, and it is still too early to tell based on the limited usage. But I think these are good times for people who like to work with open-weight and local (or privately hosted) models.</p><p><em>This magazine is a personal passion project, and your support helps keep it alive. If you would like to contribute, there are a few great ways:</em></p><ul><li><p><em><strong><a href="https://amzn.to/4fqvn0D" rel="">Grab a copy of my book</a></strong><span>. Build a Large Language Model (From Scratch) walks you through building an LLM step by step, from tokenizer to training.</span></em></p></li></ul><ul><li><p><em><strong><a href="https://www.manning.com/livevideo/master-and-build-large-language-models" rel="">Check out the video course</a></strong><span>. There‚Äôs now a 17-hour video course based on the book, available from Manning. It follows the book closely, section by section, and works well both as a standalone or as a code-along resource. The video course is ad-free (unlike the YouTube version) and has a cleaner, more structured format. It also contains 5 additional hours of pre-requisite video material created by Abhinav Kimothi.</span></em></p></li></ul><ul><li><p><em><strong><a href="https://magazine.sebastianraschka.com/subscribe" rel="">Subscribe</a></strong><span>. A paid subscription helps to make my writing sustainable and gives you access to additional contents.</span></em></p></li></ul><p><em>Thanks for reading, and for helping support independent research!</em></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!wVLk!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!wVLk!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 424w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 848w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!wVLk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg" width="1456" height="878" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:878,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/$s_!wVLk!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 424w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 848w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Engineering.fyi ‚Äì Search across tech engineering blogs in one place (219 pts)]]></title>
            <link>https://engineering.fyi/</link>
            <guid>44855157</guid>
            <pubDate>Sun, 10 Aug 2025 13:44:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://engineering.fyi/">https://engineering.fyi/</a>, See on <a href="https://news.ycombinator.com/item?id=44855157">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div data-slot="card"><p>How Airbnb upgrades tens of thousands of pods on dozens of Kubernetes clusters to new Istio versions</p></div><div data-slot="card"><p>MCP UI extends the Model Context Protocol to enable AI agents to return fully interactive UI components. It solves the critical challenge that commerce experiences require visual and interactive elements like product selectors, image galleries, and cart flows. This open-source protocol allows agents to embed commerce components while maintaining control through an intent-based messaging system, delivering shopping experiences that go far beyond traditional text-only AI interactions.</p></div><div data-slot="card"><p>What if you could control any device using only subtle hand movements? New research from Meta‚Äôs Reality Labs is pointing even more firmly toward wrist-worn devices using surface electromyography (s‚Ä¶</p></div><div data-slot="card"><div data-slot="card-header"><p>The Google Developer Program is rolling out major updates to make its tools and community more accessible and powerful. These enhancements include a new flexible monthly subscription tier, a centralized GDP Forum for collaboration, and increased Gemini CLI access for all members.</p></div><div data-slot="card-content"><p><span>Chris Demeke, Kevin Flores</span></p></div></div><div data-slot="card"><p>Working Together to Accelerate AI Adoption</p></div><div data-slot="card"><p>FlashList v2 is a complete rewrite, delivering faster load times, improved scrolling performance, and precise rendering without requiring item size estimates. It powers thousands of lists in the Shopify mobile app and is now production-ready.</p></div><div data-slot="card"><p>Google introduces Veo 3 Fast, an optimized model for speed and price, along with new image-to-video capabilities for both Veo 3 and Veo 3 Fast, enabling developers to efficiently create high-quality video content from text or still images, with varying pricing based on the model and audio inclusion, now available in the Gemini API.</p></div><div data-slot="card"><div data-slot="card-header"><p>The Gemini Embedding model enhances AI applications, particularly through context engineering, which is being successfully adopted by various organizations across industries to power context-aware systems, leading to significant improvements in performance, accuracy, and efficiency.</p></div><div data-slot="card-content"><p><span>Vishal Dharmadhikari, Janie Zhang</span></p></div></div><div data-slot="card"><div data-slot="card-header"><p>LangExtract is a new open-source Python library powered by Gemini models for extracting structured information from unstructured text, offering precise source grounding, reliable structured outputs using controlled generation, optimized long-context extraction, interactive visualization, and flexible LLM backend support.</p></div><div data-slot="card-content"><p><span>Akshay Goel, Atilla Kiraly</span></p></div></div><div data-slot="card"><div data-slot="card-header"><p>Max's journey introduces LQRax, a JAX-native LQR solver, which exemplifies the growing JAX robotics ecosystem that includes tools like Brax, MJX, and JaxSim, highlighting the benefits of JAX for computational efficiency in optimal control and simulation, and for seamlessly integrating model-based and learning-based approaches.</p></div><div data-slot="card-content"><p><span>Srikanth Kilaru, Max Muchen Sun</span></p></div></div><div data-slot="card"><div data-slot="card-header"><p>ExecuTorch is the PyTorch inference framework for edge devices developed by Meta with support from industry leaders like Arm, Apple, and Qualcomm.&nbsp; Running machine learning (ML) models on-device is‚Ä¶</p></div><div data-slot="card-content"><p><span>PyTorch Edge Team in collaboration with Family of Apps</span></p></div></div><div data-slot="card"><p>How to achieve high availability with distributed databases on Kubernetes</p></div><div data-slot="card"><div data-slot="card-header"><p>Co-hosted by Ashley Oldacre and Christina Warren, People of AI podcast's Season 5 will focus on the builders in the space of AI, highlighting the unique journeys, challenges, and triumphs of these innovators.</p></div><div data-slot="card-content"><p><span>Ashley Oldacre, Christina Warren</span></p></div></div><div data-slot="card"><div data-slot="card-header"><p>Opal is a new experimental tool from Google Labs that helps you compose prompts into dynamic, multi-step mini-apps using natural language, removing the need for code, allowing users to build and deploy shareable AI apps with powerful features and seamless integration with existing Google tools.</p></div><div data-slot="card-content"><p><span>Ali Modarres, Bill Byrne, Paul Lewis</span></p></div></div><div data-slot="card"><div data-slot="card-header"><p>Apigee helps enterprises integrate large language models (LLMs) into existing API ecosystems securely and scalably, addressing challenges like authentication and authorization not fully covered by the evolving Model Context Protocol (MCP), and offering an open-source MCP server example that demonstrates how to implement enterprise-ready API security for AI agents.</p></div><div data-slot="card-content"><p><span>Antony Arul, Ruben Gonzalez</span></p></div></div><div data-slot="card"><p>Automatically review your PRs with Bugbot</p></div><div data-slot="card"><div data-slot="card-header"><p>New AI capabilities for popular frameworks in Firebase Studio include AI-optimized templates, streamlined integration with Firebase backend services, and the ability to fork workspaces for experimentation and collaboration, making AI-assisted app development more intuitive and faster for developers worldwide.</p></div><div data-slot="card-content"><p><span>Jeanine Banks, Vikas Anand</span></p></div></div><div data-slot="card"><p><span>Lavanya Verma, Ryan Hang, Sung Whang, Joseph Wang</span></p></div><div data-slot="card"><div data-slot="card-header"><p>Gemini 2.5 Flash-Lite, previously in preview, is now stable and generally available. This cost-efficient model is ~1.5x faster than 2.0 Flash-Lite and 2.0 Flash, offers high quality, and includes 2.5 family features like a 1 million-token context window and multimodality.</p></div><div data-slot="card-content"><p><span>Logan Kilpatrick, Zach Gleicher</span></p></div></div><div data-slot="card"><div data-slot="card-header"><p>Gemini's advanced capability for conversational image segmentation allows intuitive interaction with visual data by understanding complex phrases, conditional logic, and abstract concepts, streamlining developer experience and opening doors for new applications in media editing, safety monitoring, and damage assessment.</p></div><div data-slot="card-content"><p><span>Paul Voigtlaender, Valentin Gabeur, Rohan Doshi</span></p></div></div><div data-slot="card"><p><span>Austin Harrison, Eddie Huang, Spencer Garth, Tim Ross, Taya Yusuf</span></p></div><div data-slot="card"><p>ChatGPT now thinks and acts, proactively choosing from a toolbox of agentic skills to complete tasks for you using its own computer.</p></div><div data-slot="card"><div data-slot="card-header"><p>Veo 3, Google‚Äôs latest AI video generation model, is now available in paid preview via the Gemini API and Google AI Studio. Unveiled at Google I/O 2025, Veo 3 can generate both video and synchronized audio, including dialogue, background sounds, and even animal noises. This model delivers realistic visuals, natural lighting, and physics, with accurate lip syncing and sound that matches on-screen action.</p></div><div data-slot="card-content"><p><span>Alisa Fortin, Luciano Martins, Seth Odoom</span></p></div></div><div data-slot="card"><p>Meta has developed an open-source AI tool to design concrete mixes that are stronger, more sustainable, and ready to build with faster‚Äîspeeding up construction while reducing environmental impact. ‚Ä¶</p></div><div data-slot="card"><p>Shopify‚Äôs Global Catalogue demonstrates the impact of multimodal LLMs on one of commerce‚Äôs hardest problems: building a unified, structured, and continuously evolving understanding of billions of product listings created by millions of merchants.</p></div><div data-slot="card"><div data-slot="card-header"><p>The Marin project aims to expand the definition of 'open' in AI to include the entire scientific process, not just the model itself, by making the complete development journey accessible and reproducible. This effort, powered by the JAX framework and its Levanter tool, allows for deep scrutiny, trust in, and building upon foundation models, fostering a more transparent future for AI research.</p></div><div data-slot="card-content"><p><span>Srikanth Kilaru, David Hall</span></p></div></div><div data-slot="card"><div data-slot="card-header"><p>The updated Agent Development Kit (ADK) simplifies and accelerates the process of building AI agents by providing the CLI with a deep, cost-effective understanding of the ADK framework, allowing developers to quickly ideate, generate, test, and improve functional agents through conversational prompts, eliminating friction and keeping them in a productive "flow" state.</p></div><div data-slot="card-content"><p><span>Julia Wiesinger, Hangfei Lin</span></p></div></div><div data-slot="card"><p>The `logprobs` feature has been officially introduced in the Gemini API on Vertex AI, provides insight into the model's decision-making by showing probability scores for chosen and alternative tokens. This step-by-step guide will walk you through how to enable and interpret this feature and apply it to powerful use cases such as confident classification, dynamic autocomplete, and quantitative RAG evaluation.</p></div><div data-slot="card"><p>Microsoft‚Äôs AI-powered code review assistant has transformed pull request workflows by automating routine checks, suggesting improvements, and enabling conversational Q&amp;A, leading to faster PR completion, improved code quality, and enhanced developer onboarding.</p></div><div data-slot="card"><p>The Gemini Embedding text model is now generally available in the Gemini API and Vertex AI. This versatile model has consistently ranked #1 on the MTEB Multilingual leaderboard since its experimental launch in March, supports over 100 languages, has a 2048 maximum input token length, and is priced at $0.15 per 1M input tokens.</p></div><div data-slot="card"><div data-slot="card-header"><p>The Apigee API hub and Developer Portals are distinct but interconnected parts of the Apigee platform that help organizations discover and manage APIs for different personas, unlocking API potential and accelerating innovation.</p></div><div data-slot="card-content"><p><span>Venkat Sadras, David Rush</span></p></div></div><div data-slot="card"><p>Our new jurisdiction resolution system (JRS) is a faster, less resource-intensive solution to the challenging problem of determining tax obligations in places with complicated, overlapping tax jurisdictions.</p></div><div data-slot="card"><div data-slot="card-header"><p>GenAI Processors is a new open-source Python library from Google DeepMind designed to simplify the development of AI applications, especially those handling multimodal input and requiring real-time responsiveness, by providing a consistent "Processor" interface for all steps from input handling to model calls and output processing, for seamless chaining and concurrent execution.</p></div><div data-slot="card-content"><p><span>Andre Elisseeff, Alexey Guseynov, Oskar Bunyan, Shrestha Basu Mallick</span></p></div></div><div data-slot="card"><p>Updates in Firebase Studio include new Agent modes, foundational support for the Model Context Protocol (MCP), and Gemini CLI integration, all designed to redefine AI-assisted development allow developers to create full-stack applications from a single prompt and integrate powerful AI capabilities directly into their workflow.</p></div><div data-slot="card"><div data-slot="card-header"><p>T5Gemma is a new family of encoder-decoder LLMs developed by converting and adapting pretrained decoder-only models based on the Gemma 2 framework, offering superior performance and efficiency compared to its decoder-only counterparts, particularly for tasks requiring deep input understanding, like summarization and translation.</p></div><div data-slot="card-content"><p><span>Biao Zhang, Paul Suganthan, Ben Hora</span></p></div></div><div data-slot="card"><div data-slot="card-header"><p>The new batch mode in the Gemini API is designed for high-throughput, non-latency-critical AI workloads, simplifying large jobs by handling scheduling and processing, and making tasks like data analysis, bulk content creation, and model evaluation more cost-effective and scalable, so developers can process large volumes of data efficiently.</p></div><div data-slot="card-content"><p><span>Lucia Loher, Vishal Dharmadhikari</span></p></div></div><div data-slot="card"><p>Commerce is a dynamic ecosystem where our mission is to empower every merchant to succeed. We optimize each step of their journey‚Äîfrom product creation to customer delivery‚Äîusing advanced tools, infrastructure, and partnerships to solve a complex optimization challenge.</p></div><div data-slot="card"><p>How the new Pro plan works and why we changed our pricing.</p></div><div data-slot="card"><p><span>Prateek Jain, Soheil Sadeghi, Mehrdad Bakhtiari</span></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Try and (302 pts)]]></title>
            <link>https://ygdp.yale.edu/phenomena/try-and</link>
            <guid>44855079</guid>
            <pubDate>Sun, 10 Aug 2025 13:32:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ygdp.yale.edu/phenomena/try-and">https://ygdp.yale.edu/phenomena/try-and</a>, See on <a href="https://news.ycombinator.com/item?id=44855079">Hacker News</a></p>
<div id="readability-page-1" class="page"><div property="content:encoded"><p><strong><br>
		I'm gonna try and change the course of hip hop again.<br>
	</strong>
</p>
<p>
		(Dr. Dre)
	</p>

<p>
	Typically, <i>try</i> can be followed by three kinds of phrases: a noun phrase (1a), an infinitival verb phrase with <i>to</i> (1b), or a verb phrase with -<i>ing</i> (1c).
</p>
<blockquote><p>
		1)	a. I'll try the salad.
	</p>
<p id="indented">
		b. I'll try to eat this horrible salad.
	</p>
<p id="indented">
		c. I'll try adding vinegar to the salad, to improve the taste.
	</p>
</blockquote>
<p>
	However, <i>try</i> can also combine with the conjunction <i>and</i>, followed by a bare verb form:
</p>
<blockquote><p>
		2)	I‚Äôll try and eat the salad.
	</p>
</blockquote>
<p>
	This usage is very similar in meaning to <i>try to</i>, if not identical, but is deemed prescriptively incorrect (Routledge 1864:579 in D. Ross 2013a:120; Partridge 1947:338, Crews et al. 1989:656 in Brook &amp; Tagliamonte 2016:320). In the next few sections, we will see that it has a number of interesting properties.
</p>


<h2>Who says this?</h2>
<p>
	<i>Try and</i> is described as more prevalent in British English than American English, but is common in both varieties (Hommerberg &amp; Tottie 2007). Brook &amp; Tagliamonte (2016) show that Canadian speakers pattern with American speakers in their usage of the construction.
</p>
<p>
	<i>Try and</i> is not a recent innovation ‚Äì it first emerged in the late 1500s, although the earliest textual attestion is from 1390 (Tottie 2012, D. Ross 2013a). Tottie (2012) provides some examples of <i>try and</i> from EEBO-TCP corpus, including this one:
</p>
<blockquote><p>
		3) ...howe and by what certaine and generall rule I mighte <b>trye and</b> throughly discerne the veritie of the catholike faithe, from the falsehood of wicked heresye... (1554)<br>
		4) You maie (saide I) <b>trie and</b> bring him in, and shewe him to her. (1569)
	</p>
</blockquote>
<p>
	<i>Webster‚Äôs Dictionary</i> (1989:919) suggests that <i>try and</i> in fact predates <i>try to</i>, and this conclusion is supported by Hommerberg &amp; Tottie (2007:60), Tottie &amp; Hoffman (2011) and Tottie (2012). However, D. Ross (2013a) disputes this, saying that ‚Äú<i>[t]ry and</i> and <i>try to</i> developed simultaneously and independently‚Äù. What is clear is that <i>try and</i> has been around for at least as long as <i>try to</i>.
</p>

<h2>Syntactic Properties</h2>
<p>
	Carden &amp; Pesetsky (1977:86) note that <i>try and</i> does not behave like a regular case of coordination. </p>
<h3>Question words are allowed</h3>
<p>One property of ‚Äòtrue‚Äô coordination is that it is subject to the <i>Coordinate Structure Constraint</i> (J. Ross 1967), which states that a <i>wh</i>-word cannot move out of one of the conjuncts. This is shown in (5).
</p>
<blockquote><p>
		5)	a. Mary [met Bill and ignored Susie].
	</p>
<p id="indented">
		b. *Who did Mary [meet Bill and ignore __]?
	</p>
</blockquote>
<p>
	However, a <i>wh</i>-word can happily be moved out of a <i>try and structure:
</i></p>
<blockquote><p>
		6) Who did Mary [try and talk to __]?
	</p>
</blockquote>
<h3>No reordering</h3>
<p>
	A second property of pseudo-coordination that distinguishes it from regular coordination is that the two conjuncts cannot be reordered. In (6), we see that regular coordination permits the order of conjuncts to be changed, while in (7) we see that the same is not possible with <i>try and</i> (De Vos 2005:59).</p>
<blockquote><p>
		7)	a. John will wash the bathroom and kill mosquitos.
	</p>
<p id="indented">
		b.	John will kill mosquitos and wash the bathroom.
</p></blockquote>
<blockquote><p>
		8)	a. John will try and kill mosquitos.
	</p>
<p id="indented">
		b.	*John will kill mosquitos and try.
</p></blockquote>
<h3><em>Both</em> is not possible</h3>
<p>
	Another piece of evidence that <i>try and</i> is not regular coordination structure comes from the unavailability of <i>both</i>. Usually, coordinated verb phrases can be preceded by <i>both</i>:</p>
<blockquote><p>
		9)	<i>Reality is Broken</i> will both [stimulate your brain and stir your soul]. [<a href="https://janemcgonigal.com/my-book/">source</a>, February 28 2017]
	</p>
</blockquote>
<p>
However, De Vos (2005:59) points out that <i>try and</i> may not be preceded by <i>both</i>:
</p>
<blockquote><p>
		10)	a. John will try and kill mosquitos.
	</p>
<p id="indented">
		b. *John will both try and kill mosquitos.
	</p>
</blockquote>
<h3>Bare form only</h3>
<p>
Unlike with regular coordination, <i>try and</i> is available only when both <i>try</i> and the verb following <i>and</i> are uninflected, which means it must occur in its bare form. Carden &amp; Pesetsky (1977)  call this the <i>bare form condition</i>. The following examples are adapted from D. Ross (2013a:111):
</p>
<blockquote><p>
		11)	a. I will try and finish the assignment.
	</p>
<p id="indented">
		b. I try and finish an assignment every day.
	</p>
<p id="indented">
		c. *I tried and finish(ed) the assignment.
	</p>
<p id="indented">
		d. *He tries and finish(es) an assignment every day.
	</p>
<p id="indented">
		e. *It‚Äôs tough when you‚Äôre trying and finish(ing) an assignment under pressure.
	</p>
</blockquote>
<h3>Dialect variation in the Bare Form Condition</h3>
<p>
	Is the bare form condition universal? D. Ross (2013a:124-5) notes that it has weakened in some dialects, though not necessarily in the same way. In dialects of Northeastern Canada, parallel inflected forms are acceptable:
</p>
<blockquote><p>
		12)	They tries and does that.
	</p>
</blockquote>
<p>
	In South African English, on the other hand, <i>try</i> may be inflected while the second verb remains a bare form (examples from D. Ross 2013a:125):
</p>
<blockquote><p>
		13)	a. Noeleen tries and find answers and solutions. [<a href="http://www.tvsa.co.za/default.asp?blogname=coming_up_on_3Talk&amp;ArticleID=2903">source</a>, August 2006]
	</p>
<p id="indented">
		b. We‚Äôre trying and get across that nature is harsh but not necessarily full of malice and cruelty. (Dereck Joubert on ‚ÄúWild about Africa,‚Äù Carte Blanche: March 18, 2007)
	</p>
</blockquote>
<h3>No separation of <i>try</i> and <i>and</i></h3>
<p>
	There are some other restrictions on the distribution of try and. Unlike with <i>try to</i>, <i>try</i> may not be separated from <i>and</i> by an adverb (Webster‚Äôs Dictionary 1989:919):
</p>
<blockquote><p>
		14)	a. Try always to tell the truth.
	</p>
<p id="indented">
		b. *Try always and tell the truth.
	</p>
</blockquote>
<p>
	Similarly, <i>try</i> may not be separated from <i>and</i> by negation (Brook &amp; Tagliamonte 2016:308):
</p>
<blockquote><p>
		15)	a. You try not to let it bother you.
	</p>
<p id="indented">
		b. *You try not and let it bother you.
	</p>
</blockquote>
<h3>No ellipsis allowed</h3>
<p>
	<i>Try and</i> is incompatible with ellipsis of the following verb phrase (Brook &amp; Tagliamonte 2016):
</p>
<blockquote><p>
		16)	a. Sure, I'll try to.
	</p>
<p id="indented">
		b. *Sure, I'll try and.
</p></blockquote>

<h2>Other instances of pseudocoordination</h2>
<p>
	Infinitival <i>to</i> can be replaced by <i>and</i> in several other cases, subject to dialectal and individual variation. Brook &amp; Tagliamonte (2016:302) state that the best candidate for a verb phrase that behaves like <i>try</i> is <i>be sure</i>:
</p>
<blockquote><p>
		17)	Be sure and visit Harry tomorrow. (Carden &amp; Pesetsky 1977:84)
	</p>
</blockquote>
<p>
	D. Ross (2013a:122) provides several examples of other verb phrases in which infinitival <i>to</i> has been replaced with <i>and</i>:
</p>
<blockquote><p>
		18)	a. <b>Mind and</b> get all right for next Saturday. (Poutsma 1905:361)
	</p>
<p id="indented">
		b. You know I go to all these different schools and I <b>start and</b> get mixed up after a while. (Hopper 2002:162)
	</p>
<p id="indented">
		c. <b>Remember and</b> wash your hair. (BNC: KE4 636, 1992)
	</p>
</blockquote>
<p>
	Another instance of pseudocoordination is found with motion verbs, such as <i>come</i> and <i>go</i>:
</p>
<blockquote><p>
		19)	a. Can you come and pick me up from the station?
	</p>
<p id="indented">
		b. I‚Äôll go and get the mop.
	</p>
</blockquote>
<p>
	D. Ross (2013b) argues that motion verb pseudocoordination has a different syntax and semantics from <i>try and</i> pseudocoordination. Syntactically, we can see that motion verb pseudocoordination is <i>not</i> subject to the bare form condition:
</p>
<blockquote><p>
		20)	a. He came and <b>picked</b> me up from the station.
	</p>
<p id="indented">
		b. She goes and <b>gets</b> lunch every day at noon.
	</p>
</blockquote>
<p>
	Semantically, <i>go and</i> entails that the event was completed, so in (21) below it is strange to use <i>go and</i> if the book was not acquired. In contrast, its non-pseudocoordination equivalent <i>go to</i> does not have this entailment.
</p>
<blockquote><p>
		21)	The man will go to/*and buy the book, even if it is sold out.
	</p>
</blockquote>
<p><em>Page contributed by Matthew Tyler on Feb 23, 2018.</em></p>
<p><em>Updates/revisions: June 27, 2018 (Katie Martin)</em></p>
<p><b>Please cite this page as:</b> Tyler, Matthew. 2018. <em>Try and</em>. <i>Yale Grammatical Diversity Project: English in North America</i>. (Available online at <a href="http://ygdp.yale.edu/phenomena/try-and">http://ygdp.yale.edu/phenomena/try-and</a>. Accessed on YYYY-MM-DD). Updated by Katie Martin (2018).</p>
<h2>References</h2>
</div><div><p>Phenomenon Dialect:&nbsp;</p><div><p>Widespread American English</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MCP: An (Accidentally) Universal Plugin System (127 pts)]]></title>
            <link>https://worksonmymachine.ai/p/mcp-an-accidentally-universal-plugin</link>
            <guid>44854860</guid>
            <pubDate>Sun, 10 Aug 2025 12:53:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://worksonmymachine.ai/p/mcp-an-accidentally-universal-plugin">https://worksonmymachine.ai/p/mcp-an-accidentally-universal-plugin</a>, See on <a href="https://news.ycombinator.com/item?id=44854860">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>There's this thing about USB-C that nobody really talks about. Not the part where we all had to buy new dongles (RIP my dongle drawer, 2010-2023). The other part.</p><p>See, we all thought USB-C was just going to be about charging things and moving files around like the other USBs. Very serious. Very purposeful. But because of the way it is it can do... other things.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!kwfs!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01350f9f-9ecd-4cb2-a28d-103a9ccc9027_1024x1024.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!kwfs!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01350f9f-9ecd-4cb2-a28d-103a9ccc9027_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!kwfs!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01350f9f-9ecd-4cb2-a28d-103a9ccc9027_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!kwfs!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01350f9f-9ecd-4cb2-a28d-103a9ccc9027_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!kwfs!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01350f9f-9ecd-4cb2-a28d-103a9ccc9027_1024x1024.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!kwfs!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01350f9f-9ecd-4cb2-a28d-103a9ccc9027_1024x1024.png" width="1024" height="1024" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/01350f9f-9ecd-4cb2-a28d-103a9ccc9027_1024x1024.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1024,&quot;width&quot;:1024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1960400,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://worksonmymachine.substack.com/i/166947463?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01350f9f-9ecd-4cb2-a28d-103a9ccc9027_1024x1024.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!kwfs!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01350f9f-9ecd-4cb2-a28d-103a9ccc9027_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!kwfs!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01350f9f-9ecd-4cb2-a28d-103a9ccc9027_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!kwfs!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01350f9f-9ecd-4cb2-a28d-103a9ccc9027_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!kwfs!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01350f9f-9ecd-4cb2-a28d-103a9ccc9027_1024x1024.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>My friend Rex connected his toaster to his monitor last week. I don't know why. The toaster doesn't know why. But it </span><em>worked</em><span>, and now Rex's toast has HDMI output.</span></p><p>Remember car cigarette lighters? Nobody uses them for cigarettes anymore. They're just universal power outlets that happen to be shaped like something from 1952. Your car doesn't care if you're charging a phone or running a personal pizza oven. The hole is the same size. The power is there.</p><p><em>The protocol doesn't judge your life choices.</em></p><p>This brings me to something I discovered about MCP (Model Context Protocol) while trying to make my calendar app order takeout. Stay with me here.</p><p>Everyone thinks MCP is for making AI assistants smarter. You know, "Claude, please read my files and understand my soul." And sure, it does that. But here's what they put in the documentation that made me spit out my morning tea:</p><blockquote><p>"MCP provides a standardized way to connect AI models to different data sources and tools."</p></blockquote><p><span>Okay but. </span><em>But</em><span>. What if you just... removed the AI part?</span></p><p><span>What if it's just "a standardized way to connect </span><s>AI models</s><span> </span><strong>literally anything</strong><span> to different data sources and tools"?</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!MBI9!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca785d13-391f-42fa-8109-7ae384ec5e99_1836x1203.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!MBI9!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca785d13-391f-42fa-8109-7ae384ec5e99_1836x1203.png 424w, https://substackcdn.com/image/fetch/$s_!MBI9!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca785d13-391f-42fa-8109-7ae384ec5e99_1836x1203.png 848w, https://substackcdn.com/image/fetch/$s_!MBI9!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca785d13-391f-42fa-8109-7ae384ec5e99_1836x1203.png 1272w, https://substackcdn.com/image/fetch/$s_!MBI9!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca785d13-391f-42fa-8109-7ae384ec5e99_1836x1203.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!MBI9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca785d13-391f-42fa-8109-7ae384ec5e99_1836x1203.png" width="1456" height="954" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ca785d13-391f-42fa-8109-7ae384ec5e99_1836x1203.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:954,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:554311,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://worksonmymachine.substack.com/i/166947463?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca785d13-391f-42fa-8109-7ae384ec5e99_1836x1203.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!MBI9!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca785d13-391f-42fa-8109-7ae384ec5e99_1836x1203.png 424w, https://substackcdn.com/image/fetch/$s_!MBI9!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca785d13-391f-42fa-8109-7ae384ec5e99_1836x1203.png 848w, https://substackcdn.com/image/fetch/$s_!MBI9!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca785d13-391f-42fa-8109-7ae384ec5e99_1836x1203.png 1272w, https://substackcdn.com/image/fetch/$s_!MBI9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca785d13-391f-42fa-8109-7ae384ec5e99_1836x1203.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Or remember when someone looked at NFTs‚Äîwhich were supposed to just </span><em>point</em><span> at images‚Äîand thought "what if the pointer... WAS the image?"</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!C2qU!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77d811eb-b5b3-44be-9a9d-e88ba57c7b38_3072x1002.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!C2qU!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77d811eb-b5b3-44be-9a9d-e88ba57c7b38_3072x1002.png 424w, https://substackcdn.com/image/fetch/$s_!C2qU!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77d811eb-b5b3-44be-9a9d-e88ba57c7b38_3072x1002.png 848w, https://substackcdn.com/image/fetch/$s_!C2qU!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77d811eb-b5b3-44be-9a9d-e88ba57c7b38_3072x1002.png 1272w, https://substackcdn.com/image/fetch/$s_!C2qU!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77d811eb-b5b3-44be-9a9d-e88ba57c7b38_3072x1002.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!C2qU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77d811eb-b5b3-44be-9a9d-e88ba57c7b38_3072x1002.png" width="1456" height="475" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/77d811eb-b5b3-44be-9a9d-e88ba57c7b38_3072x1002.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:475,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:417583,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://worksonmymachine.substack.com/i/166947463?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77d811eb-b5b3-44be-9a9d-e88ba57c7b38_3072x1002.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!C2qU!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77d811eb-b5b3-44be-9a9d-e88ba57c7b38_3072x1002.png 424w, https://substackcdn.com/image/fetch/$s_!C2qU!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77d811eb-b5b3-44be-9a9d-e88ba57c7b38_3072x1002.png 848w, https://substackcdn.com/image/fetch/$s_!C2qU!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77d811eb-b5b3-44be-9a9d-e88ba57c7b38_3072x1002.png 1272w, https://substackcdn.com/image/fetch/$s_!C2qU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77d811eb-b5b3-44be-9a9d-e88ba57c7b38_3072x1002.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>For those of you who don‚Äôt get the idea, copy and paste this into your url bar: data:application/json;base64,eyJuYW1lIjogIkJhZyAjNzQ4IiwgImRlc2NyaXB0aW9uIjogIkxvb3QgaXMgcmFuZG9taXplZCBhZHZlbnR1cmVyIGdlYXIgZ2VuZXJhdGVkIGFuZCBzdG9yZWQgb24gY2hhaW4uIFN0YXRzLCBpbWFnZXMsIGFuZCBvdGhlciBmdW5jdGlvbmFsaXR5IGFyZSBpbnRlbnRpb25hbGx5IG9taXR0ZWQgZm9yIG90aGVycyB0byBpbnRlcnByZXQuIEZlZWwgZnJlZSB0byB1c2UgTG9vdCBpbiBhbnkgd2F5IHlvdSB3YW50LiIsICJpbWFnZSI6ICJkYXRhOmltYWdlL3N2Zyt4bWw7YmFzZTY0LFBITjJaeUI0Yld4dWN6MGlhSFIwY0RvdkwzZDNkeTUzTXk1dmNtY3ZNakF3TUM5emRtY2lJSEJ5WlhObGNuWmxRWE53WldOMFVtRjBhVzg5SW5oTmFXNVpUV2x1SUcxbFpYUWlJSFpwWlhkQ2IzZzlJakFnTUNBek5UQWdNelV3SWo0OGMzUjViR1UrTG1KaGMyVWdleUJtYVd4c09pQjNhR2wwWlRzZ1ptOXVkQzFtWVcxcGJIazZJSE5sY21sbU95Qm1iMjUwTFhOcGVtVTZJREUwY0hnN0lIMDhMM04wZVd4bFBqeHlaV04wSUhkcFpIUm9QU0l4TURBbElpQm9aV2xuYUhROUlqRXdNQ1VpSUdacGJHdzlJbUpzWVdOcklpQXZQangwWlhoMElIZzlJakV3SWlCNVBTSXlNQ0lnWTJ4aGMzTTlJbUpoYzJVaVBsTm9iM0owSUZOM2IzSmtQQzkwWlhoMFBqeDBaWGgwSUhnOUlqRXdJaUI1UFNJME1DSWdZMnhoYzNNOUltSmhjMlVpUGtScGRtbHVaU0JTYjJKbElHOW1JSFJvWlNCR2IzZzhMM1JsZUhRK1BIUmxlSFFnZUQwaU1UQWlJSGs5SWpZd0lpQmpiR0Z6Y3owaVltRnpaU0krU0c5dlpEd3ZkR1Y0ZEQ0OGRHVjRkQ0I0UFNJeE1DSWdlVDBpT0RBaUlHTnNZWE56UFNKaVlYTmxJajVRYkdGMFpXUWdRbVZzZER3dmRHVjRkRDQ4ZEdWNGRDQjRQU0l4TUNJZ2VUMGlNVEF3SWlCamJHRnpjejBpWW1GelpTSStSR2wyYVc1bElGTnNhWEJ3WlhKelBDOTBaWGgwUGp4MFpYaDBJSGc5SWpFd0lpQjVQU0l4TWpBaUlHTnNZWE56UFNKaVlYTmxJajVEYUdGcGJpQkhiRzkyWlhNOEwzUmxlSFErUEhSbGVIUWdlRDBpTVRBaUlIazlJakUwTUNJZ1kyeGhjM005SW1KaGMyVWlQazVsWTJ0c1lXTmxQQzkwWlhoMFBqeDBaWGgwSUhnOUlqRXdJaUI1UFNJeE5qQWlJR05zWVhOelBTSmlZWE5sSWo1VWFYUmhibWwxYlNCU2FXNW5QQzkwWlhoMFBqd3ZjM1puUGc9PSJ9</figcaption></figure></div><p>The protocol meant for storing references became a protocol for storing reality. It's like using a library card as the actual book.</p><p><span>Here's where it gets even better. The more MCP servers people build for AI, the more capabilities </span><em>every</em><span> app can have. It's like:</span></p><ol><li><p>Someone builds an MCP server for their AI to access Spotify</p></li><li><p>Your workout app can now generate playlists</p></li><li><p>You didn't write any Spotify code</p></li><li><p>The Spotify MCP developer doesn't know your app exists</p></li><li><p>Everyone wins?</p></li></ol><p>It's like a potluck where everyone brings their specialty dish, but instead of food, it's functionality. And instead of eating, you're... actually, this metaphor is falling apart. But you get it.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Laga!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fea5180-6f0c-4642-8a2f-9c2f6ab5e6ab_1980x1539.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Laga!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fea5180-6f0c-4642-8a2f-9c2f6ab5e6ab_1980x1539.png 424w, https://substackcdn.com/image/fetch/$s_!Laga!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fea5180-6f0c-4642-8a2f-9c2f6ab5e6ab_1980x1539.png 848w, https://substackcdn.com/image/fetch/$s_!Laga!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fea5180-6f0c-4642-8a2f-9c2f6ab5e6ab_1980x1539.png 1272w, https://substackcdn.com/image/fetch/$s_!Laga!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fea5180-6f0c-4642-8a2f-9c2f6ab5e6ab_1980x1539.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Laga!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fea5180-6f0c-4642-8a2f-9c2f6ab5e6ab_1980x1539.png" width="1456" height="1132" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6fea5180-6f0c-4642-8a2f-9c2f6ab5e6ab_1980x1539.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1132,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:629338,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://worksonmymachine.substack.com/i/166947463?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fea5180-6f0c-4642-8a2f-9c2f6ab5e6ab_1980x1539.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Laga!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fea5180-6f0c-4642-8a2f-9c2f6ab5e6ab_1980x1539.png 424w, https://substackcdn.com/image/fetch/$s_!Laga!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fea5180-6f0c-4642-8a2f-9c2f6ab5e6ab_1980x1539.png 848w, https://substackcdn.com/image/fetch/$s_!Laga!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fea5180-6f0c-4642-8a2f-9c2f6ab5e6ab_1980x1539.png 1272w, https://substackcdn.com/image/fetch/$s_!Laga!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fea5180-6f0c-4642-8a2f-9c2f6ab5e6ab_1980x1539.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>The beautiful chaos is that every MCP server built for Claude or ChatGPT or whatever becomes a free plugin for </span><em>anything</em><span> that speaks MCP. It's accidentally creating a universal plugin ecosystem. Nobody planned this (I don‚Äôt think). It's just happening.</span></p><p>They keep saying MCP is like USB-C for AI. But what does that actually mean?</p><p><span>USB-C isn't special because it's a port. It's special because it's a </span><em>possibility space</em><span>. It's a hole that says "put something here and we'll figure it out." Power? Sure. Data? Why not. Video? Apparently yes. Toaster control protocols? Rex says absolutely.</span></p><p>MCP is the same thing but for functionality. It's not saying "I'm for AI." It's saying "I'm a well-designed hole. Put something here."</p><p><span>So we‚Äôre building this thing called </span><strong>APM</strong><span> (</span><a href="https://actionsperminute.io/" rel="">Actions Per Minute</a><span>). On paper, it's a task management app. In reality? It's a shape-shifter that becomes whatever you plug into it.</span></p><p>The entire plugin system? Just MCP servers.</p><p>Want spell check? MCP server.  </p><p>Want it to order coffee when you complete 10 tasks? MCP server.  </p><p>Want your AI agents to respond like peons from Warcraft 3 when you assign them a task? Of course you do, and that MCP server is already written and ready to use.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!cfIp!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3871464-0023-4b56-8235-fad217a9a232_3045x3018.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!cfIp!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3871464-0023-4b56-8235-fad217a9a232_3045x3018.png 424w, https://substackcdn.com/image/fetch/$s_!cfIp!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3871464-0023-4b56-8235-fad217a9a232_3045x3018.png 848w, https://substackcdn.com/image/fetch/$s_!cfIp!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3871464-0023-4b56-8235-fad217a9a232_3045x3018.png 1272w, https://substackcdn.com/image/fetch/$s_!cfIp!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3871464-0023-4b56-8235-fad217a9a232_3045x3018.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!cfIp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3871464-0023-4b56-8235-fad217a9a232_3045x3018.png" width="1456" height="1443" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e3871464-0023-4b56-8235-fad217a9a232_3045x3018.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1443,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1310775,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://worksonmymachine.substack.com/i/166947463?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3871464-0023-4b56-8235-fad217a9a232_3045x3018.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!cfIp!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3871464-0023-4b56-8235-fad217a9a232_3045x3018.png 424w, https://substackcdn.com/image/fetch/$s_!cfIp!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3871464-0023-4b56-8235-fad217a9a232_3045x3018.png 848w, https://substackcdn.com/image/fetch/$s_!cfIp!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3871464-0023-4b56-8235-fad217a9a232_3045x3018.png 1272w, https://substackcdn.com/image/fetch/$s_!cfIp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3871464-0023-4b56-8235-fad217a9a232_3045x3018.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Every great protocol gets used for something its creators never imagined:</p><ul><li><p>HTTP was for academic papers. Now it runs civilization.</p></li><li><p>Bluetooth was for hands-free calling. Now it unlocks your front door.</p></li><li><p>USB was for keyboards and mice. Now it charges your emotional support portable fan.</p></li></ul><p>MCP thinks it's for giving context to AI models.</p><p>But really? It's just a really good protocol for making things talk to other things.</p><p>And in a world where Rex's toast has HDMI output, maybe that's exactly what we need.</p><p>---</p><p><strong>P.S.</strong><span> If you build an MCP server that makes your computer emit the smell of fresh bread, we need to talk.</span></p><p><strong>P.P.S.</strong><span> We‚Äôve just opened up early access for APM. Build something weird. Build something useful. Build something that makes us question our life choices. I believe in you.</span></p><p><em>(Somewhere, a protocol is being used exactly as intended. This is deeply suspicious.)</em></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Booting 5000 Erlangs on Ampere One 192-core (151 pts)]]></title>
            <link>https://underjord.io/booting-5000-erlangs-on-ampere-one.html</link>
            <guid>44854525</guid>
            <pubDate>Sun, 10 Aug 2025 11:41:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://underjord.io/booting-5000-erlangs-on-ampere-one.html">https://underjord.io/booting-5000-erlangs-on-ampere-one.html</a>, See on <a href="https://news.ycombinator.com/item?id=44854525">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
        
        <small>2025-08-05</small>
        <p>
            Underjord is an artisanal consultancy doing consulting in Elixir, Nerves with an accidental speciality in marketing and outreach. If
            you
            like
            the writing you should really <a href="https://underjord.io/services.html">try the pro version</a>.</p>
        <p>In the previous post on <a href="https://underjord.io/500-virtual-linux-devices-on-arm64.html">500 virtual linux devices on ARM64</a> I hinted that I expected serious improvements if we got KVM working. Well. We‚Äôre there. Let‚Äôs see what we got going on.</p>
<p><em>Disclosure: I am running a conference called <a href="https://goatmire.com/">Goatmire Elixir</a> which Ampere is a sponsor of. This post is not part of the sponsorship exchange as such. It is prep for my talk for the conference which uses the hardware they lent me. So this is your transparency notice, but fundamentally I am not making comparisons on whether they are better or not. I‚Äôm learning and sharing about qemu and virtual Linux machines. Now I‚Äôd love if they paid me to shill them a bit later and I‚Äôd be transparent about that too. But this is not that :)</em></p>
<p>To recap. We have an Ampere One 192-core machine with 1 TB of RAM. The goal is to run as many virtual Linux IoT devices <em>using the Nerves framework</em>. We got 500 of them last time before I tried pushing any further. I also got a bit further on the same setup when I tried. Maybe 1000, I don‚Äôt recall exactly. But there have been developments, so read on!</p>
<p>Briefly on Nerves: the framework treats the BEAM virtual machine like the OS and essentially only uses Linux for a kernel, drivers and the like. This means we can write much if not all of the embedded device in a productive high-level language running on a provenly robust and reliable environment with memory safety and solid recovery strategies. And it means your cloud integration developer doesn‚Äôt risk seg-faulting the entire device while mangling JSON back and forth. Nerves also brings some best-practice tooling and conventions. Your init process is <a href="https://github.com/nerves-project/erlinit">erlinit</a>, your updates use <a href="https://github.com/fwup-home/fwup">fwup</a> to provide A/B partitions and factory reset, auto-failback, validation of firmware viability, disk encryption, delta updates, streaming updates and a bunch more.</p>
<p>The most interesting development is the thing you can probably learn the most from. Frank Hunleth who has been my co-conspirator and a massive help saved me from fighting u-boot by .. writing another bootloader. Introducing <a href="https://github.com/fhunleth/little_loader">little_loader</a>. This adorable tractor will load up your ARM64 qemu device, consult the uboot environment that Nerves uses, find a Linux kernel from information in that and then boot. Consequently it enables the A/B upgrade features and everything else that makes Nerves great.</p>
<p>Writing a boot loader is a little bit ridiculous. Frank knows his way around C and apparently ChatGPT knows a fair bit about ARM and qemu. Enough to be dangerous. And where it was wrong he could rummage around until he found the way. How he does what he does is beyond me but the result is a very small boot loader that you can probably read through and understand. So if you are curious about booting ARM64 or about how qemu starts things the code should be a worthwhile read.</p>
<p>We got a bit tangled up in EL1 vs EL2 when we only ever needed EL1 to work. EL2 on ARM is what you‚Äôd run under if you want to be able to run VMs in your VMs so you can VM while you VM. And the version of qemu + KVM I got from Ubuntu doesn‚Äôt seem to support that. We weren‚Äôt interested in it either. At some point we might explore EL3 for secure boot and whatnot. Only time will tell.</p>
<p>One of the weirder challenges and something we haven‚Äôt disentangled yet is that we have some compilation issue where using the toolchains I was using the non-debug build would hang while the debug one runs fine. For now I run the debug build of the bootloader. I think it was fine from GCC 15? Anyway, hopefully we pin that down at some point. But it tripped us up a few times when the bootloader would hang due that issue rather than any actual problems with the implementation.</p>
<p>KVM didn‚Äôt really require anything extra aside from making sure we didn‚Äôt go to EL2. And when we tried it on MacOS it worked great with HVF as well. Host-based ARM64 VMs are ridiculously fast and practical. As in booting to the full IEx prompt in single-digit seconds instead of double-digit. And they use about 500Mb less memory. And see, that‚Äôs important. Because we want to shove as many as we can into this server I got access to.</p>
<h2 id="accelerated-on-host">Accelerated on host</h2>
<p>My very hacky project for running this stuff is <a href="https://github.com/lawik/amproj">available here</a>. This code is cribbed from <code>simple.sh</code>:</p>

  <div data-file="simple.sh">
    <p><span>shell</span>
      <span>simple.sh</span>
    </p>
    <div><pre tabindex="0"><code data-lang="shell">  qemu-system-aarch64 <span>\
</span><span></span>	-machine virt,accel<span>=</span>kvm <span>\
</span><span></span>	-cpu host <span>\
</span><span></span>	-smp <span>1</span> <span>\
</span><span></span>	-m 150M <span>\
</span><span></span>	-kernel ../little_loader/little_loader.elf <span>\
</span><span></span>	-netdev user,id<span>=</span>eth0 <span>\
</span><span></span>	-device virtio-net-device,netdev<span>=</span>eth0,mac<span>=</span>de:ad:be:ef:00:01 <span>\
</span><span></span>	-global virtio-mmio.force-legacy<span>=</span>false <span>\
</span><span></span>	-drive <span>if</span><span>=</span>none,file<span>=</span>/space/disks/special.img,format<span>=</span>raw,id<span>=</span>vdisk <span>\
</span><span></span>	-device virtio-blk-device,drive<span>=</span>vdisk,bus<span>=</span>virtio-mmio-bus.0 <span>\
</span><span></span>	-nographic</code></pre></div>
  </div>

<p>To go through it. We use <code>qemu-system-aarch64</code> to emulate an ARM64 machine. <code>aarch64</code> is the common shortname for ARM64, except sometimes on MacOS where I hear it can be <code>arm64</code>. We specify the <code>machine</code> to be <a href="https://www.qemu.org/docs/master/system/arm/virt.html">virt</a>. Previously we‚Äôd leave it there but now we use an <a href="https://www.qemu.org/docs/master/system/introduction.html">accelerator</a> named KVM (<a href="https://linux-kvm.org/page/Main_Page">Kernel-based Virtual Machine</a>). It is the virtualization mechanism included with Linux and qemu can integrate with that to accelerate the execution. This also requires <code>-cpu host</code>, meaning, we are no longer trying to emulate a <code>cortex-a53</code> processor. We are trying to run on the host processor, whatever that is. <code>host</code> means that we emulate the host CPU, or at least as much as qemu and the KVM accelerator can support of what the host can do. This is where we drop about 500Mb of memory overhead. We no longer have to have a pretend ARM chip in memory because we have an actual ARM chip to run on. That‚Äôs my understanding at least. Would love notes on that.</p>
<p>We only give it 1 core via <code>-smp 1</code> and we give it 150Mb memory with <code>-m 150</code>. Skipping ahead we give it virtual Ethernet and a <a href="https://docs.kernel.org/driver-api/virtio/virtio.html">virtio</a> block storage drive. You‚Äôll see a lot of virt and virtio when doing this stuff. And with <code>-nographic</code> we tell it to not bother trying to pop up a GUI window, so we get our console in the terminal. I‚Äôve done all the work over SSH so that‚Äôs definitely my preference.</p>
<p>The disk we provide runs from the raw disk image file <code>special.img</code> which was generated using <code>fwup</code> based on the Nerves project <code>amproj</code> I mentioned earlier. If you build that project with <code>mix firmware</code> you can then run:</p>

  <div>
    <p><span>shell</span>
      
    </p>
    <div><pre tabindex="0"><code data-lang="shell">fwup -a -i amproj.fw -d special.img -t complete</code></pre></div>
  </div>

<p>That‚Äôll give you an image file that contains a full Nerves system. The stuff written to disk is:</p>
<ul>
<li>A uboot env formatted chunk of data. We don‚Äôt use uboot this time but we used that format.</li>
<li>A linux kernel, not on a filesystem. Just written to the disk. RAW!</li>
<li>An MBR and some partitions:
<ul>
<li>Root filesystem A (squashfs, read-only)</li>
<li>Root filesystem B (squashfs, read-only)</li>
<li>Application data partition (f2fs, read/write)</li>
</ul>
</li>
</ul>
<p>The uboot env is used to tell the bootloader important things about the A/B upgrade process as well as where to find the kernel to load as well as what <a href="https://www.kernel.org/doc/html/v4.14/admin-guide/kernel-parameters.html">kernel cmdline</a> to use which is how we tell it what root filesystem to use.</p>
<p>The only config I put into the loader is to set the offset where it can expect the uboot-env and I set that at build-time.</p>
<h2 id="promising-results">Promising results</h2>
<p>NervesCloud received 3389 simultaneous connected devices before the server hit me with the OOM killer. It was probably running a few more but around there. So each VM is:</p>
<ul>
<li>Bootloader</li>
<li>Linux</li>
<li>erlinit</li>
<li>BEAM/ERTS</li>
<li>Nerves base functionality</li>
<li><a href="https://github.com/nerves-hub/nerves_hub_link">NervesHubLink</a> for connecting to NervesHub</li>
</ul>
<p>I have had 3000 devices running stable and then I started to see ‚Äúfun‚Äù challenges. For one thing, our NervesCloud hosts were looking a bit tight on memory because all these devices connect from the US west coast and we were only running a single node in that region. I scaled that up a smidge to make sure I didn‚Äôt bother any paying customers.</p>
<p>The VMs are super well-behaved, the Ampere CPU just works. The memory usage is roughly where I‚Äôd expect it. 150-250 total I think. There are probably things I can do to make it behave a little more tighter. Will explore that if time allows.</p>
<p>Then I ran my first demo workloads. As the purveyor of the finest Over-the-Air updates for embedded devices we here at NervesCloud.. I kid. But I wanted to shove lots of updates at them and see what that did. The updates process is a lot of compression, decompression and IO. Probably mostly IO-bound but if the devices would be struggling for CPU that‚Äôd be noticeable. If the memory usage exploded, that‚Äôd be noticed very quickly.</p>
<p>It worked. Not really any problems. I limited the concurrency of the update to 1000 and it couldn‚Äôt hand out the updates faster than they completed so it tended to hover around 200-300 concurrent updates happening. Or at least that‚Äôs my understanding of what happened. Did I mention the KVM setup is pretty fast?</p>
<p>I logged some issues about UI behavior as I was watching things live and trying to adjust things. It seems like good guy Nate Shoemaker already has a fix in flight for this. There may be more details. When you get a lot of progress reports the LiveView UI perhaps shouldn‚Äôt try to refresh all the things all the time.</p>
<h2 id="memory-tuning">Memory tuning</h2>
<p>Frank gave me some tips about tuning Linux memory usage and tuning BEAM memory usage. When I looked into his advice I ended up doing a few things:</p>
<p>For BEAM VM, we <a href="https://github.com/lawik/amproj/commit/cb5919eee5b5c58321cf67aaff388c5db05a1ccc#diff-b9fbf8080c62b37068a3fefe69eeed4d0b7e801049c87b9aaaf721c2d5ed47afR38">change the allocators</a>. This should use less memory and probably trades off in raw performance. Which is fine for this purpose.</p>
<p>Erlang release, <a href="https://github.com/lawik/amproj/commit/cb5919eee5b5c58321cf67aaff388c5db05a1ccc#diff-b9fbf8080c62b37068a3fefe69eeed4d0b7e801049c87b9aaaf721c2d5ed47afR26">use default mode</a> instead of embedded. Which probably makes it boot a bit faster, makes it use less memory but it could lead to surprising delays and growing memory usage later if it loads code ad-hoc. The use of embedded mode is helpful in making the release behave much more consistently, is my understanding.</p>
<p>Made <a href="https://github.com/nerves-project/nerves_system_qemu_aarch64/commit/149a0545312df0d422e44975babe9c9b15247ce7">a bunch of adjustments to Linux memory usage</a>. Using <code>zram</code> was suggested by Frank. Then I checked with (famous ML model) Claude to get hints about what knobs were available to tune on Linux because Frank hinted that it might be caching a bit much and I never know where to start when it comes to what I can do to the Linux kernel. It had some suggestions, I looked those up, found articles that matched the claims that this might reduce memory usage. Changing swappiness, dirty ratios and <code>vfs_cache_pressure</code> like I knew what I was doing and it sure seems to have improved things.</p>
<p>I know I could play with different allocators, my co-founder Josh has been doing that for NervesCloud recently. I think I could also do something with virtual balloons to reclaim memory and essentially over-provision VMs but I haven‚Äôt got there yet.</p>
<p>This memory tuning led to some interesting further runs where we ran a solid 5100 devices and I could have pushed it a bit further. I just didn‚Äôt have time and could be bothered to do more math at the time. The VMs are now started with 110 MB of RAM on the inside and they seem to run steady around 160 MB RES according to htop. The people I‚Äôve talked to at Ampere indicate that I‚Äôm probably running the most VMs anyone has ever ran on their hardware. Which is fun. I‚Äôm not even running tiny VMs. I could make a Buildroot system that does nothing and run another gajillion probably. But this is much closer to a real device and workload.</p>
<h2 id="the-utility-of-it-all">The utility of it all</h2>
<p>Honestly, getting a chance to run significant, not massive, but significant workloads against a SaaS is pretty useful. But the work we‚Äôve put in now means we can tidy up this Nerves system and make it part of supported Nerves tooling. This would make it easy to run stuff ‚Äúon device‚Äù without physical hardware. It would make running more detailed tests of Nerves functionality much more feasible as well. Essentially you‚Äôd need an ARM64 Linux box with KVM or an Apple Silicon Mac and you‚Äôd get the blazing fast ones. Or you can absolutely get by with the emulated, more demanding things from the x86 side of things. There is a lot we can do with a full-featured qemu-system for ARM devices.</p>
<p>While my experimentation is a bit of a stunt and mostly for the joy of experimentation and Frank‚Äôs bootloader is mostly about learning the end result is still that we have produced something we should get good mileage out of.</p>
<p>Heck that MacOS thing. I just tried the <code>DELAY=1 COUNT=200 CHUNK=10 ./run.exs</code> after modifying the script to use <code>hvf</code>instead of <code>kvm</code> on my M2 MacBook Air. I think I had 50 VMs when I ran out of disk. Solvable problem, but not throwing out my photo library right this minute.</p>
<h2 id="further-work">Further work</h2>
<p>I need to look at how KVM and <a href="https://en.wikipedia.org/wiki/Non-uniform_memory_access">NUMA</a> interact and if/how I can pin. I don‚Äôt think I‚Äôll hit problems where caches and pinning matter all that much but it would feel better. When the VMs are at rest after booting the overall system CPU usage is generally less than 20% running thousands of VMs. Mostly idle, yes, but there are things happening in all of them.</p>
<p>Should run the workload with some graphs to see what is actually happening big picture. Right now I‚Äôm mostly going ‚Äúhey, it is STILL running, eh!?‚Äù. Which is fine enough when figuring out if it fits in memory.</p>
<h2 id="tidying-up">Tidying up</h2>
<p>We are in the process of tidying up <a href="https://github.com/nerves-project/nerves_system_qemu_aarch64">nerves_system_qemu_aarch64</a> and then it should get a proper release and some docs. It has a mix task for generating an appropriate qemu command for you. So this all becomes a part of Nerves. Over time we should be able to build some really nice tooling based off of this. And if you have ideas you should be able to pick it up and run with it already.</p>
<p>Really enjoying this deeper dive into things I‚Äôve only been at the periphery of. Learning a lot of Linux, getting to really get into it with qemu, performance tuning for both the BEAM, Linux and virtualization. It is a ton of fun to see how far you can push the hardware.</p>
<p>Alright, that‚Äôs enough words. Let me know what you think and if there is anything in particular you‚Äôd like me try in and around this. Thanks for reading, hit me up on <a href="mailto:lars@underjord.io">lars@underjord.io</a> or <a href="https://hachyderm.io/@lawik">@lawik@hachyderm.io</a> or wherever you find me.</p>

        <p>
            Underjord is an artisanal consultancy doing consulting in Elixir, Nerves with an accidental speciality in marketing and outreach. If
            you
            like
            the writing you should really <a href="https://underjord.io/services.html">try the pro version</a>.</p>
        <p>
            Note: Or try the videos on <a href="https://youtube.com/c/underjord">the YouTube
                channel</a>.
        </p>
        
    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Open Lovable (116 pts)]]></title>
            <link>https://github.com/mendableai/open-lovable</link>
            <guid>44854120</guid>
            <pubDate>Sun, 10 Aug 2025 10:10:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/mendableai/open-lovable">https://github.com/mendableai/open-lovable</a>, See on <a href="https://news.ycombinator.com/item?id=44854120">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
<p dir="auto"><h2 tabindex="-1" dir="auto">Open Lovable</h2><a id="user-content-open-lovable" aria-label="Permalink: Open Lovable" href="#open-lovable"></a></p>
<p dir="auto">Chat with AI to build React apps instantly.</p>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/ee68ebd089163b65192416765f028cb913aa1cc4b475f4f4b94e54b0809ebeb9/68747470733a2f2f6d65646961312e67697068792e636f6d2f6d656469612f76312e59326c6b505463354d4749334e6a4578626d5a746148466c654752734d544e6c61574e7964476469616e49344e475134644868795a6a42306432566b636a52796558427563435a6c634431324d563970626e526c636d35686246396e61575a66596e6c666157516d593351395a772f5a46564c574d61366456736b5158307175312f67697068792e676966"><img src="https://camo.githubusercontent.com/ee68ebd089163b65192416765f028cb913aa1cc4b475f4f4b94e54b0809ebeb9/68747470733a2f2f6d65646961312e67697068792e636f6d2f6d656469612f76312e59326c6b505463354d4749334e6a4578626d5a746148466c654752734d544e6c61574e7964476469616e49344e475134644868795a6a42306432566b636a52796558427563435a6c634431324d563970626e526c636d35686246396e61575a66596e6c666157516d593351395a772f5a46564c574d61366456736b5158307175312f67697068792e676966" alt="Open Lovable Demo" width="100%" data-animated-image="" data-canonical-src="https://media1.giphy.com/media/v1.Y2lkPTc5MGI3NjExbmZtaHFleGRsMTNlaWNydGdianI4NGQ4dHhyZjB0d2VkcjRyeXBucCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/ZFVLWMa6dVskQX0qu1/giphy.gif"></a>
</p></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Setup</h2><a id="user-content-setup" aria-label="Permalink: Setup" href="#setup"></a></p>
<ol dir="auto">
<li><strong>Clone &amp; Install</strong></li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/mendableai/open-lovable.git
cd open-lovable
npm install"><pre>git clone https://github.com/mendableai/open-lovable.git
<span>cd</span> open-lovable
npm install</pre></div>
<ol start="2" dir="auto">
<li><strong>Add <code>.env.local</code></strong></li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="# Required
E2B_API_KEY=your_e2b_api_key  # Get from https://e2b.dev (Sandboxes)
FIRECRAWL_API_KEY=your_firecrawl_api_key  # Get from https://firecrawl.dev (Web scraping)

# Optional (need at least one AI provider)
ANTHROPIC_API_KEY=your_anthropic_api_key  # Get from https://console.anthropic.com
OPENAI_API_KEY=your_openai_api_key  # Get from https://platform.openai.com (GPT-5)
GROQ_API_KEY=your_groq_api_key  # Get from https://console.groq.com (Fast inference - Kimi K2 recommended)"><pre><span><span>#</span> Required</span>
<span>E2B_API_KEY</span><span>=</span><span>your_e2b_api_key<span>  <span>#</span> Get from https://e2b.dev (Sandboxes)</span></span>
<span>FIRECRAWL_API_KEY</span><span>=</span><span>your_firecrawl_api_key<span>  <span>#</span> Get from https://firecrawl.dev (Web scraping)</span></span>

<span><span>#</span> Optional (need at least one AI provider)</span>
<span>ANTHROPIC_API_KEY</span><span>=</span><span>your_anthropic_api_key<span>  <span>#</span> Get from https://console.anthropic.com</span></span>
<span>OPENAI_API_KEY</span><span>=</span><span>your_openai_api_key<span>  <span>#</span> Get from https://platform.openai.com (GPT-5)</span></span>
<span>GROQ_API_KEY</span><span>=</span><span>your_groq_api_key<span>  <span>#</span> Get from https://console.groq.com (Fast inference - Kimi K2 recommended)</span></span></pre></div>
<ol start="3" dir="auto">
<li><strong>Run</strong></li>
</ol>

<p dir="auto">Open <a href="http://localhost:3000/" rel="nofollow">http://localhost:3000</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">MIT</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Writing simple tab-completions for Bash and Zsh (192 pts)]]></title>
            <link>https://mill-build.org/blog/14-bash-zsh-completion.html</link>
            <guid>44854035</guid>
            <pubDate>Sun, 10 Aug 2025 09:50:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mill-build.org/blog/14-bash-zsh-completion.html">https://mill-build.org/blog/14-bash-zsh-completion.html</a>, See on <a href="https://news.ycombinator.com/item?id=44854035">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<article>

<div id="preamble">
<p><em>Li Haoyi, 7 August 2025</em></p>
<p>Shell tab-completions can be very handy, but setting them up is complicated by the fact
that half your users would be using Bash-on-Linux, while the other half will be
using Zsh-on-OSX, each of which has different tab-completion APIs. Furthermore, most
users exploring an unfamiliar CLI tool using tab completion appreciate showing a
description along with each completion so they can read what it is, but that‚Äôs
normally only available on Zsh and not on Bash.</p>
<p>But with some work, you can make your tab-completions work on both shells, including
nice quality-of-life features like completion descriptions. This blog post will explore how it
can be done, based on our recent experience implementing this in the <a href="https://mill-build.org/">Mill build tool</a>
version <a href="https://github.com/com-lihaoyi/mill/blob/main/changelog.adoc#103">1.0.3</a>,
providing the great tab-completion experience you see below in a way that works across
both common shells. Hopefully based on this, you will know enough and have enough reference
examples to set up Bash and Zsh completions for your own command-line tooling.</p>
<div>
<p><img src="https://mill-build.org/blog/_images/CompletionDescriptions.png" alt="CompletionDescriptions">
</p>
</div>
<div>
<p><img src="https://mill-build.org/blog/_images/CompletionDescriptions2.png" alt="CompletionDescriptions2">
</p>
</div>
</div>
<div>
<h2 id="_basic_tab_completion"><a href="#_basic_tab_completion"></a>Basic Tab Completion</h2>
<div>
<p>The basic way tab-completion works in shells like Bash or Zsh is to register a handler
function that is called when a user presses <code>&lt;TAB&gt;</code> at the command line. This handler
function is then given the words currently written, and the index of the word the
user‚Äôs cursor is currently over. From this information, the completion function generates
a list of strings that are possible completions for the word at that index, and
return it to the shell. At a glance, this looks something like:</p>
<div>
<pre><code data-lang="bash">_generate_foo_completions() {
  local idx=$1; shift
  local words=( "$@" )
  local current_word=${words[idx]}

  local array=(apple apricot banana cherry durian)
  for elem in "${array[@]}"; do
    if [[ $elem == "$current_word"* ]]; then echo "$elem"; fi
  done
}

_complete_foo_bash() {
  local raw=($(_generate_foo_completions "$COMP_CWORD" "${COMP_WORDS[@]}"))
  COMPREPLY=( "${raw[@]}" )
}

_complete_foo_zsh() {
  local -a raw
  raw=($(_generate_foo_completions "$CURRENT" "${words[@]}"))
  compadd -- $raw
}

if [ -n "${ZSH_VERSION:-}" ]; then
  autoload -Uz compinit
  compinit
  compdef _complete_foo_zsh foo
elif [ -n "${BASH_VERSION:-}" ]; then
  complete -F _complete_foo_bash foo
fi</code></pre>
</div>
<div>
<ul>
<li>
<p><code>_generate_foo_completions</code> is a dummy function used
for demonstration purposes that prints out a hardcoded set of completions,
but in a real scenario would be the logic that generates completions for
your specific app or CLI tool.</p>
</li>
<li>
<p><code>_complete_foo_bash</code> and <code>_complete_foo_zsh</code> are the shell-specific
completion functions that pass the current word to <code>_generate_foo_completions</code>
and wire up the results to each shell‚Äôs unique completion APIs. Bash completion
functions need to set the <code>COMPREPLY</code> environment variable, while Zsh completion
functions need to call <code>compadd</code> (or one of the other similar functions)</p>
</li>
<li>
<p>This example snippet would typically be put (or <code>source</code>ed) in your
<code>~/.bashrc</code>, <code>~/.bash_profile</code>, and <code>~/.zshrc</code> so the <code>if</code>/<code>elif</code>/<code>fi</code> block at
the bottom registers the relevant hooks when the shell starts.
These hook into tab-completion whenever <code>foo</code> is the
first word at the prompt.</p>
</li>
</ul>
</div>
<p>For example, the Mill build tool provides a <code>./mill mill.tabcomplete/install</code>
builtin that automatically updates these files and instructs the user to
restart the shell or <code>source</code> the relevant script to begin using completions:</p>
<div>
<pre><code data-lang="console">$ ./mill mill.tabcomplete/install
[1/1] mill.tabcomplete.TabCompleteModule.install
Writing to /Users/lihaoyi/.cache/mill/download/mill-completion.sh
Writing to /Users/lihaoyi/.bash_profile
Writing to /Users/lihaoyi/.zshrc
Writing to /Users/lihaoyi/.bashrc
Please restart your shell or `source ~/.cache/mill/download/mill-completion.sh` to enable completions</code></pre>
</div>
<p>Although the Shell syntax can be very finnicky, e.g. passing arrays to as
function arguments via <code>"${words[@]}"</code>, the actual underlying logic here isn‚Äôt
too complicated. <code>_complete_foo_bash</code> and <code>_complete_foo_zsh</code> take the
local variables from the shell, pass it to <code>_generate_foo_completions</code>
that uses them to return the possible completions, and passes the completions
back to the shell via <code>COMPREPLY</code> or <code>compadd</code>.</p>
<p>You can try this out live by pasting it into your Bash or Zsh shell and
typing <code>foo &lt;TAB&gt;</code> or <code>foo a&lt;TAB&gt;</code>. Note that you don‚Äôt
actually need a <code>foo</code> command installed:</p>
<div>
<pre><code data-lang="console">$ foo &lt;TAB&gt;
apple    apricot  banana   cherry   durian

$ foo a&lt;TAB&gt;
apple    apricot</code></pre>
</div>
<p>That‚Äôs all you need to get a basic tab-completer working. In real usage"</p>
<div>
<ul>
<li>
<p><code>foo</code> would be the name of the command the user would invoke your CLI program with
(e.g. <code>mill</code>)</p>
</li>
<li>
<p><code>_generate_foo_completions</code> would be your bespoke logic
to print out a line-separated list of completions. This could be a hard-coded list
for programs that change infrequently, or it could actually invoke your binary and
ask it what completions are available for the given input (what <code>mill</code> does).</p>
</li>
<li>
<p>While this example only looks up <code>words[idx]</code> to try and find a prefix
match for the current word, the completer is allowed to use the entirety of <code>words</code>
to decide what completions to offer, e.g. based on what flags or command-names are present in that array</p>
</li>
</ul>
</div>
<p>Note that when you register completion hooks for <code>foo</code> in Bash and Zsh, they apply
to commands like <code>./foo</code> as well. This is handy for programs like Mill, Maven, or Gradle
which typically use a <code>./mill</code> <a href="https://mill-build.org/mill/cli/installation-ide.html#_bootstrap_scripts" class="page">Bootstrap Script</a>
to run:</p>
<div>
<pre><code data-lang="console">$ ./foo a&lt;TAB&gt;
apple    apricot</code></pre>
</div>
</div>
</div>
<div>
<h2 id="_zsh_completion_descriptions"><a href="#_zsh_completion_descriptions"></a>Zsh Completion Descriptions</h2>
<div>
<p>The completions above work and provide a basic level of assistance for users of your CLI, but
it would be nice for users if they could also see a description of each command they could
complete in the terminal, as is done in the Mill build tool:</p>
<div>
<p><img src="https://mill-build.org/blog/_images/CompletionDescriptions.png" alt="CompletionDescriptions">
</p>
</div>
<p>To do this, we can make <code>_generate_foo_completions</code> generate an array of
longer strings containing both the completion and a description. Bash does not support
completion descriptions by default so we trim off the description,
but in Zsh we pass both the <code>trimmed</code> completion-words as well as the <code>raw</code> words and
descriptions to <code>compadd -d raw‚Äâ‚Äî‚Äâ$trimmed</code> as two parallel arrays.</p>
<div>
<pre><code data-lang="bash">_generate_foo_completions() {
  local idx=$1; shift
  local words=( "$@" )
  local current_word=${words[idx]}

  local array=(
    "apple: a common fruit"
    "apricot: sour fruit with a large stone"
    "banana: starchy and high in potassium"
    "cherry: small and sweet with a large pit"
    "durian: stinky spiky fruit"
  )
  for elem in "${array[@]}"; do
    if [[ $elem == "$current_word"* ]]; then echo "$elem"; fi
  done
}

_complete_foo_bash() {
  local IFS=$'\n'
  local raw=($(_generate_foo_completions "$COMP_CWORD" "${COMP_WORDS[@]}"))
  local trimmed=()
  for d in "${raw[@]}"; do trimmed+=( "${d%%:*}" ); done

  COMPREPLY=( "${trimmed[@]}" )
}

_complete_foo_zsh() {
  local -a raw trimmed
  local IFS=$'\n'
  raw=($(_generate_foo_completions "$CURRENT" "${words[@]}"))

  for d in $raw; do trimmed+=( "${d%%:*}" ); done
  compadd -d raw -- $trimmed
}

if [ -n "${ZSH_VERSION:-}" ]; then
  autoload -Uz compinit
  compinit
  compdef _complete_foo_zsh foo
elif [ -n "${BASH_VERSION:-}" ]; then
  complete -F _complete_foo_bash foo
fi</code></pre>
</div>
<p>Zsh would then display the <code>raw</code> lines including both the completion-word as well
as the descriptions when displaying the completion options, but use the <code>trimmed</code>
lines which only contain the completion-words when completing the line</p>
<div>
<pre><code data-lang="console">$ foo a&lt;TAB&gt;
$ foo ap

$ foo ap&lt;TAB&gt;
apple: a common fruit                          apricot: sour fruit with a large stone

$ foo app&lt;TAB&gt;
$ foo apple</code></pre>
</div>
<p>However in this scenario the descriptions are entirely ignored by Bash. Because Bash
does not have a concept of tab-complete descriptions, in Bash we only pass the <code>trimmed</code>
word-completions to <code>COMPREPLY</code> and discard the <code>raw</code> lines containing the descriptions.</p>
</div>
</div>
<div>
<h2 id="_hacking_bash_completion_descriptions"><a href="#_hacking_bash_completion_descriptions"></a>Hacking Bash Completion Descriptions</h2>
<div>
<p>To make Bash show completion "descriptions", we can take advantage of the fact
that the completions are generated dynamically every time we call
<code>_generate_foo_completions</code>, and Bash and Zsh only inserts text
that is a common prefix to all completion options</p>

<p>Therefore, if we have multiple differing word-completions, we can actually append
whatever we want to the right of those words in <code>_generate_foo_completions</code>!
This "appended text" will be shown to users if there are multiple completions
available, but since the word-completions differ, Bash will never insert the entire word,
and thus never insert the appended text either.</p>
<p>The code below implements this: if there is only one completion we trim off the description
following the <code>:</code> off as normal, but if there‚Äôs more than one completion we leave the
description intact for the user to see</p>
<div>
<pre><code data-lang="bash">_complete_foo_bash() {
  local IFS=$'\n'
  local raw=($(_generate_foo_completions "$COMP_CWORD" "${COMP_WORDS[@]}"))
  local trimmed=()
  if (( ${#raw[@]} == 1 )); then
    trimmed=( "${raw[0]%%:*}" )
  else
    trimmed=( "${raw[@]}" )
  fi

  COMPREPLY=( "${trimmed[@]}" )
}</code></pre>
</div>
<p>Now when I use autocomplete in Bash, I can see the descriptions for each item, but when
the tab-completion actually completes the token it only completes the word itself and
does not include the description!</p>
<div>
<pre><code data-lang="console">$ foo &lt;TAB&gt;
apple: a common fruit                     cherry: small and sweet with a large pit
apricot: sour fruit with a large stone    durian: stinky spiky fruit
banana: starchy and high in potassium

$ foo a&lt;TAB&gt;
$ foo ap

$ foo ap&lt;TAB&gt;
apple: a common fruit                   apricot: sour fruit with a large stone


$ foo app&lt;TAB&gt;
$ foo apple</code></pre>
</div>
<p>In this section, we only needed to make changes to the <code>_complete_foo_bash</code> function,
as the Zsh completion logic in <code>_complete_foo_zsh</code> is completely unchanged.</p>
</div>
</div>
<div>
<h2 id="_showing_single_completion_descriptions"><a href="#_showing_single_completion_descriptions"></a>Showing Single-Completion Descriptions</h2>
<div>
<p>The last quality of life feature we will add is the ability to show completion
descriptions when tabbing on a complete word:</p>

<p>For example, the Mill build tool does this so if you‚Äôre not sure what a flag or command
does, you can press <code>&lt;TAB&gt;</code> on it to see more details:</p>
<div>
<p><img src="https://mill-build.org/blog/_images/CompletionSingleDescription.png" alt="CompletionSingleDescription">
</p>
</div>
<p>Tab-completion is a common way to explore unfamiliar APIs, and just because someone
finished writing a flat or command doesn‚Äôt mean they aren‚Äôt curious about what
it does! But while Zsh tab-completion displays descriptions when multiple
options match the prefix, and we managed to hack Bash tab-completion to do the same
thing, neither displays any information if the word you are tab-completing is already
complete.</p>
<p>This behavior can be annoying, if the user wants to see the description, they will
need to first:</p>
<div>
<ul>
<li>
<p>Delete enough characters to make the token match multiple completions</p>
</li>
<li>
<p>Press <code>&lt;TAB&gt;</code></p>
</li>
<li>
<p>Visually scan the multiple completions printed to find the word description
they care about</p>
</li>
<li>
<p>Type back in all the missing characters so they can run the command</p>
</li>
</ul>
</div>
<p>To solve this, we can hack Bash and Zsh to print tab-completion descriptions even
if the token is already a complete word. We do this by checking if the token
is a complete word, and if so adding a second "dummy" completion: this makes
the tab-completion ambiguous, which cases Bash and Zsh to print out the completions
and descriptions for the user to see.</p>
<p>Doing this in <code>_complete_foo_bash</code> looks like the following:</p>
<div>
<pre><code data-lang="bash">_complete_foo_bash() {
  local IFS=$'\n'
  local raw=($(_generate_foo_completions "$COMP_CWORD" "${COMP_WORDS[@]}"))
  local trimmed=()
  trimmed+=( "${raw[@]}" )

  if (( ${#raw[@]} == 1 )); then
    trimmed+=( "${raw[0]%%:*}" )
  fi

  COMPREPLY=( "${trimmed[@]}" )
}</code></pre>
</div>
<p>Instead of checking the length of <code>raw</code> to decide whether we add a trimmed
and non-trimmed lines to <code>trimmed</code>, we now instead <em>always</em> add the non-trimmed lines
that contain the completion descriptions, and in the case where there‚Äôs only
one line we then add an additional word-only completion with the description
trimmed off.</p>
<p>This means that all completions are ambiguous and will print the description -
even completions with a single real choice - but the additional trimmed line
when there is only 1 real choice ensures that the description text never gets
inserted into the user‚Äôs command</p>
<p>In Zsh, this can be similarly done via:</p>
<div>
<pre><code data-lang="bash">_complete_foo_zsh() {
  local -a raw trimmed
  local IFS=$'\n'
  raw=($(_generate_foo_completions "$CURRENT" "${words[@]}"))

  for d in $raw; do trimmed+=( "${d%%:*}" ); done
  if (( ${#raw} == 1 )); then
    trimmed+=( "${raw[1]}" )
    raw+=( "${trimmed[1]}" )
  fi

  compadd -d raw -- $trimmed
}</code></pre>
</div>
<p>The change here is similar to the Bash snippet above: when the number of completions is 1,
we add an additional completion to make it ambiguous so Zsh prints the description. But
because Zsh expects to pass two parallel arrays of descriptions and tokens to <code>compadd</code>,
our <code>if</code> block needs to append items to both <code>trimmed</code> and <code>raw</code>.</p>
<p>Using this, it now looks like</p>
<div>
<pre><code data-lang="console">$ foo apple&lt;TAB&gt;
apple                  apple: a common fruit</code></pre>
</div>
<p>Although the UI is not quite perfect - the word <code>apple</code> gets duplicated twice -
this nevertheless achieves the original goal of letting users <code>&lt;TAB&gt;</code> on an
already-completed flag or command to see the description or documentation for that word.</p>
</div>
</div>
<div>
<h2 id="_conclusion"><a href="#_conclusion"></a>Conclusion</h2>
<div>
<p>At this point, our final code looks like this:</p>
<div>
<pre><code data-lang="bash">_generate_foo_completions() {
  local idx=$1; shift
  local words=( "$@" )
  local current_word=${words[idx]}

  local array=(
    "apple: a common fruit"
    "apricot: sour fruit with a large stone"
    "banana: starchy and high in potassium"
    "cherry: small and sweet with a large pit"
    "durian: stinky spiky fruit"
  )
  for elem in "${array[@]}"; do
    if [[ $elem == "$current_word"* ]]; then echo "$elem"; fi
  done
}

_complete_foo_bash() {
  local IFS=$'\n'
  local raw=($(_generate_foo_completions "$COMP_CWORD" "${COMP_WORDS[@]}"))
  local trimmed=()
  trimmed+=( "${raw[@]}" )

  if (( ${#raw[@]} == 1 )); then
    trimmed+=( "${raw[0]%%:*}" )
  fi

  COMPREPLY=( "${trimmed[@]}" )
}

_complete_foo_zsh() {
  local -a raw trimmed
  local IFS=$'\n'
  raw=($(_generate_foo_completions "$CURRENT" "${words[@]}"))

  for d in $raw; do trimmed+=( "${d%%:*}" ); done
  if (( ${#raw} == 1 )); then
    trimmed+=( "${raw[1]}" )
    raw+=( "${trimmed[1]}" )
  fi

  compadd -d raw -- $trimmed
}

if [ -n "${ZSH_VERSION:-}" ]; then
  autoload -Uz compinit
  compinit
  compdef _complete_foo_zsh foo
elif [ -n "${BASH_VERSION:-}" ]; then
  complete -F _complete_foo_bash foo
fi</code></pre>
</div>
<p>And can be used in both Bash or Zsh to provide an identical user experience:</p>
<div>
<ul>
<li>
<p>Showing possible tab-completions when there are multiple available</p>
</li>
<li>
<p>Showing command or flag descriptions (even though this is not natively supported by Bash)</p>
</li>
<li>
<p>Performing partial or entire-word completions</p>
</li>
<li>
<p>Showing the description or documentation when <code>&lt;TAB&gt;</code>ing on an already-completed word</p>
</li>
</ul>
</div>
<div>
<pre><code data-lang="console">$ foo &lt;TAB&gt;
apple: a common fruit                     banana: starchy and high in potassium     durian: stinky spiky fruit
apricot: sour fruit with a large stone    cherry: small and sweet with a large pit

$ foo a&lt;TAB&gt;
$ foo ap

$ foo ap&lt;TAB&gt;
apple: a common fruit                   apricot: sour fruit with a large stone

$ foo app&lt;TAB&gt;
$ foo apple

$ foo apple&lt;TAB&gt;
apple                  apple: a common fruit</code></pre>
</div>
<p>The actual docs for each shell‚Äôs tab-completion system contains a lot more detail (e.g.
<a href="https://zsh.sourceforge.io/Doc/Release/Completion-System.html">72 pages</a> for Zsh!), and
there are definitely many different ways you can set up your tab-completion scripts.
This blog post just aims to provide the simplest working example that works in both
Bash and Zsh, so hopefully you can understand it well enough to integrate into
your own projects.</p>
</div>
</div>

</article>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Abogen ‚Äì Generate audiobooks from EPUBs, PDFs and text (250 pts)]]></title>
            <link>https://github.com/denizsafak/abogen</link>
            <guid>44853064</guid>
            <pubDate>Sun, 10 Aug 2025 05:56:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/denizsafak/abogen">https://github.com/denizsafak/abogen</a>, See on <a href="https://news.ycombinator.com/item?id=44853064">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><h2 tabindex="-1" dir="auto">abogen <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/abogen/assets/icon.ico"><img width="40px" title="abogen icon" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/abogen/assets/icon.ico"></a></h2><a id="user-content-abogen-" aria-label="Permalink: abogen " href="#abogen-"></a></div>
<p dir="auto"><a href="https://github.com/denizsafak/abogen/actions"><img src="https://github.com/denizsafak/abogen/actions/workflows/test_pip.yml/badge.svg" alt="Build Status"></a>
<a href="https://github.com/denizsafak/abogen/releases/latest"><img src="https://camo.githubusercontent.com/80f34663a4a26d077050a300729c88d9b130c4da02f733425c2146b15ea99fa1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f64656e697a736166616b2f61626f67656e" alt="GitHub Release" data-canonical-src="https://img.shields.io/github/v/release/denizsafak/abogen"></a>
<a href="https://pypi.org/project/abogen/" rel="nofollow"><img src="https://camo.githubusercontent.com/6c83cc79631886146a58d279a7b42aca1febcc6e93417836ad81ded7547cbf5e/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f61626f67656e" alt="Abogen PyPi Python Versions" data-canonical-src="https://img.shields.io/pypi/pyversions/abogen"></a>
<a href="https://github.com/denizsafak/abogen/releases/latest"><img src="https://camo.githubusercontent.com/1ddac32b3e7b4a76e66e2ad641d8f0a3d0c9cc3e0defa693f169cdc84fccfc7b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6f732d77696e646f77732532302537432532306c696e75782532302537432532306d61636f732532302d626c7565" alt="Operating Systems" data-canonical-src="https://img.shields.io/badge/os-windows%20%7C%20linux%20%7C%20macos%20-blue"></a>
<a href="https://github.com/psf/black"><img src="https://camo.githubusercontent.com/5bf9e9fa18966df7cb5fac7715bef6b72df15e01a6efa9d616c83f9fcb527fe2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667" alt="Code style: black" data-canonical-src="https://img.shields.io/badge/code%20style-black-000000.svg"></a>
<a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/02e67a3f818beb1ab87c3b8fd2b7403260b1a8f77fb0899dc3ff0e683f5067ed/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d6d61726f6f6e2e737667" alt="License: MIT" data-canonical-src="https://img.shields.io/badge/License-MIT-maroon.svg"></a></p>
<p dir="auto">Abogen is a powerful text-to-speech conversion tool that makes it easy to turn ePub, PDF, or text files into high-quality audio with matching subtitles in seconds. Use it for audiobooks, voiceovers for Instagram, YouTube, TikTok, or any project that needs natural-sounding text-to-speech, using <a href="https://huggingface.co/hexgrad/Kokoro-82M" rel="nofollow">Kokoro-82M</a>.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/abogen.png"><img title="Abogen Main" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/abogen.png" width="380"></a> <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/abogen2.png"><img title="Abogen Processing" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/abogen2.png" width="380"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Demo</h2><a id="user-content-demo" aria-label="Permalink: Demo" href="#demo"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description demo.mp4">demo.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/39929354/437639906-cb66512d-0a52-48c3-bda4-f1e6a03fb8d6.mp4?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTQ4MTg1MDEsIm5iZiI6MTc1NDgxODIwMSwicGF0aCI6Ii8zOTkyOTM1NC80Mzc2Mzk5MDYtY2I2NjUxMmQtMGE1Mi00OGMzLWJkYTQtZjFlNmEwM2ZiOGQ2Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA4MTAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwODEwVDA5MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNjZjA4YjU5MmNlODcxMzMxMDQwYjBlNWQ4YjAzMDRmZjUyM2E5YjU0MTZmODRhOTYzODFlYzI0ZDZmYzYyNTkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.rWfnnubrVzoCKOG2qJlSqfRQTfboP0i2XrHy75xbOOI" data-canonical-src="https://private-user-images.githubusercontent.com/39929354/437639906-cb66512d-0a52-48c3-bda4-f1e6a03fb8d6.mp4?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTQ4MTg1MDEsIm5iZiI6MTc1NDgxODIwMSwicGF0aCI6Ii8zOTkyOTM1NC80Mzc2Mzk5MDYtY2I2NjUxMmQtMGE1Mi00OGMzLWJkYTQtZjFlNmEwM2ZiOGQ2Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA4MTAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwODEwVDA5MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNjZjA4YjU5MmNlODcxMzMxMDQwYjBlNWQ4YjAzMDRmZjUyM2E5YjU0MTZmODRhOTYzODFlYzI0ZDZmYzYyNTkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.rWfnnubrVzoCKOG2qJlSqfRQTfboP0i2XrHy75xbOOI" controls="controls" muted="muted">

  </video>
</details>

<blockquote>
<p dir="auto">This demo was generated in just 5&nbsp;seconds, producing ‚àº1&nbsp;minute of audio with perfectly synced subtitles. To create a similar video, see <a href="https://github.com/denizsafak/abogen/tree/main/demo">the demo guide</a>.</p>
</blockquote>
<div dir="auto"><h2 tabindex="-1" dir="auto"><code>How to install?</code> <a href="https://pypi.org/project/abogen/" rel="nofollow"><img src="https://camo.githubusercontent.com/6c83cc79631886146a58d279a7b42aca1febcc6e93417836ad81ded7547cbf5e/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f61626f67656e" alt="Abogen Compatible PyPi Python Versions" data-canonical-src="https://img.shields.io/pypi/pyversions/abogen"></a></h2><a id="user-content-how-to-install-" aria-label="Permalink: How to install?" href="#how-to-install-"></a></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Windows</h3><a id="user-content-windows" aria-label="Permalink: Windows" href="#windows"></a></p>
<p dir="auto">Go to <a href="https://github.com/espeak-ng/espeak-ng/releases/latest">espeak-ng latest release</a> download and run the *.msi file.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">OPTION 1: Install using script</h4><a id="user-content-option-1-install-using-script" aria-label="Permalink: OPTION 1: Install using script" href="#option-1-install-using-script"></a></p>
<ol dir="auto">
<li><a href="https://github.com/denizsafak/abogen/archive/refs/heads/main.zip">Download</a> the repository</li>
<li>Extract the ZIP file</li>
<li>Run <code>WINDOWS_INSTALL.bat</code> by double-clicking it</li>
</ol>
<p dir="auto">This method handles everything automatically - installing all dependencies including CUDA in a self-contained environment without requiring a separate Python installation. (You still need to install <a href="https://github.com/espeak-ng/espeak-ng/releases/latest">espeak-ng</a>.)</p>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">You don't need to install Python separately. The script will install Python automatically.</p>
</div>
<p dir="auto"><h4 tabindex="-1" dir="auto">OPTION 2: Install using pip</h4><a id="user-content-option-2-install-using-pip" aria-label="Permalink: OPTION 2: Install using pip" href="#option-2-install-using-pip"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Create a virtual environment (optional)
mkdir abogen &amp;&amp; cd abogen
python -m venv venv
venv\Scripts\activate

# For NVIDIA GPUs:
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128

# For AMD GPUs:
# Not supported yet, because ROCm is not available on Windows. Use Linux if you have AMD GPU.

# Install abogen
pip install abogen"><pre><span><span>#</span> Create a virtual environment (optional)</span>
mkdir abogen <span>&amp;&amp;</span> <span>cd</span> abogen
python -m venv venv
venv<span>\S</span>cripts<span>\a</span>ctivate

<span><span>#</span> For NVIDIA GPUs:</span>
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128

<span><span>#</span> For AMD GPUs:</span>
<span><span>#</span> Not supported yet, because ROCm is not available on Windows. Use Linux if you have AMD GPU.</span>

<span><span>#</span> Install abogen</span>
pip install abogen</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Mac</h3><a id="user-content-mac" aria-label="Permalink: Mac" href="#mac"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Install espeak-ng
brew install espeak-ng

# Create a virtual environment (recommended)
mkdir abogen &amp;&amp; cd abogen
python3 -m venv venv
source venv/bin/activate

# Install abogen
pip3 install abogen"><pre><span><span>#</span> Install espeak-ng</span>
brew install espeak-ng

<span><span>#</span> Create a virtual environment (recommended)</span>
mkdir abogen <span>&amp;&amp;</span> <span>cd</span> abogen
python3 -m venv venv
<span>source</span> venv/bin/activate

<span><span>#</span> Install abogen</span>
pip3 install abogen</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Linux</h3><a id="user-content-linux" aria-label="Permalink: Linux" href="#linux"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Install espeak-ng
sudo apt install espeak-ng # Ubuntu/Debian
sudo pacman -S espeak-ng # Arch Linux
sudo dnf install espeak-ng # Fedora

# Create a virtual environment (recommended)
mkdir abogen &amp;&amp; cd abogen
python3 -m venv venv
source venv/bin/activate

# Install abogen
pip3 install abogen

# For NVIDIA GPUs:
# Already supported, no need to install CUDA separately.

# For AMD GPUs:
# After installing abogen, we need to uninstall the existing torch package
pip3 uninstall torch 
pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.4"><pre><span><span>#</span> Install espeak-ng</span>
sudo apt install espeak-ng <span><span>#</span> Ubuntu/Debian</span>
sudo pacman -S espeak-ng <span><span>#</span> Arch Linux</span>
sudo dnf install espeak-ng <span><span>#</span> Fedora</span>

<span><span>#</span> Create a virtual environment (recommended)</span>
mkdir abogen <span>&amp;&amp;</span> <span>cd</span> abogen
python3 -m venv venv
<span>source</span> venv/bin/activate

<span><span>#</span> Install abogen</span>
pip3 install abogen

<span><span>#</span> For NVIDIA GPUs:</span>
<span><span>#</span> Already supported, no need to install CUDA separately.</span>

<span><span>#</span> For AMD GPUs:</span>
<span><span>#</span> After installing abogen, we need to uninstall the existing torch package</span>
pip3 uninstall torch 
pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.4</pre></div>
<div dir="auto"><p dir="auto">Tip</p><p dir="auto">If you get <code>WARNING: The script abogen-cli is installed in '/home/username/.local/bin' which is not on PATH.</code> error, run the following command to add it to your PATH:</p>
<div dir="auto" data-snippet-clipboard-copy-content="echo &quot;export PATH=\&quot;/home/$USER/.local/bin:\$PATH\&quot;&quot; >> ~/.bashrc &amp;&amp; source ~/.bashrc"><pre><span>echo</span> <span><span>"</span>export PATH=<span>\"</span>/home/<span>$USER</span>/.local/bin:<span>\$</span>PATH<span>\"</span><span>"</span></span> <span>&gt;&gt;</span> <span>~</span>/.bashrc <span>&amp;&amp;</span> <span>source</span> <span>~</span>/.bashrc</pre></div>
</div>
<div dir="auto"><p dir="auto">Tip</p><p dir="auto">If you get "No matching distribution found" error, try installing it on supported Python (3.10 to 3.12). You can use <a href="https://github.com/pyenv/pyenv">pyenv</a> to manage multiple Python versions easily in Linux. Watch this <a href="https://www.youtube.com/watch?v=MVyb-nI4KyI" rel="nofollow">video</a> by NetworkChuck for a quick guide.</p>
</div>
<blockquote>
<p dir="auto">Special thanks to <a href="https://github.com/hg000125">@hg000125</a> for his contribution in <a href="https://github.com/denizsafak/abogen/issues/23" data-hovercard-type="issue" data-hovercard-url="/denizsafak/abogen/issues/23/hovercard">#23</a>. AMD GPU support is possible thanks to his work.</p>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto"><code>How to run?</code></h2><a id="user-content-how-to-run" aria-label="Permalink: How to run?" href="#how-to-run"></a></p>
<p dir="auto">If you installed using pip, you can simply run the following command to start Abogen:</p>

<div dir="auto"><p dir="auto">Tip</p><p dir="auto">If you installed using the Windows installer <code>(WINDOWS_INSTALL.bat)</code>, It should have created a shortcut in the same folder, or your desktop. You can run it from there. If you lost the shortcut, Abogen is located in <code>python_embedded/Scripts/abogen.exe</code>. You can run it from there directly.</p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto"><code>How to use?</code></h2><a id="user-content-how-to-use" aria-label="Permalink: How to use?" href="#how-to-use"></a></p>
<ol dir="auto">
<li>Drag and drop any ePub, PDF, or text file (or use the built-in text editor)</li>
<li>Configure the settings:
<ul dir="auto">
<li>Set speech speed</li>
<li>Select a voice (or create a custom voice using voice mixer)</li>
<li>Select subtitle generation style (by sentence, word, etc.)</li>
<li>Select output format</li>
<li>Select where to save the output</li>
</ul>
</li>
<li>Hit Start</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto"><code>In action</code></h2><a id="user-content-in-action" aria-label="Permalink: In action" href="#in-action"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/abogen.gif"><img title="Abogen in action" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/abogen.gif" data-animated-image=""></a></p> 
<p dir="auto">Here‚Äôs Abogen in action: in this demo, it processes ‚àº3,000 characters of text in just 11 seconds and turns it into 3 minutes and 28 seconds of audio, and I have a low-end <strong>RTX&nbsp;2060&nbsp;Mobile laptop GPU</strong>. Your results may vary depending on your hardware.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><code>Configuration</code></h2><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Options</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Input Box</strong></td>
<td>Drag and drop <code>ePub</code>, <code>PDF</code>, or <code>.TXT</code> files (or use built-in text editor)</td>
</tr>
<tr>
<td><strong>Queue options</strong></td>
<td>Add multiple files to a queue and process them in batch, with individual settings for each file. See <a href="#queue-mode">Queue mode</a> for more details.</td>
</tr>
<tr>
<td><strong>Speed</strong></td>
<td>Adjust speech rate from <code>0.1x</code> to <code>2.0x</code></td>
</tr>
<tr>
<td><strong>Select Voice</strong></td>
<td>First letter of the language code (e.g., <code>a</code> for American English, <code>b</code> for British English, etc.), second letter is for <code>m</code> for male and <code>f</code> for female.</td>
</tr>
<tr>
<td><strong>Voice mixer</strong></td>
<td>Create custom voices by mixing different voice models with a profile system. See <a href="#voice-mixer">Voice Mixer</a> for more details.</td>
</tr>
<tr>
<td><strong>Voice preview</strong></td>
<td>Listen to the selected voice before processing.</td>
</tr>
<tr>
<td><strong>Generate subtitles</strong></td>
<td><code>Disabled</code>, <code>Sentence</code>, <code>Sentence + Comma</code>, <code>1 word</code>, <code>2 words</code>, <code>3 words</code>, etc. (Represents the number of words in each subtitle entry)</td>
</tr>
<tr>
<td><strong>Output voice format</strong></td>
<td><code>.WAV</code>, <code>.FLAC</code>, <code>.MP3</code>, <code>.OPUS (best compression)</code> and <code>M4B (with chapters)</code> (Special thanks to <a href="https://github.com/jborza">@jborza</a> for chapter support in PR <a href="https://github.com/denizsafak/abogen/pull/10" data-hovercard-type="pull_request" data-hovercard-url="/denizsafak/abogen/pull/10/hovercard">#10</a>)</td>
</tr>
<tr>
<td><strong>Output subtitle format</strong></td>
<td>Configures the subtitle format as <code>SRT (standard)</code>, <code>ASS (wide)</code>, <code>ASS (narrow)</code>, <code>ASS (centered wide)</code>, or <code>ASS (centered narrow)</code>.</td>
</tr>
<tr>
<td><strong>Replace single newlines with spaces</strong></td>
<td>Replaces single newlines with spaces in the text. This is useful for texts that have imaginary line breaks.</td>
</tr>
<tr>
<td><strong>Save location</strong></td>
<td><code>Save next to input file</code>, <code>Save to desktop</code>, or <code>Choose output folder</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Book handler options</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Chapter Control</strong></td>
<td>Select specific <code>chapters</code> from ePUBs or <code>chapters + pages</code> from PDFs.</td>
</tr>
<tr>
<td><strong>Save each chapter separately</strong></td>
<td>Save each chapter in e-books as a separate audio file.</td>
</tr>
<tr>
<td><strong>Create a merged version</strong></td>
<td>Create a single audio file that combines all chapters. (If <code>Save each chapter separately</code> is disabled, this option will be the default behavior.)</td>
</tr>
<tr>
<td><strong>Save in a project folder with metadata</strong></td>
<td>Save the converted items in a project folder with available metadata files.</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Menu options</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Theme</strong></td>
<td>Change the application's theme using <code>System</code>, <code>Light</code>, or <code>Dark</code> options.</td>
</tr>
<tr>
<td><strong>Configure max words per subtitle</strong></td>
<td>Configures the maximum number of words per subtitle entry.</td>
</tr>
<tr>
<td><strong>Configure max lines in log window</strong></td>
<td>Configures the maximum number of lines to display in the log window.</td>
</tr>
<tr>
<td><strong>Separate chapters audio format</strong></td>
<td>Configures the audio format for separate chapters as <code>wav</code>, <code>flac</code>, <code>mp3</code>, or <code>opus</code>.</td>
</tr>
<tr>
<td><strong>Create desktop shortcut</strong></td>
<td>Creates a shortcut on your desktop for easy access.</td>
</tr>
<tr>
<td><strong>Open config directory</strong></td>
<td>Opens the directory where the configuration file is stored.</td>
</tr>
<tr>
<td><strong>Open cache directory</strong></td>
<td>Opens the cache directory where converted text files are stored.</td>
</tr>
<tr>
<td><strong>Clear cache files</strong></td>
<td>Deletes cache files created during the conversion or preview.</td>
</tr>
<tr>
<td><strong>Check for updates at startup</strong></td>
<td>Automatically checks for updates when the program starts.</td>
</tr>
<tr>
<td><strong>Disable Kokoro's internet access</strong></td>
<td>Prevents Kokoro from downloading models or voices from HuggingFace Hub, useful for offline use.</td>
</tr>
<tr>
<td><strong>Reset to default settings</strong></td>
<td>Resets all settings to their default values.</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto"><code>Voice Mixer</code></h2><a id="user-content-voice-mixer" aria-label="Permalink: Voice Mixer" href="#voice-mixer"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/voice_mixer.png"><img title="Abogen Voice Mixer" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/voice_mixer.png"></a></p>
<p dir="auto">With voice mixer, you can create custom voices by mixing different voice models. You can adjust the weight of each voice and save your custom voice as a profile for future use. The voice mixer allows you to create unique and personalized voices. (Huge thanks to <a href="https://github.com/jborza">@jborza</a> for making this possible through his contributions in <a href="https://github.com/denizsafak/abogen/pull/5" data-hovercard-type="pull_request" data-hovercard-url="/denizsafak/abogen/pull/5/hovercard">#5</a>)</p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><code>Queue Mode</code></h2><a id="user-content-queue-mode" aria-label="Permalink: Queue Mode" href="#queue-mode"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/queue.png"><img title="Abogen queue mode" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/queue.png"></a></p>
<p dir="auto">Abogen supports <strong>queue mode</strong>, allowing you to add multiple files to a processing queue. This is useful if you want to convert several files in one batch.</p>
<ul dir="auto">
<li>You can add text files (<code>.txt</code>) directly using the <strong>Add files</strong> button in the Queue Manager. To add PDF or EPUB files, use the input box in the main window and click the <strong>Add to Queue</strong> button.</li>
<li>Each file in the queue keeps the configuration settings that were active when it was added. Changing the main window configuration afterward does <strong>not</strong> affect files already in the queue.</li>
<li>You can view each file's configuration by hovering over them.</li>
</ul>
<p dir="auto">Abogen will process each item in the queue automatically, saving outputs as configured.</p>
<blockquote>
<p dir="auto">Special thanks to <a href="https://github.com/jborza">@jborza</a> for adding queue mode in PR <a href="https://github.com/denizsafak/abogen/pull/35" data-hovercard-type="pull_request" data-hovercard-url="/denizsafak/abogen/pull/35/hovercard">#35</a></p>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto"><code>About Chapter Markers</code></h2><a id="user-content-about-chapter-markers" aria-label="Permalink: About Chapter Markers" href="#about-chapter-markers"></a></p>
<p dir="auto">When you process ePUB or PDF files, Abogen converts them into text files stored in your cache directory. When you click "Edit," you're actually modifying these converted text files. In these text files, you'll notice tags that look like this:</p>
<div data-snippet-clipboard-copy-content="<<CHAPTER_MARKER:Chapter Title>>"><pre><code>&lt;&lt;CHAPTER_MARKER:Chapter Title&gt;&gt;
</code></pre></div>
<p dir="auto">These are chapter markers. They are automatically added when you process ePUB or PDF files, based on the chapters you select. They serve an important purpose:</p>
<ul dir="auto">
<li>Allow you to split the text into separate audio files for each chapter</li>
<li>Save time by letting you reprocess only specific chapters if errors occur, rather than the entire file</li>
</ul>
<p dir="auto">You can manually add these markers to plain text files for the same benefits. Simply include them in your text like this:</p>
<div data-snippet-clipboard-copy-content="<<CHAPTER_MARKER:Introduction>>
This is the beginning of my text...  

<<CHAPTER_MARKER:Main Content>> 
Here's another part...  "><pre><code>&lt;&lt;CHAPTER_MARKER:Introduction&gt;&gt;
This is the beginning of my text...  

&lt;&lt;CHAPTER_MARKER:Main Content&gt;&gt; 
Here's another part...  
</code></pre></div>
<p dir="auto">When you process the text file, Abogen will detect these markers automatically and ask if you want to save each chapter separately and create a merged version.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/chapter_marker.png"><img src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/chapter_marker.png" alt="Abogen Chapter Marker"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><code>About Metadata Tags</code></h2><a id="user-content-about-metadata-tags" aria-label="Permalink: About Metadata Tags" href="#about-metadata-tags"></a></p>
<p dir="auto">Similar to chapter markers, it is possible to add metadata tags for <code>M4B</code> files. This is useful for audiobook players that support metadata, allowing you to add information like title, author, year, etc. Abogen automatically adds these tags when you process ePUB or PDF files, but you can also add them manually to your text files. Add metadata tags <strong>at the beginning of your text file</strong> like this:</p>
<div data-snippet-clipboard-copy-content="<<METADATA_TITLE:Title>>
<<METADATA_ARTIST:Author>>
<<METADATA_ALBUM:Album Title>>
<<METADATA_YEAR:Year>>
<<METADATA_ALBUM_ARTIST:Album Artist>>
<<METADATA_COMPOSER:Narrator>>
<<METADATA_GENRE:Audiobook>>"><pre><code>&lt;&lt;METADATA_TITLE:Title&gt;&gt;
&lt;&lt;METADATA_ARTIST:Author&gt;&gt;
&lt;&lt;METADATA_ALBUM:Album Title&gt;&gt;
&lt;&lt;METADATA_YEAR:Year&gt;&gt;
&lt;&lt;METADATA_ALBUM_ARTIST:Album Artist&gt;&gt;
&lt;&lt;METADATA_COMPOSER:Narrator&gt;&gt;
&lt;&lt;METADATA_GENRE:Audiobook&gt;&gt;
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto"><code>Supported Languages</code></h2><a id="user-content-supported-languages" aria-label="Permalink: Supported Languages" href="#supported-languages"></a></p>
<div data-snippet-clipboard-copy-content="# üá∫üá∏ 'a' => American English, üá¨üáß 'b' => British English
# üá™üá∏ 'e' => Spanish es
# üá´üá∑ 'f' => French fr-fr
# üáÆüá≥ 'h' => Hindi hi
# üáÆüáπ 'i' => Italian it
# üáØüáµ 'j' => Japanese: pip install misaki[ja]
# üáßüá∑ 'p' => Brazilian Portuguese pt-br
# üá®üá≥ 'z' => Mandarin Chinese: pip install misaki[zh]"><pre><code># üá∫üá∏ 'a' =&gt; American English, üá¨üáß 'b' =&gt; British English
# üá™üá∏ 'e' =&gt; Spanish es
# üá´üá∑ 'f' =&gt; French fr-fr
# üáÆüá≥ 'h' =&gt; Hindi hi
# üáÆüáπ 'i' =&gt; Italian it
# üáØüáµ 'j' =&gt; Japanese: pip install misaki[ja]
# üáßüá∑ 'p' =&gt; Brazilian Portuguese pt-br
# üá®üá≥ 'z' =&gt; Mandarin Chinese: pip install misaki[zh]
</code></pre></div>
<p dir="auto">For a complete list of supported languages and voices, refer to Kokoro's <a href="https://huggingface.co/hexgrad/Kokoro-82M/blob/main/VOICES.md" rel="nofollow">VOICES.md</a>. To listen to sample audio outputs, see <a href="https://huggingface.co/hexgrad/Kokoro-82M/blob/main/SAMPLES.md" rel="nofollow">SAMPLES.md</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><code>MPV Config</code></h2><a id="user-content-mpv-config" aria-label="Permalink: MPV Config" href="#mpv-config"></a></p>
<p dir="auto">I highly recommend using <a href="https://mpv.io/installation/" rel="nofollow">MPV</a> to play your audio files, as it supports displaying subtitles even without a video track. Here's my <code>mpv.conf</code>:</p>
<div data-snippet-clipboard-copy-content="# --- MPV Settings ---
save-position-on-quit
keep-open=yes
# --- Subtitle ---
sub-ass-override=no
sub-margin-y=50
sub-margin-x=50
# --- Audio Quality ---
audio-spdif=ac3,dts,eac3,truehd,dts-hd
audio-channels=auto
audio-samplerate=48000
volume-max=200"><pre><code># --- MPV Settings ---
save-position-on-quit
keep-open=yes
# --- Subtitle ---
sub-ass-override=no
sub-margin-y=50
sub-margin-x=50
# --- Audio Quality ---
audio-spdif=ac3,dts,eac3,truehd,dts-hd
audio-channels=auto
audio-samplerate=48000
volume-max=200
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto"><code>Docker Guide</code></h2><a id="user-content-docker-guide" aria-label="Permalink: Docker Guide" href="#docker-guide"></a></p>
<p dir="auto">If you want to run Abogen in a Docker container:</p>
<ol dir="auto">
<li><a href="https://github.com/denizsafak/abogen/archive/refs/heads/main.zip">Download the repository</a> and extract, or clone it using git.</li>
<li>Go to <code>abogen</code> folder. You should see <code>Dockerfile</code> there.</li>
<li>Open your termminal in that directory and run the following commands:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="# Build the Docker image:
docker build --progress plain -t abogen .

# Note that building the image may take a while.
# After building is complete, run the Docker container:

# Windows
docker run --name abogen -v %cd%:/shared -p 5800:5800 -p 5900:5900 --gpus all abogen

# Linux
docker run --name abogen -v $(pwd):/shared -p 5800:5800 -p 5900:5900 --gpus all abogen

# MacOS
docker run --name abogen -v $(pwd):/shared -p 5800:5800 -p 5900:5900 abogen

# We expose port 5800 for use by a web browser, 5900 if you want to connect with a VNC client."><pre><span><span>#</span> Build the Docker image:</span>
docker build --progress plain -t abogen <span>.</span>

<span><span>#</span> Note that building the image may take a while.</span>
<span><span>#</span> After building is complete, run the Docker container:</span>

<span><span>#</span> Windows</span>
docker run --name abogen -v %cd%:/shared -p 5800:5800 -p 5900:5900 --gpus all abogen

<span><span>#</span> Linux</span>
docker run --name abogen -v <span><span>$(</span>pwd<span>)</span></span>:/shared -p 5800:5800 -p 5900:5900 --gpus all abogen

<span><span>#</span> MacOS</span>
docker run --name abogen -v <span><span>$(</span>pwd<span>)</span></span>:/shared -p 5800:5800 -p 5900:5900 abogen

<span><span>#</span> We expose port 5800 for use by a web browser, 5900 if you want to connect with a VNC client.</span></pre></div>
<p dir="auto">Abogen launches automatically inside the container.</p>
<ul dir="auto">
<li>You can access it via a web browser at <a href="http://localhost:5800/" rel="nofollow">http://localhost:5800</a> or connect to it using a VNC client at <code>localhost:5900</code>.</li>
<li>You can use <code>/shared</code> directory to share files between your host and the container.</li>
<li>For later use, start it with <code>docker start abogen</code> and stop it with <code>docker stop abogen</code>.</li>
</ul>
<p dir="auto">Known issues:</p>
<ul dir="auto">
<li>Audio preview is not working inside container (ALSA error).</li>
<li><code>Open cache directory</code> and <code>Open configuration directory</code> options in settings not working. (Tried pcmanfm, did not work with Abogen).</li>
</ul>
<p dir="auto">(Special thanks to <a href="https://www.reddit.com/user/geo38/" rel="nofollow">@geo38</a> from Reddit, who provided the Dockerfile and instructions in <a href="https://www.reddit.com/r/selfhosted/comments/1k8x1yo/comment/mpe0bz8/" rel="nofollow">this comment</a>.)</p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><code>Similar Projects</code></h2><a id="user-content-similar-projects" aria-label="Permalink: Similar Projects" href="#similar-projects"></a></p>
<p dir="auto">Abogen is a standalone project, but it is inspired by and shares some similarities with other projects. Here are a few:</p>
<ul dir="auto">
<li><a href="https://github.com/santinic/audiblez">audiblez</a>: Generate audiobooks from e-books. <strong>(Has CLI and GUI support)</strong></li>
<li><a href="https://github.com/plusuncold/autiobooks">autiobooks</a>: Automatically convert epubs to audiobooks</li>
<li><a href="https://github.com/mateogon/pdf-narrator">pdf-narrator</a>: Convert your PDFs and EPUBs into audiobooks effortlessly.</li>
<li><a href="https://github.com/p0n1/epub_to_audiobook">epub_to_audiobook</a>: EPUB to audiobook converter, optimized for Audiobookshelf</li>
<li><a href="https://github.com/DrewThomasson/ebook2audiobook">ebook2audiobook</a>: Convert ebooks to audiobooks with chapters and metadata using dynamic AI models and voice cloning</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto"><code>Roadmap</code></h2><a id="user-content-roadmap" aria-label="Permalink: Roadmap" href="#roadmap"></a></p>
<ul>
<li> Add OCR scan feature for PDF files using docling/teserract.</li>
<li> Add chapter metadata for .m4a files. (Issue <a href="https://github.com/denizsafak/abogen/issues/9" data-hovercard-type="issue" data-hovercard-url="/denizsafak/abogen/issues/9/hovercard">#9</a>, PR <a href="https://github.com/denizsafak/abogen/pull/10" data-hovercard-type="pull_request" data-hovercard-url="/denizsafak/abogen/pull/10/hovercard">#10</a>)</li>
<li> Add support for different languages in GUI.</li>
<li> Add voice formula feature that enables mixing different voice models. (Issue <a href="https://github.com/denizsafak/abogen/issues/1" data-hovercard-type="issue" data-hovercard-url="/denizsafak/abogen/issues/1/hovercard">#1</a>, PR <a href="https://github.com/denizsafak/abogen/pull/5" data-hovercard-type="pull_request" data-hovercard-url="/denizsafak/abogen/pull/5/hovercard">#5</a>)</li>
<li> Add support for kokoro-onnx (If it's necessary).</li>
<li> Add dark mode.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto"><code>Troubleshooting</code></h2><a id="user-content-troubleshooting" aria-label="Permalink: Troubleshooting" href="#troubleshooting"></a></p>
<p dir="auto">If you encounter any issues while running Abogen, try launching it from the command line with:</p>

<p dir="auto">This will start Abogen in command-line mode and display detailed error messages. Please open a new issue on the <a href="https://github.com/denizsafak/abogen/issues">Issues</a> page with the error message and a description of your problem.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><code>Contributing</code></h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">I welcome contributions! If you have ideas for new features, improvements, or bug fixes, please fork the repository and submit a pull request.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">For developers and contributors</h3><a id="user-content-for-developers-and-contributors" aria-label="Permalink: For developers and contributors" href="#for-developers-and-contributors"></a></p>
<p dir="auto">If you'd like to modify the code and contribute to development, you can <a href="https://github.com/denizsafak/abogen/archive/refs/heads/main.zip">download the repository</a>, extract it and run the following commands to build <strong>or</strong> install the package:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Go to the directory where you extracted the repository and run:
pip install -e .      # Installs the package in editable mode
pip install build     # Install the build package
python -m build       # Builds the package in dist folder (optional)
abogen                # Opens the GUI"><pre><span><span>#</span> Go to the directory where you extracted the repository and run:</span>
pip install -e <span>.</span>      <span><span>#</span> Installs the package in editable mode</span>
pip install build     <span><span>#</span> Install the build package</span>
python -m build       <span><span>#</span> Builds the package in dist folder (optional)</span>
abogen                <span><span>#</span> Opens the GUI</span></pre></div>
<p dir="auto">Feel free to explore the code and make any changes you like.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><code>Credits</code></h2><a id="user-content-credits" aria-label="Permalink: Credits" href="#credits"></a></p>
<ul dir="auto">
<li>Abogen uses <a href="https://github.com/hexgrad/kokoro">Kokoro</a> for its high-quality, natural-sounding text-to-speech synthesis. Huge thanks to the Kokoro team for making this possible.</li>
<li>Thanks to <a href="https://github.com/wojiushixiaobai">@wojiushixiaobai</a> for <a href="https://github.com/wojiushixiaobai/Python-Embed-Win64">Embedded Python</a> packages. These modified packages include pip pre-installed, enabling Abogen to function as a standalone application without requiring users to separately install Python in Windows.</li>
<li>Thanks to creators of <a href="https://github.com/aerkalov/ebooklib">EbookLib</a>, a Python library for reading and writing ePub files, which is used for extracting text from ePub files.</li>
<li>Special thanks to the <a href="https://www.riverbankcomputing.com/software/pyqt/" rel="nofollow">PyQt</a> team for providing the cross-platform GUI toolkit that powers Abogen's interface.</li>
<li>Icons: <a href="https://icons8.com/icon/aRiu1GGi6Aoe/usa" rel="nofollow">US</a>, <a href="https://icons8.com/icon/t3NE3BsOAQwq/great-britain" rel="nofollow">Great Britain</a>, <a href="https://icons8.com/icon/ly7tzANRt33n/spain" rel="nofollow">Spain</a>, <a href="https://icons8.com/icon/3muzEmi4dpD5/france" rel="nofollow">France</a>, <a href="https://icons8.com/icon/esGVrxg9VCJ1/india" rel="nofollow">India</a>, <a href="https://icons8.com/icon/PW8KZnP7qXzO/italy" rel="nofollow">Italy</a>, <a href="https://icons8.com/icon/McQbrq9qaQye/japan" rel="nofollow">Japan</a>, <a href="https://icons8.com/icon/zHmH8HpOmM90/brazil" rel="nofollow">Brazil</a>, <a href="https://icons8.com/icon/Ej50Oe3crXwF/china" rel="nofollow">China</a>, <a href="https://icons8.com/icon/uI49hxbpxTkp/female" rel="nofollow">Female</a>, <a href="https://icons8.com/icon/12351/male" rel="nofollow">Male</a>, <a href="https://icons8.com/icon/21698/adjust" rel="nofollow">Adjust</a> and <a href="https://icons8.com/icon/GskSeVoroQ7u/voice-id" rel="nofollow">Voice Id</a> icons by <a href="https://icons8.com/" rel="nofollow">Icons8</a>.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto"><code>License</code></h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project is available under the MIT License - see the <a href="https://github.com/denizsafak/abogen/blob/main/LICENSE">LICENSE</a> file for details.
<a href="https://github.com/hexgrad/kokoro">Kokoro</a> is licensed under <a href="https://github.com/hexgrad/kokoro/blob/main/LICENSE">Apache-2.0</a> which allows commercial use, modification, distribution, and private use.</p>
<div dir="auto"><p dir="auto">Important</p><p dir="auto">Subtitle generation currently works only for English. This is because Kokoro provides timestamp tokens only for English text. If you want subtitles in other languages, please request this feature in the <a href="https://github.com/hexgrad/kokoro">Kokoro project</a>. For more technical details, see <a href="https://github.com/hexgrad/kokoro/blob/6d87f4ae7abc2d14dbc4b3ef2e5f19852e861ac2/kokoro/pipeline.py#L383">this line</a> in the Kokoro's code.</p>
</div>
<blockquote>
<p dir="auto">Tags: audiobook, kokoro, text-to-speech, TTS, audiobook generator, audiobooks, text to speech, audiobook maker, audiobook creator, audiobook generator, voice-synthesis, text to audio, text to audio converter, text to speech converter, text to speech generator, text to speech software, text to speech app, epub to audio, pdf to audio, content-creation, media-generation</p>
</blockquote>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Melonking Website (104 pts)]]></title>
            <link>https://melonking.net/</link>
            <guid>44852582</guid>
            <pubDate>Sun, 10 Aug 2025 03:38:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://melonking.net/">https://melonking.net/</a>, See on <a href="https://news.ycombinator.com/item?id=44852582">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="everything">
            
            

            <canvas id="field" width="1400" height="900"></canvas>

            <svg id="svg_sides" width="100%" height="100%">
                <defs>
                    <pattern id="pattern" patternUnits="userSpaceOnUse" width="128" height="128">
                        <image xlink:href="/images/pdj50186.png" x="0" y="0" width="128" height="128"></image>
                    </pattern>
                    <mask id="svg_mask_side">
                        <rect x="0" y="0" width="100%" height="100%" fill="white"></rect>
                        <image x="50%" y="50%" width="1200" height="1200" href="/images/e-pome-mask.svg" transform="translate(-600,-600)"></image>
                    </mask>
                </defs>
                <rect id="svg_overlay_side" x="0" y="0" width="100%" height="100%"></rect>
            </svg>

            <div id="wrapper">
                

                <p><a href="https://melonking.net/melon.html"><img id="enter" src="https://melonking.net/images/enter_0034-new.gif" onmouseover="enterAudio.play();"></a></p><p>
                    You are now exiting the information superhighway!<br>
                    <span>Enable Auto-Play Audio - Works best in Firefox!</span><br>
                    Welcome to Melonland :^]
                </p>

                
            </div>

            <p>Music: johnny_ripper - by the sea ‚ò∫</p>

            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Just Buy Nothing: A fake online store to combat shopping addiction (208 pts)]]></title>
            <link>https://justbuynothing.com/</link>
            <guid>44851590</guid>
            <pubDate>Sun, 10 Aug 2025 00:12:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://justbuynothing.com/">https://justbuynothing.com/</a>, See on <a href="https://news.ycombinator.com/item?id=44851590">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[GPT-5: Overdue, overhyped and underwhelming. And that's not the worst of it (274 pts)]]></title>
            <link>https://garymarcus.substack.com/p/gpt-5-overdue-overhyped-and-underwhelming</link>
            <guid>44851557</guid>
            <pubDate>Sun, 10 Aug 2025 00:06:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://garymarcus.substack.com/p/gpt-5-overdue-overhyped-and-underwhelming">https://garymarcus.substack.com/p/gpt-5-overdue-overhyped-and-underwhelming</a>, See on <a href="https://news.ycombinator.com/item?id=44851557">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>GenerativeAI had a truly bad week. The late and underwhelming arrival of GPT-5 wasn‚Äôt even the worst part.  But before we get to the worst part (spoiler alert: a new research paper that I will discuss towards the end), let‚Äôs review GPT-5‚Äôs shambolic debut.</p><p>This was supposed to be the week when OpenAI finally cemented its dominance. The long rumored GPT-5 was about to arrive. Sam Altman was so cocky that in advance of the livestream debut  he posted a screen grab from a Star War film, Rogue One:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!BP8o!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50372667-531f-4982-a300-36ac73ce6c82_1202x837.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!BP8o!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50372667-531f-4982-a300-36ac73ce6c82_1202x837.png 424w, https://substackcdn.com/image/fetch/$s_!BP8o!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50372667-531f-4982-a300-36ac73ce6c82_1202x837.png 848w, https://substackcdn.com/image/fetch/$s_!BP8o!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50372667-531f-4982-a300-36ac73ce6c82_1202x837.png 1272w, https://substackcdn.com/image/fetch/$s_!BP8o!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50372667-531f-4982-a300-36ac73ce6c82_1202x837.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!BP8o!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50372667-531f-4982-a300-36ac73ce6c82_1202x837.png" width="1202" height="837" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/50372667-531f-4982-a300-36ac73ce6c82_1202x837.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:837,&quot;width&quot;:1202,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:618063,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://garymarcus.substack.com/i/170534403?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50372667-531f-4982-a300-36ac73ce6c82_1202x837.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!BP8o!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50372667-531f-4982-a300-36ac73ce6c82_1202x837.png 424w, https://substackcdn.com/image/fetch/$s_!BP8o!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50372667-531f-4982-a300-36ac73ce6c82_1202x837.png 848w, https://substackcdn.com/image/fetch/$s_!BP8o!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50372667-531f-4982-a300-36ac73ce6c82_1202x837.png 1272w, https://substackcdn.com/image/fetch/$s_!BP8o!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50372667-531f-4982-a300-36ac73ce6c82_1202x837.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>People ate it up. It got almost six million views.</p><p>The cockiness continued at the opening of the livestream. Altman, ever the showman, claimed</p><blockquote><p><em>We think you will love using GPT-5 much more than any previous Al. It is useful it is smart it is fast [and[ intuitive. GPT-3 was sort of like talking to a high school student.</em></p><p><em><span>There were flashes of brilliance lots of annoyance but people started to use it and get some value out of it. GPT-4o maybe it was like talking to a college student‚Ä¶. With GPT-5 now it's like talking to an expert ‚Äî- a legitimate PhD level expert in anything any area you need on demand they can help you with whatever your goals are. </span><strong> </strong></em></p></blockquote><p>What the mainstream media mostly hasn‚Äôt told you yet is that a few days later, hardly anybody is buying Altman‚Äôs story. </p><p><a href="https://www.change.org/p/please-keep-gpt-4o-available-on-chatgpt?source_location=topics_page&amp;pt=AVBldGl0aW9uAPoMPR0AAAAAaJWTPzdi6sUzNzk5ZDJjOA%3D%3D" rel="">3,000 people hated GPT-5 so much they petitioned ‚Äî successfully ‚Äî to get one of the older models back</a><span>. At OpenAI reddit, usually quite pro OpenAI, the lead post was this:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!QwV2!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1a2195-045d-4426-93ef-03e5689c4a99_1223x1489.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!QwV2!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1a2195-045d-4426-93ef-03e5689c4a99_1223x1489.png 424w, https://substackcdn.com/image/fetch/$s_!QwV2!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1a2195-045d-4426-93ef-03e5689c4a99_1223x1489.png 848w, https://substackcdn.com/image/fetch/$s_!QwV2!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1a2195-045d-4426-93ef-03e5689c4a99_1223x1489.png 1272w, https://substackcdn.com/image/fetch/$s_!QwV2!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1a2195-045d-4426-93ef-03e5689c4a99_1223x1489.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!QwV2!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1a2195-045d-4426-93ef-03e5689c4a99_1223x1489.png" width="1223" height="1489" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1f1a2195-045d-4426-93ef-03e5689c4a99_1223x1489.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1489,&quot;width&quot;:1223,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:556073,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:&quot;&quot;,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://garymarcus.substack.com/i/170534403?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1a2195-045d-4426-93ef-03e5689c4a99_1223x1489.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/$s_!QwV2!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1a2195-045d-4426-93ef-03e5689c4a99_1223x1489.png 424w, https://substackcdn.com/image/fetch/$s_!QwV2!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1a2195-045d-4426-93ef-03e5689c4a99_1223x1489.png 848w, https://substackcdn.com/image/fetch/$s_!QwV2!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1a2195-045d-4426-93ef-03e5689c4a99_1223x1489.png 1272w, https://substackcdn.com/image/fetch/$s_!QwV2!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1a2195-045d-4426-93ef-03e5689c4a99_1223x1489.png 1456w" sizes="100vw"></picture></div></a></figure></div><p><span>As they say on Twitter, Altman‚Äôs Death Star tweet </span><a href="https://x.com/garymarcus/status/1953887884341391775?s=61" rel="">didn‚Äôt age well</a><span>.</span></p><p><span>Meanwhile, as for that </span><em>Star Wars </em><span>movie</span><em>,</em><span> more than a few people end up wondering if Altman has ever watched the film.  For those unfamilar, what happens next is‚Ä¶  the Rebel Alliance blows up the Death Star.</span></p><p>¬ß</p><p>OpenAI basically blew itself up ‚Äì  and not in a good way.  Aside from a few influencers who praise every new model, the dominant reaction was major disappointment. </p><p>A system that could have gone a week without the community finding boatloads of ridiculous errors and hallucinations would have genuinely impressed me. </p><p><span>Instead, within hours, people were posting </span><a href="https://x.com/colin_fraser/status/1953668411029909892?s=61" rel="">the usual ridiculous errors</a><span>. A Hacker News thread </span><a href="https://news.ycombinator.com/item?id=44827210" rel="">brutally dissected the live, vibe-coded  demo of the Bernoulli effect</a><span>. Multiple posts identified benchmarks where</span><a href="https://x.com/burny_tech/status/1953767104366146037?s=61" rel=""> performance was subpar</a><span>. (Not just the ARC-AGI-2 I had noted in my hot take a few days ago, either). Still others found the </span><a href="https://x.com/scaling01/status/1954292296704250005?s=61" rel="">new automatic ‚Äúrouting‚Äù mechanism to be a mess</a><span>.   It was essentially the same experience as with every earlier model. Big promises, stupid errors.</span></p><p><span>But this time, the reaction was different. Because </span><em>expectations</em><span> were through the roof, a </span><em>huge</em><span> number of people viewed GPT 5 as a major letdown.  By the end of the night, OpenAI‚Äôs street cred had dramatically fallen. On the question of ‚Äúwhich company [will have] the best AI model at the end of August‚Äù, a Polymarket poll charted OpenAI </span><a href="https://x.com/scaling01/status/1953515099257282763?s=61" rel="">dropping from 75% to 14% in the space of an hour</a><span>.</span></p><p><span>Typical was a comment from Andres Franco, on X ‚ÄúGPT 5 has been a huge letdown, way more than I expected‚Äù. Another reader, previously an OpenAI fan, told me ‚Äúo3 was a shit good model, [whereas GPT-5] was </span><a href="https://x.com/anoop_331/status/1954211361853964420?s=61%20https://x.com/anoop_331/status/1954211361853964420?s=61" rel="">an utter disappointment, especially given the kind of hype towards its release.</a><span>‚Äù An NBA President DM‚Äôd me to say ‚Äúchatgpt 5 still failed my two fav problems to give LLMs‚Äù. </span></p><p><span>Loads of people seemed to </span><a href="https://x.com/egidemurisa/status/1954189835012383164?s=61" rel="">sincerely expect GPT-5 was going to be AGI</a><span>. It doesn‚Äôt take decades of training to see that GPT-4 was not that.</span></p><p><span>Even my anti-fan club (‚ÄúGary haters‚Äù in modern parlance) were forced to give </span><em>me</em><span>  props. Tweets like ‚Äú</span><a href="https://x.com/mgonto/status/1953839860013207669?s=61" rel="">The saddest thing in my day is that @garymarcus is right</a><span>‚Äù became trendy.  </span></p><p><span>With a more positive framing, freelance journalist Bryan McMahon wrote to me, ‚Äú</span><em>We all saw GPT-5‚Äôs reveal fall flat yesterday‚Äîso flat, in fact, that many online dubbed it ‚ÄúGary Marcus Day‚Äù for proving your consistent criticism about the structural flaws of large language models correct</em><span>.‚Äù </span></p><p>¬ß</p><p><span>And, indeed, </span><a href="https://open.substack.com/pub/garymarcus/p/what-to-expect-when-youre-expecting-62e?r=8tdk6&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcomeOnShare=false" rel="">much as I anticipated here two weeks ago</a><span>, the problems I have been pointing out over the last quarter century still lingered. Consider for example the </span><a href="https://open.substack.com/pub/garymarcus/p/generative-ais-crippling-and-widespread?r=8tdk6&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcomeOnShare=false" rel="">critique I gave re: chess and world models</a><span> at the end of  June. My go-to source on this, Mathieu Acher, quickly confirmed</span><strong><a href="https://blog.mathieuacher.com/GPT5-IllegalChessBench/" rel=""> </a></strong><a href="https://blog.mathieuacher.com/GPT5-IllegalChessBench/" rel="">that GPT-5 still struggles with following the rules</a><span>. A Tufts professor sent me a further example, in which GPT-5 becomes completely lost in the course of discussing a simple chess problem.</span></p><p>Or take visual comprehension:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!dPhm!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0fd7cde-8fdb-4fb6-b126-615b8a5cb835_1359x2132.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!dPhm!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0fd7cde-8fdb-4fb6-b126-615b8a5cb835_1359x2132.png 424w, https://substackcdn.com/image/fetch/$s_!dPhm!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0fd7cde-8fdb-4fb6-b126-615b8a5cb835_1359x2132.png 848w, https://substackcdn.com/image/fetch/$s_!dPhm!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0fd7cde-8fdb-4fb6-b126-615b8a5cb835_1359x2132.png 1272w, https://substackcdn.com/image/fetch/$s_!dPhm!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0fd7cde-8fdb-4fb6-b126-615b8a5cb835_1359x2132.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!dPhm!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0fd7cde-8fdb-4fb6-b126-615b8a5cb835_1359x2132.png" width="1359" height="2132" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f0fd7cde-8fdb-4fb6-b126-615b8a5cb835_1359x2132.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:2132,&quot;width&quot;:1359,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1081753,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://garymarcus.substack.com/i/170534403?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0fd7cde-8fdb-4fb6-b126-615b8a5cb835_1359x2132.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!dPhm!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0fd7cde-8fdb-4fb6-b126-615b8a5cb835_1359x2132.png 424w, https://substackcdn.com/image/fetch/$s_!dPhm!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0fd7cde-8fdb-4fb6-b126-615b8a5cb835_1359x2132.png 848w, https://substackcdn.com/image/fetch/$s_!dPhm!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0fd7cde-8fdb-4fb6-b126-615b8a5cb835_1359x2132.png 1272w, https://substackcdn.com/image/fetch/$s_!dPhm!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0fd7cde-8fdb-4fb6-b126-615b8a5cb835_1359x2132.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>The </span><a href="https://garymarcus.substack.com/p/dont-ride-this-bike-generative-ais" rel="">challenge of parts and wholes in generative images</a><span> that Ernest Davis and I discussed here in December fared no better. (Some argued that this is because GPT-5 is still using an older models for generating images, but given that the new thing was supposed to be tantamount to AGI and ‚Äúfully multimodal‚Äù that hardly seems like a compelling excuse.)</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!6xcx!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9048ce3b-83cd-4256-8b82-7c3c33e5bdc0_1598x1213.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!6xcx!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9048ce3b-83cd-4256-8b82-7c3c33e5bdc0_1598x1213.jpeg 424w, https://substackcdn.com/image/fetch/$s_!6xcx!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9048ce3b-83cd-4256-8b82-7c3c33e5bdc0_1598x1213.jpeg 848w, https://substackcdn.com/image/fetch/$s_!6xcx!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9048ce3b-83cd-4256-8b82-7c3c33e5bdc0_1598x1213.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!6xcx!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9048ce3b-83cd-4256-8b82-7c3c33e5bdc0_1598x1213.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!6xcx!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9048ce3b-83cd-4256-8b82-7c3c33e5bdc0_1598x1213.jpeg" width="1456" height="1105" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9048ce3b-83cd-4256-8b82-7c3c33e5bdc0_1598x1213.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1105,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1323690,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://garymarcus.substack.com/i/170534403?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9048ce3b-83cd-4256-8b82-7c3c33e5bdc0_1598x1213.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!6xcx!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9048ce3b-83cd-4256-8b82-7c3c33e5bdc0_1598x1213.jpeg 424w, https://substackcdn.com/image/fetch/$s_!6xcx!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9048ce3b-83cd-4256-8b82-7c3c33e5bdc0_1598x1213.jpeg 848w, https://substackcdn.com/image/fetch/$s_!6xcx!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9048ce3b-83cd-4256-8b82-7c3c33e5bdc0_1598x1213.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!6xcx!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9048ce3b-83cd-4256-8b82-7c3c33e5bdc0_1598x1213.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>I am pretty sure most, if not all, PhDs in mechanical engineering could do better. So could anybody working in a bike shop, and for that matter maybe your kid brother or sister, too. </p><p><span>√âmile Torres has a good round-up of</span><a href="https://www.realtimetechpocalypse.com/p/gpt-5-is-by-far-the-best-ai-system" rel=""> many more immediately-uncovered blunders</a><span>. Cameron Williams found examples </span><a href="https://x.com/wasgo/status/1953883545845244121?s=61" rel="">in basic reading and summarization</a><span>.</span></p><p>¬ß</p><p><span>For all that, GPT-5 is not a </span><em>terrible</em><span> model. I played with it for about an hour, and it actually got several of my initial queries right (some initial problems with counting ‚Äúr‚Äôs in blueberries had already been corrected, for example). It only fell apart altogether when I experimented with images. </span></p><p>But the reality is that GPT-5 just not that different from anything that came before. And that‚Äôs the point. GPT-4 was widely seen as a radical advance over GPT-3; GPT-3 was widely seen as a radical advance over GPT-2. GPT-5 is barely better than last month‚Äôs flavor of the month (Grok 4); on some metrics (ARC-AGI-2) it‚Äôs actually worse. </p><p><span>People had grown to expect miracles, but GPT-5 is just the latest incremental advance. And it felt rushed at that, </span><a href="https://x.com/explodemeow102/status/1954192504623931839?s=61" rel="">as one meme showed</a><span>.</span></p><p><span>The one prediction I got most deeply </span><em>wrong</em><span> was in thinking that with so much at stake OpenAI would save the name GPT-5 for something truly remarkable. I honestly didn‚Äôt think OpenAI would burn the brand name on something so mid. </span></p><p>I was wrong.</p><p>¬ß</p><p>For a year or two I have been speculating that OpenAI might take a serious hit if GPT-5 was disappointing. We may finally soon find out.</p><p>Certainly, in a rational world, their valuation would take a hit.</p><ul><li><p>They no longer have anything like a clear technical lead.</p></li><li><p>GPT-5 is unlikely to be ahead of the pack for more than a couple months. (And Grok 4 Heavy is already better on the ARC-AGI-2 measure)</p></li><li><p>Many of their best people have left.</p></li><li><p>Many of those people left to start competitors.</p></li><li><p>Elon is moving faster. Anthropic and Google and many others are nipping at their heels. Their relationship with Microsoft has frayed.</p></li><li><p>OpenAI still isn‚Äôt making profit.</p></li><li><p>Instead they are being forced to cut prices.</p></li><li><p>People are wising up that LLMs are not in fact AGI-adjacent.</p></li><li><p>People are becoming more skeptical about the company and its CEO.</p></li></ul><p>OpenAI has the name brand recognition, and good UX. Will that be enough to sustain a $300-500B valuation? Hard to know.</p><p>¬ß</p><p>By rights, Altman‚Äôs reputation should by now be completely burned. This is a man who joked in September 2023 that ‚ÄúAGI has been achieved internally‚Äù, told us in January of this year in his blog that  ‚ÄúWe are now confident we know how to build AGI as we have traditionally understood it‚Äù.  Just two days ago he hold us that as quoted above) interacting with GPT-5 we ‚Äúlike talking to ‚Ä¶ legitimate PhD level expert in anything‚Äù. </p><p>In hindsight, that was all bullshit.</p><p>And the worst part? Altman brought it all on himself. Had he not kept hinting at the moon, people might have been fine with just another incremental update. </p><p>¬ß</p><p>He may not even be the right CEO for OpenAI anymore:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!R7_Q!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6daea3f0-070b-4b0b-b79d-3ec85846261d_1234x487.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!R7_Q!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6daea3f0-070b-4b0b-b79d-3ec85846261d_1234x487.png 424w, https://substackcdn.com/image/fetch/$s_!R7_Q!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6daea3f0-070b-4b0b-b79d-3ec85846261d_1234x487.png 848w, https://substackcdn.com/image/fetch/$s_!R7_Q!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6daea3f0-070b-4b0b-b79d-3ec85846261d_1234x487.png 1272w, https://substackcdn.com/image/fetch/$s_!R7_Q!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6daea3f0-070b-4b0b-b79d-3ec85846261d_1234x487.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!R7_Q!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6daea3f0-070b-4b0b-b79d-3ec85846261d_1234x487.png" width="1234" height="487" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6daea3f0-070b-4b0b-b79d-3ec85846261d_1234x487.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:487,&quot;width&quot;:1234,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:105688,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://garymarcus.substack.com/i/170534403?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6daea3f0-070b-4b0b-b79d-3ec85846261d_1234x487.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!R7_Q!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6daea3f0-070b-4b0b-b79d-3ec85846261d_1234x487.png 424w, https://substackcdn.com/image/fetch/$s_!R7_Q!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6daea3f0-070b-4b0b-b79d-3ec85846261d_1234x487.png 848w, https://substackcdn.com/image/fetch/$s_!R7_Q!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6daea3f0-070b-4b0b-b79d-3ec85846261d_1234x487.png 1272w, https://substackcdn.com/image/fetch/$s_!R7_Q!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6daea3f0-070b-4b0b-b79d-3ec85846261d_1234x487.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>¬ß</p><p><span>So ok, all this is obviously no bueno for OpenAI. But what of the field of generative AI as a whole?  It‚Äôs not like other systems are faring much better. The psychologist Jonathan Shedler was absolutely brutal </span><a href="https://x.com/jonathanshedler/status/1953853841918898600?s=61" rel="">in a takedown of Grok</a><span>, writing in part about Grok‚Äôs summary of one of his own papers:</span></p><blockquote><p><em>I'm the author of the paper @grok describes here. It's among the most read and cited articles on psychotherapy outcome-required reading in grad programs around the world</em></p><p><em>Grok gets literally everything wrong</em></p><p><em>The paper shows psychodynamic therapy is as or more effective than</em></p><p><em>CBT. Grok says the exact opposite</em></p><p><em>The title of the paper is literally, "The efficacy of psychodynamic psychotherapy."</em></p><p><em>The effect size for psychodynamic therapy for the major study in the paper was .97. Grok says it's 33. The number .33 does not appear anywhere in the paper.</em></p><p><em>Al seems to know everything‚Äîuntil it's a topic where you have firsthand knowledge</em></p></blockquote><p>How is AI going to invent new science when it can‚Äôt even accurately report existing science?</p><p>¬ß</p><p><span>But I have kept you in suspense long enough. At the beginning, and in the subtitle, I hinted that there was even </span><em>worse</em><span> news. </span></p><p><span>The </span><em>real</em><span> news is </span><a href="https://arxiv.org/pdf/2508.01191" rel="">a breaking study from Arizona State University</a><span> that fully vindicates what I have told you for nearly 30 years‚Äîand more recently what Apple  told you‚Äîabout the core weakness of LLMs: their inability to generalize broadly. </span></p><p><span>The physicist Steve Hsu wrote a great summary on X; in every way it vindicates both the unfairly-maligned but significant </span><a href="https://open.substack.com/pub/garymarcus/p/a-knockout-blow-for-llms?r=8tdk6&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcomeOnShare=false" rel="">Apple reasoning paper</a><span> and </span><a href="https://open.substack.com/pub/garymarcus/p/a-knockout-blow-for-llms?r=8tdk6&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcomeOnShare=false" rel="">the core ideas that I have been pushing about distribution shift for the last three decades</a><span>:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!abjz!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F722447c6-b5e1-4d50-9239-3436044cdced_1286x2131.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!abjz!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F722447c6-b5e1-4d50-9239-3436044cdced_1286x2131.png 424w, https://substackcdn.com/image/fetch/$s_!abjz!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F722447c6-b5e1-4d50-9239-3436044cdced_1286x2131.png 848w, https://substackcdn.com/image/fetch/$s_!abjz!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F722447c6-b5e1-4d50-9239-3436044cdced_1286x2131.png 1272w, https://substackcdn.com/image/fetch/$s_!abjz!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F722447c6-b5e1-4d50-9239-3436044cdced_1286x2131.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!abjz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F722447c6-b5e1-4d50-9239-3436044cdced_1286x2131.png" width="1286" height="2131" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/722447c6-b5e1-4d50-9239-3436044cdced_1286x2131.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:2131,&quot;width&quot;:1286,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1046850,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://garymarcus.substack.com/i/170534403?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F722447c6-b5e1-4d50-9239-3436044cdced_1286x2131.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!abjz!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F722447c6-b5e1-4d50-9239-3436044cdced_1286x2131.png 424w, https://substackcdn.com/image/fetch/$s_!abjz!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F722447c6-b5e1-4d50-9239-3436044cdced_1286x2131.png 848w, https://substackcdn.com/image/fetch/$s_!abjz!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F722447c6-b5e1-4d50-9239-3436044cdced_1286x2131.png 1272w, https://substackcdn.com/image/fetch/$s_!abjz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F722447c6-b5e1-4d50-9239-3436044cdced_1286x2131.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Reading the abstract (Chain of Thought reasoning is ‚Äúa brittle mirage that vanishes when it is pushed beyond training distributions‚Äù) practically gave me deja vu. In 1998 I wrote that ‚Äúuniversals are pervasive in language and reasoning‚Äù  but showed experimentally that neural networks of that era could not reliably ‚Äúextend universals outside [a] training space of examples‚Äù. </p><p>The ASU team showed that exactly the same thing was true even in the  latest, greatest models. Throw in every gadget invented since 1998, and the Achilles‚Äô Heel I identified then still remains. That‚Äôs startling. Even I didn‚Äôt expect that.</p><p><span>And, crucially, the failure to generalize adequately outside distribution tells us </span><em>why </em><span>all the dozens of shots on goal at building ‚ÄúGPT-5 level models‚Äù keep missing their target. It‚Äôs not an accident. That failing is </span><em>principled.</em></p><p>¬ß</p><p>We have been fed a steady diet of bullshit for the last several years.</p><p>‚Ä¢&nbsp;General purpose agents that turn out to suck so badly people struggle to find real-world use cases for them. (Any one remember Facebook M, a decade ago?)</p><p>‚Ä¢&nbsp;Allegedly godlike models that turn out to be incremental advances.</p><p>‚Ä¢&nbsp;Claims like ‚ÄúWe now know how to build AGI‚Äù that never turn out to be true.</p><p>‚Ä¢&nbsp;Promises for world-changing science that rarely materialize.</p><p>‚Ä¢&nbsp;Driverless cars that still are only available in couple percent of the world‚Äôs cities.</p><p>‚Ä¢&nbsp;Promises to Congress (AI to filter our fake news! Regulation for AI) that quickly turn to be bogus.</p><p>‚Ä¢&nbsp;Fantasies about timelines, what Ilya saw, and endless influencer hype.</p><p>‚Ä¢&nbsp;Cherry-picked studies, benchmark-gaming, and now even vibe-coded graphs, with zero transparency about how systems work or how they have been trained; public science is in the rear view mirror.</p><p>I love AI. (Or at least what I optimistically imagine it could be.)</p><p>But I hate this bullshit.</p><p><span>What‚Äôs changed is that a lot of other people are tiring of it, too. In Zeynep Tufekci‚Äòs words,  the term AGI has become ‚Äú</span><a href="https://x.com/zeynep/status/1953842912661291048?s=61" rel="">a tool of obfuscation directed [at] investors and the public.</a><span>‚Äù</span></p><p>¬ß</p><p><span>In many ways, my work here, in the context of publicly explaining the limits of the pure scaling approach‚Äîwhich is literally how this very Substack began in May 2022, </span><a href="https://garymarcus.substack.com/p/the-new-science-of-alt-intelligence" rel="">nearly three and half  years ago</a><span>‚Äîis done. Nobody with intellectual integrity should still believe that pure scaling will get us to AGI. You could say the same about my by now 27-year-old mission to get the field to recognize the centrality of the distribution shift problem. Even some of the tech bros are waking up to the reality that ‚ÄúAGI in 2027‚Äù was marketing, not reality.</span></p><p>GPT-5 may be a moderate quantitative improvement (and it may be cheaper) but it still fails in all the same qualitative ways as its predecessors, on chess, on reasoning, in vision;  even sometimes on counting] and basic math..  Hallucinations linger. Dozens of shots on goal (Grok, Claude, Gemini) etc have invariably faced the same problems. Distribution shift has never been solved.</p><p><span>That‚Äôs exactly what it means to hit a wall, and exactly the particular set of obstacles I described </span><a href="https://archive.ph/6hEYS" rel="">in my most notorious (and prescient) paper</a><span>, in 2022. Real progress on some dimensions, but  stuck in place on others.</span></p><p><span>Ultimately, the idea that scaling alone might get us to AGI is a </span><em>hypothesis. </em></p><p>No hypothesis has ever been given more benefit of the doubt, nor more funding.  After half a trillion dollars in that direction, it is obviously time to move on. The disappointing performance of GPT-5 should make that enormously clear.</p><p><span>Pure scaling simply isn‚Äôt the path to AGI. It turns out that attention, the key component in LLMs, and the focus of the justly famous Transformer paper, is not fact ‚Äú</span><a href="https://arxiv.org/abs/1706.03762" rel="">all you need</a><span>‚Äù.</span></p><p><span>All I am saying is give </span><a href="https://open.substack.com/pub/garymarcus/p/how-o3-and-grok-4-accidentally-vindicated?r=8tdk6&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcomeOnShare=false" rel="">neurosymbolic AI </a><span>with </span><a href="https://open.substack.com/pub/garymarcus/p/generative-ais-crippling-and-widespread?r=8tdk6&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcomeOnShare=false" rel="">explicit world models</a><span> a chance. Only once we have systems that can reason about enduring representations of the world, including but not to limited to abstract symbolic ones, will we have a genuine shot at AGI.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!vWPg!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0abe7b82-9aed-411a-8bcd-fe91e390f9f2_1022x932.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!vWPg!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0abe7b82-9aed-411a-8bcd-fe91e390f9f2_1022x932.jpeg 424w, https://substackcdn.com/image/fetch/$s_!vWPg!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0abe7b82-9aed-411a-8bcd-fe91e390f9f2_1022x932.jpeg 848w, https://substackcdn.com/image/fetch/$s_!vWPg!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0abe7b82-9aed-411a-8bcd-fe91e390f9f2_1022x932.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!vWPg!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0abe7b82-9aed-411a-8bcd-fe91e390f9f2_1022x932.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!vWPg!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0abe7b82-9aed-411a-8bcd-fe91e390f9f2_1022x932.jpeg" width="1022" height="932" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0abe7b82-9aed-411a-8bcd-fe91e390f9f2_1022x932.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:932,&quot;width&quot;:1022,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:117585,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://garymarcus.substack.com/i/170534403?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0abe7b82-9aed-411a-8bcd-fe91e390f9f2_1022x932.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!vWPg!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0abe7b82-9aed-411a-8bcd-fe91e390f9f2_1022x932.jpeg 424w, https://substackcdn.com/image/fetch/$s_!vWPg!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0abe7b82-9aed-411a-8bcd-fe91e390f9f2_1022x932.jpeg 848w, https://substackcdn.com/image/fetch/$s_!vWPg!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0abe7b82-9aed-411a-8bcd-fe91e390f9f2_1022x932.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!vWPg!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0abe7b82-9aed-411a-8bcd-fe91e390f9f2_1022x932.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>This art was </span><em>licensed.</em></figcaption></figure></div><p>PS  For expository purposes, I told a little white lie above, and pretended that there was only one truly devastating new scientific finding about LLMs this week. But the aforementioned ‚Äúmirage‚Äù is not the only problem. There‚Äôs actually another‚Äîan entirely different can of worms‚Äîthat I will be talking about in the not too distant future. Stay tuned. And stay to the end for a final postscript.</p><p>PPS Bonus content, sound up, for my personal favorite meme of the week, sent to me (and created by) a retired VFX editor who has taken an interest in AI:</p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GPTs and Feeling Left Behind (210 pts)]]></title>
            <link>https://whynothugo.nl/journal/2025/08/06/gpts-and-feeling-left-behind/</link>
            <guid>44851214</guid>
            <pubDate>Sat, 09 Aug 2025 23:07:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://whynothugo.nl/journal/2025/08/06/gpts-and-feeling-left-behind/">https://whynothugo.nl/journal/2025/08/06/gpts-and-feeling-left-behind/</a>, See on <a href="https://news.ycombinator.com/item?id=44851214">Hacker News</a></p>
<div id="readability-page-1" class="page"><header><small><span><a href="https://whynothugo.nl/">‚Äπ back home</a></span></small><small><span>2025-08-06</span>
<span><a href="https://whynothugo.nl/tags/ai">#ai</a>
<a href="https://whynothugo.nl/tags/gpt">#gpt</a>
<a href="https://whynothugo.nl/tags/llm">#llm</a></span></small></header><article><p>Every time that I read some blog post about ‚Äúcoding with AI‚Äù, or how cool new
models write entire libraries by themselves, I feel like I‚Äôm lagging behind,
like I‚Äôm missing out on some big, useful tool, and my skills are about to become
obsolete very soon.</p><p>So I try different models and tools, and it‚Äôs all incredibly underwhelming. It‚Äôs
honestly hard to believe that people get work done using these tools, because I
can spend a few hours on them (without getting even close to finishing the task
at hand) and realise that I could have done it myself in 25 minutes.</p><p>I tell myself <em>‚Äúlearning to use Vim took a long time, but then it paid off‚Äù</em>
eventually. But I could (slowly) write text with Vim the first day. I can spend
an entire day with a GPT and produce nothing of value.</p><p>GPTs work great for finding the exact word to complete a sentence. They‚Äôre
surprisingly good at finding the exact type annotation for a Python function.
They can find nuanced bugs in a single function which I copy-paste into the GPT.
But anything beyond writing a simple function always leads to useless junk.
Often times, they solve big problems by just importing a library that does not
exist, and calling a function which does the bulk of the logic. ChatGPT told me
the other day ‚Äúif you don‚Äôt want any dependencies, you‚Äôre going to have to
implement it yourself‚Äù. But couldn‚Äôt actually implement the necessary code.
Large portions of code have lots of hidden logic bugs, and when they fix one
they introduce another.</p><p>And then I see another post on Hacker News, about somebody using GPTs and how
they achieved great results on this and that. Part of me wants to think that
those articles are fake to generate hype, but the reality is that several of
them are written by well-known developers who‚Äôve been around for over a decade.
Some of the results are actually publicly available online.</p><p>I‚Äôm in a state where I can‚Äôt reconcile my own results with other people‚Äôs
results. I hear people saying <em>‚Äúthis hammer is indestructible‚Äù</em>, but when I pick
it up, it‚Äôs just origami: made of paper, intricate, delicate, very cool-looking
but I can‚Äôt even hammer a tomato with it.</p></article><p>Have comments or want to discuss this topic?<br>Send an email to my public inbox: <a href="mailto:~whynothugo/public-inbox@lists.sr.ht?subject=Re:%20GPTs%20and%20feeling%20left%20behind">~whynothugo/public-inbox@lists.sr.ht</a>.<br>Or feel free to reply privately by email: <a href="mailto:hugo@whynothugo.nl?subject=Re:%20GPTs%20and%20feeling%20left%20behind">hugo@whynothugo.nl</a>.</p><p>‚Äî ¬ß ‚Äî</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How I code with AI on a budget/free (517 pts)]]></title>
            <link>https://wuu73.org/blog/aiguide1.html</link>
            <guid>44850913</guid>
            <pubDate>Sat, 09 Aug 2025 22:27:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wuu73.org/blog/aiguide1.html">https://wuu73.org/blog/aiguide1.html</a>, See on <a href="https://news.ycombinator.com/item?id=44850913">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <!-- Navigation Top -->
      <p><b>Page 1/3</b> &nbsp;
        <span>&nbsp;‚Ä¢&nbsp;</span>
        <a href="https://wuu73.org/blog/aiguide2.html">--&gt;</a>
      </p>
      
      <p>
        Last updated: July 2025
      </p>
      <h2>How I Code with AI on a budget/free</h2>
      <h2>My Browser Setup: The Free AI Buffet</h2>
      <p>
        First things first, I have a browser open loaded with tabs pointing to
        the free tiers of powerful AI models. Why stick to one when you can get
        multiple perspectives for free? My typical lineup includes:
      </p>
      <ul>
        <li>
          At least one tab of
          <a href="https://platform.openai.com/playground/prompts?models=o3" target="_blank" rel="noopener noreferrer">OpenAI Playground</a>. If you set your account's data settings to allow OpenAI to use your
          data for model training, you get free tokens to use on GPT-4.5, o3,
          and other models.
        </li>
        <li>
          At least one tab but more like three of Google
          <a href="https://aistudio.google.com/" target="_blank" rel="noopener noreferrer">Gemini AI Studio</a>
          (Gemini 2.5 Pro/Flash are often free and unlimited here).
          <br>
          Also, try
          <a href="https://gemini.google.com/app" target="_blank" rel="noopener noreferrer">Google Gemini 2.5 Pro</a>
          (different than AI Studio, has better image generation and deep
          research; I always have a couple tabs of this along with a couple tabs
          of AI Studio).
        </li>
        <li>
          Couple tabs of
          <a href="https://poe.com/" target="_blank" rel="noopener noreferrer">Poe.com</a>
          usually set to Claude 4 or o4-mini for its free daily credits on
          premium models.
        </li>
        <li>
          Several tabs of
          <a href="https://openrouter.ai/" target="_blank" rel="noopener noreferrer">OpenRouter</a>, set to several models, some free models, some not.
        </li>
        <li>
          At least one tab for
          <a href="https://chatgpt.com/" target="_blank" rel="noopener noreferrer">ChatGPT</a>
          (the free version is still useful).
        </li>
        <li>
          At least one tab for
          <a href="https://www.perplexity.ai/" target="_blank" rel="noopener noreferrer">Perplexity AI</a>, especially good for research-heavy questions.
        </li>
        <li>
          At least one tab for
          <a href="https://chat.deepseek.com/" target="_blank" rel="noopener noreferrer">Deepseek</a>
          (v3 and r1 are free on their web interface, though watch the context
          limit).
        </li>
        <li>
          One tab for
          <a href="https://grok.com/" target="_blank" rel="noopener noreferrer">Grok.com</a>. Good, free and seemingly unlimited for general use and deep
          research/image editing. I mainly use the deep research feature,
          similar to perplexity.
        </li>
        <li>
          <a href="https://phind.com/" target="_blank" rel="noopener noreferrer">Phind</a>
          is another free one, it tries to show you flowcharts/diagram visuals.
        </li>
        <li>
          <a href="https://lmarena.ai/" target="_blank" rel="noopener noreferrer">lmarena.ai</a>
          offers free access to Claude Opus 4 and Sonnet 4 and others. Free Opus
          4 is so good.
        </li>
      </ul>
      <p>
          <a href="https://claude.ai/new" target="_blank" rel="noopener noreferrer">Claude.ai</a>
          - Free but sometimes so limited it's annoying to use, so I use other
          sites/ways to access Claude like Cody extension, Copilot, etc.
        </p>
      <div id="grokAccordion">
        
        <p>
            Grok offers free compute and uncensored image generation, which can
            be useful when other models' safety systems interfere with
            legitimate tasks. However, it is run by someone who may be
            manipulating the company to promote Nazi-adjacent views and
            misinformation, possibly attempting to sway users in that direction.
            Reports indicate that Grok has been instructed to lie about
            historical atrocities, including topics such as genocide in Africa.
            While the misinformation appears to be mostly on X, if you keep in
            mind to restrict usage to coding or be cautious knowing it might be
            programmed with questionable motives, it can occasionally be useful.
          </p>
      </div>
      
      <h2>A smarter, cheaper workflow: Focused Context</h2>
      <p>
        When you use AI in web chat's (the chat interfaces like AI Studio,
        ChatGPT, Openrouter, instead of thru an IDE or agent framework) are
        almost always better at solving problems, and coming up with solutions
        compared to the agents like Cline, Trae, Copilot.. Not always, but
        usually.
      </p>
      <p>
        When you use things like Cursor, Cline, Roo Code for everything, they
        are sending tons of text to the AI about how to use their tools, how to
        use or activate MCP server stuff, edit files, etc it "dumbs it down" too
        much. It gets confused. People end up paying for the most expensive best
        models to do everything and even that isn't enough to get over the
        dumbing down effect from the AIs getting tons of unneeded information
        unrelated to your problem.
      </p>
      <p>
        So when that happens, I use my tool to generate the right context to
        solve my problem. Then I paste it into one of the many AI web chat's
        (sometimes more than one, since they sometimes give different answers)
        and just ask it questions or ask it to code review, to try to figure out
        why x is happening when y is happening...etc then when it figures out a
        solution.. i have it write a prompt for Cline or another agent type
        thing to do the actual file edits. GPT 4.1 can handle this just fine and
        I have unlimited. No reason to be wasting Claude credits to edit files.
        No reason to be sending Claude a bunch of crap it doesn't need making it
        dumb. I can use Claude to plan out anything or fix really hard problems,
        cheap, using Openrouter web chat then just paste it back in Cline and
        let it run.
      </p>
      <p>
        After doing this for a while, you really get a feel for which models
        excel at which types of tasks.
      </p>
      <div>
        <p>
          <strong>How AI Code Prep Helps (Example Prompt Structure):</strong>
        </p>
        <p>
          <em>Can you help me figure out why my program does x instead of y?</em>
        </p>
        <p>
          Then,
          <a href="https://wuu73.org/aicp" target="_blank" rel="noopener noreferrer">AI Code Prep GUI</a>
          (for Windows, Mac, Linux, and web) steps in. It recursively scans your
          project folder (subfolders, sub-subfolders, you name it) and grabs the
          code, formatting it nicely for AI like this:
        </p>
        <p>The context block generated by AI Code Prep looks like this:</p>
        <div>
          <p>Can you help me figure out why my program does x instead of y?</p><p>
          fileName.js:
          <br>
          &lt;code&gt;
          <br>
          ... the contents of the file..
          <br>
          &lt;/code&gt;
          </p><p>
          nextFile.py:
          <br>
          &lt;code&gt;
          <br>
          import example
          <br>
          ...etc
          <br>
          &lt;/code&gt;
          </p><p>Can you help me figure out why my program does x instead of y?</p>
        </div>
        <p>
          It writes it twice if you have that option enabled, which helps get
          the AI to focus better on your question/prompt. You can choose to have
          it on top, bottom, or both. OpenAI claims this helps, I haven't really
          tested to see if that's true but it seems logical.
        </p>
        <p>
          On Windows, you just right-click somewhere inside your project folder
          (or on the folder itself) and select "AI Code Prep GUI" from the
          context menu (look at the screenshots on the site). A GUI window pops
          up, usually with the right code files pre-selected. It smartly tries
          to skip things you likely don't need, like <code>node_modules</code>,
          <code>.git</code>, etc. If its guess isn't perfect, you can easily
          check or uncheck files.
        </p>
        <p>
          This is super useful when your project is huge and blows past an AI's
          context limit. You can manually curate exactly what the AI needs to
          see.
        </p>
        <p>
          The problem with many coding agents like
          <a href="https://cline.bot/" target="_blank" rel="noopener noreferrer">Cline</a>, Github Copilot, Cursor, Windsurf, etc., is that they often send
          either WAY too much context or WAY too little. This is why they can
          seem dumb or ineffective sometimes. Sometimes, you just gotta do
          things yourself, use a tool like mine to select the files yourself,
          but it helps auto-select the code files while skipping the stuff you
          probably don't need (but still have the option to add what you want
          with the checkboxes) and then dump that curated context into several
          AIs (especially the free web ones!).
        </p>
        <p>
          Sure, there are other context-generating tools, but many are
          command-line only, or need a public GitHub repo link. What if your
          code is private? What if you want to keep it local? What if you prefer
          checkboxes on a GUI? For something like this a GUI makes sense.
        </p>
      </div>
      <p>
        Note: This page isn't updated with all the latest AI Code Prep GUI
        features, check
        <a href="https://wuu73.org/aicp" target="_blank" rel="noopener noreferrer">wuu73.org/aicp</a>
        for latest major upgrade
      </p>
      <!-- Navigation Bottom -->
      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Abusing Entra OAuth for fun and access to internal Microsoft applications (314 pts)]]></title>
            <link>https://research.eye.security/consent-and-compromise/</link>
            <guid>44850681</guid>
            <pubDate>Sat, 09 Aug 2025 21:59:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://research.eye.security/consent-and-compromise/">https://research.eye.security/consent-and-compromise/</a>, See on <a href="https://news.ycombinator.com/item?id=44850681">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				
<p>This blog is about how I got access to over 22 internal Microsoft services and how you might be vulnerable too.</p>



<p>After my <a href="https://www.youtube.com/watch?v=uowTmPomYcg">last talk at the 38C3 conference in Hamburg</a>, this was the top comment on YouTube.</p>



<figure><img data-recalc-dims="1" fetchpriority="high" decoding="async" width="802" height="260" data-attachment-id="3418" data-permalink="https://research.eye.security/consent-and-compromise/image-5/" data-orig-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-4.png?fit=802%2C260&amp;ssl=1" data-orig-size="802,260" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-4.png?fit=300%2C97&amp;ssl=1" data-large-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-4.png?fit=802%2C260&amp;ssl=1" src="https://i0.wp.com/research.eye.security/wp-content/uploads/image-4.png?resize=802%2C260&amp;ssl=1" alt="Comment from a YouTube user expressing concerns about cybersecurity talks" srcset="https://i0.wp.com/research.eye.security/wp-content/uploads/image-4.png?w=802&amp;ssl=1 802w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-4.png?resize=300%2C97&amp;ssl=1 300w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-4.png?resize=768%2C249&amp;ssl=1 768w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-4.png?resize=800%2C260&amp;ssl=1 800w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-4.png?resize=512%2C166&amp;ssl=1 512w" sizes="(max-width: 802px) 100vw, 802px"></figure>



<p>Well, this story definitely falls in the category ‚Äúsomeone stumbling around finding horrifying vulnerabilities‚Äù. Although this time I was not even having issues, I was just distracted from a boring task.</p>



<p>You see, I was writing some documentation the other day, when my Eye fell on one of those aka.ms links. For those of you who don‚Äôt know, aka.ms is the <a href="https://github.com/microsoft/aka">URL shortener service used by Microsoft</a>.</p>



<p>My mind started to wonder. Where would you end up if you would just visit <a href="https://aka.ms/" rel="nofollow">https://aka.ms</a>, so without any shortener link included?</p>


<div>
<figure><img data-recalc-dims="1" decoding="async" width="755" height="710" data-attachment-id="3419" data-permalink="https://research.eye.security/consent-and-compromise/image-6/" data-orig-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-5.png?fit=755%2C710&amp;ssl=1" data-orig-size="755,710" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-5.png?fit=300%2C282&amp;ssl=1" data-large-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-5.png?fit=755%2C710&amp;ssl=1" src="https://i0.wp.com/research.eye.security/wp-content/uploads/image-5.png?resize=755%2C710&amp;ssl=1" alt="Microsoft sign-in screen prompting users to enter their email, phone, or Skype for account access." srcset="https://i0.wp.com/research.eye.security/wp-content/uploads/image-5.png?w=755&amp;ssl=1 755w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-5.png?resize=300%2C282&amp;ssl=1 300w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-5.png?resize=512%2C481&amp;ssl=1 512w" sizes="(max-width: 755px) 100vw, 755px"></figure></div>


<p>A login screen. This is probably where Microsoft employees manage their links. I did wonder though, what would happen if I simply tried logging in here myself with my own Microsoft account? Surely, that would not work, right?</p>


<div>
<figure><img data-recalc-dims="1" decoding="async" width="1024" height="576" data-attachment-id="3421" data-permalink="https://research.eye.security/consent-and-compromise/image-7/" data-orig-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-6.png?fit=1322%2C744&amp;ssl=1" data-orig-size="1322,744" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-6.png?fit=300%2C169&amp;ssl=1" data-large-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-6.png?fit=1024%2C576&amp;ssl=1" src="https://i0.wp.com/research.eye.security/wp-content/uploads/image-6-1024x576.png?resize=1024%2C576&amp;ssl=1" alt="Microsoft account selection screen with error message indicating the selected user account does not exist in the specified tenant." srcset="https://i0.wp.com/research.eye.security/wp-content/uploads/image-6.png?resize=1024%2C576&amp;ssl=1 1024w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-6.png?resize=300%2C169&amp;ssl=1 300w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-6.png?resize=768%2C432&amp;ssl=1 768w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-6.png?resize=1200%2C675&amp;ssl=1 1200w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-6.png?resize=512%2C288&amp;ssl=1 512w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-6.png?resize=920%2C518&amp;ssl=1 920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-6.png?w=1322&amp;ssl=1 1322w" sizes="(max-width: 1024px) 100vw, 1024px"></figure></div>


<p>Of course not! You need an account in the Microsoft tenant, otherwise, no luck. Back to writing that boring documentation. But what was that? <a href="https://akasearch.net/">An indexing service for aka.ms links</a>? Interesting.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1024" height="407" data-attachment-id="3427" data-permalink="https://research.eye.security/consent-and-compromise/image-10/" data-orig-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-9.png?fit=2228%2C885&amp;ssl=1" data-orig-size="2228,885" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-9.png?fit=300%2C119&amp;ssl=1" data-large-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-9.png?fit=1024%2C407&amp;ssl=1" src="https://i0.wp.com/research.eye.security/wp-content/uploads/image-9.png?resize=1024%2C407&amp;ssl=1" alt="A screenshot of akaSearch, a website for searching Microsoft's aka.ms links, displaying a list of links with titles and corresponding URLs." srcset="https://i0.wp.com/research.eye.security/wp-content/uploads/image-9.png?resize=1024%2C407&amp;ssl=1 1024w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-9.png?resize=300%2C119&amp;ssl=1 300w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-9.png?resize=768%2C305&amp;ssl=1 768w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-9.png?resize=1536%2C610&amp;ssl=1 1536w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-9.png?resize=2048%2C814&amp;ssl=1 2048w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-9.png?resize=1200%2C477&amp;ssl=1 1200w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-9.png?resize=512%2C203&amp;ssl=1 512w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-9.png?resize=920%2C365&amp;ssl=1 920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-9.png?resize=1600%2C636&amp;ssl=1 1600w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-9.png?resize=1920%2C763&amp;ssl=1 1920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-9.png?w=2228&amp;ssl=1 2228w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure>



<p>And one of those links points to an eng.ms domain. What is that?</p>


<div>
<figure><img data-recalc-dims="1" decoding="async" width="755" height="710" data-attachment-id="3419" data-permalink="https://research.eye.security/consent-and-compromise/image-6/" data-orig-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-5.png?fit=755%2C710&amp;ssl=1" data-orig-size="755,710" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-5.png?fit=300%2C282&amp;ssl=1" data-large-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-5.png?fit=755%2C710&amp;ssl=1" src="https://i0.wp.com/research.eye.security/wp-content/uploads/image-5.png?resize=755%2C710&amp;ssl=1" alt="Microsoft sign-in screen prompting users to enter their email, phone, or Skype for account access." srcset="https://i0.wp.com/research.eye.security/wp-content/uploads/image-5.png?w=755&amp;ssl=1 755w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-5.png?resize=300%2C282&amp;ssl=1 300w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-5.png?resize=512%2C481&amp;ssl=1 512w" sizes="(max-width: 755px) 100vw, 755px"></figure></div>


<p>Another login screen! It did not work the first time, but maybe I can login here! The result surprised me.</p>


<div>
<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="754" height="1024" data-attachment-id="3509" data-permalink="https://research.eye.security/consent-and-compromise/image-45/" data-orig-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-44.png?fit=796%2C1081&amp;ssl=1" data-orig-size="796,1081" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-44.png?fit=221%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-44.png?fit=754%2C1024&amp;ssl=1" src="https://i0.wp.com/research.eye.security/wp-content/uploads/image-44.png?resize=754%2C1024&amp;ssl=1" alt="A Microsoft consent prompt requesting permissions to access user profile information for the EngineeringHub application, indicating it is unverified and not published by Microsoft." srcset="https://i0.wp.com/research.eye.security/wp-content/uploads/image-44.png?resize=754%2C1024&amp;ssl=1 754w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-44.png?resize=221%2C300&amp;ssl=1 221w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-44.png?resize=768%2C1043&amp;ssl=1 768w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-44.png?resize=512%2C695&amp;ssl=1 512w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-44.png?w=796&amp;ssl=1 796w" sizes="auto, (max-width: 754px) 100vw, 754px"></figure></div>


<p>I got a consent prompt, asking me to give consent to share my basic profile information with this application. Sure, why not? Does that mean this is an application I can access?</p>



<p>After clicking ‚Äúaccept‚Äù, I get a 500 Internal Server Error message. I‚Äôm pretty sure that was this server trying to tell me that this application was not meant to be accessed by me. But now I‚Äôm intrigued, what is eng.ms? I still had that documentation to write, but that had to wait. </p>



<p>I started subdomain enumeration on the domain eng.ms and found an interesting subdomain: rescue.eng.ms. This gave me another login screen and another consent prompt. But this time it gave me access using my own Microsoft 365 account! Notice my name in the upper right corner?</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1024" height="844" data-attachment-id="3424" data-permalink="https://research.eye.security/consent-and-compromise/image-8/" data-orig-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-7.png?fit=1638%2C1350&amp;ssl=1" data-orig-size="1638,1350" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-7.png?fit=300%2C247&amp;ssl=1" data-large-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-7.png?fit=1024%2C844&amp;ssl=1" src="https://i0.wp.com/research.eye.security/wp-content/uploads/image-7.png?resize=1024%2C844&amp;ssl=1" alt="Screenshot of the Engineering Hub Rescue page showing various team sections such as Cloud + AI Platform, Experiences &amp; Devices, Finance Group, and Gaming, along with a search bar at the top." srcset="https://i0.wp.com/research.eye.security/wp-content/uploads/image-7.png?resize=1024%2C844&amp;ssl=1 1024w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-7.png?resize=300%2C247&amp;ssl=1 300w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-7.png?resize=768%2C633&amp;ssl=1 768w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-7.png?resize=1536%2C1266&amp;ssl=1 1536w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-7.png?resize=1200%2C989&amp;ssl=1 1200w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-7.png?resize=512%2C422&amp;ssl=1 512w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-7.png?resize=920%2C758&amp;ssl=1 920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-7.png?resize=1600%2C1319&amp;ssl=1 1600w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-7.png?w=1638&amp;ssl=1 1638w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure>



<p>I quickly realized that I should not have access here. This Engineering Hub was some sort of portal for Microsoft Engineers. I must admit, my knee-jerk reaction as a red-teamer was to search for the phrase ‚Äúpassword‚Äù. What can I say, I could not help myself. It returned 13,252 hits, all referring to internal Microsoft procedures and product groups. I was clearly not supposed to access this.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1024" height="454" data-attachment-id="3932" data-permalink="https://research.eye.security/consent-and-compromise/attachment/001/" data-orig-file="https://i0.wp.com/research.eye.security/wp-content/uploads/001.png?fit=1608%2C713&amp;ssl=1" data-orig-size="1608,713" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="001" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/research.eye.security/wp-content/uploads/001.png?fit=300%2C133&amp;ssl=1" data-large-file="https://i0.wp.com/research.eye.security/wp-content/uploads/001.png?fit=1024%2C454&amp;ssl=1" src="https://i0.wp.com/research.eye.security/wp-content/uploads/001.png?resize=1024%2C454&amp;ssl=1" alt="Screenshot of the Engineering Hub Rescue page displaying search results for the term 'password', with several links and references to internal documentation related to password management." srcset="https://i0.wp.com/research.eye.security/wp-content/uploads/001.png?resize=1024%2C454&amp;ssl=1 1024w, https://i0.wp.com/research.eye.security/wp-content/uploads/001.png?resize=300%2C133&amp;ssl=1 300w, https://i0.wp.com/research.eye.security/wp-content/uploads/001.png?resize=768%2C341&amp;ssl=1 768w, https://i0.wp.com/research.eye.security/wp-content/uploads/001.png?resize=1536%2C681&amp;ssl=1 1536w, https://i0.wp.com/research.eye.security/wp-content/uploads/001.png?resize=1200%2C532&amp;ssl=1 1200w, https://i0.wp.com/research.eye.security/wp-content/uploads/001.png?resize=512%2C227&amp;ssl=1 512w, https://i0.wp.com/research.eye.security/wp-content/uploads/001.png?resize=920%2C408&amp;ssl=1 920w, https://i0.wp.com/research.eye.security/wp-content/uploads/001.png?resize=1600%2C709&amp;ssl=1 1600w, https://i0.wp.com/research.eye.security/wp-content/uploads/001.png?w=1608&amp;ssl=1 1608w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure>



<p>I stopped here and reported this to the Microsoft Security Response Centre (MSRC). But what had happened here? Why was I able to access this internal portal and might other services be vulnerable in the same way?</p>



<p>To answer that question, we need to have a closer look at how Entra ID handles authentication and authorization for multi-tenant applications.</p>



<p><strong>Multi-Tenant Applications</strong></p>



<p>When developers want to use Entra for authentication to a web application, the application needs to first be registered at Entra. This is called an ‚ÄúApp Registration‚Äù or ‚ÄúApplication Object‚Äù. These applications can be set to accept users from the tenant where they are registered (single-tenant), or users from any tenant (multi-tenant). They can also be configured to accept ‚Äúpersonal Microsoft accounts‚Äù.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1024" height="250" data-attachment-id="3431" data-permalink="https://research.eye.security/consent-and-compromise/image-11/" data-orig-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-10.png?fit=1902%2C464&amp;ssl=1" data-orig-size="1902,464" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-10.png?fit=300%2C73&amp;ssl=1" data-large-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-10.png?fit=1024%2C250&amp;ssl=1" src="https://i0.wp.com/research.eye.security/wp-content/uploads/image-10.png?resize=1024%2C250&amp;ssl=1" alt="A screenshot of supported account types for a Microsoft application indicating options for user access across different organizational directories." srcset="https://i0.wp.com/research.eye.security/wp-content/uploads/image-10.png?resize=1024%2C250&amp;ssl=1 1024w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-10.png?resize=300%2C73&amp;ssl=1 300w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-10.png?resize=768%2C187&amp;ssl=1 768w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-10.png?resize=1536%2C375&amp;ssl=1 1536w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-10.png?resize=1200%2C293&amp;ssl=1 1200w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-10.png?resize=512%2C125&amp;ssl=1 512w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-10.png?resize=920%2C224&amp;ssl=1 920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-10.png?resize=1600%2C390&amp;ssl=1 1600w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-10.png?w=1902&amp;ssl=1 1902w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure>



<p>App developers are then tasked to configure the authorization server (login.microsoftonline.com in most cases) in application logic and all four options above have a corresponding desired value for the authorization server URL.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1024" height="371" data-attachment-id="3433" data-permalink="https://research.eye.security/consent-and-compromise/image-12/" data-orig-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-11.png?fit=2273%2C824&amp;ssl=1" data-orig-size="2273,824" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-11.png?fit=300%2C109&amp;ssl=1" data-large-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-11.png?fit=1024%2C371&amp;ssl=1" src="https://i0.wp.com/research.eye.security/wp-content/uploads/image-11.png?resize=1024%2C371&amp;ssl=1" alt="A screenshot showing a table comparing different Microsoft Entra authentication values and their descriptions." srcset="https://i0.wp.com/research.eye.security/wp-content/uploads/image-11.png?resize=1024%2C371&amp;ssl=1 1024w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-11.png?resize=300%2C109&amp;ssl=1 300w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-11.png?resize=768%2C278&amp;ssl=1 768w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-11.png?resize=1536%2C557&amp;ssl=1 1536w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-11.png?resize=2048%2C742&amp;ssl=1 2048w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-11.png?resize=1200%2C435&amp;ssl=1 1200w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-11.png?resize=512%2C186&amp;ssl=1 512w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-11.png?resize=920%2C334&amp;ssl=1 920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-11.png?resize=1600%2C580&amp;ssl=1 1600w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-11.png?resize=1920%2C696&amp;ssl=1 1920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-11.png?w=2273&amp;ssl=1 2273w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure>



<p>So single-tenant applications should be configured to use <code>https://login.microsoftonline.com/&lt;tenantid&gt;</code> or <code>https://login.microsoftonline.com/&lt;domain&gt;</code> as the authorization server, while multi-tenant applications that also accept personal Microsoft accounts should use <code>https://login.microsoftonline.com/common</code>.</p>



<p>The Engineering Hub was configured as a multi-tenant application and redirected me to the /common endpoint. </p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1024" height="709" data-attachment-id="3437" data-permalink="https://research.eye.security/consent-and-compromise/image-13/" data-orig-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-12.png?fit=1350%2C935&amp;ssl=1" data-orig-size="1350,935" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-12.png?fit=300%2C208&amp;ssl=1" data-large-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-12.png?fit=1024%2C709&amp;ssl=1" src="https://i0.wp.com/research.eye.security/wp-content/uploads/image-12.png?resize=1024%2C709&amp;ssl=1" alt="Diagram illustrating the authentication flow for requests to a multi-tenant application using Entra ID, showing steps for user authentication, consent, and token return." srcset="https://i0.wp.com/research.eye.security/wp-content/uploads/image-12.png?resize=1024%2C709&amp;ssl=1 1024w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-12.png?resize=300%2C208&amp;ssl=1 300w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-12.png?resize=768%2C532&amp;ssl=1 768w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-12.png?resize=1200%2C831&amp;ssl=1 1200w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-12.png?resize=512%2C355&amp;ssl=1 512w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-12.png?resize=920%2C637&amp;ssl=1 920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-12.png?w=1350&amp;ssl=1 1350w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure>



<p>This allowed me to login (step 1 &amp; 2) with my own ‚Äúwork or school account‚Äù, which triggered a Consent prompt (Step 3), as the application was not used in my own tenant before. When I gave it consent, it was instantiated into a Service Principal or ‚ÄúEnterprise Application‚Äù (step 4) in my own tenant and returned an access token (step 5).</p>



<p>This access token was issued by my own tenant. The ‚Äúiss‚Äù (issuer) and ‚Äútid‚Äù (tenant ID) claims were set to values corresponding with my own tenant. Any conditional access policies or user assignment took place in my own tenant according to my own configuration. I was authenticated &amp; authorized, only by the wrong authority. And nowhere in application logic was this checked.</p>


<div>
<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="600" height="600" data-attachment-id="3442" data-permalink="https://research.eye.security/consent-and-compromise/image-14/" data-orig-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-13.png?fit=600%2C600&amp;ssl=1" data-orig-size="600,600" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-13.png?fit=300%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-13.png?fit=600%2C600&amp;ssl=1" src="https://i0.wp.com/research.eye.security/wp-content/uploads/image-13.png?resize=600%2C600&amp;ssl=1" alt="Illustration of a cartoon character with a police badge and sunglasses, holding a baton, with the text 'RESPECT MY AUTHORITY' displayed prominently." srcset="https://i0.wp.com/research.eye.security/wp-content/uploads/image-13.png?w=600&amp;ssl=1 600w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-13.png?resize=300%2C300&amp;ssl=1 300w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-13.png?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-13.png?resize=400%2C400&amp;ssl=1 400w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-13.png?resize=200%2C200&amp;ssl=1 200w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-13.png?resize=148%2C148&amp;ssl=1 148w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-13.png?resize=296%2C296&amp;ssl=1 296w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-13.png?resize=512%2C512&amp;ssl=1 512w" sizes="auto, (max-width: 600px) 100vw, 600px"></figure></div>


<p>This made me wonder. What other Microsoft services could be impacted by this misconfiguration? Let‚Äôs find out!</p>



<p><strong>Mapping the Microsoft Attack Surface</strong></p>



<p>I enumerated subdomains for microsoft.com, azure.com, azure.net, office365.com, office.com, office.net, windows.net, and any .ms domains owned by Microsoft.</p>



<p>This resulted in 102,672 (sub)domains, of which 70,043 resolved to an IP address, 41,890 responded to HTTPS and 1,406 used Entra ID for authentication. </p>



<p>For all 1,406 applications, I checked the URL and parsed the ‚Äúclient_id‚Äù parameter to get the Application ID. Any multi-tenant application can be looked up at the Azure AD Graph API at</p>



<pre><code>https://graph.windows.net/myorganization/applicationRefs/&lt;APP_ID&gt;/?api-version=1.6-internal</code></pre>



<p>Single-tenant applications will not give any result, but multi-tenant applications do. It turns out that of the 1,406 applications, 176 were configured as multi-tenant.</p>



<p>Interestingly enough, most of these redirected visitors to the /&lt;tenantid&gt; endpoint. Remember that this was supposed to be used only for single-tenant applications. Developers were probably not even aware their application was configured as multi-tenant. But this URL is just a client-side setting. The only effect is against which tenant you are authenticating. For /common and /organizations, you will authenticate against your own tenant, for /&lt;tenantid&gt;, you authenticate against that tenant.</p>



<p><strong>Previous Research</strong></p>



<p>In 2023, <a href="https://www.wiz.io/blog/azure-active-directory-bing-misconfiguration">Wiz discovered</a> that for any multi-tenant application, if you replace /common or /organizations with /&lt;tenantid&gt; during authentication, you will receive an access token issued by the resource tenant.</p>



<p><a href="https://msrc.microsoft.com/blog/2023/03/guidance-on-potential-misconfiguration-of-authorization-of-multi-tenant-applications-that-use-azure-ad/">Microsoft patched this</a> and stopped issuing tokens if the user does not exist in the resource tenant. Today, we are going to do it the other way around, we are going to replace /&lt;tenantid&gt; with /common to force authentication against our own tenant. This can easily be done in Burp with ‚Äúmatch and replace rules‚Äù.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1024" height="282" data-attachment-id="3447" data-permalink="https://research.eye.security/consent-and-compromise/image-15/" data-orig-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-14.png?fit=2400%2C661&amp;ssl=1" data-orig-size="2400,661" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-14.png?fit=300%2C83&amp;ssl=1" data-large-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-14.png?fit=1024%2C282&amp;ssl=1" src="https://i0.wp.com/research.eye.security/wp-content/uploads/image-14.png?resize=1024%2C282&amp;ssl=1" alt="Configuration settings for HTTP match and replace rules in a proxy for Microsoft applications." srcset="https://i0.wp.com/research.eye.security/wp-content/uploads/image-14.png?resize=1024%2C282&amp;ssl=1 1024w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-14.png?resize=300%2C83&amp;ssl=1 300w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-14.png?resize=768%2C212&amp;ssl=1 768w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-14.png?resize=1536%2C423&amp;ssl=1 1536w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-14.png?resize=2048%2C564&amp;ssl=1 2048w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-14.png?resize=1200%2C331&amp;ssl=1 1200w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-14.png?resize=512%2C141&amp;ssl=1 512w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-14.png?resize=920%2C253&amp;ssl=1 920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-14.png?resize=1600%2C441&amp;ssl=1 1600w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-14.png?resize=1920%2C529&amp;ssl=1 1920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-14.png?w=2400&amp;ssl=1 2400w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure>



<p>But there were still some more hurdles to take. Some applications required user assignment. This was easily done now in my own tenant.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1024" height="578" data-attachment-id="3449" data-permalink="https://research.eye.security/consent-and-compromise/image-16/" data-orig-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-15.png?fit=1538%2C868&amp;ssl=1" data-orig-size="1538,868" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-15.png?fit=300%2C169&amp;ssl=1" data-large-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-15.png?fit=1024%2C578&amp;ssl=1" src="https://i0.wp.com/research.eye.security/wp-content/uploads/image-15.png?resize=1024%2C578&amp;ssl=1" alt="A screenshot of an Azure Active Directory role assignment interface showing a meme depicting a person claiming to be an administrator alongside the role selection options." srcset="https://i0.wp.com/research.eye.security/wp-content/uploads/image-15.png?resize=1024%2C578&amp;ssl=1 1024w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-15.png?resize=300%2C169&amp;ssl=1 300w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-15.png?resize=768%2C433&amp;ssl=1 768w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-15.png?resize=1536%2C867&amp;ssl=1 1536w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-15.png?resize=1200%2C677&amp;ssl=1 1200w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-15.png?resize=512%2C289&amp;ssl=1 512w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-15.png?resize=920%2C519&amp;ssl=1 920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-15.png?w=1538&amp;ssl=1 1538w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure>



<p>Other applications gave error messages during consent.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1024" height="157" data-attachment-id="3452" data-permalink="https://research.eye.security/consent-and-compromise/image-18/" data-orig-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-17.png?fit=2427%2C372&amp;ssl=1" data-orig-size="2427,372" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-17.png?fit=300%2C46&amp;ssl=1" data-large-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-17.png?fit=1024%2C157&amp;ssl=1" src="https://i0.wp.com/research.eye.security/wp-content/uploads/image-17.png?resize=1024%2C157&amp;ssl=1" alt="Error message from a Microsoft application indicating access denial with detailed error description." srcset="https://i0.wp.com/research.eye.security/wp-content/uploads/image-17.png?resize=1024%2C157&amp;ssl=1 1024w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-17.png?resize=300%2C46&amp;ssl=1 300w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-17.png?resize=768%2C118&amp;ssl=1 768w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-17.png?resize=1536%2C235&amp;ssl=1 1536w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-17.png?resize=2048%2C314&amp;ssl=1 2048w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-17.png?resize=1200%2C184&amp;ssl=1 1200w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-17.png?resize=512%2C78&amp;ssl=1 512w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-17.png?resize=920%2C141&amp;ssl=1 920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-17.png?resize=1600%2C245&amp;ssl=1 1600w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-17.png?resize=1920%2C294&amp;ssl=1 1920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-17.png?w=2427&amp;ssl=1 2427w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1024" height="291" data-attachment-id="3451" data-permalink="https://research.eye.security/consent-and-compromise/image-17/" data-orig-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-16.png?fit=2212%2C628&amp;ssl=1" data-orig-size="2212,628" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-16.png?fit=300%2C85&amp;ssl=1" data-large-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-16.png?fit=1024%2C291&amp;ssl=1" src="https://i0.wp.com/research.eye.security/wp-content/uploads/image-16.png?resize=1024%2C291&amp;ssl=1" alt="Error message on Microsoft sign-in screen indicating trouble signing in due to application access issues." srcset="https://i0.wp.com/research.eye.security/wp-content/uploads/image-16.png?resize=1024%2C291&amp;ssl=1 1024w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-16.png?resize=300%2C85&amp;ssl=1 300w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-16.png?resize=768%2C218&amp;ssl=1 768w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-16.png?resize=1536%2C436&amp;ssl=1 1536w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-16.png?resize=2048%2C581&amp;ssl=1 2048w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-16.png?resize=1200%2C341&amp;ssl=1 1200w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-16.png?resize=512%2C145&amp;ssl=1 512w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-16.png?resize=920%2C261&amp;ssl=1 920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-16.png?resize=1600%2C454&amp;ssl=1 1600w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-16.png?resize=1920%2C545&amp;ssl=1 1920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-16.png?w=2212&amp;ssl=1 2212w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure>



<p>Both of these errors occurred when application objects define resources (other application objects) they require access to. These other applications could be configured as single-tenant applications, which can not be instantiated in my tenant. Or the application is configured in a way that it already expects the service principal to be there. Required resource access can be found at the Azure AD Graph API at</p>



<pre><code>https://graph.windows.net/myorganization/applicationRefs/&lt;APP_ID&gt;/?api-version=1.6-internal</code></pre>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1024" height="585" data-attachment-id="3455" data-permalink="https://research.eye.security/consent-and-compromise/image-19/" data-orig-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-18.png?fit=1530%2C874&amp;ssl=1" data-orig-size="1530,874" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-18.png?fit=300%2C171&amp;ssl=1" data-large-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-18.png?fit=1024%2C585&amp;ssl=1" src="https://i0.wp.com/research.eye.security/wp-content/uploads/image-18.png?resize=1024%2C585&amp;ssl=1" alt="Code snippet illustrating required resource access in an Azure AD application configuration." srcset="https://i0.wp.com/research.eye.security/wp-content/uploads/image-18.png?resize=1024%2C585&amp;ssl=1 1024w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-18.png?resize=300%2C171&amp;ssl=1 300w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-18.png?resize=768%2C439&amp;ssl=1 768w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-18.png?resize=1200%2C685&amp;ssl=1 1200w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-18.png?resize=512%2C292&amp;ssl=1 512w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-18.png?resize=920%2C526&amp;ssl=1 920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-18.png?w=1530&amp;ssl=1 1530w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure>



<p>Luckily, there is a workaround here. By skipping the consent flow and instantiating a service principal directly without consent, we can at least instantiate each application individually that is configured as a multi-tenant application.</p>



<pre><code>New-AzureADServicePrincipal -AccountEnabled $true -AppId $app_id `
 -AppRoleAssignmentRequired $true -Tags {WindowsAzureActiveDirectoryIntegratedApp}</code></pre>



<p>This creates a service principal without asking consent or checking availability of required resource access. You do still need to assign the user to the application afterwards and give consent to all resources that are available.</p>



<p><strong>Internal Microsoft Applications</strong></p>



<p>We now had all the required tools to consent &amp; compromise. It turned out that 22 internal Microsoft applications were vulnerable and exposed internal data. Some examples.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="833" height="364" data-attachment-id="3933" data-permalink="https://research.eye.security/consent-and-compromise/attachment/002/" data-orig-file="https://i0.wp.com/research.eye.security/wp-content/uploads/002.png?fit=833%2C364&amp;ssl=1" data-orig-size="833,364" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="002" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/research.eye.security/wp-content/uploads/002.png?fit=300%2C131&amp;ssl=1" data-large-file="https://i0.wp.com/research.eye.security/wp-content/uploads/002.png?fit=833%2C364&amp;ssl=1" src="https://i0.wp.com/research.eye.security/wp-content/uploads/002.png?resize=833%2C364&amp;ssl=1" alt="Screenshot of a software interface displaying the Risk Register section with options for viewing risks, threat models, outage relationships, and risk reporting." srcset="https://i0.wp.com/research.eye.security/wp-content/uploads/002.png?w=833&amp;ssl=1 833w, https://i0.wp.com/research.eye.security/wp-content/uploads/002.png?resize=300%2C131&amp;ssl=1 300w, https://i0.wp.com/research.eye.security/wp-content/uploads/002.png?resize=768%2C336&amp;ssl=1 768w, https://i0.wp.com/research.eye.security/wp-content/uploads/002.png?resize=512%2C224&amp;ssl=1 512w" sizes="auto, (max-width: 833px) 100vw, 833px"></figure>



<p>This ‚ÄúRisk Register‚Äù contained [<em>redacted on request by Microsoft</em>].</p>



<p>Another interesting application was the Security Intelligence Platform. This application contained security intelligence datasets with names that speak for themselves.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1024" height="575" data-attachment-id="3469" data-permalink="https://research.eye.security/consent-and-compromise/image-24/" data-orig-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-23.png?fit=2400%2C1348&amp;ssl=1" data-orig-size="2400,1348" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-23.png?fit=300%2C169&amp;ssl=1" data-large-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-23.png?fit=1024%2C575&amp;ssl=1" src="https://i0.wp.com/research.eye.security/wp-content/uploads/image-23.png?resize=1024%2C575&amp;ssl=1" alt="Screenshot of the Security Intelligence Platform showing datasets with names like 'AAD_GroupMembers' and 'AAD_UserData', along with options to request access." srcset="https://i0.wp.com/research.eye.security/wp-content/uploads/image-23.png?resize=1024%2C575&amp;ssl=1 1024w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-23.png?resize=300%2C169&amp;ssl=1 300w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-23.png?resize=768%2C431&amp;ssl=1 768w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-23.png?resize=1536%2C863&amp;ssl=1 1536w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-23.png?resize=2048%2C1150&amp;ssl=1 2048w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-23.png?resize=1200%2C674&amp;ssl=1 1200w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-23.png?resize=512%2C288&amp;ssl=1 512w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-23.png?resize=920%2C517&amp;ssl=1 920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-23.png?resize=1600%2C899&amp;ssl=1 1600w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-23.png?resize=1920%2C1078&amp;ssl=1 1920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-23.png?w=2400&amp;ssl=1 2400w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1024" height="598" data-attachment-id="3470" data-permalink="https://research.eye.security/consent-and-compromise/image-25/" data-orig-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-24.png?fit=2310%2C1350&amp;ssl=1" data-orig-size="2310,1350" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-24.png?fit=300%2C175&amp;ssl=1" data-large-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-24.png?fit=1024%2C598&amp;ssl=1" src="https://i0.wp.com/research.eye.security/wp-content/uploads/image-24.png?resize=1024%2C598&amp;ssl=1" alt="A grid of application service request cards, each displaying the application name and a 'Request Access' button, set on a blue background." srcset="https://i0.wp.com/research.eye.security/wp-content/uploads/image-24.png?resize=1024%2C598&amp;ssl=1 1024w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-24.png?resize=300%2C175&amp;ssl=1 300w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-24.png?resize=768%2C449&amp;ssl=1 768w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-24.png?resize=1536%2C898&amp;ssl=1 1536w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-24.png?resize=2048%2C1197&amp;ssl=1 2048w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-24.png?resize=1200%2C701&amp;ssl=1 1200w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-24.png?resize=512%2C299&amp;ssl=1 512w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-24.png?resize=920%2C538&amp;ssl=1 920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-24.png?resize=1600%2C935&amp;ssl=1 1600w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-24.png?resize=1920%2C1122&amp;ssl=1 1920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-24.png?w=2310&amp;ssl=1 2310w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure>



<p>Unfortunately for us, access to these datasets still required admin approval. I wondered how those access requests work. It has a button ‚ÄúAsk Sippy‚Äù. Maybe Sippy knows?</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1024" height="386" data-attachment-id="3473" data-permalink="https://research.eye.security/consent-and-compromise/image-27/" data-orig-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-26.png?fit=2330%2C878&amp;ssl=1" data-orig-size="2330,878" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-26.png?fit=300%2C113&amp;ssl=1" data-large-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-26.png?fit=1024%2C386&amp;ssl=1" src="https://i0.wp.com/research.eye.security/wp-content/uploads/image-26.png?resize=1024%2C386&amp;ssl=1" alt="A chat interface displaying a virtual assistant named Sippy, providing responses about accessing admin portals and information requests." srcset="https://i0.wp.com/research.eye.security/wp-content/uploads/image-26.png?resize=1024%2C386&amp;ssl=1 1024w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-26.png?resize=300%2C113&amp;ssl=1 300w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-26.png?resize=768%2C289&amp;ssl=1 768w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-26.png?resize=1536%2C579&amp;ssl=1 1536w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-26.png?resize=2048%2C772&amp;ssl=1 2048w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-26.png?resize=1200%2C452&amp;ssl=1 1200w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-26.png?resize=512%2C193&amp;ssl=1 512w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-26.png?resize=920%2C347&amp;ssl=1 920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-26.png?resize=1600%2C603&amp;ssl=1 1600w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-26.png?resize=1920%2C724&amp;ssl=1 1920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-26.png?w=2330&amp;ssl=1 2330w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure>



<p>At least this application allowed me to search the entire Microsoft tenant for Service Principal Names and user accounts.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1024" height="561" data-attachment-id="3475" data-permalink="https://research.eye.security/consent-and-compromise/image-28/" data-orig-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-27.png?fit=2400%2C1315&amp;ssl=1" data-orig-size="2400,1315" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-27.png?fit=300%2C164&amp;ssl=1" data-large-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-27.png?fit=1024%2C561&amp;ssl=1" src="https://i0.wp.com/research.eye.security/wp-content/uploads/image-27.png?resize=1024%2C561&amp;ssl=1" alt="Screenshot of the Security Intelligence Platform showing an access request form with fields for AAD Object Id, Comms Aliases, Access Package Name, and Justification." srcset="https://i0.wp.com/research.eye.security/wp-content/uploads/image-27.png?resize=1024%2C561&amp;ssl=1 1024w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-27.png?resize=300%2C164&amp;ssl=1 300w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-27.png?resize=768%2C421&amp;ssl=1 768w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-27.png?resize=1536%2C842&amp;ssl=1 1536w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-27.png?resize=2048%2C1122&amp;ssl=1 2048w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-27.png?resize=1200%2C658&amp;ssl=1 1200w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-27.png?resize=512%2C281&amp;ssl=1 512w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-27.png?resize=920%2C504&amp;ssl=1 920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-27.png?resize=1600%2C877&amp;ssl=1 1600w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-27.png?resize=1920%2C1052&amp;ssl=1 1920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-27.png?w=2400&amp;ssl=1 2400w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1024" height="464" data-attachment-id="3476" data-permalink="https://research.eye.security/consent-and-compromise/image-29/" data-orig-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-28.png?fit=2397%2C1086&amp;ssl=1" data-orig-size="2397,1086" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-28.png?fit=300%2C136&amp;ssl=1" data-large-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-28.png?fit=1024%2C464&amp;ssl=1" src="https://i0.wp.com/research.eye.security/wp-content/uploads/image-28.png?resize=1024%2C464&amp;ssl=1" alt="Screenshot of the Security Intelligence Platform's access request interface, displaying fields for AAD Object ID, Access Package Name, and Justification for SPN access request." srcset="https://i0.wp.com/research.eye.security/wp-content/uploads/image-28.png?resize=1024%2C464&amp;ssl=1 1024w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-28.png?resize=300%2C136&amp;ssl=1 300w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-28.png?resize=768%2C348&amp;ssl=1 768w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-28.png?resize=1536%2C696&amp;ssl=1 1536w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-28.png?resize=2048%2C928&amp;ssl=1 2048w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-28.png?resize=1200%2C544&amp;ssl=1 1200w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-28.png?resize=512%2C232&amp;ssl=1 512w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-28.png?resize=920%2C417&amp;ssl=1 920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-28.png?resize=1600%2C725&amp;ssl=1 1600w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-28.png?resize=1920%2C870&amp;ssl=1 1920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-28.png?w=2397&amp;ssl=1 2397w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure>



<p>I even found a logfile that contained Authorization Codes for all the users that had logged in. </p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1024" height="114" data-attachment-id="3478" data-permalink="https://research.eye.security/consent-and-compromise/image-30/" data-orig-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-29.png?fit=2286%2C254&amp;ssl=1" data-orig-size="2286,254" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-29.png?fit=300%2C33&amp;ssl=1" data-large-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-29.png?fit=1024%2C114&amp;ssl=1" src="https://i0.wp.com/research.eye.security/wp-content/uploads/image-29.png?resize=1024%2C114&amp;ssl=1" alt="Code snippet showing log data from a Microsoft application, including page IDs, URLs, user IDs, and timestamps." srcset="https://i0.wp.com/research.eye.security/wp-content/uploads/image-29.png?resize=1024%2C114&amp;ssl=1 1024w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-29.png?resize=300%2C33&amp;ssl=1 300w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-29.png?resize=768%2C85&amp;ssl=1 768w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-29.png?resize=1536%2C171&amp;ssl=1 1536w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-29.png?resize=2048%2C228&amp;ssl=1 2048w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-29.png?resize=1200%2C133&amp;ssl=1 1200w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-29.png?resize=512%2C57&amp;ssl=1 512w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-29.png?resize=920%2C102&amp;ssl=1 920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-29.png?resize=1600%2C178&amp;ssl=1 1600w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-29.png?resize=1920%2C213&amp;ssl=1 1920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-29.png?w=2286&amp;ssl=1 2286w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure>



<p>How unfortunate that these can only be redeemed once‚Ä¶</p>



<p>The next application was actually a really big forest of connected applications.</p>


<div>
<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="904" height="1024" data-attachment-id="3479" data-permalink="https://research.eye.security/consent-and-compromise/image-31/" data-orig-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-30.png?fit=930%2C1054&amp;ssl=1" data-orig-size="930,1054" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-30.png?fit=265%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-30.png?fit=904%2C1024&amp;ssl=1" src="https://i0.wp.com/research.eye.security/wp-content/uploads/image-30.png?resize=904%2C1024&amp;ssl=1" alt="Screenshot of a Microsoft permissions request page displaying an unverified app 'MediaCreation-thanos-WebPortal' along with permission details." srcset="https://i0.wp.com/research.eye.security/wp-content/uploads/image-30.png?resize=904%2C1024&amp;ssl=1 904w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-30.png?resize=265%2C300&amp;ssl=1 265w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-30.png?resize=768%2C870&amp;ssl=1 768w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-30.png?resize=512%2C580&amp;ssl=1 512w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-30.png?resize=920%2C1043&amp;ssl=1 920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-30.png?w=930&amp;ssl=1 930w" sizes="auto, (max-width: 904px) 100vw, 904px"></figure></div>


<p>This turned out to be the ‚ÄúMedia Creation‚Äù service. It promised great things to come. Well bring it on.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1024" height="220" data-attachment-id="3481" data-permalink="https://research.eye.security/consent-and-compromise/image-32/" data-orig-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-31.png?fit=2391%2C514&amp;ssl=1" data-orig-size="2391,514" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-31.png?fit=300%2C64&amp;ssl=1" data-large-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-31.png?fit=1024%2C220&amp;ssl=1" src="https://i0.wp.com/research.eye.security/wp-content/uploads/image-31.png?resize=1024%2C220&amp;ssl=1" alt="Screenshot of the MediaBuilder portal, featuring the text 'Great things come to those who don't wait. Build in the cloud today.' with a Microsoft Corporation copyright notice." srcset="https://i0.wp.com/research.eye.security/wp-content/uploads/image-31.png?resize=1024%2C220&amp;ssl=1 1024w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-31.png?resize=300%2C64&amp;ssl=1 300w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-31.png?resize=768%2C165&amp;ssl=1 768w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-31.png?resize=1536%2C330&amp;ssl=1 1536w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-31.png?resize=2048%2C440&amp;ssl=1 2048w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-31.png?resize=1200%2C258&amp;ssl=1 1200w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-31.png?resize=512%2C110&amp;ssl=1 512w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-31.png?resize=920%2C198&amp;ssl=1 920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-31.png?resize=1600%2C344&amp;ssl=1 1600w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-31.png?resize=1920%2C413&amp;ssl=1 1920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-31.png?w=2391&amp;ssl=1 2391w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure>



<p>What media is it actually building? Is that‚Ä¶ Windows?</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1024" height="645" data-attachment-id="3482" data-permalink="https://research.eye.security/consent-and-compromise/image-33/" data-orig-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-32.png?fit=2142%2C1350&amp;ssl=1" data-orig-size="2142,1350" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-32.png?fit=300%2C189&amp;ssl=1" data-large-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-32.png?fit=1024%2C645&amp;ssl=1" src="https://i0.wp.com/research.eye.security/wp-content/uploads/image-32.png?resize=1024%2C645&amp;ssl=1" alt="Screenshot of a web interface displaying details for a media creation buildable artifact, with sections for properties like BuildableArtifactId, RequestGraphId, FQBN, and more." srcset="https://i0.wp.com/research.eye.security/wp-content/uploads/image-32.png?resize=1024%2C645&amp;ssl=1 1024w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-32.png?resize=300%2C189&amp;ssl=1 300w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-32.png?resize=768%2C484&amp;ssl=1 768w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-32.png?resize=1536%2C968&amp;ssl=1 1536w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-32.png?resize=2048%2C1291&amp;ssl=1 2048w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-32.png?resize=1200%2C756&amp;ssl=1 1200w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-32.png?resize=512%2C323&amp;ssl=1 512w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-32.png?resize=920%2C580&amp;ssl=1 920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-32.png?resize=1600%2C1008&amp;ssl=1 1600w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-32.png?resize=1920%2C1210&amp;ssl=1 1920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-32.png?w=2142&amp;ssl=1 2142w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure>



<p>Let‚Äôs dive in. There are logfiles.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1024" height="901" data-attachment-id="3484" data-permalink="https://research.eye.security/consent-and-compromise/image-34/" data-orig-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-33.png?fit=1309%2C1152&amp;ssl=1" data-orig-size="1309,1152" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-33.png?fit=300%2C264&amp;ssl=1" data-large-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-33.png?fit=1024%2C901&amp;ssl=1" src="https://i0.wp.com/research.eye.security/wp-content/uploads/image-33.png?resize=1024%2C901&amp;ssl=1" alt="A list of log files associated with a task, including file names, sizes, and options to view or download each file." srcset="https://i0.wp.com/research.eye.security/wp-content/uploads/image-33.png?resize=1024%2C901&amp;ssl=1 1024w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-33.png?resize=300%2C264&amp;ssl=1 300w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-33.png?resize=768%2C676&amp;ssl=1 768w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-33.png?resize=1200%2C1056&amp;ssl=1 1200w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-33.png?resize=512%2C451&amp;ssl=1 512w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-33.png?resize=920%2C810&amp;ssl=1 920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-33.png?w=1309&amp;ssl=1 1309w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure>



<p>And one of these even contains some private key. </p>


<div>
<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="787" height="261" data-attachment-id="3485" data-permalink="https://research.eye.security/consent-and-compromise/image-35/" data-orig-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-34.png?fit=787%2C261&amp;ssl=1" data-orig-size="787,261" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-34.png?fit=300%2C99&amp;ssl=1" data-large-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-34.png?fit=787%2C261&amp;ssl=1" src="https://i0.wp.com/research.eye.security/wp-content/uploads/image-34.png?resize=787%2C261&amp;ssl=1" alt="Screenshot of a string containing a private key and associated public key, likely related to an application or service configuration." srcset="https://i0.wp.com/research.eye.security/wp-content/uploads/image-34.png?w=787&amp;ssl=1 787w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-34.png?resize=300%2C99&amp;ssl=1 300w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-34.png?resize=768%2C255&amp;ssl=1 768w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-34.png?resize=512%2C170&amp;ssl=1 512w" sizes="auto, (max-width: 787px) 100vw, 787px"></figure></div>


<p>ESD stands for ‚ÄúElectronic Software Distribution‚Äù and is used to generate licence keys. Could it be we can now generate licence keys?</p>



<p>This application also gave us access to an interesting API.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1024" height="477" data-attachment-id="3487" data-permalink="https://research.eye.security/consent-and-compromise/image-36/" data-orig-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-35.png?fit=1516%2C706&amp;ssl=1" data-orig-size="1516,706" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-35.png?fit=300%2C140&amp;ssl=1" data-large-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-35.png?fit=1024%2C477&amp;ssl=1" src="https://i0.wp.com/research.eye.security/wp-content/uploads/image-35.png?resize=1024%2C477&amp;ssl=1" alt="Screenshot of a Windows Build API documentation for Source Lookup with endpoints for DeltaForgeSource operations, accompanied by a meme featuring a quote that suggests witnessing unbelievable things." srcset="https://i0.wp.com/research.eye.security/wp-content/uploads/image-35.png?resize=1024%2C477&amp;ssl=1 1024w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-35.png?resize=300%2C140&amp;ssl=1 300w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-35.png?resize=768%2C358&amp;ssl=1 768w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-35.png?resize=1200%2C559&amp;ssl=1 1200w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-35.png?resize=512%2C238&amp;ssl=1 512w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-35.png?resize=920%2C428&amp;ssl=1 920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-35.png?w=1516&amp;ssl=1 1516w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure>



<p>And finally gave us RCE on their build infrastructure by defining a new tool.</p>


<div>
<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1024" height="821" data-attachment-id="3488" data-permalink="https://research.eye.security/consent-and-compromise/image-37/" data-orig-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-36.png?fit=1198%2C960&amp;ssl=1" data-orig-size="1198,960" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-36.png?fit=300%2C240&amp;ssl=1" data-large-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-36.png?fit=1024%2C821&amp;ssl=1" src="https://i0.wp.com/research.eye.security/wp-content/uploads/image-36.png?resize=1024%2C821&amp;ssl=1" alt="A form for creating a new version of a tool in a Microsoft application, displaying fields for name, tool path, locator type, source URI, package name, version, and tag." srcset="https://i0.wp.com/research.eye.security/wp-content/uploads/image-36.png?resize=1024%2C821&amp;ssl=1 1024w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-36.png?resize=300%2C240&amp;ssl=1 300w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-36.png?resize=768%2C615&amp;ssl=1 768w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-36.png?resize=512%2C410&amp;ssl=1 512w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-36.png?resize=920%2C737&amp;ssl=1 920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-36.png?w=1198&amp;ssl=1 1198w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure></div>


<p>Next to these services, we also got access to several more internal Microsoft applications.</p>



<ul>
<li>Engage ACE Hub</li>



<li>Responsible AI Ops Platform</li>



<li>Billing Account of Microsoft Internal (BAMI) portal</li>



<li>CPET webservice</li>



<li>HxSDK Documentation</li>



<li>Hardware Inventory API</li>



<li>Electronic Label Management</li>



<li>Quality Checkpoint</li>



<li>Ready to Deploy app</li>



<li>Bing ads SA Diagnostic Tool</li>



<li>SBS tool (Copilot Human Correlation Tool)</li>



<li>Secure Devices Portal</li>



<li>Azure Subscription Hub SLM API</li>
</ul>



<p>This research highlights the risk a single misconfiguration in a large environment can pose. The problem lies in a shared responsibility when deploying these applications. When the application developers rely on other teams to register their applications in Entra, both will think they have set up their part correctly.</p>



<p><strong>Are you vulnerable?</strong></p>



<p>So is this vulnerability still out there? Yes! 2% of our own clients were affected. What should you do?</p>



<ul>
<li>Only use Multi-Tenant applications when necessary</li>



<li>If using Multi-Tenant applications, make sure to check the ‚Äúiss‚Äù or ‚Äútid‚Äù claim in the access token in application logic</li>
</ul>



<p>We have written a small PowerShell script to quickly identify all Multi-Tenant applications in your own Entra environment and their respective redirect URIs.</p>



<pre><code>&gt; .\ListMultiTenantApplications.ps1
Potentially vulnerable App Registrations found:

DisplayName                        AppId                                RedirectUris
-----------                        -----                                ------------
Eye Security Secret App            8123db1e-3ae6-4068-abcd-f45acafee99c https://somepath.eye.security
Eye Security Research Blog         74561b55-4eee-4db9-dead-c80ababee56d https://research.eye.security/rest/oauth2-credential/callback</code></pre>



<p>Check if any of the redirect URI‚Äôs is reachable from the internet and make sure all listed applications are checking the ‚Äúiss‚Äù or ‚Äútid‚Äù claim in the access token in application logic.</p>



<p>The PowerShell script can be <a href="https://github.com/the1bernard/ListMultiTenantApplications/tree/main">downloaded here</a>.</p>



<p><strong>Timeline &amp; Reward</strong></p>



<p>I submitted the first four cases to the MSRC in November 2024 but got distracted by work for a while. Microsoft scaled up a project team in the meantime and this resulted in a race between the Microsoft internal Azure Security Variant Hunting Team and me to submit more vulnerable services. I won 17 of the 18 submissions I did in January 2025. Almost all cases were resolved within two months and this got me third place on the <a data-type="link" data-id="https://msrc.microsoft.com/blog/2025/05/congratulations-to-the-top-msrc-2025-q1-security-researchers/" href="https://msrc.microsoft.com/blog/2025/05/congratulations-to-the-top-msrc-2025-q1-security-researchers/">MSRC Q1 leaderboard</a>.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1024" height="504" data-attachment-id="3491" data-permalink="https://research.eye.security/consent-and-compromise/image-38/" data-orig-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-37.png?fit=2202%2C1084&amp;ssl=1" data-orig-size="2202,1084" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-37.png?fit=300%2C148&amp;ssl=1" data-large-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-37.png?fit=1024%2C504&amp;ssl=1" src="https://i0.wp.com/research.eye.security/wp-content/uploads/image-37.png?resize=1024%2C504&amp;ssl=1" alt="A congratulatory message from the Microsoft Security Response Center recognizing top researchers in the Q1 2025 Security Researcher Leaderboard." srcset="https://i0.wp.com/research.eye.security/wp-content/uploads/image-37.png?resize=1024%2C504&amp;ssl=1 1024w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-37.png?resize=300%2C148&amp;ssl=1 300w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-37.png?resize=768%2C378&amp;ssl=1 768w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-37.png?resize=1536%2C756&amp;ssl=1 1536w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-37.png?resize=2048%2C1008&amp;ssl=1 2048w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-37.png?resize=1200%2C591&amp;ssl=1 1200w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-37.png?resize=512%2C252&amp;ssl=1 512w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-37.png?resize=920%2C453&amp;ssl=1 920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-37.png?resize=1600%2C788&amp;ssl=1 1600w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-37.png?resize=1920%2C945&amp;ssl=1 1920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-37.png?w=2202&amp;ssl=1 2202w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure>



<p>Now as you can imagine, this research made me rich! The total amount of bug bounties was‚Ä¶</p>


<div>
<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="183" height="152" data-attachment-id="3493" data-permalink="https://research.eye.security/consent-and-compromise/image-39/" data-orig-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-38.png?fit=183%2C152&amp;ssl=1" data-orig-size="183,152" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-38.png?fit=183%2C152&amp;ssl=1" data-large-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-38.png?fit=183%2C152&amp;ssl=1" src="https://i0.wp.com/research.eye.security/wp-content/uploads/image-38.png?resize=183%2C152&amp;ssl=1" alt="An illustration of a bag of money with a dollar sign, representing a financial concept or bounty."></figure></div>


<p>What?</p>



<p>After my <a href="https://www.youtube.com/watch?v=uowTmPomYcg">last talk at the 38C3 conference in Hamburg </a>one of the comments on YouTube said</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1024" height="232" data-attachment-id="3495" data-permalink="https://research.eye.security/consent-and-compromise/image-40/" data-orig-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-39.png?fit=1390%2C315&amp;ssl=1" data-orig-size="1390,315" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-39.png?fit=300%2C68&amp;ssl=1" data-large-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-39.png?fit=1024%2C232&amp;ssl=1" src="https://i0.wp.com/research.eye.security/wp-content/uploads/image-39.png?resize=1024%2C232&amp;ssl=1" alt="Screenshot of a YouTube comment discussing a talk on bug hunting at Microsoft." srcset="https://i0.wp.com/research.eye.security/wp-content/uploads/image-39.png?resize=1024%2C232&amp;ssl=1 1024w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-39.png?resize=300%2C68&amp;ssl=1 300w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-39.png?resize=768%2C174&amp;ssl=1 768w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-39.png?resize=1200%2C272&amp;ssl=1 1200w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-39.png?resize=512%2C116&amp;ssl=1 512w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-39.png?resize=920%2C208&amp;ssl=1 920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-39.png?w=1390&amp;ssl=1 1390w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure>



<p>So, what happened? Bug hunting at Microsoft was supposed to be an infinite money glitch!</p>



<p>Well, we‚Äôre not done yet.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1024" height="472" data-attachment-id="3497" data-permalink="https://research.eye.security/consent-and-compromise/image-41/" data-orig-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-40.png?fit=1472%2C678&amp;ssl=1" data-orig-size="1472,678" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-40.png?fit=300%2C138&amp;ssl=1" data-large-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-40.png?fit=1024%2C472&amp;ssl=1" src="https://i0.wp.com/research.eye.security/wp-content/uploads/image-40.png?resize=1024%2C472&amp;ssl=1" alt="Screenshot showing a Microsoft permissions request screen with details about the application 'MicrosoftRewardsBrt' and options to select a user role." srcset="https://i0.wp.com/research.eye.security/wp-content/uploads/image-40.png?resize=1024%2C472&amp;ssl=1 1024w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-40.png?resize=300%2C138&amp;ssl=1 300w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-40.png?resize=768%2C354&amp;ssl=1 768w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-40.png?resize=1200%2C553&amp;ssl=1 1200w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-40.png?resize=512%2C236&amp;ssl=1 512w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-40.png?resize=920%2C424&amp;ssl=1 920w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-40.png?w=1472&amp;ssl=1 1472w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure>



<p>This final application titled ‚ÄúRewards Support Tool‚Äù, allowed managing rewards. And I think I do deserve a reward for this, not?</p>



<p>So, let‚Äôs navigate this interesting menu.</p>


<div>
<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="467" height="1024" data-attachment-id="3499" data-permalink="https://research.eye.security/consent-and-compromise/image-42/" data-orig-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-41.png?fit=616%2C1350&amp;ssl=1" data-orig-size="616,1350" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-41.png?fit=137%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-41.png?fit=467%2C1024&amp;ssl=1" src="https://i0.wp.com/research.eye.security/wp-content/uploads/image-41.png?resize=467%2C1024&amp;ssl=1" alt="Screenshot of the Rewards Support Tool displaying various menu options like 'Lookup User Details', 'Create User', and 'Offers'." srcset="https://i0.wp.com/research.eye.security/wp-content/uploads/image-41.png?resize=467%2C1024&amp;ssl=1 467w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-41.png?resize=137%2C300&amp;ssl=1 137w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-41.png?resize=512%2C1122&amp;ssl=1 512w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-41.png?w=616&amp;ssl=1 616w" sizes="auto, (max-width: 467px) 100vw, 467px"></figure></div>


<p>And go to the ‚ÄúRebate‚Äù page.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="732" height="905" data-attachment-id="3502" data-permalink="https://research.eye.security/consent-and-compromise/image-44/" data-orig-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-43.png?fit=732%2C905&amp;ssl=1" data-orig-size="732,905" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-43.png?fit=243%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/research.eye.security/wp-content/uploads/image-43.png?fit=732%2C905&amp;ssl=1" src="https://i0.wp.com/research.eye.security/wp-content/uploads/image-43.png?resize=732%2C905&amp;ssl=1" alt="Screenshot of the 'Rewards Support Tool' page displaying a rebate submission form with fields for PUID, amount, currency, email, phone number, and a code." srcset="https://i0.wp.com/research.eye.security/wp-content/uploads/image-43.png?w=732&amp;ssl=1 732w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-43.png?resize=243%2C300&amp;ssl=1 243w, https://i0.wp.com/research.eye.security/wp-content/uploads/image-43.png?resize=512%2C633&amp;ssl=1 512w" sizes="auto, (max-width: 732px) 100vw, 732px"></figure>



<p>Now just enter any amount, currency, PayPal ID, phone number and code, and hit ‚ÄúPayout‚Äù.</p>



<p>Turns out, hacking Microsoft still <strong>is</strong> an infinite money glitch.</p>



<h3>About Eye Security</h3>



<p>We are a European cybersecurity company focused on 24/7 threat monitoring, incident response, and cyber insurance. Our research team performs proactive scans and threat intelligence operations across the region to defend our customers and their supply chains.</p>



<p>Learn more at&nbsp;<a href="https://eye.security/" target="_blank" rel="noreferrer noopener">https://eye.security/</a>&nbsp;and&nbsp;<a href="https://www.linkedin.com/company/eyesecurity/">follow us on LinkedIn</a>&nbsp;to help us spread the word.</p>

			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Curious about the training data of OpenAI's new GPT-OSS models? I was too (206 pts)]]></title>
            <link>https://twitter.com/jxmnop/status/1953899426075816164</link>
            <guid>44850260</guid>
            <pubDate>Sat, 09 Aug 2025 21:10:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/jxmnop/status/1953899426075816164">https://twitter.com/jxmnop/status/1953899426075816164</a>, See on <a href="https://news.ycombinator.com/item?id=44850260">Hacker News</a></p>
Couldn't get https://twitter.com/jxmnop/status/1953899426075816164: Error: Request failed with status code 400]]></description>
        </item>
    </channel>
</rss>