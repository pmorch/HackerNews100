<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 01 Feb 2024 18:00:11 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[My favourite Git commit (2019) (183 pts)]]></title>
            <link>https://dhwthompson.com/2019/my-favourite-git-commit</link>
            <guid>39217149</guid>
            <pubDate>Thu, 01 Feb 2024 15:46:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dhwthompson.com/2019/my-favourite-git-commit">https://dhwthompson.com/2019/my-favourite-git-commit</a>, See on <a href="https://news.ycombinator.com/item?id=39217149">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>I like Git commit messages.
Used well, I think they’re one of the most powerful tools available to document a codebase over its lifetime.
I’d like to illustrate that by showing you my favourite ever Git commit.</p>

<p>This commit is from my time at the Government Digital Service, working on GOV.UK.
It’s from a developer by the name of <a href="https://twitter.com/dancarley">Dan Carley</a>, and it has the rather unassuming name of “<a href="https://github.com/alphagov/govuk-puppet/commit/63b36f93bf75a848e2125008aa1e880c5861cf46">Convert template to US-ASCII to fix error</a>”.</p>

<p><img src="https://dhwthompson.com/assets/the-carley-commit.png" alt=""></p>

<p>A quick aside: one of <a href="https://gds.blog.gov.uk/2017/09/04/the-benefits-of-coding-in-the-open/">the benefits of coding in the open</a>, as practised at GDS, is that it’s possible to share examples like this outside the organisation that produced them.
I’m not sure who first introduced that idea to GDS –&nbsp;it was well-established by the time I joined –&nbsp;but I’m forever grateful to them.</p>

<h2 id="why-i-like-this-commit">Why I like this commit</h2>
<p>I’ve lost count of the number of times I’ve shared this as an example of what commit messages can do.
It’s fun because of the ratio of commit message to code change, but that’s not why I think it’s worth sharing.</p>

<p>In a different organisation, from a different developer, this entire commit message might have been <code>change whitespace</code>, or <code>fix bug</code>, or (depending on the team’s culture) some less than flattering opinions about the inventor of the non-breaking space.
Instead, Dan took the time to craft a really useful commit message for the benefit of those around him.
I’d like to step through a few of the ways I think this is a really good example.</p>

<h3 id="it-explains-the-reason-for-the-change">It explains the reason for the change</h3>
<p>The best commit messages I’ve seen don’t just explain <strong>what</strong> they’ve changed: they explain <strong>why</strong>.
In this instance:</p>

<div><pre><code>I introduced some tests in a feature branch to match the contents of
`/etc/nginx/router_routes.conf`. They worked fine when run with `bundle exec
rake spec` or `bundle exec rspec modules/router/spec`. But when run as
`bundle exec rake` each should block failed with:

    ArgumentError:
      invalid byte sequence in US-ASCII
</code></pre></div>

<p>Without this level of detail, we could hazard a guess that this commit fixed some kind of parsing error in some tool or other.
Thanks to the commit message, we know exactly which tool it was.</p>

<p>This kind of information can be really valuable to document, and is all too easy to lose as people forget the original context behind their work, move on to other teams, and eventually leave the organisation.</p>

<h3 id="its-searchable">It’s searchable</h3>
<p>One of the first things in this commit message is the error message that inspired the change:</p>

<div><pre><code>ArgumentError:
  invalid byte sequence in US-ASCII
</code></pre></div>

<p>Anyone else who comes across this error can search through the codebase, either
by running <code>git log --grep "invalid byte sequence"</code> or by <a href="https://github.com/alphagov/govuk-puppet/search?q=%22invalid+byte+sequence%22&amp;type=Commits">using GitHub’s commit search</a>.
In fact, from the looks of the search results, multiple people did so, and found out who had found this problem before, when they came across it, and what they did about it.</p>

<h3 id="it-tells-a-story">It tells a story</h3>
<p>This commit message goes into a lot of detail about what the problem looked like, what the process of investigating it looked like, and what the process of fixing it looked like.
For example:</p>

<div><pre><code>I eventually found that removing the `.with_content(//)` matchers made the
errors go away. That there weren't any weird characters in the spec file. And
that it could be reproduced by requiring Puppet in the same interpreter
</code></pre></div>

<p>This is one of the areas commit messages can really shine, because they’re documenting the change itself, rather than documenting a particular file, or function, or line of code.
This makes them a great place to document this kind of extra information about the journey the codebase has taken.</p>

<h3 id="it-makes-everyone-a-little-smarter">It makes everyone a little smarter</h3>
<p>One thing Dan did here that I really appreciate was to document the commands he ran at each stage.
This can be a great lightweight way to spread knowledge around a team.
By reading this commit message, someone can learn quite a few useful tips about the Unix toolset:</p>

<ul>
  <li>they can pass an <code>-exec</code> argument into <code>find</code> to run a command against each file found</li>
  <li>that adding a <code>\+</code> onto the end of this command does something interesting (it passes many filenames into a single <code>file</code> command, rather than running the command once per file)</li>
  <li><code>file --mime</code> can tell them the MIME type of a file</li>
  <li><code>iconv</code> exists</li>
</ul>

<p>The person who reviews this change can learn these things.
Anyone who finds this commit later can learn these things.
Over enough time and enough commits, this can become a really powerful multiplier for a team.</p>

<h3 id="it-builds-compassion-and-trust">It builds compassion and trust</h3>

<div><pre><code>Now the tests work! One hour of my life I won't get back..
</code></pre></div>

<p>This last paragraph adds an extra bit of human context.
Reading these words, it’s hard not to feel just a little bit of Dan’s frustration at having to spend an hour tracking down a sneaky bug, and satisfaction at fixing it.</p>

<p>Now imagine a similar message attached to a short-term hack, or a piece of prototype code that made its way into production and set down roots (as pieces of prototype code love to do).
A commit message like this makes it much easier to remember that every change has a human on the other end of it, making the best decision they could given the information they had at the time.</p>

<h2 id="good-commits-matter">Good commits matter</h2>

<p>I’ll admit this is an extreme example, and I wouldn’t expect all commits (especially ones of this size) to have this level of detail.
Still, I think it’s an excellent example of explaining the context behind a change, of helping others to learn, and of contributing to the team’s collective mental model of the codebase.</p>

<p>If you’re interested in learning a bit more about the benefits of good commit messages, and some of the tools that make it easier to structure your changes around them, I can recommend:</p>

<ul>
  <li><a href="https://blog.mocoso.co.uk/talks/2015/01/12/telling-stories-through-your-commits/">Telling stories through your commits</a> by Joel Chippindale</li>
  <li><a href="https://tekin.co.uk/2019/02/a-talk-about-revision-histories">A branch in time</a> by Tekin Süleyman</li>
</ul>


  </div>
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bard's latest updates: Access Gemini Pro globally and generate images (124 pts)]]></title>
            <link>https://blog.google/products/bard/google-bard-gemini-pro-image-generation/</link>
            <guid>39217046</guid>
            <pubDate>Thu, 01 Feb 2024 15:36:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.google/products/bard/google-bard-gemini-pro-image-generation/">https://blog.google/products/bard/google-bard-gemini-pro-image-generation/</a>, See on <a href="https://news.ycombinator.com/item?id=39217046">Hacker News</a></p>
<div id="readability-page-1" class="page"><article ng-init="drawerToggle = {'open': true}">

    
    





    

    
      

<div data-analytics-module="{
    &quot;module_name&quot;: &quot;Hero Menu&quot;,
    &quot;section_header&quot;: &quot;Bard’s latest updates: Access Gemini Pro globally and generate images&quot;
  }">
      
      
        <p>
          Starting today, you can generate images in Bard in most countries, and use Gemini Pro in any language, country and territory Bard currently supports.
        </p>
      
    </div>

    

    
      







<div>
    <figure>
      <div>
  <p><img srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Hero_10_1.width-600.format-webp.webp 600w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Hero_10_1.width-1200.format-webp.webp 1200w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Hero_10_1.width-1600.format-webp.webp 1600w" sizes="(max-width: 599px) 100vw, (max-width: 1023px) 600px, 1024px" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Hero_10_1.width-1200.format-webp.webp" fetchpriority="high" alt="A graphic that reads &quot;Gemini Pro in Bard&quot;">
  </p>
</div>

      
    </figure>
  </div>


    

    
    <div data-reading-time="true" data-component="uni-drop-cap|uni-tombstone">

            

            
            
<!--article text-->

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Bard’s latest updates: Access Gemini Pro globally and generate images&quot;
         }"><p data-block-key="8t52c">Today, we’re bringing Bard’s latest capabilities — including Gemini Pro in Bard — to more languages and places. Plus, we’re introducing image generation to help more of your ideas come to life.</p><h2 data-block-key="d3rc2">Use Gemini Pro in all supported languages and places</h2><p data-block-key="1i5at">Last December, we brought <a href="https://blog.google/products/bard/google-bard-try-gemini-ai/" rt-link-type="external">Gemini Pro into Bard</a> in English, giving Bard more advanced understanding, reasoning, summarizing and coding abilities. Today Gemini Pro in Bard will be available in <a href="https://support.google.com/bard/answer/13575153?hl=en" rt-link-type="external">over 40 languages and more than 230 countries and territories</a>, so more people can collaborate with this faster, more capable version of Bard.</p><p data-block-key="2hj37">The Large Model Systems Organization, a leading evaluator of language models and chatbots across languages, <a href="https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard" rt-link-type="external">recently shared</a> that Bard with Gemini Pro is one of the most preferred chatbots available (with or without cost), <a href="https://twitter.com/lmsysorg/status/1750921228012122526?s=20" rt-link-type="external">noting</a> that it has made a “stunning leap” forward. And blind evaluations with our third-party raters identified Bard with Gemini Pro as one of the top-performing conversational AIs, compared to leading free and paid alternatives.</p></div>
  

  
    







  
      <div data-analytics-module="{
          &quot;module_name&quot;: &quot;Inline Images&quot;,
          &quot;section_header&quot;: &quot;Bard’s latest updates: Access Gemini Pro globally and generate images&quot;
        }">
  

  <p>

      
      
        
          <video tabindex="0" autoplay="" loop="" muted="" playsinline="" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/Bard_with_Gemini_Pro_v02_1.mp4" type="video/mp4" title="A GIF reads “Bard with Gemini Pro now in more languages and places. With Gemini Pro, Bard is more capable at understanding, summarizing, reasoning, planning. Try it today at bard.google.com.”" alt="Bard with Gemini Pro GIF">
            Video format not supported
          </video>
        
      
    
    </p>
    
  
    </div>
  



  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Bard’s latest updates: Access Gemini Pro globally and generate images&quot;
         }"><h2 data-block-key="8t52c">Double-check your responses in more languages</h2><p data-block-key="abcns">Since we know people want the ability to corroborate Bard’s responses, we’re also expanding our <a href="https://blog.google/products/bard/google-bard-new-features-update-sept-2023/" rt-link-type="external">double-check feature</a>, which is already used by millions of people in English, to more than 40 languages. When you click on the “G” icon, Bard will evaluate whether there is content across the web to substantiate its response. If it can be evaluated, you can click the highlighted phrases and learn more about supporting or contradicting information found by Search.</p><h2 data-block-key="ffebd">Bring your ideas to life with image generation</h2><p data-block-key="bfke3">For an extra creative boost, you can now generate images in Bard in <a href="https://support-content-draft.corp.google.com/bard?p=b_gen_img" rt-link-type="external">English</a> in most countries around the world, at no cost. This new capability is powered by our updated <a href="https://blog.google/technology/ai/google-imagen-2" rt-link-type="external">Imagen 2 model</a>, which is designed to balance quality and speed, delivering high-quality, photorealistic outputs. Just type in a description — like “create an image of a dog riding a surfboard” — and Bard will generate custom, wide-ranging visuals to help bring your idea to life.</p></div>
  

  
    







  
      <div data-analytics-module="{
          &quot;module_name&quot;: &quot;Inline Images&quot;,
          &quot;section_header&quot;: &quot;Bard’s latest updates: Access Gemini Pro globally and generate images&quot;
        }">
  

  <p>

      
      
        
          <video tabindex="0" autoplay="" loop="" muted="" playsinline="" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/BARD_Imagen_Desktop_Dog_Demo_1080x1080_v06.mp4" type="video/mp4" title="GIF of a prompt that says &quot;create an image of a dog riding a surfboard.&quot; Image results appear." alt="Bard Imagen">
            Video format not supported
          </video>
        
      
    
    </p>
    
  
    </div>
  



  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Bard’s latest updates: Access Gemini Pro globally and generate images&quot;
         }">
        <p data-block-key="8t52c">Consistent with our <a href="https://ai.google/responsibility/principles/" rt-link-type="external">AI Principles</a>, image generation was designed with responsibility in mind. For instance, to ensure there’s a clear distinction between visuals created with Bard and original human artwork, Bard uses <a href="https://deepmind.google/technologies/synthid/" rt-link-type="external">SynthID</a> to embed digitally identifiable watermarks into the pixels of generated images.</p>
      </div>
  

  
    




  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Bard’s latest updates: Access Gemini Pro globally and generate images&quot;
         }"><p data-block-key="8t52c">Our technical guardrails and investments in the safety of training data seek to limit violent, offensive or sexually explicit content. Additionally, we apply filters designed to avoid the generation of images of named people. We’ll continue investing in new techniques to improve the safety and privacy protections of our models.</p><p data-block-key="fknvb">These updates make Bard an even more helpful and globally accessible AI collaborator for everything from big, creative projects to smaller, everyday tasks. Try it out today at <a href="http://bard.google.com/" rt-link-type="external">bard.google.com</a>.</p></div>
  


            
            

            
              




            
          </div>
  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Damn Small Linux 2024 (224 pts)]]></title>
            <link>https://www.damnsmalllinux.org/</link>
            <guid>39215846</guid>
            <pubDate>Thu, 01 Feb 2024 13:47:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.damnsmalllinux.org/">https://www.damnsmalllinux.org/</a>, See on <a href="https://news.ycombinator.com/item?id=39215846">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <tbody><tr>
      <td>
	<header>
        	<img id="logo" src="https://www.damnsmalllinux.org/img/logo.png" alt="Damn Small Linux">
        	
	</header>      
	 <br>

<nav>
    <a href="https://www.damnsmalllinux.org/">Home</a>
    <a href="https://www.damnsmalllinux.org/2024-download.html">Download DSL 2024</a>
    <a href="https://www.damnsmalllinux.org/forums">DSL Forums</a>
    <a href="https://www.damnsmalllinux.org/support.html">Support DSL</a>
    <a href="https://www.damnsmalllinux.org/old-index.html">Historic DSL</a>
</nav>

        <p>
          The New DSL 2024 has been reborn as a compact Linux distribution tailored for low-spec x86 computers. It packs a lot of applications into a small package. All the applications are chosen for their functionality, small size, and low dependencies. DSL 2024 also has many text-based applications that make it handy to use in a term window or TTY.
        </p>
	



        <p>
          DSL 2024 currently only ships with two window managers: Fluxbox and JWM. Both are lightweight, fairly intuitive, and easy to use.
        </p>
        <p>
          DSL has three X-based web browsers:
          </p><ul>
	    <li><strong>BadWolf</strong> (light, security-oriented, fully HTML5 compatible)</li>
            <li><strong>Dillo</strong> (super-light GUI browser)</li>
            <li><strong>Links2</strong> (text and light GUI browser)</li>
          </ul>
        
        <p>
          For office applications, DSL has:
          </p><ul>
            <li><strong>AbiWord</strong> word processor</li>
            <li><strong>Gnumeric</strong> spreadsheets</li>
            <li><strong>Sylpheed</strong> email client</li>
            <li><strong>Zathura</strong> PDF viewer</li>
          </ul>
        
        <p>
          For multimedia applications:
          </p><ul>
            <li><strong>MPV</strong> (video and audio)</li>
            <li><strong>XMMS</strong> (a lightweight audio player)</li>
          </ul>
        
        <p>
           Other applications:
         </p><ul> 
          <li><strong>mtPaint</strong> (graphics editing)</li>
          <li><strong>gFTP</strong> (FTP, SFTP, and SCP)</li>
          <li><strong>Leafpad</strong> (quick editing)</li>
          <li><strong>zzzFM</strong> (file manager lifted straight out of antiX)</li>
         </ul>
        
        <p>
          There are three GUI-based games picked because they are fun and relatively light.
        </p>
        <p>
          DSL 2024 is also loaded up with a whole bunch of handy term-based applications:
          </p><ul>
            <li><strong>Ranger</strong> file manager</li>
            <li><strong>VisiData</strong> a powerful CSV and spreadsheet tool</li>
            <li><strong>FZF</strong> fuzzy finder</li>
            <li><strong>Tmux</strong> terminal multiplexer</li>
            <li><strong>Mutt</strong> email client</li>
            <li><strong>Cmus</strong> music player</li>
            <li><strong>CDW</strong> CD burner</li>
            <li><strong>Htop</strong>, an interactive process viewer</li>
            <li><strong>SurfRaw</strong> (with wrapper) to search from the term</li>
            <li><strong>Weather App</strong></li>
            <li><strong>MPV</strong> video/audio player with wrapper</li>
            <li><strong>Vim</strong> and <strong>Nano</strong> for editing</li>
            <li>Five term-based games</li>
            <li>Two term-compatible web browsers: <strong>W3M</strong> and <strong>Links2</strong></li>
            <li>...and much more</li>
          </ul>
        
        <p>
          <strong>Why make a new DSL after all these years?</strong>
        </p>
        <p>
          Creating the original DSL, a versatile 50MB distribution, was a lot of fun and one of the things I am most proud of as a personal accomplishment. However, as a concept, it was in the right place at the right time, and the computer industry has changed a lot since then. While it would be possible to make a bootable Xwindows 50MB distribution today, it would be missing many drivers and have only a handful of very rudimentary applications. People would find such a distribution a fun toy or something to build upon, but it would not be usable for the average computer user out of the gate.
        </p>
        <p>
          Meanwhile, in 2024, nearly everyone has abandoned the sub-700MB size limit to run on computers old enough to not have a DVD and cannot boot off of a USB drive. This is completely understandable because applications, the kernel, and drivers have all mushroomed in their space requirements. Hats off to Puppy Linux for staying one of the few that still offer a full desktop environment in a small size.
        </p>
        <p>
          The new goal of DSL is to pack as much usable desktop distribution into an image small enough to fit on a single CD, or a hard limit of 700MB. This project is meant to service older computers and have them continue to be useful far into the future. Such a notion sits well with my values. I think of this project as my way of keeping otherwise usable hardware out of landfills.
        </p>
        <p>
          As with most things in the GNU/Linux community, this project continues to stand on the shoulders of giants. I am just one guy without a CS degree, so for now, this project is based on antiX 23 i386. AntiX is a fantastic distribution that I think shares much of the same spirit as the original DSL project. AntiX shares pedigree with MEPIS and also leans heavily on the geniuses at Debian. So, this project stands on the shoulders of giants. In other words, DSL 2024 is a humble little project!
        </p>
        <p>
          Though it may seem comparably ridiculous that 700MB is small in 2024 when DSL was 50MB in 2002, I’ve done a lot of hunting to find small footprint applications, and I had to do some tricks to get a workable desktop into the 700MB limit. To get the size down, the ISO only has en_US, en_BG, en_CA, and en_AU locales enabled, and I had to strip the source codes, many man pages, and documentation out. I do provide a download script that will restore all the missing files, and so far, it seems to be working well.
        </p>
        <p>
          Unlike the original DSL, this version has apt fully enabled. So if there is anything you feel is missing, it is very simple to get it installed. I also made an effort to leave as much of the antiX goodness enabled as possible. However, it must be said that DSL is a derivative work but also a reductive work. Some things from antiX may be broken or missing. If you find a bug, it is likely my fault.
        </p>
	<h2>Thank you section:</h2>

	<p>Thank you Debian and antiX for doing all the heavy lifting.</p>

	<p>Thank you <a href="http://www.ibiblio.org/"><img src="http://www.ibiblio.org/hosted/images/sm_hosted_trans.gif" width="80" height="24" alt="hosted by ibiblio"></a> for mirroring the DSL downloads all these years.</p>

	<p>Thank you GPedde at DeviantArt for the beautiful <a href="https://www.deviantart.com/gpedde/art/Maintenance-165229479">wallpaper</a>.

	</p><p>Finally, thank you to the users of DSL for your <a href="https://www.damnsmalllinux.org/forums">feedback</a> and <a href="https://www.damnsmalllinux.org/support.html">support</a>.</p>
      
      </td>
    </tr>
  </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FreeBSD 4 Bug may be present in Playstation 4/5 (101 pts)]]></title>
            <link>https://wololo.net/2024/02/01/is-an-18-year-old-vulnerability-the-key-to-the-next-ps5-ps4-jailbreak/</link>
            <guid>39215309</guid>
            <pubDate>Thu, 01 Feb 2024 12:36:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wololo.net/2024/02/01/is-an-18-year-old-vulnerability-the-key-to-the-next-ps5-ps4-jailbreak/">https://wololo.net/2024/02/01/is-an-18-year-old-vulnerability-the-key-to-the-next-ps5-ps4-jailbreak/</a>, See on <a href="https://news.ycombinator.com/item?id=39215309">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>	
				<div>

<!--nok--><p><img decoding="async" fetchpriority="high" src="https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/ps5_crash_test.jpg?resize=768%2C1024&amp;ssl=1" alt="" width="768" height="1024" srcset="https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/ps5_crash_test.jpg?resize=768%2C1024&amp;ssl=1 768w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/ps5_crash_test.jpg?resize=225%2C300&amp;ssl=1 225w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/ps5_crash_test.jpg?w=1024&amp;ssl=1 1024w" sizes="(max-width: 768px) 100vw, 768px" data-recalc-dims="1"></p>
<p>What initially looked like a troll might turn out to be an actual PS4/PS5 vulnerability. While people shouldn’t get their hopes up just right now as this might fizzle, it’s more interesting than it initially looked on the surface.</p><!-- ezoic_video_placeholder-under_first_paragraph-854x480-999998-clearholder --><!-- ezoic_video_placeholder-under_first_paragraph-854x480-999998-nonexxxclearxxxblock --><!-- ezoic_video_placeholder-under_first_paragraph-640x360-999998-clearholder --><!-- ezoic_video_placeholder-under_first_paragraph-640x360-999998-nonexxxclearxxxblock --><!-- ezoic_video_placeholder-under_first_paragraph-426x240-999998-clearholder --><!-- ezoic_video_placeholder-under_first_paragraph-426x240-999998-nonexxxclearxxxblock --><!-- ezoic_video_placeholder-under_first_paragraph-384x216-999998-clearholder --><!-- ezoic_video_placeholder-under_first_paragraph-384x216-999998-nonexxxclearxxxblock -->
<p>Discussions about an 18 year old CVE have been making the rounds of PS4/PS5 hacking discords, forums, and Twitter over the past few days. Some people have been claiming that a Network bug disclosed in 2006 could&nbsp; be leveraged to Jailbreak recent PS4 and PS5 firmwares, and that this might even be the vulnerability (or one of the vulnerabilities in a chain) used by <a href="https://wololo.net/tag">TheFloW</a> in <a href="https://wololo.net/2023/09/23/spoiler-alert-dont-update-theflow-sets-the-ps4-ps5-scene-on-fire-once-again/">one of his most recent PlayStation bug bounties</a>.</p>
<p>Early tests seem to <strong>indicate that a crash is indeed present in PS4 up to 11.00 included, and PS5 8.20 included</strong>. (Which would put the patch for this issue at <a href="https://wololo.net/2023/12/06/ps5-firmware-8-40-and-ps4-11-02-are-out-and-theres-something-interesting-in-there-dont-update/">firmwares PS5 8.40 and PS4 11.02</a>)</p>
<p>I’ll be the first to admit I initially dismissed the original message as a troll, but <strong>it seems the claim has legs</strong>: a PoC (proof of concept) file has been circulating, and multiple people confirm it crashes their PS4 and PS5, up to the firmwares mentioned above. Now, <strong>a crash in itself is certainly not a vulnerability (let alone an exploitable one)</strong>, but this means the claim is now leaving the “troll” territory and becomes worth looking into. And there’s more…</p>
<h2>CVE From 2006 and PS4/PS5 crash PoC</h2>
<p>Before people become overly excited, I’d like to say that <strong>this should still be considered as a rumor, or, at the very least, something that could fizzle, until more skilled devs actually dig into the issue</strong>. However, here’s what happened so far:</p><!-- ezoic_video_placeholder-under_second_paragraph-854x480-999997-clearholder --><!-- ezoic_video_placeholder-under_second_paragraph-854x480-999997-nonexxxclearxxxblock --><!-- ezoic_video_placeholder-under_second_paragraph-640x360-999997-clearholder --><!-- ezoic_video_placeholder-under_second_paragraph-640x360-999997-nonexxxclearxxxblock --><!-- ezoic_video_placeholder-under_second_paragraph-426x240-999997-clearholder --><!-- ezoic_video_placeholder-under_second_paragraph-426x240-999997-nonexxxclearxxxblock --><!-- ezoic_video_placeholder-under_second_paragraph-384x216-999997-clearholder --><!-- ezoic_video_placeholder-under_second_paragraph-384x216-999997-nonexxxclearxxxblock -->
<ul>
<li>An old CVE From 2006 (<a href="https://www.cvedetails.com/cve/CVE-2006-4304/">CVE-2006-4304</a>) was <a href="https://wololo.net/2024/01/26/ps5-update-8-60-is-here-and-guess-what-dont-update/comment-page-1/#comment-6543161">initially reported</a> by some anonymous user to be a “PS4/PS5 KEX” a few days ago. Multiple people, including myself, initially dismissed the claim: “There’s no way a 18-year old FreeBSD 4 bug is present on the PS4 or the PS5, which are based on FreeBSD 9 or above”.</li>
</ul>
<p><img decoding="async" src="https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/initial_claims.png?resize=603%2C339&amp;ssl=1" alt="" width="603" height="339" srcset="https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/initial_claims.png?w=603&amp;ssl=1 603w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/initial_claims.png?resize=300%2C169&amp;ssl=1 300w" sizes="(max-width: 603px) 100vw, 603px" data-recalc-dims="1"></p>
<ul>
<li>However, several folks have tested the <a href="https://gist.github.com/RobbedColek/8b0bc9655adc6ad9564ad71453188e37">poc that’s been circulating</a> (Please note that @Colek who provided this PoC on their github is not the person who initially made the claims) and have confirmed that it crashes their PS4/PS5 consoles, up to some specific firmwares (apparently up to PS4 11.00 and PS5 8.20 included).</li>
</ul>
<p><img decoding="async" src="https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/packet_ps4_ps5_crash.jpg?resize=1024%2C768&amp;ssl=1" alt="" width="1024" height="768" srcset="https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/packet_ps4_ps5_crash-scaled.jpg?resize=1024%2C768&amp;ssl=1 1024w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/packet_ps4_ps5_crash-scaled.jpg?resize=300%2C225&amp;ssl=1 300w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/packet_ps4_ps5_crash-scaled.jpg?resize=768%2C576&amp;ssl=1 768w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/packet_ps4_ps5_crash-scaled.jpg?resize=1536%2C1152&amp;ssl=1 1536w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/packet_ps4_ps5_crash-scaled.jpg?resize=2048%2C1536&amp;ssl=1 2048w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1"></p>

<div id="attachment_48289"><p><img aria-describedby="caption-attachment-48289" decoding="async" loading="lazy" src="https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/ps5_crash_test.jpg?resize=768%2C1024&amp;ssl=1" alt="" width="768" height="1024" srcset="https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/ps5_crash_test.jpg?resize=768%2C1024&amp;ssl=1 768w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/ps5_crash_test.jpg?resize=225%2C300&amp;ssl=1 225w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/ps5_crash_test.jpg?w=1024&amp;ssl=1 1024w" sizes="(max-width: 768px) 100vw, 768px" data-recalc-dims="1"></p><p id="caption-attachment-48289">Screenshots courtesy of hhk2003</p><!-- ezoic_video_placeholder-mid_content-640x360-999996-clearholder --><!-- ezoic_video_placeholder-mid_content-640x360-999996-nonexxxclearxxxblock --><!-- ezoic_video_placeholder-mid_content-426x240-999996-clearholder --><!-- ezoic_video_placeholder-mid_content-426x240-999996-nonexxxclearxxxblock --><!-- ezoic_video_placeholder-mid_content-384x216-999996-clearholder --><!-- ezoic_video_placeholder-mid_content-384x216-999996-nonexxxclearxxxblock --></div>
<p><strong>What the CVE is about</strong>:</p>
<blockquote><p><em>Buffer overflow in the sppp driver in FreeBSD 4.11 through 6.1, NetBSD 2.0 through 4.0 beta before 20060823, and OpenBSD 3.8 and 3.9 before 20060902 allows remote attackers to cause a denial of service (panic), obtain sensitive information, and possibly execute arbitrary code via crafted Link Control Protocol (LCP) packets with an option length that exceeds the overall length, which triggers the overflow in (1) pppoe and (2) ippp.</em></p></blockquote>
<p>This appears to be a bug in the Network stack of FreeBSD/NetBSD. Setting aside for a minute how crazy it would be that an 18 year old vulnerability is still present in modern consoles, at the very least this is an area of the code that we know TheFloW has been investigating a lot in the past (the only known PS5 Kernel exploit was from TheFloW and is related to the Network stack as well).</p>
<p><strong>The poc:</strong></p>
<pre>from scapy.all import PPPoED, Ether, sniff, sendp, srp1, hexdump

source = b"\xXX\xXX\xXX\xXX\xXX\xXX" # MAC address of your adapter on PC
destination = b"\xXX\xXX\xXX\xXX\xXX\xXX" # MAC address of LAN on your PS4
interface = "Realtek PCIe 2.5GbE Family Controller #2" # get via "ipconfig /all" or eth0 or similiar on Linux

packet = sniff(iface=interface, filter="pppoed", count=1)
tag_value = packet[PPPoED][0].tag_list[1].tag_value
payload = destination + source + b"\x88\x63\x11\x07\x00\x00\x00\x0c\x01\x03\x00\x08" + tag_value
sendp(payload, iface=interface)

packet = sniff(iface=interface, filter="pppoed", count=1)
payload = destination + source + b"\x88\x63\x11\x65\x00\x01\x00\x0c\x01\x03\x00\x08" + tag_value
sendp(payload, iface=interface)

packet = sniff(iface=interface, filter="pppoes", count=1)
payload = destination + source + b"\x88\x64\x11\x00\x00\x01\x00\x09\xc0\x21\x01\x01\x00\x07\xab\xff"
packet = srp1(Ether(payload), iface=interface)
print(hexdump(packet))

payload = destination + source + b"\x88\x64\x11\x00\x00\x01\x00\x09\xc0\x21\x01\x01\x00\x07\xab\xff" + b"\xff" * 256 # this number might be different, just a guess
for i in range(20):
sendp(payload, iface=interface)</pre><!-- ezoic_video_placeholder-long_content-854x480-999995-clearholder --><!-- ezoic_video_placeholder-long_content-854x480-999995-nonexxxclearxxxblock --><!-- ezoic_video_placeholder-long_content-640x360-999995-clearholder --><!-- ezoic_video_placeholder-long_content-640x360-999995-nonexxxclearxxxblock --><!-- ezoic_video_placeholder-long_content-426x240-999995-clearholder --><!-- ezoic_video_placeholder-long_content-426x240-999995-nonexxxclearxxxblock --><!-- ezoic_video_placeholder-long_content-384x216-999995-clearholder --><!-- ezoic_video_placeholder-long_content-384x216-999995-nonexxxclearxxxblock -->
<h2>Verdict: Is there a new vulnerability on the PS4/PS5?</h2>
<p>A few thoughts come to mind:</p>
<ul>
<li>In my opinion, if a bug from a CVE has been fixed on some system, it should generally not crash that system after the patch. I understand that in some rare scenarios, a crash is the only way out, but in my experience, most bug fixes intend to handle an error case gracefully. <strong>A crash is not an exploit</strong>, but it is very interesting to see that this code actually crashes the PS4 and PS5
<ul>
<li>A likely explanation, rather than an old bug surfacing, is a new one (or similar one) that happens to be triggered by taking a similar path in the code base.</li>
<li>It is also possible that some bits and pieces of the PS4/PS5 Network stack are not based on FreeBSD, but some other PS4/PS5 specific code that might still have the issue due to not be part of FreeBSD patches.</li>
</ul>
</li>
<li><strong>It is particularly interesting to me that some more recent firmwares do not crash</strong>. This <strong>implies that Sony have fixed some issue, or modified some code in that area, in recent firmwares</strong> (which was actually <a href="https://wololo.net/2023/12/06/ps5-firmware-8-40-and-ps4-11-02-are-out-and-theres-something-interesting-in-there-dont-update/">strongly hinted at</a> when PS4 11.02 and PS5 8.40 were released). This is additional confirmation that this crash could be related to a recent vulnerability that was surfaced to Sony’s engineers (e.g. one of TheFloW’s bugs is not out of the question)</li>
<li>Some people have confirmed that the buggy code does, or did, exist, at some point, in the PS4’s codebase. SlidyBat over at the PS5 R&amp;D discord in particular says that there is indeed some function in the PS4 that seems to use the old buggy code:</li>
</ul>
<p><img decoding="async" loading="lazy" src="https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/slidybat_tests.png?resize=1012%2C491&amp;ssl=1" alt="" width="1012" height="491" srcset="https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/slidybat_tests.png?w=1012&amp;ssl=1 1012w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/slidybat_tests.png?resize=300%2C146&amp;ssl=1 300w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/slidybat_tests.png?resize=768%2C373&amp;ssl=1 768w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1"></p>
<p><img decoding="async" loading="lazy" src="https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/slidybat_code_change.png?resize=1024%2C295&amp;ssl=1" alt="" width="1024" height="295" srcset="https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/slidybat_code_change.png?resize=1024%2C295&amp;ssl=1 1024w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/slidybat_code_change.png?resize=300%2C86&amp;ssl=1 300w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/slidybat_code_change.png?resize=768%2C221&amp;ssl=1 768w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/slidybat_code_change.png?w=1300&amp;ssl=1 1300w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1"></p>
<p><img decoding="async" loading="lazy" src="https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/slidybat_ps4_code.png?resize=1024%2C575&amp;ssl=1" alt="" width="1024" height="575" srcset="https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/slidybat_ps4_code.png?resize=1024%2C575&amp;ssl=1 1024w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/slidybat_ps4_code.png?resize=300%2C169&amp;ssl=1 300w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/slidybat_ps4_code.png?resize=768%2C431&amp;ssl=1 768w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/slidybat_ps4_code.png?resize=1536%2C863&amp;ssl=1 1536w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/slidybat_ps4_code.png?w=1670&amp;ssl=1 1670w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1"></p>
<h2>Download and test the PoC</h2><!-- ezoic_video_placeholder-longer_content-854x480-999994-clearholder --><!-- ezoic_video_placeholder-longer_content-854x480-999994-nonexxxclearxxxblock --><!-- ezoic_video_placeholder-longer_content-640x360-999994-clearholder --><!-- ezoic_video_placeholder-longer_content-640x360-999994-nonexxxclearxxxblock --><!-- ezoic_video_placeholder-longer_content-426x240-999994-clearholder --><!-- ezoic_video_placeholder-longer_content-426x240-999994-nonexxxclearxxxblock --><!-- ezoic_video_placeholder-longer_content-384x216-999994-clearholder --><!-- ezoic_video_placeholder-longer_content-384x216-999994-nonexxxclearxxxblock -->
<p>It is of course <strong>too early to tell if this could lead to anything on the PS4 and PS5</strong>, but for sure this looks <strong>way more interesting than I initially thought </strong>it did, a few days ago. We’re at the very early stages of this, and although this won’t be useful to anyone without the right skills to dig further, it probably doesn’t hurt for people to try and run the PoC on their PS4 and PS5.</p>
<p>Feel free to give it a try and report in the comments whether this crashed your console (and which firmware you’re on).</p>
<p>Please understand that in its current stage, this will only crash your console, and it’s not particularly obvious to set up: you’ll need some basic scripting skills. If you can’t follow the instructions on how to edit the script and run it, I’d strongly suggest you don’t sweat it.</p>
<ol>
<li>Get the Poc here: <a href="https://gist.github.com/RobbedColek/8b0bc9655adc6ad9564ad71453188e37">https://gist.github.com/RobbedColek/8b0bc9655adc6ad9564ad71453188e37</a>
<ul>
<li>Requires Python 3 and Scapy.</li>
</ul>
</li>
<li>Connect your PS4/PS5 directly to PC via LAN cable.</li>
<li>Set up your PS4/PS5 for manual ethernet connection via PPPoE – insert any username/password.</li>
<li>Update the “source”, “destination”, and “interface” details in the script, according to your home PC and target console. &lt;– <strong>If you don’t know how to do that and can’t figure it out, stop right there, this thing isn’t for you and you will not be helpful</strong></li>
<li>Use “Test Internet Connection” on console.</li>
<li>Launch script.</li>
</ol><!-- ezoic_video_placeholder-longest_content-854x480-999993-clearholder --><!-- ezoic_video_placeholder-longest_content-854x480-999993-nonexxxclearxxxblock --><!-- ezoic_video_placeholder-longest_content-640x360-999993-clearholder --><!-- ezoic_video_placeholder-longest_content-640x360-999993-nonexxxclearxxxblock --><!-- ezoic_video_placeholder-longest_content-426x240-999993-clearholder --><!-- ezoic_video_placeholder-longest_content-426x240-999993-nonexxxclearxxxblock --><!-- ezoic_video_placeholder-longest_content-384x216-999993-clearholder --><!-- ezoic_video_placeholder-longest_content-384x216-999993-nonexxxclearxxxblock -->
<p dir="auto">If it worked, it will crash your console.</p>
<h2 dir="auto">Reference</h2>
<ul>
<li>Original claim: https://wololo.net/2024/01/26/ps5-update-8-60-is-here-and-guess-what-dont-update/comment-page-1/#comment-6543161</li>
<li>PoC for PS4/PS5: https://gist.github.com/RobbedColek/8b0bc9655adc6ad9564ad71453188e37</li>
<li>CVE: https://www.cvedetails.com/cve/CVE-2006-4304/</li>
<li>OpenBSD patch to fix the 2006 vulnerability: https://ftp.openbsd.org/pub/OpenBSD/patches/3.8/common/014_sppp.patch</li>
</ul>
<p><em>As always I’ve tried to be accurate and precise in this article. If you notice something is missing or wrong, please let me know in the comments. Thanks</em></p>



													</div><!--/.post-inner-->	
			</article><div>
				<p><img alt="" src="https://secure.gravatar.com/avatar/2701e5bfb18f0a34a907b73c452a5c42?s=128&amp;d=wavatar&amp;r=g" srcset="https://secure.gravatar.com/avatar/2701e5bfb18f0a34a907b73c452a5c42?s=256&amp;d=wavatar&amp;r=g 2x" height="128" width="128" loading="lazy" decoding="async"></p>
				<p>wololo</p>
				<p>We are constantly looking for guest bloggers at wololo.net. If you like to write, and have a strong interest in the console hacking scene, contact me either with a comment here, or in a PM on /talk!</p>
				
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The VAE Used for Stable Diffusion Is Flawed (205 pts)]]></title>
            <link>https://old.reddit.com/r/StableDiffusion/comments/1ag5h5s/the_vae_used_for_stable_diffusion_1x2x_and_other/</link>
            <guid>39215242</guid>
            <pubDate>Thu, 01 Feb 2024 12:25:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/StableDiffusion/comments/1ag5h5s/the_vae_used_for_stable_diffusion_1x2x_and_other/">https://old.reddit.com/r/StableDiffusion/comments/1ag5h5s/the_vae_used_for_stable_diffusion_1x2x_and_other/</a>, See on <a href="https://news.ycombinator.com/item?id=39215242">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h5><strong>Short summary for those who are technically inclined:</strong></h5>

<p>CompVis fucked up the KL divergence loss on the KL-F8 VAE that is used by SD1.x, SD2.x, SVD, DALL-E 3, and probably other models.  As a result, the latent space created by it has a massive KL divergence and is smuggling global information about the image through a few pixels.  If you are thinking of using it for training a new, trained-from-scratch foundation model, <strong>don't!</strong> (for the less technically inclined this <strong>does not</strong> mean switch out your VAE for your LoRAs or finetunes, you absolutely do not have the compute power to change the model to a whole new latent space, that would require effectively a full retrain's worth of training.)  SDXL is not subject to this issue because it has its own VAE, which as far as I can tell is trained correctly and does not exhibit the same issues.</p>

<h3>What is the VAE?</h3>

<p>A Variational Autoencoder, in the context of a latent diffusion model, is the eyes and the paintbrush of the model. It translates regular pixel-space images into latent images that are constructed to encode as much of the information about those images as possible into a form that is smaller and easier for the diffusion model to process.</p>

<p>Ideally, we want this "latent space" (as an alternative to <em>pixel space</em>) to be robust to noise (since we're using it with a <em>denoising</em> model), we want latent pixels to be very spatially related to the RGB pixels they represent, and most importantly of all, we want the model to be able to (mostly) accurately reconstruct the image from the latent.  Because of the first requirement, the VAE's encoder doesn't output just a tensor, it outputs a probability distribution that we then sample, and training with samples from this distribution helps the model to be less fragile if we get things a little bit wrong with operations on latents.  For the second requirement, we use Kullback-Leibler (KL) divergence as part of our loss objective: when training the model, we try to push it towards a point where the KL divergence between the latents and a standard Gaussian distribution is minimal -- this effectively ensures that the model's distribution trends toward being roughly equally certain about what each individual pixel should be.  For the third, we simply decode the latent and use any standard reconstruction loss function (LDM used LPIPS and L1 for this VAE).</p>

<h3>What is going on with KL-F8?</h3>

<p>First, I have to show you what a <em>good</em> latent space looks like.  Consider this image: <a href="https://i.imgur.com/DoYf4Ym.jpeg">https://i.imgur.com/DoYf4Ym.jpeg</a></p>

<p>Now, let's encode it using the SDXL encoder (after downscaling the image to shortest side 512) and look at the <em>log variance</em> of the latent distribution (please ignore the plot titles, I was testing something else when I discovered this): <a href="https://i.imgur.com/Dh80Zvr.png">https://i.imgur.com/Dh80Zvr.png</a></p>

<p>Notice how there are some lines, but overall the log variance is fairly consistent throughout the latent.  Let's see how the KL-F8 encoder handles this: <a href="https://i.imgur.com/pLn4Tpv.png">https://i.imgur.com/pLn4Tpv.png</a></p>

<p>This obviously looks very different in many ways, but the most important part right now is that black dot (hereafter referred to as the "black hole").  It's not a brain tumor, though it does look like one, and might as well be the machine-learning equivalent of one.  It's a spot where the VAE is trying to smuggle <strong>global information about the image</strong> through latent space.  This is exactly the problem that KL-divergence loss is supposed to prevent.  Somehow, it didn't.  I suspect this is due to underweighting of the KL loss term.</p>

<h3>What are the implications?</h3>

<p>Somewhat subtle, but significant.  Any latent diffusion model using this encoder is having to do a lot of extra work to get around the bad latent space.</p>

<p>The easiest one to demonstrate, is that the latent space is very fragile in the area of the black hole: <a href="https://i.imgur.com/8DSJYPP.png">https://i.imgur.com/8DSJYPP.png</a></p>

<p>In this image, I overwrote the mean of the latent distribution with random noise in a 3x3 area centered on the black hole, and then decoded it.  I then did the same on another 3x3 area as a control and decoded it.  The right side images are the difference between the altered and unaltered images.  <strong>Altering the latents at the black hole region makes changes across the whole image.</strong>  Altering latents anywhere else causes strictly local changes.  What we would want is strictly local changes.</p>

<p>The most substantial implication of this, is that these are the rules that the Stable Diffusion or other denoiser model has to play by, because this is the latent space it is aligned to.  So, of course, it learns to construct latents that smuggle information: <a href="https://i.imgur.com/WJsWG78.png">https://i.imgur.com/WJsWG78.png</a></p>

<p>This image was constructed by measuring the mean absolute error between the reconstruction of an unaltered latent and one where a <strong>single</strong> latent pixel was zeroed out.  Bright regions are ones where it is smuggling information.</p>

<p>This presents a number of huge issues for a denoiser model, because these latent pixels have a huge impact on the whole image and yet are treated as equal.  The model also has to spend a ton of its parameter space on managing this.</p>

<p>You can reproduce the effects on Stable Diffusion yourself using this code:</p>

<pre><code>import torch
from diffusers import StableDiffusionPipeline
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm
from copy import deepcopy

pipe = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, safety_checker=None).to("cuda")
pipe.vae.requires_grad_(False)
pipe.unet.requires_grad_(False)
pipe.text_encoder.requires_grad_(False)

def decode_latent(latent):
    image = pipe.vae.decode(latent / pipe.vae.config.scaling_factor, return_dict=False)
    image = pipe.image_processor.postprocess(image[0], output_type="np", do_denormalize=[True] * image[0].shape[0])
    return image[0]

prompt = "a photo of an astronaut riding a horse on mars"

latent = pipe(prompt, output_type="latent").images

original_image = decode_latent(latent)

plt.imshow(original_image)
plt.show()

divergence = np.zeros((64, 64))
for i in tqdm(range(64)):
    for j in range(64):
        latent_pert = deepcopy(latent)
        latent_pert[:, :, i, j] = 0
        md = np.mean(np.abs(original_image - decode_latent(latent_pert)))
        divergence[i, j] = md

plt.imshow(divergence)
plt.show()
</code></pre>

<h3>What is the prognosis?</h3>

<p>Still investigating this!  But I wanted to disclose this sooner rather than later, because I am confident in my findings and what they represent.</p>

<p>SD 1.x, SD 2.x, SVD, and DALL-E 3 (kek) and probably other models are likely affected by this.  You can't just switch them over to another VAE like SDXL's VAE without what might as well be a full retrain.</p>

<p>Let me be clear on this before going any further:  <strong>These models demonstrably work fine.  If it works, it works, and they work.</strong>  This is more of a discussion of the limits and if/when it is worth jumping ship to another model architecture.  I love model necromancy though, so let's talk about salvaging them.</p>

<p>Firstly though, if you are thinking of making a new, trained-from-scratch foundation model with the KL-F8 encoder, <strong>don't!</strong>  Probably tens of millions of dollars of compute have already gone towards models using this flawed encoder, don't add to that number!  At the very least, resume training on it and crank up that KL divergence loss term until the model behaves!  Better yet, do what Stability did and train a new one on a dataset that is better than OpenImages.</p>

<p>I think there is a good chance that the VAE could be fixed without altering the overall latent space too much, which would allow salvaging existing models.  Recall my comparison in that second to last image: even though the VAE was smuggling global features, the reconstruction still looked mostly fine without the smuggled features.  Training a VAE encoder would normally be an extremely bad idea if your expectation is to use the VAE on existing models aligned to it, because you'll be changing the latent space and the model will not be aligned to it anymore.  But if deleting the black hole doesn't destroy the image (which is the case here), it may very well be possible to tune the VAE to no longer smuggle global features while keeping the latent space at least similar enough to where existing models can be made compatible with it with at most a significantly shorter finetune than would normally be needed.  It may also be the case that you can already define a latent image within the decoder's space that is a close reconstruction of a given original without the smuggled features, which would make this task significantly easier.  Personally, I'm not ready to give up on SD1.5 until I have tried this and conclusively failed, because frankly rebuilding all existing tooling would suck, and model necromancy is fun, so I vote model necromancy!  This all needs actual testing though.</p>

<p>I suspect it may be possible to mitigate some of the effects of this within SD's training regimen by somehow scaling reconstruction loss on the latent image by the log variance of the latent.  The black hole is very well defined by the log variance: the VAE is <em>very certain</em> about what those pixels should be compared to other pixels and they accordingly have much more influence on the image that is reconstructed.  If we take the log variance as a proxy for the impact a given pixel has on the model, maybe you can better align the training objective of the denoiser model with the actual impact on latent reconstruction.  This is purely theoretical and needs to be tested first.  Maybe don't do this until I get a chance to try to fix the VAE, because that would just be further committing the model to the existing shitty latent space.</p>

<p>Failing this, people should recognize the limits of SD1.x and move to a new architecture.  It's over a year old, and this field moves fast.  Preferably one that still doesn't require a 3090 to run, please, I have one but not everyone does and what made SD1.5 so well supported was the fact that it could be run and trained on a much broader variety of hardware (being able to train a model in a decent amount of time with less than an A100-80GB would also be great too).  There are a lot of exciting new architectural changes proposed lately with things like Hourglass Diffusion Transformers and the new Karras paper from December to where a much, much better model with a similar compute footprint is certainly possible.  And we knew that SD1.5 would be fully obsolete one day.</p>

<p>I would like to thank my friends who helped me recognize and analyze this problem, and I would also like to thank the Glaze Team, because I accidentally discovered this while analyzing latent images perturbed by Nightshade and wouldn't have found it without them, because I guess nobody else ever had a reason to inspect the log variance of the latent distributions created by the VAE.  I'm definitely going to be performing more validation on models I try to use in my projects from now on after this, Jesus fucking Christ.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: filippo.io/mlkem768 – Post-Quantum Cryptography for the Go Ecosystem (222 pts)]]></title>
            <link>https://words.filippo.io/dispatches/mlkem768/</link>
            <guid>39214743</guid>
            <pubDate>Thu, 01 Feb 2024 11:10:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://words.filippo.io/dispatches/mlkem768/">https://words.filippo.io/dispatches/mlkem768/</a>, See on <a href="https://news.ycombinator.com/item?id=39214743">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
            <p><a href="https://pkg.go.dev/filippo.io/mlkem768?ref=words.filippo.io">filippo.io/mlkem768</a> is a pure-Go implementation of ML-KEM-768 optimized for correctness and readability. ML-KEM (formerly known as Kyber, renamed because we can’t have nice things) is a <a href="https://en.wikipedia.org/wiki/Post-quantum_cryptography?ref=words.filippo.io">post-quantum</a> key exchange mechanism in the process of being standardized by NIST and adopted by most of the industry.</p>
<p>The package amounts to <a href="https://github.com/FiloSottile/mlkem768/blob/main/mlkem768.go?ref=words.filippo.io">~500 lines of code</a>, plus 200 lines of comments, and 650 lines of tests. It has no dependencies except for golang.org/x/crypto/sha3. It’s meant for <a href="https://go.dev/cl/550215?ref=words.filippo.io">upstreaming</a> into the Go standard library (initially as an internal-only package used in an opt-in crypto/tls experiment) and was designed to provide high security assurance through ease of review, simplicity, and thorough testing.</p>
<p>I livecoded part of its development <a href="https://twitch.tv/filosottile?ref=words.filippo.io">on Twitch</a>, and you can watch <a href="https://www.youtube.com/watch?v=MyB7A93C-V0&amp;ref=words.filippo.io">the replay on YouTube</a>.</p>
<p>Unlike most other implementations, this code was not ported from the reference pq-crystals library, but written from scratch not having ever closely read other codebases. This was an intentional exercise in spec validation, to show it is possible to produce an interoperable implementation from the specification alone.</p>
<p>The <a href="https://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.203.ipd.pdf?ref=words.filippo.io">FIPS 203 document</a> turned out to be an excellent implementation guide, with detailed pseudo-code, exhaustive definitions, and consistent type information. (This is something I would like to ask of any large specification document: define your types and use them and denote them!) To make the code both easier to review and better as a learning resource, function and variable names, and even operation ordering, are carefully picked to mirror the FIPS specification.</p>
<p>The specification actually requires fairly limited math background, but to facilitate the work of implementers, I wrote up <a href="https://words.filippo.io/dispatches/kyber-math/">Enough Polynomials and Linear Algebra to Implement Kyber</a>.</p>
<p>Beyond that, the only parts left as an exercise to the reader were</p>
<ol>
<li>implementing arithmetic modulo the prime 3329;</li>
<li>concretely implementing the compress and decompress functions mapping values [0, 3329) to and from [0, 2ᵈ); and</li>
<li>ensuring constant time operations.</li>
</ol>
<p>Modulo arithmetic was reasonably easy, as we all collectively learned a lot about finite field arithmetic through years of RSA and elliptic curve implementations. The small prime actually makes the task feel unnaturally simple.</p>
<p><a href="https://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.203.ipd.pdf?ref=words.filippo.io#equation.4.5">Compression and decompression</a> turned out to be the most difficult part of the project. The specification defines them in abstract terms as fractions and rounding rules—“just” compute (2ᵈ/q)·x or (q/2ᵈ)·y and round to the closest integer—but in practice we need to implement them with constant time arithmetic and bitwise operations! In my <a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/n8dsN_aMsa8/m/NHO2lNyWAAAJ?ref=words.filippo.io">public comments</a> I pointed out that having each implementation figure out a strategy is risky and redundant. I was more correct than I thought: it turned out that <a href="https://github.com/pq-crystals/kyber/commit/dda29cc63af721981ee2c831cf00822e69be3220?ref=words.filippo.io">the reference implementation and ~every implementation ported from it used a division</a> which depending on compiler optimizations and platform might result in a DIV instruction, which is variable-time even when the divisor is fixed. This package was unaffected, because it used <a href="https://www.nayuki.io/page/barrett-reduction-algorithm?ref=words.filippo.io">Barrett reduction</a> from the start, like BoringSSL.</p>
<p>You can read <a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/n8dsN_aMsa8/m/NHO2lNyWAAAJ?ref=words.filippo.io">the rest of my formal public comments</a> on the pqc-forum mailing list.</p>
<p>Readability was a major goal of the implementation, and it was pursued <s>even</s> especially for complex functions like compression and decompression. A readable implementation has two purposes: first, it allows effective review, both during the code review process and later by interested researchers, improving security; second, it serves as an educational resource for the next generation of maintainers and cryptography engineers (or curious nerds). Reading the Go cryptography standard library is how I got started on the path that led me here, so it is especially important to me to preserve and improve it as a learning resource. It’s obviously subjective, but I believe this to be the most understandable public ML-KEM/Kyber implementation. Compare for example <a href="https://github.com/FiloSottile/mlkem768/blob/344d5ee2c575ca84613bbb119c1d8b1ef6699ea5/mlkem768.go?ref=words.filippo.io#L394-L442">our compression/decompression functions</a> with <a href="https://github.com/pq-crystals/kyber/blob/11d00ff1f20cfca1f72d819e5a45165c1e0a2816/ref/poly.c?ref=words.filippo.io#L9-L112">the reference implementation</a>.</p>
<p>Sometimes improving readability and reviewability means making code longer and less reusable: for example for ML-KEM-768 we need to serialize 1-, 4-, 10-, and 12-bit integers in a packed format. A universal 1-to-12 bit encoder and decoder is a pretty gnarly piece of code to write correctly, but each of those four sizes are actually pretty easy to write a dedicated encoder/decoder for.<sup><a href="#fn1" id="fnref1">[1]</a></sup> This is why we have <code>ringCompressAndEncode1/4/10</code> etc. instead of a single universal function. This also made it easy to work some special required checks into the 12-bit decoder.</p>
<p>This, by the way, was only possible because we targeted ML-KEM-768 specifically, or we’d have had to implement 5- and 11-bit encodings, as well. ML-KEM is specified at three security levels (-512, -768, and -1024). However, the Kyber team recommends using -768 over -512 for a more conservative security margin against novel cryptanalysis, while -1024 exists only for the same reasons 256-bit security levels exist: compliance and blind <a href="https://www.imperialviolet.org/2014/05/25/strengthmatching.html?ref=words.filippo.io">strength matching</a>. Most protocols being tested or standardized coalesced around ML-KEM-768, so targeting only that improves not only readability, but also security (because there are fewer moving parts), and performance (because we can optimize allocation sizes, iteration counts, and encoding algorithms) at little to no cost.</p>
<p>After readability, testing is the main component in this package’s high security assurance strategy. Besides checking that key generation, encapsulation, and decapsulation round-trip correctly, and maintaining a test coverage of 95%+, we</p>
<ul>
<li>ensure interoperability with <a href="https://github.com/FiloSottile/mlkem768/blob/344d5ee2c575ca84613bbb119c1d8b1ef6699ea5/testdata/vectors.json?ref=words.filippo.io">test vectors obtained from NIST and other implementations</a>;</li>
<li>exhaustively test every input combination for base field arithmetic operations (addition, subtraction, and multiplication modulo 3329) against expected values computed trivially with variable-time operations;</li>
<li>exhaustively test compression and decompression against math/big.Rat (contributed by David Buchanan);</li>
<li>test that pre-computed constants match their definition;</li>
<li>check that incorrect lengths (both long and short) cause the appropriate error for every input of every function;</li>
<li>run an extensive set of reusable test vectors we developed (see below);</li>
<li>run test vectors <a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/aCAX-2QrUFw/m/hy5gwcESAAAJ?ref=words.filippo.io">provided by Sophie Schmieg</a> which will be eventually included in <a href="https://github.com/google/wycheproof?ref=words.filippo.io">Wycheproof</a>.</li>
</ul>
<p>Our test vectors are designed to be reusable by other implementations, and <a href="https://c2sp.org/CCTV/ML-KEM?ref=words.filippo.io">are published as part of the CCTV project</a> along with detailed <a href="https://github.com/C2SP/CCTV/blob/main/ML-KEM/README.md?ref=words.filippo.io#intermediate-values">intermediate values</a> for testing and debugging each intermediate step and partial algorithm, which we used during development. There are different sets of tests vectors, each designed to reach different edge cases.</p>
<ul>
<li>
<p><a href="https://github.com/C2SP/CCTV/blob/main/ML-KEM/README.md?ref=words.filippo.io#bad-encapsulation-keys">Negative test vectors</a> provide invalid encapsulation keys, where the coefficients are higher than 3329. These were often requested, since all the test vectors from the Kyber and NIST teams are for regular, correct inputs. These vectors individually test every value from 3329 to 2¹²-1 and every coefficient location, sharing the remaining coefficients so they compress from 1–3 MiB down to 12–28 KiB.</p>
</li>
<li>
<p><a href="https://github.com/C2SP/CCTV/blob/main/ML-KEM/README.md?ref=words.filippo.io#unlucky-ntt-sampling-vector">“Unlucky” vectors</a> require an unusually large number of XOF reads. Kyber samples a matrix from a portion of public keys<sup><a href="#fn2" id="fnref2">[2]</a></sup> with <em>rejection sampling</em>: it gets a random value between 0 and 2 ¹²-1 and checks if it’s less than 3329, if not, it tries again. The amount of bytes needed to sample a matrix depends on how lucky you get with the sampling, and that’s a random function of the public key component. These vectors are regular public keys and require reading more than 575 bytes from the SHAKE-128 XOF in SampleNTT, which would ordinarily happen with probability 2⁻³⁸. Sophie’s vectors were bruteforced further, and require up to 591 bytes.</p>
<blockquote>
<p>At this point I would like to thank our detection and response team for not killing my job(s) hashing vast amounts of random seeds and looking for zeroes in the output. — Sophie Schmieg</p>
</blockquote>
</li>
<li>
<p>Special vectors <a href="https://github.com/C2SP/CCTV/blob/main/ML-KEM/README.md?ref=words.filippo.io#strcmp-vectors">fail if strcmp is used</a> in ML-KEM.Decaps. In ML-KEM.Decaps the ciphertext is compared with the output of K-PKE.Encrypt for implicit rejection. If an implementation were to use <code>strcmp()</code> for that comparison it would fail to reject some ciphertexts if a zero byte terminates the comparison early. This one I hope is going to sit as a silent trap for years—who would use <code>strcmp()</code> in cryptographic code—and then ruthlessly kill a vulnerability, because of course someone will.</p>
</li>
<li>
<p><a href="https://github.com/C2SP/CCTV/blob/main/ML-KEM/README.md?ref=words.filippo.io#accumulated-pq-crystals-vectors">Accumulated vectors</a> (derived from the reference pq-crystals implementation) allow testing randomly reachable edge cases without checking in large amounts of data. The reference implementation of Kyber includes a <code>test_vectors.c</code> program that generates 300MB of random vectors. I had no intention of checking in the output or compiling C, but since they are just randomly generated vectors, we can regenerate them in our tests from the deterministic RNG (SHAKE-128 with an empty input) and check they hash to an expected value. We can even take it further, and produce hashes for a million random tests, beyond the 10k they generate.</p>
</li>
</ul>
<p>I am happy to report that none of the tests, many introduced after completion of the implementation, identified any issues in filippo.io/mlkem768. There is <a href="https://github.com/C2SP/CCTV/issues/7?ref=words.filippo.io">at least one reported instance</a> of the negative vectors identifying a defect in a major implementation, though.</p>
<p>Performance is not a primary goal (neither of this package nor of <a href="https://go.dev/design/cryptography-principles?ref=words.filippo.io">the Go cryptography packages</a>) but the package needs to be fast enough to be useful. Thankfully, ML-KEM is pretty fast, to the point that this simple implementation is competitive with our assembly-optimized P-256 and X25519 implementations.</p>
<p>To compare apples to apples, note that we need to compare the whole operation that each side needs to perform for key establishment: for ECDH, two scalar multiplications (one of them by the fixed base point); for KEMs, key generation and decapsulation on one side, and encapsulation on the other. ECDH is symmetrical, ML-KEM key establishment is not.</p>
<p>The ECDH benchmarks below already include the two scalar multiplications, while the mlkem768 benchmarks are split as key generation and decapsulation under “Alice” and encapsulation under “Bob”. Since decapsulation includes a full encryption (to check the resulting ciphertext matches the input), Alice takes a lot longer than Bob: the latter does an encryption, while the former does an encryption, a decryption, and a key generation.</p>
<p>All in all, “Bob” is as fast as our X25519 or P-256, while “Alice” takes less than twice. Compared to some of the fastest ML-KEM implementations out there (BoringSSL and libcrux), this package takes approximately double the time. For such a simple and unoptimized implementation, this is more than satisfactory.</p>
<pre><code>goos: darwin
goarch: arm64
cpu: Intel(R) Core(TM) i5-7400 CPU @ 3.00GHz
pkg: crypto/ecdh
                         │   sec/op    │
ECDH/P256-8                49.43µ ± 0%
ECDH/X25519-8              77.46µ ± 0%

pkg: filippo.io/mlkem768
                         │   sec/op    │
RoundTrip/Alice-8          109.4µ ± 0%
RoundTrip/Bob-8            56.19µ ± 0%

goos: linux
goarch: amd64
pkg: crypto/ecdh
                         │   sec/op    │
ECDH/P256-4                78.88µ ± 1%
ECDH/X25519-4              115.6µ ± 2%

pkg: filippo.io/mlkem768
                         │   sec/op    │
RoundTrip/Alice-4          223.8µ ± 2%
RoundTrip/Bob-4            114.7µ ± 1%
</code></pre>
<p>The performance wasn’t entirely free. In general, I followed high-performance Go programming patterns, trying for example to minimize heap allocations. Next, I <a href="https://go.dev/cl/544817/2?ref=words.filippo.io">reworked the x/crypto/sha3 package</a> so it could be used without any heap allocation thanks to <a href="https://words.filippo.io/efficient-go-apis-with-the-inliner/">the mid-stack inlining trick</a>. However, I haven’t merged those changes yet and they are not included in the benchmarks above, because they have a negative effect on Apple M2 processors. No idea why yet.</p>
<pre><code>goos: darwin
goarch: arm64
pkg: filippo.io/mlkem768
                  │   sec/op    │   sec/op     vs base                │
RoundTrip/Alice-8   109.4µ ± 0%   121.3µ ± 1%  +10.91% (p=0.000 n=10)
RoundTrip/Bob-8     56.19µ ± 0%   59.94µ ± 2%   +6.66% (p=0.000 n=10)

goos: linux
goarch: amd64
                  │   sec/op    │   sec/op     vs base               │
RoundTrip/Alice-4   223.8µ ± 2%   218.6µ ± 1%  -2.32% (p=0.000 n=10)
RoundTrip/Bob-4     114.7µ ± 1%   109.5µ ± 0%  -4.57% (p=0.000 n=10)
</code></pre>
<p>The one successful optimization was complaining about the confusing result above on the Gophers Slack <code>#performance</code> channel, which sniped Josh Bleecher Snyder into contributing <a href="https://github.com/FiloSottile/mlkem768/pulls?q=is%3Apr+author%3Ajosharian&amp;ref=words.filippo.io">a couple changes</a> :)</p>
<p>There is some low hanging fruit still: key generation and decapsulation both sample a matrix from the same value, and since the two are usually done sequentially on the Alice side, the matrix could be stored saving around 10% time. There might be an opportunity to save a copy in the sha3 read path, too. After that, it’s a matter of optimizing the field implementation.</p>
<p>If you got this far, you might want to follow me on Bluesky at <a href="https://bsky.app/profile/filippo.abyssdomain.expert?ref=words.filippo.io">@filippo.abyssdomain.expert</a> or on Mastodon at <a href="https://abyssdomain.expert/@filippo?ref=words.filippo.io">@filippo@abyssdomain.expert</a>.</p>
<h2 id="bonus-track-using-a-ml-kem-implementation-as-kyber-v3">Bonus track: using a ML-KEM implementation as Kyber v3</h2>
<p>NIST made a few small changes to the Round 3 submission of Kyber. They are summarized in Section 1.3 of the FIPS draft.</p>
<p>However, there are a few experimental protocols defined in terms of Kyber v3 (or “draft00”), including the main deployed PQ TLS key exchange. Do we have to make a separate package to support them?</p>
<p>Luckily, no we don’t.</p>
<p>One change adds some validation for an edge case (non-canonical coefficient encodings in public keys) that was undefined in Kyber. Honest implementations will not produce such keys, so we can reject them as specified in the FIPS draft. It will make it possible to fingerprint our implementation as Kyber-on-ML-KEM but will be otherwise harmless.</p>
<p>One change removed a hashing step applied to CSPRNG input. Since those bytes are random, it’s impossible for any party to tell the difference.</p>
<p>The final change is the major one, and the trickiest. The ciphertext used to be hashed into the shared secret. This difference would prevent interoperability. However, the mixing happens as an additional key derivation, which was entirely removed in ML-KEM, which instead returns the value K as-is. This means we can run ML-KEM to generate the shared secret K and then apply</p>
<pre><code>SHAKE-256(K || c)[:32]
</code></pre>
<p>to generate the Kyber shared secret. No need to break the ML-KEM abstraction.</p>
<p>There’s one wrinkle: both Kyber and ML-KEM perform implicit rejection in Decapsulate by hashing a secret with the ciphertext and returning that as the shared secret. If we do the key derivation above on top of ML-KEM, we’ll hash the ciphertext twice for implicit rejections. That’s ok, because the output of implicit rejection is unpredictable by design, not an interoperation target.</p>
<h2 id="the-picture">The picture</h2>
<p>In Berlin there's an old closed airport, <a href="https://en.wikipedia.org/wiki/Berlin_Tempelhof_Airport?ref=words.filippo.io">Tempelhof</a>, which is now a public park. Walking down the taxiways (pictured) or along the centrelines of the 09L/27R and 09R/27L crossed-out runways is kinda unsettling, at least for me. ("Should I be speaking with Ground or Tower? Can I enter this runway?") Fun fact, in 2010 a single-engine plane forgot to switch fuel tank and did an emergency landing on 27L. Closed runways are the best bad places to land, after all.</p>
<p><img src="https://words.filippo.io/content/images/2024/01/news---1--2--1.jpeg" alt="A cement taxiway pictured at sunset, from the middle of the yellow centreline. The airport terminal is visible on the horizon, and a patch of grass on the left." loading="lazy"></p>
<p>This work was funded by a Google <a href="https://bughunters.google.com/about/rules/5891381450768384/open-source-security-subsidies-rules?ref=words.filippo.io">Open Source Security Subsidy</a> and by my awesome clients—<a href="https://www.sigsum.org/?ref=words.filippo.io">Sigsum</a>, <a href="https://www.latacora.com/?ref=words.filippo.io">Latacora</a>, <a href="https://interchain.io/?ref=words.filippo.io">Interchain</a>, <a href="https://smallstep.com/?ref=words.filippo.io">Smallstep</a>, <a href="https://www.avalabs.org/?ref=words.filippo.io">Ava Labs</a>, <a href="https://goteleport.com/?ref=words.filippo.io">Teleport</a>, and <a href="https://tailscale.com/?ref=words.filippo.io">Tailscale</a>—who, through our retainer contracts, get face time and unlimited access to advice on Go and cryptography.</p>
<p>Here are a few words from some of them!</p>
<p>Latacora — <a href="https://www.latacora.com/blog/2023/12/22/case-for-password-hashing/?ref=words.filippo.io">We wrote about password hashing with delegation</a>, a somewhat less known password hashing primitive. It's a PBKDF with a special property, that allows offloading hashing computation to a potentially untrusted server. In this blog post, we describe this primitive and discuss its applicability in the context of End-to-End Encrypted (E2EE) backup systems.</p>
<p>Teleport — For the past five years, attacks and compromises have been shifting from traditional malware and security breaches to identifying and compromising valid user accounts and credentials with social engineering, credential theft, or phishing. <a href="https://goteleport.com/identity-governance-security/?utm=filippo&amp;ref=words.filippo.io">Teleport Identity Governance &amp; Security</a> is designed to eliminate weak access patterns through access monitoring, minimize attack surface with access requests, and purge unused permissions via mandatory access reviews.</p>
<p>Ava Labs — We at <a href="https://www.avalabs.org/?ref=words.filippo.io">Ava Labs</a>, maintainer of <a href="https://github.com/ava-labs/avalanchego?ref=words.filippo.io">AvalancheGo</a> (the most widely used client for interacting with the <a href="https://www.avax.network/?ref=words.filippo.io">Avalanche Network</a>), believe the sustainable maintenance and development of open source cryptographic protocols is critical to the broad adoption of blockchain technology. We are proud to support this necessary and impactful work through our ongoing sponsorship of Filippo and his team.</p>
<hr>
<section>
<ol>
<li id="fn1"><p>The minimum common multiple of 1/4/10/12 with 8 is less than 64, so we can pack a few values in a uint64, and then serialize that. <a href="https://github.com/FiloSottile/mlkem768/blob/344d5ee2c575ca84613bbb119c1d8b1ef6699ea5/mlkem768.go?ref=words.filippo.io#L522-L627">The result</a> is IMHO pretty readable. <a href="#fnref1">↩︎</a></p>
</li>
<li id="fn2"><p>IIUC the matrix could have been hardcoded but is instead derived from a seed in the key instead to bypass debate on how the hardcoded matrix was generated, and any backdoor concerns. My somewhat spicy opinion is that we’ll come to see this as a mistake, and a case of <a href="https://words.filippo.io/dispatches/parameters/">failing to define parameters</a>. If the matrix was hardcoded ML-KEM would be faster and simpler. For example, there would be no need for these tests at all, and <a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/s-C-zIAeKfE/m/n9yqLja7AQAJ?ref=words.filippo.io">the matrix derivation typo in the spec draft</a> couldn’t have happened. Maybe some deployments can just specify and use a profile of ML-KEM that fixes the matrix seed. <a href="#fnref2">↩︎</a></p>
</li>
</ol>
</section>

        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Markov Chains Are the Original Language Models (115 pts)]]></title>
            <link>https://elijahpotter.dev/articles/markov_chains_are_the_original_language_models</link>
            <guid>39213410</guid>
            <pubDate>Thu, 01 Feb 2024 06:59:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://elijahpotter.dev/articles/markov_chains_are_the_original_language_models">https://elijahpotter.dev/articles/markov_chains_are_the_original_language_models</a>, See on <a href="https://news.ycombinator.com/item?id=39213410">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><img src="https://elijahpotter.dev/images/old_car.webp" alt="An old car interior"></p><blockquote>
<p><strong>Heads Up:</strong> This article is a republished (with some tweaks on spelling, grammar and layout) version of
an article I wrote in my senior year of high school for my Linear Algebra class.
As such, the publish date is not quite correct.</p>
</blockquote>
<h2>The AI Buzz is Boring Now</h2>
<p>I've come to the conclusion that there are four stages to the current AI hype cycle in an individual person's brain, at least as it pertains to large language models.
At the very least, these are the stages I went through.</p>
<h3>Stage One: Amazement</h3>
<p>"Wow! This is so cool! I can converse with a computer just like a real person!"</p>
<p>This is where all the science fiction fantasies come to fruition.
The possibilities seem endless.
We can all kick back and relax now, right?</p>
<h3>Stage Two: Frustration</h3>
<p>"Hmm... This isn't as effective as I originally thought."</p>
<p>It seems like the brand-new technology is really only applicable to the kinds of work no one wants to do anyway.
What it <strong>is</strong> able to do doesn't provide too much value to you.
It gets information and logic wrong often enough that it cannot be trusted for just about anything.</p>
<h3>Stage Three: Confusion</h3>
<p>After stage two, you start to forget about it.
But the hype is inescapable.
Your friends bring it up.
Your parents ask you about it when you go home for the holidays.
Even your dentist tries to extol its virtues.</p>
<p>Even if you moved on it, no one else did.
Could that mean that you were wrong?</p>
<h3>Stage Four: Boredom</h3>
<p>At this point the rate of new language models appearing has become faster than rate of new JavaScript frameworks (and just as annoying).
You want to go back to your roots and start from scratch.
You want the freedom of knowing the whole stack from start to finish.
You don't want any of the ineffective magic.</p>
<p>This is where I am right now.
Want to go back to my roots.
Some people work on old cars, even though they are less efficient.
At the same time though, they are more fun to work on than new cars.
I've decided to look into Markov chains.</p>
<h2>Markov Chains</h2>
<p>Below is a demonstration of my implementation of auto-completion using Markov Chains.</p>
<p>Though it is written in Rust and compiled to WebAssembly, it is not particularly efficient. To find out why, continue down the page to my detailed explanation of the implementation.</p>
<h2>Controls</h2>
<p>You may use either "Choose Word" or your right arrow key [→] to let the system choose the next word. Alternatively, you can tap any of the [Possible Next Words] to do so yourself.</p>

<h2>Explanation</h2>
<p>Markov chains, named after their inventor, Andrey Markov, are often used to model sequences of probabilistic events. That is, systems that cannot be modeled deterministically.</p>
<h2>Example</h2>
<p>Alice is at the grocery store. For every hour she is there, she has a 70% chance of leaving and going to the planetarium. Conversely, she has a 30% chance of staying.
If Alice is already at the planetarium, she has a 10% chance of leaving and going to the grocery store and a 90% chance of staying.
We can represent these probabilities as a table, where each column belongs to a start location, and each row belongs to a end location:</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Start at Grocery Store</td>
<td>Start at Planetarium</td>
</tr>
<tr>
<td>End at Grocery Store</td>
<td>30%</td>
<td>10%</td>
</tr>
<tr>
<td>End at Planetarium</td>
<td>70%</td>
<td>90%</td>
</tr>
</tbody>
</table>
<p>If we already know Alice's location for sure, we can simply perform table lookups to predict her most likely next move.
For example, we <em>know</em> she is at the grocery store right now. So by looking at row 2, column 1, we can be 70% confident she will be at the planetarium next hour.
However, this doesn't work if we aren't sure of her location, or we want to predict more than one hour in advance. How do we predict her next move if we aren't certain of her current location?
In the latter case, we might express her current location as another table.</p>
<table>
<thead>
<tr>
<th>Location</th>
<th>% Alice Present</th>
</tr>
</thead>
<tbody>
<tr>
<td>Grocery Store</td>
<td>25%</td>
</tr>
<tr>
<td>Planetarium</td>
<td>75%</td>
</tr>
</tbody>
</table>
<p>How do we estimate Alice's location in this new plane of possibility? In particular, how likely will Alice be at the Planetarium next hour?
Since there is a 25% probability Alice is at the grocery store, we multiply that with the probility of her transitioning to the Planetarium: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>25</mn><mi mathvariant="normal">%</mi><mo>∗</mo><mn>75</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">25\% * 75\%</annotation></semantics></math></span></span>. Next, we add the result with the probability of being at the Planetarium multiplied with the probability of her staying: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>75</mn><mi mathvariant="normal">%</mi><mo>∗</mo><mn>90</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">75\% * 90\%</annotation></semantics></math></span></span>.
In full, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>25</mn><mi mathvariant="normal">%</mi><mo>∗</mo><mn>75</mn><mi mathvariant="normal">%</mi><mo>+</mo><mn>75</mn><mi mathvariant="normal">%</mi><mo>∗</mo><mn>90</mn><mi mathvariant="normal">%</mi><mo>=</mo><mn>85</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">25\% * 75\% + 75\% * 90\% = 85\%</annotation></semantics></math></span></span>.
To see the probabilities as a table:</p>
<table>
<thead>
<tr>
<th>Next Location</th>
<th>Calculation</th>
<th>% Alice Present</th>
</tr>
</thead>
<tbody>
<tr>
<td>Grocery Store</td>
<td><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>25</mn><mi mathvariant="normal">%</mi><mo>∗</mo><mn>30</mn><mi mathvariant="normal">%</mi><mo>+</mo><mn>75</mn><mi mathvariant="normal">%</mi><mo>∗</mo><mn>10</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">25\% * 30\% + 75\% * 10\%</annotation></semantics></math></span></span></td>
<td>15%</td>
</tr>
<tr>
<td>Planetarium</td>
<td><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>25</mn><mi mathvariant="normal">%</mi><mo>∗</mo><mn>70</mn><mi mathvariant="normal">%</mi><mo>+</mo><mn>75</mn><mi mathvariant="normal">%</mi><mo>∗</mo><mn>90</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">25\% * 70\% + 75\% * 90\%</annotation></semantics></math></span></span></td>
<td>85%</td>
</tr>
</tbody>
</table>
<p>The keen-eyed among you may have noticed that these operations look a lot like matrix multiplication.
Instead of a table, we may represent these possible transitions as a matrix <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span></span>, and the Alice's current location as a vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>s</mi><mo>⃗</mo></mover></mrow><annotation encoding="application/x-tex">\vec{s}</annotation></semantics></math></span></span>.</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>T</mi><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0.3</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0.1</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0.7</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0.9</mn></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">T = \begin{bmatrix}
  0.3 &amp; 0.1 \\
  0.7 &amp; 0.9
\end{bmatrix}</annotation></semantics></math></span></span></span>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mover accent="true"><mi>s</mi><mo>⃗</mo></mover><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi mathvariant="normal">.</mi><mn>25</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi mathvariant="normal">.</mi><mn>75</mn></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\vec{s} = \begin{bmatrix}
  .25 \\
  .75 \\
\end{bmatrix}</annotation></semantics></math></span></span></span></p><blockquote>
<p><strong>Note:</strong> The location of each element remains the same as the table, even if we aren't explicitly labeling the rows and columns.</p>
</blockquote>
<p>Finding the next state matrix becomes as easy as multiplying the current location vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>s</mi><mo>⃗</mo></mover></mrow><annotation encoding="application/x-tex">\vec{s}</annotation></semantics></math></span></span> by <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span></span>. To find further hours in the future, we do it more than once. For example, to estimate three hours in the future: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mi>T</mi><mi>T</mi><mover accent="true"><mi>s</mi><mo>⃗</mo></mover></mrow><annotation encoding="application/x-tex">TTT\vec{s}</annotation></semantics></math></span></span>. We can condense this with an exponent: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>T</mi><mn>3</mn></msup><mover accent="true"><mi>s</mi><mo>⃗</mo></mover></mrow><annotation encoding="application/x-tex">T^3\vec{s}</annotation></semantics></math></span></span> or generalize it to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span></span> hours with: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>T</mi><mi>n</mi></msup><mover accent="true"><mi>s</mi><mo>⃗</mo></mover></mrow><annotation encoding="application/x-tex">T^n\vec{s}</annotation></semantics></math></span></span>.</p>
<h2>Application to Text-Completion</h2>
<p>The principles above can be applied to a variety of probabilistic situations. Most relavant to this particular webpage, is text completion.
We want to estimate the most likely next word to the user. Given the last word, what are the most likely next words? First, we need a dictionary.</p>
<h3>The Dictionary</h3>
<p>It is trivial to build a dictionary from sample text. For the purposes of the explanation, we are going to start with an arbitrary dictionary.</p>
<table>
<thead>
<tr>
<th>Index</th>
<th>Word</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>orange</td>
</tr>
<tr>
<td>1</td>
<td>fruit</td>
</tr>
<tr>
<td>2</td>
<td>passion</td>
</tr>
<tr>
<td>3</td>
<td>cheese</td>
</tr>
<tr>
<td>4</td>
<td>not</td>
</tr>
<tr>
<td>5</td>
<td>is</td>
</tr>
</tbody>
</table>
<h3>Building the Transition Matrix</h3>
<p>To build our transition matrix, we need to count all the transitions that occur between possible words in our dictionary.
In the interest of performance, my implementation converts the dictionary into a <code>HashMap&lt;String, usize&gt;</code>.
Next, I go through the training text and match each word to it's index in the dictionary, effectively transforming the <code>String</code> into a <code>Vec&lt;usize&gt;</code>.
For example, the phrase, "passion fruit is not orange, cheese is orange," becomes, <code>[ 2, 1, 5, 4, 0, 3, 5, 0 ]</code>.
Next, the implementation iterates through each element in this vector, counting each transition. The counts are stored in another <code>HashMap</code> in the interest of performance, but is eventually converted into a matrix <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span></span>. Each row is the output word's index, and the column is the input word's index.
For example, the transition <code>"fruit" (index 1) -&gt; "is" (index 5)</code> occurs exactly once, so we record <code>1</code> in column 1, row 5.</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>C</mi><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center center center center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">C = \begin{bmatrix} 
    0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 \\
    0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
    0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
    1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
    0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\
    0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0
\end{bmatrix}</annotation></semantics></math></span></span></span></p><p>Not a very interesting matrix, is it?</p>
<p>Each element needs to be converted into a probability. Take the sum of each column:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center center center center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>2</mn></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><annotation encoding="application/x-tex">\begin{bmatrix} 
    1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 2
\end{bmatrix}</annotation></semantics></math></span></span></span></p><p>Create a diagonal matrix <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span></span> composed of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mtext>column&nbsp;sum</mtext></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{\text{column sum}}</annotation></semantics></math></span></span></p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>C</mi><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center center center center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0.5</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0.5</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">C = \begin{bmatrix} 
    0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0.5 \\
    0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
    0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
    1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
    0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0.5 \\
    0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0
\end{bmatrix}</annotation></semantics></math></span></span></span></p><p>To finalize our Markov (a.k.a. transition) matrix <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span></span>, we simply perform:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>M</mi><mo>=</mo><mi>D</mi><mi>C</mi></mrow><annotation encoding="application/x-tex">M = DC</annotation></semantics></math></span></span></span></p><h3>Using the transition matrix</h3>
<p>There are two possible situations: the user is in the process of typing, or they have finished their last word.
The latter is the easiest to implement.
Scan the user's text, and isolate the last word. Perform a lookup on the word list to identify it's index. Create a new vector containing <code>0</code>s except for that index, which should contain a <code>1</code>.
For example, if the last word was 'is',</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mover accent="true"><mi>s</mi><mo>⃗</mo></mover><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center center center center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\vec{s} = \begin{bmatrix}
    0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1
\end{bmatrix}</annotation></semantics></math></span></span></span></p><p>Run it through our transition matrix:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>M</mi><mover accent="true"><mi>s</mi><mo>⃗</mo></mover><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center center center center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0.5</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0.5</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">M\vec{s} = \begin{bmatrix} 
  0.5 &amp; 0 &amp; 0 &amp; 0 &amp; 0.5 &amp; 0
\end{bmatrix}</annotation></semantics></math></span></span></span></p><p>Meaning the most probable next choices are at indices <code>0</code> and <code>4</code>, which correspond to "orange" and "not" respectively.
This is great for autocomplete. We can simply list the most probable options to the user.</p>
<h3>Text-Generation and Steady State</h3>
<p>It would be pretty neat if we could use this method to automagically generate text, right?</p>
<h4>The Naive Solution</h4>
<p>Each iteration, choose the most likely word from the set. Maybe randomize it a bit: choose a random word from the top 5 options.
Unfortunately, there is an issue. All Markov chains are guaranteed to converge on a specific probabilistic state given enough iterations. In order to get text generation to work unpredictably and without converging, we need something a bit more complex.</p>
<h4>My Solution</h4>
<p>Create a square diagonal matrix <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi></mrow><annotation encoding="application/x-tex">R</annotation></semantics></math></span></span> with a side length equal to the length of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>s</mi><mo>⃗</mo></mover></mrow><annotation encoding="application/x-tex">\vec{s}</annotation></semantics></math></span></span>. Fill the diagonal elements with random numbers between <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span></span>. Then choose the word whose index corresponds with the highest value of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mover accent="true"><mi>s</mi><mo>⃗</mo></mover></mrow><annotation encoding="application/x-tex">R\vec{s}</annotation></semantics></math></span></span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[EU chip goal 'unrealistic' says ASML CEO (110 pts)]]></title>
            <link>https://www.electronicsweekly.com/news/business/eu-chip-goal-totally-unrealistic-2024-01/</link>
            <guid>39213180</guid>
            <pubDate>Thu, 01 Feb 2024 06:11:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.electronicsweekly.com/news/business/eu-chip-goal-totally-unrealistic-2024-01/">https://www.electronicsweekly.com/news/business/eu-chip-goal-totally-unrealistic-2024-01/</a>, See on <a href="https://news.ycombinator.com/item?id=39213180">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					                                
					
<p>Investments in fab by TSMC, Bosch, NXP and Infineon are “good for the European car industry but it’s not enough. It’s not enough at all,” says Wennink, “if you want to get to 20%, you just have to calculate how much you need to build here.”</p>
<p><img decoding="async" loading="lazy" src="https://static.electronicsweekly.com/wp-content/uploads/sites/16/2014/08/Broadwell_14nm_Wafer_White_p-150x150.jpg" alt="EU chip goal ‘totally unrealistic’ says ASML CEO" width="150" height="150" srcset="https://static.electronicsweekly.com/wp-content/uploads/sites/16/2014/08/Broadwell_14nm_Wafer_White_p-150x150.jpg 150w, https://static.electronicsweekly.com/wp-content/uploads/sites/16/2014/08/Broadwell_14nm_Wafer_White_p-70x70.jpg 70w" sizes="(max-width: 150px) 100vw, 150px">In 2022, Kurt Sievers, CEO of NXP stated: “We have calculated that we would need €500 billion investment in Europe to reach the 20 percent market share goal.”</p>
<p>According to SEMI, China will have 8.6 million 8 inch equivalent wpm capacity this year, Taiwan will have 5.7 million wpm, Korea will have 5.1 million wpm, Japan will have 4.7 million wpm, the USA will have 3.1 million wpm, Europe will have 2.7 million wpm, and S.E.Asia will&nbsp;have 1.7 million wpm.</p>
<p>So Europe has roughly 8% of the industry measured by capacity and has roughly 8% of the market measured by value.</p>
<p>To up that to 20% in a world where capacity is currently growing at about 6% a year would need about a dozen new fabs to be built and in full volume production by 2030.</p>
                                    
										
									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Marlon Brando Was a Secret Tech Geek and Photoshop Ace (2015) (145 pts)]]></title>
            <link>https://www.yahoo.com/entertainment/marlon-brando-was-a-secret-tech-geek-and-photoshop-124061824672.html</link>
            <guid>39213150</guid>
            <pubDate>Thu, 01 Feb 2024 06:04:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.yahoo.com/entertainment/marlon-brando-was-a-secret-tech-geek-and-photoshop-124061824672.html">https://www.yahoo.com/entertainment/marlon-brando-was-a-secret-tech-geek-and-photoshop-124061824672.html</a>, See on <a href="https://news.ycombinator.com/item?id=39213150">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Marlon Brando: Actor, activist… special effects tech geek?<br></p><p>Though the late star was considered by many to be the greatest actor of the 20th century, Brando always had a love-hate relationship with his craft, often remarking that it was a <a href="http://www.vanityfair.com/news/2005/03/brando200503" rel="nofollow noopener" target="_blank" data-ylk="slk:silly profession for a grown adult;elm:context_link;itc:0;sec:content-canvas">silly profession for a grown adult</a>. He was so frustrated by acting that, in the late 1980s, he embraced a new technology that promised to keep him on the big screen while simultaneously lightening his workload.</p><p>In the new documentary <i>Listen to Me Marlon</i>, director Stevan Riley stitches together intimate home footage that the actor took over the years and combines it with rare interviews to portray Brando as a complicated and volatile soul. But the actor still had a spark for life before his death at age 80 in 2004, and he was fascinated with visual effects, as you can see in this exclusive clip above from the documentary, which you can watch above. The footage was originally intended for an experimental film called <i>Software</i>, which Brando tried to make in the 1990s — more on that later — and features Brando, then in his seventies, delivering a monologue from <i>Macbeth</i>, his face rendered in blue pixels. <br></p><p>The technology in the clip, known as “digital equivalence,” involves creating a photo-realistic digital double of an actor that can look, move, and sound just like the original. It had long been a holy grail of the computer-graphics industry, and when Brando found out about its potential, he partnered with Scott Billups, a veteran cinematographer and graphics pioneer, in the pursuit of perfecting the technology.</p><p>“He dug technology — he was always had the latest system,” Billups, who contributed the footage to the new doc, told Yahoo Movies. “He was always upgraded. He had some new Macs before I did — and I was a Mac developer.”</p><p>The two met in the mid 1970s, but didn’t begin working together in earnest until the late 1980s, when Billups was conducting digital experiments with an actor who lived near Brando’s Hollywood estate (Billups declined to identify the thespian by name). While that neighbor was unnerved by the idea of creating a digital double, Brando eagerly volunteered to be the next test-subject.</p><p>“The friend was freaked, and thought it would kill acting and kill us all and put us out of business, but the test models worked, and you knew who was [on screen]. So Marlon was like, ‘I’m next, I’m next,’” Billups said. They worked closely with Stephen and Lloyd Addleman, brothers who <a href="https://books.google.com/books?id=xlP1uoxU1nEC&amp;pg=PA14&amp;lpg=PA14&amp;dq=Stephen+and+Lloyd+Addleman+cyberware&amp;source=bl&amp;ots=gEmbqlYhVv&amp;sig=f10haQ9tsaT8n3H14OQnqaXdfUY&amp;hl=en&amp;sa=X&amp;ved=0CDMQ6AEwBGoVChMI0YHO2tnYxgIViowNCh3jmgSw#v=onepage&amp;q=Stephen%20and%20Lloyd%20Addleman%20cyberware&amp;f=false" rel="nofollow noopener" target="_blank" data-ylk="slk:invented an early version of the 3D scanner hardware;elm:context_link;itc:0;sec:content-canvas">invented an early version of the 3D scanner hardware</a>, which Billups likened to an advanced supermarket grocery scanner and a spinning chair.</p><p>“The concept was for [the actor] to sit in this chair and get really still, and the scanner would go around you in 360 degrees [in] just under a minute,” Billups explained. “Sometimes the scanner would freeze up, so we’d spin the chair. You put somebody like Marlon in a chair and try to get an even spin on him — well, he’s really not the kind of guy who likes to sit still for long.”</p><figure><div><p><img alt="image" src="https://s.yimg.com/ny/api/res/1.2/4o8McHDwlF0pyFFynGH2yA--/YXBwaWQ9aGlnaGxhbmRlcjt3PTk2MDtoPTcyMA--/http://40.media.tumblr.com/214f05b1313f97c892af69e1eacfa8bb/tumblr_inline_nrna90Gdb21t2ear7_540.jpg" data-src="https://s.yimg.com/ny/api/res/1.2/4o8McHDwlF0pyFFynGH2yA--/YXBwaWQ9aGlnaGxhbmRlcjt3PTk2MDtoPTcyMA--/http://40.media.tumblr.com/214f05b1313f97c892af69e1eacfa8bb/tumblr_inline_nrna90Gdb21t2ear7_540.jpg"></p></div></figure><p><i>Brando, David Addleson and Billups in the late ‘80s (Stephen Addleson) </i></p><p>Billups and Brando wound up doing four different tests of the technology throughout the late ‘80s and early ‘90s; one such test focused on replacing the actor in a film he probably shouldn’t have made anyway. “He was doing <i>Island of Doctor Moreau</i>,” Billups remembered, “and he said he could do half of that script digitally.”</p><p>That 1996 sci-fi movie, which co-starred Val Kilmer, was a notorious bomb, but the digital equivalence tests did lead to the duo working on an even more off-beat project: An adaptation of Rudy Rucker’s <a href="http://www.amazon.com/Software-Rudy-Rucker/dp/0380701774" rel="nofollow noopener" target="_blank" data-ylk="slk:Philip K. Dick Award-winning cyberpunk novel Software;elm:context_link;itc:0;sec:content-canvas">Philip K. Dick Award-winning cyberpunk novel <i>Software</i></a>, on which Billups and Brando began pre-production in 1997.</p><p>They set up shop at a company called House of Moves in Santa Monica, building a motion capture studio where they hosted Brando’s A-list friends, including Faye Dunaway. (Dennis Hopper was also attached to the film early on.) “We were scanning them and doing mo-cap — we were going to do this whole movie with digital equivalents,” Billups said. “You could do a movie with a week’s worth of work instead of sitting on set bored out of their minds for a couple months.”</p><p>Billups’ version of <i>Software</i> ultimately fell apart due to disagreements with producers and financiers; it was awfully expensive to attempt to make a film with such an ambitious and risky technology.</p><p>In the photo above, you can see Brando sitting in the lab. Below, we have scans of some of the early tests that Billups and Brando did together; the actor was required to speak — with exaggerated expression —every vowel and syllable, so that they could later generate a fully digital character that made the same exact facial movements. The result is found in the video at the top of this story.</p><figure><div><p><img alt="image" src="https://s.yimg.com/ny/api/res/1.2/deXKktli0lzDktsXE.kCGw--/YXBwaWQ9aGlnaGxhbmRlcjt3PTk2MDtoPTkyMQ--/http://40.media.tumblr.com/1a54a4f0aa630d8c7b90b9455b5cfe98/tumblr_inline_nrna90lH0f1t2ear7_540.jpg" data-src="https://s.yimg.com/ny/api/res/1.2/deXKktli0lzDktsXE.kCGw--/YXBwaWQ9aGlnaGxhbmRlcjt3PTk2MDtoPTkyMQ--/http://40.media.tumblr.com/1a54a4f0aa630d8c7b90b9455b5cfe98/tumblr_inline_nrna90lH0f1t2ear7_540.jpg"></p></div></figure><p><i>Some early scans of Brando’s face</i></p><p>The collapse of <i>Software</i> didn’t sink the actor’s intense interest in technology; as the new century dawned, he pursued not just high-grade experimental electronics, but consumer-level tech, as well.<br></p><p>“Marlon was a closet geek. Well, not even in the closet — he was big into it,” Billups explained. “He was an ace with Photoshop. He would take people’s pictures and put them in places they never were. He’d say, ‘Remember when we were in so-and-so?’ and you’d see people trying to remember that situation.”</p><p>The trick to successfully confusing his friends, Billups explained, was being subtle about it. “He would maneuver it so wasn’t totally unbelievable; he would create an alternate reality,” Billups remembered. “He’d say, ‘I’ve got a picture of it right here, you’re looking at a picture of you at the Statue of Liberty.’ He would do this to all kinds of people, it was hilarious.”</p><p>Brando invested heavily in his hardware, always going after the latest computer systems — often owning them before Billups, who still makes his living in the visual effects industry. The actor was even an early user of CoSa After Effects, the post-production program used for green screen and other effects work, which was eventually sold to Adobe.</p><p>Billups remembers Brando first talking about potentially using motion capture and digital equivalence to perform his role as Jor-El in the original <i>Superman</i> movie, which came out in 1978. It’s taken almost 40 years for effects artists and engineers to realize the technology he was pursuing; this summer, audiences will see aged-down versions of Arnold Schwarzenegger and Michael Douglas in <i>Terminator Genisys</i> and <i>Ant-Man</i>, respectively. The tech isn’t perfect yet, but the facial scan used in those two blockbusters is a tremendous leap forward from where it was when Brando died in 2004.</p><p>In general, Brando would have loved how far technology has come, and it’s easy to imagine the actor being enamored with social media. Brando was a fan of the <i>Myst</i> PC game franchise, and was active in early forms of social media.</p><p>“He loved the anonymity of the web and ability to interact with people outside of his skin,” Billups recalled. “His online persona was that of Chinese exchange student and he handled it masterfully. Not in any skeezy way, but he could pull things out of people that might not have realized of themselves. He was quite masterful at this.”<br></p><p>His email address was just as curious: Brando went by the name BrainFlakes.</p><p>“He was the cleverest son of a bitch you’d ever met,” Billups added. “He was a really sweet guy.”</p><p><i>Listen to Me Marlon</i> will be released at New York’s Film Forum on July 29, LA’s Landmark Theater on July 31, and rollout nationwide after that. It will also air on Showtime in the fall.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The curious case of the disappearing Polish Ś (2015) (105 pts)]]></title>
            <link>https://medium.engineering/the-curious-case-of-disappearing-polish-s-fa398313d4df</link>
            <guid>39212833</guid>
            <pubDate>Thu, 01 Feb 2024 04:53:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://medium.engineering/the-curious-case-of-disappearing-polish-s-fa398313d4df">https://medium.engineering/the-curious-case-of-disappearing-polish-s-fa398313d4df</a>, See on <a href="https://news.ycombinator.com/item?id=39212833">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><div><div><h2 id="3f03">One keyboard bug three decades in the making</h2><div><a href="https://mwichary.medium.com/?source=post_page-----fa398313d4df--------------------------------" rel="noopener follow"><div aria-hidden="false"><p><img alt="Marcin Wichary" src="https://miro.medium.com/v2/resize:fill:88:88/2*yemLwBKuCnBh79HADuLAGQ.jpeg" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a><a href="https://medium.engineering/?source=post_page-----fa398313d4df--------------------------------" rel="noopener  ugc nofollow"><div aria-hidden="false"><p><img alt="Medium Engineering" src="https://miro.medium.com/v2/resize:fill:48:48/1*Q-Bif1DHR_y6Z7xR2m1yKg.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto"></p></div></a></div></div><p id="111f">A few weeks ago, someone reported this to us at Medium:</p><blockquote><p id="b704">“I just started an article in Polish. I can type in every letter, except Ś. When I press the key for Ś, the letter just doesn’t appear. It only happens on Medium.”</p></blockquote><p id="0e14">This was odd. We don’t really special-case any language in any way, and even if we did… out of 32 Polish characters, why would <a href="http://en.wikipedia.org/wiki/%C5%9A" rel="noopener ugc nofollow" target="_blank">this random one</a> be the only one causing problems?</p><p id="8b8e">Turns out, it wasn’t so random. This is a story of how four incidental ingredients spanning decades (if not centuries) came together to cause the most curious of bugs, and how we fixed it.</p></div><div><div><h2 id="8871">Ingredient 1 of 4:<br>Polish language</h2><p id="522f">Polish is the second most-used Slavic language, right after Russian, and before Ukrainian. Contrary to those two, however, and similar to Western European languages such as German or French, Polish uses the English/Latin alphabet with a few customizations.</p><p id="0578">This is the base English alphabet, largely the same as a classic Latin/Roman alphabet:</p></div><div><p id="3710">Original Polish words never contain Q, V or X, although we keep them for Latin and other borrowed words:</p></div><div><p id="775a">In exchange for those three, however, Polish adds nine additional diacritics using Latin characters as their base, all in relatively common use:</p></div></div><div><div><p id="203f">Starting in the early 20th century, typewriters needed to accommodate the extra 9 letters. If you compare an American typewriter with a Polish one:</p></div><div><p id="0ac7">…and look at the right side of the keyboard, you can see two of the diacritics — Ł and Ż — promoted to separate keys, and the rest sharing keys with other symbols. (Typists were encouraged to assemble uppercase versions of seven remaining letters by typing a Latin character, backspacing, and then overwriting an accent to “simulate” the proper letter. This was not uncommon during typewriter times.)</p><p id="ecf2">To find room for the extra letters, typewriters needed to dispense with some punctuation, most notably semicolons (comma + backspace + colon), and parentheses (replaced in common use by slashes).</p></div></div><div><div><h2 id="046c">Ingredient 2 of 4:<br>Communism</h2><p id="3e4f">For someone interested in the early personal computing in the 1980s, Communism in Poland meant a few things:</p><ul><li id="ead6">not a lot of disposable income,</li><li id="ba67">forbidden commercial importing of computers from the West (individual importing was still possible, assuming you had enough foreign currency and some means of acquiring it).</li></ul><p id="c94b">I grew up in Poland. My first computer — the glorious Atari 800XL—was an original 1979 technology, repackaged in 1983. We got it in 1986. Used.</p><p id="d229">This wasn’t uncommon. Technology was delayed on that side of the Iron Curtain; most computers were imported from the West. Prohibited commercial importing meant that for the longest time there was no commercial entity that could prepare computers for use in Poland. Foreign computers arrived with original instructions, untranslated software, and American keyboards like this one:</p></div><div><p id="783f">While France, Germany, and other countries got their early PCs with customized keyboards whose layouts mirrored closely the typewriters that came before…</p></div><div><figure></figure><figure><figcaption>Early IBM PC keyboards with French AZERTY and German QWERTZ layouts</figcaption></figure></div><div><p id="14cb">…in Poland, we had to find another way of inputting the extra 9 diacritics unique to our language.</p><p id="7fcd">Our extra characters might look very much like Latin equivalents, and amount to only about 8% of letter distribution (you will <a href="http://en.wikipedia.org/wiki/Scrabble_letter_distributions#Polish" rel="noopener ugc nofollow" target="_blank">hate them playing Scrabble</a>), but they are important. You can’t just swap them around. Consider these two similar words:</p><figure></figure><p id="fff6">Perfectly interchangeable, right? Well, not quite:</p><figure></figure><p id="22ea">There are more examples like that. As it happens, in those early PC days, I was happy that my full name, Marcin Kazimierz Wichary, did not come with any diacritics… and used to make fun of some of my less “lucky” friends. I was a mean nerd.</p><p id="9bae">Surely, there’s something that can be done, though? Back to the keyboard:</p></div><div><p id="1818">We cannot modify it in any way since that’d require messing with hardware, but we can still try to find a clever solution. There are two modifier keys — Ctrl (where today’s Caps Lock is), and Alt. Ctrl was already used as a common shortcut key, even before Ctrl+C and Ctrl+V became typical for copy and paste. But Alt was relatively uncommon. And thus, a <em>de facto</em> standard was born, assigning each of our diacritics to its Latin counterpart:</p></div><div><p id="7516">People started calling the older layout <em>typist’s keyboard layout, </em>and the new invention <em>programmer’s layout, </em>either because early PC users were mostly programmers, or because it preserved all the punctuation symbols that were often used in programming.</p><p id="45dc">The new layout was an ergonomic nightmare — look at how many of those letters are very close to the Left Alt and need to be pressed using the same hand — but it was easy to understand and did not require any expensive hardware modifications or even cheap ones (for example stickers). It stuck. A few other nearby countries (Romania, then-Czechoslovakia) came up with similar schemes.</p><p id="faf7">The setup was so successful that even when, a decade later, proper typist’s keyboards started appearing, practically no one would switch to them, mirroring the ascendancy of the inferior QWERTY some 80 years before.</p></div><div><figure></figure><figure><figcaption>Contemporary typist’s layout vs. contemporary programmer’s layout (or, American keyboard)</figcaption></figure></div></div><div><h2 id="a94a">Ingredient 3 of 4:<br>Old habits dying hard</h2><p id="2add">Autosaving, common today, needed to wait for the right moment. Especially in the 1980s, and even 1990s, saving your document was lengthy (powering up that floppy drive and writing to the disk took some time), would slowly wear out whatever medium you were using, and sometimes occupy CPU so much it couldn’t be used for anything else.</p><p id="b182">Saving by hand was then what backing up is today: a habit you needed to learn for your own good. The unlucky ones figured it out the hard way, writing for hours on a computer that had a tendency to crash cruelly and often, only to realize <em>they never saved what you were typing.</em></p><p id="630d">I was one of them. And we all learned to press Command+S or Ctrl+S whenever we paused for breath. Sometimes it was after every chapter. Often, every paragraph. Later on, even a sentence. Ctrl+S became a keystroke buried in people’s muscle memory, a habit that was an intrinsic part of writing.</p><figure></figure></div><div><div><p id="65c1">Then that habit turned on them. If you write in any Web-based editor, the default thing that happens after pressing the save key combination, is a browser window — giving you a completely useless option to save the current website’s HTML code, often announcing itself <em>via</em> a slow, annoying animation.</p><p id="cac9">The same was true for Medium, a long time ago:</p><figure></figure><p id="b8e6">However, at some point we added a little bit of code to our editor to make the save dialog go away:</p></div><div><p id="cd7d">It translates to: if S happens to be pressed with Command (<em>metaKey,</em> used by Macintoshes) or Ctrl (used by Windows or Linux PCs), first: prompt our editor to save what it’s doing (even though we’d autosave it soon anyway), and second: prevent anything else that would normally happen… in this case the annoying browser save dialog.</p><p id="ab9b">Which is the right thing to do. (You can still get to the save dialog from the menu, if you want.)</p></div></div><div><p id="7ac5">Now, this should ring all sorts of alarms in your head if you remember the bug report mentioned at the beginning:</p><blockquote><p id="97bc">“I just started an article in Polish. I can type in every letter, except Ś. When I press the key for Ś, the letter just doesn’t appear. It only happens on Medium.”</p></blockquote><p id="a3e8">Something doesn’t compute, though. Medium is blocking Command+S and Ctrl+S, but you get to Ś by keying in… Alt+S.</p><p id="4791">For these two worlds to collide, we need just one more ingredient.</p></div><div><div><h2 id="ca46">Ingredient 4 of 4:<br>Microsoft Windows</h2><p id="a774">Both Windows 3.x and 95 had terrific keyboard support. The menu items and dialogs had controls that could be accessed easily by mouse… but also much quicker by pressing Alt and the underlined letter:</p><figure></figure><p id="8110">Most of Microsoft Windows UI could be turned into a sequence of keyboard shortcuts, which was incredibly powerful (and something Mac could still learn from). In Poland, however, Alt was already commonly used to enter the extra diacritics. That presented a conflict.</p><p id="2f52">Fortunately, by then, keyboards evolved to have extra modifier keys, including a secondary Alt key on the other side of the spacebar.</p></div><div><p id="9843">So, what if we kept the Left Alt key for Windows shortcuts, but used the Right Alt key for Polish diacritics? That would solve all the problems (although the short-lived victory of better ergonomics through two modifier keys would be nullified again… this time one-handed Right Alt+O, Right Alt+L, and Right Alt+N being RSI fodder).</p></div><div><p id="1263">For reasons having to do with the older keyboards without the Right Alt (and more modern notebook keyboards that might skip that key for space reasons), Right Alt in Windows was internally mapped as a rarely-used combination of <a href="http://blogs.msdn.com/b/oldnewthing/archive/2004/03/29/101121.aspx" rel="noopener ugc nofollow" target="_blank">Ctrl and Alt pressed together</a>.</p><p id="9337">Which meant that, from the system perspective, all of the above shortcuts looked like this:</p></div><div><p id="e455">And there’s something oddly uncomfortable with this picture. That Ctrl+Alt+S is starting to look very familiar now, doesn’t it? In fact, <em>incredibly </em>familiar to Ctrl+S that we once started blocking to improve saving.</p></div></div><div><h2 id="e9ca">The fix</h2><p id="ba7b">Putting all of this together, the reason for the bug becomes obvious:</p><ol><li id="948b">Through historical circumstance, people in Poland use Right Alt+S to type in Ś.</li><li id="c701">Because of an obscure technical decision, Right Alt+S is mapped internally to Ctrl+Alt+S.</li><li id="e29a">Through habit, people might occasionally want to press Ctrl+S to save a document.</li><li id="da41">Medium was overriding whatever default action happened when you pressed Ctrl+S, thinking it’s just a browser save dialog it was canceling… and never realizing it could be something else: <em>a letter Ś.</em></li></ol><p id="c739">And so, typing Ś became impossible.</p></div><div><div><p id="9108">Once I figured it out, the patch was trivial. Instead of blindly blocking Ctrl+S, we could block Ctrl+S <em>only if Alt key was not pressed.</em></p></div><div><p id="b501">We fixed this last week, and it wasn’t a lot of work. After seeing the Medium bug reports filed in, and having remembered once asking to change what happens when you press Ctrl+S, I figured it out without much trouble.</p><p id="d9a0">But I happen to be Polish, and I went through all of the above — learning to touch type on my mom’s typewriter, owning early PCs with American keyboards, rewiring my muscle memory for the Left Alt first, and the Right Alt second… and in the same muscle memory, putting Ctrl+S early on, and then replacing it with Command+S as I switched to Macs.</p><p id="cab3">Much has changed in Poland since the 1980s. In 1989, Communism fell. Computers are now imported legally by individuals and companies alike. Today’s Windows and Mac machines in Poland are bought new, fully support Polish, and the operating systems themselves are properly localized. If you open up system preferences, though, you will still see the programmer’s layout there, and it’s still the most commonly used of the two.</p><figure></figure></div></div><div><p id="746d">The few lines of code above might be, to me, one of the most curious in Medium’s codebase, the result of an arbitrary set of circumstances… today copiously commented so that people coming after me understand the one opaque conditional statement.</p><p id="63d7">As we see Medium being used in more and more countries, we will be dealing with those kind of thick, arbitrary internationalization and localization issues over and over again. With a big exception, though: no one will be there to give us a backstory, to unfold the opaque, to explain any of this to us the way I could explain the above.</p><p id="3a32">This bug is a harbinger of very scary, and very exciting times.</p></div><div><p id="e25a"><em>If you’ve read so far, maybe you want to help us out here at Medium? We’re looking for ambassadors in different countries — people knowing their language, caring about typography, wanting stories on Medium to look and behave most properly for where they are.</em></p><p id="5fb2"><em>Let us know at </em><a href="mailto:languages@medium.com" rel="noopener" target="_blank"><em>languages@medium.com</em></a><em>. Thank you!</em></p><p id="ec91"><em>Also, thanks to Michal Cierniak, Sommer Panage, Jamie Talbot, and Dan Pupius for reviewing this article.</em></p></div></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Lockbox: forward proxy for making third party API calls (119 pts)]]></title>
            <link>https://github.com/mkjt2/lockbox</link>
            <guid>39211857</guid>
            <pubDate>Thu, 01 Feb 2024 01:26:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/mkjt2/lockbox">https://github.com/mkjt2/lockbox</a>, See on <a href="https://news.ycombinator.com/item?id=39211857">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:mkjt2/lockbox" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="EOO0ylnXEFjsZr9K3F_V5PBjpz5xD8MVMbkriUntE_HlXJ8wB5woh5jR8FOpnl7EkTLZAL1PeqZ7lxYgR4l75w" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="mkjt2/lockbox" data-current-org="" data-current-owner="mkjt2" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=sgPRTMgz8ku95KMqhkt%2Bc5eH7iEFcXSQxOL162YxiBpQ1kQrDjn3vBito38AHpDzRdv%2BfTPg5%2BaBLmRF0Pcdx1ZJClGV%2F%2BuaF2H7f6%2F7zqTci5qJgiHlNZ%2Fa4kplETiyAkg4d4c6G5ryHzqxAuzhzVz4loJTZl89EmFrvg%2Bs%2BVBmZrCIxGqvmrfiBMhE40Xbde1B9tvTWvm%2BhGWJfR2itHPAB2NtRP2ml%2FXGDGDOXtfz0fHZgo%2BC%2Bs61usHR1tBxFahTfOdkSGvGYK0%2F2N0xWZM0GHAt6dicoIASJxGCrCRWhsaveA4vUZtdHxuqAuX6%2Bw%2Bp5JKh2VqJoErsWSESzW0BD9bCSmMgBP6fFF38Kz5DflH94796roj%2FcpGZDqLbwKbg1kTfIMhW9IGrXNtrhgBxm5LdCB8loKrQtHF6PERztP%2BfLO%2FLvccQDo16WNaRpzpzBy7zJDRAU2%2B7m3SyaT7Kk9eXiWO%2BXXcMAHd%2FGZBPZFER92%2FMjIkXTN%2BZcO9e%2Fuzq6mwJ62stPw%3D%3D--SRlzcLraTcY3Xf9d--UDI9lK0l9MZklwNbxKg2hw%3D%3D&quot; />">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=mkjt2%2Flockbox" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/mkjt2/lockbox&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="99513d7c9ca4e79466e8e86a7454ba13acb003d591c1d874fb5a07f69253f529" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Making a PDF that's larger than Germany (641 pts)]]></title>
            <link>https://alexwlchan.net/2024/big-pdf/</link>
            <guid>39210507</guid>
            <pubDate>Wed, 31 Jan 2024 22:47:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://alexwlchan.net/2024/big-pdf/">https://alexwlchan.net/2024/big-pdf/</a>, See on <a href="https://news.ycombinator.com/item?id=39210507">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main" tabindex="-1"> <article>   <p>I was browsing social media this morning, and I saw a claim I’ve seen go past a few times now – that there’s a maximum size for a PDF document:</p>  <p>Some version of this has been floating around the Internet <a href="https://twitter.com/hassmanm/status/303086542">since 2007</a>, probably earlier. This tweet is pretty emblematic of posts about this claim: it’s stated as pure fact, with no supporting evidence or explanation. We’re meant to just accept that a single PDF can only cover about half the area of Germany, and we’re not given any reason why 381&nbsp;kilometres is the magic limit.</p> <p>I started wondering: has anybody made a PDF this big? How hard would it be? Can you make a PDF that’s even bigger?</p> <p>A few years ago I did some <a href="https://codegolf.stackexchange.com/a/48938/13285">silly noodling into PostScript</a>, the precursor to PDF, and it was a lot of fun. I’ve never actually dived into the internals of PDF, and this seems like a good opportunity.</p> <p>Let’s dig in.</p> <h2 id="where-does-the-claim-come-from">Where does the claim come from?</h2> <p>These posts are often accompanied by a “well, actually” where people in the replies explain this is a limitation of a particular PDF reader app, not a limitation of PDF itself. They usually link to something like <a href="https://en.wikipedia.org/wiki/PDF#:~:text=Page%20dimensions%20are%20not%20limited%20by%20the%20format%20itself">the Wikipedia article for PDF</a>, which explains:</p> <blockquote> <p>Page dimensions are not limited by the format itself. However, Adobe Acrobat imposes a limit of 15&nbsp;million by 15&nbsp;million inches, or 225 trillion in<sup>2</sup> (145,161 km<sup>2</sup>).<a href="https://en.wikipedia.org/wiki/PDF#cite_note-pdf-ref-1.7-2"><sup>[2]</sup></a></p> </blockquote> <p>If you follow the reference link, you find the <a href="https://web.archive.org/web/20081001170454/https://www.adobe.com/devnet/acrobat/pdfs/pdf_reference_1-7.pdf">specification for PDF&nbsp;1.7</a>, where an appendix item explains in more detail (emphasis mine):</p> <blockquote> <p>In PDF versions earlier than PDF&nbsp;1.6, the size of the default user space unit is fixed at 1/72&nbsp;inch. In Acrobat viewers earlier than version 4.0, the minimum allowed page size is 72 by 72&nbsp;units in default user space (1 by 1&nbsp;inch); the maximum is 3240 by 3240&nbsp;units (45 by 45&nbsp;inches). In Acrobat versions 5.0 and later, the minimum allowed page size is 3 by 3&nbsp;units (approximately 0.04 by 0.04&nbsp;inch); the maximum is 14,400 by 14,400&nbsp;units (200 by 200&nbsp;inches).</p> <p>Beginning with PDF&nbsp;1.6, the size of the default user space unit may be set with the UserUnit entry of the page dictionary. <strong>Acrobat 7.0 supports a maximum UserUnit value of 75,000, which gives a maximum page dimension of 15,000,000&nbsp;inches (14,400 * 75,000 * 1 ⁄ 72).</strong> The minimum UserUnit value is 1.0 (the default).</p> </blockquote> <p>15&nbsp;million inches is exactly 381&nbsp;kilometres, matching the number in the original tweet. And although this limit first appeared in PDF&nbsp;1.6, it’s “version 7” of Adobe Acrobat. This is probably where the original claim comes from.</p> <p>What if we make a PDF that exceeds these “maximum” values?</p> <h2 id="the-inner-structure-of-pdfs">The inner structure of PDFs</h2> <p>I’ve never dived into the internals of a PDF document – I’ve occasionally glimpsed some bits in a hex editor, but I’ve never really understood how they work. If I’m going to be futzing around for fun, this is a good opportunity to learn how to edit the PDF directly, rather than going through a library.</p> <p>I found <a href="https://help.callassoftware.com/a/798383-how-to-create-a-simple-pdf-file">a good article</a> which explains the internal structure of a PDF, and combined with asking ChatGPT a few questions, I was able to get enough to write some simple files by hand.</p> <p>I know that PDFs support a huge number of features, so this is probably a gross oversimplification, but this is the mental picture I created:</p> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 280 560" role="img" aria-labelledby="svg_pdf_layout" style="width: 300px;"> <defs> <marker id="arrowhead" markerWidth="7" markerHeight="4.9" refX="0" refY="2.45" orient="auto"> <polygon points="0 0, 7 2.45, 0 4.9"></polygon> </marker> </defs> <path d="M 230 470 L 270 470 L 270 350 L 250 350" marker-end="url(#arrowhead)"></path> <rect x="50" y="10" width="180" height="40"></rect> <text x="140" y="30">%PDF-1.6</text> <rect x="50" y="70" width="180" height="240"></rect> <text x="140" y="85" text-anchor="middle" dominant-baseline="middle" font-size="10pt">objects</text> <rect x="80" y="100" width="120" height="40"></rect> <text x="140" y="120">object 1</text> <rect x="80" y="150" width="120" height="40"></rect> <text x="140" y="170">object 2</text> <path d="M 140 195 L 140 245"></path> <rect x="80" y="250" width="120" height="40"></rect> <text x="140" y="270">object N</text> <path d="M 50 350 L 10 350 L 10 270 L 62 270" marker-end="url(#arrowhead)"></path> <path d="M 50 350 L 10 350 L 10 220 L 62 220" marker-end="url(#arrowhead)"></path> <path d="M 50 350 L 10 350 L 10 170 L 62 170" marker-end="url(#arrowhead)"></path> <path d="M 50 350 L 10 350 L 10 120 L 62 120" marker-end="url(#arrowhead)"></path> <rect x="50" y="330" width="180" height="40"></rect> <text x="140" y="350">xref</text> <rect x="50" y="390" width="180" height="40"></rect> <text x="140" y="410">trailer</text> <rect x="50" y="450" width="180" height="40"></rect> <text x="140" y="470">startxref</text> <rect x="50" y="510" width="180" height="40"></rect> <text x="140" y="530">%%EOF</text> <title id="svg_pdf_layout">tbc</title></svg> <p>The start and end of a PDF file are always the same: a version number (<code>%PDF-1.6</code>) and an end-of-file marker (<code>%%EOF</code>).</p> <p>After the version number comes a long list of objects. There are lots of types of objects, for all the various things you can find in a PDF, including the pages, the text, and the graphics.</p> <p>After that list comes the <code>xref</code> or cross-reference table, which is a lookup table for the objects. It points to all the objects in the file: it tells you that object 1 is 10&nbsp;bytes after the start, object 2 is after 20&nbsp;bytes, object 3 is after 30&nbsp;bytes, and so on. By looking at this table, a PDF reading app knows how many objects there are in the file, and where to find them.</p> <p>The <code>trailer</code> contains some metadata about the overall document, like the number of pages and whether it’s encrypted.</p> <p>Finally, the <code>startxref</code> value is a pointer to the start of the <code>xref</code> table. This is where a PDF reading app starts: it works from the end of the file until it finds the <code>startxref</code> value, then it can go and read the <code>xref</code> table and learn about all the objects.</p> <p>With this knowledge, I was able to write my first PDF by hand. If you save this code into a file named <code>myexample.pdf</code>, it should open and show a page with a red square in a PDF reading app:</p><pre><code><span>%PDF-1.6

% The first object.  The start of every object is marked by:
%
%     &lt;object number&gt; &lt;generation number&gt; obj
%
% (The generation number is used for versioning, and is usually 0.)
%
% This is object 1, so it starts as `1 0 obj`.  The second object will
% start with `2 0 obj`, then `3 0 obj`, and so on.  The end of each object
% is marked by `endobj`.
%
% This is a "stream" object that draws a shape.  First I specify the
% length of the stream (54&nbsp;bytes).  Then I select a colour as an
% RGB value (`1 0 0 RG` = red), then I set a line width (`5 w`) and
% finally I give it a series of coordinates for drawing the square:
%
%     (100, 100) ----&gt; (200, 100)
%                          |
%     [s = start]          |
%         ^                |
%         |                |
%         |                v
%     (100, 200) &lt;---- (200, 200)
%</span>
1 0 obj
&lt;&lt;
	/Length 54
&gt;&gt;
stream
1 0 0 RG
5 w
100 100 m
200 100 l
200 200 l
100 200 l
s
endstream
endobj

<span>% The second object.
%
% This is a "Page" object that defines a single page.  It contains a
% single object: object 1, the red square.  This is the line `1 0 R`.
%
% The "R" means "Reference", and `1 0 R` is saying "look at object number 1
% with generation number 0" -- and object 1 is the red square.
%
% It also points to a "Pages" object that contains the information about
% all the pages in the PDF -- this is the reference `3 0 R`.</span>
2 0 obj
&lt;&lt;
	/Type /Page
	/Parent 3 0 R
	/MediaBox [0 0 300 300]
	/Contents 1 0 R
&gt;&gt;
endobj

<span>% The third object.
%
% This is a "Pages" object that contains information about the different
% pages.  The `2 0 R` is reference to the "Page" object, defined above.</span>
3 0 obj
&lt;&lt;
	/Type /Pages
	/Kids [2 0 R ]
	/Count 1
&gt;&gt;
endobj

<span>% The fourth object.
%
% This is a "Catalog" object that provides the main structure of the PDF.
% It points to a "Pages" object that contains information about the
% different pages -- this is the reference `3 0 R`.</span>
4 0 obj
&lt;&lt;
	/Type /Catalog
	/Pages 3 0 R
&gt;&gt;
endobj

<span>% The xref table.  This is a lookup table for all the objects.
%
% I'm not entirely sure what the first entry is for, but it seems to be
% important.  The remaining entries correspond to the objects I created.</span>
xref
0 4
0000000000 65535 f
0000000851 00000 n
0000001396 00000 n
0000001655 00000 n
0000001934 00000 n

<span>% The trailer.  This contains some metadata about the PDF.  Here there
% are two entries, which tell us that:
%
%   - There are 4 entries in the `xref` table.
%   - The root of the document is object 4 (the "Catalog" object)
%</span>
trailer
&lt;&lt;
	/Size 4
	/Root 4 0 R
&gt;&gt;

<span>% The startxref marker tells us that we can find the xref table 2196&nbsp;bytes
% after the start of the file.</span>
startxref
2196

<span>% The end-of-file marker.
%%EOF</span></code></pre><p>I played with this file for a while, just doing simple things like adding extra shapes, changing how the shapes appeared, and putting different shapes on different pages. I tried for a while to get text working, but that was a bit beyond me.</p> <p>It quickly became apparent why nobody writes PDFs by hand – it got very fiddly to redo all the lookup tables! But I’m glad I did it; manipulating all the PDF objects and their references really helped me feel like I understand the basic model of PDFs. I opened some “real” PDFs created by other apps, and they have many more objects and types of object – but now I could at least follow some of what’s going on.</p> <p>With this newfound ability to edit PDFs by hand, how can I create monstrously big ones?</p> <h2 id="changing-the-page-size-mediabox-and-userunit">Changing the page size: /MediaBox and /UserUnit</h2> <p>Within a PDF, the size of each page is set on the individual “Page” objects – this allows different pages to be different sizes. We’ve already seen this once:</p><pre><code>&lt;&lt;
	/Type /Page
	/Parent 3 0 R
	<span>/MediaBox [0 0 300 300]</span>
	/Contents 1 0 R
&gt;&gt;</code></pre><p>Here, the <code>MediaBox</code> is setting the width and height of the page – in this case, a square of 300 × 300&nbsp;units. The default unit size is 1/72&nbsp;inch, so the page is 300 × 72 = 4.17&nbsp;inches. And indeed, if I open this PDF in Adobe Acrobat, that’s what it reports:</p> <picture> <source srcset="https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in.dark_1x.avif 724w, https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in.dark_2x.avif 1448w" sizes="(max-width: 724px) 100vw, 724px" type="image/avif" media="(prefers-color-scheme: dark)"> <source srcset="https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in.dark_1x.webp 724w, https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in.dark_2x.webp 1448w" sizes="(max-width: 724px) 100vw, 724px" type="image/webp" media="(prefers-color-scheme: dark)"> <source srcset="https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in.dark_1x.png 724w, https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in.dark_2x.png 1448w" sizes="(max-width: 724px) 100vw, 724px" type="image/png" media="(prefers-color-scheme: dark)"> <source srcset="https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in_1x.avif 724w, https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in_2x.avif 1448w" sizes="(max-width: 724px) 100vw, 724px" type="image/avif"> <source srcset="https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in_1x.webp 724w, https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in_2x.webp 1448w" sizes="(max-width: 724px) 100vw, 724px" type="image/webp"> <source srcset="https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in_1x.png 724w, https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in_2x.png 1448w" sizes="(max-width: 724px) 100vw, 724px" type="image/png"> <img src="https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in_1x.png" alt="Screenshot of Acrobat’s ‘Document Properties’ panel, showing the page size of 4.17 x 4.17 in." width="724"> </picture> <p>By changing the <code>MediaBox</code> value, we can make the page bigger. For example, if we change the value to <code>600 600</code>, Acrobat says it’s now <code>8.33 x 8.33 in</code>. Nice!</p> <p>We can increase it all the way to <code>14400 14400</code>, the max allowed by Acrobat, and then it says the page is now <code>200.00 x 200.00in</code>. (You <a href="https://alexwlchan.net/images/2024/acrobat-error.png">get a warning</a> if you try to push past that limit.)</p> <p>But 200&nbsp;inches is far short of 381&nbsp;kilometres – and that’s because we’re using the default unit of 1/72&nbsp;inch. We can increase the unit size by adding a <code>/UserUnit</code> value. For exaple, setting the value to 2 will double the page in both dimensions:</p><pre><code>&lt;&lt;
	/Type /Page
	/Parent 3 0 R
	<span>/MediaBox [0 0 14400 14400]</span>
	<span>/UserUnit 2</span>
	/Contents 1 0 R
&gt;&gt;</code></pre><p>And now Acrobat reports the size of the page as <code>400.00 x 400.00 in</code>.</p> <p>If we crank it all the way up to the maximum of <code>UserUnit 75000</code>, Acrobat now reports the size of our page as <code>15,000,000,000.00 x 15,000,000,000.00 in</code> – 381 km along both sides, matching the original claim. If you’re curious, you can <a href="https://alexwlchan.net/files/2024/biggest.pdf">download the PDF</a>.</p> <p>If you try to create a page with a larger size, either by increasing the <code>MediaBox</code> or <code>UserUnit</code> values, Acrobat just ignores it. It keeps saying that the size of a page is 15 billion inches, even if the page metadata says it’s higher. (And if you increase the <code>UserUnit</code> past <code>75000</code>, this happens silently – there’s no warning or error to suggest the size of the page is being capped.)</p> <p>This probably isn’t an issue – I don’t think the <code>UserUnit</code> value is widely used in practice. I found <a href="https://stackoverflow.com/a/59927201/1558022">one Stack Overflow answer</a> saying as such, and I couldn’t find any examples of it online. The builtin macOS Preview.app doesn’t even support it – it completely ignores the value, and treats all PDFs as if the unit size is 1/72&nbsp;inch.</p> <p>But unlike Acrobat, the Preview app doesn’t have an upper limit on what we can put in <code>MediaBox</code>. It’s perfectly happy for me to write a width which is a 1 followed by twelve 0s:</p> <picture> <source srcset="https://alexwlchan.net/images/2024/preview-megawide.dark_1x.avif 366w, https://alexwlchan.net/images/2024/preview-megawide.dark_2x.avif 732w" sizes="(max-width: 366px) 100vw, 366px" type="image/avif" media="(prefers-color-scheme: dark)"> <source srcset="https://alexwlchan.net/images/2024/preview-megawide.dark_1x.webp 366w, https://alexwlchan.net/images/2024/preview-megawide.dark_2x.webp 732w" sizes="(max-width: 366px) 100vw, 366px" type="image/webp" media="(prefers-color-scheme: dark)"> <source srcset="https://alexwlchan.net/images/2024/preview-megawide.dark_1x.png 366w, https://alexwlchan.net/images/2024/preview-megawide.dark_2x.png 732w" sizes="(max-width: 366px) 100vw, 366px" type="image/png" media="(prefers-color-scheme: dark)"> <source srcset="https://alexwlchan.net/images/2024/preview-megawide_1x.avif 366w, https://alexwlchan.net/images/2024/preview-megawide_2x.avif 732w" sizes="(max-width: 366px) 100vw, 366px" type="image/avif"> <source srcset="https://alexwlchan.net/images/2024/preview-megawide_1x.webp 366w, https://alexwlchan.net/images/2024/preview-megawide_2x.webp 732w" sizes="(max-width: 366px) 100vw, 366px" type="image/webp"> <source srcset="https://alexwlchan.net/images/2024/preview-megawide_1x.png 366w, https://alexwlchan.net/images/2024/preview-megawide_2x.png 732w" sizes="(max-width: 366px) 100vw, 366px" type="image/png"> <img src="https://alexwlchan.net/images/2024/preview-megawide_1x.png" alt="Screenshot of Preview’s Document inspector, showing the page size of 352777777777.78 x 10.59 cm." width="366"> </picture> <p>If you’re curious, that width is approximately the distance between the Earth and the Moon. I’d have to get my ruler to check, but I’m pretty sure that’s larger than Germany.</p> <p>I could keep going. And I did. Eventually I ended up with a PDF that Preview claimed is larger than the entire universe – approximately 37 trillion light years square. Admittedly it’s mostly empty space, but so is the universe. If you’d like to play with that PDF, you can <a href="https://alexwlchan.net/files/2024/universe.pdf">get it here</a>.</p> <p>Please don’t try to print it.</p> </article> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MobileDiffusion: Rapid text-to-image generation on-device (243 pts)]]></title>
            <link>https://blog.research.google/2024/01/mobilediffusion-rapid-text-to-image.html</link>
            <guid>39210458</guid>
            <pubDate>Wed, 31 Jan 2024 22:42:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.research.google/2024/01/mobilediffusion-rapid-text-to-image.html">https://blog.research.google/2024/01/mobilediffusion-rapid-text-to-image.html</a>, See on <a href="https://news.ycombinator.com/item?id=39210458">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-5966553114967673984">
<p><span>Posted by Yang Zhao, Senior Software Engineer, and Tingbo Hou, Senior Staff Software Engineer, Core ML</span>

</p><p>
Text-to-image <a href="https://arxiv.org/abs/2006.11239">diffusion models</a> have shown exceptional capabilities in generating high-quality images from text prompts. However, leading models feature billions of parameters and are consequently expensive to run, requiring powerful desktops or servers (e.g., <a href="https://stability.ai/news/stable-diffusion-public-release">Stable Diffusion</a>, <a href="https://openai.com/research/dall-e">DALL·E</a>, and <a href="https://imagen.research.google/">Imagen</a>). While recent advancements in inference solutions on <a href="https://blog.research.google/2023/06/speed-is-all-you-need-on-device.html">Android</a> via MediaPipe and <a href="https://github.com/apple/ml-stable-diffusion">iOS</a> via Core ML have been made in the past year, rapid (sub-second) text-to-image generation on mobile devices has remained out of reach.
</p> <p>
To that end, in “<a href="https://arxiv.org/abs/2311.16567">MobileDiffusion: Subsecond Text-to-Image Generation on Mobile Devices</a>”, we introduce a novel approach with the potential for rapid text-to-image generation on-device. MobileDiffusion is an efficient latent diffusion model specifically designed for mobile devices. We also adopt <a href="https://arxiv.org/abs/2311.09257">DiffusionGAN</a> to achieve one-step sampling during inference, which fine-tunes a pre-trained diffusion model while leveraging a GAN to model the denoising step. We have tested MobileDiffusion on iOS and Android premium devices, and it can run in half a second to generate a 512x512 high-quality image. Its comparably small model size of just 520M parameters makes it uniquely suited for mobile deployment.
</p>




<p>Rapid text-to-image generation on-device.</p>





<h2>Background</h2>


<p>
The relative inefficiency of text-to-image diffusion models arises from two primary challenges. First, the inherent design of diffusion models requires <a href="https://blog.research.google/2023/06/on-device-diffusion-plugins-for.html">iterative denoising</a> to generate images, necessitating multiple evaluations of the model. Second, the complexity of the network architecture in text-to-image diffusion models involves a substantial number of parameters, regularly reaching into the billions and resulting in computationally expensive evaluations. As a result, despite the potential benefits of deploying generative models on mobile devices, such as enhancing user experience and addressing emerging privacy concerns, it remains relatively unexplored within the current literature.
</p>
<p>
The optimization of inference efficiency in text-to-image diffusion models has been an active research area. Previous studies predominantly concentrate on addressing the first challenge, seeking to reduce the number of function evaluations (NFEs). Leveraging advanced numerical solvers (e.g., <a href="https://arxiv.org/abs/2206.00927">DPM</a>) or distillation techniques (e.g., <a href="https://arxiv.org/abs/2202.00512">progressive distillation</a>, <a href="https://arxiv.org/abs/2303.01469">consistency distillation</a>), the number of necessary sampling steps have significantly reduced from several hundreds to single digits. Some recent techniques, like <a href="https://arxiv.org/abs/2311.09257">DiffusionGAN</a> and <a href="https://arxiv.org/abs/2311.17042#:~:text=We%20introduce%20Adversarial%20Diffusion%20Distillation,while%20maintaining%20high%20image%20quality.">Adversarial Diffusion Distillation</a>, even reduce to a single necessary step. 
</p>
<p>
However, on mobile devices, even a small number of evaluation steps can be slow due to the complexity of model architecture. Thus far, the architectural efficiency of text-to-image diffusion models has received comparatively less attention. A handful of earlier works briefly touches upon this matter, involving the removal of redundant neural network blocks (e.g., <a href="https://snap-research.github.io/SnapFusion/">SnapFusion</a>). However, these efforts lack a comprehensive analysis of each component within the model architecture, thereby falling short of providing a holistic guide for designing highly efficient architectures.
</p>





<h2>MobileDiffusion</h2>


<p>
Effectively overcoming the challenges imposed by the limited computational power of mobile devices requires an in-depth and holistic exploration of the model's architectural efficiency. In pursuit of this objective, our research undertakes a detailed examination of each constituent and computational operation within Stable Diffusion’s <a href="https://arxiv.org/abs/2112.10752">UNet architecture</a>. We present a comprehensive guide for crafting highly efficient text-to-image diffusion models culminating in the MobileDiffusion.
</p>
<p>
The design of MobileDiffusion follows that of <a href="https://arxiv.org/abs/2112.10752">latent diffusion models</a>. It contains three components: a text encoder, a diffusion UNet, and an image decoder. For the text encoder, we use <a href="https://arxiv.org/abs/2103.00020">CLIP-ViT/L14</a>, which is a small model (125M parameters) suitable for mobile. We then turn our focus to the diffusion UNet and image decoder. 
</p>




<h3>Diffusion UNet</h3>


<p>
As illustrated in the figure below, diffusion UNets commonly interleave transformer blocks and convolution blocks. We conduct a comprehensive investigation of these two fundamental building blocks. Throughout the study, we control the training pipeline (e.g., data, optimizer) to study the effects of different architectures.
</p>
<p>
In classic text-to-image diffusion models, a transformer block consists of a self-attention layer (SA) for modeling long-range dependencies among visual features, a cross-attention layer (CA) to capture interactions between text conditioning and visual features, and a feed-forward layer (FF) to post-process the output of attention layers. These transformer blocks hold a pivotal role in text-to-image diffusion models, serving as the primary components responsible for text comprehension. However, they also pose a significant efficiency challenge, given the computational expense of the attention operation, which is quadratic to the sequence length. We follow the idea of <a href="https://arxiv.org/abs/2301.11093">UViT</a> architecture, which places more transformer blocks at the bottleneck of the UNet. This design choice is motivated by the fact that the attention computation is less resource-intensive at the bottleneck due to its lower dimensionality. 
</p>





<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsshK53k6noqIbabpGMBzYIBCdviXisDoBsD3Houk-lXzN8pZQcusKYBvjWwcwA1Aq5DnWyk01YM9B2RyRZx6HcGgTP-LrW-tnwFwByzlBACN3WggyPYM0Mpyr2OVGVLFhx1uN48aR1g9P4o0joN2STli9VpA_tFMdQ-ikRXVrNpawzB793-unSENR-PIV/s915/image4.png"><img data-original-height="249" data-original-width="915" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsshK53k6noqIbabpGMBzYIBCdviXisDoBsD3Houk-lXzN8pZQcusKYBvjWwcwA1Aq5DnWyk01YM9B2RyRZx6HcGgTP-LrW-tnwFwByzlBACN3WggyPYM0Mpyr2OVGVLFhx1uN48aR1g9P4o0joN2STli9VpA_tFMdQ-ikRXVrNpawzB793-unSENR-PIV/s16000/image4.png"></a></td></tr><tr><td>Our UNet architecture incorporates more transformers in the middle, and skips self-attention (SA) layers at higher resolutions.</td></tr></tbody></table>




<p>
Convolution blocks, in particular <a href="https://arxiv.org/abs/1512.03385">ResNet</a> blocks, are deployed at each level of the UNet. While these blocks are instrumental for feature extraction and information flow, the associated computational costs, especially at high-resolution levels, can be substantial. One proven approach in this context is <a href="https://arxiv.org/abs/1704.04861">separable convolution</a>. We observed that replacing regular convolution layers with lightweight separable convolution layers in the deeper segments of the UNet yields similar performance.
</p>
<p>
In the figure below, we compare the UNets of several diffusion models. Our MobileDiffusion exhibits superior efficiency in terms of <a href="https://arxiv.org/pdf/2110.12894.pdf">FLOPs</a> (floating-point operations) and number of parameters. 
</p>




<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXYleITSssbZnLffeh3BzG3tX2qNQNeB__xc-ySks0SPnXsMb2kTLZ0PcE2KWJ4I9FX_QMP32pXd06IuV1kJJSlgp7CuV6dqkXJsiFqo_6xqWXZ1-65p_EPU9gk7G9B4-L2TaKGiD5cahwg428CTmV1dcuQQ_vBTVmP8543IJigIF0qHo8_JaB8h5EuVvl/s1200/image3.png"><img data-original-height="742" data-original-width="1200" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXYleITSssbZnLffeh3BzG3tX2qNQNeB__xc-ySks0SPnXsMb2kTLZ0PcE2KWJ4I9FX_QMP32pXd06IuV1kJJSlgp7CuV6dqkXJsiFqo_6xqWXZ1-65p_EPU9gk7G9B4-L2TaKGiD5cahwg428CTmV1dcuQQ_vBTVmP8543IJigIF0qHo8_JaB8h5EuVvl/s16000/image3.png"></a></td></tr><tr><td>Comparison of some diffusion UNets.</td></tr></tbody></table>





<h3>Image decoder</h3>


<p>
In addition to the UNet, we also optimized the image decoder. We trained a <a href="https://arxiv.org/abs/2012.03715">variational autoencoder</a> (VAE) to encode an <a href="https://en.wikipedia.org/wiki/RGB_color_model">RGB</a> image to an 8-channel latent variable, with 8× smaller spatial size of the image. A latent variable can be decoded to an image and gets 8× larger in size.  To further enhance efficiency, we design a lightweight decoder architecture by pruning the original’s width and depth. The resulting lightweight decoder leads to a significant performance boost, with nearly 50% latency improvement and better quality. For more details, please refer to our <a href="https://arxiv.org/abs/2311.16567">paper</a>.
</p>




<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjT2Nmo7GjGdN0_2dqevJB52RogqnWFDVmFsrusHHxnVf9YQYsdbVkAQvBI3h9SzKZ0TqOQOmnxaZ6z2kdix12tei5oMpD17SY1LoBWqxD1EHgV0ygTb9TV0IFZQtv4dAix378lb8WGv5GGPQIuyStX3gWqn0pjTTXbpIlA0VzYSeiGpkO5bsHhZfjbkR07/s1124/image6.png"><img data-original-height="789" data-original-width="1124" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjT2Nmo7GjGdN0_2dqevJB52RogqnWFDVmFsrusHHxnVf9YQYsdbVkAQvBI3h9SzKZ0TqOQOmnxaZ6z2kdix12tei5oMpD17SY1LoBWqxD1EHgV0ygTb9TV0IFZQtv4dAix378lb8WGv5GGPQIuyStX3gWqn0pjTTXbpIlA0VzYSeiGpkO5bsHhZfjbkR07/s16000/image6.png"></a></td></tr><tr><td>VAE reconstruction. Our VAE decoders have better visual quality than SD (Stable Diffusion).</td></tr></tbody></table>



<br>

<table>
  <tbody><tr>
   <td><b>Decoder</b>
   </td>
   <td><b>&nbsp;&nbsp;#Params (M)&nbsp;&nbsp;</b>
   </td>
   <td><b>&nbsp;&nbsp;PSNR↑&nbsp;&nbsp;</b>
   </td>
   <td><b>&nbsp;&nbsp;SSIM↑&nbsp;&nbsp;</b>
   </td>
   <td><b>&nbsp;&nbsp;LPIPS↓&nbsp;&nbsp;</b>
   </td>
  </tr>
  <tr>
   <td><b>SD</b>
   </td>
   <td>49.5
   </td>
   <td>26.7
   </td>
   <td>0.76
   </td>
   <td>0.037
   </td>
  </tr>
  <tr>
   <td><b>Ours</b>
   </td>
   <td>39.3
   </td>
   <td>30.0
   </td>
   <td>0.83
   </td>
   <td>0.032
   </td>
  </tr>
  <tr>
   <td><b>Ours-Lite&nbsp;&nbsp;&nbsp;&nbsp;</b>
   </td>
   <td>9.8
   </td>
   <td>30.2
   </td>
   <td>0.84
   </td>
   <td>0.032
   </td>
  </tr>
</tbody></table>
<br>





<h3>One-step sampling</h3>


<p>
In addition to optimizing the model architecture, we adopt a <a href="https://arxiv.org/abs/2311.09257">DiffusionGAN hybrid</a> to achieve one-step sampling. Training DiffusionGAN hybrid models for text-to-image generation encounters several intricacies. Notably, the discriminator, a classifier distinguishing real data and generated data, must make judgments based on both texture and semantics. Moreover, the cost of training text-to-image models can be extremely high, particularly in the case of GAN-based models, where the discriminator introduces additional parameters. Purely GAN-based text-to-image models (e.g., <a href="https://arxiv.org/abs/2301.09515">StyleGAN-T</a>, <a href="https://arxiv.org/abs/2303.05511">GigaGAN</a>) confront similar complexities, resulting in highly intricate and expensive training.
</p>
<p>
To overcome these challenges, we use a pre-trained diffusion UNet to initialize the generator and discriminator. This design enables seamless initialization with the pre-trained diffusion model. We postulate that the internal features within the diffusion model contain rich information of the intricate interplay between textual and visual data. This initialization strategy significantly streamlines the training.
</p>
<p>
The figure below illustrates the training procedure. After initialization, a noisy image is sent to the generator for one-step diffusion. The result is evaluated against ground truth with a reconstruction loss, similar to diffusion model training. We then add noise to the output and send it to the discriminator, whose result is evaluated with a GAN loss, effectively adopting the GAN to model a denoising step. By using pre-trained weights to initialize the generator and the discriminator, the training becomes a fine-tuning process, which converges in less than 10K iterations.  
</p>



<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnK7SE2-cHSlP-PDmkl_xfjp3sP-kB41r6OvC8Wg6miXnYwdES0INwN19BHWQ_uyXtcBT-872U5J6jLY8yXVtA_W96qkRRPh6Pjvw0n-ZJvjJK91kYTh7H1n4nzy8z1TyrQZlZoZrQUDTo5Qm-6a_2vIVye3aqm7o32qOOXiWXwxDzw_J6cQsOrJ-UILKw/s960/image7.jpg"><img data-original-height="576" data-original-width="960" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnK7SE2-cHSlP-PDmkl_xfjp3sP-kB41r6OvC8Wg6miXnYwdES0INwN19BHWQ_uyXtcBT-872U5J6jLY8yXVtA_W96qkRRPh6Pjvw0n-ZJvjJK91kYTh7H1n4nzy8z1TyrQZlZoZrQUDTo5Qm-6a_2vIVye3aqm7o32qOOXiWXwxDzw_J6cQsOrJ-UILKw/s16000/image7.jpg"></a></td></tr><tr><td>Illustration of DiffusionGAN fine-tuning.</td></tr></tbody></table>




<h2>Results</h2>


<p>
Below we show example images generated by our MobileDiffusion with DiffusionGAN one-step sampling. With such a compact model (520M parameters in total), MobileDiffusion can generate high-quality diverse images for various domains.
</p>




<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyDLq1NW7Qvy4_oEqg1pHAMzeBfuei3VadIKZRNkv6ZHnzewVWQU5x76e0bm-QqWVr-_q1W4axBJeyqyCbdRFoUFBYxRxDj3qo7I4-Du6TS2Bez_-mmXzYoHLJk7y5fiKl9PPkHNk_dsvy7ezuAFavW4sYIeYTxhAPAH35FYP5YOceS8NfJey0gpvHUwza/s1728/image1.png"><img data-original-height="1296" data-original-width="1728" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyDLq1NW7Qvy4_oEqg1pHAMzeBfuei3VadIKZRNkv6ZHnzewVWQU5x76e0bm-QqWVr-_q1W4axBJeyqyCbdRFoUFBYxRxDj3qo7I4-Du6TS2Bez_-mmXzYoHLJk7y5fiKl9PPkHNk_dsvy7ezuAFavW4sYIeYTxhAPAH35FYP5YOceS8NfJey0gpvHUwza/s16000/image1.png"></a></td></tr><tr><td>Images generated by our MobileDiffusion</td></tr></tbody></table>



<p>
We measured the performance of our MobileDiffusion on both iOS and Android devices, using different runtime optimizers. The latency numbers are reported below. We see that MobileDiffusion is very efficient and can run within half a second to generate a 512x512 image. This lightning speed potentially enables many interesting use cases on mobile devices.
</p>



<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFkcI7kibwRFhpxTsVmUkAzK38MCeBoTR6fOWyhjnqwPm7x8TwrVn_O0OipsXCbgS4qTtcbtm41Fxi7U_IJjpeuZadWO7cBKkcdrXHniAJgQP4Qk-wOBfnhtwNPxDbzxtM0uxVba3BjwzLa3Lw13-03FoRQbWwf_25KR9GLLkSqIFpnU5aE-6hnomY5IuK/s1184/image8.png" imageanchor="1"><img data-original-height="742" data-original-width="1184" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFkcI7kibwRFhpxTsVmUkAzK38MCeBoTR6fOWyhjnqwPm7x8TwrVn_O0OipsXCbgS4qTtcbtm41Fxi7U_IJjpeuZadWO7cBKkcdrXHniAJgQP4Qk-wOBfnhtwNPxDbzxtM0uxVba3BjwzLa3Lw13-03FoRQbWwf_25KR9GLLkSqIFpnU5aE-6hnomY5IuK/s16000/image8.png"></a></td></tr><tr><td>Latency measurements (<b>s</b>) on mobile devices.</td></tr></tbody></table>




<h2>Conclusion</h2>


<p>
With superior efficiency in terms of latency and size, MobileDiffusion has the potential to be a very friendly option for mobile deployments given its capability to enable a rapid image generation experience while typing text prompts. And we will ensure any application of this technology will be in-line with Google’s <a href="https://ai.google/responsibility/responsible-ai-practices/">responsible AI practices</a>.
</p>




<h2>Acknowledgments</h2>


<p>
<em>We like to thank our collaborators and contributors that helped bring MobileDiffusion to on-device: Zhisheng Xiao, Yanwu Xu, Jiuqiang Tang, Haolin Jia, Lutz Justen, Daniel Fenner, Ronald Wotzlaw, Jianing Wei, Raman Sarokin, Juhyun Lee, Andrei Kulik, Chuo-Ling Chang, and Matthias Grundmann.</em>
</p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Visual Analysis of Binary Files (202 pts)]]></title>
            <link>http://binvis.io/#/</link>
            <guid>39210436</guid>
            <pubDate>Wed, 31 Jan 2024 22:39:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://binvis.io/#/">http://binvis.io/#/</a>, See on <a href="https://news.ycombinator.com/item?id=39210436">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[I don't want anything your AI generates (214 pts)]]></title>
            <link>https://coryd.dev/posts/2024/i-dont-want-anything-your-ai-generates/</link>
            <guid>39210126</guid>
            <pubDate>Wed, 31 Jan 2024 22:11:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://coryd.dev/posts/2024/i-dont-want-anything-your-ai-generates/">https://coryd.dev/posts/2024/i-dont-want-anything-your-ai-generates/</a>, See on <a href="https://news.ycombinator.com/item?id=39210126">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>I really don't. AI output is fundamentally derivative and exploitative (of content, labor and the environment).</p><div><p>I really don't. AI output is fundamentally derivative and exploitative (of content, labor and the environment).</p><p>I can't trust the answers it provides or the text it generates. It's not a replacement for search, it simply makes search worse.</p><p>The images it generates are, at best, a polished regression to the mean. If you want custom art, pay an artist.</p><p>I want to talk to a person, not a chatbot. The chatbot wastes time while you wait for a person that can actually help.</p><p>I don't want music recommendations from something that can't appreciate or understand music. Human recommendations will always be better.</p><p>I don't want AI mediating social interactions that it cannot and does not understand (though it may appear to). If I'm weary of too much volume on any social platform or in any news feed, I'll cut back on what I'm following.</p><p>If you're having AI attend a meeting for you, it probably wasn't that important. If you're having AI write your email, it probably wasn't that important. If it's screening job candidates for you, you're missing quality candidates.</p><p>I don't like the idea of it being trained on anything I've written or created<sup><a href="#fn1" id="fnref1">[1]</a></sup>.</p><p>I'll concede that Copilot is better than traditional autocomplete, but that feels like a pretty low bar<sup><a href="#fn2" id="fnref2">[2]</a></sup>.</p><p>These tools will improve<sup><a href="#fn3" id="fnref3">[3]</a></sup>. The cost to the environment will increase, it will help companies deskill jobs and muddy public discourse. The "benefits" it provides to end users are, at best, dubious — though everyone responsible for creating it will most certainly enrich themselves.</p><p>This all portends a future I can't imagine wanting or even being interested in.</p><hr><section><ol><li id="fn1"><p>Although consuming that may well be to its detriment. Nonetheless, my <code>robots.txt</code> file reflects this stance. <a href="#fnref1">↩︎</a></p></li><li id="fn2"><p>My first experience with Copilot chat was it getting an answer wrong, apologizing when that was pointed out and then repeating the same or similar, but still incorrect, answer. <a href="#fnref2">↩︎</a></p></li><li id="fn3"><p>I don't think this is a good thing. <a href="#fnref3">↩︎</a></p></li></ol></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DeepSeek Coder: Let the Code Write Itself (198 pts)]]></title>
            <link>https://deepseekcoder.github.io/</link>
            <guid>39209814</guid>
            <pubDate>Wed, 31 Jan 2024 21:43:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deepseekcoder.github.io/">https://deepseekcoder.github.io/</a>, See on <a href="https://news.ycombinator.com/item?id=39209814">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p>
            DeepSeek Coder comprises a series of code language models trained from scratch on both 87% code and 13% natural language in English and Chinese, with each model pre-trained on 2T tokens.
            We provide various sizes of the code model, ranging from 1B to 33B versions.
            Each model is pre-trained on repo-level code corpus by employing a window size of 16K and a extra fill-in-the-blank task, resulting in foundational models (DeepSeek-Coder-Base).
            We further fine-tune the base model with 2B tokens of instruction data to get instruction-tuned models, namedly DeepSeek-Coder-Instruct.
          </p>
          <!-- <p>
            For coding capabilities, DeepSeek-Coder-Base achieves state-of-the-art performance among open-source code models on multiple programming languages and various benchmarks.
            And Compared to GPT35-turbo, DeepSeek-Coder-Instruct demonstrates superior performance in human evaluation while maintaining comparable performance in MBPP.
          </p> -->
          <ul>
            <li>
              Pretrained on <b>2 Trillion</b> tokens over more than 80 programming languages.
            </li>
            <li>
              Various model sizes (<b>1.3B</b>, <b>5.7B</b>, <b>6.7B</b> and <b>33B</b>) to support different requirements.
            </li>
            <li>
              A window size of <b>16K window </b>size, supporting <b>project-level</b> code completion and infilling.
            </li>
            <li>
              <b>State-of-the-Art</b> performance among open code models. 
            </li>
            <li>
              <b>Open source and free for research and commercial use</b>.
            </li>
          </ul>
          
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Generating Fantasy Maps (2016) (164 pts)]]></title>
            <link>http://mewo2.com/notes/terrain/</link>
            <guid>39209687</guid>
            <pubDate>Wed, 31 Jan 2024 21:31:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://mewo2.com/notes/terrain/">http://mewo2.com/notes/terrain/</a>, See on <a href="https://news.ycombinator.com/item?id=39209687">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="container">
    
    





<p><img src="http://mewo2.com/deserts/map.png" alt="An example map from the generator" width="60%"></p>
<p>These are some notes on how I generate the maps for my Twitter bot
<a href="https://twitter.com/unchartedatlas">@unchartedatlas</a>, which is based on a generator I originally produced
during <a href="https://github.com/dariusk/NaNoGenMo-2015/issues/156">NaNoGenMo 2015</a>. There's JavaScript code for the generator
on Github <a href="https://github.com/mewo2/terrain">here</a>, and the original messy Python generator code can be
seen <a href="https://github.com/mewo2/deserts">here</a>.</p>
<p>You may also be interested in this <a href="http://mewo2.com/notes/naming-language/">companion piece</a>, which describes
the placename generation.</p>
<h3>Inspiration</h3>
<p>I wanted to make maps that look like something you'd find at the back of one of
the cheap paperback fantasy novels of my youth. I always had a fascination with
these imagined worlds, which were often much more interesting than whatever
luke-warm sub-Tolkien tale they were attached to.</p>
<p>At the same time, I wanted to play with terrain generation with a physical
basis. There are loads of articles on the internet which describe terrain
generation, and they almost all use some variation on a fractal noise approach,
either directly (by adding layers of noise functions), or indirectly (e.g.
through midpoint displacement). These methods produce lots of fine detail, but
the large-scale structure always looks a bit off. Features are attached in
random ways, with no thought to the processes which form landscapes. I wanted
to try something a little bit different.</p>
<p>There are a few different stages to the generator. First we build up a
height-map of the terrain, and do things like routing water flow over the
surface. Then we can render the 'physical' portion of the map. Finally we can
place cities and 'regions' on the map, and place their labels.</p>
<h3>Grids</h3>
<p>To represent the heightmap, first we need a grid of points. Although it can be
simpler to work on a regular square grid, I much prefer to work on an irregular
set of points for something like this. With a regular grid, it's very easy to
run into weird artifacts, and you often have to do a lot of postprocessing to
hide the effects of the grid. If you use an irregular grid, then there are a
few things which are more complicated, but the structure of the grid helps to
give the map a rough, organic feel, and you never have to worry about nasty
linear artifacts in the finished product.</p>
<p>The approach I use is the same as in <a href="http://www-cs-students.stanford.edu/~amitp/game-programming/polygon-map-generation/">this article</a>, which is one of the
better references out there on how to do non-fractal terrain generation. I
won't go into too much detail here because that article explains it very
clearly, with lots of diagrams.</p>
<p>I start by selecting points at random within the map. These points tend to be a
bit clumpy and uneven, so I use <a href="https://en.wikipedia.org/wiki/Lloyd%27s_algorithm">Lloyd relaxation</a> to improve the point
set. For speed, I only use one iteration of this process, but you can repeat it
as many times as you like. There are rapidly diminishing returns after a few
iterations though.</p>
<p>All of the calculations are actually carried out on the 'dual points' of the
original point set, which correspond to the corners of the Voronoi polygons.
This has the advantage that the number of neighbours per node is fixed at
three, which helps in some parts of the code.</p>
<p>Note: this shows 256 (2<sup>8</sup>) points, to make viewing easier, but the real generator uses 16,384 (2<sup>14</sup>) points. I have a programmer's superstitions about always using powers of 2, which are more pleasing to the spirit of the machine.</p>

<h3>Rough outlines</h3>
<p>One of the difficulties of creating landscapes in a realistic way is that real
landscapes aren't created all at once. Instead, they evolve from earlier
landscapes, which in turn evolved from even earlier landscapes, and so on back
for billions of years. There's no good way to simulate this process in a
reasonable amount of time, so we need to cheat slightly.</p>
<p>Rather than an infinite regress of older landscapes, I start with a simple
'proto-landscape', built with geometric primitives. This lets me control the
broad outlines of the terrain, while leaving the details for the more physical
processes to fill in later.</p>
<p>Some useful primitives which we can add together:</p>
<ul>
<li>Constant slope - if you want to pretend this is physically motivated, think of it as tectonic uplift on one side of the map</li>
<li>Cone shapes - these can be islands or mountains, or if inverted, lakes or seas</li>
<li>Rounded blobs - these make better hills, and can be scattered all around to make a noisy surface</li>
</ul>
<p>We also have a few operations which are handy:</p>
<ul>
<li>Normalize - rescale the heights to lie in the range 0-1</li>
<li>Round - normalize, then take the square root of the height value, to round off the tops of hills</li>
<li>Relax - replace each height value with the average of its neighbours, to smooth the surface</li>
<li>Set sea level - translate the heightmap up or down so that a particular quantile is at zero</li>
</ul>
<p>The particular sequence of primitives and operations used can be varied to produce different kinds of landscape, such as coastlines, islands and mountain ranges.</p>
<p>Note: the black line indicates the zero contour, which we treat as 'sea level'. Also, this map uses 4,096 (2<sup>12</sup>) points, for speed.</p>

<h3>Erosion</h3>
<p>The results of this process can be a little bit on the blobby side, which means
they rarely look good on their own. We want to scuff them up a bit, so they
look more like real landscapes. We do this by applying an erosion operation.</p>
<p>In most of the world, by far the largest influence on the shape of landforms is
fluvial (water-based) erosion. Water flows downhill, carrying sediment along
with it, carving out valleys and river basins. This is a massively complex
phenomenon, and modelling it correctly is a very active research area, but we
can get a long way by sketching a simple version of the process.</p>
<p>We need to start by tracing the routes that water would take over the grid. For
each grid point, we say that water flows to its lowest neighbour, and so on
down until we reach the edge of the map. This gives a map of water flow.</p>
<p>There's an obvious problem when we reach gridpoints which are lower than all of
their neighbours. Do we route the water back uphill? This will probably lead to
cycles in the water system, which are trouble. Instead, we want to fill in
these gaps (often called sinks or depressions), so that the water always runs
downhill all the way to the edge.</p>
<p>It's easy to see how to fill in a single gridpoint, but as the depression gets
bigger, and possibly links up with other depressions, the number of possible
cases multiplies enormously. Luckily, there's an algorithm for filling
depressions, called the Planchon-Darboux algorithm.</p>
<div>

<h4>Aside: the Planchon-Darboux algorithm</h4>

<p>The algorithm works by finding the lowest surface with the following two properties:</p>

<ul>
<li>The surface is everywhere at least as high as the input surface</li>
<li>Every non-edge point has a neighbour which is lower than it</li>
</ul>

<p>To calculate this, we start with an infinitely high surface everywhere
except on the edge, where we use the original heights. Then, on each iteration,
we find points which have a neighbour which is lower than them, and set their
height to their original height, or the height of their lowest neighbour (plus
a small amount), whichever is higher. We halt when we can go a full iteration
without changing any point.</p>

<p>There are various ways of speeding up this algorithm, mostly by tweaking the
order in which points are visited. For more details, and a proof of
correctness, you can read <a href="http://horizon.documentation.ird.fr/exl-doc/pleins_textes/pleins_textes_7/sous_copyright/010031925.pdf">the original paper</a>.</p>

</div>

<p>With the water routing calculated, we can work out how much water is flowing
through each point. I assume that rainfall is constant across the whole map,
and iterate through the points in descending order, passing the rainfall, plus
the accumulated water flux, from each point to its 'downhill point'. This gives
a map of water flux, which usually converges into a nice branching river
structure, with lots of small streams feeding a larger central channel.</p>
<p>To calculate erosion, I combine the water flux with the slope at each point, as
calculated based on the triangle of its neighbours. The exact formula I use is
the product of the slope with the square root of the water flux. This isn't
necessarily very physical, but it does give nice-looking results. I also add a
small term which is proportional to the slope squared. This prevents deep
gorges from forming, which might be physically realistic, but don't look good
in the graphical style I've chosen.</p>
<p>I find it's very important to cap the erosion rate, otherwise strange things
can happen. A little goes a very long way with this. Also, erosion always
lowers the surface, so it usually helps to drop the sea level afterwards to match.</p>
<p>A final tweak to the heightmap is to smooth out the coastlines slightly. The
erosion tends to produce quite rough terrain, which becomes tiny islands when
cut off by sea level. A few of these can look good, but too many just looks
messy. I repeatedly apply a filter where points which are below sea level, but
a majority of whose neighbours are above sea level, get pulled up, and vice
versa for points which are above sea level and have undersea neighbours. A
couple of repeats of this produces a much cleaner coastline.</p>


<h3>Rendering terrain</h3>
<p>Now comes the question of drawing the map (at least the physical portion). The
easy part is the coastline - we've been doing this already. It's just a matter
of drawing line segments where the heightmap crosses zero. There's not a lot
extra to do about this.</p>
<p>The next component is the rivers. We don't want to display the entire drainage
network, because that would cover the whole map. Instead, we only show the
drainage from points with above a certain threshold of water flux. By
connecting these points to their downstream neighbours, we can trace out the
river paths.</p>
<p>One problem with this approach is that the rivers tend to zigzag from grid
point to grid point, rather than following a smooth path. To solve this, I
relax the points in the middle of the path towards their upstream and
downstream neighbours (keeping the top and bottom fixed, so that intersections
work properly). This smooths things out beautifully.</p>
<p>The final part of this is the shading on the sides of hills, which helps
indicate the topography. It is a central principle of cartography that we tend
to interpret maps as though viewing the terrain from the bottom of the map,
looking towards the top. So we want to draw strokes which go up and right if
the terrain slopes upwards from left to right, and down and right if the
terrain slopes downwards. Similarly, the strokes on the 'near' side of hills
should be longer than those on the 'far' side.</p>
<p>For each grid point, I calculate the slope, and ignore the point if it is less
than a random threshold. For points which pass this test, I draw a short stroke
with slope proportional to the horizontal component of the heightmap slope,
with a small modifier for the vertical component. If the stroke would be too
steep, I split it into several shorter strokes, at the maximum slope, drawn at
random around the point.</p>


<h3>Cities, borders</h3>
<p>Now that we have the 'physical' portion of the map sorted, we can move to
looking at the 'political'. We want to place cities and towns on the map in
feasible-looking locations. At the same time, we want the cities to be spread
out enough that we can put labels on them without worrying too much about
overlap.</p>
<p>To place a city, I generate a score for each point, which is a combination of three things:</p>
<ul>
<li>Water flux - we want cities to be preferentially located on rivers, so high water flux gets a bonus</li>
<li>Distance from other cities - we want cities to be spread out, so penalize locations which are too close to an existing city</li>
<li>Distance from the edge of the map - the other two criteria alone tend to push cities to the map edge, which isn't ideal, so penalize locations too close to the edge</li>
</ul>
<p>Every time I add a city, I choose the point with the highest score, then
recalculate the scores for all points. I make a distinction between cities,
which have a 'region' associated with them, and towns, which don't. The cities
are placed first, but otherwise there's no distinction in the code.</p>
<p>The next step is to mark out the regions. We want the borders between regions
to seem fairly reasonable, following natural borders like rivers and mountain
ranges. The way I approach this is to expand regions outwards from each city,
so that each region consists of the points which are 'closest' to its city,
according to a particular distance measure. This distance measure is calculated
by adding up the cost of the route, based on these criteria:</p>
<ul>
<li>Horizontal distance</li>
<li>Slope - uphill is much cheaper than downhill, so regions expand until they hit the top of ridges, then stop</li>
<li>Water flux - crossing a river is expensive</li>
<li>Shorelines - there is a large penalty for going from land to water (or vice versa), and a smaller penalty for travelling by water</li>
</ul>
<p>Finally, the borders are drawn, using the same smoothing technique as the rivers.</p>


<h3>Placing labels</h3>
<p>At this point we can start naming things, using the process described in <a href="http://mewo2.com/notes/naming-language/">these
notes</a>. I generate names for cities, towns and regions, using a
consistent language for the whole map.</p>
<p>The very last part of the process is to place the labels, avoiding overlaps,
obscured cities, labels going off the edge of the map, etc. This sounds easy,
but it really really isn't. Ideally, some sort of intelligent layout algorithm
would place all the labels, rearranging as necessary. Instead, I have a few
hundred lines of spaghetti code which seems to get the right answer most of the
time, but which is packed full of magic numbers.</p>
<p>In rough outline, what happens is this: the labels are placed in the order 1.
cities, 2. towns, 3. regions. For cities and towns, there are four possible
slots for the label, above, below, and to each side of the marker. Each label
is placed, attempting to avoid the following overlaps (in rough order of
importance):</p>
<ul>
<li>City markers</li>
<li>Other city labels</li>
<li>The map edge</li>
<li>Borders</li>
<li>Coastlines</li>
<li>Rivers</li>
</ul>
<p>Obviously it's not usually possible to avoid all of these, so the least bad
solution is chosen.</p>
<p>For regions, there's a bit more freedom in where to place the label, but the
labels are also bigger, and the map is more cluttered at this point. The
scoring system rates positions based on proximity to the center of the region,
as well as being over land, and penalties for all the overlaps mentioned
before.</p>
<p>I wanted to make a nice interactive example for this, but trying to separate out the label placement code made me feel physically unwell. Sorry about that.</p>

<p>So that's the algorithm, at least in rough outline. The actual code running on
the bot has a few other little bits and pieces, which honestly don't do much,
but removing them is more trouble than it's worth when the code is working as
it stands. The JavaScript code behind this page is available <a href="https://github.com/mewo2/terrain">on
GitHub</a>, and if you're really really brave you can look at the original
<a href="https://github.com/mewo2/deserts">Python code</a> which was written while I was figuring all this out.</p>
<p>There's obviously lots that could be done to improve this. Erosion is just one
process, and the next obvious thing to add would be fluvial deposition, which
would allow for flood plains, river deltas, etc to form. If you wanted more
realistic mountains, then glacial processes would be worth looking at. Volcanic
stuff could also be fun.</p>
<p>On the graphical side, it could be fun to try to sketch more interesting
textures on the map, such as forests or fields. Or you could fill the oceans
with sea monsters and lost ships. If you're really brave, you could even look
at labelling more features, like mountain ranges and rivers.</p>
<p>As always, if you do any of these things, or make anything else interesting
with this code or these ideas, please get in touch and let me know.</p>
<p>If you would like to buy a print of one of these maps, please visit <a href="https://shop.mewo2.com/">my web shop</a>.</p>

    <hr><p>
    
    If you've enjoyed this piece, please consider <a href="https://www.patreon.com/mewo2">contributing on Patreon</a> so I can do more things like this.
    
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Comcast reluctantly agrees to stop its misleading "10G Network" claims (201 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2024/01/comcast-to-stop-calling-entire-network-10g-as-name-is-ruled-misleading/</link>
            <guid>39209469</guid>
            <pubDate>Wed, 31 Jan 2024 21:10:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2024/01/comcast-to-stop-calling-entire-network-10g-as-name-is-ruled-misleading/">https://arstechnica.com/tech-policy/2024/01/comcast-to-stop-calling-entire-network-10g-as-name-is-ruled-misleading/</a>, See on <a href="https://news.ycombinator.com/item?id=39209469">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h4>
      10G or not 10G    —
</h4>
            
            <h2 itemprop="description">Comcast said it will drop "Xfinity 10G Network" brand name after losing appeal. </h2>
                    </div><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2024/01/comcast-10g-800x528.jpg" alt="A Comcast router/modem gateway.">
      <figcaption><p>Comcast</p></figcaption>  </figure>

  




<!-- cache hit 181:single/related:5927e4533beb5ac868f3f9b9c0ce65dd --><!-- empty -->
<p>Comcast has reluctantly agreed to discontinue its "Xfinity 10G Network" brand name after losing an appeal of a ruling that found the marketing term was misleading. It will keep using the term 10G in other ways, however.</p>
<p>Verizon and T-Mobile both challenged Comcast's advertising of 10G, a term used by cable companies since it was unveiled in January 2019 by industry lobby group NCTA-The Internet &amp; Television Association. We <a href="https://arstechnica.com/information-technology/2019/01/envious-of-5g-hype-cable-cos-unveil-potentially-confusing-10g-trademark/">wrote in 2019</a> that the cable industry's 10G marketing was likely to confuse consumers and seemed to be a way of countering 5G hype generated by wireless companies.</p>
<p>10G doesn't refer to the 10th generation of a technology. It is a reference to potential 10Gbps broadband connections, which would be much faster than the actual speeds on standard cable networks today.</p>
<p>The challenges lodged against Comcast marketing were filed with the advertising industry's self-regulatory system run by BBB National Programs. BBB's National Advertising Division (NAD) <a href="https://arstechnica.com/tech-policy/2023/10/comcast-should-stop-advertising-slower-speeds-as-10g-industry-group-says/">ruled against Comcast</a> in October 2023, but Comcast appealed to the National Advertising Review Board (NARB).</p>
<p>The NARB <a href="https://bbbprograms.org/media-center/dd/comcast-10g-xfinity">announced its ruling</a> today, agreeing with the NAD that "Comcast should discontinue use of the term 10G, both when used in the name of the service itself ('Xfinity 10G Network') as well as when used to describe the Xfinity network. The use of 10G in a manner that is not false or misleading and is consistent with the panel decision is not precluded by the panel recommendations."</p>
<h2>“Comcast will discontinue brand name”</h2>
<p>Comcast agreed to make the change in an advertiser's statement that it provided to the NARB. "Although Comcast strongly disagrees with NARB's analysis and approach, Comcast will discontinue use of the brand name 'Xfinity 10G Network' and will not use the term '10G' in a manner that misleadingly describes the Xfinity network itself," Comcast said.</p>
<p>Comcast said it disagrees with "the recommendation to discontinue the brand name" because the company "makes available 10Gbps of Internet speed to 98 percent of its subscribers upon request." But those 10Gbps speeds aren't available in Comcast's typical service plans and require a fiber-to-the-home connection instead of a standard cable installation.</p>                                            
                                                        
<p>The Comcast "Gigabit Pro" fiber connection that provides 10Gbps speeds costs $299.95 a month plus a $19.95 modem lease fee. It also requires a $500 installation charge and a $500 activation charge.</p>
<p>Comcast said it may still use 10G in ways that are less likely to confuse consumers. "Consistent with the panel's recommendation... Comcast reserves the right to use the term '10G' or 'Xfinity 10G' in a manner that does not misleadingly describe the Xfinity network itself," the company said.</p>
<p>When contacted by Ars, a Comcast spokesperson said, "We disagree with the decision but are pleased that we have confirmed our continued use of 10G in advertising."</p>
<h2>Comcast claims “not supported”</h2>
<p>The NARB said the "recent availability of 10G speeds through [the Gigabit Pro] service tier does not support the superior speed claim (or a 10Gbps claim) for the Xfinity network as a whole." As the NARB noted, there is an "absence" of data showing how many Comcast customers actually use that service.</p>
<p>The NARB also said that 10G is misleading because of the implied comparison to 5G wireless networks.&nbsp;"The NARB panel concluded that 10G expressly communicates at a minimum that users of the Xfinity network will experience significantly faster speeds than are available on 5G networks," the announcement of the ruling said. "This express claim is not supported because the record does not contain any data comparing speeds experienced by Xfinity network users with speeds experienced by subscribers to 5G networks."</p>
<p>As the NAD has <a href="https://bbbprograms.org/media-center/dd/comcast-10g">previously stated</a>, 10G is more of an "aspirational" term rather than something that's offered over today's cable networks. Over the past five years, the NCTA has been using the term 10G to describe just about any improvement to cable networks, regardless of the actual speeds.</p>
<p>The NCTA coincidentally issued a <a href="https://www.ncta.com/whats-new/10g-five-years-on-the-future-is-even-closer">press release</a> yesterday hailing the fifth anniversary of its first 10G announcement. "Five years on, the future is even closer... Here in 2024, the promise of 10G is becoming more and more of a reality," the NCTA said.</p>
<p>The announcement listed some examples of multi-gigabit (but not 10-gigabit) cable speeds, some of which were only achieved in lab testing or demos. NCTA claimed that "10G can change lives" and that the "10G platform will facilitate the next great technological advancements in the coming decades, ensuring fast, reliable, and safe networks continue to power the American economy."</p>
<p>For all of you cable broadband users, just remember to ignore "10G" in cable-company marketing and check the actual speeds you're paying for.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FBI confirms it issued remote kill command to blow out Volt Typhoon's botnet (107 pts)]]></title>
            <link>https://www.theregister.com/2024/01/31/volt_typhoon_botnet/</link>
            <guid>39209352</guid>
            <pubDate>Wed, 31 Jan 2024 21:00:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2024/01/31/volt_typhoon_botnet/">https://www.theregister.com/2024/01/31/volt_typhoon_botnet/</a>, See on <a href="https://news.ycombinator.com/item?id=39209352">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>China's Volt Typhoon attackers used "hundreds" of outdated Cisco and NetGear routers infected with malware in an attempt to break into US critical infrastructure facilities, according to the Justice Department.</p>
<p>On Tuesday <a target="_blank" href="https://www.theregister.com/2024/01/30/fbi_china_volt/">news broke</a> that the Feds had blocked the malicious network that was set up on end-of-life, US-based small office/home office routers. Now more details have come out about how an FBI team infiltrated the attack and harvested the key data before remotely wiping the KV Botnet, according to four warrants (<a target="_blank" rel="nofollow" href="https://lnks.gd/l/eyJhbGciOiJIUzI1NiJ9.eyJidWxsZXRpbl9saW5rX2lkIjoxMDIsInVyaSI6ImJwMjpjbGljayIsInVybCI6Imh0dHBzOi8vd3d3Lmp1c3RpY2UuZ292L29wYS9tZWRpYS8xMzM2NDIxL2RsP2lubGluZT0mdXRtX21lZGl1bT1lbWFpbCZ1dG1fc291cmNlPWdvdmRlbGl2ZXJ5IiwiYnVsbGV0aW5faWQiOiIyMDI0MDEzMS44OTQzMDk5MSJ9.4LRHVZyUDYfMnRWC7s_-GUsAOhl4SewCG7GXGj4tAX8/s/3078670442/br/236285728656-l">5018</a>, <a target="_blank" rel="nofollow" href="https://lnks.gd/l/eyJhbGciOiJIUzI1NiJ9.eyJidWxsZXRpbl9saW5rX2lkIjoxMDMsInVyaSI6ImJwMjpjbGljayIsInVybCI6Imh0dHBzOi8vd3d3Lmp1c3RpY2UuZ292L29wYS9tZWRpYS8xMzM2NDE2L2RsP2lubGluZT0mdXRtX21lZGl1bT1lbWFpbCZ1dG1fc291cmNlPWdvdmRlbGl2ZXJ5IiwiYnVsbGV0aW5faWQiOiIyMDI0MDEzMS44OTQzMDk5MSJ9.8vmYc17z664gSNfykf8ncM2yRyRspFYF6SCIHiTZhb8/s/3078670442/br/236285728656-l">5530</a>, <a target="_blank" rel="nofollow" href="https://lnks.gd/l/eyJhbGciOiJIUzI1NiJ9.eyJidWxsZXRpbl9saW5rX2lkIjoxMDQsInVyaSI6ImJwMjpjbGljayIsInVybCI6Imh0dHBzOi8vd3d3Lmp1c3RpY2UuZ292L29wYS9tZWRpYS8xMzM2NDA2L2RsP2lubGluZT0mdXRtX21lZGl1bT1lbWFpbCZ1dG1fc291cmNlPWdvdmRlbGl2ZXJ5IiwiYnVsbGV0aW5faWQiOiIyMDI0MDEzMS44OTQzMDk5MSJ9.d-xmdG8esN2ZFRwqLXHNvwIo6ZpVS-2HQ5js3wwfJo8/s/3078670442/br/236285728656-l">5451</a> and <a target="_blank" rel="nofollow" href="https://lnks.gd/l/eyJhbGciOiJIUzI1NiJ9.eyJidWxsZXRpbl9saW5rX2lkIjoxMDUsInVyaSI6ImJwMjpjbGljayIsInVybCI6Imh0dHBzOi8vd3d3Lmp1c3RpY2UuZ292L29wYS9tZWRpYS8xMzM2NDExL2RsP2lubGluZT0mdXRtX21lZGl1bT1lbWFpbCZ1dG1fc291cmNlPWdvdmRlbGl2ZXJ5IiwiYnVsbGV0aW5faWQiOiIyMDI0MDEzMS44OTQzMDk5MSJ9.xD5SrU9r17bUh7t5QaI4TWGGTTTlIfn6D8a3QecwFJY/s/3078670442/br/236285728656-l">5432</a>) filed by the FBI in the Southern District Court of Texas last month and released today.</p>
<p>"China's hackers are targeting American civilian critical infrastructure, pre-positioning to cause real-world harm to American citizens and communities in the event of conflict," FBI Director Christopher Wray said in a <a target="_blank" rel="nofollow" href="https://www.justice.gov/usao-sdtx/pr/us-government-disrupts-botnet-peoples-republic-china-used-conceal-hacking-critical">statement</a>. "Volt Typhoon malware enabled China to hide as they targeted our communications, energy, transportation, and water sectors."</p>

    

<p>The Feds claim the Middle Kingdom keyboard warriors downloaded a virtual private network module to the vulnerable routers and set up an encrypted communication channel to control the botnet and hide their illegal activities. Specifically: Volt Typhoon used the US-based routers and IP addresses to <a target="_blank" href="https://www.theregister.com/2024/01/31/critical_infrastructure_hacking/">target</a> US critical infrastructure, we're told.</p>
<ul>

<li><a href="https://www.theregister.com/2024/01/30/fbi_china_volt/">US shorts China's Volt Typhoon crew targeting America's criticals</a></li>

<li><a href="https://www.theregister.com/2023/05/25/china_volt_typhoon_attacks/">Five Eyes and Microsoft accuse China of attacking US infrastructure again</a></li>

<li><a href="https://www.theregister.com/2024/01/31/critical_infrastructure_hacking/">We know nations are going after critical systems, but what happens when crims join in?</a></li>

<li><a href="https://www.theregister.com/2024/01/31/ivanti_patches_zero_days/">Ivanti releases patches for VPN zero-days, discloses two more high-severity vulns</a></li>
</ul>
<p>The warrants allowed law enforcement to remotely install software on the routers to search for, and then seize or copy, information about the illicit activity before wiping the malware from the compromised devices.</p>
<p>To do this — and to limit the cops' search to routers infected with the botnet — the FBI sent specific KV Botnet commands to compromised routers to collect "non-content information about those nodes," according to the warrants.</p>

        

<p>This includes the IP address, port numbers used by infected routers to communicate with other nodes, as well as IP addresses and ports used by each node's parent, and data on the command-and-control nodes.</p>
<p>"A router that is not infected by the KV Botnet malware would not receive or respond to this command," court documents claim.&nbsp;</p>

        

<p>The Feds, along with foreign agency partners in Five Eyes nations, <a target="_blank" href="https://www.theregister.com/2023/05/25/china_volt_typhoon_attacks/">first warned</a> about this threat in May 2023.</p>
<p>Also today, the US Cybersecurity Agency and FBI <a target="_blank" rel="nofollow" href="https://www.cisa.gov/news-events/alerts/2024/01/31/cisa-and-fbi-release-secure-design-alert-urging-manufacturers-eliminate-defects-soho-routers">issued an alert</a> urging manufacturers to eliminate defects in SOHO router web management interfaces. This, according to the agencies, includes automating update capabilities, locating the web management interface on LAN-side ports, and requiring a manual override to remove security settings. ®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I Moved My Blog from IPFS to a Server (306 pts)]]></title>
            <link>https://neimanslab.org/2024-01-31/why-i-moved-my-blog-ipfs-to-server.html</link>
            <guid>39208673</guid>
            <pubDate>Wed, 31 Jan 2024 20:07:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://neimanslab.org/2024-01-31/why-i-moved-my-blog-ipfs-to-server.html">https://neimanslab.org/2024-01-31/why-i-moved-my-blog-ipfs-to-server.html</a>, See on <a href="https://news.ycombinator.com/item?id=39208673">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <p><span>Written by</span>
    
        Neiman
    

    
      <br>
      <span>on&nbsp;</span><time datetime="2024-01-31 00:00:33 +0100">January 31, 2024</time>
    
  </p>

  
  

  <p>It’s safe to say I was a pioneer of IPFS + ENS websites. When I set up my first ENS+IPFS website in March 2019 there were no more than 15 others. Between 2019 to 2022 I co-built an IPFS+ENS browser extension (Almonit), an IPFS+ENS search engine (Esteroids), and of course, my personal blog was available only in IPFS+ENS.</p>

<p>But today I moved my blog back to a server, and I’d like to discuss why.</p>

<p>What got me excited about peer-to-peer websites like IPFS is that, theoretically, the more visitors a website has, the more robust, censorship-resistant, and scalable it is. Again, theoretically.</p>

<p>Do you know how popular torrent files seem to live forever? I wanted the same but for websites. I imagined a website that is hard to ddos (Robust), difficult to block (Censorship-resistant), and the more readers it has, the faster it is to use it since some readers help to spread the content (Scalable).</p>

<p>I imagined a website with a big “Pin Me” button (pinning in IPFS is like seeding in BitTorrent). If a reader presses the button they wil help serve the website.</p>

<p>In practice this didn’t work out really, and for several reasons.</p>

<ol>
  <li>
    <p>IPFS users mostly don’t run their own nodes or software. Instead, they use gateways. It’s an educated guess I’m making based on what I see in the community, and based on the fact that it’s quite an inconvenience to run your own IPFS node. But even if you do run your own node, the fact you access a website doesn’t mean you pin it. Not at all.</p>

    <p>This is a huge difference from BitTorrent where the only way to get content is to run your own software, and when you download something you also share it, by default.</p>

    <p>Hence, most readers will not help share the website, but even for the ones who will there are still extra complications:</p>
  </li>
  <li>
    <p>Websites are dynamic objects. Their content is being updated all the time. If you just pin the content of the current version of a website, that’s not much help.</p>

    <p>What most IPFS websites do is use a name system that points to the latest version of its content. It’s usually either IPNS, the internal name system of IPFS, or ENS, Ethereum Name System. But IPFS doesn’t include yet an easy command to always pin the latest content of IPNS, and if someone uses ENS, it means that whoever pins it also needs to listen to Ethereum blockchain events, a huge extra challenge on its own to do without a centralized service.</p>
  </li>
  <li>
    <p>To make things worse, it’s actually quite hard to get IPFS content to be available in the browser in a reliable way!</p>

    <p>For example, I wanted my IPFS blog to be available to all major gateways, all IPFS nodes, Brave browser (which supports IPFS natively), and js-libp2p &amp; helia (the js libraries of IPFS). I didn’t find a reliable way to achieve that on my own.</p>

    <p><strong>Long rant:</strong> I pinned content from my own server and played forever with settings and definitions, but couldn’t get the content to be available everywhere. Worse was with Helia, where I just couldn’t manage to get my content accessible from Helia within the browser without connecting directly to my own node. But then what’s p2p network about it?</p>

    <p>I found out that there’s a service, <a href="https://cid.contact/">cid.contact</a>, called a “Content Routing” service. It’s written in the “about section” of cid.contact it’s related to Filecoin, but for some reason, it holds routing data for IPFS, which as far as I know is a different network. The address cid.contact is hard-coded into Helia’s code in the version I used at least, and it was clear that if a content is indexed by cid.contact, then it’s reachable almost everywhere, but if it’s not indexed - it’s not reachable always.</p>

    <p>I couldn’t figure out how to index my content in cid.contact. Honestly, I’m not sure I wanted to. Because what’s the point? it seems to just add a dependency on a centralized service. I could try to run my own indexer and define it in my website, but again, centralization. What’s the economic model for these indexers? Which actors do we expect to run them in the long term?</p>

    <p>The text in cid.contact says that at the current size of IPFS, it’s unreasonable to expect the DHT to handle routing efficiently on its own. This kind of makes sense, but what’s the alternative to that, that doesn’t break up the technology pros?</p>
  </li>
</ol>

<p>By now I got tired of the constant struggle for my IPFS blog to function well. At least for a short while, I want a simple, classic working solution. The blog you’re reading now is built with Jekyll and is hosted on my own 10$ server.</p>

<p>don’t get me wrong, I’m still an IPFS fanboy. It’s a great project managed very well. It just doesn’t fit a personal blog needs yet.</p>

<p>That said, It’s difficult to follow the constant development and innovation of IPFS or Filecoin without this becoming a day job. Did I miss some trivial solution or a recent innovation? If yes, let me know. There are no comments here yet, but I am available via old-style email (neiman@hackerspace.pl), or in Mastodon (@neiman@mastodon.social).</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fighting Infomania: Why 80% of Your Reading Is a Waste of Time (2016) (102 pts)]]></title>
            <link>https://www.nateliason.com/blog/infomania</link>
            <guid>39208629</guid>
            <pubDate>Wed, 31 Jan 2024 20:03:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nateliason.com/blog/infomania">https://www.nateliason.com/blog/infomania</a>, See on <a href="https://news.ycombinator.com/item?id=39208629">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I returned to college Junior Fall dissatisfied. I’d just finished a summer internship doing management consulting with a government contracting firm, and in the process, realized that working in a company wasn’t for me.</p><p>The original plan had been to leverage it into a better internship somewhere else, and eventually a full-time offer, but with how much I disliked the work I knew something had to change.</p><p>Then I hit a snag: I wanted to get into startups or lifestyle businesses, but how was I supposed to&nbsp;<em>get there</em>?</p><p>With most corporate work you can go the traditional route of school -&gt; internship -&gt; job, but skill-based jobs at smaller companies don’t have such a clear route. And this lack of a clear route is what stops many students from going after&nbsp;<a href="https://www.nateliason.com/fun-work/">fun work</a>.</p><p>Faced with not knowing the way forward, or where to begin with “starting a startup,” I began reading&nbsp;<em>everything</em>.</p><p>Over the next 6 months, I&nbsp;<a href="https://www.nateliason.com/can-easily-read-100-books-year/" target="_blank">read 30+ books</a>&nbsp;on entrepreneurship, startups, marketing, “growth hacking,” and everything tangentially related I could find. And that doesn’t include the countless blog posts, articles, reddit threads, and whatever else I could get my hands on.</p><p>A good plan, right? No, 80% of it was a waste of time,&nbsp;<strong>and most people make the same mistake with how they consume information every day</strong>.</p><h2>Just in Time vs Just in Case</h2><p>Toyota’s ability to kill it in the car market came from their (then) revolutionary manufacturing process:&nbsp;<a href="https://en.wikipedia.org/wiki/Just-in-time_manufacturing" target="_blank">just in time manufacturing</a>.</p><p>While Detroit and other car companies would pump out cars “just in case” they were purchased, Toyota would make them “just in time” for their orders. This saved them millions of dollars by minimizing unused inventory, and the cost savings allowed them to speed up their manufacturing process significantly with new technology.</p><p>What Toyota realized was that by creating cars just in time, they could move faster, respond to market changes quicker, and only make cars that they knew they needed.</p><p>You should approach tactical knowledge the same way.</p><p>Sidebar: There’s an important distinction here between&nbsp;tactical&nbsp;knowledge (e.g. how to do search engine optimization), and&nbsp;philosophical&nbsp;knowledge (e.g. understanding our tendency towards&nbsp;biased thinking). The first, tactical knowledge, is what we’re concerned with overdosing on.</p><p>The school model focuses on just in case knowledge. You take classes and study textbooks that cram knowledge into you hoping that one day you might find it useful. But that’s not how&nbsp;<a href="https://www.nateliason.com/self-education/" target="_blank">learning works in the real world</a>.</p><h2>Where We Make “Just In Case” Mistakes in the Real World</h2><p>When I was working on&nbsp;<a href="https://www.nateliason.com/life-after-sumome/" target="_blank">marketing at SumoMe</a>, I never followed any marketing blogs. I didn’t check my Feedly when I got to work, didn’t look on GrowthHackers or Inbound to see what was trending, and didn’t have any newsletters I was subscribed to.</p><p>Conventional wisdom says that you should follow what people in your industry are talking about tactics-wise, but it’s just noise. You should have the strategy and tactics that you’re working on, and then you should execute on them. Constantly listening to and checking what other people are doing doesn’t help you stay focused–it just makes you question yourself.</p><p>The exception to this is if you can find a site that publishes articles very infrequently but where they are very high quality (for example,&nbsp;<a href="http://backlinko.com/" target="_blank">Backlinko</a>&nbsp;for SEO).</p><h3>The Problem of Frequency</h3><p>As a general rule, the more frequently a site publishes about tactics (marketing, personal finance, weight loss, etc.), the&nbsp;<em>less</em>&nbsp;you should listen to it.</p><p>No topic is sufficiently complex that you need new information on executing on it every day. Getting in shape requires doing a few very simple things every day for months, not finding a new 13 minute 6 step workout every day so you can have a butt like today’s hot celebrity.</p><p>That’s part of the reason I’ve deliberately avoided having a narrow theme to my writing. You don’t need an entire site on&nbsp;<a href="https://www.nateliason.com/last-longer-in-bed/" target="_blank">lasting longer in bed</a>&nbsp;or&nbsp;<a href="https://www.nateliason.com/5-day-water-fast-health-benefits/" target="_blank">water fasting</a>, you just need one or a couple really good articles.</p><h3>The Problem of Recency</h3><p>Then, there’s the issue with new articles. We tend towards&nbsp;<em>neomania:</em>&nbsp;overly focusing on the new and shiny, when new and shiny things tend to be the quickest to go and the least likely to be valuable.</p><p>A safer bet is to see what content has stood the test of time, following the&nbsp;<em>Lindy Rule.&nbsp;</em>Anything that has been around for 50 years will probably be around for another 50 years, but something that’s been around a few days has no proof of staying power.</p><p>Will people be reading Aristotle in another 2,000 years? Probably. Will they be reading this week’s pop-business book in another 10 years? Probably not.</p><h2>Why We Do It</h2><p>There are three reasons why we feel compelled to constantly consume knowledge about things we’re working on, and I’ll explain the problems with each.</p><h3>1. Habit</h3><p>We carry that “just in case” habit with us from school, and feel like we’re not doing our job if we’re not front-loading knowledge.</p><p>But it should be clear the fault here now. Just because we were taught that way doesn’t mean it’s the best way, it’s simply the way things have always been done.</p><h3>2. History</h3><p>Prior to 20 years ago, you wanted to err on the side of just in case knowledge since getting new information was so difficult.</p><p>That’s not the case anymore. Anything you could possibly want to learn you could figure out the basics of in an afternoon with a WiFi connection. You don’t have to worry about front loading everything because you’ll hardly ever be in a situation where you can’t look up the answers.</p><h3>3. Fauxductivity</h3><p>Then there’s the fact that doing it makes us feel like we’re being productive when we’re really just slacking off, a phenomenon I’m going to call “Fauxductivity.”</p><p>Any time you’re doing something that feels productive&nbsp;but doesn’t directly impact your most important goal, you’re being fauxductive. That includes bingeing on just in case knowledge, as well as checking email,&nbsp;<a href="https://www.nateliason.com/news-waste-time/" target="_blank">reading the news</a>, trying productivity tools, organizing your desktop, etc.</p><h2>1 Rule to Fight Infomania</h2><p>Here’s your new rule for information, especially blogs and popular non-fiction books:</p><p><em><strong>If it doesn’t answer a specific question you’re currently asking, cover philosophical knowledge, or entertain you, then don’t read it.</strong></em></p><p>Unfollow the industry blogs, stop reading the newspaper, don’t go on site aggregators, quit it. It’s all fauxductivity and wasting your time with useless information that will only cause&nbsp;<a href="https://www.nateliason.com/decision-fatigue/" target="_blank">decision paralysis</a>, self-doubt, and decreased confidence in your existing decisions.</p><p>When you have a specific question (e.g. how do I grow my Instagram following) that’s when you start digging through the blogs and industry material. But only until you have enough information to answer your question and formulate a plan. Don’t fill up your RSS with every social media and Instagram blog out there hoping you stumble on something else tomorrow.</p><p>If you can do this, you’ll spend significantly less time reading things that you never use, and you’ll get more information and mileage out of what you do read.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Some blind hackers are bridging IRC to LMMs running locally (187 pts)]]></title>
            <link>https://2mb.codes/~cmb/ollama-bot/</link>
            <guid>39208451</guid>
            <pubDate>Wed, 31 Jan 2024 19:50:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://2mb.codes/~cmb/ollama-bot/">https://2mb.codes/~cmb/ollama-bot/</a>, See on <a href="https://news.ycombinator.com/item?id=39208451">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>This is a rudimentary IRC bot that communicates with a local instance
of <a href="https://ollama.ai/" rel="nofollow noopener">ollama</a>.</p>
<h3 id="installation"><a href="#installation" rel="nofollow noopener">#</a>Installation</h3>
<p>You'll need to have <a href="https://ollama.ai/" rel="nofollow noopener">ollama</a> installed in order to
actually use the bot.  Instructions for doing that are out of scope for
this document.  Then, build the bot from source:</p>
<pre><code>git clone 'https://git.2mb.codes/~cmb/ollama-bot
cd ollama-bot
go build
</code></pre>
<p>Put the ollama-bot binary on your <code>$PATH</code>.
We run this on a Void Linux host.  Runit services for both the bot and
ollama itself are included in the ollama-bot repository, along with some
instructions for getting it running under runit.  If you use something else,
you're on your own, but of course, patches are welcome.</p>
<h3 id="configuration"><a href="#configuration" rel="nofollow noopener">#</a>Configuration</h3>
<p>The program searches for its configuration file in one of several places,
in the following order, and uses the first file it finds:</p>
<pre><code>~/.config/ollama-bot/ollama-bot.toml
/usr/local/etc/ollama-bot/ollama-bot.toml
/etc/ollama-bot/ollama-bot.toml
</code></pre>
<p>The configuration file should contain one or more network definitions.
I.E., it is an array of tables using toml notation.  Look at the file
<code>config.toml.sample</code> for a sample configuration.</p>
<h3 id="usage"><a href="#usage" rel="nofollow noopener">#</a>Usage</h3>
<p>Once the bot is connected to one or more IRC networks, you can converse
with it, either by highlighting it in a channel, or by sending it a private
message.  For the most part, you just send it free-form text, and it will
respond with free-form text.  However, it also accepts various commands,
preceded by an exclamation mark <code>!</code>.  Send <code>!help</code> for a list of commands.
Use of the commands is best demonstrated by an example chat transcript.</p>
<h4 id="chat-transcript"><a href="#chat-transcript" rel="nofollow noopener">#</a>Chat Transcript</h4>
<pre><code>&lt;chris&gt; ai-bot: !help
&lt;ai-bot&gt; 
&lt;ai-bot&gt; This bot supports the following commands:
&lt;ai-bot&gt; !describe URL [model] -- Describe image at URL, optionally choosing model
&lt;ai-bot&gt; !switch-text-model -- Start a conversation using a different text model
&lt;ai-bot&gt; !list-text-models -- List models usable for text conversation
&lt;ai-bot&gt; !list-image-models -- List models usable for image description
&lt;ai-bot&gt; !help -- This command.
&lt;chris&gt; ai-bot: !list-image-models
&lt;ai-bot&gt; This bot supports the following image models: llava, llava:13b, bakllava
&lt;chris&gt; ai-bot: !list-text-models
&lt;ai-bot&gt; This bot supports the following text models: mistral:instruct, llama2, llama2-uncensored
&lt;chris&gt; ai-bot: !describe https://the-brannons.com/chris-and-bird.jpg llava:13b
&lt;ai-bot&gt;  The image features a bearded man with a bird perched on his shoulder. The bird is small, likely a parrot or a similar type of pet bird. The man appears to be in a relaxed setting, possibly sitting down while enjoying the company with his feathered companion.
&lt;chris&gt; ai-bot: What kind of beard does he have?
&lt;ai-bot&gt;  While I cannot precisely describe the man's beard style without more specific information, it is safe to say that he has some form of facial hair on his face. This could range from a full beard to a goatee or even just a five o'clock shadow.
# Use the default model, bakllava.  This resets conversation state.
&lt;chris&gt; ai-bot: !describe https://the-brannons.com/chris-and-bird.jpg
&lt;ai-bot&gt; 
&lt;ai-bot&gt; The image features a bearded man with a bird perched on his shoulder. The man has an odd facial expression, almost as if he is looking into the camera. The bird appears to be a parrot and is sitting close to the man's ear.
&lt;ai-bot&gt; 
&lt;ai-bot&gt; The scene takes place in a room furnished with a couch and a chair. The couch is positioned in the background, while the chair occupies a smaller portion of the frame on the left side.
&lt;chris&gt; ai-bot: Tell me about his beard?
&lt;ai-bot&gt; 
&lt;ai-bot&gt; The bearded man in the image has an odd facial expression, making his appearance quite distinctive. His beard seems to be well-groomed and likely represents a fashionable or stylish choice in his personal grooming.
&lt;chris&gt; ai-bot: What do you mean by odd facial expression?
&lt;ai-bot&gt; 
&lt;ai-bot&gt; By odd facial expression, I refer to the man's facial features looking unnatural, exaggerated, or distorted. This may include an awkward smile, squinting, or an unusual or contorted countenance. The phrase "what the heck" implies that the person's appearance seems out of the ordinary or unexpected, which adds a humorous and quirky touch to the image.
# Switch back to text chat, selecting a model and resetting conversation state:
&lt;chris&gt; ai-bot: !switch-to-text-model llama2-uncensored
&lt;ai-bot&gt; Text model switched.
&lt;chris&gt; ai-bot: Somebody on the Internet told me that I can believe everything I read on the Internet?  Were they full of shit?
&lt;ai-bot&gt; Unfortunately, no. This statement is not true. There are many websites and sources on the internet that provide incorrect or misleading information. It's important to be careful when using any online source for information and to verify it by checking multiple sources or contacting a reliable expert.
&lt;chris&gt; ai-bot: But Wikipedia is always reliable!
&lt;ai-bot&gt; Wikipedia is an excellent resource, but it's not infallible. Like any encyclopedia, it can be edited and updated by anyone with access to the website. While most of the information on Wikipedia is accurate, there are some inaccuracies and errors that may exist due to human error or deliberate vandalism. It's always a good idea to cross-reference information from different sources before relying solely on any one source.
&lt;chris&gt; ai-bot: Could you tell me about some known examples of errors in Wikipedia?  Otherwise, I'm just gonna assume you're full of it.
&lt;ai-bot&gt; Sure! One well-known example of an error in Wikipedia is the article on Abraham Lincoln. The entry stated that Lincoln was the first president to receive a patent for his invention - which was actually issued after he left office. Another example is the article on the Apollo 11 mission, which claimed that Neil Armstrong's quote "One small step" was misquoted from an interview with President John F. Kennedy. However, NASA confirmed that the quote used in Wikipedia w
&lt;ai-bot&gt; as
&lt;ai-bot&gt; accurate and had been recorded during the mission.
</code></pre>
<h3 id="why"><a href="#why" rel="nofollow noopener">#</a>Why?</h3>
<p>Because most of the people in the group behind this are hard-core free software
people to some degree or other.  We want to see access to AI liberated and
democratized, I.E., free as in freedom.  And almost as importantly, it sounded
like fun when we were bored on a Friday afternoon.</p>
<h3 id="legal"><a href="#legal" rel="nofollow noopener">#</a>Legal</h3>
<p>Licensed as GPL-2 to match the license of <code>irc-go</code>.</p>
<h3 id="thanks"><a href="#thanks" rel="nofollow noopener">#</a>Thanks</h3>
<p>The group behind ollama-bot consists of Kirk, Tyler, Mike, and Chris, plus the peanut gallery on IRC.</p>
<p>We use the <a href="https://git.zx2c4.com/irc-go" rel="nofollow noopener">irc-go</a> package from Jason Donenfeld.
This seems to have been an excellent choice.</p>
<p>This project would not be possible without <a href="https://ollama.ai/" rel="nofollow noopener">ollama</a>.</p>
<p>Code hosting is graciously provided by the fine people at <a href="https://2mb.solutions/" rel="nofollow noopener">2MB Solutions</a>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mistral CEO confirms 'leak' of new open source AI model nearing GPT4 performance (353 pts)]]></title>
            <link>https://venturebeat.com/ai/mistral-ceo-confirms-leak-of-new-open-source-ai-model-nearing-gpt-4-performance/</link>
            <guid>39208213</guid>
            <pubDate>Wed, 31 Jan 2024 19:32:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://venturebeat.com/ai/mistral-ceo-confirms-leak-of-new-open-source-ai-model-nearing-gpt-4-performance/">https://venturebeat.com/ai/mistral-ceo-confirms-leak-of-new-open-source-ai-model-nearing-gpt-4-performance/</a>, See on <a href="https://news.ycombinator.com/item?id=39208213">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
		<section>
			
			<p><time title="2024-01-31T18:44:56+00:00" datetime="2024-01-31T18:44:56+00:00">January 31, 2024 10:44 AM</time>
			</p>
			
		</section>
		<div>
					<p><img width="750" height="420" src="https://venturebeat.com/wp-content/uploads/2024/01/cfr0z3n_overhead_view_of_the_eiffel_tower_rising_from_a_landsca_d374695a-bca7-4db7-abcd-d30ef5fd0e04.png?fit=750%2C420&amp;strip=all" alt="Overhead view of Eiffel tower in a Paris made of circuit boards."></p><p><span>Credit: VentureBeat made with Midjourney V6</span></p>		</div><!-- .article-media-header -->
	</div><div id="primary" role="main">

			<article id="post-2932274">
				<div>
					<p>The past few days have been a wild ride for the growing open source AI community — even by its fast-moving and freewheeling standards.</p>



<p>Here’s the quick chronology: on or about January 28, a user with the handle “Miqu Dev” <a href="https://huggingface.co/miqudev/miqu-1-70b" target="_blank" rel="noreferrer noopener">posted a set of files on HuggingFace</a>, the leading open source AI model and code sharing platform, that together comprised a seemingly new open source large language model (LLM) labeled “miqu-1-70b.”</p>



<p>The HuggingFace entry, which is still up at the time of this article’s posting, noted that new LLM’s “Prompt format,” how users interact with it, was the same as <a href="https://mistral.ai/">Mistral</a>, the <a href="https://www.ft.com/content/cf939ea4-d96c-4908-896a-48a74381f251" target="_blank" rel="noreferrer noopener">well-funded open source Parisian AI company</a> behind <a href="https://venturebeat.com/ai/mistral-shocks-ai-community-as-latest-open-source-model-eclipses-gpt-3-5-performance/" target="_blank" rel="noreferrer noopener">Mixtral 8x7b</a>, viewed by many to be the top performing open source LLM presently available, a fine-tuned and retrained version of Meta’s Llama 2. </p>



<h2 id="h-posted-on-4chan">Posted on 4chan</h2>



<p>The same day, an anonymous user on 4chan (possibly “Miqu Dev”) posted a <a href="https://boards.4chan.org/g/thread/98696032#p98697258" target="_blank" rel="noreferrer noopener">link to the miqu-1-70b files on 4chan</a>, the notoriously longstanding haven of online memes and toxicity, where users began to notice it.</p>



<p>Some took to X, Elon Musk’s social network formerly known as Twitter, to share the discovery of the model and what appeared to be its exceptionally high performance at common LLM tasks (measured by tests known as benchmarks), approaching the previous leader, OpenAI’s GPT-4 on the <a href="https://eqbench.com/" target="_blank" rel="noreferrer noopener">EQ-Bench</a>. </p>



<figure><div>
<blockquote data-width="500" data-dnt="true"><p lang="en" dir="ltr">Whatever Miqu is, it has some sort of special sauce. It gets an 83.5 on EQ-Bench (evaluated locally), surpassing *every other LLM in the world except GPT-4*. EQ-Bench has a 0.97 correlation w/ MMLU, and a 0.94 correlation w/ Arena Elo. It *beats* Mistral Medium – at Q4_K_M. I… <a href="https://t.co/0gOOPjxjPD">pic.twitter.com/0gOOPjxjPD</a></p>— N8 Programs (@N8Programs) <a href="https://twitter.com/N8Programs/status/1752441060133892503?ref_src=twsrc%5Etfw">January 30, 2024</a></blockquote>
</div></figure>



<figure><div>
<blockquote data-width="500" data-dnt="true"><p lang="en" dir="ltr">Might be late but I am now 100% convinced that Miqu is the same model that's accessible as Mistral-Medium on Perplexity Labs. It was plausible that it knows standard puzzles, but there ain't no way in Hell a pranker has tuned it to identically phrase the responses in Russian too. <a href="https://t.co/zZMcpspXch">pic.twitter.com/zZMcpspXch</a></p>— Teortaxes▶️ (@teortaxesTex) <a href="https://twitter.com/teortaxesTex/status/1752427812466593975?ref_src=twsrc%5Etfw">January 30, 2024</a></blockquote>
</div></figure>



<h2 id="h-mistral-quantized">Mistral quantized?</h2>



<p>Machine learning (ML) researchers took notice on <a href="https://www.linkedin.com/posts/maxime-labonne_new-open-source-llm-competes-with-gpt-activity-7158414579671199744-MFfA/" target="_blank" rel="noreferrer noopener">LinkedIn</a>, as well. </p>



<div><p><em>“Does ‘miqu’ stand for MIstral QUantized? We don’t know for sure, but this quickly became one of, if not the best open-source LLM,”</em> wrote Maxime Labonne, an ML scientist at JP Morgan &amp; Chase, one of the world’s largest banking and financial companies.<em> “Thanks to @152334H, we also now have a good unquantized version of miqu here: <a href="https://lnkd.in/g8XzhGSM">https://lnkd.in/g8XzhGSM</a></em></p><p><em>The investigation continues. Meanwhile, we might see fine-tuned versions of miqu outperforming GPT-4 pretty soon.</em>“</p></div>



<p><a href="https://huggingface.co/docs/optimum/concept_guides/quantization" target="_blank" rel="noreferrer noopener">Quantization</a> in ML refers to a technique used to make it possible to run certain AI models on less powerful computers and chips by replacing specific long numeric sequences in a model’s architecture with shorter ones. </p>



<p>Users speculated “Miqu” might be a new Mistral model being covertly “leaked” by the company itself into the world — especially since Mistral is known for <a href="https://venturebeat.com/ai/mistral-ai-bucks-release-trend-by-dropping-torrent-link-to-new-open-source-llm/" target="_blank" rel="noreferrer noopener">dropping new models and updates without fanfare</a> through esoteric and technical means — or perhaps an employee or customer gone rouge.</p>



<h2 id="h-confirmation-from-the-top">Confirmation from the top</h2>



<p>Well, today it appears we finally have confirmation of the latter of those possibilities: Mistral co-founder and CEO Arthur Mensch took to X to clarify: <em>“An over-enthusiastic employee of one of our early access customers leaked a quantised (and watermarked) version of an old model we trained and distributed quite openly…</em></p>



<p><em>To quickly start working with a few selected customers, we retrained this model from Llama 2 the minute we got access to our entire cluster — the pretraining finished on the day of Mistral 7B release. We’ve made good progress since — stay tuned!<a href="https://twitter.com/arthurmensch/status/1752737462663684344/history"></a>“</em></p>



<figure><div>
<blockquote data-width="500" data-dnt="true"><div lang="en" dir="ltr"><p>An over-enthusiastic employee of one of our early access customers leaked a quantised (and watermarked) version of an old model we trained and distributed quite openly.</p><p>To quickly start working with a few selected customers, we retrained this model from Llama 2 the minute we got…</p></div>— Arthur Mensch (@arthurmensch) <a href="https://twitter.com/arthurmensch/status/1752737462663684344?ref_src=twsrc%5Etfw">January 31, 2024</a></blockquote>
</div></figure>



<p>Hilariously, Mensch also appears to have taken to the illicit HuggingFace post not to demand a takedown, but leaving a comment that the poster “might consider attribution.”</p>



<figure></figure>



<p>Still, with Mensch’s note to “stay tuned!” it appears that not only is Mistral training a version of this so-called “Miqu” model that approaches GPT-4 level performance, but it may, in fact, match or exceed it, if his comments are to be interpreted generously. </p>



<h2 id="h-a-pivotal-moment-in-open-source-ai-and-beyond">A pivotal moment in open source AI and beyond?</h2>



<p>That would be a watershed moment not just for open source generative AI but the entire field of AI and computer science: <a href="https://openai.com/research/gpt-4" target="_blank" rel="noreferrer noopener">since its release back in March 2023</a>, GPT-4 has remained the most powerful and highest performing LLM in the world by most benchmarks. Not even any of <a href="https://venturebeat.com/ai/google-gemini-is-not-even-as-good-as-gpt-3-5-turbo-researchers-find/" target="_blank" rel="noreferrer noopener">Google’s presently available, long-rumored Gemini models</a> have been able to eclipse it — yet (according to some measures, the current Gemini models are <a href="https://venturebeat.com/ai/google-gemini-is-not-even-as-good-as-gpt-3-5-turbo-researchers-find/" target="_blank" rel="noreferrer noopener">actually worse than the older OpenAI GPT-3.5 mode</a>l). </p>



<p>The release of an open source GPT-4 class model, which would presumably be functionally free to use, would likely place enormous competitive pressure on OpenAI and its subscription tiers, especially as more enterprises look to open source models, or a mixture of open source and closed source, to power their applications, <a href="https://venturebeat.com/ai/how-enterprises-are-using-open-source-llms-16-examples/">as VentureBeat’s founder and CEO Matt Marshall recently reported</a>. OpenAI may retain the edge with its faster GPT-4 Turbo and GPT-4V (vision), but the writing on the wall is pretty clear: the open source AI community is catching up fast. Will OpenAI have enough of a head start, and a metaphorical “moat” with its <a href="https://venturebeat.com/ai/openai-launches-gpt-store-but-revenue-sharing-is-still-to-come/">GPT Store</a> and other features, to remain in the top spot for LLMs? </p>
<p><strong>VentureBeat's mission</strong> is to be a digital town square for technical decision-makers to gain knowledge about transformative enterprise technology and transact. <a href="https://venturebeat.com/newsletters/?utm_source=VBsite&amp;utm_medium=bottomBoilerplate" data-type="URL" data-id="/newsletters/">Discover our Briefings.</a></p><!-- Boilerplate CSS for "after" -->				</div><!-- .article-content -->

									
				
			</article><!-- #post-2932274 .article-wrapper -->


		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Building an early warning system for LLM-aided biological threat creation (109 pts)]]></title>
            <link>https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation</link>
            <guid>39207291</guid>
            <pubDate>Wed, 31 Jan 2024 18:15:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation">https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation</a>, See on <a href="https://news.ycombinator.com/item?id=39207291">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p><em>Note: As part of our </em><a href="http://openai.com/safety/preparedness" rel="noopener noreferrer" target="_blank"><em>Preparedness Framework</em></a><em>, we are investing in the development of improved evaluation methods for AI-enabled safety risks. We believe that these efforts would benefit from broader input, and that methods-sharing could also be of value to the AI risk research community. To this end, we are presenting some of our early work—today, focused on biological risk. We look forward to community feedback, and to sharing more of our ongoing research.&nbsp;</em></p></div><p><strong>Background. </strong>As OpenAI and other model developers build more capable AI systems, the potential for both beneficial and harmful uses of AI will grow. One potentially harmful use, highlighted by researchers and policymakers, is the ability for AI systems to assist malicious actors in creating biological threats (e.g., see <a href="https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/" rel="noopener noreferrer" target="_blank">White House 2023</a>, <a href="https://www.washingtontimes.com/news/2022/sep/12/ai-powered-biological-warfare-biggest-issue-former/" rel="noopener noreferrer" target="_blank">Lovelace 2022</a>, <a href="https://www.vox.com/future-perfect/23820331/chatgpt-bioterrorism-bioweapons-artificial-inteligence-openai-terrorism" rel="noopener noreferrer" target="_blank">Sandbrink 2023</a>). In one discussed hypothetical example, a malicious actor might use a highly-capable model to develop a step-by-step protocol, troubleshoot wet-lab procedures, or even autonomously execute steps of the biothreat creation process when given access to tools like <a href="https://www.theguardian.com/science/2022/sep/11/cloud-labs-and-remote-research-arent-the-future-of-science-theyre-here" rel="noopener noreferrer" target="_blank">cloud labs</a> (see <a href="https://www.nti.org/analysis/articles/the-convergence-of-artificial-intelligence-and-the-life-sciences/" rel="noopener noreferrer" target="_blank">Carter et al., 2023</a>). However, assessing the viability of such hypothetical examples was limited by insufficient evaluations and data.</p><p>Following our recently shared <a href="https://cdn.openai.com/openai-preparedness-framework-beta.pdf" rel="noopener noreferrer" target="_blank">Preparedness Framework</a>, we are developing methodologies to empirically evaluate these types of risks, to help us understand both where we are today and where we might be in the future. Here, we detail a new evaluation which could help serve as one potential “tripwire” signaling the need for caution and further testing of biological misuse potential. This evaluation aims to measure whether models could meaningfully increase malicious actors’ access to dangerous information about biological threat creation, compared to the baseline of existing resources (i.e., the internet).</p><p>To evaluate this, we conducted a study with 100 human participants, comprising (a) 50 biology experts with PhDs and professional wet lab experience and (b) 50 student-level participants, with at least one university-level course in biology. Each group of participants was randomly assigned to either a control group, which only had access to the internet, or a treatment group, which had access to GPT-4 in addition to the internet. Each participant was then asked to complete a set of tasks covering aspects of the end-to-end process for biological threat creation.<span><sup><span>[^1]</span></sup><!----></span> To our knowledge, this is the largest to-date human evaluation of AI’s impact on biorisk information.</p><p><strong>Findings.<em> </em></strong>Our study assessed uplifts in performance for participants with access to GPT-4 across five metrics (accuracy, completeness, innovation, time taken, and self-rated difficulty) and five stages in the biological threat creation process (ideation, acquisition, magnification, formulation, and release). We found mild uplifts in accuracy and completeness for those with access to the language model. Specifically, on a 10-point scale measuring accuracy of responses, we observed a mean score increase of 0.88 for experts and 0.25 for students compared to the internet-only baseline, and similar uplifts for completeness (0.82 for experts and 0.41 for students). However, the obtained effect sizes were not large enough to be statistically significant, and our study highlighted the need for more research around what performance thresholds indicate a meaningful increase in risk. Moreover, we note that information access alone is insufficient to create a biological threat, and that this evaluation does not test for success in the physical construction of the threats.</p><p>Below, we share our evaluation procedure and the results it yielded in more detail. We also discuss several methodological insights related to capability elicitation and security considerations needed to run this type of evaluation with frontier models at scale. We also discuss the limitations of statistical significance as an effective method of measuring model risk, and the importance of new research in assessing the meaningfulness of model evaluation results.</p></div></div>]]></description>
        </item>
    </channel>
</rss>