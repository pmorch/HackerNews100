<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 13 Jul 2024 20:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Disney's Internal Slack Breached? NullBulge Leaks 1.1 TiB of Data (130 pts)]]></title>
            <link>https://hackread.com/disneys-internal-slack-breached-nullbulge-leak-data/</link>
            <guid>40955693</guid>
            <pubDate>Sat, 13 Jul 2024 18:05:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hackread.com/disneys-internal-slack-breached-nullbulge-leak-data/">https://hackread.com/disneys-internal-slack-breached-nullbulge-leak-data/</a>, See on <a href="https://news.ycombinator.com/item?id=40955693">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				
<p>Hacktivist group NullBulge claims to have breached Disney, leaking 1.1 TiB of internal Slack data. The leak allegedly includes messages, files, code, and more. This comes amidst breaches affecting AT&amp;T and Ticketmaster.</p>


<p>A self-proclaimed hacktivist group named NullBulge, aiming to “protect artists’ rights and ensure fair compensation for their work,” claims to have breached Disney and leaked 1.1 TiB (1.2 TB) of the company’s internal Slack infrastructure. These claims were posted on the notorious cybercrime and hacker platform <strong><a href="https://hackread.com/breach-forums-return-clearnet-dark-web-fbi-seizure/" target="_blank" data-type="post" data-id="117113" rel="noreferrer noopener">Breach Forums</a></strong> on July 12, 2024.</p>



<p>The breach, which is yet to be verified, allegedly contains a complete copy of the company’s Slack communications used by their development team including messages, files, and other data exchanged within the Slack workspace.</p>



<p>The hackers further claim the dump includes “almost 10,000 channels, every message and file possible, unreleased projects, raw images, code, logins, links to internal API/web pages, and more!”</p>


<div>
<figure><a href="https://hackread.com/wp-content/uploads/2024/07/disneys-internal-slack-hacked-nullbulge-leak-data-1.jpg"><img decoding="async" width="1024" height="405" src="https://hackread.com/wp-content/uploads/2024/07/disneys-internal-slack-hacked-nullbulge-leak-data-1-1024x405.jpg" alt="Disney's Internal Slack Hacked? NullBulge Leaks 1.1 TiB of Data" srcset="https://hackread.com/wp-content/uploads/2024/07/disneys-internal-slack-hacked-nullbulge-leak-data-1-1024x405.jpg 1024w, https://hackread.com/wp-content/uploads/2024/07/disneys-internal-slack-hacked-nullbulge-leak-data-1-300x119.jpg 300w, https://hackread.com/wp-content/uploads/2024/07/disneys-internal-slack-hacked-nullbulge-leak-data-1-768x304.jpg 768w, https://hackread.com/wp-content/uploads/2024/07/disneys-internal-slack-hacked-nullbulge-leak-data-1-1536x608.jpg 1536w, https://hackread.com/wp-content/uploads/2024/07/disneys-internal-slack-hacked-nullbulge-leak-data-1-380x150.jpg 380w, https://hackread.com/wp-content/uploads/2024/07/disneys-internal-slack-hacked-nullbulge-leak-data-1-800x317.jpg 800w, https://hackread.com/wp-content/uploads/2024/07/disneys-internal-slack-hacked-nullbulge-leak-data-1-1160x459.jpg 1160w, https://hackread.com/wp-content/uploads/2024/07/disneys-internal-slack-hacked-nullbulge-leak-data-1.jpg 1713w" sizes="(max-width: 1024px) 100vw, 1024px"></a><figcaption>NullBulge on Breach Forums (Screenshot: Hackread.com)</figcaption></figure></div>


<p>NullBulge also used X (formerly Twitter) to <strong><a href="https://x.com/NullBulgeGroup/status/1811656487514546466" target="_blank" rel="noreferrer noopener nofollow">announce</a> </strong>the alleged hack, stating, “Disney has had their entire dev Slack dumped. 1.1 TiB of files and chat messages. Anything we could get our hands on, we downloaded and packaged up. Want to see what goes on behind the doors? Go grab it.”</p>


<h3 id="nullbulge-group-who-why-and-how"><strong>NullBulge Group: Who, Why, and How</strong></h3>



<p>The origins of the NullBulge Group are unknown. However, their official website claims the group aims to protect artists’ rights and ensure fair compensation for their work. <a href="https://x.com/ValeryMarchive/status/1811785640834383931" target="_blank" rel="noreferrer noopener nofollow"><strong>Rumours</strong></a> suggest that NullBulge might be linked to the LockBit ransomware gang, as they appear to be using LockBit’s leaked builder.</p>


<div>
<figure><a href="https://hackread.com/wp-content/uploads/2024/07/disneys-internal-slack-hacked-nullbulge-leak-data-2.jpg"><img decoding="async" width="889" height="685" src="https://hackread.com/wp-content/uploads/2024/07/disneys-internal-slack-hacked-nullbulge-leak-data-2.jpg" alt="Disney's Internal Slack Hacked? NullBulge Leaks 1.1 TiB of Data" srcset="https://hackread.com/wp-content/uploads/2024/07/disneys-internal-slack-hacked-nullbulge-leak-data-2.jpg 889w, https://hackread.com/wp-content/uploads/2024/07/disneys-internal-slack-hacked-nullbulge-leak-data-2-300x231.jpg 300w, https://hackread.com/wp-content/uploads/2024/07/disneys-internal-slack-hacked-nullbulge-leak-data-2-768x592.jpg 768w, https://hackread.com/wp-content/uploads/2024/07/disneys-internal-slack-hacked-nullbulge-leak-data-2-380x293.jpg 380w, https://hackread.com/wp-content/uploads/2024/07/disneys-internal-slack-hacked-nullbulge-leak-data-2-800x616.jpg 800w" sizes="(max-width: 889px) 100vw, 889px"></a><figcaption>NullBulge on Twitter (Screenshot: Hackread.com)</figcaption></figure></div>


<p>As for Disney, in recent years, the company has faced criticism and legal issues regarding the payment of fair shares to artists and writers. Prominent figures like Neil Gaiman have <strong><a href="https://www.slashfilm.com/623157/neil-gaiman-calls-out-disney-for-failing-to-pay-royalties-to-writers-and-artists/" target="_blank" rel="noreferrer noopener nofollow">highlighted</a> </strong>that Disney has stopped paying royalties to some writers and artists for works that include novelizations and graphic novels of Disney-owned properties. This issue affected various creators who worked on popular franchises such as “Star Wars” and “Alien.</p>



<p>The problem came into the spotlight when author Alan Dean Foster publicly stated that he had not received royalties for his “Star Wars” and “Alien” novels after Disney acquired the respective franchises. </p>



<p>Despite some high-profile settlements, many writers and artists <a href="https://wdwnt.com/2022/05/writers-and-artists-still-fighting-to-receive-overdue-royalties-from-disney/" target="_blank" rel="noreferrer noopener nofollow"><strong>continue to struggle</strong></a> to get their due payments. Organizations like the Science Fiction &amp; Fantasy Writers of America (SFWA) have been actively campaigning for these creators, forming task forces to pressure Disney into fulfilling its financial obligations​.</p>



<p>Hackread.com has reached out to Disney for comment. Meanwhile, VX-Underground, an online malware repository, <a href="https://x.com/vxunderground/status/1811783405127749647" target="_blank" rel="noreferrer noopener nofollow"><strong>tweeted</strong></a> that if proven legitimate, the hack could be the work of infostealer malware. </p>


<p>Nevertheless, the alleged data breach is just another in a series affecting companies based in the United States. On July 12, 2024, <strong><a href="https://hackread.com/att-data-breach-hackers-steal-call-text-records/" target="_blank" rel="noreferrer noopener">AT&amp;T announced</a> </strong>that hackers had stolen call records and text message logs of “nearly all” customers, impacting over 110 million Americans.</p>



<p>Meanwhile, the <a href="https://hackread.com/hackers-ticketmaster-data-breach-560m-users-sale/" target="_blank" data-type="post" data-id="117148" rel="noreferrer noopener"><strong>Ticketmaster data breach</strong></a> continues to cause headaches for Live Nation as hackers leaked <a href="https://hackread.com/hackers-leak-10-million-ticketmaster-tickets/" target="_blank" data-type="post" data-id="118713" rel="noreferrer noopener"><strong>10 million ticketing barcodes</strong> </a>related to top celebrities’ concerts. The hackers are demanding an $8 million ransom to stop future leaks.</p>







<ol>
<li><a href="https://hackread.com/disney-accounts-sold-on-dark-web-marketplaces/" target="_blank" rel="noreferrer noopener">Disney+ accounts being sold on dark web marketplaces</a></li>



<li><a href="https://hackread.com/vpn-security-entertainment-rising-digital-risks/" target="_blank" rel="noreferrer noopener">Security and Entertainment Essentials Amid Rising Digital Risks</a></li>



<li><a href="https://hackread.com/lego-marketplace-bricklink-hacked-website-down/" target="_blank" rel="noreferrer noopener">LEGO Market BrickLink Hacked; Site Down Amid Unusual Activity</a></li>



<li><a href="https://hackread.com/nickelodeon-data-leak-interview-with-ghostytongue/" target="_blank" rel="noreferrer noopener">Nickelodeon Data Leak Labeled ‘Old’: Interview with @GhostyTongue</a></li>



<li><a href="https://hackread.com/soap2day-shut-down-alternatives-free-legal-paid/" target="_blank" rel="noreferrer noopener">Soap2day Shuts Down Permanently – Free Legal and Paid Alternatives</a></li>
</ol>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Houston-area residents enter sixth day without power, air conditioning (148 pts)]]></title>
            <link>https://www.cnn.com/2024/07/13/weather/beryl-houston-texas-power-outages-heat-saturday/index.html</link>
            <guid>40954800</guid>
            <pubDate>Sat, 13 Jul 2024 16:01:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnn.com/2024/07/13/weather/beryl-houston-texas-power-outages-heat-saturday/index.html">https://www.cnn.com/2024/07/13/weather/beryl-houston-texas-power-outages-heat-saturday/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=40954800">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-image-variation="image" data-breakpoints="{&quot;image--eq-extra-small&quot;: 115, &quot;image--eq-small&quot;: 300}" data-uri="cms.cnn.com/_components/image/instances/clyjlfdum00003b6iga2ax6h2@published" data-name="GettyImages-2161787786.jpg" data-component-name="image" data-observe-resizes="" data-original-ratio="0.6655574043261231" data-original-height="1600" data-original-width="2404" data-url="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-2161787786.jpg?c=original" data-editable="lede" data-freewheel-lede="true">
       <picture><source height="383" width="680" media="(max-width: 479px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-2161787786.jpg?c=16x9&amp;q=h_383,w_680,c_fill/f_webp" type="image/webp"><source height="653" width="1160" media="(min-width: 480px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-2161787786.jpg?c=16x9&amp;q=h_653,w_1160,c_fill/f_webp" type="image/webp"><source height="605" width="1075" media="(min-width: 960px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-2161787786.jpg?c=16x9&amp;q=h_605,w_1075,c_fill/f_webp" type="image/webp"><source height="833" width="1480" media="(min-width: 1280px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-2161787786.jpg?c=16x9&amp;q=h_833,w_1480,c_fill/f_webp" type="image/webp"><img src="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-2161787786.jpg?c=16x9&amp;q=h_833,w_1480,c_fill" alt="The Coleman family sit together on their front porch as they get some air on Friday in Houston, Texas." onload="this.classList.remove('image__dam-img--loading')" onerror="imageLoadError(this)" height="1600" width="2404"></picture>
    </div><div data-editable="content" itemprop="articleBody" data-reorderable="content">
                    <p><cite>
      <span data-editable="location"></span>
      <span data-editable="source">CNN</span>
        &nbsp;—&nbsp;
    </cite>
</p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjkaapz000lfyqdd3d6h2qr@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Hundreds of thousands of southeast Texas residents are entering the sixth day in a row suffering brutal heat without air conditioning and many are scrambling to find cool shelters, food, safe drinking water and health care resources.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjkm9hi00053b6k1kv7twys@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            More than 760,000 power customers in southeast Texas <a href="https://poweroutage.us/area/state/texas" target="_blank">remain without power</a> after Hurricane Beryl slammed the Gulf Coast Monday, leaving at least 10 people dead in Texas, two dead in Vermont and one dead in Louisiana.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjkn0zb00083b6k0ykq9cir@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Hospitals, assisted living facilities, schools and water treatment plants are scraping for resources after the outages debilitated infrastructure across the region. That has led to mounting frustrations from residents that Houston’s main utility provider – CenterPoint Energy – was not more prepared for the storm and renewed concerns over the state’s power grid.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjm5ub9001h3b6kvj1gbp21@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Pregnant Houston resident Jordyn Rush, 32, worries for her health as the intense heat deprives her of sleep and her fridge sits empty while the outage drags on, making life difficult as she prepares to undergo a C-section in 12 days.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjm8ovd001j3b6kzs9xin8k@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “It’s been an absolute nightmare,” Rush said. “I’m on the verge of like a mental breakdown. At this point, I can’t tell you the last time I was able to get a full night of sleep because it’s so hot. The lack of sleep is definitely wearing on me.”
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjqk88s001c3b6jyagf4pt8@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            CenterPoint Energy says it’s hoping to restore power to an additional 350,000 customers by Sunday, according to a statement Friday evening. Still, <a href="https://www.cnn.com/2024/07/12/weather/houston-texas-power-outages-heat-friday/index.html">half a million Houston-area homes and businesses </a>may not have their power restored until next week, the utility company said, even as triple digit heat indices remain in the forecast.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjpqbys000l3b6jr7jxbeb7@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Lt. Gov. Dan Patrick criticized the utility in a <a href="https://x.com/LtGovTX/status/1811744100950982729" target="_blank">post </a>Friday, saying “we should not have roughly a million homes and businesses without power this far out.”
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjpjvns000g3b6jnaytfeux@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “We should not have had almost every traffic light down through yesterday, with many still down. We should not have had nine fire stations without power,” Patrick said. “Seniors in assisted living and nursing homes should have been more of a priority for power restoration.”
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjkn0zb000a3b6kz9fijhe2@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Since Beryl pounded the region, heat advisories have been issued every day for the greater Houston area, with high temperatures in the 90s and triple-digit heat indices. Combined with power outages, alarmingly dangerous consequences have ensued.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjkn0zb000b3b6k835ovpnr@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            As residents desperately try to cool their homes with generators, carbon monoxide poisoning has become a serious concern.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjqn5qh00003b6jpzjdk5oh@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            In Fort Bend County, a neighboring suburb to Houston, more than 41 people suffered carbon monoxide poisoning, Judge KP George said at a news conference Friday. In Harris County, at least two people have died from carbon monoxide poisoning and fire departments received more than 200 carbon monoxide poisoning calls in 24 hours, local officials said.
    </p>

  


    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjqro2500033b6jnkgghuzg@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The US Department of Health and Human Services on Friday declared a public health emergency for Texas.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjqrtpi00053b6jvag4gbt7@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “The combination of severe heat and limited access to electricity is dangerous, especially for vulnerable populations and those relying on electricity-dependent durable medical equipment and certain healthcare services,” said Dawn O’Connell, Assistant Secretary for Preparedness and Response, in a news release.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjkn0zb000c3b6kyyw14e7y@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Senior care facilities and residents who rely on electric medical devices are particularly at risk. A 71-year-old woman woman died near Crystal Beach after her oxygen machine ran out of battery power and her generator shut down. CrowdSource Rescue, a nonprofit primarily serving as a search-and-rescue group, identified roughly 120 senior living facilities that need assistance and helped deliver generators and supplies to 16 so far.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjkn0zb000d3b6ks90490w3@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            A dozen Houston area hospitals are in a state of “internal disaster” and more than 40 dialysis clinics are struggling with outages, Texas Emergency Management Chief&nbsp;Nim&nbsp;Kidd said in a news conference Thursday. Backed up hospitals prompted city officials to organize overflow beds in an indoor sports stadium, Lt. Gov. Patrick said Tuesday.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjqsxfn00083b6jehhkkyvn@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            In addition to health care facilities feeling the strain, residents at home are struggling to find food and water.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjkn0zb000e3b6kfi7wxnq7@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Countless families have lost food in their warming fridges as many stores are shuttered, leaving government offices, food banks and other public services scrambling to distribute food to underserved areas. Scores of homes are also without drinking water as storm damage and power outages have left 135 wastewater treatment plants offline, according to Kidd.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjmbtz900013b6kjzpexzcn@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            For Rush, who has gestational diabetes, it’s been difficult to stay on the strict diet she needs to be on.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjmbuvh00033b6kvxlkxyrr@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “I have to go in next week to do a pre-anesthesia lab draw and I’m hoping that my labs aren’t crazy from like a lack of nutrition and water,” she said. “My heart burn is out of control. I feel like I’m getting dehydrated.”
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjpx973000o3b6jy8ifpiij@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Beryl swept through the Houston area in an unusually early start to hurricane season, knocking out power for more than 2 million customers, only about two months after a <a href="https://www.cnn.com/2024/05/17/weather/flooding-south-storms-houston-friday/index.html">powerful derecho </a>in the area damaged skyscrapers, downed transmission towers and left downtown dark.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyk3jfjp00013b6kfbxefnyp@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The Federal Emergency Management Agency granted Harris County and 14 other Texas counties financial assistance for impacted families as they face substantial repairs for homes damaged or destroyed by Hurricane Beryl.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjn1ffs00093b6ke0bm6det@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            In the meantime, officials are racing to find ways to help residents beat the sweltering heat.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjqwsud000a3b6jucb6bk91@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Galveston, located south&nbsp;of&nbsp;Houston, is using transit buses to give some residents an “air conditioned respite” as portions of the city remain without power after Hurricane Beryl, according to City Manager Brian Maxwell. The city has also been distributing ice and providing portable showers. In <a href="https://www.facebook.com/photo/?fbid=882807483888748&amp;set=a.244913831011453" target="_blank">Fort Bend County</a>, emergency medical services is providing oxygen refills, electricity to power oxygen concentrators and rides to cooling centers.
    </p>

<div data-image-variation="image" data-breakpoints="{&quot;image--eq-extra-small&quot;: 115, &quot;image--eq-small&quot;: 300}" data-uri="cms.cnn.com/_components/image/instances/clyjlfqw100023b6ip1b1pri1@published" data-name="GettyImages-2160977658.jpg" data-component-name="image" data-observe-resizes="" data-original-ratio="0.6666666666666666" data-original-height="1600" data-original-width="2400" data-url="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-2160977658-20240713035442198.jpg?c=original" data-editable="settings">
       <picture><source height="1600" width="2400" media="(min-width: 1280px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-2160977658-20240713035442198.jpg?q=w_1110,c_fill/f_webp" type="image/webp"><source height="1600" width="2400" media="(min-width: 960px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-2160977658-20240713035442198.jpg?q=w_1015,c_fill/f_webp" type="image/webp"><source height="1600" width="2400" media="(min-width: 480px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-2160977658-20240713035442198.jpg?q=w_1160,c_fill/f_webp" type="image/webp"><source height="1600" width="2400" media="(max-width: 479px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-2160977658-20240713035442198.jpg?q=w_680,c_fill/f_webp" type="image/webp"><img src="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-2160977658-20240713035442198.jpg?q=w_1110,c_fill" alt="CenterPoint foreign assistance crews work to restore power lines on Thursday in Houston, Texas." onload="this.classList.remove('image__dam-img--loading')" onerror="imageLoadError(this)" height="1600" width="2400" loading="lazy"></picture>
    </div>

  

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjpzb53000q3b6j9shmcvtd@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The crisis comes more than three years after massive power outages hit the state. In February 2021, a <a href="https://www.cnn.com/2021/02/21/weather/texas-winter-storm-timeline/index.html">deep freeze </a>killed more than 200 people and left millions of customers without power and heat for days. Now, the ongoing outages have turned attention back to the state’s power grid.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjksvpf000q3b6k4vlgf53i@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Gov. Greg Abbott has requested an investigation into CenterPoint Energy and other electric companies in the wake of the outages, Patrick said at a news conference Thursday.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjktbf9000u3b6ke32uyt4z@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Patrick on Friday criticized CenterPoint Energy’s communication with customers and <a href="https://x.com/LtGovTX/status/1811744100950982729" target="_blank">said</a> they should have been ready for any storm hitting the Houston area.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjktbf9000w3b6k95r5z06m@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “People have a right to be extremely frustrated with CenterPoint. People are suffering through terribly oppressive heat, a lack of food and gasoline availability, debris everywhere, and much more,” Patrick said. “The poor and most vulnerable are suffering the most.”
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjktbf9000x3b6kyv8whz23@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            CenterPoint in a <a href="https://www.centerpointenergy.com/en-us/corporate/about-us/connections/25" target="_blank">statement</a> Friday said it had been tracking Beryl’s trajectory and preparing for its impact nine days before it made landfall. The company said it had increased crew members from 3,000 to 12,000 in response to the storm.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjoy7ml00053b6klxfj76i3@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            CenterPoint CEO Jason Wells told the <a href="https://www.houstonchronicle.com/business/energy/article/centerpoint-outage-wells-ceo-19567556.php" target="_blank" rel="noopener noreferrer">Houston Chronicle</a> he is proud of the progress the company has made toward restoration but acknowledged customers’ frustrations about not knowing when they will have power again.
    </p>

  


    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjktbfa000y3b6k6oasa9my@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “I think we could do a better job of communicating expectations with our customers, and I personally own that,” Wells said.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjntsyp000b3b6kdgeju7s0@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            During a news conference Thursday, Patrick was asked by a reporter whether the state is considering modifying the power grid, which&nbsp;<a href="https://www.cnn.com/2023/06/22/politics/texas-power-grid-heat-what-matters/index.html">has encountered several issues over the years</a>&nbsp;due to severe weather.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjntu1l000d3b6kf0d8bji0@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “The grid is a whole different issue which we’re addressing, have been addressing, and will continue to address,” Patrick said. “The power is down because the lines are down, and the transmission lines are down primarily because trees fell on them.”
    </p>

<div data-image-variation="image" data-breakpoints="{&quot;image--eq-extra-small&quot;: 115, &quot;image--eq-small&quot;: 300}" data-uri="cms.cnn.com/_components/image/instances/clyjoxdzc00033b6kdsy90w5s@published" data-name="GettyImages-2161789296.jpg" data-component-name="image" data-observe-resizes="" data-original-ratio="0.6655574043261231" data-original-height="1600" data-original-width="2404" data-url="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-2161789296.jpg?c=original" data-editable="settings">
       <picture><source height="1600" width="2404" media="(min-width: 1280px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-2161789296.jpg?q=w_1110,c_fill/f_webp" type="image/webp"><source height="1600" width="2404" media="(min-width: 960px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-2161789296.jpg?q=w_1015,c_fill/f_webp" type="image/webp"><source height="1600" width="2404" media="(min-width: 480px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-2161789296.jpg?q=w_1160,c_fill/f_webp" type="image/webp"><source height="1600" width="2404" media="(max-width: 479px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-2161789296.jpg?q=w_680,c_fill/f_webp" type="image/webp"><img src="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-2161789296.jpg?q=w_1110,c_fill" alt="Larry Nelson's unpowered AC unit sits in the window seal of his home in the Third Ward neighborhood on Friday in Houston, Texas." onload="this.classList.remove('image__dam-img--loading')" onerror="imageLoadError(this)" height="1600" width="2404" loading="lazy"></picture>
    </div>

  

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjkv5rf00173b6kw00usvm2@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Houston resident Destinee Rideaux, 33, is displaced for the second time after a hurricane – and this time it’s because her home is too hot to live in.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjo4314000i3b6kvuiqc87l@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Because her apartment gets as hot as 96 degrees during the day, she’s been forced to stay at different friends’ apartments each night and carry around her belongings in her car. And Rideaux says she’s “exhausted.”
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjo7btv000k3b6kapy7e6xn@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            It brings back her memories of being displaced from her home in Iowa, Louisiana, during Hurricane Laura in 2020, she said.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjo8lrf000m3b6kzzvjhef0@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “We evacuated the day she hit. The next morning, we were able to get footage of the home and it was flattened. It was picked up and thrown across the yard and flattened. And everything we had was thrown across the yard,” she told CNN.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjohjyb00003b6klu0x6v6o@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            For Rush, the crisis is a repeat of the derecho in May, when her home didn’t have power for six days.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjol08300023b6kbqcqo61i@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            ﻿“We had to clear our fridge completely for the second time in less than two months. It’s our second time in less than two months of being displaced from our home,” Rush said. “We are left without any communication or updates, so we have no idea what kind of plans we need to make or if we need to make preparations to find a hotel because most of the hotels here are completely booked.”
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjo90qw000o3b6kwmzjzdem@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            After Rideaux’s Louisiana house was destroyed, she stayed with friends in the Houston area, and then decided to stay there permanently for a fresh start.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjod660000q3b6kf43p7ez9@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “It’s frustrating to be back in this position of just displacement,” Rideaux&nbsp;said. “During Laura, every day I didn’t know what that day would bring, where I was going to go. You also have the mental thing going on of like becoming a burden on other people.”
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjoea5a000s3b6kaz2gje3e@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            But Rideaux said this time isn’t nearly as difficult because she’ll be able to return to her home eventually.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clyjofmz8000u3b6ku2e0s1xe@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “Once the power comes back on, I have a home to go to,” she said.
    </p>

  <p data-uri="cms.cnn.com/_components/footnote/instances/clyjot2sv00003b6kgysgji8s@published" data-editable="text" data-article-gutter="true">
  CNN’s Kara Mihm, Sarah Dewberry, Ed Lavandera,&nbsp;Ashley Killough and Elizabeth Wolfe contributed to this report.
</p>

                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA["Firefox added [ad tracking] and has already turned it on without asking you" (186 pts)]]></title>
            <link>https://mastodon.social/@mcc/112775362045378963</link>
            <guid>40954535</guid>
            <pubDate>Sat, 13 Jul 2024 15:08:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mastodon.social/@mcc/112775362045378963">https://mastodon.social/@mcc/112775362045378963</a>, See on <a href="https://news.ycombinator.com/item?id=40954535">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[The Illustrated AlphaFold (136 pts)]]></title>
            <link>https://elanapearl.github.io/blog/2024/the-illustrated-alphafold/</link>
            <guid>40954497</guid>
            <pubDate>Sat, 13 Jul 2024 15:00:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://elanapearl.github.io/blog/2024/the-illustrated-alphafold/">https://elanapearl.github.io/blog/2024/the-illustrated-alphafold/</a>, See on <a href="https://news.ycombinator.com/item?id=40954497">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <h2 id="introduction">Introduction</h2> <h3 id="who-should-read-this">Who should read this</h3> <p>Do you want to understand exactly how AlphaFold3 works? The architecture is quite complicated and the description in the paper can be overwhelming, so we made a much more friendly (but just as detailed!) visual walkthrough.</p> <p>This is mostly written for an ML audience and multiple points assume familiarity with the steps of attention. If you’re rusty, see Jay Alammar’s <a href="https://jalammar.github.io/illustrated-transformer/" rel="external nofollow noopener" target="_blank">The Illustrated Transformer</a> for a thorough visual explanation. That post is one of the best explanations of a model architecture at the level of individual matrix operations and also the inspiration for the diagrams and naming.</p> <p>There are already many great explanations of the motivation for protein structure prediction, the CASP competition, model failure modes, debates about evaluations, implications for biotech, etc. so we don’t focus on any of that. Instead we explore the <em>how</em>.</p> <p><em>How are these molecules represented in the model and what are all of the operations that convert them into a predicted structure?</em></p> <p>This is probably more exhaustive than most people are looking for, but if you want to understand all the details and you like learning via diagrams, this should help :)</p> <h3 id="architecture-overview">Architecture Overview</h3> <p>We’ll start by pointing out that goals of the model are a bit different than previous AlphaFold models: instead of just predicting the structure of individual protein sequences (AF2) or protein complexes (AF-multimeter), it predicts the structure of a protein, optionally complexed with other proteins, nucleic acids, or small molecules, all from sequence alone. So while previous AF models only had to represent sequences of standard amino acids, AF3 has to represent more complex input types, and thus there is a more complex featurization/tokenization scheme. Tokenization is described in its own section, but for now just know that when we say “token” it either represents a single amino acid (for proteins), nucleotide (for DNA/RNA), or an individual atom if that atom is not part of a standard amino acid/nucleotide.</p> <div> <p><b>Interactive Table of Contents</b></p> <figure> <div> <p><img src="https://elanapearl.github.io/assets/img/af3_post/full_arch_for_labeling.png" alt="Full Architecture" usemap="#image-map"></p>              </div> <map name="image-map"> <area shape="rect" alt="Tokenization" title="Tokenization" coords="0.58,50.38,8.35,74.06" href="#tokenization" data-target="overlay-1"> <area shape="rect" alt="Retrieval" title="Retrieval" coords="8.35,3.26,22.16,40.98" href="#retrieval-create-msa-and-templates" data-target="overlay-2"> <area shape="rect" alt="Create atom-level representations" title="Create atom-level representations" coords="8.35,41.48,22.16,68.92" href="#create-atom-level-representations" data-target="overlay-3"> <area shape="rect" alt="Atom Transformer" title="Atom Transformer" coords="22.23,23.43,28.16,69.80" href="#update-atom-level-representations-atom-transformer" data-target="overlay-4"> <area shape="rect" alt="Atom-Level to Token-Level" title="Atom-Level to Token-Level" coords="28.16,40.23,31.89,69.80" href="#aggregate-atom-level--token-level" data-target="overlay-5"> <area shape="rect" alt="Template module" title="Template module" coords="34.56,34.59,40.56,56.02" href="#template-module" data-target="overlay-6"> <area shape="rect" alt="MSA module" title="MSA module" coords="42.15,34.21,48.16,56.02" href="#msa-module" data-target="overlay-7"> <area shape="rect" alt="Pairformer" title="Pairformer" coords="49.67,34.21,63.52,69.80" href="#pairformer-module" data-target="overlay-8"> <area shape="rect" alt="Diffusion module" title="Diffusion module" coords="68.37,41.48,90.20,70.18" href="#diffusion-module" data-target="overlay-9"> <area shape="rect" alt="Confidence and Loss" title="Confidence and loss" coords="90.96,20.05,99.71,47.37" href="#4-loss-function-and-other-training-details" data-target="overlay-10"> <area shape="rect" alt="Other training details" title="Other training details" coords="90.96,47.37,99.71,66.17" href="#other-training-details" data-target="overlay-11"> <area shape="rect" alt="Input preparation" title="Input preparation" coords="8.35,87.84,23.46,97.74" href="#1-input-preparation" data-target="overlay-12"> <area shape="rect" alt="Representation learning" title="Representation learning" coords="38.76,87.84,58.97,97.49" href="#2-representation-learning" data-target="overlay-13"> <area shape="rect" alt="Structure prediction" title="Structure prediction" coords="71.69,88.22,89.37,96.74" href="#3-structure-prediction" data-target="overlay-14"> </map> </figure>   <p>Full architecture. If you click on any part of the architecture, it will take you to that section of the post. If you resize the page, you might need to refresh to keep the interactive part working. (Diagram modified from AF3 paper)</p> </div> <p> Throughout the post, we highlight where you are in this diagram so you don't get lost! </p> <p>The model can be broken down into 3 main sections:</p> <ol> <li> <a href="#1-input-preparation"><strong>Input Preparation</strong></a> The user provides sequences of some molecules to predict structures for and these need to be embedded into numerical tensors. Furthermore, the model retrieves a collection of other molecules that are presumed to have similar structures to the user-provided molecules. The input preparation step identifies these molecules and also embeds these as their own tensors.</li> <li> <a href="#2-representation-learning"><strong>Representation learning</strong></a> Given the Single and Pair tensors created in section 1, we use many variants of attention to update these representations.</li> <li> <a href="#3-structure-prediction"><strong>Structure prediction</strong></a> We use these improved representations, and the original inputs created in section 1 to predict the structure using conditional diffusion.</li> </ol> <p> Skip to a specific section by via its name here or the by clicking the relevant part of the architecture in the diagram above. </p> <p>We also have additional sections describing 4. <a href="#4-loss-function-and-other-training-details"><strong>the loss function, confidence heads, and other relevant training details</strong></a> and 5. <a href="#ml-musings"><strong>some thoughts on the model from an ML trends perspective</strong></a>.</p> <h3 id="notes-on-the-variables-and-diagrams">Notes on the variables and diagrams</h3> <p>Throughout the model a protein complex is represented in two primary forms: the “single” representation which represents all the tokens in our protein complex, and a “pair” representation which represents the relationships (e.g. distance, potential interactions) between all pairs of amino acids / atoms in the complex. Each of these can be represented at an atom-level or a token-level, and will always be shown with these names (as established in the AF3 paper) and colors:</p> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/single_and_pair_rep-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/single_and_pair_rep-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/single_and_pair_rep-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/single_and_pair_rep.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure>  </div> <ul> <li>The diagrams abstract away the model weights and only visualize how the shapes of activations change</li> <li>The activation tensors are always labeled with the dimension names used in the paper and the sizes of the diagrams vaguely aim to follow when these dimensions grow/shrink. <d-footnote>The hidden dimension names usually start with "c" for "channel". For reference the main dimensions used are c<sub>z</sub>=128, c<sub>m</sub>=64, c<sub>atom</sub>=128, c<sub>atompair</sub>=16, c<sub>token</sub>=768, c<sub>s</sub>=384.</d-footnote> </li> <li>Whenever possible, the names above the tensors in this (and every) diagram match the names of the tensors use in the AF3 supplement. Typically, a tensor maintains its name as it goes through the model. However, in some cases, we use different names to distinguish between versions of a tensor at different stages of processing. For example, in the atom-level single representation, <strong><span>c</span></strong> represents the initial atom-level single representation while <strong><span>q</span></strong> represents the updated version of this representation as it progresses through the Atom Transformer.</li> <li>We also ignore most of the LayerNorms for simplicity but they are used <em>everywhere</em>.</li> </ul> <hr> <h2 id="1-input-preparation">1. Input Preparation</h2> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/input_prep-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/input_prep-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/input_prep-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/input_prep.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>The actual input a user provides to AF3 is the sequence of one protein and optionally additional molecules. The goal of this section is to convert these sequences into a series of 6 tensors that will be used as the input to the main trunk of the model as outlined in this diagram. These tensors are <strong><span>s</span></strong>, our token-level single representation, <strong><span>z</span></strong>, our token-level pair representation, <strong><span>q</span></strong>, our atom-level single representation, <strong><span>p</span></strong>, our atom-level pair representation, <strong><span>m</span></strong>, our MSA representation, and <strong><span>t</span></strong>, our template representation.</p> <p>This section contains:</p> <ul> <li> <a href="#tokenization"><strong>Tokenization</strong></a> describes how molecules are tokenized and clarifyies the difference between atom-level and token-level</li> <li> <a href="#retrieval-create-msa-and-templates"><strong>Retrieval (Create MSA and Templates)</strong></a> expalains why and how we include additional inputs to the model. It creates our MSA (<strong><span>m</span></strong>) and structure templates (<strong><span>t</span></strong>).</li> <li> <a href="#create-atom-level-representations"><strong>Create Atom-Level Representations</strong></a> creates our first atom-level representations <strong><span>q</span></strong> (single) and <strong><span>p</span></strong> (pair) and includes information about generated conformers of the molecules.</li> <li> <a href="#update-atom-level-representations-atom-transformer"><strong>Update Atom-Level Representations (Atom Transformer)</strong></a> is the main “Input Embedder” block, also called the “Atom Transformer”, which gets repreated 3 times and updates the atom-level single representation (<strong><span>q</span></strong>). The building blocks introduced here (<a href="#1-adaptive-layernorm"><strong>Adaptive LayerNorm</strong></a>, <a href="#2-attention-with-pair-bias"><strong>Attention with Pair Bias</strong></a>, <a href="#3-conditioned-gating"><strong>Conditioned Gating</strong></a>, and <a href="#4-conditioned-transition"><strong>Conditioned Transition</strong></a>) are also relevant later in the model.</li> <li> <a href="#aggregate-atom-level--token-level"><strong>Aggregate Atom-Level -&gt; Token-Level</strong></a> takes our atom-level representations (<strong><span>q</span></strong>, <strong><span>p</span></strong>) and aggregates all the atoms that at part of multi-atom tokens to create token-level representations <strong><span>s</span></strong> (single) and <strong><span>z</span></strong> (pair) and includes information from the MSA (<strong><span>m</span></strong>) and any user-provided information about known bonds that involve ligands.</li> </ul> <h2 id="tokenization">Tokenization</h2> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/summaries/tokenize-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/summaries/tokenize-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/summaries/tokenize-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/summaries/tokenize.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>See where this fits into the full architecture</p> </div> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/tokens-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/tokens-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/tokens-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/tokens.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>In AF2, as the model only represented proteins with a fixed set of amino acids, each amino acid was represented with its own token. This is maintained in AF3, but additional tokens are also introduced for the additional molecule types that AF3 can handle:</p> <ul> <li>Standard amino acid: 1 token (as per AF2)</li> <li>Standard nucleotide: 1 token</li> <li>Non-standard amino acids or nucleotides (methylated nucleotide, amino acid with post-translational modification, etc.): 1 token <em>per atom</em> </li> <li>Other molecules: 1 token <em>per atom</em> </li> </ul> <p>As a result, we can think of some tokens (like those for amino acids) as being associated with multiple atoms, while other tokens (like those for an atom in a ligand) are associated with only a single atom. So, while a protein with 35 standard amino acids (likely &gt; 600 atoms) would be represented by 35 tokens, a ligand with 35 atoms would also be represented by 35 tokens.</p> <h2 id="retrieval-create-msa-and-templates">Retrieval (Create MSA and Templates)</h2> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/summaries/retrieval-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/summaries/retrieval-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/summaries/retrieval-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/summaries/retrieval.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>See where this fits into the full architecture</p> </div> <p>One of the key early steps in AF3 is something akin to Retrieval Augmented Generation <a href="https://aws.amazon.com/what-is/retrieval-augmented-generation" rel="external nofollow noopener" target="_blank">RAG</a> in language models. We find similar sequences to our protein and RNA sequences of interest (collected into a multiple sequence alignment, “MSA”), and any structures related to those (called the “templates”), then include them as additional inputs to the model called <strong><span>m</span></strong> and <strong><span>t</span></strong>, respectively.</p> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/MSA_and_templates-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/MSA_and_templates-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/MSA_and_templates-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/MSA_and_templates.jpg" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>(Image from AF2)</p> </div> <details> <summary>Why do we want to include MSA and templates?</summary> <p> Versions of the same protein found in different species can be quite structurally and sequentially similar. By aligning these together into a Multiple Sequence Alignment (MSA), we can look at how an individual position in a protein sequence has changed throughout evolution. You can think about an MSA for a given protein as a matrix where each row is the sequence of the analogous protein from a different species. It has been shown that the conservation patterns found along the column of a specific position in the protein can reflect how critical it is for that position to have certain amino acids present, and the relationships between different columns reflect relationships between amino acids (i.e. if two amino acids are physically interacting, the changes in their amino acids will likely be correlated across evolution). Thus, MSAs are often used to enrich representations of single proteins.</p> <p> Similarly, if any of these proteins have known structures, those are also likely to inform the structure of this protein. Instead of searching for full structures, only individual chains of the proteins are used. This resembles the practice of homology modeling, in which the structure of a query protein is modeled based on templates from known protein structures that are presumed to be similar. </p> </details> <details> <summary>So how are these sequences and structures retrieved?</summary> First, a genetic search is done searching for any protein or RNA chains that resemble any input protein or RNA chains. This does not involve any training and relies upon existing Hidden Markov Model (HMM) based methods<d-footnote>Specifically, they use jackhmmer, HHBlits, and nhmmer</d-footnote> to scan multiple protein databases and RNA databases for relevant hits. Then these sequences are aligned to each other to construct an MSA with N<sub>MSA</sub> sequences. As the computational complexity of the model scales with N<sub>MSA</sub> they limit this to N<sub>MSA</sub> &lt; 2<sup>14</sup>. Typically, MSAs are constructed from individual protein chains but, as described in <a href="https://www.biorxiv.org/content/10.1101/2021.10.04.463034v2.full.pdf" rel="external nofollow noopener" target="_blank">AF-multimer</a>, instead of just concatenating the separate MSAs together into a block diagonal matrix, certain chains from the same species can be 'paired' as described <a href="https://www.biorxiv.org/content/10.1101/240754v3.full.pdf" rel="external nofollow noopener" target="_blank">here</a>. This way, the MSA does not have to be as large and sparse, and evolutionary information can be learned about relationships between chains. <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/multi_chain_MSA-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/multi_chain_MSA-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/multi_chain_MSA-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/multi_chain_MSA.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> Then, for each protein chain, they use another HMM-based method (hmmsearch) to find sequences in the Protein Data Bank (PDB) that resemble the constructed MSA. The highest quality structures are selected and up to 4 of these are sampled to be included as "templates". </details> <p>The only new part of these retrieval steps compared to AF-multimer is the fact that we now do this retrieval for RNA sequences in addition to protein sequences. Note that this is not traditionally called “retrieval” as the practice of using structural templates to guide protein structure modeling has been common practice in the field of <a href="https://en.wikipedia.org/wiki/Homology_modeling" rel="external nofollow noopener" target="_blank">homology modeling</a> long before the term RAG existed. However, even though AlphaFold doesn’t explicitly refer to this process as retrieval, it does quite resemble what has now been popularized as RAG.</p> <p><strong>How do we represent these templates?</strong></p> <p>From our template search, we have a 3D structure for each of our templates and information about which tokens are in which chains. First, the euclidean distances between all pairs of tokens in a given template are calculated. For tokens associated with multiple atoms, a representative <span>“center atom”</span> is used to calculate distances. This would be the <span>C<sub>ɑ</sub></span> atom for amino acids and <span>C<sup>1</sup>’</span> atom for standard nucleotides.</p> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/center_atoms-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/center_atoms-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/center_atoms-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/center_atoms.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Highlighting <span>"center atoms"</span> in single-token building blocks</p> </div> <p>This generates a N<sub>token</sub> x N<sub>token</sub> matrix for each template. However, instead of representing each distance as a numerical value, the distances get discretized into a “distogram” (a histogram of distances).<d-footnote>Specifically, the values are binned into 38 bins between 3.15A and 50.75A and there's 1 additional bin for any distances bigger than that.</d-footnote></p> <p>To each distogram, we then append metadata about which chain <d-footnote>In molecular complexes, a chain refers to a distinct molecule or part of a molecule. This can be a protein chain (a sequence of amino acids), a DNA or RNA chain (a sequence of nucleotides), or other biomolecules. AlphaFold uses chain information to differentiate between parts of a complex, helping it predict how these parts interact to form the overall structure</d-footnote> each token belongs to, whether this token was resolved in the crystal structure, and information about local distances within each amino acid. We then mask out this matrix such that we only look at distances within each chain (e.g., we ignore the distances between chain A and chain B) as they “make no attempt to select templates… to gain information about inter-chain interactions”‘<d-footnote> It is not specified why, but note that while there is no inter-chain interactions in the templates, they do incorporate them the MSA construction. </d-footnote>.</p> <h2 id="create-atom-level-representations">Create Atom-Level Representations</h2> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/summaries/make-atom-level-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/summaries/make-atom-level-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/summaries/make-atom-level-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/summaries/make-atom-level.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>See where this fits into the full architecture</p> </div> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/make_atom_rep-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/make_atom_rep-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/make_atom_rep-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/make_atom_rep.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>To create <strong><span>q</span></strong>, our atom-level single representation, we need to pull all our atom-level features. The first step is to calculate a “reference conformer” for each amino acid, nucleotide, and ligand. While we do not yet know the structure of the entire complex, we have strong priors on the local structures of each individual component. A conformer (short for <a href="https://www.sciencedirect.com/topics/chemistry/conformational-isomer#:~:text=Conformations%20or%20conformational%20isomers%20have,the%20same%20configuration%2C%20if%20chiral." rel="external nofollow noopener" target="_blank">conformational isomer</a>) is a 3D arrangement of atoms in a molecule that is generated by sampling rotations about single bonds. Each amino acid has a “standard” conformer which is just one of the low-energy conformations this amino acid can exist in, which can be retrieved through a look-up. However, each small molecule requires its own conformation generation. These are generated with <a href="https://rdkit.org/docs/RDKit_Book.html#conformer-generation" rel="external nofollow noopener" target="_blank">RDKit’s ETKDGv3</a>, an algorithm that combines experimental data and torsion angle preferences to produce 3D conformers.</p> <p>Then we concatenate the information from this conformer (relative location) with each atom’s charge, atomic number, and other identifiers. Matrix <strong><span>c</span></strong> stores this information for all the atoms in our sequences<d-footnote>In the AF3 supplement, the atom-level matrices (<b><span>c</span></b>, and <b><span>q</span></b>) are typically referred to in their vector forms (<i>e.g.</i> <b><span>c<sub>l</sub></span></b> or <b><span>c<sub>m</sub></span></b>), where l and m are used to index atoms.</d-footnote>. We then use <strong><span>c</span></strong> to initialize our atom-level pair representation <strong><span>p</span></strong> to store the relative distances between atoms. Because we only know reference distances within each token, we use a mask (<strong>v</strong>) to ensure this initial distance matrix only represents distances we’ve calculated in the conformer generation. We also include a linear embedding of the inverse square of the distances, add to it a projection of <strong><span>c<sub>l</sub></span></strong> and <strong><span>c<sub>m</sub></span></strong>, and update this with a few more linear layers with residual connections<d-footnote>The AF3 paper doesn't really clarify why this additional inverse distance step is performed or contain ablations for their effect of it; so, as with many of the steps we will discuss, we can only assume they were empirically shown to be useful.</d-footnote><d-footnote>In the AF3 supplement, the <b><span>p</span></b> tensor is typically referred to in its vector form <b><span>p<sub>l,m</sub></span></b> (where this represents the relationship between atom l and atom m).</d-footnote>.</p> <p>Finally, we make a copy of our atom-level single representation, calling this copy <strong><span>q</span></strong>. This matrix <strong><span>q</span></strong> is what we will be updating going forward, but <strong><span>c</span></strong> does get saved and used later.</p> <h2 id="update-atom-level-representations-atom-transformer">Update Atom-Level Representations (Atom Transformer)</h2> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/summaries/atom-transformer-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/summaries/atom-transformer-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/summaries/atom-transformer-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/summaries/atom-transformer.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>See where this fits into the full architecture</p> </div> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/atom_transformer-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/atom_transformer-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/atom_transformer-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/atom_transformer.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>Having generated <strong><span>q</span></strong> (representation of all the atoms) and <strong><span>p</span></strong> (representation of each pair of atoms), we now want to update these representations based on other atoms nearby. Anytime AF3 applies attention at the atom-level, we use a module called the Atom Transformer. The atom transformer is a series of blocks that use attention to update <strong><span>q</span></strong> using both <strong><span>p</span></strong> and the original representation of <strong><span>q</span></strong> called <strong><span>c</span></strong>. As <strong><span>c</span></strong> does not get updated by the Attention Transformer, it can be thought of as a residual connection to the starting representation.</p> <p>The Atom Transformer mostly follows a standard transformer structure using layer norm, attention, then an MLP transition. However, each step has been adapted to include additional input from <strong><span>c</span></strong> and <strong><span>p</span></strong> (including a secondary input here is sometimes referred to as “conditioning”.) There is also a ‘gating’ step between the attention and MLP blocks. Going through each of these 4 steps in more detail:</p> <h3 id="1-adaptive-layernorm">1. Adaptive LayerNorm</h3>  <p>Adaptive LayerNorm (AdaNorm) is a variant of LayerNorm with one simple extension. Recall that for a given input matrix, traditional LayerNorm learns two parameters (a scaling factor gamma and a bias factor beta) that adjust the mean and standard deviation of each of the channels in our matrix. Instead of learning fixed parameters for gamma and beta, AdaNorm learns a function to generate gamma and beta adaptively based on the input matrix. However, instead of generating the parameters based on the input getting re-scaled (in the Atom Transformer this is <strong><span>q</span></strong>), a secondary input (<strong><span>c</span></strong> in the Atom Transformer) is used to predict the gamma and beta that re-scale the mean and standard deviation of <strong><span>q</span></strong>.</p> <h3 id="2-attention-with-pair-bias">2. Attention with Pair Bias</h3> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/atom_attn_w_pair_bias-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/atom_attn_w_pair_bias-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/atom_attn_w_pair_bias-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/atom_attn_w_pair_bias.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>Atom-Level Attention with Pair-Bias can be thought of as an extension of self-attention. Like in self-attention, the queries, keys, and values all come from the same 1D sequence (our single representation, <strong><span>q</span></strong>). However, there are 3 differences:</p> <ol> <li> <p><strong>Pair-biasing</strong>: after the dot product of the queries and keys are calculated, a linear projection of the pair representation is added as a bias to scale the attention weights. Note that this operation does not involve any information from <strong><span>q</span></strong> being used to update <strong><span>p</span></strong>, just one way flow from the pair representation to <strong><span>q</span></strong>. The reasoning for this is that atoms that have a stronger pairwise relationship should attend to each other more strongly and <strong><span>p</span></strong> is effectively already encoding an attention map.</p> </li> <li> <p><strong>Gating</strong>: In addition to the queries, keys, and values, we create an additional projection of <strong><span>q</span></strong> that is passed through a sigmoid, to squash the values between 0 and 1. Our output is multiplied by this “gate” right before all the heads are re-combined. This effectively forces the model to ignore some of what it learned in this attention process. This type of gating appears frequently in AF3 and is discussed more in the ML-musings section. To briefly elaborate, because the model is constantly adding the outputs of each section to the residual stream, this gating mechanism can be thought of as the model’s way to specify what information does or does not get saved in this residual stream. It is presumably named a “gate” after the similar “gates” in LSTM which uses a sigmoid to learn a filter for what inputs get added to the running cell state.</p> </li> <li> <p><strong>Sparse attention</strong>:</p> </li> </ol> <table> <tbody><tr> <td> </td> <td> Because the number of atoms can be much larger than the number of tokens, we do not run full attention at this step, rather, we use a type of sparse attention (called Sequence-local atom attention) in which the attention is effectively run in local groups where groups of 32 atoms at a time can all attend to 128 other atoms. Sparse attention patterns are more thoroughly described <a href="https://medium.com/@vishal09vns/sparse-attention-dad17691478" rel="external nofollow noopener" target="_blank">elsewhere on the internet</a>. </td> <td> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/sparse_attn_pattern-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/sparse_attn_pattern-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/sparse_attn_pattern-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/sparse_attn_pattern.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </td> </tr> </tbody></table> <h3 id="3-conditioned-gating">3. Conditioned Gating</h3> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/conditioned_gating-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/conditioned_gating-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/conditioned_gating-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/conditioned_gating.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>We apply another gate to our data, but this time the gate is generated from our origin atom-level single matrix, <strong><span>c</span></strong><d-footnote>As with so many steps, it is unclear why it is done this way and what the benefit of conditioning on the original representation <b><span>c</span></b> does as opposed to learning the gate from the primary single representation <b><span>q</span></b></d-footnote>.</p> <h3 id="4-conditioned-transition">4. Conditioned Transition</h3> <p>This step is equivalent to the MLP layers in a transformer, and is called “conditioned” because the MLP is sandwiched in between Adaptive LayerNorm (Step 1 of Atom Transformer) and Conditional Gating (Step 3 of Atom Transformer) which both depend on <strong><span>c</span></strong>.</p> <p>The only other piece of note in this section is that AF3 uses SwiGLU in the transition block instead of ReLU. The switch from ReLU → SwiGLU happened with AF2 → AF3 and has been a common change in many recent architectures so we visualize it here.</p> <p>With a ReLU-based transition layer (as in AF2), we take the activations, project them up to 4x the size, apply a ReLU, then down-project them back to their original size. When using SwiGLU (in AF3), the input activation creates two intermediate up-projections, one of which goes through a swish non-linearity (improved variant of ReLU), then these are multiplied before down-projecting. The diagram below shows the differences:</p> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/swiglu-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/swiglu-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/swiglu-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/swiglu.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h2 id="aggregate-atom-level--token-level">Aggregate Atom-Level → Token-Level</h2> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/summaries/atom-to-token-level-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/summaries/atom-to-token-level-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/summaries/atom-to-token-level-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/summaries/atom-to-token-level.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>See where this fits into the full architecture</p> </div> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/aggregate_atom_to_token-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/aggregate_atom_to_token-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/aggregate_atom_to_token-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/aggregate_atom_to_token.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>While the data so far has all been stored at an atom-level, the representation learning section of AF3 from here onwards operates at the token-level. To create these token-level representations, we first project our atom-level representation to a larger dimension (c<sub>atom</sub>=128, c<sub>token</sub>=384). Then, we take the mean over all atoms assigned to the same token. Note that this only applies to the atoms associated with standard amino acids and nucleotides (by taking the mean across all atoms attached to the same token), while the rest remain unchanged<d-footnote>The AF3 paper describes these molecule types as having a representative atom per token (the center atom). Recall that this is the C<sub>α</sub> atom for amino acids and C<sup>1</sup>' atom for standard nucleotides. So while we mostly consider this reduced representation as "token space", we can also think of each token as representing a single atom (either a representative C<sub>α</sub>/C<sup>1</sup>' atom or an individual atom).</d-footnote>.</p> <p>Now that we are working in “token space”, we concatenate our token-level features and statistics from our MSA (where available)<d-footnote>e.g., The amino acid type (dim = 32), distribution of amino acids at this position in our MSA (dim = 32), and the deletion mean at this token (dim = 1) from our MSA. Note that these values will be zero for ligand atoms not associated with an MSA.</d-footnote>. This matrix, <strong><span>s<sup>inputs</sup></span></strong>, having grown a bit from these concatenations, is projected back down to c<sub>token</sub>, and called <strong><span>s<sup>init</sup></span></strong>: the starting representation of our sequence that will be updated in the representation learning section. Note that <strong><span>s<sup>init</sup></span></strong> gets updated in the representation learning section, but <strong><span>s<sup>inputs</sup></span></strong> are saved to be used later in the structure prediction section.</p> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/make_token_pair-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/make_token_pair-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/make_token_pair-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/make_token_pair.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>Now that we have created <strong><span>s<sup>init</sup></span></strong>, our initialized single representation, the next step is to initialize our pair representation <strong><span>z<sup>init</sup></span></strong>. The pair representation is a three dimensional tensor, but it’s easiest to think of it as a heatmap-like 2D matrix with an implicit depth dimension of c<sub>z</sub>=128 channels. So, entry <strong><span>z<sub>i,j</sub></span></strong> of our pair representation is a c<sub>z</sub> dimensional vector meant to store information about the relationship between token i and token j in our token sequence. We have created an analogous atom-level matrix <strong><span>p</span></strong>, and we follow a similar process here at the token-level.</p> <p>To initialize <strong><span>z<sub>i,j</sub></span></strong>, we use a linear projection to make the channel dimension of our sequence representation match that of the pair representation (384 → 128) and add the resulting <strong><span>s<sub>i</sub></span></strong> and <strong><span>s<sub>j</sub></span></strong>. To this, we add a relative positional encoding, <strong><span>p<sub>i,j</sub></span></strong><d-footnote>This encoding consists of a<sup>rel_pos</sup>, a one-hot encoding of the offset of the two token ids in token space (or set to a maximum of 65 if the two tokens are not on the same chain), a<sup>rel_token</sup>, a one-hot encoding of the offset of the two token ids in token space (or set to a maximum of 65 if the tokens are part of different amino acids or nucleotides), and a<sup>rel_chain</sup>, encoding the offset of the two chains the tokens are on. We project this concatenated encoding into the dimensionality of <b><span>z</span></b> too.</d-footnote>. If the user has also specified particular bonds between tokens, those are linearly embedded here and added to that entry in the pair representation.</p> <p>Now we’ve successfully created and embedded all of the inputs that will be used in the rest of our model:</p> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/input_prep_summary-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/input_prep_summary-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/input_prep_summary-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/input_prep_summary.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>For Step 2, we will set aside the atom-level representations (<strong><span>c</span></strong>, <strong><span>q</span></strong>, <strong><span>p</span></strong>) and focus on updating our token-level representations <strong><span>s</span></strong> and <strong><span>z</span></strong> in the next section (with the help of <strong><span>m</span></strong> and <strong><span>t</span></strong>).</p> <h2 id="2-representation-learning">2. Representation Learning</h2> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/rep_learning_arch-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/rep_learning_arch-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/rep_learning_arch-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/rep_learning_arch.jpg" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>(Diagram modified from full AF3 architecture diagram)</p> </div> <p>This section is the majority of the model, often referred to as the “trunk”, as it is where most of the computation is done. We call it the representation learning section of the model, as the goal is to learn improved representations of our token-level “single” (<strong><span>s</span></strong>) and “pair” (<strong><span>z</span></strong>) tensors initialized above. <d-footnote> Recall that we refer to the "single" sequence representations, these are not necessarily the sequence of one protein, but rather the concatenated sequence of all the atoms or tokens in our structure (which could contain multiple separate molecules).</d-footnote></p> <p>This section contains:</p> <ol> <li> <strong>Template module</strong> updates <strong><span>z</span></strong> using the structure templates <strong><span>t</span></strong> </li> <li> <strong>MSA module</strong> first updates the MSA <strong><span>m</span></strong>, then adds it to the token-level pair representation <strong><span>z</span></strong>. In this section we spend significant time on two operations: <ul> <li> <a href="#outer-product-mean">The Outer Product Mean</a> enables <strong><span>m</span></strong> to influence <strong><span>z</span></strong> </li> <li> <a href="#row-wise-gated-self-attention-using-only-pair-bias">MSA Row-wise Gated Self-Attention Using Only Pair Bias</a> updates <strong><span>m</span></strong> based on <strong><span>z</span></strong> and is a simplified version of attention with pair-bias (intended for MSAs)</li> </ul> </li> <li> <strong>Pairformer</strong> updates <strong><span>s</span></strong> and <strong><span>z</span></strong> with geometry-inspired (triangle) attention. This section mostly describes the triangle operations (used extensively throughout both AF2 and AF3). <ul> <li> <a href="#why-look-at-triangles">Why look at triangles?</a> explains some intuition for the triangle operations</li> <li> <a href="#triangle-updates">Triangle Updates</a> and <a href="#triangle-attention">Triangle Attention</a> both update <strong><span>z</span></strong> using methods similar to self-attention, but inspired by the triangle inequality</li> <li> <a href="#single-attention-with-pair-bias">Single Attention With Pair Bias</a> updates <strong><span>s</span></strong> based on <strong><span>z</span></strong> and is the token-level equivalent of attention with pair-bias (intended for single sequences)</li> </ul> </li> </ol> <p>Each individual block is repeated multiple times, and then the output of the whole section is fed back into itself again as input and the process is repeated (this is called recycling).</p> <h2 id="template-module">Template Module</h2> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/summaries/templates-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/summaries/templates-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/summaries/templates-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/summaries/templates.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>See where this fits into the full architecture</p> </div> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/template_module-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/template_module-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/template_module-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/template_module.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>Each template (N<sub>templates</sub>=2 in the diagram) goes through a linear projection and is added together with a linear projection of our pair representation (<strong><span>z</span></strong>). This newly combined matrix goes through a series of operations called the Pairformer Stack (described in depth later). Finally, all of the templates are averaged together and go through - you guessed it - another linear layer.<d-footnote>This is both called the template module and template embedder depending on where you look in the AF3 supplement, but they seem to just refer to the same thing.</d-footnote> Interestingly, this last linear layer has a ReLU as the non-linearity which wouldn’t be particularly notable except for the fact that it is one of only two places ReLU is used as the non-linearity in AF3. As always, can only hypothesize as to why this was selected.</p> <h2 id="msa-module">MSA Module</h2> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/summaries/msa-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/summaries/msa-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/summaries/msa-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/summaries/msa.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>See where this fits into the full architecture</p> </div> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/msa_module-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/msa_module-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/msa_module-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/msa_module.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Architecture of MSA Module. {Diagram from AF3}</p> </div> <p>This module greatly resembles what was called “Evoformer” in AF2, and the goal of it is to simultaneously improve the MSA and pair representations. It does a series of operations independently on these two representations then also enables cross-talk between them.</p> <p>The first step is to subsample the rows of the MSA, rather than use all rows of the MSA previously generated (which could be up to 16k), then add a projected version of our single representation to this subsampled MSA.</p> <h3 id="outer-product-mean">Outer Product Mean</h3> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/outer_product_mean-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/outer_product_mean-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/outer_product_mean-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/outer_product_mean.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>Next, we take the MSA representation and incorporate it into the pair representation via the “Outer Product Mean”. Comparing two columns of the MSA reveals information about the relationships between two positions in the sequence (<em>e.g.</em> how correlated are these two positions in the sequence across evolution). For each pair of token indices i,j, we iterate over all evolutionary sequences, taking the outer product of <strong><span>m<sub>s,i</sub></span></strong> and <strong><span>m<sub>s,j</sub></span></strong>, then averaging these across all the evolutionary sequences. We then flatten this outer product, project it back down, and add this to the pair representation <strong><span>z<sub>i,j</sub></span></strong> (full details in diagram). While each outer product only compares values <em>within</em> a given sequence <strong><span>m<sub>s</sub></span></strong>, when we take the mean of these, that mixes information <em>across</em> sequences. <em>This is the only point in the model where information is shared across evolutionary sequences.</em> This is a significant change to reduce the computational complexity of the Evoformer in AF2.</p> <h3 id="row-wise-gated-self-attention-using-only-pair-bias">Row-wise gated self-attention using only pair bias</h3> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/row_wise_gated_self_attn-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/row_wise_gated_self_attn-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/row_wise_gated_self_attn-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/row_wise_gated_self_attn.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>Having updated the pair representation based on the MSA, the model next updates the MSA based on the pair representation. This specific update pattern is called <strong>row-wise gated self attention <em>using only pair bias</em></strong>, and is a simplified version of <strong>self attention <em>with pair bias</em></strong>, discussed in the Atom Transformer section, applied to every sequence (row) in the MSA independently. It is inspired by attention, but instead of using queries and keys to determine what other positions each token should attend to, we just use the existing relationships between tokens stored in our pair representation <strong><span>z</span></strong>.</p> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/attn_score_from_bias-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/attn_score_from_bias-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/attn_score_from_bias-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/attn_score_from_bias.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>In the pair representation, each <strong><span>z<sub>i,j</sub></span></strong> is a vector containing information about the relationship between tokens i and j. When the tensor <strong><span>z</span></strong> gets projected down to a matrix, each <strong><span>z<sub>i,j</sub></span></strong> vector becomes a scalar that can be used to determine how much token i should attend to token j. After applying row-wise softmax, these are now equivalent to attention scores, which are used to create a weighted average of the values as a typical attention map would.</p> <p>Note that there is no information shared across the evolutionary sequences in the MSA as it is run independently for each row.</p> <h3 id="updates-to-pair-representation">Updates to pair representation</h3> <p>The last step of the MSA module is to update the pair representation through a series of steps referred to as triangle updates and attention. These triangle operations are described below with Pairformer, where they are used again. There are also some transition blocks that use SwiGLU to up/down project the matrix as was done in the Atom Transformer.</p> <h2 id="pairformer-module">Pairformer module</h2> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/summaries/pairformer-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/summaries/pairformer-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/summaries/pairformer-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/summaries/pairformer.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>See where this fits into the full architecture</p> </div> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/pairformer_module-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/pairformer_module-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/pairformer_module-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/pairformer_module.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Diagram from AF3 supplement</p> </div> <p>Having updated our pair representation based on the templates and MSA Module, we now ignore them for the rest of the model. Instead, only the updated pair representation (<strong><span>z</span></strong>) and single representation (<strong><span>s</span></strong>) enter the Pairformer and are used to update each other. As the transition blocks have already been described, this section focuses on the Triangle Updates and Triangle Attention, then briefly explains how the Single Attention with Pair Bias differs from the variant described earlier. These triangle-based layers were first introduced in AF2 are one of the pieces that not only remained in AF3, but now are even more present in the architecture, so they get quite a bit of attention.</p> <h3 id="why-look-at-triangles">Why look at triangles?</h3> <p>The guiding principle here is the idea of the triangle inequality: “the sum of any two sides of a triangle is greater than or equal to the third side”. Recall that each <strong><span>z<sub>i,j</sub></span></strong> in the pair tensor encodes the relationship between positions i and j in the sequence. While it does not literally encode the physical distances between pairs of tokens, let’s think about it for a moment as if it did. If we imagine that each <strong><span>z<sub>i,j</sub></span></strong> is the distance between two amino acids and we know <strong><span>z<sub>i,j</sub></span></strong>=1 and <strong><span>z<sub>j,k</sub></span></strong>=1. By the triangle inequality <strong><span>z<sub>i,k</sub></span></strong> cannot be larger than \(\sqrt{2}\). Knowing two of the distances gives us a strong belief about what the third distance must be. The goal of triangle updates and triangle attention are to try to encode these geometric constraints into the model.</p> <p>The triangle inequality is not enforced in the model but rather, it is encouraged through ensuring each position <strong><span>z<sub>i,j</sub></span></strong> is updated by looking at all possible triplets of positions (<strong>i</strong>,<strong>j</strong>,<strong>k</strong>) at a time. So <strong><span>z<sub>i,j</sub></span></strong> is updated based on <strong><span>z<sub>j,k</sub></span></strong> and <strong><span>z<sub>i,k</sub></span></strong> for all other atoms k. Because <strong><span>z</span></strong> represents the complex physical relationship between these tokens, rather than merely their distance, these relationships can be directional. So for <strong><span>z<sub>i,j</sub></span></strong>, we also want to encourage consistency with <strong><span>z<sub>k,i</sub></span></strong> and <strong><span>z<sub>k,j</sub></span></strong> for all atoms k. If we think of the atoms as a graph, with <strong><span>z</span></strong> as a directed adjacency matrix, it makes sense that AlphaFold calls these “outgoing edges” and “incoming edges”.</p> <p>Consider row i=0 of this adjacency matrix, and let’s say we want to update <strong><span>z<sub>0,2</sub></span></strong>, which has been highlighted in purple. The idea behind the update is that if we know the distances between 0→1 and 2→1, that gives us some constraints on what 0→2 can be. Similarly, if we know the distances between 0→3 and 2→3, this also gives us a constraint on 0→2. This would apply for all atoms k.</p> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/adjacency_matrix-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/adjacency_matrix-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/adjacency_matrix-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/adjacency_matrix.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>So, in the triangle updates and attention, we effectively look at all directed paths for 3 nodes in this graph (a.k.a triangles, hence the name!).</p> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/triangle_paths-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/triangle_paths-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/triangle_paths-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/triangle_paths.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h3 id="triangle-updates">Triangle Updates</h3> <p>Having carefully looked at the triangle operations from a graph theory perspective, we can see how this is implemented with tensor operations. In the outgoing update, every position <strong><span>z<sub>i,j</sub></span></strong> in the pair representation gets updated independently based on a weighted combination of the other elements in the same row (<strong><span>z<sub>i,j</sub></span></strong>), where the weighting of each <strong><span>z<sub>i,k</sub></span></strong> is based on the third element in its outgoing edge triangle (<strong><span>z<sub>j,k</sub></span></strong>).</p> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/triangle_update_outgoing-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/triangle_update_outgoing-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/triangle_update_outgoing-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/triangle_update_outgoing.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>Practically, we take three linear projections of <strong><span>z</span></strong> (called a, b, and g). To update <strong><span>z<sub>i,j</sub></span></strong>, we take an element-wise multiplication of <strong>row i from a</strong> and <strong>row j from b</strong>. We then sum over all these rows (different values of k), and gate with our g projection.</p> <p> At this point you might notice that gating is used all throughout this architecture! </p> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/triangle_update_incoming-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/triangle_update_incoming-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/triangle_update_incoming-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/triangle_update_incoming.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>For the incoming update, we effectively do the same thing but flipping the rows with the columns, so to update <strong><span>z<sub>i,j</sub></span></strong> we take a weighted sum of the other elements in the same column (<strong><span>z<sub>k,j</sub></span></strong>), where the weighting of each <strong><span>z<sub>k,j</sub></span></strong> is based on the third element in its outgoing edge triangle (<strong><span>z<sub>k,i</sub></span></strong>). After creating the same linear projections, we take an element-wise multiplication of <strong>column</strong> i from a and <strong>column</strong> j from b, and sum over all the <strong>rows of this matrix</strong>. You’ll find that these operations exactly mirror the graph-theory adjacency view described above.</p> <h3 id="triangle-attention">Triangle Attention</h3> <p>After our two triangle update steps, we also update each <strong><span>z<sub>i,j</sub></span></strong> using <strong>triangle attention</strong> for the outgoing edges and triangle attention for the incoming edges. The AF3 paper refers to the “outgoing edges” as attention “around starting node” and “incoming edges” as attention “around ending node”.</p> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/triangle_attn_starting-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/triangle_attn_starting-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/triangle_attn_starting-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/triangle_attn_starting.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>To build up to triangle attention, it can be helpful to start with typical self-attention over a 1D sequence. Recall that queries, keys, and values are all transformations of the original 1D sequence. An attention variant called <a href="https://arxiv.org/abs/1912.12180" rel="external nofollow noopener" target="_blank">axial attention</a> extends this to matrices by applying independent 1D self-attention over the different axes of a 2D matrix (the rows, then the columns). Triangle attention adds the triangle principle we discussed earlier to this, updating <strong><span>z<sub>i,j</sub></span></strong> by incorporating <strong><span>z<sub>i,k</sub></span></strong> and <strong><span>z<sub>j,k</sub></span></strong> for all atoms k. Specifically, in the “starting node” case, to calculate the attention scores along row i (to determine how much <strong><span>z<sub>i,j</sub></span></strong> should be influenced by <strong><span>z<sub>i,k</sub></span></strong>), we do a query-key comparison between <strong><span>z<sub>i,j</sub></span></strong> and <strong><span>z<sub>i,k</sub></span></strong> as usual, then bias the attention based on <strong><span>z<sub>j,k</sub></span></strong> as is shown above.</p> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/triangle_attn_ending-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/triangle_attn_ending-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/triangle_attn_ending-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/triangle_attn_ending.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>For the “ending node” case, we again swap rows for columns. For <strong><span>z<sub>i,j</sub></span></strong>, the keys and values will both come from column i of <strong><span>z</span></strong>, while the bias will come from column j. So, when comparing the query <strong><span>z<sub>i,j</sub></span></strong> with the key <strong><span>z<sub>k,i</sub></span></strong>, we bias that attention score based on <strong><span>z<sub>k,j</sub></span></strong>. Then, once we have attention scores over all k, we use our values vectors from column i.</p> <h3 id="single-attention-with-pair-bias">Single Attention with Pair Bias</h3> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/single_attn_w_pair_bias-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/single_attn_w_pair_bias-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/single_attn_w_pair_bias-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/single_attn_w_pair_bias.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>Now that we’ve updated our pair representation with these four triangle steps, we pass the pair representation through a Transition block as described above. Finally, we want to update our single representation (<strong><span>s</span></strong>) using this new updated pair representation (<strong><span>z</span></strong>), so we will use single attention with pair bias, pictured below. This is identical to Single Attention with Pair Bias described<d-footnote>For reference, in the AF3 supplement, Single Attention with Pair Bias is also referred to as "Attention Pair Bias"</d-footnote> in the Atom Transformer section, but at the token-level. As it operates on the token-level, it uses full attention as opposed to the block-wise sparse pattern used when operating at the atom-level.</p> <p>We repeat the Pairformer for 48 blocks, eventually creating <strong><span>s<sup>trunk</sup></span></strong> and <strong><span>z<sup>trunk</sup></span></strong>.</p> <h2 id="3-structure-prediction">3. Structure Prediction</h2> <h2 id="basics-of-diffusion">Basics of Diffusion</h2> <p>Now, with these refined representations, we are ready to use <strong><span>s</span></strong> and <strong><span>z</span></strong> to predict the structure of our complex. One of the changes introduced in AF3 is that entire structure prediction is based on atom-level diffusion. Existing posts more thoroughly explain the <a href="https://www.superannotate.com/blog/diffusion-models" rel="external nofollow noopener" target="_blank">intuition</a> and <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/" rel="external nofollow noopener" target="_blank">math</a> for diffusion, but the basic idea of a Diffusion Model is to start with real data, add random noise to your data, then train a model to predict what noise was added. Noise is iteratively added to the data over a series of T timesteps to create a sequence of T variants of each datapoint. We call the original data point x<sub>t=0</sub> and the fully noised version x<sub>t=T</sub>. During training, at timestep t, the model is given the x<sub>t</sub> and predicts what noise was added between x<sub>t-1</sub> and x<sub>t</sub>. We take a gradient step on the predicted noise added compared to the actual noise that had been added.</p> <p>Then, at inference time, we simply start with random noise, which is equivalent to x<sub>t=T</sub>. For every time step, we predict the noise the model thinks has been added, and remove that predicted noise. After a pre-specified number of timesteps, we end up with a fully “denoised” datapoint that should resemble the original data from our dataset.</p> <p>Conditional Diffusion lets the model ‘condition’ these de-noising predictions on some input. Practically this means that for each step of the model, it takes three inputs:</p> <ol> <li>The current noisy iteration of our generation</li> <li>A representation of the current time step we are at</li> <li>The information we want to condition on (this could be a caption for an image to generate, or properties for a protein).</li> </ol> <p>As a result, the final generation is not just a random example that resembles the training data distribution, but should specifically match the information represented by this conditioning vector.</p> <p>With AF3, the data we learn to de-noise is a matrix <strong><span>x</span></strong> with the x,y,z coordinates of all the atoms in our sequences. During training, we add Gaussian noise to these coordinates until they are effectively fully random. Then at inference time, we start with random coordinates. At each time step, we first randomly rotate and translate our entire predicted complex. This data-augmentation teaches the model that any rotation and translation of our complex is equally valid, and replaces the much more complicated Invariant Point Attention used in AF2. <d-footnote>AF2 had developed a complicated architecture called Invariant Point Attention meant to enforce equivariance to translations and rotations. This led to a vigorous debate over the importance of IPA in AF2's success. In AF3, this is dropped in favor of a much simpler approach: applying random rotations and translations as data-augmentations to help the model learn such equivariances naturally. So here we simply randomly rotate all atoms' coordinates around the center of our current generation (the mean over all atoms' coordinates), and randomly sample a translation in each dimension (x, y, and z) from a N(0,1) Gaussian. It appears from the algorithm that the translation is universal, that is the same translation is applied to every atom in our current generation. This type of data augmentation was popularize with CNNs but in the past few years, equivariant architectures like IPA have been considered an more efficient and elegant approach to solve the same problem. Thus, when AF3 replaced equivariant attention with data-augmentation, it sparked a lot of internet discussions.</d-footnote> We then add a small amount of noise to the coordinates to encourage more heterogeneous generations.<d-footnote>It benefits us for the model to generate several slightly different variations. At inference time, we can score each using our confidence head, and return only the generation with the highest score.</d-footnote> Finally, we predict a de-noising step using the Diffusion Module. We cover this module in more detail below:</p> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/coordinates_for_diffusion-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/coordinates_for_diffusion-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/coordinates_for_diffusion-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/coordinates_for_diffusion.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure><p> Data (coordinates) to get de-noised </p></div> <h2 id="diffusion-module">Diffusion Module</h2> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/summaries/diffusion-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/summaries/diffusion-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/summaries/diffusion-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/summaries/diffusion.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>See where this fits into the full architecture</p> </div> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/diffusion_module-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/diffusion_module-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/diffusion_module-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/diffusion_module.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>In each de-noising diffusion step, we condition our prediction on multiple representations of the input sequences:</p> <ul> <li>the outputs of the trunk (our post-Pairformer updated <strong><span>s</span></strong> and <strong><span>z</span></strong>, now called <strong><span>s<sup>trunk</sup></span></strong> and <strong><span>z<sup>trunk</sup></span></strong>)</li> <li>the initial atom and token-level representations of the sequence created in the input embedder that have not gone through the trunk (<strong><span>s<sup>inputs</sup></span></strong>, <strong><span>c<sup>inputs</sup></span></strong>)</li> </ul> <p>The AF3 paper breaks down its diffusion process into 4 steps that involve moving from tokens to atoms, back to tokens, and back to atoms:</p> <ol> <li><a href="#1-prepare-token-level-conditioning-tensors"><strong>Prepare token-level conditioning tensors</strong></a></li> <li><a href="#2-prepare-atom-level-tensors-apply-atom-level-attention-and-aggregate-back-to-token-level"><strong>Prepare atom-level conditioning tensors, update them using the Atom Transformer, and aggregate them back to token-level</strong></a></li> <li><a href="#3-apply-attention-at-the-token-level"><strong>Apply attention at the token-level, and project back to atoms</strong></a></li> <li><a href="#4-apply-attention-at-the-atom-level-to-predict-atom-level-noise-updates"><strong>Apply attention at the atom-level to predict atom-level noise updates</strong></a></li> </ol> <h3 id="1-prepare-token-level-conditioning-tensors">1. Prepare token-level conditioning tensors</h3>  <p>To initialize our token-level conditioning representation, we concatenate <strong><span>z<sup>trunk</sup></span></strong> to the relative positional encodings then project this larger representation back down and pass it through several residual-connection transition blocks.</p> <p>Similarly, for our token-level single representation, we concatenate the very first representation of the input created at the start of the model (<strong><span>s<sup>inputs</sup></span></strong>) and our current representation (<strong><span>s<sup>trunk</sup></span></strong>), then project it back down to its original size. We then create a Fourier embedding based on the current diffusion time step<d-footnote>More specifically, the amount of noise associated with this timestep in the Noise Schedule</d-footnote>, add that to our single representation, and pass that combination through several Transition blocks. By including the diffusion time step in the conditioning input here, it ensures the model is aware of the timestep in the diffusion process when making de-noising predictions, and so predicts the right scale of noise to remove for this timestep.</p> <h3 id="2-prepare-atom-level-tensors-apply-atom-level-attention-and-aggregate-back-to-token-level">2. Prepare atom-level tensors, apply atom-level attention, and aggregate back to token-level</h3> <p>At this point, our conditioning vectors are storing information at a per-token level, but we want to also run attention at the atom-level. To address this, we take our initial atom-level representations of the input created in the Embedding section (<strong><span>c</span></strong> and <strong><span>p</span></strong>), and update them based on the current token-level representations, to create atom-level conditioning tensors.</p>  <p>Next, we scale the atom’s current coordinates (<strong><span>x</span></strong>) by the variance of the data, effectively creating “dimensionless” coordinates with unit variance (called <strong><span>r</span></strong>). We then update <strong><span>q</span></strong> based on <strong><span>r</span></strong> such that <strong><span>q</span></strong> is now aware of the atom’s current location. Finally, we update <strong><span>q</span></strong> with the Atom Transformer (which also takes the pair representation as input), and aggregate the atoms back to tokens as we’ve previously seen.<d-footnote>Recall from the input preparation section that Atom Transformer runs sparse attention over the atoms, and all steps (layer norm, attention, gating) are conditioned on the conditioning tensor <b><span>c</span></b>.</d-footnote></p> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/agg_back_to_token_level-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/agg_back_to_token_level-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/agg_back_to_token_level-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/agg_back_to_token_level.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>At the end of this step, we return</p> <ul> <li> <strong><span>q</span></strong>: updated atom representation after incorporating information about the atom’s current coordinates</li> <li> <strong><span>a</span></strong>: token-level aggregated form of <span>q</span>, capturing coordinates and sequence information</li> <li> <strong><span>c</span></strong>: atom representation for conditioning based on the trunk</li> <li> <strong><span>p</span></strong>: our updated atom-pair representation for conditioning</li> </ul> <h3 id="3-apply-attention-at-the-token-level">3. Apply attention at the token-level</h3> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/diffusion_transformer-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/diffusion_transformer-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/diffusion_transformer-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/diffusion_transformer.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>The goal of this step is to apply attention to update our token-level representation of the atom coordinates and sequence information, <span>a</span>. This step uses the Diffusion Transformer visualized during input preparation, which mirrors the Atom Transformer but for tokens.</p> <h3 id="4-apply-attention-at-the-atom-level-to-predict-atom-level-noise-updates">4. Apply attention at the atom-level to predict atom-level noise updates</h3> <p>Now, we return to atom space. We use our updated <strong><span>a</span></strong> (token-level representations based on current “center atom” locations) to update <strong><span>q</span></strong> (atom-level representation of all atoms based on current location) using the Atom Transformer. As was done in step 3, we broadcast our tokens representation to match the number of atoms we started with (selectively duplicating the tokens that represent multiple atoms), and run the Atom Transformer. Most importantly, one last linear layer maps this atom-level representation <strong><span>q</span></strong> back to R<sup>3</sup>. This is the key step: we’ve used all these conditioning representations to generate coordinate updates <strong><span>r<sup>update</sup></span></strong> for all atoms. Now, because we generated these in the “dimensionless” space <span>r<sub>l</sub></span>, we carefully re-scale<d-footnote>This careful scaling involves both the variance of our data, and the noise schedule based on our current timestep, so that our updates are smaller and smaller as we get deeper into the de-noising process.</d-footnote> the updates from <strong><span>r<sup>update</sup></span></strong> to their form with non-unit variance, <strong><span>x<sup>update</sup></span></strong>, and apply the updates to <strong><span>x<sub>l</sub></span></strong>.</p> <p>With that, we’ve completed our tour through the main architecture of AlphaFold 3! Now we provide some additional information about the loss function, auxiliary confidence heads, and training details.</p> <h2 id="4-loss-function-and-other-training-details">4. Loss Function and Other Training Details</h2> <h2 id="loss-function-and-confidence-heads">Loss function and confidence heads</h2> <p>\(L_{\text{loss}} = L_{\text{distogram}}*\alpha_{\text{distogram}}+L_{\text{diffusion}}*\alpha_{\text{diffusion}}+L_{\text{confidence}}*\alpha_{\text{confidence}}\)</p> <p>The loss is a weighted sum of 3 terms:</p> <ul> <li> <strong>L<sub>distogram</sub></strong> which evaluates the accuracy of the predicted distogram at a token-level</li> <li> <strong>L<sub>diffusion</sub></strong> which evaluates the accuracy of the predicted distogram at an atom-level. It looks at all pairwise distances then includes additional terms to prioritize distances between nearby atoms and atoms involved in protein-ligand bonds.</li> <li> <strong>L<sub>confidence</sub></strong> which evaluates the model’s self-awareness about which structures are likely to be inaccurate</li> </ul> <h3 id="ldistogram">L<sub>distogram</sub> </h3> <p>The output of our model is atom-level coordinates, which can easily be used to create an atom-level distogram<d-footnote>Recall how the distograms were initially created by binning pairwise distances between atoms</d-footnote>. However, this loss evaluates a token-level distogram. To get the xyz coordinates for tokens, we just use the coordinate of the “center atom”. As these distogram distances are categorical, the predicted distogram is then compared to the true distogram via cross entropy.</p> <h3 id="ldiffusion">L<sub>diffusion</sub> </h3> <p>The diffusion loss itself is a weighted sum of three terms each computed over the atom positions, additionally scaled by the amount of noise<d-footnote>t<sup>^</sup>, the sampled noise level for the current time step, and σ<sub>data</sub>, the variance of the data which scales the amount of noise at each time step</d-footnote> added at the current time step: \(L_{\text{diffusion}} = (L_{\text{MSE}} + L_{\text{bond}} * \alpha_{\text{bond}}) * (\hat{t}^2 + \sigma_{\text{data}}^2)/(\hat{t}+\sigma_{\text{data}})^2 + L_{\text{smooth_lddt}}\)</p> <ul> <li> <strong>L<sub>MSE</sub></strong> is a version of the distogram loss we just discussed, but over all atoms rather just “center atoms” (and with DNA, RNA, and ligand atoms upweighted). Additionally, it looks at the mean squared error between positions, rather than binning them into a distogram.</li> <li> <strong>L<sub>bond</sub></strong> aims to ensure the accuracy of bond lengths for protein-ligand bonds by adding an additional MSE loss on the difference in predicted and ground-truth distograms for atom-pairs that are part of protein-ligand bonds.<d-footnote>There are various stages of training and α<sub>bond</sub> is set to 0 in the initial stages, so this term is only introduced later.</d-footnote> </li> <li> <strong>L<sub>smooth_LDDT</sub></strong> (smoothed local distance difference test) is yet another variant of the distogram loss that tries to capture the accuracy of local distances. An atom-pair’s predicted distance “passes the test” if it is within a given threshold of the atom-pair’s true distance. To make this metric smooth and differentiable, we pass the difference between predicted and ground-truth distograms through a sigmoid centered on the test’s threshold. We can think of this as generating a probability (between 0 and 1) that this atom-pair passes the test. We take the average of four “tests’’ with increasingly tight thresholds (4, 2, 1, and .5 Å). Using this loss encourages the model to reduce the probability of failing each test. Finally, to make the test “local”, we ignore the loss for an atom-pair if that atom-pair’s ground truth distance is large, as we only want the model to focus on accurately predicting an atom’s distances to nearby atoms<d-footnote>Specifically, for an atom-pair l,m, we ignore the loss for l and m if l and m are more than 30 Å away if atom m is part of a nucleotide. We ignore the loss for l and m if they are more than 15 Å away from each other and m is not a nucleotide (so is part of a protein or ligand).</d-footnote>.</li> </ul> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/smooth_lddt-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/smooth_lddt-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/smooth_lddt-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/smooth_lddt.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h3 id="lconfidence">L<sub>confidence</sub> </h3> <p>The goal of this loss is not to improve the accuracy of the structure, but rather to teach the model to predict its own accuracy. This loss is a weighted sum of 4 terms that each correspond to a method of evaluating the quality of a predicted structure:</p> \[L_{\text{confience}} = L_{\text{plDDT}} + L_{\text{PDE}} + L_{\text{resolved}} + L_{\text{PAE}} * \alpha_{\text{PAE}}\] <ul> <li> <p><strong>lDDT</strong> Atom-level “local distance difference test”, capturing the expected accuracy of an atom’s predicted distances to nearby atoms.</p> </li> <li> <p><strong>PAE</strong> Predicted alignment error between token i’s predicted and the ground-truth positions. We first rotate and translate the predicted token i and ground-truth token i into the frame of token j. That is, if we assume for a moment token j is in exactly its ground-truth position, we predict how close token i is to where it should be, based on its relation to token j.</p> </li> <li> <p><strong>PDE</strong> Predicted distance error between tokens, capturing the accuracy of predicted differences between all pairs of tokens.</p> </li> <li> <p><strong>Experimentally resolved prediction</strong> The model predicts which atoms were experimentally resolved (not every atom is experimentally resolved in every crystal structure).</p> </li> </ul> <p>To get these confidence losses for each of the metrics, AF3 predicts values for these error metrics, then these error metrics are calculated on the predicted structure, and the loss is based on the difference between these two. So even if the structure is really incorrect and the PAE is high, if the predicted PAE is also high, the L<sub>pae</sub> will be low.</p> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/confidence_arch-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/confidence_arch-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/confidence_arch-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/confidence_arch.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>These confidence predictions are generated mid-way through the diffusion process. At a selected diffusion step t, the predicted coordinates <strong><span>r<sup>t</sup></span></strong> are used to update the single and pair representations created in the representation learning trunk. The predicted errors are then calculated from linear projections of the updated pair representation (for PAE and PDE) or this updated single representation (pLDDT and experimentally resolved). Then, the actual error metrics are calculated based on the same generated atom coordinates (process described below, if interested) for comparison.</p> <p>While these terms are included in the confidence head loss, gradients from these terms are only used to update the confidence prediction heads and do not affect the rest of the model.</p> <details> <summary>How are the actual error metrics calculated?</summary> <p><b>pLDDT:</b> The LDDT for atom l is calculated in the following way: in the current predicted structure, we calculate the distance between atom l and a set of atoms R that is indexed by m, and compare this to the ground truth equivalent. To be in this set, an atom m must be part of a polymer chain, within 15 or 30 Å of l depending on the molecule m is a part of, and the center atom of a token. We then calculate four binary distance tests with increasingly tight thresholds (4, 2, 1, and .5 Å) and take the average pass rate, and sum over the atoms in R. We bin this percentage into 50 bins between 0 and 1.</p> <p>At inference time, we have a pLDDT head. This head takes the single representation of a given token, repeats it out across all the atoms "attached" to this token<d-footnote>Technically, the max number of atoms attached to any token, so that we can stack tensors</d-footnote>, and projects all those atom-level representations to the 50 bins of our pLDDT_l. We treat these as logits across the 50 "classes", use a softmax to convert to probabilities, and take a multi-class classification loss across the bins.</p> <p><b>Predicted Alignment Error (PAE):</b> Every token is considered to have a frame, that is a 3D coordinate frame created from three atoms (called a, b, c) involved in that token. Atom b within those three atoms forms the origin in this frame. In cases where each token has a single atom "attached", the center atom of the frame is the single atom of the token, and the two other nearest tokens of the same entity (e.g., same ligand) form the basis of the frame. For every token pair (i,j) we re-express the predicted coordinates of the center atom for token_i using the frame of token_j. We do the same for the ground-truth coordinates of the center atom of token_i. The euclidean distance between these transformed true and predicted coordinates of the center atom of token_i is our alignment error, binned into 64 bins. We predict this alignment error from the pair representation <b><span>z<sub>i,j</sub></span></b>, projecting it to 64 dimensions that we treat as logits and convert to probabilities with a softmax. We train this head with a classification loss, which each bin as a class. See <a href="https://www.ebi.ac.uk/training/online/courses/alphafold/inputs-and-outputs/evaluating-alphafolds-predicted-structures-using-confidence-scores/pae-a-measure-of-global-confidence-in-alphafold-predictions/" rel="external nofollow noopener" target="_blank">here</a> for additional details.</p> <p>Third, AF3 predicts the distance error (PDE) between tokens. The true distance error is calculated by taking the distance between center atoms for every token pair, and binning these distances over 64 uniformly-sized bins from 0 Å to 32 Å. The predicted distance error comes from projecting the pair representation <b><span>z<sub>i,j</sub></span></b> plus the pair representation <b><span>z<sub>j,i</sub></span></b> into 64 dimensions that we again treat as logits, and again convert to probabilities with a softmax.</p> <p>Finally, AF3 predicts whether each atom was experimentally resolved in the ground-truth structure. Similar to the pLDDT head, we repeat the <b><span>s<sub>i</sub></span></b> single representation out for the number of atoms this token represents, and project to 2 dimensions and use a binary classification loss.</p> </details> <hr> <h2 id="other-training-details">Other Training Details</h2> <p>Now that the architecture is covered, the last pieces are some of the additional training details.</p> <h3 id="recycling">Recycling</h3> <p>As introduced in AF2, AF3 recycles its weights; that is, rather than making the model deeper, the model weights are re-used and inputs are run through the modules multiple times to continually improve the representations. Diffusion inherently uses recycling at inference time, as the model is trained to incorporate the timestep information and use the same model weights for every time step.</p> <h3 id="cross-distillation">Cross-distillation</h3> <p>AF3 uses a mix of synthetic training data generated by itself (via self-distillation) but also by AF2, via <a href="https://link.springer.com/article/10.1007/s11263-024-02002-0" rel="external nofollow noopener" target="_blank">cross-distillation</a>. Specifically, the authors note that, by switching to the diffusion-based generative module, the model stopped producing the characteristic “spaghetti” regions that allowed users of AF2 to visually identify low-confidence and likely disordered regions. Just visually looking at the diffusion-based generations, all regions appeared equally high in confidence, making it more difficult to identify potential hallucinations.</p> <p>To solve this problem, they included generations from AF2 and AF-Multimer in the training data for AF3, allowing the model to learn that, when AF2 was not confident in its prediction, it should output these unfolded regions and to “instruct” AF3 to do the same.<d-footnote>Nucleic acids and small molecules in distillation datasets had to be removed as they could not be processed by AF2 and AF-multimer. However, once previous models generated new predicted structures, and these structures got aligned to the originals, the removed molecules were added back in. If adding these back in created new atom clashes, the whole structure was excluded, to avoid accidentally teaching the model to accept clashes.</d-footnote></p> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/cross_distillation-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/cross_distillation-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/cross_distillation-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/cross_distillation.jpg" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>(Diagram from AF3 paper)</p> </div> <h3 id="cropping-and-training-stages">Cropping and Training Stages</h3> <p>While no part of the model has an explicit restriction on the length of the input sequences, the memory and compute requirements increase significantly with sequence length (recall the multiple O(N<sub>tokens</sub><sup>3</sup> operations)). Thus, for efficiency the proteins get randomly cropped. As introduced in AF-multimer, because we want to model the interactions between multiple chains, the random cropping needs to include all of these. They use 3 methods for cropping and all 3 of these are used in different proportions depending on the training data (ex: PDB crystal structure vs disordered PDB complex vs distillation, etc.)</p> <ul> <li>Contiguous cropping: Contiguous sequences of amino acids are selected for each chain</li> <li>Spatial cropping: Amino acids are selected based on distance to a reference atom (typically this atom is part of a specific chain or binding interface of interest)</li> <li>Spatial interface cropping: Similar to spatial cropping, but based on distances to atoms that specifically at a binding interface.</li> </ul> <p>While a model trained on random crops of 384 can be applied to longer sequences, to improve the model’s ability to handle these sequences, it is iteratively fine-tuned on larger sequence lengths. The mix of datasets and other training details is also varied in each training stage as is shown in the table below.</p> <div> <figure> <picture> <source srcset="https://elanapearl.github.io/assets/img/af3_post/training_stages-480.webp 480w,https://elanapearl.github.io/assets/img/af3_post/training_stages-800.webp 800w,https://elanapearl.github.io/assets/img/af3_post/training_stages-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://elanapearl.github.io/assets/img/af3_post/training_stages.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>(Table from AF3 supplement)</p> </div> <h3 id="clashing">Clashing</h3> <p>The authors note that AF3’s loss does not include a clash penalty for overlapping atoms. While switching to a diffusion-based structure module means the model could in theory predict two atoms to be in the same location, this seems to be minimal after training. That said, AF3 does employ a clashing penalty when ranking generated structures.</p> <h3 id="batch-sizes">Batch sizes</h3> <p>Although the diffusion process sounds quite involved, it is still significantly less computationally expensive than the trunk of the model. Thus, the AF3 authors found that it is more efficient from a training perspective to expand the batch size of the model after the trunk. So for each input structure, it gets run through the embedding and trunk, then 48 independent data-augmented versions of the structure are applied, and these 48 structures are all trained in parallel.</p> <p><strong>That’s it for the training process!</strong> There are some other small details but this is probably already more than you need, and if you’ve made it this far, the rest should be easy to pick up from reading the AF3 supplement.</p> <h2 id="ml-musings">ML Musings</h2> <p>Having walked so thoroughly through the architecture of AF3 and its comparisons to AF2, it is interesting how the choices made by the authors fit into broader Machine Learning trends.</p> <h3 id="alphafold-as-retrieval-augmented-generation">AlphaFold as Retrieval-Augmented Generation</h3> <p>At the time AF2 was released, it was not common to include retrievals from the training set at inference time. In the case of AF, utilizing an MSA and template search. MSA-based methods were being used for protein modeling, but this type of retrieval was less used in other areas of Deep Learning (i.e., ResNets do not embed relevant training images at inference time when classifying a new image in Computer Vision, for example). Although AF3 reduces the emphasis on the MSA compared to AF2 (it is no longer operated on and updated in the 48 blocks of the Evoformer/Pairformer), they still incorporate both the MSA and templates, even as other protein prediction models such as ESMFold have dropped retrieval in favor of fully parametric inference.</p> <p>Interestingly, some of the largest and most successful Deep Learning models now often include similar additional information at inference time. While the details of the retrieval systems are not always disclosed, Large Language Models routinely use Retrieval Augmented Generation systems such as a traditional web search at inference time to orient the model toward relevant information (even if that information was likely already in its training data) that should guide inference. It will be interesting to see how the use of directly relevant examples at inference time develops in the future.</p> <h3 id="pair-bias-attention">Pair-Bias Attention</h3> <p>One of the major components of AF2 that is even more present in AF3 is Pair-Bias Attention. That is, attention where the queries, keys, and values all originate from the same source (like in self-attention), but where there is a bias term added to the attention map from another source. This effectively acts as a light-touch version of information sharing, without full cross-attention. Pair-Bias Attention appears in almost every module. While this type of attention is now used in other protein modeling architectures, we have not seen this particular type of cross-biasing used in other fields (although that does not mean it hasn’t been done!). Perhaps it only works well here because the pair-representation is naturally analogous to a self-attention map already, but is an intriguing alternative to pure self or pure cross-attention.</p> <h3 id="self-supervised-training">Self-supervised training</h3> <p>Self-supervised models like ESM have been able to achieve impressive results at predicting protein structure by replacing the MSA embedding with a “probabilistic MSA” using self-supervised pre-training. In AF2, the model had an additional task that predicted masked tokens from the MSA, achieving a similar self-supervision, but that was removed with AF3. We have not seen commentary from the authors on why they did not use any self-supervised language modeling pre-training approach on the MSA, and in fact decreased the compute used to process the MSA. Three possible reasons self-supervised learning is not used to initialize the MSA embeddings are 1) they viewed the massive pre-training phase as a suboptimal use of compute 2) they tried it and found that including a small MSA module outperformed pre-trained embeddings and was worth the additional inference-time cost or 3) utilizing a mix of pre-trained embeddings for amino acid tokens and randomly initialized embeddings for DNA/RNA/ligands would not be compatible or underperformed fully supervised training on their hybrid atom-token structure. <d-footnote>By focusing on self-supervision tasks, models in the ESM family are also much more simple than AF3 (although they don't handle DNA/RNA/ligands and have slightly different goals.) It is interesting to watch that as some models aim to maximize architectural simplicity, AlphaFold remains this complicated! </d-footnote></p> <h3 id="classification-vs-regression">Classification vs. Regression</h3> <p>As in AF2, AF3 continues to use a mix of MSE and binned classification losses. The classification components are interesting as, if the model predicts a distogram bin that is only off-by-one, it gets no “credit” for being close rather than way off. It is unclear what informed this design decision, but perhaps the authors found the gradients to be more stable than working with several different MSE losses, and perhaps the per-atom losses saw so many gradient steps that the additional signal from a continuous loss would not have proven beneficial.</p> <h3 id="similarities-to-recurrent-architectures-eg-lstms">Similarities to Recurrent Architectures (e.g LSTMs)</h3> <p>AF3’s architecture incorporates several design elements reminiscent of recurrent neural networks that are not typically found in traditional transformers:</p> <ul> <li>Extensive Gating: AF3 uses gating mechanisms throughout its architecture to control information flow in the residual stream. This is more akin to the gating in LSTMs or GRUs than the standard feed-forward nature of normal transformer layers.</li> <li>Iterative Processing with Weight Reuse: AF3 applies the same weights multiple times to progressively refine its predictions. This process, involving both recycling and the diffusion model, resembles how recurrent networks process sequential data over time steps using a shared set of weights. It differs from standard transformers, which typically make predictions in a single forward pass. This approach allows AF3 to iteratively improve its protein structure predictions without increasing the number of parameters.</li> <li>Adaptive Computation: The recycling is also similar to the iterative updating used in diffusion and quite related to the idea of adaptive compute time <a href="https://arxiv.org/abs/1603.08983" rel="external nofollow noopener" target="_blank">(ACT)</a>, originally introduced to dynamically determine how much compute to use for RNNs and more recently used in <a href="https://arxiv.org/pdf/2404.02258" rel="external nofollow noopener" target="_blank">Mixture-of-Depths</a> to achieve a similar goal with transformers. This contrasts with the fixed depth of standard transformers and theoretically would allow the model to apply more processing to challenging inputs.</li> </ul> <p>It was shown in the AF2 ablations that the recycling was important, but there was little disucssion on the importance of gating. Presumably it helps with training stability as in LSTMs but it is interesting that it is so prevalent here yet not in many other transformer-based architectures.</p> <h3 id="cross-distillation-1">Cross-distillation</h3> <p>The use of AF2 generations to re-introduce its distinctive style specifically for low-confidence regions is very interesting. If there is a lesson here, it may be the most practical of all: If your previous model is doing one specific thing better than your new model, you can try cross-distillation to get the best of both worlds!</p> <hr> <p>If you’ve made it this far, thanks for reading and hope it was helpful!! If you have any questions / comments / corrections / feedback feel free to reach out to us on twitter (<a href="https://twitter.com/ElanaPearl/" rel="external nofollow noopener" target="_blank">E</a>, <a href="https://twitter.com/JakeSilberg" rel="external nofollow noopener" target="_blank">J</a>) or email (<a href="mailto:epsimon@stanford.edu" target="_blank">E</a>, <a href="mailto:jsilberg@stanford.edu" target="_blank">J</a>) !</p> <p>Special thanks to <a href="https://x.com/kristyacarp" rel="external nofollow noopener" target="_blank">Kristy Carpenter</a>, <a href="https://twitter.com/nickevanjoseph" rel="external nofollow noopener" target="_blank">Nicholas Joseph</a>, <a href="https://swansonkyle.com/" rel="external nofollow noopener" target="_blank">Kyle Swanson</a>, and <a href="https://karamarieliu.github.io/" rel="external nofollow noopener" target="_blank">Kara Liu</a> for giving feedback on this 💜</p> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Disruptor-rs: better latency and throughput than crossbeam (111 pts)]]></title>
            <link>https://github.com/nicholassm/disruptor-rs</link>
            <guid>40954104</guid>
            <pubDate>Sat, 13 Jul 2024 13:47:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/nicholassm/disruptor-rs">https://github.com/nicholassm/disruptor-rs</a>, See on <a href="https://news.ycombinator.com/item?id=40954104">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/05a6856d8fed3c2822bd7f44b63cd77538dafd8262dda04d3ea45ece8b53a131/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f762f646973727570746f72"><img src="https://camo.githubusercontent.com/05a6856d8fed3c2822bd7f44b63cd77538dafd8262dda04d3ea45ece8b53a131/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f762f646973727570746f72" alt="Crates.io" data-canonical-src="https://img.shields.io/crates/v/disruptor"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/3ce91432a392db91a0062e515331056315bc4cc64498dfb36efa94d2848b1558/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f642f646973727570746f72"><img src="https://camo.githubusercontent.com/3ce91432a392db91a0062e515331056315bc4cc64498dfb36efa94d2848b1558/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f642f646973727570746f72" alt="Crates.io" data-canonical-src="https://img.shields.io/crates/d/disruptor"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/nicholassm/disruptor-rs/actions/workflows/build_and_test.yml/badge.svg"><img src="https://github.com/nicholassm/disruptor-rs/actions/workflows/build_and_test.yml/badge.svg" alt="Build"></a>
<a href="https://codecov.io/gh/nicholassm/disruptor-rs" rel="nofollow"><img src="https://camo.githubusercontent.com/cafcaf0cbed3d6076b0788f10e99bb37178bba7ddc1204e0145467878521345d/68747470733a2f2f636f6465636f762e696f2f67682f6e6963686f6c6173736d2f646973727570746f722d72732f67726170682f62616467652e7376673f746f6b656e3d565730334b30414d4930" alt="codecov" data-canonical-src="https://codecov.io/gh/nicholassm/disruptor-rs/graph/badge.svg?token=VW03K0AMI0"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Disruptor</h2><a id="user-content-disruptor" aria-label="Permalink: Disruptor" href="#disruptor"></a></p>
<p dir="auto">This library is a low latency, inter-thread communication library written in Rust.</p>
<p dir="auto">It's heavily inspired by the brilliant
<a href="https://github.com/LMAX-Exchange/disruptor">Disruptor library from LMAX</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto">Add the following to your <code>Cargo.toml</code> file:</p>

<p dir="auto">To read details of how to use the library, check out the documentation on <a href="https://docs.rs/disruptor" rel="nofollow">docs.rs/disruptor</a>.</p>
<p dir="auto">Here's a minimal example demonstrating both single and batch publication. Note, batch publication should be used whenever possible for best latency and throughput (see benchmarks below).</p>
<div dir="auto" data-snippet-clipboard-copy-content="use disruptor::*;

// The event on the ring buffer.
struct Event {
    price: f64
}

fn main() {
    // Factory closure for initializing events in the Ring Buffer.
    let factory = || { Event { price: 0.0 }};

    // Closure for processing events.
    let processor = |e: &amp;Event, sequence: Sequence, end_of_batch: bool| {
        // Your processing logic here.
    };

    let size = 64;
    let mut producer = disruptor::build_single_producer(size, factory, BusySpin)
        .handle_events_with(processor)
        .build();

    // Publish single events into the Disruptor via the `Producer` handle.
    for i in 0..10 {
        producer.publish(|e| {
            e.price = i as f64;
        });
    }

    // Publish a batch of events into the Disruptor.
    producer.publish_batch(5, |iter| {
        for e in iter { // `iter` is guaranteed to yield 5 events.
            e.price = 42.0;
        }
    });
}// At this point, the Producer instance goes out of scope and when the
 // processor is done handling all events then the Disruptor is dropped
 // as well."><pre><span>use</span> disruptor<span>::</span><span>*</span><span>;</span>

<span>// The event on the ring buffer.</span>
<span>struct</span> <span>Event</span> <span>{</span>
    <span>price</span><span>:</span> <span>f64</span>
<span>}</span>

<span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
    <span>// Factory closure for initializing events in the Ring Buffer.</span>
    <span>let</span> factory = || <span>{</span> <span>Event</span> <span>{</span> <span>price</span><span>:</span> <span>0.0</span> <span>}</span><span>}</span><span>;</span>

    <span>// Closure for processing events.</span>
    <span>let</span> processor = |<span>e</span><span>:</span> <span>&amp;</span><span>Event</span><span>,</span> <span>sequence</span><span>:</span> <span>Sequence</span><span>,</span> <span>end_of_batch</span><span>:</span> <span>bool</span>| <span>{</span>
        <span>// Your processing logic here.</span>
    <span>}</span><span>;</span>

    <span>let</span> size = <span>64</span><span>;</span>
    <span>let</span> <span>mut</span> producer = disruptor<span>::</span><span>build_single_producer</span><span>(</span>size<span>,</span> factory<span>,</span> <span>BusySpin</span><span>)</span>
        <span>.</span><span>handle_events_with</span><span>(</span>processor<span>)</span>
        <span>.</span><span>build</span><span>(</span><span>)</span><span>;</span>

    <span>// Publish single events into the Disruptor via the `Producer` handle.</span>
    <span>for</span> i <span>in</span> <span>0</span>..<span>10</span> <span>{</span>
        producer<span>.</span><span>publish</span><span>(</span>|e| <span>{</span>
            e<span>.</span><span>price</span> = i <span>as</span> <span>f64</span><span>;</span>
        <span>}</span><span>)</span><span>;</span>
    <span>}</span>

    <span>// Publish a batch of events into the Disruptor.</span>
    producer<span>.</span><span>publish_batch</span><span>(</span><span>5</span><span>,</span> |iter| <span>{</span>
        <span>for</span> e <span>in</span> iter <span>{</span> <span>// `iter` is guaranteed to yield 5 events.</span>
            e<span>.</span><span>price</span> = <span>42.0</span><span>;</span>
        <span>}</span>
    <span>}</span><span>)</span><span>;</span>
<span>}</span><span>// At this point, the Producer instance goes out of scope and when the</span>
 <span>// processor is done handling all events then the Disruptor is dropped</span>
 <span>// as well.</span></pre></div>
<p dir="auto">The library also supports pinning threads on cores to avoid latency induced by context switching.
A more advanced usage demonstrating this and with multiple producers and multiple interdependent consumers could look like this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="use disruptor::*;
use std::thread;

struct Event {
    price: f64
}

fn main() {
    let factory = || { Event { price: 0.0 }};

    // Closure for processing events.
    let h1 = |e: &amp;Event, sequence: Sequence, end_of_batch: bool| {
        // Processing logic here.
    };
    let h2 = |e: &amp;Event, sequence: Sequence, end_of_batch: bool| {
        // Some processing logic here.
    };
    let h3 = |e: &amp;Event, sequence: Sequence, end_of_batch: bool| {
        // More processing logic here.
    };

    let mut producer1 = disruptor::build_multi_producer(64, factory, BusySpin)
        // `h2` handles events concurrently with `h1`.
        .pined_at_core(1).handle_events_with(h1)
        .pined_at_core(2).handle_events_with(h2)
            .and_then()
            // `h3` handles events after `h1` and `h2`.
            .pined_at_core(3).handle_events_with(h3)
        .build();

    // Create another producer.
    let mut producer2 = producer1.clone();

    // Publish into the Disruptor.
    thread::scope(|s| {
        s.spawn(move || {
            for i in 0..10 {
                producer1.publish(|e| {
                    e.price = i as f64;
                });
            }
        });
        s.spawn(move || {
            for i in 10..20 {
                producer2.publish(|e| {
                    e.price = i as f64;
                });
            }
        });
    });
}// At this point, the Producers instances go out of scope and when the
 // processors are done handling all events then the Disruptor is dropped
 // as well."><pre><span>use</span> disruptor<span>::</span><span>*</span><span>;</span>
<span>use</span> std<span>::</span>thread<span>;</span>

<span>struct</span> <span>Event</span> <span>{</span>
    <span>price</span><span>:</span> <span>f64</span>
<span>}</span>

<span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
    <span>let</span> factory = || <span>{</span> <span>Event</span> <span>{</span> <span>price</span><span>:</span> <span>0.0</span> <span>}</span><span>}</span><span>;</span>

    <span>// Closure for processing events.</span>
    <span>let</span> h1 = |<span>e</span><span>:</span> <span>&amp;</span><span>Event</span><span>,</span> <span>sequence</span><span>:</span> <span>Sequence</span><span>,</span> <span>end_of_batch</span><span>:</span> <span>bool</span>| <span>{</span>
        <span>// Processing logic here.</span>
    <span>}</span><span>;</span>
    <span>let</span> h2 = |<span>e</span><span>:</span> <span>&amp;</span><span>Event</span><span>,</span> <span>sequence</span><span>:</span> <span>Sequence</span><span>,</span> <span>end_of_batch</span><span>:</span> <span>bool</span>| <span>{</span>
        <span>// Some processing logic here.</span>
    <span>}</span><span>;</span>
    <span>let</span> h3 = |<span>e</span><span>:</span> <span>&amp;</span><span>Event</span><span>,</span> <span>sequence</span><span>:</span> <span>Sequence</span><span>,</span> <span>end_of_batch</span><span>:</span> <span>bool</span>| <span>{</span>
        <span>// More processing logic here.</span>
    <span>}</span><span>;</span>

    <span>let</span> <span>mut</span> producer1 = disruptor<span>::</span><span>build_multi_producer</span><span>(</span><span>64</span><span>,</span> factory<span>,</span> <span>BusySpin</span><span>)</span>
        <span>// `h2` handles events concurrently with `h1`.</span>
        <span>.</span><span>pined_at_core</span><span>(</span><span>1</span><span>)</span><span>.</span><span>handle_events_with</span><span>(</span>h1<span>)</span>
        <span>.</span><span>pined_at_core</span><span>(</span><span>2</span><span>)</span><span>.</span><span>handle_events_with</span><span>(</span>h2<span>)</span>
            <span>.</span><span>and_then</span><span>(</span><span>)</span>
            <span>// `h3` handles events after `h1` and `h2`.</span>
            <span>.</span><span>pined_at_core</span><span>(</span><span>3</span><span>)</span><span>.</span><span>handle_events_with</span><span>(</span>h3<span>)</span>
        <span>.</span><span>build</span><span>(</span><span>)</span><span>;</span>

    <span>// Create another producer.</span>
    <span>let</span> <span>mut</span> producer2 = producer1<span>.</span><span>clone</span><span>(</span><span>)</span><span>;</span>

    <span>// Publish into the Disruptor.</span>
    thread<span>::</span><span>scope</span><span>(</span>|s| <span>{</span>
        s<span>.</span><span>spawn</span><span>(</span><span>move</span> || <span>{</span>
            <span>for</span> i <span>in</span> <span>0</span>..<span>10</span> <span>{</span>
                producer1<span>.</span><span>publish</span><span>(</span>|e| <span>{</span>
                    e<span>.</span><span>price</span> = i <span>as</span> <span>f64</span><span>;</span>
                <span>}</span><span>)</span><span>;</span>
            <span>}</span>
        <span>}</span><span>)</span><span>;</span>
        s<span>.</span><span>spawn</span><span>(</span><span>move</span> || <span>{</span>
            <span>for</span> i <span>in</span> <span>10</span>..<span>20</span> <span>{</span>
                producer2<span>.</span><span>publish</span><span>(</span>|e| <span>{</span>
                    e<span>.</span><span>price</span> = i <span>as</span> <span>f64</span><span>;</span>
                <span>}</span><span>)</span><span>;</span>
            <span>}</span>
        <span>}</span><span>)</span><span>;</span>
    <span>}</span><span>)</span><span>;</span>
<span>}</span><span>// At this point, the Producers instances go out of scope and when the</span>
 <span>// processors are done handling all events then the Disruptor is dropped</span>
 <span>// as well.</span></pre></div>
<p dir="auto">If you need to store some state in the processor thread which is neither <code>Send</code> nor <code>Sync</code>, e.g. a <code>Rc&lt;RefCell&lt;i32&gt;&gt;</code>, then you can create a closure for initializing that state and pass it along with the processing closure when you build the Disruptor. The Disruptor will then pass a mutable reference to your state on each event. As an example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="use std::{cell::RefCell, rc::Rc};
use disruptor::*;

struct Event {
    price: f64
}

#[derive(Default)]
struct State {
    data: Rc<RefCell<i32>>
}

fn main() {
    let factory = || { Event { price: 0.0 }};
    let initial_state = || { State::default() };

    // Closure for processing events *with* state.
    let processor = |s: &amp;mut State, e: &amp;Event, _: Sequence, _: bool| {
        // Mutate your custom state:
        *s.data.borrow_mut() += 1;
    };

    let size = 64;
    let mut producer = disruptor::build_single_producer(size, factory, BusySpin)
        .handle_events_and_state_with(processor, initial_state)
        .build();

    for i in 0..10 {
        producer.publish(|e| {
            e.price = i as f64;
        });
    }
}"><pre><span>use</span> std<span>::</span><span>{</span>cell<span>::</span><span>RefCell</span><span>,</span> rc<span>::</span><span>Rc</span><span>}</span><span>;</span>
<span>use</span> disruptor<span>::</span><span>*</span><span>;</span>

<span>struct</span> <span>Event</span> <span>{</span>
    <span>price</span><span>:</span> <span>f64</span>
<span>}</span>

<span>#<span>[</span>derive<span>(</span><span>Default</span><span>)</span><span>]</span></span>
<span>struct</span> <span>State</span> <span>{</span>
    <span>data</span><span>:</span> <span>Rc</span><span>&lt;</span><span>RefCell</span><span>&lt;</span><span>i32</span><span>&gt;</span><span>&gt;</span>
<span>}</span>

<span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
    <span>let</span> factory = || <span>{</span> <span>Event</span> <span>{</span> <span>price</span><span>:</span> <span>0.0</span> <span>}</span><span>}</span><span>;</span>
    <span>let</span> initial_state = || <span>{</span> <span>State</span><span>::</span><span>default</span><span>(</span><span>)</span> <span>}</span><span>;</span>

    <span>// Closure for processing events *with* state.</span>
    <span>let</span> processor = |<span>s</span><span>:</span> <span>&amp;</span><span>mut</span> <span>State</span><span>,</span> <span>e</span><span>:</span> <span>&amp;</span><span>Event</span><span>,</span> _<span>:</span> <span>Sequence</span><span>,</span> _<span>:</span> <span>bool</span>| <span>{</span>
        <span>// Mutate your custom state:</span>
        <span>*</span>s<span>.</span><span>data</span><span>.</span><span>borrow_mut</span><span>(</span><span>)</span> += <span>1</span><span>;</span>
    <span>}</span><span>;</span>

    <span>let</span> size = <span>64</span><span>;</span>
    <span>let</span> <span>mut</span> producer = disruptor<span>::</span><span>build_single_producer</span><span>(</span>size<span>,</span> factory<span>,</span> <span>BusySpin</span><span>)</span>
        <span>.</span><span>handle_events_and_state_with</span><span>(</span>processor<span>,</span> initial_state<span>)</span>
        <span>.</span><span>build</span><span>(</span><span>)</span><span>;</span>

    <span>for</span> i <span>in</span> <span>0</span>..<span>10</span> <span>{</span>
        producer<span>.</span><span>publish</span><span>(</span>|e| <span>{</span>
            e<span>.</span><span>price</span> = i <span>as</span> <span>f64</span><span>;</span>
        <span>}</span><span>)</span><span>;</span>
    <span>}</span>
<span>}</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul>
<li> Single Producer Single Consumer (SPSC).</li>
<li> Multi Producer Single Consumer (MPSC).</li>
<li> Multi Producer Multi Consumer (MPMC) with consumer interdependencies.</li>
<li> Busy-spin wait strategies.</li>
<li> Batch publication of events.</li>
<li> Batch consumption of events.</li>
<li> Thread affinity can be set for the event processor thread(s).</li>
<li> Set thread name of each event processor thread.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Design Choices</h2><a id="user-content-design-choices" aria-label="Permalink: Design Choices" href="#design-choices"></a></p>
<p dir="auto">Everything in the library is about low-latency and this heavily influences all choices made in this library.
As an example, you cannot allocate an event and <em>move</em> that into the ringbuffer. Instead, events are allocated on startup to ensure they are co-located in memory to increase cache coherency.
However, you can still allocate a struct on the heap and move ownership to a field in the event on the Ringbuffer.
As long as you realize that this can add latency, because the struct is allocated by one thread and dropped by another.
Hence, there's synchronization happening in the allocator.</p>
<p dir="auto">There's also no use of dynamic dispatch - everything is monomorphed.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Performance</h2><a id="user-content-performance" aria-label="Permalink: Performance" href="#performance"></a></p>
<p dir="auto">The SPSC and MPSC Disruptor variants have been benchmarked and compared to Crossbeam. See the code in the <code>benches/spsc.rs</code> and <code>benches/mpsc.rs</code> files.</p>
<p dir="auto">The results below of the SPSC benchmark are gathered from running the benchmarks on a 2016 Macbook Pro running a 2,6 GHz Quad-Core Intel Core i7. So on a modern Intel Xeon the numbers should be even better. Furthermore, it's not possible to isolate cores on Mac and pin threads which would produce even more stable results. This is future work.</p>
<p dir="auto">If you have any suggestions to improving the benchmarks, please feel free to open an issue.</p>
<p dir="auto">To provide a somewhat realistic benchmark not only burst of different sizes are considered but also variable pauses between bursts: 0 ms, 1 ms and 10 ms.</p>
<p dir="auto">The latencies below are the mean latency per element with 95% confidence interval (standard <code>criterion</code> settings). Capturing all latencies and calculating misc. percentiles (and in particular the max latency) is future work. However, I expect the below measurements to be representative for the actual performance you can achieve in a real application.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">No Pause Between Bursts</h2><a id="user-content-no-pause-between-bursts" aria-label="Permalink: No Pause Between Bursts" href="#no-pause-between-bursts"></a></p>
<p dir="auto"><em>Latency:</em></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Burst Size</th>
<th>Crossbeam</th>
<th>Disruptor</th>
<th>Improvement</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>65 ns</td>
<td>32 ns</td>
<td>51%</td>
</tr>
<tr>
<td>10</td>
<td>68 ns</td>
<td>9 ns</td>
<td>87%</td>
</tr>
<tr>
<td>100</td>
<td>29 ns</td>
<td>8 ns</td>
<td>72%</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><em>Throughput:</em></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Burst Size</th>
<th>Crossbeam</th>
<th>Disruptor</th>
<th>Improvement</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>15.2M / s</td>
<td>31.7M / s</td>
<td>109%</td>
</tr>
<tr>
<td>10</td>
<td>14.5M / s</td>
<td>117.3M / s</td>
<td>709%</td>
</tr>
<tr>
<td>100</td>
<td>34.3M / s</td>
<td>119.7M / s</td>
<td>249%</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">1 ms Pause Between Bursts</h2><a id="user-content-1-ms-pause-between-bursts" aria-label="Permalink: 1 ms Pause Between Bursts" href="#1-ms-pause-between-bursts"></a></p>
<p dir="auto"><em>Latency:</em></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Burst Size</th>
<th>Crossbeam</th>
<th>Disruptor</th>
<th>Improvement</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>63 ns</td>
<td>33 ns</td>
<td>48%</td>
</tr>
<tr>
<td>10</td>
<td>67 ns</td>
<td>8 ns</td>
<td>88%</td>
</tr>
<tr>
<td>100</td>
<td>30 ns</td>
<td>9 ns</td>
<td>70%</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><em>Throughput:</em></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Burst Size</th>
<th>Crossbeam</th>
<th>Disruptor</th>
<th>Improvement</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>15.9M / s</td>
<td>30.7M / s</td>
<td>93%</td>
</tr>
<tr>
<td>10</td>
<td>14.9M / s</td>
<td>117.7M / s</td>
<td>690%</td>
</tr>
<tr>
<td>100</td>
<td>33.8M / s</td>
<td>105.0M / s</td>
<td>211%</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">10 ms Pause Between Bursts</h2><a id="user-content-10-ms-pause-between-bursts" aria-label="Permalink: 10 ms Pause Between Bursts" href="#10-ms-pause-between-bursts"></a></p>
<p dir="auto"><em>Latency:</em></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Burst Size</th>
<th>Crossbeam</th>
<th>Disruptor</th>
<th>Improvement</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>51 ns</td>
<td>32 ns</td>
<td>37%</td>
</tr>
<tr>
<td>10</td>
<td>67 ns</td>
<td>9 ns</td>
<td>87%</td>
</tr>
<tr>
<td>100</td>
<td>30 ns</td>
<td>10 ns</td>
<td>67%</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><em>Throughput:</em></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Burst Size</th>
<th>Crossbeam</th>
<th>Disruptor</th>
<th>Improvement</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>19.5M / s</td>
<td>31.6M / s</td>
<td>62%</td>
</tr>
<tr>
<td>10</td>
<td>14.9M / s</td>
<td>114.5M / s</td>
<td>668%</td>
</tr>
<tr>
<td>100</td>
<td>33.6M / s</td>
<td>105.0M / s</td>
<td>213%</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Conclusion</h2><a id="user-content-conclusion" aria-label="Permalink: Conclusion" href="#conclusion"></a></p>
<p dir="auto">There's clearly a difference between the Disruptor and the Crossbeam libs. However, this is not because the Crossbeam library is not a great piece of software. It is. The Disruptor trades CPU and memory resources for lower latency and higher throughput and that is why it's able to achieve these results. The Disruptor also excels if you can publish batches of events as
demonstrated in the benchmarks with bursts of 10 and 100 events.</p>
<p dir="auto">Both libraries greatly improves as the burst size goes up but the Disruptor's performance is more resilient to the pauses between bursts which is one of the design goals.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Related Work</h2><a id="user-content-related-work" aria-label="Permalink: Related Work" href="#related-work"></a></p>
<p dir="auto">There are multiple other Rust projects that mimic the LMAX Disruptor library:</p>
<ol dir="auto">
<li><a href="https://github.com/polyfractal/Turbine">Turbine</a></li>
<li><a href="https://github.com/sklose/disrustor">Disrustor</a></li>
</ol>
<p dir="auto">A key feature that this library supports is multiple producers from different threads that neither of the above libraries support (at the time of writing).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributions</h2><a id="user-content-contributions" aria-label="Permalink: Contributions" href="#contributions"></a></p>
<p dir="auto">You are welcome to create a Pull-Request or open an issue with suggestions for improvements.</p>
<p dir="auto">Changes are accepted solely at my discretion and I will focus on whether the changes are a good fit for the purpose and design of this crate.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Roadmap</h2><a id="user-content-roadmap" aria-label="Permalink: Roadmap" href="#roadmap"></a></p>
<p dir="auto">Empty! All the items have been implemented.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[For the Colonel, It Was Finger-Lickin’ Bad (1976) (125 pts)]]></title>
            <link>https://kottke.org/16/08/for-the-colonel-it-was-fingerlickin-bad</link>
            <guid>40952650</guid>
            <pubDate>Sat, 13 Jul 2024 08:30:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kottke.org/16/08/for-the-colonel-it-was-fingerlickin-bad">https://kottke.org/16/08/for-the-colonel-it-was-fingerlickin-bad</a>, See on <a href="https://news.ycombinator.com/item?id=40952650">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-container">
<p>

posted <time datetime="2016-08-26T17:26:06Z">Aug 26 @ 01:26 PM</time> by <a href="http://www.kottke.org/">Jason Kottke</a><span>  ·  <span>gift link</span></span>



</p>




<p><img src="https://kottke.org/cdn-cgi/image/format=auto,fit=scale-down,width=1200,metadata=none/plus/misc/images/kfc-finger-lickin-bad.jpg?1" srcset="https://kottke.org/cdn-cgi/image/format=auto,fit=scale-down,width=500,metadata=none/plus/misc/images/kfc-finger-lickin-bad.jpg?1 500w, https://kottke.org/cdn-cgi/image/format=auto,fit=scale-down,width=1200,metadata=none/plus/misc/images/kfc-finger-lickin-bad.jpg?1 1200w" sizes="(max-width: 500px) 500px, 1200px" loading="lazy" width="1200" height="650" alt="KFC Finger Lickin Bad"></p>

<p>Here’s a gem from the archive of the NY Times. One day in September 1976, NY Times food critic Mimi Sheraton and Colonel Harland Sanders <a href="http://www.nytimes.com/1976/09/09/archives/for-the-colonel-it-was-fingerlickin-bad.html">stopped into a Manhattan Kentucky Fried Chicken</a>. The Colonel, then estranged from the company he founded, strolled into the kitchen after glad-handing some patrons and proceeded to tear into the quality of the food:</p>

<blockquote><p>Once in the kitchen, the colonel walked over to a vat full of frying chicken pieces and announced, ‘That’s much too black. It should be golden brown. You’re frying for 12 minutes — that’s six minutes too long. What’s more, your frying fat should have been changed a week ago. That’s the worst fried chicken I’ve ever seen. Let me see your mashed potatoes with gravy, and how do you make them?”</p>
	
<p>When Mr. Singleton explained that he first mixed boiling water into the instant powdered potatoes, the colonel interrupted. “And then you have wallpaper paste,” he said. “Next suppose you add some of this brown gravy stuff and then you have sludge.” “There’s no way anyone can get me to swallow those potatoes,” he said after tasting some. “And this cole slaw. This cole slaw! They just won’t listen to me. It should he chopped, not shredded, and it should be made with Miracle Whip. Anything else turns gray. And there should be nothing in it but cabbage. No carrots!”</p></blockquote>

<p>Sanders sold his company to an investment group in 1964, which took the company public two years later and eventually sold to a company called Heublein. After selling, Sanders officially still worked for the company as an advisor but grew more and more dissatisfied with it, as evidenced by the story above. When the company HQ moved to Tennessee, the Colonel was quoted as saying:</p>

<blockquote><p>This ain’t no goddam Tennessee Fried Chicken, no matter what some slick, silk-suited son-of-a-bitch says.</p></blockquote>

<p>And he got sued by a KFC franchisee after he <a href="https://books.google.com/books?id=ZskrsscU7HkC&amp;pg=SA4-PA17&amp;lpg=SA4-PA17&amp;dq=My+God,+that+gravy+is+horrible!+sanders&amp;source=bl&amp;ots=zfKAaLx1Rx&amp;sig=iZUPCcCs3LlSlsdnEGKkbQzEJx0&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjTpZn_3t3OAhVMCsAKHaviCY8Q6AEILTAC#v=onepage&amp;q=My%20God%2C%20that%20gravy%20is%20horrible!%20sanders&amp;f=false">commented</a>:</p>

<blockquote><p>My God, that gravy is horrible. They buy tap water for 15 to 20 cents a thousand gallons and then mix it with flour and starch and end up with pure wallpaper paste. And I know wallpaper paste, by God, because I’ve seen my mother make it.</p>

<p>To the “wallpaper paste” they add some sludge and sell it for 65 or 75 cents a pint. There’s no nutrition in it and the ought not to be allowed to sell it.</p>

<p>And another thing. That new crispy chicken is nothing in the world but a damn fried doughball stuck on some chicken.</p></blockquote>

<p>Colonel Sanders: serving up chicken and sick burns with equal spiciness. (via <a href="https://twitter.com/mccanner">@mccanner</a>)</p>

<ul><li><a href="https://kottke.org/tag/food">food</a></li><li><a href="https://kottke.org/tag/Harland%20Sanders">Harland Sanders</a></li><li><a href="https://kottke.org/tag/KFC">KFC</a></li><li><a href="https://kottke.org/tag/Mimi%20Sheraton">Mimi Sheraton</a></li><li><a href="https://kottke.org/tag/restaurants">restaurants</a></li></ul>








</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I made a drag and drop CSS grid generator (199 pts)]]></title>
            <link>https://cssgridgenerator.io/</link>
            <guid>40952509</guid>
            <pubDate>Sat, 13 Jul 2024 07:44:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cssgridgenerator.io/">https://cssgridgenerator.io/</a>, See on <a href="https://news.ycombinator.com/item?id=40952509">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>CSS grid generator is a tool that helps developers create custom CSS grid layouts more easily. The generator allows users to specify the number of columns, rows, the gutter size.</p><div><h2>How to use CSS grid generator?</h2><ul><li><span>1.</span> Customize the number of columns, rows, and gaps to fit your needs.</li><li><span>2.</span> Click the square with + sign to add a new element to the grid.</li><li><span>3.</span> Resize the DIV using the<!-- --> <b>handle in the bottom right corner.</b></li><li><span>4.</span> Drag and drop the DIV to reposition it as desired.</li><li><span>5.</span> Finally, copy the generated HTML and CSS code and paste it into your project.</li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ad-tech setting 'Privacy-Preserving Attribution' is opt-out in Firefox 128 (175 pts)]]></title>
            <link>https://gladtech.social/@cuchaz/112775302929069283</link>
            <guid>40952330</guid>
            <pubDate>Sat, 13 Jul 2024 06:56:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gladtech.social/@cuchaz/112775302929069283">https://gladtech.social/@cuchaz/112775302929069283</a>, See on <a href="https://news.ycombinator.com/item?id=40952330">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[WebContainers: Dev environments. In your web app (113 pts)]]></title>
            <link>https://webcontainers.io/</link>
            <guid>40952248</guid>
            <pubDate>Sat, 13 Jul 2024 06:31:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://webcontainers.io/">https://webcontainers.io/</a>, See on <a href="https://news.ycombinator.com/item?id=40952248">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-v-669faec9="" id="VPContent" data-v-5a346dfe=""><!--[--><div data-v-6d7c391d=""><header data-v-6d7c391d="" data-v-0856bbef=""><div data-v-0856bbef=""><h2 data-v-0856bbef=""><strong data-v-0856bbef="">Dev environments.</strong><br data-v-0856bbef="">In your <strong data-v-0856bbef="">web app.</strong></h2><p data-v-0856bbef="">From interactive tutorials to full-blown IDEs, build instant, interactive coding experiences backed by WebContainers: the trusted, browser-based runtime from <a href="https://stackblitz.com/" target="_blank" data-v-0856bbef="">StackBlitz</a>.</p><p data-v-0856bbef=""><a href="https://webcontainers.io/guides/introduction" data-v-0856bbef="" data-v-11452584=""><!--[-->Get started<!--]--></a><a href="https://forms.default.com/360757" data-v-0856bbef="" data-v-11452584=""><!--[-->Book a demo<!--]--></a></p></div></header><h3 data-v-6d7c391d="">Battle-tested by cutting-edge teams</h3><!--[--><div data-v-be173c96="" data-v-60437d36=""><!--[--><!--[--><div data-v-de3caf89="" data-v-be173c96="" data-v-681eb775=""><p><img src="https://webcontainers.io/img/theme/sveltekit-light.svg" data-v-de3caf89=""></p><!----><!----><p data-v-de3caf89="">On the SvelteKit team, we've fantasized for years about being able to build fully interactive learning material for full stack frameworks.<strong><br>With WebContainers it went from 'impossible' to 'easy' almost overnight.</strong></p></div><div data-v-de3caf89="" data-v-be173c96="" data-v-681eb775=""><!----><p><img src="https://webcontainers.io/img/logos/egghead-light.svg" data-v-de3caf89=""><img src="https://webcontainers.io/img/logos/egghead-dark.svg" data-v-de3caf89=""></p><p data-v-de3caf89=""><strong>As a team working on educational products, StackBlitz WebContainers has been an invaluable tool for us.</strong> The ability to embed full-stack applications with customisable, interactive coding environment directly into our products has greatly enhanced the learning experience for our users.</p><div data-v-de3caf89=""><p><img src="https://webcontainers.io/img/testimonials/vojta_holik.png" data-v-de3caf89=""></p><div data-v-de3caf89=""><p data-v-de3caf89="">Vojta Holik</p><p data-v-de3caf89="">Designer &amp; Developer, Egghead.io</p></div></div></div><div data-v-de3caf89="" data-v-be173c96="" data-v-681eb775=""><!----><!----><!----><p data-v-de3caf89="">WebContainers solve the final frontier in JavaScript developer experience - making full-stack Node.js projects run in the browser as lightweight and disposable and secure as frontend REPLs. <br><strong>Every PR, every npm library maintainer, every devtool company with a Node.js SDK, can benefit from this!</strong></p><div data-v-de3caf89=""><p><img src="https://webcontainers.io/img/testimonials/swyx_shawn_wang.jpg" data-v-de3caf89=""></p><p data-v-de3caf89="">swyx</p></div></div><div data-v-de3caf89="" data-v-be173c96="" data-v-681eb775=""><p><img src="https://webcontainers.io/img/testimonials/scrimba.svg" data-v-de3caf89=""></p><!----><!----><p data-v-de3caf89="">I have worked with Web container API for a couple of weeks at Scrimba to make a pooc of backend support. And I can say it's a solid piece of technology. Things just work, and it's also quite fast. <strong>I'm super excited about the GA since it will unlock so much opportunities for OSS projects and the industry at large.</strong></p><div data-v-de3caf89=""><p><img src="https://webcontainers.io/img/testimonials/abdellah_alaoui.png" data-v-de3caf89=""></p><div data-v-de3caf89=""><p data-v-de3caf89="">Abdellah Alaoui</p><p data-v-de3caf89="">Fullstack hacker, Scrimba</p></div></div></div><div data-v-de3caf89="" data-v-be173c96="" data-v-681eb775=""><!----><p><img src="https://webcontainers.io/img/testimonials/xata.png" data-v-de3caf89=""><img src="https://webcontainers.io/img/testimonials/xata.png" data-v-de3caf89=""></p><p data-v-de3caf89=""><strong>The WebContainer API is a landmark on the way we think docs.</strong> Creating interactive docs and snippets just became so much more feasible! With Server-side code running on the browser, setting up a playground to securely learn Node.js SDKs and compilers  became feasible and even fun!</p><div data-v-de3caf89=""><p><img src="https://webcontainers.io/img/testimonials/atila_fassina.png" data-v-de3caf89=""></p><div data-v-de3caf89=""><p data-v-de3caf89="">Atila Fassina</p><p data-v-de3caf89="">DX Engineer at Xata</p></div></div></div><div data-v-de3caf89="" data-v-be173c96="" data-v-681eb775=""><p><img src="https://webcontainers.io/img/testimonials/astro-full-light.svg" data-v-de3caf89=""></p><!----><!----><p data-v-de3caf89=""><strong>WebContainers represent a fundamental shift in what is possible in the browser. I'm incredibly excited about the potential this tech unlocks,</strong> from secure, browser-based development environments to highly interactive educational content. </p><div data-v-de3caf89=""><p><img src="https://webcontainers.io/img/testimonials/nate_moore.jpg" data-v-de3caf89=""></p><div data-v-de3caf89=""><p data-v-de3caf89="">Nate Moore</p><p data-v-de3caf89="">Senior Software Engineer, The Astro Technology Company</p></div></div></div><div data-v-de3caf89="" data-v-be173c96="" data-v-681eb775=""><!----><p><img src="https://webcontainers.io/img/testimonials/suborbital.svg" data-v-de3caf89=""><img src="https://webcontainers.io/img/testimonials/suborbital.svg" data-v-de3caf89=""></p><p data-v-de3caf89="">For such a powerful piece of tech I was so impressed by how clear to use the API is. Also running WebContainers inside WebContainers had me 🤯</p><div data-v-de3caf89=""><p><img src="https://webcontainers.io/img/testimonials/ramon_huidobro.png" data-v-de3caf89=""></p><div data-v-de3caf89=""><p data-v-de3caf89="">Ramón Huidobro</p><p data-v-de3caf89="">Developer Advocate at Suborbital Software Systems</p></div></div></div><div data-v-de3caf89="" data-v-be173c96="" data-v-681eb775=""><!----><p><img src="https://webcontainers.io/img/testimonials/retune.png" data-v-de3caf89=""><img src="https://webcontainers.io/img/testimonials/retune.png" data-v-de3caf89=""></p><p data-v-de3caf89="">At re:tune, we have been building the missing frontend for GPT-3, on a mission to empower everyone to build AI-first software at the speed of thought. <strong>WebContainers set the stage for our AI-native IDE</strong> - with a copilot that can not only read and write code, but can also understand and operate in the full runtime context across server and client!</p><div data-v-de3caf89=""><p><img src="https://webcontainers.io/img/testimonials/retune-dj.jpg" data-v-de3caf89=""></p></div></div><div data-v-de3caf89="" data-v-be173c96="" data-v-681eb775=""><!----><!----><!----><p data-v-de3caf89="">Running chess in a terminal, running a terminal in the browser, check mate!<br><strong>The best position to be in is a creative one and the StackBlitz WebContainers allow that.</strong></p><div data-v-de3caf89=""><p><img src="https://webcontainers.io/img/testimonials/manus_nijhoff.png" data-v-de3caf89=""></p></div></div><!--]--><!--]--></div><!--]--></div><div data-v-6d7c391d=""><p><img src="https://webcontainers.io/img/features/elements-01-nodeLIGHT-02-DARK.png" data-v-6d7c391d=""></p><h2 data-v-6d7c391d="">Power your web app with<br data-v-6d7c391d="">the <strong data-v-6d7c391d="">WebContainer API</strong></h2><p data-v-6d7c391d="">Create unmatched user experiences by integrating Node.js directly into your web app.<br data-v-6d7c391d="">Build fully-branded products without connecting to external servers or directing users away to third-party apps.</p><div data-v-bd3c3473="" data-v-60437d36="" data-v-6d7c391d=""><!--[--><!--[--><div data-v-e2782662="" data-v-bd3c3473="" data-v-681eb775=""><h4 data-v-e2782662="">Run native package managers</h4><p data-v-e2782662="">Run the native versions of <code>npm</code>, <code>pnpm</code>, and <code>yarn</code>, all in the browser, all in your app, up to 10x faster than local.</p></div><div data-v-e2782662="" data-v-bd3c3473="" data-v-681eb775=""><h4 data-v-e2782662="">Full browser support</h4><p data-v-e2782662="">Run WebContainer in all major browsers, from Chromium-based, to Firefox or Safari TP.</p></div><div data-v-e2782662="" data-v-bd3c3473="" data-v-681eb775=""><h4 data-v-e2782662="">All major frameworks</h4><p data-v-e2782662="">Instantly spin up disposable environments running any major modern framework.</p></div><div data-v-e2782662="" data-v-bd3c3473="" data-v-681eb775=""><h4 data-v-e2782662="">Run Wasm out of the box</h4><p data-v-e2782662="">Port your favorite language or framework to Wasm to run it in WebContainers. Yes, really.</p></div><!--]--><!--]--></div></div><div data-v-6d7c391d=""><h3 data-v-6d7c391d="">Set up in only a few steps</h3></div><div data-v-6d7c391d=""><h3 data-v-6d7c391d="">Build rich development experiences not possible before</h3><div data-v-09544a0d="" data-v-60437d36="" data-v-6d7c391d=""><!--[--><!--[--><div data-v-cf776150="" data-v-09544a0d="" data-v-681eb775=""><p data-v-cf776150="">Interactive tutorials</p><p data-v-cf776150="">Learn SvelteKit, a full stack framework, within their custom editor, running on WebContainers, all in the browser.</p><p><a href="https://learn.svelte.dev/tutorial/introducing-sveltekit" target="_blank" rel="nofollow" data-v-cf776150="">learn.svelte.dev</a></p></div><div data-v-cf776150="" data-v-09544a0d="" data-v-681eb775=""><p data-v-cf776150="">Low code / no code</p><p data-v-cf776150="">A stress-free editor enabling non-technical contributors to make their own PRs with a live, disposable preview to confirm an error-free build.</p><p><a href="https://developer.stackblitz.com/codeflow/integrating-web-publisher#what-is-web-publisher" target="_blank" rel="nofollow" data-v-cf776150="">Web Publisher by StackBlitz</a></p></div><div data-v-cf776150="" data-v-09544a0d="" data-v-681eb775=""><p data-v-cf776150="">AI applications</p><p data-v-cf776150="">re:tune is setting the stage for AI-native IDEs - with a copilot that can understand and operate in the full runtime context across server and client.</p><p><a href="https://retune.so/" target="_blank" rel="nofollow" data-v-cf776150="">retune.so</a></p></div><!--]--><!--]--></div></div><div data-v-6d7c391d=""><div data-v-6d7c391d=""><h2 data-v-6d7c391d="">Support for every team</h2><p data-v-6d7c391d="">Small startups, open source maintainers, and Fortune 500 enterprises all enjoy access to StackBlitz's committed product support, features and improvements.</p></div><div data-v-6d7c391d=""><h2 data-v-6d7c391d="">Slash server costs</h2><p data-v-6d7c391d="">WebContainer API only requires the compute power of your local CPU and browser, eliminating the cost and overhead of managing remote servers.</p></div><div data-v-6d7c391d=""><h2 data-v-6d7c391d="">Ship faster</h2><p data-v-6d7c391d="">No additional teams to build or maintain. Leave the technical support to us and focus on actually shipping your product.</p></div></div><div data-v-6d7c391d=""><h2 data-v-6d7c391d="">Leverage the tech <strong data-v-6d7c391d="">we use<br data-v-6d7c391d=""> in our own products.</strong></h2><p data-v-6d7c391d="">Years of development by our full-time engineering team, front-line feedback from leading community partners, and funded R&amp;D into future technological possibilities make WebContainer more robust by the day.</p><a href="https://webcontainers.io/guides/quickstart" data-v-6d7c391d="" data-v-11452584=""><!--[-->Get started!<!--]--></a></div><!--]--></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[gpu.cpp: A lightweight library for portable low-level GPU computation (124 pts)]]></title>
            <link>https://github.com/AnswerDotAI/gpu.cpp</link>
            <guid>40952182</guid>
            <pubDate>Sat, 13 Jul 2024 06:12:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/AnswerDotAI/gpu.cpp">https://github.com/AnswerDotAI/gpu.cpp</a>, See on <a href="https://news.ycombinator.com/item?id=40952182">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">gpu.cpp</h2><a id="user-content-gpucpp" aria-label="Permalink: gpu.cpp" href="#gpucpp"></a></p>
<p dir="auto">gpu.cpp is a lightweight library that makes portable GPU compute with C++ simple.</p>
<p dir="auto">It focuses on general purpose native GPU computation, leveraging the WebGPU
specification as a portable low-level GPU interface. This means we can drop in
GPU code in C++ projects and have it run on Nvidia, Intel, AMD, and other GPUs.
The same C++ code can work on a wide variety of laptops, workstations, mobile
devices or virtually any hardware with Vulkan, Metal, or DirectX support.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Technical Objectives: Lightweight, Fast Iteration, and Low Boilerplate</h2><a id="user-content-technical-objectives-lightweight-fast-iteration-and-low-boilerplate" aria-label="Permalink: Technical Objectives: Lightweight, Fast Iteration, and Low Boilerplate" href="#technical-objectives-lightweight-fast-iteration-and-low-boilerplate"></a></p>
<p dir="auto">With gpu.cpp we want to enable a high-leverage library for individual developers and researchers to incorporate GPU computation into programs relying on nothing more than a standard C++ compiler as tooling. Our goals are:</p>
<ul dir="auto">
<li>High power-to-weight ratio API: Provide the smallest API surface area that can cover the full range of GPU compute needs.</li>
<li>Fast compile/run cycles: Ensure projects can build nearly instantaneously, compile/run cycles should be &lt;5 seconds on a modern laptop.</li>
<li>Minimal dependencies and tooling overhead: A standard clang C++ compiler should be enough, no external library dependencies beyond the WebGPU native implementation.</li>
</ul>
<p dir="auto">The implementation aims for a small API surface area with minimum boilerplate. There are a small number of library operations to carry out an broad range of low-level GPU operations. We avoid abstractions that add layers of indirection, making the mapping between the gpu.cpp library to raw WebGPU API clear when it's needed.</p>
<p dir="auto">In this spirit of fast experimentation, we also want near- instantaneous C++ builds taking no more than a second or two even on modestly capable personal computing devices. With this in mind, we not only keep the API surface area small, but also keep the implementation small and we also provide a prebuilt binary of the Dawn native WebGPU implementation.</p>
<p dir="auto">The core library implementation in the header-only <code>gpu.h</code> source code is around 1000 lines of code. In addition to enabling instantaneous, semi-interactive compilation cycles, the small implementation surface area keeps maintenance burden low and the velocity of improvements high.
We also pre-build Google's Dawn WebGPU implementation as a shared library binary. This allows builds to link the shared library with each build and incorporate Google's powerful native WebGPU implementation without paying the cost of re-compiling Dawn during development cycles.</p>
<p dir="auto">For more advanced users and release deployments, we include <code>cmake</code> examples for building both Dawn with gpu.cpp end-to-end, but this is not required nor recommended for most users to get started.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick Start: Building and Running</h2><a id="user-content-quick-start-building-and-running" aria-label="Permalink: Quick Start: Building and Running" href="#quick-start-building-and-running"></a></p>
<p dir="auto">To build a gpu.cpp project, you will need to have installed on your system:</p>
<ul dir="auto">
<li><code>clang++</code> compiler installed with support for C++17.</li>
<li><code>python3</code> and above, to run the script which downloads the Dawn shared library.
make to build the project.</li>
<li><code>make</code> to build the project.</li>
<li>Only on Linux systems - Vulkan drivers. If Vulkan is not installed, you can run <code>sudo apt install libvulkan1 mesa-vulkan-drivers vulkan-tools</code> to install them.</li>
</ul>
<p dir="auto">The only library dependency of gpu.cpp is a WebGPU implementation. Currently we support the Dawn native backend, but we plan to support other targets and WebGPU implementations (web browsers or other native implementations such as wgpu). Currently we support MacOS, Linux, and Windows (via WSL).</p>
<p dir="auto">Optionally, Dawn can be built from scratch with gpu.cpp using the cmake build scripts provided - see the -cmake targets in the Makefile. However, this is recommended for advanced users only. Building Dawn dependencies with cmake takes much longer than using the precompiled Dawn shared library.</p>
<p dir="auto">After cloning the repo, from the top-level gpu.cpp, you should be able to build and run the hello world GELU example by typing:</p>

<p dir="auto">The first time you build and run the project this way, it will download a prebuilt shared library for the Dawn native WebGPU implementation automatically (using the setup.py script). This places the Dawn shared library in the third_party/lib directory. Afterwards you should see <code>libdawn.dylib</code> on MacOS or <code>libdawn.so</code> on Linux. This download only occurs once.</p>
<p dir="auto">The build process itself should take a few seconds. If the build and executions is successful, you should see the output of the GELU computation:</p>
<div data-snippet-clipboard-copy-content="Hello gpu.cpp!
--------------

  gelu(0.00) = 0.00
  gelu(0.10) = 0.05
  gelu(0.20) = 0.12
  gelu(0.30) = 0.19
  gelu(0.40) = 0.26
  gelu(0.50) = 0.35
  gelu(0.60) = 0.44
  gelu(0.70) = 0.53
  gelu(0.80) = 0.63
  gelu(0.90) = 0.73
  gelu(1.00) = 0.84
  gelu(1.10) = 0.95
  ...

Computed 10000 values of GELU(x)"><pre><code>Hello gpu.cpp!
--------------

  gelu(0.00) = 0.00
  gelu(0.10) = 0.05
  gelu(0.20) = 0.12
  gelu(0.30) = 0.19
  gelu(0.40) = 0.26
  gelu(0.50) = 0.35
  gelu(0.60) = 0.44
  gelu(0.70) = 0.53
  gelu(0.80) = 0.63
  gelu(0.90) = 0.73
  gelu(1.00) = 0.84
  gelu(1.10) = 0.95
  ...

Computed 10000 values of GELU(x)
</code></pre></div>
<p dir="auto">If you need to clean up the build artifacts, you can run:</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Hello World Tutorial: A GELU Kernel</h2><a id="user-content-hello-world-tutorial-a-gelu-kernel" aria-label="Permalink: Hello World Tutorial: A GELU Kernel" href="#hello-world-tutorial-a-gelu-kernel"></a></p>
<p dir="auto">As a real-world example for how to use gpu.cpp, let's start with a practical-but-simple example of a GPU kernel from neural networks.</p>
<p dir="auto">GELU is a non-linear embarassingly parallel operation often used in modern large language model transformer-based architectures.</p>
<p dir="auto">It takes as input a vector of floats and applies the GELU function to each element of the vector. The function is nonlinear, attenuating values below zero to near zero, approximating the y = x identity function for large positive values. For values close to zero, GELU smoothly interpolates between the identity function and the zero function.</p>
<p dir="auto">The GELU code below will illustrate the three main aspects of setting up a GPU computation with gpu.cpp:</p>
<ol dir="auto">
<li>
<p dir="auto">The code that runs on the GPU (in WebGPU Shading Language, or WGSL), implementing the compute opporation.</p>
</li>
<li>
<p dir="auto">The code that runs on the CPU (in C++) that sets up the GPU computation by allocating and preparing resources. For high performance, this code should be run ahead-of-time from the hot paths of the application.</p>
</li>
<li>
<p dir="auto">The code that runs on the CPU (in C++) that dispatches the GPU computation and retrieves the results. The key concern of hot-path dispatch code is to eliminate or minimize any unnecessary resource allocation or data movement (offloading such concerns to step 2). A secondary consideration is that GPU dispatches are asynchronous. We work with standard C++ asynchronous primitives to manage the asynchronous aspect of kernel dispatch.</p>
</li>
</ol>
<p dir="auto">Here's a GELU kernel implemented (based on the CUDA implementation in <a href="https://github.com/karpathy/llm.c">llm.c</a>) as an on-device WGSL shader and invoked from the host using gpu.cpp library functions and types. It can be compiled using a standard C++ compiler (we recommend Clang):</p>
<div data-snippet-clipboard-copy-content="#include <array>
#include <cstdio>
#include <future>

#include &quot;gpu.h&quot;

using namespace gpu; // createContext, createTensor, createKernel,
                     // createShader, dispatchKernel, wait, toCPU
                     // Bindings, Tensor, Kernel, Context, Shape, kf32

static const char *kGelu = R&quot;(
const GELU_SCALING_FACTOR: f32 = 0.7978845608028654; // sqrt(2.0 / PI)
@group(0) @binding(0) var<storage, read_write> inp: array<{{precision}}>;
@group(0) @binding(1) var<storage, read_write> out: array<{{precision}}>;
@compute @workgroup_size({{workgroupSize}})
fn main(
    @builtin(global_invocation_id) GlobalInvocationID: vec3<u32>) {
    let i: u32 = GlobalInvocationID.x;
    if (i < arrayLength(&amp;inp)) {
        let x: f32 = inp[i];
        out[i] = select(0.5 * x * (1.0 + tanh(GELU_SCALING_FACTOR 
                 * (x + .044715 * x * x * x))), x, x > 10.0);
    }
}
)&quot;;

int main(int argc, char **argv) {
  Context ctx = createContext();
  static constexpr size_t N = 10000;
  std::array<float, N> inputArr, outputArr;
  for (int i = 0; i < N; ++i) {
    inputArr[i] = static_cast<float>(i) / 10.0; // dummy input data
  }
  Tensor input = createTensor(ctx, Shape{N}, kf32, inputArr.data());
  Tensor output = createTensor(ctx, Shape{N}, kf32);
  std::promise<void> promise;
  std::future<void> future = promise.get_future();
  Kernel op = createKernel(ctx, createShader(kGelu, /* 1-D workgroup size */ 256, kf32),
                           Bindings{input, output},
                           /* number of workgroups */ {cdiv(N, 256), 1, 1});
  dispatchKernel(ctx, op, promise);
  wait(ctx, future);
  toCPU(ctx, output, outputArr.data(), sizeof(outputArr));
  for (int i = 0; i < 16; ++i) {
    printf(&quot;  gelu(%.2f) = %.2f\n&quot;, inputArr[i], outputArr[i]);
  }
  return 0;
}"><pre><code>#include &lt;array&gt;
#include &lt;cstdio&gt;
#include &lt;future&gt;

#include "gpu.h"

using namespace gpu; // createContext, createTensor, createKernel,
                     // createShader, dispatchKernel, wait, toCPU
                     // Bindings, Tensor, Kernel, Context, Shape, kf32

static const char *kGelu = R"(
const GELU_SCALING_FACTOR: f32 = 0.7978845608028654; // sqrt(2.0 / PI)
@group(0) @binding(0) var&lt;storage, read_write&gt; inp: array&lt;{{precision}}&gt;;
@group(0) @binding(1) var&lt;storage, read_write&gt; out: array&lt;{{precision}}&gt;;
@compute @workgroup_size({{workgroupSize}})
fn main(
    @builtin(global_invocation_id) GlobalInvocationID: vec3&lt;u32&gt;) {
    let i: u32 = GlobalInvocationID.x;
    if (i &lt; arrayLength(&amp;inp)) {
        let x: f32 = inp[i];
        out[i] = select(0.5 * x * (1.0 + tanh(GELU_SCALING_FACTOR 
                 * (x + .044715 * x * x * x))), x, x &gt; 10.0);
    }
}
)";

int main(int argc, char **argv) {
  Context ctx = createContext();
  static constexpr size_t N = 10000;
  std::array&lt;float, N&gt; inputArr, outputArr;
  for (int i = 0; i &lt; N; ++i) {
    inputArr[i] = static_cast&lt;float&gt;(i) / 10.0; // dummy input data
  }
  Tensor input = createTensor(ctx, Shape{N}, kf32, inputArr.data());
  Tensor output = createTensor(ctx, Shape{N}, kf32);
  std::promise&lt;void&gt; promise;
  std::future&lt;void&gt; future = promise.get_future();
  Kernel op = createKernel(ctx, createShader(kGelu, /* 1-D workgroup size */ 256, kf32),
                           Bindings{input, output},
                           /* number of workgroups */ {cdiv(N, 256), 1, 1});
  dispatchKernel(ctx, op, promise);
  wait(ctx, future);
  toCPU(ctx, output, outputArr.data(), sizeof(outputArr));
  for (int i = 0; i &lt; 16; ++i) {
    printf("  gelu(%.2f) = %.2f\n", inputArr[i], outputArr[i]);
  }
  return 0;
}
</code></pre></div>
<p dir="auto">Here we see the GPU code is quoted in a domain specific language called WGSL (WebGPU Shading Language). In a larger project, you might store this code in a separate file to be loaded at runtime (see <a href="https://github.com/AnswerDotAI/gpu.cpp/tree/main/examples/shadertui">examples/shadertui</a> for a demonstration of live WGSL code re-loading).</p>
<p dir="auto">The CPU code in main() sets up the host coordination for the GPU computation.
We can think of the use of gpu.cpp library as a collection of GPU nouns and
verbs.</p>
<p dir="auto">The "nouns" are GPU resources modeled by the type definitions of the library
and the "verbs" actions on GPU resources, modeled by the functions of the
library. The ahead-of-time resource acquisition functions are prefaced with
<code>create*</code>, such as:</p>
<ul dir="auto">
<li><code>createContext()</code> - constructs a reference to the GPU device context (<code>Context</code>).</li>
<li><code>createTensor()</code> - acquires a contiguous buffer on the GPU (<code>Tensor</code>).</li>
<li><code>createShader()</code> - constructs WGSL code string to run on the GPU) (<code>ShaderCode</code>)</li>
<li><code>createKernel()</code> - constructs a handle to resources for the GPU computation (<code>Kernel</code>), which combines bindings to GPU buffers from <code>createTensor()</code> with the computation definition from <code>createShader()</code>.</li>
</ul>
<p dir="auto">These resource acquisition functions are tied to resource types for interacting with the GPU:</p>
<ul dir="auto">
<li><code>Context</code> - a handle to the state of resources for interacting with the GPU device.</li>
<li><code>Tensor</code> - a buffer of data on the GPU.</li>
<li><code>ShaderCode</code> - the code for a shader program that can be dispatched to the
GPU. This is a thin wrapper around a WGSL string but also includes the
workgroup size the code is designed to run with.</li>
<li><code>Kernel</code> - a GPU program that can be dispatched to the GPU. This accepts a
<code>ShaderCode</code> and a list of <code>Tensor</code> resources to bind for the dispatch
computation. This takes an argument <code>Bindings</code> that is a list of <code>Tensor</code> instances and should map the bindings declared at the top of the WGSL code. In this example there's two bindings corresponding to the <code>input</code> buffer on the GPU and the <code>ouptut</code> buffer on the GPU.</li>
</ul>
<p dir="auto">In this example, the GELU computation is performed only once and the program immediately exits so preparing resources and dispatch are side-by-side. Other examples in the <a href="https://github.com/AnswerDotAI/gpu.cpp/blob/main/examples/">examples/</a> directory illustrate how resource acquisition is prepared ahead of time and dispatch occurs in the hot path like a render, model inference, or simulation loop.</p>
<p dir="auto">Besides the <code>create*</code> resource acquisition functions, there are a few more "verbs" in the gpu.cpp library for handling dispatching execution to the GPU and data movement:</p>
<ul dir="auto">
<li><code>dispatchKernel()</code> - dispatches a <code>Kernel</code> to the GPU for computation. This is an asynchronous operation that returns immediately.</li>
<li><code>wait()</code> - blocks until the GPU computation is complete. This is a standard C++ future/promise pattern.</li>
<li><code>toCPU()</code> - moves data from the GPU to the CPU. This is a synchronous operation that blocks until the data is copied.</li>
<li><code>toGPU()</code> - moves data from the CPU to the GPU. This is a synchronous operation that blocks until the data is copied. In this particular example, <code>toGPU()</code> is not used because there's only one data movement from CPU to GPU in the program and that happens when the <code>createTensor()</code> function is called.</li>
</ul>
<p dir="auto">This example is available in <a href="https://github.com/AnswerDotAI/gpu.cpp/blob/main/examples/hello_world/run.cpp">examples/hello_world/run.cpp</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Other Examples: Matrix Multiplication, Physics Sim, and SDF Rendering</h2><a id="user-content-other-examples-matrix-multiplication-physics-sim-and-sdf-rendering" aria-label="Permalink: Other Examples: Matrix Multiplication, Physics Sim, and SDF Rendering" href="#other-examples-matrix-multiplication-physics-sim-and-sdf-rendering"></a></p>
<p dir="auto">You can explore the example projects in
<a href="https://github.com/AnswerDotAI/gpu.cpp/blob/main/examples/">examples/</a> which
illustrate how to use gpu.cpp as a library.</p>
<p dir="auto">After you have run <code>make</code> in the top-level directory which retrieves the prebuilt Dawn shared library, you can run each example by navigating to its directory and running <code>make</code> from the example's directory.</p>
<p dir="auto">An example of tiled matrix multiplication is in <a href="https://github.com/AnswerDotAI/gpu.cpp/blob/main/examples/matmul/">examples/matmul</a>. This implements a WebGPU version of the first few kernels of Simon Boehm's <a href="https://siboehm.com/articles/22/CUDA-MMM" rel="nofollow">How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog</a> post. It is only weakly optimized (up to 1D blocktiling, kernel number 4) at ~ 1.2+ TFLOPs on a Macbook Pro M1 laptop, which has a theoretical peak of 10.4 TFLOPs. Contributions to optimize this further are welcome - kernels 5-9 of Simon's post would be a natural starting point.</p>
<p dir="auto">A parallel physics simulation of an ensemble of double pendulums simulated in parallel with different initial conditions on the GPU is shown in <a href="https://github.com/AnswerDotAI/gpu.cpp/tree/main/examples/physics">examples/physics</a>.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/AnswerDotAI/gpu.cpp/blob/main/docs/images/matmul.png"><img src="https://github.com/AnswerDotAI/gpu.cpp/raw/main/docs/images/matmul.png" alt="matmul example output" width="40%"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/AnswerDotAI/gpu.cpp/blob/main/docs/images/pendulum.gif"><img src="https://github.com/AnswerDotAI/gpu.cpp/raw/main/docs/images/pendulum.gif" alt="physics example animated gif" width="42%" data-animated-image=""></a>
</p>
<p dir="auto">We also show some examples of signed distance function computations, rendered in the terminal as ascii. A 3D SDF of spheres is shown in <a href="https://github.com/AnswerDotAI/gpu.cpp/tree/main/examples/render%5D">examples/render</a> and a shadertoy-like live-reloading example is in <a href="https://github.com/AnswerDotAI/gpu.cpp/tree/main/examples/shadertui">examples/shadertui</a>.</p>
<p dir="auto">Interestingly, with a starting example, LLMs such as Claude 3.5 Sonnet can be quite capable at writing low-level WGSL code for you - the other shaders in the shadertui example are written by the LLM.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/AnswerDotAI/gpu.cpp/blob/main/docs/images/shadertui.gif"><img src="https://github.com/AnswerDotAI/gpu.cpp/raw/main/docs/images/shadertui.gif" alt="shadertui example animated gif" width="88%" data-animated-image=""></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Who is gpu.cpp for?</h2><a id="user-content-who-is-gpucpp-for" aria-label="Permalink: Who is gpu.cpp for?" href="#who-is-gpucpp-for"></a></p>
<p dir="auto">gpu.cpp is aimed at enabling projects requiring portable on-device GPU computation with minimal implementation complexity and friction. Some example use cases are:</p>
<ul dir="auto">
<li>Development of GPU algorithms to be run on personal computing devices</li>
<li>Direct standalone implementations of neural network models</li>
<li>Physics simulations and simulation environments</li>
<li>Multimodal applications - audio and video processing</li>
<li>Offline graphics rendering</li>
<li>ML inference engines and runtimes</li>
<li>Parallel compute intensive data processing applications</li>
</ul>
<p dir="auto">Although gpu.cpp is meant for any general purpose GPU computation and not strictly AI, one area we're interested in is pushing the limits exploring the intersection of new algorithms for post-training and on-device compute.</p>
<p dir="auto">To date, AI research has primarily been built with CUDA as the priveledged first-class target. CUDA has been dominant at large scale training and inference but at the other end of the the spectrum in the world of GPU compute on personal devices, there exists far more heterogeneity in the hardware and software stack.</p>
<p dir="auto">GPU compute in this personal device ecosystem has been largely limited to a small group of experts such as game engine developers and engineers working directly on ML compilers or inference runtimes. Along with that, implementing against the Vulkan or even WebGPU API directly tends to be targeted mostly towards infrastrcture scale efforts - game engines, production ML inference engines, large software packages.</p>
<p dir="auto">We want to make it easier for a broader range of projects to harness the power of GPUs on personal devices. With a small amount of code, we can access the GPU at a low-level, focusing on directly implementing algorithms rather than the scaffolding and tech stack around the GPU. For example, in our AI research there's much to explore with the various forms of dynamic/conditional post-training computation - dynamic use of adapters, sparsity, model compression, realtime multimodal integrations etc.</p>
<p dir="auto">gpu.cpp lets us implement and drop-in any algorithm with fine-grained control of data movement and GPU code, and explore outside boundaries of what is supported by existing production-oriented inference runtimes. At the same time we can write code that is portable and immediately usable on a wide variety of and GPU vendors and compute form factors - workstations, laptops, mobile, or even emerging hardware platforms such as AR/VR and robotics.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">What gpu.cpp is not</h2><a id="user-content-what-gpucpp-is-not" aria-label="Permalink: What gpu.cpp is not" href="#what-gpucpp-is-not"></a></p>
<p dir="auto">gpu.cpp is meant for developers with some familiarity with C++ and GPU programming. It is not a high-level numerical computing or machine learning framework or inference engine, though it can be used in support of such implementations.</p>
<p dir="auto">Second, in spite of the name, WebGPU has native implementations decoupled from the web and the browser. gpu.cpp leverages WebGPU as a portable <em>native</em> GPU API first and foremost, with the possibility of running in the browser being being a convenient additional benefit in the future.</p>
<p dir="auto">If you find it counerintuitive, as many do, that WebGPU is a native technology and not just for the web, watch Elie Michel's excellent talk <a href="https://www.youtube.com/watch?v=qHrx41aOTUQ" rel="nofollow">"WebGPU is Not Just About the Web"</a>.</p>
<p dir="auto">Finally, the focus of gpu.cpp is general-purpose GPU computation rather than rendering/graphics on the GPU, although it can be useful for offline rendering or video processing use cases. We may explore directions with graphics in the future, but for now our focus is GPU compute.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Limitations and Upcoming Features</h2><a id="user-content-limitations-and-upcoming-features" aria-label="Permalink: Limitations and Upcoming Features" href="#limitations-and-upcoming-features"></a></p>
<p dir="auto"><em>API Improvements</em> - gpu.cpp is a work-in-progress and there are many features and improvements to come. At this early stage, we expect the API design to evolve as we identify improvements / needs from use cases. In particular, the handling of structured parameters and asynchronous dispatch will undergo refinement and maturation in the short-term.</p>
<p dir="auto"><em>Browser Targets</em> - In spite of using WebGPU we haven't tested builds targeting the browser yet though this is a short-term priority.</p>
<p dir="auto"><em>Reusable Kernels and Shader Library</em> - Currently the core library is strictly the operations and types for interfacing with the WebGPU API, with some specific use case example WGSL implementations in <code>examples/</code>. Over time, as kernel implementations mature we may migrate some of the reusable operations from specific examples into a small reusable kernel library.</p>
<p dir="auto"><em>More Use Case Examples and Tests</em> - Expect an iteration loop of use cases to design tweaks and improvements, which in turn make the use cases cleaner and easier to write. One short term use cases to flesh out the kernels from <a href="https://github.com/karpathy/llm.c">llm.c</a> in WebGPU form. As these mature into a reusable kernel library, we hope to help realize the potential for WebGPU compute in AI.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Troubleshooting</h2><a id="user-content-troubleshooting" aria-label="Permalink: Troubleshooting" href="#troubleshooting"></a></p>
<p dir="auto">If you run into issues building the project, please open an issue.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgements</h2><a id="user-content-acknowledgements" aria-label="Permalink: Acknowledgements" href="#acknowledgements"></a></p>
<p dir="auto">gpu.cpp makes use of:</p>
<ul dir="auto">
<li><a href="https://dawn.googlesource.com/dawn" rel="nofollow">Dawn</a> as the WebGPU implementation</li>
<li><a href="https://github.com/jspanchu/webgpu-dawn-binaries">webgpu-dawn-binaries</a> by
@jspanchu to build a binary artifact of Dawn.</li>
<li><a href="https://github.com/eliemichel/WebGPU-distribution">webgpu-distribution</a> by
@eliemichel for cmake builds.</li>
</ul>
<p dir="auto">Thanks also to fellow colleagues at Answer.AI team for their support, testing help, and feedback.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Discord Community and Contributing</h2><a id="user-content-discord-community-and-contributing" aria-label="Permalink: Discord Community and Contributing" href="#discord-community-and-contributing"></a></p>
<p dir="auto">Join our community in the <code>#gpu-cpp</code> channel on the <a href="https://discord.gg/zmJVhXsC7f" rel="nofollow">AnswerDotAI Discord with this invite link</a>. Feel free to get in touch via X <a href="https://twitter.com/austinvhuang" rel="nofollow">@austinvhuang</a> as well.</p>
<p dir="auto">Feedback, issues and pull requests are welcome.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Wireless Amiga Tank Mouse (109 pts)]]></title>
            <link>https://lyonsden.net/wireless-tank-mouse/</link>
            <guid>40951877</guid>
            <pubDate>Sat, 13 Jul 2024 04:58:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lyonsden.net/wireless-tank-mouse/">https://lyonsden.net/wireless-tank-mouse/</a>, See on <a href="https://news.ycombinator.com/item?id=40951877">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<p>I missed the initial <a href="https://www.kickstarter.com/projects/lukas-remis/tank-mouse-your-new-amiga-mouse" target="_blank" rel="noopener nofollow">Kickstarter</a> for this ‘Wireless Tank Mouse’ back in 2022 but now that they are on general sale I decided to pick one up from <a href="https://sordan.ie/product/1494/tank-mouse-white-beige-usb-wireless-bluetooth-amiga-pc--adapter/" target="_blank" rel="noopener">Sordan.ie</a> to see if it was any good and hopefully use it with my Commodore and <a href="https://lyonsden.net/my-mega65-is-finally-here/" target="_blank" rel="noopener">MEGA65</a>&nbsp;machines.</p>

<p>This slideshow requires JavaScript.</p>

<p>The packaging is certainly very attractive featuring a very 80’s neon VHS style design on the front and a multitude of specs and features plastered over the sides and back of the box.</p>

<div id="attachment_8248"><p><img decoding="async" aria-describedby="caption-attachment-8248" data-attachment-id="8248" data-permalink="https://lyonsden.net/wireless-tank-mouse/img_4026/" data-orig-file="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4026-scaled.jpg" data-orig-size="2560,1920" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;2.2&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;iPhone 15 Pro Max&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1720022258&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;2.2200000286119&quot;,&quot;iso&quot;:&quot;320&quot;,&quot;shutter_speed&quot;:&quot;0.016666666666667&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="IMG_4026" data-image-description="" data-image-caption="" data-medium-file="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4026-300x225.jpg" data-large-file="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4026-1024x768.jpg" tabindex="0" role="button" src="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4026-1024x768.jpg" alt="Wireless Tank Mouse" width="730" height="548" srcset="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4026-1024x768.jpg 1024w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4026-300x225.jpg 300w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4026-768x576.jpg 768w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4026-1536x1152.jpg 1536w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4026-2048x1536.jpg 2048w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4026-356x267.jpg 356w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4026-730x548.jpg 730w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4026-1070x803.jpg 1070w" sizes="(max-width: 730px) 100vw, 730px"></p><p id="caption-attachment-8248">Wireless Tank Mouse</p></div>

<p>In the flesh it certainly looks the part and is very similar to the Amiga tank mouse we all know and love. Of course there’s two striking differences, the first of which is that this is now an optical mouse with the ball having been relegated to the history books. The second is the lack of a wire as this is now fully wireless, utilising either Bluetooth or 2.4Ghz to transmit the necessary signals back to your computer. You can switch between these options using a little 3-way switch underneath. This same switch is also used to power the mouse off when it’s placed in the middle position.</p>

<div id="attachment_8249"><p><img decoding="async" aria-describedby="caption-attachment-8249" data-attachment-id="8249" data-permalink="https://lyonsden.net/wireless-tank-mouse/img_4027/" data-orig-file="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4027-scaled.jpg" data-orig-size="2560,1920" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;2.2&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;iPhone 15 Pro Max&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1720022276&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;2.2200000286119&quot;,&quot;iso&quot;:&quot;400&quot;,&quot;shutter_speed&quot;:&quot;0.02&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="IMG_4027" data-image-description="" data-image-caption="" data-medium-file="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4027-300x225.jpg" data-large-file="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4027-1024x768.jpg" tabindex="0" role="button" src="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4027-1024x768.jpg" alt="Wireless Tank Mouse" width="730" height="548" srcset="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4027-1024x768.jpg 1024w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4027-300x225.jpg 300w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4027-768x576.jpg 768w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4027-1536x1152.jpg 1536w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4027-2048x1536.jpg 2048w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4027-356x267.jpg 356w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4027-730x548.jpg 730w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4027-1070x803.jpg 1070w" sizes="(max-width: 730px) 100vw, 730px"></p><p id="caption-attachment-8249">Wireless Tank Mouse underside</p></div>

<p>An optional extra that I purchased was the Tom adapter which came packaged separately in a little plastic bag. I have to confess to wondering why it was called Tom for a little while until I remembered the Tom and Jerry cartoons and then it all made perfect sense! Without this little gizmo the mouse will only work with modern computers which would defeat the purpose of it for me.</p>

<div id="attachment_8239"><p><img loading="lazy" decoding="async" aria-describedby="caption-attachment-8239" data-attachment-id="8239" data-permalink="https://lyonsden.net/wireless-tank-mouse/img_4016/" data-orig-file="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4016-scaled.jpg" data-orig-size="2560,1920" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;2.2&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;iPhone 15 Pro Max&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1720022139&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;2.2200000286119&quot;,&quot;iso&quot;:&quot;1250&quot;,&quot;shutter_speed&quot;:&quot;0.020833333333333&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="IMG_4016" data-image-description="" data-image-caption="" data-medium-file="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4016-300x225.jpg" data-large-file="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4016-1024x768.jpg" tabindex="0" role="button" src="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4016-1024x768.jpg" alt="" width="730" height="548" srcset="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4016-1024x768.jpg 1024w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4016-300x225.jpg 300w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4016-768x576.jpg 768w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4016-1536x1152.jpg 1536w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4016-2048x1536.jpg 2048w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4016-356x267.jpg 356w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4016-730x548.jpg 730w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4016-1070x803.jpg 1070w" sizes="(max-width: 730px) 100vw, 730px"></p><p id="caption-attachment-8239">Wireless Tank Mouse with Tom adapter</p></div>

<p>The Tom device is basically just a USB to DB9 adapter with some clever electronics to convert the USB signals into something old Commodore computers can understand. Compared to most of the Tom adapters available on eBay this one seems to be extremely compact and is actually referred to as a ‘Micro Tom’. It’s housed in a really nice 3D printed case that holds everything together securely.</p>

<p>This slideshow requires JavaScript.</p>

<p>When placed next to my original Amiga Tank mouse you can see just how close the design is. The main visible difference, besides the obvious lack of a wire, is that the buttons are a little narrower and spaced further apart on the new mouse. There’s a good reason for this that I will get to later on.</p>

<p>This slideshow requires JavaScript.</p>

<p>Opening up the battery compartment reveals a small USB receiver tucked away at the bottom.</p>

<div id="attachment_8251"><p><img loading="lazy" decoding="async" aria-describedby="caption-attachment-8251" data-attachment-id="8251" data-permalink="https://lyonsden.net/wireless-tank-mouse/img_4029/" data-orig-file="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4029-scaled.jpg" data-orig-size="2560,1920" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;2.2&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;iPhone 15 Pro Max&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1720022301&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;2.2200000286119&quot;,&quot;iso&quot;:&quot;400&quot;,&quot;shutter_speed&quot;:&quot;0.024390243902439&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="IMG_4029" data-image-description="" data-image-caption="" data-medium-file="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4029-300x225.jpg" data-large-file="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4029-1024x768.jpg" tabindex="0" role="button" src="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4029-1024x768.jpg" alt="Wireless tank mouse" width="730" height="548" srcset="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4029-1024x768.jpg 1024w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4029-300x225.jpg 300w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4029-768x576.jpg 768w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4029-1536x1152.jpg 1536w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4029-2048x1536.jpg 2048w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4029-356x267.jpg 356w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4029-730x548.jpg 730w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4029-1070x803.jpg 1070w" sizes="(max-width: 730px) 100vw, 730px"></p><p id="caption-attachment-8251">Wireless tank mouse battery compartment and USB receiver storage</p></div>

<p>This USB receiver can be plugged directly into a modern computer should you wish (Mac, Linux or PC) so you can use the mouse with those systems (or emulators running on them). If your computer has Bluetooth then you can leave the receiver in its cubby hole and simply pair it with your PC using that instead.</p>

<div id="attachment_8252"><p><img loading="lazy" decoding="async" aria-describedby="caption-attachment-8252" data-attachment-id="8252" data-permalink="https://lyonsden.net/wireless-tank-mouse/img_4030/" data-orig-file="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4030-scaled.jpg" data-orig-size="2560,1920" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;2.2&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;iPhone 15 Pro Max&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1720022311&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;2.2200000286119&quot;,&quot;iso&quot;:&quot;800&quot;,&quot;shutter_speed&quot;:&quot;0.025&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="IMG_4030" data-image-description="" data-image-caption="" data-medium-file="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4030-300x225.jpg" data-large-file="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4030-1024x768.jpg" tabindex="0" role="button" src="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4030-1024x768.jpg" alt="USB Receiver" width="730" height="548" srcset="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4030-1024x768.jpg 1024w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4030-300x225.jpg 300w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4030-768x576.jpg 768w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4030-1536x1152.jpg 1536w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4030-2048x1536.jpg 2048w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4030-356x267.jpg 356w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4030-730x548.jpg 730w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4030-1070x803.jpg 1070w" sizes="(max-width: 730px) 100vw, 730px"></p><p id="caption-attachment-8252">USB Receiver</p></div>

<p>To use the mouse with Commodore machines you need to insert the USB receiver into the Tom adapter.</p>

<div id="attachment_8254"><p><img loading="lazy" decoding="async" aria-describedby="caption-attachment-8254" data-attachment-id="8254" data-permalink="https://lyonsden.net/wireless-tank-mouse/img_4039-2/" data-orig-file="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4039-scaled.jpg" data-orig-size="2560,1920" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;2.2&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;iPhone 15 Pro Max&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1720022474&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;2.2200000286119&quot;,&quot;iso&quot;:&quot;400&quot;,&quot;shutter_speed&quot;:&quot;0.025&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="IMG_4039" data-image-description="" data-image-caption="" data-medium-file="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4039-300x225.jpg" data-large-file="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4039-1024x768.jpg" tabindex="0" role="button" src="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4039-1024x768.jpg" alt="Tom adapter with USB receiver plugged in" width="730" height="548" srcset="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4039-1024x768.jpg 1024w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4039-300x225.jpg 300w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4039-768x576.jpg 768w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4039-1536x1152.jpg 1536w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4039-2048x1536.jpg 2048w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4039-356x267.jpg 356w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4039-730x548.jpg 730w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4039-1070x803.jpg 1070w" sizes="(max-width: 730px) 100vw, 730px"></p><p id="caption-attachment-8254">Tom adapter with USB receiver plugged in</p></div>

<p>It was a really tight fit, so much so that I felt the need to double check I was putting it in correctly before risking applying more force to get the job done!</p>

<div id="attachment_8253"><p><img loading="lazy" decoding="async" aria-describedby="caption-attachment-8253" data-attachment-id="8253" data-permalink="https://lyonsden.net/wireless-tank-mouse/img_4037/" data-orig-file="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4037-scaled.jpg" data-orig-size="2560,1920" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;2.2&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;iPhone 15 Pro Max&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1720022417&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;2.2200000286119&quot;,&quot;iso&quot;:&quot;400&quot;,&quot;shutter_speed&quot;:&quot;0.025&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="IMG_4037" data-image-description="" data-image-caption="" data-medium-file="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4037-300x225.jpg" data-large-file="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4037-1024x768.jpg" tabindex="0" role="button" src="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4037-1024x768.jpg" alt="Wireless tank mouse batteries" width="730" height="548" srcset="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4037-1024x768.jpg 1024w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4037-300x225.jpg 300w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4037-768x576.jpg 768w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4037-1536x1152.jpg 1536w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4037-2048x1536.jpg 2048w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4037-356x267.jpg 356w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4037-730x548.jpg 730w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4037-1070x803.jpg 1070w" sizes="(max-width: 730px) 100vw, 730px"></p><p id="caption-attachment-8253">Let there be (green) light!</p></div>

<p>It’s powered by a couple of AAA batteries (rechargeable ones appear to work fine) which helps to add a bit of weight to it. I know some people like their mice light but I’m not one of them so the batteries went some way towards compensating for the lack of the ball.</p>

<h4>Mousing Around</h4>

<p>Before trying the wireless tank mouse with my retro machines I gave it a quick go in my Windows 11 PC and it worked immediately and completely flawlessly. I also tried it with my MorphOS Apple Mac Mini G4 with similar success. When used with modern machines you can use it with your favourite emulator for a more immersive experience or even as your daily driver should you wish.</p>
<p>In addition to the obligatory left and right mouse buttons the wireless tank mouse actually fully supports mouse scrolling too. Now you may well be thinking how’s that possible – it has no scroll wheel? Well remember earlier on when I mentioned about the buttons being spaced further apart? That’s because the designer has ingeniously incorporated a touch sensitive strip between the two buttons. You can simply swipe up and down with your finger to scroll in those directions. It works really well too. Obviously it lacks the tactile feedback of an actual wheel but having one of those would ruin the whole aesthetic.</p>

<div id="attachment_8258"><p><img loading="lazy" decoding="async" aria-describedby="caption-attachment-8258" data-attachment-id="8258" data-permalink="https://lyonsden.net/wireless-tank-mouse/img_4043/" data-orig-file="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4043-scaled.jpg" data-orig-size="2560,1920" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;1.78&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;iPhone 15 Pro Max&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1720022865&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;6.7649998656528&quot;,&quot;iso&quot;:&quot;500&quot;,&quot;shutter_speed&quot;:&quot;0.016666666666667&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="IMG_4043" data-image-description="" data-image-caption="" data-medium-file="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4043-300x225.jpg" data-large-file="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4043-1024x768.jpg" tabindex="0" role="button" src="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4043-1024x768.jpg" alt="wireless tank mouse with my A1200" width="730" height="548" srcset="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4043-1024x768.jpg 1024w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4043-300x225.jpg 300w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4043-768x576.jpg 768w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4043-1536x1152.jpg 1536w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4043-2048x1536.jpg 2048w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4043-356x267.jpg 356w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4043-730x548.jpg 730w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4043-1070x803.jpg 1070w" sizes="(max-width: 730px) 100vw, 730px"></p><p id="caption-attachment-8258">Using the wireless tank mouse with my A1200</p></div>

<p>I have plenty of mice for my modern computers though, I got this to use with my Amiga mainly so I plugged the Tom adapter into port 1 of my A1200 and powered it on, and it just worked. No messing around, no pairing, no software to install, just plug and play. The scrolling support worked seamlessly too, (I installed scroll wheel support for my existing wired Amga mouse years ago). Tracking was super accurate and smooth and the mouse was just as comfortable to hold as ever – in fact more so now as I’m not constantly fighting against the cable in the limited space I have available to use it!</p>

<div id="attachment_8269"><p><img loading="lazy" decoding="async" aria-describedby="caption-attachment-8269" data-attachment-id="8269" data-permalink="https://lyonsden.net/wireless-tank-mouse/img_4150/" data-orig-file="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4150-scaled.jpg" data-orig-size="2560,1920" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;2.2&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;iPhone 15 Pro Max&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1720727660&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;2.2200000286119&quot;,&quot;iso&quot;:&quot;1250&quot;,&quot;shutter_speed&quot;:&quot;0.03030303030303&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="IMG_4150" data-image-description="" data-image-caption="" data-medium-file="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4150-300x225.jpg" data-large-file="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4150-1024x768.jpg" tabindex="0" role="button" src="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4150-1024x768.jpg" alt="" width="730" height="548" srcset="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4150-1024x768.jpg 1024w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4150-300x225.jpg 300w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4150-768x576.jpg 768w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4150-1536x1152.jpg 1536w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4150-2048x1536.jpg 2048w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4150-356x267.jpg 356w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4150-730x548.jpg 730w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4150-1070x803.jpg 1070w" sizes="(max-width: 730px) 100vw, 730px"></p><p id="caption-attachment-8269">Wireless tank mouse ‘Tom’ adapter (on the right)</p></div>

<p>Satisfied that it worked with my Amiga I fired up my MEGA65 and loaded up GEOS65 and sure enough it worked really well with that too.</p>

<div id="attachment_8270"><p><img loading="lazy" decoding="async" aria-describedby="caption-attachment-8270" data-attachment-id="8270" data-permalink="https://lyonsden.net/wireless-tank-mouse/img_4146/" data-orig-file="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4146-scaled.jpg" data-orig-size="2560,1920" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;2.2&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;iPhone 15 Pro Max&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1720727535&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;2.2200000286119&quot;,&quot;iso&quot;:&quot;160&quot;,&quot;shutter_speed&quot;:&quot;0.0125&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="IMG_4146" data-image-description="" data-image-caption="<p>GEOS 65 running on my mEGA65 controlled by my wireless tank mouse</p>
" data-medium-file="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4146-300x225.jpg" data-large-file="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4146-1024x768.jpg" tabindex="0" role="button" src="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4146-1024x768.jpg" alt="" width="730" height="548" srcset="https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4146-1024x768.jpg 1024w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4146-300x225.jpg 300w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4146-768x576.jpg 768w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4146-1536x1152.jpg 1536w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4146-2048x1536.jpg 2048w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4146-356x267.jpg 356w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4146-730x548.jpg 730w, https://lyonsden.net/wordpress_s/wp-content/uploads/2024/07/IMG_4146-1070x803.jpg 1070w" sizes="(max-width: 730px) 100vw, 730px"></p><p id="caption-attachment-8270">GEOS 65 running on my MEGA65 controlled by my wireless tank mouse</p></div>

<p>Next I invoked ‘GO64’ and switched my MEGA65 into C64 mode to load up a BASIC <a href="https://lyonsden.net/a-c64-type-in-basic-listing-in-2021/" target="_blank" rel="noopener">Solitaire</a> game that I typed in a few years ago. This was designed to work the the Commodore 1351 mouse and sure enough this also worked like a charm with just occasional jittering. Incidentally this is also a game that really benefits from switching the CPU to 40Mhz mode – no more lag when shuffling the cards!</p>
<p>I tried it with my real C64C computer and ran into my first problem. It simply refused to work. After a bit of RTFM (or RTFW in this case) I discovered the following…</p>

<h4>Switching between C64 and Amiga Mode</h4>

<p>Out of the box the Tom adapter is configured to run in Amiga mode. Consequently when plugging it into a C64 it just doesn’t work. (Same for the alternative C64 CORE in the MEGA65). Thankfully it can be switched into C64 1351 mode quite easily by plugging in a USB mouse, holding down the middle button (usually the scroll wheel) at bootup and whilst still holding it down press the left mouse button a few times until the red LED flashes 3 times and then you can let go. The adapter will now behave like a 1351 mouse forever until you change it back.</p>
<p>I did come a little unstuck trying to change it back to Amiga mode though. Either I have a slightly different model of Tom adapter or whoever wrote the instructions on the Sordan site got a bit confused. Thankfully after about 30 minutes of swapping different mice, power cycling, cursing and failing I figured it out. To switch to Amiga mode I had to hold down the right mouse button at bootup and then rapidly press the left button until the red LED flashed just the one time and then let go. (The instructions tell you to do the complete opposite which makes it flash twice which appears to be Atari ST mode).</p>

<h4>What do I think of it?</h4>

<p>All in this is a fantastic product that works with a huge range of machines, both old and new. It oozes nostalgia thanks to its design but at the same time embraces modernity by swapping the ball for an optical sensor, the wire for wireless connectivity and adds ‘invisible’ scrolling support beneath the plastic case. It works with pretty much every machine I own; all my Amigas, CD32, MEGA65, Commodore 64, Mac Mini running MorphOS and even my PC’s and emulators. My only criticism is the method of switching modes on the Tom adapter is a bit of a faff. I would much prefer a little 3-way toggle switch than trying to pull off some kind of weird secret handshake with the mouse. Other than that though the wireless tank mouse seems to be a jack of all trades and a master of them all and I’m really glad I picked one up.</p>


				
					
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Introduction to Calvin and Hobbes: Sunday Pages 1985-1995 (2001) (228 pts)]]></title>
            <link>http://timhulsizer.com/cwords/cintro.html</link>
            <guid>40951800</guid>
            <pubDate>Sat, 13 Jul 2024 04:37:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://timhulsizer.com/cwords/cintro.html">http://timhulsizer.com/cwords/cintro.html</a>, See on <a href="https://news.ycombinator.com/item?id=40951800">Hacker News</a></p>
<div id="readability-page-1" class="page">

<b>
<center>
Back To the <a href="http://timhulsizer.com/cwords/calvin.html">Calvin &amp; Hobbes Page</a><br>
Back To the <a href="http://timhulsizer.com/cwords/cbooks.html">C&amp;H Books Page</a><br>
Back To the <a href="http://timhulsizer.com/cwords/index.html">Main Page</a><br>

<hr>

<h3>Introduction</h3>
<b>By Bill Watterson (C)2001
</b><p>From the book, "Calvin and Hobbes - Sunday Pages 1985 - 1995"</p></center>

</b>

<p>It's been five years since the end of <i>Calvin and Hobbes</i>, 
 the longest time I can remember in which I haven't drawn cartoons. 
 <i>Calvin and Hobbes</i> was a wonderful experience, but it was an 
 all-consuming career. When I quit the strip, I put my cartoons in 
 boxes, and jumped into other interests. I haven't really considered 
 the strip since, so at the invitation to do this show, I thought 
 it might be time to look back at some of my work. 

</p><p>My first reaction in going through my old cartoons was some 
 amazement at the size and weight of the pile. For most successful 
 comic strips, ten years is just a drop in the bucket, but even that 
 amount of time yields a huge amount of material. It's no wonder 
 that decade seems like a blur. 

</p><p>Going through my old strips is sort of like looking at old 
 photographs of myself: they're personal and familiar, yet somewhat 
 bizarre at the same time. There are cartoons I've drawn that are 
 the equivalent of pictures of my younger self wearing yellow 
 pants: I know I'm responsible for that, but what on earth was I 
 thinking? As my tastes have changed, and as I've learned more, I 
 imagine that I would do many strips quite differently today. Not 
 better necessarily, but certainly differently. I was twenty-eight 
 when <i>Calvin and Hobbes</i> was first published, and, of course, I would 
 make other choices now at age forty-three. 

</p><p>It's also sort of strange to see a record of my own learning 
 curve. Pick up a given strip, and I see how I struggled with 
 various writing and drawing problems, or how I finally surmounted 
 one. I remember sometimes feeling that the strip was better written 
 than I could actually write, and better drawn than I could actually 
 draw. I learned a great deal over the years by trying to push the 
 strip beyond my own abilities, and I'm very proud that <i>Calvin and 
 Hobbes</i> explored and developed all the way to the end. By the final 
 years, I see naturalness or a sense of inevitability to the drawing 
 and writing that is very satisfying. 


</p><p>I'm more appreciative of this kind of grace since returning to 
 the awkward stages of new learning curves. Of course, I'd also say 
 the times have caught up with some of my strips. It's frankly a 
 little discouraging to see how ordinary some of them look now. When 
 <i>Calvin and Hobbes</i> first appeared, it was somewhat surprising to 
 treat reality as subjective, and to draw a strip with multiple 
 viewpoints, juxtaposing Calvin's vision with what others saw. I did  
 this simply as a way to put the reader in Calvin's head and to 
 reveal his imaginative personality. Now these juxtapositions are a 
 visual game for many comic strips, and after all these years, I 
 suspect readers know where this sort of joke is headed as soon as 
 they see it. The novelty cannot be recaptured. 

</p><p>Novelty, however, is probably overrated anyway. The <i>Calvin and 
 Hobbes</i> strips that hold up best, to my eye anyway, are the ones 
 where the characters seem big, vivid, and full of life, and where 
 the strip's world seems genuine and inviting. Punchlines come and 
 go, but something in the friendship between Calvin and Hobbes seems  
 to hold a small piece of truth. Expressing something real and 
 honest is, for me, the joy and the importance of cartooning. 

</p><p>The Sunday strips were usually the cartoons I had the most fun 
 with, and for this show I've chosen a few Sunday strips from each 
 year that I think show off the strip's strengths. 

</p><p>I have fond memories of reading the Sunday comics when I was a 
 kid. As far as I was concerned, the Sunday comics were the whole 
 reason for newspapers to exist. On weekdays, I read only the strips 
 I liked; but on Sundays, I read them all, and often several times. 
 The Sunday comics were always the most fun to look at, so when I 
 finally got the chance to draw my own comic strip, I knew I wanted 
 to make the Sunday Calvin and Hobbes something special. It took me 
 a little while to learn to use the larger Sunday space effectively. 
 It requires a somewhat different pace for the humor, and, of 
 course, a big color panel is no place to find out that you don't 
 know how to draw the back of your character's head. The Sunday 
 strip shows off both strengths and weaknesses. 

</p><p>Occasionally I would see that an idea I'd written for a Sunday 
 strip was not as substantial as I'd hoped it would be, and I'd 
 realize that some of the panels and dialogue weren't adding 
 anything significant to the story. If that were the case, I'd 
 remove everything extraneous and use the trimmed idea for a daily 
 strip instead. I held the Sundays to a different standard: any idea 
 for the Sunday strip had to need the extra space. I felt a Sunday 
 strip should do something that was impossible the rest of the week. 

</p><p>Over the years, I learned that daily strips are better suited for 
 certain kinds of ideas, while Sunday strips are better for others. 
 The daily strip is quick and to the point, perfect for a simple 
 observation, or a short exchange between characters. Daily strips 
 are also better for long stories, where a certain suspense can be 
 fostered by continuing the story day after day, and the reader can 
 remember what happened previously. 

</p><p>Extended conversations with real back and forth dialogue, however, 
 don't work very well in four tiny panels - the dialogue balloons 
 crowd out the drawings and the strip loses its grace. In a Sunday 
 strip, you can spread out, and let the characters yap a bit. This 
 is often funny in itself, and it's a wonderful way to let the 
 characters' personalities emerge. It also lets you explore a topic 
 a bit more fully. 

</p><p>You can talk about things without reducing them to one-liners 
 right away. And, of course, in today's minuscule comics, if an idea 
 requires any real drawing, the Sunday strip is the only possible 
 place for it. Likewise, any complex storytelling problem-a strip 
 illustrating a long expanse of time, for example, or an event 
 depicted in a succession of very tiny moments-is futile in the 
 daily format. Calvin's fantasies generally migrated to the Sunday 
 page for this reason. 

</p><p>In short, the Sunday page offered unique opportunities, and I 
 deliberately tried to come up with ideas that could take advantage 
 of them. 

</p><p>I usually wrote the Sunday strips separately from the dailies. 
 For the daily strips, I tried to write an entire month's worth of 
 ideas before inking any of them. This allowed a long period for 
 editing and rewriting. I was less able to do this for the Sunday 
 strips because the Sundays need to be drawn weeks further in 
 advance and because the strips took so much longer to draw. If at 
 all possible, however, I would try to keep two or three Sunday 
 ideas ahead of the deadlines. I always wanted to reserve the option 
 of abandoning an idea that didn't stand up to a few weeks of 
 scrutiny. 

</p><p>For those who are interested in technical matters, the early 
 strips were drawn on any cheap pad of Bristol board the local art 
 supply store happened to stock. The paper was usually rather thin 
 and sometimes the sheet wouldn't accept the ink consistently (bad 
 sizing or something), which would make drawing aggravating and time 
 consuming. Eventually I switched to heavier Strathmore Bristol 
 board, which was much nicer. I used a 2H pencil to rough in the 
 drawing, and then inked with a small sable brush and India ink. I 
 did as little pencil work as possible in order to keep the inking 
 more spontaneous, although the more elaborate panels required more 
 preliminary drawing. For lettering, I used a Rapidograph cartridge 
 pen. I drew the dialogue balloons and a few odds and ends with a 
 crow quill pen. To cover up unwanted marks, I used various brands 
 of Wite-Out, and in the early days, typewriter correction fluid. 
 (Remember typewriters?) No doubt this stuff will eat through the 
 paper or turn green in a few years, but as the original cartoons 
 were intended for reproduction, not picture frames and gallery 
 walls, I did not overly concern myself with archival issues or, for 
 that matter, neatness. At some point along the way, however, I did 
 ask the syndicate to send the printers a quality reproduction of 
 the Sunday cartoon, rather than the original drawing, in order to 
 reduce the amount of tape, registration marks, and general 
 crunchings and manglings to which the drawings had previously been 
 subjected. 

</p><p>Coloring the strips was a slow and tedious process. My 
 syndicate gave me a printed sheet showing numbered squares of color, 
 each a mixture of various percentages of red, yellow, and blue. 
 Using this sheet as a guide, I taped some tracing paper over the 
 finished cartoon, and painted watercolor approximations of the 
 available colors in the areas I wanted. This would give me a very 
 rough idea of what the newspaper version might look like. Then I 
 numbered each little spot of color. As the Sunday strips became 
 more visually complex, and as I started to use color more 
 deliberately for effects, this process became a real chore. These 
 days, I believe much of it can be done with a few clicks of a mouse. 

</p><p>Colors take on different characteristics when placed next to 
 other colors (a neutral-seeming gray might look greenish and dark 
 next to one color, but brownish and pale in relation to another). 
 Because of this, I came up with one little trick for coloring the 
 strip. I cut out each of the color squares provided by the printer, 
 so I had a stack of colors (like paint chips), rather than a sheet. 
 By laying out the cut squares and physically placing one color next 
 to the others I expected to use, I could see exactly how each color 
 behaved in that particular context. As I got better at this, I was 
 able to choose approprial "palettes" for each strip, and create 
 moods with color. One strip might call for contrasting, bright 
 colors; another strip might be done with a limited group of soft, 
 warm colors; another idea might call for a close range of grays and 
 darks, and so on. If I made Calvin's skin a dull pink-gray to 
 suggest dim lighting at night, I would have to find a dull 
 yellow-gray that would suggest his hair in the same light. These 
 challenges took an inordinate amount of time for work on deadline, 
 but I was often quite proud of the results. A comic strip should 
 always be fun to look at, and good use of color can contribute to 
 that appeal More than that, color creates its own emotional impact, 
 which can make the drawing more expressive. 

</p><p>The half-page Sunday format required certain guaranteed panel 
 divisions. The strip had to be drawn in three rows of equal height, 
 and there was one unmovable panel division within each row. This 
 allowed editors to reduce and reconfigure the strip to suit their 
 particular space needs. The same strip could run in several shapes 
 by restacking the panels. 

</p><p>Editors commonly removed the entire top row altogether, so in 
 essence, a third of the strip had to be wasted on "throwaway 
 panels" that many readers would never see. The fixed panel divisions 
 were also annoying because they limited my ability to compose the 
 strip to best suit the idea. For example, they often forced a small 
 panel where I needed more space for words. 

</p><p>Of course, a big part of cartooning is learning to work 
 effectively within tight space constraints. Much of cartooning's 
 power comes from its ability to do more with less: when the 
 drawings and ideas are distilled to their essences, the result can 
 be more beautiful and powerful for having eliminated the clutter. 
 That said, there is a point at which simplification thwarts good 
 storytelling. You can't condense Moby Dick into a paragraph and get 
 the same effect. Over the years, my frustration increased and I 
 became convinced that I could draw a better comic strip than the 
 current newspaper format was permitting. Looking at examples of 
 comics from the 1930s, when a Sunday strip could fill an entire 
 page, I was amazed by the long-forgotten possibilities out there. 

</p><p>I took a sabbatical after resolving a long and emotionally 
 draining fight to prevent <i>Calvin and Hobbes</i> from being 
 merchandised. Looking for a way to rekindle my enthusiasm for the 
 duration of a new contract term, I proposed a redesigned Sunday 
 format that would permit more panel flexibility. To my surprise and 
 delight, Universal responded with an offer to market the strip as 
 an unbreakable half page (more space than I'd dared to ask for), 
 despite the expected resistance of editors. 

</p><p>To this day, my syndicate assures me that some editors liked the 
 new format, appreciated the difference, and were happy to run the 
 larger strip, but I think it's fair to say that this was not the 
 most common reaction. The syndicate had warned me to prepare for 
 numerous cancellations of the Sunday feature, but after a few weeks 
 of dealing with howling, purple-faced editors, the syndicate 
 suggested that papers could reduce the strip to the size tabloid 
 newspapers used for their smaller sheets of paper. Another strip 
 could then run vertically down the side. Consequently, while some 
 papers, primarily in larger markets, ran the strip as a half page, 
 other papers reduced it. In some of the latter papers (including 
 the one I read at the time), I actually lost ground: the new Sunday 
 strip was printed even smaller than before. I was in no mood to 
 take on new fights, so I focused on the bright side: I had complete 
 freedom of design and there were virtually no cancellations. 

</p><p>For all the yelling and screaming by outraged editors, I remain 
 convinced that the larger Sunday strip gave newspapers a better 
 product and made the comics section more fun for readers. Comics 
 are a visual medium. A strip with a lot of drawing can be exciting 
 and add some variety. Proud as I am that I was able to draw a 
 larger strip, I don't expect to see it happen again any time soon. 
 In the newspaper business, space is money, and I suspect most 
 editors would still say that the difference is not worth the cost. 
 Sadly, the situation is a vicious circle: because there's no room 
 for better artwork, the comics are simply drawn; because they're 
 simply drawn, why should they have more room? 

</p><p>Business controversies aside, the new format opened up new ways 
 to tell stories, and I drew different kinds of strips as a result. 
 I could write and draw the strip exactly as I imagined it, so it 
 truly challenged my abilities. Whereas Sunday strips had previously 
 taken me a full day to draw and color, a complex strip would now 
 take me well into a second day to finish. Deadlines discourage this 
 kind of indulgence, and I had to steal that extra time from what 
 would have been some semblance of an ordinary life, but I was 
 thrilled to expand the strip's world. 

</p><p>Laying out the panels became a job in itself, now that I was no 
 longer confined to horizontal rows. could place boxes anywhere and 
 any size, but the reader's eye needs to flow naturally to the 
 proper panels without confusion, and big panels need to be designed 
 in such a way that they don't divert attention and spoil surprises. 
 The graphic needs of each panel must be accommodated and the panels 
 themselves should form a pleasing arrangement so the entire page is 
 attractive, balanced, and unified as well. Here again I looked for 
 guidance in the gorgeous Sunday pages of George Herriman's <i>Krazy Kat.</i>
 
</p><p>The new Sunday format necessitated a change in the format of my 
 book collections as well. Having won a bigger strip in newspapers, 
 I wanted the book reproductions to reflect the strip's new impact 
 as much as possible by printing the Sunday strips large. This 
 resulted in the rather awkward horizontal format of my later books. 
 They stick out of bookshelves, but the strips look nice. From this 
 point on, the Sunday strips were reproduced in color with each 
 collection, not just in the "treasury" collections, as before. 
 (Here's a piece of trivia: because of the timing of the book format 
 change, the cartoons from the <i>Snow Goons</i> collection were 
 never put in a treasury book, so those Sunday strips have been 
 reprinted only in black-and-white.)
 
</p><p>Ten years after starting <i>Calvin and Hobbes</i>, I ended the 
 strip. As much as I knew I'd miss the characters, the decision was 
 long anticipated on my part. Professionally, I had accomplished far 
 more than I'd ever set out to do and there were no more mountains I 
 wanted to climb. Creatively, my interests were shifting away from 
 cartooning toward painting, where I could develop my drawing skills 
 further. And personally, I wanted to restore some balance to my 
 life. I had given the strip all my time and energy for a decade 
 (and was happy to do so), but now I was that much older and I 
 wanted to work at a more thoughtful pace, out of the limelight, and 
 without the pressures and restrictions of newspapers. 

</p><p>The final Calvin and Hobbes strip was a Sunday strip. The 
 deadline for Sunday strips being early, I drew it well before 
 writing the daily strips that would eventually precede it in the 
 newspaper. I very much wanted to hit the right note for this final 
 strip. I think it worked, but it was a bittersweet strip to draw. 

</p><p>Since <i>Calvin and Hobbes</i>, I've been teaching myself how to 
 paint, and trying to learn something about music. I have no 
 background in either subject, and there are certainly days when I 
 wonder what made me trade proficiency and understanding in one 
 field for clumsiness and ignorance in these others. On better 
 days, I enjoy having so many new challenges and surprises. Even so, 
 these new endeavors have only deepened my appreciation for comics. 
 I no longer take quite so much for granted the versatility of 
 comics and their ability to depict complex ideas in a beautiful, 
 accessible, and entertaining form. For all their seeming simplicity, 
 the expressive possibilities of comics rival those of any other art 
 form. Five years after <i>Calvin and Hobbes</i>, I love the comics 
 as much as ever. 

</p><p>Bill Watterson
<br>Summer 2001 

</p><center><hr><b>
Back To the <a href="http://timhulsizer.com/cwords/calvin.html">Calvin &amp; Hobbes Page</a><br>
Back To the <a href="http://timhulsizer.com/cwords/cbooks.html">C&amp;H Books Page</a><br>
Back To the <a href="http://timhulsizer.com/cwords/index.html">Main Page</a><br>

</b>
<hr>



</center></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Use a Work Journal to Recover Focus Faster and Clarify Your Thoughts (796 pts)]]></title>
            <link>https://fev.al/posts/work-journal/</link>
            <guid>40950584</guid>
            <pubDate>Sat, 13 Jul 2024 00:05:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fev.al/posts/work-journal/">https://fev.al/posts/work-journal/</a>, See on <a href="https://news.ycombinator.com/item?id=40950584">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
    
    <p>You’re working on the most complex problem in computer science: fixing permissions on a deployment pipeline. It’s been 4 days you started on that simple task already. Your manager explained to you in no uncertain terms that your performance on the subject is well below the expectations she has from a midterm intern. Your colleagues stay as far away as possible from you to avoid getting tainted by your shameful failure. 4 days of sleepless afternoons, seeing that freaking status turning to “build failed” everytime, bringing you to tears. The weather is shit, rain taping on the window of your overpriced basement suite, reflecting the state of your soul. You never felt so alone. Even your partner left you, you loser!</p>

<p>But this time you got it. This time you have a strategy beyond clicking on “retry failed steps” again and again. You finally read the logs, and you have an idea. You think you finely grasped what might be wrong. It’s a long shot: you’re gonna clear the credential cache, get an elevation you’re missing, force a permission sync, reset the service connection to the cluster, then downgrade the stupid auth library you’re using to a version that was hacked 6y ago but still works, setup everything again, then rollback. Your brain is making the connections, you got it, you’re better than that. You’re sorting through 23 different documentation tabs, waiting for the elevation to succeed, summoning all precious focus you got.</p>

<p>A red bubble shows up on IM. Conditioned by years of desperately reading your texts as soon as they arrived to create yourself what passes for a social life, your hand doesn’t even consults the sentient part of your cortex, and just moves to the little icon. <em>click</em>. It’s Mitch, your PM. He’s asking the url for a doc he wrote, and complains that it’s so complex to find doc in this organization.</p>

<p>This is a trap meant to get your focus out. Not this time. <strong>You’re better than that</strong>. You ignore his message, look at the elevation command, trigger it. Copy the id of the request that you need to preciously keep to finish the elevation to your clipboard.</p>

<p>4 minutes later you get a call request from your manager. You answer.</p>

<blockquote>
  <p>Hey, Mitch is saying he needs a doc urgently and you’re not answering his IM. Can you get to it?</p>
</blockquote>

<p><em>poof</em></p>

<p>Where the freak was I?</p>

<p><img src="https://fev.al/img/2024/focus.png" alt="poof"> 
<em>Credit: <a href="https://monkeyuser.com/">monkeyuser.com</a>. notice how much better it conveys my story.</em></p>

<p>Like everyone, I sometimes struggle to maintain focus. This was particularly true when I was a manager switching context all day long, this is also true as a  dev working on three antagonistic projects, including one that’s very consultory in nature, and working with processes that sometimes take hours to complete.</p>

<p>The typical situation is that I start something, switch to something else, get into a meeting, forget the very essence of what I was doing. Turn on autopilot, read all my emails, all my IMs. Then it’s 5PM, I’m exhausted, and I realize I’m at the same stage I was at 8AM, tell myself I should really achieve something that day, and open HN.</p>

<p>It was so bad at the beginning of this year that I seriously started wondering if I had ADHD<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup>. I started working on something badly documented. As in: there’s no documentation, the people who built that thing are halfway across the world and they don’t even really work for the company anymore. It wasn’t even just interruption, but also procrastination. I felt so frustrated by the situation that I started writing that in my daily notes on Obsidian.</p>

<blockquote>
  <p>I asked Berna how to turn on super-compression 5000, and she’s not answering again! I tried it with the <code>--yo-compress-shit really-well-like-5000-or-something</code> and it didn’t work. It still spits me that <code>yo is not proper English, be civilized</code> error. What the hell can I do about that.</p>
</blockquote>

<p>Mitch called me to ask for that doc again, which I gave him, not without mentioning that little “star” icon in the url bar. And then got back at it.</p>

<p>And boy oh boy did that help! I just re-read what I was doing, and boom! I was back in.</p>

<p>I started listing all the commands I was running, and their results. Writing down my train of thoughts, the things I was doing and what I wanted to do next. And I have been doing that for the past 3-4 months. I feel like I invented something new. It helps me think more clearly, and restore the context so, so much faster when I switch between things. I’m almost looking forward to an interruption to get a chance to marvel again at my genius!</p>

<p>Except that it’s nothing new, right? “Writing helps you organize your thoughts more clearly”: everyone and their grandmother know that! Writing a plan, writing a diary? People keep listing how transformative that’s been for them. I’m not proposing a new framework. I’m just saying - every movie has a scientist recording themselves on one of these shitty little cassette recorders. They might be onto something. Write notes of what you’re doing and what you’re thinking. When you drop the pen and get back at it, read the last bit. That’s it.</p>

<p>I’ve just been too lazy to ever do it. Or not necessarily lazy, but more: I didn’t trust the tool enough to think it was a good use of my time, and instead just mash on the keyboard till it works. After all, I’m writing pages of text, of which I will never read more than a fraction. But that’s not the point. The point is structure, and the point is caching.</p>

<p>I guess that’s kind of it: if you’re having trouble switching between things or getting focused, try writing what you’re doing, and read the last couple sentences when you resume. Maybe it will help you. Maybe it won’t. Or maybe I’m an idiot who needs crutches. But hey, who knows!</p>

<h2 id="notes">notes</h2>



  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Crafting Interpreters (399 pts)]]></title>
            <link>https://craftinginterpreters.com/</link>
            <guid>40950235</guid>
            <pubDate>Fri, 12 Jul 2024 23:00:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://craftinginterpreters.com/">https://craftinginterpreters.com/</a>, See on <a href="https://news.ycombinator.com/item?id=40950235">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<p><em>Crafting Interpreters</em> contains everything you need to implement a
full-featured, efficient scripting language. You’ll learn both high-level
concepts around parsing and semantics and gritty details like bytecode
representation and garbage collection. Your brain will light up with new ideas,
and your hands will get dirty and calloused. It’s a blast.</p>

<p>Starting from <code>main()</code>, you build a language that features rich
syntax, dynamic typing, garbage collection, lexical scope, first-class
functions, closures, classes, and inheritance. All packed into a few thousand
lines of clean, fast code that you thoroughly understand because you write each
one yourself.</p>

<p>The book is available in four delectable formats:</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Optimizing the Lichess Tablebase Server (196 pts)]]></title>
            <link>https://lichess.org/@/revoof/blog/optimizing-the-tablebase-server/MetV0ZQd</link>
            <guid>40949943</guid>
            <pubDate>Fri, 12 Jul 2024 22:14:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lichess.org/@/revoof/blog/optimizing-the-tablebase-server/MetV0ZQd">https://lichess.org/@/revoof/blog/optimizing-the-tablebase-server/MetV0ZQd</a>, See on <a href="https://news.ycombinator.com/item?id=40949943">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Recently our <a href="https://lichess.org/@/lichess/blog/7-piece-syzygy-tablebases-are-complete/W3WeMyQA">our 7 piece Syzygy tablebase server</a> was struggling to complete its periodic RAID integrity check while being hammered with tablebase requests. We decided to try a new approach, using <a href="https://man7.org/linux/man-pages/man7/lvmraid.7.html#DATA_INTEGRITY">dm-integrity on LVM</a>. Now, instead of periodically checking every data block, we passively check blocks whenever they are read.</p>
<p>17 TiB of tablebases are unwieldy, so to do this migration without hours of downtime, we set up a second server with the new approach. This also allowed us to run controlled benchmarks on the full set of tablebases, before finally doing the switch and retiring the old server.</p>
<p>We're trying to get the most out of the following new hardware:</p>
<ul>
<li>32 GiB RAM unchanged</li>
<li>2 x 201 GiB NVMe, where the previous server didn't have any SSD space. The rest of the 476 GiB disks is reserved for OS and working space</li>
<li>6 x 5.46 TiB HDD, where the previous server had only 5 disks</li>
</ul>
<p>The current operating system is Debian bookworm with default I/O schedulers:</p>
<pre><code>root@bwrdd:~# uname -a
Linux bwrdd 6.1.0-21-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.90-1 (2024-05-03) x86_64 GNU/Linux
root@bwrdd:~# cat /sys/class/block/nvme0n1/queue/scheduler
[none] mq-deadline
root@bwrdd:~# cat /sys/class/block/sda/queue/scheduler
[mq-deadline] none
</code></pre>
<h2>Monitoring important as ever</h2>
<p>RAID 5 is a good fit here, allowing recovery from any single disk failure, and distributing random reads across all disks. My first attempt was:</p>
<pre><code>$ lvcreate --type raid5 --raidintegrity y --raidintegrityblocksize 512 --name tables --size 21T vg-hdd # Oops
</code></pre>
<p>Peformance numbers in initial tests were decent, but we would have left a lot on the table if we didn't have monitoring to catch that not all disks were indeed participating equally.</p>
<p><img src="https://image.lichess1.org/display?h=0&amp;op=resize&amp;path=revoof:ublogBody:0yd54XyjtR2R:VUTWHQqX.png&amp;w=800&amp;sig=3fc8b923017ef07ac43342d38a06c1665b34d092" alt="Reads not distributed evenly"><br>Physical disk read activity with bad raid setup</p>
<p>That's because omitting <code>--stripes</code> does <em>not</em> default to use all physical volumes.</p>
<h2>Benchmark results (overview)</h2>
<p>In normal conditions the server receives between 10 and 35 requests per second. We record 1 million requests in the production environment to replay them in a controlled benchmark. In the chosen scenario, 12 parallel clients each sequentially submit requests from the production log.</p>
<p>Tables are lazily opened, and application and OS caches are lazily populated. So the first 800k response times are ignored as a warmup. We analyse the response times for the remaining 200k requests.</p>
<p>On average, response times are plenty fast, but tail latencies are high. So this is our focus for any optimizations. We'll unpack the results in a moment, but here are the empirical distribution functions (ECDFs) with 30ms added to each response time for an overview.</p>
<p><img src="https://image.lichess1.org/display?h=0&amp;op=resize&amp;path=revoof:ublogBody:KfbJv0yt3hyH:W2HFO1ha.png&amp;w=800&amp;sig=324cebd8631a951e7a67b9ec74831391dfcbca1e" alt="ECDFs with 30ms offset"><br>ECDFs</p>
<p>For a given response time on the x axis (log scale!) you can see which proportion of requests is faster. Or for a given proportion on the y axis (think percentile), you can read off the corresponding response time on the x axis.</p>
<p>The added constant seems artificial, but it's just viewing the results from the point of view of a client with 30ms ping time. Otherwise the log scaled x-axis would overemphasize the importance of a few milliseconds at the low end.</p>
<h2>mmap with higher tail latencies than pread</h2>
<p>Our Syzygy tablebase implementation <a href="https://github.com/niklasf/shakmaty-syzygy">shakmaty-syzygy</a> now offers an interface to plug in different ways of opening and reading from table files. The main contenders are:</p>
<ul>
<li><a href="https://man7.org/linux/man-pages/man2/mmap.2.html">Map</a> table files into memory. After the file has been mapped, disk reads happen transparently when accessing the respective memory region, so no further system calls are needed. Unfortunately, that also means reads look just as infallible as normal memory accesses, so that errors can only be handled out of band, via signals.</li>
<li><a href="https://man7.org/linux/man-pages/man2/pwrite.2.html">pread(2)</a>, one system call per read, with read error reporting via the return value.</li>
</ul>
<p>More robust error handling would probably be enough to justify using <code>pread</code> for a server implementation, but surprisingly, the diagram above shows that <code>pread</code> also peforms <em>better</em> in the scenario we care about. Perhaps that is because sometimes transparently reading a single memory-mapped data block across page boundaries may end up issuing two disk reads</p>
<pre><code>while (...)
{
    uint8_t d = *ptr++;
}
</code></pre>
<p>whereas</p>
<pre><code>uint8_t buf[MAX_BLOCK_SIZE];
ssize_t res = pread(fd, buf, block_size, offset);
</code></pre>
<p>immediately reveals how much data will be read.</p>
<p>Now, before you change your chess engine to use <code>pread</code>: Tablebases in engine matches are typically used only if enough fast storage for all WDL tables is available. The typical range of response times is not even visible in the graph above. Here, the saved syscall overhead is significant, so that memory mapping performs better.</p>
<p><img src="https://image.lichess1.org/display?h=0&amp;op=resize&amp;path=revoof:ublogBody:TLBrF0isfFgq:FbA5f9fq.png&amp;w=800&amp;sig=8aa103c6da4a742e53ed5e78949542a0231afb79" alt="ECDFs without offset"><br>Zoom on ECDFs: mmap saves syscall overhead</p>
<h2><code>MADV_RANDOM</code> / <code>POSIX_FADV_RANDOM</code> counter-productive</h2>
<p>The next surprise looking at the results above, is that <a href="https://man7.org/linux/man-pages/man2/posix_fadvise.2.html"><code>posix_fadvise(fd, 0, 0, POSIX_FADV_RANDOM)</code></a> or its equivalent for memory maps are actually mostly counter-productive. <code>POSIX_FADV_RANDOM</code> is intended to alleviate pressure on the page cache, by hinting to the operating system that file accesses are going to be random and automatic read-ahead is likely pointless.</p>
<p>Perhaps tablebase access patterns when people are analysing endgames are not so random afterall.</p>
<p>Again, this may differ for chess engines, where probes may be more likely to be scattered across different possible endgames.</p>
<h2>Table prefixes on limited SSD space</h2>
<p>To decide how to use the limited SSD space, let's have a look at the anatomy of a single table probe. The position will be encoded as an integer index, based on encoding information from the table header. Then we need to find the compressed data block that contains the result for the particular index. Syzygy provides a "sparse" block length list, which points close to the correct entry in the block length list, which is then used to find the relevant data block.</p>
<div>
<table>
<thead>
<tr><th>Table section sizes</th><th>WDL</th><th>DTZ</th><th>Total</th></tr>
</thead>
<tbody>
<tr><td>Headers and sparse block length lists</td><td>38 GiB</td><td>9 GiB</td><td>47 GiB</td></tr>
<tr><td>Block length lists</td><td>274 GiB</td><td>64 GiB</td><td>339 GiB</td></tr>
<tr><td>Compressed data blocks</td><td>8433 GiB</td><td>8458 GiB</td><td>16891 GiB</td></tr>
</tbody>
</table>
</div>
<p>We could certainly use the SSD space for an additional layer of adaptive caching, to cache hot list entries and data blocks. But since we're trying to improve tail latencies in particular, it makes sense to think about the worst case. By putting the sparse block length lists and the block length lists on SSD storage, hot or cold, we can guarantee a maximum of 1 slow disk read per table probe.</p>
<p>In our case that doesn't quite fit when using the SSD space in RAID 1 (mirrored), but since this optimization is optional, we can give up redundancy and use RAID 0.</p>
<h2>Parallelizing reads</h2>
<p>In chess engines, a typical tablebase request will be for a single WDL value. But for the user interface we instead want to display DTZ values for all moves.</p>
<p><img src="https://image.lichess1.org/display?h=0&amp;op=resize&amp;path=revoof:ublogBody:JGE5DJVrlUwR:VQu8XPZu.png&amp;w=800&amp;sig=66061784df25478d0e120a193664ce6dbd2ee481" alt="Screenshot of tablebase response"><br>Tablebase explorer</p>
<p>That, together with Syzygy's internal resolution of captures, will cause the average request to issue 23 WDL probes and 70 DTZ probes. In the initial implementation handling of requests was parallelized, but probes within each request were executed sequentially.</p>
<p>In the benchmark results we can see that using more fine grained parallelism has some overhead at the low end, but significantly reduces tail latencies. Of course the disks can not really physically handle that many parallel reads, but now the I/O scheduler is more likely to plan them in a way that will finish each request as soon as possible, and can better plan the order of all involved disk accesses (minimizing time until the disk's read head is at the next requested sector).</p>
<h2>Performance in production</h2>
<p>Finally, it's good to confirm that optimizations in the benchmark scenario actually help in production. Here are response time charts sliced together.</p>
<p><img src="https://image.lichess1.org/display?h=0&amp;op=resize&amp;path=revoof:ublogBody:hil5PjrHZMoy:Hc1Y8NLW.png&amp;w=800&amp;sig=fd4b864738e2060844341e192d20cfd51fa86efd" alt="Standard chess response times significantly reduced in production"><br>Standard chess response times</p>
<hr>
<p>Raw data at <a href="https://github.com/niklasf/lila-tablebase-bench-tool">https://github.com/niklasf/lila-tablebase-bench-tool</a></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Things I know about Git commits (108 pts)]]></title>
            <link>https://www.jvt.me/posts/2024/07/12/things-know-commits/</link>
            <guid>40949229</guid>
            <pubDate>Fri, 12 Jul 2024 20:42:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jvt.me/posts/2024/07/12/things-know-commits/">https://www.jvt.me/posts/2024/07/12/things-know-commits/</a>, See on <a href="https://news.ycombinator.com/item?id=40949229">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span>Written by</span>
Jamie Tanna<br><span>on&nbsp;</span><time datetime="2024-07-12T20:01:09+0100">July 12, 2024</time><br><span><a href="http://creativecommons.org/licenses/by-nc-sa/4.0/legalcode"><i></i> CC-BY-NC-SA-4.0</a>
<a href="http://www.apache.org/licenses/LICENSE-2.0"><i></i> Apache-2.0</a></span><br><span>7 mins</span></p></div><div><p>This is an article that's been swimming around in my head for ~5 weeks now, and may become a "living post" that I keep updated over time.</p><p>In no particular order, some things I've learned about Git commits and commit history, over the last 12 years. This is a mix of experience in companies with teams of 2-12 people, as well as in Open Source codebases with a vast range of contributors.</p><ol><li>Git has different uses - a collaboration tool, a backup tool, a documentation tool</li><li>Git commit messages are excellent</li><li>I've never met anyone who likes reading commit messages as much as me</li><li>Finding out why a change was made is easier sorting through commits than it is through an issue/bug tracker</li><li>It's better to have a commit that says "Various fixes. DEV-123" than it is to have "Various fixes"</li><li>It's worse to have a commit that says "Various fixes. DEV-123" when the issue itself has no useful information</li><li>Rebase-merging is my preference. Then squash-merge, then merge</li><li>If you don't learn how to rebase, you're missing out on a good skill</li><li>People who say "just delete the repo" when things go wrong really frustrate me</li><li>Learn how to use <code>git reflog</code>, and it'll save you deleting a repo and work that was recoverable</li><li>Learn how to use <code>git reflog</code>, and you'll be able to save yourself from mistakes that aren't that bad</li><li>No amount of learning fancy tools and commands saves you from fucking up every once in a while</li><li>My most recent botched rebase was last week, and I needed <code>git reflog</code> to help un-fuck it</li><li>Learn how to <a href="https://www.jvt.me/posts/2021/10/23/undo-force-push/">undo a force push</a> and then how to <a href="https://www.jvt.me/posts/2018/09/18/safely-force-git-push/">more safely force push</a> (remember the <code>=ref</code>!!)</li><li>Squashing is a waste of well-written atomic commits</li><li>Squashing is better than 100 crap commits</li><li>Squashing, and writing a good commit message at the time of merge is good</li><li>Squashing, and then not re-editing the message is the worst</li><li>Squashing, when you have 100 crap commits, and then not re-editing the message is <strong>a crime</strong></li><li>Squashing, and then not re-editing the message is worse than a merge commit from a branch with 100 crap commits</li><li>Writing a well documented PR/MR description but not using that to inform the squash-merge message is a waste of time</li><li>Writing commit messages have helped me pick up on missing test cases, missing documentation, or invalid thought processes, as it helps me rewrite <em>why</em> the changes are being made</li><li>Using your <code>git log</code> as an indication for standup updates is valid</li><li>I can't be bothered to sign my commits (unless I'm forced to)</li><li>If I have to sign my commits, SSH key signing makes it almost not awful</li><li>If you're moving files between repos, you need to keep the history intact, <a href="https://www.jvt.me/posts/2018/06/01/git-subtree-monorepo/">using <code>git subtree</code></a></li><li>Commits should be atomic - all the code and tests, and configuration changes should all be in there</li><li>I spend a good chunk of time ensuring that each commit passes CI checks atomically</li><li>Some people do horrific things, like split their implementation and test code from each other</li><li>It's OK to put documentation in a separate commit - we don't have to have a whole end-to-end feature delivery in a single commit</li><li>Repos that use squash-merges suck</li><li>As a maintainer of Open Source projects, I like squash-merge, so I can rewrite contributors' commit messages</li><li>Sometimes it's not worth coaching how to write a given commit message</li><li>The people around you shape the way you write commits</li><li>Do the work up front to make your history atomic</li><li>It's much more painful to split a mega-commit into atomic commits after the fact</li><li>Splitting work atomically can be good for improving your reward drive - you can get many more things done</li><li>Atomic commits work really well with <a href="https://www.jvt.me/posts/2022/04/12/prefactor/">prefactoring</a></li><li>Sometimes prefactor commits can go into separate PRs (especially if squash-merge is used)</li><li>Writing commit messages can take longer than the implementation</li><li>The commit message can be an order of magnitude larger than the number of lines changed in a commit</li><li>If you end up writing a lot of "and"s or "also" in a commit message, you may be trying to do too many things</li><li>Trawling through Git commit history in the past has helped unlock a number of cases where I could then understand the <em>why</em> without the original authors there to answer my questions</li><li>Commit messages are a great point to reflect on not just what you've done, but <strong>why</strong></li><li>Why is more important than what - anyone can look at the diff and generally work out what changes were made, but the intent behind it is the special sauce</li><li>If you only write what changed, you're annoying, and I dislike you</li><li>A commit that explains what is better than a commit that just has "fixes"</li><li><span><a href="https://cbea.ms/">Chris Beams</a></span>' article, <a href="https://cbea.ms/git-commit/"><em>How to Write a Git Commit Message</em></a> is still an excellent post and a great place to start, just under 10 years later!</li><li>Commits are a point-in-time explanation of the assumptions and the state of the world for the committer. Don't be too hard on them</li><li>I don't want to read AI/LLM rewrites of your changes - either write it yourself, or call it <code>Various fixes</code></li><li>There needs to be a way to add an annotation (maybe using <code>git notes</code>) to a previous message to correct assumptions</li><li>I won't write perfect commit messages up front - they'll sometimes be as much as <code>rew! add support for SBOMs</code> or <code>sq</code>, or <a href="https://www.jvt.me/posts/2019/01/10/git-commit-fixup/">use <code>git commit --fixup</code></a></li><li>I will, generally, break off very good atomic chunks of work into commits</li><li>I will split atomic commits into multiple commits, sometimes</li><li>Make sure you <a href="https://www.jvt.me/posts/2019/01/12/self-code-review/">review your own code changes</a> before you send it out for review to your collaborators</li><li>Reviewing your commit messages should be as important as reviewing the code changes</li><li>Getting all your contributors to invest the same amount of care into the commit history is a losing battle</li><li>Trying to police commit history is going to be painful</li><li>Trying to mandate reviews of commit messages as part of code review is going to be painful</li><li>Trying to police commit history does lead to a greater level of documentation and consideration around changes int he codebase</li><li>Making implicit assumptions explicit is really useful</li><li>Introducing <code>commitlint</code> can be useful, but also frustrating</li><li>It's nicer to have your collaborators want to write good commit messages than you having to force them</li><li>Some people don't write, and that's alright</li><li>Writing is a skill</li><li>I'm not perfect at writing (commit messages)</li><li>Sometimes I can't be arsed to write the perfect message</li><li>Sometimes I write some really great commit messages, and impress myself</li><li>Using a <a href="https://www.jvt.me/posts/2017/04/17/commit-templates/">template for your Git commit messages</a> is a good nudge to doing it right</li><li><a href="https://www.jvt.me/posts/2019/01/10/git-commit-fixup/"><code>fixup</code> commits</a> and <code>git rebase --autosquash</code> has been one of the best Git tips I've learned</li><li>I value working on a team with a diverse set of perspectives, skills and approaches to work</li><li>But I also really value having a team who writes atomic commits with well-written commit messages</li><li>Commit message writing is as useful as writing well-refined user stories/tickets</li><li><code>git commit -m sq</code> is probably my most-run command</li><li>Using <code>git add -p</code> and <code>git commit -p</code> are hugely important for atomic commits</li><li>Never use <code>git add -u</code> or <code>git add .</code></li><li>Learn when you can use <code>git add -u</code> or <code>git add .</code></li><li>I really need to look into tools like Graphite, <code>git-branchless</code> and other means to provide a stacked PR setup</li><li>Using <a href="https://www.conventionalcommits.org/en/v1.0.0/">conventional commits</a> with <a href="https://github.com/semantic-release/semantic-release">semantic-release</a> or <a href="https://github.com/go-semantic-release/semantic-release">go-semantic-release</a> can make a huge difference when wanting to release automagically and often</li><li>Using <a href="https://www.conventionalcommits.org/en/v1.0.0/">conventional commits</a> as a framework for your commits can be really useful</li><li>Using <a href="https://www.conventionalcommits.org/en/v1.0.0/">conventional commits</a>, as someone with ADHD, reduces the need for thinking at times and can allow you to focus more on what the changes are</li><li>Using <a href="https://www.conventionalcommits.org/en/v1.0.0/">conventional commits</a> helps you work out when you're trying to do too much in a commit</li><li>I <a href="https://www.jvt.me/posts/2023/10/04/blogging-neurodiversity/">think through writing</a> so commit messages help understand <a href="https://www.youtube.com/watch?v=1qdyNoe3q2A">why I did the thing</a></li><li>It can be better to write a good commit message than a piece of documentation, stored elsewhere</li><li>It can be better to write a good commit message than a code comment</li><li>Give people space to learn</li><li>Give people space to fail</li><li>Remember you weren't so great at one point in time</li><li>Documentation is rad. Do more of it</li></ol></div></div>]]></description>
        </item>
    </channel>
</rss>