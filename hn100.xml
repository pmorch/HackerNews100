<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 11 Dec 2025 18:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Days since last GitHub incident (112 pts)]]></title>
            <link>https://github-incidents.pages.dev/</link>
            <guid>46233798</guid>
            <pubDate>Thu, 11 Dec 2025 16:52:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github-incidents.pages.dev/">https://github-incidents.pages.dev/</a>, See on <a href="https://news.ycombinator.com/item?id=46233798">Hacker News</a></p>
<div id="readability-page-1" class="page">
    Days since last Github service disruption: 0
  


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Things I want to say to my boss (143 pts)]]></title>
            <link>https://www.ithoughtaboutthatalot.com/2025/the-things-i-want-to-say-to-my-boss</link>
            <guid>46233570</guid>
            <pubDate>Thu, 11 Dec 2025 16:35:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ithoughtaboutthatalot.com/2025/the-things-i-want-to-say-to-my-boss">https://www.ithoughtaboutthatalot.com/2025/the-things-i-want-to-say-to-my-boss</a>, See on <a href="https://news.ycombinator.com/item?id=46233570">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>I’m sitting down to write this in a gap between jobs. The downtime is strange, like the world has stopped moving but my thoughts haven’t caught up. Other than replaying the shit that went down during the last six months – or to put it more bluntly, the reasons I left, I don’t quite know what to do with myself.</p><p>What happened wasn’t unique. And that’s the part that bothers me most.&nbsp;</p><p>It’s the same stuff I hear from friends, colleagues, people I trust across the industry.</p></div><p>I know this is anonymous, but if you think this is about you, then I hope you do your team a favour and listen.</p><div><p>It’s the performance of ‘care’ from leadership. Saying one thing loudly and proudly, yet doing another quietly, repeatedly.</p><p>I know this is anonymous, but if you think this is about you, then I hope you do your team a favour and listen.</p><h2><strong>The things I wish I could say</strong></h2><p>You can’t fake care. People feel it. In small moments, in the gaps between your words, in the way you prioritise your business over their wellbeing. Care is a practice, not a performance. If you only care when outsiders are watching, you’re just performing.&nbsp;</p><p>Communication isn’t optional or a one-way thing. Consistency and honesty build trust. Inconsistency and silence destroy it. If you communicate more externally than with your team, your culture will break down slowly over time.&nbsp;</p><p>Ideas stop being shared because “what’s the point?” It’s not like you’re really listening. Meetings become quieter because speaking up feels risky. Colleagues start shrinking, not because their talent fades, but because the space to use it gets narrower.</p></div><div><p>I hope you learn that leadership is more than LinkedIn posts and conference talks.&nbsp;</p><p>It’s the day-to-day choices you make when nobody’s applauding.</p></div><div><p>Burnout isn’t a sign of commitment, it’s a sign of organisational failure. If your best people are exhausted, withdrawn, or like shadows of who they once were, that’s not a resource problem. That’s a You problem.</p><p>By the time you notice a culture is broken, the damage has already been done. People have mentally checked out, or quietly left, or stayed but stopped believing.</p><h2><strong>What I hope (though I’m not holding my breath)</strong></h2><p>I hope you learn that leadership is more than LinkedIn posts and conference talks.&nbsp;</p><p>It’s the day-to-day choices you make when nobody’s applauding. It’s the way you treat people when they’re tired, honest, unwell or “inconvenient”. It’s whether your words match your actions, and whether you’re brave enough to admit when they don’t.</p><p>I hope you realise that people don’t leave because they’re unwilling. They leave because you didn’t take care of them. You don’t get to call yourself “people-first” when every decision proves otherwise.&nbsp;</p><p>I hope you learn that if you focus on making money instead of the team lining your pockets, you will end up with a broken team and no money.</p><h2><strong>What good leadership actually looks like</strong></h2><p>Good leadership isn’t complicated, but it is demanding. It asks more of you than your job title does. It asks for self-awareness, not slogans. It asks you to trade the armour of performance for the discomfort of being accountable.</p></div><div><p>In the end, good leadership is never proven by what you say about yourself. It’s proven by what people say when you’re not in the room.</p><p>And trust me, they’re talking.</p></div><div><p>It’s showing up before the crisis, not after. It’s noticing when someone’s energy changes and checking in, not waiting for them to break. It’s understanding the difference between being busy and being present.</p><p>It’s making decisions with people, not about them. It’s protecting your team from unnecessary chaos rather than generating it. It’s recognising that transparency isn’t a risk, but how trust stays alive.</p><p>It’s creating conditions where people want to speak — not because they’re brave, but because it’s safe. Where the loudest voices don’t automatically win.</p><p>It’s understanding that care is not soft. It’s not indulgent. It’s not a blocker to delivery. It’s the foundation that makes delivery possible. Care is the thing that keeps people willing to stay, to try, to believe. Care is taking responsibility for the things you say and do, and the culture that results in.</p><p>If you want loyalty, creativity, honesty, energy, you must earn them. You earn them by being the kind of leader whose actions make it obvious that people matter. Not because it’s good PR. Because it’s your job. And because people matter, and they deserve it.</p><p>In the end, good leadership is never proven by what you say about yourself. It’s proven by what people say when you’re not in the room.</p><p>And trust me, they’re talking.</p><p>I've given you too much of my time, attention and energy in 2025. So in 2026, I plan to do the opposite and not give you any more.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[iPhone Typos? It's Not Just You – The iOS Keyboard Is Broken [video] (248 pts)]]></title>
            <link>https://www.youtube.com/watch?v=hksVvXONrIo</link>
            <guid>46232528</guid>
            <pubDate>Thu, 11 Dec 2025 15:25:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=hksVvXONrIo">https://www.youtube.com/watch?v=hksVvXONrIo</a>, See on <a href="https://news.ycombinator.com/item?id=46232528">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Disney making $1B investment in OpenAI, will allow characters on Sora AI (276 pts)]]></title>
            <link>https://www.cnbc.com/2025/12/11/disney-openai-sora-characters-video.html</link>
            <guid>46231585</guid>
            <pubDate>Thu, 11 Dec 2025 14:12:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2025/12/11/disney-openai-sora-characters-video.html">https://www.cnbc.com/2025/12/11/disney-openai-sora-characters-video.html</a>, See on <a href="https://news.ycombinator.com/item?id=46231585">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="RegularArticle-ArticleBody-5" data-module="ArticleBody" data-test="articleBody-2" data-analytics="RegularArticle-articleBody-5-2"><div id="Placeholder-ArticleBody-Video-108240561" data-test="VideoPlaceHolder" role="region" tabindex="0" data-vilynx-id="7000398052" aria-labelledby="Placeholder-ArticleBody-Video-108240561"><p><img src="https://image.cnbcfm.com/api/v1/image/108240562-17654620331765462030-42932533857-1080pnbcnews.jpg?v=1765462032&amp;w=750&amp;h=422&amp;vtcrop=y" alt="Disney and OpenAI reach three-year licensing agreement"><span></span><span></span></p></div><div><p><span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-1"><a href="https://www.cnbc.com/quotes/DIS/">The Walt Disney Co.</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> on Thursday announced it will make a $1 billion equity investment in <a href="https://www.cnbc.com/2025/12/09/openai-slack-ceo-denise-dresser-chief-revenue-officer.html">OpenAI</a> and will allow users to make videos with its copyrighted characters on its <a href="https://www.cnbc.com/2025/11/04/openai-sora-android.html">Sora</a> app.</p><p>OpenAI launched Sora in September, and it allows users to create&nbsp;<a href="https://www.cnbc.com/2025/10/02/openai-invite-code-sora-2-censorship.html">short videos</a>&nbsp;by simply typing in a prompt. </p><p>As part of the startup's new three-year licensing agreement with Disney, Sora users will be able make content with&nbsp;more than 200 characters across Disney, Marvel, Pixar and Star Wars starting next year. </p><p>"The rapid advancement of <a href="https://www.cnbc.com/ai-artificial-intelligence/">artificial intelligence</a> marks an important moment for our industry, and through this collaboration with OpenAI we will thoughtfully and responsibly extend the reach of our storytelling through generative AI, while respecting and protecting creators and their works," Disney CEO <a href="https://www.cnbc.com/bob-iger/">Bob Iger</a> said in a statement.</p><p>As part of the agreement, Disney said it will receive warrants to purchase additional equity and will become a major OpenAI customer. </p><p>Disney is deploying OpenAI's chatbot, ChatGPT, to its employees and will work with its technology to build new tools and experiences, according to a release. </p><p>When Sora launched this fall, the app rocketed to <a href="https://www.cnbc.com/2025/10/03/openai-sora-apple-app-store.html">the top</a> of <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-8"><a href="https://www.cnbc.com/quotes/AAPL/">Apple's</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> App Store and generated a <a href="https://www.cnbc.com/2025/10/04/sora-openai-video-app.html">storm of controversy</a> as users flooded the platform with videos of popular brands and characters. </p><p>The <a href="https://www.cnbc.com/2025/10/07/openais-sora-2-must-stop-allowing-copyright-infringement-mpa-says.html">Motion Picture Association</a>&nbsp;said in October that OpenAI needed to take "immediate and decisive action" to prevent copyright infringement on Sora. </p><p>OpenAI CEO Sam Altman said more "granular control" over character generation was coming, according to <a href="https://blog.samaltman.com/sora-update-number-1" target="_blank">a blog post</a> following the launch.</p></div><div id="ArticleBody-InlineImage-108240672" data-test="InlineImage"><p>Disney CEO Bob Iger and OpenAI CEO Sam Altman appearing on CNBC on Dec. 11th, 2025.</p><p>CNBC</p></div><div><p>As AI startups have rapidly changed the way that people can interact with content online, media companies, including Disney, have kicked off a series of fresh legal battles to try and protect their intellectual property.</p><p>Disney sent a cease and desist letter to <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-12"><a href="https://www.cnbc.com/quotes/GOOGL/">Google</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> late on Wednesday alleging the company infringed its copyrights on a "massive scale." In the letter, which was viewed by CNBC, Disney said Google has been using its copyrighted works to train models and distributing copies of its protected content without authorization.</p><p>CNBC has reached out to Google for comment on the letter.</p><p>Universal and Disney have sued the AI image creator&nbsp;<a href="https://www.cnbc.com/2025/06/11/disney-universal-midjourney-ai-copyright.html">Midjourney</a>, alleging that the company improperly used and distributed AI-generated characters from their movies. Disney also sent a&nbsp;<a href="https://www.cnbc.com/2025/09/30/disney-cease-and-desist-characterai-copyright.html">cease and desist</a>&nbsp;letter to&nbsp;<a href="https://www.cnbc.com/2025/08/01/human-ai-relationships-love-nomi.html">Character.AI</a>&nbsp;in September, warning the startup to stop using its copyrighted characters without authorization.</p><p>Disney's deal with OpenAI suggests the company isn't ruling out AI platforms entirely.</p></div><div id="RegularArticle-RelatedContent-1"><h2>Read more CNBC tech news</h2><div><ul><li><a href="https://www.cnbc.com/2025/12/11/oracle-shares-plummet-dragging-down-ai-stocks-nvidia-coreweave.html">Oracle shares plummet, dragging down AI stocks</a></li><li><a href="https://www.cnbc.com/2025/12/11/big-tech-microsoft-amazon-google-india-billions-in-investment.html">Over $50 billion in under 24 hours: Why Big Tech is doubling down on investing in India</a></li><li><a href="https://www.cnbc.com/2025/12/10/ciscos-stock-closes-at-record-for-first-time-since-dot-com-peak-2000.html">Cisco's stock closes at record for first time since dot-com peak in 2000</a></li><li><a href="https://www.cnbc.com/2025/12/10/nvidia-backed-starcloud-trains-first-ai-model-in-space-orbital-data-centers.html">Nvidia-backed Starcloud trains first AI model in space as orbital data center race heats up</a></li></ul></div></div><div><p>The companies said they have affirmed a commitment to the use of AI that "protects user safety and the rights of creators" and "respects the creative industries," according to the release. </p><p>OpenAI has also agreed to maintain "robust controls" to prevent illegal or harmful content from being generated on its platforms. </p><p>Some of the characters available through the deal include Mickey Mouse, Ariel, Cinderella, Iron Man and Darth Vader. Disney and OpenAI said the agreement does not include any talent likeness or voices. </p><p>Users will also be able to draw from the same intellectual property while using ChatGPT Images, where they can use natural language prompts to create images.&nbsp;</p><p>"Disney is the global gold standard for storytelling, and we're excited to partner to allow Sora and ChatGPT Images to expand the way people create and experience great content," Altman said in a statement.</p><p>Curated selections of Sora videos will also be available to watch on Disney's streaming platform Disney+.</p><p><em>Disclosure: Comcast is the parent company of NBCUniversal, which owns CNBC. Versant would become the new parent company of CNBC upon Comcast's planned spinoff of Versant.</em></p><p><strong>WATCH: </strong><a href="https://www.cnbc.com/video/2025/10/10/why-sora-2-by-openai-has-hollywood-worried.html">We tested OpenAI’s Sora 2 AI-video app to find out why Hollywood is worried</a></p></div><div id="Placeholder-ArticleBody-Video-108210371" data-test="VideoPlaceHolder" role="region" tabindex="0" data-vilynx-id="7000391611" aria-labelledby="Placeholder-ArticleBody-Video-108210371"><p><img src="https://image.cnbcfm.com/api/v1/image/108210376-OpenAI_Hollywood_C_Clean.jpg?v=1760071837&amp;w=750&amp;h=422&amp;vtcrop=y" alt="We tested OpenAI’s Sora 2 AI-video app to find out why Hollywood is worried"><span></span><span></span></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Craft software that makes people feel something (111 pts)]]></title>
            <link>https://rapha.land/craft-software-that-makes-people-feel-something/</link>
            <guid>46231274</guid>
            <pubDate>Thu, 11 Dec 2025 13:45:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rapha.land/craft-software-that-makes-people-feel-something/">https://rapha.land/craft-software-that-makes-people-feel-something/</a>, See on <a href="https://news.ycombinator.com/item?id=46231274">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p>So, I woke up today. Got my coffee, family went to sleep, and I have a free afternoon.</p>

<p>I thought about writing something. I may delete this article, but if you are reading this, it means I went through with it.</p>

<p>Recently, people have been asking me why I’m pausing Boo to work on a programming language. I think it would actually be cool to write down how I feel.</p>

<p>Boo is a code editor I created solely for myself; I never had the intention of making it a mainstream editor. Of course, it would be fun if people used it, but that was never my goal. This year I got it working in a functional state, where I can actually use it for my daily work. It has innovative human-keyboard navigation and replaces the LSP system with something faster and less costly for the OS. So why on earth am I not open-sourcing it? That’s what people keep asking me.</p>

<p>First, let’s go step by step.</p>

<p>My mind isn’t really moved by the idea that it would be a success or a failure — the end user of Boo is me. I don’t feel it’s there yet; in fact, I think software should inspire us. Working on Rio Terminal and Boo in my free time — both written in Rust and sharing many similarities — affects my joy, because it starts to become something automatic. Both have similar architecture, language, release process, and etcetera.</p>

<p>Since I was a kid, I liked to build Lego blocks. That’s probably what I did the most besides playing football or video games. The fun thing about Lego is that one day you can build a castle, and the next day you can build a ship. Not necessarily using the same pieces and colors — you can actually add a lot of stuff that’s external to what you have, like a wood stick.</p>

<p>When programming becomes repetitive, the odds of you creating something that makes people go “wow” are reduced quite a bit. It isn’t a rule, of course. You need to be inspired to make inspiring software.</p>

<p>I always use the example of <a href="https://en.wikipedia.org/wiki/The_Legend_of_Zelda:_Breath_of_the_Wild">The Legend of Zelda: Breath of the Wild</a>. This game is so well crafted that I know people who don’t even like video games but bought a console just to play it — and once they finished, they sold everything. This is what I’m talking about: taking time to build something so that once people try it, they remember it for as long as they live.</p>

<p>Boo isn’t a business. I don’t need or want to make money out of it. I don’t have a deadline, nor do I want to create another VS Code. I don’t feel like forcing it to happen.</p>

<p>In that case, I don’t necessarily need to stop building Lego blocks, right? I’ll just park it there, and when the inspiration comes back, I’ll pick it up where it was. That being said, I paused Boo, and I am working on my own programming language. Eventually, my idea is to rewrite Boo to use it.</p>

<p>“Wow! That’s a lot of work.” Indeed. But it’s my hobby stuff. I’ve always loved programming languages, and I am having a blast learning more about binaries and compilers. So, I don’t really feel I need to follow people’s cake recipe for success. That’s how my mind works, and I will stick with it.</p>

<p>By the way, this article was written using Boo.</p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[French supermarket's Christmas advert is worldwide hit (without AI) [video] (110 pts)]]></title>
            <link>https://www.youtube.com/watch?v=Na9VmMNJvsA</link>
            <guid>46231187</guid>
            <pubDate>Thu, 11 Dec 2025 13:35:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=Na9VmMNJvsA">https://www.youtube.com/watch?v=Na9VmMNJvsA</a>, See on <a href="https://news.ycombinator.com/item?id=46231187">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Meta shuts down global accounts linked to abortion advice and queer content (316 pts)]]></title>
            <link>https://www.theguardian.com/global-development/2025/dec/11/meta-shuts-down-global-accounts-linked-to-abortion-advice-and-queer-content</link>
            <guid>46230072</guid>
            <pubDate>Thu, 11 Dec 2025 11:26:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/global-development/2025/dec/11/meta-shuts-down-global-accounts-linked-to-abortion-advice-and-queer-content">https://www.theguardian.com/global-development/2025/dec/11/meta-shuts-down-global-accounts-linked-to-abortion-advice-and-queer-content</a>, See on <a href="https://news.ycombinator.com/item?id=46230072">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>Meta has removed or restricted dozens of accounts belonging to abortion access providers, queer groups and reproductive health organisations in the past weeks in what campaigners call one of the “biggest waves of censorship” on its platforms in years.</p><p>The takedowns and restrictions began in October and targeted the Facebook, <a href="https://www.theguardian.com/technology/instagram" data-link-name="in body link" data-component="auto-linked-tag">Instagram</a> and WhatsApp accounts of more than 50 organisations worldwide, some serving tens of thousands of people – in what appears to be a growing push by Meta to limit reproductive health and queer content across its platforms. Many of these were from Europe and the UK, however the bans also affected groups serving women in Asia, Latin America and the Middle East.</p><p>Repro Uncensored, an NGO tracking digital censorship against movements focused on gender, health and justice, said that it had tracked 210 incidents of account removals and severe restrictions affecting these groups this year, compared with 81 last year.</p><p>Meta denied an escalating trend of censorship. “Every organisation and individual on our platforms is <a href="https://transparency.meta.com/policies/community-standards/" data-link-name="in body link">subject to the same set of rules</a>, and any claims of enforcement based on group affiliation or advocacy are baseless,” it said in a statement, adding that its policies on abortion-related content had not changed.</p><figure id="ade83e20-9ddd-4875-994b-bd0be693edef" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-2"><picture><source srcset="https://i.guim.co.uk/img/media/976a0fd5e30e02c3c3dc2ac1e6cdb93f0ca63219/0_0_1195_831/master/1195.jpg?width=620&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/976a0fd5e30e02c3c3dc2ac1e6cdb93f0ca63219/0_0_1195_831/master/1195.jpg?width=620&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/976a0fd5e30e02c3c3dc2ac1e6cdb93f0ca63219/0_0_1195_831/master/1195.jpg?width=605&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/976a0fd5e30e02c3c3dc2ac1e6cdb93f0ca63219/0_0_1195_831/master/1195.jpg?width=605&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/976a0fd5e30e02c3c3dc2ac1e6cdb93f0ca63219/0_0_1195_831/master/1195.jpg?width=445&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/976a0fd5e30e02c3c3dc2ac1e6cdb93f0ca63219/0_0_1195_831/master/1195.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 320px)"><img alt="A black box that says ‘We suspended your account, The Queer Agenda’ in the middle of squares of social media content" src="https://i.guim.co.uk/img/media/976a0fd5e30e02c3c3dc2ac1e6cdb93f0ca63219/0_0_1195_831/master/1195.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" width="445" height="309.4518828451883" loading="lazy"></picture></div><figcaption data-spacefinder-role="inline"><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>In a recent purge queer and sex-positive accounts were banned.</span> Photograph: Courtesy of Repro Uncensored</figcaption></figure><p>Campaigners say the actions indicate that Meta is taking its Trump-era approach to women’s health and LGBTQ+ issues global. Earlier this year, <a href="https://www.theguardian.com/world/2025/jan/29/abortion-pills-instagram-shadow-banning" data-link-name="in body link">it appeared to “shadow-ban” or remove the accounts</a> of organisations on Instagram or Facebook helping Americans to find abortion pills. Shadow-banning is when a social media platform severely restricts the visibility of a user’s content without telling the user.</p><p>In this latest purge, it blocked abortion hotlines in countries where abortion is legal, banned queer and sex-positive accounts in Europe, and removed posts with even non-explicit, cartoon depictions of nudity.</p><p>“Within this last year, especially since the new US presidency, we have seen a definite increase in accounts being taken down – not only in the US, but also worldwide as a ripple effect,” said Martha Dimitratou, executive director of Repro Uncensored.</p><figure id="7b7aef26-bc34-49f9-bdcc-ae4b35df01d8" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-3"><picture><source srcset="https://i.guim.co.uk/img/media/9cbcb14759839e4807385822bff8b17500bd20b3/0_0_7819_5213/master/7819.jpg?width=620&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/9cbcb14759839e4807385822bff8b17500bd20b3/0_0_7819_5213/master/7819.jpg?width=620&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/9cbcb14759839e4807385822bff8b17500bd20b3/0_0_7819_5213/master/7819.jpg?width=605&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/9cbcb14759839e4807385822bff8b17500bd20b3/0_0_7819_5213/master/7819.jpg?width=605&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/9cbcb14759839e4807385822bff8b17500bd20b3/0_0_7819_5213/master/7819.jpg?width=445&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/9cbcb14759839e4807385822bff8b17500bd20b3/0_0_7819_5213/master/7819.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 320px)"><img alt="Mark Zuckerberg and Donald Trump sit at a table laughing; Trump has his hand on Zuckerberg’s back." src="https://i.guim.co.uk/img/media/9cbcb14759839e4807385822bff8b17500bd20b3/0_0_7819_5213/master/7819.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" width="445" height="296.68563754955875" loading="lazy"></picture></div><figcaption data-spacefinder-role="inline"><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>US president Donald Trump jokes with Meta CEO Mark Zuckerberg, left, as he hosts tech leaders for a dinner in the state dining room of the White House in Washington DC in September 2025. </span> Photograph: Saul Loeb/AFP/Getty Images</figcaption></figure><p>“This has been, to my knowledge, at least one of the biggest waves of censorship we are seeing,” she said.</p><p><br>
 Campaigners have accused Meta of being condescending and unresponsive, with the company offering only vague reasons why certain accounts were taken down – and appearing unwilling to engage.</p><p>In one email shared with the Guardian, a Meta consultant appears to invite a number of reproductive health organisations to a closed-door online briefing about “the challenges that you are facing with Meta’s content moderation policies”.</p><p>The email says the meeting “will not be an opportunity to raise critiques of Meta’s practices or to offer recommendations for policy changes”.</p><p>Dimitratou said such closed-door meetings had happened before, saying they “reinforce the power imbalance that allows big tech to decide whose voices are amplified and whose are silenced”.</p><p>In another instance, a Meta employee counselled an affected organisation in a personal message to simply move away from the platform entirely and start a mailing list, saying that bans were likely to continue. Meta said it did not send this message.</p><p>Meta’s recent takedowns are part of a broader pattern of the company purging accounts, and then – at times – appearing to backtrack after public pressure, said Carolina Are, a fellow at Northumbria University’s Centre for Digital Citizens.</p><p>“It wouldn’t be as much of a problem if platforms’ appeals actually worked, but they don’t. And appeals are the basis of any democratic justice system,” she added.</p><p>Meta said that it aimed to reduce enforcement mistakes against accounts on its platform, but added that the appeals process for banned accounts had become frustratingly slow.</p><p>Organisations affected by the bans include Netherlands-registered Women Help Women, a nonprofit offering information about abortion to women worldwide, including in Brazil, the Philippines and Poland. It fields about 150,000 emails from women each year, said its executive director, Kinga Jelinska.</p><figure id="e2e87436-142f-43c1-8a29-3b8c38178c5b" data-spacefinder-role="supporting" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-4"><picture><source srcset="https://i.guim.co.uk/img/media/ad4ef5cc572acfe48bcfaa2b1bfe5bc3dcc41e80/0_0_1259_889/master/1259.jpg?width=380&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 1300px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 1300px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/ad4ef5cc572acfe48bcfaa2b1bfe5bc3dcc41e80/0_0_1259_889/master/1259.jpg?width=380&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 1300px)"><source srcset="https://i.guim.co.uk/img/media/ad4ef5cc572acfe48bcfaa2b1bfe5bc3dcc41e80/0_0_1259_889/master/1259.jpg?width=300&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 980px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 980px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/ad4ef5cc572acfe48bcfaa2b1bfe5bc3dcc41e80/0_0_1259_889/master/1259.jpg?width=300&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 980px)"><source srcset="https://i.guim.co.uk/img/media/ad4ef5cc572acfe48bcfaa2b1bfe5bc3dcc41e80/0_0_1259_889/master/1259.jpg?width=620&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/ad4ef5cc572acfe48bcfaa2b1bfe5bc3dcc41e80/0_0_1259_889/master/1259.jpg?width=620&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/ad4ef5cc572acfe48bcfaa2b1bfe5bc3dcc41e80/0_0_1259_889/master/1259.jpg?width=605&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/ad4ef5cc572acfe48bcfaa2b1bfe5bc3dcc41e80/0_0_1259_889/master/1259.jpg?width=605&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/ad4ef5cc572acfe48bcfaa2b1bfe5bc3dcc41e80/0_0_1259_889/master/1259.jpg?width=445&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/ad4ef5cc572acfe48bcfaa2b1bfe5bc3dcc41e80/0_0_1259_889/master/1259.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 320px)"><img alt="A black box that says ‘We suspended your page’ in the middle of squares of social media content" src="https://i.guim.co.uk/img/media/ad4ef5cc572acfe48bcfaa2b1bfe5bc3dcc41e80/0_0_1259_889/master/1259.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" width="445" height="314.2216044479746" loading="lazy"></picture></div><figcaption data-spacefinder-role="inline"><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>The feminist group Women Help Women had their page banned by Meta in November, but it has since been reinstated.</span> Photograph: Courtesy of Repro Uncensored</figcaption></figure><p>Women Help Women has been on <a href="https://www.theguardian.com/technology/facebook" data-link-name="in body link" data-component="auto-linked-tag">Facebook</a> for 11 years, said Jelinska, and while its account had been suspended before, this was the first time it was banned outright. The ban could be “life-threatening”, she said, pushing some women towards dangerous, less reliable information sources. Little explanation was given for the ban.</p><p>A message from Meta to the group dated 13 November said its page “does not follow our Community Standards on prescription drugs”, adding: “We know this is disappointing, but we want to keep Facebook safe and welcoming for everyone.”</p><p>“It’s a very laconic explanation, a feeling of opacity,” Jelinska said. “They just removed it. That’s it. We don’t even know which post it was about.”</p><p>Meta said more than half of the accounts flagged by Repro Uncensored have been reinstated, including Women Help Women which it said was taken down in error. “The disabled accounts were correctly removed for violating a variety of our policies including our <a href="https://transparency.meta.com/policies/community-standards/human-exploitation/" data-link-name="in body link">Human Exploitation</a> policy,” it added.</p><p>Jacarandas was founded by a group of young feminists when abortion was <a href="https://www.theguardian.com/global-development/2022/feb/22/colombia-legalises-abortion-in-move-celebrated-as-historic-victory-by-campaigners" data-link-name="in body link">decriminalised in Colombia in 2022</a>, to advise women and girls on how to get a free, legal abortion. The group’s executive director, Viviana Monsalve, said its WhatsApp helpline had been blocked then reinstated three times since October. The WhatsApp account is currently banned and Monsalve said they had received little information from Meta about whether this would continue.</p><p>“We wrote [Meta] an email and said, ‘hey, we are a feminist organisation. We work in abortion. <a href="https://www.theguardian.com/world/abortion" data-link-name="in body link" data-component="auto-linked-tag">Abortion</a> is allowed in Colombia up to 24 weeks. It’s allowed to give information about it,’” said Monsalve.</p><p>Without Meta’s cooperation, Monsalve said it was difficult to plan for the future. “You are not sure if [a ban] will happen tomorrow or after tomorrow, because they didn’t answer anything.”</p><figure id="c3516402-bc9c-4ff6-b3c1-41c5dc0fb476" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:27,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;Meta and Google accused of restricting reproductive health information&quot;,&quot;elementId&quot;:&quot;c3516402-bc9c-4ff6-b3c1-41c5dc0fb476&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/global-development/2024/mar/27/meta-and-google-accused-of-restricting-reproductive-health-information&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;design&quot;:0,&quot;display&quot;:0,&quot;theme&quot;:0}}"></gu-island></figure><p>Meta said: “Our policies and enforcement regarding abortion medication-related content have not changed: we allow posts and ads promoting healthcare services like abortion, as well as discussion and debate around them, as long as they follow our policies.”</p><p>While groups such as Jacarandas and Women Help Women had their accounts removed outright, other groups said that they increasingly faced Meta restricting their posts and shadow-banning their content.</p><p>Fatma Ibrahim, the director of the Sex Talk Arabic, a UK-based platform which offers Arabic-language content on sexual and reproductive health, said that the organisation had received a message almost every week from Meta over the past year saying that its page “didn’t follow the rules” and would not be suggested to other people, based on posts related to sexuality and sexual health.</p><figure id="3efc4ddb-c1c7-4bb7-ac98-61189cfd0704" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-5"><picture><source srcset="https://i.guim.co.uk/img/media/72d60b6755dcd49881de61a16b2ad609c37a28a7/0_0_1629_1192/master/1629.jpg?width=620&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/72d60b6755dcd49881de61a16b2ad609c37a28a7/0_0_1629_1192/master/1629.jpg?width=620&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/72d60b6755dcd49881de61a16b2ad609c37a28a7/0_0_1629_1192/master/1629.jpg?width=605&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/72d60b6755dcd49881de61a16b2ad609c37a28a7/0_0_1629_1192/master/1629.jpg?width=605&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/72d60b6755dcd49881de61a16b2ad609c37a28a7/0_0_1629_1192/master/1629.jpg?width=445&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/72d60b6755dcd49881de61a16b2ad609c37a28a7/0_0_1629_1192/master/1629.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 320px)"><img alt="An illustration of a naked man and woman walking along a path with an arm around each others’ wait while pink hearts float around, one covering their bottoms." src="https://i.guim.co.uk/img/media/72d60b6755dcd49881de61a16b2ad609c37a28a7/0_0_1629_1192/master/1629.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" width="445" height="325.6230816451811" loading="lazy"></picture></div><figcaption data-spacefinder-role="inline"><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>An Instagram post from The Sex Talk Arabic that triggered a nudity warning and was removed by Meta.</span> Photograph: Courtesy of Thesextalkarabic</figcaption></figure><p>Two weeks ago, these messages escalated to a warning, in which Meta noted its new policies on nudity and removed a post from the Sex Talk Arabic’s page. The offending <a href="https://www.instagram.com/p/DJMherqt6rA/?igsh=MW9zd254dTJ0MXhtbQ%3D%3D" data-link-name="in body link">post was an artistic depiction of a naked couple</a>, obscured by hearts.</p><p>Ibrahim said the warning was “condescending”, and that Meta’s moderation was US-centric and lacked context.</p><p>“Despite the profits they make from our region, they don’t invest enough to understand the social issues women fight against and why we use social media platforms for such fights,” she said.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A “frozen” dictionary for Python (162 pts)]]></title>
            <link>https://lwn.net/SubscriberLink/1047238/25c270b077849dc0/</link>
            <guid>46229467</guid>
            <pubDate>Thu, 11 Dec 2025 09:51:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/SubscriberLink/1047238/25c270b077849dc0/">https://lwn.net/SubscriberLink/1047238/25c270b077849dc0/</a>, See on <a href="https://news.ycombinator.com/item?id=46229467">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<blockquote>
<div>
<h3>Welcome to LWN.net</h3>
<p>
The following subscription-only content has been made available to you 
by an LWN subscriber.  Thousands of subscribers depend on LWN for the 
best news from the Linux and free software communities.  If you enjoy this 
article, please consider <a href="https://lwn.net/subscribe/">subscribing to LWN</a>.  Thank you
for visiting LWN.net!
</p></div>
</blockquote>

<p>
Dictionaries are ubiquitous in Python code; they are the data structure of
choice for a wide variety of tasks.  But dictionaries are mutable, which
makes them problematic for sharing data in concurrent code.  Python has
added various concurrency features to the language over the last decade or
so—<a href="https://lwn.net/Articles/726600/">async</a>, <a href="https://lwn.net/Articles/947138/">free threading without the global interpreter lock</a>
(GIL), and <a href="https://lwn.net/Articles/941090/">independent subinterpreters</a>—but users must work out their own
solution for an immutable dictionary that can be safely shared by
concurrent code.  There are existing modules that could be used, but a recent proposal, <a href="https://peps.python.org/pep-0814/">PEP 814</a> ("Add frozendict
built-in type"), looks to bring the feature to the language itself.
</p>

<p>
Victor Stinner <a href="https://discuss.python.org/t/pep-814-add-frozendict-built-in-type/104854">announced
the PEP</a> that he and Donghee Na have authored in a post to the PEPs
category of the <a href="https://discuss.python.org/">Python discussion
forum</a> on November 13.  The idea has come up before, including in <a href="https://peps.python.org/pep-0416/">PEP 416</a>, which has essentially
the same title as 814 and was authored by Stinner back in 2012.  It was
rejected by Guido van Rossum at the time, in part due to its target: a <a href="https://lwn.net/Articles/574215/">Python sandbox</a> that never
really panned out.
</p>

<h4><tt>frozendict</tt></h4>

<p>
The idea is fairly straightforward: add <tt>frozendict</tt> as a new
immutable type to the
language's <a href="https://docs.python.org/3/library/builtins.html#module"><tt>builtins</tt>
module</a>.  As Stinner put it:
</p><blockquote>
We expect <tt>frozendict</tt> to be safe by design, as it prevents any unintended modifications. This addition benefits not only CPython's standard library, but also third-party maintainers who can take advantage of a reliable, immutable dictionary type.
</blockquote>


<p>
While <tt>frozendict</tt> has a lot in common with the <a href="https://docs.python.org/3/library/stdtypes.html#mapping-types-dict"><tt>dict</tt></a>
built-in type, it is <a href="https://peps.python.org/pep-0814/#inherit-from-dict">not a subclass of <tt>dict</tt></a>; instead, it
is a subclass of the base <a href="https://docs.python.org/3/library/functions.html#object"><tt>object</tt></a>
type.  The <tt>frozendict()</tt> constructor can be used to create one in
various ways:
</p><pre>    fd = frozendict()           # empty
    fd = frozendict(a=1, b=2)   # frozen { 'a' : 1, 'b' : 2 }
    d = { 'a' : 1, 'b' : 2 }
    fd = frozendict(d)          # same
    l = [ ( 'a', 1 ), ( 'b', 2 ) ]
    fd = frozendict(l)          # same
    fd2 = frozendict(fd)        # same
    assert d == fd == fd2       # True
</pre>


<p>
As with dictionaries, the keys for a <tt>frozendict</tt> must be immutable,
thus <a href="https://docs.python.org/3/glossary.html#term-hashable">hashable</a>,
but the values may or may not be.  For example, a list is a legitimate type
for a value in either type of dictionary, but it is mutable, making the
dictionary as a whole (frozen or not) mutable.  However, if all of the
values stored in a <tt>frozendict</tt> are immutable, it is also immutable,
so it can be hashed and used in places where that is required
(e.g. dictionary keys, set elements, or entries in a <a href="https://docs.python.org/3/library/functools.html#functools.lru_cache"><tt>functools.lru_cache</tt></a>).
</p>

<p>
As might be guessed, based on the last line of the example above, frozen
dictionaries that are hashable can be compared for equality with other
dictionaries of either type.  In addition, neither the <a href="https://docs.python.org/3/library/functions.html#hash"><tt>hash()</tt></a>
value nor the equality test depend on the insertion order of the
dictionary, though that order is preserved in a frozen dictionary (as it is
in the regular variety).  So:
</p><pre>    d = { 'a' : 1, 'b' : 2 }
    fd = frozendict(d)
    d2 = { 'b' : 2, 'a' : 1 }
    fd2 = frozendict(d2)
    assert d == d2 == fd == fd2

    # frozendict unions work too, from the PEP
    &gt;&gt;&gt; frozendict(x=1) | frozendict(y=1)
    frozendict({'x': 1, 'y': 1})
    &gt;&gt;&gt; frozendict(x=1) | dict(y=1)
    frozendict({'x': 1, 'y': 1})
</pre><p>
For the unions, a new frozen dictionary is created in both cases; the
"</p><tt>|=</tt><p>" union-assignment operator also works by generating a new
</p><tt>frozendict</tt><p> for the result.
</p>

<p>
Iteration over a <tt>frozendict</tt> works as expected; the type implements
the <a href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Mapping"><tt>collections.abc.Mapping</tt></a>
abstract base class, so <tt>.items()</tt> returns an iterable of key-value
tuples, while <tt>.keys()</tt> and <tt>.values()</tt> provide the keys and
values of the frozen dictionary.
For the most part, a
<tt>frozendict</tt> acts like a <tt>dict</tt> that cannot change; the
specific differences between the two are <a href="https://peps.python.org/pep-0814/#differences-between-dict-and-frozendict">listed
in the PEP</a>.  It also contains a <a href="https://peps.python.org/pep-0814/#possible-candidates-for-frozendict-in-the-stdlib">lengthy
list</a> of places in the standard library where a <tt>dict</tt> could be switched to a
<tt>frozendict</tt> to "<q>enhance safety and prevent unintended modifications</q>".
</p>

<h4>Discussion</h4>

<p>
The reaction to the PEP was generally positive, with the usual suggestions
for tweaks and more substantive additions to the proposal.  Stinner kept
the discussion focused on the proposal at hand for the most part.  One part
of the proposal was troubling to some: converting a <tt>dict</tt> to a
<tt>frozendict</tt> was described as an O(n) shallow copy.  Daniel F
Moisset <a href="https://discuss.python.org/t/pep-814-add-frozendict-built-in-type/104854/19">thought</a>
that it would make sense to have an in-place transformation that could be
O(1) instead.  He proposed adding a <tt>.freeze()</tt> method that would
essentially just change the type of a <tt>dict</tt> object to
<tt>frozendict</tt>. 
</p>

<p>
However, changing the type of an existing object is fraught with peril, as Brett
Cannon <a href="https://discuss.python.org/t/pep-814-add-frozendict-built-in-type/104854/20">described</a>:

</p><blockquote>
But now you have made that dictionary frozen for everyone who holds a reference to it, which means side-effects at a distance in a way that could be unexpected (e.g. context switch in a thread and now suddenly you're going to get an exception trying to mutate what was a dict a microsecond ago but is now frozen). That seems like asking for really nasty debugging issues just to optimize some creation time.
</blockquote>



<p>
The PEP is not aimed at performance, he continued, but is meant to help
"<q>lessen bugs in concurrent code</q>".  Moisset <a href="https://discuss.python.org/t/pep-814-add-frozendict-built-in-type/104854/23">noted</a>,
that dictionaries can already change in unexpected ways via
<a href="https://docs.python.org/3/library/stdtypes.html#dict.clear"><tt>.clear()</tt></a>
or <a href="https://docs.python.org/3/library/stdtypes.html#dict.update"><tt>.update()</tt></a>,
thus the debugging issues already exist.  He recognized that the
authors may not want to tackle that as part of the PEP, but wanted to try
to ensure that an O(1) transformation was not precluded in the future.
</p>

<p>
Cannon's <a href="https://discuss.python.org/t/pep-814-add-frozendict-built-in-type/104854/47">strong
objection</a> is to changing the type of the object directly.  <a href="https://discuss.python.org/t/pep-814-add-frozendict-built-in-type/104854/24">Ben
Hsing</a> and <a href="https://discuss.python.org/t/pep-814-add-frozendict-built-in-type/104854/48">"Nice
Zombies"</a> proposed ways to construct a new <tt>frozendict</tt> without
requiring the shallow copy—thus O(1)—by either moving the hash table to a
newly created <tt>frozendict</tt>, while clearing the dictionary, or by
using a copy-on-write scheme for the table.  As Steve Dower <a href="https://discuss.python.org/t/pep-814-add-frozendict-built-in-type/104854/58">noted</a>,
that optimization can be added later as long as the PEP does not specify
that the operation <i>must</i> be O(n), which would be a silly thing to do,
but that it sometimes happens "<q>because it makes people stop
complaining</q>", he said in a footnote.  In light of the discussion, the
PEP <a href="https://peps.python.org/pep-0814/#method-to-convert-dict-to-frozendict">specifically
defers</a> that optimization to a later time, suggesting that it could also
be done for other frozen types (<a href="https://docs.python.org/3/library/stdtypes.html#tuple"><tt>tuple</tt></a>
and <a href="https://docs.python.org/3/library/stdtypes.html#frozenset"><tt>frozenset</tt></a>),
perhaps by resurrecting <a href="https://peps.python.org/pep-0351/">PEP
351</a> ("The freeze protocol").
</p>

<p>
On December 1, Stinner <a href="https://discuss.python.org/t/pep-814-add-frozendict-built-in-type/104854/89">announced</a>
that the PEP had been <a href="https://github.com/python/steering-council/issues/325">submitted to
the steering council</a> for pronouncement.  Given that Na is on the
council, though will presumably recuse himself from deciding on this PEP,
he probably has a pretty good sense for how it might be received by the group.
So it seems likely that the PEP has a good chance of being approved.  The
availability of the
free-threaded version of the language (i.e. without the GIL) means that more
multithreaded Python programs are being created, so having a safe way to share
dictionaries 
between threads will be a boon.
</p><br clear="all"><table>
           <tbody><tr><th colspan="2">Index entries for this article</th></tr>
           <tr><td><a href="https://lwn.net/Archives/PythonIndex/">Python</a></td><td><a href="https://lwn.net/Archives/PythonIndex/#Dictionaries">Dictionaries</a></td></tr>
            <tr><td><a href="https://lwn.net/Archives/PythonIndex/">Python</a></td><td><a href="https://lwn.net/Archives/PythonIndex/#Python_Enhancement_Proposals_PEP-PEP_814">Python Enhancement Proposals (PEP)/PEP 814</a></td></tr>
            </tbody></table><br clear="all">

               <br clear="all">
               <hr>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Cost of a Closure in C (158 pts)]]></title>
            <link>https://thephd.dev/the-cost-of-a-closure-in-c-c2y</link>
            <guid>46228597</guid>
            <pubDate>Thu, 11 Dec 2025 07:21:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thephd.dev/the-cost-of-a-closure-in-c-c2y">https://thephd.dev/the-cost-of-a-closure-in-c-c2y</a>, See on <a href="https://news.ycombinator.com/item?id=46228597">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
    <header id="main">
        
    </header>

    <section>
    
            <p>I had a vague idea that closures could have a variety of performance implications; I did not believe that so many of the chosen and potential designs for C and C++ extensions ones, however, were so…<!--more--> suboptimal.</p>

<p>But, before we get into how these things perform and what the cost of their designs are, we need to talk about what Closures are.</p>

<h2 id="closures">“Closures”?</h2>

<p>Closures in this instance are programming language constructs that includes data alongside instructions that are not directly related to their input (arguments) and their results (return values). They can be seen as a “generalization” of the concept of a function or function call, in that a function call is a “subset” of closures (e.g., the set of closures that do not include this extra, spicy data that comes from places outside of arguments and returns). These generalized functions and generalized function objects hold the ability to do things like work with “instance” data that is not passed to it directly (i.e., variables surrouding the closure off the stack) and, usually, some way to carry around more data than is implied by their associated function signature.</p>

<p>Pretty much all recent and modern languages include something for Closures unless they are deliberately developing for a target audience or for a source code design that is too “low level” for such a concept (such as Stack programming languages, Bytecode languages, or ones that fashion themselves as assembly-like or close to it). However, we’re going to be focusing on and looking specifically at Closures in C and C++, since this is going to be about trying to work with and – eventually – standardize something for ISO C that works for everyone.</p>

<p>First, let’s show a typical problem that arises in C code to show why closure solutions have popped up all over the C ecosystem, then talk about it in the context of the various solutions.</p>

<h2 id="the-closure-problem">The Closure Problem</h2>

<p>The closure problem can be neatly described by as “how do I get extra data to use within this <code>qsort</code> call?”. For example, consider setting this variable, <code>in_reverse</code>, as part of a bit of command line shenanigans, to change how a sort happens:</p>

<div><pre><code><span>#include</span> <span>&lt;stdlib.h&gt;</span><span>
#include</span> <span>&lt;string.h&gt;</span><span>
#include</span> <span>&lt;stddef.h&gt;</span><span>
</span>
<span>static</span> <span>int</span> <span>in_reverse</span> <span>=</span> <span>0</span><span>;</span>

<span>int</span> <span>compare</span><span>(</span><span>const</span> <span>void</span><span>*</span> <span>untyped_left</span><span>,</span> <span>const</span> <span>void</span><span>*</span> <span>untyped_right</span><span>)</span> <span>{</span>
  <span>const</span> <span>int</span><span>*</span> <span>left</span> <span>=</span> <span>untyped_left</span><span>;</span>
  <span>const</span> <span>int</span><span>*</span> <span>right</span> <span>=</span> <span>untyped_right</span><span>;</span>
  <span>return</span> <span>(</span><span>in_reverse</span><span>)</span> <span>?</span> <span>*</span><span>right</span> <span>-</span> <span>*</span><span>left</span> <span>:</span> <span>*</span><span>left</span> <span>-</span> <span>*</span><span>right</span><span>;</span>
<span>}</span>

<span>int</span> <span>main</span><span>(</span><span>int</span> <span>argc</span><span>,</span> <span>char</span><span>*</span> <span>argv</span><span>[])</span> <span>{</span>
  <span>if</span> <span>(</span><span>argc</span> <span>&gt;</span> <span>1</span><span>)</span> <span>{</span>
    <span>char</span><span>*</span> <span>r_loc</span> <span>=</span> <span>strchr</span><span>(</span><span>argv</span><span>[</span><span>1</span><span>],</span> <span>'r'</span><span>);</span>
    <span>if</span> <span>(</span><span>r_loc</span> <span>!=</span> <span>NULL</span><span>)</span> <span>{</span>
      <span>ptrdiff_t</span> <span>r_from_start</span> <span>=</span> <span>(</span><span>r_loc</span> <span>-</span> <span>argv</span><span>[</span><span>1</span><span>]);</span>
      <span>if</span> <span>(</span><span>r_from_start</span> <span>==</span> <span>1</span> <span>&amp;&amp;</span> <span>argv</span><span>[</span><span>1</span><span>][</span><span>0</span><span>]</span> <span>==</span> <span>'-'</span> <span>&amp;&amp;</span> <span>strlen</span><span>(</span><span>r_loc</span><span>)</span> <span>==</span> <span>1</span><span>)</span> <span>{</span>
        <span>in_reverse</span> <span>=</span> <span>1</span><span>;</span>
      <span>}</span> 
    <span>}</span>
  <span>}</span>
  <span>int</span> <span>list</span><span>[]</span> <span>=</span> <span>{</span> <span>2</span><span>,</span> <span>11</span><span>,</span> <span>32</span><span>,</span> <span>49</span><span>,</span> <span>57</span><span>,</span> <span>20</span><span>,</span> <span>110</span><span>,</span> <span>203</span> <span>};</span>
  <span>qsort</span><span>(</span><span>list</span><span>,</span> <span>(</span><span>sizeof</span><span>(</span><span>list</span><span>)</span><span>/</span><span>sizeof</span><span>(</span><span>*</span><span>list</span><span>)),</span> <span>sizeof</span><span>(</span><span>*</span><span>list</span><span>),</span> <span>compare</span><span>);</span>
	
  <span>return</span> <span>list</span><span>[</span><span>0</span><span>];</span>
<span>}</span>
</code></pre></div>

<p>This uses a <code>static</code> variable to have it persist between both the <code>compare</code> function calls that <code>qsort</code> makes and the <code>main</code> call which (potentially) changes its value to be <code>1</code> instead of <code>0</code>. Unfortunately, this isn’t always the best idea for more complex programs that don’t fit within a single snippet:</p>

<ul>
  <li>it is impossible to have different “copies” of a <code>static</code> variable, meaning all mutations done in all parts of the program that can see <code>in_reverse</code> are responsible for knowing the state before and after (e.g., heavily stateful programming of state that you may not own / cannot see);</li>
  <li>working on <code>static</code> data may produce thread contention/race conditions in more complex programs;</li>
  <li>using <code>_Thread_local</code> instead of <code>static</code> only solves the race condition problem but does not solve the “shared across several places on the same thread” problem;</li>
  <li>referring to specific pieces of data or local pieces of data (like <code>list</code> itself) become impossible;</li>
</ul>

<p>and so on, and so forth. This is the core of the problem here. It becomes more pronounced when you want to do things with function and data that are a bit more complex, such as <a href="https://rosettacode.org/wiki/Man_or_boy_test">Donald Knuth’s “Man-or-Boy” test code</a>.</p>

<p>The solutions to these problems come in 4 major flavors in C and C++ code.</p>

<ul>
  <li>Just reimplement the offending function to take a userdata pointer so you can pass whatever data you want (typical C solution, e.g. going from <code>qsort</code> as the sorting function to BSD’s <code>qsort_r</code><sup id="fnref:bsd-qsort_r" role="doc-noteref"><a href="#fn:bsd-qsort_r" rel="footnote">1</a></sup> or Annex K’s <code>qsort_s</code><sup id="fnref:annex-k-qsort_s" role="doc-noteref"><a href="#fn:annex-k-qsort_s" rel="footnote">2</a></sup>).</li>
  <li>Use GNU Nested Functions to just Refer To What You Want Anyways.</li>
  <li>Use Apple Blocks to just Refer To What You Want Anyways.</li>
  <li>Use C++ Lambdas and some elbow grease to just Refer To What You Want Anyways.</li>
</ul>

<p>Each solution has drawbacks and benefits insofar as usability and design, but as a quick overview we’ll show what it’s like using <code>qsort</code> (or <code>qsort_r</code>/<code>qsort_s</code>, where applicable). Apple Blocks, for starters, looks like this:</p>

<div><pre><code><span>#include</span> <span>&lt;stdlib.h&gt;</span><span>
#include</span> <span>&lt;string.h&gt;</span><span>
#include</span> <span>&lt;stddef.h&gt;</span><span>
</span>
<span>int</span> <span>main</span><span>(</span><span>int</span> <span>argc</span><span>,</span> <span>char</span><span>*</span> <span>argv</span><span>[])</span> <span>{</span>
	<span>// local, non-static variable</span>
	<span>int</span> <span>in_reverse</span> <span>=</span> <span>0</span><span>;</span>

	<span>// value changed in-line</span>
	<span>if</span> <span>(</span><span>argc</span> <span>&gt;</span> <span>1</span><span>)</span> <span>{</span>
		<span>char</span><span>*</span> <span>r_loc</span> <span>=</span> <span>strchr</span><span>(</span><span>argv</span><span>[</span><span>1</span><span>],</span> <span>'r'</span><span>);</span>
		<span>if</span> <span>(</span><span>r_loc</span> <span>!=</span> <span>NULL</span><span>)</span> <span>{</span>
			<span>ptrdiff_t</span> <span>r_from_start</span> <span>=</span> <span>(</span><span>r_loc</span> <span>-</span> <span>argv</span><span>[</span><span>1</span><span>]);</span>
			<span>if</span> <span>(</span><span>r_from_start</span> <span>==</span> <span>1</span> <span>&amp;&amp;</span> <span>argv</span><span>[</span><span>1</span><span>][</span><span>0</span><span>]</span> <span>==</span> <span>'-'</span> <span>&amp;&amp;</span> <span>strlen</span><span>(</span><span>r_loc</span><span>)</span> <span>==</span> <span>1</span><span>)</span> <span>{</span>
				<span>in_reverse</span> <span>=</span> <span>1</span><span>;</span>
			<span>}</span> 
		<span>}</span>
	<span>}</span>
	
	<span>int</span> <span>list</span><span>[]</span> <span>=</span> <span>{</span> <span>2</span><span>,</span> <span>11</span><span>,</span> <span>32</span><span>,</span> <span>49</span><span>,</span> <span>57</span><span>,</span> <span>20</span><span>,</span> <span>110</span><span>,</span> <span>203</span> <span>};</span>
	
	<span>qsort_b</span><span>(</span><span>list</span><span>,</span> <span>(</span><span>sizeof</span><span>(</span><span>list</span><span>)</span><span>/</span><span>sizeof</span><span>(</span><span>*</span><span>list</span><span>)),</span> <span>sizeof</span><span>(</span><span>*</span><span>list</span><span>),</span>
		<span>// Apple Blocks are Block Expressions, meaning they do not have to be stored</span>
		<span>// in a variable first</span>
		<span>^</span><span>(</span><span>const</span> <span>void</span><span>*</span> <span>untyped_left</span><span>,</span> <span>const</span> <span>void</span><span>*</span> <span>untyped_right</span><span>)</span> <span>{</span>
			<span>const</span> <span>int</span><span>*</span> <span>left</span> <span>=</span> <span>untyped_left</span><span>;</span>
			<span>const</span> <span>int</span><span>*</span> <span>right</span> <span>=</span> <span>untyped_right</span><span>;</span>
			<span>return</span> <span>(</span><span>in_reverse</span><span>)</span> <span>?</span> <span>*</span><span>right</span> <span>-</span> <span>*</span><span>left</span> <span>:</span> <span>*</span><span>left</span> <span>-</span> <span>*</span><span>right</span><span>;</span>
		<span>}</span>
	<span>);</span>
	
	<span>return</span> <span>list</span><span>[</span><span>0</span><span>];</span>
<span>}</span>
</code></pre></div>

<p>and GNU Nested Functions look like this:</p>

<div><pre><code><span>#include</span> <span>&lt;stdlib.h&gt;</span><span>
#include</span> <span>&lt;string.h&gt;</span><span>
#include</span> <span>&lt;stddef.h&gt;</span><span>
</span>
<span>int</span> <span>main</span><span>(</span><span>int</span> <span>argc</span><span>,</span> <span>char</span><span>*</span> <span>argv</span><span>[])</span> <span>{</span>
	<span>// local, non-static variable</span>
	<span>int</span> <span>in_reverse</span> <span>=</span> <span>0</span><span>;</span>

	<span>// modify variable in-line</span>
	<span>if</span> <span>(</span><span>argc</span> <span>&gt;</span> <span>1</span><span>)</span> <span>{</span>
		<span>char</span><span>*</span> <span>r_loc</span> <span>=</span> <span>strchr</span><span>(</span><span>argv</span><span>[</span><span>1</span><span>],</span> <span>'r'</span><span>);</span>
		<span>if</span> <span>(</span><span>r_loc</span> <span>!=</span> <span>NULL</span><span>)</span> <span>{</span>
			<span>ptrdiff_t</span> <span>r_from_start</span> <span>=</span> <span>(</span><span>r_loc</span> <span>-</span> <span>argv</span><span>[</span><span>1</span><span>]);</span>
			<span>if</span> <span>(</span><span>r_from_start</span> <span>==</span> <span>1</span> <span>&amp;&amp;</span> <span>argv</span><span>[</span><span>1</span><span>][</span><span>0</span><span>]</span> <span>==</span> <span>'-'</span> <span>&amp;&amp;</span> <span>strlen</span><span>(</span><span>r_loc</span><span>)</span> <span>==</span> <span>1</span><span>)</span> <span>{</span>
				<span>in_reverse</span> <span>=</span> <span>1</span><span>;</span>
			<span>}</span> 
		<span>}</span>
	<span>}</span>
	
	<span>int</span> <span>list</span><span>[]</span> <span>=</span> <span>{</span> <span>2</span><span>,</span> <span>11</span><span>,</span> <span>32</span><span>,</span> <span>49</span><span>,</span> <span>57</span><span>,</span> <span>20</span><span>,</span> <span>110</span><span>,</span> <span>203</span> <span>};</span>
	
	<span>// GNU Nested Function definition, can reference `in_reverse` directly</span>
	<span>// is a declaration/definition, and cannot be used directly inside of `qsort`</span>
	<span>int</span> <span>compare</span><span>(</span><span>const</span> <span>void</span><span>*</span> <span>untyped_left</span><span>,</span> <span>const</span> <span>void</span><span>*</span> <span>untyped_right</span><span>)</span> <span>{</span>
		<span>const</span> <span>int</span><span>*</span> <span>left</span> <span>=</span> <span>untyped_left</span><span>;</span>
		<span>const</span> <span>int</span><span>*</span> <span>right</span> <span>=</span> <span>untyped_right</span><span>;</span>
		<span>return</span> <span>(</span><span>in_reverse</span><span>)</span> <span>?</span> <span>*</span><span>right</span> <span>-</span> <span>*</span><span>left</span> <span>:</span> <span>*</span><span>left</span> <span>-</span> <span>*</span><span>right</span><span>;</span>
	<span>}</span>
	<span>// use in the sort function without the need for a `void*` parameter</span>
	<span>qsort</span><span>(</span><span>list</span><span>,</span> <span>(</span><span>sizeof</span><span>(</span><span>list</span><span>)</span><span>/</span><span>sizeof</span><span>(</span><span>*</span><span>list</span><span>)),</span> <span>sizeof</span><span>(</span><span>*</span><span>list</span><span>),</span> <span>compare</span><span>);</span>
	
	<span>return</span> <span>list</span><span>[</span><span>0</span><span>];</span>
<span>}</span>
</code></pre></div>

<p>or, finally, C++-style Lambdas:</p>

<div><pre><code><span>#define __STDC_WANT_LIB_EXT1__ 1
</span>
<span>#include</span> <span>&lt;stdlib.h&gt;</span><span>
#include</span> <span>&lt;string.h&gt;</span><span>
#include</span> <span>&lt;stddef.h&gt;</span><span>
</span>
<span>int</span> <span>main</span><span>(</span><span>int</span> <span>argc</span><span>,</span> <span>char</span><span>*</span> <span>argv</span><span>[])</span> <span>{</span>
	<span>int</span> <span>in_reverse</span> <span>=</span> <span>0</span><span>;</span>
	
	<span>if</span> <span>(</span><span>argc</span> <span>&gt;</span> <span>1</span><span>)</span> <span>{</span>
		<span>char</span><span>*</span> <span>r_loc</span> <span>=</span> <span>strchr</span><span>(</span><span>argv</span><span>[</span><span>1</span><span>],</span> <span>'r'</span><span>);</span>
		<span>if</span> <span>(</span><span>r_loc</span> <span>!=</span> <span>NULL</span><span>)</span> <span>{</span>
			<span>ptrdiff_t</span> <span>r_from_start</span> <span>=</span> <span>(</span><span>r_loc</span> <span>-</span> <span>argv</span><span>[</span><span>1</span><span>]);</span>
			<span>if</span> <span>(</span><span>r_from_start</span> <span>==</span> <span>1</span> <span>&amp;&amp;</span> <span>argv</span><span>[</span><span>1</span><span>][</span><span>0</span><span>]</span> <span>==</span> <span>'-'</span> <span>&amp;&amp;</span> <span>strlen</span><span>(</span><span>r_loc</span><span>)</span> <span>==</span> <span>1</span><span>)</span> <span>{</span>
				<span>in_reverse</span> <span>=</span> <span>1</span><span>;</span>
			<span>}</span> 
		<span>}</span>
	<span>}</span>
	
	<span>// lambdas are expressions, but we can assign their unique variable types with `auto`</span>
	<span>auto</span> <span>compare</span> <span>=</span> <span>[</span><span>&amp;</span><span>](</span><span>const</span> <span>void</span><span>*</span> <span>untyped_left</span><span>,</span> <span>const</span> <span>void</span><span>*</span> <span>untyped_right</span><span>)</span> <span>{</span>
		<span>const</span> <span>int</span><span>*</span> <span>left</span> <span>=</span> <span>(</span><span>const</span> <span>int</span><span>*</span><span>)</span><span>untyped_left</span><span>;</span>
		<span>const</span> <span>int</span><span>*</span> <span>right</span> <span>=</span> <span>(</span><span>const</span> <span>int</span><span>*</span><span>)</span><span>untyped_right</span><span>;</span>
		<span>return</span> <span>(</span><span>in_reverse</span><span>)</span> <span>?</span> <span>*</span><span>right</span> <span>-</span> <span>*</span><span>left</span> <span>:</span> <span>*</span><span>left</span> <span>-</span> <span>*</span><span>right</span><span>;</span>
	<span>};</span>

	<span>int</span> <span>list</span><span>[]</span> <span>=</span> <span>{</span> <span>2</span><span>,</span> <span>11</span><span>,</span> <span>32</span><span>,</span> <span>49</span><span>,</span> <span>57</span><span>,</span> <span>20</span><span>,</span> <span>110</span><span>,</span> <span>203</span> <span>};</span>	

	<span>// C++ Lambdas don't automatically make a trampoline, so we need to provide</span>
	<span>// one ourselves for the `qsort_s/r` case so we can call the lambda</span>
	<span>auto</span> <span>compare_trampoline</span> <span>=</span> <span>[](</span><span>const</span> <span>void</span><span>*</span> <span>left</span><span>,</span> <span>const</span> <span>void</span><span>*</span> <span>right</span><span>,</span> <span>void</span><span>*</span> <span>user</span><span>)</span> <span>{</span>
		<span>typeof</span><span>(</span><span>compare</span><span>)</span><span>*</span> <span>p_compare</span> <span>=</span> <span>user</span><span>;</span>
		<span>return</span> <span>(</span><span>*</span><span>p_compare</span><span>)(</span><span>left</span><span>,</span> <span>right</span><span>);</span>
	<span>};</span>
	<span>qsort_s</span><span>(</span><span>list</span><span>,</span> <span>(</span><span>sizeof</span><span>(</span><span>list</span><span>)</span><span>/</span><span>sizeof</span><span>(</span><span>*</span><span>list</span><span>)),</span> <span>sizeof</span><span>(</span><span>*</span><span>list</span><span>),</span> <span>compare_trampoline</span><span>,</span> <span>&amp;</span><span>compare</span><span>);</span>

	<span>return</span> <span>list</span><span>[</span><span>0</span><span>];</span>
<span>}</span>
</code></pre></div>

<p>To solve this gaggle of problems, pretty much every semi-modern language (that isn’t assembly-adjacent or based on some kind of state/stack programming) provide some idea of being able to associate some set of data with one or more function calls. And, particularly for Closures, this is done in a local way without passing it as an explicit argument. As it turns out, all of those design choices – including the ones in C – have pretty significant consequences on not just usability, but performance.</p>

<h2 id="not-a-big-overview">Not A Big Overview</h2>

<p>This article is <strong>NOT</strong> going to talk in-depth about the <strong>design</strong> of all of the alternatives or other languages. We’re focused on the actual cost of the extensions and what they mean. A detailed overview of the design tradeoffs, their security implications, and other problems, can be read at the <a href="https://thephd.dev/future_cxx/papers/C%20-%20Functions%20with%20Data%20-%20Closures%20in%20C.html">ISO C Proposal for Functions with Closures here</a>; it also gets into things like Security Implications, ABI, current implementation impact, and more of the various designs. The discussion in the paper is pretty long and talks about the dozens of aspects of each solution down to both the design aspect and the implementation quirks. We encourage you to dive into that proposal and read it to figure out if there’s something more specific you care about insofar as some specific design portion. But, this article is going to be concerned about one thing and one thing only:</p>

<h2 id="purrrrrrrformance-3">Purrrrrrrformance <strong>:3</strong>!</h2>

<p>In order to measure this cost, we are going to take Knuth’s Man-or-Boy test and benchmark various styles of implementation in C and C++ using various different extensions / features for the Closure problem. The Man-or-Boy test is an efficient measure of how well your programming language can handle referring to <em>specific</em> entities while engaging in a large degree of recursion and self-reference. It can stress test various portions of how your program creates and passes around data associated with a function call, and if your programming language design is so goofy that it can’t refer to a specific instance of a variable or function argument, it will end up producing the wrong answer and breaking horrifically.</p>

<h2 id="anatomy-of-a-benhcmark-raw-c">Anatomy of a Benhcmark: Raw C</h2>

<p>Here is the core of the Man-or-Boy test, as implemented in raw C. This implementation<sup id="fnref:idk-benchmarks-closures" role="doc-noteref"><a href="#fn:idk-benchmarks-closures" rel="footnote">3</a></sup> and all the others are available online for us all to scrutinize and yell at me for messing up, to make sure I’m not slandering your favorite solution for Closures in this space.</p>

<div><pre><code><span>// ...</span>

<span>static</span> <span>int</span> <span>eval</span><span>(</span><span>ARG</span><span>*</span> <span>a</span><span>)</span> <span>{</span>
	<span>return</span> <span>a</span><span>-&gt;</span><span>fn</span><span>(</span><span>a</span><span>);</span>
<span>}</span>

<span>static</span> <span>int</span> <span>B</span><span>(</span><span>ARG</span><span>*</span> <span>a</span><span>)</span> <span>{</span>
	<span>int</span> <span>k</span>    <span>=</span> <span>*</span><span>a</span><span>-&gt;</span><span>k</span> <span>-=</span> <span>1</span><span>;</span>
	<span>ARG</span> <span>args</span> <span>=</span> <span>{</span> <span>B</span><span>,</span> <span>&amp;</span><span>k</span><span>,</span> <span>a</span><span>,</span> <span>a</span><span>-&gt;</span><span>x1</span><span>,</span> <span>a</span><span>-&gt;</span><span>x2</span><span>,</span> <span>a</span><span>-&gt;</span><span>x3</span><span>,</span> <span>a</span><span>-&gt;</span><span>x4</span> <span>};</span>
	<span>return</span> <span>A</span><span>(</span><span>&amp;</span><span>args</span><span>);</span>
<span>}</span>

<span>static</span> <span>int</span> <span>A</span><span>(</span><span>ARG</span><span>*</span> <span>a</span><span>)</span> <span>{</span>
	<span>return</span> <span>*</span><span>a</span><span>-&gt;</span><span>k</span> <span>&lt;=</span> <span>0</span> <span>?</span> <span>eval</span><span>(</span><span>a</span><span>-&gt;</span><span>x4</span><span>)</span> <span>+</span> <span>eval</span><span>(</span><span>a</span><span>-&gt;</span><span>x5</span><span>)</span> <span>:</span> <span>B</span><span>(</span><span>a</span><span>);</span>
<span>}</span>

<span>// ...</span>
</code></pre></div>

<p>You will notice that there is a big, fat, ugly <code>ARG*</code> parameter hanging around all of these functions. That is because, as stated before, plain ISO C cannot handle passing the data around unless it’s part of a function’s arguments. Because the actual core of the Man-or-Boy experiment is the ability to refer to specific values of <code>k</code> that exist during the recursive run of the program, we need to actually <strong>modify the function signature</strong> and thereby cheat some of the implicit Man-or-Boy requirements of not passing the value in directly. Here’s what <code>ARG</code> looks like:</p>

<div><pre><code><span>typedef</span> <span>struct</span> <span>arg</span> <span>{</span>
	<span>int</span> <span>(</span><span>*</span><span>fn</span><span>)(</span><span>struct</span> <span>arg</span><span>*</span><span>);</span>
	<span>int</span><span>*</span> <span>k</span><span>;</span>
	<span>struct</span> <span>arg</span> <span>*</span><span>x1</span><span>,</span> <span>*</span><span>x2</span><span>,</span> <span>*</span><span>x3</span><span>,</span> <span>*</span><span>x4</span><span>,</span> <span>*</span><span>x5</span><span>;</span>
<span>}</span> <span>ARG</span><span>;</span>

<span>static</span> <span>int</span> <span>f_1</span><span>(</span><span>ARG</span><span>*</span> <span>_</span><span>)</span> <span>{</span>
	<span>return</span> <span>-</span><span>1</span><span>;</span>
<span>}</span>

<span>static</span> <span>int</span> <span>f0</span><span>(</span><span>ARG</span><span>*</span> <span>_</span><span>)</span> <span>{</span>
	<span>return</span> <span>0</span><span>;</span>
<span>}</span>

<span>static</span> <span>int</span> <span>f1</span><span>(</span><span>ARG</span><span>*</span> <span>_</span><span>)</span> <span>{</span>
	<span>return</span> <span>1</span><span>;</span>
<span>}</span>

<span>static</span> <span>int</span> <span>eval</span><span>(</span><span>ARG</span><span>*</span> <span>a</span><span>)</span> <span>{</span>
	<span>// ...</span>
<span>}</span>
<span>// ...</span>
</code></pre></div>

<p>And this is how it gets used in the main body of the function in order to compute the right answer and benchmark it:</p>

<div><pre><code><span>static</span> <span>void</span> <span>normal_functions_rosetta</span><span>(</span><span>benchmark</span><span>::</span><span>State</span><span>&amp;</span> <span>state</span><span>)</span> <span>{</span>
	<span>const</span> <span>int</span> <span>initial_k</span>  <span>=</span> <span>k_value</span><span>();</span>
	<span>const</span> <span>int</span> <span>expected_k</span> <span>=</span> <span>expected_k_value</span><span>();</span>
	<span>int64_t</span> <span>result</span>       <span>=</span> <span>0</span><span>;</span>

	<span>for</span> <span>(</span><span>auto</span> <span>_</span> <span>:</span> <span>state</span><span>)</span> <span>{</span>
		<span>int</span> <span>k</span>     <span>=</span> <span>initial_k</span><span>;</span>
		<span>ARG</span> <span>arg1</span>  <span>=</span> <span>{</span> <span>f1</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span> <span>};</span>
		<span>ARG</span> <span>arg2</span>  <span>=</span> <span>{</span> <span>f_1</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span> <span>};</span>
		<span>ARG</span> <span>arg3</span>  <span>=</span> <span>{</span> <span>f_1</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span> <span>};</span>
		<span>ARG</span> <span>arg4</span>  <span>=</span> <span>{</span> <span>f1</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span> <span>};</span>
		<span>ARG</span> <span>arg5</span>  <span>=</span> <span>{</span> <span>f0</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span> <span>};</span>
		<span>ARG</span> <span>args</span>  <span>=</span> <span>{</span> <span>B</span><span>,</span> <span>&amp;</span><span>k</span><span>,</span> <span>&amp;</span><span>arg1</span><span>,</span> <span>&amp;</span><span>arg2</span><span>,</span> <span>&amp;</span><span>arg3</span><span>,</span> <span>&amp;</span><span>arg4</span><span>,</span> <span>&amp;</span><span>arg5</span> <span>};</span>
		<span>int</span> <span>value</span> <span>=</span> <span>A</span><span>(</span><span>&amp;</span><span>args</span><span>);</span>
		<span>result</span> <span>+=</span> <span>value</span> <span>==</span> <span>expected_k</span> <span>?</span> <span>1</span> <span>:</span> <span>0</span><span>;</span>
	<span>}</span>

	<span>if</span> <span>(</span><span>result</span> <span>!=</span> <span>state</span><span>.</span><span>iterations</span><span>())</span> <span>{</span>
		<span>state</span><span>.</span><span>SkipWithError</span><span>(</span><span>"failed: did not produce the right answer!"</span><span>);</span>
	<span>}</span>
<span>}</span>

<span>BENCHMARK</span><span>(</span><span>normal_functions_rosetta</span><span>);</span>
</code></pre></div>

<p>Everything within the <code>for (auto _ : state) { ... }</code> is benchmarked. For those paying attention to the code and find it looking familiar, it’s because that code is the basic structure all Google Benchmark<sup id="fnref:google-benchmark" role="doc-noteref"><a href="#fn:google-benchmark" rel="footnote">4</a></sup> code finds itself looking like. I’ve wanted to swap to Catch2<sup id="fnref:catch2-benchmark" role="doc-noteref"><a href="#fn:catch2-benchmark" rel="footnote">5</a></sup> for a long time now to change to their benchmarking infrastructure, but I’ve been stuck on Google Benchmark because I’ve made a lot of graph-making tools based on its JSON output and I have not vetted Catch2’s JSON output yet to see if it has all of the necessary bits ‘n’ bobbles I use to de-dedup runs and compute statistics.</p>

<p>Everything outside is setup (the part above the <code>for</code> loop) or teardown/test correction (the part below the <code>for</code> loop). The initialization of the <code>ARG args</code>s cannot be moved outside of the measuring loop because each invocation of <code>A</code> – the core of the Man-or-Boy experiment – modifies the <code>k</code> of the ARG parameter, so all of them have to be inside. Conceivably, <code>arg1 .. 5</code> could be moved out of the loop, but I am very tired of looking at the eight or nine variations of this code so someone else can move it and tell me if Clang or GCC has lots of compiler optimization sauce and doesn’t understand that those 5 <code>argI</code>s can be hoisted out of the loop.</p>

<p>The value <code>k</code> is <code>10</code>, and <code>expected_k</code> is <code>-67</code>. The expected, returned <code>k</code> value is dependent on the input <code>k</code> value, which controls how deep the Man-or-Boy test would recurse on itself to produce its answer. Therefore, to prevent GCC and Clang and other MEGA POWERFUL PILLAR COMPILERS from optimizing the entire thing out and just replacing the benchmark loop with <code>ret -67</code>, both <code>k_value()</code> and <code>expected_k_value()</code> come from a Dynamic Link Library (<code>.dylib</code> on MacOS, <code>.so</code> on *nix platforms, <code>.dll</code> on Windows platforms) to make sure that NO amount of optimization (Link Time Optimization/Link Time Code Generation, Inlining Optimization, Cross-Translation Unit Optimization, and Automatic Constant Expression Optimization) from C or C++ compilers could fully preempt all forms of computation.</p>

<p>This allows us to know, for sure, that we’re actually measuring something and not just testing how fast a compiler can load a number into a register and test it against <code>state.iterations()</code>. And, since we know for sure, we can now talk the general methodology.</p>

<h2 id="methodology">Methodology</h2>

<p>The tests were ran on a dying 13-inch 2020 MacBook Pro M1 that has suffered several toddler spills and two severe falls. It has 16 GB of RAM and is son MacOS 15.7.2 Sequoia at the time the test was taken, using the stock MacOS AppleClang Compiler and the stock <code>brew install gcc</code> compiler in order to produce the numbers seen on December 6th, 2025.</p>

<p>There 2 measures being conducted: Real Time and CPU Time. The time is gathered by running a single iteration of the code within the <code>for</code> loop anywhere from a couple thousand to hundreds of thousands of times to produce confidence in that run of the benchmark. This is then averaged to produce the first point. The process is repeated 50 times, repeating that many iterations to build further confidence in the measurement. All 50 means are used as the points for the values, and the average of all of those 50 means is then used as the height of a bar in a bar graph.</p>

<p>The bars are presented side-by-side as a horizontal bar chart with 11 categories of C or C++ code being measured. The 11 categories are:</p>

<ol>
  <li><code>no-op</code>: Literally doing nothing. It’s just there to test environmental noise and make sure none of our benchmarks are so off-base that we’re measuring noise rather than computation. Helps keep us grounded in reality.</li>
  <li><code>Lambdas (No Function Helpers)</code>: a solution using C++-style lambdas. Rather than using helper functions like <code>f0</code>, <code>f1</code>, and <code>f_1</code>, we compute a raw lambda that stores the value meant to be returned for the Man-or-Boy test (<code>return i;</code>) in the lambda itself and then pass that uniquely-typed lambda to the core of the test. The entire test is templated and uses a fake <code>recursion</code> template parameter to halt the recursion after a certain depth.</li>
  <li><code>Lambdas</code>: The same as above but actually using <code>int f0(void)</code>, etc. helper functions at the start rather than lambdas. Reduces inliner pressure by using “normal” types which do not add to the generated number of lambda-typed, recursive, templated function calls.</li>
  <li><code>Lambdas (std::function_ref)</code>: The same as above, but rather than using a function template to handle each uniquely-typed lambda like a precious baby bird, it instead erases the lambda behind a <code>std::function_ref&lt;int(void)&gt;</code>. This allows the recursive function to retain exactly one signature.</li>
  <li><code>Lambdas (std::function)</code>: The same as above, but replaces <code>std::function_ref&lt;int(void)&gt;</code> with <code>std::function&lt;int(void)&gt;</code>. This is its allocating, C++03-style type.</li>
  <li><code>Lambdas (Rosetta Code)</code>: The code straight out of the C++11 Rosetta Code Lambda section on the Man-or-Boy Rosetta Code implementation.</li>
  <li><code>Apple Blocks</code>: Uses Apple Blocks to implement the test, along with the <code>__block</code> specifier to refer directly to certain variables on the stack.</li>
  <li><code>GNU Nested Functions (Rosetta Code)</code>: The code straight out of the C Rosetta Code section on the Man-or-Boy Rosetta Code implementation.</li>
  <li><code>GNU Nested Functions</code>: GNU Nested Functions similar to the Rosetta Code implementation, but with some slight modifications in a hope to potentially alleviate some stack pressure if possible by using regular helper functions like <code>f0</code>, <code>f1</code>, and <code>f_1</code>.</li>
  <li><code>Custom C++ Class</code>: A custom-written C++ class using a discriminated union to decide whether its doing a straight function call or attemping to engage in the Man-or-Boy recursion.</li>
  <li><code>C++03 shared_ptr (Rosetta Code)</code>: A C++ class using <code>std::enable_shared_from_this</code> and <code>std::shared_ptr</code> with a virtual function call to invoke the “right” function call during recursion.</li>
</ol>

<p>The two compilers tested are Apple Clang 17 and GCC 15. There are two graph images because one is for Apple Clang and the other is for GCC. This is particularly important because neither compiler implements the other’s closure extension (Clang does Apple Blocks but not Nested Functions, while GCC does Nested Functions in exclusively its C frontend but does not implement Apple Blocks<sup id="fnref:gcc-apple-blocks" role="doc-noteref"><a href="#fn:gcc-apple-blocks" rel="footnote">6</a></sup>).</p>

<h2 id="the-results">The Results</h2>

<p>Ta-da!</p>

<p><img src="https://thephd.dev/assets/img/2025/12/appleclang17_closure_linear.png" alt=""></p>

<p><sub><sub><sub><a href="https://thephd.dev/assets/img/2025/12/appleclang17_closure_linear.png.txt">For the vision-impaired, a text description is available.</a></sub></sub></sub></p>

<p><img src="https://thephd.dev/assets/img/2025/12/gcc15_closure_linear.png" alt=""></p>

<p><sub><sub><sub><a href="https://thephd.dev/assets/img/2025/12/gcc15_closure_linear.png.txt">For the vision-impaired, a text description is available.</a></sub></sub></sub></p>

<p>… Oh. That looks <strong>awful</strong>.</p>

<p>It turns out that some solutions are so dogwater that it completely screws up our viewing graphs. But, it does let us know that Lambdas used the Rosetta Code style are so unbelievably awful that it is several orders of magnitude more expensive than any other solution presented! One has to wonder what the hell is going on in the code snippet there, but first we need to make the graphs more legible. To do this we’re going to be using the (slightly deceptive) <strong>LOGARITHMIC SCALING</strong>. This is a bit deadly to do because it tends to mislead people about how much of a change there is, so please pay attention to the <strong>potential order of magnitude gains and losses</strong> when going from one bar graph to another.</p>

<p><img src="https://thephd.dev/assets/img/2025/12/appleclang17_closure_logarithmic.png" alt="">
<sub><sub><sub><a href="https://thephd.dev/assets/img/2025/12/appleclang17_closure_logarithmic.png.txt">For the vision-impaired, a text description is available.</a></sub></sub></sub></p>

<p><img src="https://thephd.dev/assets/img/2025/12/gcc15_closure_logarithmic.png" alt=""></p>

<p><sub><sub><sub><a href="https://thephd.dev/assets/img/2025/12/gcc15_closure_logarithmic.png.txt">For the vision-impaired, a text description is available.</a></sub></sub></sub></p>

<p>There we go. Now we can talk about the various solutions and – in particular – why “lambdas” have 4 different entries with such wildly differing performance profiles. First up, let’s talk about the clear performance winners.</p>

<h2 id="lambdas-on-top">Lambdas: On Top!</h2>

<p>Not surprising to anyone who has been checked in to C++, lambdas that are used directly and not type-erased are on top. This means there’s a one-to-one mapping between a function call and a given bit of execution. We are cheating by using a constant parameter to stop the uniquely-typed lambdas being passed into the functions from recursing infinitely, which makes the Man-or-Boy function look like this:</p>

<div><pre><code><span>template</span> <span>&lt;</span><span>int</span> <span>recursion</span> <span>=</span> <span>0</span><span>&gt;</span>
<span>static</span> <span>int</span> <span>a</span><span>(</span><span>int</span> <span>k</span><span>,</span> <span>const</span> <span>auto</span><span>&amp;</span> <span>x1</span><span>,</span> <span>const</span> <span>auto</span><span>&amp;</span> <span>x2</span><span>,</span> <span>const</span> <span>auto</span><span>&amp;</span> <span>x3</span><span>,</span> <span>const</span> <span>auto</span><span>&amp;</span> <span>x4</span><span>,</span> <span>const</span> <span>auto</span><span>&amp;</span> <span>x5</span><span>)</span> <span>{</span>
	<span>if</span> <span>constexpr</span> <span>(</span><span>recursion</span> <span>==</span> <span>11</span><span>)</span> <span>{</span>
		<span>::</span><span>std</span><span>::</span><span>cerr</span> <span>&lt;&lt;</span> <span>"This should never happen and this code should never have been generated."</span> <span>&lt;&lt;</span> <span>std</span><span>::</span><span>endl</span><span>;</span>
		<span>::</span><span>std</span><span>::</span><span>terminate</span><span>();</span>
		<span>return</span> <span>0</span><span>;</span>
	<span>}</span>
	<span>else</span> <span>{</span>
		<span>auto</span> <span>B</span> <span>=</span> <span>[</span><span>&amp;</span><span>](</span><span>this</span> <span>const</span> <span>auto</span><span>&amp;</span> <span>self</span><span>)</span> <span>{</span> <span>return</span> <span>a</span><span>&lt;</span><span>recursion</span> <span>+</span> <span>1</span><span>&gt;</span><span>(</span><span>--</span><span>k</span><span>,</span> <span>self</span><span>,</span> <span>x1</span><span>,</span> <span>x2</span><span>,</span> <span>x3</span><span>,</span> <span>x4</span><span>);</span> <span>};</span>
		<span>return</span> <span>k</span> <span>&lt;=</span> <span>0</span> <span>?</span> <span>x4</span><span>()</span> <span>+</span> <span>x5</span><span>()</span> <span>:</span> <span>B</span><span>();</span>
	<span>}</span>
<span>}</span>
</code></pre></div>

<p>Every <code>B</code> is its own unique type and we are not erasing that unique type when using the expression as an initializer to <code>B</code>. This means that when we call <code>a</code> again with <code>B</code> (the <code>self</code> in this lambda here using Deduced This, a C++23 feature that cannot be part of the C version of lambdas) which means we need to use <code>auto</code> parameters (a shortcut way of writing template parameters) to take it. But, since every parameter is unique, and every <code>B</code> is unique, calling this recursively means that, eventually, C++ compilers will actually just completely crash out/toss out-of-memory errors/say we’ve compile-time recursed too hard, or similar. That’s why the compile-time <code>if constexpr</code> on the extra, templated <code>recursion</code> parameter needs to have some arbitrary limit. Because we know <code>k</code> starts at 10 for this test, we just have some bogus limit of “11”.</p>

<p>This results in a very spammy recursive chain of function calls, where the actual generated names of these template functions is <strong>far</strong> more complex than <code>a</code> and can run the compiler into the ground / cause quite a bit of instantiations if you let <code>recursion</code> get to a high enough value. But, once you add the limit, the compiler gets perfect information about this recursive call all the way to every leaf, and thus is able to not only optimize the hell out of it, but refuse to generate the other frivolous code it knows won’t be useful.</p>

<h3 id="lambdas-are-also-fast-even-when-type-erased">Lambdas are also Fast, even when Type-Erased</h3>

<p>You can observe a slight bump up in performance penalty when a Lambda is erased by a <code>std::function_ref</code>. This is a low-level, non-allocating, non-owning, slim “view” type that is analogous to what a language-based wide function pointer type would be in C. From this, it allows us to <em>guess</em> how good Lambdas in C would be even if you had to hide them behind a non-unique type.</p>

<p>The performance metrics are about equivalent to if you hand-wrote a C++ class with a custom <code>operator()</code> that uses a discriminated union, no matter which compiler gets used to do it. It’s obviously not as fast as having access to a direct function call and being able to slurp-inline optimize, but the performance difference is acceptable when you do not want to engage in a large degree of what is called “monomorphisation” of a genric routine or type. And, indeed, outside of macros, C has no way of doing this innately that isn’t runtime-based.</p>

<p>A very strong contender for a good solution!</p>

<h3 id="lambdas-on-bottom-too">Lambdas: On…. Bottom, too?</h3>

<p>One must wonder, then, why the <code>std::function</code> Lambdas and the Rosetta Code Lambdas are either bottom-middle-of-the-road or absolutely-teary-eyed-awful.</p>

<p>Starting off, the <code>std::function</code> Lambdas are bad because of exactly that: <code>std::function</code>. <code>std::function</code> is not a “cheap” closure; it is a potentially-allocating, meaty, owning function abstraction. This means that it’s safe to make one and pass it around and store it and call it later; the cost of this is, obviously, that you’re allocating (when the type is big enough) for that internal storage. Part of this is alleviated by using <code>const std::function&lt;int(void)&gt;&amp;</code> parameters, taking things by reference and only generating a new object when necessary. This prevents copying on every function call. Both the Rosetta Lambdas and regular <code>std::function</code> Lambdas code does the by-reference parameters bit, though, so where does the difference come in? It actually has to do with the Captures. Here’s how <code>std::function</code> Lambdas defines the recursive, self-referential lambda and uses it:</p>

<div><pre><code><span>using</span> <span>f_t</span> <span>=</span> <span>std</span><span>::</span><span>function</span><span>&lt;</span><span>int</span><span>(</span><span>void</span><span>)</span><span>&gt;</span><span>;</span>

<span>inline</span> <span>static</span> <span>int</span> <span>A</span><span>(</span><span>int</span> <span>k</span><span>,</span> <span>const</span> <span>f_t</span><span>&amp;</span> <span>x1</span><span>,</span> <span>const</span> <span>f_t</span><span>&amp;</span> <span>x2</span><span>,</span> <span>const</span> <span>f_t</span><span>&amp;</span> <span>x3</span><span>,</span> <span>const</span> <span>f_t</span><span>&amp;</span> <span>x4</span><span>,</span> <span>const</span> <span>f_t</span><span>&amp;</span> <span>x5</span><span>)</span> <span>{</span>
	<span>f_t</span> <span>B</span> <span>=</span> <span>[</span><span>&amp;</span><span>]</span> <span>{</span> <span>return</span> <span>A</span><span>(</span><span>--</span><span>k</span><span>,</span> <span>B</span><span>,</span> <span>x1</span><span>,</span> <span>x2</span><span>,</span> <span>x3</span><span>,</span> <span>x4</span><span>);</span> <span>};</span>
	<span>return</span> <span>k</span> <span>&lt;=</span> <span>0</span> <span>?</span> <span>x4</span><span>()</span> <span>+</span> <span>x5</span><span>()</span> <span>:</span> <span>B</span><span>();</span>
<span>}</span>
</code></pre></div>

<p>And, here is how the Rosetta Code Lambdas defines the recursive, self-referential lambda and uses it:</p>

<div><pre><code><span>using</span> <span>f_t</span> <span>=</span> <span>std</span><span>::</span><span>function</span><span>&lt;</span><span>int</span><span>(</span><span>void</span><span>)</span><span>&gt;</span><span>;</span>

<span>inline</span> <span>static</span> <span>int</span> <span>A</span><span>(</span><span>int</span> <span>k</span><span>,</span> <span>const</span> <span>f_t</span><span>&amp;</span> <span>x1</span><span>,</span> <span>const</span> <span>f_t</span><span>&amp;</span> <span>x2</span><span>,</span> <span>const</span> <span>f_t</span><span>&amp;</span> <span>x3</span><span>,</span> <span>const</span> <span>f_t</span><span>&amp;</span> <span>x4</span><span>,</span> <span>const</span> <span>f_t</span><span>&amp;</span> <span>x5</span><span>)</span> <span>{</span>
	<span>f_t</span> <span>B</span> <span>=</span> <span>[</span><span>=</span><span>,</span> <span>&amp;</span><span>k</span><span>,</span> <span>&amp;</span><span>B</span><span>]</span> <span>{</span> <span>return</span> <span>A</span><span>(</span><span>--</span><span>k</span><span>,</span> <span>B</span><span>,</span> <span>x1</span><span>,</span> <span>x2</span><span>,</span> <span>x3</span><span>,</span> <span>x4</span><span>);</span> <span>};</span>
	<span>return</span> <span>k</span> <span>&lt;=</span> <span>0</span> <span>?</span> <span>x4</span><span>()</span> <span>+</span> <span>x5</span><span>()</span> <span>:</span> <span>B</span><span>();</span>
<span>}</span>
</code></pre></div>

<p>The big problem here is in the use of the <code>=</code>. What <code>=</code> by itself in the front of a lambda capture clause means is “copy all the visible variables in and hold onto that copy” (unless the capture for that following variable is “overridden” by a <code>&amp;var</code>, address capture). Meanwhile, the <code>&amp;</code> is the opposite: it means “refer to all the visible variables directly by their address and do not copy them in”. So, while the <code>std::function</code> Lambda is (smartly) referring to stuff directly without copying because we know for the Man-or-Boy test that referring to things directly is not an unsafe operation, the general <code>=</code> causes that for the several dozen recursive iterations through the function, it is copying all five allocating <code>std::function</code> arguments. So the first call creates a <code>B</code> that copies everything in, and then passes that in, and then the next call copies the previous <code>B</code> and the 4 normal functions, and then passes that in to the next <code>B</code>, and then it copies <strong>both</strong> previous <code>B</code>’s, and this stacks for the depth of the callgraph (some 10 times since <code>k = 10</code> to start).</p>

<p>You can imagine how much that completely screws with the performance, and it explains why the Rosetta Code Lambdas code behaves so poorly in terms of performance. But, this also raises a question: if referring to everything by-reference saves so much speed, then why does GNU Nested Functions – in all its variants – perform so poorly? After all, Nested Functions capture everything by reference / by address, exactly like a lambda does with <code>[&amp;]</code>.</p>

<p>Similarly, if allocating over and over again was so expensive, how come Apple Blocks and C++03 <code>shared_ptr</code> Rosetta Code-style versions of the Man-or-Boy test don’t perform nearly as badly as the Rosetta Code Lambdas? Are we not copying the value of the arguments into a newly created Apple Block and, thusly, tanking the performance metrics? Well, as it turns out, there’s many reasons for these things, so let’s start with GNU Nested Functions.</p>

<h2 id="nested-functions-and-the-stack">Nested Functions and The Stack</h2>

<p>I’ve written about it <a href="https://thephd.dev/lambdas-nested-functions-block-expressions-oh-my">dozens of times</a> now, but the prevailing and most common implementation of Nested Functions is with an executable stack. The are <a href="https://thephd.dev/_vendor/future_cxx/papers/C%20-%20Functions%20with%20Data%20-%20Closures%20in%20C.html#intro-nested.functions-security">a lot of security and other implications for this</a>, but all you need to understand is that the <em>reason</em> GCC did this is because it was an at-the-time slick encoding of both the <em>location</em> of the variables and the <em>routine</em> itself. Allocating a chunk of data off of the current programming stack means that the “environment context”/”this closure” pointer has the same anchoring address as the routine itself. This means you can encode both the location of the data to know what to access <em>and</em> the address of a function’s entry point into a single thing that works with your typical setup-and-call convention that comes with invoking a standard ISO C function pointer.</p>

<p>But think about that, briefly, in terms of optimization.</p>

<p>You are using the function’s stack frame at that precise point in the program as the “base address” for this executable code. That base address also means that all the variables associated with it need to be <strong>reachable</strong> from that base address: i.e., that things are not stuffed in registers, but that you are referring to the same variables as modified by the enclosing function around your nested function. Principally, this means that your function needs to have all of the following now so that GNU Nested Functions <em>actually</em> work.</p>

<ul>
  <li>A stack that is executable so that the base address used for the trampoline can be run succinctly.</li>
  <li>A real function frame that exists somewhere in memory to serve as the base address for the trampoline.</li>
  <li>Real objects in memory backing the names of the captured variables accesses.</li>
</ul>

<p>This all seems like regular consequences, until you tack on the second order affects from the point of optimization.</p>

<ul>
  <li>A stack that now has both data and instructions all blended into itself.</li>
  <li>A real function frame, which means no ommission of a frame pointer and no collapsing / inlining of that function frame.</li>
  <li>Real objects that all have their address taken that are tied to the function frame, which must be memory-accessible and which the compiler now has a hard time telling if they can simply be exchanged through registers or if the need to <strong>actually</strong> sit somewhere in memory.</li>
</ul>

<p>In other words: GNU Nested Functions have created the perfect little storm for what might be the best optimizer-murderer. The reason it performs so drastically poorly (worse than even allocating lambdas inside of a <code>std::function</code> or C++03-style virtual function calls inside of a bulky, nasty C++ <code>std::shared_ptr</code>) by a whole order of magnitude or more is that everything about Nested Functions and their current implementation is basically Optimizer Death. If the compiler can’t see through everything – and the Man-or-Boy test with a non-constant value of <code>k</code> and <code>expected_k</code> – GNU Nested Functions deteriorate rapidly. It takes every core optimization technique that we’ve researched and maximized on in the last 30 years and puts a shotgun to the side of its head once it can’t pre-compute <code>k</code> and <code>expected_k</code>.</p>

<p>The good news is that GCC has completed a new backing implementation for GNU Nested Functions, which uses a heap-based trampoline. Such a trampoline does not interfere with the stack, would allow for omission of frame pointers while referring directly to the data itself (which may prevent the wrecking of specific kinds of inlining optimizations), and does not need an executable stack (just a piece of memory from ✨somewhere✨ it can mark executable). This may have performance closer to Apple Blocks, but we don’t have a build of the latest GCC to test it with. But, when we do, we can simply add the compilation flag <code>-ftrampoline-impl=heap</code> to the two source files in CMake and then let the benchmarks run again to see how it stacks up!</p>

<p>Finally, there is a <em>minor</em> performance degredation because our benchmarking software is in C++ and this extension exists exclusively in the C frontend of GCC. That means I have to use an <code>extern</code> function call within the benchmark loop to get to the actual code. Within the function call, however, all of this stuff should be optimized down, so the cost of a <em>single</em> function call’s stack frame shouldn’t be so awful, but I expect to try to dig into this better to help make sure the <code>extern</code> of a C function call isn’t making things dramatically worse than they are. Given it’s a different translation unit and it’s <strong>not</strong> being compiled as a separate static or dynamic library, it should still link together and optimize cleanly, but given how bad it’s performing? Every possible issue is on the table.</p>

<h2 id="what-about-apple-blocks">What about Apple Blocks?</h2>

<p>Apple Blocks are not the fastest, but they the best of the C extensions while being the worst of the “fast” solutions. They are not faster than just hacking the <code>ARG*</code> into the function signature and using regular normal C function calls, unfortunately, and that’s likely due to their shared, heap-ish nature. The saddest part about Apple Blocks is that it works using a Blocks Runtime that is already as optimized as it can possibly be: Clang and Apple both document that whie the Blocks Runtime does manage an Automatic Reference Counted (ARC) Heap of Block pointers, when a Block is first created it will literally have its memory stored on the stack rather than in the heap. In order to move it to the heap, one must call <code>Block_copy</code> to trigger the “normal” heap-based shenanigans. We never call <code>Block_copy</code>, so this is with as-fast-as-possible variable access and management with few allocations.</p>

<p>It’s very slightly disappointing that: normal C functions with an <code>ARG*</code> blob; a custom C++ class using a discriminated union and <code>operator()</code>; any mildly conscientious use of lambdas; and, any other such shenanigans perform better than the very best Apple Blocks has to offer. One has to imagine that all of the ARC management functions made to copy the <code>int^(void)</code> “hat-style” function pointers, even if they end up not doing much for the data stored on the stack, impacted the results here. But, this is also somewhat good news: because Apple Block hat pointers are cheaply-copiable entities (they are just pointers to a Block object), it means that even if we copy all of the arguments into the closure every function call, that copying is about as cheap as it can get. Obivously, as regular “Lambdas” and “Lambas (No Function Helpers)” demonstrate, being able to just slurp everything up by address/by reference – including visible function arguments – with <code>[&amp;]</code> saves us a teensy, tiny bit of time<sup id="fnref:apple-blocks-parameters" role="doc-noteref"><a href="#fn:apple-blocks-parameters" rel="footnote">7</a></sup>.</p>

<p>The cheapness of <code>int^(void)</code> hat-pointer function types is likely the biggest saving grace for Apple Blocks in this benchmark. In the one place we need to be careful, we rename the input argument <code>k</code> to <code>arg_k</code> and then make a <code>__block</code> variable to actually refer to a shared <code>int k</code> (and get the right answer):</p>

<div><pre><code><span>static</span> <span>int</span> <span>a</span><span>(</span><span>int</span> <span>arg_k</span><span>,</span> <span>fn_t</span> <span>^</span> <span>x1</span><span>,</span> <span>fn_t</span> <span>^</span> <span>x2</span><span>,</span> <span>fn_t</span> <span>^</span> <span>x3</span><span>,</span> <span>fn_t</span> <span>^</span> <span>x4</span><span>,</span> <span>fn_t</span> <span>^</span> <span>x5</span><span>)</span> <span>{</span>
	<span>__block</span> <span>int</span> <span>k</span>    <span>=</span> <span>arg_k</span><span>;</span>
	<span>__block</span> <span>fn_t</span> <span>^</span> <span>b</span> <span>=</span> <span>^</span><span>(</span><span>void</span><span>)</span> <span>{</span> <span>return</span> <span>a</span><span>(</span><span>--</span><span>k</span><span>,</span> <span>b</span><span>,</span> <span>x1</span><span>,</span> <span>x2</span><span>,</span> <span>x3</span><span>,</span> <span>x4</span><span>);</span> <span>};</span>
	<span>return</span> <span>k</span> <span>&lt;=</span> <span>0</span> <span>?</span> <span>x4</span><span>()</span> <span>+</span> <span>x5</span><span>()</span> <span>:</span> <span>b</span><span>();</span>
<span>}</span>
</code></pre></div>

<p>All of the <code>x1</code>, <code>x2</code>, and <code>x3</code> – like the bad Lambda case – are copied over and over and over again. One could change the name of all the arugments <code>arg_xI</code> and then have an <code>xI</code> variable inside that is marked <code>__block</code>, but that’s more effort and very unlikely to have any serious impact on the code while possibly degrading performance for the setup of multiple shared variables that all have to also be ARC-reference-counted and be stored inside each and every new <code>b</code> block that is created.</p>

<h2 id="a-brief-aside-self-referencing-functionsclosures">A Brief Aside: Self-Referencing Functions/Closures</h2>

<p>It’s also important to note that just writing this:</p>

<div><pre><code><span>static</span> <span>int</span> <span>a</span><span>(</span><span>int</span> <span>arg_k</span><span>,</span> <span>fn_t</span> <span>^</span> <span>x1</span><span>,</span> <span>fn_t</span> <span>^</span> <span>x2</span><span>,</span> <span>fn_t</span> <span>^</span> <span>x3</span><span>,</span> <span>fn_t</span> <span>^</span> <span>x4</span><span>,</span> <span>fn_t</span> <span>^</span> <span>x5</span><span>)</span> <span>{</span>
	<span>__block</span> <span>int</span> <span>k</span>    <span>=</span> <span>arg_k</span><span>;</span>
	<span>fn_t</span> <span>^</span> <span>b</span> <span>=</span> <span>^</span><span>(</span><span>void</span><span>)</span> <span>{</span> <span>return</span> <span>a</span><span>(</span><span>--</span><span>k</span><span>,</span> <span>b</span><span>,</span> <span>x1</span><span>,</span> <span>x2</span><span>,</span> <span>x3</span><span>,</span> <span>x4</span><span>);</span> <span>};</span>
	<span>return</span> <span>k</span> <span>&lt;=</span> <span>0</span> <span>?</span> <span>x4</span><span>()</span> <span>+</span> <span>x5</span><span>()</span> <span>:</span> <span>b</span><span>();</span>
<span>}</span>
</code></pre></div>

<p>(no <code>__block</code> on the <code>b</code> variable) is actually a huge bug. Apple Blocks, like older C++ Lambdas, cannot technically refer to “itself” inside. You have to refer to the “self” by capturing the variable it set to. For those who use C++ and are familiar with the lambdas over there, it’s like making sure you capture the variable you initialize with the lambda by reference while <em>also</em> making sure it has a concrete type. It can only be escaped by using <code>auto</code> and Deducing This, or some other combination of referential-use. That is:</p>

<ul>
  <li><code>auto x = [&amp;x](int v) { if (v != limit) x(v + 1); return v + 8; }</code> does not compile, as the type <code>auto</code> isn’t figured out yet;</li>
  <li><code>std::function_ref&lt;int(int)&gt; x = [&amp;x](int v) { if (v != limit) x(v + 1); return v + 8; }</code> compiles but due to C++ shenanigans produces a dangling reference to a temporary lambda that dies after the full expression (the initialization);</li>
  <li><code>std::function&lt;int(int)&gt; x = [&amp;x](int v) { if (v != limit) x(v + 1); return v + 8; }</code> compiles and works with no segfaults because <code>std::function</code> allocates, and the reference to itself <code>&amp;x</code> is just fine.</li>
  <li>and, finally, <code>auto x = [](this const auto&amp; self, int v) { if (v != limit) self(v + 1); return v + 8; }</code> which compiles and works with no segfaults because the invisible <code>self</code> parameter is just a reference to the current object.</li>
</ul>

<p>The problem with the most recent Apple Blocks snippet just above is that it’s the equivalent of doing</p>

<ul>
  <li><code>std::function&lt;int(int)&gt; x = [x](int v) { if (v != limit) x(v + 1); return v + 8; }</code></li>
</ul>

<p>Notice that there’s no <code>&amp;x</code> in the lambda initializer’s capture list. It’s copying an (uninitialized) variable by-value into the lambda. This is what Apple Blocks set into a variable that does not have a <code>__block</code> specifier, like in our bad code case with <code>b</code>.</p>

<p>All variations of this on all implementations which allow for self-referencing allow this and compile some form of this. You would imagine some implementations would warn about this, but this is leftover nonsense from allowing a variable to refer to itself in its initialization. The obvious reason this happens in C and C++ is because you can create self-referential structures, but unfortunately neither languages provided a safe way to do this generally. C++23’s Deducing This does not work inside of regular functions and non-objects, so good luck applying to other places and other extensions. The only extension which does not suffer this problem is GNU Nested Functions, because it creates a function declaration / definition rather than a variable with an initializer. Thus, this code from the benchmarks works:</p>

<div><pre><code><span>inline</span> <span>static</span> <span>int</span> <span>gnu_nested_functions_a</span><span>(</span><span>int</span> <span>k</span><span>,</span> <span>int</span> <span>xl</span><span>(</span><span>void</span><span>),</span> <span>int</span> <span>x2</span><span>(</span><span>void</span><span>),</span> <span>int</span> <span>x3</span><span>(</span><span>void</span><span>),</span> <span>int</span> <span>x4</span><span>(</span><span>void</span><span>),</span> <span>int</span> <span>x5</span><span>(</span><span>void</span><span>))</span> <span>{</span>
	<span>int</span> <span>b</span><span>(</span><span>void</span><span>)</span> <span>{</span>
		<span>return</span> <span>gnu_nested_functions_a</span><span>(</span><span>--</span><span>k</span><span>,</span> <span>b</span><span>,</span> <span>xl</span><span>,</span> <span>x2</span><span>,</span> <span>x3</span><span>,</span> <span>x4</span><span>);</span>
	<span>}</span>
	<span>return</span> <span>k</span> <span>&lt;=</span> <span>0</span> <span>?</span> <span>x4</span><span>()</span> <span>+</span> <span>x5</span><span>()</span> <span>:</span> <span>b</span><span>();</span>
<span>}</span>
</code></pre></div>

<p>And it has the semantics one would expect, unlike how Blocks, Lambdas, or others with default by-value copying works.</p>

<p>In the general case, this is what the paper <code>__self_func</code> was going to solve<sup id="fnref:__self_func" role="doc-noteref"><a href="#fn:__self_func" rel="footnote">8</a></sup>, but… that’s going to need some time for me to convince WG14 that maybe it IS actually a good idea. We can probably just keep writing the buggy code a few dozen more times for the recursion case and keep leaving it error prone, but I’ll try my best to convince them one more time that the above situation is very not-okay.</p>

<h2 id="thinking-it-over">Thinking It Over</h2>

<p>While the Man-or-Boy test isn’t exactly the end-all, be-all performance test, due to flexing both (self)-referential data and utilization of local copies with recursion, it is surprisingly suitable for figuring out if a closure design is decent enough in a mid to high-level programming language. It also gives me some confidence that, at the very least, the baseline for performance of statically-known, compile-time understood, non-type erased, callable Closure object will have the best implementation quality and performance tradeoffs for a language like ISO C no matter the compiler implementation.</p>

<p>In the future, at some point, I’ll have to write about <strong>why</strong> that is. It’s a bit upside-down from the perspective of readers of this blog to <strong>first</strong> address performance and then later write about the design, but it’s nice to make sure we’re not designing ourselves into a bad performance corner at the offset of this whole adventure.</p>

<h2 id="learned-insights">Learned Insights</h2>

<p>Surprising nobody, the more information the compiler is allowed to accrue (the Lambda design), the better its ability to make the code fast. What might be slightly more surprising is that a <strong>slim</strong>, <strong>compact</strong> layer of type erasure – not a bulky set of Virtual Function Calls (C++03 <code>shared_ptr</code> Rosetta Code design) – does not actually cost much at all (Lambdas with <code>std::function_ref</code>). This points out something else that’s part of the ISO C proposal for Closures (but not formally in its wording): Wide Function Pointers.</p>

<p>The ability to make a thin <code>{ some_function_type* func; void* context; }</code> type backed by the compiler in C would be extremely powerful. Martin Uecker has a proposal that has received interest and passing approval in the Committee, but it would be nice to <a href="https://thephd.dev/_vendor/future_cxx/papers/C%20-%20Functions%20with%20Data%20-%20Closures%20in%20C.html#appendix-wide.function.pointer">move it along in a nice direction</a>. My suggestion is having <code>%</code> as a modifier, so it can be used easily since wide function pointers are an extremely prevalent concept. Being able to write something like the following would be very easy and helpful.</p>

<div><pre><code><span>typedef</span> <span>int</span><span>(</span><span>compute_fn_t</span><span>)(</span><span>int</span><span>);</span>

<span>int</span> <span>do_computation</span><span>(</span><span>int</span> <span>num</span><span>,</span> <span>compute_fn_t</span><span>%</span> <span>success_modification</span><span>);</span>
</code></pre></div>

<p>A wide function pointer type like this would also be traditionally convertible from a number of already-existing extensions, too, where GNU Nested Functions, Apple Blocks, C++-style Lambdas, and more could create the appropriate wide function pointer type to be cheaply used. Additionally, it also works for FFI: things like Go closures already use GCC’s <code>__builtin_call_with_static_chain</code> to transport through their Go functions in C. Many other functions from other languages could be cheaply and efficiently bridged with this, without having to come up with hairbrained schemes about where to put a <code>void* userdata</code> or some kind of implicit context pointer / implicit environment pointer.</p>

<h2 id="existing-extensions">Existing Extensions?</h2>

<p>Unfortunately – except for the borland closure annotation – there’s too many things that are performance-stinky about both GNU Nested Functions and Apple Blocks. It’s no wonder GCC is trying to add <code>-ftrampoline-impl=heap</code> to the story of GNU Nested Functions; they might be able to tighten up that performance and make it more competitive with Apple Blocks. But, unfortunately, since it is heap-based, there’s a real chance that its <strong>maximum</strong> performance ceiling is only as good as Apple Blocks, and <strong>not</strong> as good as a C++-style Lambda.</p>

<p>Both GNU Nested Functions and Apple Blocks – as they are implemented – do not really work well in ISO C. GNU Nested Functions because their base design and most prevalent implementation are performance-awful, but also Apple Blocks because of the copying and indirection runtime of Blocks that manage ARC pointers providing a hard upper limit on how good the performance can actually be in complex cases.</p>

<p>Regular C code, again, performs middle-of-the-road here. It’s not the worst of it, but it’s not the best at all, which means there’s some room beneath how we could go having the C code run. While it’s hard to fully trust the Rosetta Code Man-or-Boy code for C as the best, it is a pretty clear example of how a “normal” C developer would do it and how it’s not actually able to hit maximum performance for this situation.</p>

<p>I wanted to add a version of regular C code that used a dynamic array with <code>static</code>s to transfer data, or a bunch of <code>thread_local</code>s, but I could not bring myself to actually care enough to write a complex association scheme from a specific invocation of the recursive function <code>a</code> and the slot of dynamic data that represented the closure’s data. I’m sure there’s schemes for it and I could think of a few, but at that point it’s such a violent contortion to get a solution that going that I figured it simply wasn’t worth the effort. But, as always,</p>

<p>pull requests are welcome. 💚</p>

<ul>
  <li>Banner and Title Photo by <a href="https://www.pexels.com/photo/person-holding-black-card-holder-928181/">Lukas, from Pexels</a></li>
</ul>







    
    </section>

    <!-- Social media shares -->
    






    <!-- Category and Tag list -->
    <div data-testid="tag-list">
    <ul>
      
        <li>Tags</li>
      

      
        <li><a data-testid="tag-link" href="https://thephd.dev/tags#C">
          <p><i></i> C</p>
        </a></li>
      
        <li><a data-testid="tag-link" href="https://thephd.dev/tags#C+standard">
          <p><i></i> C standard</p>
        </a></li>
      
        <li><a data-testid="tag-link" href="https://thephd.dev/tags#C%2B%2B">
          <p><i></i> C++</p>
        </a></li>
      
        <li><a data-testid="tag-link" href="https://thephd.dev/tags#blocks">
          <p><i></i> blocks</p>
        </a></li>
      
        <li><a data-testid="tag-link" href="https://thephd.dev/tags#closures">
          <p><i></i> closures</p>
        </a></li>
      
        <li><a data-testid="tag-link" href="https://thephd.dev/tags#functions">
          <p><i></i> functions</p>
        </a></li>
      
        <li><a data-testid="tag-link" href="https://thephd.dev/tags#performance">
          <p><i></i> performance</p>
        </a></li>
      
        <li><a data-testid="tag-link" href="https://thephd.dev/tags#purrformance">
          <p><i></i> purrformance</p>
        </a></li>
      
        <li><a data-testid="tag-link" href="https://thephd.dev/tags#%F0%9F%93%8A">
          <p><i></i> 📊</p>
        </a></li>
      
    </ul>
  </div>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Incomplete list of mistakes in the design of CSS (176 pts)]]></title>
            <link>https://wiki.csswg.org/ideas/mistakes</link>
            <guid>46227619</guid>
            <pubDate>Thu, 11 Dec 2025 04:20:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wiki.csswg.org/ideas/mistakes">https://wiki.csswg.org/ideas/mistakes</a>, See on <a href="https://news.ycombinator.com/item?id=46227619">Hacker News</a></p>
<div id="readability-page-1" class="page"><div class="page">

<p>
That should be corrected if anyone invents a time machine. :P
</p>
<ul>
<li><p><code>white-space: nowrap</code> should be <code>white-space: no-wrap</code></p>
<ul>
<li><p> and line wrapping behavior should not have been added to <code>white-space</code></p>
</li>
</ul>
</li>
<li><p><code>animation-iteration-count</code> should just have been <code>animation-count</code> (like <code>column-count</code>!)</p>
</li>
<li><p><code>vertical-align</code> should not apply to table cells. Instead the CSS3 alignment properties should exist in Level 1.</p>
</li>
<li><p><code>vertical-align: middle</code> should be <code>text-middle</code> or <code>x-middle</code> because it's not really in the middle, and such a name would better describes what it does.</p>
</li>
<li><p> Percentage heights should be calculated against <code>fill-available</code> rather than being undefined in auto situations.</p>
</li>
<li><p> Table layout should be sane.</p>
</li>
<li><p> Box-sizing should be <code>border-box</code> by default.</p>
</li>
<li><p><code>background-size</code> with one value should duplicate its value, not default the second one to <code>auto</code>. Ditto <code>translate()</code>.</p>
</li>
<li><p><code>background-position</code> and <code>border-spacing</code> (all 2-axis properties) should take *vertical* first, to match with the 4-direction properties like <code>margin</code>.</p>
</li>
<li><p> Not quite a mistake, because it was a reasonable default for the 90s, but it would be more helpful since then if `background-repeat` defaulted to `no-repeat`.</p>
</li>
<li><p> The 4-value shorthands like <code>margin</code> should go counter-clockwise (so that the inline-start value is before the block-end and inline-end values instead of after them).</p>
</li>
<li><p><code>z-index</code> should be called <code>z-order</code> or <code>depth</code> and should Just Work on all elements (like it does on flex items).</p>
</li>
<li><p><code>word-wrap</code>/<code>overflow-wrap</code> should not exist. Instead, <code>overflow-wrap</code> should be a keyword on 'white-space', like <code>nowrap</code> (<code>no-wrap</code>).</p>
</li>
<li><p> The top and bottom margins of a single box should never have been allowed to collapse together automatically as this is the <strong>root of all margin-collapsing evil</strong>.</p>
</li>
<li><p> Partial collapsing of margins instead of weird rules to handle min/max-heights?</p>
</li>
<li><p> Tables (like other non-blocks, e.g. flex containers) should form pseudo-stacking contexts.</p>
</li>
<li><p> The <code>currentColor</code> keyword should have retained the dash, <code>current-color</code>, as originally specified. Likewise all other color multi-word keyword names.</p>
</li>
<li><p> There should have been a predictable color naming system (like CNS) instead of the arbitrary X11 names which were eventually adopted.</p>
</li>
<li><p><code>border-radius</code> should have been <code>corner-radius</code>.</p>
</li>
<li><p> Absolutely-positioned replaced elements should stretch when opposite offset properties (e.g. left+right) are set, instead of being start-aligned.</p>
</li>
<li><p> The <code>hyphens</code> property should be called <code>hyphenate</code>. (It's called <code>hyphens</code> because the XSL:FO people objected to <code>hyphenate</code>.)</p>
</li>
<li><p><code>rgba()</code> and <code>hsla()</code> should not exist, <code>rgb()</code> and <code>hsl()</code>  should have gotten an optional fourth parameter instead (and the alpha value should have used the same format as R, G, and B or S and L).</p>
</li>
<li><p> Descendant combinator should have been <code>»</code> and indirect sibling combinator should have been <code>++</code>, so there's some logical relationships among the selectors' ascii art</p>
</li>
<li><p> The <code>*-blend-mode</code> properties should've just been <code>*-blend</code></p>
</li>
<li><p> The syntax of unicode ranges should have consistent with the rest of <abbr title="Cascading Style Sheets">CSS</abbr>, like <code>u0001-u00c8</code>.</p>
</li>
<li><p> Unicode ranges should not have had a separate microsyntax with their own tokenization or token handling.  The tokenization hacks required either to make selectors (e.g., u+a) handle things that are unicode-range tokens, or make unicode-range handle the other huge range of tokens (numbers and dimensions with and without scientific notation, identifiers, +) are both horrible.</p>
</li>
<li><p><code>font-family</code> should have required the font name to be quoted (like all other values that come from “outside” <abbr title="Cascading Style Sheets">CSS</abbr>).  The rules for handling unquoted font names make parsing <code>font</code> stupid, as it requires a <code>font-size</code> value for disambiguation.</p>
</li>
<li><p> Flexbox should have been less crazy about <code>flex-basis</code> vs <code>width</code>/<code>height</code>.  Perhaps: if <code>width</code>/<code>height</code> is <code>auto</code>, use <code>flex-basis</code>; otherwise, stick with <code>width</code>/<code>height</code> as an inflexible size.  (This also makes min/max width/height behavior fall out of the generic definition.)</p>
</li>
<li><p><del><code>:empty</code> should have been <code>:void</code>, and <code>:empty</code> should select items that contain only white space</del> FIXED in the <abbr title="specification">spec</abbr>, waiting for implementations to check for Web-compat…</p>
</li>
<li><p><code>table-layout: fixed; width: auto</code> should result in a fill-available table with fixed-layout columns.</p>
</li>
<li><p><code>text-orientation</code> should have had <code>upright</code> as the initial value (given the latest changes to 'writing-mode').</p>
</li>
<li><p> The <code>@import</code> rule is required to (a) always hit the network unless you specify cache headers, and (b) construct fresh CSSStyleSheet objects for every import, even if they're identical. It should have had more aggressive <abbr title="Uniform Resource Locator">URL</abbr>-based deduping and allowed sharing of stylesheet objects.</p>
</li>
<li><p> Selectors have terrible future-proofing. We should have split on top-level commas, and only ignored unknown/invalid segments, not the entire thing.</p>
</li>
<li><p><code>:link</code> should have had the <code>:any-link</code> semantics all along.</p>
</li>
<li><p> The <code>flex</code> shorthand (and <code>flex-shrink</code> and <code>flex-grow</code> longhands) should accept <code>fr</code> units instead of bare numbers to represent flex fractions.</p>
</li>
<li><p> The <code>display</code> property should be called <code>display-type</code>.</p>
</li>
<li><p> The <code>list-style</code> properties should be called <code>marker-style</code>, and <code>list-item</code> renamed to <code>marked-block</code> or something.</p>
</li>
<li><p> The <code>text-overflow</code> property should always apply, not be dependent on <code>overflow</code></p>
</li>
<li><p><code>line-height: &lt;percentage&gt;</code> should compute to the equivalent <code>line-height: &lt;number&gt;</code>, so that it effectively inherits as a percentage not a length</p>
</li>
<li><p><code>::placeholder</code> should be <code>::placeholder-text</code> and <code>:placeholder-shown</code> should be <code>:placeholder</code></p>
</li>
<li><p><code>overflow: scroll</code> should introduce a stacking context</p>
</li>
<li><p><code>size</code> should have been a shorthand for <code>width</code> and <code>height</code> instead of an <code>@page</code> property with a different definition</p>
</li>
<li><p> We probably should have avoided mixing keywords (<code>span</code>) with idents in the <a href="https://github.com/w3c/csswg-drafts/issues/1137" title="https://github.com/w3c/csswg-drafts/issues/1137" rel="ugc nofollow">grid properties</a>, possibly by using functional notation (like <code>span(2)</code>).</p>
</li>
<li><p> Comments shouldn't have been allowed basically everywhere in <abbr title="Cascading Style Sheets">CSS</abbr> (compare to <abbr title="HyperText Markup Language">HTML</abbr>, which basically only allows them where content goes), because it makes them basically unrepresentable in the object model, which in turn makes building editing directly on top of the object model impossible</p>
</li>
<li><p> The alignment properties in Flexbox should have been writing-mode relative, not flex-flow relative, and thus could have reasonably understandable names like <code>align-inline-*</code> and <code>align-block-*</code>.</p>
</li>
<li><p><code>shape-outside</code> should have had <code>wrap-</code> in the name somehow, as people assume the shape should also clip the content as in <code>clip-path</code>.</p>
</li>
<li><p> It shouldn't be <code>!important</code> —&nbsp;that reads to engineers as “not important”. We should have picked another way to write this.</p>
</li>
</ul>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Vibe coding is mad depressing (225 pts)]]></title>
            <link>https://law.gmnz.xyz/vibe-coding-is-mad-depressing/</link>
            <guid>46227422</guid>
            <pubDate>Thu, 11 Dec 2025 03:50:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://law.gmnz.xyz/vibe-coding-is-mad-depressing/">https://law.gmnz.xyz/vibe-coding-is-mad-depressing/</a>, See on <a href="https://news.ycombinator.com/item?id=46227422">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    

    

    
        

        <p>
            <i>
                <time datetime="2025-12-11T03:28Z">
    11 Dec, 2025
</time>
            </i>
        </p>
    

    <p>I’ve been in the mobile development industry for almost 15 years, and this AI/LLM era might be the worst.</p>
<p>My work are mostly freelance, gigs, hourly, milestones, and I could say 90% of my experience are greenfield projects. I don’t have any apps on my own, I make a living coding apps for others.</p>
<h3 id="before-ai">Before AI</h3><p>Back in the day, during a client kickoff they usually hand me a document with a UI prototype and a list of features. Then, you start from scratch, File - New Project. <code>git init</code> that shit and you’re on your way.</p>
<p>Everything was calm, clients just wanted a weekly or monthly feedback because they know how hard mobile development is you know. No pressure. You can focus on the great work, clean code, proper variable naming, proper git commit, all that stuffs.</p>
<p>In 2-3 months you get an alpha or beta build out, and clients are very happy. They can’t believe there idea, has now transformed into something they can play with.</p>
<h3 id="start-of-ai-era">Start of AI era</h3><p>Fast forward to today, or maybe it started around 2-3 years ago. Nothing wrong with it at first. Like any freelancer, I try to adopt with the latest trends.</p>
<p>At first it was just code snippets.</p>
<blockquote>
<p>Hey! I asked AI for this code, do you think this will work? I think you should use it.</p>
</blockquote>
<p>Okay, so this non-technical person is sending me codes now.</p>
<p>I mostly reply with</p>
<blockquote>
<p>It’s alright I got some working code blocks that worked in production perfectly fine. Thanks though!</p>
</blockquote>
<p>But, then this code snippets get larger and larger as time goes on. I'm thankful for this suggestions of course. But, it's just additional work when you're coding and you get this AI source code and then you have to think on how to merge this code with a different coding style and variable names into your codebase.</p>
<h3 id="vibe-coding-era">Vibe Coding era</h3><p>The first clues started when a client, who I thought was a software developer, starts merging his own code through the <code>main</code> branch, without warning. No pull request, just straight <code>git push --force origin main</code>.</p>
<p>As I started to checkout what the code was about, I started seeing this emojis inside the <code>print()</code> statements. I thought, this is so odd and unprofessional.</p>
<p><img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/lawgimenez/17am.webp" alt="Screenshot 2025-12-11 at 10"></p>
<p>I tried to Google search the macOS shortcut for emojis, to match this person's vibe. This fella must really like emojis you know. It turns out, AI code has a lot of emojis along with it.</p>
<p>The other sign was how the branching, and merging works with AI. And maybe feature request? I really don't know. For example, one vibe coded project has 1,227 branches and counting. I haven't merged one yet, I let the client deal with that.</p>
<p><img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/lawgimenez/41am.webp" alt="Screenshot 2025-12-11 at 10"></p>
<p>Last time, I checked this Xcode project did not compiled. Or anything close to it.</p>
<p>And the last thing that made me snapped was, all this vibed source code were located inside one file <code>ContentView</code>. To anyone who's not familiar, <code>ContentView</code> is the first SwiftUI file created when you start a new Xcode project.</p>
<p><img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/lawgimenez/20pm.webp" alt="Screenshot 2025-12-09 at 12"></p>
<p>All the UI logic, view models, model are located inside that file. Worst part, this is currently live in the App Store.</p>
<h3 id="conclusion">Conclusion</h3><p>I totally get it, everyone has to make a living. Creating an app is one of them. I just feel sad with how AI has bastardized my profession, which I worked hard for the last 15 years. There is no best practices anymore, no proper process, no meaningful back and forth. Just dealing with thousands and thousands of lines of code at every project kickoff.</p>


    

    
        

        
            


        

        
            
        
    


  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Patterns.dev (491 pts)]]></title>
            <link>https://www.patterns.dev/</link>
            <guid>46226483</guid>
            <pubDate>Thu, 11 Dec 2025 01:18:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.patterns.dev/">https://www.patterns.dev/</a>, See on <a href="https://news.ycombinator.com/item?id=46226483">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <h3>
            We offer a modern perspective on patterns
          </h3>
          <p>
            A common critique of design patterns is that they needlessly add
            complexity.
          </p>
          <p>
            Our perspective is that patterns are valuable for solving specific
            problems, often helping to 
            <strong>communicate comminalities in code problems</strong> for humans.
            If a project doesn't have those problems, there isn't a need
            to apply them. Patterns can also be very language or framework-specific
            (e.g. React), which can often mean thinking beyond the scope of just
            the original GoF design patterns.
          </p>
        </div><div>
          <h3>
            We help you scale your webapps for performance
          </h3>
          <p>
            Learn about web performance patterns for loading your code more
            efficiently. Unsure how to think about modern approaches to loading
            or rendering user-experiences? We've got you covered.
          </p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[When would you ever want bubblesort? (2023) (106 pts)]]></title>
            <link>https://buttondown.com/hillelwayne/archive/when-would-you-ever-want-bubblesort/</link>
            <guid>46224311</guid>
            <pubDate>Wed, 10 Dec 2025 21:45:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://buttondown.com/hillelwayne/archive/when-would-you-ever-want-bubblesort/">https://buttondown.com/hillelwayne/archive/when-would-you-ever-want-bubblesort/</a>, See on <a href="https://news.ycombinator.com/item?id=46224311">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
        
            <p>There are very few universal rules in software engineering, but there are are a lot of <em>near</em>-universal principles. Things like "prefer composition to inheritance" is near-universal. I love finding the rare situations where these principles don't hold, like where you do want <a href="https://buttondown.email/hillelwayne/archive/when-to-prefer-inheritance-to-composition/" target="_blank">inheritance over composition</a>. A similar near-universal principle is "don't use <a href="https://en.wikipedia.org/wiki/Bubble_sort" target="_blank">bubblesort</a>". Some would even say it's a universal rule, with Donald Knuth writing "bubble sort seems to have nothing to recommend it, except a catchy name and the fact that it leads to some interesting theoretical problems".<sup id="fnref:cite"><a href="#fn:cite">1</a></sup> But Knuth's <a href="https://en.wikipedia.org/wiki/Knuth_reward_check" target="_blank">been wrong before</a>, so let's see if this universal rule is only <em>near</em>-universal.</p>
<p>Theoretically, bubblesort is faster than quick or mergesort for small arrays. This makes it useful as part of a larger sorting strategy: most of the fast-in-principle sorting algorithms work by recursively sorting subpartitions of an array, ie if you apply quicksort to 2^20 random integers, at some point you're sorting 2^17 8-integer subpartitions. Switching over to bubblesort for those subpartitions would be a nice optimization. </p>
<p>Many production sorting algorithms do use a hybrid approach, but they overwhelmingly use <a href="https://en.wikipedia.org/wiki/Insertion_sort" target="_blank">insertion sort</a> instead. Insertion sort is very fast for small arrays and it's also <a href="https://nicknash.me/2012/10/12/knuths-wisdom/" target="_blank">better at using the hardware</a>. On some very particular hardwares bubblesort stills ends up better, like in this <a href="http://www.sci.utah.edu/~wald/Publications/2019/rtgems/ParticleSplatting.pdf" target="_blank">NVIDIA study</a>, but you probably don't have that hardware.</p>
<p>So that's one use-case, albeit one still dominated by a different algorithm. It's interesting that NVIDIA used it here because gamedev has a situation that's uniquely useful to bubblesort, based on two of its properties:</p>
<ol>
<li>While the algorithm is very slow overall, each individual step is very fast and easily suspendable.</li>
<li>Each swap leaves the array more ordered than it was before. Other sorts can move values <em>away</em> from their final positions in intermediate stages.</li>
</ol>
<p>This makes it really good when you want to do a fixed amount of sorting work per frame. Say you have a bunch of objects on a screen, where some objects can occlude others. You want to render the objects closest to the camera <em>first</em> because then you can determine which objects it hides, and then save time rendering those objects. There's no correctness cost for rendering objects out of order, just a potential performance cost. So while your array doesn't <em>need</em> to be ordered, the more ordered it is the happier you are. But you also can't spend too much time running a sorting algorithm, because you have a pretty strict realtime constraint. Bubble sort <a href="https://discussions.unity.com/t/depth-sorting-of-billboard-particles-how-can-i-do-it/5053" target="_blank">works pretty well here</a>. You can run it a little bit of a time at each frame and get a better ordering than when you started.</p>
<p>That reminds me of one last use-case I've heard, apocryphally. Let's say you have a random collection of randomly-colored particles, and you want to animate them sorting into a rainbow spectrum. If you make each frame of the animation one pass of bubblesort, the particles will all move smoothly into the right positions. I couldn't find any examples in the wild, so with the help of GPT4 I hammered out a crappy visualization. Code is <a href="https://gist.github.com/hwayne/85488f755066d8aa57cd147875e97b72" target="_blank">here</a>, put it <a href="https://editor.p5js.org/" target="_blank">here</a>.</p>
<p>(After doing that I suspect this isn't actually done in practice, in favor of running a better sort to calculate each particles final displacement and then animating each particles moving directly, instead of waiting to move for each bubblesort pass. I haven't mocked out an example but I think that'd look a lot smoother.)</p>
<p>So there you go, three niche use cases for bubblesort. You'll probably never need it.</p>
<hr>
<h3>New Quanta Article!</h3>
<p>Okay so I didn't actually write this one, but I played a role in it happening! A while back a friend visited, and we were chatting about his job at quanta. At the time he was working on this <a href="https://www.quantamagazine.org/complexity-theorys-50-year-journey-to-the-limits-of-knowledge-20230817/" target="_blank">mammoth article on metacomplexity theory</a>, so naturally the topic of <a href="https://buttondown.email/hillelwayne/archive/problems-harder-than-np-complete/" target="_blank">problems harder than NP-complete</a> came up and I recommend he check out Petri net reachability. So he did, and then he wrote <a href="https://www.quantamagazine.org/an-easy-sounding-problem-yields-numbers-too-big-for-our-universe-20231204/" target="_blank">An Easy-Sounding Problem Yields Numbers Too Big for Our Universe</a>. Gosh this is so exciting! </p>

        
    
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple Services Experiencing Outage (128 pts)]]></title>
            <link>https://www.apple.com/support/systemstatus/</link>
            <guid>46223577</guid>
            <pubDate>Wed, 10 Dec 2025 20:47:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.apple.com/support/systemstatus/">https://www.apple.com/support/systemstatus/</a>, See on <a href="https://news.ycombinator.com/item?id=46223577">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="ac-globalfooter" lang="en-US" data-analytics-region="global footer" role="contentinfo" aria-labelledby="ac-gf-label">
		        <h2 id="ac-gf-label">Apple Footer</h2>
		        <nav aria-label="Breadcrumbs" role="navigation">
		            <a href="https://www.apple.com/">
		                
		                <span>Apple</span>
		                <span></span>
		                <span></span>
		            </a>
		            <div>
		                <ol vocab="http://schema.org/" typeof="BreadcrumbList">
		                    <li property="itemListElement" typeof="ListItem">
		                        <span property="name">System Status</span>
		                        <meta property="position" content="1">
		                    </li>
		                </ol>
		            </div>
		        </nav>
		        <section>
					<div x-ms-format-detection="none"><p>
						More ways to shop: <a href="https://www.apple.com/retail/" onclick="sendAnalytics(this), trackFooterLinkClick(this)" data-analytics-title="find an apple store">Find an Apple Store</a> or <a href="https://locate.apple.com/" onclick="sendAnalytics(this), trackFooterLinkClick(this)" data-analytics-title="other retailers or resellers" data-analytics-exit-link="">other retailer</a> near you. <span>Or call 1-800-MY-APPLE.</span>
					</p></div>
					<div>
						<p><a href="https://www.apple.com/choose-country-region/" title="Choose your country or region" aria-label="United States. Choose your country or region" onclick="sendAnalyticsInnerHtml(this), trackCountrySelectorLinkClick(this)" data-analytics-title="choose your country">United States</a>
					</p></div>
					<div>
						<p>Copyright ©
							<span id="footer_msg_year"></span> Apple Inc. All rights reserved.</p>
						<div>
							<p><a href="https://www.apple.com/legal/privacy/" onclick="sendAnalytics(this), trackFooterSuperLinkClick(this)" data-analytics-title="privacy policy">Privacy Policy</a>
							<a href="https://www.apple.com/legal/internet-services/terms/site.html" onclick="sendAnalytics(this), trackFooterSuperLinkClick(this)" data-analytics-title="terms of use">Terms of Use</a>
							<a href="https://www.apple.com/us/shop/goto/help/sales_refunds" onclick="sendAnalytics(this), trackFooterSuperLinkClick(this)" data-analytics-title="sales and refunds">Sales and Refunds</a>
							<a href="https://www.apple.com/legal/" onclick="sendAnalytics(this), trackFooterSuperLinkClick(this)" data-analytics-title="legal">Legal</a>
							<a href="https://www.apple.com/sitemap/" onclick="sendAnalytics(this), trackFooterSuperLinkClick(this)" data-analytics-title="site map">Site Map</a>
						</p></div>
					</div>
				</section>				
		    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[EFF Launches Age Verification Hub as Resource Against Misguided Laws (102 pts)]]></title>
            <link>https://www.eff.org/press/releases/eff-launches-age-verification-hub-resource-against-misguided-laws</link>
            <guid>46223389</guid>
            <pubDate>Wed, 10 Dec 2025 20:35:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.eff.org/press/releases/eff-launches-age-verification-hub-resource-against-misguided-laws">https://www.eff.org/press/releases/eff-launches-age-verification-hub-resource-against-misguided-laws</a>, See on <a href="https://news.ycombinator.com/item?id=46223389">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span data-contrast="auto">SAN FRANCISCO—With ill-advised and dangerous age verification laws proliferating across the United States and around the world, creating surveillance and censorship regimes that will be used to harm both youth and adults, the Electronic Frontier Foundation has launched </span><a href="https://www.eff.org/age" target="_blank" rel="noopener noreferrer"><span data-contrast="none">a new resource hub</span></a><span data-contrast="auto"> that will sort through the mess and help people fight back.</span><span data-ccp-props="{}">&nbsp;</span></p>
<p><span data-contrast="auto">To mark the hub's launch, EFF will host a Reddit AMA (“Ask Me Anything”) next week and a free&nbsp;</span><a href="https://www.eff.org/livestream-age" target="_blank" rel="noopener noreferrer"><span data-contrast="none">livestreamed panel discussion</span></a><span data-contrast="auto"> on January 15 highlighting the dangers of these misguided laws.</span><span data-ccp-props="{}">&nbsp;</span></p>
<p><span data-contrast="auto">“These restrictive mandates strike at the foundation of the free and open internet,” said EFF Activist Molly Buckley. “While they are wrapped in the legitimate concern about children's safety, they operate as tools of censorship, used to block people young and old from viewing or sharing information that the government deems ‘harmful’ or ‘offensive.’ They also create surveillance systems that critically undermine online privacy, and chill access to vital online communities and resources. Our new resource hub is a one-stop shop for information that people can use to fight back and redirect lawmakers to things that will actually help young people, like a comprehensive privacy law.”</span><span data-ccp-props="{}">&nbsp;</span></p>
<p><span data-contrast="auto">Half of U.S. states have enacted some sort of online age verification law. At the federal level, a House Energy and Commerce subcommittee last week held a hearing on “Legislative Solutions to Protect Children and Teens Online.” While many of the 19 bills on that hearing’s agenda involve age verification, none would truly protect children and teens. Instead, they threaten to make it harder to access content </span><a href="https://www.eff.org/deeplinks/2024/03/thousands-young-people-told-us-why-kids-online-safety-act-will-be-harmful-minors" target="_blank" rel="noopener noreferrer"><span data-contrast="none">that can be crucial, even lifesaving, for some kids</span></a><span data-contrast="auto">.</span><span data-ccp-props="{}">&nbsp;</span></p>
<p><span data-contrast="auto">It’s not just in the U.S.&nbsp; Effective this week, </span><a href="https://www.esafety.gov.au/about-us/industry-regulation/social-media-age-restrictions" target="_blank" rel="noopener noreferrer"><span data-contrast="none">a new Australian law</span></a><span data-contrast="auto"> requires social media platforms to take reasonable steps to prevent Australians under the age of 16 from creating or keeping an account.</span><span data-ccp-props="{}">&nbsp;</span></p>
<p><span data-contrast="auto">We all want young people to be safe online. However, age verification is not the panacea that regulators and corporations claim it to be; in fact, it could undermine the safety of many.</span><span data-ccp-props="{}">&nbsp;</span></p>
<p><span data-contrast="auto">Age verification laws generally require online services to check, estimate, or verify all users’ ages—often through invasive tools like government ID checks, biometric scans, or other dubious “age estimation” methods—before granting them access to certain online content or services. These methods are often inaccurate and always privacy-invasive, demanding that users hand over sensitive and immutable personal information that links their offline identity to their online activity. Once that valuable data is collected, it can easily be leaked, hacked, or misused.&nbsp;</span><span data-ccp-props="{}">&nbsp;</span></p>
<p><span data-contrast="auto">To truly protect everyone online, including children, EFF advocates for </span><a href="https://www.eff.org/privacyfirst" target="_blank" rel="noopener noreferrer"><span data-contrast="none">a comprehensive data privacy law.</span></a><span data-ccp-props="240}">&nbsp;</span></p>
<p><span data-contrast="auto">EFF will host a Reddit AMA on </span><a href="https://www.reddit.com/r/privacy/" target="_blank" rel="noopener noreferrer"><span data-contrast="none">r/privacy</span></a><span data-contrast="auto"> from Monday, Dec. 15 at 12 p.m. PT through Wednesday, Dec. 17 at 5 p.m. PT, with EFF attorneys, technologists, and activists answering questions about age verification on all three days.</span><span data-ccp-props="240}">&nbsp;</span></p>
<p><span data-contrast="auto">EFF will host a free </span><a href="https://www.eff.org/livestream-age" target="_blank" rel="noopener noreferrer"><span data-contrast="none">livestream panel discussion</span></a><span data-contrast="auto"> about age verification at 12 p.m. PDT on Thursday, Jan. 15. Panelists will include Cynthia Conti-Cook, Director of Research and Policy at the </span><a href="https://thecrcr.org/" target="_blank" rel="noopener noreferrer"><span data-contrast="none">Collaborative Research Center for Resilience</span></a><span data-contrast="auto">; </span><span data-contrast="auto">a representative </span><span data-contrast="auto">of </span><a href="https://genzforchange.org/" target="_blank" rel="noopener noreferrer"><span data-contrast="none">Gen Z for Change</span></a><span data-contrast="auto">; EFF Director of Engineering </span><a href="https://www.eff.org/about/staff/alexis-hancock" target="_blank" rel="noopener noreferrer"><span data-contrast="none">Alexis Hancock</span></a><span data-contrast="auto">; and EFF Associate Director of State Affairs </span><a href="https://www.eff.org/about/staff/rindala-alajaji" target="_blank" rel="noopener noreferrer"><span data-contrast="none">Rindala Alajaji</span></a><span data-contrast="auto">. RSVP at </span><a href="https://www.eff.org/livestream-age" target="_blank" rel="noopener noreferrer"><span data-contrast="none">https://www.eff.org/livestream-age</span></a><span data-contrast="auto">.</span><span data-ccp-props="240}">&nbsp;</span></p>
<p><b><span data-contrast="auto">For the age verification resource hub:</span></b> <a href="https://www.eff.org/age"><span data-contrast="none">https://www.eff.org/age</span></a><span data-ccp-props="240}">&nbsp;</span></p>
<p><b><span data-contrast="auto">For the Reddit AMA:</span></b> <a href="https://www.reddit.com/r/privacy/"><span data-contrast="none">https://www.reddit.com/r/privacy/</span></a> <span data-ccp-props="240}">&nbsp;</span></p>
<p><b><span data-contrast="auto">For the Jan. 15 livestream:</span></b> <a href="https://www.eff.org/livestream-age"><span data-contrast="none">https://www.eff.org/livestream-age</span></a> <span data-ccp-props="{}">&nbsp;</span></p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Getting a Gemini API key is an exercise in frustration (748 pts)]]></title>
            <link>https://ankursethi.com/blog/gemini-api-key-frustration/</link>
            <guid>46223311</guid>
            <pubDate>Wed, 10 Dec 2025 20:29:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ankursethi.com/blog/gemini-api-key-frustration/">https://ankursethi.com/blog/gemini-api-key-frustration/</a>, See on <a href="https://news.ycombinator.com/item?id=46223311">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>  <article>   <section>  <p>Last week, I started working on a new side-project. It’s a standard React app partly made up of run-of-the-mill CRUD views—a perfect fit for LLM-assisted programming. I reasoned that if I could get an LLM to quickly write the boring code for me, I’d have more time to focus on the interesting problems I wanted to solve.</p>
<p>I’ve pretty much settled on Claude Code as my coding assistant of choice, but I’d been hearing great things about Google’s Gemini 3 Pro. Despite my aversion to Google products, I decided to try it out on my new codebase.</p>
<p>I already had <a href="https://github.com/google-gemini/gemini-cli">Gemini CLI</a> installed, but that only gave me access to Gemini 2.5 with rate limits. I wanted to try out Gemini 3 Pro, and I wanted to avoid being rate limited. I had some spare cash to burn on this experiment, so I went looking for ways to pay for a Gemini Pro plan, if such a thing existed.</p>
<p>Thus began my grand adventure in trying to give Google my money.</p>
<h2 id="what-is-a-gemini-really">What is a Gemini, really?</h2>
<p>The name “Gemini” is so overloaded that it barely means anything. Based on the context, Gemini could refer to:</p>
<ul>
<li>The chatbot available at <a href="https://gemini.google.com/">gemini.google.com</a>.</li>
<li>The mobile app that lets you use the same Gemini chatbot on your <a href="https://apps.apple.com/us/app/google-gemini/id6477489729">iPhone</a> or <a href="https://play.google.com/store/apps/details?id=com.google.android.apps.bard&amp;hl=en_US">Android</a>.</li>
<li>The <a href="https://gemini.google/us/assistant/">voice assistant</a> on Android phones.</li>
<li>The AI features <a href="https://workspace.google.com/resources/ai/">built into Google Workspace</a>, Firebase, Colab, BigQuery, and other Google products.</li>
<li>Gemini CLI, an agentic coding tool for your terminal that works the same way as Claude Code or OpenAI Codex.</li>
<li>The <a href="https://codeassist.google/">Gemini Code Assist</a> suite of products, which includes extensions for various IDEs, a GitHub app, and Gemini CLI.</li>
<li>The <a href="https://en.wikipedia.org/wiki/Gemini_(language_model)">underlying LLM</a> powering all these products.</li>
<li>Probably three more products by the time I finish writing this blog post.</li>
</ul>
<p>To make things even more confusing, Google has at least three different products just for agentic coding: Gemini Code Assist (Gemini CLI is a part of this suite of products), <a href="https://jules.google/">Jules</a>, and <a href="https://antigravity.google/">Antigravity</a>.</p>
<p>And then there’s a bunch of other GenAI stuff that is powered by Gemini but doesn’t have the word Gemini in the name: <a href="https://cloud.google.com/vertex-ai?hl=en">Vertex AI Platform</a>, <a href="https://aistudio.google.com/api-keys">Google AI Studio</a>, <a href="https://notebooklm.google/">NotebookLM</a>, and who knows what else.</p>
<p>I just wanted to plug my credit card information into a form and get access to a coding assistant. Instead, I was dunked into an alphabet soup of products that all seemed to do similar things and, crucially, didn’t have any giant “Buy Now!” buttons for me to click.</p>
<p>In contrast, both Anthropic and OpenAI have two primary ways you can access their products: via their consumer offerings at <a href="https://claude.ai/">claude.ai</a> and <a href="https://chatgpt.com/">chatgpt.com</a> respectively, or via API credits that you can buy through their respective <a href="https://platform.claude.com/">developer</a> <a href="https://platform.openai.com/">consoles</a>. In each case, there is a form field where you can plug in your credit card details, and a big, friendly “Buy Now!” button to click.</p>
<p>After half an hour of searching the web, I did the obvious thing and asked the free version of Gemini (the chatbot, not one of those other Geminis) what to do:</p>
<blockquote>
<p>How do I pay for the pro version of Gemini so i can use it in the terminal for writing code? I specifically want to use the Gemini 3 Pro model.</p>
</blockquote>
<p>It thought for a suspiciously long time and told me that Gemini 3 Pro required a developer API key to use. Since the new model is still in preview, it’s not yet available on any of the consumer plans. When I asked follow up questions about pricing, it told me that “Something went wrong”. Which translates to: we broke something, but we won’t tell you how to fix it.</p>
<p>So I asked Claude for help. Between the two LLMs, I was able to figure out how to create an API key for the Gemini I wanted.</p>
<h2 id="creating-an-api-key-is-easy">Creating an API key is easy</h2>
<p>Google AI Studio is supposed to be the all-in-one dashboard for Google’s generative AI models. This is where you can experiment with model parameters, manage API keys, view logs, and manage billing for your projects.</p>
<p>I logged into Google AI Studio and <a href="https://aistudio.google.com/api-keys">created a new API key</a>. This part was pretty straightforward: I followed the on-screen instructions and had a fresh new key housed under a project in a few seconds. I then verified that my key was working with Gemini CLI.</p>
<p>It worked! Now all that was left to do was to purchase some API credits. Back in Google AI Studio, I saw a link titled “Set up billing” next to my key. It looked promising, so I clicked it.</p>
<p>That’s where the fun <em>really</em> began.</p>
<h2 id="google-doesnt-want-my-money">Google doesn’t want my money</h2>
<p>The “Set up billing” link kicked me out of Google AI Studio and into Google Cloud Console, and my heart sank. Every time I’ve logged into Google Cloud Console or AWS, I’ve wasted hours upon hours reading outdated documentation, gazing in despair at graphs that make no sense, going around in circles from dashboard to dashboard, and feeling a strong desire to attain freedom from this mortal coil.</p>
<p>Turns out I can’t just put $100 into my Gemini account. Instead, I must first create a Billing Account. After I’ve done that, I must associate it with a project. <em>Then</em> I’m allowed to add a payment method to the Billing Account. And <em>then</em>, if I’m lucky, my API key will turn into a paid API key with Gemini Pro privileges.</p>
<p>So I did the thing. The whole song and dance. Including the mandatory two-factor OTP verification that every Indian credit card requires. At the end of the process, I was greeted with a popup telling me I had to verify my payment method before I’d be allowed to use it.</p>
<p>Wait. Didn’t I <em>just</em> verify my payment method? When I entered the OTP from my bank?</p>
<p>Nope, turns out Google hungers for more data. Who’d have thunk it?</p>
<p>To verify my payment method <em>for reals</em>, I had to send Google a picture of my government-issued ID and the credit card I’d just associated with my Billing Account. I had to ensure all the numbers on my credit card were redacted by manually placing black bars on top of them in an image editor, leaving only my name and the last four digits of the credit card number visible.</p>
<p>This felt unnecessarily intrusive. But by this point, I was too deep in the process to quit. I was invested. I needed my Gemini 3 Pro, and I was willing to pay any price.</p>
<p>The upload form for the government ID rejected my upload twice before it finally accepted it. It was the same exact ID every single time, just in different file formats. It wanted a PNG file. Not a JPG file, nor a PDF file, but a PNG file. Did the upload form mention that in the instructions? Of course not.</p>
<p>After jumping through all these hoops, I received an email from Google telling me that my verification will be completed in a few days.</p>
<p>A <em>few days</em>? Nothing to do but wait, I suppose.</p>
<h2 id="403-forbidden">403 Forbidden</h2>
<p>At this point, I closed all my open Cloud Console tabs and went back to work. But when I was fifteen minutes into writing some code by hand like a Neanderthal, I received a second email from Google telling me that my verification was complete.</p>
<p>So for the tenth time that day, I navigated to AI Studio. For the tenth time I clicked “Set up billing” on the page listing my API keys. For the tenth time I was told that my project wasn’t associated with a billing account. For the tenth time I associated the project with my new billing account. And finally, after doing all of this, the “Quota tier” column on the page listing my API keys said “Tier 1” instead of “Set up billing”.</p>
<p>Wait, Tier 1? Did that mean there were other tiers? What were tiers, anyway? Was I already on the best tier? Or maybe I was on the worst one? Not important. The important part was that I had my API key and I’d managed to convince Google to charge me for it.</p>
<p>I went back to the Gemini CLI, ran the <code>/settings</code> command, and turned on the “Enable experimental features” option. I ran the <code>/models</code> command, which told me that Gemini 3 Pro was now available.</p>
<p>Success? Not yet.</p>
<p>When I tried sending a message to the LLM, it failed with this 403 error:</p>
<pre tabindex="0" data-language="json"><code><span><span>{</span></span>
<span><span>  "error"</span><span>: {</span></span>
<span><span>    "message"</span><span>: </span><span>"{</span><span>\n</span><span>  \"</span><span>error</span><span>\"</span><span>: {</span><span>\n</span><span>    \"</span><span>code</span><span>\"</span><span>: 403,</span><span>\n</span><span>    \"</span><span>message</span><span>\"</span><span>: </span><span>\"</span><span>The caller does not have permission</span><span>\"</span><span>,</span><span>\n</span><span>    \"</span><span>status</span><span>\"</span><span>:</span><span>\"</span><span>PERMISSION_DENIED</span><span>\"\n</span><span>  }</span><span>\n</span><span>}</span><span>\n</span><span>"</span><span>,</span></span>
<span><span>    "code"</span><span>: </span><span>403</span><span>,</span></span>
<span><span>    "status"</span><span>: </span><span>"Forbidden"</span></span>
<span><span>  }</span></span>
<span><span>}</span></span></code></pre>
<p>Is that JSON inside a string inside JSON? Yes. Yes it is.</p>
<p>To figure out if my key was even working, I tried calling the Gemini API from JavaScript, reproducing the basic example from <a href="https://ai.google.dev/gemini-api/docs#javascript">Google’s own documentation</a>.</p>
<p>No dice. I ran into the exact same error.</p>
<p>I then tried talking to Gemini 3 Pro using the <a href="https://aistudio.google.com/prompts/new_chat">Playground</a> inside Google AI Studio. It showed me a toast message saying <code>Failed to generate content. Please try again.</code> The chat transcript said <code>An internal error has occurred.</code></p>
<p>At this point I gave up and walked away from my computer. It was already 8pm. I’d been trying to get things to work since 5pm. I needed to eat dinner, play <em>Clair Obscur</em>, and go to bed. I had no more time to waste and no more fucks to give.</p>
<h2 id="your-account-is-in-good-standing-at-this-time">Your account is in good standing at this time</h2>
<p>Just as I was getting into bed, I received an email from Google with this subject line:</p>
<blockquote>
<p>Your Google Cloud and APIs billing account XXXXXX-XXXXXX-XXXXXX is in good standing at this time.</p>
</blockquote>
<p>With the message inside saying:</p>
<blockquote>
<p>Based on the information you provided and further analysis by Google, we have reinstated your billing account XXXXXX-XXXXXX-XXXXXX. Your account is in good standing, and you should now have full access to your account and related Project(s) and Service(s).</p>
</blockquote>
<p>I have no idea what any of this means, but Gemini 3 Pro started working correctly after I received this email. It worked in the Playground, directly by calling the API from JavaScript, and with Gemini CLI.</p>
<p>Problem solved, I guess. Until Google mysteriously decides that my account is no longer in good standing.</p>
<h2 id="this-was-a-waste-of-time">This was a waste of time</h2>
<p>This was such a frustrating experience that I still haven’t tried using Gemini with my new codebase, nearly a week after I made all those sacrifices to the Gods of Billing Account.</p>
<p>I understand why the process for getting a Gemini API key is so convoluted. It’s designed for large organizations, not an individual developers trying to get work done; it serves the bureaucracy, not the people doing the work; it’s designed for maximum compliance with government regulations, not for efficiency or productivity.</p>
<p>Google doesn’t want my money unless I’m an organization that employs ten thousand people.</p>
<p>In contrast to Google, Anthropic and OpenAI are much smaller and much more nimble. They’re able to make the process of setting up a developer account quick and easy for those of us who just want to get things done. Unlike Google, they haven’t yet become complacent. They need to compete for developer mindshare if they are to survive a decade into the future. Maybe they’ll add the same level of bureaucracy to their processes as they become larger, but for now they’re fairly easy to deal with.</p>
<p>I’m still going to try using Gemini 3 Pro with Gemini CLI as my coding assistant, but I’ll probably cap the experiment to a month. Unless Gemini 3 Pro is a massive improvement over its competitors, I’ll stick to using tools built by organizations that want me as a customer.</p>  </section>   </article>    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I got an Nvidia GH200 server for €7.5k on Reddit and converted it to a desktop (336 pts)]]></title>
            <link>https://dnhkng.github.io/posts/hopper/</link>
            <guid>46222237</guid>
            <pubDate>Wed, 10 Dec 2025 19:19:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dnhkng.github.io/posts/hopper/">https://dnhkng.github.io/posts/hopper/</a>, See on <a href="https://news.ycombinator.com/item?id=46222237">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a href="https://dnhkng.github.io/assets/img/main.jpg"><img src="https://dnhkng.github.io/assets/img/main.jpg" alt="Grace Hopper Desktop" loading="lazy"></a></p><h2 id="introduction"><span>Introduction</span><a href="#introduction"><i></i></a></h2><p>Running large language models locally has always been a game of compromise. You either spend \$10,000+ on consumer GPUs that can barely handle 70 B parameter models, or you dream about enterprise hardware you’ll never afford. The Grace-Hopper platform—Nvidia’s unified CPU-GPU superchip architecture—represents the kind of dream-rig AI infrastructure LocalLlama drools over, with systems typically costing well over \$100,000 and exclusively available to data centers and research institutions.</p><p>So when I stumbled across a Grace-Hopper system being sold for 10K euro on Reddit, my first thought was “obviously fake.” My second thought was “I wonder if he’ll take 7.5K euro?”.</p><p>This is the story of how I bought enterprise-grade AI hardware designed for liquid-cooled server racks, converted it to air cooling, survived multiple near-disasters (including GPUs reporting temperatures of 16 million degrees), and ended up with a desktop that can run 235B parameter models at home. It’s a tale of questionable decisions, creative problem-solving, and what happens when you try to turn datacenter equipment into a daily driver.</p><p>If you’ve ever wondered what it takes to run truly large models locally, or if you’re just here to watch someone disassemble $80,000 worth of hardware with nothing but hope and isopropanol, you’re in the right place.</p><h2 id="the-deal"><span>The Deal</span><a href="#the-deal"><i></i></a></h2><p>Early this year, while browsing r/LocalLLaMA/new, I came across a <a href="https://www.reddit.com/r/LocalLLaMA/comments/1m65iga/frankenserver_for_sale_at_a_steep_discount_2x96gb/"><em>ridiculously good deal</em></a>. How good? These were the specs for the server offered for 10K euro, and a serious upgrade to my 4x RTX 4090 rig:</p><h3 id="specs"><span>Specs:</span><a href="#specs"><i></i></a></h3><ul><li>2x Nvidia Grace-Hopper Superchip</li><li>2x 72-core Nvidia Grace CPU</li><li>2x Nvidia Hopper H100 Tensor Core GPU</li><li>2x 480GB of LPDDR5X memory with error-correction code (ECC)</li><li>2x 96GB of HBM3 memory</li><li>1152GB of total fast-access memory</li><li>NVLink-C2C: 900 GB/s of bandwidth</li><li>Programmable from 1000W to 2000W TDP (CPU + GPU + memory)</li><li>1x High-efficiency 3000W PSU 230V to 48V</li><li>2x PCIe Gen4 M.2 22110/2280 slots on board</li><li>4x FHFL PCIe Gen5 x16</li></ul><blockquote><p><strong>UPDATE</strong>:Since I bought this, DDR5 RAM prices have become insane. 960GB of fast DDR5 now costs more than what I paid for the whole Grace-Hopper system 🤯</p></blockquote><p><em>Obviously fake</em> I thought, because</p><ol><li>H100s cost about <strong>30-40,000 euro each</strong>, and this system has <em>two of them</em></li><li>Grace-Hopper NVL2 systems are basically not for sale for consumers anyway!</li></ol><p>The Reddit thread explained the reason the system was being sold cheap:</p><blockquote><p>The main reason why is that it is a Frankensystem converted from liquid-cooled to aircooled. Also it is not very pretty and not rackable, because it has a 48V power supply attached. It is originally directly from Nvidia.</p></blockquote><p>I immediately offered to buy it, because why not? If it was a scam, I could always back out, but I wanted to be first in line!</p><p>It turns out I live near the seller, and he runs an online shop that <a href="https://gptshop.ai/">sells modified Nvidia server equipment as desktops</a>. It still seemed pretty risky, so I did some research and <a href="https://www.youtube.com/watch?v=YBIpJhN5YsE">found a video review</a> of one of his Desktops on Youtube. With the deal now seeming at least plausible, and the seller only a two-hour drive away and agreeing to take cash, it was time to take a Bavarian road trip.</p><p>I arrived at a farmhouse in a small forest, and met Bernhard the proprietor of <a href="https://gptshop.ai/">GPTshop.ai</a>. He showed me a nice workshop (plasma cutters, an electronics lab, etc.) from which he fabricates custom cases for the high-end H100 desktops he builds. These desktops seem pretty damn nice, so it’s unfortunate that his webshop gives off shady vibes; the business registration in the Cayman Islands definitely doesn’t help. What I can say though is that this item was <em>heavily</em> discounted, and not what he usually sells.</p><blockquote><p><strong>Disclaimer</strong>: I have zero affiliation with GPTshop.ai beyond handing them a stack of cash and receiving a dust-covered server in return. If this were a sponsored post, they probably wouldn’t let me mention the 16 million degree GPU temperatures or the part where I had to free-solder components while praying to the electronics gods.</p></blockquote><h2 id="disassembling-the-grace-hopper-server"><span>Disassembling the Grace Hopper server</span><a href="#disassembling-the-grace-hopper-server"><i></i></a></h2><p><a href="https://dnhkng.github.io/assets/img/open.jpeg"><img src="https://dnhkng.github.io/assets/img/open.jpeg" alt="Arrival" loading="lazy"></a></p><p>The server itself was not in great condition. These things run <em>extremely</em> loud and high-throughput fans, and these had sucked in a lot of dust, coating the mainboard so heavily I couldn’t tell the color of the PCB. However, it booted up and ran OK, so I handed over a wad of cash, strapped it into the backseat of my car with the seatbelt (it weighed ~20 kg), and drove it home.</p><p>Did I mention it’s loud? Firing up the system is physically painful. There are 8x Sunon dual-fan modules, and each is as loud as a powerful vacuum cleaner, but with a much higher and more annoying pitch. With all 8 running at full power, hearing protection is necessary - I could hear the system running in my basement with the windows closed from 50 meters away! My wife immediately (and quite fairly), banned its use at home. We both work home-office and it was simply too loud for online meetings. But I had other plans anyway…</p><p>First things first, I of course quickly decided and then proceeded to strip down the server, after first photo-documenting the various connectors between the various PCBs, modules and mainboard.</p><h2 id="cleaning-the-server"><span>Cleaning the Server</span><a href="#cleaning-the-server"><i></i></a></h2><p><a href="https://dnhkng.github.io/assets/img/cleaning.jpg"><img src="https://dnhkng.github.io/assets/img/cleaning.jpg" alt="Cleaning" loading="lazy"></a></p><p>The majority of the dust was vacuumed off during disassembly, but there was clearly a lot more under the Grace-Hopper modules. After removing those as well, I decided to go with a full washdown of the mainboard.</p><p>I purchased a few litres of Isopropanol, and with a soft brush I went over the whole board a few times to get the remaining fine dust from inside connectors and between SMD-component pins.</p><p>I suspected there might also be dust <em>inside</em> the Grace-Hopper modules, but actually, I really just wanted to pop them open to poke around.</p><p>The mainboard went on my heated floor to dry for a week, while I moved on to replacing the cooling system.</p><h2 id="a-new-water-cooling-system"><span>A new Water Cooling system</span><a href="#a-new-water-cooling-system"><i></i></a></h2><p><a href="https://dnhkng.github.io/assets/img/adapter_plate.jpg"><img src="https://dnhkng.github.io/assets/img/adapter_plate.jpg" alt="Adapter Plate" loading="lazy"></a></p><p>I had looked into building a custom water-cooling block, but I was worried about leaks, when I found cheap <a href="https://www.arctic.de/en/Liquid-Freezer-III-420/ACFRE00137A">all-in-one water cooling systems</a> for ~40 euro each on sale. Two per GH200 module would be sufficient, so I carefully measured the dimensions of the GPU die and CPU, as well as screw locations, and threw those into Fusion 360 to model up an adapter block.</p><p>I have a Bambu X1, which came in very handy for prototyping the adapter blocks. The tolerances have to be very tight, so I printed several cut-away versions to make sure there was solid contact to the bare GPU die, and a safe margin from contact to fragile parts.</p><p>The parts were then sent for CNC milling, and were delivered as the mainboard was finished drying. After using yet more isopropanol to clean off the machining oil, they were mounted without much fuss.</p><h2 id="assembling-the-desktop"><span>Assembling the Desktop</span><a href="#assembling-the-desktop"><i></i></a></h2><p><a href="https://dnhkng.github.io/assets/img/assembly.jpg"><img src="https://dnhkng.github.io/assets/img/assembly.jpg" alt="Assembly" loading="lazy"></a></p><p>My go-to material for this kind of project is ProfilAlu from eBay. It’s cheap, stiff, and delivered pre-cut for assembly. I put together a design in Fusion 360, and had the parts in a few days. The various mounts however were much more work. I needed to design a few dozen custom mounts for the various PCBs and air-filter fixings; this used up a few kilos of filament to get things just right.</p><h2 id="disasters"><span>Disaster(s)</span><a href="#disasters"><i></i></a></h2><h3 id="critical-fan-errors"><span>Critical Fan Errors</span><a href="#critical-fan-errors"><i></i></a></h3><p>The system didn’t start to boot anymore. Checking the logs, I saw 16 critical errors, one for each fan in the 8 pairs:</p><blockquote><div><table><tbody><tr><td>4</td><td>08/06/25</td><td>19:24:08 CEST</td><td>Fan FAN_5_F</td><td>Lower Critical going low</td><td>Asserted</td><td>Reading 0 &lt; Threshold 2156 RPM</td></tr><tr><td>5</td><td>08/06/25</td><td>19:24:08 CEST</td><td>Fan FAN_6_R</td><td>Lower Critical going low</td><td>Asserted</td><td>Reading 0 &lt; Threshold 2156 RPM</td></tr><tr><td>6</td><td>08/06/25</td><td>19:24:08 CEST</td><td>Fan FAN_8_F</td><td>Lower Critical going low</td><td>Asserted</td><td>Reading 0 &lt; Threshold 2156 RPM</td></tr><tr><td>7</td><td>08/06/25</td><td>19:24:08 CEST</td><td>Fan FAN_5_R</td><td>Lower Critical going low</td><td>Asserted</td><td>Reading 0 &lt; Threshold 2156 RPM</td></tr><tr><td>8</td><td>08/06/25</td><td>19:24:08 CEST</td><td>Fan FAN_7_F</td><td>Lower Critical going low</td><td>Asserted</td><td>Reading 0 &lt; Threshold 2156 RPM</td></tr><tr><td>9</td><td>08/06/25</td><td>19:24:08 CEST</td><td>Fan FAN_8_R</td><td>Lower Critical going low</td><td>Asserted</td><td>Reading 0 &lt; Threshold 2156 RPM…</td></tr></tbody></table></div></blockquote><p>With the fans removed, the <a href="https://en.wikipedia.org/wiki/Intelligent_Platform_Management_Interface#Baseboard_management_controller">BMC (Baseboard Management Controller)</a> immediately panicked, and shut down the mainboard to prevent thermal damage, even with the water coolers in place. So, I disabled the fan-check subsystem.</p><div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
</pre></td><td><pre># stops the service for the current session
systemctl stop phosphor-sensor-monitor.service

# prevents the service from starting on the next boot
systemctl disable phosphor-sensor-monitor.service
</pre></td></tr></tbody></table></code></p></div><p>Who needs hardware monitoring? ¯\_(ツ)_/¯</p><h3 id="nuclear-fusion"><span>Nuclear Fusion?</span><a href="#nuclear-fusion"><i></i></a></h3><p>Great! I could start the boot process, and even reach login! But only about 1 time in 4… Not optimal. And even logged in, the server would crash within 2 minutes.</p><p>Looking into the BMC logs, I saw:</p><div><table><tbody><tr><td>Sep 23 08:20:18</td><td>oberon-bmc</td><td>shutdown_ok_mon[1478]</td><td>event: FALLING EDGE offset: 26 timestamp: [571.615238550]</td></tr><tr><td>Sep 23 08:20:18</td><td>oberon-bmc</td><td>power-status[1493]</td><td>event: FALLING EDGE offset: 18 timestamp: [571.632491062]</td></tr><tr><td>Sep 23 08:20:18</td><td><strong>oberon-bmc</strong></td><td><strong>shutdown_ok_mon[545]</strong></td><td><strong>SHDN_OK_L-I = 0</strong></td></tr><tr><td>Sep 23 08:20:18</td><td>oberon-bmc</td><td>shutdown_ok_mon[545]</td><td>Asserting SYS_RST_IN_L-O to hold host in reset</td></tr><tr><td>Sep 23 08:20:18</td><td>oberon-bmc</td><td>shutdown_ok_mon[545]</td><td>gpioset SYS_RST_IN_L-O = 0</td></tr><tr><td>Sep 23 08:20:18</td><td>oberon-bmc</td><td>power-status[697]</td><td>gpioset SYS_RST_IN_L-O = 0</td></tr><tr><td>Sep 23 08:20:18</td><td>oberon-bmc</td><td>power-status[697]</td><td>Set SYS_RST_IN_L-O=0</td></tr></tbody></table></div><p>So, a Critical Failure at 08:20:18:</p><ul><li>SHDN_OK_L-I signal goes low (falling edge detected)</li><li>This immediately triggers a shutdown sequence</li><li>System powers off within ~30 seconds of successful boot</li></ul><p>But why?!!? I had shut down the hardware monitoring.</p><p>Diving deeper into the logs:</p><div><table><tbody><tr><td>Oct 05 10:15:00</td><td>oberon-bmc</td><td>ipmid[520]</td><td>thresholdChanged: Assert</td></tr><tr><td>Oct 05 10:15:00</td><td>oberon-bmc</td><td>ipmid[520]</td><td>thresholdChanged: Assert</td></tr><tr><td>Oct 05 10:15:00</td><td>oberon-bmc</td><td>ipmid[520]</td><td>thresholdChanged: Assert</td></tr><tr><td>Oct 05 10:15:00</td><td>oberon-bmc</td><td>satellitesensor[2351]</td><td>Sensor HGX_GPU_1_TEMP_1 high threshold 92 assert: value 1.67772e+07 raw data nan</td></tr><tr><td>Oct 05 10:15:00</td><td>oberon-bmc</td><td>satellitesensor[2351]</td><td>Sensor HGX_GPU_1_TEMP_1 high threshold 89 assert: value 1.67772e+07 raw data nan</td></tr><tr><td>Oct 05 10:15:00</td><td>oberon-bmc</td><td>satellitesensor[2351]</td><td>Sensor HGX_GPU_1_TEMP_1 high threshold 87 assert: value 1.67772e+07 raw data nan</td></tr><tr><td>Oct 05 10:15:00</td><td>oberon-bmc</td><td>phosphor-fru-fault-monitor[524]</td><td>/xyz/openbmc_project/logging/entry/496 created</td></tr><tr><td>Oct 05 10:15:00</td><td>oberon-bmc</td><td>phosphor-fru-fault-monitor[524]</td><td>/xyz/openbmc_project/logging/entry/497 created</td></tr><tr><td>Oct 05 10:15:00</td><td>oberon-bmc</td><td>sensor-monitor[499]</td><td>Starting 1000ms HardShutdownAlarmHigh shutdown timer due to sensor /xyz/openbmc_project/sensors/temperature/HGX_GPU_0_TEMP_1 value 16777214</td></tr></tbody></table></div><blockquote><p><strong>Warning:</strong> Your GPU should not reach 16,777,214 Celsius during boot. Imagine what would happen under load!</p></blockquote><p>This took some time to debug, as I was quite sure the sensors could not physically handle reading temperatures over 16 million Celsius… But then I noticed something interesting about that specific number:</p><div><table><thead><tr><th>Decimal</th><th>Binary</th><th>Hex</th></tr></thead><tbody><tr><td>16,777,214</td><td>1111 1111 1111 1111 1111 1110</td><td>0xFFFFFE</td></tr></tbody></table></div><p>This is <code>2²⁴ - 2</code>, which is suspiciously close to the maximum value of a 24-bit unsigned integer. In the hardware world, this is the equivalent of a sensor throwing up its hands and screaming “I have no idea what’s happening!” When hardware can’t read a value properly—whether due to a loose connection, damaged circuit, or initialization failure—it often returns the maximum (or near-maximum) representable value. It’s like the digital version of a shrug.</p><p>The logs confirmed this theory: seeing <code>1.67772e+07</code> (16,777,214) wasn’t evidence that my GPU had achieved nuclear fusion temperatures 🔥—it was evidence that the temperature sensor had simply stopped working. And if a sensor error is intermittent, the most likely culprit is a loose connection or physical damage.</p><p>After spending way too long pursuing software solutions (because who wants to disassemble everything <em>again</em>?), I finally accepted the inevitable and broke out the screwdrivers.</p><p><a href="https://dnhkng.github.io/assets/img/fix.jpg"><img src="https://dnhkng.github.io/assets/img/fix.jpg" alt="Fix" loading="lazy"></a></p><p>I happened to have bought a new microscope earlier this year, and it turned out to be the perfect tool for diagnosing and fixing the issue. Near one of the modules, I found some damaged surface mount components. The damage must have happened after cleaning, probably during the reassembly of the modules with the copper adapters. They weigh over 2 kg, so a slight bump would have easily caused this damage. Amazingly, the tiny components were still attached to the traces, and so I could measure them easily: a 100 nF capacitor, and 4.7k resistor (both of which I had on-hand, as they are standard values for decoupling circuits). The bad news? I had huge “0805” sized parts (2mm long), these were tiny “0402” (1mm long). And one of the traces was just gone.</p><p>With some very fiddly soldering, and scratching off the solder mask on the PCB to expose more trace, I was able to ‘free solder’ the parts into a wonderful 3D sculpture which was then liberally coated in UV-curing mask resin, set, and then held in place with sticky tape. Very professional. After reassembly, the system booted smoothly.</p><h2 id="final-touches"><span>Final Touches</span><a href="#final-touches"><i></i></a></h2><p><a href="https://dnhkng.github.io/assets/img/touches.jpg"><img src="https://dnhkng.github.io/assets/img/touches.jpg" alt="Fix" loading="lazy"></a></p><p>I 3D printed a few extra parts:</p><ul><li>Mounts for the E1.S 8TB SSD I found cheap online</li><li>A full rear-panel, that mounts the 3KW 48V power supply</li><li>Cool-looking mesh to protect the water-cooling radiators and dust filters</li></ul><p>Getting the actual GPU working was also painful, so I’ll leave the details here for future adventurers:</p><div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre></td><td><pre># Data Center/HGX-Series/HGX H100/Linux aarch64/12.8 seem to work!
wget https://us.download.nvidia.com/tesla/570.195.03/NVIDIA-Linux-aarch64-570.195.03.run

# Tell the driver to completely ignore the NVLINK and it should allow the GPUs to initialise independently over PCIe !!!!   This took a week of work to find, thanks Reddit!

# create a modprobe config file:
sudo nano /etc/modprobe.d/nvidia-disable-nvlink.conf

# add the driver option
options nvidia NVreg_NvLinkDisable=1

# update the boot files:
sudo update-initramfs -u

# reboot
sudo reboot
</pre></td></tr></tbody></table></code></p></div><h2 id="benchmarks"><span>Benchmarks</span><a href="#benchmarks"><i></i></a></h2><p>That’s what you’re here for, maybe? I have only just started, but after compiling the latest Llama.cpp version using 144 cores in 90 seconds, here’s some benchmarks on larger LLMs:</p><p><a href="https://dnhkng.github.io/assets/img/benchmarks.jpg"><img src="https://dnhkng.github.io/assets/img/benchmarks.jpg" alt="Benchmarks" loading="lazy"></a></p><div><table><thead><tr><th>Model</th><th>Prompt Processing</th><th>Token Generation</th></tr></thead><tbody><tr><td>gpt-oss-120b-Q4_K_M</td><td>2974.79</td><td>195.84</td></tr><tr><td>GLM-4.5-Air-Q4_K_M</td><td>1936.65</td><td>100.71</td></tr><tr><td>Qwen3-235B-A22B-Instruct-2507-Q4_K</td><td>1022.79</td><td>65.90</td></tr></tbody></table></div><p>This is pretty unoptimized, but it’s looking promising so far! During the LLM tests I hit around 300W per GPU, far from the 900W max.</p><h2 id="cost-breakdown"><span>Cost Breakdown</span><a href="#cost-breakdown"><i></i></a></h2><p>Here’s what the entire build actually cost me, from the initial purchase to the final touches:</p><div><table><thead><tr><th>Component</th><th>Description</th><th>Cost (EUR)</th></tr></thead><tbody><tr><td>Grace-Hopper Server</td><td>2x GH200 superchips with H100 GPUs (the Frankenstein special)</td><td>€7,500</td></tr><tr><td>Storage</td><td>‘like-new’ used 8TB E1.S NVMe SSD</td><td>€250</td></tr><tr><td>Custom Water Cooling Adapters</td><td>2x CNC-milled copper mounting plates for AIO coolers</td><td>€700</td></tr><tr><td>AIO Water Coolers</td><td>4x Arctic Liquid Freezer III 420 (B-Ware)</td><td>€180</td></tr><tr><td>Structural Frame</td><td>Extruded aluminum profiles, pre-cut and delivered</td><td>€200</td></tr><tr><td>3D Printing Filament</td><td>1kg black PLA for custom mounts and brackets</td><td>€20</td></tr><tr><td>Hardware</td><td>Nuts, bolts, and mounting hardware</td><td>€50</td></tr><tr><td>Cleaning Supplies</td><td>5 liters of 99.9% isopropanol (used liberally throughout)</td><td>€20</td></tr><tr><td>Aesthetics</td><td>LED lighting strip (because RGB makes it faster)</td><td>€10</td></tr><tr><td><strong>Total</strong></td><td>&nbsp;</td><td><strong>€8,930</strong></td></tr></tbody></table></div><p>Not included: hearing protection (absolutely necessary), the microscope I already owned (but proved essential), several failed 3D prints, and the emotional cost of seeing “16,777,214°C” in system logs.</p><h2 id="conclusion"><span>Conclusion</span><a href="#conclusion"><i></i></a></h2><p>So, was it worth it? I now have a desktop that can run 235B parameter models at home for less than the cost of a single H100. It required disassembling $80,000 worth of enterprise hardware, debugging sensors that reported temperatures approaching the surface of the sun, and free-soldering components under a microscope. Your mileage may vary. Literally: I had to drive two hours to pick this thing up.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The future of Terraform CDK (125 pts)]]></title>
            <link>https://github.com/hashicorp/terraform-cdk</link>
            <guid>46222165</guid>
            <pubDate>Wed, 10 Dec 2025 19:14:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/hashicorp/terraform-cdk">https://github.com/hashicorp/terraform-cdk</a>, See on <a href="https://news.ycombinator.com/item?id=46222165">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">The Future of Terraform CDK</h2><a id="user-content-the-future-of-terraform-cdk" aria-label="Permalink: The Future of Terraform CDK" href="#the-future-of-terraform-cdk"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Sunset Notice</h2><a id="user-content-sunset-notice" aria-label="Permalink: Sunset Notice" href="#sunset-notice"></a></p>
<p dir="auto">Terraform CDK (CDKTF) will sunset and be archived on December 10, 2025. HashiCorp, an IBM Company, will no longer maintain or develop the project after that date. Unfortunately, Terraform CDK did not find product-market fit at scale. HashiCorp, an IBM Company, has chosen to focus its investments on Terraform core and its broader ecosystem.</p>
<p dir="auto">As of December 10, 2025, Terraform CDK will be archived on GitHub, and the documentation will reflect its deprecated status. The archived code will remain available on GitHub, but it will be read-only. No further updates, fixes, or improvements (including compatibility updates) will be made.</p>
<p dir="auto">You will be able to continue to use Terraform CDK at your own risk. Terraform CDK is licensed under the Mozilla Public License (MPL). HashiCorp, an IBM Company, does not apply any additional restrictions. We encourage community forks if there’s interest in continuing development independently.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Migration to HCL</h2><a id="user-content-migration-to-hcl" aria-label="Permalink: Migration to HCL" href="#migration-to-hcl"></a></p>
<p dir="auto">You can use the following command to generate Terraform-compatible .tf files directly from your Terraform CDK project:</p>
<p dir="auto"><code>cdktf synth --hcl</code></p>
<p dir="auto">This will produce readable HCL configuration files, making it easier to migrate away from Terraform CDK. After running the command, you can use standard Terraform CLI commands (<code>terraform init</code>, <code>terraform plan</code>, <code>terraform apply</code>) to continue managing your infrastructure. Please note that while this helps bootstrap your configuration, you may still need to review and adjust the generated files for clarity, organization, or best practices.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Note on AWS CDK</h3><a id="user-content-note-on-aws-cdk" aria-label="Permalink: Note on AWS CDK" href="#note-on-aws-cdk"></a></p>
<p dir="auto">If your infrastructure is defined in Terraform CDK but also tightly integrated with AWS CDK, you may find it more consistent to migrate directly to the AWS CDK ecosystem. If you are not using AWS CDK, we highly recommend migrating to standard Terraform and HCL for long-term support and ecosystem alignment.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">FAQ</h2><a id="user-content-faq" aria-label="Permalink: FAQ" href="#faq"></a></p>
<p dir="auto">Q: Is CDKTF still being developed?</p>
<p dir="auto">A: No. CDKTF will sunset and be archived on December 10, 2025. HashiCorp, an IBM Company, will no longer maintain or develop the project after that date.</p>
<p dir="auto">Q: Why is CDKTF being sunset?</p>
<p dir="auto">A: CDKTF did not find product-market fit at scale. We’ve chosen to focus our investments on Terraform core and its broader ecosystem.</p>
<p dir="auto">Q: Will CDKTF be removed from GitHub?</p>
<p dir="auto">A: CDKTF will be archived on GitHub, and documentation will reflect its deprecated status.</p>
<p dir="auto">Q: Can I still use CDKTF after it's sunset?</p>
<p dir="auto">A: Yes, the archived code will remain available on GitHub, but it will be read-only. No further updates, fixes, or improvements will be made.</p>
<p dir="auto">Q: Will CDKTF continue to support new versions of Terraform or providers?</p>
<p dir="auto">A: No. Compatibility updates will not be made after the EOL date.</p>
<p dir="auto">Q: Can I fork CDKTF and maintain it myself?</p>
<p dir="auto">A: Yes. CDKTF is open source, and we encourage community forks if there’s interest in continuing development independently.</p>
<p dir="auto">Q: Can I keep using CDKTF?</p>
<p dir="auto">A: You may continue to use it at your own risk. HashiCorp, an IBM Company, will no longer be maintaining it.</p>
<p dir="auto">Q: Is there a migration tool?</p>
<p dir="auto">A: You can use the following command to generate Terraform-compatible .tf files directly from your CDKTF project:</p>
<p dir="auto"><code>cdktf synth --hcl</code></p>
<p dir="auto">This will produce readable HCL configuration files, making it easier to migrate away from CDKTF. After running the command, you can use standard Terraform CLI commands (terraform init, terraform plan, terraform apply) to continue managing your infrastructure. Please note that while this helps bootstrap your configuration, you may still need to review and adjust the generated files for clarity, organization, or best practices.</p>
<p dir="auto">Q: What migration guidance can we provide to customers?</p>
<p dir="auto">A: For users looking to migrate away from CDKTF:</p>
<p dir="auto">If your infrastructure is defined in CDKTF but also tightly integrated with AWS CDK, you may find it more consistent to migrate directly to the AWS CDK ecosystem.</p>
<p dir="auto">If you are not using AWS CDK, we highly recommend migrating to standard Terraform and HCL for long-term support and ecosystem alignment.</p>
<hr>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/hashicorp/terraform-cdk/workflows/Release/badge.svg"><img src="https://github.com/hashicorp/terraform-cdk/workflows/Release/badge.svg" alt=""></a>
<a href="https://badge.fury.io/js/cdktf" rel="nofollow"><img src="https://camo.githubusercontent.com/10ce5cc6a21f56b89861b61ba073172f046370b8e1c4e40ce900926cfe14be10/68747470733a2f2f62616467652e667572792e696f2f6a732f63646b74662e737667" alt="npm version" data-canonical-src="https://badge.fury.io/js/cdktf.svg"></a>
<a href="https://badge.fury.io/py/cdktf" rel="nofollow"><img src="https://camo.githubusercontent.com/b8c9163a6c7596a5d407c29c4906776d19fa5f8a6cf23758ba20f000b8091abf/68747470733a2f2f62616467652e667572792e696f2f70792f63646b74662e737667" alt="PyPI version" data-canonical-src="https://badge.fury.io/py/cdktf.svg"></a>
<a href="https://badge.fury.io/nu/HashiCorp.Cdktf" rel="nofollow"><img src="https://camo.githubusercontent.com/7566fa476d9e6e443710536cb5e5635f7e297c1593868fef94b077f9b6d85e11/68747470733a2f2f62616467652e667572792e696f2f6e752f4861736869436f72702e43646b74662e737667" alt="NuGet version" data-canonical-src="https://badge.fury.io/nu/HashiCorp.Cdktf.svg"></a>
<a href="https://search.maven.org/artifact/com.hashicorp/cdktf" rel="nofollow"><img src="https://camo.githubusercontent.com/0330c15b9403354d389b692ffd865dc136d878ec8fc9b336da962625aad35b30/68747470733a2f2f696d672e736869656c64732e696f2f6d6176656e2d63656e7472616c2f762f636f6d2e6861736869636f72702f63646b74663f636f6c6f723d627269676874677265656e" alt="Maven Central" data-canonical-src="https://img.shields.io/maven-central/v/com.hashicorp/cdktf?color=brightgreen"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">CDK for Terraform</h2><a id="user-content-cdk-for-terraform" aria-label="Permalink: CDK for Terraform" href="#cdk-for-terraform"></a></p>
<p dir="auto">Cloud Development Kit for Terraform (CDKTF) allows you to use familiar
programming languages to define cloud infrastructure and provision it through
HashiCorp Terraform. This gives you access to the entire Terraform ecosystem without learning HashiCorp Configuration Language (HCL) and lets you leverage the power of your existing toolchain for testing, dependency management, etc.</p>
<p dir="auto">We currently support TypeScript, Python, Java, C#, and Go.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/hashicorp/terraform-cdk/blob/main/docs/terraform-platform.png"><img src="https://github.com/hashicorp/terraform-cdk/raw/main/docs/terraform-platform.png" alt="terraform platform"></a></p>
<p dir="auto">CDKTF includes two packages:</p>
<ul dir="auto">
<li><a href="https://github.com/hashicorp/terraform-cdk/blob/main/packages/cdktf-cli">cdktf-cli</a> - A CLI that allows users to run commands to initialize, import, and synthesize CDK for Terraform applications.</li>
<li><a href="https://github.com/hashicorp/terraform-cdk/blob/main/packages/cdktf">cdktf</a> - A library for defining Terraform resources using programming constructs.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Get Started</h2><a id="user-content-get-started" aria-label="Permalink: Get Started" href="#get-started"></a></p>
<p dir="auto">Choose a language:</p>
<ul dir="auto">
<li><a href="https://developer.hashicorp.com/terraform/tutorials/cdktf/cdktf-build?in=terraform%2Fcdktf&amp;variants=cdk-language%3Atypescript" rel="nofollow">TypeScript</a></li>
<li><a href="https://developer.hashicorp.com/terraform/tutorials/cdktf/cdktf-build?in=terraform%2Fcdktf&amp;variants=cdk-language%3Apython" rel="nofollow">Python</a></li>
<li><a href="https://developer.hashicorp.com/terraform/tutorials/cdktf/cdktf-build?in=terraform%2Fcdktf&amp;variants=cdk-language%3Ajava" rel="nofollow">Java</a></li>
<li><a href="https://developer.hashicorp.com/terraform/tutorials/cdktf/cdktf-build?in=terraform%2Fcdktf&amp;variants=cdk-language%3Acsharp" rel="nofollow">C#</a></li>
<li><a href="https://developer.hashicorp.com/terraform/tutorials/cdktf/cdktf-build?in=terraform%2Fcdktf&amp;variants=cdk-language%3Ago" rel="nofollow">Go</a></li>
</ul>
<blockquote>
<p dir="auto"><strong>Hands-on:</strong> Try the tutorials in the <a href="https://learn.hashicorp.com/collections/terraform/cdktf" rel="nofollow">CDK for Terraform</a> collection on HashiCorp Learn.</p>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto">Documentation</h2><a id="user-content-documentation" aria-label="Permalink: Documentation" href="#documentation"></a></p>
<p dir="auto">Refer to the <a href="https://developer.hashicorp.com/terraform/cdktf" rel="nofollow">CDKTF documentation</a> for more detail about how to build and manage CDKTF applications, including:</p>
<ul dir="auto">
<li>
<p dir="auto"><a href="https://developer.hashicorp.com/terraform/cdktf/concepts/cdktf-architecture" rel="nofollow">Application Architecture</a>: Learn the tools and processes that CDKTF uses to leverage the Terraform ecosystem and convert code into Terraform configuration files. It also explains the major components of a CDKTF application and how those pieces fit together.</p>
</li>
<li>
<p dir="auto"><a href="https://developer.hashicorp.com/terraform/cdktf/create-and-deploy/project-setup" rel="nofollow">Project Setup</a>: Learn how to create a new CDKTF project from a pre-built or custom template. Also learn how to convert an existing HCL project into a CDKTF application.</p>
</li>
<li>
<p dir="auto"><a href="https://developer.hashicorp.com/terraform/cdktf/test/unit-tests" rel="nofollow">Unit Tests</a>: Learn how to test your application in Typescript with jest.</p>
</li>
<li>
<p dir="auto"><a href="https://developer.hashicorp.com/terraform/cdktf/examples-and-guides/examples" rel="nofollow">Examples</a>: Reference example projects in every supported language and review explanatory videos and other resources.</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Community</h2><a id="user-content-community" aria-label="Permalink: Community" href="#community"></a></p>
<p dir="auto">The development team would love your feedback to help guide the project.</p>
<ul dir="auto">
<li>Contribute using the <a href="https://github.com/hashicorp/terraform-cdk/blob/main/CONTRIBUTING.md">CONTRIBUTING.md</a> guide.</li>
<li>Ask a question on the HashiCorp <a href="https://discuss.hashicorp.com/" rel="nofollow">Discuss</a> using the <a href="https://discuss.hashicorp.com/c/terraform-core/cdk-for-terraform/" rel="nofollow">terraform-cdk</a> category.</li>
<li>Report a <a href="https://github.com/hashicorp/terraform-cdk/issues/new?assignees=&amp;labels=bug&amp;template=bug-report.md&amp;title=">bug</a> or request a new <a href="https://github.com/hashicorp/terraform-cdk/issues/new?assignees=&amp;labels=enhancement&amp;template=feature-request.md&amp;title=">feature</a>.</li>
<li>Browse all <a href="https://github.com/hashicorp/terraform-cdk/issues">open issues</a>.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Build</h2><a id="user-content-build" aria-label="Permalink: Build" href="#build"></a></p>
<p dir="auto">For prerequisites, refer to the <a href="https://github.com/hashicorp/terraform-cdk/blob/main/CONTRIBUTING.md#prerequisites">following</a>.</p>
<p dir="auto">Clone the project repository.</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/hashicorp/terraform-cdk.git"><pre>git clone https://github.com/hashicorp/terraform-cdk.git</pre></div>
<p dir="auto">Download dependencies.</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd terraform-cdk/
yarn install"><pre><span>cd</span> terraform-cdk/
yarn install</pre></div>
<p dir="auto">Build the project and packages.</p>

</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Super Mario 64 for the PS1 (277 pts)]]></title>
            <link>https://github.com/malucard/sm64-psx</link>
            <guid>46221925</guid>
            <pubDate>Wed, 10 Dec 2025 18:58:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/malucard/sm64-psx">https://github.com/malucard/sm64-psx</a>, See on <a href="https://news.ycombinator.com/item?id=46221925">Hacker News</a></p>
<div id="readability-page-1" class="page"><p dir="auto">This repo does not include all assets necessary for compiling the game.
An original copy of the game is required to extract the assets.</p><div data-snippet-clipboard-copy-content="sm64
├── actors: object behaviors, geo layout, and display lists
├── assets: animation and demo data
│   ├── anims: animation data
│   └── demos: demo data
├── bin: C files for ordering display lists and textures
├── build: output directory
├── data: behavior scripts, misc. data
├── doxygen: documentation infrastructure
├── enhancements: example source modifications
├── include: header files
├── levels: level scripts, geo layout, and display lists
├── lib: N64 SDK code
├── sound: sequences, sound samples, and sound banks
├── src: C source code for game
│   ├── audio: audio code
│   ├── buffers: stacks, heaps, and task buffers
│   ├── engine: script processing engines and utils
│   ├── game: behaviors and rest of game source
│   ├── goddard: rewritten Mario intro screen
│   ├── goddard_og: backup of original Mario intro screen
│   ├── menu: title screen and file, act, and debug level selection menus
│   └── port: port code, audio and video renderer
├── text: dialog, level names, act names
├── textures: skybox and generic texture data
└── tools: build tools"><pre><code>sm64
├── actors: object behaviors, geo layout, and display lists
├── assets: animation and demo data
│   ├── anims: animation data
│   └── demos: demo data
├── bin: C files for ordering display lists and textures
├── build: output directory
├── data: behavior scripts, misc. data
├── doxygen: documentation infrastructure
├── enhancements: example source modifications
├── include: header files
├── levels: level scripts, geo layout, and display lists
├── lib: N64 SDK code
├── sound: sequences, sound samples, and sound banks
├── src: C source code for game
│   ├── audio: audio code
│   ├── buffers: stacks, heaps, and task buffers
│   ├── engine: script processing engines and utils
│   ├── game: behaviors and rest of game source
│   ├── goddard: rewritten Mario intro screen
│   ├── goddard_og: backup of original Mario intro screen
│   ├── menu: title screen and file, act, and debug level selection menus
│   └── port: port code, audio and video renderer
├── text: dialog, level names, act names
├── textures: skybox and generic texture data
└── tools: build tools
</code></pre></div><p dir="auto">Pull requests are welcome. For major changes, please open an issue first to
discuss what you would like to change.</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Terrain Diffusion: A Diffusion-Based Successor to Perlin Noise (147 pts)]]></title>
            <link>https://arxiv.org/abs/2512.08309</link>
            <guid>46221594</guid>
            <pubDate>Wed, 10 Dec 2025 18:37:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2512.08309">https://arxiv.org/abs/2512.08309</a>, See on <a href="https://news.ycombinator.com/item?id=46221594">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2512.08309">View PDF</a>
    <a href="https://arxiv.org/html/2512.08309v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>For decades, procedural worlds have been built on procedural noise functions such as Perlin noise, which are fast and infinite, yet fundamentally limited in realism and large-scale coherence. We introduce Terrain Diffusion, an AI-era successor to Perlin noise that bridges the fidelity of diffusion models with the properties that made procedural noise indispensable: seamless infinite extent, seed-consistency, and constant-time random access. At its core is InfiniteDiffusion, a novel algorithm for infinite generation, enabling seamless, real-time synthesis of boundless landscapes. A hierarchical stack of diffusion models couples planetary context with local detail, while a compact Laplacian encoding stabilizes outputs across Earth-scale dynamic ranges. An open-source infinite-tensor framework supports constant-memory manipulation of unbounded tensors, and few-step consistency distillation enables efficient generation. Together, these components establish diffusion models as a practical foundation for procedural world generation, capable of synthesizing entire planets coherently, controllably, and without limits.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Alexander Goslin [<a href="https://arxiv.org/show-email/c780ef89/2512.08309" rel="nofollow">view email</a>]      <br>    <strong>[v1]</strong>
        Tue, 9 Dec 2025 07:10:35 UTC (12,075 KB)<br>
</p></div></div>]]></description>
        </item>
    </channel>
</rss>