<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 24 Dec 2023 18:00:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Generals.io – Capture enemy generals to defeat them (136 pts)]]></title>
            <link>https://generals.io/</link>
            <guid>38752385</guid>
            <pubDate>Sun, 24 Dec 2023 09:38:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://generals.io/">https://generals.io/</a>, See on <a href="https://news.ycombinator.com/item?id=38752385">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Bill Watterson Commencement Speech (1990) (153 pts)]]></title>
            <link>https://web.mit.edu/jmorzins/www/C-H-speech.html</link>
            <guid>38751452</guid>
            <pubDate>Sun, 24 Dec 2023 05:47:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://web.mit.edu/jmorzins/www/C-H-speech.html">https://web.mit.edu/jmorzins/www/C-H-speech.html</a>, See on <a href="https://news.ycombinator.com/item?id=38751452">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<td><img src="http://www.okidoki.com/calvin_and_hobbes/pics/head/04c.gif" width="153" height="39"></td>
<td colspan="2"><span size="+1"><b>H</b></span>ere's the text of a speech Bill
Watterson gave at <a href="http://www.kenyon.edu/">Kenyon College</a>,
Gambier Ohio, to the 1990 graduating class.
<center>
<b>SOME THOUGHTS ON THE REAL WORLD BY ONE WHO GLIMPSED IT AND FLED</b>
<br>
Bill Watterson<br> Kenyon College Commencement<br> May 20,
1990</center>
<p>
I have a recurring dream about Kenyon. In it, I'm walking to the post
office on the way to my first class at the start of the school year.
Suddenly it occurs to me that I don't have my schedule memorized, and
I'm not sure which classes I'm taking, or where exactly I'm supposed to
be going.
<br>
As I walk up the steps to the postoffice, I realize I don't have my box
key, and in fact, I can't remember what my box number is. I'm certain
that everyone I know has written me a letter, but I can't get them. I
get more flustered and annoyed by the minute. I head back to Middle
Path, racking my brains and asking myself, "How many more years
until I graduate? ...Wait, didn't I graduate already?? How old AM
I?" Then I wake up.
</p><p>
Experience is food for the brain. And four years at Kenyon is a rich
meal. I suppose it should be no surprise that your brains will probably
burp up Kenyon for a long time. And I think the reason I keep having the
dream is because its central image is a metaphor for a good part of
life: that is, not knowing where you're going or what you're doing.
<br>
I graduated exactly ten years ago. That doesn't give me a great deal of
experience to speak from, but I'm emboldened by the fact that I can't
remember a bit of MY commencement, and I trust that in half an hour, you
won't remember of yours either.
</p><p>
In the middle of my sophomore year at Kenyon, I decided to paint a copy
of Michelangelo's "Creation of Adam" from the Sistine Chapel
on the ceiling of my dorm room. By standing on a chair, I could reach
the ceiling, and I taped off a section, made a grid, and started to copy
the picture from my art history book.
<br>
Working with your arm over your head is hard work, so a few of my more
ingenious friends rigged up a scaffold for me by stacking two chairs on
my bed, and laying the table from the hall lounge across the chairs and
over to the top of my closet. By climbing up onto my bed and up the
chairs, I could hoist myself onto the table, and lie in relative comfort
two feet under my painting. My roommate would then hand up my paints,
and I could work for several hours at a stretch.
</p><p>
The picture took me months to do, and in fact, I didn't finish the work
until very near the end of the school year. I wasn't much of a painter
then, but what the work lacked in color sense and technical flourish, it
gained in the incongruity of having a High Renaissance masterpiece in a
college dorm that had the unmistakable odor of old beer cans and older
laundry.<br>
The painting lent an air of cosmic grandeur to my room, and
it seemed to put life into a larger perspective. Those boring, flowery
English poets didn't seem quite so important, when right above my head
God was transmitting the spark of life to man.<br>
My friends and I liked the finished painting so much in fact, that we
decided I should ask permission to do it. As you might expect, the
housing director was curious to know why I wanted to paint this elaborate
picture on my ceiling a few weeks before school let out. Well, you don't
get to be a sophomore at Kenyon without learning how to fabricate ideas
you never had, but I guess it was obvious that my idea was being proposed
retroactively. It ended up that I was allowed to paint the picture, so
long as I painted over it and returned the ceiling to normal at the end
of the year. And that's what I did.
</p><p>
Despite the futility of the whole episode, my fondest memories of
college are times like these, where things were done out of some
inexplicable inner imperative, rather than because the work was
demanded. Clearly, I never spent as much time or work on any authorized
art project, or any poli sci paper, as I spent on this one act of
vandalism.
</p><p>
It's surprising how hard we'll work when the work is done just for
ourselves. And with all due respect to John Stuart Mill, maybe
utilitarianism is overrated. If I've learned one thing from being a
cartoonist, it's how important playing is to creativity and happiness.
My job is essentially to come up with 365 ideas a year.<br>
If you ever want to find out just how uninteresting you really are,
get a job where the quality and frequency of your thoughts determine
your livelihood. I've found that the only way I can keep writing
every day, year after year, is to let my mind wander into new
territories. To do that, I've had to cultivate a kind of mental
playfulness.
</p><p>
We're not really taught how to recreate constructively. We need to do
more than find diversions; we need to restore and expand ourselves. Our
idea of relaxing is all too often to plop down in front of the
television set and let its pandering idiocy liquefy our brains. Shutting
off the thought process is not rejuvenating; the mind is like a car
battery-it recharges by running.<br>
You may be surprised to find how quickly daily routine and the demands
of "just getting by: absorb your waking hours. You may be surprised
matters of habit rather than thought and inquiry. You may be surprised
to find how quickly you start to see your life in terms of other people's
expectations rather than issues. You may be surprised to find out how
quickly reading a good book sounds like a luxury.
</p><p>
At school, new ideas are thrust at you every day. Out in the world,
you'll have to find the inner motivation to search for new ideas on your
own. With any luck at all, you'll never need to take an idea and squeeze
a punchline out of it, but as bright, creative people, you'll be called
upon to generate ideas and solutions all your lives. Letting your mind
play is the best way to solve problems.<br>
For me, it's been liberating to put myself in the mind of a fictitious
six year-old each day, and rediscover my own curiosity. I've been amazed
at how one ideas leads to others if I allow my mind to play and wander.
I know a lot about dinosaurs now, and the information has helped me out
of quite a few deadlines.<br>
A playful mind is inquisitive, and learning is fun. If you indulge your
natural curiosity and retain a sense of fun in new experience, I think
you'll find it functions as a sort of shock absorber for the bumpy road
ahead.
</p><p>
<br>
So, what's it like in the real world? Well, the food is better, but
beyond that, I don't recommend it.
</p><p>
I don't look back on my first few years out of school with much
affection, and if I could have talked to you six months ago, I'd have
encouraged you all to flunk some classes and postpone this moment as
long as possible. But now it's too late.<br>
Unfortunately, that was all the advice I really had. When I was sitting
where you are, I was one of
the lucky few who had a cushy job waiting for me. I'd drawn political
cartoons for the Collegian for four years, and the Cincinnati Post had
hired me as an editorial cartoonist. All my friends were either dreading
the infamous first year of law school, or despondent about their chances
of convincing anyone that a history degree had any real application
outside of academia.
</p><p>
Boy, was I smug.
</p><p>
As it turned out, my editor instantly regretted his decision to hire me.
By the end of the summer, I'd been given notice; by the beginning of
winter, I was in an unemployment line; and by the end of my first year
away from Kenyon, I was broke and living with my parents again. You can
imagine how upset my dad was when he learned that Kenyon doesn't give
refunds.<br> Watching my career explode on the lauchpad caused some soul
searching. I eventually admitted that I didn't have what it takes to be
a good political cartoonist, that is, an interest in politics, and I
returned to my firs love, comic strips.<br> For years I got nothing but
rejection letters, and I was forced to accept a real job.
</p><p>
A REAL job is a job you hate. I designed car ads and grocery ads in the
windowless basement of a convenience store, and I hated every single
minute of the 4-1/2 million minutes I worked there. My fellow prisoners
at work were basically concerned about how to punch the time clock at
the perfect second where they would earn another 20 cents without doing
any work for it.<br> It was incredible: after every break, the entire
staff would stand around in the garage where the time clock was, and
wait for that last click. And after my used car needed the head gasket
replaced twice, I waited in the garage too.
</p><p>
It's funny how at Kenyon, you take for granted that the people around
you think about more than the last episode of Dynasty. I guess that's
what it means to be in an ivory tower.
</p><p>
Anyway, after a few months at this job, I was starved for some life of
the mind that, during my lunch break, I used to read those poli sci
books that I'd somehow never quite finished when I was here. Some of
those books were actually kind of interesting. It was a rude shock to
see just how empty and robotic life can be when you don't care about
what you're doing, and the only reason you're there is to pay the
bills.<br> Thoreau said, </p><blockquote>"the mass of men lead lives of
quiet desperation."</blockquote><br> That's one of those dumb
cocktail quotations that will strike fear in your heart as you get
older. Actually, I was leading a life of loud desperation.<p>

When it seemed I would be writing about "Midnite Madness
Sale-abrations" for the rest of my life, a friend used to console
me that cream always rises to the top. I used to think, so do people who
throw themselves into the sea.</p><p> <br>


I tell you all this because it's worth recognizing that there is no such
thing as an overnight success. You will do well to cultivate the
resources in yourself that bring you happiness outside of success or
failure. The truth is, most of us discover where we are headed when we
arrive. At that time, we turn around and say, yes, this is obviously
where I was going all along. It's a good idea to try to enjoy the
scenery on the detours, because you'll probably take a few.</p><p>

I still haven't drawn the strip as long as it took me to get the job. To
endure five years of rejection to get a job requires either a faith in
oneself that borders on delusion, or a love of the work. I loved the
work.<br> Drawing comic strips for five years without pay drove home the
point that the fun of cartooning wasn't in the money; it was in the
work. This turned out to be an important realization when my break
finally came.</p><p>

Like many people, I found that what I was chasing wasn't what I caught.
I've wanted to be a cartoonist since I was old enough to read cartoons,
and I never really thought about cartoons as being a business. It never
occurred to me that a comic strip I created would be at the mercy of a
bloodsucking corporate parasite called a syndicate, and that I'd be
faced with countless ethical decisions masquerading as simple business
decisions.<br> To make a business decision, you don't need much
philosophy; all you need is greed, and maybe a little knowledge of how
the game works.</p><p>

As my comic strip became popular, the pressure to capitalize on that
popularity increased to the point where I was spending almost as much
time screaming at executives as drawing. Cartoon merchandising is a $12
billion dollar a year industry and the syndicate understandably wanted a
piece of that pie. But the more I though about what they wanted to do
with my creation, the more inconsistent it seemed with the reasons I
draw cartoons.<br> Selling out is usually more a matter of buying in.
Sell out, and you're really buying into someone else's system of values,
rules and rewards.<br> The so-called "opportunity" I faced
would have meant giving up my individual voice for that of a
money-grubbing corporation. It would have meant my purpose in writing
was to sell things, not say things. My pride in craft would be
sacrificed to the efficiency of mass production and the work of
assistants. Authorship would become committee decision. Creativity would
become work for pay. Art would turn into commerce. In short, money was
supposed to supply all the meaning I'd need.<br> What the syndicate
wanted to do, in other words, was turn my comic strip into everything
calculated, empty and robotic that I hated about my old job. They would
turn my characters into television hucksters and T-shirt sloganeers and
deprive me of characters that actually expressed my own thoughts.</p><p>

On those terms, I found the offer easy to refuse. Unfortunately, the
syndicate also found my refusal easy to refuse, and we've been fighting
for over three years now. Such is American business, I guess, where the
desire for obscene profit mutes any discussion of conscience.</p><p> <br>


You will find your own ethical dilemmas in all parts of your lives, both
personal and professional. We all have different desires and needs, but
if we don't discover what we want from ourselves and what we stand for,
we will live passively and unfulfilled. Sooner or later, we are all
asked to compromise ourselves and the things we care about. We define
ourselves by our actions. With each decision, we tell ourselves and the
world who we are. Think about what you want out of this life, and
recognize that there are many kinds of success.<br> Many of you will be
going on to law school, business school, medical school, or other
graduate work, and you can expect the kind of starting salary that, with
luck, will allow you to pay off your own tuition debts within your own
lifetime.</p><p>

But having an enviable career is one thing, and being a happy person is
another.</p><p>

Creating a life that reflects your values and satisfies your soul is a
rare achievement. In a culture that relentlessly promotes avarice and
excess as the good life, a person happy doing his own work is usually
considered an eccentric, if not a subversive. Ambition is only
understood if it's to rise to the top of some imaginary ladder of
success. Someone who takes an undemanding job because it affords him the
time to pursue other interests and activities is considered a flake. A
person who abandons a career in order to stay home and raise children is
considered not to be living up to his potential-as if a job title and
salary are the sole measure of human worth.<br> You'll be told in a
hundred ways, some subtle and some not, to keep climbing, and never be
satisfied with where you are, who you are, and what you're doing. There
are a million ways to sell yourself out, and I guarantee you'll hear
about them.</p><p>

To invent your own life's meaning is not easy, but it's still allowed,
and I think you'll be happier for the trouble.<br> Reading those turgid
philosophers here in these remote stone buildings may not get you a job,
but if those books have forced you to ask yourself questions about what
makes life truthful, purposeful, meaningful, and redeeming, you have the
Swiss Army Knife of mental tools, and it's going to come in handy all
the time.</p><p>

I think you'll find that Kenyon touched a deep part of you. These have
been formative years. Chances are, at least of your roommates has taught
you everything ugly about human nature you ever wanted to know.<br> With
luck, you've also had a class that transmitted a spark of insight or
interest you'd never had before. Cultivate that interest, and you may
find a deeper meaning in your life that feeds your soul and spirit. Your
preparation for the real world is not in the answers you've learned, but
in the questions you've learned how to ask yourself.<br> Graduating from
Kenyon, I suspect you'll find yourselves quite well prepared indeed.
</p><p>
<br>
I wish you all fulfillment and happiness. Congratulations on your
achievement.
</p><p>
<br>
Bill Watterson
</p></td>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Unveiling the big leap in Ruby 3.3's IRB (172 pts)]]></title>
            <link>https://railsatscale.com/2023-12-19-irb-for-ruby-3-3/</link>
            <guid>38750459</guid>
            <pubDate>Sun, 24 Dec 2023 02:15:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://railsatscale.com/2023-12-19-irb-for-ruby-3-3/">https://railsatscale.com/2023-12-19-irb-for-ruby-3-3/</a>, See on <a href="https://news.ycombinator.com/item?id=38750459">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>In this blog post, we will delve into the major enhancements introduced in Ruby 3.3’s <a href="https://github.com/ruby/irb">IRB</a>,
as well as what’s currently planned for the next year.</p>

<p>In a nutshell, Ruby 3.3’s IRB offers <a href="#advancing-debugging-with-irb">improved debugging capabilities</a>,
an <a href="#enhancing-autocompletion">enhanced autocompletion experience</a>,
and an <a href="#quality-of-life-improvements">overall improved user experience</a>.</p>

<p>It’s worth noting that even if you don’t upgrade to Ruby 3.3 immediately, you can still install the same version of IRB
(<a href="https://github.com/ruby/irb/releases/tag/v1.11.0"><code>v1.11.0</code></a>) in any Ruby 2.7+ projects by adding <code>gem "irb"</code> to its Gemfile.</p>

<h3 id="table-of-contents">Table Of Contents</h3>

<ul>
  <li><a href="#advancing-debugging-with-irb">Advancing Debugging with IRB</a>
    <ul>
      <li><a href="#accessing-irbrdbg-from-bindingbreak--debugger">Accessing <code>irb:rdbg</code> from <code>binding.break</code> / <code>debugger</code></a></li>
    </ul>
  </li>
  <li><a href="#enhancing-autocompletion">Enhancing Autocompletion</a>
    <ul>
      <li><a href="#addressing-completion-dropdown-issues">Addressing Completion Dropdown Issues</a></li>
      <li><a href="#customizing-dropdown-ui-with-relineface">Customizing Dropdown UI with <code>Reline::Face</code></a></li>
      <li><a href="#experimenting-with-more-accurate-completion-using-rbs">Experimenting with More Accurate Completion Using RBS</a></li>
    </ul>
  </li>
  <li><a href="#quality-of-life-improvements">Quality of Life Improvements</a>
    <ul>
      <li><a href="#pager-support">Pager Support</a></li>
      <li><a href="#omitting-return-value-inspection-with-">Omitting Return Value Inspection with <code>;</code></a></li>
      <li><a href="#history-command">History Command</a></li>
      <li><a href="#streamlined-prompt">Streamlined Prompt</a></li>
      <li><a href="#enhanced-show_source-command">Enhanced <code>show_source</code> Command</a></li>
    </ul>
  </li>
  <li><a href="#whats-on-the-horizon-for-ruby-34">What’s on the Horizon for Ruby 3.4?</a>
    <ul>
      <li><a href="#the-help-command-will-print-help-message">The <code>help</code> Command Will Print Help Message</a></li>
      <li><a href="#enhancing-extensibility-command-and-helper-method">Enhancing Extensibility: Command and Helper Method</a></li>
    </ul>
  </li>
  <li><a href="#acknowledgements">Acknowledgements</a></li>
</ul>

<h2 id="advancing-debugging-with-irb">Advancing Debugging with IRB</h2>

<p>In Ruby 3.2’s IRB (<code>v1.6</code>), we introduced a set of shortcut commands such as <code>debug</code> and <code>next</code> that allowed for a swift
transition to the <a href="https://github.com/ruby/debug"><code>debug</code> gem</a>’s session from a <code>binding.irb</code> breakpoint.
(To avoid confusion, I’ll refer to the <code>debug</code> gem’s session as <code>rdbg</code> from here on.)</p>

<p>This was a significant improvement. However, with the switch, users would instantly lose certain IRB features, such as multi-line input:</p>

<div><pre><code>irb<span>(</span>main<span>)</span>:001:0&gt; debug
<span>(</span>ruby<span>)</span> <span>if </span><span>true
eval </span>error: <span>(</span>rdbg<span>)</span>/test.rb:1: syntax error, unexpected end-of-input, expecting <span>`</span><span>then</span><span>' or '</span><span>;</span><span>' or '</span><span>\n</span><span>'
if true
       ^
nil
(rdbg)
</span></code></pre></div>

<p>This year, we’ve taken the user experience to a new level by introducing the <code>irb:rdbg</code> session, which allows users to execute all <code>rdbg</code> commands without exiting the IRB session.</p>

<div><pre><code>irb<span>(</span>main<span>)</span>:001&gt; debug
irb:rdbg<span>(</span>main<span>)</span>:002<span>*</span> <span>if </span><span>true
</span>irb:rdbg<span>(</span>main<span>)</span>:003<span>*</span>   puts <span>"Multi-line input and more!"</span>
irb:rdbg<span>(</span>main<span>)</span>:004&gt; end
Multi-line input and more!
nil
irb:rdbg<span>(</span>main<span>)</span>:005&gt; th
<span># `th` (show all threads) is a example of a command only present in a `rdbg` session</span>
<span>--</span><span>&gt;</span> <span>#0 (sleep)@test.rb:2:in `&lt;main&gt;'</span>
irb:rdbg<span>(</span>main<span>)</span>:006&gt;
</code></pre></div>

<p>This deep integration offers several advantages:</p>

<ul>
  <li>Access to both <a href="https://github.com/ruby/irb?tab=readme-ov-file#commands">IRB</a> and <a href="https://github.com/ruby/debug?tab=readme-ov-file#debug-command-on-the-debug-console">rdbg’s commands</a></li>
  <li>Multi-line input</li>
  <li>Autocompletion</li>
  <li>Symbol aliases to commands, like <code>@</code> (<code>whereami</code>) and <code>$</code> (<code>show_source</code>)</li>
  <li><a href="#pager-support">Pager support</a></li>
</ul>

<p>But most importantly, it offers users a debugging experience equivalent to <code>pry-byebug</code>.</p>

<h3 id="accessing-irbrdbg-from-bindingbreak--debugger">Accessing <code>irb:rdbg</code> from <code>binding.break</code> / <code>debugger</code></h3>

<p>With the <code>debug</code> gem <code>v1.9.0</code>, which will also be part of Ruby 3.3, you can configure it to run <code>irb:rdbg</code> either by:</p>

<ul>
  <li>Setting its <code>irb_console</code> config through the <code>RUBY_DEBUG_IRB_CONSOLE=1</code> environment variable</li>
  <li>Running the <code>irb</code> command in a <code>rdbg</code> session</li>
</ul>

<p>Doing so will give you the same <code>irb:rdbg</code> session in your <code>debug</code> console:</p>

<div><pre><code><span>$ RUBY_DEBUG_IRB_CONSOLE</span><span>=</span>1 bundle <span>exec </span>rdbg test.rb
<span>[</span>1, 1] <span>in </span>test.rb
<span>=&gt;</span>   1| a <span>=</span> 1
<span>=&gt;</span><span>#0    &lt;main&gt; at test.rb:1</span>
irb:rdbg<span>(</span>main<span>)</span>:002&gt;
</code></pre></div>

<h2 id="enhancing-autocompletion">Enhancing Autocompletion</h2>

<h3 id="addressing-completion-dropdown-issues">Addressing Completion Dropdown Issues</h3>

<p>In Ruby 3.1, IRB introduced the autocompletion feature. While it has proven to be a useful feature for many users, it
also came with some issues that were hard to overlook:</p>

<ul>
  <li>The dropdown could push up the prompt when the candidate list is long. This not only distracts users but also pushes up
the output history, forcing users to frequently scroll up and down through the session.</li>
  <li>The dropdown’s colors are not configurable, making text hard to read with some terminal themes.</li>
  <li>Under certain conditions, the dropdown could erase previously rendered content.</li>
</ul>

<p>Here’s a short demonstration of these issues:</p>

<p><img src="https://railsatscale.com/2023-12-19-irb-for-ruby-3-3/images/autocompletion-issues.gif" alt="autocompletion issues demo" width="80%"></p>

<p>However, thanks to several fixes on the <a href="https://github.com/ruby/reline">Reline</a> gem (a pure-Ruby replacement of <code>readline</code>),
these issues have been addressed.</p>

<p><img src="https://railsatscale.com/2023-12-19-irb-for-ruby-3-3/images/improved-autocompletion.gif" alt="improved autocompletion demo" width="80%"></p>

<p>We are aware that there are other existing issues, such as the lack of debouncing or tab-completion options.
And we will keep improving the feature in future releases.</p>

<h3 id="customizing-dropdown-ui-with-relineface">Customizing Dropdown UI with <code>Reline::Face</code></h3>

<p>With <a href="https://github.com/ruby/reline"><code>Reline</code></a> <code>v0.4.0+</code>, you can use the <code>Reline::Face</code> class to customize the dropdown
UI’s style in your <code>.irbrc</code>.</p>

<p>For instance, this snippet will change the dropdown from its default color scheme to a black-and-white theme:</p>

<div><pre><code><span>Reline</span><span>::</span><span>Face</span><span>.</span><span>config</span><span>(</span><span>:completion_dialog</span><span>)</span> <span>do</span> <span>|</span><span>conf</span><span>|</span>
  <span>conf</span><span>.</span><span>define</span> <span>:default</span><span>,</span> <span>foreground: :white</span><span>,</span> <span>background: :black</span>
  <span>conf</span><span>.</span><span>define</span> <span>:enhanced</span><span>,</span> <span>foreground: :black</span><span>,</span> <span>background: :white</span>
  <span>conf</span><span>.</span><span>define</span> <span>:scrollbar</span><span>,</span> <span>foreground: :white</span><span>,</span> <span>background: :black</span>
<span>end</span>
</code></pre></div>

<p>The result can be seen below:</p>

<figure>
  <img src="https://railsatscale.com/2023-12-19-irb-for-ruby-3-3/images/black-and-white-completion-dropdown-with-reline-face.png">
  <figcaption>left: default, right: with the customization</figcaption>
</figure>

<h3 id="experimenting-with-more-accurate-completion-using-rbs">Experimenting with More Accurate Completion Using RBS</h3>

<p>Currently, IRB’s autocompletion is powered by a combination of regular expressions and runtime information from <code>Binding</code>.</p>

<p>This provides a decent result for the first level of completion, but can’t support chained method calls.</p>

<p>For example, when the user types <code>'Ruby'.up</code>, IRB can list <code>upcase</code> and <code>upcase!</code>, because it knows <code>'Ruby'</code> is a <code>String</code>.</p>

<p>But when the user types <code>'Ruby'.upcase.</code>, IRB can’t make another suggestion.
This is because it cannot evaluate <code>'Ruby'.upcase</code> to get its value as it risks causing side-effects
(for example, calling <code>User.all.</code> in a Rails app).</p>

<p>However, there is another way we can get an object’s information without evaluation in Ruby: gradual typing.
Therefore, this year <a href="https://github.com/tompng">@tompng</a> initiated the <a href="https://github.com/ruby/repl_type_completor"><code>repl_type_completor</code></a> project to explore if it’s a feasible replacement for the current regexp-based completor.</p>

<p>To give it a try, you need to add this to your Gemfile:</p>

<div><pre><code><span>gem</span> <span>"repl_type_completor"</span><span>,</span> <span>group: </span><span>[</span><span>:development</span><span>,</span> <span>:test</span><span>]</span>
</code></pre></div>

<p>And to activate it:</p>

<ul>
  <li>Pass the <code>--type-completor</code> flag when starting IRB</li>
  <li>Or add this to your <code>irbrc</code> file:</li>
</ul>

<div><pre><code><span>IRB</span><span>.</span><span>conf</span><span>[</span><span>:COMPLETOR</span><span>]</span> <span>=</span> <span>:type</span> <span># default is :regexp</span>
</code></pre></div>

<p>For more details, please refer to IRB readme’s <a href="https://github.com/ruby/irb?tab=readme-ov-file#type-based-completion">type completion section</a>.</p>

<h2 id="quality-of-life-improvements">Quality of Life Improvements</h2>



<p>This year, IRB has introduced pager support on the output of:</p>

<ul>
  <li>The <code>show_source</code>, <code>ls</code>, and <code>show_cmds</code> commands</li>
  <li>The new <code>history</code> command</li>
  <li>Evaluation result</li>
</ul>

<p>When these outputs’ height exceeds your terminal’s, IRB will now paginate them:</p>

<p><img src="https://railsatscale.com/2023-12-19-irb-for-ruby-3-3/images/pager-demo.gif" alt="pager demo" width="90%"></p>

<p>This offers several benefits:</p>

<ul>
  <li>You can freely scroll up and down with your arrow-up/down keys</li>
  <li>You won’t overscroll to some previous IRB output</li>
  <li>By typing <code>/[search_term]</code>, like <code>/foo</code>, inside a pager, you can easily search any text that contains <code>foo</code>
    <ul>
      <li>This could be extremely useful when inspecting record(s) in a Rails console</li>
    </ul>
  </li>
</ul>

<p>We understand that for some users or under certain conditions, a pager could be more of a hindrance than a help.
Therefore, it’s also possible to disable it with the <code>--no-pager</code> flag, or by adding this to your <code>irbrc</code> file:</p>

<div><pre><code><span>IRB</span><span>.</span><span>conf</span><span>[</span><span>:USE_PAGER</span><span>]</span> <span>=</span> <span>false</span>
</code></pre></div>

<h3 id="omitting-return-value-inspection-with-">Omitting Return Value Inspection with <code>;</code></h3>

<p>Even though paging evaluation result should make long output easier to inspect, sometimes we simply don’t want to see
the return value of certain expressions. For example, the result of <code>users = User.all</code> when it returns hundreds of records.</p>

<p>In such cases, the new IRB allows users to omit the return value by adding a <code>;</code> at the end, like <code>users = User.all;</code>.</p>

<h4 id="example">Example</h4>

<div><pre><code>irb<span>(</span>main<span>)</span>:001&gt; long_string <span>=</span> <span>"foo"</span> <span>*</span> 10000<span>;</span>
irb<span>(</span>main<span>)</span>:002&gt; long_string.size
<span>=&gt;</span> 30000
</code></pre></div>

<h3 id="history-command">History Command</h3>

<p>IRB now has a <code>history</code> command to display all stored input history (which by default is limited to <code>1000</code>).</p>

<p>Since we usually have a long input history, this command also comes with a <code>-g [term]</code> flag for filtering.</p>

<pre><code>irb(main):006&gt; history -g self
1000: history -g self
803: self
769: self
731: watch self @foo
698: break self.inspect
584: self
582: self
</code></pre>

<h3 id="streamlined-prompt">Streamlined Prompt</h3>

<p>As you may have noticed, IRB’s prompt is now a bit shorter:</p>

<h4 id="before">Before</h4>



<h4 id="after">After</h4>



<p>The removed part is the indent level number, which became redundant after IRB implemented a robust auto-indent feature
for multi-line input.</p>

<h3 id="enhanced-show_source-command">Enhanced <code>show_source</code> Command</h3>

<p>The <code>show_source</code> command has always been an essential tool for many IRB users, especially for debugging.
This year it received two enhancements that will make it even more useful:</p>

<ul>
  <li>You can now use <code>-s</code> to get the method’s super definition if it has one</li>
  <li>It can now display private methods too</li>
</ul>

<h4 id="the--s-flag">The <code>-s</code> Flag</h4>

<p>Given this script:</p>

<div><pre><code><span>class</span> <span>Foo</span>
  <span>def</span> <span>foo</span>
    <span>"foo"</span>
  <span>end</span>
<span>end</span>

<span>class</span> <span>Bar</span> <span>&lt;</span> <span>Foo</span>
  <span>def</span> <span>foo</span>
    <span>super</span> <span>+</span> <span>"bar"</span>
  <span>end</span>
<span>end</span>
</code></pre></div>

<p>You can now get both <code>Bar#foo</code> and its super method (<code>Foo#foo</code>)’s definition:</p>

<pre><code>irb(main):001&gt; show_source Bar#foo

From: test.rb:8

  def foo
    super + "bar"
  end

=&gt; nil
irb(main):002&gt; show_source Bar#foo -s

From: test.rb:2

  def foo
    "foo"
  end

=&gt; nil
</code></pre>

<p>You can also stack the flag, like <code>-ss</code>, to walk up the super method chain. And if IRB can’t find the super definition anymore,
it’ll notify you with:</p>

<pre><code>irb(main):003&gt; show_source Bar#foo -ss
Error: Couldn't locate a super definition for Bar#foo
=&gt; nil
</code></pre>

<h2 id="whats-on-the-horizon-for-ruby-34">What’s on the Horizon for Ruby 3.4?</h2>

<h3 id="the-help-command-will-print-help-message">The <code>help</code> Command Will Print Help Message</h3>

<p>When we use a terminal-based application, the first thing we normally do is to type <code>help</code> and learn how to use it.</p>

<p>However, due to historical reasons, IRB’s <code>help</code> command opens up an <code>ri</code> input for Ruby document lookup instead.
This unconventional design can make it hard, especially for new Ruby developers, to learn IRB’s usage.
So we decided to change it in several steps:</p>

<ol>
  <li>In Ruby 3.2, we added <code>show_cmds</code> to print the help message and <code>show_doc</code> command as an alias to the <code>help</code> command.</li>
  <li>In Ruby 3.3, IRB will warn users that <code>help</code> is going to be repurposed to act as <code>show_cmds</code>.</li>
  <li>In early 2024, we plan to release IRB <code>v2.0</code>, with the repurposing of <code>help</code> command being one of the breaking changes.</li>
</ol>

<h3 id="enhancing-extensibility-command-and-helper-method">Enhancing Extensibility: Command and Helper Method</h3>

<p>Several major Ruby web frameworks, such as <a href="https://github.com/rails/rails">Rails</a> and <a href="https://github.com/hanami/hanami">Hanami</a>,
utilize IRB as a platform for their consoles. Additionally, other libraries extend IRB with their custom features via commands.</p>

<p>However, until now, there have been no standard APIs on the IRB side to accommodate new commands or helper methods.
This situation leads to several challenges:</p>

<ul>
  <li>IRB is unable to display these extended features in its help message.</li>
  <li>Refactoring the relevant parts becomes challenging because they are essentially public when private APIs are used directly.</li>
  <li>Projects need to devise their own ways to utilize IRB, often resulting in similar but slightly different solutions.</li>
</ul>

<p>To address these issues, we plan to provide official APIs and relevant documentation to help libraries and applications extend IRB.
Our goal is to transform IRB from not just a great tool, but also into a great platform for other tools.</p>

<h2 id="acknowledgements">Acknowledgements</h2>

<p>The remarkable progress we see on IRB was made possible by the IRB/Reline maintainers team:</p>

<ul>
  <li><a href="https://github.com/tompng">Tomoya Ishida (tompng)</a></li>
  <li><a href="https://github.com/ima1zumi">Mari Imaizumi (ima1zumi)</a></li>
  <li><a href="https://github.com/hasumikin">Hitoshi Hasumi (hasumikin)</a></li>
  <li><a href="https://github.com/st0012">Stan Lo (st0012)</a></li>
</ul>

<p>And also, a big thank you to all the community contributors.</p>

  </div>
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Can Microsoft Flight Simulator help me learn to fly (or make me a better pilot)? (363 pts)]]></title>
            <link>https://aviation.stackexchange.com/questions/738/can-microsoft-flight-simulator-help-me-learn-to-fly-or-make-me-a-better-pilot</link>
            <guid>38750411</guid>
            <pubDate>Sun, 24 Dec 2023 02:08:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aviation.stackexchange.com/questions/738/can-microsoft-flight-simulator-help-me-learn-to-fly-or-make-me-a-better-pilot">https://aviation.stackexchange.com/questions/738/can-microsoft-flight-simulator-help-me-learn-to-fly-or-make-me-a-better-pilot</a>, See on <a href="https://news.ycombinator.com/item?id=38750411">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
<p>I believe that there's an element to this question which has not been covered.  This is very much a personal response.</p>

<p>Your question specifically asks whether it can help you to learn how to fly or become a better pilot.  This is actually two questions in one.</p>

<p>Physical and mental limitations not withstanding, I would say that just about anyone can learn how to fly.  But I don't believe that everyone who can learn how to fly could be a good pilot.  Flying and pilotage are very much more than successfully operating a flying machine in order to fly.  It's even a lot more than doing that and following the procedures (ATC, operating in controlled airspace etc) which accompany it.</p>

<p>Being a good pilot requires a certain aptitude and attitude (and I'm not talking about what you see on the AH).  It needs, let's not beat about the bush, a reasonably good IQ and education.</p>

<p>A pilot becomes good when the operation of the machine or the following of the procedures is not enough to to produce a safe, successful conclusion.  A good pilot avoids the traps and pitfalls that catch the unwary and have proved the rule, all to often, that in the ongoing contest between the earth and flimsy machines arriving in other than controlled circumstances, the earth has yet to lose.  </p>

<p>A good pilot takes care of the machine and it's passengers.  A good pilot can deal with the unexpected and make sound decisions to continue a flight or not or perhaps even to not commit aviation at all.  A good pilot has situational awareness which tells them, via sixth sense, that the bizjet calling left base is a potential threat and is already looking by the time the tower calls.</p>

<p>But much more than this, there are some very important elements to flying which a sim can never provide.</p>

<p>There is the emotional response; that thrill, that feeling of privilege, that unquantifiable human response to flying that is so much more than operating the machine.  There is also a set of skills and mental and physical responses without which, it is not possible to be a "good pilot" (IMHO).</p>

<p>Let me give a brief background of where I'm coming from and a concrete example of what I mean.</p>

<p>I have only a couple of hundred hours.  A handful on fixed wing, the rest on helicopters.  I also have about 3500 hours "flying" big tin on VATSIM (if you are serious about PC simming and don't know about VATSIM, Google for it right now!).</p>

<p>In VATSIM, I can operate a 777 (my favourite) very successfully including all of the related procedures which VATSIM does a remarkably good job of simulating.  I can fly a SID, follow my planned route, follow a STAR and do a visual onto 26R at Heathrow without breaking sweat, talking to and complying with ATC all the way. I can deal with an unexpected hold or a last minute change of arrival without fluster. I know this all works because I have also been lucky enough to do this in a "real" sim (737) and I had no problems at all using the automatics and hand flying the machine for the first 500 and last 1000 feet to depart and arrive safely back at Heathrow.</p>

<p>However, when I was learning to fly a helicopter, the reality of it all was very different.  Hovering is like learning to ride a bike.  I would say anyone with reasonable co-ordination can learn to do it.  I can hover all helicopters I've flown with ease.  Without even thinking about it.  Thing is, I can't really tell you how to do it and I have never been able to keep a PC sim helicopter in the air for more than a few minutes without getting into horrible shape.  Try explaining to a child how to ride a bike.  I reckon it's impossible.</p>

<p>When you switch from lurching around the sky with your instructor calling "I have control" every 20 seconds to that magical moment when you are suddenly in a steady hover, there are some things which simply cannot be simulated which your brain needs.  </p>

<p>The beginners mistake is to focus on the ground, I think most people are looking about 15 metres ahead.  You cannot succeed like that.  What you want to do is focus your eyes well into the distance and let your peripheral vision do the work.  It's almost subconscious and I can't even tell you how it all works but you do realise that you are moving over the ground with subtle cues coming in from your periphery.  </p>

<p>The second, and more important set of cues, are those that come through the "seat of your pants".  With a little experience, you just "know" when the helicopter moves, even before your peripheral vision has picked up the movement and you've already put that pressure on the cyclic (and it generally is pressure, not movement) to arrest the movement before the machine has deviated from it's position.  There are always small movements but to an external observer, you are sitting there, in the air, without so much as a ripple.</p>

<p>Subtle sounds are also important.  I can tell you pretty much where the rotor RPM is without looking at the tach and can certainly tell instantly when it's going down.  The second cue is from the tach but by that time, I'm already on the collective to adjust.  The tach just qualifies what I know and shows me that I'm doing the right thing to correct.</p>

<p>I'm sure that the fixed wing guys could provide good parallels.</p>

<p>My experience on sims tells me that you can indeed learn how to operate the machine and prosecute the procedures around it but it cannot help you to fly, nor to be a good (or better) pilot.  Where a sim can help is in teaching you the operation and procedures to the point where your brain is free to concentrate on flying and pilotage and your heart free to enjoy the sensations, because the mechanical stuff has moved into your subconscious and has become muscle memory.</p>

<p>Flying is emotional; it's passionate; it's determination; it's personal conquest; it runs in your veins. It's many things more than a sim can ever provide.</p>

<p>If you don't get "Oh I have slipped the surly bonds", then you are an operator, not a flyer.</p>

<p>[EDIT]
Am I a good pilot?  I'd say I'm average, and striving to be better, but that, in reality, probably describes most pilots, since it is impossible for most to be better than average.  I do recognise that I'm in that statistically dangerous zone where I have enough hours to think I'm good but not enough hours to prove it.  In my experience, ego and cahonas tell many pilots that they <strong>are</strong> good pilots ;)</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Limits to Growth (1972) (112 pts)]]></title>
            <link>https://donellameadows.org/the-limits-to-growth-now-available-to-read-online/</link>
            <guid>38750138</guid>
            <pubDate>Sun, 24 Dec 2023 01:23:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://donellameadows.org/the-limits-to-growth-now-available-to-read-online/">https://donellameadows.org/the-limits-to-growth-now-available-to-read-online/</a>, See on <a href="https://news.ycombinator.com/item?id=38750138">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h3>The complete text of the original <em>Limits to Growth</em> study is now accessible for free online, thanks to a partnership between the Dartmouth College Library, Dennis Meadows, and the Sustainability Institute.</h3>
<p><em><img fetchpriority="high" decoding="async" src="https://1a0c26.p3cdn2.secureserver.net/wp-content/userfiles/Picture-25-196x272-custom.png" alt="Limits to Growth cover" width="196" height="272">The Limits to Growth</em>, originally published in 1972, was a groundbreaking study that modeled the dynamics of our human presence on the planet. The team behind it, led by Dennis Meadows, found that continuing with a “business as usual” growth model would likely lead to environmental and economic collapse within a century. At the time, their discoveries sparked huge controversy among scientists, scholars, and the general public. In the decades since, those discoveries have been supported by patterns of growth, environmental health, and resource use.</p>
<p>Today, forty years after its release, <em>The Limits to Growth</em> remains an important resource for anyone hoping to understand more about the complex system that is our planet. We are honored and excited to make it freely available online. Each page of the publication has been carefully scanned and optimized for digital viewing, and all the original figures are included. The digital work has been assigned a Creative Commons BY-NC license, so we encourage you to download it and pass it along to your friends!</p>
<p><strong><a href="http://issuu.com/dartmouth_college_library/docs/the_limits_to_growth?e=1347206/1573259" target="_blank">Visit the Issuu page turner app to read <em>The Limits to Growth</em> online</a></strong></p>
<p><strong><a href="https://1a0c26.p3cdn2.secureserver.net/wp-content/userfiles/Limits-to-Growth-digital-scan-version.pdf">Download a PDF copy of <em>The Limits to Growth</em></a></strong></p>
<p><strong>An <a href="http://collections.dartmouth.edu/teitexts/meadows/meadows_ltg-normalize.html" target="_blank">HTML version of the study</a> is also available</strong>, complete with in-line figures from the original study. The HTML document makes it easy to download individual graphs and charts and to quickly search the text of the document for keywords and phrases.</p>
<p>Thanks so much to the Dartmouth Library for their hard work making this amazing resource available to our online community! Even more information about the study, its authors, and the digitization project can be found at <a href="http://www.dartmouth.edu/~library/digital/publishing/meadows/ltg/" target="_blank">the Dartmouth College website</a>.</p>
<p><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img decoding="async" src="https://cdn.printfriendly.com/+OK" alt="Print Friendly, PDF &amp; Email"></a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[San Francisco’s rent prices have never returned to pre-2020 levels (183 pts)]]></title>
            <link>https://www.sfchronicle.com/realestate/article/sf-rent-prices-18534829.php</link>
            <guid>38750079</guid>
            <pubDate>Sun, 24 Dec 2023 01:14:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sfchronicle.com/realestate/article/sf-rent-prices-18534829.php">https://www.sfchronicle.com/realestate/article/sf-rent-prices-18534829.php</a>, See on <a href="https://news.ycombinator.com/item?id=38750079">Hacker News</a></p>
Couldn't get https://www.sfchronicle.com/realestate/article/sf-rent-prices-18534829.php: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Ask yourself dumb questions and answer them (2020) (186 pts)]]></title>
            <link>https://terrytao.wordpress.com/career-advice/ask-yourself-dumb-questions-and-answer-them/</link>
            <guid>38749473</guid>
            <pubDate>Sat, 23 Dec 2023 23:50:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://terrytao.wordpress.com/career-advice/ask-yourself-dumb-questions-and-answer-them/">https://terrytao.wordpress.com/career-advice/ask-yourself-dumb-questions-and-answer-them/</a>, See on <a href="https://news.ycombinator.com/item?id=38749473">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<blockquote>
<p><em>Don’t just read it; fight it! Ask your own questions, look for your own examples, discover your own proofs. Is the hypothesis necessary? Is the converse true? What happens in the classical special case? What about the degenerate cases? Where does the proof use the hypothesis?</em> (<a href="http://en.wikipedia.org/wiki/Paul_Halmos">Paul Halmos</a>, “I want to be a mathematician”)</p>
</blockquote>
<p>When you learn mathematics, whether in books or in lectures, you generally only see the end product – very polished, clever and elegant presentations of a mathematical topic.</p>
<p>However, the process of discovering <em>new</em> mathematics is much messier, full of the pursuit of directions which were naïve, fruitless or uninteresting.</p>
<p>While it is tempting to just ignore all these “failed” lines of inquiry, actually they turn out to be essential to one’s deeper understanding of a topic, and (via the process of elimination) finally zeroing in on the correct way to proceed.</p>
<p>So one should be unafraid to ask “stupid” questions, challenging conventional wisdom on a subject; the answers to these questions will occasionally lead to a surprising conclusion, but more often will simply tell you why the conventional wisdom is there in the first place, which is well worth knowing.</p>
<p>For instance, given a standard lemma in a subject, you can ask what happens if you delete a hypothesis, or attempt to strengthen the conclusion; if a simple result is usually proven by method X, you can ask whether it can be proven by method Y instead; the new proof may be less elegant than the original, or may not work at all, but in either case it tends to illuminate the relative power of methods X and Y, which can be useful when the time comes to prove less standard lemmas.</p>
<p>It’s also acceptable, when listening to a seminar, to ask “dumb” but constructive questions to help clarify some basic issue in the talk (e.g. whether statement X implied statement Y in the argument, or vice versa; whether a terminology introduced by the speaker is related to a very similar sounding terminology that you already knew about; and so forth). If you don’t ask, you might be lost for the remainder of the talk; and usually speakers appreciate the feedback (it shows that at least one audience member is paying attention!) and the opportunity to explain things better, both to you and to the rest of the audience. However, questions which do not immediately enhance the flow of the talk are probably best left to after the end of the talk.</p>

<ul>
<li>Martin Schwartz, “<a href="http://jcs.biologists.org/content/121/11/1771.full?sid=44d5ae18-0f2f-4944-b256-7d35968c9c23">The importance of stupidity in scientific research</a>“, <span>Journal of Cell Science&nbsp;</span><span>2008&nbsp;</span><span>121:&nbsp;</span><span>1771.</span></li>
</ul>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[StreamDiffusion: A Pipeline-Level Solution for Real-Time Interactive Generation (335 pts)]]></title>
            <link>https://github.com/cumulo-autumn/StreamDiffusion</link>
            <guid>38749434</guid>
            <pubDate>Sat, 23 Dec 2023 23:42:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/cumulo-autumn/StreamDiffusion">https://github.com/cumulo-autumn/StreamDiffusion</a>, See on <a href="https://news.ycombinator.com/item?id=38749434">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">StreamDiffusion</h2>
<p dir="auto"><a href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/README.md">English</a> | <a href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/README-ja.md">日本語</a></p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/assets/demo_07.gif"><img src="https://github.com/cumulo-autumn/StreamDiffusion/raw/main/assets/demo_07.gif" width="90%" data-animated-image=""></a>
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/assets/demo_09.gif"><img src="https://github.com/cumulo-autumn/StreamDiffusion/raw/main/assets/demo_09.gif" width="90%" data-animated-image=""></a>
</p>
<h2 tabindex="-1" dir="auto">StreamDiffusion: A Pipeline-Level Solution for Real-Time Interactive Generation</h2>
<p dir="auto"><strong>Authors:</strong> <a href="https://www.linkedin.com/in/akio-kodaira-1a7b98252/" rel="nofollow">Akio Kodaira</a>, <a href="https://www.chenfengx.com/" rel="nofollow">Chenfeng Xu</a>, Toshiki Hazama, <a href="https://twitter.com/__ramu0e__" rel="nofollow">Takanori Yoshimoto</a>, <a href="https://www.linkedin.com/in/kohei--ohno/" rel="nofollow">Kohei Ohno</a>, <a href="https://me.ddpn.world/" rel="nofollow">Shogo Mitsuhori</a>, <a href="https://twitter.com/toni_nimono" rel="nofollow">Soichi Sugano</a>, <a href="https://twitter.com/hanyingcl" rel="nofollow">Hanying Cho</a>, <a href="https://zhijianliu.com/" rel="nofollow">Zhijian Liu</a>, <a href="https://scholar.google.com/citations?hl=en&amp;user=ID9QePIAAAAJ" rel="nofollow">Kurt Keutzer</a></p>
<p dir="auto">StreamDiffusion is an innovative diffusion pipeline designed for real-time interactive generation. It introduces significant performance enhancements to current diffusion-based image generation techniques.</p>
<p dir="auto"><a href="https://arxiv.org/abs/2312.12491" rel="nofollow"><img src="https://camo.githubusercontent.com/47151ae40041cc004f656a042aafc1b5877958bbcc298d169fca2d778994138a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323330372e30343732352d6233316231622e737667" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2307.04725-b31b1b.svg"></a>
<a href="https://huggingface.co/papers/2312.12491" rel="nofollow"><img src="https://camo.githubusercontent.com/dbe4a949263f6758ffe4b0757f52aa29772753ceebcca38af41aef80b720f57f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d7061706572732d79656c6c6f77" alt="Hugging Face Papers" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-papers-yellow"></a></p>
<p dir="auto">We sincerely thank <a href="https://twitter.com/AttaQjp" rel="nofollow">Taku Fujimoto</a> and <a href="https://twitter.com/radamar" rel="nofollow">Radamés Ajna</a> and Hugging Face team for their invaluable feedback, courteous support, and insightful discussions.</p>
<h2 tabindex="-1" dir="auto">Key Features</h2>
<ol dir="auto">
<li>
<p dir="auto"><strong>Stream Batch</strong></p>
<ul dir="auto">
<li>Streamlined data processing through efficient batch operations.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Residual Classifier-Free Guidance</strong> - <a href="#residual-cfg-rcfg">Learn More</a></p>
<ul dir="auto">
<li>Improved guidance mechanism that minimizes computational redundancy.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Stochastic Similarity Filter</strong> - <a href="#stochastic-similarity-filter">Learn More</a></p>
<ul dir="auto">
<li>Improves GPU utilization efficiency through advanced filtering techniques.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>IO Queues</strong></p>
<ul dir="auto">
<li>Efficiently manages input and output operations for smoother execution.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Pre-Computation for KV-Caches</strong></p>
<ul dir="auto">
<li>Optimizes caching strategies for accelerated processing.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Model Acceleration Tools</strong></p>
<ul dir="auto">
<li>Utilizes various tools for model optimization and performance boost.</li>
</ul>
</li>
</ol>
<p dir="auto">When images are produced using our proposed StreamDiffusion pipeline in an environment with <strong>GPU: RTX 4090</strong>, <strong>CPU: Core i9-13900K</strong>, and <strong>OS: Ubuntu 22.04.3 LTS</strong>.</p>
<table>
<thead>
<tr>
<th>model</th>
<th>Denoising Step</th>
<th>fps on Txt2Img</th>
<th>fps on Img2Img</th>
</tr>
</thead>
<tbody>
<tr>
<td>SD-turbo</td>
<td>1</td>
<td>106.16</td>
<td>93.897</td>
</tr>
<tr>
<td>LCM-LoRA <br>+<br> KohakuV2</td>
<td>4</td>
<td>38.023</td>
<td>37.133</td>
</tr>
</tbody>
</table>
<p dir="auto">Feel free to explore each feature by following the provided links to learn more about StreamDiffusion's capabilities. If you find it helpful, please consider citing our work:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@article{kodaira2023streamdiffusion,
      title={StreamDiffusion: A Pipeline-level Solution for Real-time Interactive Generation},
      author={Akio Kodaira and Chenfeng Xu and Toshiki Hazama and Takanori Yoshimoto and Kohei Ohno and Shogo Mitsuhori and Soichi Sugano and Hanying Cho and Zhijian Liu and Kurt Keutzer},
      year={2023},
      eprint={2312.12491},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}"><pre>@article{kodaira2023streamdiffusion,
      title={StreamDiffusion: A Pipeline-level Solution <span>for</span> Real-time Interactive Generation},
      author={Akio Kodaira and Chenfeng Xu and Toshiki Hazama and Takanori Yoshimoto and Kohei Ohno and Shogo Mitsuhori and Soichi Sugano and Hanying Cho and Zhijian Liu and Kurt Keutzer},
      year={2023},
      eprint={2312.12491},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}</pre></div>
<h2 tabindex="-1" dir="auto">Installation</h2>
<h3 tabindex="-1" dir="auto">Step0: clone this repository</h3>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/cumulo-autumn/StreamDiffusion.git"><pre>git clone https://github.com/cumulo-autumn/StreamDiffusion.git</pre></div>
<h3 tabindex="-1" dir="auto">Step1: Make Environment</h3>
<p dir="auto">You can install StreamDiffusion via pip, conda, or Docker(explanation below).</p>
<div dir="auto" data-snippet-clipboard-copy-content="conda create -n streamdiffusion python=3.10
conda activate streamdiffusion"><pre>conda create -n streamdiffusion python=3.10
conda activate streamdiffusion</pre></div>
<p dir="auto">OR</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m venv .venv
# Windows
.\.venv\Scripts\activate
# Linux
source .venv/bin/activate"><pre>python -m venv .venv
# Windows
.\.venv\Scripts\activate
# Linux
source .venv/bin/activate</pre></div>
<h3 tabindex="-1" dir="auto">Step2: Install PyTorch</h3>
<p dir="auto">Select the appropriate version for your system.</p>
<p dir="auto">CUDA 11.8</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip3 install torch==2.1.0 torchvision==0.16.0 xformers --index-url https://download.pytorch.org/whl/cu118"><pre>pip3 install torch==2.1.0 torchvision==0.16.0 xformers --index-url https://download.pytorch.org/whl/cu118</pre></div>
<p dir="auto">CUDA 12.1</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip3 install torch==2.1.0 torchvision==0.16.0 xformers --index-url https://download.pytorch.org/whl/cu121"><pre>pip3 install torch==2.1.0 torchvision==0.16.0 xformers --index-url https://download.pytorch.org/whl/cu121</pre></div>
<p dir="auto">details: <a href="https://pytorch.org/" rel="nofollow">https://pytorch.org/</a></p>
<h3 tabindex="-1" dir="auto">Step3: Install StreamDiffusion</h3>
<h4 tabindex="-1" dir="auto">For User</h4>
<p dir="auto">Install StreamDiffusion</p>
<div dir="auto" data-snippet-clipboard-copy-content="#for Latest Version (recommended)
pip install git+https://github.com/cumulo-autumn/StreamDiffusion.git@main#egg=streamdiffusion[tensorrt]


#or


#for Stable Version
pip install streamdiffusion[tensorrt]"><pre><span><span>#</span>for Latest Version (recommended)</span>
pip install git+https://github.com/cumulo-autumn/StreamDiffusion.git@main#egg=streamdiffusion[tensorrt]


<span><span>#</span>or</span>


<span><span>#</span>for Stable Version</span>
pip install streamdiffusion[tensorrt]</pre></div>
<p dir="auto">Install TensorRT extension and pywin32
(※※pywin32 is required only for Windows.)</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m streamdiffusion.tools.install-tensorrt
# If you use Windows, you need to install pywin32 
pip install pywin32"><pre>python -m streamdiffusion.tools.install-tensorrt
<span><span>#</span> If you use Windows, you need to install pywin32 </span>
pip install pywin32</pre></div>
<h4 tabindex="-1" dir="auto">For Developer</h4>
<div dir="auto" data-snippet-clipboard-copy-content="python setup.py develop easy_install streamdiffusion[tensorrt]
python -m streamdiffusion.tools.install-tensorrt"><pre>python setup.py develop easy_install streamdiffusion[tensorrt]
python -m streamdiffusion.tools.install-tensorrt</pre></div>
<h3 tabindex="-1" dir="auto">Docker Installation (TensorRT Ready)</h3>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/cumulo-autumn/StreamDiffusion.git
cd StreamDiffusion
docker build -t stream-diffusion:latest -f Dockerfile .
docker run --gpus all -it -v $(pwd):/home/ubuntu/streamdiffusion stream-diffusion:latest"><pre>git clone https://github.com/cumulo-autumn/StreamDiffusion.git
<span>cd</span> StreamDiffusion
docker build -t stream-diffusion:latest -f Dockerfile <span>.</span>
docker run --gpus all -it -v <span><span>$(</span>pwd<span>)</span></span>:/home/ubuntu/streamdiffusion stream-diffusion:latest</pre></div>
<h2 tabindex="-1" dir="auto">Quick Start</h2>
<p dir="auto">You can try StreamDiffusion in <a href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/examples"><code>examples</code></a> directory.</p>
<table>
<thead>
<tr>
<th><a target="_blank" rel="noopener noreferrer" href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/assets/demo_02.gif"><img src="https://github.com/cumulo-autumn/StreamDiffusion/raw/main/assets/demo_02.gif" alt="画像3" data-animated-image=""></a></th>
<th><a target="_blank" rel="noopener noreferrer" href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/assets/demo_03.gif"><img src="https://github.com/cumulo-autumn/StreamDiffusion/raw/main/assets/demo_03.gif" alt="画像4" data-animated-image=""></a></th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/assets/demo_04.gif"><img src="https://github.com/cumulo-autumn/StreamDiffusion/raw/main/assets/demo_04.gif" alt="画像5" data-animated-image=""></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/assets/demo_05.gif"><img src="https://github.com/cumulo-autumn/StreamDiffusion/raw/main/assets/demo_05.gif" alt="画像6" data-animated-image=""></a></td>
</tr>
</tbody>
</table>
<h2 tabindex="-1" dir="auto">Real-Time Txt2Img Demo</h2>
<p dir="auto">There is an interactive txt2img demo in <a href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/demo/realtime-txt2img"><code>demo/realtime-txt2img</code></a> directory!</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/assets/demo_01.gif"><img src="https://github.com/cumulo-autumn/StreamDiffusion/raw/main/assets/demo_01.gif" width="100%" data-animated-image=""></a>
</p>
<h2 tabindex="-1" dir="auto">Usage Example</h2>
<p dir="auto">We provide a simple example of how to use StreamDiffusion. For more detailed examples, please refer to <a href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/examples"><code>examples</code></a> directory.</p>
<h3 tabindex="-1" dir="auto">Image-to-Image</h3>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
from diffusers import AutoencoderTiny, StableDiffusionPipeline
from diffusers.utils import load_image

from streamdiffusion import StreamDiffusion
from streamdiffusion.image_utils import postprocess_image

# You can load any models using diffuser's StableDiffusionPipeline
pipe = StableDiffusionPipeline.from_pretrained(&quot;KBlueLeaf/kohaku-v2.1&quot;).to(
    device=torch.device(&quot;cuda&quot;),
    dtype=torch.float16,
)

# Wrap the pipeline in StreamDiffusion
stream = StreamDiffusion(
    pipe,
    t_index_list=[32, 45],
    torch_dtype=torch.float16,
)

# If the loaded model is not LCM, merge LCM
stream.load_lcm_lora()
stream.fuse_lora()
# Use Tiny VAE for further acceleration
stream.vae = AutoencoderTiny.from_pretrained(&quot;madebyollin/taesd&quot;).to(device=pipe.device, dtype=pipe.dtype)
# Enable acceleration
pipe.enable_xformers_memory_efficient_attention()


prompt = &quot;1girl with dog hair, thick frame glasses&quot;
# Prepare the stream
stream.prepare(prompt)

# Prepare image
init_image = load_image(&quot;assets/img2img_example.png&quot;).resize((512, 512))

# Warmup >= len(t_index_list) x frame_buffer_size
for _ in range(2):
    stream(init_image)

# Run the stream infinitely
while True:
    x_output = stream(init_image)
    postprocess_image(x_output, output_type=&quot;pil&quot;)[0].show()
    input_response = input(&quot;Press Enter to continue or type 'stop' to exit: &quot;)
    if input_response == &quot;stop&quot;:
        break"><pre><span>import</span> <span>torch</span>
<span>from</span> <span>diffusers</span> <span>import</span> <span>AutoencoderTiny</span>, <span>StableDiffusionPipeline</span>
<span>from</span> <span>diffusers</span>.<span>utils</span> <span>import</span> <span>load_image</span>

<span>from</span> <span>streamdiffusion</span> <span>import</span> <span>StreamDiffusion</span>
<span>from</span> <span>streamdiffusion</span>.<span>image_utils</span> <span>import</span> <span>postprocess_image</span>

<span># You can load any models using diffuser's StableDiffusionPipeline</span>
<span>pipe</span> <span>=</span> <span>StableDiffusionPipeline</span>.<span>from_pretrained</span>(<span>"KBlueLeaf/kohaku-v2.1"</span>).<span>to</span>(
    <span>device</span><span>=</span><span>torch</span>.<span>device</span>(<span>"cuda"</span>),
    <span>dtype</span><span>=</span><span>torch</span>.<span>float16</span>,
)

<span># Wrap the pipeline in StreamDiffusion</span>
<span>stream</span> <span>=</span> <span>StreamDiffusion</span>(
    <span>pipe</span>,
    <span>t_index_list</span><span>=</span>[<span>32</span>, <span>45</span>],
    <span>torch_dtype</span><span>=</span><span>torch</span>.<span>float16</span>,
)

<span># If the loaded model is not LCM, merge LCM</span>
<span>stream</span>.<span>load_lcm_lora</span>()
<span>stream</span>.<span>fuse_lora</span>()
<span># Use Tiny VAE for further acceleration</span>
<span>stream</span>.<span>vae</span> <span>=</span> <span>AutoencoderTiny</span>.<span>from_pretrained</span>(<span>"madebyollin/taesd"</span>).<span>to</span>(<span>device</span><span>=</span><span>pipe</span>.<span>device</span>, <span>dtype</span><span>=</span><span>pipe</span>.<span>dtype</span>)
<span># Enable acceleration</span>
<span>pipe</span>.<span>enable_xformers_memory_efficient_attention</span>()


<span>prompt</span> <span>=</span> <span>"1girl with dog hair, thick frame glasses"</span>
<span># Prepare the stream</span>
<span>stream</span>.<span>prepare</span>(<span>prompt</span>)

<span># Prepare image</span>
<span>init_image</span> <span>=</span> <span>load_image</span>(<span>"assets/img2img_example.png"</span>).<span>resize</span>((<span>512</span>, <span>512</span>))

<span># Warmup &gt;= len(t_index_list) x frame_buffer_size</span>
<span>for</span> <span>_</span> <span>in</span> <span>range</span>(<span>2</span>):
    <span>stream</span>(<span>init_image</span>)

<span># Run the stream infinitely</span>
<span>while</span> <span>True</span>:
    <span>x_output</span> <span>=</span> <span>stream</span>(<span>init_image</span>)
    <span>postprocess_image</span>(<span>x_output</span>, <span>output_type</span><span>=</span><span>"pil"</span>)[<span>0</span>].<span>show</span>()
    <span>input_response</span> <span>=</span> <span>input</span>(<span>"Press Enter to continue or type 'stop' to exit: "</span>)
    <span>if</span> <span>input_response</span> <span>==</span> <span>"stop"</span>:
        <span>break</span></pre></div>
<h3 tabindex="-1" dir="auto">Text-to-Image</h3>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
from diffusers import AutoencoderTiny, StableDiffusionPipeline

from streamdiffusion import StreamDiffusion
from streamdiffusion.image_utils import postprocess_image

# You can load any models using diffuser's StableDiffusionPipeline
pipe = StableDiffusionPipeline.from_pretrained(&quot;KBlueLeaf/kohaku-v2.1&quot;).to(
    device=torch.device(&quot;cuda&quot;),
    dtype=torch.float16,
)

# Wrap the pipeline in StreamDiffusion
# Requires more long steps (len(t_index_list)) in text2image
# You recommend to use cfg_type=&quot;none&quot; when text2image
stream = StreamDiffusion(
    pipe,
    t_index_list=[0, 16, 32, 45],
    torch_dtype=torch.float16,
    cfg_type=&quot;none&quot;,
)

# If the loaded model is not LCM, merge LCM
stream.load_lcm_lora()
stream.fuse_lora()
# Use Tiny VAE for further acceleration
stream.vae = AutoencoderTiny.from_pretrained(&quot;madebyollin/taesd&quot;).to(device=pipe.device, dtype=pipe.dtype)
# Enable acceleration
pipe.enable_xformers_memory_efficient_attention()


prompt = &quot;1girl with dog hair, thick frame glasses&quot;
# Prepare the stream
stream.prepare(prompt)

# Warmup >= len(t_index_list) x frame_buffer_size
for _ in range(4):
    stream()

# Run the stream infinitely
while True:
    x_output = stream.txt2img()
    postprocess_image(x_output, output_type=&quot;pil&quot;)[0].show()
    input_response = input(&quot;Press Enter to continue or type 'stop' to exit: &quot;)
    if input_response == &quot;stop&quot;:
        break"><pre><span>import</span> <span>torch</span>
<span>from</span> <span>diffusers</span> <span>import</span> <span>AutoencoderTiny</span>, <span>StableDiffusionPipeline</span>

<span>from</span> <span>streamdiffusion</span> <span>import</span> <span>StreamDiffusion</span>
<span>from</span> <span>streamdiffusion</span>.<span>image_utils</span> <span>import</span> <span>postprocess_image</span>

<span># You can load any models using diffuser's StableDiffusionPipeline</span>
<span>pipe</span> <span>=</span> <span>StableDiffusionPipeline</span>.<span>from_pretrained</span>(<span>"KBlueLeaf/kohaku-v2.1"</span>).<span>to</span>(
    <span>device</span><span>=</span><span>torch</span>.<span>device</span>(<span>"cuda"</span>),
    <span>dtype</span><span>=</span><span>torch</span>.<span>float16</span>,
)

<span># Wrap the pipeline in StreamDiffusion</span>
<span># Requires more long steps (len(t_index_list)) in text2image</span>
<span># You recommend to use cfg_type="none" when text2image</span>
<span>stream</span> <span>=</span> <span>StreamDiffusion</span>(
    <span>pipe</span>,
    <span>t_index_list</span><span>=</span>[<span>0</span>, <span>16</span>, <span>32</span>, <span>45</span>],
    <span>torch_dtype</span><span>=</span><span>torch</span>.<span>float16</span>,
    <span>cfg_type</span><span>=</span><span>"none"</span>,
)

<span># If the loaded model is not LCM, merge LCM</span>
<span>stream</span>.<span>load_lcm_lora</span>()
<span>stream</span>.<span>fuse_lora</span>()
<span># Use Tiny VAE for further acceleration</span>
<span>stream</span>.<span>vae</span> <span>=</span> <span>AutoencoderTiny</span>.<span>from_pretrained</span>(<span>"madebyollin/taesd"</span>).<span>to</span>(<span>device</span><span>=</span><span>pipe</span>.<span>device</span>, <span>dtype</span><span>=</span><span>pipe</span>.<span>dtype</span>)
<span># Enable acceleration</span>
<span>pipe</span>.<span>enable_xformers_memory_efficient_attention</span>()


<span>prompt</span> <span>=</span> <span>"1girl with dog hair, thick frame glasses"</span>
<span># Prepare the stream</span>
<span>stream</span>.<span>prepare</span>(<span>prompt</span>)

<span># Warmup &gt;= len(t_index_list) x frame_buffer_size</span>
<span>for</span> <span>_</span> <span>in</span> <span>range</span>(<span>4</span>):
    <span>stream</span>()

<span># Run the stream infinitely</span>
<span>while</span> <span>True</span>:
    <span>x_output</span> <span>=</span> <span>stream</span>.<span>txt2img</span>()
    <span>postprocess_image</span>(<span>x_output</span>, <span>output_type</span><span>=</span><span>"pil"</span>)[<span>0</span>].<span>show</span>()
    <span>input_response</span> <span>=</span> <span>input</span>(<span>"Press Enter to continue or type 'stop' to exit: "</span>)
    <span>if</span> <span>input_response</span> <span>==</span> <span>"stop"</span>:
        <span>break</span></pre></div>
<p dir="auto">You can make it faster by using SD-Turbo.</p>
<h3 tabindex="-1" dir="auto">Faster generation</h3>
<p dir="auto">Replace the following code in the above example.</p>
<div dir="auto" data-snippet-clipboard-copy-content="pipe.enable_xformers_memory_efficient_attention()"><pre><span>pipe</span>.<span>enable_xformers_memory_efficient_attention</span>()</pre></div>
<p dir="auto">To</p>
<div dir="auto" data-snippet-clipboard-copy-content="from streamdiffusion.acceleration.tensorrt import accelerate_with_tensorrt

stream = accelerate_with_tensorrt(
    stream, &quot;engines&quot;, max_batch_size=2,
)"><pre><span>from</span> <span>streamdiffusion</span>.<span>acceleration</span>.<span>tensorrt</span> <span>import</span> <span>accelerate_with_tensorrt</span>

<span>stream</span> <span>=</span> <span>accelerate_with_tensorrt</span>(
    <span>stream</span>, <span>"engines"</span>, <span>max_batch_size</span><span>=</span><span>2</span>,
)</pre></div>
<p dir="auto">It requires TensorRT extension and time to build the engine, but it will be faster than the above example.</p>
<h2 tabindex="-1" dir="auto">Optionals</h2>
<h3 tabindex="-1" dir="auto">Stochastic Similarity Filter</h3>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/assets/demo_06.gif"><img src="https://github.com/cumulo-autumn/StreamDiffusion/raw/main/assets/demo_06.gif" alt="demo" data-animated-image=""></a></p>
<p dir="auto">Stochastic Similarity Filter reduces processing during video input by minimizing conversion operations when there is little change from the previous frame, thereby alleviating GPU processing load, as shown by the red frame in the above GIF. The usage is as follows:</p>
<div dir="auto" data-snippet-clipboard-copy-content="stream = StreamDiffusion(
    pipe,
    [32, 45],
    torch_dtype=torch.float16,
)
stream.enable_similar_image_filter(
    similar_image_filter_threshold,
    similar_image_filter_max_skip_frame,
)"><pre><span>stream</span> <span>=</span> <span>StreamDiffusion</span>(
    <span>pipe</span>,
    [<span>32</span>, <span>45</span>],
    <span>torch_dtype</span><span>=</span><span>torch</span>.<span>float16</span>,
)
<span>stream</span>.<span>enable_similar_image_filter</span>(
    <span>similar_image_filter_threshold</span>,
    <span>similar_image_filter_max_skip_frame</span>,
)</pre></div>
<p dir="auto">There are the following parameters that can be set as arguments in the function:</p>
<h4 tabindex="-1" dir="auto"><code>similar_image_filter_threshold</code></h4>
<ul dir="auto">
<li>The threshold for similarity between the previous frame and the current frame before the processing is paused.</li>
</ul>
<h4 tabindex="-1" dir="auto"><code>similar_image_filter_max_skip_frame</code></h4>
<ul dir="auto">
<li>The maximum interval during the pause before resuming the conversion.</li>
</ul>
<h3 tabindex="-1" dir="auto">Residual CFG (RCFG)</h3>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/assets/cfg_conparision.png"><img src="https://github.com/cumulo-autumn/StreamDiffusion/raw/main/assets/cfg_conparision.png" alt="rcfg"></a></p>
<p dir="auto">RCFG is a method for approximately realizing CFG with competitive computational complexity compared to cases where CFG is not used. It can be specified through the cfg_type argument in the StreamDiffusion. There are two types of RCFG: one with no specified items for negative prompts RCFG Self-Negative and one where negative prompts can be specified RCFG Onetime-Negative. In terms of computational complexity, denoting the complexity without CFG as N and the complexity with a regular CFG as 2N, RCFG Self-Negative can be computed in N steps, while RCFG Onetime-Negative can be computed in N+1 steps.</p>
<p dir="auto">The usage is as follows:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# w/0 CFG
cfg_type = &quot;none&quot;
# CFG
cfg_type = &quot;full&quot;
# RCFG Self-Negative
cfg_type = &quot;self&quot;
# RCFG Onetime-Negative
cfg_type = &quot;initialize&quot;
stream = StreamDiffusion(
    pipe,
    [32, 45],
    torch_dtype=torch.float16,
    cfg_type=cfg_type,
)
stream.prepare(
    prompt=&quot;1girl, purple hair&quot;,
    guidance_scale=guidance_scale,
    delta=delta,
)"><pre><span># w/0 CFG</span>
<span>cfg_type</span> <span>=</span> <span>"none"</span>
<span># CFG</span>
<span>cfg_type</span> <span>=</span> <span>"full"</span>
<span># RCFG Self-Negative</span>
<span>cfg_type</span> <span>=</span> <span>"self"</span>
<span># RCFG Onetime-Negative</span>
<span>cfg_type</span> <span>=</span> <span>"initialize"</span>
<span>stream</span> <span>=</span> <span>StreamDiffusion</span>(
    <span>pipe</span>,
    [<span>32</span>, <span>45</span>],
    <span>torch_dtype</span><span>=</span><span>torch</span>.<span>float16</span>,
    <span>cfg_type</span><span>=</span><span>cfg_type</span>,
)
<span>stream</span>.<span>prepare</span>(
    <span>prompt</span><span>=</span><span>"1girl, purple hair"</span>,
    <span>guidance_scale</span><span>=</span><span>guidance_scale</span>,
    <span>delta</span><span>=</span><span>delta</span>,
)</pre></div>
<p dir="auto">The delta has a moderating effect on the effectiveness of RCFG.</p>
<h2 tabindex="-1" dir="auto">Development Team</h2>
<p dir="auto"><a href="https://twitter.com/cumulo_autumn" rel="nofollow">Aki</a>,
<a href="https://twitter.com/AttaQjp" rel="nofollow">Ararat</a>,
<a href="https://twitter.com/Chenfeng_X" rel="nofollow">Chenfeng Xu</a>,
<a href="https://twitter.com/ddPn08" rel="nofollow">ddPn08</a>,
<a href="https://twitter.com/ArtengMimi" rel="nofollow">kizamimi</a>,
<a href="https://twitter.com/__ramu0e__" rel="nofollow">ramune</a>,
<a href="https://twitter.com/hanyingcl" rel="nofollow">teftef</a>,
<a href="https://twitter.com/toni_nimono" rel="nofollow">Tonimono</a>,
<a href="https://twitter.com/IMG_5955" rel="nofollow">Verb</a>,</p>
<p dir="auto">(*alphabetical order)
<br></p>
<h2 tabindex="-1" dir="auto">Acknowledgements</h2>
<p dir="auto">The video and image demos in this GitHub repository were generated using <a href="https://huggingface.co/latent-consistency/lcm-lora-sdv1-5" rel="nofollow">LCM-LoRA</a> + <a href="https://civitai.com/models/136268/kohaku-v2" rel="nofollow">KohakuV2</a> and <a href="https://arxiv.org/abs/2311.17042" rel="nofollow">SD-Turbo</a>.</p>
<p dir="auto">Special thanks to <a href="https://latent-consistency-models.github.io/" rel="nofollow">LCM-LoRA authors</a> for providing the LCM-LoRA and Kohaku BlueLeaf (<a href="https://twitter.com/KBlueleaf" rel="nofollow">@KBlueleaf</a>) for providing the KohakuV2 model and ,to <a href="https://ja.stability.ai/" rel="nofollow">Stability AI</a> for <a href="https://arxiv.org/abs/2311.17042" rel="nofollow">SD-Turbo</a>.</p>
<p dir="auto">KohakuV2 Models can be downloaded from  <a href="https://civitai.com/models/136268/kohaku-v2" rel="nofollow">Civitai</a>  and <a href="https://huggingface.co/KBlueLeaf/kohaku-v2.1" rel="nofollow">Hugging Face</a>.</p>
<p dir="auto">SD-Turbo is also available on <a href="https://huggingface.co/stabilityai/sd-turbo" rel="nofollow">Hugging Face Space</a>.</p>
<h2 tabindex="-1" dir="auto">Contributors</h2>
<a href="https://github.com/cumulo-autumn/StreamDiffusion/graphs/contributors">
  <img src="https://camo.githubusercontent.com/850d0f07f10bcc27c428009a76e9837c0546b93021c6caee911f4758fc440e7a/68747470733a2f2f636f6e747269622e726f636b732f696d6167653f7265706f3d63756d756c6f2d617574756d6e2f53747265616d446966667573696f6e" data-canonical-src="https://contrib.rocks/image?repo=cumulo-autumn/StreamDiffusion">
</a>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Crown shyness (302 pts)]]></title>
            <link>https://en.wikipedia.org/wiki/Crown_shyness</link>
            <guid>38749170</guid>
            <pubDate>Sat, 23 Dec 2023 23:00:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://en.wikipedia.org/wiki/Crown_shyness">https://en.wikipedia.org/wiki/Crown_shyness</a>, See on <a href="https://news.ycombinator.com/item?id=38749170">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
							

						<p>From Wikipedia, the free encyclopedia</p>
					</div><div lang="en" dir="ltr" id="mw-content-text">

<figure typeof="mw:File/Thumb"><a href="https://en.wikipedia.org/wiki/File:Dryobalanops_Aromatica_canopy.jpg"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/5d/Dryobalanops_Aromatica_canopy.jpg/300px-Dryobalanops_Aromatica_canopy.jpg" decoding="async" width="300" height="225" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/5/5d/Dryobalanops_Aromatica_canopy.jpg/450px-Dryobalanops_Aromatica_canopy.jpg 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/5/5d/Dryobalanops_Aromatica_canopy.jpg/600px-Dryobalanops_Aromatica_canopy.jpg 2x" data-file-width="1020" data-file-height="765"></a><figcaption>Canopy of <i><a href="https://en.wikipedia.org/wiki/Dryobalanops_aromatica" title="Dryobalanops aromatica">D. aromatica</a></i> at the <a href="https://en.wikipedia.org/wiki/Forest_Research_Institute_Malaysia" title="Forest Research Institute Malaysia">Forest Research Institute Malaysia</a> displaying crown shyness</figcaption></figure>
<figure typeof="mw:File/Thumb"><a href="https://en.wikipedia.org/wiki/File:River_of_Blue.jpg"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/5a/River_of_Blue.jpg/300px-River_of_Blue.jpg" decoding="async" width="300" height="200" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/5/5a/River_of_Blue.jpg/450px-River_of_Blue.jpg 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/5/5a/River_of_Blue.jpg/600px-River_of_Blue.jpg 2x" data-file-width="2048" data-file-height="1365"></a><figcaption>Trees at <a href="https://en.wikipedia.org/wiki/Plaza_San_Mart%C3%ADn_(Buenos_Aires)" title="Plaza San Martín (Buenos Aires)">Plaza San Martín (Buenos Aires)</a>, Argentina</figcaption></figure>
<p><b>Crown shyness</b> (also <i>canopy disengagement</i>,<sup id="cite_ref-JWG_2008_1-0"><a href="#cite_note-JWG_2008-1">[1]</a></sup> <i>canopy shyness</i>,<sup id="cite_ref-2"><a href="#cite_note-2">[2]</a></sup> or <i>inter-crown spacing</i><sup id="cite_ref-FEP_1984_3-0"><a href="#cite_note-FEP_1984-3">[3]</a></sup>) is a feature observed in some tree species, in which the <a href="https://en.wikipedia.org/wiki/Crown_(botany)" title="Crown (botany)">crowns</a> of fully stocked trees do not touch each other, instead forming a <a href="https://en.wikipedia.org/wiki/Canopy_(biology)" title="Canopy (biology)">canopy</a> with channel-like gaps.<sup id="cite_ref-4"><a href="#cite_note-4">[4]</a></sup><sup id="cite_ref-5"><a href="#cite_note-5">[5]</a></sup>
This is most prevalent among trees of the same species, but also occurs between trees of different species.<sup id="cite_ref-AJR_1988_6-0"><a href="#cite_note-AJR_1988-6">[6]</a></sup><sup id="cite_ref-KP_1973_7-0"><a href="#cite_note-KP_1973-7">[7]</a></sup> There exist many hypotheses as to why crown shyness is an <a href="https://en.wikipedia.org/wiki/Adaptive_behavior" title="Adaptive behavior">adaptive behavior</a>, and research suggests that it might inhibit spread of <a href="https://en.wikipedia.org/wiki/Leaf_miner" title="Leaf miner">leaf-eating</a> insect <a href="https://en.wikipedia.org/wiki/Larvae" title="Larvae">larvae</a>.<sup id="cite_ref-8"><a href="#cite_note-8">[8]</a></sup>
</p>
<meta property="mw:PageProp/toc">
<h2><span id="Possible_physiological_explanations">Possible physiological explanations</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Crown_shyness&amp;action=edit&amp;section=1" title="Edit section: Possible physiological explanations"><span>edit</span></a><span>]</span></span></h2>
<p>The exact physiological basis of crown shyness is uncertain.<sup id="cite_ref-AJR_1988_6-1"><a href="#cite_note-AJR_1988-6">[6]</a></sup> It has been discussed in scientific literature since the 1920s.<sup id="cite_ref-9"><a href="#cite_note-9">[9]</a></sup> The variety of hypotheses and experimental results might suggest that there are multiple mechanisms across different species, an example of <a href="https://en.wikipedia.org/wiki/Convergent_evolution" title="Convergent evolution">convergent evolution</a>.<sup>[<i><a href="https://en.wikipedia.org/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (February 2023)">citation needed</span></a></i>]</sup>
</p><p>Some hypotheses contend that the interdigitation of canopy branches leads to "reciprocal pruning" of adjacent trees. Trees in windy areas suffer physical damage as they collide with each other during <a href="https://en.wikipedia.org/wiki/Wind" title="Wind">winds</a>. The abrasions and collisions induce a crown shyness response. Studies suggest that lateral branch growth is largely uninfluenced by neighbours until disturbed by mechanical abrasion.<sup id="cite_ref-10"><a href="#cite_note-10">[10]</a></sup> If the crowns are artificially prevented from colliding in the winds, they gradually fill the canopy gaps.<sup id="cite_ref-11"><a href="#cite_note-11">[11]</a></sup> This  explains instances of crown shyness between branches of the same organism. Proponents of this idea cite that shyness is particularly seen in conditions conducive to this pruning, including windy forests, stands of flexible trees, and early succession forests where branches are flexible and limited in lateral movement.<sup id="cite_ref-AJR_1988_6-2"><a href="#cite_note-AJR_1988-6">[6]</a></sup><sup id="cite_ref-ROL_1980_12-0"><a href="#cite_note-ROL_1980-12">[12]</a></sup> According to this theory, variable flexibility in lateral branches greatly influences the degree of crown shyness.
</p><p>Similarly, some research suggests that constant abrasion at growth nodules disrupts bud tissue such that it is unable to continue with lateral growth. Australian forester M.R. Jacobs, who studied the crown shyness patterns in <a href="https://en.wikipedia.org/wiki/Eucalyptus" title="Eucalyptus">eucalyptus</a> in 1955, believed that the trees' growing tips were sensitive to abrasion, resulting in canopy gaps.<sup id="cite_ref-13"><a href="#cite_note-13">[13]</a></sup> Miguel Franco (1986) observed that the branches of <i><a href="https://en.wikipedia.org/wiki/Picea_sitchensis" title="Picea sitchensis">Picea sitchensis</a></i> (Sitka spruce) and <i><a href="https://en.wikipedia.org/wiki/Larix_kaempferi" title="Larix kaempferi">Larix kaempferi</a></i> (Japanese larch) suffered physical damage due to abrasion, which killed the leading shoots.<sup id="cite_ref-14"><a href="#cite_note-14">[14]</a></sup><sup id="cite_ref-15"><a href="#cite_note-15">[15]</a></sup>
</p><p>A prominent hypothesis is that canopy shyness has to do with mutual light sensing by adjacent plants. The photoreceptor-mediated <a href="https://en.wikipedia.org/wiki/Shade_avoidance" title="Shade avoidance">shade avoidance</a> response is a well-documented behavior in a variety of plant species.<sup id="cite_ref-16"><a href="#cite_note-16">[16]</a></sup> Neighbor detection is thought to be a function of several unique photoreceptors. Plants can sense the proximity of neighbors by sensing backscattered <a href="https://en.wikipedia.org/wiki/Far-red" title="Far-red">far-red</a> light, a task widely thought to be accomplished by the activity of the <a href="https://en.wikipedia.org/wiki/Phytochrome" title="Phytochrome">phytochrome</a> photoreceptors.<sup id="cite_ref-17"><a href="#cite_note-17">[17]</a></sup> Many species of plant respond to an increase in far-red light (and, by extension, encroaching neighbors) by directing growth away from the far-red stimulus and by increasing the rate of elongation.<sup id="cite_ref-18"><a href="#cite_note-18">[18]</a></sup> Similarly, plants use blue light to induce the shade-avoidance response, likely playing a role in the recognition of neighboring plants,<sup id="cite_ref-19"><a href="#cite_note-19">[19]</a></sup> though this was just starting to be recognised in 1988.<sup id="cite_ref-20"><a href="#cite_note-20">[20]</a></sup>
</p><p>The characterization of these behaviors might suggest that crown shyness is simply the result of mutual shading based on well-understood shade avoidance responses.<sup id="cite_ref-AJR_1988_6-3"><a href="#cite_note-AJR_1988-6">[6]</a></sup><sup id="cite_ref-F.S.P._Ng_1997_34–37_21-0"><a href="#cite_note-F.S.P._Ng_1997_34–37-21">[21]</a></sup> Malaysian scholar <a href="https://en.wikipedia.org/wiki/Francis_S.P._Ng" title="Francis S.P. Ng">Francis S.P. Ng</a>, who studied <i><a href="https://en.wikipedia.org/wiki/Dryobalanops_aromatica" title="Dryobalanops aromatica">Dryobalanops aromatica</a></i>, suggested that the growing tips were sensitive to light levels and stopped growing when nearing the adjacent foliage due to the induced shade.<sup id="cite_ref-AJR_1988_6-4"><a href="#cite_note-AJR_1988-6">[6]</a></sup><sup id="cite_ref-F.S.P._Ng_1997_34–37_21-1"><a href="#cite_note-F.S.P._Ng_1997_34–37-21">[21]</a></sup>
</p><p>A 2015 study has suggested that <i><a href="https://en.wikipedia.org/wiki/Arabidopsis_thaliana" title="Arabidopsis thaliana">Arabidopsis thaliana</a></i> shows different leaf placement strategies when grown amongst kin and unrelated conspecifics, shading dissimilar neighbors and avoiding kin. This response was shown to be contingent on the proper functioning of multiple photosensory modalities.<sup id="cite_ref-22"><a href="#cite_note-22">[22]</a></sup> A 1998 study proposed similar systems of photoreceptor-mediated inhibition of growth as explanations of crown shyness,<sup id="cite_ref-AJR_1988_6-5"><a href="#cite_note-AJR_1988-6">[6]</a></sup><sup id="cite_ref-F.S.P._Ng_1997_34–37_21-2"><a href="#cite_note-F.S.P._Ng_1997_34–37-21">[21]</a></sup> though a causal link between photoreceptors and crown asymmetry had yet to be experimentally proven. This might explain instances of intercrown spacing that are only exhibited between conspecifics.<sup id="cite_ref-AJR_1988_6-6"><a href="#cite_note-AJR_1988-6">[6]</a></sup><sup id="cite_ref-KP_1973_7-1"><a href="#cite_note-KP_1973-7">[7]</a></sup>
</p>
<h2><span id="Species">Species</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Crown_shyness&amp;action=edit&amp;section=2" title="Edit section: Species"><span>edit</span></a><span>]</span></span></h2>
<p>Trees that display crown shyness patterns include:
</p>
<ul><li>Species of <i><a href="https://en.wikipedia.org/wiki/Dryobalanops" title="Dryobalanops">Dryobalanops</a></i>, including <i><a href="https://en.wikipedia.org/wiki/Dryobalanops_lanceolata" title="Dryobalanops lanceolata">Dryobalanops lanceolata</a></i><sup id="cite_ref-23"><a href="#cite_note-23">[23]</a></sup> and <i><a href="https://en.wikipedia.org/wiki/Dryobalanops_aromatica" title="Dryobalanops aromatica">Dryobalanops aromatica</a></i> (kapur)</li>
<li>Some species of <a href="https://en.wikipedia.org/wiki/Eucalypt" title="Eucalypt">eucalypt</a><sup id="cite_ref-RGF_2004_24-0"><a href="#cite_note-RGF_2004-24">[24]</a></sup></li>
<li><i><a href="https://en.wikipedia.org/wiki/Pinus_contorta" title="Pinus contorta">Pinus contorta</a></i> or lodgepole pine<sup id="cite_ref-JWG_2008_1-1"><a href="#cite_note-JWG_2008-1">[1]</a></sup></li>
<li><i><a href="https://en.wikipedia.org/wiki/Avicennia_germinans" title="Avicennia germinans">Avicennia germinans</a></i> or black mangrove<sup id="cite_ref-FEP_1984_3-1"><a href="#cite_note-FEP_1984-3">[3]</a></sup></li>
<li><i><a href="https://en.wikipedia.org/wiki/Schefflera" title="Schefflera">Schefflera</a> pittieri</i><sup id="cite_ref-AJR_1988_6-7"><a href="#cite_note-AJR_1988-6">[6]</a></sup><sup id="cite_ref-ROL_1980_12-1"><a href="#cite_note-ROL_1980-12">[12]</a></sup></li>
<li><i><a href="https://en.wikipedia.org/wiki/Clusia_alata" title="Clusia alata">Clusia alata</a></i><sup id="cite_ref-AJR_1988_6-8"><a href="#cite_note-AJR_1988-6">[6]</a></sup><sup id="cite_ref-ROL_1980_12-2"><a href="#cite_note-ROL_1980-12">[12]</a></sup></li>
<li>K. Paijmans observed crown shyness in a multi-species group of trees, comprising <i><a href="https://en.wikipedia.org/wiki/Celtis_spinosa" title="Celtis spinosa">Celtis spinosa</a></i> and <i><a href="https://en.wikipedia.org/w/index.php?title=Pterocymbium_beccarii&amp;action=edit&amp;redlink=1" title="Pterocymbium beccarii (page does not exist)">Pterocymbium beccarii</a></i><sup id="cite_ref-KP_1973_7-2"><a href="#cite_note-KP_1973-7">[7]</a></sup></li></ul>
<h2><span id="References">References</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Crown_shyness&amp;action=edit&amp;section=3" title="Edit section: References"><span>edit</span></a><span>]</span></span></h2>
<div>
<ol>
<li id="cite_note-JWG_2008-1"><span>^ <a href="#cite_ref-JWG_2008_1-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-JWG_2008_1-1"><sup><i><b>b</b></i></sup></a></span> <span><cite id="CITEREFGoudiePolssonOtt2008">Goudie, James W.; Polsson, Kenneth R.; Ott, Peter K. (2008). <a rel="nofollow" href="https://books.google.com/books?id=FB3YaZKPoi4C&amp;pg=PA39">"An empirical model of crown shyness for lodgepole pine (Pinus contorta var. latifolia [Engl.] Critch.) in British Columbia"</a>. <i>Forest Ecology and Management</i>. <b>257</b> (1): 321–331. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1016%2Fj.foreco.2008.09.005">10.1016/j.foreco.2008.09.005</a>. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/9781437926163" title="Special:BookSources/9781437926163"><bdi>9781437926163</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Forest+Ecology+and+Management&amp;rft.atitle=An+empirical+model+of+crown+shyness+for+lodgepole+pine+%28Pinus+contorta+var.+latifolia+%5BEngl.%5D+Critch.%29+in+British+Columbia&amp;rft.volume=257&amp;rft.issue=1&amp;rft.pages=321-331&amp;rft.date=2008&amp;rft_id=info%3Adoi%2F10.1016%2Fj.foreco.2008.09.005&amp;rft.isbn=9781437926163&amp;rft.aulast=Goudie&amp;rft.aufirst=James+W.&amp;rft.au=Polsson%2C+Kenneth+R.&amp;rft.au=Ott%2C+Peter+K.&amp;rft_id=https%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3DFB3YaZKPoi4C%26pg%3DPA39&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-2"><span><b><a href="#cite_ref-2">^</a></b></span> <span><cite id="CITEREFPeter_ThomasJohn_Packham2007">Peter Thomas; John Packham (26 July 2007). <a rel="nofollow" href="https://books.google.com/books?id=0Ntvos9aaC8C&amp;pg=PA12"><i>Ecology of Woodlands and Forests: Description, Dynamics and Diversity</i></a>. Cambridge University Press. p.&nbsp;12. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-0-521-83452-0" title="Special:BookSources/978-0-521-83452-0"><bdi>978-0-521-83452-0</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Ecology+of+Woodlands+and+Forests%3A+Description%2C+Dynamics+and+Diversity&amp;rft.pages=12&amp;rft.pub=Cambridge+University+Press&amp;rft.date=2007-07-26&amp;rft.isbn=978-0-521-83452-0&amp;rft.au=Peter+Thomas&amp;rft.au=John+Packham&amp;rft_id=https%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3D0Ntvos9aaC8C%26pg%3DPA12&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-FEP_1984-3"><span>^ <a href="#cite_ref-FEP_1984_3-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-FEP_1984_3-1"><sup><i><b>b</b></i></sup></a></span> <span><cite id="CITEREFPutzParkerArchibald1984">Putz, Francis E.; Parker, Geoffrey G.; Archibald, Ruth M. (1984). <a rel="nofollow" href="https://repository.si.edu/bitstream/handle/10088/17792/serc_Putz_etal_1984_AmMidlNat_112_24_28.pdf">"Mechanical Abrasion and Intercrown Spacing"</a> <span>(PDF)</span>. <i>American Midland Naturalist</i>. <b>112</b> (1): 24–28. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.2307%2F2425452">10.2307/2425452</a>. <a href="https://en.wikipedia.org/wiki/JSTOR_(identifier)" title="JSTOR (identifier)">JSTOR</a>&nbsp;<a rel="nofollow" href="https://www.jstor.org/stable/2425452">2425452</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=American+Midland+Naturalist&amp;rft.atitle=Mechanical+Abrasion+and+Intercrown+Spacing&amp;rft.volume=112&amp;rft.issue=1&amp;rft.pages=24-28&amp;rft.date=1984&amp;rft_id=info%3Adoi%2F10.2307%2F2425452&amp;rft_id=https%3A%2F%2Fwww.jstor.org%2Fstable%2F2425452%23id-name%3DJSTOR&amp;rft.aulast=Putz&amp;rft.aufirst=Francis+E.&amp;rft.au=Parker%2C+Geoffrey+G.&amp;rft.au=Archibald%2C+Ruth+M.&amp;rft_id=https%3A%2F%2Frepository.si.edu%2Fbitstream%2Fhandle%2F10088%2F17792%2Fserc_Putz_etal_1984_AmMidlNat_112_24_28.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-4"><span><b><a href="#cite_ref-4">^</a></b></span> <span><cite id="CITEREFNorsiha_A._and_Shamsudin2015">Norsiha A. and Shamsudin (2015-04-25). <a rel="nofollow" href="http://www.frim.gov.my/shorea-resinosa-another-jigsaw-puzzle-in-the-sky/">"Shorea resinosa&nbsp;: Another jigsaw puzzle in the sky"</a>. Forest Research Institute Malaysia.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Shorea+resinosa+%3A+Another+jigsaw+puzzle+in+the+sky&amp;rft.pub=Forest+Research+Institute+Malaysia&amp;rft.date=2015-04-25&amp;rft.au=Norsiha+A.+and+Shamsudin&amp;rft_id=http%3A%2F%2Fwww.frim.gov.my%2Fshorea-resinosa-another-jigsaw-puzzle-in-the-sky%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-5"><span><b><a href="#cite_ref-5">^</a></b></span> <span><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Canadian+Journal+of+Forest+Research&amp;rft.atitle=Crown+shyness+in+lodgepole+pine+stands+of+varying+stand+height%2C+density+and+site+index+in+the+upper+foothills+of+Alberta&amp;rft.volume=36&amp;rft.issue=9&amp;rft.pages=2104-2111&amp;rft.date=2006&amp;rft_id=info%3Adoi%2F10.1139%2Fx06-107&amp;rft.aulast=Fish&amp;rft.aufirst=H&amp;rft.au=Lieffers%2C+VJ&amp;rft.au=Silins%2C+U&amp;rft.au=Hall%2C+RJ&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-AJR_1988-6"><span>^ <a href="#cite_ref-AJR_1988_6-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-AJR_1988_6-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-AJR_1988_6-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-AJR_1988_6-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-AJR_1988_6-4"><sup><i><b>e</b></i></sup></a> <a href="#cite_ref-AJR_1988_6-5"><sup><i><b>f</b></i></sup></a> <a href="#cite_ref-AJR_1988_6-6"><sup><i><b>g</b></i></sup></a> <a href="#cite_ref-AJR_1988_6-7"><sup><i><b>h</b></i></sup></a> <a href="#cite_ref-AJR_1988_6-8"><sup><i><b>i</b></i></sup></a></span> <span><cite id="CITEREFRebertus1988">Rebertus, Alan J (1988). <a rel="nofollow" href="ftp://169.158.189.34/pub/Biotropica/1980s/1988/20-4/Biotropica-1988-20-4-p338.pdf">"Crown shyness in a tropical cloud forest"</a> <span>(PDF)</span>. <i>Biotropica</i>. <b>20</b> (4): 338–339. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.2307%2F2388326">10.2307/2388326</a>. <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/issn/0006-3606">0006-3606</a>. <a href="https://en.wikipedia.org/wiki/JSTOR_(identifier)" title="JSTOR (identifier)">JSTOR</a>&nbsp;<a rel="nofollow" href="https://www.jstor.org/stable/2388326">2388326</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Biotropica&amp;rft.atitle=Crown+shyness+in+a+tropical+cloud+forest&amp;rft.volume=20&amp;rft.issue=4&amp;rft.pages=338-339&amp;rft.date=1988&amp;rft.issn=0006-3606&amp;rft_id=https%3A%2F%2Fwww.jstor.org%2Fstable%2F2388326%23id-name%3DJSTOR&amp;rft_id=info%3Adoi%2F10.2307%2F2388326&amp;rft.aulast=Rebertus&amp;rft.aufirst=Alan+J&amp;rft_id=ftp%3A%2F%2F169.158.189.34%2Fpub%2FBiotropica%2F1980s%2F1988%2F20-4%2FBiotropica-1988-20-4-p338.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span><sup><span>[<i><a href="https://en.wikipedia.org/wiki/Wikipedia:Link_rot" title="Wikipedia:Link rot"><span title="&nbsp;Dead link tagged November 2019">permanent dead link</span></a></i>]</span></sup></span>
</li>
<li id="cite_note-KP_1973-7"><span>^ <a href="#cite_ref-KP_1973_7-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-KP_1973_7-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-KP_1973_7-2"><sup><i><b>c</b></i></sup></a></span> <span><cite id="CITEREFK._Paijmans1973">K. Paijmans (1973). <a rel="nofollow" href="http://scholarspace.manoa.hawaii.edu/bitstream/handle/10125/802/v27n3-260-268.pdf">"Plant Succession on Pago and Witori Volcanoes, New Britain"</a> <span>(PDF)</span>. <i>Pacific Science</i>. University of Hawaii Press. <b>27</b> (3): 60–268. <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/issn/0030-8870">0030-8870</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Pacific+Science&amp;rft.atitle=Plant+Succession+on+Pago+and+Witori+Volcanoes%2C+New+Britain&amp;rft.volume=27&amp;rft.issue=3&amp;rft.pages=60-268&amp;rft.date=1973&amp;rft.issn=0030-8870&amp;rft.au=K.+Paijmans&amp;rft_id=http%3A%2F%2Fscholarspace.manoa.hawaii.edu%2Fbitstream%2Fhandle%2F10125%2F802%2Fv27n3-260-268.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-8"><span><b><a href="#cite_ref-8">^</a></b></span> <span><cite><a rel="nofollow" href="http://www.zoo.org/document.doc?id=1110">"Tropical Rain Forest"</a>. Woodland Park Zoo. p.&nbsp;37.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Tropical+Rain+Forest&amp;rft.pages=37&amp;rft.pub=Woodland+Park+Zoo&amp;rft_id=http%3A%2F%2Fwww.zoo.org%2Fdocument.doc%3Fid%3D1110&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-9"><span><b><a href="#cite_ref-9">^</a></b></span> <span><cite><a rel="nofollow" href="https://www.for.gov.bc.ca/hfd/library/FIA/2008/FSP_Y083088.pdf">"TASS III: Simulating the management, growth and yield of complex stands"</a> <span>(PDF)</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=TASS+III%3A+Simulating+the+management%2C+growth+and+yield+of+complex+stands&amp;rft_id=https%3A%2F%2Fwww.for.gov.bc.ca%2Fhfd%2Flibrary%2FFIA%2F2008%2FFSP_Y083088.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-10"><span><b><a href="#cite_ref-10">^</a></b></span> <span><cite id="CITEREFFranco1986">Franco, M (14 August 1986). "The influences of neighbours on the growth of modular organisms with an example from trees". <i>Philosophical Transactions of the Royal Society of London. B, Biological Sciences</i>. <b>313</b> (1159): 313, 209–225. <a href="https://en.wikipedia.org/wiki/Bibcode_(identifier)" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" href="https://ui.adsabs.harvard.edu/abs/1986RSPTB.313..209F">1986RSPTB.313..209F</a>. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1098%2Frstb.1986.0034">10.1098/rstb.1986.0034</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Philosophical+Transactions+of+the+Royal+Society+of+London.+B%2C+Biological+Sciences&amp;rft.atitle=The+influences+of+neighbours+on+the+growth+of+modular+organisms+with+an+example+from+trees&amp;rft.volume=313&amp;rft.issue=1159&amp;rft.pages=313%2C+209-225&amp;rft.date=1986-08-14&amp;rft_id=info%3Adoi%2F10.1098%2Frstb.1986.0034&amp;rft_id=info%3Abibcode%2F1986RSPTB.313..209F&amp;rft.aulast=Franco&amp;rft.aufirst=M&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-11"><span><b><a href="#cite_ref-11">^</a></b></span> <span><cite id="CITEREFVictor_Lieffers">Victor Lieffers. <a rel="nofollow" href="https://web.archive.org/web/20150925144427/http://www.sfmn.ales.ualberta.ca/en/Publications/~/media/sfmn/Publications/ResearchNotes/Documents/RN_E36_CrownShyness_low.ashx">"Crown shyness in maturing boreal forest stands"</a>. <i>SFM Network Research Note Series</i>. <b>36</b>. <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/issn/1715-0981">1715-0981</a>. Archived from <a rel="nofollow" href="http://www.sfmn.ales.ualberta.ca/en/Publications/~/media/sfmn/Publications/ResearchNotes/Documents/RN_E36_CrownShyness_low.ashx">the original</a> on 2015-09-25<span>. Retrieved <span>2015-08-23</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=SFM+Network+Research+Note+Series&amp;rft.atitle=Crown+shyness+in+maturing+boreal+forest+stands&amp;rft.volume=36&amp;rft.issn=1715-0981&amp;rft.au=Victor+Lieffers&amp;rft_id=http%3A%2F%2Fwww.sfmn.ales.ualberta.ca%2Fen%2FPublications%2F~%2Fmedia%2Fsfmn%2FPublications%2FResearchNotes%2FDocuments%2FRN_E36_CrownShyness_low.ashx&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-ROL_1980-12"><span>^ <a href="#cite_ref-ROL_1980_12-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-ROL_1980_12-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-ROL_1980_12-2"><sup><i><b>c</b></i></sup></a></span> <span><cite id="CITEREFLawtonPutz">Lawton, RO; Putz, Francis E. "The vegetation of the Monteverde Cloud Forest Reserve". <i>Brenesia</i>. <b>18</b>: 101–116.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Brenesia&amp;rft.atitle=The+vegetation+of+the+Monteverde+Cloud+Forest+Reserve&amp;rft.volume=18&amp;rft.pages=101-116&amp;rft.aulast=Lawton&amp;rft.aufirst=RO&amp;rft.au=Putz%2C+Francis+E&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-13"><span><b><a href="#cite_ref-13">^</a></b></span> <span><cite id="CITEREFMaxwell_Ralph_Jacobs1955">Maxwell Ralph Jacobs (1955). <a rel="nofollow" href="http://trove.nla.gov.au/version/28637821"><i>Growth Habits of the Eucalypts</i></a>. Forestry and Timber Bureau.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Growth+Habits+of+the+Eucalypts&amp;rft.pub=Forestry+and+Timber+Bureau&amp;rft.date=1955&amp;rft.au=Maxwell+Ralph+Jacobs&amp;rft_id=http%3A%2F%2Ftrove.nla.gov.au%2Fversion%2F28637821&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-14"><span><b><a href="#cite_ref-14">^</a></b></span> <span><cite id="CITEREFM._Franco1986">M. Franco (14 August 1986). "The Influences of Neighbours on the Growth of Modular Organisms with an Example from Trees". <i>Philosophical Transactions of the Royal Society B</i>. <b>313</b> (1159): 209–225. <a href="https://en.wikipedia.org/wiki/Bibcode_(identifier)" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" href="https://ui.adsabs.harvard.edu/abs/1986RSPTB.313..209F">1986RSPTB.313..209F</a>. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1098%2Frstb.1986.0034">10.1098/rstb.1986.0034</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Philosophical+Transactions+of+the+Royal+Society+B&amp;rft.atitle=The+Influences+of+Neighbours+on+the+Growth+of+Modular+Organisms+with+an+Example+from+Trees&amp;rft.volume=313&amp;rft.issue=1159&amp;rft.pages=209-225&amp;rft.date=1986-08-14&amp;rft_id=info%3Adoi%2F10.1098%2Frstb.1986.0034&amp;rft_id=info%3Abibcode%2F1986RSPTB.313..209F&amp;rft.au=M.+Franco&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-15"><span><b><a href="#cite_ref-15">^</a></b></span> <span><cite id="CITEREFWilsonAgnewRoxburgh2019">Wilson, J. Bastow; Agnew, Andrew D.Q.; Roxburgh, Stephen H. (2019). <a rel="nofollow" href="https://www.cambridge.org/core/books/abs/nature-of-plant-communities/interactions-between-species/5B1A7C7580CBFC055BC5FEDB05724301">"2: Interactions between Species"</a>. <i>The Nature of Plant Communities</i>. Cambridge: Cambridge University Press. pp.&nbsp;24–65. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1017%2F9781108612265.004">10.1017/9781108612265.004</a>. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/9781108612265" title="Special:BookSources/9781108612265"><bdi>9781108612265</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=2%3A+Interactions+between+Species&amp;rft.btitle=The+Nature+of+Plant+Communities&amp;rft.pages=24-65&amp;rft.pub=Cambridge%3A+Cambridge+University+Press&amp;rft.date=2019&amp;rft_id=info%3Adoi%2F10.1017%2F9781108612265.004&amp;rft.isbn=9781108612265&amp;rft.aulast=Wilson&amp;rft.aufirst=J.+Bastow&amp;rft.au=Agnew%2C+Andrew+D.Q.&amp;rft.au=Roxburgh%2C+Stephen+H.&amp;rft_id=https%3A%2F%2Fwww.cambridge.org%2Fcore%2Fbooks%2Fabs%2Fnature-of-plant-communities%2Finteractions-between-species%2F5B1A7C7580CBFC055BC5FEDB05724301&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-16"><span><b><a href="#cite_ref-16">^</a></b></span> <span><cite id="CITEREFBallaréScopelSánchez1990">Ballaré, CL; Scopel, AL; Sánchez, RA (19 January 1990). "Far-red radiation reflected from adjacent leaves: an early signal of competition in plant canopies". <i>Science</i>. <b>247</b> (4940): 329–32. <a href="https://en.wikipedia.org/wiki/Bibcode_(identifier)" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" href="https://ui.adsabs.harvard.edu/abs/1990Sci...247..329B">1990Sci...247..329B</a>. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1126%2Fscience.247.4940.329">10.1126/science.247.4940.329</a>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a>&nbsp;<a rel="nofollow" href="https://pubmed.ncbi.nlm.nih.gov/17735851">17735851</a>. <a href="https://en.wikipedia.org/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a>&nbsp;<a rel="nofollow" href="https://api.semanticscholar.org/CorpusID:39622241">39622241</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Science&amp;rft.atitle=Far-red+radiation+reflected+from+adjacent+leaves%3A+an+early+signal+of+competition+in+plant+canopies&amp;rft.volume=247&amp;rft.issue=4940&amp;rft.pages=329-32&amp;rft.date=1990-01-19&amp;rft_id=info%3Adoi%2F10.1126%2Fscience.247.4940.329&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A39622241%23id-name%3DS2CID&amp;rft_id=info%3Apmid%2F17735851&amp;rft_id=info%3Abibcode%2F1990Sci...247..329B&amp;rft.aulast=Ballar%C3%A9&amp;rft.aufirst=CL&amp;rft.au=Scopel%2C+AL&amp;rft.au=S%C3%A1nchez%2C+RA&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-17"><span><b><a href="#cite_ref-17">^</a></b></span> <span><cite id="CITEREFBallareSanchezScopelCasal1987">Ballare, C. L.; Sanchez, R. A.; Scopel, Ana L.; Casal, J. J.; Ghersa, C. M. (September 1987). "Early detection of neighbour plants by phytochrome perception of spectral changes in reflected sunlight". <i>Plant, Cell and Environment</i>. <b>10</b> (7): 551–557. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1111%2F1365-3040.ep11604091">10.1111/1365-3040.ep11604091</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Plant%2C+Cell+and+Environment&amp;rft.atitle=Early+detection+of+neighbour+plants+by+phytochrome+perception+of+spectral+changes+in+reflected+sunlight.&amp;rft.volume=10&amp;rft.issue=7&amp;rft.pages=551-557&amp;rft.date=1987-09&amp;rft_id=info%3Adoi%2F10.1111%2F1365-3040.ep11604091&amp;rft.aulast=Ballare&amp;rft.aufirst=C.+L.&amp;rft.au=Sanchez%2C+R.+A.&amp;rft.au=Scopel%2C+Ana+L.&amp;rft.au=Casal%2C+J.+J.&amp;rft.au=Ghersa%2C+C.+M.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-18"><span><b><a href="#cite_ref-18">^</a></b></span> <span><cite id="CITEREFBallaréScopelSánchez1997">Ballaré, CL; Scopel, AL; Sánchez, RA (June 1997). <a rel="nofollow" href="https://doi.org/10.1046%2Fj.1365-3040.1997.d01-112.x">"Foraging for light: photosensory ecology and agricultural implications"</a>. <i>Plant, Cell and Environment</i>. <b>20</b> (6): 820–825. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<span title="Freely accessible"><a rel="nofollow" href="https://doi.org/10.1046%2Fj.1365-3040.1997.d01-112.x">10.1046/j.1365-3040.1997.d01-112.x</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Plant%2C+Cell+and+Environment&amp;rft.atitle=Foraging+for+light%3A+photosensory+ecology+and+agricultural+implications&amp;rft.volume=20&amp;rft.issue=6&amp;rft.pages=820-825&amp;rft.date=1997-06&amp;rft_id=info%3Adoi%2F10.1046%2Fj.1365-3040.1997.d01-112.x&amp;rft.aulast=Ballar%C3%A9&amp;rft.aufirst=CL&amp;rft.au=Scopel%2C+AL&amp;rft.au=S%C3%A1nchez%2C+RA&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1046%252Fj.1365-3040.1997.d01-112.x&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-19"><span><b><a href="#cite_ref-19">^</a></b></span> <span><cite id="CITEREFJansenGabaGreenberg1998">Jansen, Marcel AK; Gaba, Victor; Greenberg, Bruce M (April 1998). "Higher plants and UV-B radiation: balancing damage, repair and acclimation". <i>Trends in Plant Science</i>. <b>3</b> (4): 131–135. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1016%2FS1360-1385%2898%2901215-1">10.1016/S1360-1385(98)01215-1</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Trends+in+Plant+Science&amp;rft.atitle=Higher+plants+and+UV-B+radiation%3A+balancing+damage%2C+repair+and+acclimation&amp;rft.volume=3&amp;rft.issue=4&amp;rft.pages=131-135&amp;rft.date=1998-04&amp;rft_id=info%3Adoi%2F10.1016%2FS1360-1385%2898%2901215-1&amp;rft.aulast=Jansen&amp;rft.aufirst=Marcel+AK&amp;rft.au=Gaba%2C+Victor&amp;rft.au=Greenberg%2C+Bruce+M&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-20"><span><b><a href="#cite_ref-20">^</a></b></span> <span><cite id="CITEREFChristieReymondPowellBernasconi1998">Christie, JM; Reymond, P; Powell, GK; Bernasconi, P; Raibekas, AA; Liscum, E; Briggs, WR (27 November 1998). "Arabidopsis NPH1: a flavoprotein with the properties of a photoreceptor for phototropism". <i>Science</i>. <b>282</b> (5394): 1698–701. <a href="https://en.wikipedia.org/wiki/Bibcode_(identifier)" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" href="https://ui.adsabs.harvard.edu/abs/1998Sci...282.1698C">1998Sci...282.1698C</a>. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1126%2Fscience.282.5394.1698">10.1126/science.282.5394.1698</a>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a>&nbsp;<a rel="nofollow" href="https://pubmed.ncbi.nlm.nih.gov/9831559">9831559</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Science&amp;rft.atitle=Arabidopsis+NPH1%3A+a+flavoprotein+with+the+properties+of+a+photoreceptor+for+phototropism.&amp;rft.volume=282&amp;rft.issue=5394&amp;rft.pages=1698-701&amp;rft.date=1998-11-27&amp;rft_id=info%3Apmid%2F9831559&amp;rft_id=info%3Adoi%2F10.1126%2Fscience.282.5394.1698&amp;rft_id=info%3Abibcode%2F1998Sci...282.1698C&amp;rft.aulast=Christie&amp;rft.aufirst=JM&amp;rft.au=Reymond%2C+P&amp;rft.au=Powell%2C+GK&amp;rft.au=Bernasconi%2C+P&amp;rft.au=Raibekas%2C+AA&amp;rft.au=Liscum%2C+E&amp;rft.au=Briggs%2C+WR&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-F.S.P._Ng_1997_34–37-21"><span>^ <a href="#cite_ref-F.S.P._Ng_1997_34–37_21-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-F.S.P._Ng_1997_34–37_21-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-F.S.P._Ng_1997_34–37_21-2"><sup><i><b>c</b></i></sup></a></span> <span><cite id="CITEREFF.S.P._Ng1997">F.S.P. Ng (1997). "Shyness in trees". <i>Nature Malaysiana</i>. <b>2</b>: 34–37.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nature+Malaysiana&amp;rft.atitle=Shyness+in+trees&amp;rft.volume=2&amp;rft.pages=34-37&amp;rft.date=1997&amp;rft.au=F.S.P.+Ng&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-22"><span><b><a href="#cite_ref-22">^</a></b></span> <span><cite id="CITEREFCrepyCasal2015">Crepy, María A.; Casal, Jorge J. (January 2015). "Photoreceptor-mediated kin recognition in plants". <i>New Phytologist</i>. <b>205</b> (1): 329–338. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1111%2Fnph.13040">10.1111/nph.13040</a>. <a href="https://en.wikipedia.org/wiki/Hdl_(identifier)" title="Hdl (identifier)">hdl</a>:<span title="Freely accessible"><a rel="nofollow" href="https://hdl.handle.net/11336%2F37860">11336/37860</a></span>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a>&nbsp;<a rel="nofollow" href="https://pubmed.ncbi.nlm.nih.gov/25264216">25264216</a>. <a href="https://en.wikipedia.org/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a>&nbsp;<a rel="nofollow" href="https://api.semanticscholar.org/CorpusID:28093742">28093742</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=New+Phytologist&amp;rft.atitle=Photoreceptor-mediated+kin+recognition+in+plants&amp;rft.volume=205&amp;rft.issue=1&amp;rft.pages=329-338&amp;rft.date=2015-01&amp;rft_id=info%3Ahdl%2F11336%2F37860&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A28093742%23id-name%3DS2CID&amp;rft_id=info%3Apmid%2F25264216&amp;rft_id=info%3Adoi%2F10.1111%2Fnph.13040&amp;rft.aulast=Crepy&amp;rft.aufirst=Mar%C3%ADa+A.&amp;rft.au=Casal%2C+Jorge+J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-23"><span><b><a href="#cite_ref-23">^</a></b></span> <span><cite id="CITEREFMargaret_LowmanSoubadra_DevyT._Ganesh2013">Margaret Lowman; Soubadra Devy; T. Ganesh (22 June 2013). <a rel="nofollow" href="https://books.google.com/books?id=hVNGAAAAQBAJ&amp;pg=PA34"><i>Treetops at Risk: Challenges of Global Canopy Ecology and Conservation</i></a>. Springer Science &amp; Business Media. p.&nbsp;34. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-1-4614-7161-5" title="Special:BookSources/978-1-4614-7161-5"><bdi>978-1-4614-7161-5</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Treetops+at+Risk%3A+Challenges+of+Global+Canopy+Ecology+and+Conservation&amp;rft.pages=34&amp;rft.pub=Springer+Science+%26+Business+Media&amp;rft.date=2013-06-22&amp;rft.isbn=978-1-4614-7161-5&amp;rft.au=Margaret+Lowman&amp;rft.au=Soubadra+Devy&amp;rft.au=T.+Ganesh&amp;rft_id=https%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3DhVNGAAAAQBAJ%26pg%3DPA34&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-RGF_2004-24"><span><b><a href="#cite_ref-RGF_2004_24-0">^</a></b></span> <span><cite id="CITEREFR._G._Florence2004">R. G. Florence (January 2004). <a rel="nofollow" href="https://books.google.com/books?id=aO1Jgc7d17UC&amp;pg=PA182"><i>Ecology and Silviculture of Eucalypt Forests</i></a>. Csiro Publishing. pp.&nbsp;182–. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-0-643-09064-4" title="Special:BookSources/978-0-643-09064-4"><bdi>978-0-643-09064-4</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Ecology+and+Silviculture+of+Eucalypt+Forests&amp;rft.pages=182-&amp;rft.pub=Csiro+Publishing&amp;rft.date=2004-01&amp;rft.isbn=978-0-643-09064-4&amp;rft.au=R.+G.+Florence&amp;rft_id=https%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3DaO1Jgc7d17UC%26pg%3DPA182&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
</ol></div>
<h2><span id="External_links">External links</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Crown_shyness&amp;action=edit&amp;section=4" title="Edit section: External links"><span>edit</span></a><span>]</span></span></h2>
<ul><li><span typeof="mw:File"><a href="https://en.wikipedia.org/wiki/File:Commons-logo.svg"><img alt="" src="https://upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/12px-Commons-logo.svg.png" decoding="async" width="12" height="16" srcset="https://upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/18px-Commons-logo.svg.png 1.5x, https://upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/24px-Commons-logo.svg.png 2x" data-file-width="1024" data-file-height="1376"></a></span> Media related to <a href="https://commons.wikimedia.org/wiki/Category:Crown_shyness" title="commons:Category:Crown shyness">Crown shyness</a> at Wikimedia Commons</li></ul>
<!-- 
NewPP limit report
Parsed by mw1352
Cached time: 20231220021903
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.337
Real time usage: 0.454
Preprocessor visited node count: 1818
Post‐expand include size: 54802
Template argument size: 2431
Highest expansion depth: 17
Expensive parser function count: 3
Unstrip recursion depth: 1
Unstrip post‐expand size: 76079
Lua time usage: 0.213
Lua memory usage: 7459340
Number of Wikibase entities loaded: 1
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  405.783      1 -total
 57.55%  233.540      1 Template:Reflist
 39.63%  160.821     16 Template:Cite_journal
 18.37%   74.546      1 Template:Short_description
 10.16%   41.213      1 Template:Cn
  9.25%   37.524      2 Template:Fix
  8.24%   33.424      2 Template:Pagetype
  7.29%   29.586      6 Template:Main_other
  6.82%   27.686      4 Template:Category_handler
  6.68%   27.101      1 Template:SDcat
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:47598638-0!canonical and timestamp 20231220021903 and revision id 1170971299. Rendering was triggered because: page-view
 -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NY Governor vetoes ban on noncompete clauses, waters down LLC transparency bill (344 pts)]]></title>
            <link>https://gothamist.com/news/ny-gov-hochul-vetoes-ban-on-noncompete-clauses-waters-down-llc-transparency-bill</link>
            <guid>38749155</guid>
            <pubDate>Sat, 23 Dec 2023 22:59:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gothamist.com/news/ny-gov-hochul-vetoes-ban-on-noncompete-clauses-waters-down-llc-transparency-bill">https://gothamist.com/news/ny-gov-hochul-vetoes-ban-on-noncompete-clauses-waters-down-llc-transparency-bill</a>, See on <a href="https://news.ycombinator.com/item?id=38749155">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>New York will create a database that will identify the names of owners of many limited liability companies within the state for the first time. But the public won’t be able to access it, thanks to a legislative amendment secured by Gov. Kathy Hochul’s office.</p><p>The Democratic governor <a href="https://gothamist.com/news/wrongful-deaths-frankensteining-and-more-heres-whats-on-ny-gov-hochuls-desk" rel="noopener" target="_blank">signed 42 bills late Friday and vetoed another 43</a>, part of an end-of-year push to clear her desk of legislation approved by state lawmakers earlier this year.</p><p>Among the bills she signed was <a href="https://gothamist.com/news/llcs-might-soon-have-to-list-their-owners-should-new-yorkers-get-a-look" rel="noopener" target="_blank">the LLC Transparency Act</a>, a measure that will force limited liability companies to list their “beneficial owners” when they create a new company or change the structure of a current one. The bill’s sponsors pushed the measure as a way for New York residents to, for example, find out who their true landlord is if their building is owned by an otherwise nameless LLC.</p><p>But Hochul negotiated an amendment to the bill that will keep that database from the public. Instead, she said, that information will only be maintained for law-enforcement purposes, such as if a district attorney needs to access it for an investigation.</p><p>“The bill as drafted was overly broad, and required changes to ensure it serves the core purpose of exposing unlawful activity while balancing personal privacy,” Hochul wrote in a memo.</p><p>Among the other bills Hochul acted on late Friday:</p><ul><li>Hochul vetoed a <a href="https://gothamist.com/news/future-of-noncompete-agreements-in-ny-in-gov-hochuls-hands" rel="noopener" target="_blank">bill that would have effectively banned noncompete agreements</a>, which employers use to prevent their employees from going to work for a competitor for a period of time after their employment.</li><li>She also vetoed a bill that would have required New York City to install recycling bins at every park, playground and historical site in the five boroughs, arguing it would have placed an unfunded mandate on the city.</li><li>She approved a bill that will move many county- and town-level elections to even-numbered years, much to the chagrin of Republicans.</li></ul><p>Major business interests, including Wall Street firms and the state Business Council, lobbied Hochul to reject the ban on noncompete clauses, arguing they are necessary to protect trade secrets and retain talent. They argued companies would move jobs out of state if New York were to enact the ban.</p><p>Hochul had previously floated a compromise that would allow the ban only to apply to those with salaries under $250,000. But in a statement, bill sponsor Sen. Sean Ryan of Buffalo said Hochul rejected the Legislature’s final offer, which he says would have ceded to the $250,000 cap but indexed it to inflation and exempted all medical workers.</p><p>In her veto message, Hochul said she was trying to find a compromise that protected middle-class and lower-wage workers while allowing businesses “to retain highly compensated talent.” She said she’s “open to future legislation that achieves that right balance.”</p><p>The bill Hochul signed <a href="https://gothamist.com/news/lawmakers-moving-some-ny-elections-to-even-years-could-nyc-be-next" rel="noopener" target="_blank">moving many town- and county-level elections to even years</a> will be phased in over the coming years, eventually aligning them with federal elections in a move supporters say is designed to boost voter turnout.</p><p>Republicans deeply opposed the measure, accusing Democrats of orchestrating the change to benefit from New York’s traditionally heavy Democratic turnout in presidential election years.</p><p>Stephen Acquario, executive director of the state Association of Counties, an organization that lobbies for county governments, criticized Hochul for signing the bill into law.</p><p>“At a time when we should be keeping the divisiveness at the federal and state levels out of our local communities, this bill does the opposite, burying the local issues that impact New Yorkers’ daily lives at the back of exceedingly long ballots,” he said in a statement.</p><p>In a statement Friday afternoon, Hochul said the measure is “about expanding to the ballot box and promoting a more inclusive democracy.”</p><p>While the measure would not apply to New York City (or other city) elections or positions like district attorney —&nbsp;which <a href="https://gothamist.com/news/lawmakers-moving-some-ny-elections-to-even-years-could-nyc-be-next" rel="noopener" target="_blank">are set by the state constitution</a> — Hochul said she would support a constitutional amendment to adjust the election calendar to “to save taxpayer dollars and avoid voter fatigue.” But such a move would be several years away; changing the constitution is a multi-year process, and then any change would have to be gradually phased in after that.</p><p>The governor’s action on the batch of bills leaves just five pieces of legislation to approve or reject by the end of the calendar year.</p><p>That includes a measure known as the Grieving Families Act, which, if signed, would make it easier for family members to seek civil damages for their grief and anguish in wrongful death cases.</p><p>Another pending bill would make changes to the state’s nascent public-campaign-finance system, which good-government advocates oppose.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Translation and accelerated solving of differential equations on GPU platforms (102 pts)]]></title>
            <link>https://arxiv.org/abs/2304.06835</link>
            <guid>38749057</guid>
            <pubDate>Sat, 23 Dec 2023 22:45:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2304.06835">https://arxiv.org/abs/2304.06835</a>, See on <a href="https://news.ycombinator.com/item?id=38749057">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
    <div><p><span>Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Utkarsh,+U">Utkarsh Utkarsh</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Churavy,+V">Valentin Churavy</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ma,+Y">Yingbo Ma</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Besard,+T">Tim Besard</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Prakitr">Prakitr Srisuma</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gymnich,+T">Tim Gymnich</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gerlach,+A+R">Adam R. Gerlach</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Edelman,+A">Alan Edelman</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Barbastathis,+G">George Barbastathis</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Braatz,+R+D">Richard D. Braatz</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Rackauckas,+C">Christopher Rackauckas</a></p></div>            
    <p><a href="https://arxiv.org/pdf/2304.06835.pdf">Download PDF</a></p><blockquote>
            <span>Abstract:</span>We demonstrate a high-performance vendor-agnostic method for massively parallel solving of ensembles of ordinary differential equations (ODEs) and stochastic differential equations (SDEs) on GPUs. The method is integrated with a widely used differential equation solver library in a high-level language (Julia's DifferentialEquations.jl) and enables GPU acceleration without requiring code changes by the user. Our approach achieves state-of-the-art performance compared to hand-optimized CUDA-C++ kernels while performing 20--100$\times$ faster than the vectorizing map (vmap) approach implemented in JAX and PyTorch. Performance evaluation on NVIDIA, AMD, Intel, and Apple GPUs demonstrates performance portability and vendor-agnosticism. We show composability with MPI to enable distributed multi-GPU workflows. The implemented solvers are fully featured -- supporting event handling, automatic differentiation, and incorporation of datasets via the GPU's texture memory -- allowing scientists to take advantage of GPU acceleration on all major current architectures without changing their model code and without loss of performance. We distribute the software as an open-source library <a href="https://github.com/SciML/DiffEqGPU.jl" rel="external noopener nofollow">this https URL</a>
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Utkarsh Utkarsh [<a href="https://arxiv.org/show-email/51fc52a7/2304.06835">view email</a>]      <br>            <strong><a href="https://arxiv.org/abs/2304.06835v1">[v1]</a></strong>
        Thu, 13 Apr 2023 21:57:51 UTC (8,296 KB)<br>
            <strong><a href="https://arxiv.org/abs/2304.06835v2">[v2]</a></strong>
        Fri, 21 Apr 2023 04:03:33 UTC (8,301 KB)<br>
    <strong>[v3]</strong>
        Mon, 13 Nov 2023 05:11:50 UTC (2,693 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How does a B-tree make queries fast? (218 pts)]]></title>
            <link>https://blog.allegro.tech/2023/11/how-does-btree-make-your-queries-fast.html</link>
            <guid>38748433</guid>
            <pubDate>Sat, 23 Dec 2023 21:33:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.allegro.tech/2023/11/how-does-btree-make-your-queries-fast.html">https://blog.allegro.tech/2023/11/how-does-btree-make-your-queries-fast.html</a>, See on <a href="https://news.ycombinator.com/item?id=38748433">Hacker News</a></p>
<div id="readability-page-1" class="page"><article itemprop="articleBody">
        <div> <p><strong>B-tree</strong> is a structure that helps to search through great amounts of data.
It was invented over 40 years ago, yet it is still employed by the majority of modern databases.
Although there are newer index structures, like LSM trees,
<strong>B-tree</strong> is unbeaten when handling most of the database queries.</p>

<p>After reading this post, you will know how <strong>B-tree</strong> organises the data and how it performs search queries.</p>

<h2 id="origins">Origins</h2>

<p>In order to understand <strong>B-tree</strong> let’s focus on <strong>Binary Search Tree (BST)</strong> first.</p>

<p>Wait, isn’t it the same?</p>

<p>What does “B” stand for then?</p>

<p>According to <a href="https://en.wikipedia.org/wiki/B-tree">wikipedia.org</a>, Edward M.&nbsp;McCreight, the inventor of B-tree, once said:</p>

<blockquote>
  <p>“the more you think about what the B in B-trees means, the better you understand B-trees.”</p>
</blockquote>

<p>Confusing <strong>B-tree</strong> with <strong>BST</strong> is a really common misconception.
Anyway, in my opinion, BST is a great starting point for reinventing B-tree.
Let’s start with a simple example of BST:</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/bst-basic.webp" alt="Binary Search Tree with three nodes"></p>

<p>The greater number is always on the right, the lower on the left. It may become clearer when we add more numbers.</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/bst-bigger.webp" alt="Binary Search Tree with seven nodes"></p>

<p>This tree contains seven numbers, but we need to visit at most three nodes to locate any number.
The following example visualizes searching for 14.
I used SQL to define the query in order to think about this tree as if it were an actual database index.</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/bst-bigger-searching.webp" alt="Searching for single node within Binary Search Tree with seven nodes"></p>

<h2 id="hardware">Hardware</h2>

<p>In theory, using Binary Search Tree for running our queries looks fine. Its time complexity (when searching) is \(O(log
n)\), <a href="https://en.wikipedia.org/wiki/B-tree">same as B-tree</a>. However, in practice, this data structure needs to work on actual hardware. An index must be
stored somewhere on your machine.</p>

<p>The computer has three places where the data can be stored:</p>

<ul>
  <li>CPU caches</li>
  <li>RAM (memory)</li>
  <li>Disk (storage)</li>
</ul>

<p>The cache is managed fully by CPUs. Moreover, it is relatively small, usually a few megabytes.
Index may contain gigabytes of data, so it won’t fit there.</p>

<p>Databases vastly use Memory (RAM). It has some great advantages:</p>

<ul>
  <li>assures fast random access (more on that in the next paragraph)</li>
  <li>its size may be pretty big (e.g. AWS RDS cloud service <a href="https://aws.amazon.com/rds/instance-types/">provides instances</a>
with a few terabytes of memory available).</li>
</ul>

<p>Cons? You lose the data when the power supply goes off. Moreover, when compared to the disk, it is pretty expensive.</p>

<p>Finally, the cons of a memory are the pros of a disk storage.
It’s cheap, and data will remain there even if we lose the power.
However, there are no free lunches!
The catch is that we need to be careful about random and sequential access.
Reading from the disk is fast, but only under certain conditions!
I’ll try to explain them simply.</p>

<h3 id="random-and-sequential-access">Random and sequential access</h3>

<p>Memory may be visualized as a line of containers for values, where every container is numbered.</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/memory.webp" alt="Simple memory visualization"></p>

<p>Now let’s assume we want to read data from containers 1, 4, and 6. It requires random access:</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/memory-random-access.webp" alt="Random access visualized on a small chunk of a memory"></p>

<p>And then let’s compare it with reading containers 3, 4, and 5. It may be done sequentially:</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/memory-sequential-access.webp" alt="Sequential access visualized on a small chunk of a memory"></p>

<p>The difference between a “random jump” and a “sequential read” can be explained based on Hard Disk Drive.
It consists of the head and the disk.</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/hdd-disk.webp" alt="Hard Disk Drive with cover removed, Public Domain image from https://en.wikipedia.org/wiki/Hard_disk_drive#/media/File:Laptop-hard-drive-exposed.jpg"></p>

<p>“Random jump” requires moving the head to the given place on the disk.
“Sequential read” is simply spinning the disk, allowing the head to read consecutive values.
When reading megabytes of data, the difference between these two types of access is enormous.
Using “sequential reads” lowers the time needed to fetch the data significantly.</p>

<p>Differences in speed between random and sequential access were researched in the article “The Pathologies of Big Data”
by Adam Jacobs, <a href="https://queue.acm.org/detail.cfm?id=1563874">published in Acm Queue</a>.
It revealed a few mind-blowing facts:</p>

<ul>
  <li>Sequential access on HDD may be hundreds of thousands of times faster than random access. 🤯</li>
  <li>It may be faster to read sequentially from the disk than randomly from the memory.</li>
</ul>

<p>Who even uses HDD nowadays?
What about SSD?
This research shows that reading fully sequentially from HDD may be faster than SSD.
However, please note that the article is from 2009 and SSD developed significantly through the last decade,
thus these results are probably outdated.</p>

<p>To sum up, the key takeaway is <strong>to prefer sequential access wherever we can</strong>.
In the next paragraph, I will explain how to apply it to our index structure.</p>

<h2 id="optimizing-a-tree-for-sequential-access">Optimizing a tree for sequential access</h2>

<p>Binary Search Tree may be represented in memory in the same way
as <a href="https://en.wikipedia.org/wiki/Binary_heap">the heap</a>:</p>

<ul>
  <li>parent node position is \(i\)</li>
  <li>left node position is \(2i\)</li>
  <li>right node position is \(2i+1\)</li>
</ul>

<p>That’s how these positions are calculated based on the example (the parent node starts at 1):</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/tree-representation-in-memory-1.webp" alt="Binary tree representation in the memory—part 1/2"></p>

<p>According to the calculated positions, nodes are aligned into the memory:</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/tree-representation-in-memory-2.webp" alt="Binary tree representation in the memory—part 2/2"></p>

<p>Do you remember the query visualized a few chapters ago?</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/bst-bigger-searching.webp" alt="Searching for single node within Binary Search Tree with seven nodes"></p>

<p>That’s what it looks like on the memory level:</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/tree-representation-in-memory-query.webp" alt="Binary tree representation in the memory - querying"></p>

<p>When performing the query, memory addresses 1, 3, and 6 need to be visited.
Visiting three nodes is not a problem; however, as we store more data, the tree gets higher.
Storing more than one million values requires a tree of height at least 20. It means
that 20 values from different places in memory must be read.
It causes completely random access!</p>

<h3 id="pages">Pages</h3>

<p>While a tree grows in height, random access is causing more and more delay.
The solution to reduce this problem is simple: grow the tree in width rather than in height.
It may be achieved by packing more than one value into a single node.</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/tree-with-3-values-in-node.webp" alt="A tree with three values in single node"></p>

<p>It brings us the following benefits:</p>

<ul>
  <li>the tree is shallower (two levels instead of three)</li>
  <li>it still has a lot of space for new values without the need for growing further</li>
</ul>

<p>The query performed on such index looks as follows:</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/tree-with-3-values-query.webp" alt="A query performed on a tree with three values in a single node"></p>

<p>Please note that every time we visit a node, we need to load all its values.
In this example, we need to load 4 values (or 6 if the tree is full) in order to reach the one we are looking for.
Below, you will find a visualization of this tree in a memory:</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/tree-with-3-values-memory.webp" alt="A tree with three values in a single node represented in a memory"></p>

<p>Compared to <a href="#optimizing-a-tree-for-sequential-access">the previous example</a> (where the tree grows in height),
this search should be faster.
We need random access only twice (jump to cells 0 and 9) and then sequentially read the rest of values.</p>

<p>This solution works better and better as our database grows. If you want to store one million values, then you need:</p>

<ul>
  <li>Binary Search Tree which has <strong>20</strong> levels</li>
</ul>

<p>OR</p>

<ul>
  <li>3-value node Tree which has <strong>10</strong> levels</li>
</ul>

<p>Values from a single node make a page.
In the example above, each page consists of three values.
A page is a set of values placed on a disk next to each other,
so the database may reach the whole page at once with one sequential read.</p>

<p>And how does it refer to the reality?
<a href="https://www.postgresql.org/docs/current/storage-toast.html#:~:text=PostgreSQL%20uses%20a%20fixed%20page,tuples%20to%20span%20multiple%20pages.">Postgres page size is 8kB</a>.
Let’s assume that 20% is for metadata, so it’s 6kB left.
Half of the page is needed to store
pointers to node’s children, so it gives us 3kB for values.
BIGINT size is 8 bytes, thus we may store ~375 values in a
single page.</p>

<p>Assuming that some pretty big tables in a database have one billion rows,
how many levels in the Postgres tree do we need to store them?
According to the calculations above,
if we create a tree that can handle 375 values in a single node,
it may store <strong>1 billion</strong> values with a tree that has only <strong>four</strong> levels.
Binary Search Tree would require 30 levels for such amount of data.</p>

<p>To sum up, placing multiple values in a single node of the tree helped us to reduce its height, thus using the benefits of sequential access.
Moreover, a B-tree may grow not only in height, but also in width (by using larger pages).</p>

<h2 id="balancing">Balancing</h2>

<p>There are two types of operations in databases: writing and reading.
In the previous section, we addressed the problems with reading the data from the B-tree.
Nonetheless, writing is also a crucial part.
When writing the data to a database, B-tree needs to be constantly updated with new values.</p>

<p>The tree shape depends on the order of values added to the tree.
It’s easily visible in a binary tree.
We may obtain trees with different depths if the values are added in an incorrect order.</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/bst-imbalance.webp" alt="Two Binary Trees with shapes depending on the order of inserted values."></p>

<p>When the tree has different depths on different nodes, it is called an unbalanced tree.
There are basically two ways of returning such a tree to a balanced state:</p>

<ol>
  <li>Rebuilding it from the very beginning just by adding the values in the correct order.</li>
  <li>Keeping it balanced all the time, as the new values are added.</li>
</ol>

<p>B-tree implements the second option. A feature that makes the tree balanced all the time is called self-balancing.</p>

<h3 id="self-balancing-algorithm-by-example">Self-balancing algorithm by example</h3>

<p>Building a B-tree can be started simply by creating a single node
and adding new values until there is no free space in it.</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/self-balancing-step-1.webp" alt="Self-balancing, step 1, Add new values until there is a free space in existing nodes."></p>

<p>If there is no space on the corresponding page, it needs to be split.
To perform a split, a „split point” is chosen.
In that case, it will be 12, because it is in the middle.
The „Split point” is a value that will be moved to the upper page.</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/self-balancing-step-2a.webp" alt="Self-balancing, step 2a, Splitting the page."></p>

<p>Now, it gets us to an interesting point where there is no upper page.
In such a case, a new one needs to be generated (and it becomes the new root page!).</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/self-balancing-step-2b.webp" alt="Self-balancing, step 2b, Generating a new root page."></p>

<p>And finally, there is some free space in the three, so value 14 may be added.</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/self-balancing-step-2c.webp" alt="Self-balancing, step 2c, Adding the 14 to B-tree."></p>

<p>Following this algorithm, we may constantly add new values to the B-tree, and it will remain balanced all the time!</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/self-balancing-step-final.webp" alt="Self-balancing, Final state of the B-tree, after adding multiple values."></p>

<p><em>At this point, you may have a valid concern that there is a lot of free space that has no chance to be
filled.
For example, values 14, 15, and 16, are on different pages, so these pages will remain with only one value and two free spaces forever.</em></p>

<p><em>It was caused by the split location choice.
We always split the page in the middle.
But every time we do a split, we may choose any split location we want.</em></p>

<p><em>Postgres has an algorithm that is run every time a split is performed!
Its implementation may be found in the <a href="https://github.com/postgres/postgres/blob/54ccfd65868c013a8c6906bc894bc5ea3640740a/src/backend/access/nbtree/nbtsplitloc.c#L87">_bt_findsplitloc() function in Postgres source code</a>.
Its goal is to leave as little free space as possible.</em></p>

<h2 id="summary">Summary</h2>

<p>In this article, you learned how a B-tree works.
All in all, it may be simply described as a Binary Search Tree with two changes:</p>

<ul>
  <li>every node may contain more than one value</li>
  <li>inserting a new value is followed by a self-balancing algorithm.</li>
</ul>

<p>Although the structures used by modern databases are usually some variants of a B-tree (like B+tree), they are still based on the original conception.
In my opinion, one great strength of a B-tree is the fact that it was designed directly to handle large amounts of data on actual hardware.
It may be the reason why the B-tree has remained with us for such a long time.</p>
</div>
    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Art of Electronics (3rd Edition) (356 pts)]]></title>
            <link>https://artofelectronics.net/</link>
            <guid>38748370</guid>
            <pubDate>Sat, 23 Dec 2023 21:23:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://artofelectronics.net/">https://artofelectronics.net/</a>, See on <a href="https://news.ycombinator.com/item?id=38748370">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
								
<h2><a href="https://artofelectronics.net/">About The Book</a></h2>

<p><img src="https://artofelectronics.net/wp-content/uploads/2014/04/book_3rd-137x140.png" alt="About The Book" width="137" height="140"></p>
<blockquote><p>“Wow. Chapter 5 details every circuit artifact that I’ve encountered&nbsp;in the past 30 years in a thorough, pragmatic, and straightforward way.&nbsp;My only ‘twinge’ is that it discloses and explains (in glorious&nbsp;graphical detail and with real part numbers) many topics that I&nbsp;thought were my personal trade secrets. I love the plots. I know that it must take an enormous effort to collate all of the device&nbsp;characteristics. It’s worth the effort. The way the data is&nbsp;presented allows the reader to get terrific perspective on a lot&nbsp;of landscape in a single view. Nice work.” — John Willison, founder, Stanford Research Systems</p></blockquote>
<p><img fetchpriority="high" decoding="async" src="https://artofelectronics.net/wp-content/uploads/2019/11/authors_0630_350w.jpg" alt="" width="350" height="293"></p>

<div>
					<p><span><strong>Counterfeit Warning:</strong> December, 2015 — buyers have reported poor quality copies (confirmed as counterfeit) being sold online at prices too low to be creditable. These are recognizable from their poor bindings and text errors (e.g., missing the ligature “fi”, thus on the author page “Wineld Hill”!). More information <a href="http://artofelectronics.net/the-book/counterfeit-editions/">here</a>. EEVBlog’s Dave Jones gets one in his Mailbag <a href="http://www.eevblog.com/2016/02/03/eevblog-847-mailbag/">here</a>. Note also that the only authorized e-book version is Kindle.</span></p></div><br>
<!--


<table cellspacing="10px" cellpadding="10px" align="center">


<tbody>


<tr>


<td><a href="http://www.cambridge.org/academic/art-electronics-free-sample-chapter" target="_blank" rel="noopener noreferrer">Download a sample chapter from Cambridge Press</a></td>


</tr>


</tbody>


</table>


-->



<table>
<tbody>
<tr>
<td><strong><span color="#666666">– 1220 large format pages</span></strong></td>
<td><strong><span color="#666666">– 80&nbsp;tables listing some 1650 components</span></strong></td>
<td><strong><span color="#666666">– 1470&nbsp;figures and 90 oscilloscope&nbsp;screenshots</span></strong></td>
</tr>
<tr>
<td><strong><span color="#666666">– Extensive practical advice</span></strong></td>
<td><strong><span color="#666666">– Back-of-the-envelope techniques</span></strong></td>
<td><strong><span color="#666666">– Exhaustive&nbsp;index</span></strong></td>
</tr>
</tbody>
</table>

<h4>Where to buy</h4>
<p><strong>Cambridge University Press</strong><span>&nbsp;–&nbsp;<a href="http://www.cambridge.org/us/academic/subjects/physics/electronics-physicists/art-electronics-3rd-edition?format=HB" target="_blank" rel="noopener noreferrer">The Art of Electronics 3rd Edition</a>&nbsp;/&nbsp;<a href="http://www.cambridge.org/us/academic/subjects/physics/electronics-physicists/learning-art-electronics-hands-lab-course">Learning the Art of Electronics 3rd Edition</a><a href="http://www.cambridge.org/us/academic/subjects/physics/electronics-physicists/art-electronics-student-manual?format=PB" target="_blank" rel="noopener noreferrer"><br>
</a></span><strong>Amazon.com</strong><span>&nbsp;–</span><span>&nbsp;</span><span><a href="https://www.amazon.com/The-Art-Electronics-Paul-Horowitz/dp/0521809266?&amp;linkCode=wey&amp;tag=maggicom0e-20" target="_blank" rel="noopener noreferrer">The Art of Electronics 3rd Edition</a>&nbsp;/&nbsp;<a href="https://www.amazon.com/Learning-Art-Electronics-Hands-On-Course/dp/0521177235?linkCode=wey&amp;tag=maggicom0e-20">Learning the Art of Electronics 3rd Edition</a>&nbsp;<a href="https://www.amazon.com/The-Art-Electronics-Student-Manual/dp/0521377099?&amp;linkCode=wey&amp;tag=maggicom0e-20" target="_blank" rel="noopener noreferrer"><br>
</a></span><strong>Adafruit Industries</strong> –<span><i>&nbsp;</i></span><a href="http://www.adafruit.com/products/2356" target="_blank" rel="noopener noreferrer">The Art of Electronics 3rd Edition</a>&nbsp;–&nbsp;<a href="http://www.adafruit.com/products/310" target="_blank" rel="noopener noreferrer">Student Manual to 2nd Edition<br>
</a><strong>Barnes and Noble</strong>&nbsp;–<em>&nbsp;</em><a href="http://www.barnesandnoble.com/w/the-art-of-electronics-paul-horowitz/1116996095?ean=9780521809269" target="_blank" rel="noopener noreferrer">The Art of electronics 3rd Edition</a> /&nbsp;<a href="http://www.barnesandnoble.com/w/learning-the-art-of-electronics-tom-hayes/1122384679" target="_blank" rel="noopener noreferrer">Learning the Art of Electronics 3rd Edition<br>
</a><strong>Amazon.co.uk (UK)</strong> – <a href="http://www.amazon.co.uk/The-Art-Electronics-Paul-Horowitz/dp/0521809266?linkCode=wey&amp;tag=maggicom0e-20" target="_blank" rel="noopener noreferrer">The Art of Electronics 3rd Edition</a>&nbsp;/&nbsp;<a href="http://www.amazon.co.uk/Learning-Art-Electronics-Hands-On-Course/dp/0521177235?linkCode=wey&amp;tag=maggicom0e-20" target="_blank" rel="noopener noreferrer">Learning the Art of Electronics 3rd Edition</a><a href="http://www.barnesandnoble.com/w/art-of-electronics-student-manual-thomas-c-hayes/1100948145?ean=9780521377096" target="_blank" rel="noopener noreferrer"><br>
</a><strong>Foyles (UK)</strong>&nbsp;– <a href="http://www.foyles.co.uk/witem/the-art-of-electronics,paul-horowitz-winfield-hill-9780521809269">The Art of Electronics 3rd Edition</a><br>
<strong>The Book Depository (Worldwide)</strong> – <a href="http://www.bookdepository.com/Art-Electronics-Paul-Horowitz/9780521809269">The Art of Electronics 3rd Edition</a></p>

					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[2TB microSD card is on the way early next year (117 pts)]]></title>
            <link>https://overkill.wtf/2tb-microsd-card-on-the-way/</link>
            <guid>38748087</guid>
            <pubDate>Sat, 23 Dec 2023 20:49:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://overkill.wtf/2tb-microsd-card-on-the-way/">https://overkill.wtf/2tb-microsd-card-on-the-way/</a>, See on <a href="https://news.ycombinator.com/item?id=38748087">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
<div>
<p>
Japanese storage maker KIOXIA has started production on its highest capacity microSDXC card ever.
</p>
<p>If you're currently <a href="https://overkill.wtf/best-games-under-10-dollars-in-the-steam-winter-sale-2023/" rel="noreferrer">adding countless games to your backlog</a> as a result of the <em>thousands</em> of titles discounted as part of <a href="https://overkill.wtf/steam-winter-sale-2023/" rel="noreferrer">the massive Steam Winter Sale</a>, then you may also find yourself in need of a bigger microSD card for your Steam Deck soon. <em>Thankfully, KIOXIA has a <strong>really</strong> sizeable solution on the way.</em></p><p><strong>The company </strong><a href="https://europe.kioxia.com/en-europe/personal/news/2023/20231220-1.html?ref=overkill.wtf" rel="noreferrer"><strong>just announced</strong></a><strong> that it has created its highest capacity microSDXC memory card ever — <em>coming in at a whopping 2TB</em>.</strong></p><p>This frankly remarkable feat was made possible by "<em>stacking sixteen 1 terabit dies of 3D flash memory</em>" all whilst maintaining the standard 0.8mm thickness.</p><div><p>ℹ️</p><p>16 Terabits = 2 Terabytes</p></div><p>The upcoming 2TB card, dubbed the '<em>EXCERIA PLUS G2</em>' is said to have read speeds of up to 100 MB a second and write speeds of up to 90 MB a second. This 2TB size now hits the <a href="https://www.sdcard.org/developers/sd-standard-overview/capacity-sd-sdhc-sdxc-sduc/?ref=overkill.wtf" rel="noreferrer">upper storage capacity of the defined SDXC standard</a>.</p><p>Japanese storage maker KIOXIA, formerly known under the trusted Toshiba name, has some fine heritage — and can essentially lay claim to having actually <em>invented flash memory</em> thanks to the efforts of <a href="https://en.wikipedia.org/wiki/Fujio_Masuoka?ref=overkill.wtf" rel="noreferrer">engineer Fujio Masuoka</a>. </p><p>All of that is to say that, although you may not be familiar with the modern KIOXIA name (<em>I sure wasn't</em>) —&nbsp;this is a really trusted name in the memory business. </p><p>Jamie Stitt, a marketing manager at KIOXIA Europe said these new "<em>enhanced cards</em>" will likely "<em>become a sought-after option by many</em>", including "<em>on-the-go gamers</em>". Yes sir. 🫡</p><figure><img src="https://overkill.wtf/content/images/2023/12/KIOXIA-EXCERIA-PLUS-G2-microSD-2TB-news.jpg" alt="" loading="lazy" width="660" height="330" srcset="https://overkill.wtf/content/images/size/w600/2023/12/KIOXIA-EXCERIA-PLUS-G2-microSD-2TB-news.jpg 600w, https://overkill.wtf/content/images/2023/12/KIOXIA-EXCERIA-PLUS-G2-microSD-2TB-news.jpg 660w"></figure><p>This card from KIOXIA (<em>and likely the others that will follow from competitors</em>) should all work nice in the likes of the Steam Deck, Lenovo Legion Go, ROG Ally and Nintendo Switch — all of which support the microSDXC standard. </p><p>We don't currently know pricing for this huge new KIOXIA card, but we imagine it will start at a rather meaty price point. </p><p>The EXCERIA PLUS G2 2TB microSD cards are now in mass production, and are <strong>expected to start shipping in the early part of next year</strong>. <em>We'll keep you posted when they do! </em></p><figure><a href="https://overkill.wtf/best-accessories-for-steam-deck-and-asus-rog-ally/"><div><p>The best accessories for Steam Deck and ASUS ROG Ally</p><p>Enhance your handheld gaming experience with the right accessories for your Steam Deck, ROG Ally, or AYANEO 2s. From microSD cards to controllers, we’ve got you covered in this guide.</p><p><img src="https://overkill.wtf/content/images/size/w256h256/2022/08/overkill_icon-1.png" alt=""><span>overkill.wtf</span><span>Kevin Wammer</span></p></div><p><img src="https://overkill.wtf/content/images/2023/06/best-accessories.webp" alt=""></p></a></figure><p>Of course, if you don't want to wait for this big 2TB card to drop, we can vouch for <a href="https://amzn.to/469go7H?ref=overkill.wtf" rel="noreferrer">SanDisk's 1TB option</a>.</p><hr><blockquote><strong>Via:</strong> <a href="https://www.neowin.net/news/kioxia-mass-produces-of-worlds-highest-capacity-2tb-microsdxc-memory-card/?ref=overkill.wtf" rel="noreferrer">Neowin</a></blockquote>
</div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cummins pickup truck engines tricked air quality controls, feds say (165 pts)]]></title>
            <link>https://www.usatoday.com/story/news/nation/2023/12/22/cummins-truck-engines-defeat-devices-feds-say/72013430007/</link>
            <guid>38747747</guid>
            <pubDate>Sat, 23 Dec 2023 20:15:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.usatoday.com/story/news/nation/2023/12/22/cummins-truck-engines-defeat-devices-feds-say/72013430007/">https://www.usatoday.com/story/news/nation/2023/12/22/cummins-truck-engines-defeat-devices-feds-say/72013430007/</a>, See on <a href="https://news.ycombinator.com/item?id=38747747">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content"><article><p>The United States Department of Justice is slamming an Indiana-based engine manufacturing company with a $1.675 billion penalty in a settlement that says the company violated the federal Clean Air Act.</p><partner-banner util-module-path="elements/partner" min-height="600" fluid="" outstream="" momentum=""></partner-banner><p>The department alleges Cummins Inc. installed devices that can bypass emissions sensors on 630,000 RAM pickup truck engines, <a href="https://www.justice.gov/opa/pr/statement-attorney-general-merrick-garland-agreement-principle-cummins-settle-alleged">according to a news release Friday</a>. The whopping financial penalty is the largest ever violation since the law was enacted in 1963 to protect the nation's air quality.</p><p>“The types of devices we allege that Cummins installed in its engines to cheat federal environmental laws have a significant and harmful impact on people’s health and safety," wrote Attorney General Merrick B. Garland. He said Cummins' engines caused excess emissions of nitrogen oxides, which can cause asthma and respiratory infections.</p><p>The company agreed to pay the $1.675 billion fine to the U.S. and the State of California to settle the claims, according to the Department of Justice. The penalty is the second largest environmental penalty in the history of the nation, according to the Department of Justice.</p><partner-banner util-module-path="elements/partner" fluid="" bottom="" lazy="" min-height="390" outstream=""></partner-banner><partner-inline util-module-path="elements/partner" placement="native-article_link" sizes="[[300, 250], [3, 3]]" min-height="250" fluid="" outstream=""></partner-inline><p>The company does not admit wrongdoing and says no one in the company acted in bad faith, said Jon Mills, a spokesperson for Cummins Inc. in an email to USA TODAY.</p><p>"The company has cooperated fully with the relevant regulators, already addressed many of the issues involved, and looks forward to obtaining certainty as it concludes this lengthy matter," reads a news release from the company.</p><partner-inline util-module-path="elements/partner" placement="native-article_link" sizes="[[300, 250], [3, 3]]" min-height="250" fluid="" outstream=""></partner-inline><cta-atoms-container-inline util-module-path="elements/cta"></cta-atoms-container-inline><h2>What is the Department of Justice penalizing Cummins Inc. for?</h2><p>Cummins Inc. allegedly installed defeat devices on the engines of hundreds of thousands of 2013 to 20199 RAM 2500 and 3500 pickup trucks, according to the Department of Justice. The DOJ also says the company installed defeat devices on the engines of 330,000 newer RAM pickup trucks.</p><p>Defeat devices are hardware or software used in vehicles to trick air pollution tests, or bypass emissions controls.</p><partner-banner util-module-path="elements/partner" fluid="" bottom="" lazy="" min-height="600" outstream="" momentum=""></partner-banner><p>The company said it has since recalled those trucks. It has also "initiated a recall of model years 2013 through 2018 RAM 2500 and 3500 trucks and previously accrued a total of $59 million for the estimated costs for executing these and other related recalls," according to a Friday news release from the company.</p><media-image image-set="bestCrop, https://www.gannett-cdn.com/-mm-/524efb2b0124e95e203f794dca9b6b2d5d98feab/c=0-50-580-485/local/-/media/2017/01/15/USATODAY/usatsports/2014-ram-ecodiesel-v6_large.jpg 4:3, https://www.gannett-cdn.com/-mm-/cbb5bbcac65cf5cac6db713e9a01ebc2fdb64774/c=90-0-490-534/local/-/media/2017/01/15/USATODAY/usatsports/2014-ram-ecodiesel-v6_large.jpg 3:4, https://www.gannett-cdn.com/-mm-/f465c41d7b7f7ca23c261feac8e7627066d48822/c=0-104-580-430/local/-/media/2017/01/15/USATODAY/usatsports/2014-ram-ecodiesel-v6_large.jpg 16:9" image-alt="Did FCA's 3.0 liter turbo V6 &quot;EcoDiesel&quot; engine have a defeat device? The Feds are investigating." credit="Fiat&nbsp;Chrysler Automobiles NV" caption="Did FCA's 3.0 liter turbo V6 &quot;EcoDiesel&quot; engine have a defeat device? The Feds are investigating." orientation="horizontal" util-module-path="elements/media"></media-image><h2>Vehicle pollution health effects</h2><p>According to the <a href="https://www.epa.gov/clean-air-act-overview/clean-air-act-text">U.S. Environmental Protection Agency</a>, high emissions of nitrogen oxides, or vehicle pollutions, can get into the air from vehicle emissions and the burning of fuel.</p><p>Those emissions "can irritate airways in the human respiratory system," according to the agency.</p><p>"Such exposures over short periods can aggravate respiratory diseases, particularly asthma, leading to respiratory symptoms (such as coughing, wheezing or difficulty breathing), hospital admissions and visits to emergency rooms," according to the agency. "Longer exposures to elevated concentrations of NO<sub>2</sub>&nbsp;may contribute to the development of asthma and potentially increase susceptibility to respiratory infections."</p><partner-banner util-module-path="elements/partner" fluid="" bottom="" lazy="" min-height="390" outstream=""></partner-banner><h2>What is the Clean Air Act?</h2><p>The Clean Air Act is a federal law that was designed to "protect and improve the nation's air quality and the stratospheric ozone layer," <a href="https://www.epa.gov/clean-air-act-overview/clean-air-act-text">according to the US Environmental Protection Agency</a>.</p><p>Congress first enacted the law in 1963 and several major and minor changes have been made to it since its inception. It's the Environmental Protection Agency's role to uphold the law.</p><p><span><strong>Communities facing air pollution </strong><a href="https://www.usatoday.com/story/news/health/2023/04/07/epa-put-new-rules-chemical-plants-reduce-air-pollution/11620822002/" target="_blank">Could get relief as EPA proposes new rules on chemical plants</a></span></p><p><em>Contact Kayla Jimenez at kjimenez@usatoday.com. Follow her on X, formerly Twitter, at @kaylajjimenez.</em></p><lit-timestamp slot="timestamp" publishdate="2023-12-23 00:52:00 +0000 UTC" updatedate="2023-12-23 00:54:38 +0000 UTC"></lit-timestamp><p><a alt="Post the article to your Facebook Timeline" data-size="large" onclick="fireNavShareAnalytics('facebook');" rel="noopener" target="_blank"><svg view-box="0 0 24 24">
                <path d="M12.6143832,21 L3.99346182,21 C3.44462725,21 3,20.5550968 3,20.006476 L3,3.99345411 C3,3.44469364 3.44469709,3 3.99346182,3 L20.006608,3 C20.5552331,3 21,3.44469364 21,3.99345411 L21,20.006476 C21,20.5551667 20.5551632,21 20.006608,21 L15.4197395,21 L15.4197395,14.029408 L17.7594454,14.029408 L18.1097832,11.3128446 L15.4197395,11.3128446 L15.4197395,9.57849053 C15.4197395,8.79198274 15.6381418,8.25600363 16.7659836,8.25600363 L18.2044917,8.25537504 L18.2044917,5.82565895 C17.9557072,5.79255313 17.1017938,5.71858885 16.108332,5.71858885 C14.0343128,5.71858885 12.6143832,6.98457234 12.6143832,9.30945332 L12.6143832,11.3128446 L10.2686707,11.3128446 L10.2686707,14.029408 L12.6143832,14.029408 L12.6143832,21 L12.6143832,21 Z"></path>
            </svg><span>Facebook</span></a>
<a alt="Tweet about this article" data-size="large" onclick="fireNavShareAnalytics('twitter')" rel="noopener" target="_blank"><svg view-box="0 0 24 24">
                <path d="M21,6.77573131 C20.338616,7.07692308 19.6265188,7.28060672 18.8795563,7.3716143 C19.6423666,6.9035753 20.2276809,6.16143012 20.5034337,5.27735645 C19.7892235,5.71072589 19,6.02600217 18.1568938,6.19501625 C17.4849445,5.45937161 16.5245642,5 15.461701,5 C13.4236661,5 11.770206,6.69555796 11.770206,8.78656555 C11.770206,9.08342362 11.8019017,9.3716143 11.8652932,9.64897075 C8.79609086,9.4907909 6.07554147,7.98483207 4.25303751,5.69122427 C3.93502377,6.2524377 3.75330164,6.9035753 3.75330164,7.59696641 C3.75330164,8.91007584 4.40517697,10.0693391 5.39619651,10.7486457 C4.79186476,10.7302275 4.22134179,10.5579632 3.72266244,10.276273 L3.72266244,10.3228602 C3.72266244,12.1581798 4.9957739,13.6890574 6.68621236,14.035753 C6.37665082,14.1245937 6.05018489,14.1690141 5.71315372,14.1690141 C5.47543582,14.1690141 5.24300053,14.1462622 5.01796091,14.1018418 C5.4881141,15.6056338 6.85103011,16.7009751 8.46751189,16.7302275 C7.20390914,17.7464789 5.61067089,18.3521127 3.88114105,18.3521127 C3.58320127,18.3521127 3.28843106,18.3347779 3,18.3001083 C4.63444268,19.3726977 6.57633386,20 8.66085578,20 C15.4543053,20 19.1679873,14.2307692 19.1679873,9.22643554 C19.1679873,9.06175515 19.1648177,8.89707476 19.1584786,8.73564464 C19.8800845,8.20151679 20.5066033,7.53521127 21,6.77573131"></path>
            </svg><span>Twitter</span></a>
<a alt="Email this article" onclick="fireNavShareAnalytics('email')" rel="noopener" target="_blank"><svg view-box="0 0 24 24">
            <path d="M3,5.8757627 C3,5.39209232 3.39269552,5 3.8926228,5 L20.1073772,5 C20.6003592,5 21,5.40389442 21,5.8757627 L21,18.1242373 C21,18.6079077 20.6073045,19 20.1073772,19 L3.8926228,19 C3.39964084,19 3,18.5961056 3,18.1242373 L3,5.8757627 Z M12,11.09375 L3,6.74107143 L3,8.48214286 L12,12.8348214 L21,8.48214286 L21,6.74107143 L12,11.09375 Z"></path>
        </svg><span>Email</span></a></p></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: An open-source, self-hostable synced narration platform for ebooks (134 pts)]]></title>
            <link>https://smoores.gitlab.io/storyteller/</link>
            <guid>38747710</guid>
            <pubDate>Sat, 23 Dec 2023 20:11:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://smoores.gitlab.io/storyteller/">https://smoores.gitlab.io/storyteller/</a>, See on <a href="https://news.ycombinator.com/item?id=38747710">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__docusaurus_skipToContent_fallback"><main><div><div><h3>Sync your audiobooks and ebooks</h3><p>Storyteller is a platform for automatically syncing audiobooks and ebooks.</p></div><div><h3>Read or listen however you choose</h3><p>Storyteller produces EPUB 3 compliant ebook files. You can read them with any ebook reader (software or hardware!) that supports EPUB Media Overlays, or you can use the dedicated<!-- --> <a href="https://smoores.gitlab.io/storyteller/docs/reading-your-books/storyteller-apps">Storyteller mobile apps</a>.</p></div><div><h3>Own your books</h3><p>Storyteller is completely<!-- --> <a href="https://smoores.gitlab.io/storyteller/docs/getting-started">self-hosted</a>. All of your books stay on your hardware, and you're free to move, copy, and back them up as needed.</p></div></div></main><p><a href="https://www.vecteezy.com/free-vector/ebook">Ebook Vectors by Vecteezy</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Beeper Mini is now open-source (122 pts)]]></title>
            <link>https://github.com/beeper/imessage</link>
            <guid>38747482</guid>
            <pubDate>Sat, 23 Dec 2023 19:49:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/beeper/imessage">https://github.com/beeper/imessage</a>, See on <a href="https://news.ycombinator.com/item?id=38747482">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">beeper-imessage</h2>
<p dir="auto">A Matrix-iMessage puppeting bridge.</p>
<h2 tabindex="-1" dir="auto">Documentation</h2>
<p dir="auto">The bridge works like any other mautrix-go bridge, so the instructions at
<a href="https://docs.mau.fi/bridges/go/setup.html" rel="nofollow">https://docs.mau.fi/bridges/go/setup.html</a> can be applied directly.
You can find precompiled binaries from the GitLab CI at
<a href="https://mau.dev/mautrix/imessagego" rel="nofollow">https://mau.dev/mautrix/imessagego</a>.</p>
<p dir="auto">Additionally, the bridge requires a registration provider running on a <a href="https://github.com/beeper/mac-registration-provider">Mac</a> or
<a href="https://github.com/beeper/phone-registration-provider">jailbroken iPhone</a>, as well as a <a href="https://github.com/beeper/registration-relay">relay server</a> to help the bridge and
registration provider connect to each other.</p>
<p dir="auto">When connecting the bridge to your Beeper account with bbctl, you don't need to
self-host the relay, you only need to run the provider.</p>
<h2 tabindex="-1" dir="auto">Discussion</h2>
<p dir="auto">Matrix room: <a href="https://matrix.to/#/#imessage:maunium.net" rel="nofollow">#imessage:maunium.net</a></p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rumble turns off access in Brazil because of government's censorship demands (104 pts)]]></title>
            <link>https://reclaimthenet.org/rumble-challenges-brazil-censorship-demands</link>
            <guid>38746460</guid>
            <pubDate>Sat, 23 Dec 2023 18:08:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://reclaimthenet.org/rumble-challenges-brazil-censorship-demands">https://reclaimthenet.org/rumble-challenges-brazil-censorship-demands</a>, See on <a href="https://news.ycombinator.com/item?id=38746460">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-id="15a99740" data-element_type="widget" data-widget_type="theme-post-content.default">
			
<p>Rumble, the popular free speech video platform, is blocking access to its services in Brazil, rather than ban specific creators on its platform. This decision follows a <a href="https://reclaimthenet.org/monark-ferreira-rumble-telegram">contentious legal dispute with Brazilian authorities</a> over content censorship demands.</p>
<p>According to a statement released by Rumble, the Brazilian courts had issued an order demanding the removal of certain creators from the platform. In response, Rumble emphasized its commitment to maintaining a free and open internet, stating that it refuses to alter its content policies or discriminate against users based on their views, regardless of popularity.</p>
<p>“Users with unpopular views are free to access our platform on the same terms as our millions of other users,” the statement from Rumble asserted. The company highlighted its dedication to not “move the goalposts” on its established content policies, standing firm against external pressures to censor creators.</p>
<p><img fetchpriority="high" decoding="async" src="https://reclaimthenet.org/wp-content/uploads/2023/12/Screenshot-2023-12-23-at-00.05.jpg.jpg" alt="" width="786" height="1036" srcset="https://reclaimthenet.org/wp-content/uploads/2023/12/Screenshot-2023-12-23-at-00.05.jpg.jpg 786w, https://reclaimthenet.org/wp-content/uploads/2023/12/Screenshot-2023-12-23-at-00.05.jpg-228x300.jpg 228w, https://reclaimthenet.org/wp-content/uploads/2023/12/Screenshot-2023-12-23-at-00.05.jpg-777x1024.jpg 777w, https://reclaimthenet.org/wp-content/uploads/2023/12/Screenshot-2023-12-23-at-00.05.jpg-768x1012.jpg 768w" sizes="(max-width: 786px) 100vw, 786px"></p>
<p>“Rumble is the only company at our scale that holds the line for free speech and American values,” Rumble CEO Chris Pavlovski posted on X. “It’s my wish, that one day, big tech joins us in this fight and responds like we do. That’s the day everyone wins. I will continue to lead by example until that day arrives.”</p>
<p>As a result of this impasse, Rumble has chosen to disable access to its platform for users in Brazil. <a href="https://reclaimthenet.org/rumble-fires-back-at-frances-censorship-demands">Similar to the situation with France</a>, this measure is part of the company’s strategy to legally challenge the demands of the Brazilian courts. While Rumble expressed disappointment over the impact this decision has on Brazilian users, who now lose access to a diverse array of content unless they <a href="https://reclaimthenet.org/recommended-vpns">use a VPN</a> to access the platform, the company remains resolute in its stance.</p>
<p>The platform’s officials hope for a reconsideration of the court’s decisions, aiming to restore service in Brazil soon.</p>

<!-- AI CONTENT END 1 -->
		</div></div>]]></description>
        </item>
    </channel>
</rss>