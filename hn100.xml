<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 10 Nov 2024 21:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[IMG_0416 (121 pts)]]></title>
            <link>https://ben-mini.github.io/2024/img-0416</link>
            <guid>42102506</guid>
            <pubDate>Sun, 10 Nov 2024 20:45:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ben-mini.github.io/2024/img-0416">https://ben-mini.github.io/2024/img-0416</a>, See on <a href="https://news.ycombinator.com/item?id=42102506">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
        <header>
          
            
            
            

  <p><time datetime="2024-11-03T00:00:00+00:00">November 3, 2024</time></p>

 <!-- Moved the date here -->
          
          


        </header>
      

      <hr>

      <section itemprop="text">
        
        <p>Between 2009 and 2012, Apple iPhones and iPod Touches included a feature called “Send to YouTube” that allowed users to upload videos directly to YouTube from the Photos app.</p>

<p><img src="https://ben-mini.github.io/assets/images/iphone-youtube.jpg" alt="iphone-youtube"></p>

<p>The feature worked… really well. In fact, <a href="https://www.macrumors.com/2009/06/25/youtube-daily-mobile-uploads-have-increased-400-since-launch-of-iphone-3gs/">YouTube reported a 1700% increase in total video uploads</a> during the first half of 2009- crediting that growth to its strong integrative ties to Apple and social networks. However, this two-click upload feature was short-lived when Apple severed ties Apple severed ties with YouTube by <a href="https://archive.nytimes.com/bits.blogs.nytimes.com/2012/08/06/apple-to-remove-youtube-app-from-iphone-and-ipad/">removing its homegrown app in 2012</a>.</p>

<p>While Send to YouTube can be thoroughly analyzed as a milestone on the “frenemy” timeline between Apple and Google, I want to explore a pleasant consequence of this moment. Apple uses the ‘IMG_XXXX’ naming convention for all images and videos captured on iOS devices, where XXXX is a unique sequence number. The first image you take is named “IMG_0001”, the second is “IMG_0002” and so on. During the Send to YouTube era of 2009 and 2012, the title of one’s YouTube video was defaulted to this naming convention. Unwitting content creators would then upload their videos on a public site with a barely-searchable name. To this day, <strong><em>there are millions of these videos.</em></strong></p>

<p><img src="https://ben-mini.github.io/assets/images/img-yt-search.png" alt="Screenshot 2024-11-03 at 10.37.58 AM"></p>

<p>Try searching for “IMG_XXXX” on YouTube, replacing “XXXX” with your favorite numbers (I used my birthday, 0416). See what you get!</p>

<p>There’s something surreal about these videos that engages you in a way you’ve never felt. None were edited, produced or paraded for mass viewing. In fact, many were likely uploaded by accident or with a misunderstanding that complete strangers could see it. YouTube automatically removes harmful or violent content, so what remains exists in a unique, almost paradoxical state: <em>forbidden, yet harmless.</em> Putting all this together, searching IMG_XXXX presents the viewer with the most authentic social feed ever seen on the Internet- in video, no less!</p>

<p>While many videos are redundant snippets of a concerts, basketball games, or kids’ recitals, you also get one-of-a-kind videos that provides a glimpse into a complete stranger’s life. You’ll see a tumultuous event that made them, their partner, or their friend say, “hey, let’s record this”. I’d like to show you three of these videos that I found in my search.</p>

<h3 id="img_0416-mar-17-2015---23-views-"><a href="https://www.youtube.com/watch?v=MR3mv5SbAi4">IMG_0416 (Mar 17, 2015) - 23 views </a></h3>

<!-- Courtesy of embedresponsively.com //-->

<p>
    <iframe src="https://www.youtube-nocookie.com/embed/MR3mv5SbAi4" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen=""></iframe>
  </p>

<p>The video shows a woman excitedly unboxing a book she received in the mail. From context clues, she seems to be a wife and mother from Memphis who’s unboxing the first published copy of her book. She thanks the friends, family, and publishers who made this happen.</p>

<p>After a quick Google Search, I was able to find the book: <u>*A Profit / Prophet to Her Husband: Are you ready to be a wife?*</u> The book is meant “to help wives understand who they are and who they were designed to be.” It clocks in at 94 pages and has 30 ratings on Amazon!  Go IMG_0416! I don’t care what you’re creating- <a href="https://ben-mini.github.io/2023/the-meaning-of-life">I’m just a fan of creators</a>. It looks like she kept at it- <a href="https://www.amazon.com/Secret-Loving-Yourself-Unconditionally-Self-Worth-ebook/dp/B086PVGPHZ?ref_=ast_author_dp">making a second book in 2020</a>!</p>

<h3 id="img_0416mov-june-24-2015---26-views"><a href="https://www.youtube.com/watch?v=JDuGXBteSno">IMG_0416.MOV (June 24, 2015) - 26 views</a></h3>

<!-- Courtesy of embedresponsively.com //-->

<p>
    <iframe src="https://www.youtube-nocookie.com/embed/JDuGXBteSno" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen=""></iframe>
  </p>

<p>The video appears to show a woman playing a matching card game that teaches you “the basic of the potash stuff” according to the cameraman. As the woman (who I assume is the cameraman’s supportive mother) flips two matching cards, she reads off the countries who produce the most <a href="https://en.wikipedia.org/wiki/Potash">potash</a>.</p>

<p>I honestly didn’t know anything about potash! Turns out that it is a mineral with large amount of potassium, which is helpful as a plant fertilizer. With Canada producing the largest reserves in the world, <a href="https://arc.net/l/quote/fdbongnj">the vast majority of Canadian potash is found in Saskatchewan</a>. I wonder if the family in the video  lives in Canada. Or, this is just another school project that teaches you random facts… I miss those!</p>

<h3 id="img_0416-feb-8-2011---114-views"><a href="https://www.youtube.com/shorts/HXBKniNtsHk">IMG_0416 (Feb 8, 2011) - 114 views</a></h3>

<!-- Courtesy of embedresponsively.com //-->

<p>
    <iframe src="https://www.youtube-nocookie.com/embed/HXBKniNtsHk" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen=""></iframe>
  </p>

<p>Let’s end of a fun one. The video shows a young man snorting powdered sugar and dealing with the consequences of it. Given his BU hoodie, Dunkin’ Donuts location, and ironic depiction of drug use, I gotta say this is a <strong>VERY</strong> Boston video.</p>

<p>What’s genuinely heartwarming is the shared laughter between the man, the camerawoman, and a motherly figure leaving Dunkin’. The camerawoman calls her “Myra” at the end, suggesting they all know each other. This mix of community at franchised restaurant, and the remark about having “nothing better to do” perfectly captures the heartbeat of American suburbia.</p>

        
      </section>

      

      

      
  

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pi Chess Board (105 pts)]]></title>
            <link>https://readymag.website/u2481798807/5057562/</link>
            <guid>42101742</guid>
            <pubDate>Sun, 10 Nov 2024 18:40:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://readymag.website/u2481798807/5057562/">https://readymag.website/u2481798807/5057562/</a>, See on <a href="https://news.ycombinator.com/item?id=42101742">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Procrastination and the fear of not being good enough (184 pts)]]></title>
            <link>https://swapnilchauhan.com/blog/procrastination-and-the-fear-of-not-being-good-enough</link>
            <guid>42101327</guid>
            <pubDate>Sun, 10 Nov 2024 17:23:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://swapnilchauhan.com/blog/procrastination-and-the-fear-of-not-being-good-enough">https://swapnilchauhan.com/blog/procrastination-and-the-fear-of-not-being-good-enough</a>, See on <a href="https://news.ycombinator.com/item?id=42101327">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[What's New in F# 9 (150 pts)]]></title>
            <link>https://learn.microsoft.com/en-us/dotnet/fsharp/whats-new/fsharp-9</link>
            <guid>42101312</guid>
            <pubDate>Sun, 10 Nov 2024 17:20:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://learn.microsoft.com/en-us/dotnet/fsharp/whats-new/fsharp-9">https://learn.microsoft.com/en-us/dotnet/fsharp/whats-new/fsharp-9</a>, See on <a href="https://news.ycombinator.com/item?id=42101312">Hacker News</a></p>
<div id="readability-page-1" class="page">
	<div>
		<a href="#main" tabindex="1">Skip to main content</a>

		<div id="unsupported-browser" hidden="">
				<p>This browser is no longer supported.</p>
				<p>Upgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support.</p>
				
			</div>
		<!-- liquid-tag banners global -->

		<!-- site header -->
		<header id="ms--site-header" data-test-id="site-header-wrapper" role="banner" itemscope="itemscope" itemtype="http://schema.org/Organization">
			
			
			
		</header>
	</div>

	<div data-bi-name="body"><div id="main-column">

						<main id="main" role="main" data-bi-name="content" lang="en-us" dir="ltr"><!-- article-header -->
							
							<!-- end article-header --><!-- end mobile-contents button  -->

							<div><h2 id="whats-new-in-f-9">What's new in F# 9</h2><div>
											<ul data-bi-name="page info" lang="en-us" dir="ltr"><li>Article</li><li><time data-article-date="" aria-label="Article review date" datetime="2024-11-12T08:00:00Z" data-article-date-source="calculated">11/12/2024</time>
															</li><li>
															
														</li></ul>
										</div><nav id="center-doc-outline" data-bi-name="intopic toc" aria-label="In this article">
											<h2 id="ms--in-this-article">In this article</h2>
										</nav><!-- <content> --><p>F# 9 introduces a range of enhancements that make your programs safer, more resilient, and performant. This article highlights the major changes in F# 9, developed in the <a href="https://github.com/dotnet/fsharp" data-linktype="external">F# open source code repository</a>.</p>
<p>F# 9 is available in .NET 9. You can download the latest .NET SDK from the <a href="https://dotnet.microsoft.com/download" data-linktype="external">.NET downloads page</a>.</p>
<h2 id="nullable-reference-types">Nullable reference types</h2>
<p>Although F# is designed to avoid <code>null</code>, it can creep in when interfacing with .NET libraries written in C#. F# now provides a type-safe way to deal with reference types that can have <code>null</code> as a valid value.</p>
<p>For more details, watch out for an <a href="https://devblogs.microsoft.com/dotnet/tag/f/" data-linktype="external">upcoming blog post about this feature</a>.</p>
<p>Here are some examples:</p>
<pre><code>// Declared type at let-binding
let notAValue: string | null = null

let isAValue: string | null = "hello world"

let isNotAValue2: string = null // gives a nullability warning

let getLength (x: string | null) = x.Length // gives a nullability warning since x is a nullable string

// Parameter to a function
let len (str: string | null) =
    match str with
    | null -&gt; -1
    | NonNull s -&gt; s.Length  // binds a non-null result

// Parameter to a function
let len (str: string | null) =
    let s = nullArgCheck "str" str // Returns a non-null string
    s.Length  // binds a non-null result

// Declared type at let-binding
let maybeAValue: string | null = hopefullyGetAString()

// Array type signature
let f (arr: (string | null)[]) = ()

// Generic code, note 'T must be constrained to be a reference type
let findOrNull (index: int) (list: 'T list) : 'T | null when 'T : not struct =
    match List.tryItem index list with
    | Some item -&gt; item
    | None -&gt; null
</code></pre>
<h2 id="discriminated-union-is-properties">Discriminated union <code>.Is*</code> properties</h2>
<p>Discriminated unions now have auto-generated properties for each case, allowing you to check if a value is of a particular case. For example, for the following type:</p>
<pre><code>type Contact =
    | Email of address: string
    | Phone of countryCode: int * number: string

type Person = { name: string; contact: Contact }
</code></pre>
<p>Previously, you had to write something like:</p>
<pre><code>let canSendEmailTo person =
    match person.contact with
    | Email _ -&gt; true
    | _ -&gt; false
</code></pre>
<p>Now, you can instead write:</p>
<pre><code>let canSendEmailTo person =
    person.contact.IsEmail
</code></pre>
<h2 id="partial-active-patterns-can-return-bool-instead-of-unit-option">Partial active patterns can return <code>bool</code> instead of <code>unit option</code></h2>
<p>Previously, partial active patterns returned <code>Some ()</code> to indicate a match and <code>None</code> otherwise. Now, they can also return <code>bool</code>.</p>
<p>For example, the active pattern for the following:</p>
<pre><code>match key with
| CaseInsensitive "foo" -&gt; ...
| CaseInsensitive "bar" -&gt; ...
</code></pre>
<p>Was previously written as:</p>
<pre><code>let (|CaseInsensitive|_|) (pattern: string) (value: string) =
    if String.Equals(value, pattern, StringComparison.OrdinalIgnoreCase) then
        Some ()
    else
        None
</code></pre>
<p>Now, you can instead write:</p>
<pre><code>let (|CaseInsensitive|_|) (pattern: string) (value: string) =
    String.Equals(value, pattern, StringComparison.OrdinalIgnoreCase)
</code></pre>
<h2 id="prefer-extension-methods-to-intrinsic-properties-when-arguments-are-provided">Prefer extension methods to intrinsic properties when arguments are provided</h2>
<p>To align with a pattern seen in some .NET libraries, where extension methods are defined with the same names as intrinsic properties of a type, F# now resolves these extension methods instead of failing the type check.</p>
<p>Example:</p>
<pre><code>type Foo() =
    member val X : int = 0 with get, set

[&lt;Extension&gt;]
type FooExt =
    [&lt;Extension&gt;]
    static member X (f: Foo, i: int) = f.X &lt;- i; f

let f = Foo()

f.X(1) // We can now call the extension method to set the property and chain further calls
</code></pre>
<h2 id="empty-bodied-computation-expressions">Empty-bodied computation expressions</h2>
<p>F# now supports empty <a href="https://learn.microsoft.com/en-us/dotnet/fsharp/language-reference/computation-expressions" data-linktype="relative-path">computation expressions</a>.</p>
<pre><code>let xs = seq { } // Empty sequence
</code></pre>
<pre><code>let html =
    div {
        p { "Some content." }
        p { } // Empty paragraph
    }
</code></pre>
<p>Writing an empty computation expression will result in a call to the computation expression builder's <code>Zero</code> method.</p>
<p>This is a more natural syntax compared to the previously available <code>builder { () }</code>.</p>
<h2 id="hash-directives-are-allowed-to-take-non-string-arguments">Hash directives are allowed to take non-string arguments</h2>
<p>Hash directives for the compiler previously only allowed string arguments passed in quotes. Now, they can take any type of argument.</p>
<p>Previously, you had:</p>
<pre><code>#nowarn "0070"
#time "on"
</code></pre>
<p>Now, you can write:</p>
<pre><code>#nowarn 0070
#time on
</code></pre>
<p>This also ties into the next two changes.</p>
<h2 id="extended-help-directive-in-fsi-to-show-documentation-in-the-repl">Extended #help directive in fsi to show documentation in the REPL</h2>
<p>The <code>#help</code> directive in F# Interactive now shows documentation for a given object or function, which you can now pass without quotes.</p>
<pre><code>&gt; #help List.map;;

Description:
Builds a new collection whose elements are the results of applying the given function
to each of the elements of the collection.

Parameters:
- mapping: The function to transform elements from the input list.
- list: The input list.
Returns:
The list of transformed elements.

Examples:
let inputs = [ "a"; "bbb"; "cc" ]

inputs |&gt; List.map (fun x -&gt; x.Length)
// Evaluates to [ 1; 3; 2 ]

Full name: Microsoft.FSharp.Collections.ListModule.map
Assembly: FSharp.Core.dll
</code></pre>
<p>See <a href="https://devblogs.microsoft.com/dotnet/enhancing-help-in-fsi/" data-linktype="external">Enhancing #help in F# Interactive blog post</a> for more details.</p>
<h2 id="allow-nowarn-to-support-the-fs-prefix-on-error-codes-to-disable-warnings">Allow #nowarn to support the FS prefix on error codes to disable warnings</h2>
<p>Previously, when you wanted to disable a warning and wrote <code>#nowarn "FS0057"</code>, you would get an <code>Invalid warning number 'FS0057'</code>. Even though the warning number is correct, it just wasn't supposed to have the <code>FS</code> prefix.</p>
<p>Now, you won't have to spend time figuring that out because the warning numbers are accepted even with the prefix.</p>
<p>All of these will now work:</p>
<pre><code>#nowarn 57
#nowarn 0057
#nowarn FS0057

#nowarn "57"
#nowarn "0057"
#nowarn "FS0057"
</code></pre>
<p>It's a good idea to use the same style throughout your project.</p>
<h2 id="warning-about-tailcall-attribute-on-non-recursive-functions-or-let-bound-values">Warning about TailCall attribute on non-recursive functions or let-bound values</h2>
<p>F# now emits a warning when you put the <code>[&lt;TailCall&gt;]</code> attribute somewhere it doesn't belong. While it has no effect on what the code does, it could confuse someone reading it.</p>
<p>For example, these usages will now emit a warning:</p>
<pre><code>[&lt;TailCall&gt;]
let someNonRecFun x = x + x

[&lt;TailCall&gt;]
let someX = 23

[&lt;TailCall&gt;]
let rec someRecLetBoundValue = nameof(someRecLetBoundValue)
</code></pre>
<h2 id="enforce-attribute-targets">Enforce attribute targets</h2>
<p>The compiler now correctly enforces the <code>AttributeTargets</code> on let values, functions, union case declarations, implicit constructors, structs, and classes. This can prevent some hard-to-notice bugs, such as forgetting to add the unit argument to an Xunit test.</p>
<p>Previously, you could write:</p>
<pre><code>[&lt;Fact&gt;]
let ``this test always fails`` =
  Assert.True(false)
</code></pre>
<p>When you ran the tests with <code>dotnet test</code>, they would pass. Since the test function is not actually a function, it was ignored by the test runner.</p>
<p>Now, with correct attribute enforcement, you will get an <code>error FS0842: This attribute is not valid for use on this language element</code>.</p>
<h2 id="updates-to-the-standard-library-fsharpcore">Updates to the <a href="https://fsharp.github.io/fsharp-core-docs/" data-linktype="external">standard library (FSharp.Core)</a></h2>
<h3 id="random-functions-for-collections">Random functions for collections</h3>
<p>The <code>List</code>, <code>Array</code>, and <code>Seq</code> modules have new functions for random sampling and shuffling. This makes F# easier to use for common data science, machine learning, game development, and other scenarios where randomness is needed.</p>
<p>All functions have the following variants:</p>
<ul>
<li>One that uses an implicit, thread-safe, shared <a href="https://learn.microsoft.com/en-us/dotnet/api/system.random" data-linktype="absolute-path">Random</a> instance</li>
<li>One that takes a <code>Random</code> instance as an argument</li>
<li>One that takes a custom <code>randomizer</code> function, which should return a float value greater than or equal to 0.0 and less than 1.0</li>
</ul>
<p>There are four functions (each with three variants) available: <code>Shuffle</code>, <code>Choice</code>, <code>Choices</code>, and <code>Sample</code>.</p>
<h4 id="shuffle">Shuffle</h4>
<p>The <code>Shuffle</code> functions return a new collection of the same type and size, with each item in a randomly mixed position. The chance to end up in any position is weighted evenly on the length of the collection.</p>
<pre><code>let allPlayers = [ "Alice"; "Bob"; "Charlie"; "Dave" ]
let round1Order = allPlayers |&gt; List.randomShuffle // [ "Charlie"; "Dave"; "Alice"; "Bob" ]
</code></pre>
<p>For arrays, there are also <code>InPlace</code> variants that shuffle the items in the existing array instead of creating a new one.</p>
<h4 id="choice">Choice</h4>
<p>The <code>Choice</code> functions return a single random element from the given collection. The random choice is weighted evenly on the size of the collection.</p>
<pre><code>let allPlayers = [ "Alice"; "Bob"; "Charlie"; "Dave" ]
let randomPlayer = allPlayers |&gt; List.randomChoice // "Charlie"
</code></pre>
<h4 id="choices">Choices</h4>
<p>The <code>Choices</code> functions select N elements from the input collection in random order, allowing elements to be selected more than once.</p>
<pre><code>let weather = [ "Raining"; "Sunny"; "Snowing"; "Windy" ]
let forecastForNext3Days = weather |&gt; List.randomChoices 3 // [ "Windy"; "Snowing"; "Windy" ]
</code></pre>
<h4 id="sample">Sample</h4>
<p>The <code>Sample</code> functions select N elements from the input collection in random order, without allowing elements to be selected more than once. N cannot be greater than the collection length.</p>
<pre><code>let foods = [ "Apple"; "Banana"; "Carrot"; "Donut"; "Egg" ]
let today'sMenu = foods |&gt; List.randomSample 3 // [ "Donut"; "Apple"; "Egg" ]
</code></pre>
<p>For a full list of functions and their variants, see (<a href="https://github.com/fsharp/fslang-design/blob/main/RFCs/FS-1135-random-functions-for-collections.md" data-linktype="external">RFC #1135</a>).</p>
<h3 id="parameterless-constructor-for-customoperationattribute">Parameterless constructor for <code>CustomOperationAttribute</code></h3>
<p>This constructor makes it easier to create a custom operation for a computation expression builder. It uses the name of the method instead of having to explicitly name it (when in most cases the name matches the method name already).</p>
<pre><code>type FooBuilder() =
    [&lt;CustomOperation&gt;]  // Previously had to be [&lt;CustomOperation("bar")&gt;]
    member _.bar(state) = state
</code></pre>
<h3 id="c-collection-expression-support-for-f-lists-and-sets">C# collection expression support for F# lists and sets</h3>
<p>When using F# lists and sets from C#, you can now use collection expressions to initialize them.</p>
<p>Instead of:</p>
<pre><code>FSharpSet&lt;int&gt; mySet = SetModule.FromArray([1, 2, 3]);
</code></pre>
<p>You can now write:</p>
<pre><code>FSharpSet&lt;int&gt; mySet = [ 1, 2, 3 ];
</code></pre>
<p>Collection expressions make it easier to use the F# immutable collections from C#. You might want to use the F# collections when you need their structural equality, which <a href="https://learn.microsoft.com/en-us/dotnet/api/system.collections.immutable" data-linktype="absolute-path">System.Collections.Immutable</a> collections don't have.</p>
<h2 id="developer-productivity-improvements">Developer productivity improvements</h2>
<h3 id="parser-recovery">Parser recovery</h3>
<p>There have been continuous improvements in parser recovery, meaning that tooling (for example, syntax highlighting) still works with code when you're in the middle of editing it and it might not be syntactically correct at all times.</p>
<p>For example, the parser will now recover on unfinished <code>as</code> patterns, object expressions, enum case declarations, record declarations, complex primary constructor patterns, unresolved long identifiers, empty match clauses, missing union case fields, and missing union case field types.</p>
<h3 id="diagnostics">Diagnostics</h3>
<p>Diagnostics, or understanding what the compiler doesn't like about your code, are an important part of the user experience with F#. There are a number of new or improved diagnostic messages or more precise diagnostic locations in F# 9.</p>
<p>These include:</p>
<ul>
<li>Ambiguous override method in object expression</li>
<li>Abstract members when used in non-abstract classes</li>
<li>Property that has the same name as a discriminated union case</li>
<li>Active pattern argument count mismatch</li>
<li>Unions with duplicated fields</li>
<li>Using <code>use!</code> with <code>and!</code> in computation expressions</li>
</ul>
<p>There is also a new compile-time error for classes with over 65,520 methods in generated <a href="https://learn.microsoft.com/en-us/dotnet/standard/managed-code#intermediate-language--execution" data-linktype="relative-path">IL</a>. Such classes aren't loadable by the CLR and result in a run-time error. (You won't author that many methods, but there have been cases with generated code.)</p>
<h3 id="real-visibility">Real visibility</h3>
<p>There is a quirk with how F# generates assemblies that results in private members being written to <a href="https://learn.microsoft.com/en-us/dotnet/standard/managed-code#intermediate-language--execution" data-linktype="relative-path">IL</a> as internal. This allows inappropriate access to private members from non-F# projects that have access to an F# project via <a href="https://learn.microsoft.com/en-us/dotnet/api/system.runtime.compilerservices.internalsvisibletoattribute" data-linktype="absolute-path"><code>InternalsVisibleTo</code></a>.</p>
<p>Now, there is an opt-in fix for this behavior available via the <code>--realsig+</code> compiler flag. Try it in your solution to see if any of your projects depend on this behavior. You can add it to your <code>.fsproj</code> files like this:</p>
<pre><code>&lt;PropertyGroup&gt;
    &lt;RealSig&gt;true&lt;/RealSig&gt;
&lt;/PropertyGroup&gt;
</code></pre>
<h2 id="performance-improvements">Performance improvements</h2>
<h3 id="optimized-equality-checks">Optimized equality checks</h3>
<p>Equality checks are now faster and allocate less memory.</p>
<p>For example:</p>
<pre><code>[&lt;Struct&gt;]
type MyId =
    val Id: int
    new id = { Id = id }

let ids = Array.init 1000 MyId
let missingId = MyId -1

// used to box 1000 times, doesn't box anymore
let _ = ids |&gt; Array.contains missingId
</code></pre>
<h4 id="benchmark-results-for-affected-array-functions-applied-to-a-2-member-struct">Benchmark results for affected array functions, applied to a 2-member struct</h4>
<p>Before:</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Mean</th>
<th>Error</th>
<th>Gen0</th>
<th>Allocated</th>
</tr>
</thead>
<tbody>
<tr>
<td>ArrayContainsExisting</td>
<td>15.48 ns</td>
<td>0.398 ns</td>
<td>0.0008</td>
<td>48 B</td>
</tr>
<tr>
<td>ArrayContainsNonexisting</td>
<td>5,190.95 ns</td>
<td>103.533 ns</td>
<td>0.3891</td>
<td>24000 B</td>
</tr>
<tr>
<td>ArrayExistsExisting</td>
<td>17.97 ns</td>
<td>0.389 ns</td>
<td>0.0012</td>
<td>72 B</td>
</tr>
<tr>
<td>ArrayExistsNonexisting</td>
<td>5,316.64 ns</td>
<td>103.776 ns</td>
<td>0.3891</td>
<td>24024 B</td>
</tr>
<tr>
<td>ArrayTryFindExisting</td>
<td>24.80 ns</td>
<td>0.554 ns</td>
<td>0.0015</td>
<td>96 B</td>
</tr>
<tr>
<td>ArrayTryFindNonexisting</td>
<td>5,139.58 ns</td>
<td>260.949 ns</td>
<td>0.3891</td>
<td>24024 B</td>
</tr>
<tr>
<td>ArrayTryFindIndexExisting</td>
<td>15.92 ns</td>
<td>0.526 ns</td>
<td>0.0015</td>
<td>96 B</td>
</tr>
<tr>
<td>ArrayTryFindIndexNonexisting</td>
<td>4,349.13 ns</td>
<td>100.750 ns</td>
<td>0.3891</td>
<td>24024 B</td>
</tr>
</tbody>
</table>
<p>After:</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Mean</th>
<th>Error</th>
<th>Gen0</th>
<th>Allocated</th>
</tr>
</thead>
<tbody>
<tr>
<td>ArrayContainsExisting</td>
<td>4.865 ns</td>
<td>0.3452 ns</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>ArrayContainsNonexisting</td>
<td>766.005 ns</td>
<td>15.2003 ns</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>ArrayExistsExisting</td>
<td>8.025 ns</td>
<td>0.1966 ns</td>
<td>0.0004</td>
<td>24 B</td>
</tr>
<tr>
<td>ArrayExistsNonexisting</td>
<td>834.811 ns</td>
<td>16.2784 ns</td>
<td>-</td>
<td>24 B</td>
</tr>
<tr>
<td>ArrayTryFindExisting</td>
<td>16.401 ns</td>
<td>0.3932 ns</td>
<td>0.0008</td>
<td>48 B</td>
</tr>
<tr>
<td>ArrayTryFindNonexisting</td>
<td>1,140.515 ns</td>
<td>22.7372 ns</td>
<td>-</td>
<td>24 B</td>
</tr>
<tr>
<td>ArrayTryFindIndexExisting</td>
<td>14.864 ns</td>
<td>0.3648 ns</td>
<td>0.0008</td>
<td>48 B</td>
</tr>
<tr>
<td>ArrayTryFindIndexNonexisting</td>
<td>990.028 ns</td>
<td>19.7157 ns</td>
<td>-</td>
<td>24 B</td>
</tr>
</tbody>
</table>
<p>You can read all the details here: <a href="https://devblogs.microsoft.com/dotnet/fsharp-developer-stories-how-weve-finally-fixed-a-9yearold-performance-issue/" data-linktype="external">F# Developer Stories: How we’ve finally fixed a 9-year-old performance issue</a>.</p>
<h3 id="field-sharing-for-struct-discriminated-unions">Field sharing for struct discriminated unions</h3>
<p>If fields in multiple cases of a struct discriminated union have the same name and type, they can share the same memory location, reducing the struct's memory footprint. (Previously, same field names weren't allowed, so there are no issues with binary compatibility.)</p>
<p>For example:</p>
<pre><code>[&lt;Struct&gt;]
type MyStructDU =
    | Length of int64&lt;meter&gt;
    | Time of int64&lt;second&gt;
    | Temperature of int64&lt;kelvin&gt;
    | Pressure of int64&lt;pascal&gt;
    | Abbrev of TypeAbbreviationForInt64
    | JustPlain of int64
    | MyUnit of int64&lt;MyUnit&gt;

sizeof&lt;MyStructDU&gt; // 16 bytes
</code></pre>
<p>Comparing to previous verion (where you had to use unique field names):</p>
<pre><code>[&lt;Struct&gt;]
type MyStructDU =
    | Length of length: int64&lt;meter&gt;
    | Time of time: int64&lt;second&gt;
    | Temperature of temperature: int64&lt;kelvin&gt;
    | Pressure of pressure: int64&lt;pascal&gt;
    | Abbrev of abbrev: TypeAbbreviationForInt64
    | JustPlain of plain: int64
    | MyUnit of myUnit: int64&lt;MyUnit&gt;

sizeof&lt;MyStructDU&gt; // 60 bytes
</code></pre>
<h3 id="integral-range-optimizations">Integral range optimizations</h3>
<p>The compiler now generates optimized code for more instances of <code>start..finish</code> and <code>start..step..finish</code> expressions. Previously, these were only optimized when the type was <code>int</code>/<code>int32</code> and the step was a constant <code>1</code> or <code>-1</code>. Other integral types and different step values used an inefficient <code>IEnumerable</code>-based implementation. Now, all of these are optimized.</p>
<p>This leads to anywhere from 1.25× up to 8× speed up in loops:</p>
<pre><code>for … in start..finish do …
</code></pre>
<p>List/array expressions:</p>
<pre><code>[start..step..finish]
</code></pre>
<p>and comprehensions:</p>
<pre><code>[for n in start..finish -&gt; f n]
</code></pre>
<h3 id="optimized-for-x-in-xs----in-list-and-array-comprehensions">Optimized <code>for x in xs -&gt; …</code> in list and array comprehensions</h3>
<p>On a related note, comprehensions with <code>for x in xs -&gt; …</code> have been optimized for lists and arrays, with notable improvements especially for arrays, with speedups up to 10× and ⅓ to ¼ allocation size.</p>

<h3 id="live-buffers-in-visual-studio">Live buffers in Visual Studio</h3>
<p>This previously opt-in feature has been thoroughly tested and is now enabled by default. The background compiler powering the IDE now works with live file buffers, meaning you don't have to save the files to disk to get the changes applied. Previously, this could cause some unexpected behavior. (Most notoriously when you tried to rename a symbol present in a file that had been edited but not saved.)</p>
<h3 id="analyzer-and-code-fix-for-removing-unnecessary-parentheses">Analyzer and code fix for removing unnecessary parentheses</h3>
<p>Sometimes extra parentheses are used for clarity, but sometimes they are just noise. For the latter case, you now get a code fix in Visual Studio to remove them.</p>
<p>For example:</p>
<pre><code>let f (x) = x // -&gt; let f x = x
let _ = (2 * 2) + 3 // -&gt; let _ = 2 * 2 + 3
</code></pre>
<h3 id="custom-visualizer-support-for-f-in-visual-studio">Custom visualizer support for F# in Visual Studio</h3>
<p>The debugger visualizer in Visual Studio now works with F# projects.</p>
<p><img src="https://learn.microsoft.com/en-us/dotnet/fsharp/media/whats-new/fsharp-9/vs-visualizer.gif" alt="debug visualizer" data-linktype="relative-path"></p>
<h3 id="signature-tooltips-shown-mid-pipeline">Signature tooltips shown mid-pipeline</h3>
<p>Previously, signature help wasn't offered in a situation like the following, where a function in the middle of a pipeline already had a complex curried parameter (for example, a lambda) applied to it. Now, the signature tooltip shows up for the next parameter (<code>state</code>):</p>
<p><img src="https://learn.microsoft.com/en-us/dotnet/fsharp/media/whats-new/fsharp-9/help.png" alt="tooltip" data-linktype="relative-path"></p>
</div>
							
							<!-- </content> -->

						</main><!-- recommendations section --><!-- end recommendations section -->

						<!-- feedback section --><div data-bi-name="open-source-feedback-section" data-open-source-feedback-section="" hidden="">
				<div>
					<span aria-hidden="true">
						<span></span>
					</span>
					<span>Collaborate with us on GitHub</span>
				</div>
				<span>
					The source for this content can be found on GitHub, where you can also create and review issues and pull requests. For more information, see <a href="https://learn.microsoft.com/contribute/content/dotnet/dotnet-contribute">our contributor guide</a>.
				</span>
			</div><!-- end feedback section -->

						<!-- feedback report section --><!-- end feedback report section --></div><div id="ms--additional-resources" data-bi-name="pageactions" role="complementary" aria-label="Additional resources">
								<h2 id="ms--additional-resources-heading" hidden="">Additional resources</h2>
								
								
								
								<nav id="side-doc-outline" data-bi-name="intopic toc" aria-label="In this article">
									<h3>In this article</h3>
								</nav>
								
							</div></div>
	<!--end of .mainContainer -->

	

	

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenID Connect specifications published as ISO standards (174 pts)]]></title>
            <link>https://self-issued.info/?p=2573</link>
            <guid>42101181</guid>
            <pubDate>Sun, 10 Nov 2024 16:53:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://self-issued.info/?p=2573">https://self-issued.info/?p=2573</a>, See on <a href="https://news.ycombinator.com/item?id=42101181">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

									<p><span><img decoding="async" src="https://self-issued.info/images/openid-logo.png" alt="OpenID logo"></span>I’m thrilled to report that the <a href="https://openid.net/connect">OpenID Connect</a> specifications have now been published as ISO/IEC standards.  They are:</p>
<ul>
<li><a href="https://www.iso.org/standard/89056.html">ISO/IEC 26131:2024 — Information technology — OpenID connect — OpenID connect core 1.0 incorporating errata set 2</a></li>
<li><a href="https://www.iso.org/standard/89057.html">ISO/IEC 26132:2024 — Information technology — OpenID connect — OpenID connect discovery 1.0 incorporating errata set 2</a></li>
<li><a href="https://www.iso.org/standard/89059.html">ISO/IEC 26133:2024 — Information technology — OpenID connect — OpenID connect dynamic client registration 1.0 incorporating errata set 2</a></li>
<li><a href="https://www.iso.org/standard/89060.html">ISO/IEC 26134:2024 — Information technology — OpenID connect — OpenID connect RP-initiated logout 1.0</a></li>
<li><a href="https://www.iso.org/standard/89061.html">ISO/IEC 26135:2024 — Information technology — OpenID connect — OpenID connect session management 1.0</a></li>
<li><a href="https://www.iso.org/standard/89062.html">ISO/IEC 26136:2024 — Information technology — OpenID connect — OpenID connect front-channel logout 1.0</a></li>
<li><a href="https://www.iso.org/standard/89063.html">ISO/IEC 26137:2024 — Information technology — OpenID connect — OpenID connect back-channel logout 1.0 incorporating errata set 1</a></li>
<li><a href="https://www.iso.org/standard/89064.html">ISO/IEC 26138:2024 — Information technology — OpenID connect — OAuth 2.0 multiple response type encoding practices</a></li>
<li><a href="https://www.iso.org/standard/89065.html">ISO/IEC 26139:2024 — Information technology — OpenID connect — OAuth 2.0 form post response mode</a></li>
</ul>
<p>I submitted the OpenID Connect specifications for publication by ISO as <a href="https://www.iso.org/deliverables-all.html#PAS">Publicly Available Specifications (PAS)</a> for the OpenID Foundation in December 2023.  Following the ISO approval vote, they are now published.  This should foster even broader adoption of OpenID Connect by enabling deployments in jurisdictions around the world that have legal requirements to use specifications from standards bodies recognized by international treaties, of which ISO is one.</p>
<p>Before submitting the specifications, the <a href="https://openid.net/wg/connect/">OpenID Connect working group</a> diligently worked through the process of applying <a href="https://self-issued.info/?p=2454">errata corrections</a> to the specifications, so that the ISO versions would have all known corrections incorporated.</p>
<p>Having successfully gone through the ISO PAS submission process once, the OpenID Foundation now plans to submit additional families of final specifications for publication by ISO.  These include the <a href="https://openid.net/specs/openid-financial-api-part-2-1_0.html">FAPI 1.0</a> specifications, and once they’re final, the <a href="https://openid.net/public-review-proposed-final-openid-connect-for-identity-assurance/">eKYC-IDA</a> specifications and <a href="https://openid.net/specs/fapi-2_0-security-profile.html">FAPI 2.0</a> specifications.</p>
<p>Thanks to all who helped us achieve this significant accomplishment!</p>
<p><a href="https://www.iso.org/standard/89056.html"><img decoding="async" src="https://self-issued.info/images/ISO-IEC_26131-2024_Cover.png" alt="ISO/IEC 26131:2024 Cover Page"></a></p>

								</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[JVM Anatomy Quarks (116 pts)]]></title>
            <link>https://shipilev.net/jvm/anatomy-quarks/</link>
            <guid>42100876</guid>
            <pubDate>Sun, 10 Nov 2024 16:09:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://shipilev.net/jvm/anatomy-quarks/">https://shipilev.net/jvm/anatomy-quarks/</a>, See on <a href="https://news.ycombinator.com/item?id=42100876">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
<div>
<p><a href="https://shipilev.net/jvm/anatomy-quarks/">"JVM Anatomy Quarks"</a> is the on-going mini-post series, where every post is describing some elementary piece of knowledge about JVM. The name underlines the fact that the single post cannot be taken in isolation, and most pieces described here are going to readily interact with each other.</p>
<p>The post should take about 5-10 minutes to read. As such, it goes deep for only a single topic, a single test, a single benchmark, a single observation. The evidence and discussion here might be anecdotal, not actually reviewed for errors, consistency, writing 'tyle, syntaxtic and semantically errors, duplicates, or also consistency. Use and/or trust this at your own risk.</p>
<div>
<p><img src="https://shipilev.net/jvm/anatomy-quarks/images/redhat-logo.svg" alt="350" width="85">
</p>
</div>
<p><strong>Aleksey Shipilëv, JVM/Performance Geek</strong><br>
Shout out at Twitter: <a href="http://twitter.com/shipilev">@shipilev</a>; Questions, comments, suggestions: <a href="mailto:aleksey@shipilev.net">aleksey@shipilev.net</a><br></p>
</div>
<div>
<h2 id="_complete_snapshots"><a href="#_complete_snapshots"></a>Complete Snapshots</h2>
<div>
<p>The series is on-going, the auto-generated complete bundles are here:<br>
  <a href="https://shipilev.net/jvm/anatomy-quarks/jvm-anatomy-quarks-complete.epub">ePUB</a> (smallest, under MB, Pandoc HTML-to-ePUB)<br>
  <a href="https://shipilev.net/jvm/anatomy-quarks/jvm-anatomy-quarks-complete.mobi">MOBI</a> (small, around MB, KindleGen ePUB-to-MOBI)<br>
  <a href="https://shipilev.net/jvm/anatomy-quarks/jvm-anatomy-quarks-complete.pdf">PDF</a> (very large — tens of MBs, high-quality wkhtmltopdf HTML-to-PDF)<br></p>
</div>
</div>
<div>
<h2 id="_individual_index"><a href="#_individual_index"></a>Individual Index</h2>

</div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Everything I've learned so far about running local LLMs (105 pts)]]></title>
            <link>https://nullprogram.com/blog/2024/11/10/</link>
            <guid>42100560</guid>
            <pubDate>Sun, 10 Nov 2024 14:53:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nullprogram.com/blog/2024/11/10/">https://nullprogram.com/blog/2024/11/10/</a>, See on <a href="https://news.ycombinator.com/item?id=42100560">Hacker News</a></p>
<div id="readability-page-1" class="page"><div lang="en">
<article>
  
  <time datetime="2024-11-10">
    November 10, 2024
  </time>
  <p>
    nullprogram.com/blog/2024/11/10/
  </p>

  <p>Over the past month I’ve been exploring the rapidly evolving world of
Large Language Models (LLM). It’s now accessible enough to run a LLM on a
Raspberry Pi smarter than the original ChatGPT (November 2022). A modest
desktop or laptop supports even smarter AI. It’s also private, offline,
unlimited, and registration-free. The technology is improving at breakneck
speed, and information is outdated in a matter of months. This article
snapshots my practical, hands-on knowledge and experiences — information I
wish I had when starting. Keep in mind that I’m a LLM layman, I have no
novel insights to share, and it’s likely I’ve misunderstood certain
aspects. In a year this article will mostly be a historical footnote,
which is simultaneously exciting and scary.</p>

<p>In case you’ve been living under a rock — as an under-the-rock inhabitant
myself, welcome! — LLMs are neural networks that underwent a breakthrough
in 2022 when trained for conversational “chat.” Through it, users converse
with a wickedly creative artificial intelligence indistinguishable from a
human, which smashes the Turing test and can be . Interacting with one for
the first time is unsettling, a feeling which will last for days. When you
bought your most recent home computer, you probably did not expect to have
a meaningful conversation with it.</p>

<p>I’ve found this experience reminiscent of the desktop computing revolution
of the 1990s, where your newly purchased computer seemed obsolete by the
time you got it home from the store. There are new developments each week,
and as a rule I ignore almost any information more than a year old. The
best way to keep up has been <a href="https://old.reddit.com/r/LocalLLaMA">r/LocalLLaMa</a>. Everything is hyped to the
stratosphere, so take claims with a grain of salt.</p>

<p>I’m wary of vendor lock-in, having experienced the rug pulled out from
under me by services shutting down, changing, or otherwise dropping my use
case. I want the option to continue, even if it means changing providers.
So for a couple of years I’d ignored LLMs. The “closed” models, accessibly
only as a service, have the classic lock-in problem, including <a href="https://arxiv.org/pdf/2307.09009">silent
degradation</a>. That changed when I learned I can run models close
to the state-of-the-art on my own hardware — the exact opposite of vendor
lock-in.</p>

<p>This article is about running LLMs, not fine-tuning, and definitely not
training. It’s also only about <em>text</em>, and not vision, voice, or other
“multimodal” capabilities, which aren’t nearly so useful to me personally.</p>

<p>To run a LLM on your own hardware you need <strong>software</strong> and a <strong>model</strong>.</p>

<h3 id="the-software">The software</h3>

<p>I’ve exclusively used the <em>astounding</em> <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a>. Other options exist,
but for basic CPU inference — that is, generating tokens using a CPU
rather than a GPU — llama.cpp requires nothing beyond a C++ toolchain. In
particular, no Python fiddling that plagues much of the ecosystem. On
Windows it will be a 5MB <code>llama-server.exe</code> with no runtime dependencies.
From just two files, EXE and GGUF (model), both designed to <a href="https://justine.lol/mmap/">load via
memory map</a>, you could likely still run the same LLM 25 years from
now, in exactly the same way, out-of-the-box on some future Windows OS.</p>

<p>Full disclosure: I’m biased because <a href="https://github.com/ggerganov/llama.cpp/blob/ec450d3b/docs/build.md">the official Windows build process is
w64devkit</a>. What can I say? These folks have good taste! That being
said, you should only do CPU inference if GPU inference is impractical. It
works reasonably up to ~10B parameter models on a desktop or laptop, but
it’s slower. My primary use case is not built with w64devkit because I’m
using CUDA for inference, which requires a MSVC toolchain. Just for fun, I
ported llama.cpp to Windows XP and ran <a href="https://huggingface.co/HuggingFaceTB/SmolLM2-360M-Instruct">a 360M model</a> on a 2008-era
laptop. It was magical to load that old laptop with technology that, at
the time it was new, would have been worth billions of dollars.</p>

<p>The bottleneck for GPU inference is video RAM, or VRAM. These models are,
well, <em>large</em>. The more RAM you have, the larger the model and the longer
the context window. Larger models are smarter, and longer contexts let you
process more information at once. <strong>GPU inference is not worth it below
8GB of VRAM</strong>. If <a href="https://huggingface.co/spaces/k-mktr/gpu-poor-llm-arena">“GPU poor”</a>, stick with CPU inference. On the
plus side, it’s simpler and easier to get started with CPU inference.</p>

<p>There are many utilities in llama.cpp, but this article is concerned with
just one: <strong><code>llama-server</code> is the program you want to run.</strong> It’s an HTTP
server (default port 8080) with a chat UI at its root, and <a href="https://github.com/ggerganov/llama.cpp/blob/ec450d3b/examples/server/README.md#api-endpoints">APIs for use
by programs</a>, including other user interfaces. A typical invocation:</p>

<div><pre><code>$ llama-server --flash-attn --ctx-size 0 --model MODEL.gguf
</code></pre></div>

<p>The context size is the largest number of tokens the LLM can handle at
once, input plus output. Contexts typically range from 8K to 128K tokens,
and depending on the model’s tokenizer, normal English text is ~1.6 tokens
per word as counted by <code>wc -w</code>. If the model supports a large context you
may run out of memory. If so, set a smaller context size, like <code>--ctx-size
$((1&lt;&lt;13))</code> (i.e. 8K tokens).</p>

<p>I do not yet understand what flash attention is about, and I don’t know
why <code>--flash-attn</code>/<code>-fa</code> is not the default (lower accuracy?), but you
should always request it because it reduces memory requirements when
active and is well worth the cost.</p>

<p>If the server started successfully, visit it (<a href="http://localhost:8080/">http://localhost:8080/</a>) to
try it out. Though of course you’ll need a model first.</p>

<h3 id="the-models">The models</h3>

<p><a href="https://huggingface.co/">Hugging Face</a> (HF) is “the GitHub of LLMs.” It’s an incredible
service that has earned that title. “Small” models are around a few GBs,
large models are hundreds of GBs, and HF <em>hosts it all for free</em>. With a
few exceptions that do not matter in practice, you don’t even need to sign
up to download models! (I’ve been so impressed that after a few days they
got a penny-pincher like me to pay for pro account.) That means you can
immediately download and try any of the stuff I’m about to discuss.</p>

<p>If you look now, you’ll wonder, “There’s a lot of stuff here, so what the
heck am I supposed to download?” That was me one month ago. For llama.cpp,
the answer is <a href="https://github.com/ggerganov/ggml/blob/8a3d7994/docs/gguf.md">GGUF</a>. None of the models are natively in GGUF.
Instead GGUFs are in a repository with “GGUF” in the name, usually by a
third party: one of the heroic, prolific GGUF quantizers.</p>

<p>(Note how nowhere does the official documentation define what “GGUF”
stands for. Get used that. This is a technological frontier, and if the
information exists at all, it’s not in the obvious place. If you’re
considering asking your LLM about this once it’s running: Sweet summer
child, we’ll soon talk about why that doesn’t work. As far as I can tell,
“GGUF” has no authoritative definition.)</p>

<p>Since llama.cpp is named after the Meta’s flagship model, their model is a
reasonable start, though it’s not my personal favorite. The latest is
Llama 3.2, but at the moment only the 1B and 3B models — that is, ~1
billion and ~3 billion parameters — work in Llama.cpp. Those are a little
<em>too</em> small to be of much use, and your computer can likely to better if
it’s not a Raspberry Pi, even with CPU inference. Llama 3.1 8B is a better
option. (If you’ve got at least 24GB of VRAM then maybe you can even do
Llama 3.1 70B.)</p>

<p>If you search for Llama 3.1 8B you’ll find two options, one qualified
“instruct” and one with no qualifier. Instruct means it was trained to
follow instructions, i.e. to chat, and that’s nearly always what you want.
The other is the “base” model which can only continue a text. (Technically
the instruct model is still just completion, but we’ll get to that later.)
It would be great if base models were qualified “Base” but, for dumb path
dependency reasons, they’re usually not.</p>

<p>You will not find GGUF in the “Files” for the instruct model, nor can you
download the model without signing up in order to agree to the community
license. Go back to the search, add GGUF, and look for the matching GGUF
model: <a href="https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF">bartowski/Meta-Llama-3.1-8B-Instruct-GGUF</a>. bartowski is
one of the prolific and well-regarded GGUF quantizers. Not only will this
be in the right format for llama.cpp, you won’t need to sign up.</p>

<p>In “Files” you will now see many GGUFs. These are different quantizations
of the same model. The original model has <a href="https://en.wikipedia.org/wiki/Bfloat16_floating-point_format">bfloat16</a> tensors, but for
merely running the model we can throw away most of that precision with
minimal damage. It will be a tiny bit dumber and less knowledgeable, but
will require substantially fewer resources. <strong>The general recommendation,
which fits my experience, is to use <code>Q4_K_M</code></strong>, a 4-bit quantization. In
general, better to run a 4-bit quant of a larger model than an 8-bit quant
of a smaller model. Once you’ve got the basics understood, experiment with
different quants and see what you like!</p>

<h3 id="my-favorite-models">My favorite models</h3>

<p>Models are trained for different trade-offs and differ in strengths and
weaknesses, so no model is best at everything — especially on “GPU-poor”
configurations. My desktop system has an RTX 3050 Ti with 8GB VRAM, and
its limitations have shaped my choices. I can comfortably run ~10B models,
and ~30B models just barely enough to test their capabilities. For ~70B I
rely on third-party hosts. My “t/s” numbers are all on this system running
4-bit quants.</p>

<p>This list omits “instruct” from the model name, but assume the instruct
model unless I say otherwise. A few are <em>bona fide</em> open source, at least
as far as LLMs practically can be, and I’ve noted the license when that’s
the case. The rest place restrictions on both use and distribution.</p>

<ul>
  <li>
    <p>Mistral-Nemo-2407 (12B) [Apache 2.0]</p>

    <p>A collaboration between <a href="https://mistral.ai/">Mistral AI</a> and Nvidia (“Nemo”), the
most well-rounded ~10B model I’ve used, and my default. Inference starts
at a comfortable 30 t/s. It’s strengths are writing and proofreading,
and it can review code nearly as well as ~70B models. It was trained for
a context length of 128K, but its <a href="https://github.com/NVIDIA/RULER">effective context length is closer to
16K</a> — a limitation I’ve personally observed.</p>

    <p>The “2407” is a date (July 2024) as version number, a versioning scheme
I wholeheartedly support. A date tells you about its knowledge cut-off
and tech level. It sorts well. Otherwise LLM versioning is a mess. Just
as open source is bad with naming, AI companies do not comprehend
versioning.</p>
  </li>
  <li>
    <p>Qwen2.5-14B and Qwen2.5-72B</p>

    <p>Qwen models, by Alibaba Cloud, impressively punch above their weight at
all sizes. 14B inference starts at 11 t/s, with capabilities on par with
Mistral Nemo. If I could run 72B on my own hardware, it would probably
be my default. I’ve been trying it through Hugging Face’s inference API.
There’s a 32B model, but it’s impractical for my hardware, so I haven’t
spent much time with it.</p>
  </li>
  <li>
    <p>Gemma-2-2B</p>

    <p>Google’s model is popular, perhaps due to its playful demeanor. For me,
the 2B model <a href="https://github.com/skeeto/scratch/blob/master/userscript/reddit-llm-translate.user.js">is great for fast translation</a>. It’s amazing that LLMs
have nearly obsoleted Google Translate, and you can run it on your home
computer. Though it’s more resource-intensive, and refuses to translate
texts it finds offensive, which sounds like a plot element from a sci-fi
story. In my translation script, I send it text marked up with HTML.
Simply <em>asking</em> Gemma to preserve the markup Just Works! The 9B model is
even better, but slower, and I’d use it instead of 2B for translating my
own messages into another language.</p>
  </li>
  <li>
    <p>Phi3.5-Mini (4B) [MIT]</p>

    <p>Microsoft’s niche is training on synthetic data. The result is a model
that does well in tests, but doesn’t work so well in practice. For me,
its strength is document evaluation. I’ve loaded the context with up to
40K-token documents — it helps that it’s a 4B model — and successfully
queried accurate summaries and data listings.</p>
  </li>
  <li>
    <p>SmolLM2-360M [Apache 2.0]</p>

    <p>Hugging Face doesn’t just host models; their 360M model is unusually
good for its size. It fits on my 2008-era, 1G RAM, Celeron, and 32-bit
operating system laptop. It also runs well on older Raspberry Pis. It’s
creative, fast, converses competently, can write poetry, and a fun toy
in cramped spaces.</p>
  </li>
  <li>
    <p>Mixtral-8x7B (48B) [Apache 2.0]</p>

    <p>Another Mistral AI model, and more of a runner up. 48B seems too large,
but this is a <a href="https://mistral.ai/news/mixtral-of-experts/">Mixture of Experts</a> (MoE) model. Inference uses only
13B parameters at a time. It’s reasonably-suited to CPU inference on a
machine with at least 32G of RAM. The model retains more of its training
inputs, more like a database, but for reasons we’ll see soon, it isn’t
as useful as it might seem.</p>
  </li>
  <li>
    <p>Llama-3.1-70B and Llama-3.1-Nemotron-70B</p>

    <p>More models I cannot run myself, but which I access remotely. The latter
bears “Nemo” because it’s an Nvidia fine-tune. If I could run 70B models
myself, Nemotron might just be my default. I’d need to spent more time
evaluating it against Qwen2.5-72B.</p>
  </li>
</ul>

<p>Most of these models have <a href="https://huggingface.co/blog/mlabonne/abliteration">abliterated</a> or “uncensored” versions, in
which refusal is partially fine-tuned out at a cost of model degradation.
Refusals are annoying — such as Gemma refusing to translate texts it
dislikes — but doesn’t happen enough for me to make that trade-off. Maybe
I’m just boring. Also refusals seem to decrease with larger contexts, as
though “in for a penny, in for a pound.”</p>

<p>The next group are “coder” models trained for programming. In particular,
they have <em>fill-in-the-middle</em> (FIM) training for generating code inside
an existing program. I’ll discuss what that entails in a moment. As far as
I can tell, they’re no better at code review nor other instruct-oriented
tasks. It’s the opposite: FIM training is done in the base model, with
instruct training applied later on top, so instruct works <em>against</em> FIM!
In other words, you get better FIM results from base models, though you
lose the ability to converse with them.</p>

<p>There will be a section on evaluation later, but I want to note now that
<em>LLMs still produce poor code</em>, even at the state-of-the-art. The rankings
here are relative to other models, not about overall capability.</p>

<ul>
  <li>
    <p>DeepSeek-Coder-V2-Lite (16B)</p>

    <p>A self-titled MoE model from <a href="https://www.deepseek.com/">DeepSeek</a>. It uses 2B parameters
during inference, making it as fast as Gemma 2 2B but as smart as
Mistral Nemo, striking a great balance, especially because it
out-competes ~30B models at code generation. If I’m playing around with
FIM, this is my default choice.</p>
  </li>
  <li>
    <p>Qwen2.5-Coder-7B</p>

    <p>Qwen Coder is a close second. It works about as well, but slightly
slower since it’s not MoE. It’s a better choice than DeepSeek if you’re
memory-constrained, or, similarly, if your choices are GPU inference
Qwen versus CPU inference DeepSeek. While writing this article, Alibaba
Cloud released a new Qwen2.5-Coder-7B that’s reported to be better. They
failed to increment the version number, which is horribly confusing. The
community has taken to calling it Qwen2.5.1. (Remember what I said about
AI companies and versions?)</p>
  </li>
  <li>
    <p>Granite-8B-Code [Apache 2.0]</p>

    <p>IBM’s line of models is named Granite. In general Granite models are
disappointing, <em>except</em> that they’re unusually good at FIM. It’s tied
in second place with Qwen2.5 7B in my experience.</p>
  </li>
</ul>

<p>I also evaluated CodeLlama, CodeGemma, Codestral, and StarCoder. Their FIM
outputs were so poor as to be effectively worthless at that task, and I
found no reason to use these models. The negative effects of instruct
training were most pronounced for CodeLlama.</p>

<h3 id="the-user-interfaces">The user interfaces</h3>

<p>I pointed out Llama.cpp’s built-in UI, and I’d used similar UIs with other
LLM software. As is typical, no UI is to my liking, especially in matters
of productivity, so I built my own, <strong><a href="https://github.com/skeeto/illume">Illume</a></strong>. This command
line program converts standard input into an API query, makes the query,
and streams the response to standard output. Should be simple enough to
integrate into any extensible text editor, but I only needed it for Vim.
Vimscript is miserable, probably the second worst programming language
I’ve ever touched, so my goal was to write as little as possible.</p>

<p>I created Illume to scratch my own itch, to support my exploration of the
LLM ecosystem. I actively break things and add features as needed, and I
make no promises about interface stability. <em>You probably don’t want to
use it.</em></p>

<p>Lines that begin with <code>!</code> are directives interpreted by Illume, chosen
because it’s unlikely to appear in normal text. A conversation alternates
between <code>!user</code> and <code>!assistant</code> in a buffer.</p>

<div><pre><code>!user
Write a Haiku about time travelers disguised as frogs.

!assistant
Green, leaping through time,
Frog tongues lick the future's rim,
Disguised in pond's guise.
</code></pre></div>

<p>It’s still a text editor buffer, so I can edit the assistant response,
reword my original request, etc. before continuing the conversation. For
composing fiction, I can request it to continue some text (which does not
require instruct training):</p>

<div><pre><code>!completion
Din the Wizard stalked the dim castle
</code></pre></div>

<p>I can stop it, make changes, add my own writing, and keep going. I ought
to spend more time practicing with it. If you introduce out-of-story note
syntax, the LLM will pick up on it, and then you can use notes to guide
the LLM’s writing.</p>

<p>While the main target is llama.cpp, I query different APIs, implemented by
different LLM software, with incompatibilities across APIs (a parameter
required by one API is forbidden by another), so directives must be
flexible and powerful. So directives can set arbitrary HTTP and JSON
parameters. Illume doesn’t try to abstract the API, but exposes it at a
low level, so effective use requires knowing the remote API. For example,
the “profile” for talking to llama.cpp looks like this:</p>

<div><pre><code>!api http://localhost:8080/v1
!:cache_prompt true
</code></pre></div>

<p>Where <code>cache_prompt</code> is a llama.cpp-specific JSON parameter (<code>!:</code>). Prompt
cache nearly always better enabled, yet for some reason it’s disabled by
default. Other APIs refuse requests with this parameter, so then I must
omit or otherwise disable it. The Hugging Face “profile” looks like this:</p>

<div><pre><code>!api https://api-inference.huggingface.co/models/{model}/v1
!:model Qwen/Qwen2.5-72B-Instruct
!&gt;x-use-cache false
</code></pre></div>

<p>For the sake of HF, Illume can interpolate JSON parameters into the URL.
The HF API caches also aggressively caches. I never want this, so I supply
an HTTP parameter (<code>!&gt;</code>) to turn it off.</p>

<p>Unique to llama.cpp is an <code>/infill</code> endpoint for FIM. It requires a model
with extra metadata, trained a certain way, but this is usually not the
case. So while Illume can use <code>/infill</code>, I also added FIM configuration
so, after reading the model’s documentation and configuring Illume for
that model’s FIM behavior, I can do FIM completion through the normal
completion API on any FIM-trained model, even on non-llama.cpp APIs.</p>

<h3 id="fill-in-the-middle-fim-tokens">Fill-in-the-Middle (FIM) tokens</h3>

<p>It’s time to discuss FIM. To get to the bottom of FIM I needed to go to
the source of truth, the original FIM paper: <a href="https://arxiv.org/abs/2207.14255">Efficient Training of
Language Models to Fill in the Middle</a>. This allowed me to understand
how these models are FIM-trained, at least enough to put that training to
use. Even so, model documentation tends to be thin on FIM because they
expect you to run their code.</p>

<p>Ultimately an LLM can only predict the next token. So pick some special
tokens that don’t appear in inputs, use them to delimit a prefix and
suffix, and middle (PSM) — or sometimes ordered suffix-prefix-middle (SPM)
— in a large training corpus. Later in inference we can use those tokens
to provide a prefix, suffix, and let it “predict” the middle. Crazy, but
<em>this actually works!</em></p>

<div><pre><code>&lt;PRE&gt;{prefix}&lt;SUF&gt;{suffix}&lt;MID&gt;
</code></pre></div>

<p>For example when filling the parentheses of <code>dist = sqrt(x*x + y*y)</code>:</p>

<div><pre><code>&lt;PRE&gt;dist = sqrt(&lt;SUF&gt;)&lt;MID&gt;x*x + y*y
</code></pre></div>

<p>To have the LLM fill in the parentheses, we’d stop at <code>&lt;MID&gt;</code> and let the
LLM predict from there. Note how <code>&lt;SUF&gt;</code> is essentially the cursor. By the
way, this is basically how instruct training works, but instead of prefix
and suffix, special tokens delimit instructions and conversation.</p>

<p>Some LLM folks interpret the paper quite literally and use <code>&lt;PRE&gt;</code>, etc.
for their FIM tokens, although these look nothing like their other special
tokens. More thoughtful trainers picked <code>&lt;|fim_prefix|&gt;</code>, etc. Illume
accepts FIM templates, and I wrote templates for the popular models. For
example, here’s Qwen (PSM):</p>

<div><pre><code>&lt;|fim_prefix|&gt;{prefix}&lt;|fim_suffix|&gt;{suffix}&lt;|fim_middle|&gt;
</code></pre></div>

<p>Mistral AI prefers square brackets, SPM, and no “middle” token:</p>

<div><pre><code>[SUFFIX]{suffix}[PREFIX]{prefix}
</code></pre></div>

<p>With these templates I could access the FIM training in models unsupported
by llama.cpp’s <code>/infill</code> API.</p>

<p>Besides just failing the prompt, the biggest problem I’ve had with FIM is
LLMs not know when to stop. For example, if I ask it to fill out this
function (i.e. assign something <code>r</code>):</p>

<div><pre><code><span>def</span> <span>norm</span><span>(</span><span>x</span><span>:</span> <span>float</span><span>,</span> <span>y</span><span>:</span> <span>float</span><span>)</span> <span>-&gt;</span> <span>float</span><span>):</span>
    <span>return</span> <span>r</span>
</code></pre></div>

<p>(Side note: Static types, including the hints here, produce better results
from LLMs, acting as guardrails.) It’s not unusual to get something like:</p>

<div><pre><code><span>def</span> <span>norm</span><span>(</span><span>x</span><span>:</span> <span>float</span><span>,</span> <span>y</span><span>:</span> <span>float</span><span>)</span> <span>-&gt;</span> <span>float</span><span>):</span>
    <span>r</span> <span>=</span> <span>sqrt</span><span>(</span><span>x</span><span>*</span><span>x</span> <span>+</span> <span>y</span><span>*</span><span>y</span><span>)</span>
    <span>return</span> <span>r</span>

<span>def</span> <span>norm3</span><span>(</span><span>x</span><span>:</span> <span>float</span><span>,</span> <span>y</span><span>:</span> <span>float</span><span>,</span> <span>z</span><span>:</span> <span>float</span><span>)</span> <span>-&gt;</span> <span>float</span><span>):</span>
    <span>r</span> <span>=</span> <span>sqrt</span><span>(</span><span>x</span><span>*</span><span>x</span> <span>+</span> <span>y</span><span>*</span><span>y</span> <span>+</span> <span>z</span><span>*</span><span>z</span><span>)</span>
    <span>return</span> <span>r</span>

<span>def</span> <span>norm4</span><span>(</span><span>x</span><span>:</span> <span>float</span><span>,</span> <span>y</span><span>:</span> <span>float</span><span>,</span> <span>z</span><span>:</span> <span>float</span><span>,</span> <span>w</span><span>:</span> <span>float</span><span>)</span> <span>-&gt;</span> <span>float</span><span>):</span>
    <span>r</span> <span>=</span> <span>sqrt</span><span>(</span><span>x</span><span>*</span><span>x</span> <span>+</span> <span>y</span><span>*</span><span>y</span> <span>+</span> <span>z</span><span>*</span><span>z</span> <span>+</span> <span>w</span><span>*</span><span>w</span><span>)</span>
    <span>return</span> <span>r</span>
</code></pre></div>

<p>Where the original <code>return r</code> became the return for <code>norm4</code>. Technically
it fits the prompt, but it’s obviously not what I want. So be ready to
mash the “stop” button when it gets out of control. The three coder models
I recommended exhibit this behavior less often. It might be more robust to
combine it with a non-LLM system that understands the code semantically
and automatically stops generation when the LLM begins generating tokens
in a higher scope. That would make more coder models viable, but this goes
beyond my own fiddling.</p>

<p>Figuring out FIM and putting it into action revealed to me that FIM is
still in its early stages, and hardly anyone is generating code via FIM. I
guess everyone’s just using plain old completion?</p>

<h3 id="so-what-are-llms-good-for">So what are LLMs good for?</h3>

<p>LLMs are fun, but what the productive uses do they have? That’s a question
I’ve been trying to answer this past month, and it’s come up shorter than
I hoped. It might be useful to establish boundaries — tasks that LLMs
definitely cannot do.</p>

<p>First, <strong>LLMs are no good if correctness cannot be readily verified</strong>.
They are untrustworthy hallucinators. Often if you’re in position to
verify LLM output, you didn’t need it in the first place. This is why
Mixtral, with its large “database” of knowledge, isn’t so useful. It also
means it’s <em>reckless and irresponsible to inject LLM output into search
results</em> — just shameful.</p>

<p>LLM enthusiasts, who ought to know better, fall into this trap anyway and
propagate hallucinations. It makes discourse around LLMs less trustworthy
than normal, and I need to approach LLM information with extra skepticism.
Case in point: Recall how “GGUF” doesn’t have an authoritative definition.
Search for one and you’ll find an obvious hallucination that made it all
the way into official IBM documentation. I won’t repeat it hear as to not
make things worse.</p>

<p>Second, <strong>LLMs have goldfish-sized working memory</strong>. That is, they’re held
back by small context lengths. Some models are trained on larger contexts,
but their <a href="https://github.com/NVIDIA/RULER">effective context length</a> is usually much smaller. In
practice, an LLM can hold several book chapters worth of comprehension “in
its head” at a time. For code it’s 2k or 3k lines (code is token-dense).
That’s the most you can work with at once. Compared to a human, it’s tiny.
There are tools like <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">retrieval-augmented generation</a> and fine-tuning
to mitigate it… <em>slightly</em>.</p>

<p>Third, <strong>LLMs are poor programmers</strong>. At best they write code at maybe an
undergraduate student level who’s read a lot of documentation. That sounds
better than it is. The typical fresh graduate enters the workforce knowing
practically nothing about software engineering. Day one on the job is the
first day of their real education. In that sense, LLMs today haven’t even
begun their real education.</p>

<p><em>Writing new code is the easy part</em>. The hard part is maintaining code,
and writing new code with that maintenance in mind. Even when an LLM
produces code that works, there’s no thought to maintenance, nor could
there be. In general the reliability of generate code follows the inverse
square law by length, and generating more than a dozen lines at a time is
fraught. I really tried, but never saw LLM output beyond 2–3 lines of code
which I would consider acceptable.</p>

<p>Quality varies substantially by language. LLMs are better at Python than
C, and better at C than assembly. I suspect it’s related to the difficulty
of the language and the quality of the input. It’s trained on lots of
terrible C — the internet is loaded with it after all — and probably the
only labeled x86 assembly it’s seen is crummy beginner tutorials. Ask it
to use SDL2 and it <a href="https://nullprogram.com/blog/2023/01/08/">reliably produces the common mistakes</a> because
it’s been trained to do so.</p>

<p>What about boilerplate? That’s something an LLM could probably do with a
low error rate, and perhaps there’s merit to it. Though the fastest way to
deal with boilerplate is to not write it at all. Change your problem to
not require boilerplate.</p>

<p>Without taking my word for it, consider how it show up in the economics:
If AI companies could deliver the productivity gains they claim, they
wouldn’t sell AI. They’d keep it to themselves and gobble up the software
industry. Or consider the software products produced by companies on the
bleeding edge of AI. It’s still the same old, bloated web garbage everyone
else is building. (My LLM research has involved navigating their awful web
sites, and it’s made be bitter.)</p>

<p>In code generation, hallucinations are less concerning. You already knew
what you wanted when you asked, so you can review it, and your compiler
will help catch problems you miss (e.g. calling a hallucinated method).
However, small context and poor code generation remain roadblocks, and I
haven’t yet made this work effectively.</p>

<p>So then, what can I do with LLMs? A list is apt because LLMs love lists:</p>

<ul>
  <li>
    <p>Proofreading has been most useful for me. I give it a document such as
an email or this article (~8,000 tokens), tell it to look over grammar,
call out passive voice, and so on, and suggest changes. I accept or
reject its suggestions and move on. Most suggestions will be poor, and
this very article was long enough that even ~70B models suggested
changes to hallucinated sentences. Regardless, there’s signal in the
noise, and it fits within the limitations outlined above. I’m still
trying to apply this technique (“find bugs, please”) to code review, but
so far success is elusive.</p>
  </li>
  <li>
    <p>Writing short fiction. Hallucinations are not a problem; they’re a
feature! Context lengths are the limiting factor, though perhaps you can
stretch it by supplying chapter summaries, also written by LLM. I’m
still exploring this. If you’re feeling lazy, tell it to offer you three
possible story branches at each turn, and you pick the most interesting.
Or even tell it to combine two of them! LLMs are clever and will figure
it out. Some genres work better than others, and concrete works better
than abstract. (I wonder if professional writers judge its writing as
poor as I judge its programming.)</p>
  </li>
  <li>
    <p>Generative fun. Have an argument with Benjamin Franklin (note: this
probably violates the <a href="https://ai.meta.com/llama/use-policy/">Acceptable Use Policy</a> of some models), hang
out with a character from your favorite book, or generate a new scene of
<a href="https://nullprogram.com/blog/2023/06/22/#76-henry-iv">Falstaff’s blustering antics</a>. Talking to historical figures
has been educational: The character says something unexpected, I look it
up the old-fashioned way to see what it’s about, then learn something
new.</p>
  </li>
  <li>
    <p>Language translation. I’ve been browsing foreign language subreddits
through Gemma-2-2B translation, and it’s been insightful. (I had no idea
German speakers were so distrustful of artificial sweeteners.)</p>
  </li>
</ul>

<p>Despite the short list of useful applications, this is the most excited
I’ve been about a new technology in years!</p>



  
  <ol></ol>

  

  <nav>
  
    
  
  
  </nav>
</article>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ASCII Delimited Text – Not CSV or Tab Delimited Text (104 pts)]]></title>
            <link>https://ronaldduncan.wordpress.com/2009/10/31/text-file-formats-ascii-delimited-text-not-csv-or-tab-delimited-text/</link>
            <guid>42100499</guid>
            <pubDate>Sun, 10 Nov 2024 14:42:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ronaldduncan.wordpress.com/2009/10/31/text-file-formats-ascii-delimited-text-not-csv-or-tab-delimited-text/">https://ronaldduncan.wordpress.com/2009/10/31/text-file-formats-ascii-delimited-text-not-csv-or-tab-delimited-text/</a>, See on <a href="https://news.ycombinator.com/item?id=42100499">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-21">
						
						
						<p><small>In <a href="https://ronaldduncan.wordpress.com/category/software/ascii/" rel="category tag">ASCII</a>, <a href="https://ronaldduncan.wordpress.com/category/software/development/" rel="category tag">development</a>, <a href="https://ronaldduncan.wordpress.com/category/software/file-formats/" rel="category tag">File Formats</a>, <a href="https://ronaldduncan.wordpress.com/category/software/" rel="category tag">software</a>, <a href="https://ronaldduncan.wordpress.com/category/technology/" rel="category tag">technology</a> on <strong>October 31, 2009</strong> at <strong>3:09 pm</strong></small></p><div>
							<p>Unfortunately a quick google search on “ASCII Delimited Text” shows that IBM and Oracle failed to read the ASCII specification and both define ASCII Delimited Text as a CSV format. &nbsp;ASCII Delimited Text should use the record separators defined as ASCII 28-31.</p>
<p>The most common formats are CSV (Comma Separated Values) and tab delimited text. &nbsp;Tab delimited text breaks when ever you have either a field with a tab or a new line in it, and CSV breaks depending on the implementation on Quotes, Commas and lines. Sadly Quotes, Commas and Tab characters are very common in text, and this makes the formats extremely bad for exporting and importing data. &nbsp;There are some other formats such as pipe (|) delimited text, and whilst better in that | is less frequently used they still suffer from being printable characters that are entered into text, and worst of all people, when they look at a file format and see the delimiter, think that it is a good idea to break things up with in fields using the same delimiter as the file format.</p>
<p>The <strong>most anoying thing</strong> about the <strong>whole problem</strong> is that it <strong>was solved by design</strong> in the <strong>ASCII character set</strong>.</p>
<p>If you use ASCII &nbsp;31 as your field separator instead of comma or tab, and ASCII 30 as your record separator instead of new line. &nbsp; Then you have a text file format that is trivial to write out and read in, with no restrictions on the text in fields or the need to try and escape characters.</p>
<p>It is even part of the design of the file encoding system. &nbsp;The ASCII standard calls these fields</p>
<ul>
<li>31 Unit Separator</li>
<li>30 Record Separator</li>
</ul>
<p>And ASCII has two more levels with Group and File Separators</p>
<ul>
<li>29 Group Separator</li>
<li>28 File Separator</li>
</ul>
<p>See <a href="http://en.wikipedia.org/wiki/Unit_separator">http://en.wikipedia.org/wiki/Unit_separator</a> and<br>
<a href="http://en.wikipedia.org/wiki/Delimiter#ASCII_Delimited_Text">http://en.wikipedia.org/wiki/Delimiter#ASCII_Delimited_Text</a></p>
<p>In summary ASCII Delimited Text is using the last 4 control characters (28-31) for their purpose as field and record delimiters and not using CSV (Comma Separated Values)</p>

			
																							</div>
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Audio Decomposition – open-source seperation of music to constituent instruments (245 pts)]]></title>
            <link>https://matthew-bird.com/blogs/Audio-Decomposition.html</link>
            <guid>42098491</guid>
            <pubDate>Sun, 10 Nov 2024 03:57:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://matthew-bird.com/blogs/Audio-Decomposition.html">https://matthew-bird.com/blogs/Audio-Decomposition.html</a>, See on <a href="https://news.ycombinator.com/item?id=42098491">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>
                <iframe src="https://www.youtube.com/embed/-i0PSxcoDH0" allowfullscreen=""></iframe>
                <iframe src="https://www.youtube.com/embed/mzPUfs9sYQE" allowfullscreen=""></iframe>
                <iframe src="https://www.youtube.com/embed/Z7D6obv12zk" allowfullscreen=""></iframe>
                <iframe src="https://www.youtube.com/embed/LkDJ9XT-klU" allowfullscreen=""></iframe>
            </p>
            <h4>Demo Vids: <a href="https://youtu.be/-i0PSxcoDH0">https://youtu.be/-i0PSxcoDH0</a>, <a href="https://youtu.be/mzPUfs9sYQE">https://youtu.be/mzPUfs9sYQE</a>, <a href="https://youtu.be/Z7D6obv12zk">https://youtu.be/Z7D6obv12zk</a>, <a href="https://youtu.be/LkDJ9XT-klU">https://youtu.be/LkDJ9XT-klU</a></h4>
            <h4>Github Repository: <a href="https://github.com/mbird1258/Audio-Decomposition">https://github.com/mbird1258/Audio-Decomposition</a></h4>
            
            <h2>Premise</h2>
            <p>My plan for this project was to create a program to turn music to sheet music. It was mainly incentivised by my own desire to turn music to sheet music and the lack (from what I could tell) of open source, simple algorithms to perform audio source separation. </p>

            <h2>Preparation</h2>
            <h3>Instrument data</h3>
            <p>The instrument data all comes from the <a href="https://theremin.music.uiowa.edu/MIS.html">University of Iowa Electronic Music Studios instrument database</a>. With these files, we find the Fourier transform of the entire wave and the envelope of the wave using the same method as documented below. </p>

            <h2>How it works</h2>
            <p>An instrument’s sound wave is mainly characterized by its fourier transform and envelope. Thus, we can use both of these to hopefully get a good idea of which instrument is playing what note. </p>

            <h3>Fourier Transform</h3>
            <p>The program’s first method of splitting music into constituent notes and instruments is by taking the fourier transform of the music file every 0.1 seconds (spectrogram), and adding together our stored fourier transform of each instrument to recreate the fourier transform of the 0.1 second window. The idea was to hopefully perfectly recreate the music at the set time as the fourier transform should represent the music played relatively well. </p>
            <p><img src="https://matthew-bird.com/assets/imgs/Audio%20Decomposition/1.png"></p><h5>Original fourier transform</h5>
            <p><img src="https://matthew-bird.com/assets/imgs/Audio%20Decomposition/2.png"></p><h5>Constituent instruments</h5>
            <p><img src="https://matthew-bird.com/assets/imgs/Audio%20Decomposition/3.png"></p><h5>Recreated fourier transform</h5>
            <p>The magnitudes for each instrument are given by solving the following matrix. The matrix is derived from taking the partial derivative of the MSE cost function by frequency(ex. FT value at 5 hz) with respect to each instrument. Each row in the matrix is a different partial derivative. (First is w.r.t cello, second is w.r.t piano, etc.)</p>
            <p><img src="https://matthew-bird.com/assets/imgs/Audio%20Decomposition/4.jpg"></p><h3>Envelope</h3>
            <p>The first step to matching the envelope of the instrument to the sound wave is to obtain the envelope itself. The envelope is the upper bound of a wave, and although there are functions to do this, they seemingly struggle with noise and certain types of sound waves. Thus, since we have to handle many different instruments at different frequencies, we need a more robust solution. </p>
            <p>To get the envelope, the function splits the sound wave into chunks, before taking the max value at each chunk. To further refine the results, the function finds the points where the envelope is below the original sound wave and adds a new point defining the envelope. </p>
            <p><img src="https://matthew-bird.com/assets/imgs/Audio%20Decomposition/5.jpg" ,="">
            <img src="https://matthew-bird.com/assets/imgs/Audio%20Decomposition/6.jpg" ,="">
            <img src="https://matthew-bird.com/assets/imgs/Audio%20Decomposition/7.jpg" ,="">
            <img src="https://matthew-bird.com/assets/imgs/Audio%20Decomposition/8.jpg" ,=""></p><p>The next step is to split the envelope of the wave into its attack, sustain, and release. The attack is the initial noise of the note, the sustain is while the note is held, and the release is when the note stops. For the instrument samples, we can take the first nonzero value of the wave to get the start of the attack. To get the point between the attack and sustain, we get the first point when the function is concave down or decreasing. To get the point between the sustain and release, we get the first point from the end where the function is increasing or concave down. To get the end of the release, we find the first point from the end where the function is nonzero. </p>
            <p>To further classify the wave, we need to take into account the main forms the wave can take. Some instruments, such as the piano, have static decay, in which they mostly follow an exponential decay shape. On the other hand, some instruments, like the violin, can increase or decrease in volume as the note is sustained. In addition to this, some audio samples in the instrument files are held until their sound expires, while others are released early. To differentiate whether the decay is static or dynamic, if the decay factor is &gt;1, or if it deviates from the decay curve by too much, it’s dynamic. To differentiate if the envelope has a release or not(AS or ASR), we look at the average rate of change across the sustain and the release, and if the rate of change of the release is lower then there is no release. </p>
            <p><img src="https://matthew-bird.com/assets/imgs/Audio%20Decomposition/9.jpg"></p><h5>Different types of wave</h5>
            <p>To deal with the music file, we first take the bandpass filter of the signal for each note frequency. With the filtered wave, we iterate through each instrument. For each instrument, we take the cross correlation of the instrument’s attack(normalized) and release(normalized) to find the start and end of each note, and then take the MSE of the instrument wave and the filtered audio to get our cost for the instrument at that time. After this, we multiply the magnitude we found in the fourier transform step and 1/(cost we found in this step) to get our final magnitudes. </p>

            <h2>Display</h2>
            <p>To display the file, we use matplotlib’s scatter plot with - shaped points to display the sheet music. Originally, I wanted to recreate the audio from the magnitudes, but it led to many issues, took a while, and made troubleshooting much harder. I also tried using matplotlib’s imshow plot, but it’s extremely inefficient in this case as most values are 0, and matplotlib needs to redraw every point regardless of if it’s on screen or not every time we pan or zoom the screen. </p>
            <p><img src="https://matthew-bird.com/assets/imgs/Audio%20Decomposition/10.png" ,=""></p><h2>Results</h2>
            <p>Overall, I think it works quite well. You can use it to make recreating sheet music better(as I did <a href="https://www.noteflight.com/scores/view/ffb92ff50357ea0bb2ab4f7ea7b50a9060c7aa1a">here</a> from <a href="https://www.youtube.com/watch?v=bDtT8VZMEHw">this video</a>), especially if you struggle with finding the right pitch or chords, and it doesn’t take too much time to run either. </p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Physical Intelligence's first generalist policy AI can finally do your laundry (196 pts)]]></title>
            <link>https://www.physicalintelligence.company/blog/pi0</link>
            <guid>42098236</guid>
            <pubDate>Sun, 10 Nov 2024 02:26:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.physicalintelligence.company/blog/pi0">https://www.physicalintelligence.company/blog/pi0</a>, See on <a href="https://news.ycombinator.com/item?id=42098236">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>Published</p><p>October 31, 2024</p><p>Email</p><p><span>research@physicalintelligence.company</span><span>Kevin Black, Noah Brown, Danny Driess, Michael Equi, Adnan Esmail, Chelsea Finn, Nick Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Kay Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, Ury Zhilinsky</span></p><p>Paper</p></div>
<video autoplay="" controls="" loop="" muted="" playsinline="" poster="https://www.physicalintelligence.company/images/p0-hero-video-poster.jpg"><source src="https://dnrjl01ydafck.cloudfront.net/v2/upload/PhysicalIntelligence_2min_v2_out.mp4" type="video/mp4"></video>
<div><p>We are living through an AI revolution: the past decade witnessed practically useful AI assistants, AI systems that can generate photorealistic images and videos, and even models that can predict the structure of proteins. But in spite of all these advances, human intelligence dramatically outpaces AI when it comes to the physical world. To paraphrase <a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Moravec%27s_paradox">Moravec’s paradox</a>, winning a game of chess or discovering a new drug represent “easy” problems for AI to solve, but folding a shirt or cleaning up a table requires solving some of the most difficult engineering problems ever conceived. To build AI systems that have the kind of physically situated versatility that people possess, we need a new approach — we need to make AI systems embodied so that they can acquire physical intelligence.</p><p>Over the past eight months, we’ve developed a general-purpose robot foundation model that we call <span>π<sub>0</sub></span> (pi-zero). We believe this is a first step toward our long-term goal of developing artificial physical intelligence, so that users can simply ask robots to perform any task they want, just like they can ask large language models (LLMs) and chatbot assistants. Like LLMs, our model is trained on broad and diverse data and can follow various text instructions. Unlike LLMs, it spans images, text, and actions and acquires physical intelligence by training on embodied experience from robots, learning to directly output low-level motor commands via a novel architecture. It can control a variety of different robots, and can either be prompted to carry out the desired task, or fine-tuned to specialize it to challenging application scenarios. An extended article on our work can be found <a href="https://www.physicalintelligence.company/download/pi0.pdf">here</a>.</p></div>

<div><h3>The promise of generalist robot policies</h3><p>Today’s robots are narrow specialists. Industrial robots are programmed for repetitive motions in choreographed settings, repeatedly making the same weld in the same spot on an assembly line or dropping the same item into the same box. Even such simple behaviors require extensive manual engineering, and more complex behaviors in messy real-world environments such as homes are simply infeasible. AI could change that, allowing robots to <em>learn</em> and follow user instructions, so that programming a new behavior is as simple as telling the robot what you want done, and the robot can itself figure out how to adapt its behavior to its environment. But this requires data. Language models and other foundation models mine data from the web, utilizing a significant fraction of all available documents. There is no such treasure trove of robot data, so to enable a robot to learn a new skill, large amounts of data need to be collected <em>with that particular robot and for that particular application</em>.</p><p>If we could train a single <em>generalist</em> robot policy that can perform a wide range of different skills and control a wide range of different robots, we would overcome this challenge: such a model would need only a little bit of data from each robot and each application. Just as a person can learn a new skill quickly by drawing on a lifetime’s worth of experience, such a generalist robot policy could be specialized to new tasks with only modest amounts of data. This would not be the first time that a generalist model beat a specialist at the specialist’s own task: language models have superseded more specialized language processing systems precisely because they can better solve those downstream specialist tasks by drawing on their diverse and general purpose pretraining. In the same way that LLMs provide a <em>foundation model</em> for language, these generalist robot policies will provide a robot foundation model for physical intelligence.</p><p>To get there, we will need to solve major technical challenges. Our first step is <span>π<sub>0</sub></span>, a prototype model that combines large-scale multi-task and multi-robot data collection with a new network architecture to enable the most capable and dexterous generalist robot policy to date. While we believe this is only a small early step toward developing truly general-purpose robot models, we think it represents an exciting step that provides a glimpse of what is to come.</p><h3>A cross-embodiment training mixture</h3></div>
<div><p><span>π<sub>0</sub></span> uses Internet-scale vision-language pre-pretraining, open-source robot
manipulation datasets, and our own datasets consisting of dexterous tasks from
8 distinct robots. The model can then perform a wide variety of tasks, via
either zero-shot prompting or fine-tuning.</p></div>
<p>Our first prototype generalist robot policy is trained on the largest robot
interaction dataset to date. The full training mixture includes both
open-source data and a large and diverse dataset of dexterous tasks that we
collected across 8 distinct robots.</p>
<div><p>Our dataset contains diverse tasks, with each task exhibiting a wide variety of motion primitives, many different objects, and various scenes.</p></div>
<div><p>The tasks in this dataset exercise different dimensions of robot dexterity while covering the range of real tasks that these robots might be asked to perform, from bussing dishes to packing items into envelopes, folding clothing, routing cables, assembling boxes, plugging in power plugs, packing food into to-go boxes, and picking up and throwing out trash. Our goal in selecting these tasks is not to solve any particular application, but to start to provide our model with a general understanding of physical interactions — an initial foundation for physical intelligence.</p><h3>Inheriting Internet-scale semantic understanding</h3><p>Beyond training on many different robots, <span>π<sub>0</sub></span> inherits semantic knowledge and visual understanding from Internet-scale pretraining by starting from a pre-trained vision-language model (VLM). VLMs are trained to model text and images on the web — widely used VLMs include GPT-4V and Gemini. We use a smaller 3 billion parameter VLM as a starting point, and adapt it for real-time dexterous robot control.</p><p>VLMs effectively transfer semantic knowledge from the web, but they are trained to output only discrete language tokens. Dexterous robot manipulation requires <span>π<sub>0</sub></span> to output motor commands at a high frequency, up to 50 times per second. To provide this level of dexterity, we developed a novel method to augment pre-trained VLMs with continuous action outputs via flow matching, a variant of diffusion models. Starting from diverse robot data and a VLM pre-trained on Internet-scale data, we train our vision-language-action flow matching model, which we can then post-train on high-quality robot data to solve a range of downstream tasks.</p></div>
<div><p><img alt="Our vision-language-action model uses a novel flow matching formulation, which augments a vision-language model pre-trained on Internet-scale data with continuous outputs. This enables high-frequency dexterous control, making it particularly well-suited for fine-tuning for complex robot manipulation tasks, such as folding laundry or assembling boxes." loading="lazy" width="569" height="414" decoding="async" data-nimg="1" src="https://www.physicalintelligence.company/images/blog-03.jpg"></p><p>Our vision-language-action model uses a novel flow matching formulation, which augments a vision-language model pre-trained on Internet-scale data with continuous outputs. This enables high-frequency dexterous control, making it particularly well-suited for fine-tuning for complex robot manipulation tasks, such as folding laundry or assembling boxes.</p></div>
<div><h3>Post-training for dexterous manipulation</h3><p>More complex and dexterous tasks may require the model to be fine-tuned to specialize it to downstream challenges. Fine-tuning the model with high-quality data for a challenging task, such as folding laundry, is analogous to the post-training process employed by LLM designers. Pre-training teaches the model about the physical world, while fine-tuning forces it to perform a particular task <em>well</em>. Let’s take a look at some of these tasks.</p></div>
<div><p>After post-training, the robot can unload the dryer, bring the clothes over to the table, and fold the clothes into a stack. The video is uncut, from a single policy operating fully autonomously.</p></div>
<p><strong>Laundry</strong>. We fine-tuned <span>π<sub>0</sub></span> to fold laundry, using either a mobile robot
or a fixed pair of arms. The goal is to get the clothing into a neat stack.
This task is exceptionally difficult for robots (...and some humans): while a
single t-shirt laid flat on the table can sometimes be folded just by repeating
a pre-scripted set of motions, a pile of tangled laundry can be crumpled in
many different ways, so it is not enough to simply move the arms through the
same motion. To our knowledge, no prior robot system has been demonstrated to
perform this task at this level of complexity.</p>

<p>Notably, by training on diverse data, we find that the robot is able to recover
when someone tries to intervene in a variety of different ways.</p>

<p><strong>Table bussing.</strong> We also fine-tuned the model to bus a table. This requires
the robot to pick up dishes and trash on the table, putting any dishes,
cutlery, or cups into a bussing bin, and putting trash into the trash bin. This
task requires the robot to handle a dizzying variety of items. One of the
exciting consequences of training <span>π<sub>0</sub></span> on large and diverse datasets was the
range of emergent strategies that the robot employed: instead of simply
grasping each item in turn, the model could stack multiple dishes to put them
into the bin together, or shake off trash from a plate into the garbage before
placing the plate into the bussing bin.</p>

<p><strong>Assembling a box.</strong> Here, the robot has to take a flattened cardboard box and
build it, folding the sides and then tucking in the flaps. This is very
difficult, because each fold and tuck might fail in unexpected ways, so the
robot needs to watch its progress and adjust as it goes. It also needs to brace
the box with both arms, even using the table, so that the partially folded box
doesn’t come apart.</p>

<div><h3>Evaluating and comparing <span>π<sub>0</sub></span> to prior models</h3><p>We compared <span>π<sub>0</sub></span> to other robot foundation models that have been proposed in the academic literature on our tasks: <a target="_blank" rel="noopener noreferrer" href="https://openvla.github.io/">OpenVLA</a>, a 7B parameter VLA model that uses discretized actions, and <a target="_blank" rel="noopener noreferrer" href="https://octo-models.github.io/">Octo</a>, a 93M parameter model that uses diffusion outputs. These tasks are very difficult compared to those that are typically used in academic experiments — for example, the tasks in the <a target="_blank" rel="noopener noreferrer" href="https://openvla.github.io/">OpenVLA evaluation</a> typically consist of single stage behaviors (e.g., “put eggplant into pot”), whereas our simplest bussing task consisting of sorting multiple objects into either a garbage bin or a bussing bin, and our more complex tasks might require multiple stages, manipulation of deformable objects, and the ability to deploy one of many possible strategies given the current configuration of the environment. These tasks are evaluated according to a scoring rubric that assigns a score of 1.0 for a fully successful completion, with “partial credit” for partially correct execution (e.g., bussing half the objects leads to a score of 0.5). The average scores across 5 zero-shot evaluation tasks are shown below, comparing the full <span>π<sub>0</sub></span> pre-trained model, <span>π<sub>0</sub></span>-small, which is a 470M parameter model that does not use VLM pre-training, <a target="_blank" rel="noopener noreferrer" href="https://openvla.github.io/">OpenVLA</a>, and <a target="_blank" rel="noopener noreferrer" href="https://octo-models.github.io/">Octo</a>. Although OpenVLA and Octo can attain non-zero performance on the easiest of these tasks (“Bussing Easy”), <span>π<sub>0</sub></span> is by far the best-performing model across all of the tasks. The small version, <span>π<sub>0</sub></span>-small, attains the second best performance, but there is more than a 2x improvement in performance from using our full-size architecture with VLM pre-training.</p></div>
<div><div><p><img alt="Zero-shot Performance Comparison Across Tasks" loading="lazy" width="1390" height="700" decoding="async" data-nimg="1" srcset="https://www.physicalintelligence.company/_next/image?url=%2Fimages%2Fp0-zero-shot-comparison.png&amp;w=1920&amp;q=75 1x, https://www.physicalintelligence.company/_next/image?url=%2Fimages%2Fp0-zero-shot-comparison.png&amp;w=3840&amp;q=75 2x" src="https://www.physicalintelligence.company/_next/image?url=%2Fimages%2Fp0-zero-shot-comparison.png&amp;w=3840&amp;q=75"></p></div><p>Average scores for <span>π<sub>0</sub></span>, <span>π<sub>0</sub></span>-small, OpenVLA, and Octo for zero-shot evaluation on 5 test tasks. Across all of the tasks, <span>π<sub>0</sub></span> consistently attains good performance, and outperforms both the small variant and the prior models.</p></div>
<div><p>We include detailed videos from our rigorous empirical evaluation below, with examples of successful and failed episodes for both our zero-shot experiments and the fine-tuning evaluation. Complete results from all experiments can be found in the <a href="https://www.physicalintelligence.company/download/pi0.pdf">full article</a>.</p><h3>Where do we go from here?</h3><p>Our mission at Physical Intelligence is to develop foundation models that can control any robot to perform any task. Our experiments so far show that such models can control a variety of robots and perform tasks that no prior robot learning system has done successfully, such as folding laundry from a hamper or assembling a cardboard box. But generalist robot policies are still in their infancy, and we have a long way to go. The frontiers of robot foundation model research include long-horizon reasoning and planning, autonomous self-improvement, robustness, and safety. We expect that the coming year will see major advances along all of these directions, but the initial results paint a promising picture for the future of robot foundation models: highly capable generalist policies that inherit semantic understanding from Internet-scale pretraining, incorporate data from many different tasks and robot platforms, and enable unprecedented dexterity and physical capability.</p><p>We also think that succeeding at this will require not only new technologies and more data, but a collective effort involving the entire robotics community. We already have collaborations underway with a number of companies and robotics labs, both to refine hardware designs for teleoperation and autonomy, and incorporate data from our partners into our pre-trained models so that we can provide access to models adapted to their specific platforms.</p><p>If you are interested in collaborating, please <a target="_blank" rel="noopener noreferrer" href="mailto:collaborate@physicalintelligence.company">reach out</a>. We are particularly excited to work with companies scaling up data collection with robots deployed for real-world applications, who are looking to collaborate on autonomy.</p><p>We are also hiring! If you'd be interested in <a href="https://www.physicalintelligence.company/join-us">joining us</a> please get in touch.</p><p>For researchers interested in our work, collaborations, or other queries, please write to <a target="_blank" rel="noopener noreferrer" href="mailto:research@physicalintelligence.company">research@physicalintelligence.company</a>.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A skeleton made from the bones of at least eight people thousands of years apart (103 pts)]]></title>
            <link>https://www.smithsonianmag.com/smart-news/archaeologists-are-bewildered-by-a-skeleton-made-from-the-bones-of-at-least-eight-people-who-died-thousands-of-years-apart-180985419/</link>
            <guid>42097856</guid>
            <pubDate>Sun, 10 Nov 2024 00:51:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.smithsonianmag.com/smart-news/archaeologists-are-bewildered-by-a-skeleton-made-from-the-bones-of-at-least-eight-people-who-died-thousands-of-years-apart-180985419/">https://www.smithsonianmag.com/smart-news/archaeologists-are-bewildered-by-a-skeleton-made-from-the-bones-of-at-least-eight-people-who-died-thousands-of-years-apart-180985419/</a>, See on <a href="https://news.ycombinator.com/item?id=42097856">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-article-body="">
        
          <figure>
            <img src="https://th-thumbnailer.cdn-si-edu.com/R12Y7ouc2E__2jyZwjRDkMKkh3A=/1000x750/filters:no_upscale():focal(792x543:793x544)/https://tf-cmsv2-smithsonianmag-media.s3.amazonaws.com/filer_public/a4/26/a426c7fb-e4cc-499d-81a3-de0eb2923e37/urn_cambridgeorg_id_binary-alt_20241019094932-63883-optimisedimage-png-s0003598x24001583_fig2.jpeg" alt="burial" itemprop="image">
            <figcaption>
              
                Researchers performed DNA testing on the bones highlighted in this image.
              <span>Paumen, Wargnies and Demory / Fédération Wallonie-Bruxelles / Veselka et al.</span>
            </figcaption>
          </figure>
        

        

        <p>Back in the 1970s, when archaeologists excavated a skeleton from an ancient graveyard in Belgium, they thought they had found a typical Roman burial. But during a recent reexamination of the bones, researchers noticed peculiarities. For instance, the skeleton’s spine appeared to be made of both adolescent and adult vertebrae.</p>

<p>“I started thinking, okay, something really weird is going on,” researcher&nbsp;<a href="https://www.barbaraveselka.nl/">Barbara Veselka</a>, an archaeologist at the&nbsp;<a href="https://www.vub.be/en">Free University of Brussels</a>, tells&nbsp;<a href="https://www.newscientist.com/article/2454310-a-bizarre-skeleton-from-a-roman-grave-has-bones-from-eight-people/"><em>New Scientist</em></a>’s Christa Lesté-Lasserre.</p>

<p>Veselka’s team used radiocarbon dating to analyze the individual bones; they also sequenced ancient DNA found in them. They found that the skeleton is made of bones from at least eight unrelated men and women, according to a recent study published in the journal <a href="https://www.cambridge.org/core/journals/antiquity/article/assembling-ancestors-the-manipulation-of-neolithic-and-galloroman-skeletal-remains-at-pommeroeul-belgium/A25B2FBB53A9DE7665F30AD14F06A22A"><em>Antiquity</em></a>.</p>

<figure>
  
    <img src="https://th-thumbnailer.cdn-si-edu.com/AqKPkC6v3w5WuKgDMEmc2QPaIBI=/fit-in/1072x0/filters:focal(384x486:385x487)/https://tf-cmsv2-smithsonianmag-media.s3.amazonaws.com/filer_public/77/ab/77abed2f-1724-4498-896b-a26e4ee360a3/urn_cambridgeorg_id_binary-alt_20241019094932-42173-optimisedimage-png-s0003598x24001583_fig4.jpeg" alt="toe" loading="lazy">
  

  <figcaption>
    
      The skeleton contained these five adult foot bones and two adolescent toe bones.
    
    
      <span>Veselka et al.</span>
    
  </figcaption>
</figure>
<p>The skeleton was found in the town of Pommerœul, near Belgium’s border with France. Placed in the fetal position, it was the only intact body in a graveyard containing 76 cremation burials. Because of a <a href="https://www.metmuseum.org/art/collection/search/246463">bone pin</a> found near the head, researchers initially concluded the burial was Roman, dating to the second or third century C.E., when the Roman Empire covered the&nbsp;<a href="https://www.britannica.com/place/Gaul-ancient-region-Europe">Gallic</a> lands of present-day France and Belgium.</p>

<p>The skeleton’s bones are actually much older, with the earliest “contributor” dying nearly 4,445 years ago, per <em>New Scientist</em>. The skeleton contains bones from multiple generations in the late&nbsp;<a href="https://www.britannica.com/event/Neolithic">Neolithic</a> period, who lived thousands of years before the Romans&nbsp;<a href="https://www.worldhistory.org/Roman_Gaul/">conquered</a> Gaul.</p>

<p>Still, the skeleton isn’t completely out of place in the Roman cemetery. According to genetic analysis, its skull belongs to a Roman woman of the third or fourth century C.E., whose DNA matches similarly-aged Roman remains in a nearby cemetery—probably siblings.</p>

<p>How did this composite Neolithic skeleton end up with a Roman skull more than 2,000 years later? As the researchers write, Gallo-Roman groups may have disturbed the old burial while interring their own dead. If it was headless, the Romans might have “completed” it by adding a skull of their own. If not, maybe they replaced the skull. Or maybe they created the entire amalgamated skeleton themselves.</p><center><blockquote><p lang="en" dir="ltr">Our paper is out! Want to know more about unusual burial practices in the Late Neolithic AND Gallo-Roman time? You can read all about it in our open access publication!<a href="https://t.co/vPnQJVAL9F">https://t.co/vPnQJVAL9F</a></p>— Dr Barbara Veselka (@VeselkaBarbara) <a href="https://twitter.com/VeselkaBarbara/status/1849104784558620989?ref_src=twsrc%5Etfw">October 23, 2024</a></blockquote> </center>
<p>“Whether the assembly of the bones occurred in the late Neolithic or in the Roman period, the presence of the ‘individual’ was clearly intentional,” write the researchers. “The bones were selected, a fitting location chosen and the elements arranged carefully to mimic the correct anatomical order.”</p>

<p>Whoever composed the burial understood enough about the human skeletal structure to create a convincing puzzle. As Veselka tells <em>New Scientist</em>, “They knew what they were doing, for sure.”</p>

<p>As <a href="https://www.macalester.edu/anthropology/facultystaff/jane-holmstrom/">Jane Holmstrom</a>, a bioarchaeologist at Macalester College who wasn’t involved in the research, tells&nbsp;<a href="https://www.livescience.com/archaeology/romans/puzzling-patchwork-skeleton-in-belgium-contains-bones-from-5-people-spanning-2-500-years"><em>Live Science</em></a>’s Kristina Killgrove, this “fascinating and complex” study may help illuminate Neolithic burial customs. Veselka notes that Pommerœul was located near a river—an enduring mark of geographic and spiritual importance. Various groups over time may have wanted to control it.</p>

<p>The composite skeleton “provides an interesting possibility of land-claiming through burial during the Neolithic,” Holmstrom adds, “with family groups within the clan asserting claim together, with the Romans furthering the land claim to assert their authority over Gaul.”</p>

        

        

        
          
  <div>
      <p>Get the latest stories in your inbox every weekday.</p>
      
    </div>


        

        

        
          


  
    
      
    
  

  


        

        
        
        
        

        
          
            <section>
              <nav>Filed Under:
                
                  
                    <a href="https://www.smithsonianmag.com/tag/ancient-civilizations/">Ancient Civilizations</a>, 
                  
                
                  
                    <a href="https://www.smithsonianmag.com/tag/archaeology/">Archaeology</a>, 
                  
                
                  
                    <a href="https://www.smithsonianmag.com/tag/bones/">Bones</a>, 
                  
                
                  
                    <a href="https://www.smithsonianmag.com/tag/death/">Death</a>, 
                  
                
                  
                    <a href="https://www.smithsonianmag.com/tag/dna/">DNA</a>, 
                  
                
                  
                    <a href="https://www.smithsonianmag.com/tag/european-history/">European History</a>, 
                  
                
                  
                    <a href="https://www.smithsonianmag.com/tag/history/">History</a>, 
                  
                
                  
                    <a href="https://www.smithsonianmag.com/tag/new-research/">New Research</a>, 
                  
                
                  
                    <a href="https://www.smithsonianmag.com/tag/rituals-and-traditions/">Rituals and Traditions</a>, 
                  
                
                  
                    <a href="https://www.smithsonianmag.com/tag/roman-empire/">Roman Empire</a>
                  
                
              </nav>
            </section>
          
        

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LLMs have reached a point of diminishing returns (127 pts)]]></title>
            <link>https://garymarcus.substack.com/p/confirmed-llms-have-indeed-reached</link>
            <guid>42097774</guid>
            <pubDate>Sun, 10 Nov 2024 00:25:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://garymarcus.substack.com/p/confirmed-llms-have-indeed-reached">https://garymarcus.substack.com/p/confirmed-llms-have-indeed-reached</a>, See on <a href="https://news.ycombinator.com/item?id=42097774">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg" width="1170" height="1227" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1227,&quot;width&quot;:1170,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:243099,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>For years I have been warning that “scaling” — eeking out improvements in AI by adding more data and more compute, without making fundamental architectural changes — would not continue forever. In my most notorious article, in March of 2022, I argued that “</span><a href="https://nautil.us/deep-learning-is-hitting-a-wall-238440/" rel="">deep learning is hitting a wall</a><span>”. Central to the argument was that pure scaling would not solve hallucinations or abstraction; I concluded that “there are serious holes in the scaling argument.” </span></p><p>And I got endless grief for it. Sam Altman implied (without saying my name, but riffing on the images in my then-recent article) I was a “mediocre deep learning skeptic”; Greg Brockman openly mocked the title. Yann LeCun wrote that deep learning wasn’t hitting a wall, and so on. Elon Musk himself made fun of me and the title earlier this year.</p><p><span>The thing is, </span><strong>in the long term, science isn’t majority rule</strong><span>. In the end, the truth generally outs. Alchemy had a good run, but it got replaced by chemistry. The truth is that scaling is running out, and that truth is, at last coming out.</span></p><p>A few days ago, the well-known venture capitalist Marc Andreesen started to spill the beans, saying on a podcast “we're increasing [graphics processing units] at the same rate, we're not getting the intelligent improvements at all out of it” –  which is basically VC-ese for “deep learning is hitting a wall.”</p><p><span>Just a few moments ago, Amir Efrati, editor of the industry trade journal </span><em>The Information</em><span> further confirmed that we have reached a period of diminishing returns, writing on X that “OpenAI's [upcoming] Orion model shows how GPT improvements are slowing down”.  </span></p><p><span>Just as I argued here in April 2024, LLMs </span><a href="https://open.substack.com/pub/garymarcus/p/evidence-that-llms-are-reaching-a?r=8tdk6&amp;utm_campaign=post&amp;utm_medium=web" rel="">have reached a point of diminishing returns</a><span>. </span></p><p>§</p><p>The economics are likely to be grim. Sky high valuation of companies like OpenAI and Microsoft are largely based on the notion that LLMs will, with continued scaling, become artificial general intelligence. As I have always warned, that’s just a fantasy. There is no principled solution to hallucinations in systems that traffic only in the statistics of language without explicit representation of facts and explicit tools to reason over those facts.</p><p>LLMs will not disappear, even if improvements diminish, but the economics will likely never make sense: additional training is expensive, the more scaling, the more costly. And, as I have been warning, everyone is landing in more or less the same place, which leaves nobody with a moat. LLMs such as they are, will become a commodity; price wars will keep revenue low. Given the cost of chips, profits will be elusive. When everyone realizes this, the financial bubble may burst quickly; even NVidia might take a hit, when people realize the extent to which its valuation was based on a false premise.</p><p>§</p><p>The sociology here has been perverse too, for a really long time. Many people (especially LeCun, but also a legion of tech influencers who followed his lead) have tried to deplatform me. </p><p>The media has done little to counter the mob psychology; they have mostly listened to people with money, with vested interests, not to scientists. Many of us, including Melanie Mitchell, Subbarao Kambahapati, Emily Bender, Ernie Davis, etc. have been emphasizing for ages that there are principled limits with LLMs. Media (with notable exceptions like Ezra Klein, who gave me a clear platform for skepticism in January 2023) has rarely listened, instead often glorifying the hype of people like Altman and Musk.</p><p>Worse, the US AI policy now, and likely in the next administration, has largely been driven by hype, and the assumption that returns for LLM scaling would not diminish.  And yet here we are at the end of 2024, and even Altman and Andreesen are perhaps starting to see it.</p><p>Meanwhile, precious little investment has been made in other approaches. If LLMs won’t get the US to trustworthy AI, and our adversaries invest in alternative approaches, we could easily be outfoxed. The US has been putting all its AI eggs in the LLM basket, and that may well prove to be an epic, massive mistake.</p><p>§</p><p>In April, when I first saw direct empirical evidence that this moment had come, I wrote (and stand by):</p><blockquote><p>If enthusiasm for GenAI dwindles and market valuations plummet, AI won’t disappear, and LLMs won’t disappear; they will still have their place as tools for statistical approximation.</p><p>But that place may be smaller; it is entirely possible that LLMs on their own will never live up to last year’s wild expectations.</p><p>Reliable, trustworthy AI is surely achievable, but we may need to go back to the drawing board to get there.</p></blockquote><p>I’m glad that the market is finally recognizing that what I’ve been saying is true. Hopefully now we can make real progress.</p><p><em><strong>Gary Marcus</strong><span> has been warning about the foundational limits to traditional neural network approaches since his 2001 book The Algebraic Mind (where he first described hallucinations), and amplified those warnings in Rebooting AI and his most recent book Taming Silicon Valley.</span></em><span> </span></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Grim Fandango (289 pts)]]></title>
            <link>https://www.filfre.net/2024/11/grim-fandango/</link>
            <guid>42097261</guid>
            <pubDate>Sat, 09 Nov 2024 22:17:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.filfre.net/2024/11/grim-fandango/">https://www.filfre.net/2024/11/grim-fandango/</a>, See on <a href="https://news.ycombinator.com/item?id=42097261">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
														<p><a href="https://www.filfre.net/2024/11/grim-fandango/10591938-grim-fandango-windows-front-cover/" rel="attachment wp-att-6154"><img fetchpriority="high" decoding="async" src="https://www.filfre.net/wp-content/uploads/2024/10/10591938-grim-fandango-windows-front-cover-251x300.jpg" alt="" width="377" height="450" srcset="https://www.filfre.net/wp-content/uploads/2024/10/10591938-grim-fandango-windows-front-cover-251x300.jpg 251w, https://www.filfre.net/wp-content/uploads/2024/10/10591938-grim-fandango-windows-front-cover.jpg 670w" sizes="(max-width: 377px) 100vw, 377px"></a></p>
<blockquote><p>My one big regret was <a href="https://www.filfre.net/2024/01/televising-the-revolution">the PlayStation version [of Broken Sword]</a>. No one thought it would sell, so we kept it like the PC version. In hindsight, I think if we had introduced direct control in this game, it would have been enormous.</p>
<p>— Charles Cecil of Revolution Software, speaking from the Department of Be Careful What You Wish For</p>
</blockquote>
<hr>

<p>One day in June of 1995, Tim Schafer came to work at LucasArts and realized that, for the first time in a long time, he didn’t have anything pressing to do. <a href="https://www.filfre.net/2021/07/full-throttle"><em>Full Throttle</em></a>, his biker movie of an adventure game, had been released several weeks before. Now, all of the initial crush of interviews and marketing logistics was behind him. A mountain had been climbed. So, as game designers do, he started to think about what his next Everest should be.</p>
<p>Schafer has told in some detail how he came up with the core ideas behind <em>Grim Fandango</em> over the course of that summer of 1995.</p>
<blockquote><p>The truth is, I had part of the Fandango idea before I did Full Throttle. I wanted to do a game that would feature those little papier-mâché folk-art skeletons from Mexico. I was looking at their simple shapes and how the bones were just painted on the outside, and I thought, “Texture maps! 3D! The bones will be on the outside! It’ll look cool!”</p>
<p>But then I was stuck. I had these skeletons walking around the Land of the Dead. So what? What did they do? Where were they going? What did they want? Who’s the main character? Who’s the villain? The mythology said that the dead walk the dark plane of the underworld known as Mictlān for four years, after which their souls arrive at the ninth plane, the land of eternal rest. Sounds pretty “questy” to me. There you have it: a game.</p>
<p>“Not cool enough,” said Peter Tscale, my lead artist. “A guy walking in a supernatural world? What’s he doing? Supernatural things? It just sounds boring to me.”</p>
<p>So, I revamped the story. Adventure games are all fantasies really, so I had to ask myself, “Who would people want to be in a game? What would people want to do?” And in the Land of the Dead, who would people rather be than Death himself? Being the Grim Reaper is just as cool as being a biker, I decided. And what does the Grim Reaper do? He picks up people who have died and carts them over from the other world. Just like a driver of a taxi or limo.</p>
<p>Okay, so that’s Manny Calavera, our main character. But who’s the bad guy? What’s the plot? I had just seen <a href="https://www.imdb.com/title/tt0071315">Chinatown</a>, and I really liked the whole water-supply/real-estate scam that Noah Cross had going there, so of course I tried to rip that off and have Manny be a real-estate salesman who got caught up in a real-estate scandal. Then he was just like the guys in <a href="https://www.imdb.com/title/tt0104348/">Glengarry Glen Ross</a>, always looking for the good leads. But why would Hector Lemans, my villain, want real estate? Why would anyone? They’re dead! They’re only souls. What do souls in the Land of the Dead want?</p>
<p>They want to get out! They want safe passage out, just like in <a href="https://www.imdb.com/title/tt0034583/">Casablanca</a>! The Land of the Dead is a transitory place, and everybody’s waiting around for their travel papers. So Manny is a travel agent, selling tickets on the big train out of town, and Hector’s stealing the tickets…</p></blockquote>
<div id="attachment_6156"><p><a href="https://www.filfre.net/2024/11/grim-fandango/glottis/" rel="attachment wp-att-6156"><img decoding="async" aria-describedby="caption-attachment-6156" src="https://www.filfre.net/wp-content/uploads/2024/10/glottis-300x225.png" alt="" width="600" height="450" srcset="https://www.filfre.net/wp-content/uploads/2024/10/glottis-300x225.png 300w, https://www.filfre.net/wp-content/uploads/2024/10/glottis.png 640w" sizes="(max-width: 600px) 100vw, 600px"></a></p><p id="caption-attachment-6156">The missing link between <em>Full Throttle</em> and <em>Grim Fandango</em> is Manny’s chauffeur and mechanic Glottis, a literal speed demon.</p></div>
<p>This, then, became the elevator pitch for <em>Grim Fandango</em>. Begin with the rich folklore surrounding Mexico’s Day of the Dead, a holiday celebrated each year just after Halloween, which combines European Christian myths about death and the afterlife with the older, indigenous ones that still haunt the Aztec ruins of Teopanzolco. Then combine it with classic film noir to wind up with Raymond Chandler in a Latino afterlife. It was nothing if not a strikingly original idea for an adventure game. But there was also one more, almost equally original part of it: to do it in 3D.</p>
<p>To hear Tim Schafer tell the story, the move away from LucasArts’s traditional pixel art and into the realm of points, polygons, and textures was motivated by his desire to deliver a more cinematic experience. By no means does this claim lack credibility; as you can gather by reading what he wrote above, Schafer was and is a passionate film buff, who tends to resort to talking in movie titles when other forms of communication fail him. The environments in previous LucasArts adventure games — even the self-consciously cinematic <em>Full Throttle</em> — could only be shown from the angle the pixel artists had chosen to drawn them from. In this sense, they were like a theatrical play, or a <em>really</em> old movie, from the time before Orson Welles emancipated his camera and let it begin to roam freely through his sets in <a href="https://www.imdb.com/title/tt0033467/"><em>Citizen Kane</em></a>. By using 3D, Schafer could become the Orson Welles of adventure games; he would be able to deliver dramatic angles and closeups as the player’s avatar moved about, would be able to put the player <em>in</em> his world rather than forever forcing her to look down on it from on-high. This is the story he still tells today, and there’s no reason to believe it isn’t true enough, as far as it goes.</p>
<p>Nevertheless, it’s only half of the full story. The other half is a messier, less idealistic tale of process and practical economics.</p>
<p>Reckoned in their cost of production per hour of play time delivered, adventure games stood apart from any other genre in their industry, and not in a good way. Building games entirely out of bespoke, single-use puzzles and assets was <em>expensive</em> in contrast to the more process-intensive genres. As time went on and gamers demanded ever bigger, prettier adventures, in higher resolutions with more colors, this became more and more of a problem. Already in 1995, when adventure games were still selling very well, the production costs that were seemingly inherent to the genre were a cause for concern. And the following year, when the genre <a href="https://www.filfre.net/2022/06/toonstruck-or-a-case-study-in-the-death-of-adventure-games">failed to produce</a> a single million-plus-selling breakout hit for the first time in half a decade, they began to look like an existential threat. At that point, LucasArts’s decision to address the issue proactively in <em>Grim Fandango </em>by switching from pixel art to 3D suddenly seemed a very wise move indeed. For a handful of Silicon Graphics workstations running 3D-modelling software could churn out images far more quickly than an army of pixel artists, at a fraction of the cost per image. If the graphics that resulted lacked some of the quirky, hand-drawn, cartoon-like personality that had marked LucasArts’s earlier adventure games, they made up for that by virtue of their flexibility: a scene could be shown from a different angle just by changing a few parameters instead of having to redraw it from scratch. This really did raise the prospect of making the more immersive games that Tim Schafer desired. But from a bean counter’s point of view, the best thing about it was the cost savings.</p>
<p>And there was one more advantage as well, one that began to seem ever more important as time went on and the market for adventure games running on personal computers continued to soften. Immersive 3D was more or less the default setting of the Sony PlayStation, which had <a href="https://www.filfre.net/2023/12/putting-the-j-in-the-rpg-part-2-playstation-for-the-win">come roaring out of Japan</a> in 1995 to seize the title of the most successful games console of the twentieth century just before the curtain fell on that epoch. In addition to its 3D hardware, the PlayStation sported a CD drive, memory cards for saving state, and a slightly older typical user than the likes of Nintendo and Sega. And yet, although a number of publishers ported their 2D computer-born adventure games to the PlayStation, they never felt entirely at home there, having been designed for a mouse rather than a game controller.<span><a role="button" tabindex="0" onclick="footnote_moveToReference_6147_1('footnote_plugin_reference_6147_1_1');" onkeypress="footnote_moveToReference_6147_1('footnote_plugin_reference_6147_1_1');"><sup id="footnote_plugin_tooltip_6147_1_1">[1]</sup></a><span id="footnote_plugin_tooltip_text_6147_1_1">A mouse was available as an accessory for the PlayStation, but it was never very popular.</span></span> A 3D adventure game with a controller-friendly interface might be a very different proposition. If it played its cards right, it would open the door to an installed base of customers five to ten times the size of the extant market for games on personal computers.</p>
<div id="attachment_6149"><p><a href="https://www.filfre.net/2024/11/grim-fandango/manny/" rel="attachment wp-att-6149"><img decoding="async" aria-describedby="caption-attachment-6149" src="https://www.filfre.net/wp-content/uploads/2024/10/manny-300x225.png" alt="" width="600" height="450" srcset="https://www.filfre.net/wp-content/uploads/2024/10/manny-300x225.png 300w, https://www.filfre.net/wp-content/uploads/2024/10/manny.png 640w" sizes="(max-width: 600px) 100vw, 600px"></a></p><p id="caption-attachment-6149">Working with 3D graphics in the late 1990s required some clever sleight of hand if they weren’t to end up looking terrible. <em>Grim Fandango’</em>s masterstroke was to make all of its characters — like the protagonist Manny Calavera, whom you see above — mere skeletons, whose faces are literally painted onto their skulls. (The characters are shown to speak by manipulating the <a href="https://www.filfre.net/2019/01/life-off-the-grid-part-1-making-ultima-underworld">texture maps</a> that represent their faces, not by manipulating the underlying 3D models themselves.) This approach gave the game a look reminiscent of another of its cinematic inspirations, Tim Burton’s <a href="https://www.imdb.com/title/tt0107688/"><em>The Nightmare Before Christmas</em></a>, whilst conveniently avoiding all of the complications of trying to render pliant flesh. A win-win, as they say. Or, as Tim Schafer said: “Instead of fighting the tech limitations of 3D, you have to embrace them and turn them into a style.”</p></div>
<p>But I’m afraid I’ve gotten slightly ahead of myself. This constellation of ideas, affordances, problems, and solutions was still in a nascent form in November of 1995, when LucasArts hired a young programmer fresh out of university by the name of Bret Mogilefsky. Mogilefsky was a known quantity already, having worked at LucasArts as a tester on and off while he was earning his high-school and university diplomas. Now, he was entrusted with the far more high-profile task of making SCUMM, LucasArts’s venerable adventure engine, safe for 3D.</p>
<p>After struggling for a few months, he concluded that this latest paradigm shift was just too extreme for an engine that had been <a href="https://www.filfre.net/2015/07/a-new-force-in-games-part-3-scumm">created on a Commodore 64</a> circa 1986 and ported and patched from there. He would have to tear SCUMM down so far in order to add 3D functionality that it would be easier and cleaner simply to make a new engine from scratch. He told his superiors this, and they gave him permission to do so — albeit suspecting all the while, Mogilefsky is convinced, that he would eventually realize that game engines are easier envisioned than implemented and come crawling back to SCUMM. By no means was he the first bright spark at LucasArts who thought he could reinvent the adventuring wheel.</p>
<p>But he did prove the first one to call his bosses’ bluff. The engine that he called GrimE (“<em>Grim</em> Engine,” but pronounced like the synonym for “dirt”) used a mixture of pre-rendered and real-time-rendered 3D. The sets in which Manny and his friends and enemies played out their dramas would be the former; the aforementioned actors themselves would be the latter. GrimE was a piebald beast in another sense as well: that of cheerfully appropriating whatever useful code Mogilefsky happened to find lying around the house at LucasArts, most notably from the first-person shooter <a href="https://www.filfre.net/2024/04/jedi-knight-plus-notes-on-an-expanded-universe"><em>Jedi Knight</em></a>.</p>
<p>Like SCUMM before it, GrimE provided relatively non-technical designers like Tim Schafer with a high-level scripting language that they could use themselves to code all of the mechanics of plot and puzzles. Mogilefsky adapted for this task Lua, a new, still fairly obscure programming language out of Brazil. It was an inspired choice. Elegant, learnable, and yet infinitely and easily extendible, Lua has gone on to become a staple language of modern game development, to be found today in such places as the wildly popular Roblox platform.</p>
<p>The most frustrating aspects of GrimE from a development perspective all clustered around the spots where its two approaches to 3D graphics rubbed against one another, producing a good deal of friction in the process. If, for example, Manny was to drink a glass of whiskey, the pre-rendered version of the glass that was part of the background set had to be artfully swapped with its real-time-rendered incarnation as soon as Manny began to interact with it. Getting such actions to look seamless absorbed vastly more time and energy than anyone had expected it to.</p>
<p>In fact, if the bean counters had been asked to pass judgment, they would have had a hard time labeling GrimE a success at all under their metrics. <em>Grim Fandango</em> was in active development for almost three full years, and may have ended up costing as much as $3 million. This was at least two and a half times as much as <em>Full Throttle</em> had cost, and placed it in the same ballpark as <a href="https://www.filfre.net/2024/04/the-curse-of-monkey-island"><em>The Curse of Monkey Island</em></a>, LucasArts’s last and most audiovisually lavish SCUMM adventure, which was released a year before <em>Grim Fandango</em>. Further, despite employing a distinctly console-like control scheme in lieu of pointing and clicking with the mouse, <em>Grim Fandango</em> would never make it to the PlayStation; GrimE ended up being just too demanding to be made to work on such limited hardware.<span><a role="button" tabindex="0" onclick="footnote_moveToReference_6147_1('footnote_plugin_reference_6147_1_2');" onkeypress="footnote_moveToReference_6147_1('footnote_plugin_reference_6147_1_2');"><sup id="footnote_plugin_tooltip_6147_1_2">[2]</sup></a><span id="footnote_plugin_tooltip_text_6147_1_2"><em>Escape from Monkey Island</em>, the only other game ever made using GrimE, was ported to the more capable PlayStation 2 in 2001.</span></span></p>
<p>All that aside, though, the new engine remained an impressive technical feat, and did succeed in realizing most of Tim Schafer’s aesthetic goals for it. Even the cost savings it apparently failed to deliver come with some mitigating factors. Making the first game with a new engine is always more expensive than making the ones that follow; there was no reason to conclude that GrimE couldn’t deliver real cost savings on LucasArts’s <em>next</em> adventure game. Then, too, for all that <em>Grim Fandango</em> wound up costing two and a half times as much as <em>Full Throttle</em>, it was also well over two and a half times as long as that game.</p>
<p>“Game production schedules are like flying jumbo jets,” says Tim Schafer. “It’s very intense at the takeoff and landing, but in the middle there’s this long lull.” The landing is the time of crunch, of course, and the crunch on <em>Grim Fandango</em> was protracted and brutal even by the industry’s usual standards, stretching out for months and months of sixteen- and eighteen-hour days. For by the beginning of 1998, the game was way behind schedule and way over budget, facing a marketplace that was growing more and more unkind to the adventure genre in general. This was not a combination to instill patience in the LucasArts executive suite. Schafer’s team did get the game done by the autumn of 1998, as they had been ordered to do in no uncertain terms, but only at a huge cost to their psychological and even physical health.</p>
<p>Bret Mogilefsky remembers coming to Schafer at one point to tell him that he just didn’t think he could go on like this, that he simply had to have a break. He was met with no sympathy whatsoever. To be fair, he probably shouldn’t have expected any. Crunch was considered par for the course in the industry during this era, and LucasArts was among the worst of its practitioners. Long hours spent toiling for ridiculously low wages — Mogilefsky was hired to be the key technical cog in this multi-million-dollar project for a salary of about $30,000 per year — were considered the price you paid for the privilege of working at The <em>Star Wars</em> Company.</p>
<p>Even setting aside the personal toll it took on the people who worked there, crunch did nothing positive for the games themselves. As we’ll see, <em>Grim Fandango</em> shows the scars of crunch most obviously in its dodgy puzzle design. Good puzzles result from a methodical, iterative process of testing and carefully considering the resulting feedback. <em>Grim Fandango</em> did not benefit from such a process, and this lack is all too plainly evident.</p>
<p>But before I continue making some of you very, very mad at me, let me take some time to note the strengths of <em>Grim Fandango</em>, which are every bit as real as its weaknesses. Indeed, if I squint just right, so that my eyes <em>only</em> take in its strengths, I have no problem understanding why it’s to be found on so many lists of “The Best Adventure Games Ever,” sometimes even at the very top.</p>
<p>There’s no denying the stuff that <em>Grim Fandango</em> does well. Its visual aesthetic, which I can best describe as 1930s Art Deco meets Mexican folk art meets 1940s gangster flick, is unforgettable. And it’s married to a script that positively crackles with wit and pathos. Our hero Manny is the rare adventure-game character who can be said to go through an actual character arc, who grows and evolves over the course of his story. The driving force behind the plot is his love for a woman named Meche. But his love isn’t the puppy love that Guybrush Threepwood has for Elaine in the <a href="https://www.filfre.net/2017/03/monkey-island-or-how-ron-gilbert-made-an-adventure-game-that-didnt-suck"><em>Monkey Island</em></a> games; the relationship is more nuanced, more adult, more <em>complicated</em>, and its ultimate resolution is all the more moving for that.</p>
<div id="attachment_6150"><p><a href="https://www.filfre.net/2024/11/grim-fandango/sprout/" rel="attachment wp-att-6150"><img decoding="async" aria-describedby="caption-attachment-6150" src="https://www.filfre.net/wp-content/uploads/2024/10/sprout-300x225.webp" alt="" width="600" height="450" srcset="https://www.filfre.net/wp-content/uploads/2024/10/sprout-300x225.webp 300w, https://www.filfre.net/wp-content/uploads/2024/10/sprout.webp 640w" sizes="(max-width: 600px) 100vw, 600px"></a></p><p id="caption-attachment-6150">How do you create real stakes in a story where everyone is already dead? The Land of the Death’s equivalent of death is “sprouting,” in which a character is turned into a bunch of flowers and forced to live another life in that form. Why shouldn’t the dead fear life as much as the living fear death?</p></div>
<p>Tim Schafer did not grow up with the Latino traditions that are such an inextricable part of <em>Grim Fandango</em>. Yet the game never feels like the exercise in clueless or condescending cultural tourism it might easily have become. On the contrary, the setting feels full-bodied, lived-in, natural. The cause is greatly aided by a stellar cast of voice actors with just the right accents. The Hollywood veteran Tony Plana, who plays Manny, is particularly good, teasing out exactly the right blend of world-weary cynicism and tarnished romanticism. And Maria Canalas, who plays Meche, is equally perfect in her role. The non-verbal soundtrack by Peter McConnell is likewise superb, a mixture of mariachi music and cool jazz that shouldn’t work but does. Sometimes it soars to the forefront, but more often it tinkles away in the background, setting the mood. You’d only notice it if it was gone — but trust me, then you would <em>really</em> notice.</p>
<p>This is a <em>big</em> game as well as a striking and stylish one — in fact, by most reckonings the biggest adventure that LucasArts ever made. Each of its four acts, which neatly correspond to the four years that the average soul must spend wandering the underworld before going to his or her final rest, is almost big enough to be a self-contained game in its own right. Over the course of <em>Grim Fandango</em>, Manny goes from being a down-on-his-luck Grim Reaper cum travel agent to a nightlife impresario, from the captain of an ocean liner to a prisoner laboring in an underwater mine. The story does arguably peak too early; the second act, an extended homage to <em>Casablanca</em> with Manny in the role of Humphrey Bogart, is so beautifully realized that much of what follows is slightly diminished by the comparison. Be that as it may, though, it doesn’t mean any of what follows is bad.</p>
<div id="attachment_6155"><p><a href="https://www.filfre.net/2024/11/grim-fandango/rubacava/" rel="attachment wp-att-6155"><img decoding="async" aria-describedby="caption-attachment-6155" src="https://www.filfre.net/wp-content/uploads/2024/10/rubacava-300x225.png" alt="" width="600" height="450" srcset="https://www.filfre.net/wp-content/uploads/2024/10/rubacava-300x225.png 300w, https://www.filfre.net/wp-content/uploads/2024/10/rubacava.png 640w" sizes="(max-width: 600px) 100vw, 600px"></a></p><p id="caption-attachment-6155">The jump cut to Manny’s new life as a bar owner in the port city of Rubacava at the beginning of the second act is to my mind the most breathtaking moment of the game, the one where you first realize how expansive its scope and ambition really are.</p></div>
<p>All told, then, I have no real beef with anyone who chooses to label <em>Grim Fandango</em> an aesthetic masterpiece. If there was an annual award for style in adventure games, this game would have won it easily in 1998, just as Tim Schafer’s <em>Full Throttle</em> would have taken the prize for 1995. Sadly, though, it seems to me that the weaknesses of both games are also the same. In both of their cases, once I move beyond the aesthetics and the storytelling and turn to the gameplay, some of the air starts to leak out of the balloon.</p>
<p>The interactive aspects of <em>Grim Fandango</em> — you know, all that stuff that actually makes it a game — are dogged by two overarching sets of problems. The first is all too typical for the adventure genre: overly convoluted, often nonsensical puzzle design. Tim Schafer was always more intrinsically interested in the worlds, characters, and stories he dreamed up than he was in puzzles. This is fair enough on the face of it; he is very, very good at those things, after all. But it does mean that he needs a capable support network to ensure that his games play as well as they look and read. He had that support for 1993’s <a href="https://www.filfre.net/2019/06/day-of-the-tentacle"><em>Day of the Tentacle</em></a>, largely in the person of his co-designer Dave Grossman; the result was one of the best adventure games LucasArts ever made, a perfect combination of inspired fiction with an equally inspired puzzle framework. Unfortunately, he was left to make <em>Full Throttle</em> on his own, and it showed. Ditto <em>Grim Fandango</em>. For all that he loved movies, the auteur model was not a great fit for Tim Schafer the game designer.</p>
<p><em>Grim Fandango</em> seldom gives you a clear idea of what it is you’re even trying to accomplish. Compare this with <em>The Curse of Monkey Island</em>, the LucasArts adventure just before this one, a game which seemed at the time to herald a renaissance in the studio’s puzzle designs. There, you’re always provided with an explicit set of goals, usually in the form of a literal shopping list. Thus even when the mechanics of the puzzles themselves push the boundaries of real-world logic, you at least have a pretty good sense of where you should be focusing your efforts. Here, you’re mostly left to guess what Tim Schafer would like to have happen to Manny next. You stumble around trying to shake something loose, trying to figure out what you can do and then doing it just because you can. By no means is it lost on me that this sense of confusion arises to a large extent because <em>Grim Fandango</em> is such a character-driven story, one which eschews the mechanistic tic-tac-toe of other adventure-game plots. But recognizing this irony doesn’t make it any less frustrating when you’re wandering around with no clue what the story wants from you.</p>
<p>Compounding the frustrations of the puzzles are the frustrations of the interface. You don’t use the mouse at all; everything is done with the numeric keypad, or, if you’re lucky enough to have one, a console-style controller. (At the time <em>Grim Fandango</em> was released, virtually no one playing games on computers did.) <em>Grim Fandango’</em>s mode of navigation is most reminiscent of the <a href="https://www.filfre.net/2023/11/putting-the-j-in-the-rpg-part-1-dorakue">console-based JRPGs</a> of its era, such as the hugely popular <a href="https://www.filfre.net/2023/12/putting-the-j-in-the-rpg-part-3-playing-final-fantasy-vii-or-old-man-yells-at-jrpg"><em>Final Fantasy VII</em></a>, which sold over 10 million copies on the PlayStation during the late 1990s. Yet in practice it’s far more irritating, because you have to interact with the environment here on a much more granular level. LucasArts themselves referred to their method of steering Manny about as a “tank” interface, a descriptor which turns out to be all too descriptive. It really does feel like you’re driving a bulky, none too agile vehicle through an obstacle course of scenery.</p>
<div id="attachment_6151"><p><a href="https://www.filfre.net/2024/11/grim-fandango/angle/" rel="attachment wp-att-6151"><img decoding="async" aria-describedby="caption-attachment-6151" src="https://www.filfre.net/wp-content/uploads/2024/10/angle-300x225.png" alt="" width="600" height="450" srcset="https://www.filfre.net/wp-content/uploads/2024/10/angle-300x225.png 300w, https://www.filfre.net/wp-content/uploads/2024/10/angle.png 640w" sizes="(max-width: 600px) 100vw, 600px"></a></p><p id="caption-attachment-6151">Make no mistake: the 3D engine makes possible some truly striking views. But too often the designers prioritize visual aesthetics over playability.</p></div>
<p>In the final reckoning, then, an approach that is fine in a JRPG makes just about every aspect of an old-school, puzzle-solving adventure game — which is what <em>Grim Fandango</em> remains in form and spirit when you strip all of the details of its implementation away — more awkward and less fun. Instead of having hotspots in the environment that light up when you pass a mouse cursor over them, as you do in a SCUMM adventure, you have to watch Manny’s head carefully as you drive him around; when it turns to look in a certain direction, that means there’s something he can interact with there. Needless to say, it’s all too easy to miss a turn of his head, and thereby to miss something vital to your progress through the game.</p>
<div id="attachment_6152"><p><a href="https://www.filfre.net/2024/11/grim-fandango/inventory/" rel="attachment wp-att-6152"><img decoding="async" aria-describedby="caption-attachment-6152" src="https://www.filfre.net/wp-content/uploads/2024/10/inventory-300x225.png" alt="" width="600" height="450" srcset="https://www.filfre.net/wp-content/uploads/2024/10/inventory-300x225.png 300w, https://www.filfre.net/wp-content/uploads/2024/10/inventory.png 640w" sizes="(max-width: 600px) 100vw, 600px"></a></p><p id="caption-attachment-6152">The inventory system is also fairly excruciating. Instead of being able to bring up a screen showing all of the items Manny is carrying, you have to cycle through them one by one by punching a key or controller button over and over, listening to him drone out their descriptions over and over as you do so. This approach precludes using one inventory object on another one, cutting off a whole avenue of puzzle design.</p></div>
<p>Now, the apologists among you — and this game does have an inordinate number of them — might respond to these complaints of mine by making reference to the old cliché that, for every door that is closed in life (and presumably in games as well), another one is opened. And in theory, the new engine really does open a door to new types of puzzles that are more tactile and embodied, that make you feel more a part of the game’s world. To Tim Schafer’s credit, he does try to include these sorts of puzzles in quite a few places. To our detriment, though, they turn out to be the worst puzzles in the game, relying on finicky positioning and timing and giving no useful feedback when you get those things slightly wrong.</p>
<p>But even when <em>Grim Fandango</em> presents puzzles that could easily have been implemented in SCUMM, they’re made way more annoying than they ought to be by the engine and interface. When you’re reduced to that final adventurer’s gambit of just trying everything on everything, as you most assuredly will be from time to time here, the exercise takes many times longer than it would using SCUMM, what with having to laboriously drive Manny about from place to place.</p>
<p>Taken as a game rather than the movie it often seems more interested in being, <em>Grim Fandango</em> boils down to a lumpy stew of overthought and thoughtlessness. In the former category, there’s an unpleasant ideological quality to its approach, with its prioritization of some hazy ethic of 3D-powered “immersion” and its insistence that no visible interface elements whatsoever can appear onscreen, even when these choices actively damage the player’s experience. This is where Sid Meier can <a href="https://www.filfre.net/2017/03/whats-the-matter-with-covert-action">helpfully step in to remind us</a> that it is the player who is meant to be having the fun in a game, not the designer.</p>
<p>The thoughtlessness comes in the lack of consideration of what <em>kind</em> of game <em>Grim Fandango</em> is meant to be. Like all big-tent gaming genres, the adventure genre subsumes a lot of different styles of game with different priorities. Some adventures are primarily about exploration and puzzle solving. And that’s fine, although one does hope that those games execute their puzzles better than this one does. But <em>Grim Fandango</em> is not primarily about its puzzles; it wants to take you on a ride, to sweep you along on the wings of a compelling story. And boy, does it have a compelling story to share with you. For this reason, it would be best served by streamlined puzzles that don’t get too much in the way of your progress. The ones we have, however, are not only frustrating in themselves but murder on the story’s pacing, undermining what ought to be <em>Grim Fandango’</em>s greatest strengths. A game like this one that is best enjoyed with a walkthrough open on the desk beside it is, in this critic’s view at least, a broken game by definition.</p>
<p>As with so many near-miss games, the really frustrating thing about <em>Grim Fandango</em> is that the worst of its problems could so easily have been fixed with just a bit more testing, a bit more time, and a few more people who were empowered to push back against Tim Schafer’s more dogmatic tendencies. For the 2015 remastered version of the game, Schafer did grudgingly agree to include an alternative point-and-click interface that is more like that of a SCUMM adventure. The results verge on the transformational. By no means does the addition of a mouse cursor remedy all of the infelicities of the puzzle design, but it does make battering your way through them considerably less painful. If my less-than-systematic investigations on YouTube are anything to go by, this so-old-it’s-new-again interface has become by far the most common way to play the game today.</p>
<div id="attachment_6153"><p><a href="https://www.filfre.net/2024/11/grim-fandango/remaster/" rel="attachment wp-att-6153"><img decoding="async" aria-describedby="caption-attachment-6153" src="https://www.filfre.net/wp-content/uploads/2024/10/remaster-300x225.jpg" alt="" width="600" height="450" srcset="https://www.filfre.net/wp-content/uploads/2024/10/remaster-300x225.jpg 300w, https://www.filfre.net/wp-content/uploads/2024/10/remaster.jpg 640w" sizes="(max-width: 600px) 100vw, 600px"></a></p><p id="caption-attachment-6153">The <em>Grim Fandango</em> remaster. Note the mouse cursor. The new interface is reportedly implemented entirely in in-engine Lua scripts rather than requiring any re-programming of the GrimE engine itself. This means that it would have been perfectly possible to include as an option in the original release.</p></div>
<p>In other places, the fixes could have been even simpler than revamping the interface. A shocking number of puzzles could have been converted from infuriating to delightful by nothing more than an extra line or two of dialog from Manny or one of the other characters. As it is, too many of the verbal nudges that do exist are too obscure by half and are given only once in passing, as part of conversations that can never be repeated. Hints for Part Four are to be found only in Part One; I defy even an elephant to remember them when the time comes to apply them. All told,<em> Grim Fandango</em> has the distinct odor of a game that no one other than those who were too close to it to see it clearly ever really tried to play before it was put in a box and shoved out the door. There was a time when seeking the feedback of outsiders was a standard part of LucasArts’s adventure-development loop. Alas, that era was long passed by the time of <em>Grim Fandango</em>.</p>
<p>Nonetheless, <em>Grim Fandango</em> was accorded a fairly rapturous reception in the gaming press when it was released in the last week of October in 1998, just in time for Halloween and the Mexican Day of the Dead which follows it on November 1. Its story, characters, and setting were justifiably praised, while the deficiencies of its interface and puzzle design were more often than not relegated to a paragraph or two near the end of the review. This is surprising, but not inexplicable. There was a certain sadness in the trade press — almost a collective guilt — about the diminished prospects of the adventure game in these latter years of the decade. Meanwhile LucasArts was still the beneficiary of a tremendous amount of goodwill, thanks to the many classics they had served up during those earlier, better years for the genre as a whole. <em>Grim Fandango</em> was held up as a sort of standard bearer for the embattled graphic adventure, the ideal mix of tradition and innovation to serve as proof that the genre was still relevant in a <a href="https://www.filfre.net/2023/04/the-next-generation-in-graphics-part-1-three-dimensions-in-software-or-quake-and-its-discontents">post-<em>Quake</em></a>, <a href="https://www.filfre.net/2024/07/starcraft-a-history-in-two-acts">post-<em>Starcraft</em></a> world.</p>
<p>For many years, the standard narrative had it that the unwashed masses of gamers utterly failed to respond to the magazines’ evangelism, that <em>Grim Fandango</em> became an abject failure in the marketplace. In more recent years, Tim Schafer has muddied those waters somewhat by claiming that the game actually sold close to half a million copies. I rather suspect that the truth is somewhere between these two extremes. Sales of a quarter of a million certainly don’t strike me as unreasonable once foreign markets are factored into the equation. Such a figure would have been enough to keep <em>Grim Fandango</em> from losing much if any money, but would have provided LucasArts with little motivation to make any more such boldly original adventure games. And indeed, LucasArts would release only one more adventure game of any stripe in their history. It would use the GrimE engine, but it would otherwise play it about as safe as it possibly could, by being yet another sequel to the venerable but beloved <em>Secret of Monkey Island</em>.</p>
<p>As I was at pains to note earlier, I do see what causes some people to rate <em>Grim Fandango</em> so highly, and I definitely don’t think any less of them for doing so. For my part, though, I’m something of a stickler on some points. To my mind, interactivity is the very quality that separates games from other forms of media, making it hard for me to pronounce a game “good” that botches it. I’ve learned to be deeply suspicious of games whose most committed fans want to talk about everything other than that which you the player actually <em>do</em> in them. The same applies when a game’s creators display the same tendency. Listening to the developers’ commentary tracks in the remastered edition of <em>Grim Fandango </em>(who would have imagined in 1998 that games would someday come with commentary tracks?), I was shocked by how little talk there was about the gameplay. It was all lighting and dialog beats and soundtrack stabs and Z-buffers instead — all of which is really, really important in its place, but none of which can yield a great game on its own. Tellingly, when the subject of puzzle design did come up, it always seemed to be in an off-hand, borderline dismissive way. “I don’t know how players are supposed to figure out this puzzle,” says Tim Schafer outright at one point. Such a statement from your lead designer is never a good sign.</p>
<p>But I won’t belabor the issue any further. Suffice to say that <em>Grim Fandango</em> is doomed to remain a promising might-have-been rather than a classic in my book. As a story and a world, it’s kind of amazing. It’s just a shame that the gameplay part of this game isn’t equally inspired.</p>
<hr>
<p><code> </code><br>
<strong>Did you enjoy this article? If so, please think about pitching in to help me make many more like it. You can pledge any amount you like.</strong></p>
<p><a href="https://www.patreon.com/DigitalAntiquarian" rel="attachment wp-att-5598"><img decoding="async" src="https://www.filfre.net/wp-content/uploads/2023/04/Patreon-300x133-1.png" alt="" width="300" height="133"></a></p>
<hr>

<p><strong>Sources:</strong> The book <em>Grim Fandango: Prima’s Official Strategy Guide</em> by Jo Ashburn. <em>Retro Gamer</em> 31 and 92; <em>Computer Gaming World</em> of November 1997, May 1998, and February 1999; <em>Ultimate PC</em> of August 1998. Plus the commentary track from the 2015 <em>Grim Fandango</em> remaster.</p>
<p>Online sources include&nbsp;<em>The International House of Mojo’</em>s <a href="https://mixnmojo.com/features/sitefeatures/LucasArts-Secret-History-13-Grim-Fandango/1">pages on the game</a>, the self-explanatory <a href="https://grimfandango.network/"><em>Grim Fandango Network</em></a>, <em>Gamespot’</em>s <a href="https://web.archive.org/web/20100201172300/http://www.gamespot.com/pc/adventure/grimfandango/review.html">vintage review of the game</a>, and Daniel Albu’s <a href="https://www.youtube.com/watch?v=_qwAzIYaGUI">YouTube conversation with Bret Mogilefsky</a>.</p>
<p>And a special thank-you to reader Matt Campbell, who shared with me the audio of a talk that Bret Mogilefsky gave at the 2005 Lua Workshop, during which he explained how he used that language in GrimE.</p>
<p><strong>Where to Get It:</strong> A modestly remastered version of <em>Grim Fandango</em> is <a href="https://www.gog.com/en/game/grim_fandango_remastered">available for digital purchase</a> at GOG.com.</p>
							
							
														
													</div></div>]]></description>
        </item>
    </channel>
</rss>