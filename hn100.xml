<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 01 Oct 2025 04:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Atuin Desktop: Runbooks That Run – Now Open Source (171 pts)]]></title>
            <link>https://blog.atuin.sh/atuin-desktop-open-source/</link>
            <guid>45431001</guid>
            <pubDate>Tue, 30 Sep 2025 20:44:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.atuin.sh/atuin-desktop-open-source/">https://blog.atuin.sh/atuin-desktop-open-source/</a>, See on <a href="https://news.ycombinator.com/item?id=45431001">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

    <article>

        <header>

                <a href="https://blog.atuin.sh/tag/news/">News</a>
            
                <p>Atuin Desktop looks like a doc, but runs like your terminal. Script blocks, embedded terminals, database clients and prometheus charts - all in one place.</p>

            <div>
                <p><a href="https://blog.atuin.sh/author/ellie/">
                                <img src="https://blog.atuin.sh/content/images/size/w160/2024/01/me2.jpg" alt="Ellie Huxtable">
                            </a>
                </p>
                
            </div>

            
        </header>

        <section>
            <figure data-kg-thumbnail="https://blog.atuin.sh/content/media/2025/09/atuin-demo-final_thumb.jpg" data-kg-custom-thumbnail="">
            <div>
                <video src="https://blog.atuin.sh/content/media/2025/09/atuin-demo-final.mp4" poster="https://img.spacergif.org/v1/1852x1600/0a/spacer.png" width="1852" height="1600" loop="" autoplay="" muted="" playsinline="" preload="metadata"></video>
                
                <div>
                        <p>
                        
                        <span>0:00</span></p><p>
                            /<span>0:27</span>
                        </p>
                        </div>
            </div>
            
        <img src="https://blog.atuin.sh/content/media/2025/09/atuin-demo-final_thumb.jpg"></figure><p>Most infrastructure is held together by five commands someone remembers when shit breaks. Docs are out of date, if they exist. The real answers? Buried in Slack threads, rotting in Notion, or trapped in someone's shell history.</p><p><a href="https://atuin.sh/?ref=blog.atuin.sh" rel="noreferrer">Atuin CLI</a>&nbsp;fixed part of this, with synced, searchable shell history. But history isn’t enough. Teams need workflows they can&nbsp;<strong>repeat, share, and trust</strong>.</p><p>That’s why we built Atuin Desktop. Runbooks that actually run. Now open beta, and fully&nbsp;<a href="https://github.com/atuinsh/desktop?ref=blog.atuin.sh" rel="noopener noreferrer nofollow">open source</a>.</p><h2 id="what-is-atuin-desktop">What is Atuin Desktop?</h2><p>Atuin Desktop looks like a doc, but runs like your terminal.&nbsp;Built to make local developer workflows repeatable, shareable, and reliable.</p><figure><div><p><img src="https://blog.atuin.sh/content/images/2025/09/CleanShot-2025-09-30-at-11.59.58@2x.png" width="2000" height="1584" loading="lazy" alt="" srcset="https://blog.atuin.sh/content/images/size/w600/2025/09/CleanShot-2025-09-30-at-11.59.58@2x.png 600w, https://blog.atuin.sh/content/images/size/w1000/2025/09/CleanShot-2025-09-30-at-11.59.58@2x.png 1000w, https://blog.atuin.sh/content/images/size/w1600/2025/09/CleanShot-2025-09-30-at-11.59.58@2x.png 1600w, https://blog.atuin.sh/content/images/size/w2400/2025/09/CleanShot-2025-09-30-at-11.59.58@2x.png 2400w" sizes="(min-width: 720px) 720px"></p><p><img src="https://blog.atuin.sh/content/images/2025/09/CleanShot-2025-09-30-at-12.56.17@2x.png" width="2000" height="1654" loading="lazy" alt="" srcset="https://blog.atuin.sh/content/images/size/w600/2025/09/CleanShot-2025-09-30-at-12.56.17@2x.png 600w, https://blog.atuin.sh/content/images/size/w1000/2025/09/CleanShot-2025-09-30-at-12.56.17@2x.png 1000w, https://blog.atuin.sh/content/images/size/w1600/2025/09/CleanShot-2025-09-30-at-12.56.17@2x.png 1600w, https://blog.atuin.sh/content/images/size/w2400/2025/09/CleanShot-2025-09-30-at-12.56.17@2x.png 2400w" sizes="(min-width: 720px) 720px"></p></div></figure><p>Runbooks should run. Workflows shouldn't live in someone's head. Docs shouldn't rot the moment you write them. Scripts, database queries, HTTP requests and Prometheus charts - all in one place.</p><ul><li><strong>Kill context switching:</strong> Chain shell <a href="https://man.atuin.sh/blocks/executable/script/?ref=blog.atuin.sh" rel="noreferrer">scripts</a>, <a href="https://man.atuin.sh/blocks/databases/?ref=blog.atuin.sh" rel="noreferrer">database</a> queries, and <a href="https://man.atuin.sh/blocks/network/http/?ref=blog.atuin.sh" rel="noreferrer">HTTP</a> requests</li><li><strong>Docs that don't rot: </strong>execute directly + stay relevant</li><li><strong>Reusable automation: </strong>dynamic runbooks with <a href="https://man.atuin.sh/templating/?ref=blog.atuin.sh" rel="noreferrer">Jinja-style templating</a></li><li><strong>Local knowledge</strong>: Build runbooks from your real shell history</li><li><strong>Collaborative</strong>: Sync and share via <a href="https://man.atuin.sh/workspaces/?ref=blog.atuin.sh" rel="noreferrer">Git</a>, or in <a href="https://man.atuin.sh/hub/getting-started/?ref=blog.atuin.sh" rel="noreferrer">real-time</a> via our Hub</li></ul><div data-layout="immersive">
                    
                        <div>
                            <p dir="ltr"><span>Back in April we&nbsp;</span><a href="https://blog.atuin.sh/atuin-desktop-runbooks-that-run/" target="_blank" rel="noopener noreferrer nofollow"><span>launched the closed beta</span></a><span>.&nbsp;</span></p><p dir="ltr"><span>Thousands of you signed up, used it at your day jobs, and told us exactly what broke. We’ve listened, rebuilt, and now it’s ready for everyone.</span></p>
                        </div>
                    
                    
                        <p><a href="https://github.com/atuinsh/desktop/releases/latest?ref=blog.atuin.sh">
                            Download
                        </a>
                        
                    </p></div><h2 id="what%E2%80%99s-new-since-april">What’s new since April?</h2><p>Our early users gave us a lot of feedback, which we've used to build something much better. </p><ul><li>Offline, file based, Git/VCS-compatible&nbsp;<a href="https://man.atuin.sh/workspaces/?ref=blog.atuin.sh" rel="noopener noreferrer nofollow">workspaces</a></li><li><a href="https://man.atuin.sh/hub/collaborative-editing/?ref=blog.atuin.sh#team-based-collaboration" rel="noopener noreferrer nofollow">Team</a>&nbsp;accounts with shared, realtime workspaces</li><li><a href="https://man.atuin.sh/blocks/executable/kubernetes?ref=blog.atuin.sh" rel="noopener noreferrer nofollow">Kubernetes</a>&nbsp;integration for live state and monitoring</li><li><a href="https://man.atuin.sh/blocks/databases/mysql?ref=blog.atuin.sh" rel="noopener noreferrer nofollow">MySQL</a>&nbsp;query blocks</li><li><a href="https://man.atuin.sh/blocks/executable/dropdown?ref=blog.atuin.sh" rel="noopener noreferrer nofollow">Dropdown</a>&nbsp;and more contextual blocks</li><li>A huge number of bug fixes, performance improvements, and UI upgrades</li></ul><h2 id="how-it%E2%80%99s-being-used">How it’s being used</h2><p>Atuin Desktop is already being used across engineering teams for serious, day-to-day work.</p><ul><li><strong>Automation and debugging:</strong> linking commands, monitoring systems, and tracking results</li><li><strong>Database operations:</strong> managing migrations, access control, and production queries</li><li><strong>Onboarding:</strong> getting started workflows new engineers can actually run</li><li><strong>Deploying and managing clusters:</strong> repeatable, documented automation for real environments</li><li><strong>Incident response:</strong> runbooks that execute instead of rotting in some internal wiki</li></ul><p>It’s become a shared system of record for the commands and processes that keep production alive.</p><h2 id="what%E2%80%99s-next">What’s next</h2><p>We’re just getting started! We've got a lot in the pipeline, including:</p><ul><li>Block dependencies and advanced execution flow</li><li>Run runbooks remotely and on CI</li><li>Audit logs and enhanced permissions</li><li>Comments and deeper collaboration</li><li>More block types<ul><li>Specify local networks, containers, and more</li><li>Tighter integration with authentication and cloud providers</li></ul></li><li>More polish, more speed, fewer bugs</li></ul><div data-layout="immersive">
                    
                        <p dir="ltr"><span>Stop copy-pasting from outdated wiki pages, and get started with Atuin Desktop</span></p>
                    
                    
                        <p><a href="https://github.com/atuinsh/desktop/releases/latest?ref=blog.atuin.sh">
                            Download
                        </a>
                        
                    </p></div><h2 id="getting-involved">Getting involved</h2><p>Atuin Desktop is now in open beta and open source under the Apache 2.0 license. Star it, fork it, break it: <a href="https://github.com/atuinsh/desktop?ref=blog.atuin.sh" rel="noopener">github.com/atuinsh/desktop</a></p><p>Infrastructure deserves better than rotting docs and tribal knowledge. Atuin Desktop is our attempt to fix that for everyone who’s ever said “I swear I’ve done this before.”</p><p><strong>Discord: </strong><a href="https://discord.gg/Fq8bJSKPHh?ref=blog.atuin.sh">discord.gg/Fq8bJSKPHh</a></p><p><strong>Forum: </strong><a href="https://forum.atuin.sh/?ref=blog.atuin.sh" rel="noreferrer">forum.atuin.sh</a></p>
        </section>

    </article>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Diff Algorithms (166 pts)]]></title>
            <link>https://flo.znkr.io/diff/</link>
            <guid>45430604</guid>
            <pubDate>Tue, 30 Sep 2025 20:09:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://flo.znkr.io/diff/">https://flo.znkr.io/diff/</a>, See on <a href="https://news.ycombinator.com/item?id=45430604">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>For software engineers, diffs are a ubiquitous method for representing changes: We use diffs to
compare different versions of the same file (e.g., during code review or when trying to understand
the history of a file), to visualize the difference of a failing test compared with its
expectation, or to apply changes to source files automatically.</p>
<p>Every project I worked on professionally or privately eventually needed a diff to visualize a change
or to apply a patch. However, I have never been satisfied with any of the freely available diff
libraries. This was never really a problem professionally, but for private projects, I have copied
and modified my own library from project to project until I mentioned this to a colleague who set me
on the path to publish my Go library (a port of a previous C++ library I used to copy and modify).
<em>Boy, did I underestimate how close my library was to publishability!</em></p>
<p>Anyway, I did it and I learned a whole lot about diff algorithms. You can find my library at
<a href="https://znkr.io/diff">znkr.io/diff</a> and what I learned in this article. I am not finished learning
yet, so I plan to update this article as my understanding continues to evolve.</p>
<h2 id="existing-diff-libraries">Existing Diff Libraries<a href="#existing-diff-libraries"></a></h2>
<p>Let me start by explaining why I am dissatisfied with existing diff libraries. There are a number of
attributes that are important to me. Not all of these attributes are important for every use case,
but a diff library that I can use for all of my use cases needs to fulfill all of them.</p>
<p>Usually, the input to a diff algorithm is text, and most diff libraries only support that. However,
I occasionally have use cases where I need to compare things that are not text. So any diff library
that only supports text doesn't meet my needs; instead, I need support for <strong>arbitrary sequences</strong>.</p>
<p>The resulting diff output is intended to be readable by humans. Quite often, especially for text, a
good way to present a diff is in the <strong>unified format</strong>. However, it's not always the best
presentation. A diff library should make it easy to output a diff in unified format, but it should
also provide a way to customize the presentation by providing a <strong>structured result</strong>.</p>
<p>Besides the presentation, the content of a diff should make it easy for humans to understand the
diff. This is a somewhat subjective criterion, but there are a number of failure cases that are
easily avoided, and there's some research into <strong>diff readability</strong> to set a benchmark. On the other
hand, diffs should be <strong>minimal</strong> in that they should be as small as possible.</p>
<p>Last but not least, it's important that a diff library has a <strong>simple API</strong> and provides good
<strong>performance</strong> in both runtime and memory usage, even in worst-case
scenarios<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>.</p>
<p>With that, we can evaluate existing diff libraries. For Go, I went through a number of libraries
and summarized them.</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Input</th>
<th>Output</th>
<th>API</th>
<th>Performance<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup></th>
<th>Diff<br>Readability</th>
<th>Diff<br>Minimality<sup id="fnref1:2"><a href="#fn:2" role="doc-noteref">2</a></sup></th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/sergi/go-diff">diffmatchpatch</a></td>
<td>❌<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup></td>
<td>❌<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup></td>
<td>🤔<sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup></td>
<td>➖➖</td>
<td>➖</td>
<td>➖</td>
</tr>
<tr>
<td><a href="https://github.com/rogpeppe/go-internal/tree/master/diff">go-internal</a></td>
<td>❌<sup id="fnref1:3"><a href="#fn:3" role="doc-noteref">3</a></sup></td>
<td>❌<sup id="fnref:6"><a href="#fn:6" role="doc-noteref">6</a></sup></td>
<td>😁</td>
<td>➕➕</td>
<td>➕➕</td>
<td>➕</td>
</tr>
<tr>
<td><a href="https://github.com/kylelemons/godebug/tree/master/diff">godebug</a></td>
<td>❌<sup id="fnref2:3"><a href="#fn:3" role="doc-noteref">3</a></sup></td>
<td>✅</td>
<td>😁</td>
<td>➖➖➖ /🧨<sup id="fnref:7"><a href="#fn:7" role="doc-noteref">7</a></sup></td>
<td>➕</td>
<td>➕➕</td>
</tr>
<tr>
<td><a href="https://github.com/mb0/diff">mb0</a></td>
<td>✅</td>
<td>❌<sup id="fnref1:4"><a href="#fn:4" role="doc-noteref">4</a></sup></td>
<td>😐<sup id="fnref:8"><a href="#fn:8" role="doc-noteref">8</a></sup></td>
<td>➖➖</td>
<td>➕</td>
<td>➕➕</td>
</tr>
<tr>
<td><a href="https://github.com/aymanbagabas/go-udiff">udiff</a></td>
<td>❌<sup id="fnref3:3"><a href="#fn:3" role="doc-noteref">3</a></sup></td>
<td>✅</td>
<td>😁</td>
<td>➕<sup id="fnref:9"><a href="#fn:9" role="doc-noteref">9</a></sup></td>
<td>➖</td>
<td>➖➖<sup id="fnref1:9"><a href="#fn:9" role="doc-noteref">9</a></sup></td>
</tr>
</tbody>
</table>
<div><p>Beware</p><p>The way I assigned ➕ and ➖ in this table doesn't follow any scientific methodology
it's merely based on running a few benchmarks and comparing a few results by hand. If you're looking
for a diff library to fulfill your needs, I would like to encourage you to do your own comparisons.
You can find the code I used for these comparisons in <a href="https://github.com/znkr/diff/tree/main/internal/benchmarks">on
github</a>.</p>
</div>
<h2 id="challenges">Challenges<a href="#challenges"></a></h2>
<p>The results suggest that it's far from trivial to implement a good diff library, and the one I had
started out with wasn't much better. To understand why the existing libraries are as they are,
we need to take a peek into the implementation.</p>
<h3 id="complexity">Complexity<a href="#complexity"></a></h3>
<p>With the exception of go-internal, all libraries use <a href="http://www.xmailserver.org/diff2.pdf">Myers'
Algorithm</a> to compute the diff. This is a standard algorithm
that returns a minimal diff and has been in use for this purpose for decades. The algorithm has a
runtime complexity of 
<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
  <semantics>
    <mrow>
      <mi>𝒪︀</mi>
      <mo form="prefix" stretchy="false">(</mo>
      <mi>N</mi>
      <mi>D</mi>
      <mo form="postfix" stretchy="false">)</mo>
    </mrow>
    <annotation encoding="application/x-tex">\mathcal{O}(ND)</annotation>
  </semantics>
</math>
 where 
<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
  <semantics>
    <mrow>
      <mi>N</mi>
    </mrow>
    <annotation encoding="application/x-tex">N</annotation>
  </semantics>
</math>
 is the number of input elements and 
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
  <semantics>
    <mrow>
      <mi>D</mi>
    </mrow>
    <annotation encoding="application/x-tex">D</annotation>
  </semantics>
</math>
 is the
edit distance between the two inputs. This means that the algorithm is very fast for inputs that are
similar, which is quite common. However, it's essentially quadratic in the worst case. That is, for
inputs that are very different, the complexity approaches 
<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
  <semantics>
    <mrow>
      <mi>𝒪︀</mi>
      <mo form="prefix" stretchy="false">(</mo>
      <msup>
        <mi>N</mi>
        <mn>2</mn>
      </msup>
      <mo form="postfix" stretchy="false">)</mo>
    </mrow>
    <annotation encoding="application/x-tex">\mathcal{O}(N^2)</annotation>
  </semantics>
</math>
. Furthermore, the
algorithm comes in two variants with a space complexity of either 
<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
  <semantics>
    <mrow>
      <mi>𝒪︀</mi>
      <mo form="prefix" stretchy="false">(</mo>
      <msup>
        <mi>N</mi>
        <mn>2</mn>
      </msup>
      <mo form="postfix" stretchy="false">)</mo>
    </mrow>
    <annotation encoding="application/x-tex">\mathcal{O}(N^2)</annotation>
  </semantics>
</math>
 or

<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
  <semantics>
    <mrow>
      <mi>𝒪︀</mi>
      <mo form="prefix" stretchy="false">(</mo>
      <mi>N</mi>
      <mo form="postfix" stretchy="false">)</mo>
    </mrow>
    <annotation encoding="application/x-tex">\mathcal{O}(N)</annotation>
  </semantics>
</math>
. Only godebug uses the variant with quadratic memory growth.</p>
<p>This means that <strong>it's relatively easy to write a well-performing diffing algorithm for small or
similar inputs, but it takes a very long time to complete for larger, less similar inputs</strong>. A
consequence of this is that we can't trust simple benchmarks; instead, we need to test the
worst-case scenario<sup id="fnref1:1"><a href="#fn:1" role="doc-noteref">1</a></sup>.</p>
<p>As always in cases like this, we can improve the performance by approximating an optimal solution.
There are a number of heuristics that reduce the time complexity by trading off diff minimality. For
example, diffmatchpatch uses a deadline to stop the search for an optimal diff, and udiff uses a
an extremely aggressive heuristic.</p>
<p>Instead of improving Myers' runtime with heuristics, it's also often possible to find a diff using
only heuristics. go-internal uses <a href="https://bramcohen.livejournal.com/73318.html">patience diff</a>. The
heuristic is good enough that it alone almost always results in a good diff with a runtime
complexity of 
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
  <semantics>
    <mrow>
      <mi>𝒪︀</mi>
      <mo stretchy="false" form="prefix">(</mo>
      <mi>N</mi>
      <mspace width="0.17em"></mspace>
      <mi lspace="0.11111em">log</mi>
      <mspace width="0.17em"></mspace>
      <mi>N</mi>
      <mo form="postfix" stretchy="false">)</mo>
    </mrow>
    <annotation encoding="application/x-tex">\mathcal{O}(N \, \log \, N)</annotation>
  </semantics>
</math>
<sup id="fnref:10"><a href="#fn:10" role="doc-noteref">10</a></sup>. An additional advantage of
this algorithm is that it produces more readable diffs. However, patience diff can fail with very
large diffs, and it can only be implemented efficiently using a hash table, which restricts the
possible applications.</p>
<div><p>Histogram Diff</p><p>Besides patience diff, there's another interesting heuristic called histogram
diff. I still have to implement it and understand it better before writing about it here, though.</p>
</div>
<h3 id="readability">Readability<a href="#readability"></a></h3>
<p>Diff algorithms usually find a minimal diff or an approximation of one. However, except for trivial
cases, there are always multiple minimal diffs. For example, this simple diff</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/example_01.diff">example_01.diff</a>
</caption>
<tbody><tr data-op="match" data-x-lineno="1" data-y-lineno="1">
                <td>1</td>
                <td>1</td>
                <td> </td>
                <td><code>a
</code></td>
            </tr><tr data-op="insert" data-y-lineno="2">
                <td></td>
                <td>2</td>
                <td>+</td>
                <td><code>b
</code></td>
            </tr><tr data-op="delete" data-x-lineno="2">
                <td>2</td>
                <td></td>
                <td>-</td>
                <td><code>c
</code></td>
            </tr><tr data-op="match" data-x-lineno="3" data-y-lineno="3">
                <td>3</td>
                <td>3</td>
                <td> </td>
                <td><code>d
</code></td>
            </tr></tbody>
</table>
<p>is as minimal as</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/example_02.diff">example_02.diff</a>
</caption>
<tbody><tr data-op="match" data-x-lineno="1" data-y-lineno="1">
                <td>1</td>
                <td>1</td>
                <td> </td>
                <td><code>a
</code></td>
            </tr><tr data-op="delete" data-x-lineno="2">
                <td>2</td>
                <td></td>
                <td>-</td>
                <td><code>c
</code></td>
            </tr><tr data-op="insert" data-y-lineno="2">
                <td></td>
                <td>2</td>
                <td>+</td>
                <td><code>b
</code></td>
            </tr><tr data-op="match" data-x-lineno="3" data-y-lineno="3">
                <td>3</td>
                <td>3</td>
                <td> </td>
                <td><code>d
</code></td>
            </tr></tbody>
</table>
<p>Not all of the minimal or near-minimal diffs have the same readability for humans. For
example<sup id="fnref:11"><a href="#fn:11" role="doc-noteref">11</a></sup>,</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/example_03.diff">example_03.diff</a>
</caption>
<tbody><tr data-op="insert" data-y-lineno="1">
                <td></td>
                <td>1</td>
                <td>+</td>
                <td><code>int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
</code></td>
            </tr><tr data-op="insert" data-y-lineno="2">
                <td></td>
                <td>2</td>
                <td>+</td>
                <td><code>{
</code></td>
            </tr><tr data-op="insert" data-y-lineno="3">
                <td></td>
                <td>3</td>
                <td>+</td>
                <td><code>    if (chunk == NULL) return 0;
</code></td>
            </tr><tr data-op="insert" data-y-lineno="4">
                <td></td>
                <td>4</td>
                <td>+</td>
                <td><code>
</code></td>
            </tr><tr data-op="insert" data-y-lineno="5">
                <td></td>
                <td>5</td>
                <td>+</td>
                <td><code>    return start &lt;= chunk-&gt;length &amp;&amp; n &lt;= chunk-&gt;length - start;
</code></td>
            </tr><tr data-op="insert" data-y-lineno="6">
                <td></td>
                <td>6</td>
                <td>+</td>
                <td><code>}
</code></td>
            </tr><tr data-op="insert" data-y-lineno="7">
                <td></td>
                <td>7</td>
                <td>+</td>
                <td><code>
</code></td>
            </tr><tr data-op="match" data-x-lineno="1" data-y-lineno="8">
                <td>1</td>
                <td>8</td>
                <td> </td>
                <td><code>void Chunk_copy(Chunk *src, size_t src_start, Chunk *dst, size_t dst_start, size_t n)
</code></td>
            </tr><tr data-op="match" data-x-lineno="2" data-y-lineno="9">
                <td>2</td>
                <td>9</td>
                <td> </td>
                <td><code>{
</code></td>
            </tr><tr data-op="match" data-x-lineno="3" data-y-lineno="10">
                <td>3</td>
                <td>10</td>
                <td> </td>
                <td><code>    if (!Chunk_bounds_check(src, src_start, n)) return;
</code></td>
            </tr><tr data-op="match" data-x-lineno="4" data-y-lineno="11">
                <td>4</td>
                <td>11</td>
                <td> </td>
                <td><code>    if (!Chunk_bounds_check(dst, dst_start, n)) return;
</code></td>
            </tr><tr data-op="match" data-x-lineno="5" data-y-lineno="12">
                <td>5</td>
                <td>12</td>
                <td> </td>
                <td><code>
</code></td>
            </tr><tr data-op="match" data-x-lineno="6" data-y-lineno="13">
                <td>6</td>
                <td>13</td>
                <td> </td>
                <td><code>    memcpy(dst-&gt;data + dst_start, src-&gt;data + src_start, n);
</code></td>
            </tr><tr data-op="match" data-x-lineno="7" data-y-lineno="14">
                <td>7</td>
                <td>14</td>
                <td> </td>
                <td><code>}
</code></td>
            </tr><tr data-op="delete" data-x-lineno="8">
                <td>8</td>
                <td></td>
                <td>-</td>
                <td><code>
</code></td>
            </tr><tr data-op="delete" data-x-lineno="9">
                <td>9</td>
                <td></td>
                <td>-</td>
                <td><code>int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
</code></td>
            </tr><tr data-op="delete" data-x-lineno="10">
                <td>10</td>
                <td></td>
                <td>-</td>
                <td><code>{
</code></td>
            </tr><tr data-op="delete" data-x-lineno="11">
                <td>11</td>
                <td></td>
                <td>-</td>
                <td><code>    if (chunk == NULL) return 0;
</code></td>
            </tr><tr data-op="delete" data-x-lineno="12">
                <td>12</td>
                <td></td>
                <td>-</td>
                <td><code>
</code></td>
            </tr><tr data-op="delete" data-x-lineno="13">
                <td>13</td>
                <td></td>
                <td>-</td>
                <td><code>    return start &lt;= chunk-&gt;length &amp;&amp; n &lt;= chunk-&gt;length - start;
</code></td>
            </tr><tr data-op="delete" data-x-lineno="14">
                <td>14</td>
                <td></td>
                <td>-</td>
                <td><code>}
</code></td>
            </tr></tbody>
</table>
<p>is much more readable than the equally minimal and correct</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/example_04.diff">example_04.diff</a>
</caption>
<tbody><tr data-op="delete" data-x-lineno="1">
                <td>1</td>
                <td></td>
                <td>-</td>
                <td><code>void Chunk_copy(Chunk *src, size_t src_start, Chunk *dst, size_t dst_start, size_t n)
</code></td>
            </tr><tr data-op="insert" data-y-lineno="1">
                <td></td>
                <td>1</td>
                <td>+</td>
                <td><code>int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
</code></td>
            </tr><tr data-op="match" data-x-lineno="2" data-y-lineno="2">
                <td>2</td>
                <td>2</td>
                <td> </td>
                <td><code>{
</code></td>
            </tr><tr data-op="delete" data-x-lineno="3">
                <td>3</td>
                <td></td>
                <td>-</td>
                <td><code>    if (!Chunk_bounds_check(src, src_start, n)) return;
</code></td>
            </tr><tr data-op="delete" data-x-lineno="4">
                <td>4</td>
                <td></td>
                <td>-</td>
                <td><code>    if (!Chunk_bounds_check(dst, dst_start, n)) return;
</code></td>
            </tr><tr data-op="insert" data-y-lineno="3">
                <td></td>
                <td>3</td>
                <td>+</td>
                <td><code>    if (chunk == NULL) return 0;
</code></td>
            </tr><tr data-op="match" data-x-lineno="5" data-y-lineno="4">
                <td>5</td>
                <td>4</td>
                <td> </td>
                <td><code>
</code></td>
            </tr><tr data-op="delete" data-x-lineno="6">
                <td>6</td>
                <td></td>
                <td>-</td>
                <td><code>    memcpy(dst-&gt;data + dst_start, src-&gt;data + src_start, n);
</code></td>
            </tr><tr data-op="insert" data-y-lineno="5">
                <td></td>
                <td>5</td>
                <td>+</td>
                <td><code>    return start &lt;= chunk-&gt;length &amp;&amp; n &lt;= chunk-&gt;length - start;
</code></td>
            </tr><tr data-op="match" data-x-lineno="7" data-y-lineno="6">
                <td>7</td>
                <td>6</td>
                <td> </td>
                <td><code>}
</code></td>
            </tr><tr data-op="match" data-x-lineno="8" data-y-lineno="7">
                <td>8</td>
                <td>7</td>
                <td> </td>
                <td><code>
</code></td>
            </tr><tr data-op="delete" data-x-lineno="9">
                <td>9</td>
                <td></td>
                <td>-</td>
                <td><code>int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
</code></td>
            </tr><tr data-op="insert" data-y-lineno="8">
                <td></td>
                <td>8</td>
                <td>+</td>
                <td><code>void Chunk_copy(Chunk *src, size_t src_start, Chunk *dst, size_t dst_start, size_t n)
</code></td>
            </tr><tr data-op="match" data-x-lineno="10" data-y-lineno="9">
                <td>10</td>
                <td>9</td>
                <td> </td>
                <td><code>{
</code></td>
            </tr><tr data-op="delete" data-x-lineno="11">
                <td>11</td>
                <td></td>
                <td>-</td>
                <td><code>    if (chunk == NULL) return 0;
</code></td>
            </tr><tr data-op="insert" data-y-lineno="10">
                <td></td>
                <td>10</td>
                <td>+</td>
                <td><code>    if (!Chunk_bounds_check(src, src_start, n)) return;
</code></td>
            </tr><tr data-op="insert" data-y-lineno="11">
                <td></td>
                <td>11</td>
                <td>+</td>
                <td><code>    if (!Chunk_bounds_check(dst, dst_start, n)) return;
</code></td>
            </tr><tr data-op="match" data-x-lineno="12" data-y-lineno="12">
                <td>12</td>
                <td>12</td>
                <td> </td>
                <td><code>
</code></td>
            </tr><tr data-op="delete" data-x-lineno="13">
                <td>13</td>
                <td></td>
                <td>-</td>
                <td><code>    return start &lt;= chunk-&gt;length &amp;&amp; n &lt;= chunk-&gt;length - start;
</code></td>
            </tr><tr data-op="insert" data-y-lineno="13">
                <td></td>
                <td>13</td>
                <td>+</td>
                <td><code>    memcpy(dst-&gt;data + dst_start, src-&gt;data + src_start, n);
</code></td>
            </tr><tr data-op="match" data-x-lineno="14" data-y-lineno="14">
                <td>14</td>
                <td>14</td>
                <td> </td>
                <td><code>}
</code></td>
            </tr><tr data-op="match" data-x-lineno="15" data-y-lineno="15">
                <td>15</td>
                <td>15</td>
                <td> </td>
                <td><code></code></td>
            </tr></tbody>
</table>
<p>Furthermore, if we relax minimality to accept approximations, the number of possible results
increases significantly.</p>
<p>For good diff readability, we have to select one solution from the many possible ones that is
readable for humans. Many people believe that the diff readability is determined by the algorithm.
However, that's only partially correct, because <strong>different <em>implementations</em> of the same algorithm
can produce vastly different results</strong>.</p>
<p>There's also been a lot of progress in the past years to improve diff readability. Perhaps the best
work about diff readability is <a href="https://github.com/mhagger/diff-slider-tools">diff-slider-tools</a> by
<a href="https://github.com/mhagger">Michael Haggerty</a>. He implemented a heuristic that's applied in a
post-processing step to improve the readability.</p>
<p>In fact, <code>example_03.diff</code> above was generated using this heuristic. The diff without the heuristic,
as generated by my implementation of Myers' linear-space variant, looks like this:</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/example_03_no_indent_heuristic.diff">example_03_no_indent_heuristic.diff</a>
</caption>
<tbody><tr data-op="insert" data-y-lineno="1">
                <td></td>
                <td>1</td>
                <td>+</td>
                <td><code>int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
</code></td>
            </tr><tr data-op="insert" data-y-lineno="2">
                <td></td>
                <td>2</td>
                <td>+</td>
                <td><code>{
</code></td>
            </tr><tr data-op="insert" data-y-lineno="3">
                <td></td>
                <td>3</td>
                <td>+</td>
                <td><code>    if (chunk == NULL) return 0;
</code></td>
            </tr><tr data-op="insert" data-y-lineno="4">
                <td></td>
                <td>4</td>
                <td>+</td>
                <td><code>
</code></td>
            </tr><tr data-op="insert" data-y-lineno="5">
                <td></td>
                <td>5</td>
                <td>+</td>
                <td><code>    return start &lt;= chunk-&gt;length &amp;&amp; n &lt;= chunk-&gt;length - start;
</code></td>
            </tr><tr data-op="insert" data-y-lineno="6">
                <td></td>
                <td>6</td>
                <td>+</td>
                <td><code>}
</code></td>
            </tr><tr data-op="insert" data-y-lineno="7">
                <td></td>
                <td>7</td>
                <td>+</td>
                <td><code>
</code></td>
            </tr><tr data-op="match" data-x-lineno="1" data-y-lineno="8">
                <td>1</td>
                <td>8</td>
                <td> </td>
                <td><code>void Chunk_copy(Chunk *src, size_t src_start, Chunk *dst, size_t dst_start, size_t n)
</code></td>
            </tr><tr data-op="match" data-x-lineno="2" data-y-lineno="9">
                <td>2</td>
                <td>9</td>
                <td> </td>
                <td><code>{
</code></td>
            </tr><tr data-op="match" data-x-lineno="3" data-y-lineno="10">
                <td>3</td>
                <td>10</td>
                <td> </td>
                <td><code>    if (!Chunk_bounds_check(src, src_start, n)) return;
</code></td>
            </tr><tr data-op="match" data-x-lineno="4" data-y-lineno="11">
                <td>4</td>
                <td>11</td>
                <td> </td>
                <td><code>    if (!Chunk_bounds_check(dst, dst_start, n)) return;
</code></td>
            </tr><tr data-op="match" data-x-lineno="5" data-y-lineno="12">
                <td>5</td>
                <td>12</td>
                <td> </td>
                <td><code>
</code></td>
            </tr><tr data-op="match" data-x-lineno="6" data-y-lineno="13">
                <td>6</td>
                <td>13</td>
                <td> </td>
                <td><code>    memcpy(dst-&gt;data + dst_start, src-&gt;data + src_start, n);
</code></td>
            </tr><tr data-op="delete" data-x-lineno="7">
                <td>7</td>
                <td></td>
                <td>-</td>
                <td><code>}
</code></td>
            </tr><tr data-op="delete" data-x-lineno="8">
                <td>8</td>
                <td></td>
                <td>-</td>
                <td><code>
</code></td>
            </tr><tr data-op="delete" data-x-lineno="9">
                <td>9</td>
                <td></td>
                <td>-</td>
                <td><code>int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
</code></td>
            </tr><tr data-op="delete" data-x-lineno="10">
                <td>10</td>
                <td></td>
                <td>-</td>
                <td><code>{
</code></td>
            </tr><tr data-op="delete" data-x-lineno="11">
                <td>11</td>
                <td></td>
                <td>-</td>
                <td><code>    if (chunk == NULL) return 0;
</code></td>
            </tr><tr data-op="delete" data-x-lineno="12">
                <td>12</td>
                <td></td>
                <td>-</td>
                <td><code>
</code></td>
            </tr><tr data-op="delete" data-x-lineno="13">
                <td>13</td>
                <td></td>
                <td>-</td>
                <td><code>    return start &lt;= chunk-&gt;length &amp;&amp; n &lt;= chunk-&gt;length - start;
</code></td>
            </tr><tr data-op="match" data-x-lineno="14" data-y-lineno="14">
                <td>14</td>
                <td>14</td>
                <td> </td>
                <td><code>}
</code></td>
            </tr></tbody>
</table>
<p>Notice that the deletion starts at the end of the preceding function and leaves a small
remainder of the function being deleted? Michael's heuristic fixes this problem and results in the
very readable <code>example_03.diff</code>.</p>
<div><p>It's not the algorithm</p><p><code>example_04.diff</code> was found using a different implementation of Myers'
linear-space variant. That is, both <code>example_03.diff</code> and <code>example_04.diff</code> used the same algorithm!
The differences stem from the implementation of that algorithm and from post-processing.</p>
</div>
<h2 id="a-new-diffing-library-for-go">A New Diffing Library for Go<a href="#a-new-diffing-library-for-go"></a></h2>
<p>I created <a href="https://znkr.io/diff">znkr.io/diff</a> to address these challenges in a way that works for
all my use cases. Let's reiterate what I want from a diffing library:</p>
<ul>
<li>The input can be text and arbitrary slices</li>
<li>The output should be possible in unified format and as a structured result</li>
<li>The API should be simple</li>
<li>The diffs should be minimal or near-minimal</li>
<li>The runtime and memory performance should be excellent</li>
</ul>
<p>This is a lot more than what any of the existing libraries provide. When I copied and modified my
old diffing library, I could adapt it to the use cases at hand. But a general-purpose diffing
library needs to be general enough to cover the vast majority of use cases. At the same time, it
needs to be extensible to make sure new features can be implemented without cluttering the API over
time.</p>
<p>Unfortunately, excellent performance and minimal results are somewhat in opposition to one another
and I ended up providing three different modes of operation: Default (balanced between performance
and minimality), Fast (sacrifice minimal results for faster speed), Optimal (minimal result whatever
the cost).</p>
<table>
<thead>
<tr>
<th>Mode</th>
<th>Input</th>
<th>Output</th>
<th>API</th>
<th>Performance<sup id="fnref2:2"><a href="#fn:2" role="doc-noteref">2</a></sup></th>
<th>Diff<br>Readability</th>
<th>Diff<br>Minimality<sup id="fnref3:2"><a href="#fn:2" role="doc-noteref">2</a></sup></th>
</tr>
</thead>
<tbody>
<tr>
<td>Default</td>
<td>✅</td>
<td>✅</td>
<td>😁</td>
<td>➕➕</td>
<td>➕➕</td>
<td>➕➕</td>
</tr>
<tr>
<td>Fast</td>
<td>✅</td>
<td>✅</td>
<td>😁</td>
<td>➕➕➕</td>
<td>➕➕</td>
<td>➕</td>
</tr>
<tr>
<td>Optimal</td>
<td>✅</td>
<td>✅</td>
<td>😁</td>
<td>➕</td>
<td>➕➕</td>
<td>➕➕</td>
</tr>
</tbody>
</table>
<div><p>Text Only</p><p>This table only applies to text (same as the table above), non-text inputs can have
a different performance (if they are not <code>comparable</code> or readability).</p>
</div>
<h3 id="api">API<a href="#api"></a></h3>
<p>To design this API, I started with the data structures that I wanted to use as a user of the API and
worked backwards from there. At a very high level, there are two structured representations of a
diff that have been useful to me: a flat sequence of all deletions, insertions, and matching
elements (called <em>edits</em>) and a nested sequence of consecutive changes (called <em>hunks</em>).</p>
<ul>
<li>Edits are what I use to represent edits in this article; they contain the full content of both
inputs and how one is transformed into the other.</li>
<li>Hunks are a great representation for unit tests, because they are empty if both inputs are
identical and they make it possible to visualize just the changes even if the inputs are large.</li>
</ul>
<h4 id="arbitrary-slices">Arbitrary Slices<a href="#arbitrary-slices"></a></h4>
<p>I started with the design for the most general case, arbitrary slices. The Go representation for
diffing slices I liked the most is this one (see also
<a href="https://pkg.go.dev/znkr.io/diff">znkr.io/diff</a>):</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/diff.go">diff.go</a>
</caption>
<tbody>
    <tr>
        <td>5</td>
        <td><code><span>// Op describes an edit operation.</span>
</code></td>
    </tr>
    <tr>
        <td>6</td>
        <td><code><span>type</span> Op int
</code></td>
    </tr>
    <tr>
        <td>7</td>
        <td><code>
</code></td>
    </tr>
    <tr>
        <td>8</td>
        <td><code><span>const</span> (
</code></td>
    </tr>
    <tr>
        <td>9</td>
        <td><code>	Match  Op = <span>iota</span> <span>// Two slice elements match</span>
</code></td>
    </tr>
    <tr>
        <td>10</td>
        <td><code>	Delete           <span>// A deletion from an element on the left slice</span>
</code></td>
    </tr>
    <tr>
        <td>11</td>
        <td><code>	Insert           <span>// An insertion of an element from the right side</span>
</code></td>
    </tr>
    <tr>
        <td>12</td>
        <td><code>)
</code></td>
    </tr>
    <tr>
        <td>13</td>
        <td><code>
</code></td>
    </tr>
    <tr>
        <td>14</td>
        <td><code><span>// Edit describes a single edit of a diff.</span>
</code></td>
    </tr>
    <tr>
        <td>15</td>
        <td><code><span>// - For Match, both X and Y contain the matching element.</span>
</code></td>
    </tr>
    <tr>
        <td>16</td>
        <td><code><span>// - For Delete, X contains the deleted element and Y is unset (zero value).</span>
</code></td>
    </tr>
    <tr>
        <td>17</td>
        <td><code><span>// - For Insert, Y contains the inserted element and X is unset (zero value).</span>
</code></td>
    </tr>
    <tr>
        <td>18</td>
        <td><code><span>type</span> Edit[T any] <span>struct</span> {
</code></td>
    </tr>
    <tr>
        <td>19</td>
        <td><code>	Op   Op
</code></td>
    </tr>
    <tr>
        <td>20</td>
        <td><code>	X, Y T
</code></td>
    </tr>
    <tr>
        <td>21</td>
        <td><code>}
</code></td>
    </tr>
    <tr>
        <td>22</td>
        <td><code>
</code></td>
    </tr>
    <tr>
        <td>23</td>
        <td><code><span>// Hunk describes a sequence of consecutive edits.</span>
</code></td>
    </tr>
    <tr>
        <td>24</td>
        <td><code><span>type</span> Hunk[T any] <span>struct</span> {
</code></td>
    </tr>
    <tr>
        <td>25</td>
        <td><code>	PosX, EndX int       <span>// Start and end position in x.</span>
</code></td>
    </tr>
    <tr>
        <td>26</td>
        <td><code>	PosY, EndY int       <span>// Start and end position in y.</span>
</code></td>
    </tr>
    <tr>
        <td>27</td>
        <td><code>	Edits      []Edit[T] <span>// Edits to transform x[PosX:EndX] to y[PosY:EndY]</span>
</code></td>
    </tr>
    <tr>
        <td>28</td>
        <td><code>}
</code></td>
    </tr></tbody>
</table>
<p>The alternatives I have seen are variations and combinations of two themes. Either using slices to
represent edit operations in <code>Hunk</code></p>
<pre><code>type Hunk[T any] struct {
	Delete []T
	Insert []T
	Match  []T
}
</code></pre>
<p>Or using indices instead of elements</p>
<pre><code>type Edit struct {
	Op         Op
	PosX, PosY []int
}
</code></pre>
<p>All of these representations work, but I found that the representations above served my use cases
best. One little quirk is that <code>Edit</code> always contains both elements. This is often unnecessary, but
there are use cases where this is very important because the elements themselves might not be equal
(e.g., if they are pointers that are compared with a custom function).</p>
<p>Once the data structures were established, it was quite obvious that the simplest way to fill them
with diff data was to write two functions <a href="https://pkg.go.dev/znkr.io/diff#Edits"><code>diff.Edits</code></a> and
<a href="https://pkg.go.dev/znkr.io/diff#Hunks"><code>diff.Hunks</code></a> to return the diffs. I made them extensible by
using <a href="https://dave.cheney.net/2014/10/17/functional-options-for-friendly-apis">functional options</a>.</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/diff.go">diff.go</a>
</caption>
<tbody>
    <tr>
        <td>30</td>
        <td><code><span>// Edits compares the contents of x and y and returns the changes necessary to convert from one to</span>
</code></td>
    </tr>
    <tr>
        <td>31</td>
        <td><code><span>// the other.</span>
</code></td>
    </tr>
    <tr>
        <td>32</td>
        <td><code><span>//
</span></code></td>
    </tr>
    <tr>
        <td>33</td>
        <td><code><span>// Edits returns one edit for every element in the input slices. If x and y are identical, the</span>
</code></td>
    </tr>
    <tr>
        <td>34</td>
        <td><code><span>// output will consist of a match edit for every input element.</span>
</code></td>
    </tr>
    <tr>
        <td>35</td>
        <td><code><span>func</span> Edits[T comparable](x, y []T, opts ...Option) []Edit[T]
</code></td>
    </tr>
    <tr>
        <td>36</td>
        <td><code>
</code></td>
    </tr>
    <tr>
        <td>37</td>
        <td><code><span>// Hunks compares the contents of x and y and returns the changes necessary to convert from one to</span>
</code></td>
    </tr>
    <tr>
        <td>38</td>
        <td><code><span>// the other.</span>
</code></td>
    </tr>
    <tr>
        <td>39</td>
        <td><code><span>//
</span></code></td>
    </tr>
    <tr>
        <td>40</td>
        <td><code><span>// The output is a sequence of hunks. A hunk represents a contiguous block of changes (insertions</span>
</code></td>
    </tr>
    <tr>
        <td>41</td>
        <td><code><span>// and deletions) along with some surrounding context.</span>
</code></td>
    </tr>
    <tr>
        <td>42</td>
        <td><code><span>func</span> Hunks[T comparable](x, y []T, opts ...Option) []Hunk[T]
</code></td>
    </tr></tbody>
</table>
<p>The options allow for future extensibility and allow changing the behavior of these functions. For
example, the option <a href="https://pkg.go.dev/znkr.io/diff#Context"><code>diff.Context(5)</code></a> configures <code>Hunks</code>
to provide 5 elements of surrounding context.</p>
<p>However, the current API still doesn't allow <em>arbitrary slices</em>; it only allows slices of
<code>comparable</code> types. To fix this, I needed two other functions that provide a function to compare
two elements. The Go standard library uses the <code>Func</code> suffix for functions like this, so I followed
the lead:</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/diff.go">diff.go</a>
</caption>
<tbody>
    <tr>
        <td>44</td>
        <td><code><span>// EditsFunc compares the contents of x and y using the provided equality comparison and returns the</span>
</code></td>
    </tr>
    <tr>
        <td>45</td>
        <td><code><span>// changes necessary to convert from one to the other.</span>
</code></td>
    </tr>
    <tr>
        <td>46</td>
        <td><code><span>func</span> EditsFunc[T any](x, y []T, eq <span>func</span>(a, b T) bool, opts ...Option) []Edit[T]
</code></td>
    </tr>
    <tr>
        <td>47</td>
        <td><code>
</code></td>
    </tr>
    <tr>
        <td>48</td>
        <td><code><span>// HunksFunc compares the contents of x and y using the provided equality comparison and returns the</span>
</code></td>
    </tr>
    <tr>
        <td>49</td>
        <td><code><span>// changes necessary to convert from one to the other.</span>
</code></td>
    </tr>
    <tr>
        <td>50</td>
        <td><code><span>func</span> HunksFunc[T any](x, y []T, eq <span>func</span>(a, b T) bool, opts ...Option) []Hunk[T]
</code></td>
    </tr></tbody>
</table>
<h4 id="text">Text<a href="#text"></a></h4>
<p>While this API works well to produce a structured result for arbitrary slices, it doesn't provide
output in unified format for text inputs. My first approach was to provide a helper function that
returns a diff in unified format: <code>diff.ToUnified(hunks []Hunk[string]) string</code>. However, this would
make getting a unified diff more complicated. Besides requiring two function calls, it would be
necessary to split the input into lines. This, in turn, can be done in different ways, e.g., by
stripping or keeping the line breaks, which opens the door to mistakes. It's much better to provide
a simple function for the entire use case.</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/textdiff.go">textdiff.go</a>
</caption>
<tbody>
    <tr>
        <td>7</td>
        <td><code><span>// Unified compares the lines in x and y and returns the changes necessary to convert from one to</span>
</code></td>
    </tr>
    <tr>
        <td>8</td>
        <td><code><span>// the other in unified format.</span>
</code></td>
    </tr>
    <tr>
        <td>9</td>
        <td><code><span>func</span> Unified[T string | []byte](x, y T, opts ...diff.Option) T
</code></td>
    </tr></tbody>
</table>
<p>I also moved this function to the <a href="https://pkg.go.dev/znkr.io/diff/textdiff"><code>textdiff</code></a> package to
highlight the difference in expected input.</p>
<p>Now, I also happen to have use cases where I need structured results for text diffs. It would be
very annoying if I had to split those into lines manually. Besides, I can make a few more
assumptions about text that allow for a slight simplification of the data structures:</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/textdiff.go">textdiff.go</a>
</caption>
<tbody>
    <tr>
        <td>11</td>
        <td><code><span>// Edit describes a single edit of a line-by-line diff.</span>
</code></td>
    </tr>
    <tr>
        <td>12</td>
        <td><code><span>type</span> Edit[T string | []byte] <span>struct</span> {
</code></td>
    </tr>
    <tr>
        <td>13</td>
        <td><code>	Op   diff.Op <span>// Edit operation</span>
</code></td>
    </tr>
    <tr>
        <td>14</td>
        <td><code>	Line T       <span>// Line, including newline character (if any)</span>
</code></td>
    </tr>
    <tr>
        <td>15</td>
        <td><code>}
</code></td>
    </tr>
    <tr>
        <td>16</td>
        <td><code>
</code></td>
    </tr>
    <tr>
        <td>17</td>
        <td><code><span>// Hunk describes a sequence of consecutive edits.</span>
</code></td>
    </tr>
    <tr>
        <td>18</td>
        <td><code><span>type</span> Hunk[T string | []byte] <span>struct</span> {
</code></td>
    </tr>
    <tr>
        <td>19</td>
        <td><code>	PosX, EndX int       <span>// Start and end line in x (zero-based).</span>
</code></td>
    </tr>
    <tr>
        <td>20</td>
        <td><code>	PosY, EndY int       <span>// Start and end line in y (zero-based).</span>
</code></td>
    </tr>
    <tr>
        <td>21</td>
        <td><code>	Edits      []Edit[T] <span>// Edits to transform x lines PosX..EndX to y lines PosY..EndY</span>
</code></td>
    </tr>
    <tr>
        <td>22</td>
        <td><code>}
</code></td>
    </tr>
    <tr>
        <td>23</td>
        <td><code>
</code></td>
    </tr>
    <tr>
        <td>24</td>
        <td><code><span>// Edits compares the lines in x and y and returns the changes necessary to convert from one to the</span>
</code></td>
    </tr>
    <tr>
        <td>25</td>
        <td><code><span>// other.</span>
</code></td>
    </tr>
    <tr>
        <td>26</td>
        <td><code><span>func</span> Edits[T string | []byte](x, y T, opts ...diff.Option) []Edit[T]
</code></td>
    </tr>
    <tr>
        <td>27</td>
        <td><code>
</code></td>
    </tr>
    <tr>
        <td>28</td>
        <td><code><span>// Hunks compares the lines in x and y and returns the changes necessary to convert from one to the</span>
</code></td>
    </tr>
    <tr>
        <td>29</td>
        <td><code><span>// other.</span>
</code></td>
    </tr>
    <tr>
        <td>30</td>
        <td><code><span>func</span> Hunks[T string | []byte](x, y T, opts ...diff.Option) []Hunk[T]
</code></td>
    </tr></tbody>
</table>
<h4 id="conclusion">Conclusion<a href="#conclusion"></a></h4>
<p>For the full API and examples for how to use it, please see the package documentation for
<a href="https://pkg.go.dev/znkr.io/diff">znkr.io/diff</a> and
<a href="https://pkg.go.dev/znkr.io/diff/textdiff">znkr.io/diff/textdiff</a>. I am certain that there are use
cases not covered by this API, but I feel confident that it can evolve to cover these use cases in
the future. For now, all my needs are fulfilled, but if you run into a situation that can't be
solved by this API or requires some contortions, please <a href="https://github.com/znkr/diff/issues/new">tell me about
it</a>.</p>
<h3 id="implementation">Implementation<a href="#implementation"></a></h3>
<p>To implement this API, we need to implement a diff algorithm. There are a couple of standard diff
algorithms that we can choose from. The choice of the algorithm as well as how it's implemented
matters for the readability of the result as well as the performance.</p>
<p>A good starting point for this project was Myers' algorithm, simply because it's the fastest
algorithm that can cover the whole API. In particular, the <code>...Func</code> variants for <code>any</code> types
instead of <code>comparable</code> can't make use of a hash map. Patience and Histogram require the use of a
hash map for an efficient implementation, so Myers' really is the only choice. Another advantage of
Myers' compared to Patience and Histogram is that it will return optimal results.</p>
<p>On the flip side, in the <a href="#existing-diff-libraries">comparison above</a>, it came out as relatively
slow compared to the patience diff algorithm and didn't produce the most readable results. It turns
out, however, that this can be mitigated and almost completely overcome for <code>comparable</code> types using
a combination of preprocessing, heuristics, and post-processing.</p>
<p>I am not going to cover the diff algorithm in detail here. There are a number of excellent articles
on the web that describe it<sup id="fnref:12"><a href="#fn:12" role="doc-noteref">12</a></sup>, but I recommend reading the
paper<sup id="fnref:13"><a href="#fn:13" role="doc-noteref">13</a></sup>: All articles I have seen try to keep a distance from the theory that
makes this algorithm work, but that's not really helpful if you want to understand how and why this
algorithm works.</p>
<h4 id="preprocessing">Preprocessing<a href="#preprocessing"></a></h4>
<p>The most impactful way to improve the performance of Myers' algorithm is to reduce the problem size.
The simplest thing to do is to strip any common prefix and suffix. This is always possible and helps
a little. However, it can also reduce diff readability, because it will consume matching elements
eagerly.</p>
<p>For example, let's say we have this change:</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/example_05.diff">example_05.diff</a>
</caption>
<tbody><tr data-op="match" data-x-lineno="1" data-y-lineno="1">
                <td>1</td>
                <td>1</td>
                <td> </td>
                <td><code>package array
</code></td>
            </tr><tr data-op="match" data-x-lineno="2" data-y-lineno="2">
                <td>2</td>
                <td>2</td>
                <td> </td>
                <td><code>
</code></td>
            </tr><tr data-op="match" data-x-lineno="3" data-y-lineno="3">
                <td>3</td>
                <td>3</td>
                <td> </td>
                <td><code>var m = []struct{
</code></td>
            </tr><tr data-op="match" data-x-lineno="4" data-y-lineno="4">
                <td>4</td>
                <td>4</td>
                <td> </td>
                <td><code>    name  string
</code></td>
            </tr><tr data-op="match" data-x-lineno="5" data-y-lineno="5">
                <td>5</td>
                <td>5</td>
                <td> </td>
                <td><code>    year  int
</code></td>
            </tr><tr data-op="match" data-x-lineno="6" data-y-lineno="6">
                <td>6</td>
                <td>6</td>
                <td> </td>
                <td><code>}{
</code></td>
            </tr><tr data-op="match" data-x-lineno="7" data-y-lineno="7">
                <td>7</td>
                <td>7</td>
                <td> </td>
                <td><code>    {
</code></td>
            </tr><tr data-op="match" data-x-lineno="8" data-y-lineno="8">
                <td>8</td>
                <td>8</td>
                <td> </td>
                <td><code>        name: "Freak Out!",
</code></td>
            </tr><tr data-op="match" data-x-lineno="9" data-y-lineno="9">
                <td>9</td>
                <td>9</td>
                <td> </td>
                <td><code>        year: 1966,
</code></td>
            </tr><tr data-op="match" data-x-lineno="10" data-y-lineno="10">
                <td>10</td>
                <td>10</td>
                <td> </td>
                <td><code>    },
</code></td>
            </tr><tr data-op="insert" data-y-lineno="11">
                <td></td>
                <td>11</td>
                <td>+</td>
                <td><code>    {
</code></td>
            </tr><tr data-op="insert" data-y-lineno="12">
                <td></td>
                <td>12</td>
                <td>+</td>
                <td><code>        name: "Absolutely Free",
</code></td>
            </tr><tr data-op="insert" data-y-lineno="13">
                <td></td>
                <td>13</td>
                <td>+</td>
                <td><code>        year: 1967,
</code></td>
            </tr><tr data-op="insert" data-y-lineno="14">
                <td></td>
                <td>14</td>
                <td>+</td>
                <td><code>    },
</code></td>
            </tr><tr data-op="match" data-x-lineno="11" data-y-lineno="15">
                <td>11</td>
                <td>15</td>
                <td> </td>
                <td><code>    {
</code></td>
            </tr><tr data-op="match" data-x-lineno="12" data-y-lineno="16">
                <td>12</td>
                <td>16</td>
                <td> </td>
                <td><code>        name: "We're Only in It for the Money",
</code></td>
            </tr><tr data-op="match" data-x-lineno="13" data-y-lineno="17">
                <td>13</td>
                <td>17</td>
                <td> </td>
                <td><code>        year: 1967,
</code></td>
            </tr><tr data-op="match" data-x-lineno="14" data-y-lineno="18">
                <td>14</td>
                <td>18</td>
                <td> </td>
                <td><code>    },
</code></td>
            </tr><tr data-op="match" data-x-lineno="15" data-y-lineno="19">
                <td>15</td>
                <td>19</td>
                <td> </td>
                <td><code>}</code></td>
            </tr></tbody>
</table>
<p>If we eagerly consume the common prefix first and then the common suffix, the first 11 lines are
all identical and the so are the last 4. This in turn would result in a different diff:</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/example_05_strip_common_prefix_and_suffix.diff">example_05_strip_common_prefix_and_suffix.diff</a>
</caption>
<tbody><tr data-op="match" data-x-lineno="1" data-y-lineno="1">
                <td>1</td>
                <td>1</td>
                <td> </td>
                <td><code>package array
</code></td>
            </tr><tr data-op="match" data-x-lineno="2" data-y-lineno="2">
                <td>2</td>
                <td>2</td>
                <td> </td>
                <td><code>
</code></td>
            </tr><tr data-op="match" data-x-lineno="3" data-y-lineno="3">
                <td>3</td>
                <td>3</td>
                <td> </td>
                <td><code>var m = []struct{
</code></td>
            </tr><tr data-op="match" data-x-lineno="4" data-y-lineno="4">
                <td>4</td>
                <td>4</td>
                <td> </td>
                <td><code>    name  string
</code></td>
            </tr><tr data-op="match" data-x-lineno="5" data-y-lineno="5">
                <td>5</td>
                <td>5</td>
                <td> </td>
                <td><code>    year  int
</code></td>
            </tr><tr data-op="match" data-x-lineno="6" data-y-lineno="6">
                <td>6</td>
                <td>6</td>
                <td> </td>
                <td><code>}{
</code></td>
            </tr><tr data-op="match" data-x-lineno="7" data-y-lineno="7">
                <td>7</td>
                <td>7</td>
                <td> </td>
                <td><code>    {
</code></td>
            </tr><tr data-op="match" data-x-lineno="8" data-y-lineno="8">
                <td>8</td>
                <td>8</td>
                <td> </td>
                <td><code>        name: "Freak Out!",
</code></td>
            </tr><tr data-op="match" data-x-lineno="9" data-y-lineno="9">
                <td>9</td>
                <td>9</td>
                <td> </td>
                <td><code>        year: 1966,
</code></td>
            </tr><tr data-op="match" data-x-lineno="10" data-y-lineno="10">
                <td>10</td>
                <td>10</td>
                <td> </td>
                <td><code>    },
</code></td>
            </tr><tr data-op="match" data-x-lineno="11" data-y-lineno="11">
                <td>11</td>
                <td>11</td>
                <td> </td>
                <td><code>    {
</code></td>
            </tr><tr data-op="insert" data-y-lineno="12">
                <td></td>
                <td>12</td>
                <td>+</td>
                <td><code>        name: "Absolutely Free",
</code></td>
            </tr><tr data-op="insert" data-y-lineno="13">
                <td></td>
                <td>13</td>
                <td>+</td>
                <td><code>        year: 1967,
</code></td>
            </tr><tr data-op="insert" data-y-lineno="14">
                <td></td>
                <td>14</td>
                <td>+</td>
                <td><code>    },
</code></td>
            </tr><tr data-op="insert" data-y-lineno="15">
                <td></td>
                <td>15</td>
                <td>+</td>
                <td><code>    {
</code></td>
            </tr><tr data-op="match" data-x-lineno="12" data-y-lineno="16">
                <td>12</td>
                <td>16</td>
                <td> </td>
                <td><code>        name: "We're Only in It for the Money",
</code></td>
            </tr><tr data-op="match" data-x-lineno="13" data-y-lineno="17">
                <td>13</td>
                <td>17</td>
                <td> </td>
                <td><code>        year: 1967,
</code></td>
            </tr><tr data-op="match" data-x-lineno="14" data-y-lineno="18">
                <td>14</td>
                <td>18</td>
                <td> </td>
                <td><code>    },
</code></td>
            </tr><tr data-op="match" data-x-lineno="15" data-y-lineno="19">
                <td>15</td>
                <td>19</td>
                <td> </td>
                <td><code>}</code></td>
            </tr></tbody>
</table>
<p>Fortunately, this is easy to fix in post processing.</p>
<p>Much more impactful, but only efficiently possible for <code>comparable</code> types, is to remove all elements
that are unique to either the left side or the right side, as those must always be deletions or
insertions. Non-<code>comparable</code> types can't be keys in a hash map in Go, which is necessary for
checking uniqueness. This preprocessing step <a href="https://github.com/znkr/diff/commit/37b4470eeb45867adcae1581907770041326e1b5">reduced the runtime by up to
99%</a> for a few
real-world worst-case diffs.</p>
<p>In contrast to the suffix and prefix removal, stripping unique elements doesn't have any readability
impact.</p>
<h4 id="heuristics">Heuristics<a href="#heuristics"></a></h4>
<p>Another very impactful way to improve the performance is <em>Anchoring</em>. It is based on <a href="https://bramcohen.livejournal.com/73318.html">patience
diff</a>. The word patience is a bit misleading, because
it's too easily associated with having to wait and it doesn't describe the heuristic very well
either. It works by finding elements that are occur exactly once on both the left and the right
side. When we matching up these unique pairs we create a segmentation of the input into smaller
parts that can be analyzed individually. Even better, we're very likely to find matching lines atop
and below such a pair of unique elements. This allows us to shrink the segments by stripping common
prefixes and suffixes. This heuristic <a href="https://github.com/znkr/diff/commit/feb7bda337f269935d80ee18e703e0940f406873">reduced the runtime by up to
95%</a>. Unfortunately,
finding unique elements and matching them up requires a hash map again which means that it can only
be used for <code>comparable</code> types.</p>
<p>There are two more heuristics that are I implemented. They help for non-<code>comparable</code> types and as a
backstop when the other heuristics don't work. Their main purpose is to avoid runaway quadratic
growth. The <em>Good Diagonal</em> heuristic stops searching for a better solution if we found a solution
that's good enough and the <em>Too Expensive</em> heuristic shortcuts the search if it becomes too
expensive which reduces the worst-case complexity from 
<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
  <semantics>
    <mrow>
      <mi>𝒪︀</mi>
      <mo form="prefix" stretchy="false">(</mo>
      <msup>
        <mi>N</mi>
        <mn>2</mn>
      </msup>
      <mo form="postfix" stretchy="false">)</mo>
    </mrow>
    <annotation encoding="application/x-tex">\mathcal{O}(N^2)</annotation>
  </semantics>
</math>
 to

<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
  <semantics>
    <mrow>
      <mi>𝒪︀</mi>
      <mo form="prefix" stretchy="false">(</mo>
      <msup>
        <mi>N</mi>
        <mn>1.5</mn>
      </msup>
      <mspace width="0.17em"></mspace>
      <mi lspace="0.11111em">log</mi>
      <mspace width="0.17em"></mspace>
      <mi>N</mi>
      <mo form="postfix" stretchy="false">)</mo>
    </mrow>
    <annotation encoding="application/x-tex">\mathcal{O}(N^1.5 \, \log \, N)</annotation>
  </semantics>
</math>
.</p>
<p>However, heuristics like this trade diff minimality for performance, this is not always desirable.
Sometimes, a minimal diff is exactly what's required.
<a href="https://pkg.go.dev/znkr.io/diff#Optimal"><code>diff.Optimal</code></a> disables these heuristics to always find a
minimal diff irrespective of the costs.</p>
<h4 id="post-processing">Post-processing<a href="#post-processing"></a></h4>
<p>We established before that a diff algorithm finds one of many possible solutions. Given such a
solution we can discover more solutions by it locally and then selecting the best solution according
to some metric. This is exactly how <a href="https://github.com/mhagger">Michael Haggerty's</a> indentation
heuristic works for text.</p>
<p>For any given diff, we can often slide the edits up or down in a way that doesn't change the meaning
of a diff. For example,</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/example_06.diff">example_06.diff</a>
</caption>
<tbody><tr data-op="match" data-x-lineno="1" data-y-lineno="1">
                <td>1</td>
                <td>1</td>
                <td> </td>
                <td><code>["foo", "bar", "baz"].map do |i|
</code></td>
            </tr><tr data-op="insert" data-y-lineno="2">
                <td></td>
                <td>2</td>
                <td>+</td>
                <td><code>  i
</code></td>
            </tr><tr data-op="insert" data-y-lineno="3">
                <td></td>
                <td>3</td>
                <td>+</td>
                <td><code>end
</code></td>
            </tr><tr data-op="insert" data-y-lineno="4">
                <td></td>
                <td>4</td>
                <td>+</td>
                <td><code>
</code></td>
            </tr><tr data-op="insert" data-y-lineno="5">
                <td></td>
                <td>5</td>
                <td>+</td>
                <td><code>["foo", "bar", "baz"].map do |i|
</code></td>
            </tr><tr data-op="match" data-x-lineno="2" data-y-lineno="6">
                <td>2</td>
                <td>6</td>
                <td> </td>
                <td><code>  i.upcase
</code></td>
            </tr><tr data-op="match" data-x-lineno="3" data-y-lineno="7">
                <td>3</td>
                <td>7</td>
                <td> </td>
                <td><code>end
</code></td>
            </tr></tbody>
</table>
<p>has the same meaning as</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/example_06_indent_heuristic.diff">example_06_indent_heuristic.diff</a>
</caption>
<tbody><tr data-op="insert" data-y-lineno="1">
                <td></td>
                <td>1</td>
                <td>+</td>
                <td><code>["foo", "bar", "baz"].map do |i|
</code></td>
            </tr><tr data-op="insert" data-y-lineno="2">
                <td></td>
                <td>2</td>
                <td>+</td>
                <td><code>  i
</code></td>
            </tr><tr data-op="insert" data-y-lineno="3">
                <td></td>
                <td>3</td>
                <td>+</td>
                <td><code>end
</code></td>
            </tr><tr data-op="insert" data-y-lineno="4">
                <td></td>
                <td>4</td>
                <td>+</td>
                <td><code>
</code></td>
            </tr><tr data-op="match" data-x-lineno="1" data-y-lineno="5">
                <td>1</td>
                <td>5</td>
                <td> </td>
                <td><code>["foo", "bar", "baz"].map do |i|
</code></td>
            </tr><tr data-op="match" data-x-lineno="2" data-y-lineno="6">
                <td>2</td>
                <td>6</td>
                <td> </td>
                <td><code>  i.upcase
</code></td>
            </tr><tr data-op="match" data-x-lineno="3" data-y-lineno="7">
                <td>3</td>
                <td>7</td>
                <td> </td>
                <td><code>end</code></td>
            </tr></tbody>
</table>
<p>We call edits that can be slid up or down <em>sliders</em>. The question is, how do we select the best
slide? Michael collected human ratings for different sliders of the same diff and used them to
develop a heuristic to match these ratings:
<a href="https://github.com/mhagger/diff-slider-tools">diff-slider-tools</a>.</p>
<p>However, this heuristic only works for text and is tuned towards code instead of prose. I decided to
make it optional. It can be enabled with the
<a href="https://pkg.go.dev/znkr.io/diff/textdiff#IndentHeuristic"><code>textdiff.IndentHeuristic</code></a> option.</p>
<h4 id="diff-representation">Diff Representation<a href="#diff-representation"></a></h4>
<p>The representation used during the execution of the diff algorithm has a surprising impact on the
algorithm performance and result readability. This is not at all obvious, and so it took me a while
to figure out that the best approach is akin to a side-by-side view of a diff: You use two <code>[]bool</code>
slices to represent the left side and the right side respectively: <code>true</code> in the left side slice
represents a deletion and on the right side an insertion. <code>false</code> is a matching element.</p>
<p>This representation has four big advantages: It can be preallocated, the order in which edits are
discovered doesn't matter, it's easy to mutate during post-processing, and it's easy to generate
other representations from it.</p>
<h2 id="open-questions">Open Questions<a href="#open-questions"></a></h2>
<ul>
<li>What exactly is the reason that two different algorithms produce different results? - I looked
into this question a little, but I haven't found a conclusive answer yet.</li>
</ul>
<h2 id="conclusion-1">Conclusion<a href="#conclusion-1"></a></h2>
<p>Diff algorithms are relatively complicated by themselves, but they pale in comparison  to what's
necessary to provide a high-quality diff library. This article tries to explain what went into
my new diff library, but there's still more that I haven't implemented yet.</p>


        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Inflammation now predicts heart disease more strongly than cholesterol (424 pts)]]></title>
            <link>https://www.empirical.health/blog/inflammation-and-heart-health/</link>
            <guid>45430498</guid>
            <pubDate>Tue, 30 Sep 2025 20:00:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.empirical.health/blog/inflammation-and-heart-health/">https://www.empirical.health/blog/inflammation-and-heart-health/</a>, See on <a href="https://news.ycombinator.com/item?id=45430498">Hacker News</a></p>
<div id="readability-page-1" class="page"><article> <div>   <p>Chronic inflammation has long been known to double your risk of heart disease, but prior to now,
inflammation has never been a SMuRF: <strong>s</strong>tandard <strong>m</strong>odifiable <strong>r</strong>isk <strong>f</strong>actor for heart disease.</p>
<p>The American College of Cardiology just released recommendations that change that. The ACC is now recommending that everyone
measure inflammation (specifically, hs-CRP) via a blood test:</p>
<blockquote>
<p>Because clinicians will not treat what they do not measure, universal screening of hsCRP in both primary and secondary prevention patients, in combination with cholesterol, represents a major clinical opportunity and is therefore recommended. <a href="https://www.jacc.org/doi/10.1016/j.jacc.2025.08.047">American College of Cardiology</a></p>
</blockquote>
<p>There were a many interesting bits of evidence that led to this recommendation. The whole <a href="https://www.jacc.org/doi/10.1016/j.jacc.2025.08.047">article, published in JACC</a>, is worth a read, but this blog post extracts a few of the most interesting parts — or at
least, the parts I thought were most interesting.</p>
<p>
Want to skip ahead and measure your inflammation? Empirical Health's <a href="https://www.empirical.health/product/comprehensive-health-panel?utm_source=blog">advanced heart health panel</a> includes hs-CRP, ApoB, Lp(a), and other critical heart health biomarkers.
</p>
<hr>
<h2 id="inflammation-hs-crp-is-a-stronger-predictor-of-heart-disease-than-cholesterol"><a href="#inflammation-hs-crp-is-a-stronger-predictor-of-heart-disease-than-cholesterol">Inflammation (hs-CRP) is a stronger predictor of heart disease than cholesterol</a></h2>
<p>For decades, LDL cholesterol (or <a href="https://www.empirical.health/blog/apob-blood-test/">ApoB</a>) has been the main focus of cardiovascular risk assessment. But
this chart shows hs-CRP is actually a <em>stronger</em> predictor of heart disease than LDL.</p>
<p><img alt="Inflammation vs LDL cholesterol" loading="lazy" decoding="async" fetchpriority="auto" width="407" height="620" src="https://www.empirical.health/_astro/ldl_vs_inflammation.B5MkNi52_Z2655L8.webp"></p>
<p>Why? In some ways, <strong>cholesterol has become a victim of its own success.</strong> We now screen the whole population
for high cholesterol, give statins to those with high LDL (or ApoB), and so then the majority of people who
end up having heart attacks have lower cholesterol than they would naturally have. This means most of
the majority of residual risk for heart attacks will be found in biomarkers that aren’t SMuRFs.</p>
<p>Inflammation (hs-CRP) is one such non-SMuRF, one perhaps one of the strongest. This is especially true
in people already on statins or those without traditional risk factors (sometimes called “SMuRF-less” patients).
In these groups, cholesterol may be well controlled, but inflammation remains a key driver of events.</p>
<p>Of course, other traditional risk factors matter <em>in addition</em> to inflammation: blood pressure, HbA1c or
insulin resistance, eGFR (kidney function), and so on.</p>
<h2 id="what-can-you-actually-do-to-lower-inflammation"><a href="#what-can-you-actually-do-to-lower-inflammation">What can you actually do to lower inflammation?</a></h2>
<p>The ACC consensus reviews a range of clinical trials testing both drugs and lifestyle interventions for lowering inflammation and reducing cardiovascular risk. Here’s a summary of the clinical trials and their results:</p>



































































































































<table><thead><tr><th>Trial Name</th><th>Drug (Class)</th><th>Sample Size (n)</th><th>Population/NYHA Functional Class</th><th>Follow-Up</th><th>Primary Endpoint</th><th>Treatment Outcome</th></tr></thead><tbody><tr><td>ATTACH</td><td>Infliximab (TNF inhibitor)</td><td>150</td><td>NYHA III/IV HF</td><td>7 mo</td><td>Clinical status (composite score)</td><td>No improvement or worsening; deaths highest in high-dose infliximab</td></tr><tr><td>ACCLAIM</td><td>IVIG</td><td>2314</td><td>NYHA II-IV HF</td><td>10.2 mo</td><td>Composite all-cause mortality and CV hospitalization</td><td>No reduction in events; trend toward benefit in NYHA III and IV</td></tr><tr><td>CANTOS</td><td>Canakinumab (anti–IL-1β)</td><td>10,061</td><td>Prior MI; hsCRP ≥2 mg/L</td><td>3.7 y (median)</td><td>Nonfatal MI, nonfatal stroke, or CV death (MACE); HF-related mortality</td><td>Reduced MACE and HF events; no effect on all-cause mortality; primary endpoint events: 3.86% vs 4.50%</td></tr><tr><td>CIRT</td><td>Methotrexate</td><td>4,786</td><td>Stable MI plus CAD</td><td>2.3 y (median)</td><td>CV event rates</td><td>No effect on CV events, inflammation, or lipids</td></tr><tr><td>CLEAR SYNERGY</td><td>Colchicine</td><td>3,056</td><td>Acute MI plus PCI</td><td>22.6 mo</td><td>Death from CV causes, recurrent MI, ischemic stroke</td><td>No significant difference in primary endpoint</td></tr><tr><td>COLCOT</td><td>Colchicine</td><td>4,745</td><td>Acute MI patients</td><td>22.6 mo</td><td>CV event rates</td><td>CV events lower than placebo</td></tr><tr><td>LoDoCo2</td><td>Colchicine</td><td>5,522</td><td>Stable CAD</td><td>28.6 mo</td><td>Composite of CV death, nonfatal MI, ischemic stroke, or ischemia-driven revasc.</td><td>CV events lower than placebo</td></tr><tr><td>GISSI-HF</td><td>Rosuvastatin (statin)</td><td>4,574</td><td>NYHA II-IV HF</td><td>3.9 y</td><td>All-cause mortality and CV hospitalization</td><td>No effect on primary endpoints</td></tr><tr><td>JUPITER</td><td>Rosuvastatin (statin)</td><td>17,802</td><td>No CVD / LDL &lt;130 mg/dL; hsCRP ≥2 mg/L</td><td>1.9 y (median)</td><td>MI, stroke, arterial revascularization, hospitalization for unstable angina, or CV death</td><td>Reduced events (HR 0.56–0.69)</td></tr><tr><td>CORONA</td><td>Rosuvastatin (statin)</td><td>5,011</td><td>NYHA II-IV HF; ischemic etiology</td><td>32.8 mo</td><td>CV death, nonfatal MI, nonfatal stroke</td><td>No effect on primary endpoint</td></tr><tr><td>OPT-CHF</td><td>Etanercept (TNF inhibitor)</td><td>1,500</td><td>NYHA II-IV HF</td><td>6 mo</td><td>Death, hospitalization, or worsening HF</td><td>No effect on primary endpoint</td></tr><tr><td>DCMP</td><td>Prednisone (corticosteroid)</td><td>84</td><td>NYHA II-IV HF; biopsy-proven myocarditis</td><td>5.7 and 12.3 mo</td><td>Improvement in LVEF, survival, or combined outcome of death or transplantation</td><td>No significant benefit</td></tr><tr><td>RENEWAL</td><td>Etanercept (TNF inhibitor)</td><td>2,048</td><td>NYHA II-IV HF</td><td>6 mo</td><td>Composite outcome of death or hospitalization</td><td>No effect on primary endpoint</td></tr></tbody></table>
<p><strong>What works</strong> to lower inflammation?</p>
<ul>
<li><strong>Statins</strong> (especially in people with high hs-CRP): Substantial reduction in events, even when LDL is normal (JUPITER trial).</li>
<li><strong>Colchicine</strong>: Reduces recurrent events in people with established heart disease (COLCOT, LoDoCo2).</li>
<li><strong>Canakinumab</strong>: Reduces events but is expensive and increases infection risk (CANTOS).</li>
<li><strong>Lifestyle</strong>: Anti-inflammatory diets (Mediterranean, DASH), regular exercise, smoking cessation, and maintaining a healthy weight all lower hs-CRP and reduce risk.</li>
</ul>
<p><strong>What doesn’t work?</strong></p>
<ul>
<li>Some anti-inflammatory drugs (methotrexate, TNF inhibitors, corticosteroids) have not shown benefit in major trials.</li>
</ul>
<hr>
<h2 id="whats-a-normal-good-or-bad-hs-crp"><a href="#whats-a-normal-good-or-bad-hs-crp">What’s a normal, good, or bad hs-CRP?</a></h2>
<p>If you’ve already measured your hs-CRP (great!), then it’s ideally below &lt;1 mg/L. hs-CRP above 3 mg/L is
high risk:</p>
<p><img alt="Inflammation vs LDL cholesterol" loading="lazy" decoding="async" fetchpriority="auto" width="638" height="456" src="https://www.empirical.health/_astro/normal-inflammation-levels.BflUT68q_2dks4G.webp"></p>
<p>(If you’re in moderate or high ranges, see the section above for what to do.)</p>
<h2 id="are-other-biomarkers-of-inflammation-relevant"><a href="#are-other-biomarkers-of-inflammation-relevant">Are other biomarkers of inflammation relevant?</a></h2>
<p>The ACC evaluated other markers: IL-6, fibrinogen, neutrophil-to-lymphocyte ratio, EPA/AA ratio, and serum amyloid A.
These have also been shown to predict cardiovascular risk, but once hs-CRP is known, don’t add more signal.</p>
<p>In other words, you’re best off simply measuring hs-CRP, and then spending money elsewhere on heart health.</p>
<h2 id="other-interesting-bits"><a href="#other-interesting-bits">Other interesting bits</a></h2>
<p>The JACC article is packed with other interesting insights. These ones were interesting:</p>
<ul>
<li><strong>Imaging biomarkers</strong> (like CT, PET, MRI, and perivascular “fat attenuation index”) can detect vascular inflammation and may help predict coronary events, but are not yet ready for routine clinical use.</li>
<li><strong>Bempedoic acid</strong> is a newer cholesterol-lowering drug that also lowers hs-CRP, but its long-term outcomes are still being studied.</li>
<li><strong>Residual inflammatory risk</strong>: Even with well-controlled LDL on statins, many people still have elevated hs-CRP and ongoing risk—so inflammation should be addressed separately from cholesterol.</li>
<li><strong>Universal hs-CRP screening</strong> is now recommended by the ACC for both people with and without established heart disease.</li>
<li><strong>Colchicine (0.5 mg/d)</strong> is now FDA-approved as an adjunct for secondary prevention in stable ASCVD, but should be avoided in people with significant kidney or liver disease.</li>
<li><strong>Novel IL-6 inhibitors</strong> are being studied as future anti-inflammatory therapies for heart disease.</li>
</ul>
<h2 id="how-to-measure-your-inflammation"><a href="#how-to-measure-your-inflammation">How to measure your inflammation</a></h2>
<p>A simple blood test for hs-CRP is widely available and inexpensive. The ACC now recommends routine hs-CRP testing for both people at risk (primary prevention) and those with established heart disease (secondary prevention).</p>
  </div> </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Boeing has started working on a 737 MAX replacement (210 pts)]]></title>
            <link>https://www.wsj.com/business/airlines/boeing-has-started-working-on-a-737-max-replacement-40a110df</link>
            <guid>45428482</guid>
            <pubDate>Tue, 30 Sep 2025 17:31:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/business/airlines/boeing-has-started-working-on-a-737-max-replacement-40a110df">https://www.wsj.com/business/airlines/boeing-has-started-working-on-a-737-max-replacement-40a110df</a>, See on <a href="https://news.ycombinator.com/item?id=45428482">Hacker News</a></p>
Couldn't get https://www.wsj.com/business/airlines/boeing-has-started-working-on-a-737-max-replacement-40a110df: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Sora 2 (270 pts)]]></title>
            <link>https://openai.com/index/sora-2/</link>
            <guid>45428122</guid>
            <pubDate>Tue, 30 Sep 2025 17:04:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/index/sora-2/">https://openai.com/index/sora-2/</a>, See on <a href="https://news.ycombinator.com/item?id=45428122">Hacker News</a></p>
Couldn't get https://openai.com/index/sora-2/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Sora 2 (613 pts)]]></title>
            <link>https://openai.com/index/sora-2/</link>
            <guid>45427982</guid>
            <pubDate>Tue, 30 Sep 2025 16:55:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/index/sora-2/">https://openai.com/index/sora-2/</a>, See on <a href="https://news.ycombinator.com/item?id=45427982">Hacker News</a></p>
Couldn't get https://openai.com/index/sora-2/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Sculptor, the Missing UI for Claude Code (145 pts)]]></title>
            <link>https://imbue.com/sculptor/</link>
            <guid>45427697</guid>
            <pubDate>Tue, 30 Sep 2025 16:35:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://imbue.com/sculptor/">https://imbue.com/sculptor/</a>, See on <a href="https://news.ycombinator.com/item?id=45427697">Hacker News</a></p>
<div id="readability-page-1" class="page"><div tabindex="-1" id="___gatsby"><header><a href="https://imbue.com/"><img src="data:image/svg+xml;base64,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" alt="Imbue logo"></a><nav><a aria-current="page" href="https://imbue.com/sculptor/"><p>Sculptor</p></a><a href="https://imbue.com/our-work/"><p>Our Work</p></a><a href="https://imbue.com/company/"><p>Company</p></a><a href="https://imbue.com/careers/"><p>Careers</p></a><a href="https://imbue.com/blog/"><p>Blog</p></a></nav></header><div><hr color="pride"><div><div><p><img src="https://imbue.com/static/parallel-agents-ac8d6f0192da6b575005401e71bbe66a.png" alt="Spin up parallel agents"></p><h3>Spin up parallel agents</h3><p>Each Claude works in its own container. You get safe execution and parallel agents without the hassle of git worktrees.</p></div><div><p><img src="https://imbue.com/static/instantly-pair-d0b4bc629d9146f1be7d0930e8200813.png" alt="Instantly test agent changes"></p><h3>Instantly test agent changes</h3><p>Switch seamlessly between agents with Pairing Mode.</p></div><div><p><img src="https://imbue.com/static/merge-f3c7f16c8450713cf46eed9700091655.png" alt="Merge without conflicts"></p><h3>Merge without conflicts</h3><p>Merge the changes you like and throw out the ones you don't. Sculptor helps you resolve merge conflicts.</p></div></div></div><div><div><div><hr color="envy"><div><p>I've been moving more and more of my coding off of Cursor and on to Sculptor btw.</p>
<p>The vibes are good, and the experience has been pretty nice.</p></div></div><div><hr color="peace"><p>Sculptor lets me maintain this level of craftiness to software development without losing the edge you get from AI tools.</p></div></div><div><div><hr color="pride"><p>Wow, this is slick!!</p><div><p>Dale Hamilton</p><p>Staff Engineer, Universe</p></div></div><div><hr color="excitement"><div><p>At first I thought, 'why do I need this container?'. But when I realized Sculptor was actually solving the pain of concurrent agents on different branches, it made total sense.</p>
<p>It's like—oh wow I don't have to manage that mess anymore.</p></div><div><p>Patrick Sulin</p><p>Technical Lead, InterSystems</p></div></div></div><div><div><hr color="joy"><p>The killer feature for me is parallelization. I can kick off multiple tasks at once without spinning up a whole new environment every time. It feels like the tooling is finally here to support the kind of workflows I've always wanted.</p><div><p>Robert Starmer</p><p>CEO, Kumulus Technologies</p></div></div><div><hr color="sadness"><p>I compared Claude Code running in Max Mode with Sculptor, and Sculptor's results and overall intelligence were better. I've already merged around 5K lines—it's a great product! Kudos to you guys.</p><div><p>Andrew Gabriel</p><p>Founder, Monostate.ai</p></div></div></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A $196 fine-tuned 7B model outperforms OpenAI o3 on document extraction (186 pts)]]></title>
            <link>https://arxiv.org/abs/2509.22906</link>
            <guid>45427634</guid>
            <pubDate>Tue, 30 Sep 2025 16:31:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2509.22906">https://arxiv.org/abs/2509.22906</a>, See on <a href="https://news.ycombinator.com/item?id=45427634">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2509.22906">View PDF</a>
    <a href="https://arxiv.org/html/2509.22906v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>This paper presents Extract-0, a 7-billion parameter language model specifically optimized for document information extraction that achieves performance exceeding models with parameter counts several orders of magnitude larger. Through a novel combination of synthetic data generation, supervised fine-tuning with Low-Rank Adaptation (LoRA), and reinforcement learning via Group Relative Policy Optimization (GRPO), Extract-0 achieves a mean reward of 0.573 on a benchmark of 1,000 diverse document extraction tasks, outperforming GPT-4.1 (0.457), o3 (0.464), and GPT-4.1-2025 (0.459). The training methodology employs a memory-preserving synthetic data generation pipeline that produces 280,128 training examples from diverse document sources, followed by parameterefficient fine-tuning that modifies only 0.53% of model weights (40.4M out of 7.66B parameters). The reinforcement learning phase introduces a novel semantic similarity-based reward function that handles the inherent ambiguity in information extraction tasks. This research demonstrates that task-specific optimization can yield models that surpass general-purpose systems while requiring substantially fewer computational resource.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Henrique Godoy [<a href="https://arxiv.org/show-email/90d16046/2509.22906" rel="nofollow">view email</a>]      <br>    <strong>[v1]</strong>
        Fri, 26 Sep 2025 20:34:43 UTC (1,102 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Launch HN: Airweave (YC X25) – Let agents search any app (133 pts)]]></title>
            <link>https://github.com/airweave-ai/airweave</link>
            <guid>45427482</guid>
            <pubDate>Tue, 30 Sep 2025 16:21:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/airweave-ai/airweave">https://github.com/airweave-ai/airweave</a>, See on <a href="https://news.ycombinator.com/item?id=45427482">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><themed-picture data-catalyst-inline="true"><picture>
  <source media="(prefers-color-scheme: dark)" srcset="https://github.com/airweave-ai/airweave/raw/main/frontend/public/logo-airweave-darkbg.svg">
  <source media="(prefers-color-scheme: light)" srcset="https://github.com/airweave-ai/airweave/raw/main/frontend/public/logo-airweave-lightbg.svg">
  <img width="1673" alt="airweave-lettermark" src="https://github.com/airweave-ai/airweave/raw/main/frontend/public/logo-airweave-darkbg.svg">
</picture></themed-picture>

<p dir="auto"><h2 tabindex="-1" dir="auto">Overview</h2><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<p dir="auto"><strong>Airweave is a tool that lets agents search any app.</strong> It connects to apps, productivity tools, databases, or document stores and transforms their contents into searchable knowledge bases, accessible through a standardized interface for agents.</p>
<p dir="auto">The search interface is exposed via REST API or MCP. When using MCP, Airweave essentially builds a semantically searchable MCP server. The platform handles everything from auth and extraction to embedding and serving.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#airweave">Airweave</a>
<ul dir="auto">
<li><a href="#overview">Overview</a></li>
<li><a href="#table-of-contents">Table of Contents</a></li>
<li><a href="#-quick-start">🚀 Quick Start</a></li>
<li><a href="#-supported-integrations">🔌 Supported Integrations</a></li>
<li><a href="#-usage">💻 Usage</a>
<ul dir="auto">
<li><a href="#frontend">Frontend</a></li>
<li><a href="#api">API</a></li>
</ul>
</li>
<li><a href="#-sdks">📦 SDKs</a>
<ul dir="auto">
<li><a href="#python">Python</a></li>
<li><a href="#typescriptjavascript">TypeScript/JavaScript</a></li>
</ul>
</li>
<li><a href="#-key-features">🔑 Key Features</a></li>
<li><a href="#-technology-stack">🔧 Technology Stack</a></li>
<li><a href="#-contributing">👥 Contributing</a></li>
<li><a href="#-license">📄 License</a></li>
<li><a href="#-connect">🔗 Connect</a></li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🚀 Quick Start</h2><a id="user-content--quick-start" aria-label="Permalink: 🚀 Quick Start" href="#-quick-start"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Managed Service: <a href="https://app.airweave.ai/" rel="nofollow">Airweave Cloud</a></h3><a id="user-content-managed-service-airweave-cloud" aria-label="Permalink: Managed Service: Airweave Cloud" href="#managed-service-airweave-cloud"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Self-hosted:</h3><a id="user-content-self-hosted" aria-label="Permalink: Self-hosted:" href="#self-hosted"></a></p>
<p dir="auto">Make sure docker and docker-compose are installed, then...</p>
<div dir="auto" data-snippet-clipboard-copy-content="# 1. Clone the repository
git clone https://github.com/airweave-ai/airweave.git
cd airweave

# 2. Build and run
chmod +x start.sh
./start.sh"><pre><span><span>#</span> 1. Clone the repository</span>
git clone https://github.com/airweave-ai/airweave.git
<span>cd</span> airweave

<span><span>#</span> 2. Build and run</span>
chmod +x start.sh
./start.sh</pre></div>
<p dir="auto">That's it! Access the dashboard at <a href="http://localhost:8080/" rel="nofollow">http://localhost:8080</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔌 Supported Integrations</h2><a id="user-content--supported-integrations" aria-label="Permalink: 🔌 Supported Integrations" href="#-supported-integrations"></a></p>

<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/asana.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/asana.svg" alt="Asana" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/bitbucket.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/bitbucket.svg" alt="Bitbucket" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/confluence.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/confluence.svg" alt="Confluence" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/dropbox.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/dropbox.svg" alt="Dropbox" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/github.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/github.svg" alt="Github" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/gmail.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/gmail.svg" alt="Gmail" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/google_calendar.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/google_calendar.svg" alt="Google Calendar" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/google_drive.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/google_drive.svg" alt="Google Drive" width="40" height="40"></a>
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/hubspot.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/hubspot.svg" alt="Hubspot" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/jira.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/jira.svg" alt="Jira" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/linear.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/linear.svg" alt="Linear" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/monday.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/monday.svg" alt="Monday" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/notion.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/notion.svg" alt="Notion" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/onedrive.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/onedrive.svg" alt="Onedrive" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/outlook_calendar.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/outlook_calendar.svg" alt="Outlook Calendar" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/outlook_mail.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/outlook_mail.svg" alt="Outlook Mail" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/postgresql.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/postgresql.svg" alt="Postgresql" width="40" height="40"></a>
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/slack.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/slack.svg" alt="Slack" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/stripe.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/stripe.svg" alt="Stripe" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/todoist.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/todoist.svg" alt="Todoist" width="40" height="40"></a>
  </p>


<p dir="auto"><h2 tabindex="-1" dir="auto">💻 Usage</h2><a id="user-content--usage" aria-label="Permalink: 💻 Usage" href="#-usage"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Frontend</h3><a id="user-content-frontend" aria-label="Permalink: Frontend" href="#frontend"></a></p>
<ul dir="auto">
<li>Access the UI at <code>http://localhost:8080</code></li>
<li>Connect sources, configure syncs, and query data</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">API</h3><a id="user-content-api" aria-label="Permalink: API" href="#api"></a></p>
<ul dir="auto">
<li>Swagger docs: <code>http://localhost:8001/docs</code></li>
<li>Create connections, trigger syncs, and search data</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">📦 SDKs</h2><a id="user-content--sdks" aria-label="Permalink: 📦 SDKs" href="#-sdks"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Python</h3><a id="user-content-python" aria-label="Permalink: Python" href="#python"></a></p>

<div dir="auto" data-snippet-clipboard-copy-content="from airweave import AirweaveSDK

client = AirweaveSDK(
    api_key=&quot;YOUR_API_KEY&quot;,
    base_url=&quot;http://localhost:8001&quot;
)
client.collections.create(
    name=&quot;name&quot;,
)"><pre><span>from</span> <span>airweave</span> <span>import</span> <span>AirweaveSDK</span>

<span>client</span> <span>=</span> <span>AirweaveSDK</span>(
    <span>api_key</span><span>=</span><span>"YOUR_API_KEY"</span>,
    <span>base_url</span><span>=</span><span>"http://localhost:8001"</span>
)
<span>client</span>.<span>collections</span>.<span>create</span>(
    <span>name</span><span>=</span><span>"name"</span>,
)</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">TypeScript/JavaScript</h3><a id="user-content-typescriptjavascript" aria-label="Permalink: TypeScript/JavaScript" href="#typescriptjavascript"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="npm install @airweave/sdk
# or
yarn add @airweave/sdk"><pre>npm install @airweave/sdk
<span><span>#</span> or</span>
yarn add @airweave/sdk</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="import { AirweaveSDKClient, AirweaveSDKEnvironment } from &quot;@airweave/sdk&quot;;

const client = new AirweaveSDKClient({
    apiKey: &quot;YOUR_API_KEY&quot;,
    environment: AirweaveSDKEnvironment.Local
});
await client.collections.create({
    name: &quot;name&quot;,
});"><pre><span>import</span> <span>{</span> <span>AirweaveSDKClient</span><span>,</span> <span>AirweaveSDKEnvironment</span> <span>}</span> <span>from</span> <span>"@airweave/sdk"</span><span>;</span>

<span>const</span> <span>client</span> <span>=</span> <span>new</span> <span>AirweaveSDKClient</span><span>(</span><span>{</span>
    <span>apiKey</span>: <span>"YOUR_API_KEY"</span><span>,</span>
    <span>environment</span>: <span>AirweaveSDKEnvironment</span><span>.</span><span>Local</span>
<span>}</span><span>)</span><span>;</span>
<span>await</span> <span>client</span><span>.</span><span>collections</span><span>.</span><span>create</span><span>(</span><span>{</span>
    <span>name</span>: <span>"name"</span><span>,</span>
<span>}</span><span>)</span><span>;</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔑 Key Features</h2><a id="user-content--key-features" aria-label="Permalink: 🔑 Key Features" href="#-key-features"></a></p>
<ul dir="auto">
<li><strong>Data synchronization</strong> from 25+ sources with minimal config</li>
<li><strong>Entity extraction</strong> and transformation pipeline</li>
<li><strong>Multi-tenant</strong> architecture with OAuth2</li>
<li><strong>Incremental updates</strong> using content hashing</li>
<li><strong>Semantic search</strong> for agent queries</li>
<li><strong>Versioning</strong> for data changes</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔧 Tech Stack</h2><a id="user-content--tech-stack" aria-label="Permalink: 🔧 Tech Stack" href="#-tech-stack"></a></p>
<ul dir="auto">
<li><strong>Frontend</strong>: React/TypeScript with ShadCN</li>
<li><strong>Backend</strong>: FastAPI (Python)</li>
<li><strong>Databases</strong>: PostgreSQL (metadata), Qdrant (vectors)</li>
<li><strong>Deployment</strong>: Docker Compose (dev), Kubernetes (prod)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">👥 Contributing</h2><a id="user-content--contributing" aria-label="Permalink: 👥 Contributing" href="#-contributing"></a></p>
<p dir="auto">We welcome contributions! Please check <a href="https://github.com/airweave-ai/airweave/blob/main/CONTRIBUTING.md">CONTRIBUTING.md</a> for details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">📄 License</h2><a id="user-content--license" aria-label="Permalink: 📄 License" href="#-license"></a></p>
<p dir="auto">Airweave is released under the <a href="https://github.com/airweave-ai/airweave/blob/main/LICENSE">MIT</a> license.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔗 Connect</h2><a id="user-content--connect" aria-label="Permalink: 🔗 Connect" href="#-connect"></a></p>
<ul dir="auto">
<li><strong><a href="https://discord.com/invite/484HY9Ehxt" rel="nofollow">Discord</a></strong> - Get help and discuss features</li>
<li><strong><a href="https://github.com/airweave-ai/airweave/issues">GitHub Issues</a></strong> - Report bugs or request features</li>
<li><strong><a href="https://x.com/airweave_ai" rel="nofollow">Twitter</a></strong> - Follow for updates</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Leaked Apple M5 9 core Geekbench scores (220 pts)]]></title>
            <link>https://browser.geekbench.com/v6/cpu/14173685</link>
            <guid>45427197</guid>
            <pubDate>Tue, 30 Sep 2025 16:00:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://browser.geekbench.com/v6/cpu/14173685">https://browser.geekbench.com/v6/cpu/14173685</a>, See on <a href="https://news.ycombinator.com/item?id=45427197">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<div>
<p>4133</p>
<p>Single-Core Score</p>
</div>
<div>
<p>15437</p>
<p>Multi-Core Score</p>
</div>
<p>
Geekbench 6.5.0 for iOS AArch64
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cerebras systems raises $1.1B Series G (114 pts)]]></title>
            <link>https://www.cerebras.ai/press-release/series-g</link>
            <guid>45427111</guid>
            <pubDate>Tue, 30 Sep 2025 15:54:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cerebras.ai/press-release/series-g">https://www.cerebras.ai/press-release/series-g</a>, See on <a href="https://news.ycombinator.com/item?id=45427111">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-slicetype="articleText" data-sanity="id=cbc365b9-f2ce-421e-971f-addf7e5f717b;type=pressRelease;path=slices;base=https%3A%2F%2Fwww.cerebras.ai"><p><em>Fidelity Management &amp; Research Company Anchors Investment with an All-Star Consortium of Investors</em></p><p>Sunnyvale, CA – September 30, 2025 – Cerebras Systems, makers of the fastest AI infrastructure in the industry, today announced the completion of an over subscribed$1.1 billion Series G funding round at $8.1 billion post-money valuation. The round was led by Fidelity Management &amp; Research Company and Atreides Management. The round included significant participation from Tiger Global, Valor Equity Partners, and 1789 Capital, as well as existing investors Altimeter, Alpha Wave, and Benchmark.</p><p>As the fastest inference provider in the world, Cerebras will use these funds to expand its pioneering technology portfolio with continued inventions in AI processor design, packaging, system design and AI supercomputers. In addition, it will expand its U.S. manufacturing capacity and its U.S. data center capacity to keep pace with the explosive demand for Cerebras products and services.</p><p>"From our inception we have been backed by the most knowledgeable investors in the industry. They have seen the historic opportunity that is AI and have chosen to invest in Cerebras,” said Andrew Feldman, co-founder and CEO, Cerebras. "We are proud to expand our consortium of best-in-world investors.”</p><h3>Inference Momentum as AI Industry Leaders Choose Cerebras</h3><p>Cerebras has experienced extraordinary growth since launching its inference service in late 2024. Over the past year, Cerebras has held the performance crown every single day, routinely demonstrating speeds more than 20X faster than Nvidia GPUs on open-source and closed source models.</p><p>"Since our founding, we have tested every AI inference provider across hundreds of models. Cerebras is consistently the fastest,” said Micah Hill-Smith, CEO of leading benchmarking firm Artificial Analysis.</p><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--><p>Cerebras’ performance advantage has led to massive demand. New real-time use cases – including code generation, reasoning, and agentic work – have increased the benefits from speed and the increased the cost of being slow, driving customers to Cerebras. Today, Cerebras is serving trillions of tokens per month, in its own cloud, on its customers premises, and across leading partner platforms.</p><p>In 2025, AI leaders including AWS, Meta, IBM, Mistral, Cognition, AlphaSense, Notion and hundreds more have chosen Cerebras, joining enterprises and governments, including GlaxoSmithKline, Mayo Clinic, the US Department of Energy, the US Department of Defense. Individual developers have also chosen Cerebras for their AI work. On Hugging Face, the leading AI hub for developers, Cerebras is the #1 inference provider with over 5 million monthly requests.</p><p>Citigroup and Barclays Capital acted as joint placement agents for the transaction.</p><h3>About Cerebras Systems</h3><p>Cerebras Systems builds the fastest AI infrastructure in the world. We are a team of pioneering computer architects, computer scientists, AI researchers, and engineers of all types. We have come together to make AI blisteringly fast through innovation and invention because we believe that when AI is fast it will change the world. Our flagship technology, the Wafer Scale Engine 3 (WSE-3) is the world’s largest and fastest AI processor.56 times larger than the largest GPU, the WSE uses a fraction of the power per unit compute while delivering inference and training more than 20 times faster than the competition.Leading corporations, research institutes and governments on four continents chose Cerebras to run their AI workloads. Cerebras solutions are available on premise and in the cloud, for further information, visit cerebras.ai or follow us on LinkedIn, X and/or Threads.</p><p>Media Contact</p><p><a href="mailto:PR@zmcommunications.com">PR@zmcommunications.com</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Visualizations of Random Attractors Found Using Lyapunov Exponents (108 pts)]]></title>
            <link>https://paulbourke.net/fractals/lyapunov/</link>
            <guid>45427059</guid>
            <pubDate>Tue, 30 Sep 2025 15:50:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://paulbourke.net/fractals/lyapunov/">https://paulbourke.net/fractals/lyapunov/</a>, See on <a href="https://news.ycombinator.com/item?id=45427059">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<center>

Written by <a href="https://paulbourke.net/fractals/">Paul Bourke</a><br>
October 2001<p>
Contribution by Philip Ham: <a href="https://paulbourke.net/fractals/lyapunov/attractor.basic">attractor.basic</a><br>
and <a href="https://paulbourke.net/fractals/lyapunov/fractalgan.py">Python implementation</a> by Johan Bichel Lindegaard.</p><p>
This document is "littered" with a
selection of attractors found using the techniques described.
</p></center>
<center>
	<img src="https://paulbourke.net/fractals/lyapunov/1.jpg" width="800" height="800"></center>
<p>
In order for a system to exhibit chaotic behaviour it must be non linear.
Representing chaotic systems on a screen or on paper leads one to 
considering a two dimensional system, an equation in two variables. 
One possible two dimensional non-linear system, the one used
here, is the quadratic map defined as follows.
</p>

<center>
x<sub>n+1</sub> = 
a<sub>0</sub> + 
a<sub>1</sub> x<sub>n</sub> +
a<sub>2</sub> x<sub>n</sub><sup>2</sup> +
a<sub>3</sub> x<sub>n</sub> y<sub>n</sub> +
a<sub>4</sub> y<sub>n</sub> +
a<sub>5</sub> y<sub>n</sub><sup>2</sup>
<br>
y<sub>n+1</sub> =
b<sub>0</sub> + 
b<sub>1</sub> x<sub>n</sub> +
b<sub>2</sub> x<sub>n</sub><sup>2</sup> +
b<sub>3</sub> x<sub>n</sub> y<sub>n</sub> +
b<sub>4</sub> y<sub>n</sub> +
b<sub>5</sub> y<sub>n</sub><sup>2</sup>
</center>
<center>
	<img src="https://paulbourke.net/fractals/lyapunov/2.jpg" width="800" height="800"></center>

<p>
The standard measure for determining whether or not a system is chaotic
is the Lyapunov exponent, normally represented by the lambda symbol.
Consider two close points at step n, x<sub>n</sub> and
x<sub>n</sub>+dx<sub>n</sub>. At the next time step they
will have diverged, namely to x<sub>n+1</sub> and
x<sub>n+1</sub>+dx<sub>n+1</sub>. It is this average
rate of divergence (or convergence) that the Lyapunov exponent captures.
Another way to think about the Lyapunov exponent is as the rate at
which information about the initial conditions is lost.
</p>

<center>
<img src="https://paulbourke.net/fractals/lyapunov/diagram.gif" width="400" height="456"></center>

<p>
There are as many Lyapunov exponents as dimensions of the phase space.
Considering a region (circle, sphere, hypersphere, etc) in phase space
then at a later time all trajectories in this region form an
n-dimensional elliptical region. The Lyapunov exponent can be calculated for
each dimension.
When talking about a single exponent one is normally referring to the
largest, this convention will be assumed from now onwards.
</p>

<center>
	<img src="https://paulbourke.net/fractals/lyapunov/3.jpg" width="800" height="800"></center>

<p>
If the Lyapunov exponent is positive then the system is chaotic and unstable.
Nearby points will diverge irrespective of how close they are. Although there
is no order the system is still deterministic!
The magnitude of the Lyapunov exponent
is a measure of the sensitivity to initial conditions, the primary
characteristic of a chaotic system.
</p>

<center>
	<img src="https://paulbourke.net/fractals/lyapunov/4.jpg" width="800" height="800"></center>

<p>
If the Lyapunov exponent is less than zero then the system attracts to
a fixed point or stable periodic orbit. These systems are non conservative
(dissipative). The absolute value of the exponent indicates the degree
of stability.
</p>

<center>
	<img src="https://paulbourke.net/fractals/lyapunov/5.jpg" width="800" height="800"></center>

<p>
If the Lyapunov exponent is zero then the system is neutrally stable, such
systems are conservative and in a steady state mode.
</p>

<center>
	<img src="https://paulbourke.net/fractals/lyapunov/6.jpg" width="800" height="800"></center>
<p>
To create the chaotic attractors shown on this page
each parameter a<sub>n</sub> and b<sub>n</sub> in the quadratic
equation above is chosen at random between some bounds (+- 2 say). 
The system so specified is generated
by iterating for some suitably large number of time steps
(eg; 100000) steps during which time the image is created
and the Lyapunov exponent computed. Note that the first few (1000) timesteps
are ignored to allow the system to settle into its "natural" behaviour.
If the Lyapunov exponent indicates
chaos then the image is saved and the program moves on to the
next random parameter set. 
</p>

<center>
	<img src="https://paulbourke.net/fractals/lyapunov/7.jpg" width="800" height="800"></center>

<p>
There are a number of ways the series may behave.
</p>
<ul>
<li><p>
It may converge to a single point, called a fixed point. These
can be detected by comparing the distances between successive
points. For numerical reasons this is safer than relying on the
Lyapunov exponent which may be infinite (logarithm of 0)</p>
</li><li><p>
It may diverge to infinity, for the range (+- 2) used here for
each parameter this is the most likely event. These are also easy
to detect and discard, indeed they need to be in order to avoid
numerical errors.</p>
</li><li><p>
It will form a periodic orbit, these are identified by their 
negative Lyapunov exponent.
</p>
</li><li><p>
It will exhibit chaos, filling in some region of the plane.
These are the solutions that "look good" and the ones we wish to
identify with the Lyapunov exponent.
</p>
</li></ul>
<center>
	<img src="https://paulbourke.net/fractals/lyapunov/8.jpg" width="800" height="800"></center>

<p>
It should be noted that there may be visually appealing structures
that are not chaotic attractors. That is, the resulting image is different
for different initial conditions and there is no single basin of attraction.
It's interesting how we "see" 3 dimensional structures in these essentially
2 dimensional systems.
</p>

<center>
	<img src="https://paulbourke.net/fractals/lyapunov/9.jpg" width="800" height="800"></center>
<p>
The software used to create these images is given here:
<a href="https://paulbourke.net/fractals/lyapunov/gen.c">gen.c</a>. On average 98% of the random
selections of (a<sub>n</sub>, b<sub>n</sub>) result
in an infinite series. This is so common because of
the range (-2 &lt;= a,b &lt;= 2) for each of the parameters
a and b, the number of infinite cases will reduce greatly with
a smaller range.
About 1% were point attractors, and
about 0.5% were periodic basins of attraction.
</p>

<center>
	<img src="https://paulbourke.net/fractals/lyapunov/10.jpg" width="800" height="800"></center><center>
<p>
<a href="https://paulbourke.net/fractals/lyapunov/shuttle.jpg"><img src="https://paulbourke.net/fractals/lyapunov/shuttle.jpg" width="800" height="600"></a><br>
<span size="-1">
<p>
Image courtesy of Robert McGregor, Space Coast of Florida.
Launch trail perhaps 30 minutes after the shuttle launch (June 2007)
dispersing from a column into a smoke ring 
due to some unusual air currents in the upper atmosphere.
</p>
</span>
</p></center>

<center>
   <img src="https://paulbourke.net/fractals/lyapunov/11.jpg" width="800" height="800"></center><center>
   <img src="https://paulbourke.net/fractals/lyapunov/12.jpg" width="800" height="800"></center><center>
   <img src="https://paulbourke.net/fractals/lyapunov/13.jpg" width="800" height="800"></center><center>
   <img src="https://paulbourke.net/fractals/lyapunov/14.jpg" width="800" height="800"></center><p>

<b>References</b></p><p>

Berge, P., Pomeau, Y., Vidal, C.,<br>
Order Within Chaos, Wiley, New York, 1984.
</p><p>

Crutchfield, J., Farmer, J., Packard, N.<br>
Chaos, Scientific American, 1986, 255, 46-47
</p><p>

Das, A., Das, Pritha, Roy, A<br>
Applicability of Lyapunov Exponent in EEG data analysis. 
Complexity International, draft manuscript.
</p><p>

Devaney, R.<br>
An Introduction to Chaotic Dynamical Systems, Addison-Wesley, 1989
</p><p>

Feigenbaum, M.,<br>
Universal behaviour in Nonlinear Systems, Los Alamos Science, 1981
</p><p>

Peitgen, H., Jurgens, H., Saupe, D<br>
Lyapunov exponents and chaotic attractors in Chaos and fractals
- new frontiers of science. Springer, new York, 1992.
</p><p>


<b>Contributions by Dmytry Lavrov</b>
</p><center>
<img src="https://paulbourke.net/fractals/lyapunov/explore1.jpg" width="800" height="600"><p>
<img src="https://paulbourke.net/fractals/lyapunov/explore2.jpg" width="800" height="600"></p></center>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Correctness and composability bugs in the Julia ecosystem (2022) (101 pts)]]></title>
            <link>https://yuri.is/not-julia/</link>
            <guid>45427021</guid>
            <pubDate>Tue, 30 Sep 2025 15:46:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://yuri.is/not-julia/">https://yuri.is/not-julia/</a>, See on <a href="https://news.ycombinator.com/item?id=45427021">Hacker News</a></p>
<div id="readability-page-1" class="page">








<p>For many years I used the <a href="https://julialang.org/">Julia</a> programming language for transforming, cleaning, analyzing, and visualizing data, doing statistics, and performing simulations.</p>

<p>I published a handful of open-source packages for things like <a href="https://github.com/JuliaGraphics/SignedDistanceFields.jl">signed distance fields</a>, <a href="https://github.com/yurivish/LowDimNearestNeighbors.jl">nearest-neighbor search</a>, and <a href="https://github.com/yurivish/TuringPatterns.jl">Turing patterns</a> (<a href="https://github.com/JuliaWeb/Hyperscript.jl">among</a> <a href="https://github.com/yurivish/Treaps.jl">others</a>), made visual explanations of Julia concepts like <a href="https://julia-guide.netlify.app/broadcasting">broadcasting</a> and <a href="https://observablehq.com/@yurivish/julia-array-notation">arrays</a>, and used Julia to make the generative art on my <a href="https://yuri.is/cardcrafting">business cards</a>.</p>

<p>I stopped using Julia a while ago, but it still sometimes comes up. When people ask, I tell them that I can no longer recommend it. I thought I’d write up my reasons why.</p>

<p>My conclusion after using Julia for many years is that there are too many correctness and composability bugs throughout the ecosystem to justify using it in just about any context where correctness matters. </p>

<p>In my experience, Julia and its packages have the highest rate of serious correctness bugs of any programming system I’ve used, and I started programming with Visual Basic 6 in the mid-2000s. </p>

<p>It might be useful to give some concrete examples.</p>

<p>Here are some correctness issues I filed:</p>

<ul>
<li><a href="https://github.com/JuliaStats/Distributions.jl/issues/1241">Sampling a probability density produces an incorrect result</a></li>
<li><a href="https://github.com/JuliaStats/StatsBase.jl/issues/642">Sampling an array can produce biased results</a></li>
<li><a href="https://github.com/JuliaLang/julia/issues/39183">The product function can produce incorrect results for 8-bit, 16-bit, and 32-bit integers</a></li>
<li><a href="https://github.com/JuliaStats/StatsBase.jl/issues/616">Fitting a histogram to a Float64 array can produce incorrect results</a></li>
<li><a href="https://github.com/JuliaLang/julia/issues/39385">Base functions sum!, prod!, any!, and all! may silently return incorrect results</a></li>
</ul>

<p>Here are comparable issues filed by others:</p>

<ul>
<li><a href="https://github.com/JuliaStats/StatsBase.jl/pull/632">Summarystats returns NaN quantiles for arrays with mean 0</a></li>
<li><a href="https://github.com/JuliaCollections/OrderedCollections.jl/issues/71">OrderedDict can corrupt keys</a></li>
<li><a href="https://github.com/JuliaLang/julia/pull/36543">Off-by-one error in dayofquarter() in leap years</a></li>
<li><a href="https://github.com/JuliaPhysics/Measurements.jl/issues/64">Incorrect simulation results when using a number type with error bars</a></li>
<li><a href="https://github.com/JuliaLang/julia/issues/36069">Pipeline with stdout=IOStream writes out of order</a></li>
<li><a href="https://github.com/JuliaLang/julia/issues/39460">Wrong results since some <code>copyto!</code> methods don’t check for aliasing</a></li>
<li><a href="https://github.com/JuliaLang/julia/issues/41096">Wrong if-else control flow</a></li>
</ul>

<p>I would hit bugs of this severity often enough to make me question the correctness of any moderately complex computation in Julia. </p>

<p>This was particularly true when trying a novel combination of packages or functions — composing together functionality from multiple sources was a significant source of bugs.</p>

<p>Sometimes the problems would be with packages that don’t compose together, and other times an unexpected combination of Julia’s features within a single package would unexpectedly fail.</p>

<p>For example, I found that the Euclidean distance from the Distances package <a href="https://github.com/JuliaStats/Distances.jl/issues/201">does not work with Unitful vectors</a>. Others discovered that Julia’s function to run external commands <a href="https://github.com/JuliaLang/julia/issues/36406">doesn’t work with substrings</a>. Still others found that Julia’s support for missing values <a href="https://github.com/JuliaLang/julia/issues/39362">breaks matrix multiplication in some cases</a>. And that the standard library’s <code>@distributed</code> macro <a href="https://github.com/JuliaLang/julia/issues/34870">didn’t work with OffsetArrays</a>.</p>

<p><a href="https://github.com/JuliaArrays/OffsetArrays.jl">OffsetArrays</a> in particular proved to be a strong source of correctness bugs. The package provides an array type that leverages Julia’s flexible <a href="https://julialang.org/blog/2017/04/offset-arrays/">custom indices feature</a> to create arrays whose indices don’t have to start at zero or one. </p>

<p>Using them would often result in out-of-bounds memory accesses, just like those one might encounter in C or C++. This would lead to segfaults if you were lucky, or, if you weren’t, to results that were quietly wrong. I once found <a href="https://github.com/JuliaLang/julia/issues/39379">a bug in core Julia</a> that could lead to out-of-bounds memory accesses even when both the user and library authors wrote correct code.</p>

<p>I filed a number of indexing-related issues with the JuliaStats organization, which stewards statistics packages like <a href="https://juliahub.com/ui/Packages/Distributions/xILW0/0.25.58">Distributions</a>, which 945 packages depend on, and <a href="https://juliahub.com/ui/Packages/StatsBase/EZjIG/0.33.16">StatsBase</a>, which 1,660 packages depend on. Here are some of them:</p>

<ul>
<li><p><a href="https://github.com/JuliaStats/StatsBase.jl/issues/646">The majority of sampling methods are unsafe and incorrect in the presence of offset axes</a></p></li>
<li><p><a href="https://github.com/JuliaStats/Distributions.jl/issues/1253">Fitting a DiscreteUniform distribution can silently return an incorrect answer</a></p></li>
<li><p><a href="https://github.com/JuliaStats/StatsBase.jl/issues/638">counteq, countne, sqL2dist, L2dist, L1dist, L1infdist, gkldiv, meanad, maxad, msd, rmsd, and psnr may return incorrect results with offset indices</a></p></li>
<li><p><a href="https://github.com/JuliaStats/Distributions.jl/issues/1265">Incorrect uses of @inbounds cause miscalculation of statistics</a></p></li>
<li><p><a href="https://github.com/JuliaStats/Distances.jl/issues/206">Colwise and pairwise can return incorrect distances</a></p></li>
<li><p><a href="https://github.com/JuliaStats/StatsBase.jl/issues/643">Showing a Weights vector wrapping an offset array accesses out-of-bounds memory</a></p></li>
</ul>

<p>The root cause behind these issues was not the indexing alone but its use together with another Julia feature, <code>@inbounds</code>, which permits Julia to remove bounds checks from array accesses.</p>

<p>For example:</p>

<pre><code>function sum(A::AbstractArray)
    r = zero(eltype(A))
    for i in 1:length(A)
        @inbounds r += A[i] # ← 🌶
    end
    return r
end
</code></pre>

<p>The code above iterates <code>i</code> from 1 to the length of the array. If you pass it an array with an unusual index range, it will access out-of-bounds memory: the array access was annotated with <code>@inbounds</code>, which removed the bounds check.</p>

<p>The example above shows how to use <code>@inbounds</code> <strong>incorrectly</strong>. However, for years it was also the official example of how to use <code>@inbounds</code> <strong>correctly</strong>. The example was situated directly above a warning explaining why it was incorrect:</p>

<figure>
<img src="https://yuri.is/not-julia/issue.png" alt="">
</figure>

<p><a href="https://github.com/JuliaLang/julia/issues/39367">That issue</a> is now fixed, but it is worrying that <code>@inbounds</code> can be so easily misused, causing silent data corruption and incorrect mathematical results.</p>

<p>In my experience, issues like these were not confined to the mathematical parts of the Julia ecosystem.</p>

<p>I encountered library bugs while trying to accomplish mundane tasks like <a href="https://github.com/quinnj/JSON3.jl/issues/63">encoding</a> <a href="https://github.com/JuliaLang/julia/issues/34249">JSON</a>, issuing <a href="https://github.com/JuliaWeb/HTTP.jl/issues/626">HTTP requests</a>, using <a href="https://github.com/apache/arrow-julia/issues/101">Arrow files</a> <a href="https://github.com/apache/arrow-julia/issues/102">together</a> with DataFrames, and <a href="https://github.com/fonsp/Pluto.jl/issues/826">editing</a> <a href="https://github.com/fonsp/Pluto.jl/issues/751">Julia</a> <a href="https://github.com/fonsp/Pluto.jl/issues/836">code</a> with <a href="https://github.com/fonsp/Pluto.jl">Pluto</a>, Julia’s reactive notebook environment.</p>

<p>When I became curious if my experience was representative, a number of Julia users privately shared similar stories. Recently, public accounts of comparable experiences have begun to surface.</p>

<p>For example, in <a href="https://kidger.site/thoughts/jax-vs-julia/">this post</a> Patrick Kidger describes his attempt to use Julia for machine learning research:</p>

<blockquote>
<p>It’s pretty common to see posts on the Julia Discourse saying “XYZ library doesn’t work”, followed by a reply from one of the library maintainers stating something like “This is an upstream bug in the new version a.b.c of the ABC library, which XYZ depends upon. We’ll get a fix pushed ASAP.”</p>
</blockquote>

<p>Here’s Patrick’s experience tracking down a correctness bug (emphasis mine):</p>

<blockquote>
<p>I remember all too un-fondly a time in which one of my Julia models was failing to train. I spent multiple months on-and-off trying to get it working, trying every trick I could think of.</p>

<p>Eventually – eventually! – I found the error: <strong>Julia/Flux/Zygote was returning incorrect gradients.</strong> After having spent so much energy wrestling with points 1 and 2 above, this was the point where I simply gave up. Two hours of development work later, I had the model successfully training… in PyTorch.</p>
</blockquote>

<p>In a <a href="https://discourse.julialang.org/t/state-of-machine-learning-in-julia/74385/4">discussion</a> about the post others responded that they, too, had similar experiences.</p>

<p><a href="https://discourse.julialang.org/t/state-of-machine-learning-in-julia/74385/13">@Samuel_Ainsworth</a>:</p>

<blockquote>
<p>Like @patrick-kidger, I have been bit by incorrect gradient bugs in Zygote/ReverseDiff.jl. This cost me weeks of my life and has thoroughly shaken my confidence in the entire Julia AD landscape. [...] In all my years of working with PyTorch/TF/JAX I have not once encountered an incorrect gradient bug.</p>
</blockquote>

<p><a href="https://discourse.julialang.org/t/state-of-machine-learning-in-julia/74385/21">@JordiBolibar</a>:</p>

<blockquote>
<p>Since I started working with Julia, I’ve had two bugs with Zygote which have slowed my work by several months. On a positive note, this has forced me to plunge into the code and learn a lot about the libraries I’m using. But I’m finding myself in a situation where this is becoming too much, and I need to spend a lot of time debugging code instead of doing climate research. </p>
</blockquote>

<p>Given Julia’s extreme generality it is not obvious to me that the correctness problems <em>can</em> be solved. Julia has no formal notion of interfaces, generic functions tend to leave their semantics unspecified in edge cases, and the nature of many common implicit interfaces has not been made precise (for example, there is no agreement in the Julia community on what a number is).</p>

<p>The Julia community is full of capable and talented people who are generous with their time, work, and expertise. But systemic problems like this can rarely be solved from the bottom up, and my sense is that the project leadership does not agree that there is a serious correctness problem. They accept the existence of individual isolated issues, but not the pattern that those issues imply. </p>

<p>At a time when Julia’s machine learning ecosystem was even less mature, for example, a co-founder of the language spoke enthusiastically about using Julia in production for self-driving cars:</p>

<figure>
<img src="https://yuri.is/not-julia/picture.png" alt="">
</figure>

<p>And while it’s possible that attitudes have shifted since I was an active member, the following quote from another co-founder, also made around the same time, serves as a good illustration of the perception gap (emphasis mine):</p>

<blockquote>
<p>I think the top-level take away here is not that Julia is a great language (although it is) and that they should use it for all the things (although that’s not the worst idea), but that its design has hit on something that has made a major step forwards in terms of our ability to achieve code reuse. <strong>It is actually the case in Julia that you can take generic algorithms that were written by one person and custom types that were written by other people and just use them together efficiently and effectively.</strong> This majorly raises the table stakes for code reuse in programming languages. Language designers should not copy all the features of Julia, but they should at the very least understand why this works so well, and be able to accomplish this level of code reuse in future designs.</p>
</blockquote>

<p>Whenever a post critiquing Julia makes the rounds, people from the community are often quick to respond that, while there have historically been some legitimate issues, things have improved substantially and most of the issues are now fixed.</p>

<p>For example:</p>

<ul>
<li>In 2016: <a href="https://news.ycombinator.com/item?id=11073642">“The legitimate issues raised in that blog post are fixed.”</a></li>
<li>In 2018: <a href="https://news.ycombinator.com/item?id=17726336">“I complained also about the ‘cowboy’ culture I saw among the Julia developers when I first started with it ... but those days are gone.”</a></li>
<li>In 2020: <a href="https://news.ycombinator.com/item?id=22138071">“In 2016 yes. But this has been very much addressed.”</a></li>
<li>In 2021: <a href="https://news.ycombinator.com/item?id=27921467">“In Julia there is no technical enforcement of consistency, but semantic meaning of generic functions is broadly respected and generic code works.”</a></li>
<li>In 2022: <a href="https://news.ycombinator.com/item?id=30939963">“Of course there are bugs, but none of them are serious.”</a></li>
<li>In 2024: <a href="https://news.ycombinator.com/item?id=41104531">“My general sense of this article is that all of these have been fixed.”</a></li>
<li>In 2025: <a href="https://news.ycombinator.com/item?id=45428134">“This is a reasonable article, but way out of date now. Almost all issues raised were solved a while ago.”</a></li>
</ul>

<p>These responses often look reasonable in their narrow contexts, but the net effect is that people’s legitimate experiences feel diminished or downplayed, and the deeper issues go unacknowledged and unaddressed.</p>

<p>My experience with the language and community over the past ten years strongly suggests that, at least in terms of basic correctness, Julia is not currently reliable or on the path to becoming reliable. For the majority of use cases the Julia team wants to service, the risks are simply not worth the rewards.</p>

<p>Ten years ago, Julia was introduced to the world with an <a href="https://julialang.org/blog/2012/02/why-we-created-julia/">inspiring and ambitious set of goals</a>. I still believe that they can, one day, be achieved—but not without revisiting and revising the patterns that brought the project to the state it is in today.</p>



<p><small>Thanks to Mitha Nandagopalan, Ben Cartwright-Cox, Imran Qureshi, Dan Luu, Elad Bogomolny, Zora Killpack, Ben Kuhn, and Yuriy Rusko for discussions and comments on earlier drafts of this post.</small></p>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Designing agentic loops (163 pts)]]></title>
            <link>https://simonwillison.net/2025/Sep/30/designing-agentic-loops/</link>
            <guid>45426680</guid>
            <pubDate>Tue, 30 Sep 2025 15:21:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://simonwillison.net/2025/Sep/30/designing-agentic-loops/">https://simonwillison.net/2025/Sep/30/designing-agentic-loops/</a>, See on <a href="https://news.ycombinator.com/item?id=45426680">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-permalink-context="/2025/Sep/30/designing-agentic-loops/">

<p>30th September 2025</p>



<p>Coding agents like Anthropic’s <a href="https://claude.com/product/claude-code">Claude Code</a> and OpenAI’s <a href="https://github.com/openai/codex">Codex CLI</a> represent a genuine step change in how useful LLMs can be for producing working code. These agents can now directly exercise the code they are writing, correct errors, dig through existing implementation details, and even run experiments to find effective code solutions to problems.</p>
<p>As is so often the case with modern AI, there is a great deal of depth involved in unlocking the full potential of these new tools.</p>
<p>A critical new skill to develop is <strong>designing agentic loops</strong>.</p>
<p>One way to think about coding agents is that they are brute force tools for finding solutions to coding problems. If you can reduce your problem to a clear goal and a set of tools that can iterate towards that goal a coding agent can often brute force its way to an effective solution.</p>
<p>My preferred definition of an LLM agent is something that <a href="https://simonwillison.net/2025/Sep/18/agents/">runs tools in a loop to achieve a goal</a>. The art of using them well is to carefully design the tools and loop for them to use.</p>
<ul>
  <li><a href="https://simonwillison.net/2025/Sep/30/designing-agentic-loops/#the-joy-of-yolo-mode">The joy of YOLO mode</a></li>
  <li><a href="https://simonwillison.net/2025/Sep/30/designing-agentic-loops/#picking-the-right-tools-for-the-loop">Picking the right tools for the loop</a></li>
  <li><a href="https://simonwillison.net/2025/Sep/30/designing-agentic-loops/#issuing-tightly-scoped-credentials">Issuing tightly scoped credentials</a></li>
  <li><a href="https://simonwillison.net/2025/Sep/30/designing-agentic-loops/#when-to-design-an-agentic-loop">When to design an agentic loop</a></li>
  <li><a href="https://simonwillison.net/2025/Sep/30/designing-agentic-loops/#this-is-still-a-very-fresh-area">This is still a very fresh area</a></li>
</ul>
<h4 id="the-joy-of-yolo-mode">The joy of YOLO mode</h4>
<p>Agents are inherently dangerous—they can make poor decisions or fall victim to malicious <a href="https://simonwillison.net/tags/prompt-injection/">prompt injection attacks</a>, either of which can result in harmful results from tool calls. Since the most powerful coding agent tool is “run this command in the shell” a rogue agent can do anything that you could do by running a command yourself.</p>
<p>To <a href="https://simonwillison.net/2025/Jun/5/wrecking-its-environment-in-a-loop/">quote Solomon Hykes</a>:</p>
<blockquote>
<p><strong>An AI agent is an LLM wrecking its environment in a loop.</strong></p>
</blockquote>
<p>Coding agents like Claude Code counter this by defaulting to asking you for approval of almost every command that they run.</p>
<p>This is kind of tedious, but more importantly, it dramatically reduces their effectiveness at solving problems through brute force.</p>
<p>Each of these tools provides its own version of what I like to call YOLO mode, where everything gets approved by default.</p>
<p>This is <em>so dangerous</em>, but it’s also key to getting the most productive results!</p>
<p>Here are three key risks to consider from unattended YOLO mode.</p>
<ol>
<li>Bad shell commands deleting or mangling things you care about.</li>
<li>Exfiltration attacks where something steals files or data visible to the agent—source code or secrets held in environment variables are particularly vulnerable here.</li>
<li>Attacks that use your machine as a proxy to attack another target—for DDoS or to disguise the source of other hacking attacks.</li>
</ol>
<p>If you want to run YOLO mode anyway, you have a few options:</p>
<ol>
<li>Run your agent in a secure sandbox that restricts the files and secrets it can access and the network connections it can make.</li>
<li>Use someone else’s computer. That way if your agent goes rogue, there’s only so much damage they can do, including wasting someone else’s CPU cycles.</li>
<li>Take a risk! Try to avoid exposing it to potential sources of malicious instructions and hope you catch any mistakes before they cause any damage.</li>
</ol>
<p>Most people choose option 3.</p>
<p>Despite the existence of <a href="https://attack.mitre.org/techniques/T1611/">container escapes</a> I think option 1 using Docker or the new Apple <a href="https://github.com/apple/container">container tool</a> is a reasonable risk to accept for most people.</p>
<p>Option 2 is my favorite. I like to use <a href="https://github.com/features/codespaces">GitHub Codespaces</a> for this—it provides a full container environment on-demand that’s accessible through your browser and has a generous free tier too. If anything goes wrong it’s a Microsoft Azure machine somewhere that’s burning CPU and the worst that can happen is code you checked out into the environment might be exfiltrated by an attacker, or bad code might be pushed to the attached GitHub repository.</p>
<p>There are plenty of other agent-like tools that run code on other people’s computers. <a href="https://simonwillison.net/tags/code-interpreter/">Code Interpreter</a> mode in both ChatGPT and <a href="https://simonwillison.net/2025/Sep/9/claude-code-interpreter/">Claude</a> can go a surprisingly long way here. I’ve also had a lot of success (ab)using OpenAI’s <a href="https://chatgpt.com/features/codex">Codex Cloud</a>.</p>
<p>Coding agents themselves implement various levels of sandboxing, but so far I’ve not seen convincing enough documentation of these to trust them.</p>
<p><strong>Update</strong>: It turns out Anthropic have their own documentation on <a href="https://www.anthropic.com/engineering/claude-code-best-practices#d-safe-yolo-mode">Safe YOLO mode</a> for Claude Code which says:</p>
<blockquote>
<p>Letting Claude run arbitrary commands is risky and can result in data loss, system corruption, or even data exfiltration (e.g., via prompt injection attacks). To minimize these risks, use <code>--dangerously-skip-permissions</code> in a container without internet access. You can follow this <a href="https://github.com/anthropics/claude-code/tree/main/.devcontainer">reference implementation</a> using Docker Dev Containers.</p>
</blockquote>
<p>Locking internet access down to a <a href="https://github.com/anthropics/claude-code/blob/5062ed93fc67f9322f807ecbf391ae4376cf8e83/.devcontainer/init-firewall.sh#L66-L75">list of trusted hosts</a> is a great way to prevent exfiltration attacks from stealing your private source code.</p>
<h4 id="picking-the-right-tools-for-the-loop">Picking the right tools for the loop</h4>
<p>Now that we’ve found a safe (enough) way to run in YOLO mode, the next step is to decide which tools we need to make available to the coding agent.</p>
<p>You can bring <a href="https://modelcontextprotocol.io/">MCP</a> into the mix at this point, but I find it’s usually more productive to think in terms of shell commands instead. Coding agents are <em>really good</em> at running shell commands!</p>
<p>If your environment allows them the necessary network access, they can also pull down additional packages from NPM and PyPI and similar. Ensuring your agent runs in an environment where random package installs don’t break things on your main computer is an important consideration as well!</p>
<p>Rather than leaning on MCP, I like to create an <a href="https://agents.md/">AGENTS.md</a> (or equivalent) file with details of packages I think they may need to use.</p>
<p>For a project that involved taking screenshots of various websites I installed my own <a href="https://shot-scraper.datasette.io/">shot-scraper</a> CLI tool and dropped the following in <code>AGENTS.md</code>:</p>
<pre><code>To take a screenshot, run:

shot-scraper http://www.example.com/ -w 800 -o example.jpg
</code></pre>
<p>Just that one example is enough for the agent to guess how to swap out the URL and filename for other screenshots.</p>
<p>Good LLMs already know how to use a bewildering array of existing tools. If you say "use <a href="https://playwright.dev/python/">playwright python</a>" or "use ffmpeg" most models will use those effectively—and since they’re running in a loop they can usually recover from mistakes they make at first and figure out the right incantations without extra guidance.</p>
<h4 id="issuing-tightly-scoped-credentials">Issuing tightly scoped credentials</h4>
<p>In addition to exposing the right commands, we also need to consider what credentials we should expose to those commands.</p>
<p>Ideally we wouldn’t need any credentials at all—plenty of work can be done without signing into anything or providing an API key—but certain problems will require authenticated access.</p>
<p>This is a deep topic in itself, but I have two key recommendations here:</p>
<ol>
<li>Try to provide credentials to test or staging environments where any damage can be well contained.</li>
<li>If a credential can spend money, set a tight budget limit.</li>
</ol>
<p>I’ll use an example to illustrate. A while ago I was investigating slow cold start times for a scale-to-zero application I was running on <a href="https://fly.io/">Fly.io</a>.</p>
<p>I realized I could work a lot faster if I gave Claude Code the ability to directly edit Dockerfiles, deploy them to a Fly account and measure how long they took to launch.</p>
<p>Fly allows you to create organizations, and you can set a budget limit for those organizations and issue a Fly API key that can only create or modify apps within that organization...</p>
<p>So I created a dedicated organization for just this one investigation, set a $5 budget, issued an API key and set Claude Code loose on it!</p>
<p>In that particular case the results weren’t useful enough to describe in more detail, but this was the project where I first realized that “designing an agentic loop” was an important skill to develop.</p>
<h4 id="when-to-design-an-agentic-loop">When to design an agentic loop</h4>
<p>Not every problem responds well to this pattern of working. The thing to look out for here are problems with <strong>clear success criteria</strong> where finding a good solution is likely to involve (potentially slightly tedious) <strong>trial and error</strong>.</p>
<p>Any time you find yourself thinking “ugh, I’m going to have to try a lot of variations here” is a strong signal that an agentic loop might be worth trying!</p>
<p>A few examples:</p>
<ul>
<li>
<strong>Debugging</strong>: a test is failing and you need to investigate the root cause. Coding agents that can already run your tests can likely do this without any extra setup.</li>
<li>
<strong>Performance optimization</strong>: this SQL query is too slow, would adding an index help? Have your agent benchmark the query and then add and drop indexes (in an isolated development environment!) to measure their impact.</li>
<li>
<strong>Upgrading dependencies</strong>: you’ve fallen behind on a bunch of dependency upgrades? If your test suite is solid an agentic loop can upgrade them all for you and make any minor updates needed to reflect breaking changes. Make sure a copy of the relevant  release notes is available, or that the agent knows where to find them itself.</li>
<li>
<strong>Optimizing container sizes</strong>: Docker container feeling uncomfortably large? Have your agent try different base images and iterate on the Dockerfile to try to shrink it, while keeping the tests passing.</li>
</ul>
<p>A common theme in all of these is <strong>automated tests</strong>. The value you can get from coding agents and other LLM coding tools is massively amplified by a good, cleanly passing test suite. Thankfully LLMs are great for accelerating the process of putting one of those together, if you don’t have one yet.</p>
<h4 id="this-is-still-a-very-fresh-area">This is still a very fresh area</h4>
<p><strong>Designing agentic loops</strong> is a very new skill—Claude Code was <a href="https://www.anthropic.com/news/claude-3-7-sonnet">first released</a> in just February 2025!</p>
<p>I’m hoping that giving it a clear name can help us have productive conversations about it. There’s <em>so much more</em> to figure out about how to use these tools as effectively as possible.</p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Kagi News (780 pts)]]></title>
            <link>https://blog.kagi.com/kagi-news</link>
            <guid>45426490</guid>
            <pubDate>Tue, 30 Sep 2025 15:09:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.kagi.com/kagi-news">https://blog.kagi.com/kagi-news</a>, See on <a href="https://news.ycombinator.com/item?id=45426490">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            
            <p><img src="https://kagifeedback.org/assets/files/2025-09-29/1759164655-533625-news.png" alt="Cartoon illustration showing multiple smartphone screens showing the Kagi News app with news in different languages."></p>

<p><strong>A comprehensive daily press review with global news. Fully private, with sources openly curated by our community.</strong></p>

<p>News is broken. We all know it, but we’ve somehow accepted it as inevitable. The endless notifications. The clickbait headlines designed to trigger rather than inform, driven by relentless ad monetization. The exhausting cycle of checking multiple apps throughout the day, only to feel more anxious and less informed than when we started. This isn’t what news was supposed to be.
We can do better, and create what news should have been all along: pure, essential information that respects your intelligence and time.</p>

<h2>Our approach: Signal over noise</h2>

<p><a href="https://kite.kagi.com/">Kagi News</a> operates on a simple principle: understanding the world requires hearing from the world. Every day, our system reads thousands of community curated RSS feeds from publications across different viewpoints and perspectives. We then distill this massive information into one comprehensive daily briefing, while clearly citing sources.</p>

<p>We strive for diversity and transparency of resources and welcome your contributions to widen perspectives. This multi-source approach helps reveal the full picture beyond any single viewpoint.</p>

<h2>Design principles that put readers first</h2>

<p><img src="https://kagifeedback.org/assets/files/2025-09-30/1759238916-838501-kaginews.gif" alt="Gif showing a demo of using the Kagi News app, scrolling through the daily press review for world news, clicking on a headline and showing the detailed summary that Kagi News creates. On the left is the Kagi News logo and download icons for Google Play and App Store."></p>

<p><strong>One daily update:</strong> We publish once per day around noon UTC, creating a natural endpoint to news consumption. This is a deliberate design choice that turns news from an endless habit into a contained ritual.</p>

<p><strong>Five-minute complete understanding:</strong> Our briefings cover everything important in just five minutes. No endless scrolling. No attention hijacking. You read, understand, and move on with your day.</p>

<p><strong>Diversity over echo chambers:</strong> Rather than personalizing feeds to match existing preferences, we expose readers to the full spectrum of global perspectives. This approach breaks down information silos instead of reinforcing them.</p>

<p><strong>Privacy by design:</strong> Your reading habits belong to you. We don’t track, profile, or monetize your attention. You remain the customer and not the product.</p>

<p><strong>Community-driven sources:</strong> Our news sources are open source and community-curated through our public GitHub repository. Anyone can propose additions, flag problems, or suggest improvements.</p>

<p><strong>Customizable:</strong> In your settings, you can select and reorder categories to match your interests and priorities. You can also adjust the number of stories shown, as well as dragging to re-order various sections, so that your briefing is focused on the depth and topics that matter most to you.</p>

<p><strong>News in your language:</strong> You can choose your preferred interface and content language. News stories are generated in their original source language, and then translated using Kagi Translate. The default mode shows regional stories in their original language without translation, and all other ones in your browser’s language.</p>

<h2>Technical implementation that respects publishers</h2>

<p>We don’t scrape content from websites. Instead, we use publicly available RSS feeds that publishers choose to provide. Publishers decide what content appears in their feeds; some include full articles, others only titles or summaries. We respect those choices completely. We’re working within the ecosystem publishers have created rather than circumventing their intentions.</p>

<p><img src="https://kagifeedback.org/assets/files/2025-09-29/1759171753-315733-review.png" alt="Review from apple store that says “I’ve avoided news feeds all of my adult life. The onslaught of information and opinions always felt like a waste of my precious time. This app is on a great track to cut through the noise.”"></p>

<h2>Ready to experience news differently?</h2>

<p>If you’re tired of news that makes you feel worse about the world while teaching you less about it, we invite you to try a different approach with Kagi News, so download it today:</p>

<ul>
<li><a href="https://kite.kagi.com/">Web</a></li>
<li><a href="https://apps.apple.com/us/app/kagi-news/id6748314243">iOS</a></li>
<li><a href="https://play.google.com/store/apps/details?id=com.kagi.news">Android</a></li>
</ul>

<p><img src="https://kagifeedback.org/assets/files/2025-09-29/1759164726-927335-kagi-news.jpg" alt="Photo showing hands holding an iPad, with Kagi News open on the Sports news section."></p>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How the AI bubble ate Y Combinator (190 pts)]]></title>
            <link>https://www.inc.com/sam-blum/how-the-ai-bubble-ate-y-combinator/91240632</link>
            <guid>45426205</guid>
            <pubDate>Tue, 30 Sep 2025 14:52:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.inc.com/sam-blum/how-the-ai-bubble-ate-y-combinator/91240632">https://www.inc.com/sam-blum/how-the-ai-bubble-ate-y-combinator/91240632</a>, See on <a href="https://news.ycombinator.com/item?id=45426205">Hacker News</a></p>
Couldn't get https://www.inc.com/sam-blum/how-the-ai-bubble-ate-y-combinator/91240632: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Largest Mass Resignation in US History as 100k Federal Workers Quit (105 pts)]]></title>
            <link>https://www.newsweek.com/largest-mass-resignation-in-us-history-as-100000-federal-workers-quit-10802162</link>
            <guid>45425892</guid>
            <pubDate>Tue, 30 Sep 2025 14:27:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.newsweek.com/largest-mass-resignation-in-us-history-as-100000-federal-workers-quit-10802162">https://www.newsweek.com/largest-mass-resignation-in-us-history-as-100000-federal-workers-quit-10802162</a>, See on <a href="https://news.ycombinator.com/item?id=45425892">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p>Some 100,000 federal workers are set to formally quit the U.S. <a href="https://www.newsweek.com/topic/government">government</a> in what will constitute the largest mass <a href="https://www.newsweek.com/topic/resignation">resignation</a> of government workers in U.S. history.</p><p>The resignations—which come as part of a program drawn up by President Donald Trump at the start of his second administration—will happen on Tuesday as Congress is facing a deadline on the same day to authorize more funding or <a href="https://www.newsweek.com/vance-shutdown-warning-meeting-trump-schumer-jeffries-10801122">risk a government shutdown</a>. </p><p>If there is no deal, the White House has ordered federal agencies to make plans for the large-scale redundancies.</p><p><em>Newsweek </em>contacted the Office of Personnel Management by email outside of normal business hours to comment on this story.</p><h2><strong>Why It Matters</strong></h2><p>The loss of federal workers will have huge impacts on operations across different parts of government and could disrupt services including in the Department of Veteran Affairs and the Social Security Administration. </p><p>An increasing number of unemployed adults will also impact the economy, with a large number of people clamoring to secure jobs in the private sector at once.</p><div><p><img id="10802438" alt="" caption="A view of the U.S. Capitol on September 29, 2025 in Washington, DC." credit="(Photo by Anna Moneymaker/Getty Images)" sourcealt="" sources="[]" fetchpriority="auto" loading="lazy" width="6000" height="4000" decoding="async" data-nimg="1" srcset="https://assets.newsweek.com/wp-content/uploads/2025/09/GettyImages-2238123307.jpg?w=1600&amp;quality=75&amp;webp=1 1x" src="https://assets.newsweek.com/wp-content/uploads/2025/09/GettyImages-2238123307.jpg?w=1600&amp;quality=75&amp;webp=1"></p></div><h2><strong>What To Know</strong></h2><p>The resignations come as, since assuming office, the <a href="https://www.newsweek.com/topic/trump-administration">Trump administration</a> has made trimming down federal bureaucracy and streamlining services one of its key priorities. To that end, Trump created the Department of Government Efficiency (<a href="https://www.newsweek.com/topic/doge">DOGE</a>), an agency spearheaded by billionaire and Tesla CEO Elon Musk, until <a href="https://www.newsweek.com/elon-musk-statement-white-house-exit-doge-2078270">he left the White House </a>in May following disagreements with the President.</p><p>The administration also implemented&nbsp;<a href="https://www.newsweek.com/donald-trumps-purge-federal-workers-spark-mass-confusion-anger-2031729" target="_blank" rel="noreferrer noopener">hiring freezes and mass layoffs</a> and in February issued a directive giving federal employees the choice of accepting a resignation package which included salary payments until the end of September or risk future layoffs. </p><p>The planned 100,000 resignations come as a part of this program, known as the Deferred Resignation Program (DRP).</p><p>According to a Senate Democrat's report in July, DRP will cost $14.8 billion as 200,000 workers will be paid their full salary and benefits while on administrative leave for up to eight months.</p><p>There are 2.4 million federal workers in the U.S., according to the Bureau of Labor Statistics (BLS). Aside from resignations, while there is no official data revealing how many people have been fired,&nbsp;but <em>The New York Times</em>&nbsp;estimates the figure at 135,000.</p><p>Meanwhile,<em> Newsweek</em> reported that fired federal workers have suffered from poor mental health, <a href="https://www.newsweek.com/fired-federal-workers-employees-jobs-market-2087217">while others have struggled to find jobs in the private sector.</a></p><h2><strong>What People Are Saying</strong></h2><p><strong>Speaking to <em>Newsweek, </em>Scott Lucas, who teaches international politics at University College Dublin</strong> said the resignations were "technically a redundancy." He added that they "will have an affect on the unemployment rate in the United States so it's going to have a political effect because that spike will look bad for the Trump administration."</p><p>He added: "The Trump administration will affect government services, it will affect basic government functions."</p><p><strong>Writing in a post on LinkedIn, Next Interior, a group affiliated with government department the Department of the Interior, </strong>said the mass resignation: "represents a massive loss in institutional knowledge and capacity across the <a href="https://www.newsweek.com/topic/federal-government">federal government</a> - the expertise, relationships, and dedicated people that make it work for taxpayers."</p><p><strong>An August <a href="https://www.opm.gov/news/secrets-of-opm/what-they-got-wrong-about-the-deferred-resignation-program-1/">memo</a> by Scott Kupor, Director at the U.S. Office of Personnel management said</strong>: "We designed the DRP as a practical, humane, and voluntary option to accelerate workforce transitions in a system that desperately needed movement. Employees were given the option to retire early and receive eight months of paid leave; in return, the government will save $20+ billion in costs,&nbsp;annually."</p><p>It added: "At OPM we’re here to fix the decades of broken systems that have put us in this position. The Deferred Resignation Program was a necessary step toward a smarter, leaner, more effective government. If that ruffles a few feathers in Washington, so be it. The American people deserve a workforce built for performance, not permanence."</p><h2><strong>What Happens Next</strong></h2><p>It remains to be seen whether the government shutdown can be avoided.</p><p>Meanwhile, while 100,000 workers are set to leave the government, a Washington D.C. federal judge has temporarily blocked the Trump administration's plans to <a href="https://www.newsweek.com/republican-nominated-judge-blocks-trump-admin-kari-lake-in-new-order-10801633">cut over 500 jobs</a> at the U.S. Agency for Global Media, a government-funded news network.</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Frank Chimero: I think we're in the lemon stage of the internet (194 pts)]]></title>
            <link>https://frankchimero.com/blog/2025/selling-lemons/</link>
            <guid>45425746</guid>
            <pubDate>Tue, 30 Sep 2025 14:14:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://frankchimero.com/blog/2025/selling-lemons/">https://frankchimero.com/blog/2025/selling-lemons/</a>, See on <a href="https://news.ycombinator.com/item?id=45425746">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
			<div>
				<!-- <figure class="full" style="margin-top: calc(-1 * var(--unit-xxl))">
					<img src="/images/borderlands.png" style="object-fit: cover; max-height: 50dvh">
				</figure> -->
				<header>
					<div>
						
						<h2>The hidden costs of the meta game</h2>
						<p><time>September 30, 2025</time>
					</p></div>
				</header>
				<p>I’ve been researching a new talk the last few weeks and along the way stumbled across a concept that’s been rattling around in my head. I am writing to share, because I find it a satisfying description for the tech flop era.</p>

<p>The idea is called “a market for lemons.” The phrase comes from a <a href="https://en.wikipedia.org/wiki/The_Market_for_Lemons">1970 paper by George Akerlof</a> that explains how information asymmetry between buyers and sellers can undermine a marketplace. Akerlof asks us to imagine ourselves buying a used car. Some cars on the lot are reliable, well-maintained gems. Others cars are lemons, the kinds of cars that can make it off the lot but are disasters waiting to happen. The sellers know which cars are which, but you, as a buyer, can’t tell the difference. That information asymmetry affects the average price in the market and eventually impacts the overall market dynamics.</p>

<p>The thinking goes like this: if a buyer can’t distinguish between good and bad, everything gets priced somewhere in the middle. If you’re selling junk, this is fantastic news—you’ll probably get paid more than your lemon is worth. If you’re selling a quality used car, this price is insultingly low. As a result, people with good cars leave the market to sell their stuff elsewhere, which pushes the overall quality and price down even further, until eventually all that’s left on the market are lemons.</p>

<p>I think we’re in the lemon stage of the internet.</p>

<hr>

<p>I thought about this last week while shopping online for a sleep mask. Brands like MZOO, YFONG, WAOAW popped up, and these seemed less like companies and more like vowel smoke ejected from a factory flue hole, then slotted into a distribution platform. The long tail of generic brands on e-commerce platforms is a textbook lemons market: good products get drowned out by these alphabet soup products, who use their higher margins to buy sponsored placement in search results. Both buyers and sellers eventually lose (and perhaps the platforms win, as long as they don’t wear out their reputation).</p>

<p>For shoppers, buying online now feels like rolling the dice on the quality of the product. For sellers, the gamble is that their survival relies more on gaming the system than actually improving the product.</p>

<p>I think the post-pandemic experience has been a collective realization that the value that drew us to certain digital products and marketplaces is gone. Much of this reduction in value gets pinned to ZIRP, but there’s another critical factor—the natural flight of value creators. As platforms matured, the users and sellers who generated real value were squeezed out by players focused on capturing value rather than creating it.</p>

<p>Once you identify a lemon market, you start to see it all over the place.</p>

<p><em>Online dating.</em> A lemon market where participants have no familiarity with one another participate in strategic self-presentation. High-quality partners (emotionally available, looking for genuine connection) can’t effectively distinguish themselves from those just seeking validation and eventually leave.</p>

<p><em>Search results.</em> A lemon market where platforms profit from sponsored placement, misaligning incentives with user needs. The first page is a minefield: sponsored listings posing as organic results, SEO content farms, affiliate aggregators. You add “reddit” to work around this, but even that has less success these days.</p>

<p><em>Social media.</em> Your feed is now professional content creators, low-effort podcast video clips, algorithmic filler reaction videos, stand-up chaff, and animals. Good ideas don’t happen frequently enough to satisfy the pace of the algorithm, so many have pivoted to newsletters or stopped posting.</p>

<p>What makes the Market for Lemons concept so appealing (and what differentiates it in my mind from <a href="https://en.wikipedia.org/wiki/Enshittification">enshittification</a> is that everyone can be acting reasonably, pursuing their own interests, and things still get worse for everyone. No one has to be evil or stupid: the platform does what’s profitable, sellers do what works, buyers try to make smart decisions, and yet the whole system degrades into something nobody actually wants.</p>

<hr>

<p>I was first introduced to the Market of Lemons by Dan Luu in an essay titled, <a href="https://danluu.com/nothing-works/">Why is it so hard to buy things that work well?</a>. Luu applies the market of lemons as a metaphor, and specifically identifies hiring as a market of lemons, because of the <a href="https://danluu.com/hiring-lemons/">information asymmetry for both companies and individuals</a>.</p>

<p>Companies have always struggled to tell the difference between great individual contributors and mediocre ones. Lacking a clear way to separate the two, they lump everyone together and rely on proxy games to evaluate skill. Candidates, for their part, walk into interviews without crucial information: whether the company is quietly dysfunctional, whether the manager they liked during interviews is about to quit, or whether the open role itself is little more than a vestige of an abandoned strategy that’s likely to be cut once the other foot drops. The usual signals of strength or weakness don’t signify much at all when it comes to hiring. Layer on the automated scale of the application process—candidates firing off applications by the hundreds, companies screening by the thousands—and the result is a highly inefficient market that wastes everyone’s time. Meaningful signals get drowned out, everyone gets lumped together, rational players opt out to the extent they are able, and the market slides steadily downward.</p>

<p>There have been countless attempts to make hiring more rational and efficient—the stuff of startup pitch deck lore. But I’m not sure hiring can ever be much more efficient, because neither side has reason to show themselves as they really are, warts and all. Idealistically, both would come straight; pragmatically, it is a game of chicken. Candidates polish résumés and present curated versions of their abilities, listing outcomes and impact statistics with dubious accuracy and provenance. Companies do the same, putting culture and mission front and center while hiding systematic dysfunctions and looming existential risks. When neither side is forthcoming, you’re left with proxies: a famous logo on a resume, a polished culture deck. Gaming the meta of the system supersedes the actual development or evaluation of skill. And, much to my disappointment, gaming the meta may, in fact, be an essential aspect of most jobs.</p>

<hr>

<p>At this point, it should be obvious how the market for lemons applies to ill-considered AI-generated content. I’ll let you sketch out that argument yourself since it’s fairly straightforward, and this thing is already long enough.</p>

<p>Instead, let’s zag and revisit my point earlier about system-gaming becoming the most viable playbook instead of focusing on the product. As a consumer and as a designer, I hope this is a temporary state before a massive recalibration. The primacy of meta-activities—optimizing for algorithms, visibility theater, consumer entrapment, externalization of costs, performative internal alignment, horse-trading amongst a set of DOA ideas—is poison. It is a road to nowhere worth going.</p>

<p>This reflects a business culture obsessed with outcomes while treating outputs as speed bumps. But outputs (code, design, the products themselves) are the load-bearing work—the actual prerequisites for the outcomes desired. Focusing on outcomes while ignoring outputs means hiding in abstractions and absolving oneself of accountability. If any output is acceptable to hit your targets, what awful things emerge at scale? What horrors happen when success detaches completely from the necessity of being good—having both skill and ethics?</p>

<p>The safest, smartest path is also the most mundane: keep the main thing the main thing. Outcomes matter, but output literally comes first. Outputs are the business to everyone outside it—what customers see, buy, and use. You can’t stay safe in abstractions forever. Eventually, you must attend to the reality of what’s in front of you, because that’s where work gets done and where assumptions get validated or falsified (because <a href="http://johnsalvatier.org/blog/2017/reality-has-a-surprising-amount-of-detail">reality has a surprising amount of detail</a>).</p>

<p>In other words, the meta ruins things for everyone. To hide in abstractions is to dodge the reality of your choices. These tactics may get you profit, but you sacrifice benefit. The climb may feel like progress, but at the end you’ll find yourself at the top of a mountain of lemons, perhaps not of your own making, but almost certainly of your own doing.</p>

			</div>
		</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Software essays that shaped me (131 pts)]]></title>
            <link>https://refactoringenglish.com/blog/software-essays-that-shaped-me/</link>
            <guid>45425568</guid>
            <pubDate>Tue, 30 Sep 2025 14:01:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://refactoringenglish.com/blog/software-essays-that-shaped-me/">https://refactoringenglish.com/blog/software-essays-that-shaped-me/</a>, See on <a href="https://news.ycombinator.com/item?id=45425568">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><article><header></header><p>I started reading software blogs before I got my first programming job 20 years ago. At this point, I’ve read thousands of blog posts and essays about software, but only a small handful stuck in my mind and changed the way I think.</p><ol><li><a href="#-by-joel-spolsky-2000">“The Joel Test: 12 Steps to Better Code” by Joel Spolsky (2000)</a></li><li><a href="#-by-alexis-king-2019">“Parse, don’t validate” by Alexis King (2019)</a></li><li><a href="#-by-fred-brooks-1986">“No Silver Bullet - Essence and Accident in Software Engineering” by Fred Brooks (1986)</a></li><li><a href="#-by-joel-spolsky-2000-1">“Choices” by Joel Spolsky (2000)</a></li><li><a href="#-by-raymond-chen-2010">“Application compatibility layers are there for the customer, not for the program” by Raymond Chen (2010)</a></li><li><a href="#-by-erik-kuefler-2014">“Don’t Put Logic in Tests” by Erik Kuefler (2014)</a></li><li><a href="#-by-julia-evans-2020">“A little bit of plain Javascript can do a lot” by Julia Evans (2020)</a></li><li><a href="#-by-dan-mckinley-2015">“Choose Boring Technology” by Dan McKinley (2015)</a></li><li><a href="#-by-terence-eden-2022">“I’ve locked myself out of my digital life” by Terence Eden (2022)</a></li><li><a href="#bonus-brad-fitzpatrick-on-parsing-user-input-2009">Bonus: Brad Fitzpatrick on parsing user input (2009)</a></li></ol><h2 id="-by-joel-spolsky-2000"><a href="https://www.joelonsoftware.com/2000/08/09/the-joel-test-12-steps-to-better-code/">“The Joel Test: 12 Steps to Better Code”</a> by Joel Spolsky (2000)<a href="#-by-joel-spolsky-2000">🔗</a></h2><p>Joel Spolsky is the greatest software blogger of all time. His essays have informed so much of my approach to software that it was hard to pick out just one, but “The Joel Test” is my favorite.</p><p>The Joel Test is a set of 12 questions that employers can ask themselves to see how well they’re investing in their software team:</p><blockquote><ol><li>Do you use source control?</li><li>Can you make a build in one step?</li><li>Do you make daily builds?</li><li>Do you have a bug database?</li><li>Do you fix bugs before writing new code?</li><li>Do you have an up-to-date schedule?</li><li>Do you have a spec?</li><li>Do programmers have quiet working conditions?</li><li>Do you use the best tools money can buy?</li><li>Do you have testers?</li><li>Do new candidates write code during their interview?</li><li>Do you do hallway usability testing?</li></ol></blockquote><p>Some of the questions are dated, but the point was never the questions themselves but rather the meta-point of the questions.</p><p>Joel was really asking employers: <strong>do you respect developers?</strong></p><p>The questions all assess whether an employer prioritizes their developers’ time and focus over things like cheap office space and short-term deadlines.</p><p>Joel published this article at the height of the dot-com boom, when skilled developers were a precious resource, but not everyone realized it, including developers themselves.</p><p>Joel’s blog always presented programmers as rare, delicate geniuses that employers needed to pursue and pamper. I liked that.</p><p>Throughout my career, I sought out employers that scored well on the Joel test, and I’m grateful to Joel for giving me the map to find them.</p><h2 id="-by-alexis-king-2019"><a href="https://lexi-lambda.github.io/blog/2019/11/05/parse-don-t-validate/">“Parse, don’t validate”</a> by Alexis King (2019)<a href="#-by-alexis-king-2019">🔗</a></h2><p>This essay is about leveraging the type system in Haskell to — wait, wait! Don’t go to sleep.</p><p>If you don’t care about type systems or Haskell, I get it. I don’t either. But this essay radically changed the way I think about software. You can use Alexis’ technique outside of Haskell in any language that supports static types (e.g., Go, C++, Rust).</p><p>The highly abridged version of the essay is that whenever you validate any data, you should convert it to a new type.</p><p>Suppose that your app has a rule limiting usernames to a maximum of 20 alphanumeric characters. The naïve solution would be to define a function that looks like this:</p><div><pre tabindex="0"><code data-lang="go"><span><span><span>func</span> <span>validateUsername</span>(username <span>string</span>) <span>error</span> { ... }
</span></span></code></pre></div><p>With the above function, you run <code>validateUsername</code> anytime you receive a username from a user.</p><p>The problem with this approach is that your code is unsafe by default. You have to remember to validate every username you receive, so it’s easy to create a code path that accidentally processes a username without validating it. And if a nefarious user notices the mistake, they can do tricky things like embed malicious code in the username field or stuff it with a billion characters to fill up your database.</p><p>Alexis’ solution is to instead use a function like this:</p><div><pre tabindex="0"><code data-lang="go"><span><span><span>func</span> <span>parseUsername</span>(raw <span>string</span>) (Username, <span>error</span>) { ... }
</span></span></code></pre></div><p>In the rest of your codebase, instead of passing around a <code>string</code> called “username,” you use a custom type: <code>Username</code>. The only function that can create a <code>Username</code> is <code>parseUsername</code>, and it applies validation rules before returning a <code>Username</code> instance.</p><p>Therefore, if you have a <code>Username</code> instance, it must contain a valid username. Otherwise, it couldn’t exist.</p><p>You can’t forget to validate a username because untrusted input will always be a <code>string</code>, and you can’t pass a <code>string</code> to a function that expects a <code>Username</code>.</p><p>Before Alexis’ essay, I thought type systems were just a fun way to distract language nerds. “Parse, don’t validate” opened my eyes to how valuable compiler features can be in improving an application’s security and reliability.</p><h2 id="-by-fred-brooks-1986"><a href="https://www.cs.unc.edu/techreports/86-020.pdf">“No Silver Bullet - Essence and Accident in Software Engineering”</a> by Fred Brooks (1986)<a href="#-by-fred-brooks-1986">🔗</a></h2><p>In college, I read <em>The Mythical Man-Month</em>, a collection of essays about software engineering by Fred Brooks, drawing on his experience directing <a href="https://en.wikipedia.org/wiki/OS/360_and_successors">IBM’s OS/360 project</a>.</p><p>The essays were hit or miss. Some felt too old to be relevant, even in 2002, but the one that stuck with me was, “No Silver Bullet.”</p><p>The essay argues that you can divide software work into two categories: essential complexity and accidental complexity.</p><p><strong>Essential complexity</strong> is the work that you absolutely have to do, regardless of your tooling and hardware. For example, if you write software that calculates bonuses for salespeople, you have to define formulas for those bonuses and cover all possible edge cases. This work is the same if you have a $5B supercomputer or a $1 microcontroller.</p><p><strong>Accidental complexity</strong> is everything else: dealing with memory leaks, waiting for your code to compile, figuring out how to use a third-party library. The better your tooling and hardware resources, the less time you spend on accidental complexity.</p><p>Given this model, Brooks concluded that it was impossible for any advancement in tooling or hardware to create a 10x improvement in developer productivity:</p><blockquote><p>How much of what software engineers now do is still devoted to the accidental, as opposed to the essential? Unless it is more than 9/10 of all effort, shrinking all the accidental activities to zero time will not give an order of magnitude improvement.</p></blockquote><p>Throughout my career, people have been trying to find ways to eliminate programmers from software. For a few years, no-code platforms generated buzz by promising non-programmers all the powers of a seasoned web developer.</p><p>Brooks’ essay always reassured me that the latest buzzword platforms could never replace developers, as the platforms focused on the accidental, not the essential. Even if the platforms could magically create working code from a functional specification, you still need someone to write the spec:</p><blockquote><p>I believe the hard part of building software to be the specification, design, and testing of this conceptual construct, not the labor of representing it and testing the fidelity of the representation.</p></blockquote><p>Modern AI has thrown a wrench into Brooks’ theory, as it actually <em>does</em> reduce essential complexity. You can hand AI an incomplete or contradictory specification, and the AI will fill in the gaps by cribbing from similar specifications.</p><p>Even if AI eliminates programming as we know it, Brooks’ essay gives me hope that we’ll still need people to manage essential complexity at whatever level of abstraction that ends up being.</p><h2 id="-by-joel-spolsky-2000-1"><a href="https://www.joelonsoftware.com/2000/04/12/choices/">“Choices”</a> by Joel Spolsky (2000)<a href="#-by-joel-spolsky-2000-1">🔗</a></h2><p>I said <a href="#-by-joel-spolsky-2000">above</a> that it was hard to pick a single favorite Joel Spolsky essay, which is why I’ve chosen two.</p><p>“Choices” is about creating user interfaces and the subtle costs of giving a user power:</p><blockquote><p><strong>Every time you provide an option, you’re asking the user to make a decision.</strong> That means they will have to think about something and decide about it. It’s not necessarily a bad thing, but, in general, you should always try to minimize the number of decisions that people have to make.</p></blockquote><p>As an example, Joel shares a ridiculous dialog that appears in Windows 98 when you try to search the help documentation:</p><p><a href="https://refactoringenglish.com/blog/software-essays-that-shaped-me/Stupidest_Dialog_Ever.gif"><img sizes="(min-width: 768px) 470px, 98vw" srcset="https://refactoringenglish.com/blog/software-essays-that-shaped-me/Stupidest_Dialog_Ever.gif 470w" src="https://refactoringenglish.com/blog/software-essays-that-shaped-me/Stupidest_Dialog_Ever.gif" alt="" loading="lazy"></a></p><p>The dialog infuriates Joel because it interrupts the user while they’re trying to get help, and it asks them to make an uninformed about database optimization. Windows was shirking a decision and pushing it onto the user.</p><p>Joel’s essay focuses on graphical user interfaces, but I think about it wherever people might encounter my code, including on the command-line or other developers calling functions I wrote. Can I make a useful decision on my user’s behalf while still giving them power over things they care about? There are countless times where Joel’s essay has saved me from pushing a decision onto the user that I could make myself.</p><h2 id="-by-raymond-chen-2010"><a href="https://devblogs.microsoft.com/oldnewthing/20100311-00/?p=14643">“Application compatibility layers are there for the customer, not for the program”</a> by Raymond Chen (2010)<a href="#-by-raymond-chen-2010">🔗</a></h2><p>Raymond Chen is one of the longest-serving developers on the Microsoft Windows team. His blog has thousands of informative, entertaining stories about the history of Windows programming, but the one I think back to most is one about compatibility mode in Windows Vista.</p><p>A customer had contacted Raymond’s team with this request:</p><blockquote><p>Hi, we have a program that was originally designed for Windows XP and Windows Server 2003, but we found that it runs into difficulties on Windows Vista. We’ve found that if we set the program into Windows XP compatibility mode, then the program runs fine on Windows Vista. What changes do we need to make to our installer so that when the user runs it on Windows Vista, it automatically runs in Windows XP compatibility mode?</p></blockquote><p>Raymond proceeds to characterize the customer’s request as follows:</p><blockquote><p>I normally toss my garbage on the sidewalk in front of the pet store, and every morning, when they open up, somebody sweeps up the garbage and tosses it into the trash. But the pet store isn’t open on Sundays, so on Sundays, the garbage just sits there. How can I get the pet store to open on Sundays, too?</p></blockquote><p>I loved this analogy. The metaphor was so funny that I didn’t realize until just now that Raymond is in the wrong. He’s making fun of a developer whose sin is expecting Windows not to break their app after a single release.</p><p>But as is the case with a lot of Raymond Chen’s writing, it’s so funny and sharp that I can look past the flaws.</p><p>Even though I disagree with the specifics, Raymond’s post is an excellent lesson in influencing user behavior.</p><p>If you want to nudge the user to do something that helps you, think carefully about the path of least resistance from the user’s perspective, because that’s the path they’ll take.</p><p>If you show the user that dumping garbage on the sidewalk completely solves their problem, they’re going to keep dumping their garbage on the sidewalk.</p><h2 id="-by-erik-kuefler-2014"><a href="https://testing.googleblog.com/2014/07/testing-on-toilet-dont-put-logic-in.html">“Don’t Put Logic in Tests”</a> by Erik Kuefler (2014)<a href="#-by-erik-kuefler-2014">🔗</a></h2><p>I’ve always loved unit testing and took great pride in my test code. That’s why I was so horrified when this essay <a href="https://testing.googleblog.com/2024/12/tech-on-toilet-driving-software.html">appeared in my bathroom</a> and revealed that I’d been writing awful tests my whole career.</p><p>Erik’s essay shows the following unit test, which has a subtle bug:</p><div><pre tabindex="0"><code data-lang="java"><span><span>@Test <span>public</span> <span>void</span> <span>shouldNavigateToPhotosPage</span>() {
</span></span><span><span>  String baseUrl = <span>"http://plus.google.com/"</span>;
</span></span><span><span>  Navigator nav = <span>new</span> Navigator(baseUrl);
</span></span><span><span>  nav.goToPhotosPage();
</span></span><span><span>  assertEquals(baseUrl + <span>"/u/0/photos"</span>, nav.getCurrentUrl());
</span></span><span><span>}
</span></span></code></pre></div><p>When I first read the essay, I thought, “That’s exactly how I write unit tests!”</p><p>Why duplicate the <code>http://plus.google.com/</code> string in two places? Create a single source of truth, just like in production code. I did this all the time, adding helper functions, variables, and loops to eliminate redundancy from my tests.</p><p>The problem with the approach above is that it masks a subtle bug. It’s actually asserting that the URL looks like this:</p><div><pre tabindex="0"><code data-lang="text"><span><span>http://plus.google.com//u/0/photos
</span></span><span><span>                      ^^
</span></span><span><span>                    whoops
</span></span></code></pre></div><p>Erik’s essay made me realize that I shouldn’t treat test code like production code at all. The two have <a href="https://mtlynch.io/good-developers-bad-tests/#test-code-is-not-like-other-code">completely different goals and constraints</a>.</p><p>Good test code should be, above all, clear. Test code doesn’t have its own test code, so the only way to verify correctness is by inspection. A test should make it blindingly obvious to the reader what behavior it asserts. In service of that goal, you can accept redundancy to reduce complexity.</p><h2 id="-by-julia-evans-2020"><a href="https://jvns.ca/blog/2020/06/19/a-little-bit-of-plain-javascript-can-do-a-lot/">“A little bit of plain Javascript can do a lot”</a> by Julia Evans (2020)<a href="#-by-julia-evans-2020">🔗</a></h2><p>As a software engineer, I was embarrassingly late to the web. For the first 10 years of my career, I only wrote code for desktop apps and backend servers. I never bothered with HTML or JavaScript until 2017.</p><p>By the time I got serious about learning frontend development, my impression was that JavaScript was a mess of a language, <a href="https://www.computer.org/csdl/magazine/co/2012/02/mco2012020007/13rRUy08MzA">hacked together in 10 days</a>, and it had drastically different behavior in different browsers. If I was going to write web apps, I wanted something modern and sleek to protect me from all of JavaScript’s bile and warts.</p><p>So, I tried the popular web frameworks of the day: Angular, React, and Vue. I learned enough Vue to make my way around, but I was still spending an enormous amount of my time on dependency issues and framework gotchas. After all the work these frontend frameworks did to fix JavaScript, web programming still sucked.</p><p>Then, I read Julia’s essay, and I realized I’d been so confident that JavaScript needed fixing that I never gave it a chance.</p><p>At the time, I was working on <a href="https://mtlynch.io/tinypilot/">the prototype of TinyPilot</a>, which would become my first commercially successful software product. TinyPilot had a web interface that I was planning to implement with Vue, but Julia’s essay inspired me to see how far I could go with plain JavaScript. No framework, no wrapper libraries, no build step, no Node.js, just regular old JavaScript. Okay, not “old” — more like <a href="https://en.wikipedia.org/wiki/ECMAScript_version_history#9th_edition_%E2%80%93_ECMAScript_2018">ES2018</a>, but you know.</p><p>I kept expecting to hit some problem where I’d need to switch to some kind of framework or builder, but it never happened. There were still some gotchas, especially around WebComponents, but it was nothing compared to the suffering I endured with Vue and Angular.</p><p>I loved being free of the frameworks. When I had a runtime error, the stack trace wasn’t some minified, transmogrified, tree-shakified fever dream of my code. I was debugging <em>my code</em>, exactly as I wrote it. Why hadn’t I tried this sooner?</p><p>My biases about JavaScript were wrong. Modern JavaScript is pretty nice. It absorbed a lot of ideas from wrapper libraries, so now you don’t need the wrappers. And browsers got their act together to ensure consistent behavior across platforms and devices.</p><p>I haven’t integrated a JavaScript framework or build step into any new project since 2020, and I’ve never looked back. Plain JavaScript gets me 90% of the benefit of frameworks with 5% of the headache.</p><h2 id="-by-dan-mckinley-2015"><a href="https://mcfunley.com/choose-boring-technology">“Choose Boring Technology”</a> by Dan McKinley (2015)<a href="#-by-dan-mckinley-2015">🔗</a></h2><p>This is an odd essay to include in this list because I’ve never actually read it.</p><p>People have quoted this essay to me, and once I understood the idea, it felt so intuitive that I didn’t need to read it. In my interview with <a href="https://corecursive.com/">CoRecursive podcast</a> host Adam Gordon Bell, he talked about how there are certain non-fiction books where, once you understand the idea, <a href="https://refactoringenglish.com/blog/interview-adam-gordon-bell/#crafting-blog-post-titles">all you need is the title</a>. “Choose Boring Technology” is that for me.</p><p>Dan’s argument is that when you start a new project, you’re tempted to use cutting-edge technology that has lots of buzz. Google just announced a new database that scales to exabytes, and it’s 40% faster than Postgres at 20% the cost. You’d be an idiot to use Postgres when this sexy new alternative is right there!</p><p>In practice, the new technology has bugs and weaknesses, but they’re not obvious to you yet; they’re not obvious to anyone yet. So, when you run into them, you’re stuck. Postgres has its issues, but after 30 years in the field, it has battle-tested solutions for any problem you’re likely to encounter.</p><p>Dan concedes that you should use new technologies sometimes but only strategically and in limited quantities. He suggests that every business gets three “innovation tokens” to spend. If you want a flashy new database, you’ll have to spend one of your tokens.</p><p>Dan’s essay dovetails naturally with Julia’s essay. I wish I’d read either of them before I wasted all that time with frontend frameworks.</p><h2 id="-by-terence-eden-2022"><a href="https://shkspr.mobi/blog/2022/06/ive-locked-myself-out-of-my-digital-life/">“I’ve locked myself out of my digital life”</a> by Terence Eden (2022)<a href="#-by-terence-eden-2022">🔗</a></h2><p>Terence Eden is a delightful and eclectic technology blogger. He writes several new posts each week, but the one that impacted me the most was “I’ve locked myself out of my digital life.”</p><p>The article plays out what would happen if lightning struck Terence’s house and destroyed all of his possessions. He keeps his passwords to everything in a password manager, but if all his devices get destroyed, he can’t access his password manager. And he can’t fall back to hardware passkeys because those were in his house, too.</p><p>I always felt like I was pretty safe about my data because I store everything on redundant drives, and I have offsite backups on three continents with two vendors.</p><p>Terence’s post got me thinking about the many credible threats that could wipe out all of my devices simultaneously: fire, flood, electrical surge, criminal investigation. All of my data is encrypted with passwords that live in my head, so add to that list memory loss, incapacitation, or death.</p><p>Online services are bad at helping users recover from disaster. I use several services that assume it’s impossible for me to ever lose my phone, let alone my email account and every digital device in my possession.</p><p>Ever since I read Terence’s essay, I’ve been thinking more about which services and devices are critical to me, and how I could recover from a scenario like the one Terence described. The next time I bought a laptop, I set it up at the library to test whether I could recover access to my password manager and critical accounts without any of the devices in my house.</p><p>I still could do a better job at digital disaster preparedness, but Terence’s post always echoes in my head whenever I think about how to secure my devices and data. What if everything was suddenly destroyed?</p><h2 id="bonus-brad-fitzpatrick-on-parsing-user-input-2009">Bonus: Brad Fitzpatrick on parsing user input (2009)<a href="#bonus-brad-fitzpatrick-on-parsing-user-input-2009">🔗</a></h2><p>It’s technically not an essay, but there’s a quote from a software interview I constantly think about.</p><p>In 2009, as a result of <a href="https://www.joelonsoftware.com/2009/09/23/the-duct-tape-programmer/">Joel Spolsky’s gushing review</a>, (yes, again with the Joel), I read <a href="https://codersatwork.com/"><em>Coders at Work</em></a>, a collection of interviews with accomplished programmers.</p><p><a href="https://en.wikipedia.org/wiki/Brad_Fitzpatrick">Brad Fitzpatrick</a>, creator of <a href="https://en.wikipedia.org/wiki/LiveJournal">LiveJournal</a> and <a href="https://en.wikipedia.org/wiki/Memcached">Memcached</a>, appears in the book as one of the interviewees. He was only 28 years old at the time, the youngest programmer in the book and also the sweariest and most entertaining.</p><p>In response to a question about ethics in software engineering, Brad goes on an impassioned rant about input validation:</p><blockquote><p>I would like to ask that everyone is consistent on their credit-card forms to like let me put in fucking spaces or hypens. Computers are good at removing that shit. Like don’t tell me how to format my numbers.</p><p>-Brad Fitzpatrick, in <em>Coders at Work</em></p></blockquote><p>I think back to this quote whenever I try to paste a phone number into a web form, and it whines that parentheses or spaces aren’t allowed. Or worse, it truncates my phone number because of the parentheses, and <em>also</em> complains that parentheses aren’t allowed.</p><p>Whenever I create input fields in my software and think about unexpected characters, I hear Brad Fitzpatrick say, “Computers are good at removing that shit.”</p></article></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[dgsh – Directed Graph Shell (141 pts)]]></title>
            <link>https://www2.dmst.aueb.gr/dds/sw/dgsh/</link>
            <guid>45425298</guid>
            <pubDate>Tue, 30 Sep 2025 13:39:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/">https://www2.dmst.aueb.gr/dds/sw/dgsh/</a>, See on <a href="https://news.ycombinator.com/item?id=45425298">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<!-- About {{{1
================================================== -->
<section id="intro">
  
<p>
The directed graph shell, <em>dgsh</em>
(<a href="https://en.wiktionary.org/wiki/Appendix:English_pronunciation">pronounced</a> /dæɡʃ/ — <em>dagsh</em>),
provides an expressive way to construct
sophisticated and efficient big data set and stream processing pipelines
using existing Unix tools as well as custom-built components.
It is a Unix-style shell (based on <em>bash</em>)
allowing the specification of pipelines
with non-linear non-uniform operations.
These form a directed acyclic process graph,
which is typically executed by multiple processor cores,
thus increasing the operation's processing throughput.
</p>
<p>
If you want to get a feeling on how <em>dgsh</em> works in practice,
skip right down to the <a href="#examples">examples</a> section.
</p>
<p>
For a more formal introduction to <em>dgsh</em> or to cite it in your work,
see:<br>
Diomidis Spinellis and Marios Fragkoulis.
<a href="http://dx.doi.org/10.1109%2FTC.2017.2695447">Extending Unix Pipelines to DAGs</a>.
<em>IEEE Transactions on Computers</em>, 2017.
doi: 10.1109/TC.2017.2695447
</p>
</section> <!-- Introduction -->

<section id="ipc"> <!-- {{{2 -->
<h2>Inter-process communication</h2>
<p>
<em>Dgsh</em> provides two new ways
for expressing inter-process communication.

</p>
<dl>
<dt>Multipipes</dt><dd> are expressed as usual Unix pipelines,
but can connect commands with more than one output or input channel.
As an example, the <code>comm</code> command supplied with <em>dgsh</em>
expects two input channels and produces on its output three
output channels: the lines appearing only in first (sorted) channel,
the lines appearing only in the second channel,
and the lines appearing in both.
Connecting the output of the <code>comm</code> command to the
<code>cat</code> command supplied with <em>dgsh</em>
will make the three outputs appear in sequence,
while connecting it to the
<code>paste</code> command supplied with <em>dgsh</em>
will make the output appear in its customary format.
</dd>

<dt>Multipipe blocks {{ ... }}</dt><dd>
a) send (multiple) input streams
received on their input side to the asynchronously-running
processes that reside within the block, and,
b) pass the output produced by the processes within the block as
(multiple) streams on their output side.
Multipipe blocks typically receive input from more than one channel
and produce more than one output channel.
For example, a multipipe block that runs <code>md5sum</code> and <code>wc -c</code>
receives two inputs and produces two outputs:
the MD5 hash of its input and the input's size.
Data to multipipe blocks are typically provided with a
<em>dgsh</em>-aware version of <code>tee</code> and collected by
<em>dgsh</em>-aware versions of programs such as
<code>cat</code> and <code>paste</code>.
</dd>

<dt>Stored values</dt> <dd>offer a convenient way for communicating
computed values between arbitrary processes on the graph.
They allow the storage of a data stream's
last record into a named buffer.
This record can be later retrieved asynchronously by one or more readers.
Data in a stored value can be piped into a process or out of it, or it can be read
using the shell's command output substitution syntax.
Stored values are implemented internally through Unix-domain sockets,
a background-running store program, <code>dgsh-writeval</code>, and
a reader program, <code>dgsh-readval</code>.
The behavior of a stored value's IO can be modified by adding flags to
<code>dgsh-writeval</code> and <code>dgsh-readval</code>.
</dd>
</dl>
</section>

<section id="syntax"> <!-- {{{2 -->
<h2>Syntax</h2>
<p>
A <em>dgsh</em> script follows the syntax of a <em>bash</em>(1) shell
script with the addition of <em>multipipe</em> blocks.
A multipipe block contains one or more <em>dgsh</em> simple commands,
other multipipe blocks, or pipelines of the previous two types of commands.
The commands in a multipipe block
are executed asynchronously (in parallel, in the background).
Data may be redirected or piped into and out of a multipipe block.
With multipipe blocks <em>dgsh</em> scripts form directed acyclic process graphs.
It follows from the above description that
multipipe blocks can be recursively composed.
</p>

<p>
As a simple example consider running the following command
directly within <em>dgsh</em>
</p>
<pre>{{ echo hello &amp; echo world &amp; }} | paste
</pre>
<p>
or by invoking <code>dgsh</code> with the command as an argument.
</p>
<pre>dgsh -c '{{ echo hello &amp; echo world &amp; }} | paste'
</pre>
<p>
The command will run <em>paste</em> with input from the two
<em>echo</em> processes to output <code>hello world</code>.
This is equivalent to running the following <em>bash</em> command,
but with the flow of data appearing in the natural left-to-right order.
</p>
<pre>paste &lt;(echo hello) &lt;(echo world)
</pre>

<p>
In the following larger example, which compares the performance of
different compression utilities, the script's standard input
is distributed to
three compression utilities (<em>xz</em>, <em>bzip2</em>, and <em>gzip</em>),
to assess their performance, and also to
<em>file</em> and <em>wc</em> to report the input data's type and size.
The <em>printf</em> commands label the data of each processing type.
All eight commands pass their output
to the <code>cat</code> command, which gathers their outputs
in order.
</p>

<pre>tee |
{{
	printf 'File type:\t'
	file -

	printf 'Original size:\t'
	wc -c

	printf 'xz:\t\t'
	xz -c | wc -c

	printf 'bzip2:\t\t'
	bzip2 -c | wc -c

	printf 'gzip:\t\t'
	gzip -c | wc -c
}} |
cat
</pre>

<p>
Formally, <em>dgsh</em> extends the syntax of the (modified) Unix Bourne-shell
when <code>bash</code> provided with the <code>--dgsh</code> argument
as follows.
</p>

<pre>&lt;dgsh_block&gt;     ::= '{{' &lt;dgsh_list&gt; '}}'

&lt;dgsh_list&gt;      ::= &lt;dgsh_list_item&gt; '&amp;'
                 &lt;dgsh_list_item&gt; &lt;dgsh_list&gt;

&lt;dgsh_list_item&gt; ::= &lt;simple_command&gt;
                 &lt;dgsh_block&gt;
                 &lt;dgsh_list_item&gt; '|' &lt;dgsh_list_item&gt;
</pre>
</section> <!-- syntax -->

<section id="tools"> <!-- {{{2 -->
<h2>Adapted tools</h2>
<p>
A number of Unix tools have been adapted to support multiple inputs
and outputs to match their natural capabilities.
This echoes a similar adaptation that was performed in the early
1970s when Unix and the shell got pipes and the pipeline syntax.
Many programs that worked with files were adjusted to work as filters.
The number of input and output channels of <em>dgsh</em>-compatible commands are
as follows, based on the supplied command-line arguments.
</p>
<table>
	<tbody><tr>
		<th>Tool</th>
		<th>Inputs</th>
		<th>Outputs</th>
		<th>Notes</th>
	</tr>
	<tr>
		<td>cat (<em>dgsh-tee</em>)</td>
		<td>0—N</td>
		<td>0—M</td>
		<td>No options are supported</td>
	</tr>
	<tr>
		<td>cmp</td>
		<td>0—2</td>
		<td>0—1</td>
		<td></td>
	</tr>
	<tr>
		<td>comm</td>
		<td>0—2</td>
		<td>0—3</td>
		<td>Output streams in order: lines only in first file, lines only in second one, and lines in both files</td>
	</tr>
	<tr>
		<td>cut</td>
		<td>0—1</td>
		<td>1—N</td>
		<td>With <code>--multistream</code> output each range into a different stream</td>
	</tr>
	<tr>
		<td>diff</td>
		<td>0—N</td>
		<td>1</td>
		<td>Typically two inputs. Compare an arbitrary number of input streams with the <code>--from-file</code> or <code>--to-file</code> options</td>
	</tr>
	<tr>
		<td>diff3</td>
		<td>0—3</td>
		<td>1</td>
		<td></td>
	</tr>
	<tr>
		<td>grep</td>
		<td>0—2</td>
		<td>0—4</td>
		<td>Available output streams (via arguments): matching files, non-matching files, matching lines, and non-matching lines</td>
	</tr>
	<tr>
		<td>join</td>
		<td>0—2</td>
		<td>1</td>
		<td></td>
	</tr>
	<tr>
		<td>paste</td>
		<td>0—N</td>
		<td>1</td>
		<td>Paste N input streams</td>
	</tr>
	<tr>
		<td>perm</td>
		<td>1—N</td>
		<td>1—N</td>
		<td>Rearrange the order of N input streams</td>
	</tr>
	<tr>
		<td>sort</td>
		<td>0—N</td>
		<td>0—1</td>
		<td>With the <code>-m</code> option, merge sort N input streams</td>
	</tr>
	<tr>
		<td>tee (<em>dgsh-tee</em>)</td>
		<td>0—N</td>
		<td>0—M</td>
		<td>Only the <code>-a</code> option is supported</td>
	</tr>
	<tr>
		<td>dgsh-readval</td>
		<td>0</td>
		<td>1</td>
		<td>Read a value from a socket</td>
	</tr>
	<tr>
		<td>dgsh-wrap</td>
		<td>0—N</td>
		<td>0—1</td>
		<td>Wrap non-dgsh commands and negotiate on their behalf</td>
	</tr>
	<tr>
		<td>dgsh-writeval</td>
		<td>1</td>
		<td>0</td>
		<td>Write a value to a socket</td>
	</tr>
</tbody></table>

<p>
In addition, POSIX user commands that receive no input
or only generate no output, when executed in a <em>dgsh</em> context
are wrapped to specify the corresponding input or output capability.
For example, an <code>echo</code> command in a multipipe block
will appear to receive no input, but will provide one output stream.
By default <code>dgsh</code> automatically wraps all other
commands as filters.
</p><dl>
<dt> Input-only </dt><dd>
read,
write.
</dd>
<dt> Output-only </dt><dd> </dd>
alias,
ar,
basename,
c99,
cal,
cflow,
command,
date,
df,
dirname,
du,
echo,
expr,
find,
getopts,
ipcrm,
jobs,
ls,
make,
man,
printf,
ps,
pwd,
tty,
type,
ulimit,
umask,
uname,
what,
who.
</dl>
<dt> No input and output </dt><dd> </dd>
dbg,
dcd,
dchgrp,
dchmod,
dchown,
dcp,
denv,
dfalse,
dfg,
dkill,
dlink,
dln,
dmesg,
dmkdir,
dmkfifo,
dmv,
dnewgrp,
dnice,
dnohup,
drenice,
drm,
drmdir,
dsleep,
dstrip,
dtest,
dtouch,
dtrue,
dunalias,
dunlink,
dwait,
dyacc.


<p>
Finally, note that any <em>dgsh</em> script will accept and generate
the number of inputs and outputs associated with the commands or
multipipe blocks at its two endpoints.
</p>
</section> <!-- Adapted tools -->


<!-- Downloading and installation {{{1
================================================== -->
<section id="download">
  

<p>
The <em>dgsh</em> suite has been tested under
Debian and Ubuntu Linux, FreeBSD, and Mac OS X.
A Cygwin port is underway.
</p>

<p>
An installation of <a href="http://www.graphviz.org/">GraphViz</a>
will allow you to visualize the <em>dgsh</em> graphs that you specify
in your programs.
</p>

    <section id="debian"> <!-- {{{2 -->
    <h2>Debian and Ubuntu GNU/Linux</h2>
    <section>
    <h3>Prerequisites</h3>
<p>
To compile and run <em>dgsh</em> you will need to have the following commands
installed on your system:
</p><pre>make automake gcc libtool pkg-config texinfo help2man autopoint bison check gperf 
git xz-utils gettext
</pre>

To test <em>dgsh</em> you will need to have the following commands
installed in your system:
<pre>wbritish wamerican libfftw3-dev csh
curl bzip2
</pre>

    <h3>Installation steps</h3>
<p>
Go through the following steps.
</p><ol>
<li>
Recursively clone the project's source code through its
<a href="https://github.com/dspinellis/dgsh">GitHub page</a>.
<pre>git clone --recursive https://github.com/dspinellis/dgsh.git
</pre>
</li>
<li>
Configure <em>bash</em> and the Unix tools adapted for <em>dgsh</em>.
<pre>make config
</pre>
</li>
<li>
Compile all programs.
<pre>make
</pre>
</li>
<li>
Install.
<pre>sudo make install
</pre>
</li>
</ol>

<p>
By default, the program and its documentation are installed under
<code>/usr/local</code>.
You can modify this by setting the <code>PREFIX</code> variable
in the `config` step, for example:
</p><pre>make PREFIX=$HOME config
make
make install
</pre>


    <h3>Testing</h3>
<p>

Issue the following command.
</p><pre>make test
</pre>


    </section>
    <section id="freebsd"> <!-- {{{2 -->
    <h2>FreeBSD</h2>
    <section>
    <h3>Prerequisites</h3>
<p>
To compile and run <em>dgsh</em> you will need to have the following packages
installed in your system:
</p><pre>devel/automake
devel/bison
devel/check
devel/git
devel/gmake
devel/gperf
misc/help2man
print/texinfo
shells/bash
</pre>

To test <em>dgsh</em> you will need to have the following ports
installed on your system:
<pre>archivers/bzip2
ftp/curl
</pre>

    <h3>Installation steps</h3>
<p>
Go through the following steps.
</p><ol>
<li>
Recursively clone the project's source code through its
<a href="https://github.com/dspinellis/dgsh">GitHub page</a>.
<pre>git clone --recursive https://github.com/dspinellis/dgsh.git
</pre>
</li>
<li>
Configure <em>bash</em> and the Unix tools adapted for <em>dgsh</em>.
<pre>gmake config
</pre>
</li>
<li>
Compile all programs.
<pre>gmake
</pre>
</li>
<li>
Install.
<pre>sudo gmake install
</pre>
</li>
</ol>

<p>
By default, the program and its documentation are installed under
<code>/usr/local</code>.
You can modify this by setting the <code>PREFIX</code> variable
in the `config` step, for example:
</p><pre>gmake PREFIX=$HOME config
gmake
gmake install
</pre>


    <h3>Testing</h3>
<p>

Issue the following command.
</p><pre>gmake test
</pre>


    </section>
</section>
</section>

<!-- Reference {{{1
================================================== -->
<section id="reference">
  
<p>
These are the manual pages for <em>dgsh</em>, the associated helper programs
and the API
in formats suitable for browsing and printing.
The commands are listed in the order of usefulness in everyday scenarios.
</p>
<dl>
<dt> dgsh </dt><dd> directed graph shell <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh.html">HTML</a>, <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh.pdf">PDF</a></dd>
<dt> dgsh-tee </dt><dd> buffer and copy or scatter standard input to one or more sinks <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-tee.html">HTML</a>, <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-tee.pdf">PDF</a></dd>
<dt> dgsh-wrap </dt><dd> allow any filter program to participate in an dgsh pipeline <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-wrap.html">HTML</a>, <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-wrap.pdf">PDF</a></dd>
<dt> dgsh-writeval </dt><dd> write values to a data store <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-writeval.html">HTML</a>, <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-writeval.pdf">PDF</a></dd>
<dt> dgsh-readval </dt><dd> data store client <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-readval.html">HTML</a>, <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-readval.pdf">PDF</a></dd>
<dt> dgsh-monitor </dt><dd> monitor data on a pipe <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-monitor.html">HTML</a>, <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-monitor.pdf">PDF</a></dd>
<dt> dgsh-parallel </dt><dd> create a semi-homongeneous dgsh parallel processing block <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-parallel.html">HTML</a>, <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-parallel.pdf">PDF</a></dd>
<dt> perm </dt><dd> permute inputs to outputs <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/perm.html">HTML</a>, <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/perm.pdf">PDF</a></dd>
<dt> dgsh-httpval </dt><dd> provide data store values through HTTP <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-httpval.html">HTML</a>, <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-httpval.pdf">PDF</a></dd>
<dt> dgsh-merge-sum </dt><dd> merge key value pairs, summing the values <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-merge-sum.html">HTML</a>, <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-merge-sum.pdf">PDF</a></dd>
<dt> dgsh-conc </dt><dd> input or output pipe concentrator for <em>dgsh</em> negotiation (used internally) <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-conc.html">HTML</a>, <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-conc.pdf">PDF</a></dd>
<dt> dgsh-enumerate </dt><dd> enumerate an arbitrary number of output channels (demonstration and <a href="http://istlab.dmst.aueb.gr/~dds/dgsh-egg.sh">debugging</a> tool) <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-enumerate.html">HTML</a>, <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-enumerate.pdf">PDF</a></dd>
<dt> dgsh_negotiate </dt><dd> API for <em>dgsh</em>-compatible
programs to specify and obtain dgsh I/O file descriptors
<a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh_negotiate.html">HTML</a>, <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh_negotiate.pdf">PDF</a></dd>
</dl>
</section>


<!--</th>
<th>Examples</th>
<th>{{{1
==================================================</th>
<th>-->
<section id="examples">
  

<section id="compress-compare"> <!-- {{{2 -->
<h2>Compression benchmark</h2>
</section>
<img src="https://www2.dmst.aueb.gr/dds/sw/dgsh/compress-compare-pretty.png" alt="Compression benchmark">
<!-- Extracted description -->
<p>
Report file type, length, and compression performance for
data received from the standard input.  The data never touches the
disk.
Demonstrates the use of an output multipipe to source many commands
from one followed by an input multipipe to sink to one command
the output of many and the use of dgsh-tee that is used both to
propagate the same input to many commands and collect output from
many commands orderly in a way that is transparent to users.
</p>
<!-- Extracted code -->
<pre>#!/usr/bin/env dgsh

tee |
{{
	printf 'File type:\t'
	file -

	printf 'Original size:\t'
	wc -c

	printf 'xz:\t\t'
	xz -c | wc -c

	printf 'bzip2:\t\t'
	bzip2 -c | wc -c

	printf 'gzip:\t\t'
	gzip -c | wc -c
}} |
cat
</pre>
<section id="commit-stats"> <!-- {{{2 -->
<h2>Git commit statistics</h2>
</section>
<img src="https://www2.dmst.aueb.gr/dds/sw/dgsh/commit-stats-pretty.png" alt="Git commit statistics">
<!-- Extracted description -->
<p>
Process the Git history, and list the authors and days of the week
ordered by the number of their commits.
Demonstrates streams and piping through a function.
</p>
<!-- Extracted code -->
<pre>#!/usr/bin/env dgsh

forder()
{
	sort |
	uniq -c |
	sort -rn
}

git log --format="%an:%ad" --date=default "$@" |
tee |
{{
	echo "Authors ordered by number of commits"
	# Order by frequency
	awk -F: '{print $1}' |
	forder

	echo "Days ordered by number of commits"
	# Order by frequency
	awk -F: '{print substr($2, 1, 3)}' |
	forder
}} |
cat
</pre>
<section id="code-metrics"> <!-- {{{2 -->
<h2>C code metrics</h2>
</section>
<img src="https://www2.dmst.aueb.gr/dds/sw/dgsh/code-metrics-pretty.png" alt="C code metrics">
<!-- Extracted description -->
<p>
Process a directory containing C source code, and produce a summary
of various metrics.
Demonstrates nesting, commands without input.
</p>
<!-- Extracted code -->
<pre>#!/usr/bin/env dgsh

{{
	# C and header code
	find "$@" \( -name \*.c -or -name \*.h \) -type f -print0 |
	tee |
	{{

		# Average file name length
		# Convert to newline separation for counting
		echo -n 'FNAMELEN: '
		tr \\0 \\n |
		# Remove path
		sed 's|^.*/||' |
		# Maintain average
		awk '{s += length($1); n++} END {
			if (n&gt;0)
				print s / n;
			else
				print 0; }'

		xargs -0 /bin/cat |
		tee |
		{{
			# Remove strings and comments
			sed 's/#/@/g;s/\\[\\"'\'']/@/g;s/"[^"]*"/""/g;'"s/'[^']*'/''/g" |
			cpp -P |
			tee |
			{{
				# Structure definitions
				echo -n 'NSTRUCT: '
				egrep -c 'struct[   ]*{|struct[   ]*[a-zA-Z_][a-zA-Z0-9_]*[       ]*{'
				#}} (match preceding openings)

				# Type definitions
				echo -n 'NTYPEDEF: '
				grep -cw typedef

				# Use of void
				echo -n 'NVOID: '
				grep -cw void

				# Use of gets
	  			echo -n 'NGETS: '
	  			grep -cw gets

				# Average identifier length
				echo -n 'IDLEN: '
				tr -cs 'A-Za-z0-9_' '\n' |
				sort -u |
				awk '/^[A-Za-z]/ { len += length($1); n++ } END {
					if (n&gt;0)
						print len / n;
					else
						print 0; }'
			}}

			# Lines and characters
			echo -n 'CHLINESCHAR: '
			wc -lc |
			awk '{OFS=":"; print $1, $2}'

			# Non-comment characters (rounded thousands)
			# -traditional avoids expansion of tabs
			# We round it to avoid failing due to minor
			# differences between preprocessors in regression
			# testing
			echo -n 'NCCHAR: '
			sed 's/#/@/g' |
			cpp -traditional -P |
			wc -c |
			awk '{OFMT = "%.0f"; print $1/1000}'

			# Number of comments
			echo -n 'NCOMMENT: '
			egrep -c '/\*|//'

			# Occurences of the word Copyright
			echo -n 'NCOPYRIGHT: '
			grep -ci copyright
		}}
	}}

	# C files
	find "$@" -name \*.c -type f -print0 |
	tee |
	{{
		# Convert to newline separation for counting
		tr \\0 \\n |
		tee |
		{{
			# Number of C files
			echo -n 'NCFILE: '
			wc -l

			# Number of directories containing C files
			echo -n 'NCDIR: '
			sed 's,/[^/]*$,,;s,^.*/,,' |
			sort -u |
			wc -l
		}}

		# C code
		xargs -0 /bin/cat |
		tee |
		{{
			# Lines and characters
			echo -n 'CLINESCHAR: '
			wc -lc |
			awk '{OFS=":"; print $1, $2}'

			# C code without comments and strings
			sed 's/#/@/g;s/\\[\\"'\'']/@/g;s/"[^"]*"/""/g;'"s/'[^']*'/''/g" |
			cpp -P |
			tee |
			{{
				# Number of functions
				echo -n 'NFUNCTION: '
				grep -c '^{'

				# Number of gotos
				echo -n 'NGOTO: '
				grep -cw goto

				# Occurrences of the register keyword
				echo -n 'NREGISTER: '
				grep -cw register

				# Number of macro definitions
				echo -n 'NMACRO: '
				grep -c '@[   ]*define[   ][   ]*[a-zA-Z_][a-zA-Z0-9_]*('
				# Number of include directives
				echo -n 'NINCLUDE: '
				grep -c '@[   ]*include'

				# Number of constants
				echo -n 'NCONST: '
				grep -ohw '[0-9][x0-9][0-9a-f]*' | wc -l

			}}
		}}
	}}

	# Header files
	echo -n 'NHFILE: '
	find "$@" -name \*.h -type f |
	wc -l

}} |
# Gather and print the results
cat
</pre>
<section id="duplicate-files"> <!-- {{{2 -->
<h2>Find duplicate files</h2>
</section>
<img src="https://www2.dmst.aueb.gr/dds/sw/dgsh/duplicate-files-pretty.png" alt="Find duplicate files">
<!-- Extracted description -->
<p>
List the names of duplicate files in the specified directory.
Demonstrates the combination of streams with a relational join.
</p>
<!-- Extracted code -->
<pre>#!/usr/bin/env dgsh

# Create list of files
find "$@" -type f |

# Produce lines of the form
# MD5(filename)= 811bfd4b5974f39e986ddc037e1899e7
xargs openssl md5 |

# Convert each line into a "filename md5sum" pair
sed 's/^MD5(//;s/)= / /' |

# Sort by MD5 sum
sort -k2 |

tee |
{{

	# Print an MD5 sum for each file that appears more than once
	awk '{print $2}' | uniq -d

	# Promote the stream to gather it
	cat
}} |
# Join the repeated MD5 sums with the corresponding file names
# Join expects two inputs, second will come from scatter
# XXX make streaming input identifiers transparent to users
join -2 2 |

# Output same files on a single line
awk '
BEGIN {ORS=""}
$1 != prev &amp;&amp; prev {print "\n"}
END {if (prev) print "\n"}
{if (prev) print " "; prev = $1; print $2}'
</pre>
<section id="spell-highlight"> <!-- {{{2 -->
<h2>Highlight misspelled words</h2>
</section>
<img src="https://www2.dmst.aueb.gr/dds/sw/dgsh/spell-highlight-pretty.png" alt="Highlight misspelled words">
<!-- Extracted description -->
<p>
Highlight the words that are misspelled in the command's first
argument.
Demonstrates stream processing with multipipes and
the avoidance of pass-through constructs to avoid deadlocks.
</p>
<!-- Extracted code -->
<pre>#!/usr/bin/env dgsh

export LC_ALL=C

tee |
{{
	# Find errors
	{{
		# Obtain list of words in text
		tr -cs A-Za-z \\n |
		tr A-Z a-z |
		sort -u

		# Ensure dictionary is compatibly sorted
		sort /usr/share/dict/words
	}} |
	# List errors as a set difference
	comm -23

	# Pass through text
	cat
}} |
grep --fixed-strings --file=- --ignore-case --color --word-regex --context=2
</pre>
<section id="word-properties"> <!-- {{{2 -->
<h2>Word properties</h2>
</section>
<img src="https://www2.dmst.aueb.gr/dds/sw/dgsh/word-properties-pretty.png" alt="Word properties">
<!-- Extracted description -->
<p>
Read text from the standard input and list words
containing a two-letter palindrome, words containing
four consonants, and words longer than 12 characters.
</p>
<!-- Extracted code -->
<pre>#!/usr/bin/env dgsh

# Consistent sorting across machines
export LC_ALL=C

# Stream input from file
cat $1 |

# Split input one word per line
tr -cs a-zA-Z \\n |
# Create list of unique words
sort -u |
tee |
{{
	# Pass through the original words
	cat

	# List two-letter palindromes
	sed 's/.*\(.\)\(.\)\2\1.*/p: \1\2-\2\1/;t
		g'

	# List four consecutive consonants
	sed -E 's/.*([^aeiouyAEIOUY]{4}).*/c: \1/;t
		g'

	# List length of words longer than 12 characters
	awk '{if (length($1) &gt; 12) print "l:", length($1);
		else print ""}'
}} |
# Paste the four streams side-by-side
paste |
# List only words satisfying one or more properties
fgrep :
</pre>
<section id="web-log-report"> <!-- {{{2 -->
<h2>Web log reporting</h2>
</section>
<img src="https://www2.dmst.aueb.gr/dds/sw/dgsh/web-log-report-pretty.png" alt="Web log reporting">
<!-- Extracted description -->
<p>
Creates a report for a fixed-size web log file read from the standard input.
Demonstrates the combined use of multipipe blocks, writeval and readval
to store and retrieve values, and functions in the scatter block.
Used to measure throughput increase achieved through parallelism.
</p>
<!-- Extracted code -->
<pre>#!/usr/bin/env dgsh

# Output the top X elements of the input by the number of their occurrences
# X is the first argument
toplist()
{
	uniq -c | sort -rn | head -$1
	echo
}

# Output the argument as a section header
header()
{
	echo
	echo "$1"
	echo "$1" | sed 's/./-/g'
}

# Consistent sorting
export LC_ALL=C

export -f toplist
export -f header


if [ -z "${DGSH_DRAW_EXIT}" ]
then
cat &lt;&lt;EOF
			WWW server statistics
			=====================

Summary
-------
EOF
fi

tee |
{{
	# Number of accesses
	echo -n 'Number of accesses: '
	dgsh-readval -l -s nAccess

	# Number of transferred bytes
	awk '{s += $NF} END {print s}' |
	tee |
	{{
		echo -n 'Number of Gbytes transferred: '
		awk '{print $1 / 1024 / 1024 / 1024}'

		dgsh-writeval -s nXBytes
	}}

	echo -n 'Number of hosts: '
	dgsh-readval -l -q -s nHosts

	echo -n 'Number of domains: '
	dgsh-readval -l -q -s nDomains

	echo -n 'Number of top level domains: '
	dgsh-readval -l -q -s nTLDs

	echo -n 'Number of different pages: '
	dgsh-readval -l -q -s nUniqPages

	echo -n 'Accesses per day: '
	dgsh-readval -l -q -s nDayAccess

	echo -n 'MBytes per day: '
	dgsh-readval -l -q -s nDayMB

	# Number of log file bytes
	echo -n 'MBytes log file size: '
	wc -c |
	awk '{print $1 / 1024 / 1024}'

	# Host names
	awk '{print $1}' |
	tee |
	{{
		# Number of accesses
		wc -l | dgsh-writeval -s nAccess

		# Sorted hosts
		sort |
		tee |
		{{

			# Unique hosts
			uniq |
			tee |
			{{
				# Number of hosts
				wc -l | dgsh-writeval -s nHosts

				# Number of TLDs
				awk -F. '$NF !~ /[0-9]/ {print $NF}' |
				sort -u |
				wc -l |
				dgsh-writeval -s nTLDs
			}}

			# Top 10 hosts
			{{
				 call 'header "Top 10 Hosts"'
				 call 'toplist 10'
			}}
		}}

		# Top 20 TLDs
		{{
			call 'header "Top 20 Level Domain Accesses"'
			awk -F. '$NF !~ /^[0-9]/ {print $NF}' |
			sort |
			call 'toplist 20'
		}}

		# Domains
		awk -F. 'BEGIN {OFS = "."}
		            $NF !~ /^[0-9]/ {$1 = ""; print}' |
		sort |
		tee |
		{{
			# Number of domains
			uniq |
			wc -l |
			dgsh-writeval -s nDomains

			# Top 10 domains
			{{
				 call 'header "Top 10 Domains"'
				 call 'toplist 10'
			}}
		}}
	}}

	# Hosts by volume
	{{
		call 'header "Top 10 Hosts by Transfer"'
		awk '    {bytes[$1] += $NF}
		END {for (h in bytes) print bytes[h], h}' |
		sort -rn |
		head -10
	}}

	# Sorted page name requests
	awk '{print $7}' |
	sort |
	tee |
	{{

		# Top 20 area requests (input is already sorted)
		{{
			 call 'header "Top 20 Area Requests"'
			 awk -F/ '{print $2}' |
			 call 'toplist 20'
		}}

		# Number of different pages
		uniq |
		wc -l |
		dgsh-writeval -s nUniqPages

		# Top 20 requests
		{{
			 call 'header "Top 20 Requests"'
			 call 'toplist 20'
		}}
	}}

	# Access time: dd/mmm/yyyy:hh:mm:ss
	awk '{print substr($4, 2)}' |
	tee |
	{{

		# Just dates
		awk -F: '{print $1}' |
		tee |
		{{

			# Number of days
			uniq |
			wc -l |
			tee |
			{{
				awk '
					BEGIN {
					"dgsh-readval -l -x -s nAccess" | getline NACCESS;}
					{print NACCESS / $1}' |
				dgsh-writeval -s nDayAccess

				awk '
					BEGIN {
					"dgsh-readval -l -x -q -s nXBytes" | getline NXBYTES;}
					{print NXBYTES / $1 / 1024 / 1024}' |
				dgsh-writeval -s nDayMB
			}}

			{{
				 call 'header "Accesses by Date"'
				 uniq -c
			}}

			# Accesses by day of week
			{{
				 call 'header "Accesses by Day of Week"'
				 sed 's|/|-|g' |
				 call '(date -f - +%a 2&gt;/dev/null || gdate -f - +%a)' |
				 sort |
				 uniq -c |
				 sort -rn
			}}
		}}

		# Hour
		{{
			call 'header "Accesses by Local Hour"'
			awk -F: '{print $2}' |
			sort |
			uniq -c
		}}
	}}
	dgsh-readval -q -s nAccess
}} |
cat
</pre>
<section id="text-properties"> <!-- {{{2 -->
<h2>Text properties</h2>
</section>
<img src="https://www2.dmst.aueb.gr/dds/sw/dgsh/text-properties-pretty.png" alt="Text properties">
<!-- Extracted description -->
<p>
Read text from the standard input and create files
containing word, character, digram, and trigram frequencies.
</p>
<!-- Extracted code -->
<pre>#!/usr/bin/env dgsh

# Consistent sorting across machines
export LC_ALL=C


# Convert input into a ranked frequency list
ranked_frequency()
{
	awk '{count[$1]++} END {for (i in count) print count[i], i}' |
	# We want the standard sort here
	sort -rn
}

# Convert standard input to a ranked frequency list of specified n-grams
ngram()
{
	local N=$1

	perl -ne 'for ($i = 0; $i &lt; length($_) - '$N'; $i++) {
		print substr($_, $i, '$N'), "\n";
	}' |
	ranked_frequency
}

export -f ranked_frequency
export -f ngram

tee |
{{
	# Split input one word per line
	tr -cs a-zA-Z \\n |
	tee |
	{{
		# Digram frequency
		call 'ngram 2 &gt;digram.txt'
		# Trigram frequency
		call 'ngram 3 &gt;trigram.txt'
		# Word frequency
		call 'ranked_frequency &gt;words.txt'
	}}

	# Store number of characters to use in awk below
	wc -c |
	dgsh-writeval -s nchars

	# Character frequency
	sed 's/./&amp;\
/g' |
	# Print absolute
	call 'ranked_frequency' |
	awk 'BEGIN {
		"dgsh-readval -l -x -q -s nchars" | getline NCHARS
		OFMT = "%.2g%%"}
		{print $1, $2, $1 / NCHARS * 100}' &gt; character.txt
}}
</pre>
<section id="static-functions"> <!-- {{{2 -->
<h2>C/C++ symbols that should be static</h2>
</section>
<img src="https://www2.dmst.aueb.gr/dds/sw/dgsh/static-functions-pretty.png" alt="C/C++ symbols that should be static">
<!-- Extracted description -->
<p>
Given as an argument a directory containing object files, show which
symbols are declared with global visibility, but should have been
declared with file-local (static) visibility instead.
Demonstrates the use of dgsh-capable comm (1) to combine data from
two sources.
</p>
<!-- Extracted code -->
<pre>#!/usr/bin/env dgsh

# Find object files
find "$1" -name \*.o |

# Print defined symbols
xargs nm |

tee |
{{

  # List all defined (exported) symbols
  awk 'NF == 3 &amp;&amp; $2 ~ /[A-Z]/ {print $3}' | sort

  # List all undefined (imported) symbols
  awk '$1 == "U" {print $2}' | sort

}} |
# Print exports that are not imported
comm -23
</pre>
<section id="map-hierarchy"> <!-- {{{2 -->
<h2>Hierarchy map</h2>
</section>
<img src="https://www2.dmst.aueb.gr/dds/sw/dgsh/map-hierarchy-pretty.png" alt="Hierarchy map">
<!-- Extracted description -->
<p>
Given two directory hierarchies A and B passed as input arguments
(where these represent a project at different parts of its lifetime)
copy the files of hierarchy A to a new directory, passed as a third
argument, corresponding to the structure of directories in B.
Demonstrates the use of <em>join</em> to process results from two
inputs and the use of <em>gather</em> to order asynchronously
produced results.
</p>
<!-- Extracted code -->
<pre>#!/usr/bin/env dgsh

if [ -z "${DGSH_DRAW_EXIT}" -a \( ! -d "$1" -o ! -d "$2" -o -z "$3" \) ]
then
  echo "Usage: $0 dir-1 dir-2 new-dir-name" 1&gt;&amp;2
  exit 1
fi

NEWDIR="$3"

export LC_ALL=C

line_signatures()
{
  find $1 -type f -name '*.[chly]' -print |
  # Split path name into directory and file
  sed 's|\(.*\)/\([^/]*\)|\1 \2|' |
  while read dir file
  do
    # Print "directory filename content" of lines with
    # at least one alphabetic character
    # The fields are separated by  and 
    sed -n "/[a-z]/s|^|$dir$file|p" "$dir/$file"
  done |
  # Error: multi-character tab '\001\001'
  sort -T `pwd` -t -k 2
}


export -f line_signatures


{{
  # Generate the signatures for the two hierarchies
  call 'line_signatures "$1"' -- "$1"
  call 'line_signatures "$1"' -- "$2"
}} |

# Join signatures on file name and content
join -t -1 2 -2 2 |

# Print filename dir1 dir2
sed 's///g' |
awk -F 'BEGIN{OFS=" "}{print $1, $3, $4}' |

# Unique occurrences
sort -u |
tee |
{{
  # Commands to copy
  awk '{print "mkdir -p '$NEWDIR'/" $3 ""}' |
  sort -u

  awk '{print "cp " $2 "/" $1 " '$NEWDIR'/" $3 "/" $1 ""}'
}} |
# Order: first make directories, then copy files
# TODO: dgsh-tee does not pass along first incoming stream
cat |
sh
</pre>
<section id="committer-plot"> <!-- {{{2 -->
<h2>Plot Git committer activity over time</h2>
</section>
<img src="https://www2.dmst.aueb.gr/dds/sw/dgsh/committer-plot-pretty.png" alt="Plot Git committer activity over time">
<!-- Extracted description -->
<p>
Process the Git history, and create two PNG diagrams depicting
committer activity over time. The most active committers appear
at the center vertical of the diagram.
Demonstrates image processing, mixining of synchronous and
asynchronous processing in a scatter block, and the use of an
dgsh-compliant join command.
</p>
<!-- Extracted code -->
<pre>#!/usr/bin/env dgsh

# Commit history in the form of ascending Unix timestamps, emails
git log --pretty=tformat:'%at %ae' |
# Filter records according to timestamp: keep (100000, now) seconds
awk 'NF == 2&amp; $1 &gt; 100000&amp; $1 &lt; '`date +%s` |
sort -n |
tee |
{{
	{{
		# Calculate number of committers
		awk '{print $2}' |
		sort -u |
		wc -l |
		tee |
		{{
			dgsh-writeval -s committers1

			dgsh-writeval -s committers2
			dgsh-writeval -s committers3
		}}

		# Calculate last commit timestamp in seconds
		tail -1 |
		awk '{print $1}'

		# Calculate first commit timestamp in seconds
		head -1 |
		awk '{print $1}'
	}} |
	# Gather last and first commit timestamp
	cat |
	# Make one space-delimeted record
	tr '\n' ' ' |
	# Compute the difference in days
	awk '{print int(($1 - $2) / 60 / 60 / 24)}' |
	# Store number of days
	dgsh-writeval -s days

	sort -k2	# &lt;timestamp, email&gt;

	# Place committers left/right of the median
	# according to the number of their commits
	awk '{print $2}' |
	sort |
	uniq -c |
	sort -n |
	awk '
		BEGIN {
			"dgsh-readval -l -x -q -s committers1" | getline NCOMMITTERS
			l = 0; r = NCOMMITTERS;}
		{print NR % 2 ? l++ : --r, $2}' |
	sort -k2	# &lt;left/right, email&gt;

}} |
# Join committer positions with commit time stamps
# based on committer email
join -j 2 |		# &lt;email, timestamp, left/right&gt;
# Order by timestamp
sort -k 2n |
tee |
{{
	# Create portable bitmap
	echo 'P1'

	{{
		dgsh-readval -l -q -s committers2
		dgsh-readval -l -q -s days
	}} |
	cat |
	tr '\n' ' ' |
	awk '{print $1, $2}'

	perl -na -e '
	  BEGIN {
	    open(my $ncf, "-|", "dgsh-readval -l -x -q -s committers3");
	    $ncommitters = &lt;$ncf&gt;;
	    @empty[$ncommitters - 1] = 0; @committers = @empty;
	  }
	  sub out {
		  print join("", map($_ ? "1" : "0", @committers)), "\n";
	  }

	  $day = int($F[1] / 60 / 60 / 24);
	  $pday = $day if (!defined($pday));

	  while ($day != $pday) {
		  out();
		  @committers = @empty;
		  $pday++;
	  }

	  $committers[$F[2]] = 1;

	  END { out(); }
	'
}} |
cat |
# Enlarge points into discs through morphological convolution
pgmmorphconv -erode &lt;(
cat &lt;&lt;EOF
P1
7 7
1 1 1 0 1 1 1
1 1 0 0 0 1 1
1 0 0 0 0 0 1
0 0 0 0 0 0 0
1 0 0 0 0 0 1
1 1 0 0 0 1 1
1 1 1 0 1 1 1
EOF
) |
tee |
{{
	# Full-scale image
	pnmtopng &gt;large.png
	# A smaller image
	pamscale -width 640 |
	pnmtopng &gt;small.png
}}
</pre>
<section id="parallel-word-count"> <!-- {{{2 -->
<h2>Parallel word count</h2>
</section>
<img src="https://www2.dmst.aueb.gr/dds/sw/dgsh/parallel-word-count-pretty.png" alt="Parallel word count">
<!-- Extracted description -->
<p>
Count number of times each word appears in the specified input file(s)
Demonstrates parallel execution mirroring the Hadoop WordCount example
via the dgsh-parallel command.
In contrast to GNU parallel, the block generated by dgsh-parallel
has N input and output streams, which can be combined by any
dgsh-compatible tool, such as dgsh-merge-sum or sort -m.
</p>
<!-- Extracted code -->
<pre>#!/usr/bin/env dgsh

# Number of processes
N=8

# Collation order for sorting
export LC_ALL=C

# Scatter input
dgsh-tee -s |
# Emulate Java's default StringTokenizer, sort, count
dgsh-parallel -n $N "tr -s ' \t\n\r\f' '\n' | sort -S 512M | uniq -c" |
# Merge sorted counts by providing N input channels
dgsh-merge-sum $(for i in $(seq $N) ; do printf '&lt;| ' ; done)
</pre>

<img src="https://www2.dmst.aueb.gr/dds/sw/dgsh/author-compare-pretty.png" alt="Venue author compare">
<!-- Extracted description -->
<p>
Given the specification of two publication venues, read a compressed
DBLP computer science bibliography from the standard input (e.g. piped
from curl -s http://dblp.uni-trier.de/xml/dblp.xml.gz or from a locally
cached copy) and output the number of papers published in each of the
two venues as well as the number of authors who have published only in
the first venue, the number who have published only in the second one,
and authors who have published in both.  The venues are specified through
the script's first two command-line arguments as a DBLP key prefix, e.g.
journals/acta/, conf/icse/, journals/software/, conf/iwpc/, or conf/msr/.
Demonstrates the use of dgsh-wrap -e to have sed(1) create two output
streams and the use of tee to copy a pair of streams into four ones.
</p>
<!-- Extracted code -->
<pre>#!/usr/bin/env dgsh

# Extract and sort author names
sorted_authors()
{
  sed -n 's/&lt;author&gt;\([^&lt;]*\)&lt;\/author&gt;/\1/p' |
  sort
}

# Escape a string to make it a valid sed(1) pattern
escape()
{
  echo "$1" | sed 's/\([/\\]\)/\\\1/g'
}

export -f sorted_authors

if [ ! "$2" -a ! "$DGSH_DOT_DRAW"] ; then
  echo "Usage: $0 key1 key2" 1&gt;&amp;2
  echo "Example: $0 conf/icse/ journals/software/" 1&gt;&amp;2
  exit 1
fi

gzip -dc |
# Output the two venue authors as two output streams
dgsh-wrap -e sed -n "
/^&lt;.*key=\"$(escape $1)/,/&lt;title&gt;/ w &gt;|
/^&lt;.*key=\"$(escape $2)/,/&lt;title&gt;/ w &gt;|" |
# 2 streams in 4 streams out: venue1, venue2, venue1, venue2
tee |
{{
  {{
    echo -n "$1 papers: "
    grep -c '^&lt;.* mdate=.* key='
    echo -n "$2 papers: "
    grep -c '^&lt;.* mdate=.* key='
  }}

  {{
    call sorted_authors
    call sorted_authors
  }} |
  comm |
  {{
    echo -n "Authors only in $1: "
    wc -l
    echo -n "Authors only in $2: "
    wc -l
    echo -n 'Authors common in both venues: '
    wc -l
  }}
}} |
cat
</pre>
<section id="ft2d"> <!-- {{{2 -->
<h2>Waves: 2D Fourier transforms</h2>
</section>
<img src="https://www2.dmst.aueb.gr/dds/sw/dgsh/ft2d-pretty.png" alt="Waves: 2D Fourier transforms">
<!-- Extracted description -->
<p>
Create two graphs:
1) a broadened pulse and the real part of its 2D Fourier transform, and
2) a simulated air wave and the amplitude of its 2D Fourier transform.
Demonstrates using the tools of the Madagascar shared research environment
for computational data analysis in geophysics and related fields.
Also demonstrates the use of two scatter blocks in the same script,
and the used of named streams.
</p>
<!-- Extracted code -->
<pre>#!/usr/bin/env dgsh

mkdir -p Fig

# The SConstruct SideBySideIso "Result" method
side_by_side_iso()
{
	vppen size=r vpstyle=n gridnum=2,1 /dev/stdin $*
}

export -f side_by_side_iso

# A broadened pulse and the real part of its 2D Fourier transform
sfspike n1=64 n2=64 d1=1 d2=1 nsp=2 k1=16,17 k2=5,5 mag=16,16 \
	label1='time' label2='space' unit1= unit2= |
sfsmooth rect2=2 |
sfsmooth rect2=2 |
tee |
{{
	sfgrey pclip=100 wanttitle=n

	sffft1 |
	sffft3 axis=2 pad=1 |
	sfreal |
	tee |
	{{
		sfwindow f1=1 | sfreverse which=3
		cat
	}} |
	sfcat axis=1 "&lt;|" |
	sfgrey pclip=100 wanttitle=n label1="1/time" label2="1/space"
}} |
call_with_stdin side_by_side_iso '&lt;|' yscale=1.25 &gt;Fig/ft2dofpulse.vpl

# A simulated air wave and the amplitude of its 2D Fourier transform
sfspike n1=64 d1=1 o1=32 nsp=4 k1=1,2,3,4 mag=1,3,3,1 \
	label1='time' unit1= |
sfspray n=32 d=1 o=0 |
sfput label2=space |
sflmostretch delay=0 v0=-1 |
tee |
{{
	sfwindow f2=1 | sfreverse which=2
	cat
}} |
sfcat axis=2 "&lt;|" |
tee |
{{
	sfgrey pclip=100 wanttitle=n

	sffft1 |
	sffft3 sign=1 |
	tee |
	{{
		sfreal
		sfimag
	}} |
	dgsh-wrap -e sfmath nostdin=y re="&lt;|" im="&lt;|" \
	  output="sqrt(re*re+im*im)" |
	tee |
	{{
		sfwindow f1=1 | sfreverse which=3
		cat
	}} |
	sfcat axis=1 "&lt;|" |
	sfgrey pclip=100 wanttitle=n label1="1/time" label2="1/space"
}} |
call_with_stdin side_by_side_iso '&lt;|' yscale=1.25 &gt;Fig/airwave.vpl

wait
</pre>
<section id="NMRPipe"> <!-- {{{2 -->
<h2>Nuclear magnetic resonance processing</h2>
</section>
<img src="https://www2.dmst.aueb.gr/dds/sw/dgsh/NMRPipe-pretty.png" alt="Nuclear magnetic resonance processing">
<!-- Extracted description -->
<p>
Nuclear magnetic resonance in-phase/anti-phase channel conversion and
processing in heteronuclear single quantum coherence spectroscopy.
Demonstrate processing of NMR data using the NMRPipe family of programs.
</p>
<!-- Extracted code -->
<pre>#!/usr/bin/env dgsh

# The conversion is configured for the following file:
# http://www.bmrb.wisc.edu/ftp/pub/bmrb/timedomain/bmr6443/timedomain_data/c13-hsqc/june11-se-6426-CA.fid/fid
var2pipe -in $1            \
 -xN            1280            -yN     256    \
 -xT            640             -yT     128    \
 -xMODE         Complex -yMODE  Complex      \
 -xSW           8000    -ySW    6000      \
 -xOBS          599.4489584     -yOBS   60.7485301      \
 -xCAR          4.73    -yCAR   118.000      \
 -xLAB          1H      -yLAB   15N      \
 -ndim          2       -aq2D   States      \
-verb  |
tee |
{{
  # IP/AP channel conversion
  # See http://tech.groups.yahoo.com/group/nmrpipe/message/389
  nmrPipe |
  nmrPipe -fn SOL |
  nmrPipe -fn SP -off 0.5 -end 0.98 -pow 2 -c 0.5 |
  nmrPipe -fn ZF -auto |
  nmrPipe -fn FT |
  nmrPipe -fn PS -p0 177 -p1 0.0 -di |
  nmrPipe -fn EXT -left -sw -verb |
  nmrPipe -fn TP |
  nmrPipe -fn COADD -cList 1 0 -time |
  nmrPipe -fn SP -off 0.5 -end 0.98 -pow 1 -c 0.5 |
  nmrPipe -fn ZF -auto |
  nmrPipe -fn FT |
  nmrPipe -fn PS -p0 0 -p1 0 -di |
  nmrPipe -fn TP |
  nmrPipe -fn POLY -auto -verb &gt;A

  nmrPipe |
  nmrPipe -fn SOL |
  nmrPipe -fn SP -off 0.5 -end 0.98 -pow 2 -c 0.5 |
  nmrPipe -fn ZF -auto |
  nmrPipe -fn FT |
  nmrPipe -fn PS -p0 177 -p1 0.0 -di |
  nmrPipe -fn EXT -left -sw -verb |
  nmrPipe -fn TP |
  nmrPipe -fn COADD -cList 0 1 -time |
  nmrPipe -fn SP -off 0.5 -end 0.98 -pow 1 -c 0.5 |
  nmrPipe -fn ZF -auto |
  nmrPipe -fn FT |
  nmrPipe -fn PS -p0 -90 -p1 0 -di |
  nmrPipe -fn TP |
  nmrPipe -fn POLY -auto -verb &gt;B

}}

# We use temporary files rather than streams, because
# addNMR mmaps its input files. The diagram displayed in the
# example shows the notional data flow.
if [ -z "${DGSH_DRAW_EXIT}" ]
then
	addNMR -in1 A -in2 B -out A+B.dgsh.ft2 -c1 1.0 -c2 1.25 -add
	addNMR -in1 A -in2 B -out A-B.dgsh.ft2 -c1 1.0 -c2 1.25 -sub
fi
</pre>
<section id="fft-block8"> <!-- {{{2 -->
<h2>FFT calculation</h2>
</section>
<img src="https://www2.dmst.aueb.gr/dds/sw/dgsh/fft-block8-pretty.png" alt="FFT calculation">
<!-- Extracted description -->
<p>
Calculate the iterative FFT for n = 8 in parallel.
Demonstrates combined use of permute and multipipe blocks.
</p>
<!-- Extracted code -->
<pre>#!/usr/bin/env dgsh

dgsh-fft-input $1 |
perm 1,5,3,7,2,6,4,8 |
{{
	{{
		dgsh-w 1 0
		dgsh-w 1 0
	}} |
	perm 1,3,2,4 |
	{{
		dgsh-w 2 0
		dgsh-w 2 1
	}}

	{{
		dgsh-w 1 0
		dgsh-w 1 0
	}} |
	perm 1,3,2,4 |
	{{
		dgsh-w 2 0
		dgsh-w 2 1
	}}
}} |
perm 1,5,3,7,2,6,4,8 |
{{
	dgsh-w 3 0

	dgsh-w 3 1

	dgsh-w 3 2

	dgsh-w 3 3
}} |
perm 1,5,2,6,3,7,4,8 |
cat
</pre>
<section id="reorder-columns"> <!-- {{{2 -->
<h2>Reorder columns</h2>
</section>
<img src="https://www2.dmst.aueb.gr/dds/sw/dgsh/reorder-columns-pretty.png" alt="Reorder columns">
<!-- Extracted description -->
<p>
Reorder columns in a CSV document.
Demonstrates the combined use of tee, cut, and paste.
</p>
<!-- Extracted code -->
<pre>#!/usr/bin/env dgsh

tee |
{{
	cut -d , -f 5-6 -

	cut -d , -f 2-4 -
}} |
paste -d ,
</pre>
<section id="dir"> <!-- {{{2 -->
<h2>Directory listing</h2>
</section>
<img src="https://www2.dmst.aueb.gr/dds/sw/dgsh/dir-pretty.png" alt="Directory listing">
<!-- Extracted description -->
<p>
Windows-like DIR command for the current directory.
Nothing that couldn't be done with <code>ls -l | awk</code>.
Demonstrates use of wrapped commands with no input (df, echo).
</p>
<!-- Extracted code -->
<pre>#!/usr/bin/env dgsh

ls -n |
tee |
{{
	# Reorder fields in DIR-like way
	awk '!/^total/ {print $6, $7, $8, $1, sprintf("%8d", $5), $9}'

	# Count number of files
	wc -l | tr -d \\n

	# Print label for number of files
	echo -n ' File(s) '

	# Tally number of bytes
	awk '{s += $5} END {printf("%d bytes\n", s)}'

	# Count number of directories
	grep -c '^d' | tr -d \\n

	# Print label for number of dirs and calculate free bytes
	df -h . | awk '!/Use%/{print " Dir(s) " $4 " bytes free"}'
}} |
cat
</pre>

</section> <!-- Examples -->

</section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An opinionated critique of Duolingo (134 pts)]]></title>
            <link>https://isomorphism.xyz/blog/2025/duolingo/</link>
            <guid>45425061</guid>
            <pubDate>Tue, 30 Sep 2025 13:19:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://isomorphism.xyz/blog/2025/duolingo/">https://isomorphism.xyz/blog/2025/duolingo/</a>, See on <a href="https://news.ycombinator.com/item?id=45425061">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

  

  <article>
    <p>During the stay-at-home grim days of 2020, I started learning Spanish on Duolingo. Having a working understanding of Spanish seemed like a sensible first step towards opening a taco truck in Mexico, in case I had to run away from my doctoral studies. This July, after about 5 years I decided to end the 1800 day streak that I managed to drag on with numerous streak freezes and minimal effort lessons. While Spanish words look less foreign, and with some focus, I am able to decipher small paragraphs of grammatically simple sentences; the effort was less than a smashing success – I certainly could not be writing this essay in Spanish.</p>

<p>If Duolingo is known for anything, it has to be <strong>their gamification approach</strong>. There is no shortage of gamification mechanics on the platform: XP; potions to double your XP; leagues of gold, silver, and all sorts of other metals and minerals; treasure chests; quests; monthly quests; and so on. I have only ever paid little attention to these mechanics. While I still haven’t entirely rejected the idea that a good RPG could be a good scaffolding to teach a language, I do not think Duolingo is one.</p>

<p>Games worth their salt are not created by bolting together a collection of numerical statistics. That is how you get <a href="https://en.wikipedia.org/wiki/Cookie_Clicker" target="_blank" rel="noopener noreferrer">cookie clicker</a>. I did not have a good understanding of how the mechanics work: if I learn 10 words, how many XP do I get for my hard work? Is the Diamond League higher or lower than the Obsidian League? I could have viewed their documentation to figure it out, but there was nothing motivating me to do so. If I collect 100 XP, what does it mean for my language skills? For that matter, why do I collect extra XP when I receive a potion? Can the XP I collect be used in a way to carefully guide me towards the specific language skills I would explore next? Navigating the mechanical <em>gameplay</em> of Duolingo is neither rewarding for its own sake, nor is it helpful towards actually learning a language.</p>

<p>Duolingo is not just a poor simulacrum of the mechanical aspects of a game, but also of <strong>the social aspects</strong> of one. Who are all these people I am on the Silver league with? Having a comparable amount of XP does not give me a sense of social connection with them. When I click a button to congratulate a friend on Duolingo, I do not truly engage with their learning journey. Indeed, it is worse than hearting an instagram photo, or upvoting a reddit thread. In those cases, I am reacting to a sliver of expression from my acquaintance. Here, I am presented only with a pre-rendered text with an abstract numerical statistic. Reacting to it is deliberately frictionless: I am presented with a wall of buttons allowing me to click them with ease and without thought. When Duolingo tells me that so and so sent me a message saying “Hey, come back and learn Spanish with me!”, I don’t admire how thoughtful and encouraging my friend is; I just notice that they clicked a button to send me a pre-generated message.</p>

<p>Interactions on Duolingo were not always of the push-button variety. Duolingo <a href="https://duolingo.hobune.stream/" target="_blank" rel="noopener noreferrer">had forums</a> where users would discuss different aspects of their language learning journey. In fact, Duolingo <a href="https://streakchaser.com/language/duolingo-forums-cant-comment/" target="_blank" rel="noopener noreferrer">would link</a> each sentence to its own forum thread for discussion – discussion, which was at the very least, helpful, and at times, eye-opening. At first, these discussion threads were locked, and later removed. My hypothesis is that for the business geniuses running Duolingo, <strong>the forums</strong> were assesed to be a liability, having to moderate which were not worth spending the dollars for. The nature of interacting with people – friends or strangers, in person or online – is that sometimes bad things would happen. Even when there is no abuse or harassment going on, there is always the risk that the other person might greet you with disagreement, or worse, apathy. <a href="https://web.archive.org/web/20250916150051/http://omegle.com/" target="_blank" rel="noopener noreferrer">Many</a> <a href="https://web.archive.org/web/20250803104055/https://www.lfgss.com/conversations/401475/" target="_blank" rel="noopener noreferrer">people</a> tend to think that the risks outweigh the benefits.</p>

<p>The gamification mechanic that I did latch on to was <strong>the Streak</strong>. I generally have been critical of the green owl, but I do think that it did help me form a good habit – a net positive, despite the minuscule magnitude. Regular Duolingo users will know that the streak can be gamed away in more than one ways. <em>Streak Freezes</em> can be bought using gems (of which I happen to have 24,053 of, somehow) or be gifted by your friends, and equipped 2 at a time. Streaks wouldn’t have their social effect if there weren’t enough people with a moderate number of people with decent streaks to be sprinkled around. Maintaining the streak, even without freezes, does not have to mean that you are learning – repeating a simple lesson from several units ago would work. My 1800 day streak didn’t mean that I spent 1800 days learning Spanish; it meant that I spent a large number of days <em>engaging with the platform</em>. I later started peeking into the Japanese and Finnish courses, and the 1800 day number includes them. If you loose interest in languages, Duolingo <a href="https://blog.duolingo.com/new-subjects/" target="_blank" rel="noopener noreferrer">tells us</a> that spending time with math or music will count towards your Streak.</p>

<p>The deficiency of Duolingo’s pedagogy was first made obvious to me by the excellent audio lessons produced by <a href="https://www.languagetransfer.org/" target="_blank" rel="noopener noreferrer">Language Transfer</a>. Going through the first few lessons of Language Transfer, I was unfazed, observing that I had already learnt what was being taught. Soon, what shocked me was how quickly Language Transfer caught up to what I had managed to learn in a couple years of time. While Duolingo is great at making sure that the user comes back to the app everyday, <strong>their pedagogy</strong> is subpar. Remaining true to gamification, Duolingo prefers to throw users head-first into translation exercises. If you do not know a word, you hover over it and you arrange a given bag of words into a sentence that is hopefully meaningful. Grammar lessons are extremely minimal. The removal of the forums dedicated to the discussion of specific sentences did not help. Understanding the course outline – knowing what is taught where, or reviewing lessons – is not easy.</p>

<p>Supposedly, <strong>the Duolingo philosophy</strong> is that if you are exposed to enough sentences, you will eventually learn how to use them. I do believe this is true, and I do believe the exercises are indispensable. However, the whole process could be greatly improved by a few more lessons interspersed in the curriculum telling the student what is going on. To see this, I would ask myself, did I always internalize the grammatically correct structures even in my own mother tongue? I think not, and my language skills have improved when my parents or teachers would point out simple grammatical mistakes. <a href="https://en.wikipedia.org/wiki/Eggcorn" target="_blank" rel="noopener noreferrer">Eggcorns</a> are a closely related amusing phenomena.</p>

<p>I cannot tell if Duolingo repeats different concepts in exercises adaptively based on your mastery, or are simply fixed in the course material. Repetition is <a href="https://en.wikipedia.org/wiki/Spaced_repetition" target="_blank" rel="noopener noreferrer">good for learning</a> but Duolingo’s repetition can be frustrating. The platform’s interface is largely built around clicking <strong>a bag of jumbled</strong> words one at a time to input a translation. Once you learn a concept well enough, most of your mental energy is spent on the finding and clicking of the words rather than the translation. Thankfully, this situation can be made better by dictating your answer, as pointed out by the <a href="https://blog.duolingo.com/sneaky-pronunciation-practice/" target="_blank" rel="noopener noreferrer">official blog</a>.</p>

<p>Duolingo does include a few other formats for their lessons. There are some stories, which are interspersed in the course. The stories are short, but silly and enjoyable. There are also audio only lessons, which are also shorter and unfortunately, not as fun. From time to time, the regular lessons also ask you to speak to the microphone but in my experience, the audio recognition seems to accept the answer even if I mumble through the words. Duolingo is also known for its usage of bizarre phrases, whose shock value generates <a href="https://www.reddit.com/r/shitduolingosays/" target="_blank" rel="noopener noreferrer">social media buzz</a> and may or may not have a positive pedagogical effect.</p>

<hr>

<p>Duolingo is a neat case study in Silicon Valley ideology. Big tech embraces <a href="https://hbr.org/2016/04/blitzscaling" target="_blank" rel="noopener noreferrer">blitz-scaling</a>: the primary goal is neither financial sustainability nor the quality of materials but making the number of users grow. The faux gamification and <a href="https://www.ft.com/content/3a9d6a6c-3681-4c93-9b8f-e6e41bea30dc" target="_blank" rel="noopener noreferrer">passive-aggressive messaging</a> may be helpful with little else, but is good for user retention. The expansionism does not stop at growing the number of users; Duolingo has decided that they must loop in <a href="https://blog.duolingo.com/new-subjects/" target="_blank" rel="noopener noreferrer">music and math</a> learners as well. As we have discussed, the maxim of <em>friction reduction</em> has guided them towards optimizing away authenticity in the user interactions on the platform.</p>

<p>In April 2025, Duolingo <a href="https://web.archive.org/web/20250516041748/https://www.linkedin.com/posts/duolingo_below-is-an-all-hands-email-from-our-activity-7322560534824865792-l9vh/?rcm=ACoAAADRbfMBYDgBD6CKFgJZnb27n5NLn5v-LAo" target="_blank" rel="noopener noreferrer">decided</a> to go AI-first. Supposedly, “to teach well, [they] need to create a massive amount of content” – so much so that “doing [it] manually does not scale”. For <a href="https://blog.duolingo.com/2024-duolingo-language-report/" target="_blank" rel="noopener noreferrer">the top ten languages</a>, I cannot imagine any reasonable person saying that the lack of study material is the main obstacle towards learning. This statement spells out what the Duolingo executives value. The Duolingo CEO is not shy to admit it. In an <a href="https://www.npr.org/transcripts/1197997573" target="_blank" rel="noopener noreferrer">interview with NPR</a>, he said the following.</p>

<blockquote>
  <p>[I]f it’s our content, as in, like, our learning content, there’s so much of that - thousands and thousands and thousands of kind of sentences and words and paragraphs. That is mostly done by computers, and we probably spot-check it. But if it’s things like the user interface of Duolingo, where we say - like, you know, the button says quit, and we have to translate, that is all done with humans. And we spend a lot of effort on that, but that’s because each one of those is highly valuable.</p>
</blockquote>

<p>Yes, <em>the button that says ‘quit’</em> is more valuable than the learning material, which is only ‘probably’ spot-checked.</p>

<hr>

<p>After I moved to Japan, I dialed up my efforts to learn Japanese. For a while, I shifted over my Duolingo habits to Japanese. Because Duolingo wasn’t my only learning material for Japanese, it was glaringly obvious very soon that the Duolingo pedagogy is unhelpful and often misleading. While the Spanish learner has to introduce themselves to a few new concepts (e.g, <em>ser vs estar</em>, or reflexive verbs), the Japanese learner faces an explosion of differences. Japanese has a <a href="https://en.wikipedia.org/wiki/Japanese_writing_system" target="_blank" rel="noopener noreferrer">writing system with three components</a>; generally uses a <a href="https://en.wikipedia.org/wiki/Topic_and_comment" target="_blank" rel="noopener noreferrer">topic-comment structure</a> and often omits the topic; has a <a href="https://en.wikipedia.org/wiki/Subject%E2%80%93object%E2%80%93verb_word_order" target="_blank" rel="noopener noreferrer">subject-object-verb order</a>; has <a href="https://en.wikipedia.org/wiki/Japanese_adjectives" target="_blank" rel="noopener noreferrer">adjectives which conjugate</a>; a lot of <a href="https://en.wikipedia.org/wiki/Japanese_counter_word" target="_blank" rel="noopener noreferrer">counting suffixes</a>; <a href="https://en.wikibooks.org/wiki/Japanese/Grammar/Sentence_ending_particles" target="_blank" rel="noopener noreferrer">sentence ending particles</a> and famously, a <a href="https://en.wikipedia.org/wiki/Honorific_speech_in_Japanese" target="_blank" rel="noopener noreferrer">complex honorific system</a>. Duolingo does not break its gamification façade to teach the user some of these concepts head-on. Instead, it pretends that translating between Japanese and English is a matter of substituting phrases and shuffling them around.</p>

<p>Since I gave up on my Duolingo streak, I have started exploring <strong>other avenues</strong> to continue learning Japanese. I participate in group lessons with a tutor once a week for an hour. Believe it or not, the tutor has more charm than <a href="https://duolingo.fandom.com/wiki/Falstaff" target="_blank" rel="noopener noreferrer">Falstaff</a>. I regularly do my flashcard kanji study with <a href="https://www.wanikani.com/" target="_blank" rel="noopener noreferrer">Wanikani</a>. A newer addition to my study routine is <a href="https://bunpro.jp/" target="_blank" rel="noopener noreferrer">Bunpro</a>. My progress has been slow but evident: when I recognize that the names of the metro stations I frequent break down into simple words, they lose a little bit of their mystery but it is a satisfying revelation.</p>

<p>These platforms are a welcome contrast against the techno-accelerationist attitude of Duolingo. Instead of trying to do it all, they are extremely niche: they only teaching one language and Wanikani is focused at teaching a very specific element of it. Wanikani maintains a <a href="https://docs.api.wanikani.com/20170710/" target="_blank" rel="noopener noreferrer">public API</a>, which makes third-party <a href="https://community.wanikani.com/t/the-new-and-improved-list-of-api-and-third-party-apps/7694" target="_blank" rel="noopener noreferrer">apps and scripts</a> possible. I praise them for their welcome attitude towards interoperability instead of trying to build a closed ecosystem. Both <a href="https://community.wanikani.com/" target="_blank" rel="noopener noreferrer">Wanikani</a> and <a href="https://community.bunpro.jp/" target="_blank" rel="noopener noreferrer">Bunpro</a> have vibrant user forums. Bunpro makes actual lessons part of their critical path, instead of hoping that the user will <em>eventually figure it out</em>. When a Bunpro user feels that their lesson was not adequate, they do not have to rely on AI generated slop – Bunpro directs users to carefully-crafted lessons by <em>other people</em> (see the ‘resource’ section at the end of this <a href="https://bunpro.jp/grammar_points/%E3%81%AD" target="_blank" rel="noopener noreferrer">page</a>, for example).</p>

  </article>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Imgur pulls out of UK as data watchdog threatens fine (302 pts)]]></title>
            <link>https://www.express.co.uk/news/uk/2115228/image-site-imgur-pulls-out</link>
            <guid>45424888</guid>
            <pubDate>Tue, 30 Sep 2025 13:01:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.express.co.uk/news/uk/2115228/image-site-imgur-pulls-out">https://www.express.co.uk/news/uk/2115228/image-site-imgur-pulls-out</a>, See on <a href="https://news.ycombinator.com/item?id=45424888">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="singleArticle" data-story="2115228" role="main"> <article> <header><h2>A popular image hosting website has stopped its services in the UK after regulators threatened a fine.</h2><div><p><time datetime="2025-09-30T11:35:00Z"> <span>12:35, Tue, Sep 30, 2025</span> </time> <time datetime="2025-09-30T11:38:22Z"> Updated: <span>12:38, Tue, Sep 30, 2025</span> </time></p></div> </header><div data-type="article-body"><div><p><picture><source type="image/avif" srcset="https://cdn.images.express.co.uk/img/dynamic/1/1200x712/secondary/imgur-founder-alan-schaaf-speaks-during-the-web-summit-2018-in-lisbon-portugal-on-november-8-6459211.avif?r=1759232302595" media="screen and (min-width:10000px)"><source type="image/webp" srcset="https://cdn.images.express.co.uk/img/dynamic/1/1200x712/secondary/imgur-founder-alan-schaaf-speaks-during-the-web-summit-2018-in-lisbon-portugal-on-november-8-6459211.webp?r=1759232302595" media="screen and (min-width:10000px)"><source type="image/jpeg" srcset="https://cdn.images.express.co.uk/img/dynamic/1/1200x712/secondary/imgur-founder-alan-schaaf-speaks-during-the-web-summit-2018-in-lisbon-portugal-on-november-8-6459211.jpg?r=1759232302595" media="screen and (min-width:10000px)"><source type="image/avif" srcset="https://cdn.images.express.co.uk/img/dynamic/1/674x400/secondary/imgur-founder-alan-schaaf-speaks-during-the-web-summit-2018-in-lisbon-portugal-on-november-8-6459211.avif?r=1759232302595" media="screen and (min-width:100000px)"><source type="image/webp" srcset="https://cdn.images.express.co.uk/img/dynamic/1/674x400/secondary/imgur-founder-alan-schaaf-speaks-during-the-web-summit-2018-in-lisbon-portugal-on-november-8-6459211.webp?r=1759232302595" media="screen and (min-width:100000px)"><source type="image/jpeg" srcset="https://cdn.images.express.co.uk/img/dynamic/1/674x400/secondary/imgur-founder-alan-schaaf-speaks-during-the-web-summit-2018-in-lisbon-portugal-on-november-8-6459211.jpg?r=1759232302595" media="screen and (min-width:100000px)"><source type="image/avif" srcset="https://cdn.images.express.co.uk/img/dynamic/1/940x/secondary/imgur-founder-alan-schaaf-speaks-during-the-web-summit-2018-in-lisbon-portugal-on-november-8-6459211.avif?r=1759232302595" media="screen and (min-width:1200px)"><source type="image/webp" srcset="https://cdn.images.express.co.uk/img/dynamic/1/940x/secondary/imgur-founder-alan-schaaf-speaks-during-the-web-summit-2018-in-lisbon-portugal-on-november-8-6459211.webp?r=1759232302595" media="screen and (min-width:1200px)"><source type="image/jpeg" srcset="https://cdn.images.express.co.uk/img/dynamic/1/940x/secondary/imgur-founder-alan-schaaf-speaks-during-the-web-summit-2018-in-lisbon-portugal-on-november-8-6459211.jpg?r=1759232302595" media="screen and (min-width:1200px)"><source type="image/avif" srcset="https://cdn.images.express.co.uk/img/dynamic/1/590x/secondary/imgur-founder-alan-schaaf-speaks-during-the-web-summit-2018-in-lisbon-portugal-on-november-8-6459211.avif?r=1759232302595" media="screen"><source type="image/webp" srcset="https://cdn.images.express.co.uk/img/dynamic/1/590x/secondary/imgur-founder-alan-schaaf-speaks-during-the-web-summit-2018-in-lisbon-portugal-on-november-8-6459211.webp?r=1759232302595" media="screen"><img src="https://cdn.images.express.co.uk/img/dynamic/1/590x/secondary/imgur-founder-alan-schaaf-speaks-during-the-web-summit-2018-in-lisbon-portugal-on-november-8-6459211.jpg?r=1759232302595" data-img="https://cdn.images.express.co.uk/img/dynamic/1/1200x712/secondary/imgur-founder-alan-schaaf-speaks-during-the-web-summit-2018-in-lisbon-portugal-on-november-8-6459211.jpg?r=1759232302595" alt="Web Summit 2018 - Day 4" title="Web Summit 2018 - Day 4" width="590" height="393"></picture></p><p><span>Imgur founder. <span>(Image: Getty)</span><span data-img="https://cdn.images.express.co.uk/img/dynamic/1/1200x712/secondary/imgur-founder-alan-schaaf-speaks-during-the-web-summit-2018-in-lisbon-portugal-on-november-8-6459211.jpg?r=1759232302595"></span></span></p></div><div><p>An image hosting platform with more than 130 million users has stopped being available in the UK <a data-link-tracking="InArticle|Link" href="https://www.express.co.uk/news/uk/339446/More-firms-breaching-data-rules">after regulators signalled their intention to impose penalties over concerns around children’s data.</a></p><p>The Information Commissioner’s Office (ICO) said that it has reached provisional findings in an investigation in the parent company of image hosting site, Imgur. Its probe was launched earlier this year, as part of the regulator's Children’s Code strategy, which is intended to set the standards for how <a data-link-tracking="InArticle|Link" href="https://www.express.co.uk/news/politics/2089661/online-safety-law-watchdog">online services handle the personal information of young people</a>.</p><p>In a statement the ICO said: “We are aware of reports that the social media platform <a data-link-tracking="InArticle|Link" href="https://www.express.co.uk/life-style/science-technology/2091749/expressvpn-online-safety-act-2025">Imgur is currently not available in the UK</a>. Imgur's decision to restrict access in the UK is a commercial decision taken by the company.”</p></div><div><p>Tim Capel, the ICO’s Interim Executive Director for Regulatory Supervision, said that the regulator had now issued a notice of intent to fine.</p><p>He said: “We reached our provisional findings on this investigation, and we issued a notice of intent to impose a monetary penalty on MediaLab on 10 September 2025.</p><p>“Our findings are provisional and the ICO will carefully consider any representations from MediaLab before taking a final decision whether to issue a monetary penalty.”</p><p>The ICO also confirmed that companies could not avoid accountability by withdrawing their services in the UK.</p><p>Mr Capel said: “We have been clear that exiting the UK does not allow an organisation to avoid responsibility for any prior infringement of data protection law, and our investigation remains ongoing.</p><p>“This update has been provided to give clarity on our investigation, and we will not be providing any further detail at this time.”</p></div><div><p>He added that protecting young people’s information remains a central focus: “Safeguarding children’s personal information is a key priority for the ICO and our Children’s code strategy outlines our key interventions in this area. Keeping children safe online is the responsibility of the companies offering online services to them and we will continue to hold them to account.”</p><p>Regulators did not disclose the potential size of the penalty for specific breaches it has identified.</p><p>Under UK law, the “notice of intent” process gives the company an opportunity to make representations before any final decision is made.</p><p>Imgur, founded in 2009 and acquired by Los Angeles-based MediaLab AI Inc in 2021, is an image hosting and sharing site popular for memes, viral content and online communities. It’s services appeared to become unavailable in the UK last night.</p><p>Imgur was approached for comment.</p></div><div data-lazy-function="readnext"> <header><h3>Read next</h3> </header><ul><li> <var></var> <a data-link-tracking="InArticle|Link" href="https://www.express.co.uk/" data-tmdatatrack="read-next" data-tmdatatrack-articleid="" data-tmdatatrack-platform="nationals" data-tmdatatrack-source="" data-tmdatatrack-index="" data-tmdatatrack-visible="">&nbsp;</a></li><li> <var></var> <a data-link-tracking="InArticle|Link" href="https://www.express.co.uk/" data-tmdatatrack="read-next" data-tmdatatrack-articleid="" data-tmdatatrack-platform="nationals" data-tmdatatrack-source="" data-tmdatatrack-index="" data-tmdatatrack-visible="">&nbsp;</a></li><li> <var></var> <a data-link-tracking="InArticle|Link" href="https://www.express.co.uk/" data-tmdatatrack="read-next" data-tmdatatrack-articleid="" data-tmdatatrack-platform="nationals" data-tmdatatrack-source="" data-tmdatatrack-index="" data-tmdatatrack-visible="">&nbsp;</a></li><li> <var></var> <a data-link-tracking="InArticle|Link" href="https://www.express.co.uk/" data-tmdatatrack="read-next" data-tmdatatrack-articleid="" data-tmdatatrack-platform="nationals" data-tmdatatrack-source="" data-tmdatatrack-index="" data-tmdatatrack-visible="">&nbsp;</a></li><li> <var></var> <a data-link-tracking="InArticle|Link" href="https://www.express.co.uk/" data-tmdatatrack="read-next" data-tmdatatrack-articleid="" data-tmdatatrack-platform="nationals" data-tmdatatrack-source="" data-tmdatatrack-index="" data-tmdatatrack-visible="">&nbsp;</a></li></ul></div><div><p>  <span></span> <span>Invalid email</span></p><p>We use your sign-up to provide content in ways you've consented to and to improve our understanding of you. This may include adverts from us and 3rd parties based on our understanding. You can unsubscribe at any time. Read our <a href="https://www.express.co.uk/privacy-notice">Privacy Policy</a></p></div></div> </article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Founder sentenced to seven years in prison for fraudulent sale to JPMorgan (157 pts)]]></title>
            <link>https://www.cnn.com/2025/09/30/business/charlie-javice-frank-sentenced-jpmorgan-intl</link>
            <guid>45424827</guid>
            <pubDate>Tue, 30 Sep 2025 12:53:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnn.com/2025/09/30/business/charlie-javice-frank-sentenced-jpmorgan-intl">https://www.cnn.com/2025/09/30/business/charlie-javice-frank-sentenced-jpmorgan-intl</a>, See on <a href="https://news.ycombinator.com/item?id=45424827">Hacker News</a></p>
Couldn't get https://www.cnn.com/2025/09/30/business/charlie-javice-frank-sentenced-jpmorgan-intl: Error: Request failed with status code 451]]></description>
        </item>
        <item>
            <title><![CDATA[Pasta Cooking Time (116 pts)]]></title>
            <link>https://www.jefftk.com/p/pasta-cooking-time</link>
            <guid>45424704</guid>
            <pubDate>Tue, 30 Sep 2025 12:40:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jefftk.com/p/pasta-cooking-time">https://www.jefftk.com/p/pasta-cooking-time</a>, See on <a href="https://news.ycombinator.com/item?id=45424704">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

    <p><span>

I generally find the numbers printed on pasta boxes for cooking time
far too high: I'll set the timer for a minute below their low-end "al
dente" time, and when I taste one it's already getting too mushy. I
decided to run a small experiment to get a better sense of how cooked
I like pasta.

</span></p><p>

I decided to use Market Basket <a href="https://en.wikipedia.org/wiki/Rigatoni">Rigatoni</a>. [1] It's a
ridged cylinder, and I measured the ridges at 1.74mm:

</p>
<p>

<a href="https://www.jefftk.com/rigatoni-ridge-thickness-big.jpg"><img src="https://www.jefftk.com/rigatoni-ridge-thickness.jpg" width="550" height="604" srcset="https://www.jefftk.com/rigatoni-ridge-thickness.jpg 550w,https://www.jefftk.com/rigatoni-ridge-thickness-2x.jpg 1100w"></a></p>


<p>

And the valleys at 1.32mm:

</p>
<p>

<a href="https://www.jefftk.com/rigatoni-valley-thickness-big.jpg"><img src="https://www.jefftk.com/rigatoni-valley-thickness.jpg" width="550" height="577" srcset="https://www.jefftk.com/rigatoni-valley-thickness.jpg 550w,https://www.jefftk.com/rigatoni-valley-thickness-2x.jpg 1100w"></a></p>


<p>

The box recommends 13-15 minutes:

</p>
<p>

<a href="https://www.jefftk.com/market-basket-rigatoni-cooking-time-big.jpg"><img src="https://www.jefftk.com/market-basket-rigatoni-cooking-time.jpg" width="550" height="453" srcset="https://www.jefftk.com/market-basket-rigatoni-cooking-time.jpg 550w,https://www.jefftk.com/market-basket-rigatoni-cooking-time-2x.jpg 1100w"></a></p>


<p>

This is a house brand pasta from a chain centered in a part of the
country with a relatively high Italian-American population, so you
might think they'd avoid the issue where Americans often cook pasta
absurdly long:

</p>
<p>

<a href="https://www.jefftk.com/market-basket-locations-big.png"><img src="https://www.jefftk.com/market-basket-locations.png" width="550" height="686" srcset="https://www.jefftk.com/market-basket-locations.png 550w,https://www.jefftk.com/market-basket-locations-2x.png 1100w"></a></p>


<p>

I boiled some water, put in the pasta, and starting at 9min I removed
a piece every 15s until I got to 14:30:

</p>
<p>

<a highlight="" href="https://www.jefftk.com/pasta-every-15s-big.jpg"><img src="https://www.jefftk.com/pasta-every-15s.jpg" width="550" height="739" srcset="https://www.jefftk.com/pasta-every-15s.jpg 550w,https://www.jefftk.com/pasta-every-15s-2x.jpg 1100w"></a></p>


<p>

Here's the minute-by-minute, cut open so you can see the center of the
noodles:

</p>
<p>

<a highlight="" href="https://www.jefftk.com/pasta-cut-open-by-time-big.jpg"><img src="https://www.jefftk.com/pasta-cut-open-by-time.jpg" width="550" height="90" srcset="https://www.jefftk.com/pasta-cut-open-by-time.jpg 550w,https://www.jefftk.com/pasta-cut-open-by-time-2x.jpg 1100w"></a></p>


<p>

My family and I tried a range of noodles, trying to <a href="https://en.wikipedia.org/wiki/Binary_search">bisect</a> our way
to the ideal cooking time.  I was happiest at 10m15s, but ok between
9m15s and 11m30s.  Julia thought 9m45s was barely underdone, while
11m45s was barely overdone.  Anna liked 10m30s.  Lily didn't like any
of them, consistently calling them "too crunchy" up through 10m45s and
then "too mushy" for 11m0s and up.  Everyone agreed that by 12m45s it
was mushy.

</p>
<p>

Instead of 13-15min, a guideline of 10-12min would make a lot more
sense in our house.  And, allegedly, the glycemic index is much lower.

</p>
<p>

My mother and her siblings grew up in Rome, and I wrote asking about
what they'd noticed here.  My uncle replied "my bias is that Americans
are wimps for soft pasta" and the others agreed.

</p>
<p>

I tried using a <a href="https://www.jefftk.com/p/christmas-microscopy">cheap
microscope</a> to investigate, whether there were interesting
structural differences, but even with an iodide stain I couldn't make
out much.  Here's 3min:

</p>
<p>

<a href="https://www.jefftk.com/rigatoni-iodide-03m00s-big.jpg"><img src="https://www.jefftk.com/rigatoni-iodide-03m00s.jpg" width="550" height="367" srcset="https://www.jefftk.com/rigatoni-iodide-03m00s.jpg 550w,https://www.jefftk.com/rigatoni-iodide-03m00s-2x.jpg 1100w"></a></p>


<p>

And 7min:

</p>
<p>

<a href="https://www.jefftk.com/rigatoni-iodide-07m00s-big.jpg"><img src="https://www.jefftk.com/rigatoni-iodide-07m00s.jpg" width="550" height="367" srcset="https://www.jefftk.com/rigatoni-iodide-07m00s.jpg 550w,https://www.jefftk.com/rigatoni-iodide-07m00s-2x.jpg 1100w"></a></p>


<p>

And 13min:

</p>
<p>

<a href="https://www.jefftk.com/rigatoni-iodide-13m00s-big.jpg"><img src="https://www.jefftk.com/rigatoni-iodide-13m00s.jpg" width="550" height="367" srcset="https://www.jefftk.com/rigatoni-iodide-13m00s.jpg 550w,https://www.jefftk.com/rigatoni-iodide-13m00s-2x.jpg 1100w"></a></p>


<p>

On the other hand, the kids and I did have fun with the microscope.

</p>
<p>

<a href="https://www.jefftk.com/jeff-nora-microscope-wrapper-big.jpg"><img src="https://www.jefftk.com/jeff-nora-microscope-wrapper.jpg" width="550" height="309" srcset="https://www.jefftk.com/jeff-nora-microscope-wrapper.jpg 550w,https://www.jefftk.com/jeff-nora-microscope-wrapper-2x.jpg 1100w"></a></p>


<p>
<br>

[1] We called these "hospital noodles" growing up, because when my
mother had been in a hospital for a long time as a kid (recovering
from being hit by an impatient driver while crossing the street) they
had served Rigatoni as their primary pasta shape.

  </p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How has mathematics gotten so abstract? (112 pts)]]></title>
            <link>https://lcamtuf.substack.com/p/how-has-mathematics-gotten-so-abstract</link>
            <guid>45424648</guid>
            <pubDate>Tue, 30 Sep 2025 12:33:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lcamtuf.substack.com/p/how-has-mathematics-gotten-so-abstract">https://lcamtuf.substack.com/p/how-has-mathematics-gotten-so-abstract</a>, See on <a href="https://news.ycombinator.com/item?id=45424648">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>Today, mathematics is regarded as a purely abstract science. On forums such as Stack Exchange, trained mathematicians may sneer at newcomers who ask for intuitive explanations of mathematical constructs. Indeed, persistently trying to relate the foundations of math to reality has become the calling card of online cranks.</p><p>I find this ironic: for millennia, mathematics was essentially a natural science. We had no philosophical explanation why 2 + 2 should be equal to 4; we just looked at what was happening in the real world and tried to capture the rules. The abstractions were important, of course, but they needed to be rooted in objectivity. The early development of algebra and geometry followed suit. It was never enough for the axioms to be internally consistent; the angles of your hypothetical triangle needed to match the physical world.</p><p>That said, even in antiquity, the reliance on intuition sometimes looked untenable. A particular cause for concern were the outcomes of thought experiments that involved repeating a task without end. The most famous example is Zeno’s paradox of motion. If you slept through that class, imagine the scenario of Achilles racing a tortoise:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!MzH8!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71d2d033-0511-48aa-bb71-b6732d01a9df_1638x397.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!MzH8!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71d2d033-0511-48aa-bb71-b6732d01a9df_1638x397.png 424w, https://substackcdn.com/image/fetch/$s_!MzH8!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71d2d033-0511-48aa-bb71-b6732d01a9df_1638x397.png 848w, https://substackcdn.com/image/fetch/$s_!MzH8!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71d2d033-0511-48aa-bb71-b6732d01a9df_1638x397.png 1272w, https://substackcdn.com/image/fetch/$s_!MzH8!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71d2d033-0511-48aa-bb71-b6732d01a9df_1638x397.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!MzH8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71d2d033-0511-48aa-bb71-b6732d01a9df_1638x397.png" width="1456" height="353" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/71d2d033-0511-48aa-bb71-b6732d01a9df_1638x397.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:353,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:106249,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://lcamtuf.substack.com/i/174503112?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71d2d033-0511-48aa-bb71-b6732d01a9df_1638x397.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!MzH8!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71d2d033-0511-48aa-bb71-b6732d01a9df_1638x397.png 424w, https://substackcdn.com/image/fetch/$s_!MzH8!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71d2d033-0511-48aa-bb71-b6732d01a9df_1638x397.png 848w, https://substackcdn.com/image/fetch/$s_!MzH8!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71d2d033-0511-48aa-bb71-b6732d01a9df_1638x397.png 1272w, https://substackcdn.com/image/fetch/$s_!MzH8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71d2d033-0511-48aa-bb71-b6732d01a9df_1638x397.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption><em><strong>Catch me if you can.</strong></em></figcaption></figure></div><p>We can reason that after a while, Achilles will catch up to the turtle’s original position (red dot); however, by the time he gets there, the animal will have moved some distance forward (yellow dot):</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!XS3G!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6567e21-8c90-4e32-a4fb-0f548afbf46b_1638x538.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!XS3G!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6567e21-8c90-4e32-a4fb-0f548afbf46b_1638x538.png 424w, https://substackcdn.com/image/fetch/$s_!XS3G!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6567e21-8c90-4e32-a4fb-0f548afbf46b_1638x538.png 848w, https://substackcdn.com/image/fetch/$s_!XS3G!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6567e21-8c90-4e32-a4fb-0f548afbf46b_1638x538.png 1272w, https://substackcdn.com/image/fetch/$s_!XS3G!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6567e21-8c90-4e32-a4fb-0f548afbf46b_1638x538.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!XS3G!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6567e21-8c90-4e32-a4fb-0f548afbf46b_1638x538.png" width="1456" height="478" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d6567e21-8c90-4e32-a4fb-0f548afbf46b_1638x538.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:478,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:144247,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://lcamtuf.substack.com/i/174503112?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6567e21-8c90-4e32-a4fb-0f548afbf46b_1638x538.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!XS3G!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6567e21-8c90-4e32-a4fb-0f548afbf46b_1638x538.png 424w, https://substackcdn.com/image/fetch/$s_!XS3G!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6567e21-8c90-4e32-a4fb-0f548afbf46b_1638x538.png 848w, https://substackcdn.com/image/fetch/$s_!XS3G!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6567e21-8c90-4e32-a4fb-0f548afbf46b_1638x538.png 1272w, https://substackcdn.com/image/fetch/$s_!XS3G!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6567e21-8c90-4e32-a4fb-0f548afbf46b_1638x538.png 1456w" sizes="100vw"></picture></div></a><figcaption><em>And they would have gotten away with it too…</em></figcaption></figure></div><p>Next, consider the time needed for Achilles to reach the yellow dot; once again, by the time he gets there, the turtle will have moved forward a tiny bit. This process can be continued indefinitely; the gap keeps getting smaller but never goes to zero, so we must conclude that Achilles can’t possibly win the race.</p><p>Amusingly, the problems caused by infinity lingered on the periphery of mathematics for centuries, fully surfacing only after we attempted to fix them with calculus. Calculus gave us a rigorous solution to the ancient puzzle: an infinite sum of time slices can be a finite number, so Achilles does catch up to the tortoise. Yet, to arrive at that result, the new field relied on the purported existence of infinitely small numbers (infinitesimals). The founders struggled to explain how to construct such entities, where to find them on the real number line (you can’t), and whether they’re safe to mix with real number algebra in the first place.</p><p>Over time, this prompted a number of mathematicians to try and build a more general model of mathematics, starting from the ground up — that is, from the principles of formal logic. In particular, one prominent faction of the movement sought to define numbers and arithmetic operations in a way that was fully independent of the physical realm.</p><p><span>In the late 19th century, Giuseppe Peano successfully answered this call. His system posits the existence of a single initial number — conventionally, zero — and then defines a successor function </span><em>S</em><span>.</span></p><p>This allows us to define numbers — really, just a collection of labels — solely in terms of a succession relationship:</p><div data-component-name="Latex"><p><span>\(\begin{alignat}{1}
1 &amp;= S(0) \\
2 &amp;= S(1) &amp;&amp;= S(S(0)) \\
3 &amp;= S(2) &amp;&amp;= S(S(S(0))) \\
4 &amp;= S(3) &amp;&amp;= S(S(S(S(0))))
\end{alignat}\)</span></p></div><p><span>While this might seem mundane, the scheme reduces many other problems to the process of induction and recursion. For example, to solve 2 + 2, we don’t need any </span><em>a priori </em><span>knowledge of what “2” or “+” means. Instead, we define addition using the following two rules:</span></p><div data-component-name="Latex"><p><span>\(\begin{alignat}{1}
a &amp;+ 0 &amp;&amp;= a &amp;&amp;\qquad (\textrm{rule 1}) \\
a &amp;+ S(b) &amp;&amp;= S(a+b) &amp;&amp;\qquad ( \textrm{rule 2}) 
\end{alignat}
\)</span></p></div><p>This notation may appear abstract, so to tease it out, let’s try to actually calculate 2 + 2. Because the second operand is non-zero, we can’t apply the first rule just yet. That said, from the construction of Peano numbers, we know that 2 = S(1). In light of this, we can rewrite 2 + 2 in a way that lets us use of the second rule:</p><div data-component-name="Latex"><p><span>\(\begin{array}{r l}
2 + 2 = 2 + S(1) &amp; \textrm{(by the construction of number 2)} \\
2 + S(1) = S(2 + 1) &amp; \textrm{(per rule 2)}
\end{array}\)</span></p></div><p>At this point, we’ve shown that 2 + 2 is the same as S(2 + 1). To solve the original equation, we still need to find the value of 2 + 1; this can be done by applying the same substitution technique once more:</p><div data-component-name="Latex"><p><span>\(\begin{array}{rl}
2 +1 = 2 + S(0) &amp; \textrm{(by construction)} \\
2 + S(0) = S(2+0) &amp; \textrm{(per rule 2)}
\end{array}\)</span></p></div><p><span>In effect, we have restated 2 + 2 as S(2 + 1), and then 2 + 1 as S(2 + 0). In that last instance, the second operand is zero, so we can finally apply rule 1. The rule says that </span><em>a + 0 = a</em><span>, so:</span></p><div data-component-name="Latex"><p><span>\(2 + 1 = S(2 + 0) = S(2)\)</span></p></div><p>Further, from the construction of Peano numbers, we know that our chosen label for S(2) is 3; therefore, 2 + 1 = 3.</p><p>With this equality established, we go back to the initial step where we expressed 2 + 2 as S(2 + 1) and get the final answer:</p><div data-component-name="Latex"><p><span>\(2 + 2 = S(2+1) = S(3) = 4\)</span></p></div><p><span>If you work with software, you might appreciate the following C code that implements roughly the same logic (</span><a href="https://godbolt.org/z/zWh5j1KEr" rel="">demo</a><span>):</span></p><blockquote><pre><code>#include &lt;stdio.h&gt;

struct number { char* label; struct number* next; }
  five  = { "5", NULL },   four = { "4", &amp;five }, three = { "3", &amp;four },
  two   = { "2", &amp;three }, one  = { "1", &amp;two },  zero  = { "0", &amp;one };

struct number* succ(struct number* num) { return num-&gt;next; }

struct number* pred(struct number* num) {
  struct number* ret = &amp;zero;
  while (succ(ret) != num) ret = succ(ret);
  return ret;
}

struct number* add_numbers(struct number* num_a, struct number* num_b) {
  if (num_b == &amp;zero) return num_a;
  return succ(add_numbers(num_a, pred(num_b)));
}

int main() {
  printf(”2 + 3 = %s\n”, add_numbers(&amp;two, &amp;three)-&gt;label);
}</code></pre></blockquote><p>In this program, instead of relying on built-in integers, we start with a unidirectional linked list of strings: “0” → “1” → “2” → “3” → “4” → “5”. This data structure encodes the successor relationship between the labels without giving them any further meaning.</p><p><span>Next, we define a trivial helper called </span><em>succ(x), </em><span>which returns the successor of </span><em>x,</em><span> along with a slightly more complicated function called </span><em>pred(x)</em><span>, which finds the element to which </span><em>x </em><span>is the successor. Finally, </span><em>add_numbers(a, b)</em><span> is a straightforward implementation of the recursive rules for Peano addition, as outlined earlier on.</span></p><p>Again, the merit of this approach is that it lets us model arithmetic without any external assumptions about the nature of numbers, the significance of the addition operator, and so forth. We used familiar labels (0, 1, 2, 3, 4, …), but we could’ve used some other ordered collection of abstract symbols (🥔, 🎵, 🐸, 🌀, 🐱, …). If so, we’d have gotten an equivalent model of math in which 🐸 + 🐸= 🐱.</p><p>Of course, Peano arithmetic is too cumbersome for everyday tasks; instead, it serves as a minimalist model for theoretical work. It is used similarly to how computer scientists use Turing machines; no one wants to browse the internet on a Turing-style computer, but if you proved that P = NP for a Turing machine, this would have implications for more practical computing architectures too.</p><p><span>Giuseppe Peano’s axiomatic approach was revolutionary and led to breakthroughs such as the </span><a href="https://lcamtuf.substack.com/p/monkeys-typewriters-and-busy-beavers" rel="">Gödel incompleteness theorem</a><span>; however, it still didn’t offer a particularly good model of infinite quantities. For that, mathematicians needed to turn to an even more exotic framework: set theory.</span></p><p>In set theory, numbers are conventionally defined as labels for specific, ordered sets. To get started with the construction process, we only need an empty set ({}), which we label as zero:</p><p>To define the successor number, we add an element to the set. To avoid inventing arbitrary new elements, we can simply embed the previously-conjured number zero in the successor set:</p><p>If this seems confusing, you can think of set as boxes. We started with an empty box with zero items inside; we then sealed the box and placed it in a larger container, so the larger box now contains a single element. For this tally, the contents of that smaller, sealed box are of no consequence.</p><p><span>After that, we can’t define the next successor as {0, 0}; this is because in set theory, every set element must be unique. That said, as discussed earlier, a “naked” element </span><em>n </em><span>is distinct from a box containing that element (i.e., a new set </span><em>{n}</em><span>), so we can do this:</span></p><p>Note that in our model, 1 = {0}, so the construction method shown above is equivalent to saying that 2 = {0, 1}.</p><p>To get to the third successor, we need to put one more element in the set. At this point, we can’t reuse 0 or 1, but we can embed the recently-created set representing 2:</p><div data-component-name="Latex"><p><span>\(3 = \{ 0, 1, 2 \} = \{ 0, \{ 0 \}, \{ 0, \{ 0 \} \} \}\)</span></p></div><p>This process can continue for as long as we’d like, e.g.:</p><div data-component-name="Latex"><p><span>\(4 = \{ 0, 1, 2, 3 \} = \{ 0, \{0\}, \{0, \{0\}\}, \{0, \{0\}, \{0, \{0\}\}\} \}
\)</span></p></div><p>In set theory, the labels we’re creating are called ordinals. Note that every ordinal is an ordered set of all the preceding ordinals, and that the set never contains itself.</p><p><span>If you’re seeing parallels to the iterative construction of Peano numbers, this is not an accident; the two approaches are conceptually similar, it’s just that in this instance, the underlying mathematical structure of each number is spelled out more explicitly. The general algorithm is that we build number </span><em>n + 1</em><span> by joining the preceding set </span><em>n</em><span> and a copy of </span><em>n </em><span>embedded inside a new set. The set-joining operation is known as union (∪), so we can formalize a Peano-like successor function for ordinals as:</span></p><p><span>Almost all mathematicians accept the existence of infinite sets; a common example would be the set of all natural numbers, ℕ. Every natural number itself is finite, but there is no upper limit on how large these numbers can get; whenever you pick some </span><em>n</em><span>, I can always best you by shouting “</span><em>n </em><span>+ 1”.</span></p><p>The ordered set of all natural numbers looks like the product of our method for constructing cardinals — that is, if we allowed the process to continue without end:</p><div data-component-name="Latex"><p><span>\(\mathbb{N} = \{0, 1, 2, 3, 4, ...\}\)</span></p></div><p>It’s tempting to ask if the set can function as an infinite ordinal — i.e., an infinite number — and if yes, what numerical properties does it have?</p><p><span>Well, we can say right off the bat that the ordinal we’re talking about wouldn’t be a member of ℕ: every element of ℕ is finite. We can also conclude that the ordinal must not be a successor to any natural number: if </span><em>n</em><span> is a member of ℕ, then so is </span><em>n + 1</em><span>, so the presence of a successor relationship would lead to the same contradiction.</span></p><p>Can such an unmoored, infinite ordinal exist? Well, that’s up to us to decide: conjuring it doesn’t lead to any outright paradoxes and opens up some weird but occasionally useful math. </p><p><span>We can name this ordinal </span><em>ω</em><span>; again, its set-theoretic representation is just:</span></p><div data-component-name="Latex"><p><span>\(\omega = \{0, 1, 2, 3, 4, ...\}  = \mathbb{N}\)</span></p></div><p>Of course, inventing a symbol isn’t much of an accomplishment; the big question is whether, under the axioms of set theory, we can derive any useful arithmetic for this mysterious entity.</p><p>In school, you might have been exposed to notation along the lines of:</p><p><span>In an </span><a href="https://lcamtuf.substack.com/p/09999-1" rel="">earlier article</a><span>, I quipped that this notation is just a glorified calculator error message — all it tells us is that the result is too large for reals:</span></p><div data-component-name="Latex"><p><span>\(\texttt{&lt;Error&gt;} + 1 = \texttt{&lt;Error&gt;}\)</span></p></div><p><span>That said, if you’re accustomed to this way of thinking about infinity, it’s tempting to assume that the rule should apply to actual infinite numbers — i.e. that </span><em>ω </em><span>should be the same as</span><em> ω</em><span> + 1</span><em>. </em><span>Let’s test that hypothesis.</span></p><p><span>In line with the Peano rules, we can express </span><em>ω</em><span> + 1 as the application of the ordinal successor operation to the first operand. As a reminder, the successor operation takes the original infinite set of natural numbers (</span><em>ω = </em><span>ℕ) and then embeds that set as a new element to construct the next ordinal. We get:</span></p><div data-component-name="Latex"><p><span>\(\omega + 1 = S(\omega) = \omega \cup \{\omega\} = \{ 0, 1, 2, 3, 4, ..., \omega \}\)</span></p></div><p><span>We have previously established that </span><em>ω </em><span>itself cannot be a member of </span><em> </em><span>ℕ, because that would make it a natural number, and therefore, a finite quantity. Yet, the newly-constructed set corresponding to </span><em>ω</em><span> + 1 evidently </span><em>does</em><span> contain that element; this tells us that the set is categorically different from ℕ. We must conclude that </span><em>ω </em><span>≠ </span><em>ω</em><span> + 1</span><em>. </em><span>More specifically, because </span><em>ω</em><span> + 1 contains </span><em>ω</em><span>, it sits higher in the rank of ordinals, and we can assert that </span><em>ω &lt; ω</em><span> + 1.</span></p><p><span>But lest we get too cozy with this new reality: addition involving infinite ordinals is not necessarily commutative! To illustrate, let’s construct the set representing 1 + </span><em>ω</em><span>. As before, we rewrite addition as a (repeated) application of the successor function to the first operand:</span></p><div data-component-name="Latex"><p><span>\(1 + \omega =  \underbrace{...S(S(S(1)))}_{\textrm{repeats } \omega \textrm{ times}}\)</span></p></div><p>Recall that we defined the ordinal 1 as a single-element set containing zero: {0}. Starting from that set and applying the successor function, we end up constructing ordinal 2 — another name for {0, 1}. The next application of the nets us 3, aka {0, 1, 2}:</p><div data-component-name="Latex"><p><span>\(\begin{align}
1 &amp;= \{0\} \\
S(1) &amp;= \{0, 1\} \\
S(S(1)) &amp;= \{0, 1, 2\} \\
...
\end{align}\)</span></p></div><p><span>If we repeat this operation </span><em>ω </em><span>times, we obtain an infinite set { 0, 1, 2, 3, 4, … }. Note that the element </span><em>ω </em><span>itself can never make it into the set: it’s not a successor of any natural number, so it can’t be reached by repeatedly incrementing one.</span></p><p><span>Upon closer inspection, the resulting 1 + </span><em>ω </em><span>set is indistinguishable from the set of natural numbers, ℕ. We know that </span><em>ω </em><span>is also just another name for</span><em> </em><span>ℕ, so we can write the following equality: 1 + </span><em>ω = ω. </em><span>Few paragraphs earlier, we showed that </span><em>ω &lt; ω + 1. </em><span>This leads to a surprising result: 1 + </span><em>ω &lt; ω + 1.</em></p><p><span>One might ask if non-commutative addition is a violation of one of the axioms of standard arithmetic. It isn’t, on a technicality: the rules apply only to finite numbers, such as the members of ℕ. Luckily for us, </span><em>ω </em><span>isn’t invited to that club.</span></p><p><span>Before we wrap up, let’s have a look at another interesting corner case: </span><em>ω + ω </em><span>(aka </span><em>ω </em><span>· 2). To calculate this ordinal, we start with </span><em>ω = </em><span>{ 0, 1, 2, 3, 4, … } and then iteratively extend the set through the successor operation. We first append </span><em>ω, </em><span>then </span><em>ω </em><span>+ 1, then </span><em>ω + 2, </em><span>and so on:</span></p><div data-component-name="Latex"><p><span>\(\omega \cdot 2 = \{ 0, 1, 2, 3, 4, ..., \omega, \omega + 1, \omega + 2, ... \}\)</span></p></div><p><span>The tail end of this ordered set is an infinite sequence of successors to </span><em>ω; </em><span>as in all the earlier cases, </span><em>ω </em><span>· 2 can’t be a member of itself, so </span><em>ω </em><span>· 2 must not be reachable by incrementing</span><em> ω. </em><span>This is analogous to how </span><em>ω </em><span>couldn’t be reached by incrementing any finite number</span><em>; </em><span>the discontinuity repeats for each multiple of </span><em>ω.</em></p><p>It’s hard not to notice that our set-theoretic numbers seem to be describe the size (element count) of the underlying set:</p><div data-component-name="Latex"><p><span>\(5 = \{0, 1, 2, 3, 4\}\)</span></p></div><p>For finite sets, this notion of size aligns with common sense. But when we look at the non-commutative addition and the discontinuities of infinite ordinals, one might start to wonder if trying to “count” elements is still a meaningful way to characterize sets in that realm.</p><p>There are several other ways to reason about this problem without resorting to counting. The simplest rule we can come up with is that one set is a strict subset of another, the first set could be described as smaller than the second one.</p><p>Another approach is to consider two sets to be of equivalent “magnitude” if you can map their elements one-to-one. It doesn’t matter which element gets mapped to which, as long as there are no orphaned members on either side:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!O98b!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50e61248-a7e5-4e13-bcc5-fd3da6abfc12_1740x988.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!O98b!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50e61248-a7e5-4e13-bcc5-fd3da6abfc12_1740x988.png 424w, https://substackcdn.com/image/fetch/$s_!O98b!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50e61248-a7e5-4e13-bcc5-fd3da6abfc12_1740x988.png 848w, https://substackcdn.com/image/fetch/$s_!O98b!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50e61248-a7e5-4e13-bcc5-fd3da6abfc12_1740x988.png 1272w, https://substackcdn.com/image/fetch/$s_!O98b!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50e61248-a7e5-4e13-bcc5-fd3da6abfc12_1740x988.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!O98b!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50e61248-a7e5-4e13-bcc5-fd3da6abfc12_1740x988.png" width="1456" height="827" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/50e61248-a7e5-4e13-bcc5-fd3da6abfc12_1740x988.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:827,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:178037,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://lcamtuf.substack.com/i/174503112?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50e61248-a7e5-4e13-bcc5-fd3da6abfc12_1740x988.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!O98b!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50e61248-a7e5-4e13-bcc5-fd3da6abfc12_1740x988.png 424w, https://substackcdn.com/image/fetch/$s_!O98b!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50e61248-a7e5-4e13-bcc5-fd3da6abfc12_1740x988.png 848w, https://substackcdn.com/image/fetch/$s_!O98b!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50e61248-a7e5-4e13-bcc5-fd3da6abfc12_1740x988.png 1272w, https://substackcdn.com/image/fetch/$s_!O98b!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50e61248-a7e5-4e13-bcc5-fd3da6abfc12_1740x988.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>One equivalent pair (top) and two non-equivalent ones (bottom).</em></figcaption></figure></div><p><span>This particular measure of equivalency on the basis of a one-to-one mapping is called </span><em>cardinality</em><span>.</span></p><p>The concepts are dead simple for finite sets, but consider a set of natural numbers next to a set of every even number (E). Obviously, E is strict subset of natural numbers, so by our first rule, we could say that E is smaller than ℕ. Yet, if we’re talking about a one-to-one mapping for a pair of infinite sets, the following approach is perfectly fine:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!2Ood!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff31ae3ac-6c8b-4137-86c8-18ccfd09d334_2088x1254.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!2Ood!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff31ae3ac-6c8b-4137-86c8-18ccfd09d334_2088x1254.png 424w, https://substackcdn.com/image/fetch/$s_!2Ood!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff31ae3ac-6c8b-4137-86c8-18ccfd09d334_2088x1254.png 848w, https://substackcdn.com/image/fetch/$s_!2Ood!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff31ae3ac-6c8b-4137-86c8-18ccfd09d334_2088x1254.png 1272w, https://substackcdn.com/image/fetch/$s_!2Ood!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff31ae3ac-6c8b-4137-86c8-18ccfd09d334_2088x1254.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!2Ood!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff31ae3ac-6c8b-4137-86c8-18ccfd09d334_2088x1254.png" width="1456" height="874" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f31ae3ac-6c8b-4137-86c8-18ccfd09d334_2088x1254.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:874,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:189004,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://lcamtuf.substack.com/i/174503112?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff31ae3ac-6c8b-4137-86c8-18ccfd09d334_2088x1254.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!2Ood!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff31ae3ac-6c8b-4137-86c8-18ccfd09d334_2088x1254.png 424w, https://substackcdn.com/image/fetch/$s_!2Ood!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff31ae3ac-6c8b-4137-86c8-18ccfd09d334_2088x1254.png 848w, https://substackcdn.com/image/fetch/$s_!2Ood!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff31ae3ac-6c8b-4137-86c8-18ccfd09d334_2088x1254.png 1272w, https://substackcdn.com/image/fetch/$s_!2Ood!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff31ae3ac-6c8b-4137-86c8-18ccfd09d334_2088x1254.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>This is legal in 47 states.</em></figcaption></figure></div><p><span>After all, the sets are infinite, so we never run out of elements. We say that the set of natural numbers has a specific cardinality — aleph-null, ℵ</span><sub>0</sub><span> — and that the cardinality of the set of even numbers is the same.</span></p><p>Do any other infinite cardinalities exist? The answer, as demonstrated by Georg Cantor, appears to be yes! Imagine some arbitrary one-to-one mapping from every single natural number to reals (ℝ), e.g.:</p><div data-component-name="Latex"><p><span>\(\begin{array}{c}
1 &amp; \rightarrow &amp; 0.\underline{\textbf{1}}23456... \\
2 &amp; \rightarrow &amp; 0.6\underline{\textbf{5}}4321... \\
3 &amp; \rightarrow &amp; 0.99\underline{\textbf{9}}999... \\
4 &amp; \rightarrow &amp; 0.454\underline{\textbf{5}}45... \\
5 &amp; \rightarrow &amp; 0.1111\underline{\textbf{1}}1... \\
6 &amp; \rightarrow &amp; 0.03133\underline{\textbf{7}}... \\
&amp; ... &amp;
\end{array}\)</span></p></div><p>There’s nothing special about these values; the only point is that we have a distinct real number assigned to every member of ℕ. There are obviously at least as many reals as there are natural numbers, so this is something we should be able to do.</p><p><span>The remaining question is whether there are any orphaned reals left once all the natural numbers are used up. To figure this out, let’s try to construct a new real number, </span><em>d</em><span>. We start by looking at the first row above:</span></p><div data-component-name="Latex"><p><span>\(1 \rightarrow 0.\underline{\textbf{1}}23456...
\)</span></p></div><p><span>We take the underlined (first) decimal digit of the real and then choose </span><em>any value other than this one</em><span> for the corresponding digit in </span><em>d</em><span>. In this case, the offending digit is 1, so we can pick 0, 2, 3, 4, 5, 6, 7, 8, or 9. Let’s go with 9:</span></p><div data-component-name="Latex"><p><span>\(d = 0.\textbf{9} \textrm{ (...to be continued)}\)</span></p></div><p><span>We then proceed to the second row, this time looking at at the second decimal digit — essentially, following the diagonal pattern underlined in the assignment diagram. Once again, for the second decimal digit of </span><em>d</em><span>, we choose any value other than the actual digit marked in row 2; since the highlighted value is 5, we can pick 2 instead:</span></p><div data-component-name="Latex"><p><span>\(d = 0.9\textbf{2} \textrm{ (...more to come)}\)</span></p></div><p>In row three, we can replace 9 with 0; in row four, let’s substitute 5 with 4. For row five, we trade 1 for 3; in row six, we use 5 instead of 7. We keep following the diagonal pattern to infinity:</p><p><span>What’s special about this result? Well, by construction, </span><em>d </em><span>differs by at least one digit from every single real number in our mapping! For example, it can never match row 1 because the first decimal digit is 9 instead of 1; it also doesn’t match row 2 because the second decimal digit is 2 instead of 5.</span></p><p><span>That is to say, </span><em>d</em><span> can’t possibly appear anywhere on the list that assigned a real to every integer. Thus, we have found an orphaned member of ℝ; the cardinality of natural numbers (ℵ</span><sub>0</sub><span>) is evidently less than the cardinality of reals (also known as the </span><em>cardinality of the continuum</em><span>).</span></p><p>Are there any cardinalities in between? Mathematicians don’t think so, but this hypothesis is provably undecidable within the traditional axioms of set theory.</p><p>Maybe? If infinity lurks in some dark corners of the physical universe, we probably have no way of ascertaining its numerical properties. In the absence of this, we have a toolkit for creating weird worlds that restate the rules of formal logic in increasingly mind-bending ways — and sometimes help prove a theorem or two.</p><p>Because the behavior of infinite sets is bizarre, there is a school of mathematics that rejects their existence. Heck, there is a small number of mathematicians who reject infinity altogether. The difficulty is that such decisions require discarding vast amounts of useful math — or at the very least, tossing out an explanation of why we’re doing that math in a particular way.</p><p>On some level, this might not be a big deal: calculus is usually still taught without providing a rigorous justification for limits or infinitesimals. On the flip side, as almost any calculus student will attest, it’s an intellectually unsatisfying approach.</p><p>Just as important, without all these wonderfully confusing notions of infinity, how do you keep the riff-raff out of math?</p><p><em><span>👉 Reader exclusive: for an essential Peano arithmetic calculator, </span><a href="https://lcamtuf.coredump.cx/peano/" rel="">click here</a><span>. </span></em></p><p><em><span>If you’re interested in beavers in addition to turtles, you’re probably going to enjoy </span><a href="https://lcamtuf.substack.com/p/monkeys-typewriters-and-busy-beavers" rel="">this article</a><span>. And if you like the content, please subscribe; there’s no better way to stay in touch with the writers you like.</span></em></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Comprehension debt: A ticking time bomb of LLM-generated code (469 pts)]]></title>
            <link>https://codemanship.wordpress.com/2025/09/30/comprehension-debt-the-ticking-time-bomb-of-llm-generated-code/</link>
            <guid>45423917</guid>
            <pubDate>Tue, 30 Sep 2025 10:37:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://codemanship.wordpress.com/2025/09/30/comprehension-debt-the-ticking-time-bomb-of-llm-generated-code/">https://codemanship.wordpress.com/2025/09/30/comprehension-debt-the-ticking-time-bomb-of-llm-generated-code/</a>, See on <a href="https://news.ycombinator.com/item?id=45423917">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
	<main id="main">
		
<article id="post-2299">
	<!-- .entry-header -->

	
	
	<div>
		
<p>An effect that’s being more and more widely reported is the increase in time it’s taking developers to modify or fix code that was generated by Large Language Models. </p>



<p>If you’ve worked on legacy systems that were written by other people, perhaps decades ago, you’ll recognise this phenomenon. Before we can safely change code, we first need to understand it – understand what it does, and also oftentimes why it does it the way it does. In that sense, this is nothing new.</p>



<p>What <em>is </em>new is the scale of the problem being created as lightning-speed code generators spew reams of unread code into millions of projects.</p>



<p>Teams that care about quality will take the time to review and understand (and more often than not, rework) LLM-generated code before it makes it into the repo. This slows things down, to the extent that any time saved using the LLM coding assistant is often canceled out by the downstream effort.</p>



<p>But <em>some </em>teams have opted for a different approach. They’re the ones checking in code nobody’s read, and that’s only been cursorily tested – if it’s been tested at all. And, evidently, there’s a <em>lot </em>of them.</p>



<p>When teams produce code faster than they can understand it, it creates what I’ve been calling “comprehension debt”. If the software gets used, then the odds are high that <em>at some point</em> that generated code will need to change. The “A.I.” boosters will say “We can just get the tool to do that”. And that might work maybe 70% of the time. </p>



<p>But those of us who’ve experimented a lot with using LLMs for code generation and modification know that there will be times when the tool just won’t be able to do it. </p>



<p>“Doom loops”, when we go round and round in circles trying to get an LLM, or a bunch of different LLMs, to fix a problem that it just doesn’t seem to be able to, are an everyday experience using this technology. Anyone claiming it doesn’t happen to them has either been extremely lucky, or is fibbing.</p>



<p>It’s pretty much guaranteed that there will be many times when we have to edit the code ourselves. The “comprehension debt” is the extra time it’s going to take us to understand it first.</p>



<p>And we’re sitting on a rapidly growing mountain of it.</p>

<div>
	<p><img referrerpolicy="no-referrer" alt="Unknown's avatar" src="https://1.gravatar.com/avatar/aadcfbd208e65a0829f5d259192a5886454439a708e425500c59c535e28ae081?s=42&amp;d=identicon&amp;r=G" srcset="https://1.gravatar.com/avatar/aadcfbd208e65a0829f5d259192a5886454439a708e425500c59c535e28ae081?s=42&amp;d=identicon&amp;r=G 1x, https://1.gravatar.com/avatar/aadcfbd208e65a0829f5d259192a5886454439a708e425500c59c535e28ae081?s=63&amp;d=identicon&amp;r=G 1.5x, https://1.gravatar.com/avatar/aadcfbd208e65a0829f5d259192a5886454439a708e425500c59c535e28ae081?s=84&amp;d=identicon&amp;r=G 2x, https://1.gravatar.com/avatar/aadcfbd208e65a0829f5d259192a5886454439a708e425500c59c535e28ae081?s=126&amp;d=identicon&amp;r=G 3x, https://1.gravatar.com/avatar/aadcfbd208e65a0829f5d259192a5886454439a708e425500c59c535e28ae081?s=168&amp;d=identicon&amp;r=G 4x" height="42" width="42" loading="lazy" decoding="async">	</p><!-- .author-avatar -->

	<!-- .author-description -->
</div><!-- .author-info -->
	</div><!-- .entry-content -->

	<!-- .entry-footer -->
</article><!-- #post-2299 -->

<!-- .comments-area -->

	<nav aria-label="Posts">
		<h2>Post navigation</h2>
		
	</nav>
	</main><!-- .site-main -->

	
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Can you use GDPR to circumvent BlueSky's adult content blocks? (102 pts)]]></title>
            <link>https://shkspr.mobi/blog/2025/09/can-you-use-gdpr-to-circumvent-blueskys-adult-content-blocks/</link>
            <guid>45423363</guid>
            <pubDate>Tue, 30 Sep 2025 08:50:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://shkspr.mobi/blog/2025/09/can-you-use-gdpr-to-circumvent-blueskys-adult-content-blocks/">https://shkspr.mobi/blog/2025/09/can-you-use-gdpr-to-circumvent-blueskys-adult-content-blocks/</a>, See on <a href="https://news.ycombinator.com/item?id=45423363">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Disqus Turned My Blog into an Ad Farm – So I Killed It (530 pts)]]></title>
            <link>https://ryansouthgate.com/goodbye-disqus/</link>
            <guid>45423268</guid>
            <pubDate>Tue, 30 Sep 2025 08:36:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ryansouthgate.com/goodbye-disqus/">https://ryansouthgate.com/goodbye-disqus/</a>, See on <a href="https://news.ycombinator.com/item?id=45423268">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><h4 id="intro">Intro<a href="#intro"></a></h4><p>This will be a short and sweet post. As I’m not big on goodbyes.</p><p>Disqus started showing ads for their “free” tier comments system a few years back. At the time, the communication they sent out via email, seemed quite laid-back and had the tone of “don’t worry about it, it’s not a big thing”. Which in part lead me to almost forget it happened.</p><p>At the time, the disqus comments system looked quite smart and sleek. I remember thinking that the ads system will possibly look smart and sleek too. Which alleviated any worries I had at the time.</p><p>WELL…….I’ve just seen the ads, and they look horrific!!!</p><h4 id="apologies">Apologies<a href="#apologies"></a></h4><p>I have a <a href="https://pi-hole.net/" target="_blank">Pihole</a> set up, so ads are blocked on my home network. When I’m out of the house, my phone is connected to a <a href="https://www.wireguard.com/" target="_blank">Wireguard VPN</a> which routes my data through my home internet, therefore - getting all the ad-blocking, Pihole goodness.</p><p>After years with Pi-hole, which now blocks over a million domains, I’ve become incredibly accustomed to a mostly ad-free web. Without realizing it, I’d forgotten what the typical internet experience feels like.</p><p>I used to get a couple of emails from Disqus, letting me know that there’s a new comment on this blog. I haven’t had many of these emails recently, so I decided to disable my adblocker for a few minutes and check out the comments.</p><p>There were none, instead I was greeted by some horribly formatted and obviously scammy ads:</p><figure><a><img data-src="./disqus_ads.webp" data-action="zoom" alt="Horrible Disqus Ads" src="https://ryansouthgate.com/goodbye-disqus/disqus_ads.webp"></a></figure><p>For the people who read this blog, I’m sorry.</p><p>I became “blind” to what the web is really like for most users. I’ve tried to keep this blog minimalist - a clean place to find answers. Those ads not only ruin that experience; they trample privacy too:</p><figure><a><img data-src="./disqus_traffic.webp" data-action="zoom" alt="Screenshot of Firefox Dev Tools. Showing a worrying amount of tracking requests" src="https://ryansouthgate.com/goodbye-disqus/disqus_traffic.webp"></a></figure><p>With this post, I’ve removed Disqus. It was making my blog worse, and frankly, they were profiting off my work and my visitor’s data.
I want this blog to be a resource for devs and technologists, free not just in money, but in freedom from unwanted tracking and invasive ads.</p><h4 id="any-alternatives">Any Alternatives?<a href="#any-alternatives"></a></h4><p>I’m not entirely sure comments are needed here. There are other ways to reach me, for example; <a href="https://github.com/ryansouthgate" target="_blank">GitHub</a> or <a href="https://twitter.com/ryan_southgate" target="_blank">Twitter/X</a>. But having a place for discussion under each post can be valuable.
If you have any recommendations for alternative commenting systems (especially those that respect privacy or are self-hosted), I’d love to hear them! Please reach out if you’ve found something that works well.</p><p>Thanks as always for reading - your trust matters to me.</p><p><em>Sorry again for the mess!</em></p><hr><hr></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Companies Are Lying About AI Layoffs (173 pts)]]></title>
            <link>https://huijzer.xyz/posts/111/companies-are-lying-about-ai-layoffs</link>
            <guid>45423088</guid>
            <pubDate>Tue, 30 Sep 2025 08:07:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://huijzer.xyz/posts/111/companies-are-lying-about-ai-layoffs">https://huijzer.xyz/posts/111/companies-are-lying-about-ai-layoffs</a>, See on <a href="https://news.ycombinator.com/item?id=45423088">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-post-link="/posts/111/companies-are-lying-about-ai-layoffs">

<p>Companies are saying that the economy and AI are causing the large amount of tech layoffs, but <a href="https://www.youtube.com/watch?v=e-Ecodxn5m4">Vanessa Wingårdh looked at the H-1B visa data</a> and noticed that the companies are lying.</p>
<p>She argues that the American employees are being replaced by cheaper H-1B visa workers.
<a href="https://www.wsj.com/politics/policy/h1b-visa-grassley-durbin-letter-tech-firms-8fe931e9">WSJ also reported on this due to some senators speaking up.</a>
As proof she uses the H-1B <a href="https://www.uscis.gov/tools/reports-and-studies/h-1b-employer-data-hub">data from the U.S. Citizenship and Immigration Services</a>.
When filtering by fiscal years 2023, 2024 and 2025, the number of Beneficiaries Approved for some of the big corporations as of 30 June 2025 are shown in the second column below.
I also searched a bit for news articles about layoffs and put those numbers in the third column.</p>
<table>
<thead>
<tr>
<th>Employer</th>
<th>Beneficiaries Approved</th>
<th>Layoffs</th>
</tr>
</thead>
<tbody>
<tr>
<td>AMAZON COM SERVICES LLC</td>
<td>19,327</td>
<td></td>
</tr>
<tr>
<td>INFOSYS LIMITED</td>
<td>17,489</td>
<td><a href="https://www.peoplematters.in/news/performance-management/infosys-layoff-spree-continues-200-employees-fired-for-the-fourth-time-45355">800</a></td>
</tr>
<tr>
<td>GOOGLE LLC</td>
<td>15,010</td>
<td><a href="https://americanbazaaronline.com/2025/08/29/google-cuts-35-workforce-offers-voluntary-exit-programs-466893/">12,000</a></td>
</tr>
<tr>
<td>MICROSOFT CORPORATION</td>
<td>14,707</td>
<td><a href="https://timesofindia.indiatimes.com/technology/tech-news/microsoft-salesforce-oracle-and-intel-among-tech-giants-that-have-cut-jobs-in-2025/articleshow/124085478.cms">9,000</a> / <a href="https://economictimes.indiatimes.com/news/international/global-trends/us-news-tech-layoffs-2025-surge-in-us-amazon-microsoft-meta-slash-thousands-of-jobs-check-full-list-of-companies-affected/articleshow/123735499.cms">15,000</a></td>
</tr>
<tr>
<td>META PLATFORMS INC</td>
<td>13,338</td>
<td><a href="https://economictimes.indiatimes.com/news/international/global-trends/us-news-tech-layoffs-2025-surge-in-us-amazon-microsoft-meta-slash-thousands-of-jobs-check-full-list-of-companies-affected/articleshow/123735499.cms">3,000</a></td>
</tr>
<tr>
<td>AMAZON.COM SERVICES LLC</td>
<td>12,629</td>
<td></td>
</tr>
<tr>
<td>APPLE INC</td>
<td>11,896</td>
<td><a href="https://apnews.com/article/apple-layoffs-tech-iphone-workers-9f10788b1d3552385ee8c7b20209ce17">600</a></td>
</tr>
<tr>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr>
<td>IBM CORPORATION</td>
<td>6,989</td>
<td><a href="https://www.techzine.eu/news/infrastructure/129800/ibm-lays-off-thousands-of-employees-again-in-new-round-of-layoffs/">thousands</a></td>
</tr>
<tr>
<td>AMAZON WEB SERVICES</td>
<td>6,667</td>
<td><a href="https://www.capacitymedia.com/article-amazons-aws-cuts">hundreds</a></td>
</tr>
<tr>
<td>INTEL CORPORATION</td>
<td>6,322</td>
<td><a href="https://timesofindia.indiatimes.com/technology/tech-news/microsoft-salesforce-oracle-and-intel-among-tech-giants-that-have-cut-jobs-in-2025/articleshow/124085478.cms">5,000</a> / <a href="https://www.pcmag.com/news/intel-confirms-mass-layoffs-over-24000-jobs-to-be-cut-this-year">24,500</a></td>
</tr>
<tr>
<td>ORACLE AMERICA INC</td>
<td>6,292</td>
<td><a href="https://www.cio.com/article/4062711/product-changes-likely-as-oracle-faces-an-estimated-10000-more-layoffs-by-december.html">10,000</a></td>
</tr>
<tr>
<td>ACCENTURE LLP</td>
<td>5,862</td>
<td><a href="https://www.deccanherald.com/business/companies/layoffs-accenture-cuts-over-11000-jobs-in-3-months-amid-ai-push-3744531">11,000</a></td>
</tr>
</tbody>
</table>
<p>A commenter on <a href="https://news.ycombinator.com/item?id=45423088#45424038">Hacker News</a> writes: "You can go on Blind, Fishbowl, any work related subreddit, etc. and hear the same story over and over and over - 'My company replaced half my department with H1Bs or simply moved it to an offshore center in India, and then on the next earnings call announced that they had replaced all those jobs with AI'. There's a reason why 'AI = Actually Indians' is a meme everywhere on the internet, and it isn't racism, it's just people observing the reality around them."</p>
<p>I searched a bit to verify these claims and it seems plausible since I could without much effort find a report from Boeing in the <a href="https://www.reddit.com/r/boeing/comments/1f1qqud/engineers_replaced_by_h1b/">2000s</a>, and that indeed the replacing of US workers by H-1Bs seems commonly agreed upon <a href="https://www.reddit.com/r/recruitinghell/comments/1nmw5c8/has_anyone_inadvertently_trained_their_h1b/">here</a>, <a href="https://www.reddit.com/r/centrist/comments/1hnodib/h1b_visa_exploited_for_decades/">here</a>, and <a href="https://www.reddit.com/r/AskALiberal/comments/1m9h4x6/what_do_you_think_of_microsoft_laying_off_workers/">here</a>.</p>
<p>Another commenter pointed out that Accenture proposed hiring <a href="https://www.reuters.com/business/autos-transportation/accenture-proposes-new-campus-indias-andhra-pradesh-eyes-adding-12000-jobs-2025-09-23/">12,000 employees in a new campus in India</a>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bcachefs removed from the mainline kernel (218 pts)]]></title>
            <link>https://lwn.net/Articles/1040120/</link>
            <guid>45423004</guid>
            <pubDate>Tue, 30 Sep 2025 07:52:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/Articles/1040120/">https://lwn.net/Articles/1040120/</a>, See on <a href="https://news.ycombinator.com/item?id=45423004">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<div><p>[Posted September 30, 2025 by corbet]
               </p></div>
<div><p>
After marking bcachefs "externally maintained" in 6.17, Linus Torvalds has
<a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=f2c61db29f27">removed
it entirely</a> for 6.18.  "<q>It's now a DKMS module, making the in-kernel
code stale, so remove it to avoid any version confusion.</q>"</p><hr>
            </div> <!-- ArticleText -->
<details open="">
      <summary><h3>Risks</h3>
      <p> Posted Sep 30, 2025 10:30 UTC (Tue)
                               by <b>patrakov</b> (subscriber, #97174)
                              [<a href="https://lwn.net/Articles/1040154/">Link</a>] 
      </p>
      </summary>
      

      
          
        
     </details>
<details open="">
      <summary><h3>Decision process</h3>
      <p> Posted Sep 30, 2025 12:44 UTC (Tue)
                               by <b>daeler</b> (subscriber, #130460)
                              [<a href="https://lwn.net/Articles/1040192/">Link</a>] (3 responses)
      </p>
      </summary>
      <p>
Just curious, no judgement: How is this decision made? Can Linus just decide that he removes something from the kernel?<br>
</p>

      
          
        
     <details open="">
      <summary><h3>Decision process</h3>
      <p> Posted Sep 30, 2025 12:56 UTC (Tue)
                               by <b>corbet</b> (editor, #1)
                              [<a href="https://lwn.net/Articles/1040195/">Link</a>] 
      </p>
      </summary>
      Yes, Linus can make that kind of decision.  He doesn't just do it on his own, though; there was a long series of public and private discussions that led up to this one.


      
          
        
     </details>
<a name="CommAnchor1040194"></a>
    <details open="">
      <summary><h3>Decision process</h3>
      <p> Posted Sep 30, 2025 12:57 UTC (Tue)
                               by <b>pizza</b> (subscriber, #46)
                              [<a href="https://lwn.net/Articles/1040194/">Link</a>] (1 responses)
      </p>
      </summary>
      

      
          
        
     <details open="">
      <summary><h3>Decision process</h3>
      <p> Posted Sep 30, 2025 13:37 UTC (Tue)
                               by <b>Lionel_Debroux</b> (subscriber, #30014)
                              [<a href="https://lwn.net/Articles/1040202/">Link</a>] 
      </p>
      </summary>
      <p>
Note that things go the other way round as well: security fixes available in Linus' Linux but missing from other kernels because the backporting process didn't occur for some reason (difficulty, interest, etc.).<br>
The older a third-party kernel version is, the more it is likely to be missing both backports for security fixes, and vulnerabilities in code introduced in newer versions but not backported to the given third-party kernel (some vendors perform large amounts of backports to their franken-kernels).<br>
</p>

      
          
        
     </details>
</details>
</details>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Geolocation and Starlink (138 pts)]]></title>
            <link>https://www.potaroo.net/ispcol/2025-09/starlinkgeo.html</link>
            <guid>45422514</guid>
            <pubDate>Tue, 30 Sep 2025 06:23:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.potaroo.net/ispcol/2025-09/starlinkgeo.html">https://www.potaroo.net/ispcol/2025-09/starlinkgeo.html</a>, See on <a href="https://news.ycombinator.com/item?id=45422514">Hacker News</a></p>
Couldn't get https://www.potaroo.net/ispcol/2025-09/starlinkgeo.html: AggregateError]]></description>
        </item>
        <item>
            <title><![CDATA[European Union Public Licence (EUPL) (214 pts)]]></title>
            <link>https://eupl.eu/</link>
            <guid>45422512</guid>
            <pubDate>Tue, 30 Sep 2025 06:23:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://eupl.eu/">https://eupl.eu/</a>, See on <a href="https://news.ycombinator.com/item?id=45422512">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p>License 1.2: [<a href="https://eupl.eu/1.2/bg/">BG</a>] [<a href="https://eupl.eu/1.2/cs/">CS</a>] [<a href="https://eupl.eu/1.2/da/">DA</a>] [<a href="https://eupl.eu/1.2/de/">DE</a>] [<a href="https://eupl.eu/1.2/el/">EL</a>] [<a href="https://eupl.eu/1.2/en/">EN</a>] [<a href="https://eupl.eu/1.2/es/">ES</a>] [<a href="https://eupl.eu/1.2/et/">ET</a>] [<a href="https://eupl.eu/1.2/fi/">FI</a>] [<a href="https://eupl.eu/1.2/fr/">FR</a>] [<a href="https://eupl.eu/1.2/hr/">HR</a>] [<a href="https://eupl.eu/1.2/hu/">HU</a>] [<a href="https://eupl.eu/1.2/it/">IT</a>] [<a href="https://eupl.eu/1.2/lt/">LT</a>] [<a href="https://eupl.eu/1.2/lv/">LV</a>] [<a href="https://eupl.eu/1.2/mt/">MT</a>] [<a href="https://eupl.eu/1.2/nl/">NL</a>] [<a href="https://eupl.eu/1.2/pl/">PL</a>] [<a href="https://eupl.eu/1.2/pt/">PT</a>] [<a href="https://eupl.eu/1.2/ro/">RO</a>] [<a href="https://eupl.eu/1.2/sk/">SK</a>] [<a href="https://eupl.eu/1.2/sl/">SL</a>] [<a href="https://eupl.eu/1.2/sv/">SV</a>].
    </p><h2>What is the EUPL?</h2>
    <p>EUPL is an acronym for "European Union Public Licence".</p>
    <p>The first EUPL draft (v.0.1) went public in June 2005. A public debate was then organised by the European Commission (IDABC). The consultation of the developers and users community was very productive and has lead to many improvements of the draft licence; 10 out of 15 articles were modified. Based on the results of these modifications (a detailed report and the draft EUPL v.0.2), the European Commission  elaborated a final version (v.1.0) that was officially approved on 9 January 2007, in three linguistic versions.</p>
    <p>By a second Decision of 9 January 2008, the European Commission validated the EUPL in all the official languages of the European Union.</p>
    <p>By a third Decision of 9 January 2009, the European Commission clarified specific points of the EUPL, publishing the version 1.1 in all the official languages of the European Union.</p>
    <p>The Commission Implementing Decision (EU) 2017/863 of 18 May 2017 updating the open source software licence EUPL to further facilitate the sharing and reuse of software developed by public administrations (OJ 19/05/2017 L128 p. 59–64 ) published the version 1.2, with extended compatibility.</p>
    <h2>Why the EUPL?</h2>
    <p>The purpose of the European Commission is first of all to distribute its own software under the licence. Some applications developed in the framework of the IDABC programme, such as Circabc, or Eusurvey have already been licensed under the EUPL in 2007. Other European Institutions are also interested in using the new licence.</p>
    <p>But why creating a new legal instrument from scratch when more than 100 other F/OSS licences exist, such as the GPL, the BSD or the OSL? The reason is that in a detailed legal study no existing licence was found to correspond to the requirements of the European Commission:</p>
    <ul>
      <li>The Licence should have equal legal value in all EU languages;</li>
      <li>The terminology regarding intellectual property rights had to be conformant with European law requirements;</li>
      <li>To be valid in all Member States, limitations of liability or warranty had to be precise, and not formulated "to the extend allowed by the law" as in most licences designed with the legal environment of the United States in mind.</li>
    </ul>
    <h2>Objectives</h2>
    <p>The main objective of the European Commission is to distribute widely and promote the use of software owned by itself and other European Institutions under an Free/Open Source Licence conform to European law requirements.</p>
    <p>The EUPL is however written in neutral terms so that a broader use might be envisaged.</p>
    <p>In addition, distribution of software should avoid the exclusive appropriation of the software even after improvement by a third party (therefore, the EUPL is a "copyleft" licence).</p>
    <h2>Who may use the EUPL?</h2>
    <p>Although the potential users of European Institutions' software are mostly other public sector administrations, there is nothing in the EUPL preventing its broader use. The EUPL could be used by anyone who holds the copyright to a piece of software. It could become – in various languages - an adequate legal interoperability instrument across Europe.</p>
    <p>Nevertheless, the EUPL purpose is not to compete with other licences. It might be used primarily by public administrations, either European or national, that would need a common licensing instrument to mutualise or share software and knowledge.</p>
    <h2>Is the EUPL compatible with the GPL and other F/OSS licences?</h2>
    <p>Yes, it is. The EUPL contains a unique compatibility clause and provides for a list of compatible copyleft licences. The GPL is one of them.</p>
    <p>For example, how would the interaction between the EUPL and the GPL play out in the case of CIRCA, an application a already distributed under the EUPL?</p>
    <p>A developer may merge the Circabc software with a GPL component, and then could license the new derivative work (another project, with a new name) under the GPL. It is not permitted to "re-license" CIRCA under the GPL. A developer will be also able to integrate CIRCA in existing GPL work called e.g. "MY-GPL-PROGRAM" and continue to license this improved work under the GPL licence that he had chosen originally.</p>
    <hr>
    <p><i>This website is not sponsored or endorsed by the <a href="https://ec.europa.eu/" target="_blank">European Commission</a> or any other institution, body or agency of the <a href="http://europa.eu/" target="_blank">European Union</a>.</i></p>
    <p><small>Created by <a href="https://www.javiercasares.com/" target="_blank">Javier Casares</a> (<a href="https://robotstxt.es/legal/" target="_blank">legal</a>) under license <a href="https://eupl.eu/">EUPL 1.2</a>.</small></p>
  </div></div>]]></description>
        </item>
    </channel>
</rss>