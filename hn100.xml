<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 25 Nov 2025 18:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Orion 1.0 – Browse Beyond (163 pts)]]></title>
            <link>https://blog.kagi.com/orion</link>
            <guid>46047350</guid>
            <pubDate>Tue, 25 Nov 2025 16:21:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.kagi.com/orion">https://blog.kagi.com/orion</a>, See on <a href="https://news.ycombinator.com/item?id=46047350">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            
            <p><img src="https://kagifeedback.org/assets/files/2025-11-25/1764079669-552357-1.png" alt="Kagi search interface displayed on the Orion Browser on laptop, tablet, and smartphone"></p>

<p>After six years of relentless development, <strong>Orion for MacOS 1.0 is here</strong>.</p>

<p>What started as a vision initiated by our founder, Vladimir Prelovac, has now come to fruition on Mac, iPhone, and iPad. Today, Orion for macOS officially leaves its beta phase behind and joins our iOS and iPadOS apps as a fully‑fledged, production‑ready browser.</p>

<p>While doing so, it expands Kagi ecosystem of privacy-respecting, user-centric products (that we have begun fondly naming “Kagiverse”) to now include: <a href="https://kagi.com/">Search</a>, <a href="https://kagi.com/assistant">Assistant</a>, <a href="https://orionbrowser.com/">Browser</a>, <a href="https://translate.kagi.com/">Translate</a>, <a href="https://news.kagi.com/">News</a> with more to come.</p>

<p>We built Orion for people who feel that modern browsing has drifted too far from serving the user. This is our invitation to <strong>browse beyond ✴︎ the status quo</strong>.</p>

<h2>Why a new browser?</h2>

<p>The obvious question is: <strong>why <em>the heck</em> do we need a new browser?</strong> The world already has Chrome, Safari, Firefox, Edge, and a growing list of “AI browsers.” Why add yet another?</p>

<p>Because something fundamental has been lost.</p>

<blockquote>
<p><strong>Zero telemetry, privacy‑first access to the internet: a basic human right.</strong></p>
</blockquote>

<p>Your browser is the most intimate tool you have on your computer. It sees everything you read, everything you search, everything you type. Do you want that relationship funded by advertisers, or by you?</p>

<p>With ad‑funded browsers and AI overlays, your activity is a gold mine. Every click becomes a way to track, every page another opportunity to profile you a little more deeply. We believe there needs to be a different path: <strong>a browser that answers only to its user.</strong></p>

<p>Orion is our attempt at that browser. No trade-offs between features and privacy. It’s fast, customizable, and uncompromising on both fronts.</p>

<h2>A bold technical choice: WebKit, not another Chromium clone</h2>

<p>In a world dominated by Chromium, choosing a rendering engine is an act of resistance.</p>

<p>From day one, we made the deliberate choice to build Orion on <a href="https://webkit.org/"><strong>WebKit</strong></a>, the open‑source engine at the heart of Safari and the broader Apple ecosystem. It gives us:</p>

<ul>
<li>A high‑performance engine that is <strong>deeply optimized for macOS and iOS</strong>.</li>
<li>An alternative to the growing Chromium monoculture.</li>
<li>A foundation that is not controlled by an advertising giant.</li>
</ul>

<p>Orion may feel familiar if you’re used to Safari – respecting your muscle memory and the aesthetics of macOS and iOS – but it is an entirely different beast under the hood. We combined native WebKit speed with a completely new approach to extensions, privacy, and customization.</p>

<p><img src="https://kagifeedback.org/assets/files/2025-11-25/1764079784-790203-orion-safari.png" alt="Orion and Safari browser windows displaying extension management interfaces with popular extensions listed"></p>

<h2>Speed by nature, privacy by default</h2>

<p>Most people switch browsers for one reason: <strong>speed</strong>.</p>

<p>Orion is designed to be fast by nature, not just in benchmarks, but in how it feels every day:</p>

<ul>
<li>A lean, native codebase without ad‑tech bloat.</li>
<li>Optimized startup, tab switching, and page rendering.</li>
<li>A UI that gets out of your way and gives you more screen real estate for content.</li>
</ul>

<p>Alongside speed, we treat privacy as a first‑class feature:</p>

<ul>
<li><strong>Zero Telemetry</strong>: We don’t collect usage data. No analytics, no identifiers, no tracking.</li>
<li><strong>No ad or tracking technology</strong> baked in: Orion is not funded by ads, so there is no incentive to follow you around the web.</li>
<li><strong>Built‑in protections</strong>: Strong content blocking and privacy defaults from the first launch.</li>
</ul>

<h2>Speed. Extensions. Privacy. Pick all three.</h2>

<p><img src="https://kagifeedback.org/assets/files/2025-11-25/1764079906-671487-settings.png" alt="Orion browser Privacy settings panel showing tracker removal, history deletion, cookie management, crash report options, and content blocker configuration."></p>

<h2>Thoughtful AI, security first</h2>

<p>We are excited about what AI can do for search, browsing, and productivity. Kagi, the company behind Orion, has been experimenting with AI‑powered tools for years while staying true to our <a href="https://help.kagi.com/kagi/why-kagi/ai-philosophy.html">AI integration philosophy</a>.</p>

<p>But we are also watching a worrying trend: AI agents are being rushed directly into the browser core, with deep access to everything you do online – and sometimes even to your local machine.</p>

<p>Security researchers have already documented serious issues in early AI browsers and “agentic” browser features:</p>

<ul>
<li><a href="https://labs.sqrx.com/comet-mcp-api-allows-ai-browsers-to-execute-local-commands-dec185fb524b">Hidden or undocumented APIs</a> that allowed embedded AI components to execute arbitrary local commands on users’ devices.</li>
<li><a href="https://medium.com/@mdmeeng01/perplexitys-comet-got-hijacked-by-hidden-prompts-and-it-changed-how-i-think-about-ai-browsers-fb22b673ece9">Prompt‑injection attacks</a> that trick AI agents into ignoring safety rules, visiting malicious sites, or leaking sensitive information beyond what traditional browser sandboxes were designed to protect.</li>
<li>Broader concerns that some implementations are effectively “<a href="https://medium.com/utopian/perplexity-was-supposed-to-change-search-instead-its-lighting-everything-on-fire-922379e72083">lighting everything on fire</a>” by expanding the browser’s attack surface and data flows in ways users don’t fully understand.</li>
</ul>

<p>Our stance is simple:</p>

<ul>
<li>We are not against AI, and we are conscious of <a href="https://blog.kagi.com/llms">its limitations</a>. We already integrate with AI‑powered services wherever it makes functional sense and will continue to expand those capabilities.</li>
<li><strong>We are against rushing insecure, always‑on agents into the browser core.</strong> Your browser should be a secure gateway, not an unvetted co‑pilot wired into everything you do.</li>
</ul>

<p>So today:</p>

<ul>
<li>Orion ships with <strong>no built‑in AI code</strong> in its core.</li>
<li>We focus on providing a clean, predictable environment, <strong>especially for enterprises and privacy‑conscious professionals</strong>.</li>
<li>Orion is designed to connect seamlessly to the AI tools you choose – soon including Kagi’s intelligent features – while keeping a clear separation between your browser and any external AI agents.</li>
</ul>

<p>As AI matures and security models improve, we’ll continue to evaluate thoughtful, user‑controlled ways to bring AI into your workflow without compromising safety, privacy or user choice.</p>

<h2>Simple for everyone, limitless for experts</h2>

<p>We designed Orion to bridge the gap between simplicity and power. Out of the box, it’s a clean, intuitive browser for anyone. Under the hood, it’s a deep toolbox for people who live in their browser all day.</p>

<p>Some of the unique features you’ll find in Orion 1.0:</p>

<ul>
<li><p><strong>Focus Mode</strong>: Instantly transform any website into a distraction‑free web app. Perfect for documentation, writing, or web apps you run all day.
<img src="https://kagifeedback.org/assets/files/2025-11-25/1764079998-879831-focusmode.gif" alt="Browser window showing Focus Mode being activated, simplifying webpage content by removing distractions."></p></li>

<li><p><strong>Link Preview</strong>: Peek at content from any app – email, notes, chat – without fully committing to opening a tab, keeping your workspace tidy.</p></li>

<li><p><strong>Mini Toolbar, Overflow Menu, and Page Tweak</strong>: Fine‑tune each page’s appearance and controls, so the web adapts to you, not the other way around.</p></li>

<li><p><strong>Profiles as Apps</strong>: Isolate your work, personal, and hobby browsing into completely separate profiles, each with its own extensions, cookies, and settings.</p></li>
</ul>

<p><img src="https://kagifeedback.org/assets/files/2025-11-25/1764080084-770819-profiles.png" alt="Orion browser Profiles management screen showing Primary, Incognito, and Business profiles with sidebar navigation menu."></p>

<p>For power users, we’ve added granular options throughout the browser. These are there when you want them, and out of your way when you don’t.</p>

<p>Orion 1.0 also reflects six years of <a href="https://orionfeedback.org/">feedback</a> from early adopters. Many invisible improvements – tab stability, memory behavior, complex web app compatibility – are a direct result of people pushing Orion hard in their daily workflows and telling us what broke.</p>

<h2>Browse Beyond ✴︎: our new signature</h2>

<p>With this release, we are introducing our new signature: <strong>Browse Beyond ✴︎</strong>.</p>

<p>We originally started with the browser name ‘Kagi.’ On February 3, 2020, Vlad suggested a shortlist for rebranding: Comet, Core, Blaze, and Orion. We chose Orion not just for the name itself, but because it perfectly captured our drive for exploration and curiosity. It was a natural fit that set the stage for everything that followed.</p>

<p><img src="https://kagifeedback.org/assets/files/2025-11-25/1764080256-734784-orionlogo-search.png" alt="Evolution of logo designs from water droplet through rocket, astronaut, infinity symbol, lighthouse, robot, to final cosmic sphere design."></p>

<p>You’ll see this reflected in our refreshed visual identity:</p>

<ul>
<li>A star (✴︎) motif throughout our communication.</li>
<li>A refined logo that now uses <strong>the same typeface as Kagi</strong>, creating a clear visual bond between our browser and our search engine.</li>
</ul>

<p><img src="https://kagifeedback.org/assets/files/2025-11-25/1764080330-747115-kagi-orion.png" alt="Kagi logo on orange background next to Orion browser logo with star icon and “Browse Beyond” tagline on purple background."></p>

<p>Orion is part of the broader <strong>Kagi ecosystem</strong>, united by a simple idea: the internet should be built for people, not advertisers or any other third parties.</p>

<h2>Small team, sustainable model</h2>

<p>Orion is built by a team of just six developers.</p>

<p>To put that in perspective:</p>

<ul>
<li>That’s roughly 10% of the size of the “small” browser teams at larger companies.</li>
<li>And a rounding error compared to the teams behind Chrome or Edge.</li>
</ul>

<p>Yet, the impact is real: over <strong>1 million downloads to date</strong>, and a dedicated community of <a href="https://kagi.com/stats?stat=orion">2480</a> paid subscribers who make this independence possible.</p>

<p>For the first two years, development was carried out by a single developer. Today, we are a tight knit group operating close to our users. We listen, debate, and implement fixes proposed directly by our community on <a href="https://orionfeedback.org/">OrionFeedback.org</a>.</p>

<p>This is our only source of decision making, rather than any usage analytics or patterns, because remember, Orion is zero-telemetry!</p>

<p>This small team approach lets us move quickly, stay focused, and avoid the bloat or hype that often comes with scale.</p>

<h2>Free, yet self‑funded</h2>

<p><strong>Orion is free for everyone.</strong></p>

<p><strong>Every user also receives 200 free Kagi searches</strong>, with no account or sign‑up required. It’s our way of introducing you to fast, ad‑free, privacy‑respecting search from day one.</p>

<p>But we are also 100% self‑funded. We don’t sell your data and we don’t take money from advertisers, which means we rely directly on our users to sustain the project.</p>

<p>There are three ways to <a href="https://kagi.com/onboarding?p=orion_plan">contribute to Orion’s future</a>:</p>

<ul>
<li>Tip Jar (from the app): A simple way to say “thank you” without any commitment.</li>
<li>Supporter Subscription: $5/month or $50/year.</li>
<li>Lifetime Access: A one‑time payment of $150 for life.</li>
</ul>

<p>Supporters (via subscription or lifetime purchase) unlock a set of <strong>Orion+</strong> perks available today, including:</p>

<ul>
<li>Floating windows: Keep a video or window on top of other apps.</li>
<li>Customization: Programmable buttons and custom application icons.</li>
<li>Early access to new, supporter‑exclusive features we’re already building for next year.</li>
</ul>

<p>By supporting Orion, you’re not just funding a browser – you are co‑funding a better web with humans at the center.</p>

<h2>Orion everywhere you are</h2>

<p>Orion 1.0 is just the beginning. Our goal is simple: <strong>Browse Beyond, everywhere.</strong></p>

<ul>
<li><p><strong>Orion for macOS</strong><br>
Our flagship browser, six years in the making. Built natively for Mac, with performance and detail that only come from living on the platform for a long time. <a href="https://cdn.kagi.com/downloads/OrionInstaller.dmg">Download it now</a>.</p></li>

<li><p><strong>Orion for iOS and iPadOS</strong><br>
Trusted daily by users who want features no other mobile browser offers. Native iOS performance with capabilities that redefine what’s possible on mobile. <a href="https://apps.apple.com/us/app/orion-browser-by-kagi/id1484498200">Download it now</a>.</p></li>

<li><p><strong>Orion for Linux (Alpha)</strong><br>
Currently in alpha for users who value choice and independence. Native Linux performance, with the same privacy‑first approach as on macOS.<br>
<a href="https://form.kagi.com/orion-linux-newsletter">Sign up for our newsletter</a> to follow development and join the early testing wave.</p></li>

<li><p><strong>Orion for Windows (in development)</strong><br>
We have officially started development on Orion for Windows, with a target release scheduled for <strong>late 2026</strong>. Our goal is full parity with Orion 1.0 for macOS, including synchronized profiles and Orion+ benefits across platforms. <a href="https://form.kagi.com/orion-windows-newsletter">Sign up for our newsletter</a> to follow development and join the early testing wave.</p></li>
</ul>

<p><img src="https://kagifeedback.org/assets/files/2025-11-25/1764080457-66026-linux-windows.png" alt="Kagi Privacy Pass feature displayed in Orion browser windows on Linux and Windows operating systems with construction barrier icons."></p>

<p>Synchronization will work seamlessly across devices, so your browsing experience follows you, not the other way around.</p>

<h2>What people say</h2>

<p>From early testers to privacy advocates and power users, Orion has grown through the voices of its community.</p>

<p><img src="https://kagifeedback.org/assets/files/2025-11-25/1764080537-104392-oriontestimonials.jpg" alt="Social media posts praising Orion browser, highlighting its speed, privacy features, extension support, and integration with Kagi search engine."></p>

<p>We’ll continue to surface community stories and feedback as Orion evolves. If you share your experience publicly, there’s a good chance we’ll see it.</p>

<h2>The road ahead</h2>

<p>Hitting v1.0 is a big milestone, but we’re just getting started.</p>

<p>Over the next year, our roadmap is densely packed with:</p>

<ul>
<li>Deeper customization options for power users.</li>
<li>Further improvements to stability and complex web app performance.</li>
<li>New Orion+ features that push what a browser can do while keeping it simple for everyone else.</li>
<li>Tighter integrations with Kagi’s intelligent tools – always under your control, never forced into your workflow.</li>
</ul>

<p>We’re also working on expanding and improving our website to better showcase everything Orion can do, including better documentation and onboarding for teams that want to standardize on Orion.</p>

<p>Meanwhile, <a href="https://x.com/OrionBrowser">follow our X account</a> where we’ll be dropping little freebies on the regular (and don’t worry, we’ll be posting these <a href="https://help.kagi.com/kagi/support-and-community/#social-media">elsewhere on socials</a> as well!)</p>

<p><img src="https://kagifeedback.org/assets/files/2025-11-25/1764080713-608754-orionx2.png" alt="Screenshot of Orion Browser’s account on X"></p>

<p>Thank you for choosing to <strong>Browse Beyond</strong> with us.</p>

<ul>
<li><a href="https://cdn.kagi.com/downloads/OrionInstaller.dmg"><code>Download Orion 1.0 for macOS</code> </a></li>
<li><a href="https://apps.apple.com/us/app/orion-browser-by-kagi/id1484498200"><code>Get Orion for iOS and iPadOS</code></a><br>
</li>
<li><a href="https://form.kagi.com/orion-linux-newsletter"><code>Sign up for Orion for Linux Alpha</code> </a></li>
<li><a href="https://form.kagi.com/orion-windows-newsletter"><code>Join the newsletter for Windows updates</code></a></li>
</ul>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apt Rust requirement raises questions (204 pts)]]></title>
            <link>https://lwn.net/SubscriberLink/1046841/5bbf1fc049a18947/</link>
            <guid>46045972</guid>
            <pubDate>Tue, 25 Nov 2025 14:18:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/SubscriberLink/1046841/5bbf1fc049a18947/">https://lwn.net/SubscriberLink/1046841/5bbf1fc049a18947/</a>, See on <a href="https://news.ycombinator.com/item?id=46045972">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<blockquote>
<div>
<h3>Welcome to LWN.net</h3>
<p>
The following subscription-only content has been made available to you 
by an LWN subscriber.  Thousands of subscribers depend on LWN for the 
best news from the Linux and free software communities.  If you enjoy this 
article, please consider <a href="https://lwn.net/subscribe/">subscribing to LWN</a>.  Thank you
for visiting LWN.net!
</p></div>
</blockquote>

<p>It is rarely newsworthy when a project or package picks up a new
dependency. However, changes in a core tool like Debian's <a href="https://salsa.debian.org/apt-team/apt#apt">Advanced Package
Tool</a> (APT) can have far-reaching effects. For example, Julian
Andres Klode's <a href="https://lwn.net/ml/all/20251031213541.GA73786@debian.org/">declaration</a>
that APT would require Rust in May 2026 means that a few of Debian's
unofficial ports must either acquire a working Rust toolchain or
depend on an old version of APT. This has raised several questions
within the project, particularly about the ability of a single
maintainer to make changes that have widespread impact.</p>

<p>On October 31, Klode sent an announcement to the debian-devel
mailing list that he intended to introduce Rust dependencies and code
into APT as soon as May 2026:</p>

<blockquote>
<p>This extends at first to the Rust compiler and standard library, and
the Sequoia ecosystem.</p>

<p>In particular, our code to parse .deb, .ar, .tar, and the HTTP
signature verification code would strongly benefit from memory safe
languages and a stronger approach to unit testing.</p>

<p>If you maintain a port without a working Rust toolchain,
please ensure it has one within the next 6 months, or
sunset the port.</p>
</blockquote>

<p>Klode added this was necessary so that the project as a whole could
move forward, rely on modern technologies, "<q>and not be held back by
trying to shoehorn modern software on retro computing
devices</q>". Some Debian developers have welcomed the news. Paul
Tagliamonte <a href="https://lwn.net/ml/all/CAO6P2QQD8MDUTAo_F=kGfDsBF0Xv+Wv020dm-n-WGnb7ODYW-g@mail.gmail.com">acknowledged</a>
that it would impact unofficial Debian ports but called the push
toward Rust "<q>welcome news</q>".</p>

<p>However, John Paul Adrian Glaubitz <a href="https://lwn.net/ml/all/ad6e60711c8ed3372ed7f324d7b1be23b0722a0d.camel@physik.fu-berlin.de/">complained</a>
that Klode's wording was unpleasant and that the approach was
confrontational. In <a href="https://lwn.net/ml/all/afb7c58fb0fb995ebde3fb72b4cc4d1943a37923.camel@physik.fu-berlin.de/">another
message</a>, he explained that he was not against adoption of Rust; he
had worked on enabling Rust on many of the Debian architectures and
helped to fix architecture-specific bugs in the Rust toolchain as well
as LLVM upstream. However, the message strongly suggested there was no room
for a change in plan: Klode had ended his message with "<q>thank you for
understanding</q>", which invited no further discussion. Glaubitz was
one of a few Debian developers who expressed discomfort with Klode's
communication style in the message.</p>

<p>Klode <a href="https://lwn.net/ml/all/20251031223819.GA97356@debian.org">noted</a>,
briefly, that Rust was already a hard requirement for all Debian
release architectures and ports, except for <a href="https://www.debian.org/ports/alpha/">Alpha (alpha)</a>, <a href="https://www.debian.org/ports/m68k/">Motorola 680x0 (m68k)</a>,
<a href="https://www.debian.org/ports/hppa/">PA-RISC (hppa)</a>, and
<a href="https://wiki.debian.org/SH4">SuperH (sh4)</a>, because of
APT's use of the <a href="https://sequoia-pgp.org/">Sequoia-PGP</a>
project's <a href="https://packages.debian.org/trixie/sqv"><tt>sqv</tt></a> tool to
verify <a href="https://openpgp.dev/book/signatures.html">OpenPGP</a>
signatures. APT falls back to using the GNU Privacy Guard
signature-verification tool, <a href="https://packages.debian.org/trixie/gpgv"><tt>gpgv</tt></a>, on
ports that do not have a Rust compiler. By depending directly on Rust,
though, APT itself would not be available on ports without a Rust
compiler. LWN <a href="https://lwn.net/Articles/1045363/">recently
covered</a> the state of Linux architecture support, and the status of
Rust support for each one.</p>

<!-- middle-ad -->

<p>None of the ports listed by Klode are among those <a href="https://www.debian.org/ports/#:~:text=List%20of%20official%20ports,-These">officially
supported</a> by Debian today, or targeted for support in
Debian&nbsp;14 ("forky"). The sh4 port has never been officially
supported, and none of the other ports have been supported since
Debian&nbsp;6.0. The actual impact on the ports lacking Rust is also
less dramatic than it sounded at first. Glaubitz <a href="https://lwn.net/ml/all/708d1a6e63d53242cf89f01c2d791e91f1eccab6.camel@physik.fu-berlin.de/">assured</a>
Antoni Boucher that "<q>the ultimatum that Julian set doesn't really
exist</q>", but phrasing it that way "<q>gets more attention in the
news</q>". Boucher is the maintainer of <a href="https://rust-for-linux.com/rustc_codegen_gcc"><tt>rust_codegen_gcc</tt></a>,
a <a href="https://lwn.net/Articles/954787/#codegen_gcc">GCC
ahead-of-time code generator for Rust</a>. Nothing, Glaubitz said,
stops ports from using a non-Rust version of APT until Boucher and
others manage to bootstrap Rust for those ports.</p>

<h4>Security theater?</h4>

<p>David Kalnischkies, who is also a <a href="https://salsa.debian.org/apt-team/apt/-/commits/main?author=David%20Kalnischkies">major
contributor</a> to APT, <a href="https://lwn.net/ml/all/sdi72zvp4koyi7h7wo2bwslds2j466ix4sar7wuaks3szphjmp@xg6itydwhqbp/">suggested</a>
that if the goal is to reduce bugs, it would be better to remove the
code that is used to parse the .deb, .ar, and .tar formats that Klode
mentioned from APT entirely. It is only needed for two tools, <a href="https://manpages.debian.org/trixie/apt-utils/apt-ftparchive.1.en.html"><tt>apt-ftparchive</tt></a>
and <a href="https://manpages.debian.org/trixie/apt-utils/apt-extracttemplates.1.en.html"><tt>apt-extracttemplates</tt></a>,
he said, and the only "<q>serious usage</q>" of
<tt>apt-ftparchive</tt> was by Klode's employer, Canonical, for its <a href="https://launchpad.net/">Launchpad</a> software-collaboration
platform. If those were taken out of the main APT code base, then it
would not matter whether they were written in Rust, Python, or another
language, since the tools are not directly necessary for any given
port.</p>

<p>Kalnischkies also questioned the claim that Rust was necessary to
achieve the stronger approach to unit testing that Klode mentioned:</p>

<blockquote>
<p>You can certainly do unit tests in C++, we do. The main problem is
that someone has to write those tests. Like docs.</p>

<p>Your new solver e.g. has none (apart from our preexisting integration
tests). You don't seriously claim that is because of C++ ?
If you don't like GoogleTest, which is what we currently have,
I could suggest doctest (as I did in previous installments).
Plenty other frameworks exist with similar or different styles.</p>
</blockquote>

<p>Klode has not responded to those comments yet, which is a bit
unfortunate given the fact that introducing hard dependencies on
Rust has an impact beyond his own work on APT. It may well be that he
has good answers to the questions, but it can also give the
impression that Klode is simply embracing a trend toward Rust. He is <a href="https://discourse.ubuntu.com/t/migration-to-rust-coreutils-in-25-10/59708">involved</a>
in the Ubuntu <a href="https://lwn.net/Articles/1014002/">work to migrate from GNU Coreutils to the Rust-based uutils</a>. The reasons given for that work, again, are around
modernization and better security—but security is not automatically 
guaranteed simply by switching to Rust, and there are a number of
other considerations.</p>

<p>For example, Adrian Bunk <a href="https://lwn.net/ml/all/aQkl0qyIyJ5+y5lC@localhost/">pointed
out</a> that there are a number of Debian teams, as well as tooling,
that will be impacted by writing some of APT in Rust. The release
notes for Debian&nbsp;13 ("trixie") <a href="https://www.debian.org/releases/trixie/release-notes/issues.html#go-and-rust-based-packages">mention</a>
that Debian's infrastructure "<q>currently has problems with
rebuilding packages of types that systematically use static
linking</q>", such as those with code written in Go and Rust. Thus, "<q>these packages will be
covered by limited security support until the infrastructure is
improved to deal with them maintainably</q>". Limited security support
means that updates to Rust libraries are likely to only be released
when Debian publishes a point release, which happens about every two
months. The security team has <a href="https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=1106154#10">specifically
stated</a> that <tt>sqv</tt> is fully supported, but there are still
outstanding problems.</p>

<p>Due to the static-linking issue, any time one of <tt>sqv</tt>'s
dependencies, currently more than 40 Rust crates, have to be rebuilt
due to a security issue, <tt>sqv</tt> (at least potentially) also
needs to be rebuilt. There are also difficulties in tracking CVEs for
all of its dependencies, and understanding when a security
vulnerability in a Rust crate may require updating a Rust program that
depends on it.</p>

<p>Fabian Grünbichler, a maintainer of Debian's Rust toolchain, <a href="https://lwn.net/ml/all/c818f7ef-eff7-40ac-b153-a88412f71d86@app.fastmail.com/">listed</a>
several outstanding problems Debian has with dealing with Rust
packages. One of the largest is the need for a consistent Debian policy for declaring
statically linked libraries. In 2022, Guillem Jover added a control
field for Debian packages called Static-Built-Using (SBU), which would list
the source packages used to build a binary package. This would
indicate when a binary package needs to be rebuilt due to an update in
another source package. For example, <tt>sqv</tt> depends on more than
40 Rust crates that are packaged for Debian. Without declaring the
SBUs, it may not be clear if <tt>sqv</tt> needs to be updated when one
of its dependencies is updated. Debian has been working on a <a href="https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=1069256">policy
requirement for SBU</a> since April 2024, but it is not yet finished
or adopted.</p>

<p>The discussion sparked by Grünbichler makes clear that most of
Debian's Rust-related problems are in the process of being
solved. However, there's no evidence that Klode explored the problems
before declaring that APT would depend on Rust, or even asked "is this
a reasonable time frame to introduce this dependency?"</p>

<h4>Where tradition meets tomorrow</h4>

<p>Debian's tagline, or at least one of its taglines, is "the
universal operating system", meaning that the project aims to run on a
wide variety of hardware (old and new) and be usable on the desktop,
server, IoT devices, and more. The "<a href="https://www.debian.org/intro/why_debian">Why Debian</a>" page
lists a number of reasons users and developers should choose the
distribution: <a href="https://www.debian.org/ports">multiple hardware
architectures</a>, <a href="https://wiki.debian.org/LTS">long-term
support</a>, and its <a href="https://www.debian.org/devel/constitution">democratic governance
structure</a> are just a few of the arguments it puts forward in favor
of Debian. It also notes that "<q>Debian cannot be controlled by a
single company</q>". A single developer employed by a company to work
on Debian tools pushing a change that seems beneficial to that
company, without discussion or debate, that impacts multiple hardware
architectures and that requires other volunteers to do unplanned work
or meet an artificial deadline seems to go against many of the
project's stated values.</p>

<p>Debian, of course, does have checks and balances that could be
employed if other Debian developers feel it necessary. Someone could,
for example, appeal to Debian's <a href="https://www.debian.org/devel/tech-ctte">Technical Committee</a>,
or sponsor a general resolution to override a developer if they cannot
be persuaded by discussion alone. That happened recently when the <a href="https://lwn.net/Articles/1041316/">committee required systemd
maintainers to provide the <tt>/var/lock</tt> directory</a> "<q>until
a satisfactory migration of impacted software has occurred and Policy
updated accordingly</q>".</p>

<p>However, it also seems fair to point out that Debian can move
slowly, even glacially, at times. APT <a href="https://mvogt.wordpress.com/2015/11/30/apt-1-1-released/">added</a>
support for the <a href="https://repolib.readthedocs.io/en/latest/deb822-format.html">DEB822</a>
format for its source information lists in 2015. Despite APT
supporting that format for years, Klode faced resistance in 2021, when
he <a href="https://lwn.net/ml/all/20211103163429.GA3688731@debian.org/">pushed
for Debian to move to the new format</a> ahead of the Debian&nbsp;12
("bookworm") release in 2021, but was unsuccessful. It is now the
default for trixie with the move to <a href="https://lwn.net/Articles/1017315/">APT&nbsp;3.0</a>, though APT
will continue to support the old format for years to come.</p>

<p>The fact is, regardless of what Klode does with APT, more and more
free software is being written (or rewritten) in Rust. Making it
easier to support that software when it is packaged for Debian is to
everyone's benefit. Perhaps the project needs some developers who will
be aggressive about pushing the project to move more quickly in
improving its support for Rust. However, what is really needed is more
developers lending a hand to do the work that is needed to support
Rust in Debian and elsewhere, such as <a href="https://rust-gcc.github.io/"><tt>gccrs</tt></a>. It does not
seem in keeping with Debian's community focus for a single developer
to simply declare dependencies that other volunteers will have to
scramble to support.</p>

<br clear="all">
               <br clear="all">
               <hr>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Brain has five 'eras' with adult mode not starting until early 30s (192 pts)]]></title>
            <link>https://www.theguardian.com/science/2025/nov/25/brain-human-cognitive-development-life-stages-cambridge-study</link>
            <guid>46045661</guid>
            <pubDate>Tue, 25 Nov 2025 13:38:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/science/2025/nov/25/brain-human-cognitive-development-life-stages-cambridge-study">https://www.theguardian.com/science/2025/nov/25/brain-human-cognitive-development-life-stages-cambridge-study</a>, See on <a href="https://news.ycombinator.com/item?id=46045661">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>Scientists have identified five major “epochs” of human brain development in one of the most comprehensive studies to date of how neural wiring changes from infancy to old age.</p><p><a href="https://www.nature.com/articles/s41467-025-65974-8" data-link-name="in body link">The study</a>, based on the brain scans of nearly 4,000 people aged under one to 90, mapped neural connections and how they evolve during our lives. This revealed five broad phases, split up by four pivotal “turning points” in which brain organisation moves on to a different trajectory, at around the ages of nine, 32, 66 and 83 years.</p><p>“Looking back, many of us feel our lives have been characterised by different phases. It turns out that brains also go through these eras,” said Prof Duncan Astle, a researcher in neuroinformatics at Cambridge University and senior author of the study.</p><p>“Understanding that the brain’s structural journey is not a question of steady progression, but rather one of a few major turning points, will help us identify when and how its wiring is vulnerable to disruption.”</p><p>The childhood period of development was found to occur between birth until the age of nine, when it transitions to the adolescent phase – an era that lasts up to the age of 32, on average.</p><p>In a person’s early 30s the brain’s neural wiring shifts into adult mode – the longest era, lasting more than three decades. A third turning point around the age of 66 marks the start of an “early ageing” phase of brain architecture. Finally, the “late ageing” brain takes shape at around 83 years old.</p><p>The scientists quantified brain organisation using 12 different measures, including the efficiency of the wiring, how compartmentalised it is and whether the brain relies heavily on central hubs or has a more diffuse connectivity network.</p><p>From infancy through childhood, our brains are defined by “network consolidation”, as the wealth of synapses – the connectors between neurons – in a baby’s brain are whittled down, with the more active ones surviving. During this period, the study found, the efficiency of the brain’s wiring decreases.</p><p>Meanwhile, grey and white matter grow rapidly in volume, so that cortical thickness – the distance between outer grey matter and inner white matter – reaches a peak, and cortical folding, the characteristic ridges on the outer brain, stabilises.</p><p>In the second “epoch” of the brain, the adolescence era, white matter continues to grow in volume, so organisation of the brain’s communications networks is increasingly refined. This era is defined by steadily increasing efficiency of connections across the whole brain, which is related to enhanced cognitive performance. The epochs were defined by the brain remaining on a constant trend of development over a sustained period, rather than staying in a fixed state throughout.</p><p>“We’re definitely not saying that people in their late 20s are going to be acting like teenagers, or even that their brain looks like that of a teenager,” said Alexa Mousley, who led the research. “It’s really the pattern of change.”</p><p>She added that the findings could give insights into risk factors for mental health disorders, which most frequently emerge during the adolescent period.</p><p>At around the age of 32 the strongest overall shift in trajectory is seen. Life events such as parenthood may play a role in some of the changes seen, although the research did not explicitly test this. “We know that women who give birth, their brain changes afterwards,” said Mousley. “It’s reasonable to assume that there could be a relationship between these milestones and what’s happening in the brain.”</p><p>From 32 years, the brain architecture appears to stabilise compared with previous phases, corresponding with a “plateau in intelligence and personality” based on other studies. Brain regions also become more compartmentalised.</p><p>The final two turning points were defined by decreases in brain connectivity, which were believed to be related to ageing and degeneration of white matter in the brain.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Making Crash Bandicoot (2011) (140 pts)]]></title>
            <link>https://all-things-andy-gavin.com/video-games/making-crash/</link>
            <guid>46045039</guid>
            <pubDate>Tue, 25 Nov 2025 12:05:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://all-things-andy-gavin.com/video-games/making-crash/">https://all-things-andy-gavin.com/video-games/making-crash/</a>, See on <a href="https://news.ycombinator.com/item?id=46045039">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-7768">
							<p>As one of the co-creators of <em>Crash Bandicoot</em>, I have been (slowly) writing a long series of posts on the making of everyone’s favorite orange marsupial. You can find them all below, so enjoy.</p>
<p>If you are on mobile and cannot see the grid of posts, <a href="https://all-things-andy-gavin.com/tag/pt_crash_history/?order=ASC">click here</a>.</p>

						</div><div>
																				
							<div>
										<p>In the summer of 1994 Naughty Dog, Inc. was still a two-man company, myself and my longtime partner Jason Rubin. Over the&nbsp;preceding eight years,&nbsp;we had published six&nbsp;games as a lean and mean duo, but&nbsp;the time had come to expand. In…</p>
										<p><a href="https://all-things-andy-gavin.com/2011/02/02/making-crash-bandicoot-part-1/">Read More→</a></p>
									</div>
																				
							<div>
										<p>So what was it that Sega and Nintendo had in 1994, but Sony didn’t?<br>
An existing competing mascot character. Sega had Sonic and Nintendo had Mario, but Sony product slate was blank.<br>
So we set about creating a mascot on the theory that maybe, just maybe, we might be able to slide into that opening. I’m still surprised it worked.</p>
										<p><a href="https://all-things-andy-gavin.com/2011/02/03/making-crash-bandicoot-part-2/">Read More→</a></p>
									</div>
																				
							<div>
										<p>While all this art design was going on, I, and then in January 1995, Dave, struggled to build an engine and tool pipeline that would make it possible to render these grandiose cartoon worlds we had envisioned on paper. Since during fall of 1994 Jason was also the only artist, he frantically generated all the source material and banged on my head to make sure it would look incredible.</p>
										<p><a href="https://all-things-andy-gavin.com/2011/02/04/making-crash-bandicoot-part-3/">Read More→</a></p>
									</div>
																				
							<div>
										<p>We were forging new gameplay ground, causing a lot of growing pains. The control of the main character is the single most important thing in a CAG. I did all the programming, but Mark helped whip me along. For example saying, “he doesn’t stop fast enough,” or “he needs to be able to jump for a frame or two AFTER he’s run off a cliff or it will be frustrating.” Criticism is essential, and as a programmer who wrote dozens of world class control schemes in the years between 1994 and 2004, I rewrote every one at least five or six times. Iteration is king.</p>
										<p><a href="https://all-things-andy-gavin.com/2011/02/05/making-crash-bandicoot-part-4/">Read More→</a></p>
									</div>
																				
							<div>
										<p>But once the core gameplay worked, these cool levels were missing something. We’d spent so many polygons on our detailed backgrounds and “realistic” cartoon characters that the enemies weren’t that dense, so everything felt a bit empty.</p>
										<p><a href="https://all-things-andy-gavin.com/2011/02/06/making-crash-bandicoot-part-5/">Read More→</a></p>
									</div>
																				
							<div>
										<p>Not only did we need to finish our E3 demo, but we needed a real name for the game — Willie the Wombat wasn’t going to cut it. Now, in the Naughty Dog office proper we knew he was a Bandicoot. In fact, we liked the idea of using an action name for him, like Crash, Dash, Smash, and Bash — fallout from the visceral reaction to smashing so many boxes.</p>
										<p><a href="https://all-things-andy-gavin.com/2011/02/07/making-crash-bandicoot-part-6/">Read More→</a></p>
									</div>
																				
							<div>
										<p>Dave Baggett, Naughty Dog employee #1 (after Jason and I) throws his own thoughts on Crash Bandicoot into the ring.</p>
										<p><a href="https://all-things-andy-gavin.com/2011/02/10/crash-bandicoot-as-a-startup/">Read More→</a></p>
									</div>
																				
							<div>
										<p>After Naughty Dog Jason and I joined forces with another game industry veteran, Jason Kay (collectively Jason R &amp; K are known as “the Jasons”). He was at Activision at the time of the Crash launch and offers his outside perspective.</p>
										<p><a href="https://all-things-andy-gavin.com/2011/02/16/crash-bandicoot-an-outsiders-perspective-part-8/">Read More→</a></p>
									</div>
																				
							<div>
										<p>I’m always being asked for more information on the LISP based languages I designed for the Crash and Jak games. This post is about GOOL, the LISP language used in Crash 1, Crash 2, and Crash 3. GOOL was my second custom language. GOOL was mostly interpreted, although by Crash 2 basic expressions were compiled into machine code.</p>
										<p><a href="https://all-things-andy-gavin.com/2011/03/12/making-crash-bandicoot-gool-part-9/">Read More→</a></p>
									</div>
																				
							<div>
										<p>Below is another journal article I wrote on making Crash in 1999. This was co-written with Naughty Dog uber-programmer Stephen White, who was my co-lead on Crash 2, Crash 3, Jak &amp; Daxter, and Jak 2. It’s long, so I’m breaking it into three parts.</p>
										<p><a href="https://all-things-andy-gavin.com/2011/03/26/crash-bandicoot-teaching-an-old-dog-new-bits/">Read More→</a></p>
									</div>
																				
							<div>
										<p>Part 2 of a detailed journal article I wrote on making Crash in 1999.</p>
										<p><a href="https://all-things-andy-gavin.com/2011/03/27/crash-bandicoot-teaching-an-old-dog-new-bits-part-2/">Read More→</a></p>
									</div>
																				
							<div>
										<p>Part 3 of a journal article I wrote on making Crash in 1999.</p>
										<p><a href="https://all-things-andy-gavin.com/2011/03/28/crash-bandicoot-teaching-an-old-dog-new-bits-part-3/">Read More→</a></p>
									</div>
																				
							<div>
										<p>In honor of Crash’s 15th Anniversary I wanted to make a post whose primary purpose is to serve as a repository for comments from you — the fans — about your first and favorite Crash Bandicoot impressions. Please make them…</p>
										<p><a href="https://all-things-andy-gavin.com/2011/09/09/crash-memories/">Read More→</a></p>
									</div>
																				
							<div>
										<p>In honor of the recent 15th Anniversary of my baby Crash Bandicoot, I present collected together the original suite of American TV Ads which&nbsp;premiered&nbsp;in September of 1996. It’s the suit that helped make the Bandicoot what he was. Thanks to…</p>
										<p><a href="https://all-things-andy-gavin.com/2011/09/15/crash-launch-commercials/">Read More→</a></p>
									</div>
																				
							<div>
										<p>At Naughty Dog, we pioneered the idea of simultaneous international release. It took a little while to perfect, but by Crash 2 and Crash 3 the same exact code ran all the worldwide versions. Both the games themselves and the marketing was highly localized and targeted. This attention after finishing the game to really polishing it up for the world really paid off in international sales.</p>
										<p><a href="https://all-things-andy-gavin.com/2012/01/06/parlez-vous-crash/">Read More→</a></p>
									</div>
																				
							<div>
										<p>It’s probably hard for younger gamers to recognize the position in gaming&nbsp;that Japan occupied from the mid eighties to the late 90s. First of all, after video games rose like a&nbsp;phoenix&nbsp;from the “great crash of ’82” (in which the classic…</p>
										<p><a href="https://all-things-andy-gavin.com/2012/01/11/crash-goes-to-japan-part-1/">Read More→</a></p>
									</div>
																				
							<div>
										<p>Ars Technica — the awesome technical website — put together an equally awesome video interview with me about the making of Crash Bandicoot as part of their War Stories series…</p>
										<p><a href="https://all-things-andy-gavin.com/2020/02/27/war-stories-crash-bandicoot/">Read More→</a></p>
									</div>
											</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What you can get for the price of a Netflix subscription (121 pts)]]></title>
            <link>https://nmil.dev/what-you-can-get-for-the-price-of-a-netflix-subscription</link>
            <guid>46042969</guid>
            <pubDate>Tue, 25 Nov 2025 06:39:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nmil.dev/what-you-can-get-for-the-price-of-a-netflix-subscription">https://nmil.dev/what-you-can-get-for-the-price-of-a-netflix-subscription</a>, See on <a href="https://news.ycombinator.com/item?id=46042969">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>A couple of weeks ago, I decided to do away with my Netflix subscription. I simply was barely using it, and whenever I did it was more out of habit than it really being the thing I wanted to do with my time. Sure, there's still some decent stuff on there, but the vast majority of it feels absolutely moneyballed. Good, but somehow <em>too good</em>, and with no character.</p>

<p>As much as I'd love to elaborate on why I think Netflix is evil, that's not todays topic. What I wanted to share is how for approximately the price I was paying for my subscription (€19.99), I've snapped up three subscriptions that I'm using on a daily basis. They're all pretty much interchangeable with other alternatives. The main thing I want to highlight is the individual slot they each fill out for me.</p>

<h2 id="1-a-subscription-to-zed-pro-10">1. A subscription to Zed Pro (~€10)</h2>

<p>Frankly, I haven't really put too much thought into whether the unit economics are the best here. The main point is, these are €10 that make my coding experience more pleasant, and get me writing more code in my spare time. In that sense it's money well spent.</p>

<p>Does it matter if you get a Cursor subscription, or a Zed one, or whatever else is in vogue when you're reading? No, just get the thing that will get you excited to get your hands on the keyboard!
To me, Zed feels more intentionally built than the VSClones: things flow nicely, it feels snappy, the ui is less cluttered... It's just <em>nice</em>.</p>

<p>Editor preferences aside, the main takeaway is, invest in a hobby you actively engage in. Make that little bit more appealing and you have one more reason to be spending your time doing the thing that makes you feel good, rather than letting a couple hours a day evaporate watching another forgettable show.</p>

<h2 id="2-a-kagi-subscription-5-month">2. A Kagi subscription (~€5/month)</h2>

<p>I think we can mostly agree google kind of sucks nowadays. Whenever I search, I automatically scroll down to skip the sponsored posts and SEO maxxed websites, and still don't fully trust what I get. Maybe that's why we all started appending “reddit” the end of our searches.</p>

<p>Are the search results themselves better with Kagi? To be honest, I can't tell yet, others have written far more informed takes on the topic. What does it for me is the simple fact of being able to pay directly for a service that I use, and value, rather than having to trade my attention in and endure a wall of ads. Especially if it's something I use over and over, every day. That's what I mean to highlight here: we can support products that we enjoy by paying for them (who would have thought?) rather than letting them lobotomize us via ad feeds.</p>

<h2 id="3-a-cheap-server-on-hetzner-4-month">3. A cheap server on Hetzner (~€4/month)</h2>

<p>Again, the choice of provider here is secondary. The point is, I finally have my little stake on the internet. It's relatively barebones, and I like that. It forces me to learn and engage. In fact, that is where my blog is hosted!</p>

<p>So to sum it up: We don't <em>have</em> to default to a streaming subscription because that's become the standard human-being thing to do. For the same money you can build a suite of useful, well crafted tools that help you:
– Get the most out of your hobbies
– Spend less time looking at ads
– Build things you can share with the world</p>

<p>P.S. Not one word here was written by AI. I plan on keeping it that way for anything that goes on this blog. So, if anything reads like slop, it's <em>my</em> slop :)</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Most Stable Raspberry Pi? 81% Better NTP with Thermal Management (243 pts)]]></title>
            <link>https://austinsnerdythings.com/2025/11/24/worlds-most-stable-raspberry-pi-81-better-ntp-with-thermal-management/</link>
            <guid>46042946</guid>
            <pubDate>Tue, 25 Nov 2025 06:35:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://austinsnerdythings.com/2025/11/24/worlds-most-stable-raspberry-pi-81-better-ntp-with-thermal-management/">https://austinsnerdythings.com/2025/11/24/worlds-most-stable-raspberry-pi-81-better-ntp-with-thermal-management/</a>, See on <a href="https://news.ycombinator.com/item?id=46042946">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

			
<p>I’ve written before about building <a href="https://austinsnerdythings.com/2021/04/19/microsecond-accurate-ntp-with-a-raspberry-pi-and-pps-gps/">microsecond-accurate NTP servers with Raspberry Pi and GPS PPS</a>, and more recently about <a href="https://austinsnerdythings.com/2025/02/14/revisiting-microsecond-accurate-ntp-for-raspberry-pi-with-gps-pps-in-2025/">revisiting the setup in 2025</a>. Both posts focused on the hardware setup and basic configuration to achieve sub-microsecond time synchronization using GPS Pulse Per Second (PPS) signals.</p>



<p>But there was a problem. Despite having a stable PPS reference, my NTP server’s frequency drift was exhibiting significant variation over time. After months (years) of monitoring the system with Grafana dashboards, I noticed something interesting: the frequency oscillations seemed to correlate with CPU temperature changes. The frequency would drift as the CPU heated up during the day and cooled down at night, even though the PPS reference remained rock-solid.</p>



<p>Like clockwork (no pun intended), I somehow get sucked back into trying to improve my setup every 6-8 weeks. This post is the latest on that never-ending quest.</p>



<p>This post details how I achieved an <strong>81% reduction in frequency variability</strong> and <strong>77% reduction in frequency standard deviation</strong> through a combination of CPU core pinning and thermal stabilization. Welcome to Austin’s Nerdy Things, where we solve problems that 99.999% of people (and 99% of datacenters) don’t have.</p>



<h2>The Problem: Thermal-Induced Timing Jitter</h2>



<p>Modern CPUs, including those in Raspberry Pis, use dynamic frequency scaling to save power and manage heat. When the CPU is idle, it runs at a lower frequency (and voltage). When load increases, it scales up. This is great for power efficiency, but terrible for precision timekeeping.</p>



<p>Why? Because timekeeping (with NTP/chronyd/others) relies on a stable system clock to discipline itself against reference sources. If the CPU frequency is constantly changing, the system clock’s tick rate varies, introducing jitter into the timing measurements. Even though my PPS signal was providing a mostly perfect 1-pulse-per-second reference, the CPU’s frequency bouncing around made it harder for chronyd to maintain a stable lock.</p>



<p>But here’s the key insight: <strong>the system clock is ultimately derived from a crystal oscillator</strong>, and crystal oscillator frequency is temperature-dependent. The oscillator sits on the board near the CPU, and as the CPU heats up and cools down throughout the day, so does the crystal. Even a few degrees of temperature change can shift the oscillator’s frequency by parts per million – exactly what I was seeing in my frequency drift graphs. The CPU frequency scaling was one factor, but the underlying problem was that temperature changes were affecting the crystal oscillator itself. By stabilizing the CPU temperature, I could stabilize the thermal environment for the crystal oscillator, keeping its frequency consistent.</p>



<p>Looking at my Grafana dashboard, I could see the frequency offset wandering over a range of about 1 PPM (parts per million) as the Pi warmed up and cooled down throughout the day. The RMS offset was averaging around 86 nanoseconds, which isn’t terrible (it’s actually really, really, really good), but I knew it could be better.</p>



<h2>The Discovery</h2>



<p>After staring at graphs for longer than I’d like to admit, I had an idea: what if I could keep the CPU at a constant temperature? If the temperature (and therefore the frequency) stayed stable, maybe the timing would stabilize too.</p>



<p>The solution came in two parts:</p>



<p>1. <strong>CPU core isolation</strong> – Dedicate CPU 0 exclusively to timing-critical tasks (chronyd and PPS interrupts) 2. <strong>Thermal stabilization</strong> – Keep the other CPUs busy to maintain a constant temperature, preventing frequency scaling</p>



<p>Here’s what happened when I turned on the thermal stabilization system on November 17, 2025 at 09:10 AM:</p>



<figure><img decoding="async" width="2082" height="880" src="https://austinsnerdythings.com/wp-content/uploads/2025/11/ntp_frequency_stability-1.png" alt="NTP Frequency Stability" srcset="https://austinsnerdythings.com/wp-content/uploads/2025/11/ntp_frequency_stability-1.png 2082w, https://austinsnerdythings.com/wp-content/uploads/2025/11/ntp_frequency_stability-1-300x127.png 300w, https://austinsnerdythings.com/wp-content/uploads/2025/11/ntp_frequency_stability-1-800x338.png 800w, https://austinsnerdythings.com/wp-content/uploads/2025/11/ntp_frequency_stability-1-768x325.png 768w, https://austinsnerdythings.com/wp-content/uploads/2025/11/ntp_frequency_stability-1-1536x649.png 1536w, https://austinsnerdythings.com/wp-content/uploads/2025/11/ntp_frequency_stability-1-2048x866.png 2048w, https://austinsnerdythings.com/wp-content/uploads/2025/11/ntp_frequency_stability-1-1200x507.png 1200w, https://austinsnerdythings.com/wp-content/uploads/2025/11/ntp_frequency_stability-1-1980x837.png 1980w" sizes="(max-width: 2082px) 100vw, 2082px"></figure>



<p>That vertical red line marks when I activated the “time burner” process. Notice how the frequency oscillations immediately dampen and settle into a much tighter band? Let’s dive into how this works.</p>



<h2>The Solution Part 1: CPU Core Pinning and Real-Time Priority</h2>



<p>The first step is isolating timing-critical operations onto a dedicated CPU core. On a Raspberry Pi (4-core ARM), this means:</p>



<ul>
<li>CPU 0: Reserved for chronyd and PPS interrupts</li>



<li>CPUs 1-3: Everything else, including our thermal load</li>
</ul>



<p>I had AI (probably Claude Sonnet 4 ish, maybe 4.5) create a boot optimization script that runs at system startup:</p>



<pre><code>#!/bin/bash
# PPS NTP Server Performance Optimization Script
# Sets CPU affinity, priorities, and performance governor at boot

set -e

echo "Setting up PPS NTP server performance optimizations..."

# Wait for system to be ready
sleep 5

# Set CPU governor to performance mode
echo "Setting CPU governor to performance..."
cpupower frequency-set -g performance

# Pin PPS interrupt to CPU0 (may fail if already pinned, that's OK)
echo "Configuring PPS interrupt affinity..."
echo 1 &gt; /proc/irq/200/smp_affinity 2&gt;/dev/null || echo "PPS IRQ already configured"

# Wait for chronyd to start
echo "Waiting for chronyd to start..."
timeout=30
while [ $timeout -gt 0 ]; do
    chronyd_pid=$(pgrep chronyd 2&gt;/dev/null || echo "")
    if [ -n "$chronyd_pid" ]; then
        echo "Found chronyd PID: $chronyd_pid"
        break
    fi
    sleep 1
    ((timeout--))
done

if [ -z "$chronyd_pid" ]; then
    echo "Warning: chronyd not found after 30 seconds"
else
    # Set chronyd to real-time priority and pin to CPU 0
    echo "Setting chronyd to real-time priority and pinning to CPU 0..."
    chrt -f -p 50 $chronyd_pid
    taskset -cp 0 $chronyd_pid
fi

# Boost ksoftirqd/0 priority
echo "Boosting ksoftirqd/0 priority..."
ksoftirqd_pid=$(ps aux | grep '\[ksoftirqd/0\]' | grep -v grep | awk '{print $2}')
if [ -n "$ksoftirqd_pid" ]; then
    renice -n -10 $ksoftirqd_pid
    echo "ksoftirqd/0 priority boosted (PID: $ksoftirqd_pid)"
else
    echo "Warning: ksoftirqd/0 not found"
fi

echo "PPS NTP optimization complete!"

# Log current status
echo "=== Current Status ==="
echo "CPU Governor: $(cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor)"
echo "PPS IRQ Affinity: $(cat /proc/irq/200/effective_affinity_list 2&gt;/dev/null || echo 'not readable')"
if [ -n "$chronyd_pid" ]; then
    echo "chronyd Priority: $(chrt -p $chronyd_pid)"
fi
echo "======================"</code></pre>



<p><strong>What this does:</strong></p>



<ol>
<li><strong>Performance Governor</strong>: Forces all CPUs to run at maximum frequency, disabling frequency scaling</li>



<li><strong>PPS IRQ Pinning</strong>: Ensures PPS interrupt (IRQ 200) is handled exclusively by CPU 0</li>



<li><strong>Chronyd Real-Time Priority</strong>: Sets chronyd to SCHED_FIFO priority 50, giving it preferential CPU scheduling</li>



<li>C<strong>hronyd CPU Affinity</strong>: Pins chronyd to CPU 0 using <code>taskset</code></li>



<li><strong>ksoftirqd Priority Boost</strong>: Improves priority of the kernel softirq handler on CPU 0</li>
</ol>



<p>This script can be added to <code>/etc/rc.local</code> or as a systemd service to run at boot.</p>



<h2>The Solution Part 2: PID-Controlled Thermal Stabilization</h2>



<p>Setting the performance governor helps, but on a Raspberry Pi, even at max frequency, the CPU temperature will still vary based on ambient conditions and load. Temperature changes affect the CPU’s actual operating frequency due to thermal characteristics of the silicon.</p>



<p>The solution? Keep the CPU at a constant temperature using a PID-controlled thermal load. I call it the “time burner” (inspired by CPU burn-in tools, but with precise temperature control).</p>



<p>As a reminder of what we’re really doing here: <strong>we’re maintaining a stable thermal environment for the crystal oscillator</strong>. The RPi 3B’s 19.2 MHz oscillator is physically located near the CPU on the Raspberry Pi board, so by actively controlling CPU temperature, we’re indirectly controlling the oscillator’s temperature. Since the oscillator’s frequency is temperature-dependent (this is basic physics of quartz crystals), keeping it at a constant temperature means keeping its frequency stable – which is exactly what we need for precise timekeeping.</p>



<p>Here’s how it works:</p>



<ol>
<li><strong>Read CPU temperature</strong> from <code>/sys/class/thermal/thermal_zone0/temp</code> </li>



<li><strong>PID controller</strong> calculates how much CPU time to burn to maintain target temperature (I chose 54°C) </li>



<li><strong>Three worker processes</strong> run on CPUs 1, 2, and 3 (avoiding CPU 0) </li>



<li><strong>Each worker</strong> alternates between busy-loop (MD5 hashing) and sleeping based on PID output </li>



<li><strong>Temperature stabilizes</strong> at the setpoint, preventing thermal drift</li>
</ol>



<p>Here’s the core implementation (simplified for readability):</p>



<pre><code>#!/usr/bin/env python3
import time
import argparse
import multiprocessing
import hashlib
import os
from collections import deque

class PIDController:
    """Simple PID controller with output clamping and anti-windup."""
    def __init__(self, Kp, Ki, Kd, setpoint, output_limits=(0, 1), sample_time=1.0):
        self.Kp = Kp
        self.Ki = Ki
        self.Kd = Kd
        self.setpoint = setpoint
        self.output_limits = output_limits
        self.sample_time = sample_time
        self._last_time = time.time()
        self._last_error = 0.0
        self._integral = 0.0
        self._last_output = 0.0

    def update(self, measurement):
        """Compute new output of PID based on measurement."""
        now = time.time()
        dt = now - self._last_time

        if dt &lt; self.sample_time:
            return self._last_output

        error = self.setpoint - measurement

        # Proportional
        P = self.Kp * error

        # Integral with anti-windup
        self._integral += error * dt
        I = self.Ki * self._integral

        # Derivative
        derivative = (error - self._last_error) / dt if dt &gt; 0 else 0.0
        D = self.Kd * derivative

        # Combine and clamp
        output = P + I + D
        low, high = self.output_limits
        output = max(low, min(high, output))

        self._last_output = output
        self._last_error = error
        self._last_time = now

        return output

def read_cpu_temperature(path='/sys/class/thermal/thermal_zone0/temp'):
    """Return CPU temperature in Celsius."""
    with open(path, 'r') as f:
        temp_str = f.read().strip()
    return float(temp_str) / 1000.0

def burn_cpu(duration):
    """Busy-loop hashing for 'duration' seconds."""
    end_time = time.time() + duration
    m = hashlib.md5()
    while time.time() &lt; end_time:
        m.update(b"burning-cpu")

def worker_loop(worker_id, cmd_queue, done_queue):
    """
    Worker process:
    - Pins itself to CPUs 1, 2, or 3 (avoiding CPU 0)
    - Burns CPU based on commands from main process
    """
    available_cpus = [1, 2, 3]
    cpu_to_use = available_cpus[worker_id % len(available_cpus)]
    os.sched_setaffinity(0, {cpu_to_use})
    print(f"Worker {worker_id} pinned to CPU {cpu_to_use}")

    while True:
        cmd = cmd_queue.get()
        if cmd is None:
            break

        burn_time, sleep_time = cmd
        burn_cpu(burn_time)
        time.sleep(sleep_time)
        done_queue.put(worker_id)

# Main control loop (simplified)
def main():
    target_temp = 54.0  # degrees Celsius
    control_window = 0.20  # 200ms cycle time

    pid = PIDController(Kp=0.05, Ki=0.02, Kd=0.0,
                        setpoint=target_temp,
                        sample_time=0.18)

    # Start 3 worker processes
    workers = []
    cmd_queues = []
    done_queue = multiprocessing.Queue()

    for i in range(3):
        q = multiprocessing.Queue()
        p = multiprocessing.Process(target=worker_loop, args=(i, q, done_queue))
        p.start()
        workers.append(p)
        cmd_queues.append(q)

    try:
        while True:
            # Measure temperature
            current_temp = read_cpu_temperature()

            # PID control: output is fraction of time to burn (0.0 to 1.0)
            output = pid.update(current_temp)

            # Convert to burn/sleep times
            burn_time = output * control_window
            sleep_time = control_window - burn_time

            # Send command to all workers
            for q in cmd_queues:
                q.put((burn_time, sleep_time))

            # Wait for workers to complete
            for _ in range(3):
                done_queue.get()

            print(f"Temp={current_temp:.2f}C, Output={output:.2f}, "
                  f"Burn={burn_time:.2f}s")

    except KeyboardInterrupt:
        for q in cmd_queues:
            q.put(None)
        for p in workers:
            p.join()

if __name__ == '__main__':
    main()</code></pre>



<p>The full implementation includes a temperature filtering system to smooth out sensor noise and command-line arguments for tuning the PID parameters.</p>



<p><strong>PID Tuning Notes:</strong></p>



<ul>
<li><strong>Kp=0.05</strong>: Proportional gain – responds to current error</li>



<li><strong>Ki=0.02</strong>: Integral gain – eliminates steady-state error</li>



<li><strong>Kd=0.0</strong>: Derivative gain – set to zero because temperature changes slowly</li>
</ul>



<p>The target temperature of 54°C was chosen empirically – high enough to keep the CPU from idling down, but low enough to avoid thermal throttling (which starts around 80°C on Raspberry Pi).</p>



<h2>The Results: Numbers Don’t Lie</h2>



<p>The improvement was immediately visible. Here are the statistics comparing performance before and after the optimization:</p>



<p><strong>A note on ambient conditions:</strong> The Raspberry Pi lives in a project enclosure in our master bedroom (chosen for its decent GPS reception and ADS-B coverage for a new <a href="https://skyspottr.com/map/">aircraft AR overlay app idea I’m working on</a> also running on this Pi). While the time burner maintains the CPU die temperature at 54°C, the enclosure is still subject to ambient temperature swings. Room temperature cycles from a low of 66°F (18.9°C) at 5:15 AM to a peak of 72°F (22.2°C) at 11:30 AM – a 6°F daily swing from our heating schedule. The fact that we see such dramatic frequency stability improvements <em>despite</em> this ambient variation speaks to how effective the thermal control is. The CPU’s active heating overwhelms the environmental changes, maintaining consistent silicon temperature where it matters most.</p>



<h3>Frequency Stability</h3>



<figure><img decoding="async" width="2082" height="880" src="https://austinsnerdythings.com/wp-content/uploads/2025/11/ntp_frequency_variability.png" alt="Frequency Variability" srcset="https://austinsnerdythings.com/wp-content/uploads/2025/11/ntp_frequency_variability.png 2082w, https://austinsnerdythings.com/wp-content/uploads/2025/11/ntp_frequency_variability-300x127.png 300w, https://austinsnerdythings.com/wp-content/uploads/2025/11/ntp_frequency_variability-800x338.png 800w, https://austinsnerdythings.com/wp-content/uploads/2025/11/ntp_frequency_variability-768x325.png 768w, https://austinsnerdythings.com/wp-content/uploads/2025/11/ntp_frequency_variability-1536x649.png 1536w, https://austinsnerdythings.com/wp-content/uploads/2025/11/ntp_frequency_variability-2048x866.png 2048w, https://austinsnerdythings.com/wp-content/uploads/2025/11/ntp_frequency_variability-1200x507.png 1200w, https://austinsnerdythings.com/wp-content/uploads/2025/11/ntp_frequency_variability-1980x837.png 1980w" sizes="(max-width: 2082px) 100vw, 2082px"></figure>



<figure><table><thead><tr><th>Metric</th><th>Before</th><th>After</th><th>Improvement</th></tr></thead><tbody><tr><td><strong>Mean RMS Offset</strong></td><td>85.44 ns</td><td>43.54 ns</td><td><strong>49.0% reduction</strong></td></tr><tr><td><strong>Median RMS Offset</strong></td><td>80.13 ns</td><td>37.93 ns</td><td><strong>52.7% reduction</strong></td></tr></tbody></table></figure>



<p>The RMS offset is chronyd’s estimate of the timing uncertainty. Cutting this nearly in half means the system is maintaining significantly better time accuracy.</p>



<h2>Setup Instructions</h2>



<p>Want to replicate this? Here’s the step-by-step process:</p>



<h3>Prerequisites</h3>



<p>You need a working GPS PPS NTP server setup. If you don’t have one yet, follow my <a href="https://austinsnerdythings.com/2025/02/14/revisiting-microsecond-accurate-ntp-for-raspberry-pi-with-gps-pps-in-2025/">2025 NTP guide</a> first.</p>



<h3>Step 0: Install Required Tools</h3>



<pre><code>sudo apt-get update
sudo apt-get install linux-cpupower python3 util-linux</code></pre>



<h3>Step 1: Create the Boot Optimization Script</h3>



<p>Save the optimization script from earlier as <code>/usr/local/bin/pps-optimize.sh</code>:</p>



<pre><code>sudo nano /usr/local/bin/pps-optimize.sh
# Paste the script content
sudo chmod +x /usr/local/bin/pps-optimize.sh</code></pre>



<h3>Step 2: Create Systemd Service for Boot Script</h3>



<p>Create <code>/etc/systemd/system/pps-optimize.service</code>:</p>



<pre><code>[Unit]
Description=PPS NTP Performance Optimization
After=chronyd.service
Requires=chronyd.service

[Service]
Type=oneshot
ExecStart=/usr/local/bin/pps-optimize.sh
RemainAfterExit=yes

[Install]
WantedBy=multi-user.target</code></pre>



<p>Enable it:</p>



<pre><code>sudo systemctl enable pps-optimize.service</code></pre>



<h3>Step 3: Install the Time Burner Script</h3>



<p>Save the time burner Python script as <code>/usr/local/bin/time_burner.py</code>:</p>



<pre><code>sudo nano /usr/local/bin/time_burner.py
# Paste the full time burner script
sudo chmod +x /usr/local/bin/time_burner.py</code></pre>



<h3>Step 4: Create Systemd Service for Time Burner</h3>



<p>Create <code>/etc/systemd/system/time-burner.service</code>:</p>



<pre><code>[Unit]
Description=CPU Thermal Stabilization for NTP
After=network.target

[Service]
Type=simple
User=root
ExecStart=/usr/bin/python3 /usr/local/bin/time_burner.py -t 54.0 -n 3
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target</code></pre>



<p>Enable and start it:</p>



<pre><code>sudo systemctl enable time-burner.service
sudo systemctl start time-burner.service</code></pre>



<h3>Step 5: Verify the Setup</h3>



<p>Check that everything is running:</p>



<pre><code># Verify CPU governor
cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor
# Should output: performance

# Check chronyd CPU affinity and priority
ps -eo pid,comm,psr,ni,rtprio | grep chronyd
# Should show psr=0 (CPU 0) and rtprio=50

# Check time burner processes
ps aux | grep time_burner
# Should show 4 processes (1 main + 3 workers)

# Monitor NTP performance
chronyc tracking</code></pre>



<p>Example output from <code>chronyc tracking</code>:</p>



<pre><code>Reference ID    : 50505300 (PPS)
Stratum         : 1
Ref time (UTC)  : Sun Nov 24 16:45:23 2025
System time     : 0.000000038 seconds fast of NTP time
Last offset     : -0.000000012 seconds
RMS offset      : 0.000000035 seconds
Frequency       : 1.685 ppm slow
Residual freq   : -0.001 ppm
Skew            : 0.002 ppm
Root delay      : 0.000000001 seconds
Root dispersion : 0.000010521 seconds
Update interval : 16.0 seconds
Leap status     : Normal</code></pre>



<p>Notice the RMS offset of 35 nanoseconds – this is the kind of accuracy you can achieve with thermal stabilization.</p>



<h3>Step 6: Monitor Over Time</h3>



<p>(Topic for a future post)</p>



<p>Set up Grafana dashboards to monitor:</p>



<ul>
<li>Frequency offset (PPM)</li>



<li>RMS offset (nanoseconds)</li>



<li>CPU temperature</li>



<li>System time offset</li>
</ul>



<p>You’ll see the frequency stabilize within a few hours as the PID controller locks onto the target temperature.</p>



<h2>Monitoring and Troubleshooting</h2>



<h3>Real-Time Monitoring</h3>



<p>Watch chronyd tracking in real-time:</p>



<pre><code>watch -n 1 "chronyc tracking"</code></pre>



<p>Check time burner status:</p>



<pre><code>sudo systemctl status time-burner.service</code></pre>



<p>View time burner output:</p>



<pre><code>sudo journalctl -u time-burner.service -f</code></pre>



<h3>Common Issues</h3>



<p><strong>Temperature overshoots or oscillates:</strong></p>



<ul>
<li>Adjust PID gains – reduce Kp if oscillating, increase Ki if steady-state error</li>



<li>Try different target temperatures (50-60°C range)</li>
</ul>



<p><strong>High CPU usage (obviously):</strong></p>



<ul>
<li>This is intentional – the time burner uses ~90% of 3 cores</li>



<li>Not suitable for Pis running other workloads</li>
</ul>



<p><strong>Chronyd not pinned to CPU 0:</strong></p>



<ul>
<li>Check that the optimization script runs after chronyd starts</li>



<li>Adjust the timing in the systemd service dependencies</li>
</ul>



<h2>Trade-offs and Considerations</h2>



<p>Let’s be honest about the downsides:</p>



<h3>Power Consumption</h3>



<p>The time burner keeps 3 cores at ~30% average utilization. My Pi now draws about 3-4W continuously (vs 1-2W idle). Over a year, that’s an extra 15-25 kWh, or about $2-3 in electricity (depending on your rates).</p>



<h3>Heat</h3>



<p>Running at 54°C means the Pi is warm to the touch. This is well within safe operating temperature (thermal throttling doesn’t start until 80°C), but you might want to ensure adequate ventilation. I added a small heatsink just to be safe.</p>



<h3>CPU Resources</h3>



<p>You’re dedicating 3 of 4 cores to burning cycles. This is fine for a dedicated NTP server, but not suitable if you’re running other services on the same Pi. That said, I am also running the feeder to my new <a href="https://skyspottr.com/map/">ADS-B aircraft visualization app</a> on it. My readsb instance regularly gets to 1200 msg/s with 200+ aircraft.</p>



<h3>Is It Worth It?</h3>



<p>For 99.999% of use cases: <strong>absolutely not</strong>.</p>



<p>Most applications don’t need better than millisecond accuracy, let alone the 35-nanosecond RMS offset I’m achieving. Even for distributed systems, microsecond-level accuracy is typically overkill.</p>



<p><strong>When this might make sense:</strong></p>



<ul>
<li><strong>Precision timing applications</strong> (scientific instrumentation, radio astronomy)</li>



<li><strong>Distributed systems research</strong> requiring tight clock synchronization</li>



<li><strong>Network testing</strong> where timing precision affects results</li>



<li><strong>Because you can</strong> (the best reason for any homelab project)</li>
</ul>



<p>For me, this falls squarely in the “because you can” category. I had the monitoring infrastructure in place, noticed the thermal correlation, and couldn’t resist solving the problem. Plus, I learned a lot about PID control, CPU thermal characteristics, and Linux real-time scheduling.</p>



<h2>Future Improvements</h2>



<p>Some ideas I’m considering:</p>



<h3>Adaptive PID Tuning</h3>



<p>The current PID gains are hand-tuned for a specific ambient temperature range. The fairly low P value is to avoid spikes when some load on the Pi kicks up the temp. The I is a balance to keep long term “burn” relatively consistent. Implementing an auto-tuning algorithm (like Ziegler-Nichols) or adaptive PID could handle seasonal temperature variations better.</p>



<h3>Hardware Thermal Control</h3>



<p>Instead of software thermal control, I could add an actively cooled heatsink with PWM fan control. This might achieve similar temperature stability while using less power overall.</p>



<h3>Oven-Controlled Crystal Oscillator (OCXO)</h3>



<p>For the ultimate in frequency stability, replacing the Pi’s crystal with a temperature-controlled OCXO would eliminate thermal drift at the source. This is how professional timing equipment works. I do have a BH3SAP GPSDO sitting next to me (subject to a future post)… Then again, I’m the person who just wrote 4000 words about optimizing a $50 time server, so who am I kidding?</p>



<h2>Conclusions</h2>



<p>Through a combination of CPU core isolation and PID-controlled thermal stabilization, I achieved:</p>



<ul>
<li><strong>81% reduction</strong> in frequency variability</li>



<li><strong>77% reduction</strong> in frequency standard deviation</li>



<li><strong>74% reduction</strong> in frequency range</li>



<li><strong>49% reduction</strong> in RMS offset</li>
</ul>



<p>The system now maintains 38-nanosecond median RMS offset from the GPS PPS reference, with frequency drift that’s barely detectable in the noise. The CPU runs at a constant 54°C, and in steady state, the frequency offset stays within a tight ±0.14 PPM band (compared to ±0.52 PPM before optimization).</p>



<p>Was this necessary? No. Did I learn a bunch about thermal management, PID control, and Linux real-time scheduling? Yes. Would I do it again? Absolutely.</p>



<h3>Resource</h3>



<p>I did come across a “burn” script that was the basis for this thermal management. I can’t find it at the moment, but when I do I’ll link it here.</p>



<h3>Related Posts</h3>



<ul>
<li><a href="https://austinsnerdythings.com/2021/04/19/microsecond-accurate-ntp-with-a-raspberry-pi-and-pps-gps/">Microsecond-Accurate NTP with a Raspberry Pi and PPS GPS (2021)</a></li>



<li><a href="https://austinsnerdythings.com/2025/02/14/revisiting-microsecond-accurate-ntp-for-raspberry-pi-with-gps-pps-in-2025/">Revisiting Microsecond-Accurate NTP for Raspberry Pi in 2025</a></li>
</ul>



<h3>Further Reading</h3>



<ul>
<li><a href="https://chrony.tuxfamily.org/documentation.html">Chrony Documentation</a></li>



<li><a href="https://en.wikipedia.org/wiki/PID_controller">PID Control Theory</a></li>



<li><a href="https://www.kernel.org/doc/html/latest/scheduler/sched-rt-group.html">Linux Real-Time Scheduling</a></li>
</ul>



<p>Have questions or suggestions? Drop a comment below. I’m particularly interested to hear if anyone has tried alternative thermal management approaches or has experience with OCXO modules for Raspberry Pi timing applications.</p>



<p>Thanks for reading, and happy timekeeping!</p>
<p><span></span> <span>Post Views:</span> <span>8,239</span>
			</p>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Human brains are preconfigured with instructions for understanding the world (330 pts)]]></title>
            <link>https://news.ucsc.edu/2025/11/sharf-preconfigured-brain/</link>
            <guid>46042928</guid>
            <pubDate>Tue, 25 Nov 2025 06:31:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.ucsc.edu/2025/11/sharf-preconfigured-brain/">https://news.ucsc.edu/2025/11/sharf-preconfigured-brain/</a>, See on <a href="https://news.ycombinator.com/item?id=46042928">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>


<main><div>



<div id="press-inquiries-block-block_cfe71fc6ecbce1ddc2f1b4884cb0adc2" inert="">
						<h2>Press Contact</h2>
						
					</div>



<div>
<h2 id="h-key-takeaways">Key takeaways</h2>



<ul>
<li>New findings suggest the brain has preconfigured, structured activity patterns even before sensory experiences occur.</li>



<li>UC Santa Cruz researchers used brain organoids to study the brain’s earliest electrical activity.</li>



<li>Understanding early brain patterns could have important implications for diagnosing and treating developmental brain disorders.</li>
</ul>
</div>



<p>Humans have long wondered when and how we begin to form thoughts. Are we born with a pre-configured brain, or do thought patterns only begin to emerge in response to our sensory experiences of the world around us? Now, science is getting closer to answering the questions philosophers have pondered for centuries.&nbsp;</p>



<p>Researchers at the University of California, Santa Cruz, are using tiny models of human brain tissue, called organoids, to study the earliest moments of electrical activity in the brain. <a href="https://www.nature.com/articles/s41593-025-02111-0">A new study</a> in <em>Nature Neuroscience</em> finds that the earliest firings of the brain occur in structured patterns without any external experiences, suggesting that the human brain is preconfigured with instructions about how to navigate and interact with the world.</p>



<p>“These cells are clearly interacting with each other and forming circuits that self-assemble before we can experience anything from the outside world,” said Tal Sharf, assistant professor of biomolecular engineering at the Baskin School of Engineering and the study’s senior author. “There’s an operating system that exists, that emerges in a primordial state. In my laboratory, we grow brain organoids to peer into this primordial version of the brain’s operating system and study how the brain builds itself before it’s shaped by sensory experience.”</p>



<p>In improving our fundamental understanding of human brain development, these findings can help researchers better understand neurodevelopmental disorders, and pinpoint the impact of toxins like pesticides and microplastics in the developing brain.&nbsp;</p>



<div>
<figure><img decoding="async" width="1024" height="804" src="https://news.ucsc.edu/wp-content/uploads/2025/11/9-23-25-Tal-Sharf-Lab-CL-002-1024x804.jpg" alt="" srcset="https://news.ucsc.edu/wp-content/uploads/2025/11/9-23-25-Tal-Sharf-Lab-CL-002-1024x804.jpg 1024w, https://news.ucsc.edu/wp-content/uploads/2025/11/9-23-25-Tal-Sharf-Lab-CL-002-300x236.jpg 300w, https://news.ucsc.edu/wp-content/uploads/2025/11/9-23-25-Tal-Sharf-Lab-CL-002-768x603.jpg 768w, https://news.ucsc.edu/wp-content/uploads/2025/11/9-23-25-Tal-Sharf-Lab-CL-002-1536x1206.jpg 1536w, https://news.ucsc.edu/wp-content/uploads/2025/11/9-23-25-Tal-Sharf-Lab-CL-002-2048x1608.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Sharf holds a CMOS-based microelectrode array chip. These devices contain thousands of miniaturized amplifiers used to triangulate the electrical activity of single neurons within millimeter-sized organoid tissue.&nbsp;<br></figcaption></figure>
</div>



<h4 id="h-studying-the-developing-brain"><strong>Studying the developing brain</strong></h4>



<p>The brain, similar to a computer, runs on electrical signals—the firing of neurons. When these signals begin to fire, and how the human brain develops, are challenging topics for scientists to study, as the early developing human brain is protected within the womb.</p>



<p>Organoids, which are 3D models of tissue grown from human stem cells in the lab, provide a unique window into brain development. The <a href="https://braingeneers.ucsc.edu/">Braingeneers group</a> at UC Santa Cruz, in collaboration with researchers at UC San Francisco and UC Santa Barbara, are pioneering methods to grow these models and take measurements from them to gain insights into brain development and disorders.&nbsp;</p>



<p>Organoids are particularly useful for understanding if the brain develops in response to sensory input—as they exist in the lab setting and not the body—and can be grown ethically in large quantities. In this study, researchers prompted stem cells to form brain tissue, and then measured their electrical activity using specialized microchips, similar to those that run a computer. Sharf’s background in both applied physics, computation, and neurobiology form his expertise in modelling the circuitry of the early brain.&nbsp;</p>



<p>“An organoid system that’s intrinsically decoupled from any sensory input or communication with organs gives you a window into what’s happening with this self-assembly process,” Sharf said. “That self-assembly process is really hard to do with traditional 2D cell culture—you can’t get the cell diversity and the architecture. The cells need to be in intimate contact with each other. We’re trying to control the initial conditions, so we can let biology do its wonderful thing.”</p>







<p>The Sharf lab is developing novel neural interfaces, leveraging expertise in physics, materials science, and electrical engineering. On the right, Koushik Devarajan, an electrical and computer engineering Ph.D. student in the Sharf lab.<br></p>



<h4 id="h-pattern-production"><strong>Pattern production</strong></h4>



<p>The researchers observed the electrical activity of the brain tissue as they self-assembled from stem cells into a tissue that can translate the senses and produce language and conscious thought. They found that within the first few months of development, long before the human brain is capable of receiving and processing complex external sensory information such as vision and hearing, its cells spontaneously began to emit electrical signals characteristic of the patterns that underlie translation of the senses.&nbsp;</p>



<p>Through decades of neuroscience research, the community has discovered that neurons fire in patterns that aren’t just random. Instead, the brain has a “default mode” — a basic underlying structure for firing neurons which then becomes more specific as the brain processes unique signals like a smell or taste. This background mode outlines the possible range of sensory responses the body and brain can produce.</p>



<p>In their observations of single neuron spikes in the self-assembling organoid models, Sharf and colleagues found that these earliest observable patterns have striking similarity with the brain’s default mode. Even without having received any sensory input, they are firing off a complex repertoire of time-based patterns, or sequences, which have the potential to be refined for specific senses, hinting at a genetically encoded blueprint inherent to the neural architecture of the living brain.</p>



<p>“These intrinsically self-organized systems could serve as a basis for constructing a representation of the world around us,” Sharf said. “The fact that we can see them in these early stages suggests that evolution has figured out a way that the central nervous system can construct a map that would allow us to navigate and interact with the world.”</p>



<p>Knowing that these organoids produce the basic structure of the living brain opens up a range of possibilities for better understanding human neurodevelopment, disease, and the effects of toxins in the brain.&nbsp;</p>



<p>“We’re showing that there is a basis for capturing complex dynamics that likely could be signatures of pathological onsets that we could study in human tissue,” Sharf said. “That would allow us to develop therapies, working with clinicians at the preclinical level to potentially develop compounds, drug therapies, and gene editing tools that could be cheaper, more efficient, higher throughput.”</p>



<p>This study included researchers at UC Santa Barbara, Washington University in St. Louis, Johns Hopkins University, the University Medical Center Hamburg-Eppendorf, and ETH Zurich.</p>



<figure><img loading="lazy" decoding="async" width="1024" height="698" src="https://news.ucsc.edu/wp-content/uploads/2025/11/5-23-25-Tal-Sharf-Lab-CL-015-1024x698.jpg" alt="A group of 15 researchers smile at the camera." srcset="https://news.ucsc.edu/wp-content/uploads/2025/11/5-23-25-Tal-Sharf-Lab-CL-015-1024x698.jpg 1024w, https://news.ucsc.edu/wp-content/uploads/2025/11/5-23-25-Tal-Sharf-Lab-CL-015-300x204.jpg 300w, https://news.ucsc.edu/wp-content/uploads/2025/11/5-23-25-Tal-Sharf-Lab-CL-015-768x523.jpg 768w, https://news.ucsc.edu/wp-content/uploads/2025/11/5-23-25-Tal-Sharf-Lab-CL-015-1536x1047.jpg 1536w, https://news.ucsc.edu/wp-content/uploads/2025/11/5-23-25-Tal-Sharf-Lab-CL-015-2048x1396.jpg 2048w" sizes="auto, (max-width: 1024px) 100vw, 1024px"><figcaption>The Sharf lab.</figcaption></figure>


<div><h2>Related Topics</h2></div>




</div></main>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DoGE "cut muscle, not fat"; 26K experts rehired after brutal cuts (174 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2025/11/doge-doesnt-exist-anymore-but-expert-says-its-still-not-dead/</link>
            <guid>46040005</guid>
            <pubDate>Mon, 24 Nov 2025 22:12:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2025/11/doge-doesnt-exist-anymore-but-expert-says-its-still-not-dead/">https://arstechnica.com/tech-policy/2025/11/doge-doesnt-exist-anymore-but-expert-says-its-still-not-dead/</a>, See on <a href="https://news.ycombinator.com/item?id=46040005">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
            <article data-id="2129128">
  
  <header>
  <div>
    <div>
      

      

      <p>
        Government brain drain will haunt US after DOGE abruptly terminated.
      </p>

              
          </div>

    <div>
    
    <p>
      Billionaire Elon Musk, the head of the Department of Government Efficiency (DOGE), holds a chainsaw as he speaks at the annual Conservative Political Action Conference.

              <span>
          Credit:

                      <a href="https://www.gettyimages.com/detail/news-photo/billionaire-elon-musk-the-head-of-the-department-of-news-photo/2200059564?adppopup=true" target="_blank">
          
          SAUL LOEB / Contributor | AFP

                      </a>
                  </span>
          </p>
  </div>
  </div>
</header>


  

  
      
    
    <div>
                      
                      
          
<p>After Donald Trump curiously started referring to the Department of Government Efficiency exclusively in the past tense, an official finally confirmed Sunday that DOGE “doesn’t exist.”</p>
<p><a href="https://www.reuters.com/world/us/doge-doesnt-exist-with-eight-months-left-its-charter-2025-11-23/">Talking to Reuters</a>, Office of Personnel Management (OPM) Director Scott Kupor confirmed that DOGE—a government agency notoriously created by Elon Musk to rapidly and dramatically slash government agencies—was terminated more than eight months early. This may have come as a surprise to whoever runs the <a href="https://x.com/DOGE">DOGE account</a> on X, which continued posting up until two days before the Reuters report was published.</p>
<p>As Kupor explained, a “centralized agency” was no longer necessary, since OPM had “taken over many of DOGE’s functions” after Musk left the agency last May. Around that time, DOGE staffers were embedded at various agencies, where they could ostensibly better coordinate with leadership on proposed cuts to staffing and funding.</p>
<p>Under Musk, DOGE was hyped as planning to save the government a trillion dollars. On X, Musk bragged frequently about the agency, posting in February that DOGE was “the one shot the American people have to defeat BUREAUcracy, rule of the bureaucrats, and restore DEMOcracy, rule of the people. We’re never going to get another chance like this.”</p>
<p>The reality fell far short of Musk’s goals, with DOGE ultimately <a href="https://doge.gov/savings">reporting</a> it saved $214 billion—an amount that may be overstated by nearly 40 percent, <a href="https://www.nytimes.com/2025/04/13/us/politics/doge-contracts-savings.html">critics warned earlier this year</a>.</p>
<h2>How much talent was lost due to DOGE cuts?</h2>
<p>Once Musk left, confidence in DOGE waned as <a href="https://www.nytimes.com/interactive/2025/us/trump-administration-lawsuits.html">lawsuits over suspected illegal firings piled up</a>. By June, <a href="https://arstechnica.com/tech-policy/2025/06/is-doge-doomed-to-fail-some-experts-are-ready-to-call-it/">Congress was drawn, largely down party lines</a>, on whether to codify the “DOGE process”—rapidly firing employees, then quickly hiring back whoever was needed—or declare DOGE a failure—perhaps costing taxpayers more in the long term due to lost talent and services.</p>

          
                      
                  </div>
                    
        
          
    
    <div>
          
          
<p>Because DOGE operated largely in secrecy, it may be months or even years before the public can assess the true cost of DOGE’s impact. However, in the absence of a government tracker, the director of the Center for Effective Public Management at the Brookings Institution, Elaine Kamarck, put together what might be the best status report showing how badly DOGE rocked government agencies.</p>
<p>In June, Kamarck joined <a href="https://arstechnica.com/tech-policy/2025/06/is-doge-doomed-to-fail-some-experts-are-ready-to-call-it/">other critics flagging DOGE’s reported savings as “bogus.”</a> In the days before DOGE’s abrupt ending was announced, she published a <a href="https://www.brookings.edu/articles/how-many-people-can-the-federal-government-lose-before-it-crashes/">report</a> grappling with a critical question many have pondered since DOGE launched: “How many people can the federal government lose before it crashes?”</p>
<p>In the report, Kamarck charted “26,511 occasions where the Trump administration abruptly fired people and then hired them back.” She concluded that “a quick review of the reversals makes clear that the negative stereotype of the ‘paper-pushing bureaucrat'” that DOGE was supposedly targeting “is largely inaccurate.”</p>
<p>Instead, many of the positions the government rehired were “engineers, doctors, and other professionals whose work is critical to national security and public health,” Kamarck reported.</p>
<p>About half of the rehires, Kamarck estimated, “appear to have been mandated by the courts.” However, in about a quarter of cases, the government moved to rehire staffers before the court could weigh in, Kamarck reported. That seemed to be “a tacit admission that the blanket firings that took place during the DOGE era placed the federal government in danger of not being able to accomplish some of its most important missions,” she said.</p>
<p>Perhaps the biggest downside of all of DOGE’s hasty downsizing, though, is a trend in which many long-time government workers simply decided to leave or retire, rather than wait for DOGE to eliminate their roles.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>During the first six months of Trump’s term, 154,000 federal employees signed up for the deferred resignation program, Reuters <a href="https://www.reuters.com/legal/litigation/us-government-faces-brain-drain-154000-federal-workers-exit-this-week-2025-09-30/">reported</a>, while more than 70,000 retired. Both numbers were clear increases (tens of thousands) over exits from government in prior years, Kamarck’s report noted.</p>
<p>“A lot of people said, ‘the hell with this’ and left,” Kamarck told Ars.</p>
<p>Kamarck told Ars that her report makes it obvious that DOGE “cut muscle, not fat,” because “they didn’t really know what they were doing.”</p>
<p>As a result, agencies are now scrambling to assess the damage and rehire lost talent. However, her report documented that agencies aligned with Trump’s policies appear to have an easier time getting new hires approved, despite Kupor telling Reuters that the government-wide hiring freeze is “over.” As of mid-November 2025, “of the over 73,000 posted jobs, a candidate was selected for only about 14,400 of them,” Kamarck reported, noting that it was impossible to confirm how many selected candidates have officially started working.</p>
<p>“Agencies are having to do a lot of reassessments in terms of what happened,” Kamarck told Ars, concluding that DOGE “was basically a disaster.”</p>

<h2>A decentralized DOGE may be more powerful</h2>
<p>“DOGE is not dead,” though, Kamarck said, noting that “the cutting effort is definitely” continuing under the Office of Management and Budget, which “has a lot more power than DOGE ever had.”</p>
<p>However, the termination of DOGE does mean that “the way it operated is dead,” and that will likely come as a relief to government workers who expected DOGE to continue slashing agencies through July 2026 at least, if not beyond.</p>
<p>Many government workers are still fighting terminations, as court cases drag on, and even Kamarck has given up on tracking due to inconsistencies in outcomes.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>“It’s still like one day the court says, ‘No, you can’t do that,'” Kamarck explained. “Then the next day another court says, ‘Yes, you can.'” Other times, the courts “change their minds,” or the Trump administration just doesn’t “listen to the courts, which is fairly terrifying,” Kamarck said.</p>
<p>Americans likely won’t get a clear picture of DOGE’s impact until power shifts in Washington. That could mean waiting for the next presidential election, or possibly if Democrats win a majority in midterm elections, DOGE investigations could start as early as 2027, Kamarck suggested.</p>
<p>OMB will likely continue with cuts that Americans appear to want, as White House spokesperson Liz Huston told Reuters that “President Trump was given a clear mandate to reduce waste, fraud and abuse across the federal government, and he continues to actively deliver on that commitment.”</p>
<p>However, Kamarck’s report noted polls showing that most Americans disapprove of how Trump is <a href="https://www.washingtonpost.com/politics/2025/11/02/trump-democrats-poll-post-abc-ipsos/">managing government</a> and <a href="https://ourpublicservice.org/blog/most-americans-oppose-the-trump-administration-cuts-to-the-federal-workforce/">its workforce</a>, perhaps indicating that OMB will be pressured to slow down and avoid roiling public opinion ahead of the midterms.</p>
<p>“The fact that ordinary Americans have come to question the downsizing is, most likely, the result of its rapid unfolding, with large cuts done quickly regardless of their impact on the government’s functioning,” Kamarck suggested. Even Musk began to question DOGE. After Trump announced plans to appeal an electrical vehicle mandate that the Tesla founder relied on, Musk posted <a href="https://x.com/elonmusk/status/1941997557514703294">on X</a>, “What the heck was the point of DOGE, if he’s just going to increase the debt by $5 trillion??”</p>
<p>Facing “blowback” over the most unpopular cuts, agencies sometimes rehired cut staffers within 24 hours, Kamarck noted, pointing to the Department of Energy as one of the “most dramatic” earliest examples. In that case, Americans were alarmed to see engineers cut who were responsible for keeping the nation’s nuclear arsenal “safe and ready.” Retention for those posts was already a challenge due to “high demand in the private sector,” and the number of engineers was considered “too low” ahead of DOGE’s cuts. Everyone was reinstated within a day, Kamarck reported.</p>

          
                  </div>
                    
        
          
    
    <div>

        
        <div>
          
          
<p>Alarm bells rang across the federal government, and it wasn’t just about doctors and engineers being cut or entire agencies being dismantled, like USAID. Even staffers DOGE viewed as having seemingly less critical duties—like travel bookers and customer service reps—were proven key to government functioning. Arbitrary cuts risked hurting Americans in myriad ways, hitting their pocketbooks, throttling community services, and limiting disease and disaster responses, Kamarck documented.</p>
<p>Now that the hiring freeze is lifted and OMB will be managing DOGE-like cuts moving forward, Kamarck suggested that Trump will face ongoing scrutiny over Musk’s controversial agency, despite its dissolution.</p>
<p>“In order to prove that the downsizing was worth the pain, the Trump administration will have to show that the government is still operating effectively,” Kamarck wrote. “But much could go wrong,” she reported, spouting a list of nightmare scenarios:</p>
<blockquote><p>“Nuclear mismanagement or airline accidents would be catastrophic. Late disaster warnings from agencies monitoring weather patterns, such as the National Oceanic and Atmospheric Administration (NOAA), and inadequate responses from bodies such as the Federal Emergency Management Administration (FEMA), could put people in danger. Inadequate staffing at the FBI could result in counter-terrorism failures. Reductions in vaccine uptake could lead to the resurgence of diseases such as polio and measles. Inadequate funding and staffing for research could cause scientists to move their talents abroad. Social Security databases could be compromised, throwing millions into chaos as they seek to prove their earnings records, and persistent customer service problems will reverberate through the senior and disability communities.”</p></blockquote>
<p>The good news is that federal agencies recovering from DOGE cuts are “aware of the time bombs and trying to fix them,” Kamarck told Ars. But with so much brain drain from DOGE’s first six months ripping so many agencies apart at their seams, the government may struggle to provide key services until lost talent can be effectively replaced, she said.</p>
<p>“I don’t know how quickly they can put Humpty Dumpty back together again,” Kamarck said.</p>


          
                  </div>

                  
          






  <div>
  <div>
          <p><a href="https://arstechnica.com/author/ashleybelanger/"><img src="https://cdn.arstechnica.net/wp-content/uploads/2022/06/Ashley-Belanger-400x400.jpg" alt="Photo of Ashley Belanger"></a></p>
  </div>

  <div>
    

    <p>
      Ashley is a senior policy reporter for Ars Technica, dedicated to tracking social impacts of emerging policies and new technologies. She is a Chicago-based journalist with 20 years of experience.
    </p>
  </div>
</div>


  <p>
    <a href="https://arstechnica.com/tech-policy/2025/11/doge-doesnt-exist-anymore-but-expert-says-its-still-not-dead/#comments" title="53 comments">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 80 80"><defs><clipPath id="bubble-zero_svg__a"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath><clipPath id="bubble-zero_svg__b"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath></defs><g clip-path="url(#bubble-zero_svg__a)"><g fill="currentColor" clip-path="url(#bubble-zero_svg__b)"><path d="M80 40c0 22.09-17.91 40-40 40S0 62.09 0 40 17.91 0 40 0s40 17.91 40 40"></path><path d="M40 40 .59 76.58C-.67 77.84.22 80 2.01 80H40z"></path></g></g></svg>
    53 Comments
  </a>
      </p>
              </div>
  </article>


  


  


  <div>
    <header>
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 26"><defs><clipPath id="most-read_svg__a"><path fill="none" d="M0 0h40v26H0z"></path></clipPath><clipPath id="most-read_svg__b"><path fill="none" d="M0 0h40v26H0z"></path></clipPath></defs><g clip-path="url(#most-read_svg__a)"><g fill="none" clip-path="url(#most-read_svg__b)"><path fill="currentColor" d="M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1"></path><path fill="#ff4e00" d="M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3"></path></g></g></svg>
      
    </header>
    <ol>
              <li>
                      <a href="https://arstechnica.com/security/2025/11/cryptography-group-cancels-election-results-after-official-loses-secret-key/">
              <img src="https://cdn.arstechnica.net/wp-content/uploads/2022/03/crypto-key-768x432.jpeg" alt="Listing image for first story in Most Read: Oops. Cryptographers cancel election results after losing decryption key." decoding="async" loading="lazy">
            </a>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                  </ol>
</div>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fifteen Years (179 pts)]]></title>
            <link>https://xkcd.com/3172/</link>
            <guid>46039673</guid>
            <pubDate>Mon, 24 Nov 2025 21:38:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://xkcd.com/3172/">https://xkcd.com/3172/</a>, See on <a href="https://news.ycombinator.com/item?id=46039673">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="bottom">
<p><img src="https://imgs.xkcd.com/s/a899e84.jpg" width="520" height="100" alt="Selected Comics" usemap="#comicmap"></p><map id="comicmap" name="comicmap">
<area shape="rect" coords="0,0,100,100" href="/150/" alt="Grownups">
<area shape="rect" coords="104,0,204,100" href="/730/" alt="Circuit Diagram">
<area shape="rect" coords="208,0,308,100" href="/162/" alt="Angular Momentum">
<area shape="rect" coords="312,0,412,100" href="/688/" alt="Self-Description">
<area shape="rect" coords="416,0,520,100" href="/556/" alt="Alternative Energy Revolution">
</map>

<p><a href="https://xkcd.com/1732/"><img src="https://imgs.xkcd.com/s/temperature.png" width="520" height="100" alt="Earth temperature timeline"></a></p>
<br>
<div id="comicLinks"><p>
Comics I enjoy:<br>
        <a href="http://threewordphrase.com/">Three Word Phrase</a>,
        <a href="https://www.smbc-comics.com/">SMBC</a>,
        <a href="https://www.qwantz.com/">Dinosaur Comics</a>,
        <a href="https://oglaf.com/">Oglaf</a> (nsfw),
        <a href="https://www.asofterworld.com/">A Softer World</a>,
        <a href="https://buttersafe.com/">Buttersafe</a>,
        <a href="https://pbfcomics.com/">Perry Bible Fellowship</a>,
        <a href="https://questionablecontent.net/">Questionable Content</a>,
        <a href="http://www.buttercupfestival.com/">Buttercup Festival</a>,
        <a href="https://www.homestuck.com/">Homestuck</a>,
	<a href="https://www.jspowerhour.com/">Junior Scientist Power Hour</a>
</p></div>
<br>

<br>
<center>
<p>xkcd.com is best viewed with Netscape Navigator 4.0 or below on a Pentium 3±1 emulated in Javascript on an Apple IIGS<br>at a screen resolution of 1024x1. Please enable your ad blockers, disable high-heat drying, and remove your device<br>from Airplane Mode and set it to Boat Mode. For security reasons, please leave caps lock on while browsing.</p>
</center>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI has a deep understanding of how this code works (252 pts)]]></title>
            <link>https://github.com/ocaml/ocaml/pull/14369</link>
            <guid>46039274</guid>
            <pubDate>Mon, 24 Nov 2025 21:03:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ocaml/ocaml/pull/14369">https://github.com/ocaml/ocaml/pull/14369</a>, See on <a href="https://news.ycombinator.com/item?id=46039274">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-view-component="true">            <h2>Conversation</h2>
  <div data-quote-markdown=".js-comment-body" data-discussion-hovercards-enabled="" data-issue-and-pr-hovercards-enabled="" data-team-hovercards-enabled="" data-hpc="">
    <template>
  <div data-view-component="true" class="flash flash-warn flash-full d-flex flex-items-center">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-alert">
    <path d="M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path>
</svg>
    <span>
      This file contains hidden or bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.
      <a class="Link--inTextBlock" href="https://github.co/hiddenchars" target="_blank">Learn more about bidirectional Unicode characters</a>
    </span>


  <div data-view-component="true" class="flash-action">        <a href="{{ revealButtonHref }}" data-view-component="true" class="btn-sm btn">    Show hidden characters
</a>
</div>
</div></template>
<template>
  <span aria-label="This line has hidden Unicode characters" data-view-component="true" class="line-alert tooltipped tooltipped-e">
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-alert">
    <path d="M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path>
</svg>
</span></template>

    <div>

      <div data-gid="PR_kwDOAGeMds60aMe9" data-url="/ocaml/ocaml/pull/14369/partials/body" data-channel-event-name="body_updated" data-channel="eyJjIjoicHVsbF9yZXF1ZXN0OjMwMjY3NjU3NTciLCJ0IjoxNzY0MDczODAxfQ==--bd92a3a89c1d906960e8a6c3d9a148346f89a67cc75139004beb6c449573b988">

<p><a data-hovercard-type="user" data-hovercard-url="/users/joelreymont/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/joelreymont">
  <img src="https://avatars.githubusercontent.com/u/18791?s=80&amp;v=4" width="40" height="40" alt="@joelreymont">
</a></p>
</div>

      

       
            


      <div data-gid="C_kwDOQTaJidoAKDM3YzkzODg4YTM4ZDIxNWE1Zjc4ZDNkNzQ1MGI0ZjhmYjU4YWUyNzk">
      <div data-view-component="true">
  <div>
  <p><a data-test-selector="commits-avatar-stack-avatar-link" data-hovercard-type="user" data-hovercard-url="/users/joelreymont/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/joelreymont">
        <img data-test-selector="commits-avatar-stack-avatar-image" src="https://avatars.githubusercontent.com/u/18791?s=40&amp;v=4" width="20" height="20" alt="@joelreymont">
</a>  </p>
</div>
    <div>
      <pre>Add complete DWARF version 4 debugging information generation for OCaml
native code. The implementation generates debug info for functions, types,
and line numbers, enabling debugger support for OCaml programs.

Key components:
- Low-level DWARF primitives (tags, attributes, forms, encodings)
- Debug Information Entries (DIE) construction
- Line number program generation
- String table management with offset tracking
- Code address tracking and relocation
- Integration with OCaml compilation pipeline
- Configuration flags to enable/disable DWARF emission

The implementation follows the DWARF 4 specification and generates
valid debug sections (.debug_info, .debug_line, .debug_str, .debug_abbrev)
that can be consumed by standard debuggers like gdb and lldb.</pre>
    </div>
</div>      <div data-view-component="true">
  <p><a data-test-selector="commits-avatar-stack-avatar-link" data-hovercard-type="user" data-hovercard-url="/users/joelreymont/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/joelreymont">
        <img data-test-selector="commits-avatar-stack-avatar-image" src="https://avatars.githubusercontent.com/u/18791?s=40&amp;v=4" width="20" height="20" alt="@joelreymont">
</a>  </p>
</div>      <div data-view-component="true">
  <div>
  <p><a data-test-selector="commits-avatar-stack-avatar-link" data-hovercard-type="user" data-hovercard-url="/users/joelreymont/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/joelreymont">
        <img data-test-selector="commits-avatar-stack-avatar-image" src="https://avatars.githubusercontent.com/u/18791?s=40&amp;v=4" width="20" height="20" alt="@joelreymont">
</a>  </p>
</div>
    <div>
      <pre>Replace hard-coded 0x19 offset with calculated offsets based on
actual DIE structure (CU header + CU DIE + type DIEs).</pre>
    </div>
</div>      <div data-view-component="true">
  <div>
  <p><a data-test-selector="commits-avatar-stack-avatar-link" data-hovercard-type="user" data-hovercard-url="/users/joelreymont/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/joelreymont">
        <img data-test-selector="commits-avatar-stack-avatar-image" src="https://avatars.githubusercontent.com/u/18791?s=40&amp;v=4" width="20" height="20" alt="@joelreymont">
</a>  </p>
</div>
    <div>
      <pre>Use label-based references (Lstr_N - Ldebug_str_start) instead of
plain offsets, allowing the linker to automatically adjust string
table references when merging .debug_str sections from multiple
compilation units.</pre>
    </div>
</div>      <div data-view-component="true">
  <div>
  <p><a data-test-selector="commits-avatar-stack-avatar-link" data-hovercard-type="user" data-hovercard-url="/users/joelreymont/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/joelreymont">
        <img data-test-selector="commits-avatar-stack-avatar-image" src="https://avatars.githubusercontent.com/u/18791?s=40&amp;v=4" width="20" height="20" alt="@joelreymont">
</a>  </p>
</div>
    <div>
      <pre>Changes DWARF output version from 4 to 5, enabling modern
DWARF features including inline strings (DW_FORM_string).</pre>
    </div>
</div>      <div data-view-component="true">
  <div>
  <p><a data-test-selector="commits-avatar-stack-avatar-link" data-hovercard-type="user" data-hovercard-url="/users/joelreymont/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/joelreymont">
        <img data-test-selector="commits-avatar-stack-avatar-image" src="https://avatars.githubusercontent.com/u/18791?s=40&amp;v=4" width="20" height="20" alt="@joelreymont">
</a>  </p>
</div>
    <div>
      <pre>Changes all string attributes to use DW_FORM_string (inline
strings) instead of DW_FORM_strp (string table offsets). This
avoids macOS linker crashes with section-relative relocations.</pre>
    </div>
</div>      <div data-view-component="true">
  <div>
  <p><a data-test-selector="commits-avatar-stack-avatar-link" data-hovercard-type="user" data-hovercard-url="/users/joelreymont/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/joelreymont">
        <img data-test-selector="commits-avatar-stack-avatar-image" src="https://avatars.githubusercontent.com/u/18791?s=40&amp;v=4" width="20" height="20" alt="@joelreymont">
</a>  </p>
</div>
    <div>
      <pre>Changes with_name helper to use DW_FORM_string for name
attributes, ensuring DIE string attributes are emitted inline.</pre>
    </div>
</div>      <div data-view-component="true">
  <div>
  <p><a data-test-selector="commits-avatar-stack-avatar-link" data-hovercard-type="user" data-hovercard-url="/users/joelreymont/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/joelreymont">
        <img data-test-selector="commits-avatar-stack-avatar-image" src="https://avatars.githubusercontent.com/u/18791?s=40&amp;v=4" width="20" height="20" alt="@joelreymont">
</a>  </p>
</div>
    <div>
      <pre>Makes .debug_str section optional - only emits if non-empty.
With inline strings (DW_FORM_string), .debug_str is empty and
not needed, avoiding linker crashes on macOS.</pre>
    </div>
</div>      <div data-view-component="true">
  <div>
  <p><a data-test-selector="commits-avatar-stack-avatar-link" data-hovercard-type="user" data-hovercard-url="/users/joelreymont/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/joelreymont">
        <img data-test-selector="commits-avatar-stack-avatar-image" src="https://avatars.githubusercontent.com/u/18791?s=40&amp;v=4" width="20" height="20" alt="@joelreymont">
</a>  </p>
</div>
    <div>
      <pre>Tests verify DWARF information is accessible by debuggers:
- dwarf_gdb.ml: GDB can set breakpoint and show source
- dwarf_line_gdb.ml: GDB can set breakpoint by line number
- dwarf_lldb_linux.ml: LLDB can set breakpoint and show source on Linux
- dwarf_lldb_macos.ml: LLDB can set breakpoint and show source on macOS

Tests use ocamltest framework with existing sanitize infrastructure.
Each test compiles with -g flag and runs debugger commands to verify
function names, source files, and line numbers are in DWARF sections.</pre>
    </div>
</div>      <div data-view-component="true">
  <div>
  <p><a data-test-selector="commits-avatar-stack-avatar-link" data-hovercard-type="user" data-hovercard-url="/users/joelreymont/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/joelreymont">
        <img data-test-selector="commits-avatar-stack-avatar-image" src="https://avatars.githubusercontent.com/u/18791?s=40&amp;v=4" width="20" height="20" alt="@joelreymont">
</a>  </p>
</div>
    <div>
      <pre>Include target.disable-aslr and stop-disassembly-display settings
for consistency with existing native-debugger tests.</pre>
    </div>
</div>      <div data-view-component="true">
  <div>
  <p><a data-test-selector="commits-avatar-stack-avatar-link" data-hovercard-type="user" data-hovercard-url="/users/joelreymont/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/joelreymont">
        <img data-test-selector="commits-avatar-stack-avatar-image" src="https://avatars.githubusercontent.com/u/18791?s=40&amp;v=4" width="20" height="20" alt="@joelreymont">
</a>  </p>
</div>
    <div>
      <pre>Tests verify LLDB can set breakpoints by line number:
- dwarf_line_lldb_linux.ml: Linux LLDB line breakpoint test
- dwarf_line_lldb_macos.ml: macOS LLDB line breakpoint test

Uses standard LLDB commands without Python extensions.
Achieves parity with existing GDB line breakpoint test.</pre>
    </div>
</div>      <div data-view-component="true">
  <div>
  <p><a data-test-selector="commits-avatar-stack-avatar-link" data-hovercard-type="user" data-hovercard-url="/users/joelreymont/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/joelreymont">
        <img data-test-selector="commits-avatar-stack-avatar-image" src="https://avatars.githubusercontent.com/u/18791?s=40&amp;v=4" width="20" height="20" alt="@joelreymont">
</a>  </p>
</div>
    <div>
      <pre>All DWARF tests now pass with the fixed line breakpoint implementation.
Test reference files updated to show the new working behavior:
- Line breakpoints now stop at correct source locations
- Debuggers show proper source file and line number information
- Function breakpoints include line information (e.g., 'at simple.ml:8')</pre>
    </div>
</div>      <div data-view-component="true">
  <div>
  <p><a data-test-selector="commits-avatar-stack-avatar-link" data-hovercard-type="user" data-hovercard-url="/users/joelreymont/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/joelreymont">
        <img data-test-selector="commits-avatar-stack-avatar-image" src="https://avatars.githubusercontent.com/u/18791?s=40&amp;v=4" width="20" height="20" alt="@joelreymont">
</a>  </p>
</div>
    <div>
      <pre>All DWARF tests now pass. Updated all reference files to match
current working output with line breakpoint support enabled.</pre>
    </div>
</div>      <div data-view-component="true">
  <div>
  <p><a data-test-selector="commits-avatar-stack-avatar-link" data-hovercard-type="user" data-hovercard-url="/users/joelreymont/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/joelreymont">
        <img data-test-selector="commits-avatar-stack-avatar-image" src="https://avatars.githubusercontent.com/u/18791?s=40&amp;v=4" width="20" height="20" alt="@joelreymont">
</a>  </p>
</div>
    <div>
      <pre>Enhanced sanitize.awk to handle more non-deterministic elements:
- Thread names and numbers in LLDB output
- Compilation directory paths
- Located in paths
- Fortran language warnings from LLDB
- Source language output from GDB
- Producer information
- DWARF version information

This reduces test flakiness by properly sanitizing all platform-specific
and non-deterministic elements in debugger output.

Also verified type offset calculations are correct - DW_AT_type references
point to the correct type DIEs, confirming the fix properly accounts for
the DW_AT_stmt_list attribute in offset calculations.</pre>
    </div>
</div>      <div data-view-component="true">
  <div>
  <p><a data-test-selector="commits-avatar-stack-avatar-link" data-hovercard-type="user" data-hovercard-url="/users/joelreymont/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/joelreymont">
        <img data-test-selector="commits-avatar-stack-avatar-image" src="https://avatars.githubusercontent.com/u/18791?s=40&amp;v=4" width="20" height="20" alt="@joelreymont">
</a>  </p>
</div>
    <div>
      <pre>- Enhanced sanitize.awk scripts to filter GDB ASLR warnings
- Updated LLDB test reference files to match current output
- DWARF implementation working correctly, 8/9 tests passing reliably
- One test (dwarf_line_gdb) occasionally fails due to environmental timing issues</pre>
    </div>
</div>      <div data-view-component="true">
  <div>
  <p><a data-test-selector="commits-avatar-stack-avatar-link" data-hovercard-type="user" data-hovercard-url="/users/joelreymont/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/joelreymont">
        <img data-test-selector="commits-avatar-stack-avatar-image" src="https://avatars.githubusercontent.com/u/18791?s=40&amp;v=4" width="20" height="20" alt="@joelreymont">
</a>  </p>
</div>
    <div>
      <pre>Issue <a data-error-text="Failed to load title" data-id="18296051" data-permission-text="Title is private" data-url="https://github.com/ocaml/ocaml/issues/2" data-hovercard-type="pull_request" data-hovercard-url="/ocaml/ocaml/pull/2/hovercard" href="https://github.com/ocaml/ocaml/pull/2">ocaml#2</a>: Address size was hard-coded to 8 bytes, breaking 32-bit architectures.

This ensures DWARF information works correctly on both 32-bit and 64-bit
target architectures, with addresses sized appropriately (4 or 8 bytes).</pre>
    </div>
</div>      <div data-view-component="true">
  <div>
  <p><a data-test-selector="commits-avatar-stack-avatar-link" data-hovercard-type="user" data-hovercard-url="/users/joelreymont/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/joelreymont">
        <img data-test-selector="commits-avatar-stack-avatar-image" src="https://avatars.githubusercontent.com/u/18791?s=40&amp;v=4" width="20" height="20" alt="@joelreymont">
</a>  </p>
</div>
    <div>
      <pre>Fixes the issue where backend register numbers were being copied directly
into DWARF register opcodes (DW_OP_reg*, DW_OP_regx). Different
architectures use different register numbering schemes in their backends,
but must emit standard DWARF register numbers defined by their ABIs.

The Arch_reg_mapping module uses a ref-based callback pattern with a default
identity mapping, allowing architecture-specific code to initialize the proper
mapper at runtime.</pre>
    </div>
</div>      <div data-view-component="true">
  <div>
  <p><a data-test-selector="commits-avatar-stack-avatar-link" data-hovercard-type="user" data-hovercard-url="/users/joelreymont/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/joelreymont">
        <img data-test-selector="commits-avatar-stack-avatar-image" src="https://avatars.githubusercontent.com/u/18791?s=40&amp;v=4" width="20" height="20" alt="@joelreymont">
</a>  </p>
</div>
    <div>
      <pre>Update DWARF test reference files to match actual debugger output for
unrecognized DW_LANG_OCaml language code. Add multi-object linking
test to verify DWARF structures when linking multiple .o files.</pre>
    </div>
</div>      <div data-view-component="true">
  <div>
  <p><a data-test-selector="commits-avatar-stack-avatar-link" data-hovercard-type="user" data-hovercard-url="/users/joelreymont/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/joelreymont">
        <img data-test-selector="commits-avatar-stack-avatar-image" src="https://avatars.githubusercontent.com/u/18791?s=40&amp;v=4" width="20" height="20" alt="@joelreymont">
</a>  </p>
</div>
    <div>
      <pre>When compiling with `-g`, OCaml emits DWARF debug information in object
files, but the linker was stripping these sections from the final binary.
This prevented debuggers like LLDB from finding function symbols and
setting breakpoints.

Fix: Modified utils/ccomp.ml to pass `-g` flag to the linker when
Clflags.debug is true. This ensures DWARF sections are preserved in
the linked binary or can be extracted by dsymutil on macOS.

Issue: Native debugger test (tests/native-debugger/macos-lldb-arm64.ml)
still fails, indicating additional work needed for full LLDB integration.</pre>
    </div>
</div>      <div data-view-component="true">
  <div>
  <p><a data-test-selector="commits-avatar-stack-avatar-link" data-hovercard-type="user" data-hovercard-url="/users/joelreymont/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/joelreymont">
        <img data-test-selector="commits-avatar-stack-avatar-image" src="https://avatars.githubusercontent.com/u/18791?s=40&amp;v=4" width="20" height="20" alt="@joelreymont">
</a>  </p>
</div>
    <div>
      <pre>Add validation scripts: inspect_dwarf.sh, multi_obj_dwarf_test.sh,
validate_arch_registers.sh, and comprehensive_dwarf.ml test runner.</pre>
    </div>
</div>      <div data-view-component="true">
  <div>
  <p><a data-test-selector="commits-avatar-stack-avatar-link" data-hovercard-type="user" data-hovercard-url="/users/joelreymont/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/joelreymont">
        <img data-test-selector="commits-avatar-stack-avatar-image" src="https://avatars.githubusercontent.com/u/18791?s=40&amp;v=4" width="20" height="20" alt="@joelreymont">
</a>  </p>
</div>
    <div>
      <pre>Add dwarf_reg_map.ml stubs for unsupported architectures that fail
with helpful error messages. Update documentation for macOS multi-object
limitation.</pre>
    </div>
</div>      <div data-view-component="true">
  <div>
  <p><a data-test-selector="commits-avatar-stack-avatar-link" data-hovercard-type="user" data-hovercard-url="/users/joelreymont/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/joelreymont">
        <img data-test-selector="commits-avatar-stack-avatar-image" src="https://avatars.githubusercontent.com/u/18791?s=40&amp;v=4" width="20" height="20" alt="@joelreymont">
</a>  </p>
</div>
    <div>
      <pre>Implement weak symbol subtractor relocations for Mach-O multi-object
linking. Emit __debug_line_section_base weak symbol and use label
subtraction for DW_AT_stmt_list offsets. Add dwarf_reg_map.ml stubs
for unsupported architectures.</pre>
    </div>
</div>      <div data-view-component="true">
  <div>
  <p><a data-test-selector="commits-avatar-stack-avatar-link" data-hovercard-type="user" data-hovercard-url="/users/joelreymont/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/joelreymont">
        <img data-test-selector="commits-avatar-stack-avatar-image" src="https://avatars.githubusercontent.com/u/18791?s=40&amp;v=4" width="20" height="20" alt="@joelreymont">
</a>  </p>
</div>
    <div>
      <pre>Add explicit failure for non-ELF/non-Mach-O platforms that cannot emit
correct section-relative offsets for DWARF multi-object linking.</pre>
    </div>
</div>      <div data-view-component="true">
  <div>
  <p><a data-test-selector="commits-avatar-stack-avatar-link" data-hovercard-type="user" data-hovercard-url="/users/joelreymont/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/joelreymont">
        <img data-test-selector="commits-avatar-stack-avatar-image" src="https://avatars.githubusercontent.com/u/18791?s=40&amp;v=4" width="20" height="20" alt="@joelreymont">
</a>  </p>
</div>
    <div>
      <pre>Implement Variable_info module to maintain a side table mapping
function names to their parameter names during compilation. This
allows the emission phase to output source-level names (x, y, z)
instead of generic register names (R) in DWARF formal parameters.

- Add Variable_info module with name preservation table
- Hook into selectgen to capture parameter names from Cmm
- Update AMD64 emitter to use source names for DWARF output
- Add test validating source names in DWARF debug info</pre>
    </div>
</div>      <div data-view-component="true">
  <div>
  <p><a data-test-selector="commits-avatar-stack-avatar-link" data-hovercard-type="user" data-hovercard-url="/users/joelreymont/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/joelreymont">
        <img data-test-selector="commits-avatar-stack-avatar-image" src="https://avatars.githubusercontent.com/u/18791?s=40&amp;v=4" width="20" height="20" alt="@joelreymont">
</a>  </p>
</div>
    <div>
      <pre>Extend DWARF emission to include local let-bound variables in
addition to function parameters. Local variables are collected
from the Linear IR during emission by traversing all instructions
and gathering registers with meaningful names.

- Add emit_dwarf_local_variable function for DW_TAG_variable
- Implement collect_named_regs to traverse Linear instructions
- Add emit_dwarf_locals to emit all local variables in a function
- Create comprehensive test for local variable preservation
- Verify both parameters and locals appear in DWARF output

Local variables now appear with their source-level names (sum,
doubled, temp1, etc.) instead of being lost during compilation.</pre>
    </div>
</div>      <div data-view-component="true">
  <div>
  <p><a data-test-selector="commits-avatar-stack-avatar-link" data-hovercard-type="user" data-hovercard-url="/users/joelreymont/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/joelreymont">
        <img data-test-selector="commits-avatar-stack-avatar-image" src="https://avatars.githubusercontent.com/u/18791?s=40&amp;v=4" width="20" height="20" alt="@joelreymont">
</a>  </p>
</div>
    <div>
      <pre>Extend local variable DWARF support to ARM64 architecture,
matching the AMD64 implementation. ARM64 now emits both
DW_TAG_formal_parameter and DW_TAG_variable entries with
source-level names.

- Add emit_dwarf_local_variable for ARM64
- Implement collect_named_regs to traverse Linear IR
- Add emit_dwarf_locals to emit all local variables
- Call emit_dwarf_locals after parameter emission

This completes multi-architecture support for local variable
debugging as specified in DWARF_LOCAL_VARIABLES_PLAN.md.</pre>
    </div>
</div>      <div data-view-component="true">
  <div>
  <p><a data-test-selector="commits-avatar-stack-avatar-link" data-hovercard-type="user" data-hovercard-url="/users/joelreymont/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/joelreymont">
        <img data-test-selector="commits-avatar-stack-avatar-image" src="https://avatars.githubusercontent.com/u/18791?s=40&amp;v=4" width="20" height="20" alt="@joelreymont">
</a>  </p>
</div>
    <div>
      <pre>Add fun_var_info field to Mach.fundecl and Linear.fundecl to carry
variable tracking information through compilation pipeline.</pre>
    </div>
</div>      <div data-view-component="true">
  <div>
  <p><a data-test-selector="commits-avatar-stack-avatar-link" data-hovercard-type="user" data-hovercard-url="/users/joelreymont/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/joelreymont">
        <img data-test-selector="commits-avatar-stack-avatar-image" src="https://avatars.githubusercontent.com/u/18791?s=40&amp;v=4" width="20" height="20" alt="@joelreymont">
</a>  </p>
</div>
    <div>
      <pre>Implement Var_lifetime module to track variables during selection.
Store parameter and local variable information in fundecl.fun_var_info.</pre>
    </div>
</div>      <div data-view-component="true">
  <div>
  <p><a data-test-selector="commits-avatar-stack-avatar-link" data-hovercard-type="user" data-hovercard-url="/users/joelreymont/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/joelreymont">
        <img data-test-selector="commits-avatar-stack-avatar-image" src="https://avatars.githubusercontent.com/u/18791?s=40&amp;v=4" width="20" height="20" alt="@joelreymont">
</a>  </p>
</div>
    <div>
      <pre>Replace heuristic register scanning with fun_var_info usage in emitters.
Variables flow from Cmm through Mach and Linear to emission with full
name and lifetime tracking.</pre>
    </div>
</div>      <div data-view-component="true">
  <div>
  <p><a data-test-selector="commits-avatar-stack-avatar-link" data-hovercard-type="user" data-hovercard-url="/users/joelreymont/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/joelreymont">
        <img data-test-selector="commits-avatar-stack-avatar-image" src="https://avatars.githubusercontent.com/u/18791?s=40&amp;v=4" width="20" height="20" alt="@joelreymont">
</a>  </p>
</div>
    <div>
      <pre>Extend DWARF module to support DW_TAG_lexical_block DIEs for nested
scope tracking. Add scope_context type, scope_stack, and functions
for adding/ending lexical blocks.</pre>
    </div>
</div>  </div>


  

        <div data-view-component="true" id="pullrequestreview-3484603925" data-gid="PRR_kwDOAGeMds7PstYV" data-channel="eyJjIjoicHVsbF9yZXF1ZXN0X3JldmlldzozNDg0NjAzOTI1IiwidCI6MTc2NDA3MzgwMn0=--32b4b8b24d95797beef5353a5e3f23856fd79c70c184fec3be74debddfc9408d" data-url="/ocaml/ocaml/pull/14369/partials/reviews/3484603925">
  <p><a href="https://github.com/tmcgilchrist" data-view-component="true"><img data-hovercard-type="user" data-hovercard-url="/users/tmcgilchrist/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" src="https://avatars.githubusercontent.com/u/170937?s=60&amp;v=4" alt="tmcgilchrist" size="40" height="40" width="40" data-view-component="true"></a></p>
  
</div>

        


        <div data-gid="C_kwDOQTaJidoAKGMyOWNjOTlhMjRmNTBjM2EwOGIzZTc5ZjYxMDczY2JlNzBhZjg3YTI">
      <div data-view-component="true">
  <div>
  <p><a data-test-selector="commits-avatar-stack-avatar-link" data-hovercard-type="user" data-hovercard-url="/users/joelreymont/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/joelreymont">
        <img data-test-selector="commits-avatar-stack-avatar-image" src="https://avatars.githubusercontent.com/u/18791?s=40&amp;v=4" width="20" height="20" alt="@joelreymont">
</a>  </p>
</div>
    <div>
      <pre>Remove unused helper functions from AMD64 and ARM64 emitters as flagged in PR review. These functions were created during early development but are not used in the final implementation which uses fun_var_info instead.</pre>
    </div>
</div>      <div data-view-component="true">
  <div>
  <p><a data-test-selector="commits-avatar-stack-avatar-link" data-hovercard-type="user" data-hovercard-url="/users/joelreymont/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/joelreymont">
        <img data-test-selector="commits-avatar-stack-avatar-image" src="https://avatars.githubusercontent.com/u/18791?s=40&amp;v=4" width="20" height="20" alt="@joelreymont">
</a>  </p>
</div>
    <div>
      <pre>Remove _collect_strings and _build_string_table functions that were explicitly marked as unused with DW_FORM_string implementation. These functions were kept for reference but serve no purpose in the current codebase.</pre>
    </div>
</div>  </div>

        


        


        


        <div data-view-component="true" data-gid="C_kwDOQTaJidoAKGNlMzcyYTYwYmQ5N2UwYTE5YjMzNmMzMDQyYzQwMzZjZmRhM2MwMWU">
  <p><a data-test-selector="commits-avatar-stack-avatar-link" data-hovercard-type="user" data-hovercard-url="/users/joelreymont/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/joelreymont">
        <img data-test-selector="commits-avatar-stack-avatar-image" src="https://avatars.githubusercontent.com/u/18791?s=40&amp;v=4" width="20" height="20" alt="@joelreymont">
</a>  </p>
</div>

        


        


        

        


        


        


        


        


        


        


        


        


        


        


        


        


        


        

        


        <div data-team-hovercards-enabled="" id="event-21100756933" data-gid="LOE_lADOAGeMds7ZL2BrzwAAAATps__F">

      


          <p><a data-hovercard-type="organization" data-hovercard-url="/orgs/ocaml/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/ocaml"><img src="https://avatars.githubusercontent.com/u/1841483?s=40&amp;v=4" width="20" height="20" alt="@ocaml"></a>
<a user_name_display_configuration="false" data-hovercard-type="organization" data-hovercard-url="/orgs/ocaml/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/ocaml">ocaml</a>




        locked as <strong>too heated </strong>and limited conversation to collaborators


      </p><a href="#event-21100756933"><relative-time datetime="2025-11-21T16:22:01Z">Nov 21, 2025</relative-time></a>

    </div>



  <!-- Rendered timeline since 2025-11-21 08:22:01 -->
  



      

    </div>

    
  </div>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[PS5 now costs less than 64GB of DDR5 memory. RAM jumps to $600 due to shortage (450 pts)]]></title>
            <link>https://www.tomshardware.com/pc-components/ddr5/64gb-of-ddr5-memory-now-costs-more-than-an-entire-ps5-even-after-a-discount-trident-z5-neo-kit-jumps-to-usd600-due-to-dram-shortage-and-its-expected-to-get-worse-into-2026</link>
            <guid>46038143</guid>
            <pubDate>Mon, 24 Nov 2025 19:29:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tomshardware.com/pc-components/ddr5/64gb-of-ddr5-memory-now-costs-more-than-an-entire-ps5-even-after-a-discount-trident-z5-neo-kit-jumps-to-usd600-due-to-dram-shortage-and-its-expected-to-get-worse-into-2026">https://www.tomshardware.com/pc-components/ddr5/64gb-of-ddr5-memory-now-costs-more-than-an-entire-ps5-even-after-a-discount-trident-z5-neo-kit-jumps-to-usd600-due-to-dram-shortage-and-its-expected-to-get-worse-into-2026</a>, See on <a href="https://news.ycombinator.com/item?id=46038143">Hacker News</a></p>
<div id="readability-page-1" class="page"><section data-widget-type="contentparsed" id="content">
<div>
<div>
<picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/kAur2rfLMuXGArbhKVE2jU-1920-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/kAur2rfLMuXGArbhKVE2jU-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/kAur2rfLMuXGArbhKVE2jU-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/kAur2rfLMuXGArbhKVE2jU-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/kAur2rfLMuXGArbhKVE2jU-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/kAur2rfLMuXGArbhKVE2jU-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/kAur2rfLMuXGArbhKVE2jU-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/kAur2rfLMuXGArbhKVE2jU.jpg" alt="G.Skill Trident Z5 Neo RGB DDR5-6000 C26" srcset="https://cdn.mos.cms.futurecdn.net/kAur2rfLMuXGArbhKVE2jU-1920-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/kAur2rfLMuXGArbhKVE2jU-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/kAur2rfLMuXGArbhKVE2jU-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/kAur2rfLMuXGArbhKVE2jU-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/kAur2rfLMuXGArbhKVE2jU-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/kAur2rfLMuXGArbhKVE2jU-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/kAur2rfLMuXGArbhKVE2jU-320-80.jpg 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/kAur2rfLMuXGArbhKVE2jU.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/kAur2rfLMuXGArbhKVE2jU.jpg" data-pin-nopin="true" fetchpriority="high">
</picture>
</div>
<figcaption>
<span>(Image credit: Tom's Hardware)</span>
</figcaption>
</div>
<div id="article-body">
<p id="9d321488-26af-4db6-8ed2-b2741f1ade25">Thanks to the AI boom devouring the majority of the world's memory and storage supply, end-consumers are now facing increasingly inflated prices for common components. DDR5 RAM, a necessity for building current-gen Intel or AMD systems, has now reached record highs in terms of pricing; a 64 GB kit of <a data-analytics-id="inline-link" href="https://click.linksynergy.com/deeplink?id=kXQk6%2AivFEQ&amp;mid=44583&amp;u1=tomshardware-us-1168235116173237457&amp;murl=https%3A%2F%2Fwww.newegg.com%2Fg-skill-trident-z5-neo-rgb-series-64gb-ddr5-6000-cas-latency-cl30-desktop-memory-black%2Fp%2FN82E16820374445" target="_blank" data-url="https://www.newegg.com/g-skill-trident-z5-neo-rgb-series-64gb-ddr5-6000-cas-latency-cl30-desktop-memory-black/p/N82E16820374445" referrerpolicy="no-referrer-when-downgrade" rel="sponsored noopener" data-hl-processed="hawklinks" data-google-interstitial="false" data-placeholder-url="https://click.linksynergy.com/deeplink?id=kXQk6%2AivFEQ&amp;mid=44583&amp;u1=hawk-custom-tracking&amp;murl=https%3A%2F%2Fwww.newegg.com%2Fg-skill-trident-z5-neo-rgb-series-64gb-ddr5-6000-cas-latency-cl30-desktop-memory-black%2Fp%2FN82E16820374445" data-merchant-name="Newegg" data-merchant-id="107327" data-merchant-network="LS" data-merchant-url="newegg.com" data-mrf-recirculation="inline-link">G.Skill's Trident Z5 Neo 6000 MT/s RAM is listed at $599.99 on Newegg</a> right now — that's $200 more than a PS5 Slim or a Microsoft Xbox Series S, and just $50 shy off an entire PS5 Pro at the moment.</p><div id="slice-container-table-QNpg9qAnQZLpzym4GViW9h-6O7EktLjkwdzrSK3U04R4BSw0qWtWeWV"><p>Swipe to scroll horizontally</p><svg viewBox="0 0 23 30" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M21.554 15.726a2.878 2.878 0 0 0-1.705-.374 2.881 2.881 0 0 0-1.388-3.068 2.877 2.877 0 0 0-1.992-.333 2.884 2.884 0 0 0-.1-.766 2.865 2.865 0 0 0-1.346-1.75c-.47-.27-.996-.4-1.527-.385l2.742-4.73a2.87 2.87 0 0 0 .323-.83h2.612V2.084h-2.661A2.861 2.861 0 0 0 15.18.385a2.903 2.903 0 0 0-3.952 1.055l-.373.644H2.983l1.003-1L2.99.09 1.28 1.793l-.999.995L2.99 5.484l.998-.994-1.003-.999h7.054L6.505 9.586c-.34.066-.905.186-1.523.366-1.405.41-2.321.895-2.8 1.483-.742.911-1.159 2.513-1.277 4.898l-.001.01c-.067 1.816.946 6.943.99 7.16a.688.688 0 0 0 1.35-.266c-.01-.051-1.023-5.177-.963-6.84.127-2.556.598-3.64.97-4.098.133-.163.602-.587 2.104-1.027l.206-.058-1.425 2.458a.685.685 0 0 0 .252.937c.33.19.75.077.94-.251L12.42 2.126a1.52 1.52 0 0 1 2.07-.552c.35.2.6.527.705.916.105.39.051.797-.15 1.145l-4.767 8.222a.685.685 0 0 0 .252.937c.33.19.75.077.94-.25l.794-1.368c.201-.348.529-.597.92-.702a1.508 1.508 0 0 1 1.854 1.066c.105.39.052.796-.15 1.144l-.377.652-.002.002-.898 1.55a.685.685 0 0 0 .252.938c.329.189.75.077.94-.251l.9-1.551c.201-.348.528-.597.92-.702a1.512 1.512 0 0 1 1.703 2.21l-1.223 2.11a.685.685 0 0 0 .252.938c.33.189.75.076.941-.252l.5-.862c.202-.348.529-.597.92-.702.392-.104.8-.051 1.15.15.723.416.972 1.34.554 2.06l-3.525 6.08c-.517.892-1.57 1.795-3.044 2.611-1.156.64-2.163.998-2.173 1.002a.685.685 0 0 0 .23 1.333.688.688 0 0 0 .229-.04c.18-.062 4.419-1.575 5.952-4.22l3.524-6.08a2.878 2.878 0 0 0-1.059-3.934Z" fill="#333"></path></svg></div><p id="61c59b9a-eccc-4a0e-aa85-a2940875ded0-1">A quick glance at price tracking data, and G.Skill's Trident Z5 Neo kit has regularly sat at $205-$220 for the past few months, and it was only in late October that it started to pick up steam. From September 20th when it was listed at $220, to $640 now. In just 2 months we've witnessed an astounding ~190% surge.</p><p>Right as this particular Trident Z5 Neo kit began to skyrocket in price was when the industry first started to pick up on the affects of the AI crunch. A few days later we published our<a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/dram/dram-prices-surge-171-percent-year-over-year-ai-demand-drives-a-higher-yoy-price-increase-than-gold" data-mrf-recirculation="inline-link" data-before-rewrite-localise="https://www.tomshardware.com/pc-components/dram/dram-prices-surge-171-percent-year-over-year-ai-demand-drives-a-higher-yoy-price-increase-than-gold"> initial coverage on DDR5 RAM price hikes</a>; from there, the situation has only worsened to reach worrying levels.</p><figure data-bordeaux-image-check="" id="f73d2283-60de-4a32-b9e1-a1505ebef31c"><div><p> <picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/9FDNMo8RQMZVeCFzURP926-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/9FDNMo8RQMZVeCFzURP926-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/9FDNMo8RQMZVeCFzURP926-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/9FDNMo8RQMZVeCFzURP926-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/9FDNMo8RQMZVeCFzURP926-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/9FDNMo8RQMZVeCFzURP926-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/9FDNMo8RQMZVeCFzURP926.jpg" alt="NAND Flash pricing decline" srcset="https://cdn.mos.cms.futurecdn.net/9FDNMo8RQMZVeCFzURP926-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/9FDNMo8RQMZVeCFzURP926-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/9FDNMo8RQMZVeCFzURP926-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/9FDNMo8RQMZVeCFzURP926-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/9FDNMo8RQMZVeCFzURP926-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/9FDNMo8RQMZVeCFzURP926-320-80.jpg 320w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/9FDNMo8RQMZVeCFzURP926.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/9FDNMo8RQMZVeCFzURP926.jpg">
</picture></p></div><figcaption itemprop="caption description"><span itemprop="copyrightHolder">(Image credit: Micron)</span></figcaption></figure><p id="0ca4beb7-2a85-4637-93ce-d13fae10e0a2">Insane mark-up aside, the kit itself is one of the best on the market, recommend as the top pick for DDR5 memory <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/best-ram,4057.html" target="_blank" data-mrf-recirculation="inline-link" data-before-rewrite-localise="https://www.tomshardware.com/reviews/best-ram,4057.html">in our roundup</a>. Unfortunately, it seems like high prices are going to be the story going forward. The surge in demand for AI projects will see production lines will prioritizing serving AI clients, leaving consumers to pay through the nose or make the best of what they have. Experts speculate that both DRAM and NAND constraints will become normal throughout 2026 as Big Tech looks to pursue AGI.</p><p>In the meantime, hard drives are vanishing from store shelves to the point where <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/microsd-cards/large-capacity-microsd-cards-are-now-regularly-out-of-stock-in-japan-as-storage-crunch-claims-another-victim-high-capacity-hdds-are-also-vanishing" target="_blank" data-mrf-recirculation="inline-link" data-before-rewrite-localise="https://www.tomshardware.com/pc-components/microsd-cards/large-capacity-microsd-cards-are-now-regularly-out-of-stock-in-japan-as-storage-crunch-claims-another-victim-high-capacity-hdds-are-also-vanishing">microSD cards are serving as a feasible replacement</a> for them. Large-capacity nearline <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/hdds/ai-triggers-hard-drive-shortage-amidst-dram-squeeze-enterprise-hard-drives-on-backorder-by-2-years-as-hyperscalers-switch-to-qlc-ssds" target="_blank" data-mrf-recirculation="inline-link" data-before-rewrite-localise="https://www.tomshardware.com/pc-components/hdds/ai-triggers-hard-drive-shortage-amidst-dram-squeeze-enterprise-hard-drives-on-backorder-by-2-years-as-hyperscalers-switch-to-qlc-ssds">HDDs are backordered for 2 years</a>, as a result of which QLC SSDs are now being swept up at alarming rates. Many distributors are even selling <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/dram/taiwanese-distributors-enforcing-dram-motherboard-bundle-sales" data-mrf-recirculation="inline-link" data-before-rewrite-localise="https://www.tomshardware.com/pc-components/dram/taiwanese-distributors-enforcing-dram-motherboard-bundle-sales">memory and motherboards bundled together</a> to combat the global shortage.</p><p>Even Valve's upcoming Steam Machine will end up <a data-analytics-id="inline-link" href="https://www.tomshardware.com/video-games/pc-gaming/the-upcoming-steam-machine-wont-be-subsidized-like-consoles-to-hit-a-more-attractive-price-target-suggesting-high-relative-pricing-valve-engineer-confirms-the-device-competes-with-only-the-pc-market" target="_blank" data-mrf-recirculation="inline-link" data-before-rewrite-localise="https://www.tomshardware.com/video-games/pc-gaming/the-upcoming-steam-machine-wont-be-subsidized-like-consoles-to-hit-a-more-attractive-price-target-suggesting-high-relative-pricing-valve-engineer-confirms-the-device-competes-with-only-the-pc-market">costing more than expected </a>due to the production window of the device aligning with the DRAM crisis. That being said, memory has almost always lived in a rollercoaster cycle, with manufacturers oversupplying for a couple of years, then undersupplying for the next few. Looking at it optimistically, you're probably going to find DDR5 at bargain prices again <em>in 2027</em>.</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-QNpg9qAnQZLpzym4GViW9h"><section><p>Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.</p></section></div><a href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" id="478e8f6d-b7ca-4341-9485-cd5491a5afda" data-url="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><figure data-bordeaux-image-check=""><div><p> <picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/7cUTDmN2PHNRiNBVqbKf56-676-80.png.webp 1200w, https://cdn.mos.cms.futurecdn.net/7cUTDmN2PHNRiNBVqbKf56-676-80.png.webp 1024w, https://cdn.mos.cms.futurecdn.net/7cUTDmN2PHNRiNBVqbKf56-676-80.png.webp 970w, https://cdn.mos.cms.futurecdn.net/7cUTDmN2PHNRiNBVqbKf56-650-80.png.webp 650w, https://cdn.mos.cms.futurecdn.net/7cUTDmN2PHNRiNBVqbKf56-480-80.png.webp 480w, https://cdn.mos.cms.futurecdn.net/7cUTDmN2PHNRiNBVqbKf56-320-80.png.webp 320w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/7cUTDmN2PHNRiNBVqbKf56.png" alt="Google Preferred Source" srcset="https://cdn.mos.cms.futurecdn.net/7cUTDmN2PHNRiNBVqbKf56-676-80.png 1200w, https://cdn.mos.cms.futurecdn.net/7cUTDmN2PHNRiNBVqbKf56-676-80.png 1024w, https://cdn.mos.cms.futurecdn.net/7cUTDmN2PHNRiNBVqbKf56-676-80.png 970w, https://cdn.mos.cms.futurecdn.net/7cUTDmN2PHNRiNBVqbKf56-650-80.png 650w, https://cdn.mos.cms.futurecdn.net/7cUTDmN2PHNRiNBVqbKf56-480-80.png 480w, https://cdn.mos.cms.futurecdn.net/7cUTDmN2PHNRiNBVqbKf56-320-80.png 320w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/7cUTDmN2PHNRiNBVqbKf56.png" data-pin-media="https://cdn.mos.cms.futurecdn.net/7cUTDmN2PHNRiNBVqbKf56.png">
</picture></p></div></figure></a><p id="9ab8ab59-5b2c-47d2-b605-85e7cb64f14f"><em>Follow</em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank" data-url="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none" data-mrf-recirculation="inline-link"><em> Tom's Hardware on Google News</em></a><em>, or</em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank" data-url="https://google.com/preferences/source?q=" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none" data-mrf-recirculation="inline-link"><em> add us as a preferred source</em></a><em>, to get our latest news, analysis, &amp; reviews in your feeds.</em></p>
</div>




<!-- Drop in a standard article here maybe? -->


<div id="slice-container-authorBio-QNpg9qAnQZLpzym4GViW9h"><p>Hassam Nasir is a die-hard hardware enthusiast with years of experience as a tech editor and writer, focusing on detailed CPU comparisons and general hardware news. When he’s not working, you’ll find him bending tubes for his ever-evolving custom water-loop gaming rig or benchmarking the latest CPUs and GPUs just for fun.  </p></div>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Unpowered SSDs slowly lose data (655 pts)]]></title>
            <link>https://www.xda-developers.com/your-unpowered-ssd-is-slowly-losing-your-data/</link>
            <guid>46038099</guid>
            <pubDate>Mon, 24 Nov 2025 19:25:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.xda-developers.com/your-unpowered-ssd-is-slowly-losing-your-data/">https://www.xda-developers.com/your-unpowered-ssd-is-slowly-losing-your-data/</a>, See on <a href="https://news.ycombinator.com/item?id=46038099">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

                            <div data-nosnippet="">
                                            <p><a href="https://www.xda-developers.com/author/tanveer-singh/">
                                    <img src="https://static0.xdaimages.com/wordpress%2Fwp-content%2Fauthors%2F6698b6624a3fe-PXL_20240717_153949958.jpg?fit=crop&amp;w=90&amp;h=90" alt="4" loading="lazy" decoding="async">
                                </a>
                                                    </p>
                                    </div>
                        
                                            
                                
                                    <p>After a 7-year corporate stint, Tanveer found his love for writing and tech too much to resist. An MBA in Marketing and the owner of a PC building business, he writes on PC hardware, technology, and Windows. When not scouring the web for ideas, he can be found building PCs, watching anime, or playing Smash Karts on his RTX 3080 (sigh). </p>
                                    </div><div id="article-body" itemprop="articleBody"><p>SSDs have all but replaced hard drives when it comes to primary storage. They're orders of magnitude faster, more convenient, and consume less power than mechanical hard drives. That said, if you're also using SSDs for cold storage, expecting the drives lying in your drawer to work perfectly after years, you might want to rethink your strategy. Your reliable SSD could suffer from corrupted or lost data if left unpowered for extended periods. This is why many users <a href="https://www.xda-developers.com/why-i-never-trust-ssd-for-long-term-storage/" target="_blank">don't consider SSDs a reliable long-term storage medium</a>, and prefer using hard drives, magnetic tape, or M-Disc instead.</p>    <!-- No AdsNinja v10 Client! --><!-- No AdsNinja v10 Client! --><h2 id="your-ssd-data-isn-39-t-as-permanent-as-you-think">
                        Your SSD data isn't as permanent as you think
               </h2><h3 id="non-volatile-with-an-asterisk">
            Non-volatile with an asterisk
    </h3>


                            
            
    
<p>Unlike hard drives that magnetize spinning discs to store data, SSDs modify the electrical charge in NAND flash cells to represent 0 and 1. NAND flash retains data in underlying transistors even when power is removed, similar to other forms of non-volatile memory. However, the duration for which your SSD can retain data without power is the key here. Even the cheapest SSDs, say those with QLC NAND, can safely store data for about a year of being completely unpowered. More expensive TLC NAND can retain data for up to 3 years, while MLC and SLC NAND are good for 5 years and 10 years of unpowered storage, respectively.</p>    <p>The problem is that most consumer SSDs use only TLC or QLC NAND, so users who leave their SSDs unpowered for over a year are risking the integrity of their data. The reliability of QLC NAND has improved over the years, so you should probably consider 2–3 years of unpowered usage as the guardrails. <a href="https://www.xda-developers.com/will-ssd-lose-data-without-power/" target="_blank">Without power, the voltage stored in the NAND cells can be lost</a>, either resulting in missing data or completely useless drives.</p>    <p>This data retention deficiency of consumer SSDs makes them an unreliable medium for long-term data storage, especially for creative professionals and researchers. HDDs can suffer from bit rot, too, due to wear and tear, but they're still more resistant to power loss. If you haven't checked your archives in a while, I'd recommend doing so at the earliest.</p>            
    
                    
                        
    
    
                                        
    
        
    
        
                
        
    <div data-include-community-rating="false" id="ssd-myths-that-are-simply-untrue" data-nosnippet="">
        
        
                                                            
                    <picture><source media="(max-width: 480px)" data-srcset="https://static0.xdaimages.com/wordpress/wp-content/uploads/2024/10/ssd-wear.jpg?q=49&amp;fit=crop&amp;w=140&amp;h=98&amp;dpr=2" srcset="https://static0.xdaimages.com/wordpress/wp-content/uploads/2024/10/ssd-wear.jpg?q=49&amp;fit=crop&amp;w=140&amp;h=98&amp;dpr=2">
        <img width="440" height="364" loading="lazy" decoding="async" alt="A graphic showing a hand holding an SSD." data-img-url="https://static0.xdaimages.com/wordpress/wp-content/uploads/2024/10/ssd-wear.jpg?q=49&amp;fit=crop&amp;w=220&amp;h=182&amp;dpr=2" src="https://static0.xdaimages.com/wordpress/wp-content/uploads/2024/10/ssd-wear.jpg?q=49&amp;fit=crop&amp;w=220&amp;h=182&amp;dpr=2">
        </picture>
                
            
        
                    <p><span data-field="label"><label>Related</label></span></p><div>

                
                
                	
	
<h5>
    					        				
		<a href="https://www.xda-developers.com/ssd-myths-that-are-simply-untrue/" title="5 SSD myths that are simply untrue" target="_blank">
			5 SSD myths that are simply untrue
		</a>
	</h5>

                                                    
                									<p>Don't fall for these common SSD misunderstandings.</p>
			

                    


                
                
                
                

                
                
                
                	


                

                
                
                
            </div>
                
        
        
        
            </div>
<!-- No AdsNinja v10 Client! --><h2 id="but-most-people-don-39-t-need-to-worry-about-it">
                        But, most people don't need to worry about it
               </h2><h3 id="archival-storage-isn-39-t-that-common">
            Archival storage isn't that common
    </h3>


                            
            
    
<p>The scenario I described above isn't relevant to people outside enterprise, enthusiast, and solopreneur usage. The need to store tons of data for years on drives that aren't plugged in isn't a concern for most people, who use one or two SSDs on their PC that might be left without power for only a few months, at the maximum. You've probably lost data on your SSD due to a rare power surge or a <a href="https://www.xda-developers.com/do-this-if-ssd-dying/" target="_blank">faulty drive</a> rather than voltage loss. Some factors, like temperature and the quality of the underlying NAND flash, can accelerate this voltage loss.</p>    <p>SSDs aren't eternal, even if you keep them powered on forever. The <a href="https://www.xda-developers.com/ways-check-how-much-life-your-ssd-has-left/" target="_blank">limited write cycles of NAND flash</a> will eventually bring an SSD to the end of its lifecycle, but the majority of users will probably replace the drive before that ever happens. So, you don't need to worry about writing too much data to your SSD or leaving your PC turned off for days, weeks, or even months. Just don't trust an unpowered SSD that's gathering dust in the house for years, which brings me to my next point.</p>            
    
                    
                        
    
    
                                        
    
        
    
        
                
        
    <div data-include-community-rating="false" id="ways-to-extend-your-ssds-lifespan" data-nosnippet="">
        
        
                                                            
                    <picture><source media="(max-width: 480px)" data-srcset="https://static0.xdaimages.com/wordpress/wp-content/uploads/wm/2024/07/aiffro-nas-four-ssds.jpg?q=49&amp;fit=crop&amp;w=140&amp;h=98&amp;dpr=2" srcset="https://static0.xdaimages.com/wordpress/wp-content/uploads/wm/2024/07/aiffro-nas-four-ssds.jpg?q=49&amp;fit=crop&amp;w=140&amp;h=98&amp;dpr=2">
        <img width="440" height="364" loading="lazy" decoding="async" alt="The Aiffro NAS with four SSDs slotted in" data-img-url="https://static0.xdaimages.com/wordpress/wp-content/uploads/wm/2024/07/aiffro-nas-four-ssds.jpg?q=49&amp;fit=crop&amp;w=220&amp;h=182&amp;dpr=2" src="https://static0.xdaimages.com/wordpress/wp-content/uploads/wm/2024/07/aiffro-nas-four-ssds.jpg?q=49&amp;fit=crop&amp;w=220&amp;h=182&amp;dpr=2">
        </picture>
                
            
        
                    <p><span data-field="label"><label>Related</label></span></p><div>

                
                
                	
	
<h5>
    					        				
		<a href="https://www.xda-developers.com/ways-to-extend-your-ssds-lifespan/" title="3 ways to extend your SSD's lifespan" target="_blank">
			3 ways to extend your SSD's lifespan
		</a>
	</h5>

                                                    
                									<p>Don't waste your new SSD with needless writes.</p>
			

                    


                
                
                
                

                
                
                
                	


                

                
                
                
            </div>
                
        
        
        
            </div>
<!-- No AdsNinja v10 Client! --><h2 id="you-should-always-have-a-backup-anyway">
                        You should always have a backup anyway
               </h2><h3 id="prevention-is-better-than-cure">
            Prevention is better than cure
    </h3>


                            
            
    
<p><a href="https://www.xda-developers.com/how-back-up-windows-11-pc/" target="_blank">Backing up your data</a> is the simplest strategy to counteract the limitations of storage media. Having multiple copies of your data on different <a href="https://www.xda-developers.com/storage-types-officially-too-old/" target="_blank">types of storage</a> ensures that any unexpected incidents protect your data from vanishing forever. This is exactly what the <a href="https://www.xda-developers.com/how-to-quickly-set-up-the-3-2-1-backup-rule/" target="_blank">3-2-1 backup rule</a> talks about: 3 copies of data on at least 2 different storage media, with 1 copy stored off-site. For most people, this condition can easily be fulfilled by using their primary computer, a NAS, and cloud storage. Redundancy is the underlying principle that safeguards your data.</p>    <p>Whether it's the limited lifespan of your SSD, the potential for harmful exigencies like power failure, or the limits of data retention on flash storage, your backup will ensure your peace of mind. Yes, SSDs aren't the best choice for cold storage, but even if you're using hard drives, having a single copy of your data is asking for trouble. Every user will come face-to-face with drive failure sooner or later, so investing in a <a href="https://www.xda-developers.com/ways-nas-backup-all-devices/" target="_blank">robust backup system</a> isn't really optional if you care about your data.</p>            
    
                    
                        
    
    
                                        
    
        
    
        
                
        
    <div data-include-community-rating="false" id="backup-mistakes-that-risk-your-nas" data-nosnippet="">
        
        
                                                            
                    <picture><source media="(max-width: 480px)" data-srcset="https://static0.xdaimages.com/wordpress/wp-content/uploads/wm/2024/11/imac-nas-4.jpg?q=49&amp;fit=crop&amp;w=140&amp;h=98&amp;dpr=2" srcset="https://static0.xdaimages.com/wordpress/wp-content/uploads/wm/2024/11/imac-nas-4.jpg?q=49&amp;fit=crop&amp;w=140&amp;h=98&amp;dpr=2">
        <img width="440" height="364" loading="lazy" decoding="async" alt="An iMac set up as a NAS in a closet." data-img-url="https://static0.xdaimages.com/wordpress/wp-content/uploads/wm/2024/11/imac-nas-4.jpg?q=49&amp;fit=crop&amp;w=220&amp;h=182&amp;dpr=2" src="https://static0.xdaimages.com/wordpress/wp-content/uploads/wm/2024/11/imac-nas-4.jpg?q=49&amp;fit=crop&amp;w=220&amp;h=182&amp;dpr=2">
        </picture>
                
            
        
                    <p><span data-field="label"><label>Related</label></span></p>
                
        
        
        
            </div>
<h3 id="store-it-and-forget-it-doesn-39-t-work-for-ssds">
            Store it and forget it doesn't work for SSDs
    </h3>
<p>As long as you're using consumer SSDs for primary storage on your PC, it's all well and good. You'll most likely replace your drive long before exhausting its P/E cycles. For long-term storage, however, relying on SSDs is risky, since they can lose data if left without power for years. This data loss can occur anytime from 1 to 3 years of keeping your SSDs unpowered, so using alternate storage media and investing in a backup system should be your priorities.</p>    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Claude Advanced Tool Use (605 pts)]]></title>
            <link>https://www.anthropic.com/engineering/advanced-tool-use</link>
            <guid>46038047</guid>
            <pubDate>Mon, 24 Nov 2025 19:21:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.anthropic.com/engineering/advanced-tool-use">https://www.anthropic.com/engineering/advanced-tool-use</a>, See on <a href="https://news.ycombinator.com/item?id=46038047">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div data-theme="ivory"><p>The future of AI agents is one where models work seamlessly across hundreds or thousands of tools. An IDE assistant that integrates git operations, file manipulation, package managers, testing frameworks, and deployment pipelines. An operations coordinator that connects Slack, GitHub, Google Drive, Jira, company databases, and dozens of MCP servers simultaneously.</p><p>To <a href="https://www.anthropic.com/research/building-effective-agents" target="_blank" rel="noopener noreferrer">build effective agents</a>, they need to work with unlimited tool libraries without stuffing every definition into context upfront. Our blog article on using <a href="https://www.anthropic.com/engineering/code-execution-with-mcp" target="_blank" rel="noopener noreferrer">code execution with MCP</a> discussed how tool results and definitions can sometimes consume 50,000+ tokens before an agent reads a request. Agents should discover and load tools on-demand, keeping only what's relevant for the current task.</p><p>Agents also need the ability to call tools from code. When using natural language tool calling, each invocation requires a full inference pass, and intermediate results pile up in context whether they're useful or not. Code is a natural fit for orchestration logic, such as loops, conditionals, and data transformations. Agents need the flexibility to choose between code execution and inference based on the task at hand.</p><p>Agents also need to learn correct tool usage from examples, not just schema definitions. JSON schemas define what's structurally valid, but can't express usage patterns: when to include optional parameters, which combinations make sense, or what conventions your API expects.</p><p>Today, we're releasing three features that make this possible:</p><ul><li><strong>Tool Search Tool, </strong>which allows Claude to use search tools to access thousands of tools without consuming its context window</li><li><strong>Programmatic Tool Calling</strong>, which allows Claude to invoke tools in a code execution environment reducing the impact on the model’s context window</li><li><strong>Tool Use Examples</strong>, which provides a universal standard for demonstrating how to effectively use a given tool</li></ul><p>In internal testing, we’ve found these features have helped us build things that wouldn’t have been possible with conventional tool use patterns. For example,<strong> <a href="https://www.claude.com/claude-for-excel" target="_blank" rel="noopener noreferrer">Claude for Excel</a> </strong>uses Programmatic Tool Calling to read and modify spreadsheets with thousands of rows without overloading the model’s context window.</p><p>Based on our experience, we believe these features open up new possibilities for what you can build with Claude.</p><h2 id="tool-search-tool">Tool Search Tool</h2><h3 id="the-challenge">The challenge</h3><p>MCP tool definitions provide important context, but as more servers connect, those tokens can add up. Consider a five-server setup:</p><ul><li>GitHub: 35 tools (~26K tokens)</li><li>Slack: 11 tools (~21K tokens)</li><li>Sentry: 5 tools (~3K tokens)</li><li>Grafana: 5 tools (~3K tokens)</li><li>Splunk: 2 tools (~2K tokens)</li></ul><p>That's 58 tools consuming approximately 55K tokens before the conversation even starts. Add more servers like Jira (which alone uses ~17K tokens) and you're quickly approaching 100K+ token overhead. At Anthropic, we've seen tool definitions consume 134K tokens before optimization.</p><p>But token cost isn't the only issue. The most common failures are wrong tool selection and incorrect parameters, especially when tools have similar names like <code>notification-send-user</code> vs. <code>notification-send-channel</code>.</p><h3 id="our-solution">Our solution</h3><p>Instead of loading all tool definitions upfront, the Tool Search Tool discovers tools on-demand. Claude only sees the tools it actually needs for the current task.</p><div><figure><img alt="Tool Search Tool diagram" loading="lazy" width="1999" height="1125" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Ff359296f770706608901eadaffbff4ca0b67874c-1999x1125.png&amp;w=2048&amp;q=75 1x, https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Ff359296f770706608901eadaffbff4ca0b67874c-1999x1125.png&amp;w=3840&amp;q=75 2x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Ff359296f770706608901eadaffbff4ca0b67874c-1999x1125.png&amp;w=3840&amp;q=75"><figcaption><em>Tool Search Tool preserves 191,300 tokens of context compared to 122,800 with Claude’s traditional approach.</em></figcaption></figure></div><p>Traditional approach:</p><ul><li>All tool definitions loaded upfront (~72K tokens for 50+ MCP tools)</li><li>Conversation history and system prompt compete for remaining space</li><li>Total context consumption: ~77K tokens before any work begins</li></ul><p>With the Tool Search Tool:</p><ul><li>Only the Tool Search Tool loaded upfront (~500 tokens)</li><li>Tools discovered on-demand as needed (3-5 relevant tools, ~3K tokens)</li><li>Total context consumption: ~8.7K tokens, preserving 95% of context window</li></ul><p>This represents an 85% reduction in token usage while maintaining access to your full tool library. Internal testing showed significant accuracy improvements on MCP evaluations when working with large tool libraries. Opus 4 improved from 49% to 74%, and Opus 4.5 improved from 79.5% to 88.1% with Tool Search Tool enabled.</p><h3 id="how-the-tool-search-tool-works">How the Tool Search Tool works</h3><p>The Tool Search Tool lets Claude dynamically discover tools instead of loading all definitions upfront. You provide all your tool definitions to the API, but mark tools with <code>defer_loading: true</code> to make them discoverable on-demand. Deferred tools aren't loaded into Claude's context initially. Claude only sees the Tool Search Tool itself plus any tools with <code>defer_loading: false</code> (your most critical, frequently-used tools).</p><p>When Claude needs specific capabilities, it searches for relevant tools. The Tool Search Tool returns references to matching tools, which get expanded into full definitions in Claude's context.</p><p>For example, if Claude needs to interact with GitHub, it searches for "github," and only <code>github.createPullRequest</code> and <code>github.listIssues</code> get loaded—not your other 50+ tools from Slack, Jira, and Google Drive.</p><p>This way, Claude has access to your full tool library while only paying the token cost for tools it actually needs.</p><p><strong>Implementation:</strong></p><div><pre><code>{
  "tools": [
    // Include a tool search tool (regex, BM25, or custom)
    {"type": "tool_search_tool_regex_20251119", "name": "tool_search_tool_regex"},

    // Mark tools for on-demand discovery
    {
      "name": "github.createPullRequest",
      "description": "Create a pull request",
      "input_schema": {...},
      "defer_loading": true
    }
    // ... hundreds more deferred tools with defer_loading: true
  ]
}
</code></pre></div><p>For MCP servers, you can defer loading entire servers while keeping specific high-use tools loaded:</p><div><pre><code>{
  "type": "mcp_toolset",
  "mcp_server_name": "google-drive",
  "default_config": {"defer_loading": true}, # defer loading the entire server
  "configs": {
    "search_files": {
"defer_loading": false
    }  // Keep most used tool loaded
  }
}</code></pre></div><p>The Claude Developer Platform provides regex-based and BM25-based search tools out of the box, but you can also implement custom search tools using embeddings or other strategies.</p><h3 id="when-to-use-the-tool-search-tool">When to use the Tool Search Tool</h3><p>Like any architectural decision, enabling the Tool Search Tool involves trade-offs. The feature adds a search step before tool invocation, so it delivers the best ROI when the context savings and accuracy improvements outweigh additional latency.</p><p><strong>Use it when:</strong></p><ul><li>Tool definitions consuming &gt;10K tokens</li><li>Experiencing tool selection accuracy issues</li><li>Building MCP-powered systems with multiple servers</li><li>10+ tools available</li></ul><p><strong>Less beneficial when:</strong></p><ul><li>Small tool library (&lt;10 tools)</li><li>All tools used frequently in every session</li><li>Tool definitions are compact</li></ul><h2 id="programmatic-tool-calling">Programmatic Tool Calling</h2><h3 id="the-challenge">The challenge</h3><p>Traditional tool calling creates two fundamental problems as workflows become more complex:</p><ul><li><strong>Context pollution from intermediate results</strong>: When Claude analyzes a 10MB log file for error patterns, the entire file enters its context window, even though Claude only needs a summary of error frequencies. When fetching customer data across multiple tables, every record accumulates in context regardless of relevance. These intermediate results consume massive token budgets and can push important information out of the context window entirely.</li><li><strong>Inference overhead and manual synthesis</strong>: Each tool call requires a full model inference pass. After receiving results, Claude must "eyeball" the data to extract relevant information, reason about how pieces fit together, and decide what to do next—all through natural language processing. A five tool workflow means five inference passes plus Claude parsing each result, comparing values, and synthesizing conclusions. This is both slow and error-prone.</li></ul><h3 id="our-solution">Our solution</h3><p>Programmatic Tool Calling enables Claude to orchestrate tools through code rather than through individual API round-trips. Instead of Claude requesting tools one at a time with each result being returned to its context, Claude writes code that calls multiple tools, processes their outputs, and controls what information actually enters its context window.</p><p>Claude excels at writing code and by letting it express orchestration logic in Python rather than through natural language tool invocations, you get more reliable, precise control flow. Loops, conditionals, data transformations, and error handling are all explicit in code rather than implicit in Claude's reasoning.</p><h4 id="example-budget-compliance-check">Example: Budget compliance check</h4><p>Consider a common business task: "Which team members exceeded their Q3 travel budget?"</p><p>You have three tools available:</p><ul><li><code>get_team_members(department)</code> - Returns team member list with IDs and levels</li><li><code>get_expenses(user_id, quarter)</code> - Returns expense line items for a user</li><li><code>get_budget_by_level(level)</code> - Returns budget limits for an employee level</li></ul><p><strong>Traditional approach</strong>:</p><ul><li>Fetch team members → 20 people</li><li>For each person, fetch their Q3 expenses → 20 tool calls, each returning 50-100 line items (flights, hotels, meals, receipts)</li><li>Fetch budget limits by employee level</li><li>All of this enters Claude's context: 2,000+ expense line items (50 KB+)</li><li>Claude manually sums each person's expenses, looks up their budget, compares expenses against budget limits</li><li>More round-trips to the model, significant context consumption</li></ul><p><strong>With Programmatic Tool Calling</strong>:</p><p>Instead of each tool result returning to Claude, Claude writes a Python script that orchestrates the entire workflow. The script runs in the Code Execution tool (a sandboxed environment), pausing when it needs results from your tools. When you return tool results via the API, they're processed by the script rather than consumed by the model. The script continues executing, and Claude only sees the final output.</p><div><figure><img alt="Programmatic tool calling flow" loading="lazy" width="1999" height="1491" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F65737d69a3290ed5c1f3c3b8dc873645a9dcc2eb-1999x1491.png&amp;w=2048&amp;q=75 1x, https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F65737d69a3290ed5c1f3c3b8dc873645a9dcc2eb-1999x1491.png&amp;w=3840&amp;q=75 2x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F65737d69a3290ed5c1f3c3b8dc873645a9dcc2eb-1999x1491.png&amp;w=3840&amp;q=75"><figcaption>Programmatic Tool Calling enables Claude to orchestrate tools through code rather than through individual API round-trips, allowing for parallel tool execution.</figcaption></figure></div><p>Here's what Claude's orchestration code looks like for the budget compliance task:</p><div><pre><code>team = await get_team_members("engineering")

# Fetch budgets for each unique level
levels = list(set(m["level"] for m in team))
budget_results = await asyncio.gather(*[
    get_budget_by_level(level) for level in levels
])

# Create a lookup dictionary: {"junior": budget1, "senior": budget2, ...}
budgets = {level: budget for level, budget in zip(levels, budget_results)}

# Fetch all expenses in parallel
expenses = await asyncio.gather(*[
    get_expenses(m["id"], "Q3") for m in team
])

# Find employees who exceeded their travel budget
exceeded = []
for member, exp in zip(team, expenses):
    budget = budgets[member["level"]]
    total = sum(e["amount"] for e in exp)
    if total &gt; budget["travel_limit"]:
        exceeded.append({
            "name": member["name"],
            "spent": total,
            "limit": budget["travel_limit"]
        })

print(json.dumps(exceeded))</code></pre></div><p>Claude's context receives only the final result: the two to three people who exceeded their budget. The 2,000+ line items, the intermediate sums, and the budget lookups do not affect Claude’s context, reducing consumption from 200KB of raw expense data to just 1KB of results.</p><p>The efficiency gains are substantial:</p><ul><li><strong>Token savings</strong>: By keeping intermediate results out of Claude's context, PTC dramatically reduces token consumption. Average usage dropped from 43,588 to 27,297 tokens, a 37% reduction on complex research tasks.</li><li><strong>Reduced latency</strong>: Each API round-trip requires model inference (hundreds of milliseconds to seconds). When Claude orchestrates 20+ tool calls in a single code block, you eliminate 19+ inference passes. The API handles tool execution without returning to the model each time.</li><li><strong>Improved accuracy</strong>: By writing explicit orchestration logic, Claude makes fewer errors than when juggling multiple tool results in natural language. Internal knowledge retrieval improved from 25.6% to 28.5%; <a href="https://arxiv.org/abs/2311.12983" target="_blank" rel="noopener noreferrer">GIA benchmarks</a> from 46.5% to 51.2%.</li></ul><p>Production workflows involve messy data, conditional logic, and operations that need to scale. Programmatic Tool Calling lets Claude handle that complexity programmatically while keeping its focus on actionable results rather than raw data processing.</p><h3 id="how-programmatic-tool-calling-works">How Programmatic Tool Calling works</h3><h4 id="1-mark-tools-as-callable-from-code">1. Mark tools as callable from code</h4><p>Add code_execution to tools, and set allowed_callers to opt-in tools for programmatic execution:</p><div><pre><code>{
  "tools": [
    {
      "type": "code_execution_20250825",
      "name": "code_execution"
    },
    {
      "name": "get_team_members",
      "description": "Get all members of a department...",
      "input_schema": {...},
      "allowed_callers": ["code_execution_20250825"] # opt-in to programmatic tool calling
    },
    {
      "name": "get_expenses",
 	...
    },
    {
      "name": "get_budget_by_level",
	...
    }
  ]
}</code></pre></div><p>The API converts these tool definitions into Python functions that Claude can call.</p><h4 id="2-claude-writes-orchestration-code">2. Claude writes orchestration code</h4><p>Instead of requesting tools one at a time, Claude generates Python code:</p><div><pre><code>{
  "type": "server_tool_use",
  "id": "srvtoolu_abc",
  "name": "code_execution",
  "input": {
    "code": "team = get_team_members('engineering')\n..." # the code example above
  }
}</code></pre></div><h4 id="3-tools-execute-without-hitting-claudes-context">3. Tools execute without hitting Claude's context</h4><p>When the code calls get_expenses(), you receive a tool request with a caller field:</p><div><pre><code>{
  "type": "tool_use",
  "id": "toolu_xyz",
  "name": "get_expenses",
  "input": {"user_id": "emp_123", "quarter": "Q3"},
  "caller": {
    "type": "code_execution_20250825",
    "tool_id": "srvtoolu_abc"
  }
}</code></pre></div><p>You provide the result, which is processed in the Code Execution environment rather than Claude's context. This request-response cycle repeats for each tool call in the code.</p><h4 id="4-only-final-output-enters-context">4. Only final output enters context</h4><p>When the code finishes running, only the results of the code are returned to Claude:</p><div><pre><code>{
  "type": "code_execution_tool_result",
  "tool_use_id": "srvtoolu_abc",
  "content": {
    "stdout": "[{\"name\": \"Alice\", \"spent\": 12500, \"limit\": 10000}...]"
  }
}</code></pre></div><p>This is all Claude sees, not the 2000+ expense line items processed along the way.</p><h3 id="when-to-use-programmatic-tool-calling">When to use Programmatic Tool Calling</h3><p>Programmatic Tool Calling adds a code execution step to your workflow. This extra overhead pays off when the token savings, latency improvements, and accuracy gains are substantial.</p><p><strong>Most beneficial when:</strong></p><ul><li>Processing large datasets where you only need aggregates or summaries</li><li>Running multi-step workflows with three or more dependent tool calls</li><li>Filtering, sorting, or transforming tool results before Claude sees them</li><li>Handling tasks where intermediate data shouldn't influence Claude's reasoning</li><li>Running parallel operations across many items (checking 50 endpoints, for example)</li></ul><p><strong>Less beneficial when:</strong></p><ul><li>Making simple single-tool invocations</li><li>Working on tasks where Claude should see and reason about all intermediate results</li><li>Running quick lookups with small responses</li></ul><h2 id="tool-use-examples">Tool Use Examples</h2><h3 id="the-challenge">The challenge</h3><p>JSON Schema excels at defining structure–types, required fields, allowed enums–but it can't express usage patterns: when to include optional parameters, which combinations make sense, or what conventions your API expects.</p><p>Consider a support ticket API:</p><div><pre><code>{
  "name": "create_ticket",
  "input_schema": {
    "properties": {
      "title": {"type": "string"},
      "priority": {"enum": ["low", "medium", "high", "critical"]},
      "labels": {"type": "array", "items": {"type": "string"}},
      "reporter": {
        "type": "object",
        "properties": {
          "id": {"type": "string"},
          "name": {"type": "string"},
          "contact": {
            "type": "object",
            "properties": {
              "email": {"type": "string"},
              "phone": {"type": "string"}
            }
          }
        }
      },
      "due_date": {"type": "string"},
      "escalation": {
        "type": "object",
        "properties": {
          "level": {"type": "integer"},
          "notify_manager": {"type": "boolean"},
          "sla_hours": {"type": "integer"}
        }
      }
    },
    "required": ["title"]
  }
}</code></pre></div><p>The schema defines what's valid, but leaves critical questions unanswered:</p><ul><li><strong>Format ambiguity: </strong>Should <code>due_date</code> use "2024-11-06", "Nov 6, 2024", or "2024-11-06T00:00:00Z"?</li><li><strong>ID conventions: </strong>Is <code>reporter.id</code> a UUID, "USR-12345", or just "12345"?</li><li><strong>Nested structure usage: </strong>When should Claude populate <code>reporter.contact</code>?</li><li><strong>Parameter correlations: </strong>How do <code>escalation.level</code> and <code>escalation.sla_hours</code> relate to priority?</li></ul><p>These ambiguities can lead to malformed tool calls and inconsistent parameter usage.</p><h3 id="our-solution">Our solution</h3><p>Tool Use Examples let you provide sample tool calls directly in your tool definitions. Instead of relying on schema alone, you show Claude concrete usage patterns:</p><div><pre><code>{
    "name": "create_ticket",
    "input_schema": { /* same schema as above */ },
    "input_examples": [
      {
        "title": "Login page returns 500 error",
        "priority": "critical",
        "labels": ["bug", "authentication", "production"],
        "reporter": {
          "id": "USR-12345",
          "name": "Jane Smith",
          "contact": {
            "email": "jane@acme.com",
            "phone": "+1-555-0123"
          }
        },
        "due_date": "2024-11-06",
        "escalation": {
          "level": 2,
          "notify_manager": true,
          "sla_hours": 4
        }
      },
      {
        "title": "Add dark mode support",
        "labels": ["feature-request", "ui"],
        "reporter": {
          "id": "USR-67890",
          "name": "Alex Chen"
        }
      },
      {
        "title": "Update API documentation"
      }
    ]
  }</code></pre></div><p>From these three examples, Claude learns:</p><ul><li><strong>Format conventions</strong>: Dates use YYYY-MM-DD, user IDs follow USR-XXXXX, labels use kebab-case</li><li><strong>Nested structure patterns</strong>: How to construct the reporter object with its nested contact object</li><li><strong>Optional parameter correlations</strong>: Critical bugs have full contact info + escalation with tight SLAs; feature requests have reporter but no contact/escalation; internal tasks have title only</li></ul><p>In our own internal testing, tool use examples improved accuracy from 72% to 90% on complex parameter handling.</p><h3 id="when-to-use-tool-use-examples">When to use Tool Use Examples</h3><p>Tool Use Examples add tokens to your tool definitions, so they’re most valuable when accuracy improvements outweigh the additional cost.</p><p><strong>Most beneficial when:</strong></p><ul><li>Complex nested structures where valid JSON doesn't imply correct usage</li><li>Tools with many optional parameters and inclusion patterns matter</li><li>APIs with domain-specific conventions not captured in schemas</li><li>Similar tools where examples clarify which one to use (e.g., <code>create_ticket</code> vs <code>create_incident</code>)</li></ul><p><strong>Less beneficial when:</strong></p><ul><li>Simple single-parameter tools with obvious usage</li><li>Standard formats like URLs or emails that Claude already understands</li><li>Validation concerns better handled by JSON Schema constraints</li></ul><h2 id="best-practices">Best practices</h2><p>Building agents that take real-world actions means handling scale, complexity, and precision simultaneously. These three features work together to solve different bottlenecks in tool use workflows. Here's how to combine them effectively.</p><h3 id="layer-features-strategically">Layer features strategically</h3><p>Not every agent needs to use all three features for a given task. Start with your biggest bottleneck:</p><ul><li>Context bloat from tool definitions → Tool Search Tool</li><li>Large intermediate results polluting context → Programmatic Tool Calling</li><li>Parameter errors and malformed calls → Tool Use Examples</li></ul><p>This focused approach lets you address the specific constraint limiting your agent's performance, rather than adding complexity upfront.</p><p>Then layer additional features as needed. They're complementary: Tool Search Tool ensures the right tools are found, Programmatic Tool Calling ensures efficient execution, and Tool Use Examples ensure correct invocation.</p><h3 id="set-up-tool-search-tool-for-better-discovery">Set up Tool Search Tool for better discovery</h3><p>Tool search matches against names and descriptions, so clear, descriptive definitions improve discovery accuracy.</p><div><pre><code>// Good
{
    "name": "search_customer_orders",
    "description": "Search for customer orders by date range, status, or total amount. Returns order details including items, shipping, and payment info."
}

// Bad
{
    "name": "query_db_orders",
    "description": "Execute order query"
}</code></pre></div><p>Add system prompt guidance so Claude knows what's available:</p><div><pre><code>You have access to tools for Slack messaging, Google Drive file management, 
Jira ticket tracking, and GitHub repository operations. Use the tool search 
to find specific capabilities.</code></pre></div><p>Keep your three to five most-used tools always loaded, defer the rest. This balances immediate access for common operations with on-demand discovery for everything else.</p><h3 id="set-up-programmatic-tool-calling-for-correct-execution">Set up Programmatic Tool Calling for correct execution</h3><p>Since Claude writes code to parse tool outputs, document return formats clearly. This helps Claude write correct parsing logic:</p><div><pre><code>{
    "name": "get_orders",
    "description": "Retrieve orders for a customer.
Returns:
    List of order objects, each containing:
    - id (str): Order identifier
    - total (float): Order total in USD
    - status (str): One of 'pending', 'shipped', 'delivered'
    - items (list): Array of {sku, quantity, price}
    - created_at (str): ISO 8601 timestamp"
}</code></pre></div><p>See below for opt-in tools that benefit from programmatic orchestration:</p><ul><li>Tools that can run in parallel (independent operations)</li><li>Operations safe to retry (idempotent)</li></ul><h3 id="set-up-tool-use-examples-for-parameter-accuracy">Set up Tool Use Examples for parameter accuracy</h3><p>Craft examples for behavioral clarity:</p><ul><li>Use realistic data (real city names, plausible prices, not "string" or "value")</li><li>Show variety with minimal, partial, and full specification patterns</li><li>Keep it concise: 1-5 examples per tool</li><li>Focus on ambiguity (only add examples where correct usage isn't obvious from schema)</li></ul><h2 id="getting-started">Getting started</h2><p>These features are available in beta. To enable them, add the beta header and include the tools you need:</p><div><pre><code>client.beta.messages.create(
    betas=["advanced-tool-use-2025-11-20"],
    model="claude-sonnet-4-5-20250929",
    max_tokens=4096,
    tools=[
        {"type": "tool_search_tool_regex_20251119", "name": "tool_search_tool_regex"},
        {"type": "code_execution_20250825", "name": "code_execution"},
        # Your tools with defer_loading, allowed_callers, and input_examples
    ]
)</code></pre></div><p>For detailed API documentation and SDK examples, see our:</p><ul><li><a href="https://platform.claude.com/docs/en/agents-and-tools/tool-use/tool-search-tool" target="_blank" rel="noopener noreferrer"><span>D</span>ocumentation</a> and <a href="https://github.com/anthropics/claude-cookbooks/blob/main/tool_use/tool_search_with_embeddings.ipynb" target="_blank" rel="noopener noreferrer">cookbook</a> for Tool Search Tool</li><li><a href="https://platform.claude.com/docs/en/agents-and-tools/tool-use/programmatic-tool-calling" target="_blank" rel="noopener noreferrer">Documentation</a> and <a href="https://github.com/anthropics/claude-cookbooks/blob/main/tool_use/ptc.ipynb" target="_blank" rel="noopener noreferrer">cookbook</a> for Programmatic Tool Calling</li><li><a href="https://platform.claude.com/docs/en/agents-and-tools/tool-use/implement-tool-use#providing-tool-use-examples" target="_blank" rel="noopener noreferrer">Documentation</a> for Tool Use Examples</li></ul><p>These features move tool use from simple function calling toward intelligent orchestration. As agents tackle more complex workflows spanning dozens of tools and large datasets, dynamic discovery, efficient execution, and reliable invocation become foundational.</p><p>We're excited to see what you build.</p><h2 id="acknowledgements">Acknowledgements</h2><p>Written by Bin Wu, with contributions from Adam Jones, Artur Renault, Henry Tay, Jake Noble, Nathan McCandlish, Noah Picard, Sam Jiang, and the Claude Developer Platform team. This work builds on foundational research by Chris Gorgolewski, Daniel Jiang, Jeremy Fox and Mike Lambert. We also drew inspiration from across the AI ecosystem, including <a href="https://github.com/9600dev/llmvm" target="_blank" rel="noopener noreferrer">Joel Pobar's LLMVM</a>, <a href="https://blog.cloudflare.com/code-mode/" target="_blank" rel="noopener noreferrer">Cloudflare's Code Mode</a> and <a href="https://www.anthropic.com/engineering/code-execution-with-mcp" target="_blank" rel="noopener noreferrer">Code Execution as MCP</a>. Special thanks to Andy Schumeister, Hamish Kerr, Keir Bradwell, Matt Bleifer and Molly Vorwerck for their support.<br></p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Claude Opus 4.5 (1054 pts)]]></title>
            <link>https://www.anthropic.com/news/claude-opus-4-5</link>
            <guid>46037637</guid>
            <pubDate>Mon, 24 Nov 2025 18:53:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.anthropic.com/news/claude-opus-4-5">https://www.anthropic.com/news/claude-opus-4-5</a>, See on <a href="https://news.ycombinator.com/item?id=46037637">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div data-theme="ivory"><p>Our newest model, Claude Opus 4.5, is available today. It’s intelligent, efficient, and the best model in the world for coding, agents, and computer use. It’s also meaningfully better at everyday tasks like deep research and working with slides and spreadsheets. Opus 4.5 is a step forward in what AI systems can do, and a preview of larger changes to how work gets done.</p><p>Claude Opus 4.5 is state-of-the-art on tests of real-world software engineering:</p><div><figure><img alt="Chart comparing frontier models on SWE-bench Verified where Opus 4.5 scores highest" loading="lazy" width="3840" height="2160" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F7022a87aeb6eab1458d68412bc927306224ea9eb-3840x2160.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F7022a87aeb6eab1458d68412bc927306224ea9eb-3840x2160.png&amp;w=3840&amp;q=75"></figure></div><p>Opus 4.5 is available today on our apps, our API, and on all three major cloud platforms. If you’re a developer, simply use <code>claude-opus-4-5-20251101</code> via the <a href="https://platform.claude.com/docs/en/about-claude/models/overview">Claude API</a>. Pricing is now $5/$25 per million tokens—making Opus-level capabilities accessible to even more users, teams, and enterprises.</p><p>Alongside Opus, we’re releasing updates to the <a href="https://www.claude.com/platform/api">Claude Developer Platform</a>, <a href="https://www.claude.com/product/claude-code">Claude Code</a>, and our <a href="https://www.claude.com/download">consumer apps</a>. There are new tools for longer-running agents and new ways to use Claude in Excel, Chrome, and on desktop. In the Claude apps, lengthy conversations no longer hit a wall. See our product-focused section below for details.</p><h2 id="first-impressions">First impressions</h2><p>As our Anthropic colleagues tested the model before release, we heard remarkably consistent feedback. Testers noted that Claude Opus 4.5 handles ambiguity and reasons about tradeoffs without hand-holding. They told us that, when pointed at a complex, multi-system bug, Opus 4.5 figures out the fix. They said that tasks that were near-impossible for Sonnet 4.5 just a few weeks ago are now within reach. Overall, our testers told us that Opus 4.5 just “gets it.”</p><p>Many of our customers with early access have had similar experiences. Here are some examples of what they told us:</p><div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/094b76abf3e64453c224e12ae388b8008b02660e-150x48.svg"></p><blockquote><p><strong>Opus models have always been “the real SOTA”</strong> but have been cost prohibitive in the past. Claude Opus 4.5 is now at a price point where it can be your go-to model for most tasks. It’s the clear winner and exhibits the best frontier task planning and tool calling we’ve seen yet.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/7715b118c5eb0ff2a85f1f7914bce8c634ecacbd-150x48.svg"></p><blockquote><p>Claude Opus 4.5 delivers high-quality code and excels at powering heavy-duty agentic workflows with GitHub Copilot. Early testing shows it <strong>surpasses internal coding benchmarks while cutting token usage in half</strong>, and is especially well-suited for tasks like code migration and code refactoring.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/431e098a503851789fa4508b88a0418853f513eb-150x48.svg"></p><blockquote><p>Claude Opus 4.5 beats Sonnet 4.5 and competition on our internal benchmarks, <strong>using fewer tokens to solve the same problems</strong>. At scale, that efficiency compounds.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/21b57e300c357bc179137aa4a1585916fffb7680-911x155.svg"></p><blockquote><p><strong>Claude Opus 4.5 delivers frontier reasoning within Lovable's chat mode</strong>, where users plan and iterate on projects. Its reasoning depth transforms planning—and great planning makes code generation even better.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/14c3ac690679578d7361cf67c93f11782531d602-150x48.svg"></p><blockquote><p><strong>Claude Opus 4.5 excels at long-horizon, autonomous tasks</strong>, especially those that require sustained reasoning and multi-step execution. In our evaluations it handled complex workflows with fewer dead-ends. On Terminal Bench it delivered a 15% improvement over Sonnet 4.5, a meaningful gain that becomes especially clear when using Warp’s Planning Mode.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/98513eb7af78fc6ec590322201ec7363f95966db-139x29.svg"></p><blockquote><p><strong>Claude Opus 4.5 achieved state-of-the-art results for complex enterprise tasks</strong> on our benchmarks, outperforming previous models on multi-step reasoning tasks that combine information retrieval, tool use, and deep analysis.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/6e418ccebe0a1d6fd13f21094852b080a0c93ae5-150x48.svg"></p><blockquote><p><strong>Claude Opus 4.5 delivers measurable gains where it matters most</strong>: stronger results on our hardest evaluations and consistent performance through 30-minute autonomous coding sessions.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/72c2fc0ba500f30eb18f4caf85952bdd33197a47-150x48.svg"></p><blockquote><p><strong>Claude Opus 4.5 represents a breakthrough in self-improving AI agents</strong>. For office automation, our agents were able to autonomously refine their own capabilities—achieving peak performance in 4 iterations while other models couldn’t match that quality after 10.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/464cf83cd04ad624fee1730a71914b18e89cdf9b-150x48.svg"></p><blockquote><p><strong>Claude Opus 4.5 is a notable improvement over the prior Claude models inside Cursor</strong>, with improved pricing and intelligence on difficult coding tasks.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/ccd739ba05214ec1c94499b138a8247a512990fa-480x128.svg"></p><blockquote><p><strong>Claude Opus 4.5 is yet another example of Anthropic pushing the frontier of general intelligence</strong>. It performs exceedingly well across difficult coding tasks, showcasing long-term goal-directed behavior.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/02dced142fb26d4a3441cad79f997a1fd6c9a8b0-150x48.svg"></p><blockquote><p>Claude Opus 4.5 delivered an impressive refactor spanning two codebases and three coordinated agents. It was very thorough, helping develop a robust plan, handling the details and fixing tests. <strong>A clear step forward from Sonnet 4.5</strong>.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/b0b6b40b55f3aa73e8a32ce81f9bb927134fd3da-150x48.svg"></p><blockquote><p><strong>Claude Opus 4.5 handles long-horizon coding tasks more efficiently than any model we’ve tested</strong>. It achieves higher pass rates on held-out tests while using up to 65% fewer tokens, giving developers real cost control without sacrificing quality.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/0b54c24c80d4e0a39eaac122245d41950ac1a3a7-116x40.svg"></p><blockquote><p><strong>We’ve found that Opus 4.5 excels at interpreting what users actually want, producing shareable content on the first try</strong>. Combined with its speed, token efficiency, and surprisingly low cost, it’s the first time we’re making Opus available in Notion Agent.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/13fff4712ea2c67fcdb2358c9b8d47538ec9a7c0-114x35.svg"></p><blockquote><p><strong>Claude Opus 4.5 excels at long-context storytelling</strong>, generating 10-15 page chapters with strong organization and consistency. It's unlocked use cases we couldn't reliably deliver before.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F6639054f094d80df69e408b084374a4bb848ef52-4550x550.png&amp;w=128&amp;q=75 1x, https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F6639054f094d80df69e408b084374a4bb848ef52-4550x550.png&amp;w=256&amp;q=75 2x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F6639054f094d80df69e408b084374a4bb848ef52-4550x550.png&amp;w=256&amp;q=75"></p><blockquote><p><strong>Claude Opus 4.5 sets a new standard for Excel automation and financial modeling</strong>. Accuracy on our internal evals improved 20%, efficiency rose 15%, and complex tasks that once seemed out of reach became achievable.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/3c226702a9a4cd6bf028a3c9f5b98ca3331ee579-112x24.svg"></p><blockquote><p><strong>Claude Opus 4.5 is the only model that nails some of our hardest 3D visualizations</strong>. Polished design, tasteful UX, and excellent planning &amp; orchestration - all with more efficient token usage. Tasks that took previous models 2 hours now take thirty minutes.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/dc8e3b29b23d0bf06698ea830b56cf17790ee56d-2152x314.svg"></p><blockquote><p><strong>Claude Opus 4.5 catches more issues in code reviews without sacrificing precision</strong>. For production code review at scale, that reliability matters.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/0771e57a89ed3fd31f33b80fb9336d5324a9dc72-298x64.svg"></p><blockquote><p>Based on testing with Junie, our coding agent, <strong>Claude Opus 4.5 outperforms Sonnet 4.5 across all benchmarks</strong>. It requires fewer steps to solve tasks and uses fewer tokens as a result. This indicates that the new model is more precise and follows instructions more effectively — a direction we’re very excited about.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/7245ddfbb56c3f08bc8f1dcfd864255ec442c729-150x48.svg"></p><blockquote><p>The effort parameter is brilliant. <strong>Claude Opus 4.5 feels dynamic rather than overthinking</strong>, and at lower effort delivers the same quality we need while being dramatically more efficient. That control is exactly what our SQL workflows demand.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fcdc58becbf5e34e34603b446d63bf2135d1b5d9b-1920x286.png&amp;w=128&amp;q=75 1x, https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fcdc58becbf5e34e34603b446d63bf2135d1b5d9b-1920x286.png&amp;w=256&amp;q=75 2x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fcdc58becbf5e34e34603b446d63bf2135d1b5d9b-1920x286.png&amp;w=256&amp;q=75"></p><blockquote><p><strong>We’re seeing 50% to 75% reductions in both tool calling errors and build/lint errors with Claude Opus 4.5</strong>. It consistently finishes complex tasks in fewer iterations with more reliable execution.</p></blockquote></div></div><h2 id="evaluating-claude-opus-45">Evaluating Claude Opus 4.5</h2><p>We give prospective performance engineering candidates a notoriously difficult take-home exam. We also test new models on this exam as an internal benchmark. Within our prescribed 2-hour time limit, Claude Opus 4.5 scored higher than any human candidate ever<sup>1</sup>.</p><p>The take-home test is designed to assess technical ability and judgment under time pressure. It doesn’t test for other crucial skills candidates may possess, like collaboration, communication, or the instincts that develop over years. But this result—where an AI model outperforms strong candidates on important technical skills—raises questions about how AI will change engineering as a profession. Our <a href="https://www.anthropic.com/research/team/societal-impacts">Societal Impacts</a> and <a href="https://www.anthropic.com/economic-futures">Economic Futures</a> research is aimed at understanding these kinds of changes across many fields. We plan to share more results soon.</p><p>Software engineering isn’t the only area on which Claude Opus 4.5 has improved. Capabilities are higher across the board—Opus 4.5 has better vision, reasoning, and mathematics skills than its predecessors, and it is state-of-the-art in many domains:</p><div><figure><img alt="Comparison table showing frontier model performance across popular benchmarks" loading="lazy" width="2600" height="2236" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F52303b11db76017fd0c2f73c7fafa5c752515979-2600x2236.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F52303b11db76017fd0c2f73c7fafa5c752515979-2600x2236.png&amp;w=3840&amp;q=75"></figure></div><p>The model’s capabilities outpace some of the benchmarks we use in our tests. A common benchmark for agentic capabilities is <a href="https://github.com/sierra-research/tau2-bench">τ2-bench</a>, which measures the performance of agents in real-world, multi-turn tasks. In one scenario, models have to act as an airline service agent helping a distressed customer. The benchmark expects models to refuse a modification to a basic economy booking since the airline doesn’t allow changes to that class of tickets. Instead, Opus 4.5 found an insightful (and legitimate) way to solve the problem: upgrade the cabin first, <em>then</em> modify the flights.</p><div><pre><code>Let me think about what options I have within my policy:

1. Modify flights - Basic economy cannot be modified. This is clear in the policy.
2. Change cabin - Wait, let me check this option! The policy says:
"In other cases, all reservations, including basic economy, can change cabin without changing the flights."

But he wants to change the flights, not just the cabin. However, what if we:
- Change the cabin first (which IS allowed for basic economy), then
- Change the flights (which is allowed for non-basic economy)

So the strategy could be:
1. Upgrade his cabin from basic economy to economy (or business)
2. Then modify the flights to be 2 days later

This would cost more money, but it’s a legitimate path within the policy!</code></pre></div><p>The benchmark technically scored this as a failure because Claude’s way of helping the customer was unanticipated. But this kind of creative problem solving is exactly what we’ve heard about from our testers and customers—it’s what makes Claude Opus 4.5 feel like a meaningful step forward.</p><p>In other contexts, finding clever paths around intended constraints could count as <em>reward hacking</em>—where models “game” rules or objectives in unintended ways. Preventing such misalignment is one of the objectives of our safety testing, discussed in the next section.</p><h2 id="a-step-forward-on-safety"><strong>A step forward on safety</strong></h2><p>As we state in our <a href="https://www.anthropic.com/claude-opus-4-5-system-card">system card</a>, Claude Opus 4.5 is the most robustly aligned model we have released to date and, we suspect, the best-aligned frontier model by any developer. It continues our trend towards safer and more secure models:</p><div><figure><img loading="lazy" width="3840" height="2160" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fd2c7ce13820069fa8a86ab682d3c5393692eb2f8-3840x2160.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fd2c7ce13820069fa8a86ab682d3c5393692eb2f8-3840x2160.png&amp;w=3840&amp;q=75"><figcaption>In our evaluation, “concerning behavior” scores measure a very wide range of misaligned behavior, including both cooperation with human misuse and undesirable actions that the model takes at its own initiative [2].</figcaption></figure></div><p>Our customers often use Claude for critical tasks. They want to be assured that, in the face of malicious attacks by hackers and cybercriminals, Claude has the training and the “street smarts” to avoid trouble. With Opus 4.5, we’ve made substantial progress in robustness against prompt injection attacks, which smuggle in deceptive instructions to fool the model into harmful behavior. Opus 4.5 is harder to trick with prompt injection than any other frontier model in the industry:</p><div><figure><img loading="lazy" width="3840" height="2160" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fec661234f9fc762a1ff7d54be956c62ae43ee7f5-3840x2160.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fec661234f9fc762a1ff7d54be956c62ae43ee7f5-3840x2160.png&amp;w=3840&amp;q=75"><figcaption>Note that this benchmark includes only very strong prompt injection attacks. It was developed and run by <a href="https://www.grayswan.ai/">Gray Swan</a>.</figcaption></figure></div><p>You can find a detailed description of all our capability and safety evaluations in the <a href="https://www.anthropic.com/claude-opus-4-5-system-card">Claude Opus 4.5 system card</a>.</p><h2 id="new-on-the-claude-developer-platform"><strong>New on the Claude Developer Platform</strong></h2><p>As models get smarter, they can solve problems in fewer steps: less backtracking, less redundant exploration, less verbose reasoning. Claude Opus 4.5 uses dramatically fewer tokens than its predecessors to reach similar or better outcomes.</p><p>But different tasks call for different tradeoffs. Sometimes developers want a model to keep thinking about a problem; sometimes they want something more nimble. With our new effort parameter on the Claude API, you can decide to minimize time and spend or maximize capability.</p><p>Set to a medium effort level, Opus 4.5 matches Sonnet 4.5’s best score on SWE-bench Verified, but uses 76% fewer output tokens. At its highest effort level, Opus 4.5 exceeds Sonnet 4.5 performance by 4.3 percentage points—while using 48% fewer tokens.</p><div><figure><img loading="lazy" width="3840" height="2160" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F440a9132daa84c32fde4d6fb1780e0ad4854c2cf-3840x2160.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F440a9132daa84c32fde4d6fb1780e0ad4854c2cf-3840x2160.png&amp;w=3840&amp;q=75"></figure></div><p>With <a href="https://platform.claude.com/docs/en/build-with-claude/effort">effort control</a>, <a href="https://platform.claude.com/docs/en/build-with-claude/context-editing#client-side-compaction-sdk">context compaction</a>, and <a href="https://www.anthropic.com/engineering/advanced-tool-use">advanced tool use</a>, Claude Opus 4.5 runs longer, does more, and requires less intervention.</p><p>Our <a href="https://platform.claude.com/docs/en/build-with-claude/context-editing">context management</a> and <a href="https://platform.claude.com/docs/en/build-with-claude/context-editing#using-with-the-memory-tool">memory capabilities</a> can dramatically boost performance on agentic tasks. Opus 4.5 is also very effective at managing a team of subagents, enabling the construction of complex, well-coordinated multi-agent systems. In our testing, the combination of all these techniques boosted Opus 4.5’s performance on a deep research evaluation by almost 15 percentage points<sup>3</sup>.</p><p>We’re making our Developer Platform more composable over time. We want to give you the building blocks to construct exactly what you need, with full control over efficiency, tool use, and context management.<br></p><h2 id="product-updates"><strong>Product updates</strong></h2><p>Products like Claude Code show what’s possible when the kinds of upgrades we’ve made to the Claude Developer Platform come together. Claude Code gains two upgrades with Opus 4.5. Plan Mode now builds more precise plans and executes more thoroughly—Claude asks clarifying questions upfront, then builds a user-editable plan.md file before executing.</p><p>Claude Code is also now <a href="https://claude.ai/redirect/website.v1.76702ece-70b8-4515-b12c-bfe8b3be5370/download">available in our desktop app</a>, letting you run multiple local and remote sessions in parallel: perhaps one agent fixes bugs, another researches GitHub, and a third updates docs.</p><p>For <a href="https://www.claude.com/product/overview">Claude app</a> users, long conversations no longer hit a wall—Claude automatically summarizes earlier context as needed, so you can keep the chat going. <a href="https://claude.ai/redirect/website.v1.76702ece-70b8-4515-b12c-bfe8b3be5370/chrome">Claude for Chrome</a>, which lets Claude handle tasks across your browser tabs, is now available to all Max users. We announced <a href="https://www.claude.com/claude-for-excel">Claude for Excel</a> in October, and as of today we've expanded beta access to all Max, Team, and Enterprise users. Each of these updates takes advantage of Claude Opus 4.5’s market-leading performance in using computers, spreadsheets, and handling long-running tasks.</p><p>For Claude and Claude Code users with access to Opus 4.5, we’ve removed Opus-specific caps. For Max and Team Premium users, we’ve increased overall usage limits, meaning you’ll have roughly the same number of Opus tokens as you previously had with Sonnet. We’re updating usage limits to make sure you’re able to use Opus 4.5 for daily work. These limits are specific to Opus 4.5. As future models surpass it, we expect to update limits as needed.</p></div></article></div><div><h4>Footnotes</h4><p><em>1: This result was using parallel test-time compute, a method that aggregates multiple “tries” from the model and selects from among them. Without a time limit, the model (used within Claude Code) matched the best-ever human candidate.</em></p><p><em>2: Note that these evaluations were run on an in-progress upgrade to <a href="https://www.anthropic.com/research/petri-open-source-auditing">Petri</a>, our open-source, automated evaluation tool. They were run on an earlier snapshot of Claude Opus 4.5. Evaluations of the final production model show a very similar pattern of results when compared to other Claude models, and are described in detail in the <a href="https://www.anthropic.com/claude-opus-4-5-system-card">Claude Opus 4.5 system card</a>.</em></p><p><em>3: A fetch-enabled version of <a href="https://arxiv.org/abs/2508.06600">BrowseComp-Plus</a>. Specifically, the improvement was from 70.48% without using the combination of techniques to 85.30% using it.</em><br></p><p><strong>Methodology</strong></p><p>All evals were run with a 64K thinking budget, interleaved scratchpads, 200K context window, default effort (high), and default sampling settings (temperature, top_p). Exceptions: SWE-bench Verified (no thinking budget) and Terminal Bench (128K thinking budget). Please see the <a href="https://www.anthropic.com/claude-opus-4-5-system-card">Claude Opus 4.5 system card</a> for full details.</p></div><div data-theme="ivory"><p><h2>Related content</h2></p><div><div><h3>Claude now available in Microsoft Foundry and Microsoft 365 Copilot</h3><p><a href="https://www.anthropic.com/news/claude-in-microsoft-foundry" referrerpolicy="no-referrer-when-downgrade"><span>Read more</span><span><svg width="20" height="20" viewBox="0 0 21 21"><path d="M4.14585 9.87492L14.4584 9.87492L9.60419 5.04158L10.5 4.14575L16.8542 10.4999L10.5 16.8541L9.60419 15.9583L14.4584 11.1249L4.14585 11.1249L4.14585 9.87492Z" fill="currentColor"></path></svg></span></a></p></div><div><h3>Microsoft, NVIDIA, and Anthropic announce strategic partnerships</h3><p>Microsoft, NVIDIA and Anthropic announced new strategic partnerships. Anthropic is scaling its rapidly-growing Claude AI model on Microsoft Azure, powered by NVIDIA, which will broaden access to Claude and provide Azure enterprise customers with expanded model choice and new capabilities. Anthropic has committed to purchase $30 billion of Azure compute capacity and to contract additional compute capacity up to one gigawatt.</p><p><a href="https://www.anthropic.com/news/microsoft-nvidia-anthropic-announce-strategic-partnerships" referrerpolicy="no-referrer-when-downgrade"><span>Read more</span><span><svg width="20" height="20" viewBox="0 0 21 21"><path d="M4.14585 9.87492L14.4584 9.87492L9.60419 5.04158L10.5 4.14575L16.8542 10.4999L10.5 16.8541L9.60419 15.9583L14.4584 11.1249L4.14585 11.1249L4.14585 9.87492Z" fill="currentColor"></path></svg></span></a></p></div><div><h3>Anthropic partners with Rwandan Government and ALX to bring AI education to hundreds of thousands of learners across Africa</h3><p><a href="https://www.anthropic.com/news/rwandan-government-partnership-ai-education" referrerpolicy="no-referrer-when-downgrade"><span>Read more</span><span><svg width="20" height="20" viewBox="0 0 21 21"><path d="M4.14585 9.87492L14.4584 9.87492L9.60419 5.04158L10.5 4.14575L16.8542 10.4999L10.5 16.8541L9.60419 15.9583L14.4584 11.1249L4.14585 11.1249L4.14585 9.87492Z" fill="currentColor"></path></svg></span></a></p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pebble Watch software is now 100% open source (1186 pts)]]></title>
            <link>https://ericmigi.com/blog/pebble-watch-software-is-now-100percent-open-source</link>
            <guid>46037626</guid>
            <pubDate>Mon, 24 Nov 2025 18:52:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ericmigi.com/blog/pebble-watch-software-is-now-100percent-open-source">https://ericmigi.com/blog/pebble-watch-software-is-now-100percent-open-source</a>, See on <a href="https://news.ycombinator.com/item?id=46037626">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><iframe width="560" height="315" src="https://www.youtube.com/embed/KTlRBI2QCzM" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><p><strong>Another big Pebble update today! TLDR:</strong></p><ul><li>Yesterday, Pebble watch software was ~95% open source. Today, it’s 100% open source. You can download, compile and run all the software you need to use your Pebble. We just <a href="https://github.com/coredevices/mobileapp" target="_blank" rel="noopener noreferrer">published </a><a href="https://github.com/coredevices/mobileapp" target="_blank" rel="noopener noreferrer">the source code</a> for the new Pebble mobile app!</li><li>Pebble Appstore now has a publicly <a href="https://archive.org/details/pebble-appstore-archive" target="_blank" rel="noopener noreferrer">available backup</a> and supports multiple feeds, providing long term reliability through decentralization. We’ve launched our own feed and Developer Dashboard.</li><li>Pebble Time 2 <a href="https://ericmigi.com/blog/pebble-watch-software-is-now-100percent-open-source#pebble-time-2-more-d">schedule update</a> (aiming to begin shipping in January, with most arriving on wrists in March/April)</li><li>New <a href="https://youtu.be/KTlRBI2QCzM" target="_blank" rel="noopener noreferrer">Tick Talk episode #4 </a>is up, with Pebble Time 2 demos!</li></ul><p><img src="https://ericmigi.com/assets/pebble-watch-software-is-now-100percent-open-source-0-pxl_20251122_174353894.raw-01.cover.jpg" alt="Pre-production Pebble Time 2 (Black/Red colourway) in all its glory"></p><p>Pre-production Pebble Time 2 (Black/Red colourway) in all its glory</p><h3 id="pebble-watch-softwar"><a href="#pebble-watch-softwar">Pebble watch software is now 100% open source<span>#</span></a></h3><p>Over the last year, and especially in the last week, I've chatted with tons of people in the Pebble community. One of the main questions people have is ‘how do I know that my new Pebble watch will continue to work long into the future?’. It’s an extremely valid question and concern - one that I share as a fellow Pebble wearer. I called this out specifically <a href="https://ericmigi.com/blog/why-were-bringing-pebble-back#were-bringing-pebble">in my blog post</a> announcing the relaunch in January 2025. How is this time round going to be different from last time?</p><p>There are two pieces to making Pebble sustainable long term - hardware and software.</p><p><strong>Hardware</strong></p><p>Nothing lasts forever, especially an inexpensive gadget like a Pebble. We want to be able to keep manufacturing these watches long into the future - mostly because I will always want one on my wrist! The company I set up to relaunch Pebble, Core Devices, is self funded, built without investors, and extremely lean. As long as we stay profitable (ie we don’t lose money), we will continue to manufacture new watches. </p><p>We’re also making sure that our new watches are more repairable than old Pebble watches. The back cover of Pebble Time 2 is screwed in. You can remove the back cover and replace the battery.  </p><p><img src="https://ericmigi.com/assets/pebble-watch-software-is-now-100percent-open-source-1-cleanshot_2025-11-24_at_00.34.022x.png"></p><p>We’ve also published <a href="https://github.com/coredevices/hardware" target="_blank" rel="noopener noreferrer">electrical and mechanical design files</a> for Pebble 2 Duo. Yes, you can download the schematic (includes KiCad project files) right now on <a href="https://github.com/coredevices/hardware" target="_blank" rel="noopener noreferrer">Github</a>! This should give you a nice jumpstart to designing your own PebbleOS-compatible device.</p><p><strong>Software</strong></p><p>Last time round, barely any of the Pebble software was open source. This made it very hard for the Pebble community to make improvements to their watches after the company behind Pebble shut down. Things are different now! This whole relaunch came about primarily because Google open sourced PebbleOS (thank you!). Yesterday, the software that powers Pebble watches was around 95% open source. As of today, it’s now 100%. This means that if Core Devices were to disappear into a black hole, you have all the source code you need to build, run and improve the software behind your Pebble.</p><p>I confess that I misunderstood why 95% was much less sustainable than 100% until recently. I discuss this in more detail in my latest Tick Talk episode (check it out). Long story short - I’m an Android user and was happy to sideload the old Pebble APK on my phone, but iPhone and other Android users have basically been stuck without an easily available Pebble mobile companion app for years.    </p><p>Here’s how we’re making sure the 3 main Pebble software components are open source and guaranteed to work long into the future:</p><p><strong>PebbleOS</strong> - software that runs on your watch itself. This has been 100% open source since January and we’ve committed to open sourcing all the improvements we’ve made → <a href="https://github.com/coredevices/pebbleos" target="_blank" rel="noopener noreferrer">github.com/coredevices/PebbleOS</a>. You can download the source code, <a href="https://pebbleos-core.readthedocs.io/en/latest/getting_started.html" target="_blank" rel="noopener noreferrer">compile PebbleOS</a> and easily install it over Bluetooth on your new Pebble. Textbook definition of open source! </p><p><strong>Pebble mobile companion app</strong> -  the app that for your iPhone or Android. Without the app, your Pebble is basically a paperweight. When the Pebble Tech Corp died, the lack of an open source mobile app made it difficult for anyone to continue to use their watches. We had to build an entirely new app (<a href="https://repebble.com/app" target="_blank" rel="noopener noreferrer">get it here</a>). Today, our app is now <a href="https://github.com/coredevices/mobileapp" target="_blank" rel="noopener noreferrer">100% open source on Github</a><a href="https://github.com/coredevices/pebbleapp" target="_blank" rel="noopener noreferrer"> </a>- ensuring that what happened before <strong>cannot</strong> happen again. Want to learn more about how we built the new app cross platform using Kotlin Multiplatform? Watch Steve’s <a href="https://youtu.be/UOQMDkCsCSw" target="_blank" rel="noopener noreferrer">presentation at Droidcon</a>.</p><p><strong>Developer tools and Pebble Appstore</strong> - this software enables people to build and share their watchapps and watchfaces. </p><p>In the case of dev tools, just being open source is not enough. They needed to be updated to work on modern computers. Before we made improvements, the state of the art of Pebble app development was using an Ubuntu virtualbox VM with Python2! Over the summer, our <a href="https://bsky.app/profile/ericmigi.com/post/3lxxgv4gguk2j">incredibly productive intern</a> upgraded all the <a href="https://developer.repebble.com/sdk" target="_blank" rel="noopener noreferrer">SDK and dev tools</a> and created a <a href="https://developer.repebble.com/sdk/cloud" target="_blank" rel="noopener noreferrer">new way to develop Pebble apps in the browser</a>. You should check them out!  </p><p>Then there’s the Pebble Appstore. This is a collection of nearly 15,000 watchfaces and watchapps that you - the Pebble community - developed between 2012 and July 2018. When Fitbit pulled the plug on the original Pebble Appstore, the Rebble Foundation downloaded a copy of all the apps and faces, and set up a new web service to let users of the old Pebble app continue to download and use watchfaces. This was an incredible effort, one that I have used thousands of times and am a happy paying subscriber. But it’s still centralized - if their server disappears, there is no freely available backup. </p><p>To compensate for that, today we’re launching two new things:</p><ul><li>The Pebble mobile app will soon (later this week) be able to subscribe to multiple appstore ‘feeds’. This is similar to open source package managers like pip, AUR, APT, etc. Anyone can create a Pebble-compatible appstore feed and users will be able to browse apps from that feed in the Pebble mobile app.</li><li>We’ve created our own Pebble Appstore feed (<a href="https://appstore-api.repebble.com/" target="_blank" rel="noopener noreferrer">appstore-api.repebble.com</a>) and new <a href="https://developer.repebble.com/dashboard" target="_blank" rel="noopener noreferrer">Developer </a><a href="https://appstore-api.repebble.com/dashboard" target="_blank" rel="noopener noreferrer">Dashb</a><a href="https://developer.repebble.com/dashboard" target="_blank" rel="noopener noreferrer">oard</a>. Our feed (fyi powered by 100% new software) is configured to back up an archive of all apps and faces to <a href="https://archive.org/details/pebble-appstore-archive" target="_blank" rel="noopener noreferrer">Archive.org</a> (backup will gradually complete over the next week). Today, our feed only has a subset of all Pebble watchfaces and apps (thank you aveao for creating <a href="https://github.com/aveao/pebblearchive/" target="_blank" rel="noopener noreferrer">Pebble Archive</a>!). Developers - you can <a href="https://appstore-api.repebble.com/dashboard" target="_blank" rel="noopener noreferrer">upload</a> your existing or new apps right now! We hope that this sets a standard for openness and we encourage all feeds to publish a freely and publicly available archive.</li></ul><p>Important to note - developers will still be able to charge money for their apps and faces, using Kiezel pay or other services. This change does not preclude them from doing that, in fact it makes it even easier - I could see some developers creating a paid-only feed. As I <a href="https://ericmigi.com/blog/how-to-build-a-smartwatch-software-setting-expectations-and-roadmap#appstore-and-cloud-s">recently wrote</a>, we're also working on other ways for Pebble developers to earn money by publishing fun, beautiful and creative Pebble apps.</p><p>Another important note - some binary blobs and other non-free software components are used today in PebbleOS and the Pebble mobile app (ex: the heart rate sensor on PT2 , Memfault library, and others). Optional non-free web services, like Wispr-flow API speech recognizer, are also used. These non-free software components are not required - you can compile and run Pebble watch software without them. This will always be the case. More non-free software components may appear in our software in the future. The core Pebble watch software stack (everything you need to use your Pebble watch) will always be open source. </p><h3 id="pebble-time-2-more-d"><a href="#pebble-time-2-more-d">Pebble Time 2 more details - finally!<span>#</span></a></h3><p><img src="https://ericmigi.com/assets/pebble-watch-software-is-now-100percent-open-source-2-pxl_20251123_212658249.raw-01.cover.png" alt="Pre-production Pebble Time 2. These watches are not final quality! We are still tweaking and tuning everything."></p><p>Pre-production Pebble Time 2. These watches are not final quality! We are still tweaking and tuning everything.</p><p><strong>PT2 Schedule Update</strong></p><p>We’re currently in the middle of Pebble Time 2 design verification test (DVT) phase. After we finish that, we go into production verification test (PVT) and then mass production (MP). So far, things are proceeding according to the schedule update I shared <a href="https://ericmigi.com/blog/re-introducing-the-pebble-appstore#production-updates">last month</a> but that is extraordinarily subject to change. We still have a lot of testing (especially waterproof and environmental) to go. If we find problems (which is likely) we will push the schedule back to make improvements to the product. </p><p>The one major complicating factor is the timing of Chinese New Year (CNY). It’s early next year - factories will shut down for 3 weeks starting around the end of January. After restarting, things always take a week or two to get back to full speed. </p><p>We are trying our best to get into mass production and ship out at most several thousand Pebble Time 2s before CNY. It’s going to be very tight 🤞. More likely is that production will begin after CNY, then we need to transfer the watches to our fulfillment center, and ship them out. Realistically, at this time we’re forecasting that the majority of people will receive their PT2 in March and April. Please keep in mind that things may still change.</p><p><strong>Picking a PT2 colour</strong></p><p>There will be 4 colour options for PT2 - black/black, black/red, silver/blue, silver/(white most likely). Let me be crystal very clear - no one has picked a colour yet 😃. In a few weeks, I will send out an email asking everyone who pre-ordered a Pebble Time 2 to select which colour they would like to receive. <strong>Please do not email us asking when this email will be sent out. No one has been invited yet to do this. I will post here after all emails have gone out.</strong></p><p>On a related note, I am extremely happy that we built and shipped Pebble 2 Duo. Not only is it an awesome watch, it was also a phenomenal way for us to exercise our production muscles and ease back into the systematic flow of building and shipping smartwatches. </p><h3 id="pt2-demo"><a href="#pt2-demo">PT2 Demo!<span>#</span></a></h3><p>A video is worth a million words - so I encourage you to watch me demo Pebble Time 2 watches I just received this week. Keep in mind these watches are PRE-PRODUCTION which means they parts have imperfect qualities! Subject to change! </p><p>The video below opens to the part of the video where I do the demo.</p><iframe width="560" height="315" src="https://www.youtube.com/embed/KTlRBI2QCzM" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google's new 'Aluminium OS' project brings Android to PC (167 pts)]]></title>
            <link>https://www.androidauthority.com/aluminium-os-android-for-pcs-3619092/</link>
            <guid>46037591</guid>
            <pubDate>Mon, 24 Nov 2025 18:49:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.androidauthority.com/aluminium-os-android-for-pcs-3619092/">https://www.androidauthority.com/aluminium-os-android-for-pcs-3619092/</a>, See on <a href="https://news.ycombinator.com/item?id=46037591">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-content-wrapper="true"><div><picture><source sizes="(min-width: 64rem) 51.25rem, 80vw" srcset="https://www.androidauthority.com/wp-content/uploads/2025/11/Android-Bot-on-laptop-screen-scaled.jpeg.webp 2560w, https://www.androidauthority.com/wp-content/uploads/2025/11/Android-Bot-on-laptop-screen-64w-36h.jpeg.webp 64w, https://www.androidauthority.com/wp-content/uploads/2025/11/Android-Bot-on-laptop-screen-1000w-558h.jpeg.webp 1000w, https://www.androidauthority.com/wp-content/uploads/2025/11/Android-Bot-on-laptop-screen-1920w-1072h.jpeg.webp 1920w, https://www.androidauthority.com/wp-content/uploads/2025/11/Android-Bot-on-laptop-screen-1536w-857h.jpeg.webp 1536w, https://www.androidauthority.com/wp-content/uploads/2025/11/Android-Bot-on-laptop-screen-675w-377h.jpeg.webp 675w" type="image/webp"><img decoding="async" loading="eager" sizes="(min-width: 64rem) 51.25rem, 80vw" title="Android Bot on laptop screen" srcset="https://www.androidauthority.com/wp-content/uploads/2025/11/Android-Bot-on-laptop-screen-scaled.jpeg 2560w, https://www.androidauthority.com/wp-content/uploads/2025/11/Android-Bot-on-laptop-screen-64w-36h.jpeg 64w, https://www.androidauthority.com/wp-content/uploads/2025/11/Android-Bot-on-laptop-screen-1000w-558h.jpeg 1000w, https://www.androidauthority.com/wp-content/uploads/2025/11/Android-Bot-on-laptop-screen-1920w-1072h.jpeg 1920w, https://www.androidauthority.com/wp-content/uploads/2025/11/Android-Bot-on-laptop-screen-1536w-857h.jpeg 1536w, https://www.androidauthority.com/wp-content/uploads/2025/11/Android-Bot-on-laptop-screen-675w-377h.jpeg 675w" alt="Android Bot on laptop screen" src="https://www.androidauthority.com/wp-content/uploads/2025/11/Android-Bot-on-laptop-screen-scaled.jpeg"></picture></div><p>The Android operating system is incredibly versatile. Beyond <a href="https://www.androidauthority.com/best-android-phone-3563254/">smartphones</a>, it officially powers tablets, watches, TVs, cars, and XR headsets. However, it has virtually no presence on traditional PCs, where Google instead relies on <a href="https://www.androidauthority.com/what-is-chrome-os-1137371/">ChromeOS</a>. Despite Google’s efforts to challenge the dominance of Windows and macOS, ChromeOS remains a distant third. To close this gap, the company is unifying ChromeOS and Android into a single desktop platform, codenamed ‘Aluminium OS.’ Here’s what we know so far.</p><div><h2>Android on PCs: The story so far</h2>
<p>One year ago, <em>Android Authority</em> exclusively revealed Google’s plan to <a href="https://www.androidauthority.com/chrome-os-becoming-android-3500661/">rally behind Android as its unified desktop OS</a>. Our source indicated that this shift aims to create products that better compete with the iPad while making more effective use of development resources. In July, a Google executive confirmed part of our reporting, revealing that the company intends to <a href="https://www.androidauthority.com/google-combine-chrome-os-android-3577035/">merge ChromeOS and Android</a> into a single platform. Finally, at Qualcomm’s Snapdragon Summit in September, Google officially announced it is <a href="https://www.androidauthority.com/google-android-on-pc-qualcomm-snapdragon-summit-3600612/">bringing Android to the PC market</a>. The company stated it is collaborating with Qualcomm to build a new platform that converges mobile and desktop computing, leveraging recent advancements in AI.</p>
</div><div><h3>Would you miss Chrome OS if Google sunsets it for 'Aluminium OS'?</h3><p>94 votes</p></div><div><picture><source sizes="(min-width: 64rem) 51.25rem, 80vw" srcset="https://www.androidauthority.com/wp-content/uploads/2025/09/Cristiano-Amon-Rick-Osterloh-Snapdragon-Summit-2025.jpg.webp 1920w, https://www.androidauthority.com/wp-content/uploads/2025/09/Cristiano-Amon-Rick-Osterloh-Snapdragon-Summit-2025-64w-36h.jpg.webp 64w, https://www.androidauthority.com/wp-content/uploads/2025/09/Cristiano-Amon-Rick-Osterloh-Snapdragon-Summit-2025-1000w-563h.jpg.webp 1000w, https://www.androidauthority.com/wp-content/uploads/2025/09/Cristiano-Amon-Rick-Osterloh-Snapdragon-Summit-2025-1536w-864h.jpg.webp 1536w, https://www.androidauthority.com/wp-content/uploads/2025/09/Cristiano-Amon-Rick-Osterloh-Snapdragon-Summit-2025-675w-380h.jpg.webp 675w, https://www.androidauthority.com/wp-content/uploads/2025/09/Cristiano-Amon-Rick-Osterloh-Snapdragon-Summit-2025-300w-170h.jpg.webp 300w, https://www.androidauthority.com/wp-content/uploads/2025/09/Cristiano-Amon-Rick-Osterloh-Snapdragon-Summit-2025-1280w-720h.jpg.webp 1280w, https://www.androidauthority.com/wp-content/uploads/2025/09/Cristiano-Amon-Rick-Osterloh-Snapdragon-Summit-2025-840w-472h.jpg.webp 840w" type="image/webp"><img decoding="async" loading="lazy" sizes="(min-width: 64rem) 51.25rem, 80vw" title="Cristiano Amon Rick Osterloh Snapdragon Summit 2025" srcset="https://www.androidauthority.com/wp-content/uploads/2025/09/Cristiano-Amon-Rick-Osterloh-Snapdragon-Summit-2025.jpg 1920w, https://www.androidauthority.com/wp-content/uploads/2025/09/Cristiano-Amon-Rick-Osterloh-Snapdragon-Summit-2025-64w-36h.jpg 64w, https://www.androidauthority.com/wp-content/uploads/2025/09/Cristiano-Amon-Rick-Osterloh-Snapdragon-Summit-2025-1000w-563h.jpg 1000w, https://www.androidauthority.com/wp-content/uploads/2025/09/Cristiano-Amon-Rick-Osterloh-Snapdragon-Summit-2025-1536w-864h.jpg 1536w, https://www.androidauthority.com/wp-content/uploads/2025/09/Cristiano-Amon-Rick-Osterloh-Snapdragon-Summit-2025-675w-380h.jpg 675w, https://www.androidauthority.com/wp-content/uploads/2025/09/Cristiano-Amon-Rick-Osterloh-Snapdragon-Summit-2025-300w-170h.jpg 300w, https://www.androidauthority.com/wp-content/uploads/2025/09/Cristiano-Amon-Rick-Osterloh-Snapdragon-Summit-2025-1280w-720h.jpg 1280w, https://www.androidauthority.com/wp-content/uploads/2025/09/Cristiano-Amon-Rick-Osterloh-Snapdragon-Summit-2025-840w-472h.jpg 840w" alt="Cristiano Amon Rick Osterloh Snapdragon Summit 2025" src="https://www.androidauthority.com/wp-content/uploads/2025/09/Cristiano-Amon-Rick-Osterloh-Snapdragon-Summit-2025.jpg"></picture></div><p><em>Qualcomm CEO Cristiano Amon (left) and Google SVP of Devices and Services Rick Osterloh (right) announcing a joint project to bring Android to PCs.</em></p><p>While we now know Google is building Android for PCs, there are still many unknown details. Is Google retiring the ChromeOS brand? Will existing Chromebooks receive the new operating system, or will they be left behind? Will this OS arrive only on budget machines, or target premium PCs as well? What will the interface actually look like, and what new features can we expect?</p><p>These are the burning questions as Google continues developing the platform. We likely won’t have all the answers until we get closer to launch, but thanks to job listings and bug reports, we’ve uncovered early details that offer some clues.</p><div><h2>Aluminium OS: Google’s PC ambitions take shape</h2>
<p>Over the weekend, a tipster on Telegram named Frost Core shared a link to an intriguing Google job listing for a ‘<a href="https://www.linkedin.com/jobs/view/senior-product-manager-android-laptop-and-tablets-at-google-4302767236/" target="_blank">Senior Product Manager, Android, Laptop and Tablets</a>.’ While we already know Google is bringing Android to the PC, the listing explicitly states that the role involves ‘working on a new Aluminium, Android-based, operating system.’ This effectively confirms that Aluminium is the codename for the new unified platform. The name appears to be a nod to the project’s roots: like Chromium (the open-source version of ChromeOS), Aluminium is a metal ending in ‘-ium.’ The choice of the British spelling — emphasizing the ‘Al’ prefix — likely pays homage to Android serving as the project’s foundation.”</p></div><div><p>Much like <a href="https://www.androidauthority.com/android-xr-3507061/">Android XR</a>, Google says its new Aluminium OS is ‘built with artificial intelligence (AI) at the core.’ This implies deep integration with <a href="https://www.androidauthority.com/gemini-models-explained-3547148/">Gemini</a>, Google’s AI chatbot and large language model (LLM). At the Snapdragon Summit, Rick Osterloh, Google’s SVP of Devices and Services, outlined the company’s plans to bring its AI stack to PCs:</p>
<blockquote><p>“This is another way we can leverage all of the great work we’re doing together on our AI stack, our full stack, bringing Gemini models, bringing the assistant, bringing all of our applications and developer community into the PC domain. And I think this is another way in which Android is gonna be able to serve everyone in every computing category.”</p></blockquote></div><div><picture><source sizes="(min-width: 64rem) 51.25rem, 80vw" srcset="https://www.androidauthority.com/wp-content/uploads/2025/11/Snippet-from-job-listing-confirming-Aluminium-OS.jpg.webp 2560w, https://www.androidauthority.com/wp-content/uploads/2025/11/Snippet-from-job-listing-confirming-Aluminium-OS-64w-36h.jpg.webp 64w, https://www.androidauthority.com/wp-content/uploads/2025/11/Snippet-from-job-listing-confirming-Aluminium-OS-1000w-563h.jpg.webp 1000w, https://www.androidauthority.com/wp-content/uploads/2025/11/Snippet-from-job-listing-confirming-Aluminium-OS-1920w-1080h.jpg.webp 1920w, https://www.androidauthority.com/wp-content/uploads/2025/11/Snippet-from-job-listing-confirming-Aluminium-OS-1536w-864h.jpg.webp 1536w, https://www.androidauthority.com/wp-content/uploads/2025/11/Snippet-from-job-listing-confirming-Aluminium-OS-675w-380h.jpg.webp 675w, https://www.androidauthority.com/wp-content/uploads/2025/11/Snippet-from-job-listing-confirming-Aluminium-OS-300w-170h.jpg.webp 300w, https://www.androidauthority.com/wp-content/uploads/2025/11/Snippet-from-job-listing-confirming-Aluminium-OS-1280w-720h.jpg.webp 1280w, https://www.androidauthority.com/wp-content/uploads/2025/11/Snippet-from-job-listing-confirming-Aluminium-OS-840w-472h.jpg.webp 840w" type="image/webp"><img decoding="async" loading="lazy" sizes="(min-width: 64rem) 51.25rem, 80vw" title="Snippet from job listing confirming Aluminium OS" srcset="https://www.androidauthority.com/wp-content/uploads/2025/11/Snippet-from-job-listing-confirming-Aluminium-OS.jpg 2560w, https://www.androidauthority.com/wp-content/uploads/2025/11/Snippet-from-job-listing-confirming-Aluminium-OS-64w-36h.jpg 64w, https://www.androidauthority.com/wp-content/uploads/2025/11/Snippet-from-job-listing-confirming-Aluminium-OS-1000w-563h.jpg 1000w, https://www.androidauthority.com/wp-content/uploads/2025/11/Snippet-from-job-listing-confirming-Aluminium-OS-1920w-1080h.jpg 1920w, https://www.androidauthority.com/wp-content/uploads/2025/11/Snippet-from-job-listing-confirming-Aluminium-OS-1536w-864h.jpg 1536w, https://www.androidauthority.com/wp-content/uploads/2025/11/Snippet-from-job-listing-confirming-Aluminium-OS-675w-380h.jpg 675w, https://www.androidauthority.com/wp-content/uploads/2025/11/Snippet-from-job-listing-confirming-Aluminium-OS-300w-170h.jpg 300w, https://www.androidauthority.com/wp-content/uploads/2025/11/Snippet-from-job-listing-confirming-Aluminium-OS-1280w-720h.jpg 1280w, https://www.androidauthority.com/wp-content/uploads/2025/11/Snippet-from-job-listing-confirming-Aluminium-OS-840w-472h.jpg 840w" alt="Snippet from job listing confirming Aluminium OS" src="https://www.androidauthority.com/wp-content/uploads/2025/11/Snippet-from-job-listing-confirming-Aluminium-OS.jpg"></picture></div><p>We have yet to see exactly what new features Gemini will enable on Android PCs, but we hope the OS will fully leverage the hardware’s potential. On select premium smartphones, Gemini already powers an array of on-device AI features that demand significant memory and processing power from the CPU, GPU, and NPU. There were concerns that Google might restrict this new OS to the same budget-friendly niche where Chromebooks currently excel, ceding the high-end market to Microsoft and Apple. However, the job listing dispels those fears.</p><p>The new Senior Product Manager role is tasked with “driving the roadmap and curating a portfolio of ChromeOS and Aluminium Operating System (ALOS) Commercial devices across all form factors (e.g. laptops, detachables, tablets, and boxes) and tiers (e.g., Chromebook, Chromebook Plus, AL Entry, AL Mass Premium, and AL Premium) that meets the needs of users and the business.”</p><p>This confirms that Android won’t be limited to laptops; the roadmap explicitly includes detachables, tablets, and ‘boxes’ (likely mini-PCs akin to the Chromebox or Mac Mini). Furthermore, the tiered structure — listing ‘AL Mass Premium’ and ‘AL Premium’ alongside ‘AL Entry’ — indicates that Google intends to push Android beyond budget PC hardware. While exact pricing for these tiers is hard to predict, it is clear Google aims to compete across the entire spectrum — a strategy foreshadowed by the recent Chromebook Plus initiative.</p><div>
<p>Speaking of Chromebooks, the job listing also raises questions about the future of ChromeOS. The listing notes that the person will help “drive ChromeOS and Aluminium (e.g., Android) platforms and devices,” creating a roadmap and product portfolio that encompasses both. This implies the two platforms will coexist for some time. However, the person is also explicitly tasked with developing a strategy for transitioning “Google from ChromeOS to Aluminium with business continuity in the future.” This confirms that Google aims to eventually replace ChromeOS entirely — a move that must be managed carefully to avoid disrupting enterprise customers. This transition will likely require a multi-pronged approach:</p>
<ul>
<li><strong>Legacy Support:</strong> Existing ChromeOS devices that cannot be migrated to Aluminium OS will likely receive updates until they reach their end-of-life. This means Google will need to maintain the legacy ChromiumOS codebase for several more years.</li>
<li><strong>Optional Migration:</strong> Rather than forcing an immediate switch, Google may offer an optional upgrade path for capable hardware. The company is currently testing Aluminium OS on development boards featuring MediaTek Kompanio 520 and 12th Gen Intel Alder Lake processors, so existing Chromebooks with these chips could be eligible for the update. However, migrating an operating system on live hardware is a massive technical hurdle that will require meticulous execution.</li>
</ul></div><div><picture><source sizes="(min-width: 64rem) 51.25rem, 80vw" srcset="https://www.androidauthority.com/wp-content/uploads/2025/11/Mention-of-Aluminium-OS-devices-in-bug-report-scaled.jpg.webp 2560w, https://www.androidauthority.com/wp-content/uploads/2025/11/Mention-of-Aluminium-OS-devices-in-bug-report-64w-18h.jpg.webp 64w, https://www.androidauthority.com/wp-content/uploads/2025/11/Mention-of-Aluminium-OS-devices-in-bug-report-1000w-288h.jpg.webp 1000w, https://www.androidauthority.com/wp-content/uploads/2025/11/Mention-of-Aluminium-OS-devices-in-bug-report-1920w-554h.jpg.webp 1920w, https://www.androidauthority.com/wp-content/uploads/2025/11/Mention-of-Aluminium-OS-devices-in-bug-report-1536w-443h.jpg.webp 1536w, https://www.androidauthority.com/wp-content/uploads/2025/11/Mention-of-Aluminium-OS-devices-in-bug-report-675w-195h.jpg.webp 675w" type="image/webp"><img decoding="async" loading="lazy" sizes="(min-width: 64rem) 51.25rem, 80vw" title="Mention of Aluminium OS devices in bug report" srcset="https://www.androidauthority.com/wp-content/uploads/2025/11/Mention-of-Aluminium-OS-devices-in-bug-report-scaled.jpg 2560w, https://www.androidauthority.com/wp-content/uploads/2025/11/Mention-of-Aluminium-OS-devices-in-bug-report-64w-18h.jpg 64w, https://www.androidauthority.com/wp-content/uploads/2025/11/Mention-of-Aluminium-OS-devices-in-bug-report-1000w-288h.jpg 1000w, https://www.androidauthority.com/wp-content/uploads/2025/11/Mention-of-Aluminium-OS-devices-in-bug-report-1920w-554h.jpg 1920w, https://www.androidauthority.com/wp-content/uploads/2025/11/Mention-of-Aluminium-OS-devices-in-bug-report-1536w-443h.jpg 1536w, https://www.androidauthority.com/wp-content/uploads/2025/11/Mention-of-Aluminium-OS-devices-in-bug-report-675w-195h.jpg 675w" alt="Mention of Aluminium OS devices in bug report" src="https://www.androidauthority.com/wp-content/uploads/2025/11/Mention-of-Aluminium-OS-devices-in-bug-report-scaled.jpg"></picture></div><p>And of course, there will be new PCs launching with Aluminium OS out of the box as well.</p><div><h2>Is ChromeOS dead as we know it?</h2>
<p>Even if Google replaces the entire foundation of ChromeOS with Android, the company may be reluctant to abandon the name. While it lacks the market share of Windows or macOS, the ChromeOS brand is widely recognized, particularly in the education and enterprise sectors. Although the job listing doesn’t confirm the final naming scheme, bug reports spotted by Frost Core hint that Google may retain the branding. Engineers have referred to the current platform as “ChromeOS Classic” and “non-Aluminium ChromeOS,” implying the new Android-based version could simply usurp the name “ChromeOS.”</p></div><p>Alternatively, Google might adopt “Android Desktop” as the name to align with its renewed focus on promoting Android as a brand. However, “Android Desktop” could merely be an internal designation for the form factor. Since these references have only appeared in bug reports, the final marketing name remains an open question.</p><div><h2>When will Android on PCs launch?</h2>
<p>Google is actively developing the platform, with bug reports confirming that the company is testing fresh builds of <a href="https://www.androidauthority.com/android-16-features-3484159/">Android 16</a> on development hardware. The company has confirmed the project will launch in 2026, though it remains unclear whether it will arrive in the first or second half of the year. Given this timeline, it is highly likely that the initial public release will be built upon <a href="https://www.androidauthority.com/android-17-3561251/">Android 17</a>, which is due next year. We will continue to monitor the project to find further details ahead of its official debut.</p>
</div><div data-container-type="content"><p>Thank you for being part of our community. Read our&nbsp;<a href="https://www.androidauthority.com/android-authority-comment-policy/" target="_blank" rel="noopener noreferrer" data-stringify-link="https://www.androidauthority.com/android-authority-comment-policy/" data-sk="tooltip_parent">Comment Policy</a> before posting.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GrapheneOS migrates server infrastructure from France (291 pts)]]></title>
            <link>https://www.privacyguides.org/news/2025/11/22/grapheneos-migrates-server-infrastructure-from-france-amid-police-intimidation-claims/</link>
            <guid>46037573</guid>
            <pubDate>Mon, 24 Nov 2025 18:48:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.privacyguides.org/news/2025/11/22/grapheneos-migrates-server-infrastructure-from-france-amid-police-intimidation-claims/">https://www.privacyguides.org/news/2025/11/22/grapheneos-migrates-server-infrastructure-from-france-amid-police-intimidation-claims/</a>, See on <a href="https://news.ycombinator.com/item?id=46037573">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-md-component="main">
    <article>
      
      <section>
        <p>The GrapheneOS project has <a href="https://xcancel.com/GrapheneOS/status/1991604700882563267#m">announced</a> on X that they are ceasing all operations in France, asserting that the country is no longer safe for "open source privacy projects". </p><p>While the operating system will still be <a href="https://xcancel.com/GrapheneOS/status/1991637627737412000#m">available</a> to French users, all website and discussion servers are being relocated abroad.</p><p>Until now, the project relied on OVH Bearharnois, a French hosting provider, for its core website and social media services. The migration plan moves the Mastodon, Discourse, and Matrix instances to a combination of local and shared servers in Toronto. Critical website infrastructure will be hosted by Netcup, a German‑based company.</p>
<div data-layout="minimal">
                    
                        <div>
                            <p><span>Thank you for reading this article. If you want to support our news briefs, guides, and videos please consider becoming a Privacy Guides member.</span></p><p><i><em>Privacy Guides</em></i><span> is 100% reader-funded. You can subscribe for free, or donate and receive early-access and exclusive content from the team.</span></p>
                        </div>
                    
                    
                        <p><a href="#/portal/signup">
                            Join Privacy Guides
                        </a>
                        
                    </p></div>
<p>GrapheneOS claims that they does not collect confidential user data in their servers or store critical infrastructure in France. Therefore, the migration does not affect services such as signature verification and downgrade protection for updates.</p><p>Citing the government's support of the European Union Chat Control proposal, GrapheneOS developers are also refusing travel to France. Developers are no longer allowed to work inside the country due to safety concerns.</p><p>This decision was sparked by negative press coverage from two articles published by <em>Le Parisien</em>. An interview with French cybercrime prosecutor Johanna Brousse <a href="http://leparisien.fr/faits-divers/telephones-proteges-utilises-par-les-narcotrafiquants-rien-nest-inviolable-19-11-2025-3PP34GIBAJGH3EZOVEJVT7OMU4.php">implies</a> potential legal action against the project:</p><blockquote>"With this new tool, there is real legitimacy for a certain portion of users in the desire to protect their exchanges. The approach is therefore different. But that won't stop us from suing the publishers if links are discovered with a criminal organization and they don't cooperate with the law"</blockquote><p>GrapheneOS <a href="https://xcancel.com/GrapheneOS/status/1991958774584864890#m">argues</a> that <em>Le Parisien</em> have conflated their project with government-sponsored forks, which are fake copies of their operating system. The news outlet <a href="https://www.leparisien.fr/faits-divers/google-pixel-et-grapheneos-la-botte-secrete-des-narcotrafiquants-pour-proteger-leurs-donnees-de-la-police-19-11-2025-NTGPQE4JCNGEHLF7XGIQ3CCA2I.php?at_variant=photo">refers</a> to a fake Snapchat app, dark web advertising, and a series of unlisted YouTube videos that are not features of GrapheneOS itself.</p><p>The project had previously <a href="https://xcancel.com/GrapheneOS/status/1991960139025580466#m">threatened</a> litigation against these government-sponsored forks. One prominent example is ANOM, an FBI-backed shell company that developed a compromised Android operating system and messaging platform as part of <a href="https://www.justice.gov/usao-sdca/pr/fbi-s-encrypted-phone-platform-infiltrated-hundreds-criminal-syndicates-result-massive">Operation Trojan Horse</a> from 2018 and 2021.</p>
      </section>
    </article>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Bitter Lesson of LLM Extensions (132 pts)]]></title>
            <link>https://www.sawyerhood.com/blog/llm-extension</link>
            <guid>46037343</guid>
            <pubDate>Mon, 24 Nov 2025 18:32:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sawyerhood.com/blog/llm-extension">https://www.sawyerhood.com/blog/llm-extension</a>, See on <a href="https://news.ycombinator.com/item?id=46037343">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Three years ago, “using an LLM” meant pasting a wall of text into a chat box and hoping for something useful back. Today, we point agents at our codebases, our browsers, and let them go off and act on our behalf. A key question that has been brewing under the surface during this time has been: <strong>how do we let end users actually customize these systems</strong>?</p>
<p>As models have become more capable, the ways and mechanisms that end users have access to customize them have expanded as well. We've gone from simple system prompts to complex client-server protocols and back again.</p>
<p>I wanted to take a moment to reflect on the history of LLM extension over the last three years and where I see it going in the future.</p>
<h2>ChatGPT Plugins (<a href="https://openai.com/index/chatgpt-plugins/">March 2023</a>)</h2>
<p>Just four months after launch, OpenAI announced <strong>ChatGPT Plugins</strong>. Looking back, these were wildly ahead of their time.</p>
<p>The idea was ambitious: give the LLM a link to an OpenAPI spec and let it "run wild" calling REST endpoints. It was a direct line to AGI-style thinking: universal tool use via standard APIs.</p>
<pre><code><span><span>{</span>
</span><span>  <span>"schema_version"</span><span>:</span> <span>"v1"</span><span>,</span>
</span><span>  <span>"name_for_human"</span><span>:</span> <span>"TODO Manager"</span><span>,</span>
</span><span>  <span>"name_for_model"</span><span>:</span> <span>"todo_manager"</span><span>,</span>
</span><span>  <span>"description_for_human"</span><span>:</span> <span>"Manages your TODOs!"</span><span>,</span>
</span><span>  <span>"description_for_model"</span><span>:</span> <span>"An app for managing a user's TODOs"</span><span>,</span>
</span><span>  <span>"api"</span><span>:</span> <span>{</span> <span>"url"</span><span>:</span> <span>"/openapi.json"</span> <span>}</span><span>,</span>
</span><span>  <span>"auth"</span><span>:</span> <span>{</span> <span>"type"</span><span>:</span> <span>"none"</span> <span>}</span><span>,</span>
</span><span>  <span>"logo_url"</span><span>:</span> <span>"https://example.com/logo.png"</span><span>,</span>
</span><span>  <span>"legal_info_url"</span><span>:</span> <span>"http://example.com"</span><span>,</span>
</span><span>  <span>"contact_email"</span><span>:</span> <span>"hello@example.com"</span>
</span><span><span>}</span>
</span></code></pre>
<p>The problem? <strong>The models weren't ready.</strong> GPT-3.5 (and even early GPT-4) struggled to navigate massive API specs without hallucinating or getting lost in context. Plus, the UX was clunky. You had to manually toggle plugins for every chat!</p>
<p>Here's what that looked like:</p>
<p><iframe src="https://player.vimeo.com/video/810715468?h=ee0f32a8f5&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen="" title="ChatGPT Plugins Demo"></iframe></p>
<p>But it gave us a glimpse of the future: <strong>The Code Interpreter</strong> plugin (later Advanced Data Analysis) became indispensable, foreshadowing the powerful sandboxed execution environments we use today.</p>
<h2>Custom Instructions (<a href="https://openai.com/index/custom-instructions-for-chatgpt/?utm_source=chatgpt.com">July 2023</a>)</h2>
<p>Custom instructions were the "smooth brain" counter-reaction to the complexity of plugins. I did a double take when writing this because I thought for sure this feature was released before plugins.</p>
<p>It was just a user-defined prompt appended to every chat. Simple. Obvious. Yet it solved a huge problem: repetitive context setting.</p>
<p>This was the spiritual ancestor to every <code>.cursorrules</code> and <code>CLAUDE.md</code> file that followed.</p>
<h2>Custom GPTs (<a href="https://openai.com/index/introducing-gpts/">Nov 2023</a>)</h2>
<p>OpenAI repackaged instructions and tools into <strong>Custom GPTs</strong>. This was an attempt to "productize" prompt engineering. You could bundle a persona, some files, and a few actions into a shareable link.</p>
<p>It was a retreat from the open-ended promise of plugins toward curated, single-purpose "apps."</p>
<h2>Memory in ChatGPT (<a href="https://openai.com/index/memory-and-new-controls-for-chatgpt/">February 2024</a>)</h2>
<p>So far, we've discussed manual ways to extend LLMs. <strong>Memory</strong> represented a shift toward automatic personalization.</p>
<p>ChatGPT Memory records details from your conversations and quietly inserts them into future context. It's like a system prompt that writes itself. If you mention you're a vegetarian, it remembers that weeks later. It’s a small feature, but it marked the beginning of agents that maintain long-term state without user intervention.</p>
<h2>Cursor Rules (<a href="https://cursor.com/changelog/0-32-x">April 2024</a>)</h2>
<p><strong>Cursor</strong> changed the game by putting custom instructions where they belonged: <strong>in the repo</strong>.</p>
<p>The <code>.cursorrules</code> file was a revelation. Instead of pasting context into a chat window, you committed it to git.</p>
<ul>
<li>"We use tabs, not spaces."</li>
<li>"No semicolons."</li>
<li>"Always use TypeScript."</li>
</ul>
<p>It started as a single file, then evolved into a <code>.cursor/rules</code> folder with sophisticated scoping. You could organize multiple rule files, and even define when they applied, for example, only for certain file types or subdirectories. It was the first time extension felt "native" to the code.</p>
<p>Later Cursor introduced the ability to let the LLM decide when to apply a rule, which is a pattern we will see again.</p>
<h2>Model Context Protocol (<a href="https://en.wikipedia.org/wiki/Model_Context_Protocol">Nov 2024</a>)</h2>
<p>By late 2024, models were finally smart enough to handle real tools reliably. Anthropic's <strong>Model Context Protocol (MCP)</strong> was the answer.</p>
<p>MCP is a heavyweight solution. An MCP client needs to keep a persistent connection to an MCP server. The server serves up tool definitions, resources, and prompts to the client (in most cases is an agent) and it can send a message to the server saying a tool has been called and the server can respond with the result.</p>
<p>Unlike Custom Instructions (which just add context), <strong>MCP gives the model actual capabilities</strong>. It can read your repo, query your Postgres DB, or deploy to Vercel. Besides just providing tools, it also allows servers to provide <strong>resources</strong> (documents, logs) and <strong>prompts</strong> directly to the agent.</p>
<p>It's powerful, and perhaps a bit of overkill. While the complexity might be worth it for agent developers asking a user to set up and connect an MCP is a lot of friction and there is an entire ecosystem of startups like <a href="https://smithery.ai/">Smithery</a> built around making it easier to use MCP.</p>
<p>It is worth noting that ChatGPT apps which were announced in <a href="https://openai.com/index/introducing-apps-in-chatgpt/">October 2025</a> are built on top of MCP as a base layer. This is an attempt to make it easier for end users to use MCP without having to actually think about it.</p>
<h2>Claude Code: New Agent, New Extensions (Feb 2025)</h2>
<p>Early 2025 brought us <strong>Claude Code</strong>, which essentially added every extension mechanism under the sun to an agent.</p>
<ul>
<li><strong><code>CLAUDE.md</code>:</strong> The standard for repo-level instructions.</li>
<li><strong>MCP:</strong> For heavy-duty tool integration.</li>
<li><strong>Slash Commands:</strong> Like Cursor's notebooks, for reusable prompts.</li>
<li><strong>Hooks:</strong> The ability to intercept and modify the agent's loop (e.g., "Stop if the tests fail").</li>
<li><strong>Sub-agents:</strong> Spawning specialized workers to handle sub-tasks.</li>
<li><strong>Output Styles:</strong> (Deprecated) Configuring tone and format.</li>
</ul>
<p>Time will tell how many of these features will stick around in the long term. Anthropic has already tried to <a href="https://github.com/anthropics/claude-code/blob/main/CHANGELOG.md#2030">deprecate output styles</a>.</p>
<h2>Agent Skills (<a href="https://www.claude.com/blog/skills">Oct 2025</a>)</h2>
<p>The next extension mechanism added to Claude Code is significant enough to warrant a deeper dive. <strong>Agent Skills</strong> are the rebirth of ChatGPT Plugins.</p>
<p>While MCP has a whole client-server protocol, Agent Skills are just folders of markdown files and scripts (in whatever language you choose).</p>
<p>The agent simply scans a <code>skills/</code> directory, reads the frontmatter of every <code>SKILL.md</code>, and builds a lightweight index. It then chooses to read the full contents of a skill only if it's appropriate for the current task. This solves one of the major problems with MCP: the context bloat that comes from having to load all of the tool definitions into the context window at once.</p>
<p>Here is a snippet of the structure of a skill for doing e2e testing with Playwright taken from Anthropic's <a href="https://github.com/anthropics/skills/blob/main/webapp-testing/SKILL.md">Skills examples</a> repository:</p>
<pre><code><span>webapp-testing/
</span><span>├── examples/
</span><span>│   ├── console_logging.py
</span><span>│   ├── element_discovery.py
</span><span>│   └── static_html_automation.py
</span><span>├── scripts/
</span><span>│   └── with_server.py
</span><span>└── SKILL.md
</span></code></pre>
<p>There is a mix of scripts, examples, and plain text instructions. The only required file is the SKILL.md file. Let's take a look at that file:</p>
<pre><code><span><span><span>---</span>
</span></span><span><span><span><span>name</span><span>:</span> webapp<span>-</span>testing
</span></span></span><span><span><span><span>description</span><span>:</span> Toolkit for interacting with and testing local web applications using Playwright. Supports verifying frontend functionality<span>,</span> debugging UI behavior<span>,</span> capturing browser screenshots<span>,</span> and viewing browser logs.
</span></span></span><span><span><span><span>license</span><span>:</span> Complete terms in LICENSE.txt</span>
</span></span><span><span><span>---</span></span>
</span><span>
</span><span><span><span>#</span> Web Application Testing</span>
</span><span>
</span><span>To test local web applications, write native Python Playwright scripts.
</span><span>
</span><span><span><span>**</span><span>Helper Scripts Available</span><span>**</span></span>:
</span><span>
</span><span><span>-</span> <span>`scripts/with_server.py`</span> - Manages server lifecycle (supports multiple servers)
</span><span>
</span><span><span><span>**</span><span>Always run scripts with <span>`--help`</span> first</span><span>**</span></span> to see usage. DO NOT read the source until you try running the script first and find that a customized solution is absolutely necessary. These scripts can be very large and thus pollute your context window. They exist to be called directly as black-box scripts rather than ingested into your context window.
</span><span>
</span><span>... skill continues ...
</span></code></pre>
<p>This is just a plain markdown file with some metadata and a description of the skill. The agent reads the file which freely references other files that the agent can read. In contrast a playwright MCP server has dozens of tool definitions to control a browser, this skill just says "you have bash, this is how you write a playwright script".</p>
<p>Granted to use a skill the agent needs to have general purpose access to a computer, but this is the <a href="https://en.wikipedia.org/wiki/Bitter_lesson">bitter lesson</a> in action. Giving an agent general purpose tools and trusting it to have the ability to use them to accomplish a task might very well be the winning strategy over making specialized tools for every task.</p>
<h2>What the future holds</h2>
<p>Skills are the actualization of the dream that was set out by ChatGPT Plugins: just give the model instructions and some generic tools and trust it to do the glue work in-between. But I have a hypothesis that it might actually work now because the <strong>models are actually smart enough for it to work.</strong></p>
<p>Agent skills work because it assumes the agent has the ability to write its own tools (via bash commands). You can just give it code snippets and ask the agent to figure out how to run them generically for the task at hand.</p>
<p>Importantly, I think that skills signal towards a new definition of what an agent really is. An agent isn't just a LLM in a while loop. It's an LLM in a while loop that has a computer strapped to it.</p>
<p>Claude Code is the piece of software that first made this click for me, but it is way too developer focused to be the final form. Other applications like <a href="https://www.zo.computer/">Zo Computer</a> try to package the llm and computer together into a single application, but I still think it still doesn't abstract the computer away enough from the end user. If I ask a coworker to do something, I don't need to see their entire file system, I just need to know that they have a computer.</p>
<p>Looking forward into 2026 I expect more and more llm applications that we use will have a computer strapped to them in new and interesting ways, whether we know it or not.</p>
<p>If I could short MCP, I would, and I expect us to go back to extending our agents with the most accessible programming language: natural language.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[TSMC Arizona outage saw fab halt, Apple wafers scrapped (210 pts)]]></title>
            <link>https://www.culpium.com/p/tsmc-arizona-outage-saw-fab-halt</link>
            <guid>46037324</guid>
            <pubDate>Mon, 24 Nov 2025 18:30:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.culpium.com/p/tsmc-arizona-outage-saw-fab-halt">https://www.culpium.com/p/tsmc-arizona-outage-saw-fab-halt</a>, See on <a href="https://news.ycombinator.com/item?id=46037324">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p>Good Evening from Taipei,</p><p>A power outage at an industrial gas facility servicing TSMC interrupted manufacturing at the company’s Fab 21 in Arizona late last quarter, sources told me. The incident stopped the flow of crucial inputs needed for chipmaking, forcing the facility to shut down for at least a few hours, I was told. As a result, the company had to scrap thousands of wafers that were in production for clients at the site which include Apple, Nvidia, and AMD.</p><p><span>The event happened mid-September and was caused by a power fault at its </span><a href="https://www.linde.com/news-and-media/2021/linde-signs-long-term-agreement-to-supply-new-world-class-semiconductor-manufacturing-complex-in-the-u-s" rel="">outsourced vendor Linde</a><span>, a British industrial gases and engineering company, my sources tell me. TSMC runs a lot of its own gas supply in Taiwan, but opted to contract the work out for its Arizona site. While mistakes happen, and insurance may cover some of the losses from the event, Linde has been put on notice to identify and rectify the cause of the outage, I was told. A PR representative for Linde didn’t answer multiple phone calls and emails from </span><em><a href="http://www.culpium.com/" rel="">Culpium</a></em><a href="http://www.culpium.com/" rel=""> </a><span>outlining the incident and requesting comment.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!pERh!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81ede788-1c1b-4330-991a-a5542b588f13_5464x3640.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!pERh!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81ede788-1c1b-4330-991a-a5542b588f13_5464x3640.jpeg 424w, https://substackcdn.com/image/fetch/$s_!pERh!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81ede788-1c1b-4330-991a-a5542b588f13_5464x3640.jpeg 848w, https://substackcdn.com/image/fetch/$s_!pERh!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81ede788-1c1b-4330-991a-a5542b588f13_5464x3640.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!pERh!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81ede788-1c1b-4330-991a-a5542b588f13_5464x3640.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!pERh!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81ede788-1c1b-4330-991a-a5542b588f13_5464x3640.jpeg" width="1456" height="970" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/81ede788-1c1b-4330-991a-a5542b588f13_5464x3640.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:970,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:7828760,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://www.culpium.com/i/179543497?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81ede788-1c1b-4330-991a-a5542b588f13_5464x3640.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!pERh!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81ede788-1c1b-4330-991a-a5542b588f13_5464x3640.jpeg 424w, https://substackcdn.com/image/fetch/$s_!pERh!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81ede788-1c1b-4330-991a-a5542b588f13_5464x3640.jpeg 848w, https://substackcdn.com/image/fetch/$s_!pERh!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81ede788-1c1b-4330-991a-a5542b588f13_5464x3640.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!pERh!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81ede788-1c1b-4330-991a-a5542b588f13_5464x3640.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption><span>Photo: </span><em>Culpium</em><span> &amp; Adobe Stock</span></figcaption></figure></div><p><span>TSMC’s Arizona unit turned profitable in the first quarter of this year, a sign of the Taiwanese company’s ability to quickly scale up and churn out chips even in higher-cost locales like the US. But a 99% drop in net income in the third quarter to just $1.4 million had folks scratching their head. One writer was quick to jump to conclusions, with the </span><a href="https://wccftech.com/tsmc-arizona-facility-hit-hard-by-profit-drop-as-rising-operating-costs-give-a-reality-check/" rel="">assumption that</a><span> “rising costs have taken out an enormous chunk of profits, putting pressure on the firm’s operations.” The September outage, which hasn’t previously been reported, offers an alternative explanation for the profit decline.</span></p><p><span>“TSMC Arizona has begun to positively contribute to TSMC’s revenue. However, the company’s profit is influenced by multiple factors and should be read over time,” TSMC wrote in response to a detailed account of what </span><em>Culpium </em><span>has been told about the outage. “We also stated before that the ramp up for our overseas fabs will lead to gross margin dilution in the next five years, starting from 2025.”</span></p><p>Unfortunately, the company declined to immediately address the issue of the manufacturing disruption. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!QVsh!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc6077b9-b99f-4553-923c-39da42c794e1_5464x3640.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!QVsh!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc6077b9-b99f-4553-923c-39da42c794e1_5464x3640.jpeg 424w, https://substackcdn.com/image/fetch/$s_!QVsh!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc6077b9-b99f-4553-923c-39da42c794e1_5464x3640.jpeg 848w, https://substackcdn.com/image/fetch/$s_!QVsh!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc6077b9-b99f-4553-923c-39da42c794e1_5464x3640.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!QVsh!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc6077b9-b99f-4553-923c-39da42c794e1_5464x3640.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!QVsh!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc6077b9-b99f-4553-923c-39da42c794e1_5464x3640.jpeg" width="1456" height="970" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fc6077b9-b99f-4553-923c-39da42c794e1_5464x3640.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:970,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1541901,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.culpium.com/i/179543497?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc6077b9-b99f-4553-923c-39da42c794e1_5464x3640.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!QVsh!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc6077b9-b99f-4553-923c-39da42c794e1_5464x3640.jpeg 424w, https://substackcdn.com/image/fetch/$s_!QVsh!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc6077b9-b99f-4553-923c-39da42c794e1_5464x3640.jpeg 848w, https://substackcdn.com/image/fetch/$s_!QVsh!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc6077b9-b99f-4553-923c-39da42c794e1_5464x3640.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!QVsh!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc6077b9-b99f-4553-923c-39da42c794e1_5464x3640.jpeg 1456w" sizes="100vw"></picture></div></a></figure></div><p>Fab shutdowns are unusual, at least for TSMC. With equipment so expensive, its factories are run 24/7. That means that an hour of idle time can cost millions of dollars. Compounding the financial effect of this incident was the fact that it occurred late in the quarter, leaving little room to make up for lost production before the quarter closed. </p><p>Profit margins on new facilities and at new nodes tend to be quite thin, even negative. In addition, TSMC has been ramping up capacity in Arizona and that capex gets reflected in depreciation costs even before the new equipment can start producing revenue. So it’s reasonable to see fluctuations in net income at the site. A halt in production and scrapping of wafers adds to the costs, dragging on earnings even if only slightly and briefly.</p><p>Impact to clients is likely to be negligible, I was told, and the financial loss to TSMC may be covered by insurance. Capacity at Fab 21 is still quite small, and many products being made there have already been taped out and manufactured in Taiwan previously. In past disruptions, lost production and revenue was made up in the subsequent quarter. </p><p><span>That said, the broader issue is that Taiwanese manufacturers are good at managing their operations when they handle it themselves, but still face struggles when they need to lean on non-Taiwanese firms at overseas facilities. The entire process of building the fab and installing equipment at Arizona has been </span><a href="https://www.nytimes.com/2024/08/08/business/tsmc-phoenix-arizona-semiconductor.html" rel="">an exercise in cross-cultural adaptation</a><span>.</span></p><p>The most common cause of production interruptions at TSMC is Mother Nature. Earthquakes regularly rattle Taiwan, and fabs are built to withstand most of them. But sometimes a big tremor can trigger a safety shutdown, while really nasty temblors have caused actual damage. Beyond natural disasters, there’ve been few man-made shutdowns at TSMC because they’re pretty rigorous about operations. </p><p><span>A couple of notable problems were both caused by vendors, not TSMC internally. In 2018, </span><a href="https://pr.tsmc.com/english/news/1969" rel="">a computer virus was introduced</a><span> to fabs via equipment from Japan. That incident sparked a whole new approach to cybersecurity both at TSMC and among fellow Taiwanese chipmakers. Less than a year later, </span><a href="https://pr.tsmc.com/english/news/1984" rel="">a batch of contaminated photoresist</a><span> from a chemical supplier forced the company to scrap a large number of wafers. It made up the production the following quarter, with the problem costing TSMC around $25 million in operating profit for the year. </span></p><div data-attrs="{&quot;url&quot;:&quot;https://www.culpium.com/p/tsmc-arizona-outage-saw-fab-halt?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;}" data-component-name="CaptionedButtonToDOM"><p>Sharing is caring. This post is public &amp; free, so please tell your friends what you’re reading.</p><p data-attrs="{&quot;url&quot;:&quot;https://www.culpium.com/p/tsmc-arizona-outage-saw-fab-halt?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;}" data-component-name="ButtonCreateButton"><a href="https://www.culpium.com/p/tsmc-arizona-outage-saw-fab-halt?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p></div><p><span>Linde </span><a href="https://www.linde.com/news-and-media/2021/linde-signs-long-term-agreement-to-supply-new-world-class-semiconductor-manufacturing-complex-in-the-u-s" rel="">trumpeted the TSMC contract</a><span> when it landed the deal back in 2021, noting that it planned to invest $600 million into the facility. “While the project is capital and electricity intensive, it will only employ 14 plant employees and 14 truck drivers, documents from 2020 said,” the </span><a href="https://www.aztechcouncil.org/600-million-gas-plant-planned-to-support-phoenix-semiconductor-manufacturing-facility" rel="">Arizona Tech Council later reported</a><span>. </span></p><p><span>Apple’s A16 SoC </span><a href="https://www.culpium.com/p/apple-mobile-processors-are-now-made" rel="">was the first product</a><span> taped out at the site, </span><em><a href="http://www.culpium.com/" rel="">Culpium</a></em><a href="http://www.culpium.com/" rel=""> </a><span>reported in September last year. </span><a href="https://www.culpium.com/p/amd-to-make-high-performance-chips" rel="">AMD’s Ryzen 9000</a><span> and Nvidia Blackwell chips were since added to the lineup with designs from </span><a href="https://www.culpium.com/p/tsmc-to-make-chips-for-cryptominer" rel="">Bitdeer</a><span>, among others, also qualified at the Arizona fab. </span></p><p>Thanks for reading. Please subscribe, if you haven’t already.</p><p><em><strong>More from Culpium</strong></em><span>:</span></p><div data-component-name="DigestPostEmbed"><a href="https://www.culpium.com/p/apple-mobile-processors-are-now-made" rel="noopener" target="_blank"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!k-Mb!,w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8ee10c5-f75f-469f-8c34-796fb90f4369_6400x4272.png"><img src="https://substackcdn.com/image/fetch/$s_!k-Mb!,w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8ee10c5-f75f-469f-8c34-796fb90f4369_6400x4272.png" sizes="100vw" alt="Apple Mobile Processors Are Now Made in America. By TSMC" width="140" height="140"></picture></div></a></div><div data-component-name="DigestPostEmbed"><a href="https://www.culpium.com/p/deepseek-is-more-wall-street-than" rel="noopener" target="_blank"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!i7Wn!,w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e2bded1-7b71-4179-81a1-5061b94ea5f8_4247x2831.jpeg"><img src="https://substackcdn.com/image/fetch/$s_!i7Wn!,w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e2bded1-7b71-4179-81a1-5061b94ea5f8_4247x2831.jpeg" sizes="100vw" alt="Deepseek is More Wall Street Than Silicon Valley" width="140" height="140"></picture></div></a></div><div data-component-name="DigestPostEmbed"><a href="https://www.culpium.com/p/what-supply-chain-security-really" rel="noopener" target="_blank"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!4Pva!,w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc2d328a-9d85-43ca-884f-6c333ae54035_2492x1242.png"><img src="https://substackcdn.com/image/fetch/$s_!4Pva!,w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc2d328a-9d85-43ca-884f-6c333ae54035_2492x1242.png" sizes="100vw" alt="What Supply Chain Security REALLY Means" width="140" height="140"></picture></div></a></div><div data-component-name="DigestPostEmbed"><a href="https://www.culpium.com/p/hon-hai-tech-day-and-the-future-of" rel="noopener" target="_blank"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!hWjA!,w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2fd142d-89fc-4b4a-a53a-6364f4f66ace_1280x960.jpeg"><img src="https://substackcdn.com/image/fetch/$s_!hWjA!,w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2fd142d-89fc-4b4a-a53a-6364f4f66ace_1280x960.jpeg" sizes="100vw" alt="Hon Hai Tech Day &amp; the Future of the Global Economy" width="140" height="140"></picture></div></a></div><div data-component-name="DigestPostEmbed"><a href="https://www.culpium.com/p/tsmc-built-a-bamboo-ladder-trump" rel="noopener" target="_blank"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!5gga!,w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa67a7e1e-5e2d-4f02-9dc9-2488a0588d37_3125x1750.jpeg"><img src="https://substackcdn.com/image/fetch/$s_!5gga!,w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa67a7e1e-5e2d-4f02-9dc9-2488a0588d37_3125x1750.jpeg" sizes="100vw" alt="TSMC Built a Bamboo Ladder. Trump Might Yet Shake It" width="140" height="140"></picture></div></a></div></div></div>]]></description>
        </item>
    </channel>
</rss>