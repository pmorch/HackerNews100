<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 14 Sep 2023 07:00:07 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Game Development Post-Unity (125 pts)]]></title>
            <link>https://www.computerenhance.com/p/game-development-post-unity</link>
            <guid>37503351</guid>
            <pubDate>Thu, 14 Sep 2023 00:19:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.computerenhance.com/p/game-development-post-unity">https://www.computerenhance.com/p/game-development-post-unity</a>, See on <a href="https://news.ycombinator.com/item?id=37503351">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9c62294-e1ff-46eb-8b1c-ad903695fa91_2253x1134.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9c62294-e1ff-46eb-8b1c-ad903695fa91_2253x1134.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9c62294-e1ff-46eb-8b1c-ad903695fa91_2253x1134.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9c62294-e1ff-46eb-8b1c-ad903695fa91_2253x1134.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9c62294-e1ff-46eb-8b1c-ad903695fa91_2253x1134.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9c62294-e1ff-46eb-8b1c-ad903695fa91_2253x1134.png" width="1456" height="733" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c9c62294-e1ff-46eb-8b1c-ad903695fa91_2253x1134.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:733,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:4255289,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9c62294-e1ff-46eb-8b1c-ad903695fa91_2253x1134.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9c62294-e1ff-46eb-8b1c-ad903695fa91_2253x1134.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9c62294-e1ff-46eb-8b1c-ad903695fa91_2253x1134.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9c62294-e1ff-46eb-8b1c-ad903695fa91_2253x1134.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>My tenure in the game industry was working </span><em>on</em><span> game engine code, not </span><em>with</em><span> game engine code. As a result, I do not have any first-hand experience choosing an off-the-shelf game engine. It’s not a decision I’ve ever had to make, and I don’t keep up with the latest developments across the myriad of engine options.</span></p><p><span>But I do follow game business trends to a certain extent, and </span><a href="https://twitter.com/cmuratori/status/1611148509100797954" rel="">for</a><span> </span><a href="https://twitter.com/cmuratori/status/1524580933357035521" rel="">well</a><span> </span><a href="https://twitter.com/cmuratori/status/1524143572164915201" rel="">over</a><span> a year now, I’ve been warning that Unity’s relationship with game developers would inexorably change for the worse.</span></p><p><span>This was not based on any inside knowledge. It was based solely on the financials they report, and the kinds of statements they make to investors in their earnings calls. If you </span><a href="https://s26.q4cdn.com/977690160/files/doc_financials/2023/q2/Unity-Software-Inc-_Earnings-Call_2023-08-02_English.pdf" rel="">read these regularly</a><span>, you get a very good sense for what a company’s priorities are in two ways: one, because you hear how they are pitching their future to investors; and two, because you can see how far in the hole they are financially, so you know whether they are likely to make major business model changes.</span></p><p><span>In Unity’s case, paying attention to their quarterly investor materials tells you many things that aren’t as common knowledge as they should be. As a small example, most developers think Unity is primarily in the business of selling game engines. They’re not! </span><em>Less than half their revenue</em><span> comes from game engines. </span><em>Over half</em><span> comes from advertising</span><em>.</em><span> Their bottom line is </span><em>more affected by the advertising market than it is by how many developers buy their engine.</em></p><p><span>There are many things like this you learn when you dig into their investor materials. In my opinion, </span><em>none</em><span> of their fundamentals boded well for Unity’s future as a game engine on which to base a game company — hence my tweets (“X’es”?) on the subject.</span></p><p><span>A few months after </span><a href="https://twitter.com/cmuratori/status/1524143572164915201?lang=en" rel="">I posted that Unity seemed likely to be absorbed</a><span>, Unity </span><a href="https://blog.unity.com/news/unity-ironsource-merger-growing-great-mobile-games" rel="">merged with IronSource</a><span>, a controversial company that has been </span><a href="https://www.fool.com/investing/2022/07/24/most-troubling-thing-about-unitys-ironsource-deal/" rel="">accused of developing malware</a><span>. This was the first indication that things might (unfortunately) be going the way I had anticipated.</span></p><p><span>Now, less than a year later, Unity has </span><a href="https://blog.unity.com/news/plan-pricing-and-packaging-updates" rel="">announced</a><span> that they will be </span><em>retroactively</em><span>(!!) changing their license terms in order to charge game developers a substantial per-unit fee, despite </span><a href="https://www.gamesindustry.biz/theres-no-royalties-no-f-ing-around-riccitiello" rel="">explicitly disavowing “royalty-like” structures in the past</a><span>. This move has sent shockwaves through the developer community, as one of Unity’s differentiating factors </span><em>was</em><span> that they only charged an up-front cost for their platform.</span></p><p><span>Developers are understandably upset by this move. Unity </span><em>already</em><span> charges high per-seat monthly rates to professional developers. At $185/mo, Unity Pro is </span><em>twenty-six times</em><span> as expensive as a subscription to Office365, and </span><em>three times</em><span> as expensive as Adobe’s “all apps” Creative Cloud subscription. At $250/mo, Unity Enterprise is among the most expensive end-user SaaS offerings a game developer might see — more expensive even than Autodesk’s Maya and 3DS MAX.</span></p><p><span>Yet despite these SaaS offerings being less expensive than Unity, to the best of my knowledge they have never tried to demand anything like a royalty from their paying customers. If they did, it didn’t last, because they do not charge any </span><em>today</em><span>.</span></p><p>Even if Unity ends up backpedaling due to backlash from this decision, it seems unlikely that developers will trust Unity going forward. Many are wondering if they should switch engines now to avoid relying on an untrustworthy partner for their core technology.</p><p><span>But switch engines </span><em>to what</em><span>?</span></p><p><span>Switching engines </span><em>from</em><span> Unity requires knowing what engine you might switch </span><em>to.</em><span> To that end, I was recently asked on X what I recommend as an engine for people who want to switch away from Unity. Since I have no opinions on this, I went ahead and asked developers to weigh in:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2915c6f4-bcfd-4988-a13c-55c7fda78de8_586x303.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2915c6f4-bcfd-4988-a13c-55c7fda78de8_586x303.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2915c6f4-bcfd-4988-a13c-55c7fda78de8_586x303.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2915c6f4-bcfd-4988-a13c-55c7fda78de8_586x303.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2915c6f4-bcfd-4988-a13c-55c7fda78de8_586x303.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2915c6f4-bcfd-4988-a13c-55c7fda78de8_586x303.png" width="586" height="303" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2915c6f4-bcfd-4988-a13c-55c7fda78de8_586x303.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:303,&quot;width&quot;:586,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:34807,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2915c6f4-bcfd-4988-a13c-55c7fda78de8_586x303.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2915c6f4-bcfd-4988-a13c-55c7fda78de8_586x303.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2915c6f4-bcfd-4988-a13c-55c7fda78de8_586x303.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2915c6f4-bcfd-4988-a13c-55c7fda78de8_586x303.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>To make it easier for people to explore the responses, I tallied how many times each alternative engine was mentioned, and I’ve prepared a list </span><strong>from most-mentioned to least-mentioned</strong><span>.</span></p><p><span>I’ve also tried to summarize a little bit about what the engine seems to be “about”, but, take that with a </span><em>huge</em><span> grain of salt. As I’ve said multiple times now, </span><em>I don’t use</em><span> off-the-shelf-engines, so I have no first-hand knowledge here. If I’ve summarized anything inaccurately, please let me know in the comments, so I can correct it!</span></p><p>I’ve also tried to include the “scripting language” supported by each platform where applicable, so that people can quickly see if the engine directly supports a particular scripting language they know. Because they often don’t have official names, I’ve used “visual” if the platform has a built-in flowchart-like visual scripting language.</p><p><strong><a href="https://godotengine.org/" rel="">Godot</a></strong><span> (C#, GDScript, visual). By far the most-mentioned alternative to Unity, Godot seems in many ways to be an open-source response to Unity: less focused on high-end engine features than something like Unreal, and more focused on being quick and easy to spin up for beginners. Common complaints seem to be that it is “not quite there yet” — perhaps because of bugs and missing features compared to Unity — and that it lacks first-class support for consoles, a feature most professional game developers now need. But it has been used in some recent notable games (</span><em>Brotato</em><span> and </span><em>Dome Keeper</em><span>), and it seems to have decent momentum behind it.</span></p><p><strong><a href="https://www.unrealengine.com/en-US" rel="">Unreal</a><span> </span></strong><span>(visual). The Unreal Engine needs no introduction. A hefty percentage of the AAA releases in any given year run on one version or another of Epic’s flagship engine. Although it offers a host of AAA features like Nanite, Lumen, and Metahuman, common complaints are that its complex nature makes it more difficult to get started, and requires more technical expertise to work with. However, as </span><a href="https://www.flibitijibibo.com/" rel="">Ethan Lee</a><span> — who </span><em>does</em><span> have a ton of experience working with off-the-shelf-engines — </span><a href="https://gist.github.com/flibitijibibo/035087d8736441786b10e8c3879d50dd" rel="">wrote a few months back</a><span>, apparently shipping a game on Unreal is actually </span><em>easier</em><span> these days in many ways than shipping one on Unity if you’re aiming for a quality release.</span></p><p><strong><a href="https://defold.com/" rel="">Defold</a><span> </span></strong><span>(Lua). Mentioned almost as many times at Godot and Unreal was an engine I’d never heard of, Defold. Apparently this is a good engine to check out if you’re developing 2D or mobile games, and it appears that </span><em>tons</em><span> of mobile-style games have been shipped on it. If you’re looking for more of a 3D engine, it is probably not what you’re looking for.</span></p><p><strong><a href="https://www.raylib.com/" rel="">RayLib</a></strong><span>. RayLib is not an engine per se, but rather a library suite that allows you to quickly build games (and apps) in a native language like C++. It was mentioned multiple times, so it seems like DIY devs enjoy using this library — but, people looking for a turnkey solution with a full editor like Unity probably won’t be interested.</span></p><p><strong><a href="https://o3de.org/" rel="">Open 3D</a></strong><span> (Lua, visual). This is actually CryEngine, which was licensed by Amazon and made into LumberYard, which was then abandoned by Amazon and made into Open 3D, which is now maintained as open-source. So, if you like CryTek, you might be very interested in Open3D! However, being a descendant of a highly-specialized AAA-focused engine does suggest it might have a harder learning curve, so it’s unclear how much of an option this is for less technical developers.</span></p><p><strong><a href="https://gamemaker.io/en" rel="">GameMaker</a></strong><span> (GML, visual). For 2D games, GameMaker is an evergreen favorite, and very beginner-friendly. Many famous 2D games were made with GameMaker — </span><em>Undertale</em><span>, </span><em>Forager</em><span>, </span><em>Hyper Light Drifter</em><span>, </span><em>Gunpoint</em><span>, </span><em>Hotline Miami</em><span>, and </span><em>Spelunky</em><span>, just to name a few. If you don’t need 3D or fancy lighting, it seems like a solid choice. But for anything more technically demanding, GameMaker unfortunately seems to top out quickly.</span></p><p><strong><a href="https://unigine.com/" rel="">Unigine</a></strong><span> (C#). Though not primarily targeted at games, Unigine was mentioned multiple times as an option, and they do list games as a first-class application of their SDK. How useful a simulation-oriented SDK would be to someone coming from Unity, however, is an open question.</span></p><p><strong><a href="https://bevyengine.org/" rel="">Bevy</a></strong><span>. For Rust aficionados, the most commonly mentioned engine was Bevy. Presumably, if you’d like to program games in Rust, this would be an interesting one to check out, despite not yet establishing much of a pedigree. That said, it also doesn’t seem to ship with a complete editing environment, but rather some embeddable tools for putting editing in the game itself. This might be more of a hard sell for people coming from Unity’s integrated editing environment.</span></p><p><strong><a href="https://flaxengine.com/" rel="">Flax</a></strong><span> (visual). Like Defold, I’d never heard of Flax, but it touts a surprisingly large feature set, and provides integrated editing capabilities out of the box. However, it doesn’t seem like there are many (or any?) notable games shipping on this engine yet, which leaves a bit of a question mark as to its viability.</span></p><p><strong><a href="https://www.cocos.com/en" rel="">Cocos</a></strong><span> (JavaScript/TypeScript). Another full-featured engine with an integrated development environment, the modern-day Cocos suite is apparently the same tool lineage that was used fifteen years ago to make </span><em>FarmVille</em><span>. Although I wasn’t able to find out much about this engine, it does appear to still be mobile-focused.</span></p><p><strong><a href="https://www.stride3d.net/" rel="">Stride</a><span> </span></strong><span>(C#). Although it was mentioned multiple times, I had a hard time finding out much about this engine. Apparently, this is Silicon Studio’s Paradox engine, which was renamed Xenko, and then later renamed Stride. I’m not sure if any notable games have shipped on it, but like Unity, it is a C#-focused engine, and comes with a full editing environment.</span></p><p><strong><a href="https://www.monogame.net/" rel="">Monogame</a><span> </span></strong><span>(C#). Microsoft’s XNA was one of the most successful game-development-targeted SDKs ever made, spawning </span><em>Bastion</em><span>, </span><em>Owlboy</em><span>, </span><em>Timespinner</em><span>, </span><em>Magicka</em><span>, </span><em>Axiom Verge</em><span>, </span><em>Serious Sam Double D</em><span>, and of course, </span><em>Stardew Valley</em><span>. Being Microsoft, they responded to this overwhelming success by </span><em>canceling the project</em><span>. Thankfully, community developers picked up the slack, and Monogame is the result. A reimplementation of the XNA 4 API, it continues to be a popular base SDK for C# developers.</span></p><p><strong>And that’s not all</strong><span>. In addition to the platforms listed above — which were mentioned multiple times — there were several others mentioned only once, so I did not include them in the list. But that doesn’t mean they aren’t worth checking out! If you’re looking for more platforms to evaluate, the other ones people mentioned were: </span><a href="https://www.construct.net/en" rel="">Construct</a><span>, </span><a href="https://www.ogre3d.org/" rel="">Ogre3D</a><span>, </span><a href="https://solar2d.com/" rel="">Solar2D</a><span>, </span><a href="https://www.harfang3d.com/en_US/" rel="">HARFANG 3D</a><span>, </span><a href="https://www.cryengine.com/" rel="">CryEngine</a><span>, </span><a href="https://fna-xna.github.io/" rel="">FNA</a><span>, </span><a href="https://libgdx.com/" rel="">libGDX</a><span>, </span><a href="https://love2d.org/" rel="">LÖVE</a><span>, </span><a href="https://fyrox.rs/" rel="">Fyrox</a><span>, </span><a href="https://c4engine.com/" rel="">C4Engine</a><span>, </span><a href="https://hazelengine.com/" rel="">Hazel</a><span>, </span><a href="https://wickedengine.net/" rel="">Wicked</a><span>, </span><a href="https://tellusim.com/" rel="">TelluSim</a><span>, and </span><a href="https://heaps.io/" rel="">heaps.io</a><span>.</span></p><p><span>The biggest thing I realized when compiling this list was that </span><em>most developers probably don’t know what all their options actually are</em><span>. I bet a lot of people use Unity by default, not because they actually looked at all these other engines and determined that Unity </span><em>in particular</em><span> was best for their game or studio.</span></p><p><span>I think a huge help to everyone right now would be if developers could </span><em>share their experiences</em><span> with alternative engines. What kind of project did you use it for? How did development go? Are there any particular links to tutorials or educational materials that might help people get started? Do you know of any good overviews?</span></p><p>I am leaving this comment section open to everyone, so anyone who has experience to share can leave it in the comments below. Please try to be polite and helpful! And most of all, please try to give honest guidance to people about the engines you’ve used. That way people looking to make the switch can make a well-educated decision about which platform is best for them.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Marvel visual effects artists unanimously vote to unionize (132 pts)]]></title>
            <link>https://www.cnbc.com/2023/09/13/marvel-vfx-artists-unanimously-vote-to-unionize.html</link>
            <guid>37502892</guid>
            <pubDate>Wed, 13 Sep 2023 23:14:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2023/09/13/marvel-vfx-artists-unanimously-vote-to-unionize.html">https://www.cnbc.com/2023/09/13/marvel-vfx-artists-unanimously-vote-to-unionize.html</a>, See on <a href="https://news.ycombinator.com/item?id=37502892">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="RegularArticle-ArticleBody-6" data-module="ArticleBody" data-test="articleBody-2" data-analytics="RegularArticle-articleBody-6-2"><div id="ArticleBody-InlineImage-106201393" data-test="InlineImage"><p>Mark Ruffalo as The Hulk in "Avengers: Endgame."</p><p>Disney | Marvel</p></div><div><p>Hollywood, already gripped by two strikes, has some new union members.</p><p>Marvel Studios' visual effects workers unanimously voted in favor of unionizing with the International Alliance of Theatrical Stage Employees, <a href="https://iatse.net/marvel-studios-vfx-workers-unanimously-vote-to-unionize-with-iatse-marking-historic-first/" target="_blank">IATSE announced</a> Wednesday. This marks the first time a unit of solely VFX workers have unionized with the group.</p><p>VFX artists have faced increased workloads and tight deadlines to complete some of the industry's biggest budget franchise films in recent years, leading to tension <a href="https://www.vulture.com/2023/02/marvel-vfx-workers-on-ant-man-and-the-wasp-quantumania.html" target="_blank">between these workers and studios</a>.</p><p>In particular, <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-3"><a href="https://www.cnbc.com/quotes/DIS/">Disney</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>, which owns Marvel Studios, required immense special effects work <a href="https://www.vulture.com/article/a-vfx-artist-on-what-its-like-working-for-marvel.html" target="_blank">in the last three years</a> to complete a massive slate of superhero films for the big screen and television shows for its streaming service Disney+. The Marvel Studios VFX crew has more than 50 workers, according to IATSE.</p><p>"I grew up dreaming of working on Marvel films, so when I started my first job at Marvel, I felt like I couldn't complain about the unpaid overtime, the lack of meal breaks, and the incredible pressure put on VFX teams to meet deadlines because I was just supposed to be grateful to be here at all," Sarah Kazuko Chow, VFX coordinator at Marvel, said in a statement.</p><p>Representatives for Disney did not immediately respond to CNBC's request for comment.</p><p>The push for unionization comes at a time when Hollywood is dealing with dual labor strikes from its writers and actors.</p><p>Like those striking, Marvel's VFX artists are interested in a labor contract that offers fair pay, health-care benefits and "a safe and sustainable working environment," said Mark Patch, VFX organizer for IATSE.</p><p>They aren't the only VFX team looking to unionize. In late August, Walt Disney Pictures' VFX staffers filed with the National Labor Relations Board to host an election to unionize.</p><p>Now that the vote is official, Marvel VFX workers must engage in collective bargaining negotiations with Marvel Studios executives in order to draft a contract. However, with the studio already locked in talks with Hollywood's scribes and yet to address contract concerns with striking actors, it could take time for the VFX artists to get to the table.</p><p>"Today's count demonstrates the unprecedented demand for unionization across new sectors of the entertainment industry is very real," Matthew Loeb, president of IATSE International, said in a statement.</p><p>IATSE represents 170,000 industry workers, from studio mechanics to wardrobe and makeup artists. In late 2021, the union faced off against the Alliance of Motion Picture and Television Producers to negotiate a new contract.</p><p><a href="https://www.cnbc.com/2021/10/04/iatse-strike-hollywood-union-workers-approve-measure-to-call-a-strike.html">The union authorized a strike</a> but was able to come to terms with the studios. <a href="https://www.cnbc.com/2021/11/15/iatse-union-narrowly-ratifies-new-contract-with-hollywood-producers.html">Its three-year contract</a> included clauses that enforced a 10-hour turnaround between shifts, 54 hours of rest over the weekend, increased health-care and pension plan funding and a 3% rate increase for every year for the duration of the contract. Stiff penalties were also put in place if these break periods were not adhered to.</p><p>Loeb told Marvel's VFX artists that it has the backing of IATSE, telling those who voted to unionize, "Your fight is our fight."</p><p><em>Disclosure: Comcast is the parent company of NBCUniversal and CNBC. NBCUniversal is a member of the Alliance of Motion Picture and Television Producers. The AMPTP is currently negotiating with striking writers and actors in Hollywood.</em></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hutter Prize for compressing human knowledge (158 pts)]]></title>
            <link>http://prize.hutter1.net/</link>
            <guid>37502329</guid>
            <pubDate>Wed, 13 Sep 2023 22:03:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://prize.hutter1.net/">http://prize.hutter1.net/</a>, See on <a href="https://news.ycombinator.com/item?id=37502329">Hacker News</a></p>
<div id="readability-page-1" class="page"><div width="100%"><tbody><tr><td><span face="Arial" size="-1">
<a name="disclaim"><b>Disclaimer:</b></a> Copying and distribution of this page
(<a href="http://prize.hutter1.net/">http://prize.hutter1.net</a>)
is permitted, provided the source is cited.
The prize will be paid if the solution reflects the spirit of the contest.
In particular decompressors (secretly) receiving any kind of "outside" information are forbidden. 
Also in order to verify your claim we need to be able to run your executable on
our machines within reasonable space and time constraints.
This is a privately run and funded contest. 
Payment of the prize cannot be legally enforced. 
The smallest claimable prize is 5'000€.
After an award, the prize formula (L) will be adapted.
Rules may change at any time to meet the goals of fairness, accuracy,
maximizing public participation, and recognizing existing practice. 
July 2006. Updated Feb.2020.
</span></td></tr></tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Don't use Discord as your Q&A forum (237 pts)]]></title>
            <link>https://kraktoos.com/posts/dont-use-discord-as-forum/</link>
            <guid>37502258</guid>
            <pubDate>Wed, 13 Sep 2023 21:54:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kraktoos.com/posts/dont-use-discord-as-forum/">https://kraktoos.com/posts/dont-use-discord-as-forum/</a>, See on <a href="https://news.ycombinator.com/item?id=37502258">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-astro-cid-egg7nqdx=""><article data-astro-cid-egg7nqdx=""><time data-astro-cid-egg7nqdx="" data-astro-transition-scope="astro-v6aqexxr-2">Sep 12 2023</time><hr data-astro-cid-egg7nqdx=""><p>Discord is not the ideal choice for the Q&amp;A forum of your next failed side-project. Seriously, please stop.</p><h3 id="why-it-sucks">Why it sucks</h3><ol><li><p>Chaos Discord can be a whirlwind of madness. Important stuff you post can vanish into the ether within seconds, drowned by a never-ending stream of messages. Sure, they introduced threads, but they still leave much to be desired, especially because of:</p></li><li><p><strong>TERRIBLE</strong> Search and Discovery Trying to find past discussions or solutions in Discord is like trying to locate a needle in a haystack blindfolded and drunk. Discord’s search is something made by the devil to check the patience of us, mere programmers. You might search for a message you sent weeks ago and come up empty-handed because you missed a couple of words or some other infuriatingly stupid reason…</p></li><li><p>The Discord Odyssey Picture this: You encounter a problem with a project that exclusively relies on Discord for support and questions. You have to embark on a quest: find their Discord link by first searching for the project name on SearX (or Goog-🤮), finding the right link, then making sure you’re logged in to Discord on your browser or app, join the server (after reading (or not?) and agreeing to the rules and choosing the roles needed to see the god damn channel), hunt down the elusive <code>help</code> keyword in the left side of the screen, and then spend an eternity searching for your issue or just asking it again. Meanwhile not finding the solution and ending up <code>bun install</code>ing a new blazingly fast new JS library…</p></li><li><p>Waaaay too ephemeral Here’s a curveball for you: what if Discord decides to pull the plug? Who can guarantee it’ll be around tomorrow? Something like Stack Overflow can and is scrapped every day, but Discord isn’t.</p></li></ol><h3 id="better-alternatives">Better Alternatives</h3><ol><li><p>Dedicated Community Forums - Check out platforms like Discourse. They’re designed for structured discussions and won’t leave your posts lost in the abyss. Good SEO for users of your software to find what they need fast.</p></li><li><p>Lean on the Pros - Websites like Stack Overflow or communities on Reddit, Lemmy, or Kbin can be excellent places to seek help.</p></li><li><p>Git It Done - If you’re dealing with code issues, rely on GitHub, Gitlab, Gitea, or any other Git-based issue tracking system, specially if your project is open source, since it’s where everyone will be when looking for your project anyways and has good enough SEO for most daily searches.</p></li></ol><p>Remember, Discord is great &amp; all but as a Q&amp;A forum, it’s like using a spatula to fix your spaceship. Think twice (or as many times as you need to realize that it’s a bad idea), and choose wisely (just not Discord)!</p></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Don’t mess with a genius (2010) (280 pts)]]></title>
            <link>https://shreevatsa.wordpress.com/2010/06/04/dont-mess-with-a-genius/</link>
            <guid>37501231</guid>
            <pubDate>Wed, 13 Sep 2023 20:21:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://shreevatsa.wordpress.com/2010/06/04/dont-mess-with-a-genius/">https://shreevatsa.wordpress.com/2010/06/04/dont-mess-with-a-genius/</a>, See on <a href="https://news.ycombinator.com/item?id=37501231">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<p>Or: What happens when Newton’s laws are violated</p>
<p>Recently, I read a book called <i><strong>Newton and the Counterfeiter</strong></i>, subtitled <i><strong>The Unknown Detective Career of the World’s Greatest Scientist</strong></i>. It focuses on an awesome phase of Isaac Newton’s later career that, like his pursuits in alchemy, gets little mention in most accounts. The story, of Newton’s job as Warden of the Mint and his efforts bringing criminals to justice, contains many elements of a modern crime thriller: including an ingenious arch-adversary, Newton visiting the gin houses of London in disguise, personally interrogating suspects, playing good cop–bad cop, and using every trick in the book, before the book had been written. The story begins, as many of them do, at the beginning.</p>
<h2>The beginning</h2>
<p>Isaac Newton, 55 years old and just recovered from his nervous breakdown, was looking for a post in the city (London), having lived in the village of Cambridge ever since his student days. As a Great Man now, he had already been rewarded with a seat in parliament (the only thing ever recorded spoken by him is a request to close the window), but it appeared harder to get him a job. Finally, his friends pulled the right strings, and Newton moved in as Warden of the Mint in 1696.</p>
<p>The job was meant to be a sinecure (he had been promised that the position “has not too much business to require more attendance than you may spare”), and no one expected him to do anything special. Today though, we can look back and confidently say that Newton is the greatest Warden the Mint ever had. Unfortunately, this is not saying much, because it appears Newton is also the only good Warden the Mint ever had.</p>
<h2>The Mint</h2>
<p>The Royal Mint at the time had two officials in charge, both appointed by the king and with no well-defined hierarchy between them. The Warden of the Mint, with a salary of 400-odd pounds a year, was in charge of the Mint’s facilities, and the Master of the Mint, with a salary of 600 pounds a year plus (more substantial) a percentage of every coin made, was in charge of the actual production of new coins.</p>
<p>When Newton moved in as Warden, the Master was the notoriously corrupt and incompetent Thomas Neale, who was so lost in his gambling habit and his numerous enterprises that the operations of the Mint were, well, not in mint condition.</p>
<p>This was a bad time, because counterfeiting, clipping, and arbitrage had weakened the economy to the extent that there was a shortage of cash everywhere, most tax payments and trade had stopped, panic was rising, and civil war was imminent.</p>
<h2>Counterfeiters</h2>
<p>Counterfeiting was easy money, and everyone took to it. The government declared it high treason, a hanging offence, but this only made juries more reluctant to hang their peers. Of those counterfeiters who were brought to trial, many escaped conviction, even one of them through a wonderful incompetence defence:</p>
<blockquote><p>
        [I]nept counterfeiters attempting to exploit the currency crisis supplied the Old Bailey with a constant diet of rapidly dispatched defendants. Perhaps the most spectacular display of incompetence came from an unnamed “inhabitant of the parish of St. Andrews Holbourn,” brought to trial accused of copying French coins. His work was astonishingly awful, and he was acquitted, the jury accepting his rather bold argument that the poor quality of his work confirmed that “he had tryed to Coin with Pewter as aforesaid for Diversion, or the like, but never was concerned in Coining any manner of Money.” Few others tried this defense.
</p></blockquote>
<h2>The Great Recoinage: Newton takes over</h2>
<p>Faced with no alternative, the government had decided that the Mint would recoin everything — the Great Recoinage was to take place. It aimed to melt and restrike, in three years, more coins than it had produced in three decades. How anyone expected it to happen with Neale in charge of it is a mystery. As the recoinage began, it quickly turned into a farce, and it was clear to everyone that it would be an impossible task.</p>
<p>Newton saw what was happening, and couldn’t stand it. He read up on the history of the Mint, studied its operations, studied Neale, accumulated all the knowledge necessary, and somehow intimidated and pushed Neale aside in a bloodless coup, and took over the Great Recoinage himself. He streamlined the production, conducting probably one of the earliest “time-and-motion studies” (he synchronized workers’ operations to the rate of their heartbeat), running the mint from 4am to midnight, and finished the “clearly impossible task” ahead of schedule.</p>
<h2>Warden duty</h2>
<div data-shortcode="caption" id="attachment_1876"><p><a href="http://www.numericana.com/arms/index.htm#newton"><img aria-describedby="caption-attachment-1876" data-attachment-id="1876" data-permalink="https://shreevatsa.wordpress.com/2010/06/04/dont-mess-with-a-genius/newton-coat/" data-orig-file="https://shreevatsa.files.wordpress.com/2010/06/newton-coat.png" data-orig-size="200,221" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}" data-image-title="Newton-coat" data-image-description="" data-image-caption="<p>Newton’s unusual coat of arms</p>
" data-medium-file="https://shreevatsa.files.wordpress.com/2010/06/newton-coat.png?w=200" data-large-file="https://shreevatsa.files.wordpress.com/2010/06/newton-coat.png?w=200" src="https://shreevatsa.files.wordpress.com/2010/06/newton-coat.png?w=700" alt="" title="Newton-coat" srcset="https://shreevatsa.files.wordpress.com/2010/06/newton-coat.png 200w, https://shreevatsa.files.wordpress.com/2010/06/newton-coat.png?w=136&amp;h=150 136w" sizes="(max-width: 200px) 100vw, 200px"></a></p><p id="caption-attachment-1876">Newton's unusual coat of arms</p></div>
<p>After saving the country from economic ruin, by doing something that wasn’t even his own job, Newton finally turned to a duty that his post actually came with — protecting the currency, by “enforcing the King’s law in and around London for all crimes committed against the currency”. This meant doing a policeman’s work — or rather, that of “a criminal investigator, interrogator, and prosecutor rolled into one”. He found the idea distasteful, not to mention the kind of men he would have to come in contact with, and requested that this be assigned to someone else, but when his request was denied he turned to the task in all seriousness.</p>
<p>Despite having hardly been a man of the world until then, he very quickly figured out what he had to do, better than anyone else had done. (In the mere four years he was Warden, he got dozens of counterfeiters hanged.)</p>
<p>He descended into the underworld … hiring men to go undercover, interrogating suspects, planting informers in prisons, the works. To avoid issues of jurisdiction he got himself appointed Justice of the Peace for nineteen counties surrounding London. Most criminals (one is almost tempted to say <em>victims</em>) were entirely unprepared against this kind of systematic prosecution, “utterly unprepared to do battle with the most disciplined mind in Europe”.</p>
<p>Except one.</p>
<h2>Arch-nemesis</h2>
<p>William Chaloner, counterfeiter, confidence trickster, and various things besides, the ingenious man who would one day challenge Sir Isaac Newton, had started small. He had set out from home — or had been thrown out — as a youth to apprentice under a nail-maker, where he learnt the basics of counterfeiting instead. Arriving in London with its oppressively exclusionary guild system, he somehow managed to survive, going through a series of professions including being a quack doctor, progressing to fortune teller, and then becoming a locator of stolen goods, at which he succeeded through the infallible trick of being the one to have stolen them in the first place.</p>
<p>[One of his most heinous sources of money was the following. Jacobite sedition—supporting the return of the deposed King James, over the reigning William of Orange—was treason and punishable with death, and there was a reward for those who gave up seditioners to the king. Chaloner tricked various printers into printing Jacobite propaganda, then used that as evidence to turn them in to be hanged, and claim his reward.]</p>
<p>Finally he turned to his true calling, that of counterfeiting. After coining a great deal of money (he once claimed to have produced more than thirty thousand pounds in his life, about four million pounds in today’s money) and getting caught a couple of times — once escaping conviction by turning informer, and the other time by coming up, along with his co-accused, with such a delightfully tangled mess of accusations and cross-accusations that everyone was let go out of confusion — he began to look for more safer avenues. He realised that for a man of his skill, making good counterfeit coins wasn’t the problem; having it untraceable back to him was. In an audacious plan, he realised that the safest place from which to pass his money was the Mint itself, and resolved to get into it.</p>
<p>He printed a couple of pamphlets giving advice to the government on how to prevent counterfeiting — here his expertise was all too evident — and even once gave a speech in parliament. Newton ignored him at first and denied him entry even to look at the machines in the mint, until Chaloner lost patience and decided to attack the man himself. (He alleged that the mint was making side money by participating in counterfeiting itself. The worst part was, some of these accusations were true: some dies <em>had</em> disappeared from the mint. Newton was put on trial and forced to defend himself, and nearly lost his job.)</p>
<p>Big mistake.</p>
<p>Newton was finally annoyed, and made it his goal to destroy him. Over the next two years, he devoted much of his life to ruining Chaloner’s. With customary ruthlessness, he set about accumulating evidence and witnesses. By now Chaloner was in custody again — bank notes and a Malt Lottery had just come into existence, and of course he counterfeited them — so he was out of the way. Newton got spies and informers planted in all the right places, he tracked down old contacts of Chaloner — friends, female coiners he’d had affairs with, wives of former associates — and subpoenaed (or just intimidated) them into giving testimony, anticipated who would try to flee to Scotland when, and prepared an impenetrable web of evidence. It is more complicated than that, and Chaloner still did his best from behind bars and the whole cat-and-mouse game has more details than I have any remaining patience to go into now :-), but you can read about them in the book. Chaloner was brought to trial. He tried every defence in succession, from pleading innocence to madness to pointing out (validly) that he was being tried by a Middlesex jury for crimes committed in London. He was convicted nevertheless, and after Newton ignored all the piteous mercy petitions he wrote, was  hanged, drawn and quartered.</p>
<h2>Newton’s later years</h2>
<p>In 1699 the worthless Neale finally died, and Newton became Master of the Mint on Christmas Day, his 57th birthday. The responsibilities of the job had already been <i>de facto</i> handled by Newton for years, but Neale had gained all the proceeds from the coining — 22,000 pounds. Newton now became the only recorded Warden to become Master. Although the Great Recoinage was over, the Mint still was in production, and Newton made 3500 the first year. He finally gave up his Cambridge professorship which he had still retained, went on to become genuinely rich for the first time, and seems to have led a contented life. Much later he lost 20000 pounds in the South Sea Bubble, the world’s first stock market crash — Newton is attributed to have said: “I can calculate the movement of the stars, but not the madness of men”.</p>
<p>He was knighted in 1705, the first scientist to be knighted (though possibly for political reasons rather than either his science or Mint work), and died in 1727, aged 84. Despite being one of the greatest and most influential scientists of all time, he wrote:</p>
<blockquote><p>
I don’t know what I may seem to the world, but as to myself, I seem to have been only like a boy playing on the sea shore, and diverting myself in now and then finding a smoother pebble or a prettier shell than ordinary, whilst the great ocean of truth lay all undiscovered around me.
</p></blockquote>
<p>His memorial at Westminster Abbey <s>bears</s> was proposed to bear the inscription: “If you doubt that such a man could exist, this monument bears witness”.</p>
<hr>
<p><span size="-2"><br>
More details:</span></p><p><span size="-2">
<ul>
<li>Thomas Levenson, <i><a href="http://books.google.com/books?id=3ogu56sdidsC">Newton and the counterfeiter: the unknown detective career of the world’s greatest scientist</a></i>, 2009. (Full disclosure: Levenson works at MIT)</li>
<li><a href="http://en.wikipedia.org/wiki/William_Chaloner">Wikipedia article on William Chaloner</a></li>
</ul>
</span></p><p><span size="-2">Others I haven’t seen or read:<br>
<a href="http://mitworld.mit.edu/video/717">Talk by Thomas Levenson, author of the book (Running Time: 1:03:30)</a><br>
<a href="http://www.guardian.co.uk/books/2009/aug/16/newton-counterfeiter-thomas-levenson">Book review in The Guardian</a><br>
<a href="http://www.telegraph.co.uk/culture/books/6022360/Newton-and-the-Counterfeiter-By-Thomas-Levenson-review.html">Book review in The Telegraph</a><br>
<a href="http://www.powells.com/review/2009_07_09.html">Book review in Powell’s books</a><br>
<a href="http://www.guardian.co.uk/books/2009/sep/05/newton-detective-thomas-levenson-review">Another book review in The Guardian</a><br>
<a href="http://www.npr.org/templates/story/story.php?storyId=105012144">Story in NPR radio [23 min 23 sec]</a><br>
<a href="http://www.executedtoday.com/2009/03/22/1699-william-chaloner-isaac-newton-counterfeiter/">Post by Levenson at Executed Today</a><br>
<a href="http://chicagoboyz.net/archives/13266.html">Long review/book abridgement! at Chicago Boyz</a><br>
Also:<br>
The author has a <a href="http://inversesquare.wordpress.com/">blog</a><br>
</span></p>

				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[When MFA isn't MFA, or how we got phished (283 pts)]]></title>
            <link>https://retool.com/blog/mfa-isnt-mfa/</link>
            <guid>37500895</guid>
            <pubDate>Wed, 13 Sep 2023 19:48:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://retool.com/blog/mfa-isnt-mfa/">https://retool.com/blog/mfa-isnt-mfa/</a>, See on <a href="https://news.ycombinator.com/item?id=37500895">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="site-main">

        <article>

            

            

            <section>
                <img src="https://retool.com/blog/content/images/2023/09/1611555539954.jpeg" alt="Snir Kodesh">
                
                <div>
                    <p datetime="2023-09-13">13 September 2023</p>
                    <p><span>•</span></p>
                    <p>6 min read</p>
                </div>
            </section>


            <div>
                    <p>On August 29, 2023, Retool notified 27 cloud customers that there had been unauthorized access to their accounts. If you’re reading this and you were not notified, don’t worry – your account was not impacted. <strong>There was no access to on-prem or managed accounts. </strong>Nevertheless, here’s what happened, with the hope that this will help apply the lessons we’ve learned and prevent more attacks across the industry.</p><h2 id="what-happened">What happened</h2><p>On August 27, 2023, we fell victim to a spear phishing attack. The attacker was able to navigate through multiple layers of security controls after taking advantage of one of our employees through a SMS-based phishing attack. </p><p>Several employees received targeted texts, claiming that a member of IT was reaching out about an account issue that would prevent open enrollment (which affects the employee’s healthcare coverage). The timing coincided with a recently announced migration of logins to Okta, and the message contained a url disguised to look like our internal identity portal. Almost all employees didn’t engage, but unfortunately one employee logged into the link provided by the attackers.</p><p>The following is a transcription of the message:</p><p><em>Hello A, This is B. I was trying to reach out in regards to your [payroll system] being out of sync, which we need synced for Open Enrollment, but i wasn’t able to get ahold of you. Please let me know if you have a minute. Thanks</em></p><p><em>You can also just visit https://retool.okta.com.[oauthv2.app]/authorize-client/xxx and I can double check on my end if it went through. Thanks in advance and have a good night A.</em></p><p>After logging into the fake portal – which included a MFA form – the attacker called the employee.</p><p>The caller claimed to be one of the members of the IT team, and deepfaked our employee’s actual voice. The voice was familiar with the floor plan of the office, coworkers, and internal processes of the company. Throughout the conversation, the employee grew more and more suspicious, but unfortunately did provide the attacker one additional multi-factor authentication (MFA) code. </p><p>The additional OTP token shared over the call was critical, because it allowed the attacker to add their own personal device to the employee’s Okta account, which allowed them to produce their own Okta MFA from that point forward. This enabled them to have an active GSuite session on that device. Google recently released the <a href="https://security.googleblog.com/2023/04/google-authenticator-now-supports.html?ref=retool.com">Google Authenticator synchronization feature</a> that syncs MFA codes to the cloud. As <a href="https://news.ycombinator.com/item?id=35690398&amp;ref=retool.com">Hacker News noted</a>, this is highly insecure, since if your Google account is compromised, so now are your MFA codes. </p><p>Unfortunately Google employs dark patterns to convince you to sync your MFA codes to the cloud, and our employee had indeed activated this “feature”. If you install Google Authenticator from the app store directly, and follow the suggested instructions, your MFA codes are by default saved to the cloud. If you want to disable it, there isn’t a clear way to “disable syncing to the cloud”, instead there is just a “unlink Google account” option. In our corporate Google account, there is also no way for an administrator to centrally disable Google Authenticator’s sync “feature”. We will get more into this later.</p><p>We use OTPs extensively at Retool: it’s how we authenticate into Google and Okta, how we authenticate into our internal VPN, and how we authenticate into our own internal instances of Retool. The fact that access to a Google account immediately gave access to all MFA tokens held within that account is the major reason why the attacker was able to get into our internal systems.</p><p>Getting access to this employee’s Google account therefore gave the attacker access to all their MFA codes. With these codes (and the Okta session), the attacker gained access to our VPN, and crucially, our internal admin systems. This allowed them to run an account takeover attack on a specific set of customers (all in the crypto industry). (They changed emails for users and reset passwords.) After taking over their accounts, the attacker poked around some of the Retool apps.</p><p>After learning of the attack, we immediately revoked all internal authenticated sessions (Okta, GSuite, etc) for employees, locked down access to the affected accounts, notified the affected customers, and restored their accounts to their original state (with original email addresses), reverting the 27 account takeovers. <br></p><p>As an aside, we’re glad that not a single on-premise Retool customer was affected. Retool on-prem operates in a “zero trust” environment, and doesn’t trust Retool cloud. It is fully self contained, and loads nothing from the cloud environment. This meant that although an attacker had access to Retool cloud, there was nothing they could do to affect on-premise customers. It’s worth noting that the vast majority of our crypto and larger customers in particular use Retool on-premise. <br></p><p>With the attack scope defined, let’s dig into what we learned, starting with our biggest point of vulnerability: software-based OTPs for MFA.<br></p><h2 id="beware-of-%E2%80%9Cmfa%E2%80%9D">Beware of “MFA”</h2><p>We have an internal Retool instance used to provide customer support; this is how the account takeovers were executed. The authentication for this instance happens through a VPN, SSO, and a final MFA system. A valid GSuite session alone would have been insufficient. </p><p>The fact that Google Authenticator syncs to the cloud is a novel attack vector. <strong>What we had originally implemented was multi-factor authentication. But through this Google update, what was previously multi-factor-authentication had silently (to administrators) become single single-factor-authentication</strong>, because control of the Okta account led to control of the Google account, which led to control of all OTPs stored in Google Authenticator. We strongly believe that Google should either eliminate their dark patterns in Google Authenticator (which encourages the saving of MFA codes in the cloud), or at least provide organizations with the ability to disable it. We have already passed this feedback on to Google.</p><h2 id="sharing-our-lessons">Sharing our lessons<br></h2><p><strong>Social engineering can affect anyone</strong></p><p>Social engineering is a very real and credible attack vector, and anyone can be made a target. If your company is large enough, there will be somebody who unwittingly clicks a link and gets phished. Especially as social engineering evolves (with AI and deepfakes, but also with more personal information being available on the internet), educating, preventing, and testing (via red teams) your employees regarding phishing attacks will become ever more important.</p><p><strong>Preventing systematic failures</strong></p><p>Even with perfect training and awareness of these attacks, mistakes will happen. Just like preventing a junior engineer from accidentally dropping the production database, there need to be systems in place to prevent human error from impacting the overall system. SMS as a second-factor has been rightly criticized for being too vulnerable to <a href="https://en.wikipedia.org/wiki/SIM_swap_scam?ref=retool.com">SIM swapping attacks</a>. Time-based one-time password (TOTP), while resilient to SIM swapping, is still vulnerable to the same social engineering threat vectors.</p><p>Hardware security keys– using <a href="https://fidoalliance.org/fido2/?ref=retool.com">FIDO2</a> provide resilience to these threats. A code can’t be disclosed to an attacker– since there isn’t a code to share in the first place! </p><p><strong>Defense in depth</strong></p><p>We’re equipped with many levels of protection, including multiple MFA codes, multiple lines of defense (Okta account, VPN, internal admin system, etc.). That said, technological solutions only go so far, and we believe that adding a human-in-the-loop is necessary (although again, not sufficient, given deepfakes) for important actions. We oftentimes bias towards technological solutions because they’re easy to self-serve, but ultimately, anything that can be done by an employee can be socially engineered out of that employee too. <br></p><p>We have already implemented this at Retool internally, and expect to implement this in Retool — the product — so our customers have access to building these kinds of human-in-the-loop workflows too. <br></p><p><strong>Trust as little as possible</strong></p><p>This incident only affected a very small subset of Retool cloud customers– no on-prem or managed accounts were impacted. We intentionally architected <a href="https://retool.com/self-hosted/?ref=retool.com">Retool on-prem</a> such that it doesn’t trust Retool cloud. Retool on-prem makes no contact to Retool cloud; it is fully self-hosted (the front-end, back-end, storage, etc. are all within your own VPC). This way, customers are fully in control of their security (as well as when they update), and do not need to trust Retool. Even though our cloud systems were compromised, there was no way for attackers to compromise Retool on-premise. We are fortunate we architected on-premise this way. <br></p><p>The vast majority of our customers in more sensitive industries (e.g. crypto, healthcare, finance, etc.) use our on-premise solution, and we encourage our customers to consider it, if security is important.<br></p><p><strong>Understand your threat model</strong></p><p>Retool is in the unique position of being a platform where you can build any sort of software imaginable. However, that also means that customers still need to treat building in Retool with the same care and attention to security that traditional codebases require.<br></p><p>We encourage you to understand your own threat model: if you are operating in an industry where apps have access to dangerous, irreversible actions, we strongly recommend customers to integrate further protections. For example, we encourage customers to require separate MFA tokens to execute actions (a first-class primitive in Retool), or escalation flows that require multiple employees to approve actions above certain thresholds (soon to become a first-class primitive in Retool). When we examined our forensics, customers who built secure apps and understood their threat matrix were effective at repelling the attack, even despite the account takeovers.</p><h3 id="conclusion">Conclusion<br></h3><p>This situation was challenging. It’s embarrassing for the employee, disheartening to cybersecurity professionals, and infuriating for our customers. For those reasons, these kinds of attacks are difficult to talk about, but we believe that they should be addressed in the open. There are clear risks here that the industry needs to address (e.g. the dark patterns in Google Authenticator). Our hope is that by publishing these attack vectors we can make the industry overall more aware, and enable cybersecurity professionals to harden their own systems. </p><p><em>Editor's note: We are actively working with law enforcement and a third party forensics firm.</em></p>
                </div>



        </article>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Kr8s – a Python client library for Kubernetes (105 pts)]]></title>
            <link>https://github.com/kr8s-org/kr8s</link>
            <guid>37500781</guid>
            <pubDate>Wed, 13 Sep 2023 19:38:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/kr8s-org/kr8s">https://github.com/kr8s-org/kr8s</a>, See on <a href="https://news.ycombinator.com/item?id=37500781">Hacker News</a></p>
<div id="readability-page-1" class="page"><p dir="auto">A simple, extensible Python client library for Kubernetes that feels familiar for folks who already know how to use <code>kubectl</code>.</p><div dir="auto" data-snippet-clipboard-copy-content="import kr8s

pods = kr8s.get(&quot;pods&quot;)"><pre><span>import</span> <span>kr8s</span>

<span>pods</span> <span>=</span> <span>kr8s</span>.<span>get</span>(<span>"pods"</span>)</pre></div><div dir="auto" data-snippet-clipboard-copy-content="from kr8s.objects import Pod

pod = Pod({
        &quot;apiVersion&quot;: &quot;v1&quot;,
        &quot;kind&quot;: &quot;Pod&quot;,
        &quot;metadata&quot;: {
            &quot;name&quot;: &quot;my-pod&quot;,
        },
        &quot;spec&quot;: {
            &quot;containers&quot;: [{&quot;name&quot;: &quot;pause&quot;, &quot;image&quot;: &quot;gcr.io/google_containers/pause&quot;,}]
        },
    })

pod.create()"><pre><span>from</span> <span>kr8s</span>.<span>objects</span> <span>import</span> <span>Pod</span>

<span>pod</span> <span>=</span> <span>Pod</span>({
        <span>"apiVersion"</span>: <span>"v1"</span>,
        <span>"kind"</span>: <span>"Pod"</span>,
        <span>"metadata"</span>: {
            <span>"name"</span>: <span>"my-pod"</span>,
        },
        <span>"spec"</span>: {
            <span>"containers"</span>: [{<span>"name"</span>: <span>"pause"</span>, <span>"image"</span>: <span>"gcr.io/google_containers/pause"</span>,}]
        },
    })

<span>pod</span>.<span>create</span>()</pre></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Earth beyond six of nine planetary boundaries (175 pts)]]></title>
            <link>https://www.science.org/doi/10.1126/sciadv.adh2458</link>
            <guid>37500752</guid>
            <pubDate>Wed, 13 Sep 2023 19:36:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.science.org/doi/10.1126/sciadv.adh2458">https://www.science.org/doi/10.1126/sciadv.adh2458</a>, See on <a href="https://news.ycombinator.com/item?id=37500752">Hacker News</a></p>
Couldn't get https://www.science.org/doi/10.1126/sciadv.adh2458: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Unity Engine to Godot Engine Exporter (120 pts)]]></title>
            <link>https://github.com/Zylann/unity_to_godot_converter</link>
            <guid>37499743</guid>
            <pubDate>Wed, 13 Sep 2023 18:11:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Zylann/unity_to_godot_converter">https://github.com/Zylann/unity_to_godot_converter</a>, See on <a href="https://news.ycombinator.com/item?id=37499743">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">Unity Engine to Godot Engine exporter</h2>
<p dir="auto">This is an experimental script that allows you to convert all scenes in your Unity project into a Godot project. It is not aimed at doing everything automatically, only things that can be converted decently.
It's only a proof of concept on simple 2D games for now, and a ton of work remains to be done if it were to support everything else.
While there are always cases where conversion is ambiguous and things to do manually, it's still fun to at least have the ability to automate this to some extent.</p>
<p dir="auto">I have other projects to work on so I won't work much on this tool for now, and I am aware that there is an abysmal amount of features it could support^^ But feel free to hack around with it and improve it if you like the idea.</p>
<h2 tabindex="-1" dir="auto">How to install</h2>
<p dir="auto">Copy this repository in your Unity project, inside a folder named <code>Editor</code>, and you should see a new <code>Godot</code> menu with options in it.</p>
<p dir="auto">Although it should not modify anything in the project, it's up to you to preserve your data if anything wrong happens :p</p>
<h2 tabindex="-1" dir="auto">Some challenges</h2>
<p dir="auto">Here is a random list of things I had to take choices, for which workarounds may or may not exist.
There may be a lot more, but you can get an idea of what this tool has to get through:</p>
<ul dir="auto">
<li>
<p dir="auto">Unity only has <code>Camera</code>, but Godot has <code>Camera2D</code> and <code>Camera</code> for 3D. Choosing which one is ambiguous, so for now I create the 2D version of the camera is orthographic AND if a hint is enabled in the exporter for 2D projects. Also, in Unity, cameras also act as viewports, which is another separate node in Godot, so I'm not sure how to even convert those. Other components are ambiguous too, such as <code>Light</code>.</p>
</li>
<li>
<p dir="auto">Godot has separate engines for 2D and 3D, but Unity only has 3D transforms with ortho camera. So the tool tries to guess what usage a GameObject is for by looking at its components. For example, if it has <code>SpriteRenderer</code>, or any of its children does, then the GameObject is converted to a <code>Node2D</code>. Otherwise, it becomes a <code>Spatial</code>. In some cases, it becomes a blank <code>Node</code> in cases where dimensions are irrelevant.</p>
</li>
<li>
<p dir="auto">Unity uses components attached to GameObjects for its functionality, but Godot uses a node tree directly. That means a single GameObject with several components may convert into one node and several child nodes. If a GameObject only has a Transform and one component, a shortcut is taken to only produce a single Godot node, eliminating the unnecessary nesting.</p>
</li>
<li>
<p dir="auto">Unity defines rigidbodies as components, but in Godot it is recommended to have such bodies as parent nodes because they control the position of their children, so instead of adding <code>RigidBodies</code> as a child nodes, they are have to be promoted as parent.</p>
</li>
<li>
<p dir="auto">Unity can have multiple scripts on the same GameObject, but Godot can only have one per node. So the converter takes the first script it finds to the root node, and create children <code>Nodes</code> for each additional script. You may have to have a manual look after conversion if you use composition a lot.</p>
</li>
<li>
<p dir="auto">Converting scripts is very complicated, so the tool rather creates stub scripts for each of them so it can still attach them to the proper final nodes, and attempts to preserve serialized variables. For example, when converting to GDScript, a C# script will be parsed for its variables which will be written as <code>export</code> on top of an empty GDScript, and the rest or the original source code is written as a big comment below them. This allows to keep configurations and keep track of what the script should be.</p>
</li>
<li>
<p dir="auto">In Sprite texture resources, Unity allows to define a scaling between pixel coordinates and world coordinates, which is 100 by default, making sprites very small. Godot uses pixels as units at all times, so the plugin attempts to undo this scaling.</p>
</li>
<li>
<p dir="auto">Unity can subdivide a 2D texture into sprites, so this almost always translates to Godot as <code>AtlasTextures</code>.</p>
</li>
<li>
<p dir="auto">Unity uses a left-handed coordinate system, and in 2D its Y axis stays upwards. In Godot, the Y axis in 2D is downwards, so the tool attempts to invert positions (not working as best as it could at the moment)</p>
</li>
<li>
<p dir="auto">Godot has no terrain system as of now, but a plugin exists for heightmaps which does not require recompilation. So the plugin could be packaged in the output project, and Unity terrains could be mostly converted to that format.</p>
</li>
<li>
<p dir="auto">Things requiring a recompilation of Godot cannot be supported, for example the Admob module needs to be integrated into Godot manually by recompiling the engine.</p>
</li>
<li>
<p dir="auto">Unity and Godot both support prefabs and nested prefabs, but I haven't worked in this part yet. On Unity side it should be a matter of using <code>PrefabUtility</code> to detect if a game object is actually an instance of a prefab, and it needs some research to see which delta-modifications are supported both by Unity and Godot.</p>
</li>
<li>
<p dir="auto">As of 3.1 Godot only saves non-default values in scene data, but this tool can't afford to know them all, so scenes generated by it may be larger than if you had created them in Godot. Saving them from Godot might get rid of the redundancy.</p>
</li>
<li>
<p dir="auto">Unity can imports 3D models as "fixed" prefabs, a bit like Godot does, so I am not sure if the tool should generate scenes for those, or let Godot do it</p>
</li>
</ul>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Unity silently removed clause that let you use TOS from version you shipped with (137 pts)]]></title>
            <link>https://old.reddit.com/r/gamedev/comments/16hnibp/unity_silently_removed_their_github_repo_to_track/</link>
            <guid>37499731</guid>
            <pubDate>Wed, 13 Sep 2023 18:09:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/gamedev/comments/16hnibp/unity_silently_removed_their_github_repo_to_track/">https://old.reddit.com/r/gamedev/comments/16hnibp/unity_silently_removed_their_github_repo_to_track/</a>, See on <a href="https://news.ycombinator.com/item?id=37499731">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>After their previous controversy with license changes, in 2019, after disagreements with Improbable, unity updated their Terms of Service, with the following statement:</p>

<blockquote>
<p>When you obtain a version of Unity, and don’t upgrade your project, we think you should be able to stick to that version of the TOS. </p>
</blockquote>

<p>As part of their "commitment to being an open platform", they made a Github repository, that tracks changes to the unity terms to "give developers full transparency about what changes are happening, and when"</p>

<p>Well, sometime around June last year, they silently deleted that Github repo.</p>

<p>April 3rd this year (slightly before the release of 2022 LTS in June), they updated their terms of service to remove the clause that was added after the 2019 controversy. That clause was as follows:</p>

<blockquote>
<p>Unity may update these Unity Software Additional Terms at any time for any reason and without notice (the “Updated Terms”) and those Updated Terms will apply to the most recent current-year version of the Unity Software, provided that, if the Updated Terms adversely impact your rights, <strong>you may elect to continue to use any current-year versions of the Unity Software</strong> (e.g., 2018.x and 2018.y and any Long Term Supported (LTS) versions for that current-year release) according to the terms that applied just prior to the Updated Terms (the “Prior Terms”). The Updated Terms will then not apply to your use of those current-year versions unless and until you update to a subsequent year version of the Unity Software (e.g. from 2019.4 to 2020.1). If material modifications are made to these Terms, Unity will endeavor to notify you of the modification.</p>
</blockquote>

<p>This clause is completely missing in the new terms of service. </p>

<p>This, along with unitys claim that "the fee applies to eligible games currently in market that continue to distribute the runtime." flies in the face of their previous annoucement of "full transparency". They're now expecting people to trust their questionable metrics on user installs, that are rife for abuse, but how can users trust them after going this far to burn all goodwill?</p>

<p>​</p>

<p>They've purposefully removed the repo that shows license changes, removed the clause that means you could avoid future license changes, then changed the license to add additional fees retroactively, with no way to opt-out. After this behaviour, are we meant to trust they won't increase these fees, or add new fees in the future?  </p>

<p>​</p>

<p>I for one, do not.</p>

<p>​</p>

<p>Sources:</p>

<p>"Updated Terms of Service and commitment to being an open platform" <a href="https://blog.unity.com/community/updated-terms-of-service-and-commitment-to-being-an-open-platform">https://blog.unity.com/community/updated-terms-of-service-and-commitment-to-being-an-open-platform</a></p>

<p>Github repo to track the license changes: <a href="https://github.com/Unity-Technologies/TermsOfService">https://github.com/Unity-Technologies/TermsOfService</a></p>

<p>Last archive of the license repo: <a href="https://web.archive.org/web/20220716084623/https://github.com/Unity-Technologies/TermsOfService">https://web.archive.org/web/20220716084623/https://github.com/Unity-Technologies/TermsOfService</a></p>

<p>New terms of service: <a href="https://unity.com/legal/editor-terms-of-service/software">https://unity.com/legal/editor-terms-of-service/software</a></p>

<p>Old terms of service: <a href="https://unity.com/legal/terms-of-service/software-legacy">https://unity.com/legal/terms-of-service/software-legacy</a></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Marvel Visual Effects Workers Vote to Unionize (130 pts)]]></title>
            <link>https://www.rollingstone.com/tv-movies/tv-movie-news/marvel-visual-effects-workers-vote-unionize-1234824215/</link>
            <guid>37499720</guid>
            <pubDate>Wed, 13 Sep 2023 18:08:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.rollingstone.com/tv-movies/tv-movie-news/marvel-visual-effects-workers-vote-unionize-1234824215/">https://www.rollingstone.com/tv-movies/tv-movie-news/marvel-visual-effects-workers-vote-unionize-1234824215/</a>, See on <a href="https://news.ycombinator.com/item?id=37499720">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

					

					


<p>The first-of-its kind shop will now enter in collective bargaining negotiations with the studio</p>
				</div><div><p>
	<span><a href="https://www.rollingstone.com/t/visual-effects/" id="auto-tag_visual-effects" data-tag="visual-effects">Visual effects</a> workers</span>  at <a href="https://www.rollingstone.com/t/marvel/" id="auto-tag_marvel" data-tag="marvel">Marvel</a> Studios have voted to form a <a href="https://www.rollingstone.com/tv-movies/tv-movie-features/marvel-visual-effects-workers-union-fight-wandavision-black-panther-1234807406/">first-of-its-kind union</a> with the International Alliance of Theatrical Stage Employees.&nbsp;</p>



<p>
	The Marvel VFX shop not only enjoyed a historic victory, but a landslide one as well: All votes cast in the election — held by mail between Aug. 21 and Sept. 11 — were in favor of unionizing, with none against. Anna George, an assistant coordinator at Marvel, spoke about watching the votes get tallied yesterday, saying, “It was so emotional hearing the yes’s and knowing we were fighting for what we deserve. What a powerful moment!”</p>



<p>
	Sarah Kazuko Chow, a VFX coordinator at Marvel, added: “I grew up dreaming of working on Marvel films, so when I started my first job at Marvel, I felt like I couldn’t complain about the unpaid overtime, the lack of meal breaks, and the incredible pressure put on VFX teams to meet deadlines because I was just supposed to be grateful to be here at all. But the reality is that every worker deserves rights, and joining IATSE means we don’t have to choose between the job we love and having identities outside of our work.”

	</p>




<p>
	Following the successful vote with the NLRB, Marvel’s VFX union will now enter into collective bargaining negotiations with Marvel. A start date for those negotiations hasn’t been announced yet. &nbsp;</p>



<p>
	A rep for Marvel and its parent company Disney did not immediately return <em>Rolling Stone</em>’s request for comment.</p>



<p>
	Underpinning the union drive were the poor working conditions visual effects professionals have endured on Marvel productions, including a lack of pay equity, grueling hours, understaffing, excessive requests for changes, and unfair turnaround times.&nbsp;</p>



<p>
	Mark Patch, a former VFX worker at Marvel who now organizes VFX crews for IATSE, recently told <em>Rolling Stone</em>, “We’ve seen [that] the lack of guardrails and protections has created an environment where the employer has unlimited control of our whole lives.”&nbsp;


</p><section>
			

	<h2 id="section-heading">

	
		Trending
	
	</h2>

	
	
</section>




<p>
	VFX crews have been a crucial part of film and TV productions since the introduction of visual effects in the first <em>Star Wars </em>films of the 1970s. But while many other backstage/behind-the-scenes crews and professions (such as production designers, editors, lighting, make-up, and props) have long been unionized under the IATSE umbrella, VFX workers largely remained non-union.&nbsp;</p>



<p>
	Marvel’s VFX crew was the first to break that barrier with the announcement last month that they planned to hold a union vote. They were quickly followed by their <a href="https://www.rollingstone.com/tv-movies/tv-movie-news/walt-disney-visual-effects-vfx-crew-file-to-unionize-1234813567/">colleagues at Walt Disney Pictures</a>, who filed for a vote with the NLRB at the end of August (the results of that election are expected on Oct. 2).&nbsp;These union drives also pointedly come amidst a historic dual strike for two of Hollywood’s most prominent unions, the <a href="https://www.rollingstone.com/tv-movies/tv-movie-news/writers-strike-wga-labor-day-message-chris-keyser-1234818024/">Writers Guild of America</a> and the <a href="https://www.rollingstone.com/tv-movies/tv-movie-pictures/actors-strike-hollywood-stars-picket-lines-1234800423/">Screen Actors Guild</a>.</p>
















</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Unity to Godot Docs (103 pts)]]></title>
            <link>https://docs.godotengine.org/en/3.1/getting_started/editor/unity_to_godot.html</link>
            <guid>37499708</guid>
            <pubDate>Wed, 13 Sep 2023 18:07:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://docs.godotengine.org/en/3.1/getting_started/editor/unity_to_godot.html">https://docs.godotengine.org/en/3.1/getting_started/editor/unity_to_godot.html</a>, See on <a href="https://news.ycombinator.com/item?id=37499708">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
    <nav data-toggle="wy-nav-shift">
      
    </nav>

    <section data-toggle="wy-nav-shift">

      
      <nav aria-label="top navigation">
        
          <i data-toggle="wy-nav-top"></i>
          <a href="https://docs.godotengine.org/en/3.1/index.html">Godot Engine</a>
        
      </nav>


      <div id="from-unity-to-godot-engine" itemprop="articleBody" role="main" itemscope="itemscope" itemtype="http://schema.org/Article">

<p>This guide provides an overview of Godot Engine from the viewpoint of a Unity user,
and aims to help you migrate your existing Unity experience into the world of Godot.</p>
<div>
<p>Note</p>
<p>This article talks about older versions of Unity. Nestable prefabs (‘Nested prefabs’) were added to Unity 2018.3. Nestable prefabs are analogous to Godot’s scenes, and allow a more Godot-like approach to scene organisation.</p>
</div>
<div id="differences">
<h2>Differences<a href="#differences" title="Permalink to this headline">¶</a></h2>
<table>
<colgroup>
<col width="9%">
<col width="39%">
<col width="52%">
</colgroup>
<thead>
<tr><th>&nbsp;</th>
<th>Unity</th>
<th>Godot</th>
</tr>
</thead>
<tbody>
<tr><td>License</td>
<td>Proprietary, closed, free license with revenue caps and usage restrictions</td>
<td>MIT license, free and fully open source without any restriction</td>
</tr>
<tr><td>OS (editor)</td>
<td>Windows, macOS, Linux (unofficial and unsupported)</td>
<td>Windows, macOS, X11 (Linux, *BSD)</td>
</tr>
<tr><td>OS (export)</td>
<td><ul>
<li><strong>Desktop:</strong> Windows, macOS, Linux</li>
<li><strong>Mobile:</strong> Android, iOS, Windows Phone, Tizen</li>
<li><strong>Web:</strong> WebAssembly or asm.js</li>
<li><strong>Consoles:</strong> PS4, PS Vita, Xbox One, Xbox 360, Wii U, Nintendo 3DS</li>
<li><strong>VR:</strong> Oculus Rift, SteamVR, Google Cardboard, Playstation VR, Gear VR, HoloLens</li>
<li><strong>TV:</strong> Android TV, Samsung SMART TV, tvOS</li>
</ul>
</td>
<td><ul>
<li><strong>Desktop:</strong> Windows, macOS, X11</li>
<li><strong>Mobile:</strong> Android, iOS</li>
<li><strong>Web:</strong> WebAssembly</li>
<li><strong>Console:</strong> See <a href="https://docs.godotengine.org/en/3.1/tutorials/platform/consoles.html#doc-consoles"><span>Console support in Godot</span></a></li>
<li><strong>VR:</strong> Oculus Rift, SteamVR</li>
</ul>
</td>
</tr>
<tr><td>Scene system</td>
<td><ul>
<li>Component/Scene (GameObject &gt; Component)</li>
<li>Prefabs</li>
</ul>
</td>
<td><a href="https://docs.godotengine.org/en/3.1/getting_started/step_by_step/scenes_and_nodes.html#doc-scenes-and-nodes"><span>Scene tree and nodes</span></a>, allowing scenes to be nested and/or inherit other scenes</td>
</tr>
<tr><td>Third-party tools</td>
<td>Visual Studio or VS Code</td>
<td><ul>
<li><a href="https://docs.godotengine.org/en/3.1/getting_started/editor/external_editor.html#doc-external-editor"><span>External editors are possible</span></a></li>
<li><a href="https://docs.godotengine.org/en/3.1/getting_started/workflow/export/exporting_for_android.html#doc-exporting-for-android"><span>Android SDK for Android export</span></a></li>
</ul>
</td>
</tr>
<tr><td>Notable advantages</td>
<td><ul>
<li>Huge community</li>
<li>Large assets store</li>
</ul>
</td>
<td><ul>
<li>Scene System</li>
<li><a href="https://docs.godotengine.org/en/3.1/getting_started/step_by_step/animations.html#doc-animations"><span>Animation Pipeline</span></a></li>
<li><a href="https://docs.godotengine.org/en/3.1/tutorials/shading/shading_reference/shading_language.html#doc-shading-language"><span>Easy to write Shaders</span></a></li>
<li>Debug on Device</li>
</ul>
</td>
</tr>
</tbody>
</table>
</div>
<div id="the-editor">
<h2>The editor<a href="#the-editor" title="Permalink to this headline">¶</a></h2>
<p>Godot Engine provides a rich-featured editor that allows you to build your games.
The pictures below display the default layouts of both editors with colored blocks to indicate common functionalities.</p>
<p><img alt="../../_images/unity-gui-overlay.png" src="https://docs.godotengine.org/en/3.1/_images/unity-gui-overlay.png">
<img alt="../../_images/godot-gui-overlay.png" src="https://docs.godotengine.org/en/3.1/_images/godot-gui-overlay.png"></p><p>While both editors may seem similar, there are many differences below the surface.
Both let you organize the project using the filesystem,
but Godot’s approach is simpler with a single configuration file, minimalist text format,
and no metadata. This makes Godot more friendly to VCS systems, such as Git, Subversion, or Mercurial.</p>
<p>Godot’s Scene panel is similar to Unity’s Hierarchy panel but, as each node has a specific function,
the approach used by Godot is more visually descriptive. It’s easier to understand
what a scene does at a glance.</p>
<p>The Inspector in Godot is more minimal, it shows only properties.
Thanks to this, objects can expose more useful parameters to the user
without having to hide functionality in language APIs. As a plus, Godot allows animating any of those properties visually.
Changing colors, textures, enumerations, or even links to resources in real-time is possible without needing to write code.</p>
<p>The Toolbar at the top of the screen is similar in both editors, offering control over project playback.
Projects in Godot run in a separate window, rather than inside the editor
(but the tree and objects can still be explored in the debugger window).</p>
<p>This approach has the disadvantage that in Godot the running game can’t be explored from different angles
(though this may be supported in the future and displaying collision gizmos in the running game is already possible),
but in exchange has several advantages:</p>
<ul>
<li>Running the project and closing it is fast (Unity has to save, run the project, close the project, and then reload the previous state).</li>
<li>Live editing is a lot more useful because changes done to the editor take effect immediately in the game and are not lost (nor have to be synced) when the game is closed. This allows fantastic workflows, like creating levels while you play them.</li>
<li>The editor is more stable because the game runs in a separate process.</li>
</ul>
<p>Finally, Godot’s top toolbar includes a menu for remote debugging.
These options allow deployment to a device (connected phone, tablet, or browser via HTML5),
and debugging/live editing on it after the game is exported.</p>
</div>
<div id="the-scene-system">
<h2>The scene system<a href="#the-scene-system" title="Permalink to this headline">¶</a></h2>
<p>This is the most important difference between Unity and Godot and the favourite feature of most Godot users.</p>
<p>Working on a ‘level’ in Unity usually means embedding all the required assets in a scene
and linking them together with components and scripts.</p>
<p>Godot’s scene system is superficially similar to Unity. A ‘level’ consists of a collection of nodes, each with its own purpose: Sprite, Mesh, Light, etc. However, in Godot the nodes are arranged in a tree. Each node can have multiple children, which makes each a subscene of the main scene.
This means you can compose a whole scene with different scenes stored in different files.</p>
<p>For example, think of a platformer level. You would compose it with multiple elements:</p>
<ul>
<li>Bricks</li>
<li>Coins</li>
<li>The player</li>
<li>The enemies</li>
</ul>
<p>In Unity, you would put all the GameObjects in the scene: the player, multiple instances of enemies,
bricks everywhere to form the ground of the level and then multiple instances of coins all over the level.
You would then add various components to each element to link them and add logic in the level: For example,
you’d add a BoxCollider2D to all the elements of the scene so that they can collide. This principle is different in Godot.</p>
<p>In Godot, you would split your whole scene into three separate, smaller scenes, and instance them in the main scene.</p>
<ol>
<li><strong>A scene for the Player alone.</strong></li>
</ol>
<p>Consider the player as an element we’d like to use in different parent scenes (for instance ‘level’ scenes). In our case, the player element needs at least an AnimatedSprite node. This node contains the sprite textures necessary for various animations (for example, a walking animation).</p>
<ol start="2">
<li><strong>A scene for the Enemy.</strong></li>
</ol>
<p>An enemy is also an element we’d like to use in several scenes. It’s almost the same
as the Player node. The only differences are the script (it needs ‘AI’ routines to generate the enemy’s behaviour)
and the sprite textures used by the AnimatedSprite node.</p>
<ol start="3">
<li><strong>A Level scene.</strong></li>
</ol>
<p>A Level scene is composed of Bricks (for platforms), Coins (for the player to collect) and a
number of instances of the Enemy scene. Each instance is a node in the Level scene tree. These instances are separate enemies,
which initially have shared behaviour and appearance as defined in the Enemy scene. You can set different properties for each Enemy node (to change its color, for example).</p>
<p>4. <strong>A Main scene.</strong>
The Main scene would be composed of one root node with 2 children: a Player instance node, and a Level instance node.
The root node can be anything, generally a “root” type such as “Node” which is the most global type,
or “Node2D” (root type of all 2D-related nodes), “Spatial” (root type of all 3D-related nodes) or
“Control” (root type of all GUI-related nodes).</p>
<p>As you can see, every scene is organized as a tree. The same goes for nodes’ properties: you don’t <em>add</em> a
collision component to a node to make it collidable like Unity does. Instead, you make this node a <em>child</em> of a
new specific node that has collision properties. Godot features various collision types nodes, depending on the usage
(see the <a href="https://docs.godotengine.org/en/3.1/tutorials/physics/physics_introduction.html#doc-physics-introduction"><span>Physics introduction</span></a>).</p>
<ul>
<li><p>What are the advantages of this system? Wouldn’t this system potentially increase the depth of the scene tree? And doesn’t Unity already allow you to organize GameObjects by putting them inside empty GameObjects?</p>
<blockquote>
<div><ul>
<li>Godot’s system is closer to the well-known object-oriented paradigm: Godot provides a number of nodes which are not clearly “Game Objects”, but they provide their children with their own capabilities: this is inheritance.</li>
<li>Godot allows the extraction of a subtree of a scene to make it a scene of its own. So if a scene tree gets too deep, it can be split into smaller subtrees. This is better for reusability, as you can include any subtree as a child of any node. Putting multiple GameObjects in an empty GameObject in Unity does not provide the same functionality.</li>
</ul>
</div></blockquote>
</li>
</ul>
</div>
<div id="project-organization">
<h2>Project organization<a href="#project-organization" title="Permalink to this headline">¶</a></h2>
<p><img alt="../../_images/unity-project-organization-example.png" src="https://docs.godotengine.org/en/3.1/_images/unity-project-organization-example.png"></p><p>There is no perfect project architecture.
Any architecture can be made to work in either Unity and Godot.</p>
<p>However, a common architecture for Unity projects is to have one Assets folder in the root directory
that contains various folders, one per type of asset: Audio, Graphics, Models, Materials, Scripts, Scenes, and so on.</p>
<p>Since Godot allows splitting scenes into smaller scenes, each scene and subscene existing as a file in the project, we recommend organizing your project a bit differently.
This wiki provides a page for this: <a href="https://docs.godotengine.org/en/3.1/getting_started/workflow/project_setup/project_organization.html#doc-project-organization"><span>Project organization</span></a>.</p>
</div>
<div id="where-are-my-prefabs">
<h2>Where are my prefabs?<a href="#where-are-my-prefabs" title="Permalink to this headline">¶</a></h2>
<p>A prefab as provided by Unity is a ‘template’ element of the scene.
It is reusable, and each instance of the prefab that exists in the scene has an existence of its own,
but all of them have the same properties as defined by the prefab.</p>
<p>Godot does not provide prefabs as such, but the same functionality is provided by its scene system:
The scene system is organized as a tree. Godot allows you to save any subtree of a scene as a scene file. This new scene can then be instanced as many times as you want, as a child of any node.
Any change you make to this new, separate scene will be applied to its instances.
However, any change you make to the instance will not have any impact on the ‘template’ scene.</p>
<p><img alt="../../_images/save-branch-as-scene.png" src="https://docs.godotengine.org/en/3.1/_images/save-branch-as-scene.png"></p><p>To be precise, you can modify the parameters of an instance in the Inspector panel.
The nodes that compose this instance are initially locked. You can unlock them if you need to by
right-clicking the instance in the Scene tree and selecting “Editable children” in the menu.
You don’t need to do this to add <em>new</em> child nodes to this node.
Remember that any new children will belong to the instance, not to the ‘template’ scene on disk.
If you want to add new children to every instance of your ‘template’ scene, then you should add them in the ‘template’ scene.</p>
<p><img alt="../../_images/editable-children.png" src="https://docs.godotengine.org/en/3.1/_images/editable-children.png">
</p></div>
<div id="glossary-correspondence">
<h2>Glossary correspondence<a href="#glossary-correspondence" title="Permalink to this headline">¶</a></h2>
<ul>
<li>GameObject -&gt; Node</li>
<li>Add a component -&gt; Inheriting</li>
<li>Prefab -&gt; Reusable Scene file</li>
</ul>
</div>
<div id="scripting-gdscript-c-and-visual-script">
<h2>Scripting: GDScript, C# and Visual Script<a href="#scripting-gdscript-c-and-visual-script" title="Permalink to this headline">¶</a></h2>
<div id="design">
<h3>Design<a href="#design" title="Permalink to this headline">¶</a></h3>
<p>Unity supports C#. C# benefits from its integration with Visual Studio and has desirable features such as static typing.</p>
<p>Godot provides its own scripting language, <a href="https://docs.godotengine.org/en/3.1/getting_started/step_by_step/scripting.html#doc-scripting"><span>GDScript</span></a> as well as support
for <a href="https://docs.godotengine.org/en/3.1/getting_started/scripting/visual_script/index.html#toc-learn-scripting-visual-script"><span>Visual Script</span></a> and <a href="https://docs.godotengine.org/en/3.1/getting_started/scripting/c_sharp/c_sharp_basics.html#doc-c-sharp"><span>C#</span></a>.
GDScript borrows its syntax from Python, but is not related to it. If you wonder about the reasoning for a custom scripting language,
please read the <a href="https://docs.godotengine.org/en/3.1/getting_started/scripting/gdscript/gdscript_basics.html#doc-gdscript"><span>GDScript basics</span></a> and <a href="https://docs.godotengine.org/en/3.1/about/faq.html#doc-faq"><span>Frequently asked questions</span></a> pages. GDScript is strongly attached to the Godot API
and doesn’t take long to learn: Between one evening for an experienced programmer and a week for a complete beginner.</p>
<p>Unity allows you to attach as many scripts as you want to a GameObject.
Each script adds a behaviour to the GameObject: For example, you can attach a script so that it reacts to the player’s controls,
and another that controls its specific game logic.</p>
<p>In Godot, you can only attach one script per node. You can use either an external GDScript file
or include the script directly in the node. If you need to attach more scripts to one node, then you may consider two solutions,
depending on your scene and on what you want to achieve:</p>
<ul>
<li>either add a new node between your target node and its current parent, then add a script to this new node.</li>
<li>or, you can split your target node into multiple children and attach one script to each of them.</li>
</ul>
<p>As you can see, it can be easy to turn a scene tree to a mess. Consider splitting any complicated scene into multiple, smaller branches.</p>
</div>
<div id="connections-groups-and-signals">
<h3>Connections: groups and signals<a href="#connections-groups-and-signals" title="Permalink to this headline">¶</a></h3>
<p>You can control nodes by accessing them via script and calling built-in
or user-defined functions on them. You can also place nodes in a group
and call functions on all nodes in this group. See more in the
<a href="https://docs.godotengine.org/en/3.1/getting_started/step_by_step/scripting_continued.html#doc-scripting-continued"><span>scripting documentation</span></a>.</p>
<p>Nodes can send a signal when a specified action occurs. A signal can
be set to call any function. You can define custom signals and specify
when they are triggered. See more in the <a href="https://docs.godotengine.org/en/3.1/getting_started/scripting/gdscript/gdscript_basics.html#doc-gdscript-signals"><span>signals documentation</span></a>.</p>
</div>
<div id="script-serialization">
<h3>Script serialization<a href="#script-serialization" title="Permalink to this headline">¶</a></h3>
<p>Unity can handle script serialization in two ways:</p>
<ul>
<li>Implicit: All public fields in a class are automatically serialized if the type is a serializable type (<code><span>Dictionary</span></code> is not serializable).</li>
<li>Explicit: Non-public fields can be serialized using the <code><span>[SerializeField]</span></code> attribute.</li>
</ul>
<p>Godot also has a built-in script serialization system, but it works only explicitly.
You can serialize any serializable type (<a href="https://docs.godotengine.org/en/3.1/tutorials/misc/binary_serialization_api.html#doc-binary-serialization-api"><span>built-in and various engine types</span></a>,
including <a href="https://docs.godotengine.org/en/3.1/classes/class_array.html#class-array"><span>Array</span></a> and <a href="https://docs.godotengine.org/en/3.1/classes/class_dictionary.html#class-dictionary"><span>Dictionary</span></a>) using the <code><span>export</span></code> keyword.
See the <a href="https://docs.godotengine.org/en/3.1/getting_started/scripting/gdscript/gdscript_basics.html#doc-gdscript-exports"><span>exports documentation</span></a> for details.</p>
<p>Unity also has a data type called <code><span>ScriptableObject</span></code> used to serialize custom asset objects.
Its equivalent in Godot is the base class for all resources: <a href="https://docs.godotengine.org/en/3.1/classes/class_resource.html#class-resource"><span>Resource</span></a>.
Creating a script that inherits <a href="https://docs.godotengine.org/en/3.1/classes/class_resource.html#class-resource"><span>Resource</span></a> will allow you to create custom serializable objects. More information about resources can be found <a href="https://docs.godotengine.org/en/3.1/getting_started/step_by_step/resources.html#doc-resources"><span>here</span></a>.</p>
</div>
</div>
<div id="using-godot-in-c">
<h2>Using Godot in C++<a href="#using-godot-in-c" title="Permalink to this headline">¶</a></h2>
<p>Godot allows you to develop your project directly in C++ by using its API, which is not possible with Unity at the moment.
As an example, you can consider Godot Engine’s editor as a “game” written in C++ using the Godot API.</p>
<p>If you are interested in using Godot in C++, you may want to start reading the <a href="https://docs.godotengine.org/en/3.1/development/cpp/introduction_to_godot_development.html#doc-introduction-to-godot-development"><span>Developing in
C++</span></a> page.</p>
</div>
</div>

    </section>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CIA bribed its own team to reject lab-leak theory, whistleblower claims (106 pts)]]></title>
            <link>https://www.science.org/content/article/cia-bribed-its-own-covid-19-origin-team-reject-lab-leak-theory-anonymous-whistleblower</link>
            <guid>37499504</guid>
            <pubDate>Wed, 13 Sep 2023 17:52:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.science.org/content/article/cia-bribed-its-own-covid-19-origin-team-reject-lab-leak-theory-anonymous-whistleblower">https://www.science.org/content/article/cia-bribed-its-own-covid-19-origin-team-reject-lab-leak-theory-anonymous-whistleblower</a>, See on <a href="https://news.ycombinator.com/item?id=37499504">Hacker News</a></p>
Couldn't get https://www.science.org/content/article/cia-bribed-its-own-covid-19-origin-team-reject-lab-leak-theory-anonymous-whistleblower: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Lantern – a PostgreSQL vector database for building AI applications (162 pts)]]></title>
            <link>https://docs.lantern.dev/blog/2023/09/13/hello-world</link>
            <guid>37499375</guid>
            <pubDate>Wed, 13 Sep 2023 17:41:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://docs.lantern.dev/blog/2023/09/13/hello-world">https://docs.lantern.dev/blog/2023/09/13/hello-world</a>, See on <a href="https://news.ycombinator.com/item?id=37499375">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__blog-post-container" itemprop="articleBody"><h2 id="-tldr">📌 TL;DR<a href="#-tldr" aria-label="Direct link to 📌 TL;DR" title="Direct link to 📌 TL;DR">​</a></h2><p>Lantern is a PostgreSQL vector database extension for building AI applications. Install and use our extension <strong><a href="https://github.com/lanterndata/lantern" target="_blank" rel="noopener noreferrer">here</a></strong>.</p><h2 id="-features-today--coming-soon">🚀 Features today + Coming soon<a href="#-features-today--coming-soon" aria-label="Direct link to 🚀 Features today + Coming soon" title="Direct link to 🚀 Features today + Coming soon">​</a></h2><p>We have the most complete feature set of all the PostgreSQL vector database extensions.</p><p><em><strong>Here’s what we support today:</strong></em></p><ul><li>Creating an AI application end to end without leaving your database (<a href="https://github.com/ezra-varady/lanterndb-semantic-image-search" target="_blank" rel="noopener noreferrer">example</a>)</li><li>Embedding generation for popular use cases (CLIP model, Hugging Face models, custom model)</li><li>Interoperability with pgvector's data type, so anyone using pgvector can switch to Lantern</li><li>Parallel index creation capabilities -- Support for creating the index outside of the database and inside another instance allows you to create an index without interrupting database workflows.</li></ul><p><em><strong>Here’s what’s coming soon:</strong></em></p><ul><li>Cloud-hosted version of Lantern</li><li>Templates and guides for building applications for different industries</li><li>Tools for generating embeddings (support for third party model API's, more local models)</li><li>Support for version control and A/B test embeddings</li><li>Autotuned index type that will choose appropriate index creation parameters</li><li>1 byte and 2 byte vector elements, and up to 8000 dimensional vectors support</li><li>Request a feature at <a href="mailto:support@lantern.dev" target="_blank" rel="noopener noreferrer">support@lantern.dev</a></li></ul><h2 id="-performance--benchmarks">📈 Performance + Benchmarks<a href="#-performance--benchmarks" aria-label="Direct link to 📈 Performance + Benchmarks" title="Direct link to 📈 Performance + Benchmarks">​</a></h2><p>Lantern is a PostgreSQL extension that creates an index to efficiently search for similar vectors.</p><p><em><strong>Important takeaways:</strong></em></p><ul><li>There's three key metrics we track. <code>CREATE INDEX</code> time, <code>SELECT</code> throughput, and <code>SELECT</code> latency.</li><li>We match or outperform <code>pgvector</code> and <code>pg_embedding</code> (Neon) on all of these metrics.</li><li>We plan to continue to make performance improvements to ensure we are the best performing database.</li></ul><p><img loading="lazy" alt="Throughput" src="https://docs.lantern.dev/assets/images/throughput-136291481b16945501e8412f5360597c.png" width="5060" height="4400"></p><p><img loading="lazy" alt="Latency" src="https://docs.lantern.dev/assets/images/latency-f0d55396252093ebb9d2b98d83d0b884.png" width="5060" height="4400"></p><p><img loading="lazy" alt="Index Creation" src="https://docs.lantern.dev/assets/images/create-08f5562d7369db6b0962df276fe0e7f8.png" width="5060" height="4400"></p><p>Our database is built on top of usearch — a state of the art implementation of HNSW, the most scalable and performant algorithm for handling vector search.</p><h2 id="-why-we-started-lantern">🌱 Why we started Lantern<a href="#-why-we-started-lantern" aria-label="Direct link to 🌱 Why we started Lantern" title="Direct link to 🌱 Why we started Lantern">​</a></h2><p>Today, there's dozens of vector databases on the market, but only TWO are built on top of PostgreSQL.</p><p><em><strong>We think it's super important to build on top of PostgreSQL</strong></em></p><ul><li>Developers know how to use PostgreSQL.</li><li>Companies already store their data on PostgreSQL.</li><li>Standalone vector databases have to rebuild all of what PostgreSQL has built for the past 30-years, including all of the optimizations on how to best store and access data.</li></ul><p>Lantern is building the most performant vector database and the best suite of tools to help developers build AI applications.</p><p>We want to help companies build useful applications using their unstructured and structured data.</p><h2 id="-asks--offers-free-airpods--advice">🎁 Asks + Offers (FREE AirPods + advice)<a href="#-asks--offers-free-airpods--advice" aria-label="Direct link to 🎁 Asks + Offers (FREE AirPods + advice)" title="Direct link to 🎁 Asks + Offers (FREE AirPods + advice)">​</a></h2><p><em><strong>Send us feedback + report bugs</strong></em></p><ul><li>Please try our extension! We expect some bugs in production, since we’re new, but we promise to patch them very quickly</li></ul><p><em><strong>Switch from pgvector, get FREE AirPods Pro</strong></em></p><ul><li>If you’re already using pgvector in production for your business, switching to Lantern is very easy</li><li>Book some time <strong><a href="https://calendly.com/narek-lantern/20min" target="_blank" rel="noopener noreferrer">here</a></strong>, and we will help switch you over for FREE and get you a pair of FREE AirPods Pro</li></ul><p><em><strong>Want to build AI applications and don’t know where to start?</strong></em></p><ul><li>Book some time <strong><a href="https://calendly.com/narek-lantern/advice-for-building-ai-applications" target="_blank" rel="noopener noreferrer">here</a></strong>, we will meet for FREE and help you get set up on Lantern</li></ul><p><em><strong>Want to contribute or join the team?</strong></em></p><ul><li>Reach out to <a href="mailto:support@lantern.dev" target="_blank" rel="noopener noreferrer">support@lantern.dev</a>, we can find an open issue or project that suits you</li><li>We are also hiring full time engineers. Send your resume to <a href="mailto:support@lantern.dev" target="_blank" rel="noopener noreferrer">support@lantern.dev</a>!</li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Unity’s new “per-install” pricing enrages the game development community (118 pts)]]></title>
            <link>https://arstechnica.com/gaming/2023/09/game-developers-unite-against-unitys-new-per-install-pricing-structure/</link>
            <guid>37499259</guid>
            <pubDate>Wed, 13 Sep 2023 17:32:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gaming/2023/09/game-developers-unite-against-unitys-new-per-install-pricing-structure/">https://arstechnica.com/gaming/2023/09/game-developers-unite-against-unitys-new-per-install-pricing-structure/</a>, See on <a href="https://news.ycombinator.com/item?id=37499259">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <header>
            <h4>
      Unity to devs: pay up    —
</h4>
            
            <h2 itemprop="description">Fees of up to $0.20 per install threaten to upend large chunks of the industry.</h2>
                    </header>
        <section>
            <div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/09/unity-blow-up-800x450.jpg" alt="Kaboom!">
      <figcaption><div><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/09/unity-blow-up.jpg" data-height="1440" data-width="2560">Enlarge</a> <span>/</span> Kaboom!</p></div><p>Aurich Lawson | Getty Images</p></figcaption>  </figure>

  




<!-- cache hit 44:single/related:051d935f794936ba78b8d1f097858897 --><!-- empty -->
<p>For years, the Unity Engine has earned goodwill from developers large and small for its royalty-free licensing structure, which meant developers incurred no extra costs based on how well a game sold. That goodwill has now been largely thrown out the window due to <a href="https://blog.unity.com/news/plan-pricing-and-packaging-updates">Unity's Tuesday announcement of a new fee structure</a> that will start charging developers on a "per-install" basis after certain minimum thresholds are met.</p>
<p>The newly introduced Unity Runtime Fee—which will go into effect on January 1, 2024—will impose different per-install costs based on the company's different subscription tiers. Those on the Unity Personal tier (which includes free basic Editor access) will be charged $0.20 per install after an individual game reaches $200,000 in annual revenue and 200,000 lifetime installs.</p>
<p>Users of Unity's Pro and Enterprise tiers (which charge a separate annual subscription for access to a more full-featured Unity Editor) will pay slightly smaller per-install fees starting at $0.125 to $0.15 after a game reaches $1 million in annual revenue and 1 million total installs. The per-install fees for the paid subscription tiers are also subject to "volume discounts" for heavily installed games, going down as low as $0.01 per install for games that are installed 1 million times per month.</p>
<p>The new fee structure will apply in the United States, Australia, Austria, Belgium, Canada, Denmark, Finland, France, Germany, Ireland, Japan, Netherlands, New Zealand, Norway, Sweden, Switzerland, South Korea, and the United Kingdom. Outside of those countries, an "emerging markets rate" ranging from $0.005 (for Enterprise subscriptions) to $0.02 (for Unity Personal users) will apply after the minimum thresholds are met.</p>                                            
                                                        
<figure><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/09/unityfees.png" data-height="1039" data-width="743" alt="A full breakdown of Unity's new per-install fee structure"><img alt="A full breakdown of Unity's new per-install fee structure" src="https://cdn.arstechnica.net/wp-content/uploads/2023/09/unityfees-640x895.png" width="640" height="895" srcset="https://cdn.arstechnica.net/wp-content/uploads/2023/09/unityfees.png 2x"></a><figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/09/unityfees.png" data-height="1039" data-width="743">Enlarge</a> <span>/</span> A full breakdown of Unity's new per-install fee structure</p></figcaption></figure>
<p>This is a major change from Unity's previous structure, which allowed developers making less than $100,000 per month to avoid fees altogether on the Personal tier. Larger developers making $200,000 or more per month, meanwhile, paid only per-seat subscription fees for access to the latest, full-featured version of the Unity Editor under the Pro or Enterprise tiers.</p>
<p>"There's no royalties, no fucking around," Unity CEO John Riccitiello memorably <a href="https://www.gamesindustry.biz/theres-no-royalties-no-f-ing-around-riccitiello">told GamesIndustry.biz</a> when rolling out the free Personal tier in 2015. "We're not nickel-and-diming people, and we're not charging them a royalty. When we say it's free, it's free."</p>
<p>Now that Unity <em>has</em> announced plans to nickel-and-dime successful Unity developers (with a fee that is not technically a royalty), the reaction from those developers has been swift and universally angry, to put it mildly. "I can say, unequivocally, if you're starting a new game project, do not use Unity," Necrosoft Games' Brandon Sheffield—a longtime Unity Engine supporter—said in <a href="https://insertcredit.com/opinion/unity/">a post entitled "The Death of Unity."</a>&nbsp;"Unity is quite simply not a company to be trusted."</p>
<p>Sheffield was far from alone in the sentiment. "<em>Gloomwood</em> will definitely be my last Unity game, likely even if they roll back the changes," developer Dillon Rogers <a href="https://twitter.com/TafferKing451/status/1701722515910127745">wrote on social media</a>.</p>
<p>"If this goes through, we'd delay content and features our players actually want to port our games elsewhere (as others are also considering)," <em>Among Us</em> developer Innersloth <a href="https://twitter.com/InnerslothDevs/status/1701731398498013575">wrote on social media</a>.</p>
<p>"Stop it. Wtf?" the developer added pointedly.</p>

                                                </div>

            
                            <nav>Page: <span>1 <a href="https://arstechnica.com/gaming/2023/09/game-developers-unite-against-unitys-new-per-install-pricing-structure/2/">2</a> <a href="https://arstechnica.com/gaming/2023/09/game-developers-unite-against-unitys-new-per-install-pricing-structure/3/">3</a> <a href="https://arstechnica.com/gaming/2023/09/game-developers-unite-against-unitys-new-per-install-pricing-structure/2/"><span>Next <span>→</span></span></a></span></nav>
            
        </section>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Uselessness of Phenylephrine (2022) (214 pts)]]></title>
            <link>https://www.science.org/content/blog-post/uselessness-phenylephrine</link>
            <guid>37499106</guid>
            <pubDate>Wed, 13 Sep 2023 17:17:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.science.org/content/blog-post/uselessness-phenylephrine">https://www.science.org/content/blog-post/uselessness-phenylephrine</a>, See on <a href="https://news.ycombinator.com/item?id=37499106">Hacker News</a></p>
Couldn't get https://www.science.org/content/blog-post/uselessness-phenylephrine: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Bug in macOS 14 Sonoma prevents our app from working (844 pts)]]></title>
            <link>https://mullvad.net/en/blog/2023/9/13/bug-in-macos-14-sonoma-prevents-our-app-from-working/</link>
            <guid>37498979</guid>
            <pubDate>Wed, 13 Sep 2023 17:05:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mullvad.net/en/blog/2023/9/13/bug-in-macos-14-sonoma-prevents-our-app-from-working/">https://mullvad.net/en/blog/2023/9/13/bug-in-macos-14-sonoma-prevents-our-app-from-working/</a>, See on <a href="https://news.ycombinator.com/item?id=37498979">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>The macOS 14 Sonoma betas and release candidate contain a bug that causes the firewall to not filter traffic correctly. As a result, our app does not work.</p>

<p>During the macOS 14 Sonoma beta period Apple introduced a bug in the macOS firewall, packet filter (PF). This bug prevents our app from working, and can result in leaks when some settings (e.g. local network sharing) are enabled. We cannot guarantee functionality or security for users on macOS 14, we have investigated this issue after the 6th beta was released and reported the bug to Apple. Unfortunately the bug is still present in later macOS 14 betas and the release candidate.</p>

<p>We have evaluated whether we can patch our VPN app in such a way that it works and keeps users secure in macOS 14. But unfortunately there is no good solution, as far as we can tell. We believe the firewall bugs must be fixed by Apple.</p>

<p>The bug affects much more than just the Mullvad VPN app. Firewall rules do not get applied properly to network traffic, and traffic that is not supposed to be allowed is allowed. We deem this to be a critical flaw in the firewall, anyone relying on PF filtering, or apps using it in the background on their macOS devices should be cautious about upgrading to macOS 14.</p>

<h2>Our recommendations</h2>

<p>MacOS 14 Sonoma is scheduled to be released on the 26th of September, if the bug is still present we recommend our users to remain on macOS 13 Ventura until it is fixed.</p>

<h2>Technical details</h2>

<p>The following steps can be taken on macOS 14 to reproduce the issue. Warning: This will clear out any firewall rules you might have loaded in PF.</p>

<p>In a terminal, create a virtual logging interface and start watching it for traffic matching the rules you will add later:</p>

<pre>sudo ifconfig pflog1 create
sudo tcpdump -nnn -e -ttt -i pflog1</pre>

<p>Write the following firewall rules to a file named <code>pfrules</code>:</p>

<pre>pass quick log (all, to pflog1) inet from any to 127.0.0.1
block drop quick log (all, to pflog1)</pre>

<p>In another terminal, enable PF and load the rules:</p>

<pre>sudo pfctl -e
sudo pfctl -f pfrules</pre>

<p>Ping the <a href="https://mullvad.net/">mullvad.net</a> webserver:</p>

<pre>ping 45.83.223.209</pre>

<h3>Expected results</h3>

<ul>
	<li>Ping is blocked, since it does not match the only <code>pass</code> rule’s requirements</li>
	<li>The traffic is logged to <code>pflog1</code>. More specifically we expect it to be logged as matching the <code>block</code> rule</li>
</ul>

<h3>Actual results</h3>

<ul>
	<li>Ping is allowed out on the internet, and the response comes back</li>
	<li>No traffic is being logged to <code>pflog1</code></li>
</ul>

<h3>Cleaning up after the experiment</h3>

<p>Disable the firewall and clear all rules.</p>

<pre>sudo pfctl -d
sudo pfctl -f /etc/pf.conf</pre>

<p>Follow our blog for future updates to this issue.</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SpaceX no longer taking losses to produce Starlink satellite antennas (164 pts)]]></title>
            <link>https://www.cnbc.com/2023/09/13/spacex-no-longer-taking-losses-to-produce-starlink-satellite-antennas.html</link>
            <guid>37498830</guid>
            <pubDate>Wed, 13 Sep 2023 16:48:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2023/09/13/spacex-no-longer-taking-losses-to-produce-starlink-satellite-antennas.html">https://www.cnbc.com/2023/09/13/spacex-no-longer-taking-losses-to-produce-starlink-satellite-antennas.html</a>, See on <a href="https://news.ycombinator.com/item?id=37498830">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="RegularArticle-ArticleBody-5" data-module="ArticleBody" data-test="articleBody-2" data-analytics="RegularArticle-articleBody-5-2"><div id="ArticleBody-InlineImage-107066835" data-test="InlineImage"><p>A Starlink satellite terminal, also known as a dish, setup in front of an RV.</p><p>SpaceX</p></div><div><p>PARIS – <a href="https://www.cnbc.com/elon-musk/">Elon Musk's</a> SpaceX is no longer absorbing the cost of the Starlink antennas that it sells with its satellite internet service, a company executive said on Wednesday, a key step to the company improving its profitability.</p><p>"We were subsidizing terminals but we've been iterating on our terminal production so much that we're no longer subsidizing terminals, which is a good place to be," Jonathan Hofeller, SpaceX vice president of Starlink and commercial sales, said during a panel at the World Satellite Business Week conference.</p><p>SpaceX sells consumer Starlink antennas, also known as user terminals, for $599 each. For more demanding Starlink customers – such as mobile, maritime, or aviation users – SpaceX sells antennas with its service in a range from $2,500 to $150,000 each.</p><p>When SpaceX first began selling its Starlink service, <a href="https://www.cnbc.com/2021/04/06/spacexs-shotwell-no-plan-for-tiered-starlink-internet-pricing.html">company leadership said</a> the terminals cost about $3,000 each to manufacture. The company improved that to about $1,300 per terminal by early 2021, and Hofeller's comments on Wednesday indicate the terminals now cost less than $600 each to make – mass production savings that Hofeller credited as "one of our keys to success."</p></div><div id="ArticleBody-InlineImage-107299939" data-test="InlineImage"><p>SpaceX Vice President of Starlink and Commercial Sales Jonathan Hofeller, second from left, speaks at the World Satellite Business Week conference in Paris, France on Sept. 13, 2023.</p><p>Michael Sheetz | CNBC</p></div><div><p>SpaceX President and COO Gwynne Shotwell said earlier this year that <a href="https://www.cnbc.com/2023/02/08/spacex-prepares-test-fire-all-starship-engines-at-once.html">Starlink "had a cash flow positive quarter" in 2022</a>. The <a href="https://www.cnbc.com/2023/08/17/spacex-reported-a-profit-in-the-first-quarter-wsj-says.html">overall company reportedly turned a profit</a> in the first quarter of 2023. </p><p>Although it was founded more than two decades ago and <a href="https://www.cnbc.com/2023/07/13/elon-musk-spacex-near-150-billion-valuation.html">valued at about $150 billion</a>, SpaceX's businesses of rockets, spacecraft, and satellites are capital intensive. In 2021, Musk said <a href="https://www.cnbc.com/2021/02/19/spacex-valuation-driven-by-elon-musks-starship-and-starlink-projects.html">Starlink was going through "a deep chasm of negative cash flow"</a> before it could become "financially viable."</p><p>The company last provided an update on its global Starlink user base in May, when it said it had about 1.5 million customers. Hofeller did not specify what that total is now, but said Starlink is "well over" that 1.5 million mark. The figure includes both consumer and enterprise customers around the world, which Hofeller said the service aims to "grow to hopefully millions and millions."</p><p>To date, SpaceX has launched over 5,000 Starlink satellites and counting.</p><p>"We're going strong, we're launching twice a week now – which is insane," Hofeller said.</p><p>Earlier on Wednesday, <a href="https://www.cnbc.com/2023/09/13/spacex-starlink-partners-with-ses-for-combined-cruise-market-service.html">European satellite operator SES announced a partnership with Starlink</a> to jointly sell their communications services to cruise ships, a market both companies currently serve. SES CEO Ruy Pinto said the arrangement is one which the companies expect to build upon with later market offerings.</p></div><div id="Placeholder-ArticleBody-Video-106868126" data-test="VideoPlaceHolder" role="region" tabindex="0" data-vilynx-id="7000186379" aria-labelledby="Placeholder-ArticleBody-Video-106868126"><p><img src="https://image.cnbcfm.com/api/v1/image/106827233-1611167159999-starlink_rural_canada_8.png?v=1611167216&amp;w=750&amp;h=422&amp;vtcrop=y" alt="SpaceX is a leader in rocket launches, but Starlink is its golden ticket"><span></span><span></span></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WASI Support in Go (160 pts)]]></title>
            <link>https://go.dev/blog/wasi</link>
            <guid>37498820</guid>
            <pubDate>Wed, 13 Sep 2023 16:27:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://go.dev/blog/wasi">https://go.dev/blog/wasi</a>, See on <a href="https://news.ycombinator.com/item?id=37498820">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-slug="/blog/wasi">
    
    <h2><a href="https://go.dev/blog/">The Go Blog</a></h2>
    

    
      
      <p>
      Johan Brandhorst-Satzkorn, Julien Fabre, Damian Gryski, Evan Phoenix, and Achille Roussel<br>
      13 September 2023
      </p>
      
      <p>Go 1.21 adds a new port targeting the WASI preview 1 syscall API through the
new <code>GOOS</code> value <code>wasip1</code>. This port builds on the existing WebAssembly
port introduced in Go 1.11.</p>
<h2 id="what-is-webassembly">What is WebAssembly?</h2>
<p><a href="https://webassembly.org/" rel="noreferrer" target="_blank">WebAssembly (Wasm)</a> is a binary instruction format
originally designed for the web. It represents a standard that allows
developers to run high-performance, low-level code directly in web browsers at
near-native speeds.</p>
<p>Go first added support for compiling to Wasm in the 1.11 release, through the
<code>js/wasm</code> port. This allowed Go code compiled using the Go compiler to be
executed in web browsers, but it required a JavaScript execution environment.</p>
<p>As the use of Wasm has grown, so have use cases outside of the browser. Many
cloud providers are now offering services that allow the user to execute Wasm
executables directly, leveraging the new
<a href="https://wasi.dev/" rel="noreferrer" target="_blank">WebAssembly System Interface (WASI)</a> syscall API.</p>
<h2 id="the-webassembly-system-interface">The WebAssembly System Interface</h2>
<p>WASI defines a syscall API for Wasm executables, allowing them to interact with
system resources such as the filesystem, the system clock, random data
utilities, and more. The latest release of the WASI spec is called
<code>wasi_snapshot_preview1</code>, from which we derive the <code>GOOS</code> name <code>wasip1</code>. New
versions of the API are being developed, and supporting them in the Go
compiler in the future will likely mean adding a new <code>GOOS</code>.</p>
<p>The creation of WASI has allowed a number of Wasm runtimes (hosts) to
standardize their syscall API around it. Examples of Wasm/WASI hosts include
<a href="https://wasmtime.dev/" rel="noreferrer" target="_blank">Wasmtime</a>, <a href="https://wazero.io/" rel="noreferrer" target="_blank">Wazero</a>,
<a href="https://wasmedge.org/" rel="noreferrer" target="_blank">WasmEdge</a>, <a href="https://wasmer.io/" rel="noreferrer" target="_blank">Wasmer</a>, and
<a href="https://nodejs.org/" rel="noreferrer" target="_blank">NodeJS</a>. There are also a number of cloud providers
offering hosting of Wasm/WASI executables.</p>
<h2 id="how-can-we-use-it-with-go">How can we use it with Go?</h2>
<p>Make sure that you have installed at least version 1.21 of Go. For this demo,
we’ll use <a href="https://docs.wasmtime.dev/cli-install.html" rel="noreferrer" target="_blank">the Wasmtime host</a> to
execute our binary. Let’s start with a simple <code>main.go</code>:</p>
<pre><code>package main

import "fmt"

func main() {
    fmt.Println("Hello world!")
}
</code></pre>
<p>We can build it for <code>wasip1</code> using the command:</p>
<pre><code>$ GOOS=wasip1 GOARCH=wasm go build -o main.wasm main.go
</code></pre>
<p>This will produce a file, <code>main.wasm</code> which we can execute with <code>wasmtime</code>:</p>
<pre><code>$ wasmtime main.wasm
Hello world!
</code></pre>
<p>That’s all it takes to get started with Wasm/WASI! You can expect almost all
the features of Go to just work with <code>wasip1</code>. To learn more about the details
of how WASI works with Go, please see
<a href="https://go.dev/issue/58141" rel="noreferrer" target="_blank">the proposal</a>.</p>
<h2 id="running-go-tests-with-wasip1">Running go tests with wasip1</h2>
<p>Building and running a binary is easy, but sometimes we want to be able to run
<code>go test</code> directly without having to build and execute the binary manually.
Similar to the <code>js/wasm</code> port, the standard library distribution included
in your Go installation comes with a file that makes this very easy. Add the
<code>misc/wasm</code> directory to your <code>PATH</code> when running Go tests and it will
run the tests using the Wasm host of your choice. This works by <code>go test</code>
<a href="https://pkg.go.dev/cmd/go#hdr-Compile_and_run_Go_program" rel="noreferrer" target="_blank">automatically executing</a>
<code>misc/wasm/go_wasip1_wasm_exec</code> when it finds this file in the <code>PATH</code>.</p>
<pre><code>$ export PATH=$PATH:$(go env GOROOT)/misc/wasm
$ GOOS=wasip1 GOARCH=wasm go test ./...
</code></pre>
<p>This will run <code>go test</code> using Wasmtime. The Wasm host used can be controlled
using the environment variable <code>GOWASIRUNTIME</code>. Currently supported values
for this variable are <code>wazero</code>, <code>wasmedge</code>, <code>wasmtime</code>, and <code>wasmer</code>. This
script is subject to breaking changes between Go versions. Note that Go
<code>wasip1</code> binaries don’t execute perfectly on all hosts yet (see
<a href="https://go.dev/issue/59907" rel="noreferrer" target="_blank">#59907</a> and
<a href="https://go.dev/issue/60097" rel="noreferrer" target="_blank">#60097</a>).</p>
<p>This functionality also works when using <code>go run</code>:</p>
<pre><code>$ GOOS=wasip1 GOARCH=wasm go run ./main.go
Hello world!
</code></pre>
<h2 id="wrapping-wasm-functions-in-go-with-gowasmimport">Wrapping Wasm functions in Go with go:wasmimport</h2>
<p>In addition to the new <code>wasip1/wasm</code> port, Go 1.21 introduces a new compiler
directive: <code>go:wasmimport</code>. It instructs the compiler to translate calls to
the annotated function into a call to the function specified by the host
module name and function name. This new compiler functionality is what allowed
us to define the <code>wasip1</code> syscall API in Go to support the new port, but it
isn’t limited to being used in the standard library.</p>
<p>For example, the wasip1 syscall API defines the
<a href="https://github.com/WebAssembly/WASI/blob/a51a66df5b1db01cf9e873f5537bc5bd552cf770/legacy/preview1/docs.md#-random_getbuf-pointeru8-buf_len-size---result-errno" rel="noreferrer" target="_blank"><code>random_get</code> function</a>,
and it is exposed to the Go standard library through
<a href="https://cs.opensource.google/go/go/+/refs/tags/go1.21.0:src/runtime/os_wasip1.go;l=73-75" rel="noreferrer" target="_blank">a function wrapper</a>
defined in the runtime package. It looks like this:</p>
<pre><code>//go:wasmimport wasi_snapshot_preview1 random_get
//go:noescape
func random_get(buf unsafe.Pointer, bufLen size) errno
</code></pre>
<p>This function wrapper is then wrapped in
<a href="https://cs.opensource.google/go/go/+/refs/tags/go1.21.0:src/runtime/os_wasip1.go;l=183-187" rel="noreferrer" target="_blank">a more ergonomic function</a>
for use in the standard library:</p>
<pre><code>func getRandomData(r []byte) {
    if random_get(unsafe.Pointer(&amp;r[0]), size(len(r))) != 0 {
        throw("random_get failed")
    }
}
</code></pre>
<p>This way, a user can call <code>getRandomData</code> with a byte slice and it will
eventually make its way to the host-defined <code>random_get</code> function. In the same
way, users can define their own wrappers for host functions.</p>
<p>To learn more about the intricacies of wrapping Wasm functions in Go, please
see <a href="https://go.dev/issue/59149" rel="noreferrer" target="_blank">the <code>go:wasmimport</code> proposal</a>.</p>
<h2 id="limitations">Limitations</h2>
<p>While the <code>wasip1</code> port passes all standard library tests, there are some
notable fundamental limitations of the Wasm architecture that may surprise
users.</p>
<p>Wasm is a single threaded architecture with no parallelism. The scheduler can
still schedule goroutines to run concurrently, and standard in/out/error is
non-blocking, so a goroutine can execute while another reads or writes, but any
host function calls (such as requesting random data using the example above)
will cause all goroutines to block until the host function call has returned.</p>
<p>A notable missing feature in the <code>wasip1</code> API is a full implementation of
network sockets. <code>wasip1</code> only defines functions that operate on already opened
sockets, making it impossible to support some of the most popular features of
the Go standard library, such as HTTP servers. Hosts like Wasmer and WasmEdge
implement extensions to the <code>wasip1</code> API, allowing the opening of network
sockets. While these extensions are not implemented by the Go compiler, there
exists a third party library,
<a href="https://github.com/stealthrocket/net" rel="noreferrer" target="_blank"><code>github.com/stealthrocket/net</code></a>, which
uses <code>go:wasmimport</code> to allow the use of <code>net.Dial</code> and <code>net.Listen</code> on
supported Wasm hosts. This enables the creation of <code>net/http</code> servers and other
network related functionality when using this package.</p>
<h2 id="the-future-of-wasm-in-go">The future of Wasm in Go</h2>
<p>The addition of the <code>wasip1/wasm</code> port is just the beginning of the Wasm
capabilities we would like to bring to Go. Please keep an eye out on
<a href="https://github.com/golang/go/issues?q=is%3Aopen+is%3Aissue+label%3Aarch-wasm" rel="noreferrer" target="_blank">the issue tracker</a>
for proposals around exporting Go functions to Wasm (<code>go:wasmexport</code>), a 32-bit
port and future WASI API compatibility.</p>
<h2 id="get-involved">Get involved</h2>
<p>If you are experimenting with and want to contribute to Wasm and Go, please get
involved! The Go issue tracker tracks all in-progress work and the #webassembly
channel on <a href="https://invite.slack.golangbridge.org/" rel="noreferrer" target="_blank">the Gophers Slack</a> is a
great place to discuss Go and WebAssembly. We look forward to hearing from you!</p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Appeals court upholds right to post public laws online (191 pts)]]></title>
            <link>https://www.eff.org/press/releases/appeals-court-upholds-publicresourceorgs-right-post-public-laws-and-regulations</link>
            <guid>37498473</guid>
            <pubDate>Wed, 13 Sep 2023 15:59:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.eff.org/press/releases/appeals-court-upholds-publicresourceorgs-right-post-public-laws-and-regulations">https://www.eff.org/press/releases/appeals-court-upholds-publicresourceorgs-right-post-public-laws-and-regulations</a>, See on <a href="https://news.ycombinator.com/item?id=37498473">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span data-contrast="auto">SAN FRANCISCO—Technical standards like fire and electrical codes developed by private organizations but incorporated into public law can be freely disseminated without any liability for copyright infringement, </span><a href="https://www.eff.org/document/opinion-7"><span data-contrast="none">a federal appeals court ruled Tuesday</span></a><span data-contrast="auto">.</span><span data-ccp-props="259}">&nbsp;</span></p>
<p><span data-contrast="auto">Tuesday’s ruling by a three-judge panel of the U.S. Court of Appeals for the District of Columbia Circuit upholds the idea that our laws belong to all of us, and we should be able to find, read, and share them free of registration requirements, fees, and other roadblocks. It's a long-awaited victory for </span><a href="https://public.resource.org/"><span data-contrast="none">Public.Resource.org</span></a><span data-contrast="auto">, a nonprofit organization founded in 2007 by open records advocate Carl Malamud of Healdsburg, Calif., and represented in this case by the Electronic Frontier Foundation (EFF) with co-counsel Fenwick &amp; West and David Halperin.</span><span data-ccp-props="259}">&nbsp;</span></p>
<p><span data-contrast="auto">“In a nation governed by the rule of law, private parties have no business controlling who can read, share, and speak the rules to which we are all subject,” EFF Legal Director Corynne McSherry said. “We are pleased that the Court of Appeals upheld what other U.S. courts, including the Supreme Court, have said for almost 200 years: No one should control access to the law.”</span><span data-ccp-props="259}">&nbsp;</span></p>
<p><span data-contrast="auto">As part of its mission of promoting public access to all kinds of government information, Public Resource acquires and posts online a wide variety of public documents, such as nonprofits’ tax returns, government-produced videos, and standards incorporated into law by reference. These standards include electrical, fire safety, and consumer safety codes that have been mandated by governments. But without Public Resource’s work, they are often difficult to access, much less share with others, which means that areas of law that profoundly affect our daily life are obscured from our view. Even courts have had trouble accessing the laws that they are supposed to apply.&nbsp;</span><span data-ccp-props="259}">&nbsp;</span></p>
<p><span data-contrast="auto">The American Society for Testing and Materials (ASTM), National Fire Protection Association Inc. (NFPA), and American Society of Heating, Refrigerating, and Air-Conditioning Engineers (ASHRAE) are organizations that develop private sector codes and standards aimed at advancing public safety, ensuring compatibility across products and services, facilitating training, and spurring innovation. </span><span data-ccp-props="259}">&nbsp;</span></p>
<p><span data-contrast="auto">ASTM, NFPA, and ASHRAE sued Public Resource in 2013 for copyright and trademark infringement and unfair competition.</span><span data-ccp-props="259}">&nbsp;</span></p>
<p><span data-contrast="auto">Affirming a trial court’s March 2022 decision, the appeals court found Tuesday that Public Resource’s use is for nonprofit, educational purposes, and that this use serves a different purpose than that of the plaintiffs. The plaintiffs “seek to advance science and industry by producing standards reflecting industry or engineering best practices,” the court wrote, while “Public Resource’s mission in republishing the standards is very different—to provide the public with a free and comprehensive repository of the law.”</span><span data-ccp-props="259}">&nbsp;</span></p>
<p><span data-contrast="auto">“Public Resource posts standards that government agencies have incorporated into law—no more and no less,” the appeals court ruled. “If an agency has given legal effect to an entire standard, then its entire reproduction is reasonable in relation to the purpose of the copying, which is to provide the public with a free and comprehensive repository of the law.”</span><span data-ccp-props="259}">&nbsp;</span></p>
<p><span data-contrast="auto">The appeals court also found that although Public Resource has been posting incorporated standards for 15 years, the plaintiffs haven’t shown any evidence that this harmed them financially. And “even if Public Resource’s postings were likely to lower demand for the plaintiffs’ standards, we would also have to consider the substantial public benefits of free and easy access to the law,” the court ruled.</span><span data-ccp-props="259}">&nbsp;</span></p>
<p><span data-contrast="auto">The appeals court rejected the plaintiffs’ argument that because they make standards available for free in online reading rooms, Public Resource’s use cannot be transformative fair use. All but one of these rooms opened after Public Resource began posting incorporated standards, the court noted, and those reading rooms fail to provide convenient access. “Among other things, text is not searchable, cannot be printed or downloaded, and cannot be magnified without becoming blurry. Often, a reader can view only a portion of each page at a time and, upon zooming in, must scroll from right to left to read a single line of text. Public Resource’s postings suffer from none of these shortcomings.”</span><span data-ccp-props="259}">&nbsp;</span></p>
<p><span data-contrast="auto">Malamud praised the appeals court’s decision and said Public Resource will redouble its efforts.</span><span data-ccp-props="259}">&nbsp;</span></p>
<p><span data-contrast="auto">“It has been over 10 years since plaintiffs filed suit in this case,” he said. “Today, the U.S. Court of Appeals has found decisively in favor of the proposition that citizens must not be relegated to economy-class access to the law. This win would not have been possible without the untiring and dedicated work by the legal team of EFF, Fenwick &amp; West, and David Halperin, as well as the dozens of amicus briefs filed on our behalf. I'm very grateful for all their help.”</span><span data-ccp-props="259}">&nbsp;</span></p>
<p><span data-contrast="auto">For the opinion of the U.S. Court of Appeals for the District of Columbia Circuit: </span><a href="https://www.eff.org/document/opinion-7"><span data-contrast="none">https://www.eff.org/document/opinion-7</span></a><span data-ccp-props="259}">&nbsp;</span></p>
<p><span data-contrast="auto">For more on the Public Resource case: </span><a href="https://www.eff.org/cases/publicresource-freeingthelaw"><span data-contrast="none">https://www.eff.org/cases/publicresource-freeingthelaw</span></a><span data-ccp-props="259}">&nbsp;</span></p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A DIY near-IR spectrometer (323 pts)]]></title>
            <link>https://caoyuan.scripts.mit.edu/ir_spec.html</link>
            <guid>37498142</guid>
            <pubDate>Wed, 13 Sep 2023 15:33:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://caoyuan.scripts.mit.edu/ir_spec.html">https://caoyuan.scripts.mit.edu/ir_spec.html</a>, See on <a href="https://news.ycombinator.com/item?id=37498142">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-section="blog" id="colorlib-main">
										<p><span>Project</span></p><h2>A $500 DIY near-IR spectrometer that would sell for $10,000</h2>
                                        <div data-animate-effect="fadeInLeft">
                                                <p>
                                                DIYing spectrometers is not new. You can make one with your phone camera, and a broken piece from a CD-ROM as a diffraction grating. There are plenty of tutorials for this online. It takes $5 to make one.
                                                </p>
                                                <p>
                                                However, a silicon based camera (CMOS sensor) only respond up to ~1100 nm in wavelength, and this is a physical limitation —— silicon has a band gap of ~1.1 eV, and you cannot excite electron-hole pairs with wavelength longer than about ~1100 nm. So we need a different semiconductor if we want to measure any light above this wavelength. A popular choice is InGaAs, whose bandgap is tunable down to ~0.4 eV by changing content of indium versus gallium. 
                                                </p>
                                                
                                                <p>
                                                But you probably don't want to know how much a InGaAs camera will cost you... While a silicon-based camera is as cheap as dirt these days, an <em>one-dimensional</em> InGaAs pixel array already costs upper few thousand dollars. Any full-blown IR spectrometer system goes way over $10k, with their fancy thermoelectric cooling and precision gratings (we actually have one in our lab). The reason why they are so expensive is that the target user group are scientific researchers, not consumers.
                                                </p>
                                                
                                                <p>
                                                Since I have been recently interested in laser optics (as a hobby) and wish to DIY some laser systems, which inevitably requires working with near-IR wavelengths (above 1100 nm), I desperately needs a way to analyze what light I'm producing out of my laser crystals. One day I tried to search for InGaAs photodiodes on DigiKey and it turns out that <a href="https://www.digikey.com/product-detail/en/advanced-photonix/0800-3111-111/0800-3111-111-ND/10294611">a single InGaAs photodiode</a> sells for ~20 bucks. While its 100x more expensive than a silicon photodiode, I figured that you can make a spectrometer just with this one photodiode, and it's definitely within reach of DIY. And here it is —— <strong>a fiber-coupled IR spectrometer that measures from 800~1600 nm</strong>. </p>
                                              </div>
										
                                        <div>
                                            <p><img src="https://caoyuan.scripts.mit.edu/images/ir_detector.jpg" alt="IR Spectrometer">
                                            </p>
                                        </div>
                                        
                                        
                                        
                                        <br>
                                        
                                        <div data-animate-effect="fadeInLeft">
                                            	<h4>Design</h4>
                                                
                                                <p>
                                                A spectrometer mainly consists of four components: slit, diffracting element, detector, and relaying optics between these parts. In our case, the input optical fiber acts as our input slit. I chose to use a 50 um core multi-mode fiber for a compromise between light throughput and spectral resolution. A 2-m long SMA-905 fiber patch cable can be purchased on AliExpress for about $40, or $70 from Thorlabs. 
                                                </p>
                                                
                                                <p>
                                                The diffracting element is typically a grating. It is important to choose a grating with the right line density to obtain reasonable dispersion (wavelength per degree of angle) at the designed order of diffraction (1st-order is most commonly used). For this project since the design wavelength is 800 nm ~ 1600nm, a line density of 600 line per mm is about right, and this gives a dispersion of 40° over the designed wavelength range. See below for a plot of angle-of-diffraction for 50° incident angle onto a 600 line/mm grating. The 2nd and 3rd orders are clearly not usable as they overlap with the incident beam.
                                                </p>
                                                <p><img src="https://caoyuan.scripts.mit.edu/images/diffangle.jpg" alt="Diffraction angle">
                                                </p>
                                                
                                                <p>
                                                Next comes the detector. How are we gonna detect light from a range of angles with only a single 'pixel' of photodiode? We mount the photodiode on a motor and scan it across! To do this, I purchased a stepper-driven linear stage from <a href="https://www.amazon.com/gp/product/B0856XJW1J/ref=ppx_yo_dt_b_asin_title_o05_s00?ie=UTF8&amp;psc=1">Amazon</a> for $50. This little stage is quite well-built and allows a resolution-per-step of 5 um, more than enough for our resolution. 
                                                </p>
                                                
                                                <p>
                                                The InGaAs photodiode mentioned above has an active area of ϕ1mm, while the image of the 50 um input slit is gonna be a bit smaller than that. So an output slit also needs to be mounted as close as possible to the photodiode, in order to make sure that the size of the photodiode does not compromise the spectral resolution. This does not need to be of great precision —— I simply used some aluminum masking tape to make a slit of ~0.3 mm wide.
                                                </p>
                                                
                                                <p>
                                                Lastly, we need optics to connect all these components together. It turns out that this part is quite expensive and tricky if you want high spectral resolution as well as high light throughput (which ultimately determines the signal/noise ratio). Basically, starting from the input slit (which can be viewed as a point source), we need to (1) defocus it into a parallel beam, (2) direct it onto the grating, and (3) focus the diffracted beam (which is  approximately parallel for each color) onto the sensor. Traditionally, the standard way is an all-mirror configuration known as the <a href="https://en.wikipedia.org/wiki/Monochromator#Czerny%E2%80%93Turner_monochromator">Czerny-Turner design</a>. The reason to use mirrors instead of lenses is to minimize chromatic aberrations, so that all wavelengths can be focus onto the same focal plane. However, the alignment of these parabolic mirrors are quite difficult without a real optics bench, so I decided to pursue a different path that is much more friendly for DIYing.
                                                </p>
                                                
                                                <p> The first step —— defocusing fiber output into parallel beam, can be achieved using a <em>fiber collimator</em>. This is basically a lens pre-aligned with its back focal point right at the interface of the optical fiber core. 
                                                </p>
                                                <div data-animate-effect="fadeInLeft">
                                                	<p><img src="https://caoyuan.scripts.mit.edu/images/fibercollimator.jpg" alt="Fiber collimator">
                                                    </p>
                                                    <p>
                                                    SMA-905 fiber collimator (picture from Thorlabs)
                                                    </p>
                                                </div>
                                                
                                                <p>
                                                One end of this device is already threaded with standard SMA-905 connector, so that the fiber patch cable can be directly connected to it. The other end is free-space output of parallel beam. Very easy to use!
                                                </p>
                                                
                                                <p>
                                                However, the lens used in the collimator does have a chromatic aberration, so that only the beam at the designed wavelength (980 nm in my case) is truly collimated. Shorter wavelengths converge a little bit and longer wavelengths diverge a little bit. Fortunately, the level of dispersion appears to be tolerable, and it can be further corrected by optimizing the location of the detector.
                                                </p>
                                                
                                                <p>
                                                The fiber collimator is not super cheap —— it sells for <a href="https://www.thorlabs.com/thorproduct.cfm?partnumber=F220SMA-980">$160</a> on Thorlabs. While there are other options, they are either even more pricy, or appears much less well-built than the Thorlabs one. I was lucky to find a pair being sold on eBay for $70 each, so I can use one on each end of the fiber.
                                                </p>
                                                
                                                <p>
                                                Next, directing the beam onto the diffraction grating is easy, as you just need a good silver-coated mirror. Again, I found one (12.7mm x 12.7mm) on eBay for $25, while a new one sells for $35 on Thorlabs. I assume that any decent mirror you can find will probably work, but don't use a dielectric mirror if it's designed for visible, as it's probably gonna be out-of-band for near-IR wavelengths that we need.
                                                </p>
                                                
                                                <p>
                                                The last piece of optics is a lens/mirror to re-focus the diffracted beam onto the detector. Our beam is a ϕ2mm circular beam for each color. To simplify things a bit again, I chose to use a <em>cylindrical lens</em> to focus the light in the plane of diffraction. There are mainly two benifits of this: (1) the focused beam is a line of ~0.5 mm wide (beam waist) and ~2 mm tall (size of the original circular beam). This makes the alignment of the diffraction plane and the detector relatively unimportant (tolerance ~1 mm). (2) The rectangular shape of the cylindrical lens is easier to mount with than a circular lens. You just need a piece of double-sided tape!
                                                </p>
                                                
                                                <p>The optical layout is shown in the figure below, together with a raytracing of the light path using <a href="https://www.mathworks.com/matlabcentral/fileexchange/45355-optometrika">Optometrika</a>.</p>
                                                <p><img src="https://caoyuan.scripts.mit.edu/images/bench.png" alt="Optical bench">
                                                </p>
                                                <p>
                                                <center>Left: optical configuration. Right: simulated intensity on the detector</center>
                                                </p>
                                                
                                                <p> The placement of the cylindrical lens (position &amp; angle) affects the focal point for different wavelength. I did not do a rigorous calculation here —— I simply resorted to a trial-and-error method to figure out the optimal placement. The tolerance appears to be quite large, probably because of its relatively long focal length (150 mm) compared to the beam size.
                                                </p>
                                                
                                                <p> The theoretical resolution (without considering optical aberrations) can be estimated based on the parameters of the optics used here. The magnification of the optical train is 150 mm (cyl. lens) / 11 mm (fiber collimator) = 13.6 times. There are three major sources of broadening:
                                                	</p><ul>
                                                    	<li>
                                                        Input slit: The 50 um fiber core creates an image of 0.05 × 13.6 = 0.68 mm spot at the screen. This corresponds to about 6 nm broadening in wavelength.
                                                        </li>
                                                        <li>
                                                        Grating: The finite beam diameter at the grating limits the spectral resolution. Resolving power of a grating (using 1st order) is equal to the number of lines being illuminated. A beam spot of 2mm has a resolving power of 1,200, so this contributes to a broadening of ~1 nm in wavelength.
                                                        </li>
                                                        <li>
                                                        Output slit: the slit at the photodiode further broadens the spectral response by about 3 nm. 
                                                        </li>
                                                    </ul>
                                                
                                                
                                                <p>You can see that the major source of broadening comes from the input slit size (fiber core size). To improve upon this without compromising light throughput, one would use a fiber collimator with larger lens/mirror and longer focal length (which is also more expensive) so that the magnification of the system is reduced. Since the input 'slit' is in fact circular, the broadening is less severe than 6 nm. My spectrometer eventually achieved a measured FWHM of 3~4 nm @ 1000 nm, and 5~6 nm @ 1500 nm, which is more than adequate for me. </p>
                                                
                                                <p>
                                                In fact, this resolution is pretty darn good considering the simplicity. For reference, <a href="https://www.edmundoptics.com/p/900---1700nm-bw-tek-ingaas-nir-spectrometer/43100/">an InGaAs spectrometer sold by Edmund Optics</a> using the same SMA-905 fiber input claims a resolution of 4 nm, and <a href="https://www.oceaninsight.com/products/spectrometers/general-purpose-spectrometer/flame-series/flame-nir/">a compact spectrometer by Ocean Insight</a> has a nominal FWHM of 10 nm. FYI, they sell for ~$12,000 and ~$8,000 respectively.
                                                
                                            </p></div>
                                        
                                        <div data-animate-effect="fadeInLeft">
                                            	<h4>Make</h4>
                                            	<p>The core of the spectrometer is a diffraction grating, which diffracts the light towards different angles according to its wavelength. Instead of using part of a CD-ROM, here I used a legitimate <em>blazed reflective grating</em> from Thorlabs, which sells for <a href="https://www.thorlabs.com/thorproduct.cfm?partnumber=GR13-0610">$70</a>. The parameters of the grating is: 600 line/mm, 12.7mm x 12.7mm x 6mm, blazed for 1000 nm. 
                                                </p>
                                                
                                                <p><img src="https://caoyuan.scripts.mit.edu/images/grating.jpg" alt="Grating 600l/mm">
                                                </p>
                                                
                                                <p>
                                                As you can see from the picture, for small square optical components, I made a small "optical mount" to clamp down the optics parts firmly without damaging them. 
                                                </p>
                                                
                                                <p><img src="https://caoyuan.scripts.mit.edu/images/ir_schem.png" alt="Electronics part"></p>
                                                
                                                <p> 
                                                The electronics part of the detector mainly consists of a transimpedance amplifier and a 24-bit analog-digital converter (ADC). The core is a AD8656 operational amplifier with an ultralow input bias current of 1 pA and extremely low noise. This allows to use a huge gain resistor R of 100 megaohm, which converts each pA to 100 uV! The photodiode is operated with zero bias voltage, or in the so called 'photovoltaic' mode, to eliminate dark current. After that, the signal is fed into AD7793, a precision low-noise ADC with digital filtering. The output data rate is programmable, with the slowest speed (4.17 Hz) giving the lowest noise of 40 nV, which is negligible compared to other sources of noise. 
                                                </p>
                                                
                                                <p>
                                                One big issue for this circuit is the power supply, which is provided by the 5V USB power. USB power is notorious for its large fluctuations and ripples from the computer switching power supply. To circumvent this, I used the best possible linear regulator on the market —— LT3042 from Analog Devices, which boasts extremely high <a href="https://en.wikipedia.org/wiki/Power_supply_rejection_ratio">Power Supply Rejection Ratio (PSRR)</a> of over 100 dB. This chip turns out to be working magically well for dealing with dirty power.
                                                </p>
                                                
                                                <p>
                                                I find that the circuit is extremely vulnerable towards capacitive coupling with any surrounding conductor with voltages, most notably myself that emanates tons of 60 Hz noise... This is mostly likely because of the high impedance node at the input of the opamp (tens of megaohm). Electrostatic shielding (NOT the typical EMI shielding) is absolutely crucial for this. I wrapped the circuit board and photodiode all around with grounded aluminum tape/foil/plate, but a better way would be to use dedicated photodiode sockets connected to coaxial cable that is fed into a PCB with double ground planes. 
                                                </p>
                                                
                                                <p>
                                                With all external sources screened out, the remaining intrinsic noise comes from three places: (1) the photodiode, (2) the amplifier, and (3) the feedback resistor R. At the frequencies (1~100 Hz) of the measurement, the amplifier noise is mainly 1/f noise, which is hard to average out. According to the datasheet, the total noise from 0.1~10 Hz is about 0.3 uVrms, which is equivalent to 3 fA at the input. The resistor R and photodiode both contribute through thermal noise, which contributes 13 fA/√Hz and 9 fA/√Hz (assuming a shunt resistance of 200 MΩ) respectively at room temperature. If we measure at the slowest speed of 4.17 Hz, the noise can be estimated by RMS sum of these values, which equals 30 fA.                            </p>
                                                
                                                <p><img src="https://caoyuan.scripts.mit.edu/images/noise.png" alt="Noise plot"></p>
                                                
                                                <p>
                                                By comparison, the plot on the left shows that the standard deviation of the signal when there is no light is about 18 fA, meaning that the circuit is essentially limited by thermal noise of the system. Further improvement will require putting the sensor and amplifier into liquid nitrogen!
                                                </p>
                                                
                                                <p>
                                                Below are some pictures of other parts of the build.
                                                </p>
                                                
                                                <p><img src="https://caoyuan.scripts.mit.edu/images/ir_all.jpg" alt="All components"></p>
                                                <p>This pictures shows all components in the IR spectrometer. Power supply (5V 2A) is attached to the left wall, controller board is top, and detector board is fixed to the linear stepper motor (<a href="https://www.amazon.com/gp/product/B0856XJW1J/ref=ppx_yo_dt_b_search_asin_title?ie=UTF8&amp;psc=1">Amazon</a>).</p>
                                                <p><img src="https://caoyuan.scripts.mit.edu/images/ir_board.jpg" alt="All components"></p>
                                                <p>This is the controller board. Microcontroller is a STM32 Nucleo-32 board, and the green one is stepper motor driver. </p>
                                                <p><img src="https://caoyuan.scripts.mit.edu/images/ir_detector_board.jpg" alt="All components"></p>
                                                <p>This is the detector board, which has the low-noise regulator, A-D converter, and transimpedance amplifier as described above. Note the shielding tape around the amplifier. Imporant for low noise! Actual photodiode is in the small piece of board seen below the stepper motor. In this picture, there is an aluminum block before the photodiode, which hosts the slit made of razor blades. Later on I switched to simply putting two pieces of black tape directly onto the front window of the photodiode, because it seems like if the slit is too far from the photodiode, light throughput is significantly reduced at large incident angles (i.e. near the end of the spectral range).   </p> 
                                                
                                                <p><img src="https://caoyuan.scripts.mit.edu/images/ir_fiber_coupler.jpg" alt="All components"></p>
                                                <p>Close-up of the fiber coupler, fixed to the base of the box by a clamp.</p>
                                                
                                                
                                          </div>
                                        
                                        <div data-animate-effect="fadeInLeft">
                                            	<h4>Performance</h4>
                                            	<p>
                                                To test the performance of the spectrometer, we need a light source with known wavelength and linewidth. For a rough alignment I use my 505 nm green laser pen, which should get refracted to the 2nd order at exactly the same angle as 1010 nm light would be refracted to the 1st order. For conversion from the position of the detector to wavelength, however, a calibration source is needed. Typically, a mercury lamp or argon-neon calibrating lamp is used for this purpose, because these elements have a few bright and sharp emission lines in the visible to near-IR range. But these dedicated lamps are quite expensive (Newport sells them for <a href="https://www.newport.com/f/pencil-style-calibration-lamps">~$300</a>). Another possibility is to use gas spectrum tubes that are used in undergrad physics labs. But these tubes require a special high-voltage power supply, which takes extra effort to acquire.  
                                                </p>
                                                
                                                <p>
                                                Later I figured out that my <em>desk lamp</em> is actually a perfect calibration light source. Be aware that it has to be a fluorescent type light bulb —— either LED bulb or incandescent bulb won't work here. The reason is that fluorescent bulbs are filled with mercury vapor plus some inert gas, which get excited by the electricity to emit light mostly in the ultraviolet. These ultraviolet photons are then converted to visible light by phosphor salts coated in the inner surface of the tube. However, ultraviolet is not all that mercury emits. Neutral mercury in fact has a couple of decently <a href="https://physics.nist.gov/PhysRefData/Handbook/Tables/mercurytable2_a.htm">strong emission lines</a> at 436, 543, 546, 1014, 1357, 1367, and 1530 nm as well. The last 4 lines are quite useful in calibrating our near-IR spectrometer. Furthermore, my desk lamp seems to be also filled with argon gas. Argon has quite some emission lines in the near-IR range, especially in the 800~1000 nm range. 
                                                </p>
                                                
                                                <p>
                                                The procedure of calibration is very simple. We take a spectrum (intensity versus distance), identify the lines, and find the correspondance between position and wavelength for at these lines. Then an interpolation smoothly connects between these key points, so that any position can be converted to wavelength or vice versa. The following figure shows an example of calibrated spectrum of my desk lamp, annotated with their emission lines.
                                                </p>
                                                
                                                <p><img src="https://caoyuan.scripts.mit.edu/images/spec_desklamp_annotated.jpg" alt="Spectrum of my desk lamp">
                                                </p>
                                                
                                                <p><img src="https://caoyuan.scripts.mit.edu/images/fwhm.png" alt="FWHM of 1014 nm peak">
                                                </p>
                                                
                                                <p>
                                                Pretty good, isn't it? I tried to align the cylindrical lens so that the 1014 nm peak is sharpest (FWHM ~3 nm, see left), while somewhat compromising the resolution at long wavelengths (FWHM 5~6 nm at 1500 nm). Overall, I'm quite satisfied with this resolution considering the investment.
                                                </p>
                                                
                                                <p> The next step is to calibrate the spectral response function, that is the amplitude of the signal per unit input flux of light at different wavelengths. This calibration can be carried out less stringently, as an error of 10% in absolute signal scale usually does not create much problem (for reflection/transmission measurements, it is the change of signal that matters anyway). I used a $10 quartz halogen lamp (a type of incandescent bulb) bought at local hardware store as a black-body source (assumed to be 3200 K) to calibrate the spectral response. 
                                                </p>
                                                
                                                <p> Let's now use it to measure something more interesting! In fact, I bought a separate fiber illuminator on eBay for $70 for measuring transmission spectrum of filters and crystals. I also built a mini test bench with translation stage for the ease of aligning the beam and the crystal:
                                                </p>
                                               
                                                <div>
                                                	<p><img src="https://caoyuan.scripts.mit.edu/images/testbench.jpg" alt="Testbench for transmission spectroscopy">
                                                    </p>
                                                </div>
                                                
                                                <p>
                                                The aperture and lens creates a (white) beam with divergence of ~2° and beam diameter adjustable from ~0.5 mm to 1 cm. On the other end of the test bench, the same fiber collimator collects the collimated light into the optics fiber to be sent to the spectrometer. A 790 nm long-pass filter is mounted right before the collimator —— this is to prevent 2nd order diffraction of visible light from interfering with near-IR measurements. With this bench, we can measure the transmission of any <em>flat</em> sample, that is things without any optical power. Measurement of optical elements like lens are more tricky because the shape of the beam changes when they are inserted, resulting in inaccurate measurements.
                                                </p>   
                                                
                                                <p>
                                                Here is the transmission curve of a Nd-doped YAG crystal (2×2×10 mm, 1% doped), which is extensively used in solid-state lasers. 
                                                </p>
                                                <div>
                                                	<p><img src="https://caoyuan.scripts.mit.edu/images/ndyag_trans.png" alt="Nd:YAG crystal absorption">
                                                </p>
                                                </div>
                                                <p>
                                                In this curve, there is a background that comes from reflection on the interfaces (which I believe is anti-reflection coated for 1310 nm), and sharp peaks that comes from absorption in the Nd:YAG crystal. Light at 808 nm, the wavelength that is most frequently used for diode-pumping of these crystals, is almost perfectly absorbed by this crystal.
                                                </p>
                                                
                                                <p> I happen to have 808 nm pump diodes at hand, so we can also do a <em>fluorescence</em> spectrum. The diode is driven with a moderate current, which produces about ~ 100 mW of 808 nm output.
                                                </p>
                                                
                                                <div>
                                                	<p><img src="https://caoyuan.scripts.mit.edu/images/ndyag_fl.png" alt="Nd:YAG crystal fluorescence">
                                                </p>
                                                </div>
                                                <p>
                                                Here I plotted the intensity in log scale to flush out all features. While 1064 nm is of course the strongest peak, all other major emission lines at 946 nm, 1116 nm, 1319 nm, 1338 nm, 1357 nm, 1414 nm, 1431 nm, and 1444 nm are also quite obvious. From this spectrum, we can approximately determine the <em>dynamic range</em> of our spectrometer. In this setting (gain 16, time constant 1/8.33 Hz), the maximum signal measurable is 800 pA, while the noise floor is about 0.2 pA. So we have a dynamic range of 4,000:1, or 72 dB. This can be further improved to 100,000:1 if we use a larger time constant and smaller gain.
                                                </p>
                                                    
                                                <p>
                                                This is pretty much it for now. I'll put more spectra here if I take more cool ones in the future. I hope this article is useful for you if you also want to build a IR spectrometer without wanting to spend $10,000!
                                                </p>
                                            </div>
									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bitty Engine: An itty bitty game engine (177 pts)]]></title>
            <link>https://paladin-t.github.io/bitty/</link>
            <guid>37497956</guid>
            <pubDate>Wed, 13 Sep 2023 15:20:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://paladin-t.github.io/bitty/">https://paladin-t.github.io/bitty/</a>, See on <a href="https://news.ycombinator.com/item?id=37497956">Hacker News</a></p>
<div id="readability-page-1" class="page"><div class="page">
      <p>
        <img src="https://paladin-t.github.io/bitty/imgs/logo.png">
      </p>

      <p>
        <a href="https://paladin-t.github.io/bitty/index.html"><b>Home</b></a> |
        <a href="https://paladin-t.github.io/bitty/documents.html">Documents</a> |
        <a href="https://paladin-t.github.io/bitty/about.html">About</a> |
        <a href="https://github.com/paladin-t/bitty">GitHub</a>
      </p>
      <p>
        <img src="https://paladin-t.github.io/bitty/imgs/floppy.gif">
        An itty bitty game engine.
      </p>

      <hr>

      

      <p>
        <a href="https://store.steampowered.com/app/1386180/" target="_blank"><img src="https://paladin-t.github.io/bitty/imgs/steam.png"></a>
        <a href="https://tonywang.itch.io/bitty" target="_blank"><img src="https://paladin-t.github.io/bitty/imgs/itch.png"></a>
      </p>
      <p>
        <i>The Steam version and the Itch version offer an identical set of features, and they are both <b>DRM-free</b>.</i>
        <br>
        A trial is available on <a href="https://store.steampowered.com/app/1386180/" target="_blank">Steam</a> also.
      </p>

      <h2>
        <a id="about-bitty-engine" href="#about-bitty-engine"></a>
        About Bitty Engine
      </h2>

      
        <p>
        Bitty Engine is a cross-platform itty bitty <b>Game Engine</b> and open-source <b>Framework</b>. The full featured engine is programmable in Lua and integrated with built-in editors. It keeps the nature of both engine's productivity, and fantasy computer/console's ease to iterate. It boosts your commercial/non-commercial projects, prototypes, game jams, or just thought experiments.
      </p>

      <p>
        <b>Why Bitty Engine?</b>
        <br>
        Bitty Engine has everything built-in for coding, graphics composing, etc; it has a full featured debugger for breakpoint, variable inspecting, stepping, and call-stack traversing; it offers a set of well-designed API with full documentation; it builds fast binaries with code and asset obfuscating, moreover its package size is small (around 10MB with an empty project, the other engines output more than 10 times bigger).
        <br>
        It is supposed to be your ultimate 2D game creating software.
      </p>

      <p>
        <b>Features</b>
        <br>
        Bitty Engine offers a set of orthogonal features that makes game development comfortable and enjoyable.
        </p><ul>
          <li>Programmable in Lua, an easy to learn and widely used programming language</li>
          <li>Debugger with scope inspector, breakpoint support and stepping</li>
          <li>Easy to use API for resources, graphics, input, audio, and more other facilities</li>
          <li>Built-in libraries for File, File System, Archive, JSON, Network, Physics, etc.</li>
          <li>Various example projects</li>
          <li>Handy tools for editing sprite, map, image, text, JSON, etc.</li>
          <li>Project can be exported into standalone binary</li>
        </ul>
      

      <p>
        <b>Technical specifications</b>

        </p><ul>
          <li>Display: configurable resolution</li>
          <li>Code: Lua, supports multiple source files</li>
          <li>Image: either true-color (PNG, JPG, BMP, TGA) or paletted, up to 1024x1024 pixels per file</li>
          <li>Palette: 256 colors with transparency support</li>
          <li>Sprite: up to 1024x1024 pixels per frame, up to 1024 frames per sprite</li>
          <li>Map: up to 4096x4096 tiles per page</li>
          <li>Font: supports Bitmap and TrueType</li>
          <li>Audio: 1 BGM channel, 4 SFX channels; supports MP3, OGG, WAV, etc.</li>
          <li>Gamepad: 6 buttons for each pad (D-Pad + A/B), up to 2 players</li>
          <li>Keyboard and mouse: supported</li>
        </ul>
      

      <h2>
        <a id="make-your-own-games" href="#make-your-own-games"></a>
        Make your own games
      </h2>

      
      <div>
        <p>
          <img src="https://paladin-t.github.io/bitty/imgs/screenshot1_edit_code.png">
          <br>
          Edit code
        </p>
        <hr>
        <p>
          <img src="https://paladin-t.github.io/bitty/imgs/screenshot2_edit_sprite.png">
          <br>
          Edit sprite
        </p>
        <hr>
        <p>
          <img src="https://paladin-t.github.io/bitty/imgs/screenshot3_edit_map.png">
          <br>
          Edit map
        </p>
        <hr>
        <p>
          <img src="https://paladin-t.github.io/bitty/imgs/screenshot4_edit_image.png">
          <br>
          Edit image
        </p>
        <hr>
        <p><span id="more-screenshots">
          <p>
            <img src="https://paladin-t.github.io/bitty/imgs/screenshot5_view_sound.png">
            <br>
            View sound
          </p>
          <hr>
          <p>
            <img src="https://paladin-t.github.io/bitty/imgs/screenshot6_view_font.png">
            <br>
            View font
          </p>
          <hr>
          <p>
            <img src="https://paladin-t.github.io/bitty/imgs/screenshot7_play.png">
            <br>
            Play
          </p>
          <hr>
          <p>
            <img src="https://paladin-t.github.io/bitty/imgs/screenshot8_palette.png">
            <br>
            Palette
          </p>
          <hr>
          <p>
            <img src="https://paladin-t.github.io/bitty/imgs/screenshot9_blend.png">
            <br>
            Blend
          </p>
          <hr>
          <p>
            <img src="https://paladin-t.github.io/bitty/imgs/screenshot10_custom_clip.png">
            <br>
            Custom clip
          </p>
          <hr>
          <p>
            <img src="https://paladin-t.github.io/bitty/imgs/screenshot11_terminal.png">
            <br>
            Terminal
          </p>
          <hr>
          <p>
            <img src="https://paladin-t.github.io/bitty/imgs/screenshot12_pathfinding.png">
            <br>
            Pathfinding
          </p>
        </span></p>
      </div>
      <p><img src="https://paladin-t.github.io/bitty/imgs/screenshot1_edit_code.png">
        <img src="https://paladin-t.github.io/bitty/imgs/screenshot2_edit_sprite.png">
        <img src="https://paladin-t.github.io/bitty/imgs/screenshot3_edit_map.png">
        <img src="https://paladin-t.github.io/bitty/imgs/screenshot4_edit_image.png">
        <img src="https://paladin-t.github.io/bitty/imgs/screenshot5_view_sound.png">
        <img src="https://paladin-t.github.io/bitty/imgs/screenshot6_view_font.png">
        <img src="https://paladin-t.github.io/bitty/imgs/screenshot7_play.png">
        <img src="https://paladin-t.github.io/bitty/imgs/screenshot8_palette.png">
        <img src="https://paladin-t.github.io/bitty/imgs/screenshot9_blend.png">
        <img src="https://paladin-t.github.io/bitty/imgs/screenshot10_custom_clip.png">
        <img src="https://paladin-t.github.io/bitty/imgs/screenshot11_terminal.png">
        <img src="https://paladin-t.github.io/bitty/imgs/screenshot12_pathfinding.png">
      </p>

      <h2>
        <a id="redistribute" href="#redistribute"></a>
        Redistribute
      </h2>

      
        <p>
        Bitty Engine helps you to make standalone binaries for Windows, MacOS, Linux and HTML (WebAssembly). It is redistributable for both non-commercial and commercial use without extra fee, your project is totally yours.
        <br>
        Put an image at "../icon.png" relative to executable to customize the icon dynamically. Put an image at "../splash.png" as well to customize the splash; the image could be transparent.
        <br>
        You can also customize redistributable binary by compiling from <a href="https://github.com/paladin-t/bitty" target="_blank">source</a>.
      </p>

      <h2>
        <a id="buy-bitty-engine" href="#buy-bitty-engine"></a>
        Get Bitty Engine
      </h2>

      

      <p>
        <i>The Steam version and the Itch version offer an identical set of features, and they are both <b>DRM-free</b>.</i>
        <br>
        Get the trial version on <a href="https://store.steampowered.com/app/1386180/" target="_blank">Steam</a> also to try out the language, libraries, editors, pipelines, etc.
      </p>

      <h2>
        <a id="system-requirements" href="#system-requirements"></a>
        System requirements
      </h2>

      <table>
          <thead>
            <tr>
              <th colspan="2">Minimum</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>OS</td>
              <td>Windows 7 or later (32/64bit), MacOS 10.7 or later (64bit), Ubuntu 14 or later (32/64bit)</td>
            </tr>
            <tr>
              <td>Processor</td>
              <td>1.5GHz</td>
            </tr>
            <tr>
              <td>Memory</td>
              <td>512 MB RAM</td>
            </tr>
            <tr>
              <td>Graphics</td>
              <td>Intel HD</td>
            </tr>
            <tr>
              <td>Storage</td>
              <td>150 MB available space</td>
            </tr>
          </tbody>
        </table>
      

      <hr>

      <p>
        © 2020 - 2023 <a href="https://paladin-t.github.io/" target="_blank">Tony Wang</a> |
        <a href="https://paladin-t.github.io/bitty/changelog.html">Changelog</a>
      </p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Skip the API, ship your database (110 pts)]]></title>
            <link>https://fly.io/blog/skip-the-api/</link>
            <guid>37497345</guid>
            <pubDate>Wed, 13 Sep 2023 14:37:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fly.io/blog/skip-the-api/">https://fly.io/blog/skip-the-api/</a>, See on <a href="https://news.ycombinator.com/item?id=37497345">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
             <img src="https://fly.io/blog/skip-the-api/assets/skip-the-api-cover.webp" alt="A child is peeking over at the answers on another child's test.">
           
<p>My favorite part about building tools is discovering their unintended uses. It’s like starting to write a murder mystery book but you have no idea who the killer is!</p>

<p>History is filled with examples of these accidental discoveries: WD-40 was originally <a href="https://en.wikipedia.org/wiki/WD-40#History">used to protect ICBMs from rust</a> and now it fixes your squeaky doorknob. Bubble wrap was <a href="https://en.wikipedia.org/wiki/Bubble_Wrap_(brand)#History">originally sold as wallpaper</a> and now it protects your Amazon packages.</p>

<p>When we started writing <a href="https://fly.io/docs/litefs/">LiteFS</a>, a distributed SQLite database, we thought it would be used to distribute data geographically so users in, say, Bucharest see response times as fast as users in San Jose. And for the most part, that’s what LiteFS users are doing.</p>

<p>But we discovered another unexpected use: replacing the API layer between services with SQLite databases.</p>
<h2 id="how-it-started"><a href="#how-it-started" aria-label="Anchor"></a>How it started</h2>
<p>In the early days of LiteFS development, we wanted to find a real-world test bed for our tool so we could smoke out any bugs that we didn’t find during automated tests. Part of  our existing infrastructure is a program called <em>Corrosion</em> that gossips state between all our servers. Corrosion tracks VM statuses, health checks, and a plethora of other information for each server and communicates this info with other servers so they can make intelligent decisions about request routing and VM placement. Corrosion keeps a fast, local copy of all this data in a SQLite database.</p>

<p>So we set up a Corrosion instance that also ran on top of LiteFS. This helped root out some bugs but we also found another use for it: making Corrosion accessible to our internal services.</p>

<p><img src="https://fly.io/blog/skip-the-api/assets/corrosion.png"></p>
<h2 id="shipping-the-kitchen-sink"><a href="#shipping-the-kitchen-sink" aria-label="Anchor"></a>Shipping the kitchen sink</h2>
<p>The typical approach to making data available between services is to spend weeks designing an API and then building a service around it. Your API design needs to take into account the different use cases of each consuming service so that it can deliver the data it needs efficiently. You don’t want your clients making a dozen API calls for every request!</p>

<p><img src="https://fly.io/blog/skip-the-api/assets/architecture.png"></p>

<p>A different approach is to skip the API design entirely and just ship the entire database to your client. You don’t need to consider the consuming service’s access patterns as they can use vanilla SQL to query and join whatever data their heart desires. That’s what we did using LiteFS.</p>

<p>While we could have set up each downstream service as a Corrosion node, gossip protocols can be chatty and we really just needed a one-way stream of updates. Setting up a read-only LiteFS instance for a new service is simple—it just needs the hostname of the upstream primary node to connect to:</p>
<div>
  <pre><code>lease:
  type: "static"
  candidate: false
  advertise-url: "http://corrosion-bridge:20202
</code></pre>
</div>

<p>And voila! You have a full, read-only copy of the database on your app.</p>
<h2 id="moving-compute-to-the-client"><a href="#moving-compute-to-the-client" aria-label="Anchor"></a>Moving compute to the client</h2>
<p>API design is notoriously difficult as it’s hard to know what your consuming services will need. Query languages such as <a href="https://graphql.org/">GraphQL</a> have even been invented for this specific problem!</p>

<p>However, GraphQL has its own limitations. It’s good for fetching raw data but it lacks built-in <a href="https://www.sqlite.org/lang_aggfunc.html">aggregation</a> &amp; advanced querying capabilities like <a href="https://www.sqlite.org/windowfunctions.html">windowing</a>. GraphQL is typically layered on top of an existing relational database that uses SQL. So why not just use SQL?</p>

<p>Additionally, performing queries on your service means that you need to handle multiple tenants competing for compute resources. Managing these tenants involves rate limiting and query timeouts so that no one client consumes all the resources.</p>

<p>By pushing a read-only copy of the database to clients,  these restrictions aren’t a concern anymore. A tenant can use 100% of its CPU for hours if it wants to. It won’t adversely affect any other tenant because the query is running on its own hardware.</p>
<h2 id="so-whats-the-downside"><a href="#so-whats-the-downside" aria-label="Anchor"></a>So what’s the downside?</h2>
<p>There’s always trade-offs with any technology and shipping read-only replicas is no different. One obvious limitation of read-only replicas is that they’re read-only. If your clients need to update data, they’ll still need an API for those mutations.</p>

<p>A less obvious downside is that the contract for a database can be less strict than an API. One benefit to an API layer is that you can change the underlying database structure but still massage data to look the same to clients. When you’re shipping the raw database, that becomes more difficult. Fortunately, many database changes, such as adding columns to a table, are backwards compatible so clients don’t need to change their code. Database views are also a great way to reshape data so it stays consistent—even when the underlying tables change.</p>

<p>Finally, shipping a database limits your ability to restrict access to data. If you have a multi-tenant database, you can’t ship that database without the client seeing all the data. One workaround for this is to use a database per tenant. SQLite databases are lightweight since they are just files on disk. This also has the added benefit of preventing queries in your application from accidentally fetching data across tenants.</p>
<h2 id="where-do-we-take-this-next"><a href="#where-do-we-take-this-next" aria-label="Anchor"></a>Where do we take this next?</h2>
<p>While this approach has worked well for some internal tooling, how does this look in the broader world of software? APIs are likely stick around for the foreseeable future so providing read-only database replicas make sense for specific use cases where those APIs aren’t a great fit.</p>

<p>Imagine being able to query all your Stripe data or your GitHub data from a local database. You could join that data on to your own dataset and perform fast queries on your own hardware.</p>

<p>While companies such as Stripe or GitHub likely colocate their tenant data into one database, many companies run an event bus using tools like Kafka which could allow them to generate per-tenant SQLite databases to then stream to customers.</p>

<p>Pushing queries out to the end user has huge benefits for both the data provider &amp; the data consumer in terms of flexibility and power.</p>

           
         </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple finally put USB-C in new iPhone, but limited to 23-year-old USB 2.0 speeds (101 pts)]]></title>
            <link>https://www.pcgamer.com/apple-finally-put-usb-c-in-the-new-iphone-but-its-inexplicably-limited-to-23-year-old-usb-20-speeds/</link>
            <guid>37497199</guid>
            <pubDate>Wed, 13 Sep 2023 14:26:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pcgamer.com/apple-finally-put-usb-c-in-the-new-iphone-but-its-inexplicably-limited-to-23-year-old-usb-20-speeds/">https://www.pcgamer.com/apple-finally-put-usb-c-in-the-new-iphone-but-its-inexplicably-limited-to-23-year-old-usb-20-speeds/</a>, See on <a href="https://news.ycombinator.com/item?id=37497199">Hacker News</a></p>
<div id="readability-page-1" class="page"><article aria-label="article" data-id="nQMUNVkt2voKY99m6HJXWT">
<header>
<nav aria-label="Breadcrumbs">
<ol>
<li>
<a href="https://www.pcgamer.com/uk/news/" aria-label="Return to News">News</a>
</li>
</ol>
</nav>


</header>
<section>
<div itemprop="image" itemscope="" itemtype="https://schema.org/ImageObject">
<div>
<picture><source type="image/webp" alt="iPhone 15" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" srcset="https://cdn.mos.cms.futurecdn.net/pUc7kajRX34gmkEmutWnpB-320-80.png.webp 320w, https://cdn.mos.cms.futurecdn.net/pUc7kajRX34gmkEmutWnpB-480-80.png.webp 480w, https://cdn.mos.cms.futurecdn.net/pUc7kajRX34gmkEmutWnpB-650-80.png.webp 650w, https://cdn.mos.cms.futurecdn.net/pUc7kajRX34gmkEmutWnpB-970-80.png.webp 970w, https://cdn.mos.cms.futurecdn.net/pUc7kajRX34gmkEmutWnpB-1024-80.png.webp 1024w, https://cdn.mos.cms.futurecdn.net/pUc7kajRX34gmkEmutWnpB-1200-80.png.webp 1200w, https://cdn.mos.cms.futurecdn.net/pUc7kajRX34gmkEmutWnpB-1920-80.png.webp 1920w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/pUc7kajRX34gmkEmutWnpB.png" data-pin-media="https://cdn.mos.cms.futurecdn.net/pUc7kajRX34gmkEmutWnpB.png"><source type="image/png" alt="iPhone 15" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" srcset="https://cdn.mos.cms.futurecdn.net/pUc7kajRX34gmkEmutWnpB-320-80.png 320w, https://cdn.mos.cms.futurecdn.net/pUc7kajRX34gmkEmutWnpB-480-80.png 480w, https://cdn.mos.cms.futurecdn.net/pUc7kajRX34gmkEmutWnpB-650-80.png 650w, https://cdn.mos.cms.futurecdn.net/pUc7kajRX34gmkEmutWnpB-970-80.png 970w, https://cdn.mos.cms.futurecdn.net/pUc7kajRX34gmkEmutWnpB-1024-80.png 1024w, https://cdn.mos.cms.futurecdn.net/pUc7kajRX34gmkEmutWnpB-1200-80.png 1200w, https://cdn.mos.cms.futurecdn.net/pUc7kajRX34gmkEmutWnpB-1920-80.png 1920w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/pUc7kajRX34gmkEmutWnpB.png" data-pin-media="https://cdn.mos.cms.futurecdn.net/pUc7kajRX34gmkEmutWnpB.png"><img src="https://cdn.mos.cms.futurecdn.net/pUc7kajRX34gmkEmutWnpB-320-80.png" alt="iPhone 15" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" srcset="https://cdn.mos.cms.futurecdn.net/pUc7kajRX34gmkEmutWnpB-320-80.png 320w, https://cdn.mos.cms.futurecdn.net/pUc7kajRX34gmkEmutWnpB-480-80.png 480w, https://cdn.mos.cms.futurecdn.net/pUc7kajRX34gmkEmutWnpB-650-80.png 650w, https://cdn.mos.cms.futurecdn.net/pUc7kajRX34gmkEmutWnpB-970-80.png 970w, https://cdn.mos.cms.futurecdn.net/pUc7kajRX34gmkEmutWnpB-1024-80.png 1024w, https://cdn.mos.cms.futurecdn.net/pUc7kajRX34gmkEmutWnpB-1200-80.png 1200w, https://cdn.mos.cms.futurecdn.net/pUc7kajRX34gmkEmutWnpB-1920-80.png 1920w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/pUc7kajRX34gmkEmutWnpB.png" data-pin-media="https://cdn.mos.cms.futurecdn.net/pUc7kajRX34gmkEmutWnpB.png"></picture>
</div>
<meta itemprop="url" content="https://cdn.mos.cms.futurecdn.net/pUc7kajRX34gmkEmutWnpB.png">
<meta itemprop="height" content="600">
<meta itemprop="width" content="338">
<figcaption itemprop="caption description">
<span itemprop="copyrightHolder">(Image credit: Apple)</span>
</figcaption>
</div>

<div id="article-body">
<p><strong>Update, September 13, 2023: </strong>Digging into the details of the new Apple handsets, it turns out there <em>is</em> a reason why the regular iPhone 15's <a href="https://www.pcgamer.com/uk/tag/usb/" data-auto-tag-linker="true">USB</a>-C port runs at 2.0 speeds while the 15 Pro has a faster 3.0-spec interface: It's got an old chunk of silicon inside it.&nbsp;</p><p>That reason isn't simply to give buyers a reason to buy the pricier handset, though obviously isn't <em>not wanted</em> by Apple. The iPhone 15 Pro sports Apple's new A17 Pro APU, complete with a new on-die USB controller block. However, the vanilla iPhone 15 makes do with the last-gen A16 Bionic chip, which lacks that new USB controller and only has the original Lightning controller onboard.&nbsp;</p><p>Exactly how Apple is implementing USB on the cheaper handset isn't clear. However, the fact it's limited to USB 2.0 speeds, and that the legacy Lightning interface likewise tops out at the same 480Mbps, makes sense in the context of the absent USB controller. Very likely, there's some kind of conversion going on between the new USB-C connector and the old Lightning controller, limiting the former to the latter's peak bandwidth.</p><p><strong>Original story, September 12, 2023:</strong> For the first time in its 16 year history, Apple is releasing an iPhone without a proprietary connector. Apple announced in <a href="https://www.youtube.com/watch?v=ZiP1l7jlIIA" target="_blank" data-url="https://www.youtube.com/watch?v=ZiP1l7jlIIA">a livestream today</a> that the new iPhone 15 will launch with USB-C, ending the 11-year reign of its Lightning port (only half a decade or so after it probably should've made this move).</p><p>As Android phones, laptops, and all sorts of accessories have largely embraced USB-C over the last few years, Apple's been one of the last remaining holdouts. There's perhaps no more famous love story in tech than the romance between Apple and ports that require proprietary cables or pricey dongles.</p><p>The iPhone 15's switch to USB-C comes a year after the <a href="https://www.theverge.com/2022/12/8/23499754/usb-c-iphone-european-union-legislation-charger-lightning-enforcement-date" target="_blank" data-url="https://www.theverge.com/2022/12/8/23499754/usb-c-iphone-european-union-legislation-charger-lightning-enforcement-date">European Union ruled</a> that starting in late 2024, new mobile devices are required to use the ubiquitous USB-C port. Apple could've avoided the requirement by dropping a physical port altogether in favor of wireless charging, but it's apparently not feeling <em>quite</em> that courageous about this year's phone upgrade.</p><p>It is, however, feeling courageous enough to sell a brand new smartphone that uses an antiquated, absurdly slow 23-year-old USB standard.</p><p>Apple didn't have much to say about the USB-C change in terms of specifics during its keynote, but it did confirm that it's releasing updated earbuds and Airpod Pros with USB-C ports. Later, while discussing the iPhone 15 Pro, Apple explained that a USB controller unique to the Pro's chip will allow for 10Gbps USB transfers—a big red flag for what it's offering in the basic phone model.</p><p>And sure enough, the full specs on <a href="https://go.redirectingat.com/?id=92X590208&amp;xcust=pcg_gb_3958822339362326500&amp;xs=1&amp;url=https%3A%2F%2Fwww.apple.com%2Fiphone-15%2F&amp;sref=https%3A%2F%2Fwww.pcgamer.com%2Fapple-finally-put-usb-c-in-the-new-iphone-but-its-inexplicably-limited-to-23-year-old-usb-20-speeds/" target="_blank" data-url="https://www.apple.com/iphone-15/" data-hl-processed="skimlinks" data-placeholder-url="https://go.redirectingat.com/?id=92X590208&amp;xcust=hawk-custom-tracking&amp;xs=1&amp;url=https%3A%2F%2Fwww.apple.com%2Fiphone-15%2F&amp;sref=https%3A%2F%2Fwww.pcgamer.com%2Fapple-finally-put-usb-c-in-the-new-iphone-but-its-inexplicably-limited-to-23-year-old-usb-20-speeds" rel="sponsored noopener" referrerpolicy="no-referrer-when-downgrade" data-google-interstitial="false" data-merchant-name="SkimLinks - apple.com" data-merchant-id="undefined" data-merchant-url="undefined" data-merchant-network="undefined">Apple's website</a> &nbsp;confirm some bad news about this upgrade to USB-C.</p><p>Apple's site indicates it will be offering a 20W USB-C charger (sold separately, of course)—slower than the common 25W charging used in Android phones. But the really bad news here is that the USB-C port on the iPhone 15 is still limited to USB 2.0, meaning it tops out at 480 Mbps transfer speeds. That's <em>ridiculous</em> for an $800 smartphone in the year 2023.</p><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" alt="iPhone 15 USB C spec" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" data-normal="https://vanilla.futurecdn.net/pcgamer/media/img/missing-image.svg" data-srcset="https://cdn.mos.cms.futurecdn.net/8Uwge4taw5xndTwKrJUNnj-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/8Uwge4taw5xndTwKrJUNnj-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/8Uwge4taw5xndTwKrJUNnj-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/8Uwge4taw5xndTwKrJUNnj-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/8Uwge4taw5xndTwKrJUNnj-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/8Uwge4taw5xndTwKrJUNnj-1200-80.jpg.webp 1200w" data-sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/8Uwge4taw5xndTwKrJUNnj.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/8Uwge4taw5xndTwKrJUNnj.jpg"><source type="image/jpeg" alt="iPhone 15 USB C spec" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" data-normal="https://vanilla.futurecdn.net/pcgamer/media/img/missing-image.svg" data-srcset="https://cdn.mos.cms.futurecdn.net/8Uwge4taw5xndTwKrJUNnj-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/8Uwge4taw5xndTwKrJUNnj-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/8Uwge4taw5xndTwKrJUNnj-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/8Uwge4taw5xndTwKrJUNnj-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/8Uwge4taw5xndTwKrJUNnj-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/8Uwge4taw5xndTwKrJUNnj-1200-80.jpg 1200w" data-sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/8Uwge4taw5xndTwKrJUNnj.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/8Uwge4taw5xndTwKrJUNnj.jpg"><img src="https://cdn.mos.cms.futurecdn.net/8Uwge4taw5xndTwKrJUNnj.jpg" alt="iPhone 15 USB C spec" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" data-normal="https://vanilla.futurecdn.net/pcgamer/media/img/missing-image.svg" data-srcset="https://cdn.mos.cms.futurecdn.net/8Uwge4taw5xndTwKrJUNnj-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/8Uwge4taw5xndTwKrJUNnj-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/8Uwge4taw5xndTwKrJUNnj-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/8Uwge4taw5xndTwKrJUNnj-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/8Uwge4taw5xndTwKrJUNnj-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/8Uwge4taw5xndTwKrJUNnj-1200-80.jpg 1200w" data-sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/8Uwge4taw5xndTwKrJUNnj.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/8Uwge4taw5xndTwKrJUNnj.jpg" srcset="https://cdn.mos.cms.futurecdn.net/8Uwge4taw5xndTwKrJUNnj-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/8Uwge4taw5xndTwKrJUNnj-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/8Uwge4taw5xndTwKrJUNnj-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/8Uwge4taw5xndTwKrJUNnj-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/8Uwge4taw5xndTwKrJUNnj-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/8Uwge4taw5xndTwKrJUNnj-1200-80.jpg 1200w"></picture></p></div><figcaption itemprop="caption description"><span itemprop="copyrightHolder">(Image credit: Apple)</span></figcaption></figure><p>Apple has actually been shipping MacBooks and iPads with USB-C for years now, so it's had plenty of time to bring the iPhone up to par with the basic charging and transfer capabilities of other USB-C devices. But the iPhone has long been a holdout, sticking to its increasingly antiquated Lightning port. That has offered iPhone owners some benefits—if you bought a dock or a lifetime supply of cables years ago, you've gotten plenty of use out of them. But Lighting has become progressively outdated.</p><p>Since introducing it in 2012, Apple hasn't increased the iPhone's Lightning port bandwidth from a paltry 480Mbps transfer speed (the maximum of the old USB 2.0 protocol it relies on). USB-C, under the latest revision currently in use, has the ability to top out at 10 Gbps speeds.</p><p>It's fair to say that Lightning has been holding the iPhone back on both the data transfer and charging fronts for a long time now, but Apple's really cutting corners here by limiting the iPhone 15 to the ancient USB 2.0 protocol, and only including the vastly faster current USB 3 standard on the Pro model. If you want USB 3, get ready to pay up another $200.</p><p>The USB 2-capable iPhone 15 will be available on September 22 starting at $799; the USB 3-capable iPhone 15 Pro will be available the same day, starting at $999.</p>
</div>
<div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent"><section><p>Sign up to get the best content of the week, and great gaming deals, as picked by the editors.</p></section></div>
<div id="slice-container-authorBio"><p>Wes has been covering games and hardware for more than 10 years, first at tech sites like <a href="https://www.nytimes.com/wirecutter/" target="_blank">The Wirecutter</a> and <a href="https://www.tested.com/" target="_blank">Tested</a> before joining the PC Gamer team in 2014. Wes plays a little bit of everything, but he'll always jump at the chance to cover emulation and Japanese games.</p>

<p>When he's not obsessively optimizing and re-optimizing a tangle of conveyor belts in Satisfactory (it's really becoming a problem), he's probably playing a 20-year-old Final Fantasy or some opaque ASCII roguelike. With a focus on writing and editing features, he seeks out personal stories and in-depth histories from the corners of PC gaming and its niche communities. 50% pizza by volume (deep dish, to be specific).</p></div>



</section>


<div id="slice-container-relatedArticles"><p><h5>Most Popular</h5></p></div>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Fennel? (184 pts)]]></title>
            <link>https://fennel-lang.org/rationale</link>
            <guid>37497131</guid>
            <pubDate>Wed, 13 Sep 2023 14:21:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fennel-lang.org/rationale">https://fennel-lang.org/rationale</a>, See on <a href="https://news.ycombinator.com/item?id=37497131">Hacker News</a></p>
<div id="readability-page-1" class="page">

<p>Fennel is a programming language that runs on the Lua runtime.</p>
<h2 id="why-lua">Why Lua?</h2>
<p>The Lua programming language is an excellent and very underrated
tool. Is it remarkably powerful yet keeps a very small footprint both
conceptually as a language and in terms of the size of its
implementation. (The reference implementation consists of about nineteen
thousand lines of C and compiles to 278kb.) Partly because it is so
simple, Lua is also extremely fast. But the most important thing about
Lua is that it's specifically designed to be put in other programs to
make them reprogrammable by the end user.</p>
<p>The conceptual simplicity of Lua stands in stark contrast to other
"easy to learn" languages like JavaScript or Python--Lua contains very
close to the minimum number of ideas needed to get the job done; only
Forth and Scheme offer a comparable simplicity. When you combine this
meticulous simplicity with the emphasis on making programs
reprogrammable, the result is a powerful antidote to prevailing trends
in technology of treating programs as black boxes out of the control of
the user.</p>
<h2 id="and-yet">And yet...</h2>
<p>So if Lua is so great, why not just use Lua? In many cases you
should! But there are a handful of shortcomings in Lua which over time
have shown to be error-prone or unclear. Fennel runs on Lua, and the
runtime semantics of Fennel are a subset of Lua's, but you can think of
Fennel as an alternate notation you can use to write Lua programs which
helps you avoid common pitfalls. This allows Fennel to focus on doing
one thing very well and not get dragged down with things like
implementing a virtual machine, a standard library, or profilers and
debuggers. Any library or tool that already works for Lua will work just
as well for Fennel.</p>
<p>The most obvious difference between Lua and Fennel is the
parens-first syntax; Fennel belongs to the Lisp family of programming
languages. You could say that this removes complexity from the grammar;
the paren-based syntax is more regular and has fewer edge cases. Simply
by virtue of being a lisp, Fennel removes from Lua:</p>
<ul>
<li>statements (everything is an expression),</li>
<li>operator precedence (there is no ambiguity about what comes first),
and</li>
<li>early returns (functions always return in tail positions).</li>
</ul>
<h2 id="variables">Variables</h2>
<p>One of the most common legitimate criticisms leveled at Lua is that
it makes it easy to accidentally use globals, either by forgetting to
add a <code>local</code> declaration or by making a typo. Fennel allows
you to use globals in the rare case they are necessary but makes it very
difficult to use them by accident.</p>
<p>Fennel also removes the ability to reassign normal locals. If you
declare a variable that will be reassigned, you must introduce it with
<code>var</code> instead. This encourages cleaner code and makes it
obvious at a glance when reassignment is going to happen. Note that Lua
5.4 introduced a similar idea with <code>&lt;const&gt;</code> variables,
but since Fennel did not have to keep decades of existing code like Lua
it was able to make the cleaner choice be the default rather than
opt-in.</p>
<h2 id="tables-and-loops">Tables and Loops</h2>
<p>Lua's notation for tables (its data structure) feels somewhat dated.
It uses curly brackets for both sequential (array-like) and key/value
(dictionary-like) tables, while Fennel uses the much more familiar
notation of using square brackets for sequential tables and curly
brackets for key/value tables.</p>
<p>In addition Lua overloads the <code>for</code> keyword for both
numeric "count from X to Y" style loops as well as more generic
iterator-based loops. Fennel uses <code>for</code> in the first case and
introduces the <code>each</code> form for the latter.</p>
<h2 id="functions">Functions</h2>
<p>Another common criticism of Lua is that it lacks arity checks; that
is, if you call a function without enough arguments, it will simply
proceed instead of indicating an error. Fennel allows you to write
functions that work this way (<code>fn</code>) when it's needed for
speed, but it also lets you write functions which check for the
arguments they expect using <code>lambda</code>.</p>
<h2 id="other">Other</h2>
<p>If you've been programming in newer languages, you are likely to be
spoiled by pervasive destructuring of data structures when binding
variables, as well as by pattern matching to write more declarative
conditionals. Both these are absent from Lua and included in Fennel.</p>
<p>Finally Fennel includes a macro system so that you can easily extend
the language to include new syntactic forms. This feature is
intentionally listed last because while lisp programmers have
historically made a big deal about how powerful it is, it is relatively
rare to encounter situations where such a powerful construct is
justified.</p>
<p>For a more detailed look at the guiding principles of Fennel from a
design perspective see <a href="https://fennel-lang.org/values">the
Values of Fennel</a>.</p>
<hr>
<p><a href="https://fennel-lang.org/">Home</a></p>
<p><a href="https://git.sr.ht/~technomancy/fennel-lang.org">source for this site</a></p>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meduza co-founder's phone infected with Pegasus (292 pts)]]></title>
            <link>https://meduza.io/en/feature/2023/09/13/the-million-dollar-reporter</link>
            <guid>37496589</guid>
            <pubDate>Wed, 13 Sep 2023 13:39:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://meduza.io/en/feature/2023/09/13/the-million-dollar-reporter">https://meduza.io/en/feature/2023/09/13/the-million-dollar-reporter</a>, See on <a href="https://news.ycombinator.com/item?id=37496589">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><figure><div role="button" tabindex="0" data-testid="image"><picture><source type="image/webp" media="(min-width: 1000px)" srcset="https://meduza.io/impro/yPR6qJpGjj7S9NAJw4j9mKYhBap_puIn61m6eq4649M/resizing_type:fit/width:1960/height:0/enlarge:1/quality:80/aHR0cHM6Ly9tZWR1/emEuaW8vaW1hZ2Uv/YXR0YWNobWVudHMv/aW1hZ2VzLzAwOS80/MDAvNjAxL29yaWdp/bmFsLzMzekl0ZEdO/Zy1VQVZNc2V0Tk42/bkEuanBn.webp 2x, https://meduza.io/impro/umGT38JJPOrtDXzree-vMsa6fnwrA3216Qv5wpchAho/resizing_type:fit/width:980/height:0/enlarge:1/quality:80/aHR0cHM6Ly9tZWR1/emEuaW8vaW1hZ2Uv/YXR0YWNobWVudHMv/aW1hZ2VzLzAwOS80/MDAvNjAxL29yaWdp/bmFsLzMzekl0ZEdO/Zy1VQVZNc2V0Tk42/bkEuanBn.webp 1x "><source type="image/png" media="(min-width: 1000px)" srcset="https://meduza.io/impro/sDQyEv0j-rvllhnj7yYhwDw-PCTvyEZPUxzNzF_EiGg/resizing_type:fit/width:1960/height:0/enlarge:1/quality:80/aHR0cHM6Ly9tZWR1/emEuaW8vaW1hZ2Uv/YXR0YWNobWVudHMv/aW1hZ2VzLzAwOS80/MDAvNjAxL29yaWdp/bmFsLzMzekl0ZEdO/Zy1VQVZNc2V0Tk42/bkEuanBn.jpg 2x, https://meduza.io/impro/Xc_JEC2D4A0l4zZgFc4OPHpenVRs-3btXnYPPy4q944/resizing_type:fit/width:980/height:0/enlarge:1/quality:80/aHR0cHM6Ly9tZWR1/emEuaW8vaW1hZ2Uv/YXR0YWNobWVudHMv/aW1hZ2VzLzAwOS80/MDAvNjAxL29yaWdp/bmFsLzMzekl0ZEdO/Zy1VQVZNc2V0Tk42/bkEuanBn.jpg 1x "><source type="image/webp" media="(min-width: 650px)" srcset="https://meduza.io/impro/ClFSrOpIpnPHYyFE8evGPSTAiwunFd72HMoVf7II_bU/resizing_type:fit/width:1300/height:0/enlarge:1/quality:80/aHR0cHM6Ly9tZWR1/emEuaW8vaW1hZ2Uv/YXR0YWNobWVudHMv/aW1hZ2VzLzAwOS80/MDAvNjAxL29yaWdp/bmFsLzMzekl0ZEdO/Zy1VQVZNc2V0Tk42/bkEuanBn.webp 2x, https://meduza.io/impro/I2vdDa2Ed6jyRQki4NfPgFNUU6a328BjR8WGU0a30OI/resizing_type:fit/width:650/height:0/enlarge:1/quality:80/aHR0cHM6Ly9tZWR1/emEuaW8vaW1hZ2Uv/YXR0YWNobWVudHMv/aW1hZ2VzLzAwOS80/MDAvNjAxL29yaWdp/bmFsLzMzekl0ZEdO/Zy1VQVZNc2V0Tk42/bkEuanBn.webp 1x "><source type="image/png" media="(min-width: 650px)" srcset="https://meduza.io/impro/NYdYuY9duVy3_CdrEsxM-jPdRNkOcl_Kvw8rE1c-_1g/resizing_type:fit/width:1300/height:0/enlarge:1/quality:80/aHR0cHM6Ly9tZWR1/emEuaW8vaW1hZ2Uv/YXR0YWNobWVudHMv/aW1hZ2VzLzAwOS80/MDAvNjAxL29yaWdp/bmFsLzMzekl0ZEdO/Zy1VQVZNc2V0Tk42/bkEuanBn.jpg 2x, https://meduza.io/impro/YRvoynY74H_uBiHaHJO7dExBu6cJ_48eBHchdKNiM58/resizing_type:fit/width:650/height:0/enlarge:1/quality:80/aHR0cHM6Ly9tZWR1/emEuaW8vaW1hZ2Uv/YXR0YWNobWVudHMv/aW1hZ2VzLzAwOS80/MDAvNjAxL29yaWdp/bmFsLzMzekl0ZEdO/Zy1VQVZNc2V0Tk42/bkEuanBn.jpg 1x "><source type="image/webp" media="(min-width: 325px)" srcset="https://meduza.io/impro/Lmn41SYFuD_5DsPRrh7QwCexuzbCXBu21ZKRIczdiqI/resizing_type:fit/width:1040/height:0/enlarge:1/quality:80/aHR0cHM6Ly9tZWR1/emEuaW8vaW1hZ2Uv/YXR0YWNobWVudHMv/aW1hZ2VzLzAwOS80/MDAvNjAxL29yaWdp/bmFsLzMzekl0ZEdO/Zy1VQVZNc2V0Tk42/bkEuanBn.webp 2x, https://meduza.io/impro/P5aB63e4V9FAbkacjqkm_wdOmrsQS_yoHeFqHxyd9Jk/resizing_type:fit/width:520/height:0/enlarge:1/quality:80/aHR0cHM6Ly9tZWR1/emEuaW8vaW1hZ2Uv/YXR0YWNobWVudHMv/aW1hZ2VzLzAwOS80/MDAvNjAxL29yaWdp/bmFsLzMzekl0ZEdO/Zy1VQVZNc2V0Tk42/bkEuanBn.webp 1x "><source type="image/png" media="(min-width: 325px)" srcset="https://meduza.io/impro/qZCo4od7aszVBswkThQIAS5R38z7JKOQ1ljJZyGvuRg/resizing_type:fit/width:1040/height:0/enlarge:1/quality:80/aHR0cHM6Ly9tZWR1/emEuaW8vaW1hZ2Uv/YXR0YWNobWVudHMv/aW1hZ2VzLzAwOS80/MDAvNjAxL29yaWdp/bmFsLzMzekl0ZEdO/Zy1VQVZNc2V0Tk42/bkEuanBn.jpg 2x, https://meduza.io/impro/6BVaC07Klo7QfTYepDaMJcrZPz3_zkZLR4T8E4zx8HY/resizing_type:fit/width:520/height:0/enlarge:1/quality:80/aHR0cHM6Ly9tZWR1/emEuaW8vaW1hZ2Uv/YXR0YWNobWVudHMv/aW1hZ2VzLzAwOS80/MDAvNjAxL29yaWdp/bmFsLzMzekl0ZEdO/Zy1VQVZNc2V0Tk42/bkEuanBn.jpg 1x "><source type="image/webp" media="(min-width: 0)" srcset="https://meduza.io/impro/I2vdDa2Ed6jyRQki4NfPgFNUU6a328BjR8WGU0a30OI/resizing_type:fit/width:650/height:0/enlarge:1/quality:80/aHR0cHM6Ly9tZWR1/emEuaW8vaW1hZ2Uv/YXR0YWNobWVudHMv/aW1hZ2VzLzAwOS80/MDAvNjAxL29yaWdp/bmFsLzMzekl0ZEdO/Zy1VQVZNc2V0Tk42/bkEuanBn.webp 2x, https://meduza.io/impro/hpW7k3PbPn8GNSJQ_XTGmsoUORQYTW1L8rJA-O4vw6A/resizing_type:fit/width:325/height:0/enlarge:1/quality:80/aHR0cHM6Ly9tZWR1/emEuaW8vaW1hZ2Uv/YXR0YWNobWVudHMv/aW1hZ2VzLzAwOS80/MDAvNjAxL29yaWdp/bmFsLzMzekl0ZEdO/Zy1VQVZNc2V0Tk42/bkEuanBn.webp 1x "><source type="image/png" media="(min-width: 0)" srcset="https://meduza.io/impro/YRvoynY74H_uBiHaHJO7dExBu6cJ_48eBHchdKNiM58/resizing_type:fit/width:650/height:0/enlarge:1/quality:80/aHR0cHM6Ly9tZWR1/emEuaW8vaW1hZ2Uv/YXR0YWNobWVudHMv/aW1hZ2VzLzAwOS80/MDAvNjAxL29yaWdp/bmFsLzMzekl0ZEdO/Zy1VQVZNc2V0Tk42/bkEuanBn.jpg 2x, https://meduza.io/impro/5xllX6xETyjghqjE0Tz8AX2MsEFenysyCjvigSHc7BA/resizing_type:fit/width:325/height:0/enlarge:1/quality:80/aHR0cHM6Ly9tZWR1/emEuaW8vaW1hZ2Uv/YXR0YWNobWVudHMv/aW1hZ2VzLzAwOS80/MDAvNjAxL29yaWdp/bmFsLzMzekl0ZEdO/Zy1VQVZNc2V0Tk42/bkEuanBn.jpg 1x "><img src="data:image/gif;base64,R0lGODlhAQABAPAAAPLy8gAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw=="></picture></div></figure><p>The public has known for years<strong> </strong>that governments around the world use software developed by an Israeli cyber-arms company to spy on journalists, opposition politicians, and activists. Investigative journalists published a series of bombshell reports in July 2021 about the widespread abuse of Pegasus, a powerful tool marketed exclusively to state clients for use against only the grisliest criminals. Earlier this summer, Meduza learned that the iPhone of our co-founder and publisher, Galina Timchenko, was infected with Pegasus mere hours before she joined a private conference in Berlin attended by colleagues in the exiled Russian independent media. This is the first confirmed case of a Pegasus attack against a Russian journalist. With help from experts at Access Now and Citizen Lab, Meduza reports what we know about this notorious spyware, how it’s been used in Europe, and which states might have spent millions of dollars to hijack Ms. Timchenko’s phone.</p><blockquote><strong>Readers, please be aware of a possible conflict of interest in this report</strong>, which focuses on Meduza co-founder and publisher Galina Timchenko. She was not involved in the preparation of this article.</blockquote><p>Galina Timchenko hurried to Meduza’s Riga newsroom on June 23. She’d just gotten a call from <span data-id="1463706" data-title="Alexey the technical director" data-body="<p>To protect Alexey’s safety, Meduza is not disclosing his surname.</p>">Alexey</span>, the head of Meduza’s technical division, telling her to come in immediately. His voice was unusually stern, and he didn’t explain the urgency. “He simply spoke in such a way that I understood it as an order,” Timchenko later recalled. “It was clear that something had happened.”</p><p>En route to the office, Timchenko wondered if one of her passwords wasn’t secure or if she’d clicked on any suspicious hyperlinks. “I thought I’d done something wrong,” she says.</p><p>Alexey was waiting for her at the doorstep. He silently pointed at her bag, which held her phone and computer. “I can’t say anything just yet,” he informed her. “We’re looking into it.” He then took Timchenko’s iPhone and MacBook.</p><p>A day earlier, Timchenko had received a curious text message from Apple and forwarded it to Meduza’s tech division. The message was one of Apple’s “<a href="https://support.apple.com/en-us/102174" rel="noopener" target="_blank">threat notifications</a>” about “state-sponsored attackers” — something the company sends to users who are “individually targeted because of who they are or what they do.” “State-sponsored attacks are highly complex, cost millions of dollars to develop, and often have a short shelf life,” Apple explains on its website.&nbsp;</p><p>The notification sent to Timchenko did not identify the state in question.</p><p>She says she put the message out of her mind after sharing it with Meduza’s technical team. Galina Timchenko has grown accustomed to such warnings. The Russian authorities have tried to hack or destroy her newsroom’s infrastructure for years. Meduza has weathered denial-of-service attacks and countless phishing attempts. Russia’s federal censor now even blocks the website outright.</p><p>To understand what Apple’s message didn’t explain, Meduza’s technical director turned to outside help to find out who these hackers were. First, he contacted human rights activists at <a href="https://www.accessnow.org/about-us/" rel="noopener" target="_blank">Access Now</a>, a nonprofit organization committed to “defending and extending” the digital civil rights of people worldwide and helping improve digital security practices. Access Now has also <a href="https://www.accessnow.org/sanctions-undermining-human-rights/" rel="noopener" target="_blank">alerted</a> the public to the collateral damage of tech sanctions on civil rights activists, journalists, and dissidents from authoritarian countries, highlighting how targeted sanctions, mass corporate pullouts, and over-compliance in Russia have helped the Kremlin to silence its critics.</p><p>Alexey also reached out to researchers at <a href="https://citizenlab.ca/about/" rel="noopener" target="_blank">Citizen Lab</a>, an interdisciplinary laboratory at the University of Toronto that investigates digital espionage against civil society, among many other things.&nbsp;</p><p>Experts at Access Now and Citizen Lab collected the data from Timchenko’s devices and performed what they call “a rapid COVID test.” The results were quick indeed, revealing that her smartphone was infected with the spyware Pegasus on February 10, 2023. This gave the hackers total access to Timchenko’s iPhone: its microphone, cameras, and memory. The attackers could see the device’s entire contents, including Timchenko’s home address, her scheduled meetings, her photographs, and even her correspondence in encrypted instant messengers. Pegasus lets you see a device’s screen directly, reading messages as they are written. It lets you download every email, text, image, and file.</p><h3><strong>Pegasus and NSO Group</strong></h3><p>NSO Group, the Israeli entity responsible for Pegasus, <a href="https://www.economist.com/culture/2023/01/19/pegasus-lifts-the-lid-on-a-sophisticated-piece-of-spyware" rel="noopener" target="_blank">insists</a> that it designed the product exclusively for the surveillance of “terrorists, criminals, and pedophiles.” The firm’s co-founders <a href="https://www.nytimes.com/2022/01/28/magazine/nso-group-israel-spyware.html" rel="noopener" target="_blank">include</a> <a href="https://www.forbes.com/sites/thomasbrewster/2016/08/25/everything-we-know-about-nso-group-the-professional-spies-who-hacked-iphones-with-a-single-text/?sh=5ed8c94e3997" rel="noopener" target="_blank">veterans</a> of Israel’s military intelligence and the Mossad, and the company sells Pegasus only to state clients.</p><p>Despite its claims about “rigorous” human rights policies, NSO Group does big business with governments around the world that have regularly used Pegasus to target critics and political adversaries, from reform-minded bishops and priests in <a href="https://newnaratif.com/pegasus-spyware-in-southeast-asia/" rel="noopener" target="_blank">Togo</a> and women’s rights activists in <a href="https://www.nytimes.com/2022/01/28/magazine/nso-group-israel-spyware.html" rel="noopener" target="_blank">Saudi Arabia</a> to journalists in <a href="https://www.washingtonpost.com/world/2021/07/19/india-nso-pegasus/" rel="noopener" target="_blank">India</a> and human rights defenders in <a href="https://citizenlab.ca/2021/11/palestinian-human-rights-defenders-hacked-nso-groups-pegasus-spyware/" rel="noopener" target="_blank">Palestine</a>. Reporters and activists tracked by NSO Group’s spyware are often arrested and sometimes even killed. For example, Saudi operatives in Istanbul murdered and dismembered Washington Post columnist Jamal Khashoggi in 2018 after numerous members of his close entourage were selected for surveillance by NSO Group customers, while the Israeli firm <a href="https://forbiddenstories.org/the-rise-and-fall-of-nso-group/" rel="noopener" target="_blank">denies</a> that its technology was used “to listen, monitor, track, or collect information regarding [Khashoggi] or his family members.”</p><p>States pay “tens of millions of dollars, if not more,” for access to Pegasus, Citizen Lab senior researcher John Scott-Railton told Meduza.</p><p>The Mexican government alone has spent at least <a href="https://timesofindia.indiatimes.com/business/india-business/pegasus-snooping-how-costly-is-the-israeli-spyware/articleshow/84893498.cms" rel="noopener" target="_blank">$61 million</a> on the technology, which it has used to spy on <a href="https://www.washingtonpost.com/world/2021/07/21/shalev-hulio-nso-surveillance/" rel="noopener" target="_blank">dangerous criminals</a> and <a href="https://www.washingtonpost.com/world/2021/11/09/mexico-pegasus-nso/" rel="noopener" target="_blank">civil society members</a> alike, including journalist <a href="https://forbiddenstories.org/pegasus-the-new-global-weapon-for-silencing-journalists/" rel="noopener" target="_blank">Cecilio Pineda</a>, who was assassinated in 2017, just a few weeks after his phone was infected with Pegasus.</p><p>Even researchers in this field aren’t sure what it costs to hack a single device using Pegasus. The spyware is more a service than anything; each NSO Group contract permits so many “simultaneous infections,” says Natalia Krapiva, Access Now’s tech-legal counsel. “For example, a client state can buy a package with 20 infections, which means it can have 20 people under surveillance at one time.”</p><p>Speaking to The Washington Post in July 2021, NSO Group co-founder Omri Lavie said attacks on journalists by his clients are “horrible,” but he argued that the main problem is a lack of regulation. “This is the price of doing business,” he explained. “Somebody has to do the dirty work.”</p><p>A kind of regulation arrived in November 2021, albeit not what Omri Lavie and his colleagues wanted. Months after an investigation by the <a href="https://www.washingtonpost.com/investigations/2021/07/18/takeaways-nso-pegasus-project/" rel="noopener" target="_blank">Pegasus Project</a> consortium exposed the spyware’s rampant, global abuse, the Biden administration added NSO Group to a <a href="https://www.washingtonpost.com/technology/2021/11/03/pegasus-nso-entity-list-spyware/" rel="noopener" target="_blank">federal blacklist</a> that bans the company from receiving American technologies. NSO spokespeople expressed “<a href="https://www.washingtonpost.com/technology/2021/11/03/pegasus-nso-entity-list-spyware/" rel="noopener" target="_blank">dismay</a>” and said the firm would lobby to reverse the White House’s decision.</p><h3><strong>‘I felt dirty’</strong></h3><p>As soon as the Pegasus infection was confirmed, Meduza’s management locked itself in Timchenko’s office for an emergency meeting. “We were all terrified,” Alexey recalls, “but we pretended we weren’t.”</p><p>Meduza editor-in-chief Ivan Kolpakov, who was traveling then, joined the meeting by teleconference. He was visibly at a loss and kept listing aloud what could have leaked: corporate passwords and correspondence, bank account balances, the names of Meduza staff, and — most dangerously — the identities of Meduza’s collaborators inside Russia.&nbsp;</p><p>It was soon clear, however, that it was impossible to assess what had been compromised. “They got everything,” Kolpakov recalls. “Everything they wanted.”</p><p>Those at the meeting say Meduza’s technical director was the only one who remained calm, but he remembers it differently: “I sat there, plugging my ears, and I tried to write out a checklist for Galya: new password, new device, new Apple ID, new SIM card.” Timchenko tried at first to “laugh it off,” says Alexey, but eventually she burst into tears:</p><p>The most unpleasant questions came from Ivan: “What documents were you working with on your iPhone? Did you activate two-factor authentication everywhere?” I already felt like I’d been stripped naked in the town square. Like someone had reached into my pocket. Like I was dirty somehow. I wanted to wash my hands! And then my partner and best friend starts interrogating me as if I’d put everyone at risk. It really hurt… But I’d have demanded the same if I were in his shoes. Ivan was just very nervous.</p><p>It is virtually <a href="https://www.cnet.com/tech/mobile/pegasus-spyware-and-citizen-surveillance-what-you-need-to-know/" rel="noopener" target="_blank">impossible</a> to prevent infection by Pegasus; it can hack any gadget running a single application vulnerable to the software, including apps preinstalled by Apple itself. A device hijacked by Pegasus isn’t easy to spot, either. For instance, Timchenko had no reason to suspect anything was amiss with her iPhone, except for moments when it seemed warmer than usual, which she attributed to her new charger.</p><p>Citizen Lab’s analysis shows attackers likely infiltrated Timchenko’s iPhone through HomeKit and iMessage. Senior researcher John Scott-Railton says his team found digital footprints unique to Pegasus. “No other spyware would have left this,” he told Meduza.<em> </em>Researchers believe Timchenko’s hackers used the so-called “<a href="https://www.calcalistech.com/ctechnews/article/b1kaznhzh" rel="noopener" target="_blank">PWNYOURHOME</a>” vulnerability, which targets iPhones’ built-in HomeKit functionality and exploits iMessage to install the spyware. Scott-Railton says this hack is possible even on devices where HomeKit was never activated.</p><p>Citizen Lab collected “forensic artifacts” from Timchenko’s iPhone showing that the device was infected with Pegasus on February 10, 2023.&nbsp;</p><h3><strong>Wild timing</strong></h3><p>As managers crowded into Galina Timchenko’s office and scrambled to assess the worst intrusion in Meduza’s history, another event back in Russia suddenly demanded the newsroom’s complete attention: a mercenary leader shot down several helicopters, seized a military base, and announced a “march on Moscow.” It was June 23, 2023, and the Pegasus hack silently took a backseat to Yevgeny Prigozhin’s mutiny as Meduza mobilized its newsroom to cover the breaking story.</p><p>When the senior staff could later contemplate the possible reasons for Timchenko’s Pegasus infection, the date of the infiltration (February 10, 2023) wasn’t immediately significant to managers. But it should have been.</p><p>On February 11, one day after Pegasus hijacked Timchenko’s iPhone, she and Kolpakov joined other representatives of Russia’s exiled independent media in Berlin at a confidential seminar organized by the Redkollegia journalistic prize committee. Media managers and lawyers attended the private conference to discuss the legal aspects of operating in Russia under the conditions of total state censorship and the mass persecution of journalists and activists. Just two weeks earlier, Russia’s Prosecutor General formally outlawed Meduza’s reporting, designating the outlet an “undesirable organization.” Timchenko recalls that colleagues meeting in Germany expected the same thing would happen to them before long.</p><p>Pegasus was already running on Timchenko’s phone when she joined the meeting in Berlin. Whoever hacked the device could have used it as a <a href="https://www.washingtonpost.com/technology/2021/07/19/apple-iphone-nso/" rel="noopener" target="_blank">wiretap</a>, remotely activating the microphone to record anything said within earshot. The hackers might have turned on the camera just as easily. “They could have used Galina’s phone like a bug to listen in on what the Russian journalists were planning,” says Access Now’s Natalia Krapiva.</p><p>“My first thought was the Russian state and the Russian intelligence agencies, of course,” recalls Timchenko. “Who else cares about me?”</p><h3><strong>The first Russian journalist</strong></h3><p>The attack against Galina Timchenko is the <span data-id="1463701" data-title="First confirmed case" data-body="<p>Others have reported Pegasus attacks, but Timchenko’s is the first infection that researchers have verified. For example, investigative journalists uncovered a <a href=&quot;https://www.theguardian.com/news/2021/jul/21/telegram-founder-pavel-durov-listed-spyware-targets-nso-leak-pegasus&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;>list of individuals</a> selected by NSO Group’s client governments that included Telegram founder Pavel Durov’s British telephone number, but there is no public record of a forensic examination of Durov’s phone to confirm if the malware was ever installed.&amp;nbsp;</p>">first confirmed case</span> of Pegasus being used against a Russian journalist. Natalia Krapiva at Access Now confessed to Meduza that she’s actually somewhat comforted to see the spyware surface here because researchers have tested the phones of nearly two dozen journalists and activists from Russia and found all manner of malware but never Pegasus. “I was afraid that [they] were being tracked by something we couldn’t detect,” she explained. “The first confirmed case was shocking, thrilling, and a relief all at once. Now, at least, we have a thread to pull.”</p><p>Identifying Pegasus infections is challenging work, even for technical experts. “These spyware programs are capable of hiding logfiles and concealing traces of their own presence on a device,” explains John Scott-Railton at Citizen Lab. “It’s a constant <a href="https://citizenlab.ca/2023/04/nso-groups-pegasus-spyware-returns-in-2022/" rel="noopener" target="_blank">technological race</a>.”&nbsp;</p><p>In 2016, it was researchers at Citizen Lab and Lookout Security who first uncovered traces of the existence of Pegasus, revealing in a <a href="https://citizenlab.ca/2016/08/million-dollar-dissident-iphone-zero-day-nso-group-uae/" rel="noopener" target="_blank">bombshell report</a> that NSO Group’s “remote monitoring solution” was used to spy on Ahmed Mansoor, an internationally recognized human rights defender based in the United Arab Emirates. In the years since this discovery, experts have <a href="https://newnaratif.com/pegasus-spyware-in-southeast-asia/" rel="noopener" target="_blank">tracked</a> Pegasus’s digital footprints and <a href="https://citizenlab.ca/2018/09/hide-and-seek-tracking-nso-groups-pegasus-spyware-to-operations-in-45-countries/" rel="noopener" target="_blank">learned</a> which states are NSO Group’s clients.</p><p>Much of Citizen Lab’s work is devoted to searching for the servers needed to run Pegasus. “It’s a service, and NSO Group sells access to it,” says Krapiva. “When it signs a contract, the company sends a whole team to the client state to organize training sessions on how to run the tool. All this requires technical infrastructure, and Citizen Lab is constantly trying to monitor it.”</p><p>Scott-Railton told Meduza that his team looks not just for the infrastructure used in attacks but also for what’s needed to extract data. “In other words,” he explained, “[we look for] all the servers where the information collected from infected devices ends up.”</p><hr><h4>A message from Galina Timchenko:</h4><p>Sometimes we become the heroes of our own stories: it's a rather strange experience to turn from the subject into the object. In my case, first as the object of an attack, and then as the object of an investigation. But it's at precisely these moments that you realize what good people you have in your corner: fellow journalists, developers, security specialists, and most importantly, readers. Millions of people in Russia who haven't give up, despite enormous pressure. Hundreds of thousands around the world who understand the value of freedom of speech. <strong>We need your help to continue our work. <a href="https://support.meduza.io/en" rel="noopener" target="_blank">Support Meduza</a>.</strong></p><hr><h3><strong>The no-no list</strong></h3><p>NSO Group says it sells its spyware only to vetted state agencies, but Israeli geopolitical interests often influence the company’s decision to work with particular partners. For these reasons, the firm reportedly <a href="https://www.washingtonpost.com/national-security/2021/07/19/us-phone-numbers-nso/" rel="noopener" target="_blank">refuses</a> to use Pegasus against either American or Russian telephone numbers.&nbsp;</p><p>“Infected phones cannot even be physically located in the United States; if one does find itself within American borders, the Pegasus software is supposed to self-destruct,” the spyware’s designers said in 2020. A year earlier, when the Estonian government bought access to Pegasus, NSO Group informed its new client that using the spyware against Russian targets is prohibited. Israel has also <a href="https://www.theguardian.com/world/2022/mar/23/israel-ukraine-pegasus-spyware-russia" rel="noopener" target="_blank">reportedly blocked</a> Ukraine from acquiring Pegasus, fearing Moscow’s wrath. “According to people close to NSO and the Israeli government, they don’t approve such infections because it will disrupt relations with these countries,” says Natalia Krapiva.&nbsp;</p><p>The company has also <a href="https://www.technologyreview.com/2020/08/19/1006458/nso-spyware-controversy-pegasus-human-rights/" rel="noopener" target="_blank">claimed</a> that Russia and China are among the nations that will “never be customers,” citing internal due diligence that scrutinizes potential clients’ track records on human rights, corruption, safety, finance, and abuse. NSO Group chief executive <a href="https://www.wsj.com/articles/head-of-israeli-cyber-firm-nso-group-reaffirms-company-commitment-to-spyware-11674738269" rel="noopener" target="_blank">Yaron Shohat</a> told The Wall Street Journal in January 2023 that the firm was “committed to its core business of supplying governments around the world who are allies of the U.S. and Israel,” despite downsizing after losing clients because of the Biden administration’s measures.</p><p>Moscow possibly has its own reasons for refusing to do business with NSO Group. Investigative journalist Andrey Soldatov has <a href="https://www.themoscowtimes.com/2021/07/21/why-is-russia-not-using-pegasus-spyware-a74572" rel="noopener" target="_blank">argued</a> that Russia’s intelligence community “is a seller, not a buyer,” on the world market for espionage technology. Soldatov says this is due both to the high quality of Russian spying tech and to the authorities’ “extreme paranoia about foreign spyware.” Revelations about Pegasus, moreover, have corroborated these concerns, showing that the data stolen from targets are transferred to servers in NSO Group’s ecosystem, meaning that Russian agencies would have to share this “information goldmine” with outsiders if they were to sign up as clients. Russia’s Federal Security Service did not respond to Meduza’s questions about Pegasus.</p><p>“We do not see evidence of Russia using NSO’s product, but that doesn’t mean we know everything,” says John Scott-Railton at Citizen Lab.</p><p>A spokesperson for NSO Group told Meduza that the company’s technologies “are only sold to allies of the U.S. and Israel, particularly in Western Europe, for the sole purpose of fighting crime and terror, aligned with the global interests of U.S. national security and governmental law enforcement agencies.”</p><p>“Pegasus systems log every attack in case there is a complaint, and — with the client’s permission — NSO can perform an after-the-fact forensic analysis,” The New York Times <a href="https://www.nytimes.com/2022/01/28/magazine/nso-group-israel-spyware.html" rel="noopener" target="_blank">reported</a> in January 2022. Six months later, NSO Group general counsel and chief compliance officer Chaim Gelfand told a European Parliament committee that these internal investigations have led to the termination of contracts in <a href="https://www.euractiv.com/section/digital/news/eu-parliaments-pegasus-committee-fires-against-nso-group/" rel="noopener" target="_blank">eight cases</a>.&nbsp;</p><p>A year earlier, however, when The Washington Post reported forensic data indicating multiple Pegasus intrusion attempts against Jamal Khashoggi’s wife in the months before his murder, NSO Group’s chief executive <a href="https://www.washingtonpost.com/nation/interactive/2021/hanan-elatr-phone-pegasus/" rel="noopener" target="_blank">said</a> a “thorough check of the firm’s client records” revealed no evidence of Pegasus used against Khashoggi or his loved ones.&nbsp;</p><p>“After hundreds of victims, we have concluded that the internal review process either doesn’t exist or exists only for show,” says Natalia Krapiva at Access Now. “When a Human Rights Watch employee was infected, NSO responded to all the questions in just a few lines: ‘Thank you, we found nothing with our current customers. Goodbye.’ Of course, they said nothing about what their past clients could have done. It’s all gaslighting.”</p><h3><strong>Kazakhstan and Azerbaijan</strong></h3><p>In its study of Galina Timchenko’s phone infection, Access Now notes that either Kazakhstan or Azerbaijan — two suspected Pegasus clients — could have carried out the attack at Moscow’s request. (According to Access Now, Uzbekistan is not believed to have been a Pegasus customer during the period in question.) “There’s a provisional theory that Russia might have asked its partners,” says Krapiva. “Kazakhstan, for example, has already blocked Meduza <a href="https://tengrinews.kz/kazakhstan_news/meduzaio-snova-zablokirovali-v-kazahstane-410580/" rel="noopener" target="_blank">twice</a> without any requests.”</p><p>As far as researchers know, however, neither Kazakhstan nor Azerbaijan has ever executed a Pegasus attack in Europe, and Timchenko was in Germany when the infection occurred.&nbsp;</p><p>Moreover, evidence collected by Citizen Lab shows that Kazakhstan does not use Pegasus beyond its borders. Scott-Railton told Meduza that Azerbaijan does use the spyware abroad, but researchers have recorded these attacks in no other country except Armenia, which could <a href="https://www.accessnow.org/publication/armenia-spyware-victims-pegasus-hacking-in-war/" rel="noopener" target="_blank">explain</a> how the phone numbers of Armenian human rights activists have been infected.</p><p>Natalia Krapiva says clients need a bonus package to use Pegasus beyond their borders: “We believe that different NSO customers can purchase different types of licenses. Some buy the rights to hack only within their country. Others buy the rights to infect a large number of countries. We still don’t understand a lot about these secret contracts, but infections outside a client’s state likely require special permission.”</p><h3><strong>Latvia, Estonia, and Germany</strong></h3><p>Timchenko’s hacked iPhone had a Latvian SIM card. Citizen Lab <a href="https://citizenlab.ca/2018/09/hide-and-seek-tracking-nso-groups-pegasus-spyware-to-operations-in-45-countries/" rel="noopener" target="_blank">recorded</a> the first Pegasus-related activity in Latvia in 2018, and experts believe Riga still uses NSO Group’s products today, says Scott-Railton.&nbsp;</p><p>Access Now also does not rule out that the Latvian intelligence community carried out the attack on Meduza’s co-founder. Just two months before Timchenko’s phone was infected, Latvia <a href="https://meduza.io/en/feature/2022/12/06/meduza-s-statement-regarding-the-revocation-of-tv-rain-s-latvian-broadcasting-license" rel="noopener" target="_blank">declared</a> another Russian media organization in exile — TV Rain — to be “a threat to the national security and public order” and canceled its local broadcasting license. “Because of the invasion of Ukraine, there’s distrust of all Russians without exception,” says Natalia Krapiva. “If such surveillance is taking place, it’s very consistent with <a href="https://www.politico.eu/article/petr-pavel-russia-czech-republic-surveillance/" rel="noopener" target="_blank">remarks</a> by the president of the Czech Republic, Petr Pavel, who said intelligence agencies should place all Russians living in the West under ‘strict surveillance’ as the price of Russia’s war against Ukraine.”</p><p>However, experts at Citizen Lab have never observed Riga using Pegasus against targets outside Latvia’s borders, and Galina Timchenko was in Berlin when her phone was compromised. (Whom exactly Riga has infected with Pegasus remains unknown.)</p><p>Ivars Ijabs, a European Parliament member from Latvia who participates in a committee investigating Pegasus in Europe, told <a href="https://www.baltictimes.com/latvian_mep_ivars_ijabs___unfortunately___spyware_has_been_sold_or_rented_out_to_some_eu_countries_and_some_use_it_/" rel="noopener" target="_blank">The Baltic Times</a> in January 2023 that his home country is not among the E.U. members using the “famous Israeli spyware.” But NGOs that monitor Pegasus attacks treat such statements with skepticism. “He’s not the first official to say such things, even in the face of evidence,” notes Krapiva.</p><p>While there’s no proof that Lithuania has used Pegasus, researchers have <a href="https://www.theguardian.com/world/2022/mar/23/israel-ukraine-pegasus-spyware-russia" rel="noopener" target="_blank">confirmed</a> that the Estonian authorities bought access to the spyware in 2019. Citizen Lab has corroborated these findings. More importantly, says Scott-Railton, his team has tracked Estonia “infecting targets beyond its borders in many E.U. countries, including in Germany.”</p><p>Acting under the “<a href="https://www.dw.com/en/german-police-secretly-bought-nso-pegasus-spyware/a-59113197" rel="noopener" target="_blank">utmost secrecy</a>,” the German Federal Criminal Police Office procured its own Pegasus access in 2019 but <a href="https://www.securityweek.com/germany-admits-police-used-controversial-pegasus-spyware/" rel="noopener" target="_blank">acknowledged</a> the purchase only two years later. Natalia Krapiva says Germany has tried, albeit unconvincingly, to defend its actions as in step with European laws and democratic values:</p><p>The <a href="https://edps.europa.eu/data-protection/our-work/publications/papers/edps-preliminary-remarks-modern-spyware_en" rel="noopener" target="_blank">report</a> by the European Data Protection Supervisor states explicitly that Pegasus in its original form is fundamentally incompatible with E.U. laws, so Germany, in its own words, is using a “special version that doesn’t violate privacy rights” — some kind of “Pegasus Lite.” But we’ve received no evidence of this, not even an idea of what a “lite” version might be. Also, the European Data Protection Supervisor <a href="https://edps.europa.eu/data-protection/our-work/publications/papers/edps-preliminary-remarks-modern-spyware_en" rel="noopener" target="_blank">concludes</a> that Pegasus in any form is fundamentally incompatible with E.U. law.</p><p>Germany’s Pegasus access reportedly came with “certain functions blocked to prevent abuse,” sources in security circles told <a href="https://www.dw.com/en/german-police-secretly-bought-nso-pegasus-spyware/a-59113197" rel="noopener" target="_blank">journalists</a>, but officials have not explained how this works practically.</p><p>John Scott-Railton at Citizen Lab says the infection of Timchenko’s phone in Berlin “is a reminder that Europe has an unresolved problem with Pegasus.” “Why Germany isn’t interested in solving this is a mystery to me,” he told Meduza. “For example, why hasn’t Berlin signed the <a href="https://www.whitehouse.gov/briefing-room/statements-releases/2023/03/30/joint-statement-on-efforts-to-counter-the-proliferation-and-misuse-of-commercial-spyware/#:~:text=Our%20efforts%20will%20allow%20us,rule%20of%20law%2C%20and%20civil" rel="noopener" target="_blank">Joint Statement on Efforts to Counter the Proliferation and Misuse of Commercial Spyware</a>? It’s been signed by 11 countries, including Denmark, France, and Sweden.”&nbsp;</p><p>Access Now points out that the four E.U. members that have become new centers of Russian anti-war emigration — Latvia, Estonia, Germany, and the Netherlands — are all suspected Pegasus users. In fact, the <a href="https://www.europarl.europa.eu/committees/en/pega/home/highlights" rel="noopener" target="_blank">E.U. PEGA Committee</a> revealed at least <a href="https://www.accessnow.org/eu-pegasus-spyware/" rel="noopener" target="_blank">14 E.U. states and 22 operators</a> of Pegasus in the European Union, and only NSO Group’s contracts with Hungary and Poland are no more. Access Now considers the attack on Galina Timchenko to be at least the fourth in a series of similar cases across Europe in the past year. (Meduza knows the details of these other attacks, but the victims have asked for privacy.)</p><p>The growing tendency in Europe to treat journalists as a threat has also started manifesting in E.U. laws, says Krapiva. The European Commission recently adopted new rules intended to protect reporters against malware, but some member states — mainly France and Sweden — watered down the language in the <a href="https://amp.theguardian.com/world/2023/jun/22/draft-eu-plans-to-allow-spying-on-journalists-are-dangerous-warn-critics" rel="noopener" target="_blank">European Media Freedom Act</a> in such a way that the law actually legitimizes the surveillance of journalists on national security grounds, Krapiva warns.</p><h3><strong>The road ahead</strong></h3><p>“I’m absolutely shocked we’re seriously discussing that a European state could have done this,” says Ivan Kolpakov, Meduza’s editor-in-chief. “I’m probably naive, but this seemed impossible to me. The consequences could be devastating, and this concerns not just the news media in exile but the media in Europe generally. If such software could be installed on the phone of a journalist from Russia, who knows what’s stopping European intelligence agencies from infecting any journalist at all.”</p><p>“I can’t reconstruct the logic of European intelligence agencies that might have installed Pegasus, and I don’t want to make assumptions,” says Galina Timchenko. “Moving forward, we’ll act in accordance with what our lawyers advise. I won’t be silent.”&nbsp;</p><p>NSO Group declined to answer Meduza’s questions about whether it knew of the attack on Timchenko and which of its clients might have staged the intrusion. The company’s spokesperson also did not say if it is aware of cases in which Pegasus has been used against journalists in European countries or against Russian nationals, or if NSO Group knows of situations where one E.U. member state spied on a target in another E.U. member state.</p><p>In any case, NSO Group admits no responsibility for the attack on Timchenko. The company’s spokesperson stressed that the firm “investigates all credible allegations of misuse” but did not say if NSO is prepared to conduct an internal investigation into the use of Pegasus against Meduza’s co-founder and publisher.</p><p>Today, Ms. Timchenko carries two phones: a new one she bought after the intrusion and the formerly infected gadget (Citizen Lab confirmed that Pegasus is no longer installed on the device). She says she decided to keep it as a souvenir. “There’s nothing on it except messages with my hairdresser and manicurist,” she says. “Let it be. It will remind me to keep looking over my shoulder.”</p><p>Given the enormous cost of using Pegasus, Timchenko is still confounded that someone infected her with the spyware. “Just what were they planning to find? They put me under a magnifying glass, hoping to catch something… Go ahead and watch, you creeps! Feast your eyes.”</p><p>Whatever happens with Timchenko’s case, NSO Group currently faces multiple lawsuits, including one from <a href="https://thehill.com/homenews/ap/ap-business/apple-to-add-lockdown-safeguard-on-iphones-ipads-macs/?email=2bd1ac6da700241a5d265ad2552e521de17fb0a4&amp;emaila=29a3bf909662b52e722ab9fa8cfe64ca&amp;emailb=7d7168def837475e171e85ff6c1d865452fcebe7dfd653fc8df6c0ec1815818f" rel="noopener" target="_blank">Apple</a>, which accuses the Israeli firm’s employees of being “amoral 21st-century mercenaries.” <a href="https://slate.com/technology/2021/07/nso-group-pegasus-spyware.html" rel="noopener" target="_blank">Amnesty International</a>, <a href="https://www.youtube.com/watch?v=_IUoH78802o" rel="noopener" target="_blank">members</a> of the European Parliament, former U.N. Freedom of Expression Special Rapporteur <a href="https://www.reuters.com/article/us-socialmedia-un-spyware/u-n-surveillance-expert-urges-global-moratorium-on-sale-of-spyware-idUSKCN1TJ2DV" rel="noopener" target="_blank">David Kaye</a>, and others have endorsed a global moratorium on the sale of all such surveillance technology until more rigorous rules and regulations can be implemented internationally.&nbsp;</p><p>“State-sponsored actors like the NSO Group spend millions of dollars on sophisticated surveillance technologies without effective accountability. That needs to change,” <a href="https://www.apple.com/newsroom/2021/11/apple-sues-nso-group-to-curb-the-abuse-of-state-sponsored-spyware" rel="noopener" target="_blank">said</a> Craig Federighi, Apple’s senior vice president of software engineering.</p><p>NSO Group has turned to <a href="https://meduza.io/feature/2023/04/03/dzho-bayden-v-2021-godu-nalozhil-sanktsii-na-kompaniyu-razrabotchika-shpionskogo-po-pegasus-a-cherez-pyat-dney-ona-poluchila-ot-vlastey-ssha-taynyy-kontrakt" rel="noopener" target="_blank">lobbying</a> as these pressures mount, especially in America. “They’re making a big effort to lift the U.S. sanctions,” Krapiva told Meduza. “Recently, <a href="https://www.theguardian.com/us-news/2023/jun/14/nso-group-spyware-pegasus-takeover" rel="noopener" target="_blank">Robert Simonds</a>, a Hollywood producer who’s worked with Adam Sandler, was eyeing an investment in NSO Group. So, they’re staying the course.”</p><p>Since June 2023, experts have analyzed the phones of several dozen Meduza employees. It’s still unknown what specific information Timchenko’s attackers were after. This ambiguity worries Meduza’s technical director, Alexey, more than anyone.</p><p>“Until I know the motive, I have to expect the worst,” says Alexey. “I deal with our security not just in a technical but in the broadest sense of the word: every day, I think through how they’re going to kill us and bring us down. Surveillance, harassment, threats — I’ve already considered all these scenarios and experienced them myself, in a sense. As for Pegasus, until we have more details, we can’t rule out that Russia could have ordered the infection and that this spying could have the most serious consequences, right up to somebody being eliminated.”</p><p>Timchenko, meanwhile, says she hasn’t yet contemplated such consequences of being watched through Pegasus. “I already look back wherever I go and watch for anyone following me in a car. Meduza’s founders have always lived like this,” she says. “If they want to do it, they’ll do it.”</p><blockquote>If you believe you may be under spyware surveillance, backup your device (here are instructions for <a href="https://support.apple.com/guide/iphone/back-up-iphone-iph3ecf67d29/ios" rel="noopener" target="_blank">iPhone</a> and <a href="https://support.google.com/googleone/answer/9149304?hl=en&amp;co=GENIE.Platform%3DAndroid" rel="noopener" target="_blank">Android</a>) to preserve evidence of a possible attack, and <a href="https://www.accessnow.org/help/" rel="noopener" target="_blank">contact</a> Access Now.</blockquote><div data-testid="spoiler"><p>According to Access Now, the following factors are reasonable grounds for checking your device for a spyware infection:</p><ul data-testid="list-block"><li>You have faced persecution by the state authorities in the past (you were a suspect in a political case, you were designated a “foreign agent,” you were attacked, or you received threats)</li><li>You or someone close to you has already been targeted in a digital attack</li><li>You or someone close to you has received notification(s) from Apple, Google, Meta, or another major tech company about possible malware attacks</li><li>You have received suspicious messages via SMS, instant messengers, or email</li><li>You have noticed “unusual login attempts” to your accounts</li></ul></div><div data-testid="material-note"><p><a href="https://meduza.io/feature/2023/09/13/my-vse-ispugalis-no-delali-vid-chto-net" rel="noopener" target="_blank">Story</a> by <strong>Lilia Yapparova</strong></p><p>Adapted for Meduza in English by <strong>Kevin Rothrock</strong></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Against LLM Maximalism (113 pts)]]></title>
            <link>https://explosion.ai/blog/against-llm-maximalism/</link>
            <guid>37495873</guid>
            <pubDate>Wed, 13 Sep 2023 12:32:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://explosion.ai/blog/against-llm-maximalism/">https://explosion.ai/blog/against-llm-maximalism/</a>, See on <a href="https://news.ycombinator.com/item?id=37495873">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>A lot of people are building truly new things with Large Language Models (LLMs), like wild interactive fiction experiences that weren’t possible before. But if you’re working on the same sort of Natural Language Processing (NLP) problems that businesses have been trying to solve for a long time, what’s the best way to use them?</p>
<p>Companies have been using language technologies for many years now, often with mixed success. The need to work with text or speech data somewhat intelligently is pretty fundamental. For instance, in most popular websites text is usually either a big part of the product (e.g. the site publishes news or commentary), the usage pattern (e.g. users write text to each other) or the input (e.g. news aggregators). Transacting in language makes up a large percentage of economic activity in general. We all spend a big part of our working lives writing, reading, speaking and listening. So, it makes sense that do-things-with-language has long been a desirable feature request for all sorts of programs. In 2014 I started working on <a href="https://spacy.io/" target="_blank" rel="noopener">spaCy</a>, and here’s an excerpt of how I explained the motivation for the library:</p>
<figure>
Computers don’t understand text. This is unfortunate, because that’s what the web almost entirely consists of. We want to recommend people text based on other text they liked. We want to shorten text to display it on a mobile screen. We want to aggregate it, link it, filter it, categorize it, generate it and correct it. spaCy provides a library of utility functions that help programmers build such products.
</figure>
<p>Today it’s hard to make the claim <em>computers don’t understand text</em> without at least adding huge asterisks or qualifications. Even if you have a particular philosophical view of what constitutes “understanding”, there’s no doubt that LLMs can construct and manipulate meaning representations sufficient for a wide range of practical purposes. They still make all sorts of mistakes, but it’s relatively rare to feel like they’ve simply failed to connect the words of your input together to form the intended meaning. The text that they generate is also extremely fluent. Sometimes it can be confidently wrong or irrelevant to your question, but it’s almost always made up of real, coherent sentences. That’s definitely new.</p>
<p>However, LLMs are not a direct solution to most of the NLP use-cases companies have been working on. They are extremely useful, but if you want to deliver reliable software you can improve over time, you can’t just write a prompt and call it a day. Once you’re past prototyping and want to deliver the best system you can, supervised learning will often give you better efficiency, accuracy and reliability than in-context learning for non-generative tasks — tasks where there is a specific right answer that you want the model to find. Applying rules and logic around your models to do data transformations or handle cases that can be fully enumerated is also extremely important.</p>
<p>For instance, let’s say you want to build an online reputation management system. You want to ingest a feed of posts from Twitter, Reddit or some other source, identify mentions of your company or products, and understand common themes between them. Perhaps you also want to monitor mentions of key competitors in a similar way. You might want to view the data in a variety of ways. For instance, you could extract a few noisy metrics, such as a general “positivity” sentiment score that you track in a dashboard, while you also produce more nuanced clustering of the posts which are reviewed periodically in more detail.</p>
<p>I don’t want to undersell how impactful LLMs are for this sort of use-case. You can give an LLM a group of comments and ask it to summarize the texts or identify key themes. And the specifics of this can change at runtime: you don’t have to target a very particular sort of summary or question that will be posed. This generative output could be a complete game-changer, finally delivering the “insights” that data science projects have generally over-promised and under-delivered. In addition to these generative components, you could also use LLMs to help with various other parts of the system. But should you?</p>
<p>LLMs are new enough, and changing quickly enough, that there’s little consensus on how best to use them. Eventually things will stabilize (or perhaps the AGI will get us all after all), and we’ll have some scars and accreted wisdom about what works and what doesn’t. In the meantime, I want to suggest some common sense.</p>
<p>One vision for how LLMs can be used is what I’ll term <em>LLM maximalist</em>. If you have some task, you try to ask the LLM to do it as directly as possible. Need the data in some format? Ask for it in the prompt. Avoid breaking down your task into several steps, as that will prevent the LLM from working through your problem end-to-end. It also introduces extra calls, and can introduce errors in the intermediate processing steps. Of course, LLMs do have limitations, for instance in the recency of their knowledge, or the size of the context you can pass in. So you do have to work around things, and use things like vector databases or other tricks. But fundamentally the LLM maximalist position is that you want to trust the LLM to solve the problem. You’re preparing for the technologies to continue to improve, and the current pain-points to keep reducing over time.</p>
<figure>
<span>
      <a href="https://explosion.ai/static/1863c4dfa57ad28dbbd68e432bde34e9/66a58/llm-maximalism_meme.jpg" target="_blank" rel="noopener">
    <span></span>
  <picture>
          <source srcset="https://explosion.ai/static/1863c4dfa57ad28dbbd68e432bde34e9/c3814/llm-maximalism_meme.webp 201w, https://explosion.ai/static/1863c4dfa57ad28dbbd68e432bde34e9/a8f46/llm-maximalism_meme.webp 401w, https://explosion.ai/static/1863c4dfa57ad28dbbd68e432bde34e9/2523c/llm-maximalism_meme.webp 653w" sizes="(max-width: 653px) 100vw, 653px" type="image/webp">
          <source srcset="https://explosion.ai/static/1863c4dfa57ad28dbbd68e432bde34e9/3ff38/llm-maximalism_meme.jpg 201w, https://explosion.ai/static/1863c4dfa57ad28dbbd68e432bde34e9/d852d/llm-maximalism_meme.jpg 401w, https://explosion.ai/static/1863c4dfa57ad28dbbd68e432bde34e9/66a58/llm-maximalism_meme.jpg 653w" sizes="(max-width: 653px) 100vw, 653px" type="image/jpeg">
          <img src="https://explosion.ai/static/1863c4dfa57ad28dbbd68e432bde34e9/66a58/llm-maximalism_meme.jpg" alt="Left: &quot;Noo you can't just mix up all the steps of your task and ask an LLM to do it all. How will you ever make a reliable and extensible system that way?&quot; Right: &quot;Haha LLM go brrr&quot;" title="Left: &quot;Noo you can't just mix up all the steps of your task and ask an LLM to do it all. How will you ever make a reliable and extensible system that way?&quot; Right: &quot;Haha LLM go brrr&quot;" loading="lazy" decoding="async">
        </picture>
  </a>
    </span>
<figcaption>It's never great to realize you're the guy on the left. But, here I am.</figcaption>
</figure>
<p>There are two big problems with this approach. One is that “working around the system’s limitations” is often going to be outright impossible. Most systems need to be much faster than LLMs are today, and on current trends of efficiency and hardware improvements, will be for the next several years. Users are pretty tolerant of latency in chat applications, but in almost any other type of user interface, you can’t wait multiple seconds for a single prediction. It’s just too slow. In our online reputation management example, you want to connect to some sort of firehose of data, like Reddit or Twitter. You can’t pass that straight into an LLM — it’s much too expensive.</p>
<p>The second problem is that the LLM maximalist approach is fundamentally not modular. Let’s say you’ve made the obvious small compromise, and you’re using a separate classifier to pre-filter texts that maybe mention your company. You’ve developed a prompt that works well with the LLM model you’re using, and you’re getting pretty good output summaries. Now you get a new request. Users want to be able to view some of the raw data. To address this need, your team decides to develop a separate view, where you display the list of mentions along with one sentence of context.</p>
<p>How should you go about this? You could craft a separate LLM prompt, where you ask it to extract the data with this second format you’ve been asked for. However, the new prompt is not guaranteed to recognize the same set of mentions as the first prompt you’ve used — inevitably, there will be some differences. This is really not great. You want to be able to link the summaries to the groups of comments they were generated from. If the sentence view is different, you can’t do that. Instead of a separate prompt, you could try to add the information to the first prompt. But now you’re outputting whole sentences, which really increases the number of tokens you generate, which is both slower and more expensive. And you’re struggling to get the same accuracy you had before with this new more complicated output format.</p>
<p>What makes a good program? It’s not only how efficiently and accurately it solves a single set of requirements, but also how reliably it can be understood, changed and improved. Programs written with the LLM maximalist approach are not good under these criteria.</p>
<p>Instead of throwing away everything we’ve learned about software design and asking the LLM to do the whole thing all at once, we can break up our problem into pieces, and treat the LLM as just another module in the system. When our requirements change or get extended, we don’t have to go back and change the whole thing. We can add new modules, or rearrange the ones we’re already happy with.</p>
<p>Breaking up the task into separate modules also helps you to see which parts really need an LLM, and what could be done more simply and reliably with another approach. Recognizing sentence boundaries in English isn’t entirely trivial (you don’t want to just use regular expressions), but it’s definitely not something you need an LLM to do. You can just call into spaCy or another library. It will be vastly faster, and you won’t have to worry that the LLM will trip over some strange input and return some entirely unexpected output.</p>
<p>The task of detecting the company mentions is also something you probably don’t need to use an LLM to do. It certainly makes sense to use an LLM for the initial prototyping — this is another huge advantage of LLMs that should not be underestimated. Rapid prototyping is enormously important. You can explore the design space efficiently, and discard ideas that aren’t worth further development. But you also need to be able to go <em>past</em> prototyping. Once you’ve found an idea that’s worth improving, you need a way to actually improve it.</p>
<p>Before you can improve any statistical component, you need to be able to evaluate it. It’s important to have some evaluation over your whole pipeline, and if you have nothing else, you can use that to judge whether some change to a component is making things better or worse (this is called “extrinsic evaluation”). But you should also evaluate your components in isolation (“intrinsic evaluation”). For components like the mention detector, this means annotating some texts with the correct labels, setting them aside, and testing your component against them after each change. For generative components, you can’t evaluate against a single set of annotations, but intrinsic evaluation is still possible, for instance using a Likert scale or A/B testing.</p>
<p>Intrinsic evaluation is like a unit test, while extrinsic evaluation is like an integration test. You do need both. It’s very common to start building an evaluation set, and find that your ideas about how you expect the component to behave are much vaguer than you realized. You need a clear specification of the component to improve it, and to improve the system as a whole. Otherwise, you’ll end up in a local maximum: changes to one component will seem to make sense in themselves, but you’ll see worse results overall, because the previous behavior was compensating for problems elsewhere. Systems like that are very difficult to improve.</p>
<p>A good rule of thumb is that you’ll want ten data points per significant digit of your evaluation metric. So if you want to distinguish 91% accuracy from 90% accuracy, you’ll want to have at least 1000 data points annotated. You don’t want to be running experiments where your accuracy figure says a 1% improvement, but actually you went from 94/103 to 96/103. You’ll end up forming superstitions, based on little but luck. That’s not a path to improvement. You need to be systematic.</p>
<p>Once you’ve annotated evaluation data, it’s usually better to just go on and annotate some training data for non-generative components. Supervised learning is very strong for tasks such as text classification, entity recognition and relation extraction. If you have a clear idea of what the component should do and can annotate data accordingly, you can usually expect to get better accuracy than an LLM with a few hundred labelled examples, using a transformer architecture sized for a single GPU, with pretrained representations. This is really just the same model architecture as an LLM, but in a more convenient size, and configured to perform exactly one task.</p>
<p>Here’s how I think LLMs should be used in NLP projects today — an approach I would call <em>LLM pragmatism</em>.</p>
<ul>
<li>Break down what you want your application to do with language into a series of predictive and generative steps.</li>
<li>Keep steps simple, and don’t ask for transformations or formatting you could easily do deterministically.</li>
<li>Put together a prototype pipeline, using LLM prompts or off-the-shelf solutions for all the predictive or generative steps.</li>
<li>Try out the pipeline in as realistic a context as you can.</li>
<li>Design some sort of extrinsic evaluation. What does success look like here? Net labour saved? Engagement? Conversions? If you can’t measure the utility of the system directly, you can use some other sort of metric, but you should try to make it as meaningful as possible. If false negatives matter more than false positives, account for that in your extrinsic evaluation metric.</li>
<li>Experiment with alternative pipeline designs. Try to create tasks where the correct answer makes sense independent of your use-case. Prefer text classification to entity recognition, and entity recognition to relation extraction (faster to annotate, accuracy is better).</li>
<li>Pick a predictive (as opposed to generative) component and spend two to five hours labelling annotation data for it.</li>
<li>Measure the LLM-powered component’s accuracy using your evaluation data.</li>
<li>Use the LLM-powered component to help you create training data, to train your own model. One approach is to simply save out the LLM-powered component’s predictions, and trust they’re good enough. This is a good thing to try if the LLM-powered component’s accuracy seems more than sufficient for your needs. If you need better accuracy than what the LLM is giving you, you need example data that’s more correct. A good approach is to load the LLM predictions into an annotation tool and fix them up.</li>
<li>Train a supervised model on your new training data, and evaluate it on the same evaluation data you used previously.</li>
<li>To decide whether you should annotate more training data, run additional experiments where you hold part of your training data back. For instance, compare how your accuracy changed when you use 100%, 80% and 50% of the data you have. This should help you determine how your accuracy might look if you had 120% or 150%. However, be aware that if your training set is small, there can be a lot of variance in your accuracy. Try a few different random seeds to figure out how much your accuracy changes simply due to chance, to help put your results into perspective.</li>
<li>Repeat this process with any other predictive components.</li>
</ul>
<p>At the end of all this, you’ll have a pipeline suitable for production. It will run much more quickly and accurately than a chain of LLM calls, and you’ll know that no matter what text is passed in, your predictive components will always give you valid output. You’ll have evaluations for the different steps, and you’ll be able to attribute errors to different components. If you need to change the system’s behavior, you’ll be able to put in new rules or transformations at different points of the pipeline, without having to go back and re-engineer your prompts or rewrite your response parsing logic.</p>
<p>Some of these steps are still a bit harder than they should be, especially if you haven’t been working with machine learning before. This is where I have high hopes for LLMs. LLMs are indeed very powerful, and they can make a lot of things much easier. They can help us build better systems, and that’s how we should use them. The approach that I’ve called <em>LLM maximalist</em> is actually unambitious. It uses LLMs to easily get to a system that is worse — worse in cost, worse in runtime, worse in reliability, worse in maintainability. Instead, we should use LLMs to help us get to systems which are better. This means leaning on LLMs much more during development, to break down knowledge barriers, create data, and otherwise improve our workflow. But the goal should be to call out to LLMs as little as possible during runtime. Let LLMs train their own cheaper and more reliable replacements.</p>
<h2 id="appendix1"><a href="#appendix1">Appendix 1: Putting it into practice </a></h2>
<p>There are lots of tools and libraries you can use to label your own data and train your own NLP models. <a href="https://huggingface.co/docs/transformers/index" target="_blank" rel="noopener nofollow noreferrer">HF Transformers</a> and <a href="https://spacy.io/" target="_blank" rel="noopener">spaCy</a> (our library) are the two most popular libraries for this. Transformers makes it easy to use a wide variety of models from recent research, and it’s closer to the underlying ML library (PyTorch). spaCy has better data structures for working with annotations, support for pipelines that mix statistical and rule-based operations, and more frameworky features for configuration, extension and project workflows.  We recently released <a href="https://github.com/explosion/spacy-llm" target="_blank" rel="noopener nofollow noreferrer"><code>spacy-llm</code></a>, an extension that lets you add LLM-powered components to your spaCy pipelines. You can mix LLM with other components, and make use of spaCy’s <a href="https://spacy.io/api/doc" target="_blank" rel="noopener"><code>Doc</code></a>, <a href="https://spacy.io/api/span" target="_blank" rel="noopener"><code>Span</code></a>, <a href="https://spacy.io/api/token" target="_blank" rel="noopener"><code>Token</code></a> and other classes to make use of the annotations.</p>
<p>Let’s say you want to create a pipeline that detects some entities, and then you want to get the sentences that the entities occur in. Here’s how that looks, building and using the pipeline in Python:</p>
<figure>
<pre><span>Usage example</span><code><span>import</span> spacy

nlp <span>=</span> spacy<span>.</span>blank<span>(</span><span>"en"</span><span>)</span>
nlp<span>.</span>add_pipe<span>(</span><span>"sentencizer"</span><span>)</span>
nlp<span>.</span>add_pipe<span>(</span>
    <span>"llm"</span><span>,</span>
    config<span>=</span><span>{</span>
        <span>"task"</span><span>:</span> <span>{</span>
            <span>"@llm_tasks"</span><span>:</span> <span>"spacy.NER.v1"</span><span>,</span>
            <span>"labels"</span><span>:</span> <span>"SAAS_PLATFORM,PROGRAMMING_LANGUAGE,OPEN_SOURCE_LIBRARY"</span>
        <span>}</span><span>,</span>
        <span>"backend"</span><span>:</span> <span>{</span>
            <span>"@llm_backends"</span><span>:</span> <span>"spacy.REST.v1"</span><span>,</span>
            <span>"api"</span><span>:</span> <span>"OpenAI"</span><span>,</span>
            <span>"config"</span><span>:</span> <span>{</span><span>"model"</span><span>:</span> <span>"text-davinci-003"</span><span>}</span><span>,</span>
        <span>}</span><span>,</span>
    <span>}</span><span>,</span>
<span>)</span>

doc <span>=</span> nlp<span>(</span><span>"There's no PyTorch bindings for Go. We just use Microsoft Cognitive Services."</span><span>)</span>
<span>for</span> ent <span>in</span> doc<span>.</span>ents<span>:</span>
    <span>print</span><span>(</span>ent<span>.</span>text<span>,</span> ent<span>.</span>label_<span>,</span> ent<span>.</span>sent<span>)</span>
</code></pre>
</figure>
<p>The <a href="https://spacy.io/api/sentencizer" target="_blank" rel="noopener"><code>sentencizer</code></a> component in spaCy uses rules to detect sentence boundaries, and the <a href="https://github.com/explosion/spacy-llm#-api" target="_blank" rel="noopener nofollow noreferrer"><code>llm</code></a> component is here configured to perform named entity recognition, with the given labels. Task handlers are provided for some other common NLP tasks, but you can also define your own functions to perform arbitrary tasks — you just need to add a decorator to your function and obey the correct signature.</p>
<p>The sentence and entity annotations (accessed via the <code>doc.sents</code> and <code>doc.ents</code> in this example) are both accessible as sequences of <code>Span</code> objects, which is like a labelled slice of the <code>Doc</code> object. You can iterate over the tokens in the span, get its start and end character offset, and (depending on the components you in your pipeline) access embeddings or compute similarities. Components can assign multiple overlapping layers of <code>Span</code> annotations, and you can design and assign extension attributes to conveniently access additional properties.</p>
<p>Obviously, I’m pretty proud of these parts of spaCy, but what do they have to do with LLMs? You can prompt an LLM with a command like, <em>“How many paragraphs in this review say something bad about the acting? Which actors do they frequently mention?”</em>. For once-off personal tasks, this is absolutely magical. But if you’re building a system and you want to calculate and display this information for every review, it’s very nice to just approach this as separate prediction tasks (tagging names, linking them to a knowledge base, and paragraph-level actor sentiment), with data structures that let you flexibly access the information. The new LLM support in spaCy now lets you plug in LLM-powered components for these prediction tasks, which is especially great for prototyping. This functionality is still quite new and experimental, but it’s already very fun to explore.</p>
<p>Another piece of tooling you’ll need is some sort of solution for
data annotation. You can just load things up in a spreadsheet or text editor, but if you’re doing this repeatedly, it’s worth using something better. We make a commercial annotation tool, <a href="https://prodi.gy/" target="_blank" rel="noopener">Prodigy</a>, which emphasizes customizability and model-assisted workflows. Prodigy isn’t free, but it’s a one-time purchase per license, and it fits well into local workflows.</p>
<p>A key design idea in Prodigy is <em>model assistance</em>: calling into a model to get initial annotations, and letting you review and fix them. This works especially well with LLMs, and we’ve been building out <a href="https://github.com/explosion/prodigy-openai-recipes" target="_blank" rel="noopener nofollow noreferrer">support for it</a> over the last six months. Prodigy v1.12 will feature integrated support for LLM annotation assistance, with support for a choice of backends, including open-source solutions you can host yourself. Prodigy also supports A/B evaluation for generative outputs, which applies especially well for LLMs. This functionality is also extended in v1.12. For instance, you can design a number of different prompts, and run a tournament between them, by answering a series of A/B evaluation questions where you pick which of two outputs is better without knowing which prompt produced them. This lets you perform prompt engineering systematically, based on decisions you can record and later review.</p>
<h2 id="appendix2"><a href="#appendix2">Appendix 2: Accuracy of supervised and in-context learning </a></h2>
<p>Large Language Models (LLMs) can be used for arbitrary prediction tasks, by constructing a prompt describing the task, giving the labels to predict, and optionally including a relatively small number of examples in the prompt. This approach doesn’t involve any direct updates to the model for the new task. However, LLMs seem to learn a general ability to continue patterns, including abstract ones, from their language model objective. The mechanics are still under investigation, but see for example Anthropic’s work on induction heads <a href="https://arxiv.org/abs/2209.11895" target="_blank" rel="noopener nofollow noreferrer">(Olsson et al., 2022)</a>.</p>
<p>In supervised learning (often referred to as fine-tuning, in the context of language models), the model is provided a set of labelled example pairs, and the weights are adjusted such that some objective function is minimized. In modern NLP, supervised learning and language model pretraining are closely linked. Knowledge about the language generalizes between tasks, so it’s desirable to somehow initialize the model with that knowledge. Language model pretraining has proven to be a very strong general answer to this requirement. In my opinion the best things to read on this are articles from when the developments were relatively fresh, such as Sebastian Ruder’s 2018 blog post <a href="https://www.ruder.io/nlp-imagenet/" target="_blank" rel="noopener nofollow noreferrer"><em>NLP’s ImageNet moment has finally arrived</em></a>.</p>
<p>OpenAI evaluated GPT-3’s in-context learning capabilities against supervised learning in a variety of configurations <a href="https://arxiv.org/pdf/2005.14165.pdf" target="_blank" rel="noopener nofollow noreferrer">(Brown et al., 2020)</a>. The results in Section 3.7, on the SuperGLUE benchmark, are the most directly relevant to general NLP prediction tasks such as entity recognition or text classification. In their experiments, OpenAI prompted GPT3 with 32 examples of each task, and found that they were able to achieve similar accuracy to the BERT baselines. These results were the first big introduction to in-context learning as a competitive approach, and they’re indeed impressive. However, they were well below the state-of-the-art accuracy when they were published, and the current state-of-the-art results on the <a href="https://super.gluebenchmark.com/leaderboard" target="_blank" rel="noopener nofollow noreferrer">SuperGLUE leaderboard</a> all involve supervised learning, not just in-context learning. Some subtasks of the SuperGLUE benchmark suite have very small training sets, and on these tasks in-context learning is competitive. I’m not aware of any current NLP benchmarks where more than a few hundred training samples are available, and the leading systems rely solely on in-context learning.</p>
<p>The point of in-context learning has never been to be the absolute highest accuracy way to have a model perform some specific task. Rather, it’s an impressive compromise: it’s extremely sample efficient (you don’t need many examples of your task), and you don’t have to pay the upfront computational cost of training. In short, the advantage of in-context learning is <em>lower overhead</em>. But the longer your project lives, the less this should be seen as a dominant advantage. The overheads get amortized away.</p>
<p>Finally, it’s important to realize that SuperGLUE and other standard NLP benchmarks are specifically designed to be quite challenging. Easy tasks don’t make good benchmarks. This is exactly opposite to NLP applications, where we want tasks to be as easy as possible. Most practical tasks don’t require powerful reasoning abilities or extensive background world knowledge, which are the things that really set LLMs apart from smaller models. Rather, practical tasks usually require the model to learn a fairly specific set of policies, and then apply them consistently. Supervised learning is a good fit for this requirement.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[VoCore – Coin-sized Linux Computer (104 pts)]]></title>
            <link>https://vocore.io/</link>
            <guid>37495731</guid>
            <pubDate>Wed, 13 Sep 2023 12:18:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://vocore.io/">https://vocore.io/</a>, See on <a href="https://news.ycombinator.com/item?id=37495731">Hacker News</a></p>
Couldn't get https://vocore.io/: Error: unable to verify the first certificate]]></description>
        </item>
        <item>
            <title><![CDATA[Stable Audio: Fast Timing-Conditioned Latent Audio Diffusion (321 pts)]]></title>
            <link>https://stability.ai/research/stable-audio-efficient-timing-latent-diffusion</link>
            <guid>37494620</guid>
            <pubDate>Wed, 13 Sep 2023 10:00:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stability.ai/research/stable-audio-efficient-timing-latent-diffusion">https://stability.ai/research/stable-audio-efficient-timing-latent-diffusion</a>, See on <a href="https://news.ycombinator.com/item?id=37494620">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-block-type="2" id="block-b32ae79894df0a16ce66">
  <h4>Introduction</h4><p>The introduction of diffusion-based generative models has revolutionized the field of generative AI over the last few years, leading to rapid improvements in the quality and controllability of generated images, video, and audio. Diffusion models working in the latent encoding space of a pre-trained autoencoder, termed “latent diffusion models”, provide significant speed improvements to the training and inference of diffusion models.&nbsp;</p><p>One of the main issues with generating audio using diffusion models is that diffusion models are usually trained to generate a fixed-size output. For example, an audio diffusion model might be trained on 30-second audio clips, and will only be able to generate audio in 30-second chunks. This is an issue when training on and trying to generate audio of greatly varying lengths, as is the case when generating full songs.</p><p>Audio diffusion models tend to be trained on randomly cropped chunks of audio from longer audio files, cropped or padded to fit the diffusion model’s training length. In the case of music, this causes the model to tend to generate arbitrary sections of a song, which may start or end in the middle of a musical phrase.</p><p>We introduce Stable Audio, a latent diffusion model architecture for audio conditioned on text metadata as well as audio file duration and start time, allowing for control over the content and length of the generated audio. This additional timing conditioning allows us to generate audio of a specified length up to the training window size.</p><p>Working with a heavily downsampled latent representation of audio allows for much faster inference times compared to raw audio. Using the latest advancements in diffusion sampling techniques, our flagship Stable Audio model is able to render 95 seconds of stereo audio at a 44.1 kHz sample rate in less than one second on an NVIDIA A100 GPU.</p><h4>Audio Samples</h4><p><strong>Music</strong></p>
</div><div data-block-type="2" id="block-yui_3_17_2_1_1694584350812_37801">
  <p>The Stable Audio models are latent diffusion models consisting of a few different parts, similar to Stable Diffusion: A variational autoencoder (VAE), a text encoder, and a U-Net-based conditioned diffusion model.</p><p>The VAE compresses stereo audio into a data-compressed, noise-resistant, and invertible lossy latent encoding that allows for faster generation and training than working with the raw audio samples themselves. We use a fully-convolutional architecture based on the <a href="https://github.com/descriptinc/descript-audio-codec" target="_blank"><span>Descript Audio Codec</span></a> encoder and decoder architectures to allow for arbitrary-length audio encoding and decoding, and high-fidelity outputs.</p><p>To condition the model on text prompts, we use the frozen text encoder of a <a href="https://github.com/LAION-AI/CLAP/" target="_blank"><span>CLAP</span></a> model trained from scratch on our dataset. The use of a CLAP model allows the text features to contain some information about the relationships between words and sounds. We use the text features from the penultimate layer of the CLAP text encoder to obtain an informative representation of the tokenized input text. These text features are provided to the diffusion U-Net through cross-attention layers.</p><p>For the timing embeddings, we calculate two properties during training time when gathering a chunk of audio from our training data: the second from which the chunk starts (termed “seconds_start”) and the overall number of seconds in the original audio file (termed “seconds_total”). For example, if we take a 30-second chunk from an 80-second audio file, with the chunk starting at 0:14, then “seconds_start” is 14, and “seconds_total” is 80. These second values are translated into per-second discrete learned embeddings and concatenated with the prompt tokens before being passed into the U-Net’s cross-attention layers. During inference, these same values are provided to the model as conditioning, allowing the user to specify the overall length of the output audio.&nbsp;</p><div><p>The diffusion model for Stable Audio is a 907M parameter U-Net based on the model used in <a href="https://arxiv.org/abs/2301.11757" target="_blank"><span>Moûsai</span></a>. It uses a combination of residual layers, self-attention layers, and cross-attention layers to denoise the input conditioned on text and timing embeddings. Memory-efficient implementations of attention were added to the U-Net to allow the model to scale more efficiently to longer sequence lengths.</p></div><h4>Dataset</h4><p>To train our flagship Stable Audio model, we used a dataset consisting of over 800,000 audio files containing music, sound effects, and single-instrument stems, as well as corresponding text metadata, provided through a deal with stock music provider AudioSparx. This dataset adds up to over 19,500 hours of audio.</p><h4>Future work and open models</h4><p>Stable Audio represents the cutting-edge audio generation research by Stability AI’s generative audio research lab, Harmonai. We continue to improve our model architectures, datasets, and training procedures to improve output quality, controllability, inference speed, and output length.</p><p>Keep an eye out for upcoming releases from Harmonai, including open-source models based on Stable Audio and training code to allow you to train your audio generation models.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[RustRover – A Standalone Rust IDE by JetBrains (111 pts)]]></title>
            <link>https://blog.jetbrains.com/rust/2023/09/13/introducing-rustrover-a-standalone-rust-ide-by-jetbrains/</link>
            <guid>37494605</guid>
            <pubDate>Wed, 13 Sep 2023 09:58:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.jetbrains.com/rust/2023/09/13/introducing-rustrover-a-standalone-rust-ide-by-jetbrains/">https://blog.jetbrains.com/rust/2023/09/13/introducing-rustrover-a-standalone-rust-ide-by-jetbrains/</a>, See on <a href="https://news.ycombinator.com/item?id=37494605">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
    <div>
                                    <p><a href="https://blog.jetbrains.com/rust/">
                        <img src="https://blog.jetbrains.com/wp-content/uploads/2020/07/icon_256.svg" alt="Rust logo">
                    </a></p>
            </div>
                        <section data-clarity-region="article">
                <div>
                    				<p><a href="https://blog.jetbrains.com/rust/category/news/">News</a>
			<a href="https://blog.jetbrains.com/rust/category/releases/">Releases</a></p><h2 id="major-updates">Introducing RustRover – A Standalone Rust IDE by JetBrains</h2>                    <div><p data-nosnippet="">Read this post in other languages:</p></div>
                    
<p>“When will there be a Rust IDE?”</p>



<p>We get this question from our users quite frequently, and today we’re happy to announce that the day has arrived. Please welcome <a href="https://www.jetbrains.com/rust/" target="_blank" rel="noopener">RustRover</a>, our standalone IDE for Rust.</p>



<p>As many of you are aware, we’ve worked for years to bring support for Rust functionality as a plugin that works in both IntelliJ IDEA and CLion. However, time and time again, we’ve received requests from the community for an IDE specifically dedicated to Rust and its ecosystem that also has features on par with existing JetBrains IDEs.</p>



<p>Today we’re opening the RustRover Early Access Program (EAP) and we’d love for you to try it, <a href="https://youtrack.jetbrains.com/newIssue?project=RUST" target="_blank" rel="noopener">give us feedback</a>, and help us shape the product. We’ll listen to your feedback and&nbsp; update frequently to ensure that the product is meeting our users’ needs. RustRover will be free during Public Preview, and the license model will be finalized closer to the date of the commercial release.</p>


<p><a href="https://jb.gg/rust_download" target="_blank" rel="noopener">DOWNLOAD</a></p>


<h3>A Commercial IDE</h3>



<p>As the number of users of the Rust plugin have grown, so have the demands for new functionality. Within RustRover we aim to take JetBrains’s Rust support to the next level – stay tuned for future updates. Consequently the investment required by us to provide such functionality is also increasing. In line with our other IDEs, and to ensure our continued sustainability as a team and as a company, RustRover will be offered under a commercial plan. After the EAP period, during which the product is free to use, we will be offering RustRover as a standalone commercial IDE or as part of the <a href="https://www.jetbrains.com/all/" target="_blank" rel="noopener">All Products Pack</a>. We aim to release RustRover before September 2024.&nbsp;</p>



<h3>Existing Open-Source Plugin&nbsp;</h3>



<p>The existing open-source plugin, which we’ve been working on for a number of years, has served as the building block for RustRover. This plugin will remain open source and freely available on GitHub and the marketplace. However, moving forward, we will be investing our efforts into RustRover, which is closed source. For the existing open-source plugin, we’ll do our best to maintain compatibility with newer versions of our IDEs, but we won’t be fixing bugs or adding new features. The existing issues that are open on GitHub, where applicable to RustRover, have already been imported into our <a href="https://youtrack.jetbrains.com/issues/RUST" target="_blank" rel="noopener">issue tracker</a>.</p>



<figure><a href="https://jb.gg/t5tsjl" target="_blank" rel="noreferrer noopener"><img decoding="async" fetchpriority="high" width="1200" height="628" src="https://blog.jetbrains.com/wp-content/uploads/2023/09/DSGN-17357-Facebook-1200x628-1.png" alt=""></a></figure>



<h3>Support for IntelliJ IDEA and CLion</h3>



<p>Like many of our IDEs, the functionality of RustRover can be installed as a plugin in IntelliJ IDEA Ultimate.&nbsp;</p>



<p>During the preview period, it will also be possible to install the plugin in CLion. However, we have yet to determine whether this will be the case once we’ve released RustRover. This is because we’re still not certain whether users will need the plugin or whether a dedicated Rust IDE will be sufficient.&nbsp;</p>



<h3>Joining the Rust Foundation</h3>



<p>We are confident that the Rust ecosystem and community will continue to grow. Otherwise we wouldn’t be betting on an IDE. On that note, we are happy to announce that JetBrains has joined the Rust Foundation. Along with the many existing members, we’ll help support the efforts of the Rust community and work to drive its future development.</p>
                    
                                                                                                                                                        <div>
                                <h4>Subscribe to Blog updates</h4>
                                
                            </div>
                            
                                                            </div>
                <a href="#"></a>
                
                
                
            </section>
                    <div>
                <p>
                    <h2>Discover more</h2>
                </p>
                
            </div>
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LÖVE: a framework to make 2D games in Lua (515 pts)]]></title>
            <link>https://love2d.org/</link>
            <guid>37494275</guid>
            <pubDate>Wed, 13 Sep 2023 09:21:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://love2d.org/">https://love2d.org/</a>, See on <a href="https://news.ycombinator.com/item?id=37494275">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			
			<nav>
				
			</nav>
			<section id="intro">
				<p>Hi there! LÖVE is an *awesome* framework you can use to make
					2D games in Lua. It's free, open-source, and works on
					Windows, Mac OS X, Linux, Android and iOS.</p>
			</section>

			<section id="download">
				<h2>Download LÖVE 11.4</h2>
				
			</section>

			<section id="features">
					<h2>Open Source</h2>
					<p>LÖVE is licensed under the liberal zlib/libpng license. This means that:</p>
					<ul>
						<li>It costs nothing.</li>
						<li>You can use it freely for commercial purposes with no limitations.</li>
					</ul>
					<p>The source can be found on <a href="https://github.com/love2d/love">GitHub</a>.</p>
				</section>

			<section id="examples">
				<h2>Examples</h2>
				<p>It’s pretty easy to get started with LÖVE, just check out these code snippets.</p>
				<section>
				<dl>
					<dt>Drawing text</dt>
					<dd><code>function love.draw()<br>	love.graphics.print("Hello World!", 400, 300)<br>end</code></dd>
					<dt>Drawing an image</dt>
					<dd><code>function love.load()<br>	whale = love.graphics.newImage("whale.png")<br>end<br>function love.draw()<br>	love.graphics.draw(whale, 300, 200)<br>end</code></dd>
					<dt>Playing a sound</dt>
					<dd><code>function love.load()<br>	sound = love.audio.newSource("music.ogg", "stream")<br>	love.audio.play(sound)<br>end</code></dd>
				</dl>
				</section>

				<p>Check out some more tutorials on <a href="https://love2d.org/wiki">the wiki.</a></p>
			</section>

			<section id="games">
				<h2>Games</h2>
				<p>LÖVE has been used for commercial projects, game jams, prototyping, and everything in between. Here are a few examples.</p>
				
			</section>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Openmoji (212 pts)]]></title>
            <link>https://github.com/hfg-gmuend/openmoji</link>
            <guid>37494015</guid>
            <pubDate>Wed, 13 Sep 2023 08:50:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/hfg-gmuend/openmoji">https://github.com/hfg-gmuend/openmoji</a>, See on <a href="https://news.ycombinator.com/item?id=37494015">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">OpenMoji</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/480224/124712580-c8b0f880-deff-11eb-8516-3def8df2a42e.png"><img width="1157" alt="openmoji-github-keyvisual" src="https://user-images.githubusercontent.com/480224/124712580-c8b0f880-deff-11eb-8516-3def8df2a42e.png"></a></p>
<p dir="auto">Open-source emojis for designers, developers and everyone else! OpenMoji is an open-source project of the HfG Schwäbisch Gmünd by Benedikt Groß, Daniel Utz, 70+ students and external contributors.</p>
<p dir="auto">👉 <a href="http://openmoji.org/" rel="nofollow">OpenMoji.org/</a></p>
<p dir="auto">Interact, create, save, and share your work! 🌈<code>#openmoji</code></p>
<p dir="auto">This GitHub repository contains all of the source files and exported png/svg files of the OpenMoji project.</p>
<p dir="auto"><g-emoji alias="warning">⚠️</g-emoji> Please note that the master branch is in active development, so if you're looking for stable production version please use one of the releases.</p>
<p dir="auto">🛠 You can check the latest work in progress developments via the <a href="https://hfg-gmuend.github.io/openmoji/" rel="nofollow">OpenMoji Dev Catalog</a> which lists all OpenMojis of the master branch.</p>
<h2 tabindex="-1" dir="auto">Table of Contents</h2>
<ul dir="auto">
<li><a href="http://openmoji.org/styleguide" rel="nofollow">Styleguide</a> Our beloved styleguide.</li>
<li><a href="https://github.com/hfg-gmuend/openmoji/blob/master/FAQ.md">FAQ</a> Check if your question has already been answered</li>
<li><a href="https://github.com/hfg-gmuend/openmoji/blob/master/CONTRIBUTING.md">Contributing</a> Pull Requests are welcome!</li>
<li><a href="https://github.com/hfg-gmuend/openmoji/blob/master/CONTRIBUTING.md#-Developer-Setup">Developer Setup</a> How to setup a environment.</li>
<li><a href="https://github.com/hfg-gmuend/openmoji/blob/master/API.md">API</a> documentation for the npm OpenMoji package.</li>
<li><a href="https://github.com/hfg-gmuend/openmoji/blob/master/font">Font</a> infos on the OpenMoji-Color and OpenMoji-Black fonts.</li>
<li><a href="http://openmoji.org/about/#team" rel="nofollow">Team</a> list of all authors and contributors.</li>
<li><a href="http://openmoji.org/about/#acknowledgement" rel="nofollow">Acknowledgements</a> Thanks!</li>
<li><a href="https://github.com/hfg-gmuend/openmoji/blob/master/CODE_OF_CONDUCT.md">Code of Conduct</a> and OpenMoji Community Statement.</li>
</ul>
<h2 tabindex="-1" dir="auto">Downloads &amp; Distribution Channels</h2>
<p dir="auto">You can download, use and "consume" OpenMoji in various ways:</p>
<ul dir="auto">
<li><a href="https://github.com/hfg-gmuend/openmoji/releases/latest">SVG</a>: Color &amp; Black (production ready)</li>
<li><a href="https://github.com/hfg-gmuend/openmoji/releases/latest">Fonts</a>: Color &amp; Black (experimental)</li>
<li><a href="https://github.com/hfg-gmuend/openmoji/releases/latest">PNG 618x618</a>: Color &amp; Black (production ready)</li>
<li><a href="https://github.com/hfg-gmuend/openmoji/releases/latest">PNG 72x72</a>: Color &amp; Black (production ready)</li>
<li><a href="https://itunes.apple.com/us/app/openmoji/id1462636288" rel="nofollow">OpenMoji app</a>: for iOS with emoji picker</li>
<li><a href="https://itunes.apple.com/us/app/openmoji/id1462636288" rel="nofollow">OpenMoji Stickers</a>: for iOS Messages app</li>
<li><a href="https://github.com/hfg-gmuend/openmoji/">OpenMoji Github</a>: <code>git clone --depth 1 https://github.com/hfg-gmuend/openmoji.git</code> The OpenMoji repo is big! It is recommended to clone it without the entire history, note the --depth flag.</li>
<li><a href="https://www.npmjs.com/package/openmoji" rel="nofollow">OpenMoji NPM Package</a>: <code>npm install openmoji</code>. You can also get individual files via <a href="https://unpkg.com/" rel="nofollow">UNPKG</a> directly e.g.: unpkg.com/openmoji@12.1.0/color/svg/1F64B.svg</li>
<li>CDN (will always fetch latest version - to pin to a version, see documentation <a href="https://www.jsdelivr.com/features#gh" rel="nofollow">here</a>): cdn.jsdelivr.net/gh/hfg-gmuend/openmoji/color/svg/1F63A.svg</li>
</ul>
<p dir="auto"><strong>Community Extensions</strong></p>
<ul dir="auto">
<li><a href="https://github.com/azadeh-afzar/OpenMoji-Jekyll-Plugin">OpenMoji Jekyll Plugin</a>: <code>gem install jekyll-openmoji</code></li>
<li><a href="https://github.com/axelpale/openmoji-spritemap-generator">OpenMoji Spritemap Generator</a>: OpenMoji combined to handy sprite images</li>
<li><a href="https://apps.fedoraproject.org/packages/hfg-gmuend-openmoji-fonts" rel="nofollow">OpenMoji in Fedora</a>: For Fedora 33 and newer, <code>sudo yum install hfg-gmuend-openmoji-fonts-all</code></li>
<li><a href="https://github.com/pavlobu/emoji-text-flow-javafx">OpenMoji in JavaFX</a>: A cross-platform JavaFX library allowing you to replace all standard emoji in extended TextFlow (EmojiTextFlow) with OpenMoji.</li>
<li><a href="https://github.com/gromain/openmoji-awesome">OpenMoji Awesome CSS Classes</a>: "Font Awesome" flavored CSS classes eg.  <code>&lt;i class="oma oma-face-with-monocle"&gt;&lt;/i&gt;</code> ready to use for websites.</li>
<li><a href="https://github.com/MEibenst/openmoji-sticker-sets">OpenMoji Sticker Sets</a>: Stickers for Telegram and WhatsApp.</li>
<li><a href="https://pub.dev/packages/flutter_openmoji" rel="nofollow">OpenMoji Flutter</a>: OpenMoji usable as Icon for the FLutter framework.</li>
</ul>
<h2 tabindex="-1" dir="auto">Attribution Requirements</h2>
<p dir="auto">As an open source project, attribution is critical from a legal, practical and motivational perspective. Please give us credits! Common places for attribution are for example: to mention us in your project README, the 'About' section or the footer on a website/in mobile apps.</p>
<p dir="auto">Attribution suggestion:</p>
<blockquote>
<p dir="auto">All emojis designed by <a href="https://openmoji.org/" rel="nofollow">OpenMoji</a> – the open-source emoji and icon project. License: <a href="https://creativecommons.org/licenses/by-sa/4.0/#" rel="nofollow">CC BY-SA 4.0</a></p>
</blockquote>
<h2 tabindex="-1" dir="auto">Anatomy of the OpenMoji Repository</h2>
<p dir="auto"><code>black/</code> and <code>color/</code> Contains all exported .png and .svg files ¹</p>
<p dir="auto"><code>data/</code> Contains the central openmoji.json with all meta informations for each emoji ¹</p>
<p dir="auto"><code>font/</code> Contains the exported OpenMoji fonts ¹</p>
<p dir="auto"><code>guidelines/</code> Contains various template files related to the styleguide ¹</p>
<p dir="auto"><code>helpers/</code> Contains various helper scripts e.g. to export to .png and .svg, generate skintones variants, enforce the OpenMoji color palette etc. ²</p>
<p dir="auto"><code>src/</code> Contains all source .svg files of OpenMoji. The files are broken up into folders and files corresponding with the Unicode groups and sub-groups ¹</p>
<p dir="auto"><code>test/</code> Automated unit tests to ensure consistency across all source .svg files ²</p>
<h2 tabindex="-1" dir="auto">License</h2>
<p dir="auto">¹ OpenMoji graphics are licensed under the Creative Commons Share Alike License 4.0 (<a href="https://creativecommons.org/licenses/by-sa/4.0/" rel="nofollow">CC BY-SA 4.0</a>)</p>
<p dir="auto"><a href="https://creativecommons.org/licenses/by-sa/4.0/" rel="nofollow"><img src="https://camo.githubusercontent.com/bdc6a3b8963aa99ff57dfd6e1e4b937bd2e752bcb1f1936f90368e5c3a38f670/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d434325323042592d2d5341253230342e302d6c69676874677265792e737667" alt="License: CC BY-SA 4.0" data-canonical-src="https://img.shields.io/badge/License-CC%20BY--SA%204.0-lightgrey.svg"></a></p>
<p dir="auto">² Code licensed under the GNU Lesser General Public License v3 (<a href="https://www.gnu.org/licenses/lgpl-3.0.en.html" rel="nofollow">LGPL-3.0</a>)</p>
<p dir="auto"><a href="https://www.gnu.org/licenses/lgpl-3.0.en.html" rel="nofollow"><img src="https://camo.githubusercontent.com/7f4c99c0c2a41c8241c33d70b9b90b4f4269161ee90a3bae5f2e01a50ec0fb90/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4c47504c25323076332d6c69676874677265792e737667" alt="License: LGPL-3.0" data-canonical-src="https://img.shields.io/badge/License-LGPL%20v3-lightgrey.svg"></a></p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What is wrong with TOML? (2019) (137 pts)]]></title>
            <link>https://hitchdev.com/strictyaml/why-not/toml/</link>
            <guid>37493964</guid>
            <pubDate>Wed, 13 Sep 2023 08:44:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hitchdev.com/strictyaml/why-not/toml/">https://hitchdev.com/strictyaml/why-not/toml/</a>, See on <a href="https://news.ycombinator.com/item?id=37493964">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-md-component="container">
      
      
        
          
        
      
      <main data-md-component="main">
        <div data-md-component="content">
              <article>
                
                  


  

<div><pre><span></span><code><span># This is a TOML document.</span>

<span>title = "TOML Example"</span>

<span>[owner]</span>
<span>name = "Tom Preston-Werner"</span>
<span>dob = 1979-05-27T07:32:00-08:00</span><span> </span><span># First class dates</span>
</code></pre></div>
<p><a href="https://github.com/toml-lang/toml">TOML</a> is a configuration designed as a sort
of "improved" <a href="https://hitchdev.com/strictyaml/why-not/ini">INI file</a>. It's analogous to this project -
<a href="https://github.com/crdoconnor/strictyaml">StrictYAML</a>, a similar attempt
to <a href="https://hitchdev.com/strictyaml/features-removed">fix YAML's flaws</a>:</p>
<div><pre><span></span><code><span># All about the character</span>
<span>name</span><span>:</span><span> </span><span>Ford Prefect</span>
<span>age</span><span>:</span><span> </span><span>42</span>
<span>possessions</span><span>:</span>
<span>-</span><span> </span><span>Towel</span>
</code></pre></div>
<p>I'm not going to argue here that TOML is the <em>worst</em> file format out there -
if you use it infrequently on small and simple files it does its job fine.</p>
<p>It's a warning though: as you scale up its usage, many bad warts start to appear.</p>
<p>Martin Vejnár, the author of PyTOML
<a href="https://github.com/avakar/pytoml/issues/15#issuecomment-217739462">argued exactly this</a>.
He initially built a TOML parser out of enthusiasm for this new format but later abandoned
it. When asked if he would like to see his library used as a dependency for pip as
part of <a href="https://www.python.org/dev/peps/pep-0518/">PEP-518</a>, he said no - and
explained why he abandoned the project:</p>
<blockquote>
<p>TOML is a bad file format. <strong>It looks good at first glance</strong>, and for really really
trivial things it is probably good. But once I started using it and the
configuration schema became more complex, I found the syntax ugly and hard to read.</p>
</blockquote>
<p>Despite this, PyPA still went ahead and used TOML for PEP-518. Fortunately
pyproject.toml <em>is</em> fairly trivial and appears just once per project
so the problems he alludes to aren't that pronounced.</p>
<p>StrictYAML, by contrast, was designed to be a language to write
<a href="https://hitchdev.com/hitchstory">readable 'story' tests</a> where there will be <em>many</em> files
per project with more complex hierarchies, a use case where TOML starts
to really suck.</p>
<p>So what specifically <em>is</em> wrong with TOML when you scale it up?</p>
<h2 id="1-its-very-verbose-its-not-dry-its-syntactically-noisy">1. It's very verbose. It's not DRY. It's syntactically noisy.</h2>
<p>In <a href="https://github.com/crdoconnor/strictyaml/blob/master/hitch/story/map.story">this example of a StrictYAML story</a>
and <a href="https://github.com/crdoconnor/strictyaml/blob/master/hitch/story/map.toml">its equivalent serialized TOML</a>
the latter ends up <a href="https://www.goodreads.com/quotes/775257-my-point-today-is-that-if-we-wish-to-count">spending</a>
<strong>50% more</strong> characters to represent the exact same data.</p>
<p>This is largely due to the design decision to have the full name of every key being
associated with every value which is <strong>not</strong> <a href="https://hitchdev.com/code-quality/least-code">DRY</a>.</p>
<p>It is also partly due to the large numbers of syntactic cruft - quotation marks
and square brackets dominate TOML documents whereas in the StrictYAML example they are
absent.</p>
<p>Shortening program lengths (and DRYing code), all other things being equal,
<a href="https://blog.codinghorror.com/diseconomies-of-scale-and-lines-of-code/">reduces the number of bugs significantly</a>
because maintenance becomes easier and deriving intent from the code becomes clearer.
What goes for Turing-complete code also applies to configuration code.</p>
<h2 id="2-tomls-hierarchies-are-difficult-to-infer-from-syntax-alone">2. TOML's hierarchies are difficult to infer from syntax alone</h2>
<p>Mapping hierarchy in TOML is determined by dots. This is simple enough for
parsers to read and understand but this alone makes it difficult to perceive
the relationships between data.</p>
<p>This has been recognized by <a href="https://github.com/leereilly/csi/blob/567e5b55f766847c9dcc7de482c0fd241fa7377a/lib/data/master.toml">many</a> TOML <a href="https://github.com/CzarSimon/simonlindgren.info/blob/a391a6345b16f2d8093f6d4c5f422399b4b901eb/simon-cv/config.toml">writers</a> who have adopted a method that
will be quite familiar to a lot of programmers - indentation that the parser ignores:</p>
<p><a href="https://github.com/gazreese/gazreese.com/blob/c4c3fa7d576a4c316f11f0f7a652ca11ab23586d/Hugo/config.toml"><img alt="Non-meaningful indentation" src="https://hitchdev.com/strictyaml/why-not/toml-indentation-1.png"></a></p>
<p>This parallels the way indentation is added in <em>lots</em> of programming languages that have syntactic markers
like brackets - e.g.  JSON, Javascript or Java are all commonly rendered with non-parsed indentation to make it
easier for humans to understand them.</p>
<p>But not Python.</p>
<p>Python, has long been a stand out exception in how it was designed -
syntactic markers are <em>not</em> necessary to infer program structure because indentation <em>is</em> the marker
that determines program structure.</p>
<p>This argument over the merits of meaningful indentation in Python has been going on for decades, and <a href="https://www.quora.com/Do-you-think-that-indentation-in-Python-is-annoying">not everybody agrees with this</a>, but it's generally
considered a good idea - usually for <a href="https://softwareengineering.stackexchange.com/questions/313034/why-should-a-language-prefer-indentation-over-explicit-markers-for-blocks">the reasons argued in this stack exchange question</a>:</p>
<ol>
<li>
<p>Python inherited the significant indentation from the (now obsolete) predecessor language ABC. ABC is one of the very few programming languages which have used usability testing to direct the design. So while discussions about syntax usually comes down to subjective opinions and personal preferences, the choice of significant indentation actually has a sounder foundation.</p>
</li>
<li>
<p>Guido van Rossum came across subtle bugs where the indentation disagreed with the syntactic grouping. Meaningful indentation fixed this class of bug. Since there are no begin/end brackets there cannot be a disagreement between grouping perceived by the parser and the human reader.</p>
</li>
<li>
<p>Having symbols delimiting blocks and indentation violates the DRY principle.</p>
</li>
<li>
<p>It does away with the typical religious C debate of "where to put the curly braces" (although TOML is not yet popular enough to inspire such religious wars over indentation... yet).</p>
</li>
</ol>
<h2 id="3-overcomplication-like-yaml-toml-has-too-many-features">3. Overcomplication: Like YAML, TOML has too many features</h2>
<p>Somewhat ironically, TOML's creator quite rightly
<a href="https://github.com/toml-lang/toml#comparison-with-other-formats">criticizes YAML for not aiming for simplicity</a>
and then falls into the same trap itself - albeit not quite as deeply.</p>
<p>One way it does this is by trying to include date and time parsing which imports
<em>all</em> of the inherent complications associated with dates and times.</p>
<p>Dates and times, as many more experienced programmers are probably aware is an unexpectedly deep rabbit hole
of <a href="https://infiniteundo.com/post/25326999628/falsehoods-programmers-believe-about-time">complications and quirky, unexpected, headache and bug inducing edge cases</a>. TOML experiences <a href="https://github.com/uiri/toml/issues/55">many</a> <a href="https://github.com/uiri/toml/issues/196">of these</a> <a href="https://github.com/uiri/toml/issues/202">edge cases</a> because of this.</p>
<p>The best way to deal with <a href="https://simplicable.com/new/accidental-complexity-vs-essential-complexity">essential complexity</a> like these is to decouple, isolate the complexity and <em>delegate</em> it to a
<a href="https://en.wikipedia.org/wiki/Unix_philosophy">specialist tool that is good at handling that specific problem</a>
which you can swap out later if required.</p>
<p>This the approach that JSON took (arguably a good decision) and it's the approach that StrictYAML takes too.</p>
<p>StrictYAML the library (as opposed to the format) has a validator that uses
<a href="https://dateutil.readthedocs.io/en/stable/">Python's most popular date/time parsing library</a> although
developers are not obliged or even necessarily encouraged to use this. StrictYAML parses everything as a
string by default and whatever validation occurs later is considered to be outside of its purview.</p>
<h2 id="4-syntax-typing">4. Syntax typing</h2>
<p>Like most other markup languages TOML has <a href="https://hitchdev.com/strictyaml/why/syntax-typing-bad">syntax typing</a> -
the <em>writer</em> of the markup decides if, for example, something should be parsed as a number
or a string:</p>
<div><pre><span></span><code><span>flt2</span><span> </span><span>=</span><span> </span><span>3.1415</span>
<span>string</span><span> </span><span>=</span><span> </span><span>"hello"</span>
</code></pre></div>
<p>Programmers will feel at home maintaining this, but non programmers tend to find the
difference between "1.5" and 1.5 needlessly confusing.</p>
<p>StrictYAML does not require quotes around any value to infer a data type because the
schema is assumed to be the single source of truth for type information:</p>
<div><pre><span></span><code><span>flt2</span><span>:</span><span> </span><span>3.1415</span>
<span>string</span><span>:</span><span> </span><span>hello</span>
</code></pre></div>
<p>In the above example it just removes two characters, but in larger documents with more
complex data, pushing type parsing decision to the schema (or assuming strings)
removes an enormous amount of syntactic noise.</p>
<p>The lack of syntax typing combined with the use of indentation instead of square brackets
to denote hierarchies makes equivalent StrictYAML documents 10-20% shorter, cleaner
and ultimately more readable.</p>
<h2 id="advantages-of-toml-still-has-over-strictyaml">Advantages of TOML still has over StrictYAML</h2>
<p>There are currently still a few:</p>
<ul>
<li>StrictYAML does not currently have an "official spec". The spec is currently just "YAML 1.2 with <a href="https://hitchdev.com/strictyaml/features-removed">features removed</a>". This has some advantages (e.g. YAML syntax highlighting in editors works just fine) but also some disadvantages (some documents will render differently).</li>
<li>StrictYAML does not yet have parsers in languages other than Python. If you'd like to write one for your language (if you don't also do validation it actually wouldn't be very complicated), contact me, I'd love to help you in any way I can - including doing a test suite and documentation.</li>
<li>Popularity.</li>
</ul>





                
              </article>
            </div>
        
      </main>
      
        

      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Amazon's Union-Busting Training Video (236 pts)]]></title>
            <link>https://www.youtube.com/watch?v=AQeGBHxIyHw</link>
            <guid>37493845</guid>
            <pubDate>Wed, 13 Sep 2023 08:30:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=AQeGBHxIyHw">https://www.youtube.com/watch?v=AQeGBHxIyHw</a>, See on <a href="https://news.ycombinator.com/item?id=37493845">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[A CD Spectrometer (188 pts)]]></title>
            <link>https://www.cs.cmu.edu/~zhuxj/astro/html/spectrometer.html</link>
            <guid>37493797</guid>
            <pubDate>Wed, 13 Sep 2023 08:26:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cs.cmu.edu/~zhuxj/astro/html/spectrometer.html">https://www.cs.cmu.edu/~zhuxj/astro/html/spectrometer.html</a>, See on <a href="https://news.ycombinator.com/item?id=37493797">Hacker News</a></p>
<div id="readability-page-1" class="page"><div face="Arial,Helvetica" size="2">
<table>
<tbody>
<tr>
<th nowrap="true"><b><a href="https://www.cs.cmu.edu/~zhuxj/astro/html/pghdarksky.html">&lt;</a> &nbsp;</b><br></th>
<th nowrap="true"><b><a href="https://www.cs.cmu.edu/~zhuxj/astro/html/index.html">Home</a> &nbsp; / &nbsp; </b><br></th>
<th nowrap="true"><b><a href="https://www.cs.cmu.edu/~zhuxj/astro/html/astronomy.html">Astronomy</a> &nbsp; / &nbsp; </b><br></th>
<th nowrap="true"><b><a href="https://www.cs.cmu.edu/~zhuxj/astro/html/equipments.html">Equipments</a> &nbsp; / &nbsp; </b><br></th>
<th nowrap="true"><b>A CD spectrometer</b><br></th>
<th nowrap="true"><b><a href="https://www.cs.cmu.edu/~zhuxj/astro/html/stfc.html">&gt;</a> &nbsp;</b><br></th>
</tr>
</tbody>
</table>
<p>
A simple spectrometer can be built from a CD and a box.  Cut a slit on one side of the box.  Place the CD on the other side with about 60 degree angle.  Look down into the openning on the box.  The slit should not be too wide, otherwise the spectrum lines will be blurred.  It should not be too narrow either, otherwise the spectrum is too dim.  I use a 0.2mm wide slit.
</p><p>
<table>
<tbody>
<tr>
<td><img src="https://www.cs.cmu.edu/~zhuxj/astro/images/equipment/spectrometer/spectrobox3.jpg" alt="structure of the CD spectroscope"></td>
</tr>
<tr>
<td><b>structure of the CD spectroscope</b></td>
</tr>
</tbody>
</table>
</p><p>
Let's look at the spectra of some common light sources.  All photos by the authors with a Nikon coolpix 995 camera.
</p><p>
(1) Solar spectrum is continuous with dark lines, i.e. the famous Fraunhofer lines.  Several Fraunhofer lines can be seen with this simple spectroscope (1c, 1d): 
C line in dark red(H-alpha, 656nm), orange D(Na,589nm), 
green E(Fe,527nm) and b1,b2(Mg,518nm),
blue F(H-beta, 486nm), 
purple G(Fe and Ca, 431nm). 
</p><p>
Interestingly the solar spectrum changes with the Sun's altitude!  
As the Sun is lower its light passes more Earth atmosphere.
Comparing (1c) and (1d) we found a new dark line 'a' in red (molecular oxygen in Earth's atmosphere).  Who can tell me what the dark band under D is?
</p><p>
(2) Incandescent light is typical black body radiation, with continuous spectrum.  No black lines.  Same with tungsten halogen lamp.
</p><p>
(3) Fluorescent light has mercury gas emitting (mostly) ultraviolet light, which activates phosphor.  The latter emits broad band visible light.   Therefore we see bright mercury spectrum lines, most obviously green 546nm, on a continuous backgroud.
</p><p>
(4) The spectrum of high pressure sodium lights changes too!  When the lamp is just on, there are several bright spectrum lines including yellow sodium at 589nm (4c).  In a few seconds as the light gets brighter, the yellow line becomes wider, and a thin dark gap emerges at the center (4d, 4e).  After the lamp stablizes, cooler sodium vapor absorbs light at 589nm, and we see a thick gap (4f, 4b).  Note the camera settings for (4c--f) are the same.
</p><p>
(5) Spectrum of white screen on a computer display. 
</p><p>
(6) Laptop display is different from a CRT display.
</p><p>
(7) Red LED emits continuous spectrum in red.
</p><p>
(8) Neon bulb has many red and orange discrete bright spectrum lines.
</p><p>
(9) The night light uses phosphors and emits a continuous spectrum.
</p><p>
(10) Compact fluorescent light is similar to a normal fluorescent light but with tri-color phosphors.  Instead of a continuous backgroup, it emits bright lines of various colors.
</p><p>
(11) The green and purple neon tubes actually contain argon and mercury, with different phosphors produce different color.  The mercury line is visible.
</p><p>
(12) I took this photo before the 
<a href="https://www.cs.cmu.edu/~zhuxj/astro/html/lunareclipse041027.html#lunareclipse041027">total lunar eclipse of 2004</a>
.  Moonlight from a full moon was collected with an 8" Dob telescope.  The spectroscope was held behind the eyepiece.  As moonlight is nothing but reflected sunlight, the spectrum looks the same as a solar spectrum (1). It is continuous with dark absort lines, i.e. the Fraunhofer lines (some of them are marked).
</p><p>
(13) Candle light has a continuous spectrum.  In the first few seconds after a candle (or a match) is lit, there is also the yellow sodium line which disappears thereafter.  If table salt is burnt, the yellow sodium line becomes prominent.  The sodium line should be double lines, but this simple device cannot resolve them.
</p><p>
(14) Metal halide lamp has a complex spectrum.
</p><p>
(15) The blue neon sign could be a mixture of argon and mercury (or phosphors).  The red neon sign truly contains NEON, which is the same as in the neon bulb (8).
</p><p>
References<br>
<a href="http://www.exo.net/~pauld/activities/CDspectrometer/cdspectrometer.html">CD spectrometer <br></a>
<a href="http://www.scitoys.com/scitoys/scitoys/light/cd_spectroscope/spectroscope.html">Science Toys <br></a>
<a href="http://mo-www.harvard.edu/Java/MiniSpectroscopy.html">MiniSpectroscopy <br></a>
<a href="http://www.physics.umd.edu/lecdem/services/demos/demosn2/n2-03.htm">University of Maryland Physics Lecture-Demonstration Facility <br></a>
<a href="http://www.batmo.com/neon.html">Neon FAQ <br></a>
</p></div></div>]]></description>
        </item>
    </channel>
</rss>