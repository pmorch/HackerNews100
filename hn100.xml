<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 29 Oct 2024 20:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Using an 8K TV as a Monitor (144 pts)]]></title>
            <link>https://daniel.lawrence.lu/blog/y2023m12d15/</link>
            <guid>41986048</guid>
            <pubDate>Tue, 29 Oct 2024 16:27:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://daniel.lawrence.lu/blog/y2023m12d15/">https://daniel.lawrence.lu/blog/y2023m12d15/</a>, See on <a href="https://news.ycombinator.com/item?id=41986048">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><header><div><ol><li><a href="#s1"><span>1</span> <span>Pros of using an 8K display</span></a><ol><li><a href="#s1.1"><span>1.1</span> <span>For programming, word processing, etc</span></a></li><li><a href="#s1.2"><span>1.2</span> <span>For photo and video editing</span></a></li><li><a href="#s1.3"><span>1.3</span> <span>For CAD work</span></a></li><li><a href="#s1.4"><span>1.4</span> <span>For gaming/media</span></a></li><li><a href="#s1.5"><span>1.5</span> <span>Cost</span></a></li><li><a href="#s1.6"><span>1.6</span> <span>Connectivity</span></a></li></ol></li><li><a href="#s2"><span>2</span> <span>Potential cons and caveats</span></a><ol><li><a href="#s2.1"><span>2.1</span> <span>Desk and mounting</span></a></li><li><a href="#s2.2"><span>2.2</span> <span>Image quality issues</span></a><ol><li><a href="#s2.2.1"><span>2.2.1</span> <span>Uniformity</span></a></li><li><a href="#s2.2.2"><span>2.2.2</span> <span>Checkerboard effect</span></a></li></ol></li><li><a href="#s2.3"><span>2.3</span> <span>Random software issues</span></a><ol><li><a href="#s2.3.1"><span>2.3.1</span> <span>Nvidia Linux drivers</span></a></li><li><a href="#s2.3.2"><span>2.3.2</span> <span>AMD Linux drivers</span></a></li><li><a href="#s2.3.3"><span>2.3.3</span> <span>Input Signal Plus</span></a></li><li><a href="#s2.3.4"><span>2.3.4</span> <span>Wake up bugs</span></a></li><li><a href="#s2.3.5"><span>2.3.5</span> <span>Having another DisplayPort device</span></a></li></ol></li><li><a href="#s2.4"><span>2.4</span> <span>Display types</span></a></li><li><a href="#s2.5"><span>2.5</span> <span>Coatings</span></a></li></ol></li><li><a href="#s3"><span>3</span> <span>Example devices</span></a></li><li><a href="#s4"><span>4</span> <span>FAQ</span></a></li></ol></div></header><div><figure id="fig1"><a href="https://i.dllu.net/IMG_0043_09ae9580aa76352c.jpg"><img src="https://i.dllu.net/IMG_0043_1200_d9403825ac2b72fc.jpg" alt="A Samsung QN800A. "></a><figcaption><a href="#fig1">FIGURE 1</a> A 65” Samsung QN800A on my desk.</figcaption></figure></div><blockquote>TLDR: If your job is to write code all day or stare at Excel spreadsheets, buy an 8K TV instead of a multi-monitor setup. You can even use the same TV for 4K 120 Hz gaming or watching movies as a bonus!</blockquote><p>For programming, word processing, and other productive work, consider getting an 8K TV instead of a multi-monitor setup.
An 8K TV will have superior image quality, resolution, and versatility compared to multiple 4K displays, at roughly the same size.
As a bonus, an 8K display is also suitable for gaming at 4K 120 Hz, or for full screen media consumption, which is not possible with multiple smaller monitors.</p><p>Currently 8K TVs are found in 55” and above. 
This is about the same width as getting two 27” monitors or two 32” monitors, both common setups for programmers and other professionals.
This is also the same physical width as ultrawide screens, but with a superior resolution of 7680 px wide instead of the common upper limit of 5120 px for ultrawide screens.</p><h2 id="s1"><a href="#s1">1</a> <span>Pros of using an 8K display</span></h2><h2 id="s1.1"><a href="#s1.1">1.1</a> <span>For programming, word processing, etc</span></h2><div><figure id="fig2"><a href="https://i.dllu.net/20190723_150428_481a0539480051a8.jpg"><img src="https://i.dllu.net/20190723_150428_1200_ec928294ca540ab6.jpg" alt="Three 27" 4k="" monitors="" "=""></a><figcaption><a href="#fig2">FIGURE 2</a> Three 27” 4K monitors. An 8K monitor would have similar pixel density but no bezels in between.</figcaption></figure></div><div><figure id="fig3"><a href="https://i.dllu.net/IMG_5245_9d3a5a9f8684b2d6.jpg"><img src="https://i.dllu.net/IMG_5245_1200_92ce89a0232d6c7c.jpg" alt=""></a><figcaption><a href="#fig3">FIGURE 3</a> Some random code displayed on an 8K display. Having seven evenly-spaced columns would be impossible on a dual 4K display setup due to bezels in the middle.</figcaption></figure></div><p>Many programmers use a multi-monitor setup, often with multiple 4K monitors.
The bezels and gaps in between the monitors introduce distractions and one is limited in how one may arrange terminals and windows across multiple displays.
With a single 8K display there is no such problem.</p><p>Consider using a tiling window manager to arrange your windows in a large display.
Tiling window managers exist for all operating systems. Here are some examples that I enjoy:</p><ul><li>For Linux: <a href="https://i3wm.org/">i3</a>, <a href="https://swaywm.org/">Sway</a>. Note that although Nvidia was previously hostile to Wayland and Sway, it now works as of 2023. There are a lot more others.</li><li>For MacOS: <a href="https://github.com/koekeishiya/yabai">Yabai</a></li></ul><p>Productivity is highly sensitive to the display of text.
For this reason, many programmers prefer to use Apple products for their retina displays.
An 8K display will have vastly superior text rendering compared to a single large 4K display, and equivalent text rendering to multiple smaller 4K displays.</p><p>TVs may have a different subpixel layout than monitors, so small text may suffer fringing.
As of writing the Samsung VA and LG IPS panels such as the QN800A have a conventional RGB or BGR subpixel structure. One may also increase the font size or use hidpi scaling which will eliminate all pixel-level concerns.</p><h2 id="s1.2"><a href="#s1.2">1.2</a> <span>For photo and video editing</span></h2><p>Although the main motivation for getting the 8K display was for programming, it is quite nice for photography too.
Apart from the obvious advantage of being able to see a large photo with a sharp resolution, having a high resolution display allows toolbars and such to be legible at a much smaller size relative to the size of the display.
This improves productivity for photo editing. Of course, appropriate hidpi settings may be used to increase the size of toolbars if desired.</p><p>An 8K TV often supports the D65-P3 colour gamut, making it appropriate for photo and video editing.
It may not arrive as well-calibrated as professional monitors out of the box, but it should be possible to calibrate any screen with a display calibrator.
However, extremely colour-sensitive work should still use professional calibrated displays suited for that purpose.</p><h2 id="s1.3"><a href="#s1.3">1.3</a> <span>For CAD work</span></h2><p>CAD work is highly dependent on visualizing fine details.
In particular, a wireframe rendering may become unintelligible if it is of insufficient resolution.
A high resolution display will allow fine details to be seen even while viewing multiple viewports without having to maximize the viewport.
However the viewport may be maximized to occupy the full screen nonetheless, a great advantage compared to multi-monitor setups.</p><h2 id="s1.4"><a href="#s1.4">1.4</a> <span>For gaming/media</span></h2><p>Although this post is mostly focused on productivity, most if not all 8K TVs can be run in 4K at 120 Hz.
Modern TVs have decent input lag in the ballpark of 10 ms and may support FreeSync.
So these are excellent for gaming on the big screen when one needs to take a break from work.
Of course, this may not suffice for competitive professional FPS twitch shooters, but it is pretty darn good.
Multi monitor productivity setups using 4k 60 Hz monitors simply cannot achieve this.</p><p>An 8K TV will also natively support 1440p gaming and media at an exact integer ratio of 3:1 without scaling artifacts that a 4K display would introduce. Perfect for playing the latest titles that your GPU isn’t fast enough to run in 4K.</p><p>Please bear in mind that most GPUs will not run games performantly in 8K and there are basically no 8K movies.</p><p>For watching movies, I set my TV to 4K 120 Hz mode as well.
The 120 Hz is divisible by both 24 fps and 30 fps.
Furthermore, if the media player lags and delays by one frame for some reason, it is still a lot smoother to delay by 1/120 s rather than 1/24 s.</p><h2 id="s1.5"><a href="#s1.5">1.5</a> <span>Cost</span></h2><div><figure id="fig4"><a href="https://s.dllu.net/uHQgpIOWxQDA0fDg.png"><img src="https://s.dllu.net/uHQgpIOWxQDA0fDg.png" alt=""></a><figcaption><a href="#fig4">FIGURE 4</a> An LG Nanocell 99 65” 8K TV was only $1500 at Best Buy.</figcaption></figure></div><p>8K TVs tend to start at around $1500 to $2000 for a 65” one.
This is about the same as getting four 32” 4K monitors.</p><p>However, many people who get a multi-monitor productivity setup also buy a separate 4K TV just for gaming or media consumption.
In this case, having one screen that does it all may save some money.</p><h2 id="s1.6"><a href="#s1.6">1.6</a> <span>Connectivity</span></h2><p>8K TVs may be driven at 8K 60 Hz with no chroma subsampling by using HDMI 2.1, which is available on all current (Nvidia RTX 4000 series and AMD 7000 series) and previous gen (Nvidia RTX 3000 series, AMD 6000 series) graphics cards.
Older computers with GPUs outputting DisplayPort 1.4 may use adapters such as the Club3D one to achieve 8K 60 Hz.</p><h2 id="s2"><a href="#s2">2</a> <span>Potential cons and caveats</span></h2><h2 id="s2.1"><a href="#s2.1">2.1</a> <span>Desk and mounting</span></h2><div><figure id="fig5"><a href="https://i.dllu.net/IMG_0040_101abca0d08bf876.jpg"><img src="https://i.dllu.net/IMG_0040_1200_59264dccd924e751.jpg" alt=""></a><figcaption><a href="#fig5">FIGURE 5</a> My 75” × 42” desk is bigger than a single bed.</figcaption></figure></div><p>When purchasing a large display, one may need to sit farther back from the display when viewing full-screen content.
As such, a deeper desk may be needed.
Most desks are only 30” (76 cm) deep, which is an insufficient distance to sit from the screen.
Please take into consideration the potential extra costs of buying a bigger desk, or consider wall-mounting the display.</p><p>My desk is an Uplift four leg standing desk with a custom 75” × 42” dimension.
You could also buy a large butcher’s block or door and mount it on a desk frame.
Large dining tables or conference room tables work well too.</p><h2 id="s2.2"><a href="#s2.2">2.2</a> <span>Image quality issues</span></h2><h3 id="s2.2.1"><a href="#s2.2.1">2.2.1</a> <span>Uniformity</span></h3><p>Due to manufacturing variance, there may be some nonuniformity in high resolution displays, leading to what is called the “dirty screen effect”. This is not expected to be an issue for programming work, but can be distracting or harmful for photographic work or media consumption. An appropriate calibration can mitigate the problem but it is still recommended to obtain a uniform, professional display for colour-critical work.</p><blockquote>The Samsung QN800A has good gray uniformity. Although the screen is fairly uniform throughout, there’s a bit of dirty screen effect in the center, which could be distracting during sports. The screen is much more uniform in near-dark scenes. Keep in mind that uniformity may vary between units.</blockquote><p>Ratings sites such as <a href="https://www.rtings.com/tv/reviews/samsung/qn800a-8k-qled">RTINGS.com</a> have good measurements of uniformity.</p><div><figure id="fig6"><a href="https://i.dllu.net/dse-large_5fe27ae594205181.jpg"><img src="https://i.dllu.net/dse-large_ef1b0594e5459fb5.jpg" alt=""></a><figcaption><a href="#fig6">FIGURE 6</a> 50% Gray uniformity test from RTINGS.com. Source: <a href="https://www.rtings.com/tv/reviews/samsung/q900-q900r-8k-qled#test_179">RTINGS.com</a>.</figcaption></figure></div><h3 id="s2.2.2"><a href="#s2.2.2">2.2.2</a> <span>Checkerboard effect</span></h3><p>Some 8K TVs have a subtle “checkerboard” effect visible at the 1px scale, such as the Samsung QN700B and QN800A unless you have Game Mode enabled.
<strong>The problem is completely gone once you enable “Game Mode” in the TV</strong>.
This has led to some review comments such as quote a <a href="https://www.bestbuy.com/site/reviews/samsung-55-class-qn700b-neo-qled-8k-smart-tv/6517114">Best Buy review comment</a>,</p><blockquote>This is not a true HDR 8K TV.
To be HDR 8K, each pixel should have the full range of brightness and dimness to comprise an image. But each pixel only seems to have half the number of steps between full dim and full bright.
So to generate certain shades and colors between full bright and black, the pixels require 2 pixels, one that is brighter and one that is dimmer where the average between the pixels is the correct shade, color or value.
For most shades and colors, this alternating pixel technique forms a diagonal patchwork that has a noticeable checkerboard effect. Images viewed up close look bad.
This isn’t noticeable with windows that are pure white and the text is black, as black text on a white background looks good and is true 8k pixel density with no grid pattern and clean edges on the text.  
[…]
My best guess of what this display is doing is… instead of 8K number of pixels each capable of displaying a full range HDR10 1024 color steps per RGB element, they have two sets of pixels each set only capable of displaying 512 steps of information with every other pixel offset in capability by half a step. They combine the two simpler pixel values to achieve the more complex shades in between.
I can hear the Samsung display engineers, “Who would sit that close anyway to be able to see that?” Well, clearly people who want to use it as a computer monitor would.
The 700B is advertised as not just an HDR screen but an HDR10+ plus screen which is 10bit, but this does not seem to be the case as a non HDR image from my computer still requires dithering to achieve the commanded colors and shades.
No other display makers are making a true 8K display at this size at this price point, so I’m going to keep it and just sit further back so I don’t notice the dithering.</blockquote><div><figure id="fig7"><a href="https://i.dllu.net/2024-07-02-10-15-24_DSCF9108_ee0275f38e0c35fe126fbdfd0ccf2b7d3f32ae47_0aac8cf0ffd2700e.jpg"><img src="https://i.dllu.net/2024-07-02-10-15-24_DSCF9108_ee0275f38e0c35fe126fbdfd0ccf2b7d3f32ae47_1a3a0a203c4597ec.jpg" alt=""></a><figcaption><a href="#fig7">FIGURE 7</a> Checkerboard effect.</figcaption></figure></div><p>This does not affect most text rendering, but I have noticed this effect when editing photos.</p><div><figure id="fig8"><a href="https://i.dllu.net/2024-07-02-10-19-06_DSCF9109_4bf015a151e2719a582bf8ad22c1d9da40e291cf_d6e0350e46252b97.jpg"><img src="https://i.dllu.net/2024-07-02-10-19-06_DSCF9109_4bf015a151e2719a582bf8ad22c1d9da40e291cf_be63f7a46e4a6cc2.jpg" alt=""></a><figcaption><a href="#fig8">FIGURE 8</a> Doesn’t typically affect text rendering significantly.</figcaption></figure></div><p>Again, the problem is completely gone once you enable Game Mode on the TV, so be sure to enable it!</p><h2 id="s2.3"><a href="#s2.3">2.3</a> <span>Random software issues</span></h2><p>Since TVs are rarely designed for PC usage as a first class citizen, there may be some weird quirks or bugs.</p><h3 id="s2.3.1"><a href="#s2.3.1">2.3.1</a> <span>Nvidia Linux drivers</span></h3><p>If using Nvidia on Linux, to get 8K 60 Hz working, you need driver <a href="https://www.phoronix.com/news/NVIDIA-535.43.02-Linux-Driver">version 535</a> or later on Linux, which was released in May 2023. Versions prior to that would only do 8K 30 Hz.</p><p>Although Nvidia on Windows has supported 8K 60 Hz as soon as the RTX 3000 series came out, on Linux it took about two years for 8K 60 Hz support to work, spawning a <a href="https://github.com/NVIDIA/open-gpu-kernel-modules/discussions/238">salty thread on GitHub</a>.</p><p>So please make sure to update your drivers!</p><p>As a minor concern, there may be some slight vertical screen tearing between the left and right halves of the display.</p><h3 id="s2.3.2"><a href="#s2.3.2">2.3.2</a> <span>AMD Linux drivers</span></h3><p>Unfortunately, as of writing, on Linux, AMD GPUs do not have HDMI 2.1 so you cannot use an 8K TV in 8K 60 Hz mode unless you use a DisplayPort to HDMI adapter.
It works fine on AMD on Windows, however.</p><p>The AMD on Linux fiasco is because the <a href="https://arstechnica.com/gadgets/2024/02/hdmi-forum-to-amd-no-you-cant-make-an-open-source-hdmi-2-1-driver/">HDMI Forum has prohibited AMD from implementing HDMI 2.1</a> in their open source Linux drivers.</p><h3 id="s2.3.3"><a href="#s2.3.3">2.3.3</a> <span>Input Signal Plus</span></h3><p>To get 8K 60 Hz working, you need to go into TV menus and enable “Input Signal Plus”, “Enhanced HDMI”, or something similar.
For some silly reason, this may be disabled by default, which will relegate you to only 4K or 30 Hz.</p><h3 id="s2.3.4"><a href="#s2.3.4">2.3.4</a> <span>Wake up bugs</span></h3><p>Sometimes if your computer goes to sleep or turns the monitor off to save energy, when waking up the TV will not detect it.
Worse, sometimes when waking up, the TV will revert to only 4K mode, and you’d have to go into the menus to toggle the “Input Signal Plus” setting to make it work properly again.
This is pretty annoying but I just disabled turning off the monitor in my OS.</p><h3 id="s2.3.5"><a href="#s2.3.5">2.3.5</a> <span>Having another DisplayPort device</span></h3><p>When having another DisplayPort device plugged into your GPU, your computer may favor the DisplayPort device upon boot.
This means that when booting up, the BIOS menu, bootloader, and such will get sent to the DisplayPort device instead of the HDMI device.</p><p>This was annoying for me because I have a Valve Index VR headset which uses DisplayPort whereas the TV, using HDMI, is my main display.
I had to unplug the Valve Index, but if anyone knows any other methods then I would be interested to hear them.</p><h2 id="s2.4"><a href="#s2.4">2.4</a> <span>Display types</span></h2><p>Currently, 8K TVs are generally available as “full array backlight” IPS or VA panels.
These will have equivalent or superior contrast compared to most IPS desktop monitors, but will have inferior contrast compared with OLED displays.
OLED 8K TVs exist, but are prohibitively expensive at $30,000 as of writing.
However, OLED displays are prone to burn in when used for productivity work and may have features such as automatic dimming for static scenes, which can be distracting while performing lengthy tasks like programming.</p><h2 id="s2.5"><a href="#s2.5">2.5</a> <span>Coatings</span></h2><p>TVs tend to have a glossy coating for superior image quality. However for brightly lit rooms there may be unavoidable reflections.
For my use case, even though my TV is right next to a window, I have never found any issues focusing on my code with a dark theme.</p><h2 id="s3"><a href="#s3">3</a> <span>Example devices</span></h2><p>Unfortunately, there are very few 8K displays on the market, and many decent ones (such as the 55” Samsung QN700B) are discontinued.
Some 8K TVs include:</p><ul><li>Samsung Q900 series (55” to 82”)</li><li>Samsung Q800T series</li><li>Samsung Q900TS series</li><li>Samsung Q950TS series</li><li>Samsung QN700B series</li><li>Samsung QN800A/B/C series</li><li>Samsung QN900A/B/C</li><li>LG Nanocell 97 series</li><li>LG Nanocell 99 series</li><li>Sony Z8H series</li><li>Sony Z9G series</li><li>TCL Class 6-series 8K</li></ul><p>There are also some crazy $30,000 OLED displays and even crazier huge 262” microled displays that cost millions.</p><p>There is also a Dell UP3218K, but it costs the same as an 8K TV and is much smaller and has many problems. So I do not recommend it unless you really don’t have the desk space. Sitting further back from a bigger screen provides the same field of view as sitting close to a smaller display, and may have less eye strain.</p><p>I am excited about <a href="https://wccftech.com/tcl-unveils-27-inch-8k-57-inch-duhd-240hz-31-inch-4k-oled-dome-several-next-gen-displays/">upcoming TCL 8K displays</a>.</p><h2 id="s4"><a href="#s4">4</a> <span>FAQ</span></h2><blockquote>Isn’t it too big?</blockquote><p>The width and pixel density is the same as two 32” monitors.</p><blockquote>But there are no 8K movies</blockquote><p>I don’t usually watch movies on this, but when I do, I set it to 4K 120 Hz mode.</p><blockquote>But can your GPU even run 8K games?</blockquote><p>I play games in 4K 120 Hz mode. Actually, some games like Factorio could benefit from 8K 60 Hz mode, but those are not graphically intensive. Theoretically, Age of Empires II would be interesting too since you could see a lot of the map at once.</p><blockquote>How about a projector?</blockquote><p>There are no 8K projectors and the pixel density for displaying lots of text is a main motivation for getting an 8K display.</p><blockquote>How about the Apple Vision Pro?</blockquote><p>Despite having much better pixel density than other VR headsets, the pixel density <a href="https://kguttag.com/2024/02/05/spreadsheet-breaks-the-apple-vision-pros-avp-eye-tracking-foveation-the-first-through-the-optics-pictures/">isn’t quite there yet for looking at lots of text such as spreadsheets</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GitHub Cuts AI Deals with Google, Anthropic (425 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2024-10-29/microsoft-s-github-unit-cuts-ai-deals-with-google-anthropic</link>
            <guid>41985915</guid>
            <pubDate>Tue, 29 Oct 2024 16:20:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2024-10-29/microsoft-s-github-unit-cuts-ai-deals-with-google-anthropic">https://www.bloomberg.com/news/articles/2024-10-29/microsoft-s-github-unit-cuts-ai-deals-with-google-anthropic</a>, See on <a href="https://news.ycombinator.com/item?id=41985915">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <section>
        <h3>Why did this happen?</h3>
        <p>Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our <a href="https://www.bloomberg.com/notices/tos">Terms of
                Service</a> and <a href="https://www.bloomberg.com/notices/tos">Cookie Policy</a>.</p>
    </section>
    <section>
        <h3>Need Help?</h3>
        <p>For inquiries related to this message please <a href="https://www.bloomberg.com/feedback">contact
            our support team</a> and provide the reference ID below.</p>
        <p>Block reference ID:</p>
    </section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New Mac Mini with M4 (382 pts)]]></title>
            <link>https://www.apple.com/newsroom/2024/10/apples-new-mac-mini-is-more-mighty-more-mini-and-built-for-apple-intelligence/</link>
            <guid>41984519</guid>
            <pubDate>Tue, 29 Oct 2024 15:00:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.apple.com/newsroom/2024/10/apples-new-mac-mini-is-more-mighty-more-mini-and-built-for-apple-intelligence/">https://www.apple.com/newsroom/2024/10/apples-new-mac-mini-is-more-mighty-more-mini-and-built-for-apple-intelligence/</a>, See on <a href="https://news.ycombinator.com/item?id=41984519">Hacker News</a></p>
<div id="readability-page-1" class="page">


	
    







 
<nav id="ac-localnav" lang="en-US" role="navigation" aria-label="Newsroom" data-analytics-region="local nav" data-sticky="">
	
    
    
        




    
    
    
	
	

</nav>





<main id="main" role="main"> 




<span id="opens-in-new-window">opens in new window</span>
<section>
<article data-analytics-activitymap-region-id="article">






    
    
    









    





    <div>
        
		
        <div>
                    
                    
                        <span>PRESS RELEASE</span>
                    
                    
                        <span>October 29, 2024</span>
                    
                    
                </div>

        <div>
                
                
                
                    <h2>
                        
    
        Apple’s all-new Mac&nbsp;mini is more mighty, more mini, and built for Apple&nbsp;Intelligence
    

                    </h2>
                
            </div>

        <div>
                
                
                    The compact, do-it-all desktop now features the power of M4 and M4&nbsp;Pro, and marks an important environmental milestone as the first carbon neutral Mac
                
            </div>

        
            
    
    
    
    
    

        

    </div>







    
    
    






  
    
    
    
    
      <figure aria-label="Media, A hand holds the all-new Mac mini.">
        <div>
             
              
              <div>
                With M4 and M4 Pro, the new Mac mini brings incredible performance and connectivity in a design that’s small enough to fit in your hand.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2024/10/apples-new-mac-mini-apples-new-mac-mini-is-more-mighty-more-mini-and-built-for-apple-intelligence/article/Apple-Mac-mini-hero.zip" download="" data-analytics-title="download image - Apple-Mac-mini-hero_big" aria-label="Download media, A hand holds the all-new Mac mini."></a>
          </div>
      </figure>
    
  






    
    
    


     
     
    
    
        <div>
             
                 <div><strong><span>CUPERTINO, CALIFORNIA</span></strong> Apple today unveiled the all-new <a href="https://youtu.be/TtFm9n3NVzE" target="_blank" rel="nofollow" data-analytics-exit-link="">Mac mini</a> powered by the M4 and new M4 Pro chips, and redesigned around Apple silicon to pack an incredible amount of performance into an even smaller form of just 5 by 5 inches. With M4, Mac mini delivers up to 1.8x faster CPU performance and 2.2x faster GPU performance over the M1 model.<sup>1</sup> With M4 Pro, it takes the advanced technologies in M4 and scales them up to tackle even more demanding workloads. For more convenient connectivity, it features front and back ports, and for the first time includes Thunderbolt 5 for faster data transfer speeds on the M4 Pro model. The new Mac mini is also built for Apple Intelligence, the personal intelligence system that transforms how users work, communicate, and express themselves while protecting their privacy. And marking an important environmental milestone, Mac mini is Apple’s first carbon neutral Mac with an over 80 percent reduction in greenhouse gas emissions across its materials, manufacturing, transportation, and customer use.<sup>2</sup> Starting at just $599 with 16GB of memory, the new Mac mini is available to pre-order today, with availability beginning November 8.
</div>
                 
             
                 <div>“The new Mac mini delivers gigantic performance in an unbelievably small design thanks to the power efficiency of Apple silicon and an innovative new thermal architecture,” said John Ternus, Apple’s senior vice president of Hardware Engineering. “Combined with the performance of M4 and the new M4 Pro chip, enhanced connectivity on both the front and back, and the arrival of Apple Intelligence, Mac mini is more capable and versatile than ever, and there is nothing else like it.”
</div>
                 
             
                 <h2><strong>Small, but Fierce</strong>
</h2>
                 
             
                 <div>The new Mac mini footprint is less than half the size of the previous design at just 5 by 5 inches, so it takes up much less space on a desk. The super-compact system is enabled by the incredible power efficiency of Apple silicon and an innovative thermal architecture, which guides air to different levels of the system, while all venting is done through the foot.
</div>
                 
             
         </div>
 

    
    
    


    
    <div data-component-list="ScrollAnimationDefault AutoPlayVideo">
        <div>The unbelievably compact form is made possible by the power-efficient performance of Apple silicon combined with an all-new innovative thermal architecture.</div>
        
            <a aria-label="Download video: Mac Mini Thermal Architecture" data-analytics-title="Download video - Mac Mini Thermal Architecture" download="" href="https://www.apple.com/newsroom/videos/videos-2024/autoplay/2024/10/apple-mac-mini-thermal-architecture/downloads/Apple-Mac-mini-thermal-architecture.zip">
            </a>
        
    </div>






    
    
    


     
     
    
    
        <div>When compared to the best-selling PC desktop in its price range, Mac mini is up to 6x faster at one-twentieth the size.<sup>1</sup> For a wide range of users, from students to aspiring creatives and small business owners, the Mac mini with M4 is a tiny powerhouse. Mac mini with M4 features a 10-core CPU, 10-core GPU, and now starts with 16GB of unified memory. Users will feel the performance of M4 in everything they do, from multitasking across everyday productivity apps to creative projects like video editing, music production, or writing and compiling code.
</div>
 

    
    
    






  
    
    
    
    
      <figure aria-label="Media, A Mac mini user works at a desk surrounded by ceramics pieces.">
        <div>
             
              
              <div>
                Mac mini is more versatile than ever — perfect for anywhere, from home offices to large businesses and beyond.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2024/10/apples-new-mac-mini-apples-new-mac-mini-is-more-mighty-more-mini-and-built-for-apple-intelligence/article/Apple-Mac-mini-lifestyle-small-business.zip" download="" data-analytics-title="download image - Apple-Mac-mini-lifestyle-small-business_big" aria-label="Download media, A Mac mini user works at a desk surrounded by ceramics pieces."></a>
          </div>
      </figure>
    
  






    
    
    


     
     
    
    
        <div>
             
                 <div><strong>When compared to the Mac mini with Intel Core i7, Mac mini with M4:</strong>
</div>
                 
             
                 <div><ul>
<li>Applies up to 2.8x more audio effect plugins in a Logic Pro project.<sup>1</sup></li>
<li>Delivers up to 13.3x faster gaming performance in World of Warcraft: The War Within.<sup>1</sup></li>
<li>Enhances photos with up to 33x faster image upscaling performance in Photomator.<sup>3</sup></li>
</ul>
</div>
                 
             
                 <div><strong>When compared to the Mac mini with M1, Mac mini with M4:</strong>
</div>
                 
             
                 <div><ul>
<li>Performs spreadsheet calculations up to 1.7x faster in Microsoft Excel.<sup>1</sup></li>
<li>Transcribes with on-device AI speech-to-text up to 2x faster in MacWhisper.<sup>1</sup></li>
<li>Merges panoramic images up to 4.9x faster in Adobe Lightroom Classic.<sup>4</sup></li>
</ul>
</div>
                 
             
                 <h2><strong>Introducing M4 Pro for Pro-Level Performance&nbsp;</strong>
</h2>
                 
             
                 <div>For users who want pro-level performance, Mac mini with M4 Pro features the world’s fastest CPU core<sup>5</sup> with lightning-fast single-threaded performance. With up to 14 cores, including 10 performance cores and four efficiency cores, M4 Pro also provides phenomenal multithreaded performance. With up to 20 cores, the M4 Pro GPU is up to twice as powerful as the GPU in M4, and both chips bring hardware-accelerated ray tracing to the Mac mini for the first time. The Neural Engine in M4 Pro is also over 3x faster than in Mac mini with M1, so on-device Apple Intelligence models run at blazing speed. M4 Pro supports up to 64GB of unified memory and 273GB/s of memory bandwidth — twice as much bandwidth as any AI PC chip —&nbsp;for accelerating AI workloads. And M4 Pro supports Thunderbolt 5, which delivers up to 120 Gb/s data transfer speeds on Mac mini, and more than doubles the throughput of Thunderbolt&nbsp;4.
</div>
                 
             
         </div>
 

    
    
    






  
    
    
    
    
      <figure aria-label="Media, The M4 Pro logo.">
        <div>
             
              
              <div>
                The M4 Pro chip takes the advanced technologies debuted in M4 and scales them up to tackle even more demanding workloads.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2024/10/apples-new-mac-mini-apples-new-mac-mini-is-more-mighty-more-mini-and-built-for-apple-intelligence/article/Apple-Mac-mini-M4-Pro-chip.zip" download="" data-analytics-title="download image - Apple-Mac-mini-M4-Pro-chip_big" aria-label="Download media, The M4 Pro logo."></a>
          </div>
      </figure>
    
  






    
    
    


     
     
    
    
        <div>
             
                 <div><strong>When compared to the Mac mini with Intel Core i7, Mac mini with M4 Pro</strong>:
</div>
                 
             
                 <div><ul>
<li>Performs spreadsheet calculations up to 4x faster in Microsoft Excel.<sup>1</sup></li>
<li><span>Executes scene-edit detection up to 9.4x faster in Adobe Premiere Pro.<sup>3</sup></span></li>
<li>Transcribes with on-device AI speech-to-text up to 20x faster in MacWhisper.<sup>1</sup></li>
<li>Processes basecalling for DNA sequencing in Oxford Nanopore MinKNOW up to 26x faster.<sup>1</sup></li>
</ul>
</div>
                 
             
                 <div><strong>When compared to the Mac mini with M2 Pro, Mac mini with M4 Pro</strong>:
</div>
                 
             
                 <div><ul>
<li>Applies up to 1.8x more audio effect plugins in a Logic Pro project.<sup>1</sup></li>
<li>Renders motion graphics to RAM up to 2x faster in Motion.<sup>6</sup></li>
<li>Completes 3D renders up to 2.9x faster in Blender.<sup>6</sup></li>
</ul>
</div>
                 
             
         </div>
 

    
    
    






  
    
    
    
    
      <figure aria-label="Media, A Mac mini user sits at a desk with three displays and a musical keyboard.">
        <div>
             
              
              <div>
                The Mac mini with M4 Pro comes with Thunderbolt 5 for the first time, which more than doubles transfer speeds, so users will be able to use faster external storage and connect to more accessories at the same time.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2024/10/apples-new-mac-mini-apples-new-mac-mini-is-more-mighty-more-mini-and-built-for-apple-intelligence/article/Apple-Mac-mini-lifestyle-music-studio.zip" download="" data-analytics-title="download image - Apple-Mac-mini-lifestyle-music-studio_big" aria-label="Download media, A Mac mini user sits at a desk with three displays and a musical keyboard."></a>
          </div>
      </figure>
    
  






    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>Upgraded Connectivity and Display Support&nbsp;</strong>
</h2>
                 
             
                 <div>The new Mac mini features a wide array of ports to drive any setup. It includes front-facing ports for more convenient access, including two USB-C ports that support USB 3, and an audio jack with support for high-impedance headphones. On the back, Mac mini with M4 includes three Thunderbolt 4 ports, while Mac mini with M4 Pro features three Thunderbolt 5 ports. Mac mini comes standard with Gigabit Ethernet, configurable up to 10Gb Ethernet for faster networking speeds, and an HDMI port for easy connection to a TV or HDMI display without an adapter. With M4, Mac mini can support up to two 6K displays and up to one 5K display, and with M4 Pro, it can support up to three 6K displays at 60Hz for a total of over 60 million pixels.
</div>
                 
             
         </div>
 

    
    
    


    
    
        
        
        
            <div role="group" aria-label="gallery" data-component-list="MixinGallery" id="mac-mini-connectivity">
            <nav role="presentation">
                <ul role="tablist">
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-aa2a6f58d0676688ce811a3d004a664a" href="#gallery-aa2a6f58d0676688ce811a3d004a664a" data-ac-gallery-trigger="gallery-aa2a6f58d0676688ce811a3d004a664a"><span>Mac mini is shown from the front.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-a000bfc085c36199d7b35221570ec3dd" href="#gallery-a000bfc085c36199d7b35221570ec3dd" data-ac-gallery-trigger="gallery-a000bfc085c36199d7b35221570ec3dd"><span>Mac mini is shown from the back.</span></a>
                        </li>
                    
                </ul>
            </nav>
            <div>
                
                    
                        
                        <div id="gallery-aa2a6f58d0676688ce811a3d004a664a" aria-labelledby="gallery-dotnav-aa2a6f58d0676688ce811a3d004a664a" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:front-facing-ports">
                                
                                <div>
                                    <div>The new front-facing ports on Mac mini include two USB-C ports and an audio jack with support for high-impedance headphones.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2024/10/apples-new-mac-mini-apples-new-mac-mini-is-more-mighty-more-mini-and-built-for-apple-intelligence/article/Apple-Mac-mini-front-facing-ports.zip" download="" data-analytics-title="download image - Apple-Mac-mini-front-facing-ports_big" aria-label="Download media, Mac mini is shown from the front."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-a000bfc085c36199d7b35221570ec3dd" aria-labelledby="gallery-dotnav-a000bfc085c36199d7b35221570ec3dd" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:back-facing-ports">
                                
                                <div>
                                    <div>On the back, the M4 model features three Thunderbolt 4 ports, and the M4 Pro model includes Thunderbolt 5 for the first time. Both models also include Gigabit Ethernet and an HDMI port.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2024/10/apples-new-mac-mini-apples-new-mac-mini-is-more-mighty-more-mini-and-built-for-apple-intelligence/article/Apple-Mac-mini-back-facing-ports.zip" download="" data-analytics-title="download image - Apple-Mac-mini-back-facing-ports_big" aria-label="Download media, Mac mini is shown from the back."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
            </div>
            
                
                
                <nav role="presentation">
                    
                        <ul>
                            <li>
                                
                            </li>
                            <li>
                                
                            </li>
                        </ul>
                    
                </nav>
            
        </div>
    


    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>A New Era with Apple Intelligence on the Mac</strong>
</h2>
                 
             
                 <div>Apple Intelligence ushers in a new era for the Mac, bringing personal intelligence to the personal computer. Combining powerful generative models with industry-first privacy protections, Apple Intelligence harnesses the power of Apple silicon and the Neural Engine to unlock new ways for users to work, communicate, and express themselves on Mac. It is available in U.S. English with macOS Sequoia 15.1. With systemwide Writing Tools, users can refine their words by rewriting, proofreading, and summarizing text nearly everywhere they write. With the newly redesigned Siri, users can move fluidly between spoken and typed requests to accelerate tasks throughout their day, and Siri can answer thousands of questions about Mac and other Apple products. New Apple Intelligence features will be available in December, with additional capabilities rolling out in the coming months. Image Playground gives users a new way to create fun original images, and Genmoji allows them to create custom emoji in seconds. Siri will become even more capable, with the ability to take actions across the system and draw on a user’s personal context to deliver intelligence that is tailored to them. In December, ChatGPT will be integrated into Siri and Writing Tools, allowing users to access its expertise without needing to jump between tools.
</div>
                 
             
         </div>
 

    
    
    


    
    
        
        
        
            <div role="group" aria-label="gallery" data-component-list="MixinGallery" id="mac-mini-and-apple-intelligence">
            <nav role="presentation">
                <ul role="tablist">
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-b75292ffad2e1c90bfcda9fb581635af" href="#gallery-b75292ffad2e1c90bfcda9fb581635af" data-ac-gallery-trigger="gallery-b75292ffad2e1c90bfcda9fb581635af"><span>A Writing Tools screen features highlighted text from a scientific paper alongside proofreading and rewriting options.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-d491a29244737305f738517e6fbbbfdb" href="#gallery-d491a29244737305f738517e6fbbbfdb" data-ac-gallery-trigger="gallery-d491a29244737305f738517e6fbbbfdb"><span>A typed request to Siri says, “Show me all files Ian sent last week.”</span></a>
                        </li>
                    
                </ul>
            </nav>
            <div>
                
                    
                        
                        <div id="gallery-b75292ffad2e1c90bfcda9fb581635af" aria-labelledby="gallery-dotnav-b75292ffad2e1c90bfcda9fb581635af" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:writing-tools">
                                
                                <div>
                                    <div>With brand-new systemwide Writing Tools powered by Apple Intelligence, users can rewrite, proofread, and summarize text virtually everywhere they type.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2024/10/apples-new-mac-mini-apples-new-mac-mini-is-more-mighty-more-mini-and-built-for-apple-intelligence/article/Apple-Mac-mini-Writing-Tools.zip" download="" data-analytics-title="download image - Apple-Mac-mini-Writing-Tools_big" aria-label="Download media, A Writing Tools screen features highlighted text from a scientific paper alongside proofreading and rewriting options."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-d491a29244737305f738517e6fbbbfdb" aria-labelledby="gallery-dotnav-d491a29244737305f738517e6fbbbfdb" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:siri">
                                
                                <div>
                                    <div>In the coming months, Siri will be even more capable on macOS Sequoia, with the ability to understand a user’s personal context and deliver intelligence that is tailored to them.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2024/10/apples-new-mac-mini-apples-new-mac-mini-is-more-mighty-more-mini-and-built-for-apple-intelligence/article/Apple-Mac-mini-type-to-Siri.zip" download="" data-analytics-title="download image - Apple-Mac-mini-type-to-Siri_big" aria-label="Download media, A typed request to Siri says, “Show me all files Ian sent last week.”"></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
            </div>
            
                
                
                <nav role="presentation">
                    
                        <ul>
                            <li>
                                
                            </li>
                            <li>
                                
                            </li>
                        </ul>
                    
                </nav>
            
        </div>
    


    
    
    


     
     
    
    
        <div>
             
                 <div>Apple Intelligence does all this while protecting users’ privacy at every step. At its core is on-device processing, and for more complex tasks, Private Cloud Compute gives users access to Apple’s even larger, server-based models and offers groundbreaking protections for personal information. In addition, users can access ChatGPT for free without creating an account, and privacy protections are built in — their IP addresses are obscured and OpenAI won’t store requests. For those who choose to connect their account, OpenAI’s data-use policies apply.
</div>
                 
             
                 <h2><strong>The First Carbon Neutral Mac&nbsp;</strong>
</h2>
                 
             
                 <div>The new Mac mini is Apple’s first carbon neutral Mac, marking a significant milestone toward <a href="https://www.apple.com/environment/" target="_blank">Apple 2030</a>, the company’s goal to be carbon neutral across the entire carbon footprint by the end of this decade.
</div>
                 
             
                 <div>Mac mini is made with over 50 percent recycled content overall, including 100 percent recycled aluminum in the enclosure, 100 percent recycled gold plating in all Apple-designed printed circuit boards, and 100 percent recycled rare earth elements in all magnets. The electricity used to manufacture Mac mini is sourced from 100 percent renewable electricity. And, to address 100 percent of the electricity customers use to power Mac mini, Apple has invested in clean energy projects around the world. Apple has also prioritized lower-carbon modes of shipping, like ocean freight, to further reduce emissions from transportation. Together, these actions have reduced the carbon footprint of Mac mini by over 80 percent.<sup>2</sup> For the small amount of remaining emissions, Apple applies high-quality carbon credits from nature-based projects, like those generated by its innovative <a href="https://www.apple.com/newsroom/2024/03/apples-restore-fund-cultivates-new-roots-in-the-atlantic-forest/" target="_blank">Restore Fund</a>.
</div>
                 
             
                 <div>In another first for Mac mini, the packaging is now entirely fiber-based, bringing Apple closer to its goal to remove plastic from its packaging by 2025.
</div>
                 
             
         </div>
 

    
    
    


    
        
        
        
        
            <figure aria-label="Media, Mac mini with a carbon neutral logo.">
                <div>
                         
                            
                            <div>
                                The new Mac mini is Apple’s first carbon neutral Mac.
                            </div>
                        
                        
                        
                        
                        <a href="https://www.apple.com/newsroom/images/2024/10/apples-new-mac-mini-apples-new-mac-mini-is-more-mighty-more-mini-and-built-for-apple-intelligence/article/Apple-Mac-mini-carbon-neutral.zip" download="" data-analytics-title="download image - Apple-Mac-mini-carbon-neutral_inline" aria-label="Download media, Mac mini with a carbon neutral logo."></a>
                    </div>
            </figure>
        
    










    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>An Unrivaled Experience with macOS Sequoia</strong>
</h2>
                 
             
                 <div><a href="https://www.apple.com/macos/macos-sequoia-preview/" target="_blank">macOS Sequoia</a> completes the new Mac mini experience with a host of exciting features, including iPhone Mirroring, allowing users to wirelessly interact with their iPhone, its apps, and notifications directly from their Mac.<sup>7</sup> Safari, the world’s fastest browser,<sup>8</sup> now offers the Highlights feature, which quickly pulls up relevant information from a site; a smarter, redesigned Reader with a table of contents and high-level summary; and a new Video Viewer to watch videos without distractions. With Distraction Control, users can hide items on a webpage that they may find disruptive to their browsing. Gaming gets even more immersive with features like Personalized Spatial Audio and improvements to Game Mode, along with a breadth of exciting titles, including the upcoming Assassin’s Creed Shadows. Easier window tiling means users can stay organized with a window layout that works best for them. The all-new Passwords app gives convenient access to passwords, passkeys, and other credentials — all stored in one place. And users can apply new, beautiful built-in backgrounds for video calls, which include a variety of color gradients and system wallpapers, or upload their own photos.
</div>
                 
             
         </div>
 

    
    
    


     
     
    
    
        <div>
             
                 
                 
             
                 <div><ul>
<li>Customers can pre-order the new Mac mini with M4 and M4 Pro starting today, Tuesday, October 29, on <a href="https://www.apple.com/store/" target="_blank">apple.com/store</a> and in the Apple Store app in 28 countries and regions, including the U.S. It will start arriving to customers, and in Apple Store locations and Apple Authorized Resellers, beginning Friday, November&nbsp;8.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Mac mini with M4 starts at <strong>$599</strong> (U.S.) and <strong>$499</strong> (U.S.) for education. Additional technical specifications are available at <a href="https://www.apple.com/mac-mini/" target="_blank">apple.com/mac-mini</a>.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Mac mini with M4 Pro starts at <strong>$1,399</strong> (U.S.) and <strong>$1,299</strong> (U.S.) for education. Additional technical specifications are available at <a href="https://www.apple.com/mac-mini/" target="_blank">apple.com/mac-mini</a>.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>New accessories with USB-C — including Magic Keyboard (<strong>$99</strong> U.S.), Magic Keyboard with Touch ID (<strong>$149</strong> U.S.), Magic Keyboard with Touch ID and Numeric Keypad (<strong>$179</strong> U.S.), Magic Trackpad (<strong>$129</strong> U.S.), Magic Mouse (<strong>$79</strong> U.S.), and Thunderbolt 5 Pro Cable (<strong>$69</strong>) —&nbsp;are available at <a href="https://www.apple.com/store/" target="_blank">apple.com/store</a>.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Apple Intelligence is available now as a free software update for Mac with M1 and later, and can be accessed in most regions around the world when the device and Siri language are set to U.S. English. The first set of features is in beta and available with macOS Sequoia 15.1, with more features rolling out in the months to come.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Apple Intelligence is quickly adding support for more languages. In December, Apple Intelligence will add support for localized English in <em>Australia</em>, <em>Canada</em>,<em> Ireland,</em> <em>New Zealand</em>, <em>South Africa</em>, and the <em>U.K.</em>, and in April, a software update will deliver expanded language support, with more coming throughout the year. Chinese, English (India), English (Singapore), French, German, Italian, Japanese, Korean, Portuguese, Spanish, Vietnamese, and other languages will be supported.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>With Apple Trade In, customers can trade in their current computer and get credit toward a new Mac. Customers can visit <a href="https://www.apple.com/shop/trade-in/" target="_blank">apple.com/shop/trade-in</a> to see what their device is worth.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>AppleCare+ for Mac provides unparalleled service and support. This includes unlimited incidents of accidental damage, battery service coverage, and 24/7 support from the people who know Mac best.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Every customer who buys directly from Apple Retail gets access to Personal Setup. In these guided online sessions, a Specialist can walk them through setup, or focus on features that help them make the most of their new device. Customers can also learn more about getting started with their new device with a Today at Apple session at their nearest Apple Store.</li>
</ul>
</div>
                 
             
         </div>
 

    
    
    




    
    
        
    


    
    
    



    
    
    




    




    
    
    





    
    
    <div>
            <ol>
<li>Testing was conducted by Apple in September and October 2024. See <a href="https://www.apple.com/mac-mini/" target="_blank">apple.com/mac-mini</a> for more information.</li>
<li>Carbon reductions are calculated against a business-as-usual baseline scenario: No use of clean electricity for manufacturing or product use, beyond what is already available on the latest modeled grid; Apple’s carbon intensity of key materials as of 2015; and Apple’s average mix of transportation modes by product line across three years. Learn more at <a href="https://www.apple.com/environment/" target="_blank">apple.com/2030</a>.</li>
<li>Results are compared to previous-generation 3.2GHz 6-core Intel Core i7-based Mac&nbsp;mini systems with Intel Iris UHD Graphics 630, 64GB of RAM, and 2TB SSD.</li>
<li>Results are compared to previous-generation Mac mini systems with Apple M1, 8-core CPU, 8-core GPU, 16GB of RAM, and 2TB SSD.</li>
<li>Testing conducted by Apple in October 2024 using shipping competitive systems and select industry-standard benchmarks.</li>
<li>Results are compared to previous-generation Mac mini systems with Apple M2 Pro, 12-core CPU, 19-core GPU, 32GB of RAM, and 8TB SSD.</li>
<li>Available on Mac computers with Apple&nbsp;silicon and Intel-based Mac computers with a T2 Security Chip. Requires that iPhone and Mac are signed in with the same Apple&nbsp;Account using two-factor authentication, iPhone and Mac are near each other and have Bluetooth and Wi-Fi turned on, and Mac is not using AirPlay or Sidecar. Some iPhone features (e.g., camera and microphone) are not compatible with iPhone&nbsp;Mirroring.</li>
<li>Testing was conducted by Apple in August 2024. See <a href="https://www.apple.com/safari/" target="_blank">apple.com/safari</a> for more information.</li>
</ol>

        </div>



    
    
    






    

















		
		
			
























		
		

</article>



</section>
</main>


	

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Writing in Pictures: Richard Scarry and the art of children's literature (174 pts)]]></title>
            <link>https://yalereview.org/article/chris-ware-richard-scarry</link>
            <guid>41983622</guid>
            <pubDate>Tue, 29 Oct 2024 13:24:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://yalereview.org/article/chris-ware-richard-scarry">https://yalereview.org/article/chris-ware-richard-scarry</a>, See on <a href="https://news.ycombinator.com/item?id=41983622">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                    
        <h2>Writing in Pictures</h2>
            
            <p><a href="https://yalereview.org/author/chris-ware">Chris Ware</a></p>

            
            
                                                                                            <figure>
    <img src="https://d181q449nqu6en.cloudfront.net/content/craft/articles/_850xAUTO_crop_center-center_none/Scarry-Cars-Trucks-Things-That-Go.jpg?mtime=20240908123939&amp;focal=none&amp;tmtime=20240909060114" alt="" loading="lazy">
    <figcaption><p>The original cover sketch for Richard Scarry’s Cars and Trucks and Things That Go, which was first published in 1974. <span>Courtesy Penguin Random House</span></p></figcaption>
</figure>
                                                                                                                                                                <div>
    <p><span>As a boy, </span>I knew I was supposed to like cars and trucks and things that go. But as an unathletic and decidedly unboyish kid, I only got close to liking one car—my mom’s blue Volkswagen Ghia, which she used to ferry me to and from school (and, when she needed some time to herself, to her parents’—my grandparents’—house for an overnight visit). In fact, I didn’t just like that car, I loved it, so much so that the day it was towed away I secretly chipped a piece of the sky-colored paint from the chassis and tearfully hid it in a little box. I never had the chance to develop such a special relationship with a truck or a bus or an airplane or anything else with a motor or wheels—in fact, such things scared me, and to this day I have never changed a tire.</p>
<p>In my grandparents’ second-floor guest room, formerly my mother’s childhood room, one bookcase had a row of children’s books slumped to the side, offering a chronological core sample of my grandmother’s attempts to busy not only her own kids, but all the grandkids who’d stayed there before me. There were the original Oz books, a copy of <em>Ferdinand the Bull</em>, Monro Leaf’s inexplicably compelling yet mildly fascistic <em>Manners Can Be Fun, </em>some 1950s and 1960s Little Golden Books purchased at the Hinky Dinky supermarket down the street, and, among many others I’ve now long forgotten, the big blue, green, and red shiny square of Richard Scarry’s <em>Best Word Book Ever. </em>The largish (even just plain large if you were smallish when holding it) book offered a visual index of the everyday puzzle pieces of life in humble, colored-in line drawings. Each page was a fresh, funny composition of some new angle on the world, making the book a sort of quotidian picture-map containing everything imaginable and unimaginable a kid might be curious about: where and how people lived, slept, ate, played, and worked.</p>
<p>The thing is, “people” weren’t anywhere to be seen in <em>Best Word Book Ever</em>. Instead, the whole world was populated by animals: rabbits, bears, pigs, cats, foxes, dogs, raccoons, lions, mice, and more. Somehow, though, that made the book’s view of life feel more real and more welcoming. A dollhouse-like cutaway view of a rabbit family in their house getting ready for their day didn’t seem to just picture the things themselves—they <em>were </em>the things themselves, exuding a grounded warmth that said, “Yes, everywhere we live in houses and cook together and get dressed, just like you.” </p>
</div>
                                                                                                                                                                                                                                                <p>
    <em>I Am a Bunny </em>stands as one of the true tranquil masterpieces of children’s book art.
    </p>
                                                                                                                                                <div>
    <p>Mirroring these rabbits, across two pages an index-like series of images depicted a bear named Kenny getting out of bed, getting dressed, and going down for breakfast. This would galvanize me to action: I’d take out the book and mimic Kenny, washing my face with a washcloth (which I never did at my own house), brush my teeth, get dressed, and make my bed. Then I’d head downstairs to the kitchen with the book under my pencil-thin arm, where my grandmother would gamely try to serve me the same breakfast Kenny was having, emulating as best she could the individual menu items: pancakes, “warm cereal,” orange juice, bacon, and toast.</p>
<p>A little later in the book, in a two-page spread titled “Mealtime,” a family of orange pigs surrounded a large dinner table laid out with plates and bowls of various foods. The lower left corner of the rightmost page cradled a wooden bowl of evenly green lettuce leaves with three tomato wedges. I don’t know why, but that drawing so thoroughly captured… something for me that, for years at my grandparents’ house, it became my standing side order. While watching television or reading or drawing in the guest room, I’d smell the English muffin toasting and the breaded pork chops and potatoes cooking, and I’d see the setting sunlight warming the house’s old wood shingles—and I’d know my grandmother would have that three-tomato salad on the side, ready for me, just like the pigs were having in Richard Scarry’s book.</p>
<div><p>I must have been a real pain in the ass as a kid. But Richard Scarry somehow made me feel safe and settled.</p></div>
<p><span>this year is </span>the 50th anniversary of Scarry’s 1974 <em>Cars and Trucks and Things That Go, </em>which strikes me as a commemoration worthy of ballyhoo, especially now that, as a dad myself, I’ve spent so much time ferrying my own daughter to and from school and birthday parties in various cars that—well, <em>mostly </em>goed. (I’ve owned five automobiles in my life, all of them cheap, one of which smoked and required the driver’s side door to be kept shut with a bungee cord hooked to the opposite armrest, stretched across both driver and passenger. What can I say? I was a young cartoonist on a cartoonist’s budget.)</p>
<p>Unlike those budget vehicles, however, the new deluxe Penguin Random House anniversary edition of <em>Cars and Trucks and Things That Go </em>is lavishly well-made, attentively reprinted with sharp black lines and warm, rich, watercolors. It includes an especially lively afterword by Scarry’s son Huck, in which he explains, using language even a kid can understand, how his dad wrote and drew the book, as well as hinting at what it was like to grow up as the son of arguably the world’s most popular and successful children’s book author.</p>
<p>Richard McClure Scarry was born on June 5, 1919, in the Dorchester neighborhood of Boston, Massachusetts. His Irish-American father, John James Scarry, ran Scarry’s Department Store so congenially and cannily that even during the Great Depression the whole family—his mother, an aunt, four brothers, and one sister—lived comfortably. According to Walter Retan and Ole Risom’s <em>The Busy, Busy World of Richard Scarry</em>, when Scarry was a boy and his mother asked him to go to the store to get provisions, he would write his grocery list not with words but with pictures. So his mother signed him up for drawing lessons at the Boston Museum of Fine Arts, where she also brought him to see the paintings and sculptures. </p>
<p>It took Scarry, who was uninterested in school, five years to receive his high school diploma, in part because he spent a fair amount of his time lazing around at the public library and visiting burlesque shows (this time, one assumes, without his mother). This disappointed his father, who pressured him into going to a local business school, a fate to which Scarry acceded but loathed so deeply he soon withdrew and re-enrolled at the Museum of Fine Arts. “You will live in a garret and eat nothing but spaghetti,” his father warned. But Richard’s mind was made up. Then, after a few classes and December 7, 1941, he was drafted.</p>
<p>Scarry later recalled his experience in the military as “the best war ever.” At basic training in New Jersey, his commanding officers discovered that he could draw, leading Scarry to be largely excused from the rigors of pushups so that he could work as a sign painter. Leapfrogging to the rank of lieutenant (a prerequisite for his new post as art director of the Army in North Africa), he arrived at the port of Casablanca in somewhat plum circumstances, tasked with creating morale-boosting propaganda by doing things like illustrating information manuals and guidebooks and drawing maps describing the worldwide progress of Allied fighting. He and his fellow officers discovered a nice villa twenty miles outside Oran where they were permitted to stay, borrowing their colonel’s Buick to drive themselves back and forth to work. Later stationed in Venice and Paris, Scarry’s experience of World War II was, well, charmed.</p>
<p>After his discharge in 1946, Scarry moved to New York, where he briefly worked as an art director for <em>Vogue </em>and then in an advertising agency before acquiring an agent, who was able to secure him an illustration job with the then brand-new but now completely forgotten <em>Holiday </em>magazine. The job paid the princely sum of $2,000. (I can confirm from personal experience that such pay has changed little since 1946; $2,000 is still the average, if not generous, going rate for magazine artwork. But plugged into an inflation calculator, $2,000 in 1942 clocks in at $34,524.73 today.) With his living expenses suddenly covered well into the future, Scarry moved into a nicer apartment. He met an advertising copywriter named Patricia Murphy, and in 1948 they got married.</p>
<p>Meanwhile, in Racine, Wisconsin, a printing company named Western Publishing and its imprint Whitman (which had found great success in the 1930s with the Big Little Books and other novelties) were hatching a new idea for children’s literature, a series that would be christened “Little Golden Books.” Up to that point, children’s books had traditionally been a $1.50-and-up Christmas gift—$25.19 in today’s inflationary dollars—luxurious gilt volumes bestowed by great aunts that told of princes and princesses and things that didn’t go anywhere at all. These new Golden Books, by contrast, were to be cheaply produced and democratically priced at twenty-five cents, and would be sold year-round at pharmacies and, as my grandmother referred to them, “dime stores.” </p>
<p>The brainchild of Georges Duplaix and Lucille Ogle, two editors at Western’s recently opened East Coast offices in Poughkeepsie, New York, Golden Books employed displaced if not just plain refugee artists from Europe like Feodor Rojankovsky, Tibor Gergely, and Gustaf Tenggren. Working in a careful, deliberate, and illuminatory style, they carefully limned every hair of every dog—think <em>The Poky Little Puppy</em>—and set every page aglow with a strangely dark, yet warm light. On the page, their paintings were frequently vignetted in darkness, almost as if the artists still felt shadowed by the lingering specter of war. These books, dismissively looked down upon by librarians, were nonetheless immediately, snot-flyingly popular, with orders mounting into the millions of copies. Such publishing numbers were astonishing then (and are even more astonishing now, when 15,000 is considered a gee-whiz success). </p>
<p>Packaged for publishers Richard Simon and Max Schuster and their vice president Albert Leventhal, the entire series was written, drawn, edited, and printed by Western Publishing. A second wave of refugee artists signed on to their roster in 1948, this time escaping Southern California and Walt Disney’s anti-union practices. Among them were John Parr Miller (designer of Dumbo and Geppetto) and the writer-artist team of Alice and Martin Provensen.</p>
<div><p>Scarry wasn’t escaping anything, but he was hired amid this group to do up a four-page promotional brochure for Golden’s push into supermarkets. Impressed by his speedy, quality work, Western followed with a four-book illustration contract, which Scarry flexingly went on to exceed, producing not four, but <em>six </em>books before the one-year deal ran out. (Busy, busy world, indeed.) Wowed, Western signed him on for more, and the Scarrys moved to Connecticut, eventually ending up in Westport, where they went skeet shooting and boating, befriended other Golden Books artists who lived in the area (including Miller and the Provensens), and attended many parties. Then, in 1951, Scarry published his first solo writing and drawing book, <em>The Great Big Car and Truck Book</em>. </p></div>
<p><em><span>the great big car and truck book</span> </em>is, in some ways, the seed-germ of <em>Cars and Trucks and Things That Go</em>. But the differences are revealing. <em>The Great Big Car and Truck Book </em>still follows the house style of Western/Little Golden Books—careful watercolor-and-gouache illustrations that illuminate a text that is about the here and now of what people do all day. Moreover, the people driving the cars and trucks aren’t finely haired rabbits, bears, and pigs, but pink-skinned 1950s human suburbanites. The effect is curious, and, given our now-idea of Scarry, very un-Scarry-like. Like Charles Schulz’s early experiments drawing actual adults in <em>Peanuts </em>(the effect of which is psychedelic), Scarry’s humans feel just, like, <em>wrong</em>. There’s also little to distinguish his work here from other magazine illustrators of the day: the people, while charming enough, are too human to empathize with—oddly blank and impersonal, and advertising-art-like. Scarry had almost always preferred to draw animal stories—<em>Good Night Little Bear, The Bunny Book</em>—in books by his wife or other writers such as his good friend, Danish emigre Ole Risom. But it would be several years before he would do the same in his own books, substituting animals for humans as well as dropping the more labored visual approach for what we now recognize as his mature work.</p>
</div>
                                                                                                                                                                                                                                                <p>
    In Busytown there’s just enough innocent mayhem and tripping and falling to hint at a darker side of things, like failing 1970s marriages and the things on television news that adults were always yelling about. 
    </p>
                                                                                                                                                <div>
    <p>There were, of course, obstacles. One of the less appealing features of Golden’s business practice was that, with rare exceptions, they offered no royalties. This arrangement nagged at Scarry, especially after his and Patricia’s son Huck was born in 1953, so in 1955 he finally asked the imposing white-haired and lavender-blue-eyed Lucille Ogle for a revised contract that included royalties—and an advance. She readily agreed. Surprised, Scarry asked why she hadn’t offered such a deal earlier. “Because you never asked,” she replied.</p>
<p>With this new contract, though, Golden started to send Scarry less work, and he began to wonder if he was being deliberately snubbed because of his higher pay rate. So even as he was producing beautifully wrought paintings for Golden Books, he sought and then secured additional paying work from Western’s competitor Doubleday. He also created some loose storybook proposals of his own authorship in a surprisingly free and zippy pencil style. Freed from the precision of painting, the linework of these sketches—dare I say, cartoons?—came alive on the page like nothing he’d drawn before. I don’t know if the roughs he produced for his efforts were all so lively, but these certainly showed a new direction, if he decided to take it. But when Scarry shopped the proposals around, Golden and every other publisher he approached turned him down, so he shelved them. </p>
</div>
                                                                                                                                                                                                                                <figure>
    <img src="https://d181q449nqu6en.cloudfront.net/content/craft/articles/_850xAUTO_crop_center-center_none/Great-Big-Car-Truck-Book-Frontispiece.jpg?mtime=20240908124019&amp;focal=none&amp;tmtime=20240909060114" alt="" loading="lazy">
    <figcaption><p>Richard Scarry’s <em>The Great Big Car and Truck Book, </em>which was published in 1951, is in some ways the seed-germ of <em>Cars and Trucks and Things That Go. </em><span>Courtesy the Estate of Richard Scarry</span></p></figcaption>
</figure>
                                                                                                                                                                <div>
    <p>In the years that followed, Scarry continued to do exceptional work in Golden’s illuminated-painting mode, most notably his 1963 book <em>I Am a Bunny, </em>with text by his friend Risom. I never read it as a child, but I can now attest to its elegant, quiet beauty, because it was my daughter’s first word book ever, and I read it to her several hundred times. I never tired of its pictures or its words, the simple zen-like magic it evokes of the inevitability of the passing seasons always somehow putting the reader in a pleasant passenger-seat view. <em>I Am a Bunny </em>stands as one of the true tranquil masterpieces of children’s book art.</p>
<p>Even as he was working on <em>I Am a Bunny, </em>Scarry was preparing a proposal for a new kind of word book done in that free pencil style, which he called <em>Best Word Book Ever</em>. This time around, Golden accepted the proposal, and when the book was published, the fully realized Scarry World exploded into view: clean, pencil-line, doodle-like drawings that seemed lively and alive, if not to live on the very page itself. <em>Richard Scarry’s Best Word Book Ever </em>was an immediate hit, becoming one of the bestselling children’s books in history, selling seven million copies by 1975 (including the one my grandmother bought). And though Scarry was only getting a royalty of eight cents a copy (a royalty that, due to the contract he signed, did not rise in value as the price of books went up), the incredible number of copies the book sold meant that money finally started rolling in for the Scarrys.</p>
<p>Scarry produced two more books for Golden in the <em>Best Word Book Ever </em>mold: <em>Busy, Busy World </em>(1965) and <em>Storybook Dictionary </em>(1967)<em>, </em>the latter of which further plied the icono-lexicographical approach of images-as-words that had made <em>Best Word Book Ever </em>so compelling—the little pictures practically ask the reader to pluck them off the page and play with them. At that point, Random House swooped in and bought Scarry’s next book, <em>What Do People Do All Day?</em>, and became his publisher from then on. (Interestingly, after a 1990s bankruptcy, Random House also became the publisher of the entire Little Golden Books catalogue.)</p>
<p>Scarry followed <em>What Do People Do All Day? </em>with a series of books all set within the same society, including (among others) <em>Great Big Schoolhouse</em>, <em>Cars and Trucks and Things That Go, </em>and <em>Busiest People Ever! </em>The Busytown books, as they came to be known—with their dictionary-like visual presentation paired with lightly slapstick situations and the presence of recurring, memorable characters like Huckle Cat, the Pig family, and my favorite, Lowly Worm—grew into a real-feeling big world that Scarry seemed to be letting little ones into. (Lowly was perhaps the first children’s book animal character with a real nod to the ADA and the myth of “dis”-ability, and cheerfully makes his linear form work in all sorts of inspiring and disarmingly moving ways.)</p>
<p>Scarry’s guides to life both reflected and bolstered kids’ lived experience and in some cases, like my own, even provided the template for it. And while often sweet and quiet in its depiction of a picture-perfect society functioning measuredly—was Busytown urban or suburban or . . . European? (Where did all those Tudor homes and corner groceries come from, anyway?)—there’s just enough innocent mayhem and tripping and falling to hint at a darker side of things, like failing 1970s marriages and the things on television news that adults were always yelling about. </p>
<p>The busiest Busytown book is <em>Cars and Trucks and Things That Go</em>. As fascinated by the industrial world as any serious truck-spotting four-year-old, Scarry captures the ballet of traffic in a sort of frozen mimesis that’s reanimated by the act of reading and page-turning itself. Every aspect of life, however flimsily related to internal combustion travel, seems herein represented: whipped-cream delivery vans, mobile libraries, jet-fuel trucks, bookshelf-maker’s cars, ant buses, two-seater crayon cars, ambulances—the lot. There’s a simple, child-sized joy in recognizing the same characters driving by again and again in animal- and vegetable- and fruit-shaped cars while being dwarfed by accurately rendered bulldozers, heavy cranes, and thundering trucks, all traveling, page by page, left to right in the direction of the book—and the left to right of reading itself—through towns and construction sites and beaches and snow, ultimately ending in a calamitous (safe!) crash and a skidding of little cars spinning leftwards and finally stopping in front of (what else?) “Home.” Adding to the delight, throughout the book a tiny character named Gold Bug (who is literally a gold bug) hides on nearly every page, giving the engaged child a chance to find him over and over again in an exercise that would today be called “interactive” but we used to just call “looking.”</p>
<div><p>The Busytown books were enormous successes in America. But Scarry wrote and drew them in Switzerland, where he decided to move in 1967 after a three-week ski vacation with his son. What seems to have been an impulsive decision starts to makes sense if you’ve spent a few days immersed in Scarry’s work writing an essay for <em>The Yale Review</em>: a decidedly un-American tone runs through much of it. By “un-American” I don’t mean anti-American. Instead, I mean there’s a top-down, citizen-as-responsible-contributor, sense-of-oneself-as-part-of-something-bigger that feels, well, civilized. Even as a kid, I noticed that something about <em>Best Word Book Ever </em>felt odd, and I decided that Scarry was a balding Englishman, tweedy with possible pipe and maybe even one of those mountaineering hats with a feather in it. He was not any of those things. But the more one looks at his work, the more one sees how the European daily grocery trip, the walk to a nearby shop or tradesman’s guild, the tiny apple car fit for a worm are not part of the blowout-all-in-for-oneself-oil-fueled-free-for-all toward which America was barreling in the late 1960s. (Except, perhaps, in <em>Cars and Trucks and Things That Go — </em>though Europe has traffic, too.) So it’s perhaps unsurprising that Scarry spent the rest of his life first in Lausanne and then Gstaad, in a lovely chalet, hardly looking back while America slowly ground itself to pieces.</p></div>
<p><span>scarry continued</span> to produce books for another two decades, all of them featuring animals in place of humans. This actually caused a mild panic at Random House when <em>What Do People Do All Day? </em>was being published, with the staff asking: Shouldn’t it be called <em>What Do Animals Do All Day? </em>The dispute was short-lived since the answer (“No!”) was so obvious, but it hints at something important about the narrative energy on which Scarry’s engine runs. In children’s books, animals are frequently introduced as the first vessels to receive the natural empathy with which children are born. See: Golden’s own <em>Baby Farm Animals </em>(pictures by Garth Williams), <em>The Lively Little Rabbit </em>(pictures by Gustaf Tenggren), <em>The Animals of Farmer Jones </em>(pictures by Scarry), and about ten thousand other children’s books (pictures by everyone else). The natural inclination to ask “do animals feel the same things we do?” is confirmed with a smile and a tuck-in, what in literary terms is cumbersomely called “anthropomorphization” but in everyday words is just “caring.”</p>
<p>As we grow up, though, the truth will out: Mrs. Cow makes a good burger, those chicken fingers were Miss Clucky, and don’t forget to check the trap to see if we caught Mr. Mouse. This all then slides into discovering that not everyone is as nice as they seem, and it’s good to protect oneself on the playground; before long, one can even end up in ROTC, heading into basic training and rolling away in a tank. Fold in the especially twenty-first-century phenomenon of the “first-person shooter” in kids’ video games—surely the most telling perversion of literary terminology America could have hoped for to permanently indict itself—and children’s literature, to say nothing of the idea of polite civilization, is easily relegated to the category of the hopelessly naïve.</p>
</div>
                                                                                                                                                                                                                                                <p>
    As fascinated by the industrial world as any serious truck-spotting four-year-old, Scarry captures the ballet of traffic in a sort of frozen mimesis that’s reanimated by the act of reading and page-turning itself.
    </p>
                                                                                                                                                <div>
    <p>Scarry studiously avoided granting cows and chickens driver’s licenses. But the pigs? Where’s the bacon in Kenny’s breakfast coming from? So his representation of animal society indeed raises some odd questions, but rarely seems to have bothered readers (with the exception of those literal-minded Random House editors). By contrast, when Art Spiegelman’s graphic novel <em>Maus: A Survivor’s Tale </em>received the Pulitzer Prize in 1992, it provoked vociferous criticism from people offended by its depiction of its characters as animals. Jews were mice (picking up on Hitler’s calling Jews “vermin” or “rats”), Germans were cats, Americans were dogs, (Christian) Poles were pigs, and the French were (of course) frogs. Not surprisingly, many readers also objected to Spiegelman choosing the comic book—a form associated with children’s literature—to tell the story of his mother’s suicide and his father’s nightmarish time in Auschwitz. </p>
<div><p>Such criticisms entirely missed the point of Spiegelman’s work. Originally written as a short comic strip story for Terry Zwigoff’s 1972 underground comic book <em>Funny Animals</em>—which Zwigoff created to benefit animal rights organizations after visiting a slaughterhouse—the strip was later expanded by Spiegelman into a novel-length work. The book turns on the idea of ideas corrupted to the level of Idea: reducing humanity to something to be exterminated by exterminating the ability to see the human being before your very eyes. Had Spiegelman drawn “actual” people, the reader would no longer be complicit in the psychological deformation of the Holocaust itself. Biographically nonfiction in its text yet fiction in its pictures, the book works an ingenious, tortuous turnaround in the mind—and the eye—of the reader. It could not have been told just in words.</p></div>
<p><span>richard scarry’s work</span> could not have been told just in words, either. As Walter Retan and Ole Risom argue, Scarry “didn’t write his stories; he drew them.” His bestselling book was not titled <em>Best Picture Book Ever, </em>even though that’s really what it is. As children, we see the world in all its detail, texture, and beauty, but when we learn the word for, say, a bird, we cease to see it as clearly or curiously as we did before we categorized and dismissed it. John Updike eloquently and beautifully captures this confounding contradiction in his short story “Pigeon Feathers,” where the main character only notices the iridescent, divine beauty in a pigeon’s plumage after he’s shot several of them to pieces in the rafters of a barn. Like it or not, just as adulthood runs roughshod over childhood, words chew images to shreds, and it’s up to the artist—or the writer or the cartoonist—to put those images back together again. Pictures are our first language for understanding the world, but that doesn’t mean they should be ignored in favor of a second. Or, as Dave Eggers once kindly put it, cartoonists (and I include Scarry in this group) needn’t be punished for having two skills instead of one. </p>
<p>Scarry drove headlong into a picture-world that he illustrated with words, a world which blossomed into life in a way that his earlier books for Golden, in which his pictures illustrated words, simply couldn’t. He kept in touch with his child self so well that, as both his biographers and other writers have highlighted, he didn’t test his books on children, because he had “remained very childlike himself.” And he knew exactly where the child inside him still lived: his kind heart.</p>
</div>
                                                                                                                                                            
                                <div>
                    <p><a href="https://yalereview.org/author/chris-ware">Chris Ware</a> is an artist, writer, and regular contributor to <em>The New Yorker</em>; his book <a href="https://bookshop.org/a/16648/9780375404535"><em>Jimmy Corrigan: The Smartest Kid on Earth</em></a> was awarded the Guardian Prize and <a href="https://bookshop.org/a/16648/9780375424335"><em>Building Stories</em></a> was chosen as a Top Ten Fiction Book by <em>The New York Times.</em> A traveling retrospective of his work began at the Centre Pompidou in 2022 and will conclude at the Centre de Cultura Contemporània in Barcelona in Spring 2025.
                            </p>
                    </div>

                            


    

    
    

            
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Shopify Is Winning Salesforce Clients, Stoking E-Commerce Rivalry (111 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2024-10-29/shopify-is-winning-salesforce-clients-stoking-e-commerce-rivalry</link>
            <guid>41983607</guid>
            <pubDate>Tue, 29 Oct 2024 13:23:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2024-10-29/shopify-is-winning-salesforce-clients-stoking-e-commerce-rivalry">https://www.bloomberg.com/news/articles/2024-10-29/shopify-is-winning-salesforce-clients-stoking-e-commerce-rivalry</a>, See on <a href="https://news.ycombinator.com/item?id=41983607">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <section>
        <h3>Why did this happen?</h3>
        <p>Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our <a href="https://www.bloomberg.com/notices/tos">Terms of
                Service</a> and <a href="https://www.bloomberg.com/notices/tos">Cookie Policy</a>.</p>
    </section>
    <section>
        <h3>Need Help?</h3>
        <p>For inquiries related to this message please <a href="https://www.bloomberg.com/feedback">contact
            our support team</a> and provide the reference ID below.</p>
        <p>Block reference ID:</p>
    </section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Launch HN: Integuru (YC W24) – Reverse-engineer internal APIs using LLMs (115 pts)]]></title>
            <link>https://github.com/Integuru-AI/Integuru</link>
            <guid>41983409</guid>
            <pubDate>Tue, 29 Oct 2024 13:00:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Integuru-AI/Integuru">https://github.com/Integuru-AI/Integuru</a>, See on <a href="https://news.ycombinator.com/item?id=41983409">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Integuru</h2><a id="user-content-integuru" aria-label="Permalink: Integuru" href="#integuru"></a></p>
<p dir="auto">An AI agent that generates integration code by reverse-engineering platforms' internal APIs.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Integuru in Action</h2><a id="user-content-integuru-in-action" aria-label="Permalink: Integuru in Action" href="#integuru-in-action"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Integuru-AI/Integuru/blob/main/integuru_demo.gif"><img src="https://github.com/Integuru-AI/Integuru/raw/main/integuru_demo.gif" alt="Integuru in action" data-animated-image=""></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">What Integuru Does</h2><a id="user-content-what-integuru-does" aria-label="Permalink: What Integuru Does" href="#what-integuru-does"></a></p>
<p dir="auto">You use <code>create_har.py</code> to generate a file containing all browser network requests, a file with the cookies, and write a prompt describing the action triggered in the browser. The agent outputs runnable Python code that hits the platform's internal endpoints to perform the desired action.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How It Works</h2><a id="user-content-how-it-works" aria-label="Permalink: How It Works" href="#how-it-works"></a></p>
<p dir="auto">Let's assume we want to download utility bills:</p>
<ol dir="auto">
<li>The agent identifies the request that downloads the utility bills.
For example, the request URL might look like this:
<div data-snippet-clipboard-copy-content="https://www.example.com/utility-bills?accountId=123&amp;userId=456"><pre><code>https://www.example.com/utility-bills?accountId=123&amp;userId=456
</code></pre></div>
</li>
<li>It identifies parts of the request that depend on other requests.
The above URL contains dynamic parts (accountId and userId) that need to be obtained from other requests.

</li>
<li>It finds the requests that provide these parts and makes the download request dependent on them.
<div data-snippet-clipboard-copy-content="GET https://www.example.com/get_account_id
GET https://www.example.com/get_user_id"><pre><code>GET https://www.example.com/get_account_id
GET https://www.example.com/get_user_id
</code></pre></div>
</li>
<li>This process repeats until the request being checked depends on no other request and only requires the authentication cookies.</li>
<li>The agent traverses up the graph, starting from nodes (requests) with no outgoing edges until it reaches the master node while converting each node to a runnable function.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Generate a dependency graph of requests to make the final request that performs the desired action.</li>
<li>Allow input variables (for example, choosing the YEAR to download a document from). This is currently only supported for graph generation. Input variables for code generation coming soon!</li>
<li>Generate code to hit all requests in the graph to perform the desired action.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Setup</h2><a id="user-content-setup" aria-label="Permalink: Setup" href="#setup"></a></p>
<ol dir="auto">
<li>Set up your OpenAI <a href="https://platform.openai.com/account/api-keys" rel="nofollow">API Keys</a> and add the <code>OPENAI_API_KEY</code> environment variable. (We recommend using models that are at least as capable as OpenAI o1-mini. Models on par with OpenAI o1-preview are ideal.)</li>
<li>Install Python requirements via poetry:

</li>
<li>Open a poetry shell:

</li>
<li>Run the following command to spawn a browser:
<div data-snippet-clipboard-copy-content="poetry run python create_har.py"><pre><code>poetry run python create_har.py
</code></pre></div>
Log into your platform and perform the desired action (such as downloading a utility bill).</li>
<li>Run Integuru:
<div data-snippet-clipboard-copy-content="poetry run python -m integuru --prompt &quot;download utility bills&quot;"><pre><code>poetry run python -m integuru --prompt "download utility bills"
</code></pre></div>
You can also run it via Jupyter Notebook <code>main.ipynb</code></li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">After setting up the project, you can use Integuru to analyze and reverse-engineer API requests for external platforms. Simply provide the appropriate .har file and a prompt describing the action that you want to trigger.</p>
<div data-snippet-clipboard-copy-content="poetry run python -m integuru --help
Usage: python -m integuru [OPTIONS]

Options:
  --model TEXT                    The LLM model to use (default is gpt-4o)
  --prompt TEXT                   The prompt for the model  [required]
  --har-path TEXT                 The HAR file path (default is
                                  ./network_requests.har)
  --cookie-path TEXT              The cookie file path (default is
                                  ./cookies.json)
  --max_steps INTEGER             The max_steps (default is 20)
  --input_variables <TEXT TEXT>...
                                  Input variables in the format key value
  --generate-code                 Whether to generate the full integration
                                  code
  --help                          Show this message and exit."><pre><code>poetry run python -m integuru --help
Usage: python -m integuru [OPTIONS]

Options:
  --model TEXT                    The LLM model to use (default is gpt-4o)
  --prompt TEXT                   The prompt for the model  [required]
  --har-path TEXT                 The HAR file path (default is
                                  ./network_requests.har)
  --cookie-path TEXT              The cookie file path (default is
                                  ./cookies.json)
  --max_steps INTEGER             The max_steps (default is 20)
  --input_variables &lt;TEXT TEXT&gt;...
                                  Input variables in the format key value
  --generate-code                 Whether to generate the full integration
                                  code
  --help                          Show this message and exit.
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Demo</h2><a id="user-content-demo" aria-label="Permalink: Demo" href="#demo"></a></p>
<p dir="auto"><a href="https://www.youtube.com/watch?v=7OJ4w5BCpQ0" rel="nofollow"><img src="https://camo.githubusercontent.com/5359393c728be0d7bba59bdb2927a90484c1f5d30e64f0ff374ca849f9a7bedc/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f374f4a34773542437051302f302e6a7067" alt="Demo Video" data-canonical-src="https://img.youtube.com/vi/7OJ4w5BCpQ0/0.jpg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Contributions to improve Integuru are welcome. Please feel free to submit issues or pull requests on the project's repository.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Info</h2><a id="user-content-info" aria-label="Permalink: Info" href="#info"></a></p>
<p dir="auto">Integuru is built by Integuru.ai. Besides our work on the agent, we take custom requests for new integrations or additional features for existing supported platforms. We also offer hosting and authentication services. If you have requests or want to work with us, reach out at <a href="mailto:richard@taiki.online">richard@taiki.online</a>.</p>
<p dir="auto">We open-source unofficial APIs that we've built already. You can find them <a href="https://github.com/Integuru-AI/APIs-by-Integuru">here</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to get the whole planet to send abuse complaints to your best friends (276 pts)]]></title>
            <link>https://delroth.net/posts/spoofed-mass-scan-abuse/</link>
            <guid>41982698</guid>
            <pubDate>Tue, 29 Oct 2024 11:17:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://delroth.net/posts/spoofed-mass-scan-abuse/">https://delroth.net/posts/spoofed-mass-scan-abuse/</a>, See on <a href="https://news.ycombinator.com/item?id=41982698">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-start"><div><p><time datetime="2024-10-29T10:00:00+0100">October 29, 2024
</time><span>- </span><span><span>11 mins read</span></span></p></div><p>It all begins with one scary email late at night just before I had to go to
sleep:</p><div><pre tabindex="0"><code>From: abuse@hetzner.com
Date: 2024-10-29 01:03:00 CET
Subject: AbuseInfo: Potential Security issue: AS24940: 195.201.9.37

We have received an abuse report from abuse@watchdogcyberdefense.com for your
IP address 195.201.9.37.

We are automatically forwarding this report on to you, for your information.
You do not need to respond, but we do expect you to check it and to resolve any
potential issues.

&gt; To assist you in understanding the situation, we have provided the relevant
&gt; log data below, with timestamps adjusted to our GMT +8 timezone:
&gt;
&gt;                 DateTime   Action      AttackClass      SourceIP Srcport Protocol   DestinationIP DestPort
&gt; 0   28-Oct-2024 19:39:11   DENIED                   195.201.9.37   36163      TCP  202.91.162.233       22
&gt; &lt;snip&gt;
&gt; 20  28-Oct-2024 20:36:33   DENIED                   195.201.9.37   22044      TCP   202.91.161.97       22
&gt; 21  28-Oct-2024 20:41:37   DENIED                   195.201.9.37    9305      TCP   202.91.163.36       22
&gt; 22  28-Oct-2024 20:50:33   DENIED                   195.201.9.37   39588      TCP  202.91.163.199       22
&gt; 23  28-Oct-2024 20:50:58   DENIED                   195.201.9.37   62973      TCP   202.91.161.41       22
&gt; 24  28-Oct-2024 20:51:50   DENIED                   195.201.9.37    3085      TCP   202.91.161.97       22
</code></pre></div><p>At first glance, this sounds pretty bad. One of my servers suddenly deciding to
start sending SSH connections to the wider internet. This is usually a pretty
strong indicator of malware compromise, and I had to act quickly if that was
the case. Luckily, I’ve worked in infosec for a while, and some years ago I
even did some freelance work doing forensics and cleanup of infected servers.</p><p>So, not completely out of my element, I was surprised when after an hour or two
I found no evidence of anything happening out of the ordinary. It’s always hard
to prove a negative, but really, the machine was fine. No odd process, no
filesystem modifications, no odd network traffic (as observed by the
hypervisor, not by the server itself which happens to be a VM - just to be
extra sure!). If it was a malware compromise incident, the malware would have
been pretty stealthy, and that runs against the idea of it having been
commanded to scan the internet - in general, a very loud and noticeable action.</p><p>I turned to the regularly running services on the machine. This is my main
datacenter-hosted server, and I run a bunch of distributed or federated
services on there:</p><ul><li>Syncthing relay.</li><li>Mastodon instance.</li><li>Tor relay (not exit, internal node only).</li><li>Matrix homeserver.</li></ul><p>After close inspection, the Tor relay does connect to a few other relays that
are hosted on port 22, but that’s a very limited set of IPs, and it doesn’t
include anything in the network that sent my ISP the abuse complaint. Unlikely
candidate. I thought maybe Matrix or Mastodon could be abused to send commanded
requests to arbitrary IP:port destinations, but logging for both indicated
nothing of the sort was (visibly) happening. The Sidekiq queue for my Mastodon
instance was also absent of any trace of this, when I’d have expected to see
e.g. retries queued if it was involved.</p><p>What was happening there? Was the abuse complaint just bogus?</p><h2 id="the-smoking-gun">The smoking gun</h2><p>Then, I noticed something in one of my <code>tcpdump</code> that was still running to
monitor traffic involving port 22 on that server. I had originally ran
<code>tcpdump</code> filtering on <code>dst port 22</code>, since this is what would show traffic
originating from my server going to remote destinations. However, for some
reason, I dropped that filter at some point, instead filtering <code>not src host 195.201.9.37</code> instead (my server’s IP). This is when this showed up:</p><div><pre tabindex="0"><code>04:14:25.286063 IP 45.187.212.68.22 &gt; 195.201.9.37.59639: Flags [R.], seq 0, ack 41396686, win 0, length 0
04:14:25.291455 IP 107.152.7.33.22 &gt; 195.201.9.37.39793: Flags [R.], seq 0, ack 1391844539, win 0, length 0
04:14:25.322255 IP 107.91.78.158.22 &gt; 195.201.9.37.48900: Flags [R.], seq 0, ack 1434896088, win 65535, length 0
</code></pre></div><p>Something <em>was</em> in fact going on. But not at all what I was expecting. Turns
out: no connections were coming out of my server and going to the port 22 of
random machines. But some random internet machines <em>were</em> in fact sending me
TCP reset packets.</p><p>If you’ve been around networking/infosec communities for a while, you might now
be screaming: backscatter! Source IP spoofing! And yeah, this was my first
thought too. Let’s do a quick aside to go into what those things mean.</p><h2 id="ip-spoofing-on-the-internet">IP spoofing on the internet</h2><p>Turns out, it’s pretty trivial to send packets to various destinations on the
Internet with a fake source IP address (of course, the destination IP needs to
be correct, since it determines… the destination). Many ISPs adhere to the
<a href="https://www.rfc-editor.org/info/bcp38" target="_blank">Best Current Practice (BCP) 38</a>, which
can be summarized by the following: “if you peer with a network, you should
only allow them to send IP packets using IP address you expect from them”.
Unfortunately, that filtering can often only be done early on in a packet’s
route to its destination. Once the packet gets to a large transit provider,
their peers expect that provider to carry traffic from the whole internet to
them, and thus are not able to do any meaningful filtering.</p><p>Which means, if you just find one transit provider which doesn’t do BCP38
filtering… you can send IP packets tagged with any source IP you want! And
unfortunately, even though the origins of BCP38 date back to 1998… there are
still network providers 25 years later that don’t implement it. APNIC has <a href="https://blog.apnic.net/2023/05/03/why-is-source-address-validation-still-a-problem/" target="_blank">a
great article from last year on the subject</a>.</p><p>The consequences in practice shouldn’t be too bad. TCP, QUIC, and generally
anything using (d)TLS requires roundtrips, which can’t happen when a source IP
is spoofed. Spoofing the source IP means that you get to send a “wrong”
packets, but the replies to that packet still get sent to the source IP you
spoofed, the spoofer doesn’t get to see them and process them. There are a few
well known abuse vectors that rely on spoofing, such as reflection DDoS, but
it’s not usually a concern.</p><p>Unless…</p><h2 id="guessing-the-motive">Guessing the motive</h2><p>Let’s come back to my <code>RST</code> packets. The main hypothesis is that someone is
using my source IP to send outbound connections to the port 22 of various
internet machines. But it doesn’t really make logical sense at a first glance.
Usually, people would do this to scan for open ports or servers with a working
SSH server. However, none of that works when you spoof a source IP, since you
don’t get to see the results of you probing!</p><p>Back in the earlier days of the internet, there used to be a technique called
“Idle Scanning”, which relied on 1. servers being way less busy than today; 2.
network stacks lacking randomization of some fields and using auto-incrementing
predictable counters. This could be used to probe whether a port is open while
spofing a source IP (for anonymity, or to bypass firewalls). But that technique
has been dead and unusable for decades.</p><p>So, maybe someone set up a scanner and typo’d their source IP in a
configuration file, causing random internet machines to think I’m initiating
connections to them? But… the traffic volume seems too low, the duration of
the weirdness too long, and really it would be a stretch anyway.</p><p>Whatever the spoofer’s motive, it’s kind of annoying. Their scan is hitting
honeypots, networks with intrusion detection systems that send (sometimes
automated) abuse complaints, and so on. I wish they’d notice that whatever
they’re doing isn’t working, because I don’t particularly enjoy getting abuse
complaints, and they put me at risk of being kicked out of my hosting provider.</p><p>… wait a minute?!</p><h2 id="the-tor-connection">The Tor connection</h2><p>I mentioned in passing earlier that one of the services I run on my server is a
Tor relay. Relays are internal nodes of the Tor network. They only carry
anonymous, encrypted traffic (in fact, usually with multiple layers of
encryption), and only between consenting opt-in nodes of the Tor network.
Relays aren’t exit nodes, they don’t talk to the open internet. A few selected
relays are also “Guard Nodes”, which can serve as the entry point to the Tor
network. These technically talk to the open internet, but still, only
consenting users connecting to the Tor network.</p><p>For that reason I originally kind of ruled out Tor having any connection (pun
intended) to this abuse issue. And I’m sure some of you were screaming about
it, but hey, you probably have the benefit of 1. hindsight; 2. not being up at
4AM running <code>tcpdump</code>.</p><p>But Tor has one peculiarity: there are actors on the internet that don’t like
it. There are many good and bad reasons for this - I personally view Tor as a
“useful neutral cesspool”, but this is not an article about ethics, and it’s
simple enough to say that some people disagree. Said people range from
“individual hacktivists” to “police forces” to “government agencies”, with
various levels of sophistications and differing techniques.</p><p>Could someone be deliberately trying to induce abuse complaints on Tor network
participants to take down parts of the network (or disincetivize running
internal nodes, which are key for the network’s health)?</p><p>Easy enough to check. I run more relay nodes, so let’s just <code>tcpdump</code> there
too. One at home on my residential IP connection, one on a Linode VPS in Japan:</p><div><pre tabindex="0"><code>04:19:14.705034 IP 198.30.233.69.22 &gt; 172.105.199.155.39998: Flags [R.], seq 0, ack 171173954, win 0, length 0
04:20:15.135733 IP 124.198.33.196.22 &gt; 172.105.199.155.23506: Flags [R.], seq 0, ack 1985822135, win 0, length 0
04:21:30.222739 IP 223.29.149.158.22 &gt; 172.105.199.155.27507: Flags [R.], seq 0, ack 3614869158, win 0, length 0

04:12:39.470366 IP 121.150.242.252.22 &gt; 77.109.152.87.57627: Flags [R.], seq 0, ack 2452733863, win 0, length 0
04:13:05.549920 IP 46.188.201.102.22 &gt; 77.109.152.87.9999: Flags [R.], seq 0, ack 3253922544, win 0, length 0
04:14:33.027326 IP 1.1.195.62.22 &gt; 77.109.152.87.52448: Flags [R.], seq 0, ack 351972505, win 0, length 0
</code></pre></div><p>Annnnnd yep, my two other relays running in completely different countries and
with completely different ISPs are seeing the same spoofed TCP SYN pattern.</p><p>This is when I sent <a href="https://lists.torproject.org/pipermail/tor-relays/2024-October/021953.html" target="_blank">an email to the <code>tor-relays</code> mailing list</a>,
where… it turns out someone had <a href="https://gitlab.torproject.org/tpo/network-health/analysis/-/issues/85" target="_blank">noticed and diagnosed the same thing</a>
a few days before. This spoofing “attack” actually started on other types of
nodes before migrating to relays, and those other nodes were hit with a much
larger volume of spoofed connections, leading to them actually getting
temporarily taken down in some cases! Proving the attack does in fact work…</p><h2 id="you-could-be-the-target-too">You could be the target too!</h2><p>To recap what’s (probably) going on:</p><ol><li>A malicious attacker has access to a network without BCP38 filtering.</li><li>They send TCP connection requests to port 22 on many random internet
machines - possibly deliberately selecting known honeypots or networks known
to send automated abuse complaints.</li><li>Those TCP connection requests use a spoofed source IP address, making the
destination machines think the spoofed source sent that connection. They
become the target of the automated abuse complaints.</li><li>With a large enough volume, the spoofed IP quickly becomes widely
blacklisted from many internet entities following blocklists, and the
hosting provider might take action due to many abuse reports and shut down
the server for being compromised / malicious.</li></ol><p>There is nothing at all in this attack that’s specific to Tor! I’m actually
surprised this is the first time I hear of this, because while ingenious,
nothing in there seems particularly difficult to do for a single motivated
attacker. You, too, can probably make your friend’s hosting provider (with
their consent, of course) shut down their server and cancel their hosting
contract by getting them flooded with well-meaning but confused abuse
complaints.</p><h2 id="conclusion">Conclusion</h2><p>The internet was broken 25 years ago and is still broken 25 years later.
Spoofed source IP addresses should not still be a problem in 2024, but the
larger internet community seems completely unwilling to enforce any kind of
rules or baseline security that would make the internet safer for everyone.
This is not just BCP38 - RPKI is a similar disaster in terms of deployment, and
has only started ramping up because it impacts large internet companies who
started enforcing requirements on their direct peers.</p><p>It’s not clear to me what the next steps are in regards to this attack. It’s
clearly already in the wild. I don’t know if it was already known and
documented. But it still seems to be working, it’s hard to track (I don’t know
of any way one could figure out the real source of a spoofed IP packet - there
is no “after the fact” traceroute, and even if there was, it would have to be
done by some upstream provider to get useful info).</p><p>However, if you now get such an abuse complaint, you might now have a better
idea what to look for and what to reply to your hosting provider to try and
convince them you are in fact a victim and not a perpetrator! Who knows, they
might even care to listen.</p><hr><p><em>This article was written in a rush a few hours before getting on a plane.
Sorry for the lack of proof-reading and potential typos!</em></p><hr></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Insiders Stealing Instagram Usernames? (123 pts)]]></title>
            <link>https://javier.computer/instagram</link>
            <guid>41981289</guid>
            <pubDate>Tue, 29 Oct 2024 09:21:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://javier.computer/instagram">https://javier.computer/instagram</a>, See on <a href="https://news.ycombinator.com/item?id=41981289">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
      
      <p>Alright, let me share the full story of how my 14-year-old <a href="https://instagram.com/">Instagram</a> account, @javier, was stolen and ended up in the
hands of a hip-hop producer—and why I believe it was an inside job.</p>

<h3 id="background">Background</h3>

<p>First off, here’s some backstory: I’ve had the @javier username since October 2010, the same month the app launched. I
joined so early, that I even have emails reporting bugs to <a href="https://x.com/joshriedel">@joshriedel</a>, Instagram’s very first employee.</p>



<p>Over the past 14 years, I’ve received countless messages to buy my account. Early on, I made a conscious decision not to
engage with these requests. Rather than blocking people, I simply ignored them or restricted their accounts when that
feature became available.</p>

<p>A few years ago, I even created a quick iMovie compilation of some of the messages I received in 2018. Since then, the
volume of messages has only increased.</p>

<div>
  <video>
    <source data-src="https://img.javier.computer/instagram/username.mp4" type="video/mp4">
    Your browser does not support the video tag.
  </video>
  
  
</div>
<p>I’ve keep taking screenshots of everyone who’s asked about acquiring my account. One interesting pattern: the majority
of these requests come from profiles without any photos. I find it so weird that people are so eager to get a username
when they don’t even share content!</p>

<g>
<!-- photos / custom  -->
<div>
    <figure>
      <picture>
        <source data-srcset="https://img.javier.computer/instagram/0_photos_01_2880.webp 2880w" type="image/webp">
        <source data-srcset="https://img.javier.computer/instagram/0_photos_01_2880.jpg 2880w">
        <img data-src="https://img.javier.computer/instagram/0_photos_01_2880.jpg" src="https://img.javier.computer/instagram/0_photos_01_2880.jpg">
      </picture>
    </figure>
    
  </div>
<!-- photos / custom  -->
<div>
    <figure>
      <picture>
        <source data-srcset="https://img.javier.computer/instagram/0_photos_02_2880.webp 2880w" type="image/webp">
        <source data-srcset="https://img.javier.computer/instagram/0_photos_02_2880.jpg 2880w">
        <img data-src="https://img.javier.computer/instagram/0_photos_02_2880.jpg" src="https://img.javier.computer/instagram/0_photos_02_2880.jpg">
      </picture>
    </figure>
    
  </div>
</g>

<h3 id="the-theft">The Theft</h3>

<p>Alright, back to the story. On Wednesday, October 23rd, around 7 PM Spanish time, after I got killed for the thousandth
time in Elden Ring, I opened Instagram and found myself logged out.</p>

<p>While I’d occasionally received notifications about suspicious activity requiring identity verification, this was
different—a complete logout. When I saw this, I knew my worst fear had finally become reality: I had lost my Instagram
account.</p>

<p>When I attempted to log back in, my password was rejected. Using the password recovery option, I discovered something
alarming: the email associated with my account had been changed to an address I didn’t recognize: r*******e@gmail.com</p>

<p>At this point, my @javier account still displayed all my content, but I knew this wouldn’t last long. I tried
Instagram’s hacked account reporting page, but it was ineffective: when I entered my username the page simply reloaded
without any confirmation.</p>

<div>
  <video>
    <source data-src="https://img.javier.computer/instagram/login.mp4" type="video/mp4">
    Your browser does not support the video tag.
  </video>
  
  
</div>
<p>After exhausting all recovery methods, I refreshed my now-inaccessible account to find all my content replaced by 4
posts. Here’s my account before and after the robbery.</p>



<p>The hackers had moved my content to a new account, @javier.typeshi, stripped of my profile photo and description,
ironically adding the text “meta official”.</p>

<!-- photos / custom  -->
<div>
    <figure>
      <picture>
        <source data-srcset="https://img.javier.computer/instagram/javier_2880.webp 2880w" type="image/webp">
        <source data-srcset="https://img.javier.computer/instagram/javier_2880.jpg 2880w">
        <img data-src="https://img.javier.computer/instagram/javier_2880.jpg" src="https://img.javier.computer/instagram/javier_2880.jpg">
      </picture>
    </figure>
    
  </div>
<p>The profile picture in that account belongs to a man also named Javier Arce. He was a journalist that worked for
@azcentral <a href="https://eu.azcentral.com/story/news/local/phoenix/2024/09/06/javier-arce-la-voz-and-arizona-republic-journalist-dies-at-age-48/75106323007">who sadly passed away from stomach cancer last month</a>.</p>

<p>Before turning to Twitter for help (where I was flooded with recovery scams), I created a new account to request my
content back. They never responded. I knew the chances were pretty low, and that they would probably try to extort me,
but hey I had to ask.</p>

<h3 id="the-investigation">The Investigation</h3>

<p>Ok, before I move on and tell you about the new owner of the account, let’s do some conspirancy talk and let me explain
how I think my account was stolen.</p>

<p>My account was well-protected with 2FA and linked to a unique, private email address that I’d never shared. I use
1Password and am extremely careful about security. I’ve never entered my Instagram credentials on any third-party
websites or apps, nor have I ever shared 2FA codes.</p>

<p>On the day of the event, I received no email or phone notification about any unusual access attempts. So here’s what I
believe it happened: it wasn’t a phising attack, it was somebody from inside the company.</p>

<p>This wouldn’t be unprecedented. <a href="https://www.cshub.com/attacks/news/meta-fires-employees-for-allegedly-hacking-into-users-accounts">Meta has previously fired employees for hacking user accounts</a>, and there’s a thriving
black market for usernames on forums like <a href="https://oguser.com/">oguser.com</a></p>

<h3 id="the-new-owner">The New Owner</h3>

<p>Back to the story. Let me tell you about the new account owner. His name is Javier “Jay” Sang. Looking at the tagged
photos, he seems to be the founder and CEO of a label called Rebel Music.</p>

<!-- photos / custom  -->
<div>
    <figure>
      <picture>
        <source data-srcset="https://img.javier.computer/instagram/jay sang_2880.webp 2880w" type="image/webp">
        <source data-srcset="https://img.javier.computer/instagram/jay sang_2880.jpg 2880w">
        <img data-src="https://img.javier.computer/instagram/jay sang_2880.jpg">
      </picture>
    </figure>
    
  </div>
<p>And do you remember how I take a screenshot of every account that ask me to sell or change my username? Jay did that in April this year. It seems he may have sought alternative means after I didn’t respond to his request lol</p>

<!-- photos / custom  -->
<div>
    <figure>
      <picture>
        <source data-srcset="https://img.javier.computer/instagram/jayrebelmusic_2880.webp 2880w" type="image/webp">
        <source data-srcset="https://img.javier.computer/instagram/jayrebelmusic_2880.jpg 2880w">
        <img data-src="https://img.javier.computer/instagram/jayrebelmusic_2880.jpg" src="https://img.javier.computer/instagram/jayrebelmusic_2880.jpg">
      </picture>
    </figure>
    
  </div>
<h3 id="current-status">Current Status</h3>

<p>As for the current situation: so far, I’ve regained control of the renamed account with all my content and messages, but
I still haven’t recovered my original username.</p>

<p>I’m receiving help from an Instagram employee, and through my network of friends and their connections I’ve made contact
with other folks at the company who have expressed interest in my case and offered their assistance. I’m very grateful
for that.</p>

<p>In fact, I doubt I would have been able to recover my content without those connections at the company. This highlights a serious issue that Instagram and Meta need to address in their account recovery process.</p>

<p>And I’m not the only person with this problem. A few days ago, I learned that @joao, another long-time user, also lost his Instagram handle in a similar way last year, and he still hasn’t heard back from Instagram.</p>

<p>For reasons unknown to me, regaining access to my content was easy, but reclaiming my original username is much harder.
And while I’m optimistic about getting my old username back, I’m sure this won’t be the last hack attempt on my account.</p>

<p>The end. Thanks for reading this far! If you’re wondering how you can help, I’d appreciate it if you shared this thread.
And if you know <a href="https://x.com/mosseri">@mosseri</a>, please let him know one of Instagram’s earliest users is frustrated and thinking of leaving.</p>




      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What happens when people with acute psychosis meet the voices in their heads? (139 pts)]]></title>
            <link>https://www.theguardian.com/news/2024/oct/29/acute-psychosis-inner-voices-avatar-therapy-psychiatry</link>
            <guid>41980986</guid>
            <pubDate>Tue, 29 Oct 2024 08:43:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/news/2024/oct/29/acute-psychosis-inner-voices-avatar-therapy-psychiatry">https://www.theguardian.com/news/2024/oct/29/acute-psychosis-inner-voices-avatar-therapy-psychiatry</a>, See on <a href="https://news.ycombinator.com/item?id=41980986">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p><span>I</span>n the summer of 2019, when Joe was 21, he went on a university rugby tour of California. One night, one of his teammates bought some cannabis edibles to share, and Joe ate some. For the next 12 hours, he believed he was in hell. He was on fire; his body was suffused with pain. His ears were filled first with incoherent screaming and then with sinister whispering. Joe’s friends thought their teammate’s bad trip was funny, even as they wrestled him away from the windows when he tried to jump from the seventh floor of their hotel.</p><p>When he woke up the next morning, Joe was still in hell. A devilish, humanoid form lurking in the periphery of his vision was telling him he had died the previous night. A chorus of other voices joined in, wailing in agony. They were entirely real to him, even though he knew they couldn’t be. He had a rugby match to play, and 10 minutes in, he couldn’t see or feel his hands; he couldn’t move. His teammates laughed as he came off the pitch. Poor old Joe.</p><p>The voices came back to the UK with him. “You’re not real,” they told him incessantly. “You’re already dead, so it doesn’t matter if you end it all again.” He saw blurred, demonic faces smirking at him, sometimes at the edge of his eye line, sometimes up against his face, too close to be in focus.</p><figure id="75c14611-ab14-42df-aff4-7dc45fd0a0d4" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:3,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;Using avatars in psychosis therapy can help those who hear voices, study finds &quot;,&quot;elementId&quot;:&quot;75c14611-ab14-42df-aff4-7dc45fd0a0d4&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/science/2024/oct/28/using-avatars-in-psychosis-therapy-can-help-those-who-hear-voices-study-finds&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;design&quot;:10,&quot;display&quot;:1,&quot;theme&quot;:0}}"></gu-island></figure><p>His parents knew he had struggled with depression and anxiety before, but Joe didn’t want to tell anyone about the voices. He drank heavily, every blackout providing temporary respite. He would walk for hours, playing music on his headphones, desperate to drown out the voices. At other times Joe would tell the voices to fuck off, shut up, leave him alone. He would find himself saying these things out loud, in public. Seeing himself reflected in the fearful eyes of those he walked past, he was terrified that he would never find a way to be normal among them again.</p><p>Joe was later told he was experiencing acute psychosis. About <a href="https://www.derbyshirehealthcareft.nhs.uk/services/mental-health-and-emotional-wellbeing/early-intervention-psychosis" data-link-name="in body link">two or three people in every 100</a> experience psychosis, when reality is disrupted by delusions or hallucinations. It can be a symptom of schizophrenia or severe depression, but can also be experienced without any other mental health condition. The acute form – the sudden, rapid onset of auditory or visual hallucinations that Joe experienced – can be triggered by drugs in people who, <a href="https://journals.sagepub.com/doi/10.1177/0020764018801690" data-link-name="in body link">because of existing biological and social factors</a>, might be predisposed to psychosis. Hearing voices is <a href="https://www.sciencedirect.com/science/article/abs/pii/S0272735809001615?via%3Dihub" data-link-name="in body link">the most common form</a> of psychosis, affecting <a href="https://pubmed.ncbi.nlm.nih.gov/22446568/" data-link-name="in body link">as many as 70%</a> of people with schizophrenia, and the voices heard tend to be persecutory and distressing. More than one in 10 people with schizophrenia end up <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1845151/" data-link-name="in body link">taking their own lives</a>.</p><p>Antipsychotic medications, the go-to treatment since the mid-20th century, can come with serious side-effects, including weight gain, exhaustion, bed-wetting, sexual dysfunction and severe constipation. And they don’t work for everyone: a quarter of people on antipsychotics will continue to hear voices. The most effective medication, Clozapine, is only used where other antipsychotics have failed because it can cause even more severe side-effects. It was developed in the 1950s; there has been little drug innovation for psychosis in recent decades. There are also non-pharmacological treatments; cognitive behavioural therapy for psychosis (CBTp), when combined with medication, <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6280425/" data-link-name="in body link">improves symptoms</a> for about 50% of people.</p><p>After hearing the voices for two and a half years, Joe went to his GP in winter 2021 and received his formal diagnosis. He was put on a low dose of antipsychotic medication, which he hated: he couldn’t get out of bed, couldn’t function and, while it helped with his visual hallucinations, the voices remained. He came off the medication after two months. Depressed, despairing and starting to spiral, he got back in touch with his GP, who told him there was something else to try: an experimental therapy, a clinical trial he could be part of, that turned the traditional treatment model for psychosis on its head.</p><p>If you hear voices, clinicians don’t generally ask what they’re saying to you, beyond whether they are asking you to harm yourself or others. “There’s been a reluctance to engage much with the content of voices,” Ben Alderson-Day, an associate professor of psychology at Durham University who specialises in psychosis, told me. “That’s in part because of a concern that if you ask voice-hearers to elaborate, you might engage in ‘collusion’: you may make [the voices] more real for people.” A clinician may diagnose a patient with psychosis, and prescribe them medication or CBT, without knowing what the patient’s voices say to them.</p><p>This new therapy demanded that voices were listened to closely, and responded to as if they were spoken by entirely real external beings. Trial participants would create an avatar of their voice: a moving, three-dimensional digital embodiment that looks and sounds like the persecutor inside their heads. They would be guided by a therapist to have a dialogue with the voice – and the hope was, through doing so, gain control over it.</p><p>Within a few weeks, the voice that told Joe he was dead – the one he so feared could be real – was manifested in colour in front of him. For the therapy to work, he needed to find the courage to look the demon in the eyes; to challenge and conquer it. If he succeeded, the voices might fade away.</p><hr><p><span>P</span>rof Julian Leff was seven years into his retirement when the idea of avatar therapy came to him. After a celebrated career as a social psychiatrist and schizophrenia specialist at University College London, Leff was sitting at home in Hampstead, pondering the results of a survey that reported the most distressing aspect of hearing voices was the feeling of helplessness. On the rare occasions when his patients had had meaningful exchanges with their voices, he knew they had felt more in control. “I thought, how can I enable the patient to have a dialogue with an invisible voice?” Leff said in an interview for a documentary made in 2018, three years before his death. “If I can somehow manage to create for the patient the image and voice of the person who they hear abusing them, maybe they could learn to overcome this awful persecution.”</p><p>Leff was awarded a small grant for a pilot study in 2008. He recruited Mark Huckvale, professor of speech, hearing and phonetic sciences at UCL, to be in charge of the tech. They tinkered with existing police identikit software, animating digitally created faces in three dimensions so they could nod, smile and maintain eye contact. They combined this with an off-the-shelf programme called Lip Synch, so that the mouth would move appropriately, and voice-changing software, so the avatar could be made to sound male or female, rougher or smoother, higher or lower, older or younger.</p><p>The avatar was a floating, moving head on a computer screen, voiced by Leff, who would be in a separate room to the patient, watching via webcam. He could speak to the patient in his own voice, guiding them through the dialogue, and then switch with the click of a mouse to the role of the avatar on the patient’s screen, its lips synched to his speech. The setup allowed him to act as a therapist to the patient and a puppeteer to the avatar. At first, the avatar would say typical lines the patient had shared with Leff: often degrading, abusive phrases. But over the course of six sessions, the dialogue would change, with the avatar yielding to the patient, transforming from omnipotent to submissive. At all times, Leff and the patient were to treat the avatar as if it were an entirely real third party.</p><p>Sixteen people – all of whom had heard voices for years, despite being on medication – participated in this pilot study. A man who had heard the devil incessantly for 16 years was instructed by Leff to tell his demon avatar he didn’t deserve to be persecuted and he should go back to hell and torment those who did. An older man, who had been woken every morning at 5am for more than three years by the voice of a woman conducting noisy business meetings in his head, was encouraged by Leff to tell her it was unprofessional to allow him to overhear her discussions. To Leff’s surprise, both of these men stopped hearing their voices entirely after only three sessions. While most patients did not experience such a dramatic change, the results were still impressive: for 13 of the 16 participants, voices remained, but they were less frequent and intrusive, and suicidal feelings were significantly reduced.</p><p>The therapy had made a significant difference to a sample group composed of people for whom all other forms of treatment had failed. But other clinicians were wary of taking the results of the pilot seriously, believing they might be a consequence of Leff’s skill as a therapist, rather than the therapy itself. “He did have a magic touch,” Tom Craig, professor of psychiatry at King’s College London (KCL), told me. But Craig was sufficiently impressed by the results to lead a randomised controlled trial on 150 patients, along with Philippa Garety, professor of clinical psychology. Leff trained Craig and the clinical psychologist Tom Ward to deliver the therapy in his place, giving them audio recordings of his sessions and a checklist of how he thought things ought to be done, which Craig and Ward turned into a manual.</p><p>“Within the first couple of cases, we thought: this is extraordinary – something’s really happening here that we’ve never seen before,” Craig said. One participant, Chris, had been persecuted for years by the voices of high court judges who denounced him for intrusive sexual thoughts. Through the therapy, Chris came to accept his sexual urges as normal, and his high court judge avatar ultimately told him he had no case to answer. Free from persecution, Chris was able to go on dates for the first time in years.</p><p>Avatar therapy was <a href="https://www.thelancet.com/journals/lanpsy/article/PIIS2215-0366(17)30427-3/fulltext" data-link-name="in body link">found to be quicker,</a> cheaper and more effective after 12 weeks than any other non-pharmacological intervention currently available for people with psychosis. It worked, even without Leff, on a larger scale, and it worked faster than the control therapy delivered by highly trained clinicians.</p><p>As for the concern that engaging with auditory hallucinations exacerbates psychosis, Al Powers, associate professor of psychiatry at Yale University, told me it was not backed by empirical data. “Despite popular wisdom about not wanting to collude with the voices, the evidence that’s emerging seems to indicate that engagement-based approaches are most effective in terms of increasing control over voices, and also achieving some degree of mastery over them.”</p><p>Despite its early success in trials, Alderson-Day warns against viewing avatar therapy as a cure-all for psychosis. “The idea of a single therapeutic option for all kinds of voices is very unlikely,” he said. “Some people’s auditory experiences aren’t even voice-like, so there won’t be content to work with,” he told me. But if avatar therapy could be quicker and more cost-effective than existing treatments, he said, it was worth pursuing.</p><p>The research team’s next step was to demonstrate that avatar therapy could work when delivered by a broad range of therapists in different locations. A new trial, <a href="https://www.avatartherapytrial.com/" data-link-name="in body link">Avatar 2</a>, began in 2021, and involved 19 trained therapists in four different sites across the UK. There were 345 individuals enrolled in it – including Joe.</p><hr><p><span>J</span>oe was nervous about designing his avatar. No one had ever asked him to describe what his voices looked or sounded like before. He had spent so much energy telling himself they couldn’t be real and now he had to manifest them in the real world. There were other challenges: like most people with psychosis, Joe heard several voices, and he experienced them more as a felt presence, rather than a single entity with a definite physical appearance and a familiar face.</p><p>Tom Ward, who was assigned to be Joe’s therapist, told me the average number of voices heard by people with psychosis is four. “We are looking for the dominant voice that’s causing the most distress.” Joe chose the voice who told him there was no point in living because he was already dead.</p><p>He and Ward began creating the avatar’s face. There was a blizzard of choices in drop-down menus on Ward’s laptop screen: <em>is it a human or non-human entity</em>? If it’s human, what is its <em>gender, age, height (tall, medium, short), ethnicity (European, </em><em>east Asian, </em><em>south Asian, African)</em>? If it’s non-human, <em>does it take the form of a devil, angel, alien, vampire, robot, witch, goblin, elf, beast</em>? Once a basic form is chosen, there are sliders to change the physiognomy: making the nose broader, thinner, shorter or longer; adjusting the eyes, brow and chin; modifying the hairstyle.</p><p>Joe found it hard to describe what the voice looked like: it was often hooded, masked, out of focus, only partially visible. It was demonic, but it didn’t look like a devil. Together, they created the head of a bald man with olive skin, like Joe’s. He chose between five versions of voices, and used sliders to change the pitch, tilt and roughness. The final voice was deeper than any human’s. That’s what made it demonic, for Joe.</p><figure id="1607a6ea-93ed-414d-b6d7-0520c2c5746b" data-spacefinder-role="immersive" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-2"><picture><source srcset="https://i.guim.co.uk/img/media/79542b4dfbf8276e166ca9b09d6654de11162926/0_0_5000_3000/master/5000.jpg?width=1300&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 1300px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 1300px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/79542b4dfbf8276e166ca9b09d6654de11162926/0_0_5000_3000/master/5000.jpg?width=1300&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 1300px)"><source srcset="https://i.guim.co.uk/img/media/79542b4dfbf8276e166ca9b09d6654de11162926/0_0_5000_3000/master/5000.jpg?width=1140&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 1140px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 1140px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/79542b4dfbf8276e166ca9b09d6654de11162926/0_0_5000_3000/master/5000.jpg?width=1140&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 1140px)"><source srcset="https://i.guim.co.uk/img/media/79542b4dfbf8276e166ca9b09d6654de11162926/0_0_5000_3000/master/5000.jpg?width=1125&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 980px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 980px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/79542b4dfbf8276e166ca9b09d6654de11162926/0_0_5000_3000/master/5000.jpg?width=1125&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 980px)"><source srcset="https://i.guim.co.uk/img/media/79542b4dfbf8276e166ca9b09d6654de11162926/0_0_5000_3000/master/5000.jpg?width=965&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 740px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 740px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/79542b4dfbf8276e166ca9b09d6654de11162926/0_0_5000_3000/master/5000.jpg?width=965&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 740px)"><source srcset="https://i.guim.co.uk/img/media/79542b4dfbf8276e166ca9b09d6654de11162926/0_0_5000_3000/master/5000.jpg?width=725&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/79542b4dfbf8276e166ca9b09d6654de11162926/0_0_5000_3000/master/5000.jpg?width=725&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/79542b4dfbf8276e166ca9b09d6654de11162926/0_0_5000_3000/master/5000.jpg?width=645&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/79542b4dfbf8276e166ca9b09d6654de11162926/0_0_5000_3000/master/5000.jpg?width=645&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/79542b4dfbf8276e166ca9b09d6654de11162926/0_0_5000_3000/master/5000.jpg?width=465&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/79542b4dfbf8276e166ca9b09d6654de11162926/0_0_5000_3000/master/5000.jpg?width=465&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 320px)"><img alt="AI Avatars Long Read | Illustration 02 | DIGITAL" src="https://i.guim.co.uk/img/media/79542b4dfbf8276e166ca9b09d6654de11162926/0_0_5000_3000/master/5000.jpg?width=465&amp;dpr=1&amp;s=none&amp;crop=none" width="465" height="279" loading="lazy"></picture></div><figcaption data-spacefinder-role="inline"><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span> Illustration: Nick Kempton</figcaption></figure><p>It wasn’t exactly right, but once Joe was alone with the screen there was something about the avatar that resonated. Ward reminded Joe he was there to support him. Before they went into their separate rooms, they had practised what the avatar was going to say, and how Joe might respond to it. Still, he felt terrified.</p><p>“You’re already dead,” the avatar told him, in a voice that was almost monotone. “You’ve been in hell all this time and this is your existence from now on.”</p><p>“If this is death, it’s exactly the same as what my idea of life was,” Joe replied, a little meekly. He was surprised by how realistic the experience was, how true to life this felt.</p><p>“You’re lying to yourself,” the voice retorted.</p><p>In his own, encouraging voice, Ward reassured Joe, reminding him to hold eye contact, to communicate in strong messages that he was in charge.</p><p>“You’re harder to get hold of today,” the avatar said towards the end of the first session. “You can’t keep it up.”</p><p>“I can keep this up for ever, and I will,” Joe replied. “It’s my life. I have the autonomy here. I’m in control.”</p><p>Joe had 12 weekly sessions. The darkest exchange came in the fourth.</p><p>“You should end it,” the avatar said, casually. “What have you done that’s of any use to anyone?”</p><p>Joe couldn’t answer this.</p><p>In his own voice, Ward interjected to remind Joe about his relationships, his family, the life he had been able to make for himself. “What the avatar is saying is actually not true,” Ward said. “Can you come back with positives?”</p><p>Ward switched back to voicing the avatar. “You agree with me deep down,” it sneered. “You haven’t done anything of use.”</p><p>“No,” Joe said, firmly. “I have a lot of good friendships. I think on balance I have had a good life. It’s been positive. I’ve got more to do.”</p><p>“You’re handling yourself better than I thought,” the avatar replied. “I’d thought you’d be falling apart by now.”</p><hr><p><span>A</span>ll therapy sessions are recorded, and the audio given to the trial participants so they can listen back in their own time and be reminded of how they managed to gain mastery over their voices. With Joe’s agreement, Ward shared the recordings of his sessions with me.</p><p>Ward had delivered avatar therapy to about 80 people before he met Joe. He told me that people with psychosis often feel disempowered and marginalised; they feel as if they don’t have the right to talk. Avatar therapy is about equipping them with the tools to answer back.</p><p>I met Ward in his office at KCL’s Institute of Psychiatry in Denmark Hill, south London. He gave me a demonstration of the avatar design process, so that when we sat down to talk, a disembodied male head was blinking from his laptop in front of us, looking shiftily from side to side throughout our conversation. It still looked like an animated police photofit to me. Perhaps, with a voice to go with it, it would be easier to suspend disbelief.</p><p>Ward told me it can be helpful for many patients to understand their voices as a coping strategy arising from a previous trauma. (While there is no consensus among clinicians about the precise relationship between trauma and hearing voices, there is widespread acceptance that there is often a <a href="https://academic.oup.com/schizophreniabulletin/article/38/4/661/1870563?login=false" data-link-name="in body link">link between the two</a>.) But discovering whether or not a patient’s voices have arisen as a response to trauma isn’t important, Ward told me; the point of the therapy is to find any explanation that gives them a sense of mastery over their voices. This is the principle followed in any psychological intervention, he said.</p><p>Using the manual created from Leff’s instructions as a guide, the therapist plots out how the voice will change as the person is supported to stand up to it. When the voice is a bully, the avatar will begin to recognise the impact of their behaviour, perhaps revealing that they too have once been a victim of bullying. When the voice is a devil, a djinn or some other malevolent spirit, the avatar will reveal that they are not actually very powerful (“‘I’m not a high-ranking demon, I’m a trickster’ – that sort of thing. Things are quite interesting when you voice demons,” Ward said). When the voice resembles someone who abused the patient when the patient was a child, the avatar gradually acknowledges that they are no longer talking to a defenceless boy or girl but instead to an adult with agency.</p><p>Psychiatry professor Al Powers told me he saw potential problems in cases where the voice represented by the avatar belonged to a person who existed in the real world. “That can negatively impact one’s conceptualisation of your relationships with the world, and your family, and other people who are important to you, and that can contain some risks,” he said.</p><p>Before anyone can gain power over their voices, they have to tell their therapist what the voices say to them. It’s often the most degrading content imaginable; racist, sexist, sexually shaming and taboo. In his clinical experience, “You’re a paedophile” is one of the most common phrases repeatedly heard, Ward said. The avatar therapist has to reassure their patient that these phrases are nothing to be ashamed of, but also has to be prepared to deploy them when they are playing the role of avatar.</p><p>For the therapy to work, the therapist has to commit to playing the role of their patient’s tormentor. “You never break the fourth wall,” Ward said. The avatar can be direct – can go for the jugular – <em>because</em> it is not the therapist, and it can lie, or say things that are wrong. Hearing the avatar say these things can give the patient enough distance for them to reflect on and respond to what they usually only hear inside their own head.</p><p>In the Avatar 2 trial, for the first time, therapists went as far as allowing avatars to say things like, “You should end it.” When I brought this up with Ward, he stiffened. This kind of content is only used in specific circumstances, he said, when the patient’s risk of suicide has been assessed as minimal. “You don’t start the first dialogue with that. It comes at a point where you know how the person engages in the dialogue; you know that there is a clinical benefit to [them] voicing a commitment to life, and you know that they will be able to do it.” (I asked three specialists who work on the treatment of psychosis, but were not involved in the trial, about the dangers of the avatar therapist voicing commands to self-harm, and they all told me that, while unconventional, in these circumstances, it would not harm the patient.)</p><p>None of the avatar trials have shown that the therapy could exacerbate psychosis, even if people drop out before they have completed the run of sessions. “People do drop out,” Ward acknowledged. “Sometimes people will say, ‘It was just a bit too much for me.’” They have monitored all the people who had the therapy, tracking any mental health deterioration or hospital admission during and after treatment, and there has been no documented evidence of any crises directly related to avatar therapy during the Avatar 2 trials.</p><p>We are used to imagining those who hear voices as fragile, but Ward sees them as extraordinarily resilient: they can survive both years of the worst kind of internal persecution from their voices, and also the stigma and discrimination their condition is met with by the general public. Nothing simulated on a computer screen could be any more traumatic, he says, than what the people he works with endure in daily life.</p><hr><p><span>B</span>efore and after Joe’s dialogues with the avatar, he and Ward discussed how Joe’s voices might have developed in response to the extreme, heightened terror he had felt during his bad trip. Joe began to think of his voices as an overactive defence mechanism, a maladaptation of his brain as it tried to keep him safe and alert in a world he’d experienced as full of danger.</p><p>“The voices are just your paranoia, your anxiety, your fight-or-flight response gone mad. Give them space, and then you can have a conversation with them,” Joe told me. “I like to think that’s the reason why they started initially, but even if it isn’t, it doesn’t really matter, because it meant I was able to talk to them.”</p><p>By Joe’s seventh session, he was having insightful, poignant conversations with the avatar.</p><p>“Things are going to shit,” it grumbled.</p><p>“It’s not a bad assessment. They aren’t going fantastically,” Joe conceded. “You and I want the same thing – things not to be shit. That’s the goal. Rest assured, we’ll get there. It might take a while.”</p><p>“It’s why you need me.”</p><p>“In a way yeah, I guess. I think we’re working towards the same goal of just being the best version of myself I can be.”</p><p>“That’s what I need from you – to be the best,” the avatar said. “It’s what I’ve always needed.”</p><p>“Yours is not always the most helpful way to go about it, is it?” Joe said. “But I appreciate the sentiment.”</p><p>The dialogue had become a strange kind of couples therapy, in which Ward was playing the avatar and the therapist.</p><p>After four years of arguing with his voice, Joe began to feel compassion – even pity – for his tormentor. In the 10th session, Joe and the avatar discussed what happened in California. Joe described what he went through that night: not only the horror of it (“It was fucking scary, wasn’t it? It felt like a Hitchcock score sounds”) but also the alienation (“I was surrounded by people who found it funny”).</p><p>“You tried to tell yourself it didn’t matter, I wasn’t real,” the avatar said. And then, with resignation, “I’m fading from your life.”</p><p>“You’ll always be there in some form,” Joe reassured it. “But yeah.”</p><p>“Is that OK with you?” it asked. Joe’s tormenting voice had become his anxious companion.</p><p>“Yeah. I can live with that. So long as we’re able to coexist,” he replied.</p><p>“Thank you for listening,” were the avatar’s final words. “Thank you for making room for me.”</p><hr><p><span>A</span>vatar 2 set out to investigate how effective the therapy could be when delivered by therapists with far less experience than Ward and Craig. Some of the participants had been living with voices for decades. Claire was in her early 50s when she enrolled in Avatar 2, at the Manchester research site. She had heard the first voice when she was 10: an adult male, casually telling her to jump out of her bedroom window. It was entirely real to her, external and authoritative, “like an adult telling me what to do”. Claire had been abused from the age of seven. She grew up in a state of constant hypervigilance.</p><p>The nasty voice, the one that told her she was a stupid bitch, arrived when she was 13. “I remember saying, ‘Oh, shut up,’ out loud, and the other girls laughed and said, ‘There’s no one there!’ And then I realised I had to be quiet about them.” The voices became one among many secrets Claire kept, alongside the abuse and her self-harm.</p><p>She spent much of her adult life in psychiatric hospitals. By the time her mental health team’s care coordinator told her about the Avatar 2 trial in 2021, Claire had tried to end her life many times. She had been diagnosed with bipolar disorder, psychotic depression and, at one point, schizoaffective disorder. She was taking antidepressants, mood stabilisers and tranquillisers, as well as antipsychotics; she had tried CBT, cognitive analytic therapy, compassion-focused therapy and group survivor therapy. Still the voices persisted.</p><p>“The state I was in at the time, I thought, they’re not going to accept me on the trial – I’m too unstable,” Claire told me. But she was given a place on the trial, coordinated by Hannah Ball. Ball had only qualified as a psychologist a year before she was trained to deliver avatar therapy. She was assigned to be Claire’s therapist. “Hannah reassured me that nobody had ever had a crisis because of avatar therapy, and I thought, <em>that will be me. I’ll be the first one</em>,” Claire said.</p><p>Claire chose to make an avatar out of the first voice, which she felt shaped all the others. While she and Ball were putting together its angular male face, with its dark eyes and spiky hair that made Claire laugh because it wasn’t quite right, the voices over her right shoulder were enraged: “Don’t do this, you stupid fucking bitch.” The avatar’s northern accent was also not quite right, but there was something about its menacing tone that jolted Claire. As soon as she heard it, it was real to her.</p><p>She felt dizzy and sick in her first session, and Ball had to constantly check in to provide reassurance in her own voice. The dialogue lasted barely 10 minutes and left Claire exhausted, but as she walked home, she was smiling: she had been able to stand up to her voices for the first time in her life. Between sessions, she listened to the audio recordings that Ball had given her to take home, so she could remember what she had achieved and steel herself for her next encounter. (Claire agreed to share these recordings with me.)</p><p>By the third week, she was answering back to the avatar, asserting herself without any prompting from her therapist.</p><p>“Stop saying such nasty things to me. I’m not going to listen to you any more while you say such nasty things,” she told the avatar.</p><p>“I’m not sure what’s got into you,” it replied.</p><p>“I’m going to lay down some rules,” Claire said. “We can still talk, but on my terms, not yours.”</p><p>By the fourth week, Claire’s voices had gone entirely. For the first time in 40 years, she was alone with her own thoughts. Quiet.</p><p>She hadn’t expected them to go. “My aim wasn’t to get rid of them – just to get along with them,” she told me. “I wasn’t quite sure I wanted to let go. I’d never really been on my own. As abusive as it was, it’s still a relationship.”</p><p>Like Joe, she had been encouraged to understand her voices as a faulty self-defence mechanism. They had been trying to look after her: when they told her to end her life, they were trying to find a way to stop her suffering. Their departure was a kind of bereavement.</p><figure id="c77b1fdc-b178-46b7-9399-9bb77acb1aee" data-spacefinder-role="immersive" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-3"><picture><source srcset="https://i.guim.co.uk/img/media/edd4e67816d6336c959173f32e58b3ffe0679140/0_0_5000_3000/master/5000.jpg?width=1300&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 1300px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 1300px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/edd4e67816d6336c959173f32e58b3ffe0679140/0_0_5000_3000/master/5000.jpg?width=1300&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 1300px)"><source srcset="https://i.guim.co.uk/img/media/edd4e67816d6336c959173f32e58b3ffe0679140/0_0_5000_3000/master/5000.jpg?width=1140&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 1140px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 1140px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/edd4e67816d6336c959173f32e58b3ffe0679140/0_0_5000_3000/master/5000.jpg?width=1140&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 1140px)"><source srcset="https://i.guim.co.uk/img/media/edd4e67816d6336c959173f32e58b3ffe0679140/0_0_5000_3000/master/5000.jpg?width=1125&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 980px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 980px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/edd4e67816d6336c959173f32e58b3ffe0679140/0_0_5000_3000/master/5000.jpg?width=1125&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 980px)"><source srcset="https://i.guim.co.uk/img/media/edd4e67816d6336c959173f32e58b3ffe0679140/0_0_5000_3000/master/5000.jpg?width=965&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 740px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 740px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/edd4e67816d6336c959173f32e58b3ffe0679140/0_0_5000_3000/master/5000.jpg?width=965&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 740px)"><source srcset="https://i.guim.co.uk/img/media/edd4e67816d6336c959173f32e58b3ffe0679140/0_0_5000_3000/master/5000.jpg?width=725&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/edd4e67816d6336c959173f32e58b3ffe0679140/0_0_5000_3000/master/5000.jpg?width=725&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/edd4e67816d6336c959173f32e58b3ffe0679140/0_0_5000_3000/master/5000.jpg?width=645&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/edd4e67816d6336c959173f32e58b3ffe0679140/0_0_5000_3000/master/5000.jpg?width=645&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/edd4e67816d6336c959173f32e58b3ffe0679140/0_0_5000_3000/master/5000.jpg?width=465&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/edd4e67816d6336c959173f32e58b3ffe0679140/0_0_5000_3000/master/5000.jpg?width=465&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 320px)"><img alt="AI Avatars Long Read | Illustration 03 | DIGITAL" src="https://i.guim.co.uk/img/media/edd4e67816d6336c959173f32e58b3ffe0679140/0_0_5000_3000/master/5000.jpg?width=465&amp;dpr=1&amp;s=none&amp;crop=none" width="465" height="279" loading="lazy"></picture></div><figcaption data-spacefinder-role="inline"><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span> Illustration: Nick Kempton</figcaption></figure><p>In the remaining sessions, Ball helped Claire accept the loss of the voices, and she had an opportunity to say a definitive goodbye. The avatar promised to stay alongside her at a distance, there if needed, but no longer interfering in her life. “I wish you all the best,” it said at the end of the final session.</p><p>“Thank you. I wish you all the best, too,” Claire replied. “I know you had good intentions at heart.”</p><p>Nearly two years on, Claire’s voices have not returned. She’s coming off all her medications. She can go out in public, eat in a noisy restaurant, do voluntary work, give interviews to a journalist – all things that once seemed impossible. “I’m stronger. I’ve gained so much. I now feel I have a life worth living.”</p><p>Ball didn’t have the same level of experience as Leff, Craig or Ward, but she was able to achieve the same outcomes using the manual they developed as her guide. I asked to see the manual, but Ward told me he couldn’t share it with me, because it was “core IP”. The Wellcome Trust, which funds the trial, has been protective of its intellectual property in the past: research teams in Denmark, Australia and Canada that have been experimenting with avatars have been told there are restrictions around calling the work “avatar therapy”.</p><p>Ball told me the manual is not a script, more a set of objectives to aim for in each session: a general structure of how the avatar should change to empower the patient. She listened to recordings of one of Ward’s cases from the first trial, did two closely supervised pilot cases and was then delivering the therapy herself, alone.</p><p>I wondered how comfortable she felt, taking on the role of a malevolent entity that has enormous power over her patient. “It requires a lot of active formulation and reformulation on the spot, and listening out for things that might change how you were initially going to approach a dialogue,” she said.</p><p>I had imagined that only skilled clinicians could be avatar therapists, but Ball was convinced that, if they were willing to take on the challenge, a very wide range of mental health practitioners could do it. “I think you need people who understand relationships and dynamics,” she said. “If you’ve got a sense of who you are as the avatar and the relationship [to the patient], you know how to respond.”</p><hr><p><span>T</span>he results of the Avatar 2 trial, published on Monday, were dramatic. Avatar therapy has been shown to deliver rapid and significant reduction in distress caused by voices. No other psychological intervention has been shown to cause such a significant reduction in the frequency of intrusive voices.</p><p>Earlier this year, the National Institute for Health and Care Excellence announced that it has found avatar therapy to be safe and effective and recommended that it be offered for testing in clinical NHS settings over the next three years. Thirty-eight people have so far been trained to deliver it in the UK, from experienced clinicians to newly qualified psychologists and nurses. The most effective psychological therapy currently offered on the NHS, CBTp, is typically delivered by qualified clinical psychologists in 16 sessions over 12 months. In comparison, avatar therapy could work out as “half the length of time, with less skilled people, so a bit cheaper, and a bit more available”, Tom Craig told me. He hopes it will be part of NHS treatment within five years.</p><p>A small number of practitioners remain hesitant about avatar therapy being delivered by support workers and less experienced psychologists. Prof Neil Thomas, director of the Voices clinic in Melbourne, and lead investigator on the Australian Amethyst avatar therapy trial, said: “Working with people who hear voices is <em>already</em> an area of specialist practice. Using technology as well makes it even more specialist. The process is actually not particularly intuitive for people that have trained in therapy – which involves being supportive to people – to have to role play a nasty voice.”</p><p>But the British team are taking things even further. A newly announced Avatar 3 trial will investigate whether the avatar could be entirely digital and voiced by an artificial intelligence, which would remove the requirement for real-time human voicing of the avatar, and mean it could be widely disseminated. Humans would always be necessary to support the person in their interaction with the avatar and help make meaning of the voices, Craig said, but that would not need to be a trained therapist. It could be “a community nurse, or a nursing assistant”.</p><p>Louise Birkedal Glenthøj, associate professor of psychology at University of Copenhagen and the trial coordinator for Challenge, the Danish trial using avatars in the treatment of psychosis, told me she feared a fully digital avatar powered by AI might have the potential to exacerbate psychosis. “As people with psychosis struggle with grasping reality,” she said, “being in a dialogue with a machine that is not controlled by a therapist might generate psychotic experiences.”</p><p>The Danish team enrolled 270 participants in a trial that investigated how people who hear voices respond to having dialogues with an avatar using virtual reality. “We thought if [we could] integrate this in fully immersive VR, then we would perhaps get some additional benefit in terms of this potentially having a greater treatment effect,” Glenthøj told me. <em>“</em>Having the therapist close by would intuitively be more secure for the patient. We capitalise on this notion of ‘it’s real but it’s not real’. It’s so real that they feel they are in this dialogue with their voice, but it’s not real, and if they take off the headset, then it’s gone.”</p><p>The VR allows the user to situate the avatar in daily life settings, such as on the bus, or in the participant’s home. They also added emotions to the face, so the avatar could smile more and look more friendly as the dialogues progress.</p><p>Glenthøj conceded that VR avatar therapy can be overwhelming for some. “We <em>do </em>see people reacting. They destabilise. They get <em>more</em> psychotic.” As a result, the Danish team progressed more slowly than clinicians on the Avatar 2 trial, and have added safety features, such as a virtual panic button, and regular contact with the participants’ primary care providers throughout treatment. They also gave participants booster sessions at three and six months after treatment, in the hope of making any positive effects more durable. The <a href="https://www.researchsquare.com/article/rs-5180922/v1" data-link-name="in body link">trial ultimately found</a> that VR avatar therapy was significantly more effective at reducing voices compared with supportive counselling.</p><p>Avatar therapy may help in treating mental health conditions beyond psychosis. <a href="https://link.springer.com/article/10.1186/s40337-023-00900-1" data-link-name="in body link">Preliminary research</a> from Ward’s team with an avatar embodying the “anorexic voice” has shown it to be a promising intervention for eating disorders. Glenthøj is researching VR-based avatar therapy for obsessive compulsive disorder. Ward also wants to investigate whether dialogues with avatars could help people who struggle with anxiety or depression. “The technology is about creating this external representation of the dark side of yourself,” Craig said. “At some level, this is about thoughts, isn’t it?”</p><p>In Australia, avatar therapy can take place via telehealth, with therapist and participant often in different parts of the country. “We’ve got a lot of people living in regional areas who have limited access to mental healthcare – let alone to specialist therapies,” Thomas told me. They have drawn on how the British team worked during lockdown to see how it can be delivered remotely.</p><p>Some therapists have tried, in the past, to guide their patients through dialogue with voices via role play, or “chair work” – where the voices are represented by an empty chair with content spoken by the patient – but both these techniques require a leap of faith. With an avatar, it’s the recreation of the voice, not the face, that makes this radical, Thomas said. “It’s called avatar therapy, and that sounds like it’s primarily about the visual representation, but not everyone has an existing image that goes with their voice. I think the auditory transformation is particularly powerful.”</p><p>“The suspension of disbelief is remarkable,” Craig told me. Even though trial participants have signed consent forms and know it is the therapist voicing the avatar, they still relate to it as if it were the voice in their head. “They are put in front of this not very wonderful computer animation, and they’re <em>right in there</em>, talking to their voice.”</p><hr><p><span>‘I</span>t was liberating just to talk to Tom [Ward] about it, because I didn’t speak to anyone else,” Joe told me over coffee in south London. He still had the imposing presence of a rugby player, but he was so softly spoken that I had to strain to hear him talk over the hubbub of the cafe.</p><figure id="a0a9aad5-ad1d-46dc-bd41-dd6cc1c71ae2" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:108,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;Therapy wars: the revenge of Freud | Oliver Burkeman&quot;,&quot;elementId&quot;:&quot;a0a9aad5-ad1d-46dc-bd41-dd6cc1c71ae2&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/science/2016/jan/07/therapy-wars-revenge-of-freud-cognitive-behavioural-therapy&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;design&quot;:10,&quot;display&quot;:1,&quot;theme&quot;:0}}"></gu-island></figure><p>A year on from avatar therapy, Joe’s voice was still there, a presence just out of eyeshot, still a distinct external entity and not just an inner monologue. But it was quieter, easier to manage and allowed him to get on with daily life. When it spoke, it was to have the same kind of conversations they’d had in his final sessions with the avatar. “I get it – you’re very on edge,” Joe would tell his voice. “I don’t feel great either. But we are just walking to work at the moment. I promise, we’re good.”</p><p>“It worked because I understood the voices more, I think,” he told me. “My general levels of anxiety stayed pretty high, but I’ve started interpreting the hallucinations as a part of the anxiety.” He still has panic attacks. The anxiety and self-doubt that existed before his bad trip are still there. “You do have to address everything that’s going on to address the voices themselves. They feed on everything else.”</p><p>Joe recently went back to his GP in search of help with his anxiety, but there was no cutting-edge experimental solution delivered by renowned psychologists for him this time. The GP put him on a waiting list for NHS talking therapy, and warned that he could be in for a very long wait.</p><p><em>Names of patients have been changed. </em></p><p><em><span data-dcr-style="bullet"></span> </em>In the UK and Ireland, <a href="https://www.samaritans.org/" data-link-name="in body link">Samaritans</a> can be contacted on freephone 116 123, or email <a href="mailto:jo@samaritans.org" data-link-name="in body link | mailto:jo@samaritans.org">jo@samaritans.org</a> or <a href="mailto:jo@samaritans.ie" data-link-name="in body link | mailto:jo@samaritans.ie">jo@samaritans.ie</a>. In the US, you can call or text the <a href="https://988lifeline.org/" data-link-name="in body link">National Suicide Prevention Lifeline</a> on 988, chat on <a href="https://988lifeline.org/chat/" data-link-name="in body link">988lifeline.org</a>, or <a href="https://www.crisistextline.org/" data-link-name="in body link">text HOME</a> to 741741 to connect with a crisis counsellor. In Australia, the crisis support service <a href="https://www.lifeline.org.au/" data-link-name="in body link">Lifeline</a> is 13 11 14. Other international helplines can be found at <a href="http://www.befrienders.org/" data-link-name="in body link">befrienders.org</a></p><p><span data-dcr-style="bullet"></span> In the UK, the charity <a href="https://www.mind.org.uk/" data-link-name="in body link">Mind</a> is available on 0300 123 3393 and <a href="https://www.childline.org.uk/" data-link-name="in body link">Childline</a> on 0800 1111. In the US, call or text <a href="https://www.mhanational.org/" data-link-name="in body link">Mental Health America</a> at 988 or chat 988lifeline.org. In Australia, support is available at <a href="https://www.beyondblue.org.au/" data-link-name="in body link">Beyond Blue</a> on 1300 22 4636, <a href="https://www.lifeline.org.au/" data-link-name="in body link">Lifeline</a> on 13 11 14, and at <a href="https://mensline.org.au/" data-link-name="in body link">MensLine</a> on 1300 789 978</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I built an app to use a QR code as my doorbell (101 pts)]]></title>
            <link>https://dingdongdoorbell.com</link>
            <guid>41980681</guid>
            <pubDate>Tue, 29 Oct 2024 08:03:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dingdongdoorbell.com">https://dingdongdoorbell.com</a>, See on <a href="https://news.ycombinator.com/item?id=41980681">Hacker News</a></p>
<div id="readability-page-1" class="page">
    
  
  <h3>Replace your doorbell with a QR code</h3>
  <p>Ding Dong Doorbell offers a versatile digital doorbell for every entrance.</p><ol> 
    <li>Get a QR code to put at your door</li>
    <li>Visitors scan the QR code using their smartphone camera</li>
    <li>Get a notification on your phone wherever you are!</li>
  </ol>
  <p>Having a doorbell does not need to be complicated.</p>
  <ul>
    <li>Instant notifications anywhere</li>
    <li>No wires, no batteries</li>
    <li>Cover multiple entrances with ease</li>
    <li>Perfect for homes, apartments, offices and temporary setups</li>
    <li>No worries that your expensive doorbell will be vandalized or stolen</li>
  </ul>

  <p>Download the app and get your doorbell QR code today!</p>

  <section id="cta">
    <p><a href="https://play.google.com/store/apps/details?id=za.co.code27.dingdong" target="_blank">
        <img src="https://dingdongdoorbell.com/static/img/GetItOnGooglePlay_Badge_Web_color_English.png" width="270" height="80">
      </a>
      <a href="https://apps.apple.com/us/app/ding-dong-doorbell/id6670178558" target="_blank">
        <img src="https://dingdongdoorbell.com/static/img/Download_on_the_App_Store_Badge_US-UK_RGB_blk_092917.svg" width="240" height="80">
      </a>
    </p>
    <a href="http://eepurl.com/i0e-HI">Join our mailing list</a>
  </section>


  

  

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[How I write code using Cursor (380 pts)]]></title>
            <link>https://www.arguingwithalgorithms.com/posts/cursor-review.html</link>
            <guid>41979203</guid>
            <pubDate>Tue, 29 Oct 2024 03:50:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.arguingwithalgorithms.com/posts/cursor-review.html">https://www.arguingwithalgorithms.com/posts/cursor-review.html</a>, See on <a href="https://news.ycombinator.com/item?id=41979203">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
<p>In forums relating to AI and AI coding in particular, I see a common inquiry
from experienced software developers: <em>Is anyone getting value out of tools like
Cursor, and is it worth the subscription price?</em></p>
<p>A few months into using Cursor as my daily driver for both personal and work
projects, I have some observations to share about whether this is a
"need-to-have" tool or just a passing fad, as well as strategies to get the most
benefit quickly which may help you if you'd like to trial it. Some of you may
have tried Cursor and found it underwhelming, and maybe some of these
suggestions might inspire you to give it another try.</p>
<p>I am not sponsored by Cursor, and I am not a product reviewer. I am neither
championing nor dunking on this as a product, but rather sharing my own
experience with it.</p>
<p><strong>Who am I, and who is the audience for this article?</strong></p>
<p>I have been writing code for 36 years in a number of languages, but
professionally focused on C-heavy computer game engines and Go/Python/JS web
development. I am expecting readers to be similarly reasonably comfortable and
productive working in large codebases, writing and debugging code in their
chosen language, etc. I would give very different advice to novices who might
want an AI to teach them programming concepts or write code for them that is way
beyond their level!</p>
<p>For me, the appeal of an AI copilot is in taking care of boilerplate and
repetitive tasks for me so I can focus on the interesting logic for any given
problem. I am also not especially interested in cranking out large quantities of
code automatically; I am highly skeptical of "lines of code written" as an
efficiency metric. I would prefer to spend less time writing the same amount of
code and more time thinking through edge cases, maintainability, etc.</p>
<p>So, without further ado:</p>
<h2>What is Cursor?</h2>
<p>Cursor<sup id="fnref:1"><a href="#fn:1">1</a></sup> is a fork of Visual Studio Code (VS Code) which has Large Language Model (LLM)
powered features integrated into the core UI. It is a proprietary product with a
free tier and a subscription option; however, the pricing sheet doesn't cover
what the actual subscriber benefits are and how they compare to competing
products. I'll try to clarify that when discussing the features below based on
my own understanding, but a quick summary:</p>
<ul>
<li><strong>Tab completion</strong>: This is a set of proprietary fine-tuned models that both
  provide code completion in the editor, as well as navigate to the next
  recommended action, all triggered by the Tab key. Only available to subscribers.</li>
<li><strong>Inline editing</strong>: This is a chat-based interface for making edits to
  selected code with a simple diff view using a foundation model such as GPT or
  Claude. Available to free and paid users.</li>
<li><strong>Chat sidebar</strong>: This is also a chat-based interface for making larger edits
  in a sidebar view, allowing more room for longer discussion, code sample
  suggestions across multiple files, etc. using a foundation model such as GPT
  or Claude. Available to free and paid users.</li>
<li><strong>Composer</strong>: This is yet another chat-based interface specifically meant for
  larger cross-codebase refactors, generating diffs for multiple files that you
  can page through and approve, also using a foundation model such as GPT or Claude.
  Available to free and paid users.</li>
</ul>
<h2>Tab completion</h2>
<p>While other LLM-powered coding tools focus on a chat experience, so far in my
usage of Cursor it's the tab completion that fits most naturally into my
day-to-day practice of coding and saves the most time. A lot of thought and
technical research has apparently gone into this feature, so that it can not
only suggest completions for a line, several lines, or a whole function, but it
can also suggest the next line to go to for the next edit. What this amounts to
is being able to make part of a change, and then auto-complete related changes
throughout the entire file just by repeatedly pressing Tab.</p>
<p>One way to use this is as a code refactoring tool on steroids. For example,
suppose I have a block of code with variable names in <code>under_score</code> notation
that I want to convert to <code>camelCase</code>. It is sufficient to rename one instance
of one variable, and then tab through all the lines that should be updated,
including the other related variables. Many tedious, error-prone tasks can be
automated in this way without having to write a script to do so:</p>
<video controls="">
  <source src="https://www.arguingwithalgorithms.com/videos/cursor-review/example1.webm" type="video/webm">
  <p>
    Your browser doesn't support HTML video. Here is a
    <a href="https://www.arguingwithalgorithms.com/videos/cursor-review/example1.webm" download="example1.webm">link to the video</a> instead.
  </p>
</video>

<p>Sometimes tab completion will indepedently find a bug and propose a fix. Many
times it will suggest imports when I add a dependency in Python or Go. If I wrap
a string in quotes, it will escape the contents appropriately. And, as with
other tools, it can write whole functions based on just the function signature
and optional docstring:</p>
<video controls="">
  <source src="https://www.arguingwithalgorithms.com/videos/cursor-review/example2.webm" type="video/webm">
  <p>
    Your browser doesn't support HTML video. Here is a
    <a href="https://www.arguingwithalgorithms.com/videos/cursor-review/example2.webm" download="example2.webm">link to the video</a> instead.
  </p>
</video>

<p>All in all, this tool feels like it is reading my mind, guessing at my next
action, and allowing me to think less about the code and more about the
architecture of I am building.</p>
<p>Also worth noting: The completions are <em>incredibly fast</em>, and I never felt a delay
waiting for a suggestion. They appear basically as soon as I stop typing. Having
too long a wait would surely be a deal-breaker for me.</p>
<p>So, what are my complaints with Tab completion? One is a minor annoyance:
Sometimes I don't see the suggestion in time and continue typing, and the
completion disappears. Once it is gone, there doesn't appear to be any way to
get it to come back, so I have to type something else and hope.</p>
<p>My other complaint is the exact opposite situation: Sometimes a completion is
dead wrong, and I intentionally dismiss it. Subsequently, but very infrequently,
I will accept a totally different completion and the previously-declined
suggestion will quietly be applied as well. This has already caused some
hard-to-track-down bugs because I wasn't aware the wrong logic had been
accepted. I haven't found these cases to be frequent enough to cancel out the
productivity boost of tab completion, but they do detract from it.</p>
<h2>Inline editing, chat sidebar, and composer</h2>
<p>As far as I can tell, these features are all very similar in their interaction
with a foundational model - I use Claude 3.5 Sonnet almost exclusively - and the
variance is in the user interface.</p>
<p>Inline editing can be invoked by selecting some code and pressing Ctrl-K/Cmd-K.
I type in the desired changes, and get a nice diff in the file that I can accept
or reject. I use this mostly to implement bits of code inside a function or make
minor refactors.</p>
<p>A good example of where this works great is if I have a loop over some tasks and
I want to parallelize them:</p>
<video controls="">
  <source src="https://www.arguingwithalgorithms.com/videos/cursor-review/example3.webm" type="video/webm">
  <p>
    Your browser doesn't support HTML video. Here is a
    <a href="https://www.arguingwithalgorithms.com/videos/cursor-review/example3.webm" download="example3.webm">link to the video</a> instead.
  </p>
</video>

<p>The chat sidebar is opened with Ctrl+L/Cmd+L, and gives more real estate for a
multi-turn conversation, though one pet peeve I have with the LLM models I've
tested so far is they will <em>always</em> return code first, rather than ask for
clarification if there is any ambiguity. The suggested code has an Apply button
that will create a diff in the currently selected file. This is useful for
larger refactors within a single file, or creating a brand new file based on the
file I have open. If additional files are relevant they can be added manually to
the context, but Cursor will try to guess which files are relevant based on the
query and an index it generates in the background.</p>
<p>Here is an example which takes an application's database API and creates a REST
API to access it, with parameter validation and correct HTTP status codes,
<em>then</em> writes a client library to access that REST API:</p>
<video controls="">
  <source src="https://www.arguingwithalgorithms.com/videos/cursor-review/example4.webm" type="video/webm">
  <p>
    Your browser doesn't support HTML video. Here is a
    <a href="https://www.arguingwithalgorithms.com/videos/cursor-review/example4.webm" download="example4.webm">link to the video</a> instead.
  </p>
</video>

<p>As another example, here I am using the chat sidebar to convert the client
library from Python to Go. Note how the loosely-typed Python is converted to
well-defined struct types and idiomatic Go including error handling! This is not
a 1:1 rewrite at all:</p>
<video controls="">
  <source src="https://www.arguingwithalgorithms.com/videos/cursor-review/example5.webm" type="video/webm">
  <p>
    Your browser doesn't support HTML video. Here is a
    <a href="https://www.arguingwithalgorithms.com/videos/cursor-review/example5.webm" download="example5.webm">link to the video</a> instead.
  </p>
</video>

<p>Finally, Composer is specifically meant for cross-file refactors. This is also
the feature I use least, but provides a better user experience for reviewing
multiple file diffs one at a time.</p>
<h2>.cursorrules file</h2>
<p>I did not realize this feature existed until I came across it in the (in my
opinion too minimal) documentation, but the various chat modalities always
include the contents of a <code>.cursorrules</code> file located at the root of the
workspace to provide additional context. I've been experimenting with using this
to inform the LLM of the repository's coding standards, common packages, and
other documentation.</p>
<p>This feature might help to solve one of the big roadblocks I have observed with
Cursor: It does not follow coding styles and patterns unless they already exist
in the same file you are editing. For example, at Khan Academy we use a
proprietary library <sup id="fnref:2"><a href="#fn:2">2</a></sup> for passing context between functions in Go. This is
used for logging, HTTP requests, etc. so the LLM needs to be able to use it.
This has been difficult in the past, but perhaps a well-written <code>.cursorrules</code>
is a good first step.</p>
<p>One current limitation is that there is only one of these files per workspace,
so a monorepo like ours containing code in multiple languages is going to be
more difficult to set up than a small repository with a small set of very
consistently styled code.</p>
<p>Also the documentation suggests that the <code>.cursorrules</code> file is only used for
the chat modalities, not the tab completion. However I've experimented with
having that file open in a pinned tab in the workspace and confirmed that it is
possible to include it in the tab completion context that way at least.</p>
<h2>Changes to my workflow</h2>
<p>The most exciting thing about a tool like Cursor is not that I can write code
faster, because honestly the actual writing of code is not the bottleneck; in
fact, I often have to slow myself down to avoid focusing too much on the code
and not enough on the high-level problem being solved. The real value is in
changing <em>how</em> I code.</p>
<p>It's still early days with this technology, but this is what I've found has
changed about how I work and what I expect to see changing in the near future:</p>
<ol>
<li>
<p>I am <em>much</em> less likely to reach for a new library or a framework. No, I'm
not going to start writing my own crypto libraries, but for small utilities it's
easy enough to let the LLM write them to my bespoke needs than to pull in a
general-purpose library. These libraries tend to start small and lightweight and
then, because they are open and used by many people, accumulate functionality
and cruft that I don't need.</p>
<p>Many of these libraries only exist to reduce boilerplate, which felt like a
necessary tradeoff when balanced against my time writing and maintaining that
boilerplate but now that I can have the LLM do it for me it feels less worth the
cost. And the cost can be substantial: Have you tried getting a Node.js project
running a year or more after it was written? You may as well start from scratch.</p>
</li>
<li>
<p>I also worry less about adhering to DRY (Don't Repeat Yourself) in my own code.
Prematurely defining abstractions can create a lot of technical debt later on,
so being able to create a lot of code with reference to other code without
trying to pull it into a function or class allows me more flexibility, and I
know that if I have to refactor shared logic out later, the LLM can help with
that too.</p>
</li>
<li>
<p>My willingness to use a language or framework I am less familiar with is much
higher. For example, I've dabbled in R for years, especially for visualizing
data. However, to be frank, I suck at it. I don't have a deep understanding of
<code>dplyr</code> and it seems like there are always a dozen different ways to accomplish
the same task. Now I describe the visualization I want, and I get correct data
manipulation and <code>ggplot</code> visualization for it. Tasks that took an hour or more
now take five minutes, so I am much less likely to give up and do it in Python
instead.</p>
<p>Maybe one of these days I'll even write something in Rust. Maybe.</p>
</li>
<li>
<p>I find myself iterating quickly on small components before integrating them
into the larger codebase. This is partly to work around the limitations of LLMs
when working with larger codebases, but it also opens up interesting ways of
working I hadn't considered before. As per the example above, I can prototype
some logic in a dynamically typed language like Python, work out the technical
details and then convert it to well-typed Go instantly to integrate into a web
application. I can have the LLM generate test data automatically, or mock up a
backend for me to write a frontend against. Why pay the tax of working in a
mature codebase while I'm still proving out an idea?</p>
</li>
</ol>
<h2>Summary</h2>
<p>Whether I'll be using Cursor in a few years or have moved on to another tool, I
can't really tell. I am confident that at the time of writing this, Cursor is
the best example of the potential of LLM coding assistants, and if you want to
explore how this type of tool might be of value I suggest you give it a spin.</p>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The secret electrostatic world of insects (132 pts)]]></title>
            <link>https://www.wired.com/story/the-secret-electrostatic-world-of-insects/</link>
            <guid>41978478</guid>
            <pubDate>Tue, 29 Oct 2024 01:31:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/the-secret-electrostatic-world-of-insects/">https://www.wired.com/story/the-secret-electrostatic-world-of-insects/</a>, See on <a href="https://news.ycombinator.com/item?id=41978478">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p><em><span>The original version</span> of</em> <a href="https://www.quantamagazine.org/the-hidden-world-of-electrostatic-ecology-20240930/"><em>this story</em></a> <em>appeared in <a href="https://www.quantamagazine.org/">Quanta Magazine</a>.</em></p><p>Imagine, for a moment, that you’re a honeybee. In many ways, your world is small. Your four delicate wings, each less than a centimeter long, transport your half-gram body through looming landscapes full of giant animals and plants. In other ways, your world is expansive, even grand. Your five eyes see colors and patterns that humans can’t, and your multisensory antennae detect odors from distant flowers.</p><p>For years, biologists have wondered whether bees have another grand sense that we lack. The static electricity they accumulate by flying—similar to the charge generated when you shuffle across carpet in thick socks—could be potent enough for them to sense and influence surrounding objects through the air. Aquatic animals such as eels, sharks, and dolphins are known to sense electricity in water, which is an excellent conductor of charge. By contrast, air is a poor conductor. But it may relay enough to influence living things and their evolution.</p><p>In 2013, <a data-offer-url="https://research-information.bris.ac.uk/en/persons/daniel-robert" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://research-information.bris.ac.uk/en/persons/daniel-robert&quot;}" href="https://research-information.bris.ac.uk/en/persons/daniel-robert" rel="nofollow noopener" target="_blank">Daniel Robert</a>, a sensory ecologist at the University of Bristol in England, broke ground in this discipline when his lab discovered that bees can <a href="https://doi.org/10.1126/science.1230883" target="_blank">detect and discriminate</a> among electric fields radiating from flowers. Since then, more experiments have documented that spiders, ticks, and other bugs can perform a similar trick.</p><p>This animal static impacts ecosystems. Parasites, such as <a href="https://doi.org/10.1016/j.cub.2023.06.021" target="_blank">ticks</a> and <a data-offer-url="https://meetings.aps.org/Meeting/MAR23/Session/B08.1" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://meetings.aps.org/Meeting/MAR23/Session/B08.1&quot;}" href="https://meetings.aps.org/Meeting/MAR23/Session/B08.1" rel="nofollow noopener" target="_blank">roundworms</a>, hitch rides on electric fields generated by larger animal hosts. In a behavior known as ballooning, spiders <a href="https://doi.org/10.1016/j.cub.2018.05.057" target="_blank">take flight</a> by extending a silk thread to catch charges in the sky, sometimes traveling hundreds of kilometers with the wind. And this year, studies from Robert’s lab revealed how static <a href="https://doi.org/10.1098/rsif.2024.0156" target="_blank">attracts pollen</a> to butterflies and moths, and may help caterpillars to <a href="https://doi.org/10.1073/pnas.2322674121" target="_blank">evade predators</a>.</p><p>This new research goes beyond documenting the ecological effects of static: It also aims to uncover whether and how evolution has fine-tuned this electric sense. Electrostatics may turn out to be an evolutionary force in small creatures’ survival that helps them find food, migrate, and infest other living things.</p><figure><p><span>Daniel Robert studies animal biophysics at the University of Bristol. His lab has accumulated studies on electrostatic sense in bees, spiders, ticks, butterflies and more.</span><span>Photograph: Courtesy of Alexander Robert</span></p></figure></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>This developing field, known as aerial electroreception, opens up a new dimension of the natural world. “I find it absolutely fascinating,” said <a data-offer-url="https://eeb.arizona.edu/person/anna-dornhaus" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://eeb.arizona.edu/person/anna-dornhaus&quot;}" href="https://eeb.arizona.edu/person/anna-dornhaus" rel="nofollow noopener" target="_blank">Anna Dornhaus</a>, a behavioral ecologist at the University of Arizona who was not involved with the work. “This whole field, studying electrostatic interactions between living animals, has the potential to uncover things that didn’t occur to us about how the world works.”</p><p>“We know from all these brilliant experiments that electric fields do have a functional role in the ecology of these animals,” said <a data-offer-url="https://research-portal.st-andrews.ac.uk/en/persons/benito-wainwright" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://research-portal.st-andrews.ac.uk/en/persons/benito-wainwright&quot;}" href="https://research-portal.st-andrews.ac.uk/en/persons/benito-wainwright" rel="nofollow noopener" target="_blank">Benito Wainwright</a>, an evolutionary ecologist at the University of St. Andrews who has studied the sensory systems of butterflies and katydids. “That’s not to say that they came on the scene originally through adaptive processes.” But now that these forces are present, evolution can act on them. Though we cannot sense these electric trails, they may guide us to animal behaviors we never imagined.</p><h2>Electrostatic Discoveries</h2><p>In 2012, <a data-offer-url="https://ornithopterus.com/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://ornithopterus.com/&quot;}" href="https://ornithopterus.com/" rel="nofollow noopener" target="_blank">Víctor Ortega-Jiménez</a> stumbled into electrostatics while playing with his 4-year-old daughter. They were using a toy wand that gathers static charge to levitate lightweight objects, such as a balloon. When they decided to test it outside, he made a startling observation.</p><figure><p><span>Studies by Víctor Ortega-Jiménez of the University of California, Berkeley revealed that a negatively charged spiderweb attracts positively charged insect prey.</span><span>Photograph: Courtesy of Víctor Ortega-Jiménez</span></p></figure></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>“My daughter put the wand close to a spiderweb, and it reacted very quickly,” recalled Ortega-Jiménez, who studies the biomechanics of animal travel at the University of California, Berkeley. The wand attracted the web. He immediately began to draw connections to his research about the strange ways insects interact with their environments.</p><p>All matter—wands, balloons, webs, air—strives for balance between its positive and negative particles (protons, electrons and ions). At an unfathomably small scale, Ortega-Jiménez’s toy buzzes with an imbalance: A motor draws negative charges inward, forcing positive charges to the wand’s surface. This is static. It’s like when you rub a balloon against your head. Friction sheds electrons from your hair to the rubber, loading it up with static charge, so that when you lift the balloon, strands of hair float with it.</p><p>In a similar way, Ortega-Jiménez considered, friction from beating insect wings could shed negative charges from body to air, leaving the insects with a positive charge while creating regions of negative static. He realized that if a web carries negative charge and insects a positive one, then a spiderweb might not just be a passive trap—it could move toward and attract its quarry electrostatically. His lab experiments revealed precisely that. Webs <a href="https://doi.org/10.1038/srep02108" target="_blank">deformed instantly</a> when jolted with static from flies, aphids, honeybees, and even water droplets. Spiders caught charged insects more easily. He saw how static electricity altered the physics of animal interactions.</p><p>The magic of animal electrostatics is all about size. Large animals don’t meaningfully experience nature’s static—we’re too big to feel it. “As humans, we are living mostly in a gravitational or fluid-dynamics world,” Ortega-Jiménez said. But for tiny beings, gravity is an afterthought. Insects can feel air’s viscosity. While the same laws of physics reign over Earth’s smallest and largest species, the balance of forces shifts with size. Intermolecular forces flex beneath the feet of water striders on a pond, capillary forces shoot water impossibly upward through a plant’s thin roots, and electrostatic forces can ensnare any oppositely charged flecks that lie in their path.</p><figure><p><span>A parasitic nematode twirls through the air, pulled toward its electrostatically charged insect host (top). Tiny particles make the electrostatic field visible (bottom).</span><span>Images: Courtesy of Víctor Ortega-Jiménez</span></p></figure><p>“Charged fleck” is an apt physical description of a pollen grain. A few years after Ortega-Jiménez noticed spiderwebs nabbing bugs, Robert’s team found that bees can <a href="https://doi.org/10.1007/s00359-017-1176-6" target="_blank">gather negatively charged pollen</a> without brushing up against it. When a bee drank nectar from a flower, the pollen shot right onto its body. “There was no contact required between the bee and the flower for that pollen to jump,” Robert said. “This is a trajectory that responds to electrostatic forces.”</p><p>The discovery suggested to Robert that electrostatics can enable a plant-pollinator mutualism, a well-known example of coevolution. This dynamic—in which a bee feeds on a flower’s nectar and gathers pollen to feed larvae, and also propagates pollen from flower to flower, enabling plant reproduction—was already well established. The potential role of static charge was brand-new.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Over the past decade, Robert has built a body of work that reveals the many ways insects and arachnids use and experience static. Ticks jump, spiders balloon, bees sense the negative charge of a flower recently visited by another positively charged bee. He even found that the charged relationship between air and insects goes both ways: Honeybee swarms shed so many negative charges that they <a href="https://doi.org/10.1016/j.isci.2022.105241" target="_blank">alter the electrical gradient</a> around them. Based on Robert’s estimates, the atmospheric charge resulting from a swarm of desert locusts rivals that of clouds and electrical storms.</p><figure><p><span>Illustration: Mark Belan/<em>Quanta Magazine</em></span></p></figure><p>Robert’s and Ortega-Jiménez’s conclusions were provocative. But to them, the physics of arthropods makes electrostatic forces inevitable. Bugs are light and angular with a high ratio of surface area to volume—“all these parameters that physicists can tell you call for higher charge density,” Robert said. “It turns out that their world is way more electrical than ours.”</p><p>Still, the experiments couldn’t conclude that the creatures control this electrostatic function, or how it evolved—if it even did evolve. Robert wondered: Is the use of static fields by bugs coincidental or adaptive?</p><h2>Static for Survival</h2><p>Sam England wears his love of nature on his sleeve. He has half a dozen animal tattoos, including a treehopper decorated with the planets of our solar system—an homage to his background in physics. The marriage of these worlds drives his curiosity: How does physics mold animal behavior? He pivoted to sensory ecology for graduate school and joined Robert’s lab at the University of Bristol to chase the hypothesis that insects actively use static to affect their environments.</p><figure><p><span>To measure the static charge carried by butterflies and moths, Sam England tied fishing line to each flier and “walked” them through a metal loop.</span><span>Photograph: Courtesy of Rebecca Ward</span></p></figure></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Because the electrostatic world is invisible to human researchers, its forces are hard to study, even before you add unpredictable creatures to the mix. “Doing research in biology can be so much harder than physics because you have to rely on live animals to do something,” England said. He wanted to test whether Lepidoptera, the order of flying insects that includes butterflies and moths, build up enough static during flight to collect pollen from the flowers they visit for nectar, as bees do. But first he had to rig up a way to measure the insects’ static charge.</p><p>A “walk” is England’s best analogy for his method of tricking the insects into staying airborne for 30 seconds. “I had to tie little lassos around their waists,” he said. He leashed each flier with fishing line and coaxed them through a metal loop fixed to measure their charge.</p><p>England studied 11 species of butterflies and moths native to various climates, ecosystems, and lifestyles. After they flew around their cages for 30 seconds—enough time to accumulate electrostatic charge—he guided them through the loop. All 11 species charged up during flight. Some reached static charge of around 5 kilovolts per meter—enough to <a href="https://doi.org/10.1098/rsif.2024.0156" target="_blank">yank negatively charged pollen</a> from 6 millimeters away, he calculated.</p><p>When lepidopterans land directly on a flower, pollen naturally sticks to their bodies. If static charge causes pollen to skip across air gaps, “it’s going to increase their efficiency as pollinators,” England said. “It makes it more likely that pollination will occur.”</p><p>To gauge static’s evolutionary significance, he looked for patterns in how the animals’ behavior in the wild correlates with their electrical charge. He found a few. For example, nocturnal moths tend to hold less charge than other species. Why? It’s possible, England speculates, that strong charges make insects more visible to predators that rely on nonvisual cues, such as static, at night. Minimizing charge could therefore help the moths survive.</p><div data-testid="feature-large-callout"><figure><p><span>New research studied static charge on 11 butterflies and moths, including the hawk moth (left) and peacock butterfly (right). The hawk moth, a nocturnal species, carries practically no charge — possibly to avoid detection by predators in the dark.</span><span>Photographs: Getty Images</span></p></figure></div><p>“It’s great new data,” Ortega-Jiménez said. He cautioned that the study’s 11 species are a modest representation of the world’s 180,000 or so lepidopterans. “For claiming electrostatic adaptation, it needs to be more broad. But it’s a good hypothesis.”</p><p>For insects to act on static information, they must be able to detect electrical fields. Microscopic hairs on bees and spiders seem to aid in sensing, according to work from Robert’s lab. England recently expanded this unresolved science by studying how the minuscule hairs of caterpillars deflect under static, to glean how electric information may help a caterpillar survive.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>When England’s team exposed caterpillars to electric fields similar to those generated by a flying wasp, caterpillars <a href="https://doi.org/10.1073/pnas.2322674121" target="_blank">displayed defensive behaviors</a> such as coiling, flailing, or biting. “This basically insinuates,” England said, that “prey and predator can detect each other just using static electricity.”</p><p>Dornhaus, the behavioral ecologist, questioned whether electroreception buys the caterpillar much time. Yet the high stakes of predator-prey conflict suggest that any advantage may count. “For the individual caterpillar, even just getting a small increase in the chance of surviving that encounter makes it an evolutionarily relevant behavior,” she said.</p><p>“Organisms are always opportunists,” said Ortega-Jiménez, who is hesitant but impressed by England’s research. He is eager for more data—ideally from wild animals—that examines naturalistic behaviors. “Who is winning this game? Who is taking more advantage of electrostatics?” he asked. “What kinds of predator and prey?”</p><div data-testid="feature-large-callout"><figure><p><span>A caterpillar of a cinnabar moth coils in a defensive posture. The larva’s sensory hairs may be able to detect static fields generated by predators such as wasps.</span><span>Photograph: Courtesy of Sam J. England</span></p></figure></div><p>As more evidence links static to survival, a story is emerging that evolution may fine-tune the capacity to sense or carry charge just like any other trait. “The fact that there’s such a diverse range of species with different ecologies is what makes it so interesting,” said Beth Harris, a graduate student in Robert’s lab. “There’s a real treasure chest to be opened.”</p><h2>Electrical Inheritance</h2><p>As work continues in Robert’s lab, the suspicion that static detection and accumulation among insects and arachnids is no accident does as well. Caterpillars with better electroreception, or nocturnal moths that carry lower charge, may better dodge predators. If they survive to reproduce more, those genes and traits—including those that help organisms sense and use static fields—could become stronger and more common in generations down the line.</p><p>It’s starting to become impossible to ignore the idea that electrostatics may be more influential in the animal kingdom than we know today. Whole ecosystems may depend on hidden electric fields. “If you suddenly took away electrostatics, I don’t think you’d get a mass extinction,” England said. “But I think we’d be surprised by how many animals would have to adapt to not using it.”</p><p>Electrostatic forces act on a scale of millimeters and centimeters, but their collective impact could be much larger. For instance, social bees such as bumblebees collect food for other colony members and larvae. Foragers make hundreds of decisions about flowers every day, and many other bees depend on those decisions. “What we think of as a fairly subtle difference on an individual level—being able to detect the flower just a second faster—could be quite significant for them evolutionarily,” said Dornhaus, who studies how bees interact with flowers.</p><p>If static charges aid pollination, they could shift plant evolution too. “Maybe some fundamental features of flowers are actually just in service of generating the correct electrostatic field,” Dornhaus said, “and because we can’t see them, we’ve ignored that whole dimension of a flower’s life.” The idea isn’t so far-fetched: In 2021, Robert’s team observed petunias releasing more <a href="https://doi.org/10.1007/s00114-021-01740-2" target="_blank">compounds that attract bugs</a> around beelike electric fields. This suggests that flowers wait until a pollinator is nearby to actively lure them closer, Robert said.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>“Humans are very visually oriented, so we tend to emphasize flowers that are showy and large,” Dornhaus said. But we already know that flowers transmit strong invisible signals, like scents or ultraviolet patterns. “It may well be that for some flowers, the electric field is actually a more prominent signal to bees than color is.”</p><p>However, evolutionary details surrounding electrostatic ecology remain murky at best. “It’s amazing, really, how little we know,” said Wainwright, the insect evolutionary ecologist. Even within better-understood visual and acoustic systems, ecologists are only beginning to connect evolutionary dots.</p><p>Because electrostatics has flown under the radar, England worries that humans unknowingly hinder the ability of animals to use these forces. “We’re spitting electrostatic stuff into the environment all the time,” he said. Electronic devices, appliances, power lines, fertilizers, and even clothing bear static charges. “If [insects are] sensitive to the wingbeat of a wasp, they’re probably sensitive to a power line, and it might be messing up that entire system.”</p><p>Since completing his doctoral work, England now studies animal vision as a postdoctoral researcher with Berlin’s Natural History Museum. He hopes to one day run his own lab to explore these conservation questions and discover new cases of aerial electroreception or electrostatic behaviors, such as mating.</p><p>“The dream would be that aerial electrostatic sensing is well known and considered to be a regular part of the sensory repertoire of animals,” he said. Realizing that dream will take more research that seeks out the evolutionary secrets of critters far smaller than us, and thereby enlarges our world.</p><hr><p><a href="https://www.quantamagazine.org/the-hidden-world-of-electrostatic-ecology-20240930/"><em>Original story</em></a> <em>reprinted with permission from</em> <a href="https://www.quantamagazine.org/">Quanta Magazine</a>, <em>an editorially independent publication of the</em> <a href="https://www.simonsfoundation.org/"><em>Simons Foundation</em></a> <em>whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.</em></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An indie studio created a game based on Stanislaw Lem's novel (128 pts)]]></title>
            <link>https://invinciblethegame.com/?hn</link>
            <guid>41978246</guid>
            <pubDate>Tue, 29 Oct 2024 00:52:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://invinciblethegame.com/?hn">https://invinciblethegame.com/?hn</a>, See on <a href="https://news.ycombinator.com/item?id=41978246">Hacker News</a></p>
Couldn't get https://invinciblethegame.com/?hn: Error: aborted]]></description>
        </item>
        <item>
            <title><![CDATA[What's New in POSIX 2024 (209 pts)]]></title>
            <link>https://blog.toast.cafe/posix2024-xcu</link>
            <guid>41978197</guid>
            <pubDate>Tue, 29 Oct 2024 00:42:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.toast.cafe/posix2024-xcu">https://blog.toast.cafe/posix2024-xcu</a>, See on <a href="https://news.ycombinator.com/item?id=41978197">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <article>
        <h2 id="toc">Table of Contents</h2>
<ul id="toc-list">
<li>
<a href="#highlights" rel="nofollow">Highlights</a><ul>
<li>
<a href="#handling-of-filenames-in-shell" rel="nofollow">Handling of Filenames in Shell</a></li>
<li>
<a href="#modern-c" rel="nofollow">Modern C</a></li>
<li>
<a href="#limits--cooperation" rel="nofollow">Limits &amp; Cooperation</a></li>
<li>
<a href="#makefiles" rel="nofollow">Makefiles</a></li>
<li>
<a href="#logging" rel="nofollow">Logging</a></li>
<li>
<a href="#internationalization" rel="nofollow">Internationalization</a></li>
<li>
<a href="#minor-changes" rel="nofollow">Minor Changes</a></li>
</ul>
</li>
<li>
<a href="#changes-index" rel="nofollow">Changes Index</a></li>
</ul>
<p>In the 1950s, computers did not really interoperate. ARPANET has not yet happened (that would become a thing in the 60s), and every operating system was typically tied to the hardware that was meant to run on. Most communication actually happened over telephone, and no company was more present in that space than the Bell System. Unfortunately, the way they were so present was through exclusive supply contracts (with its subsidiary Western Electric) and a vast array of patents that it would refuse to license to competitors. So they got an antitrust suit aimed at them, which after seven years of litigation culminated in the 1956 consent decree. The Bell System was broken up, obliged to license all of its patents royalty-free, and barred from entering any industry other than telecommunications. So they made Unix.</p>
<p>Unix was unique, because the focus was on the software (since Bell couldn’t compete in this space anyway, as per the above). An evolution of Multics, it was developed on a PDP-7 (by cross-compiling). They then ported a compiler-compiler to it, leading to the development of B. Once their internal needs outgrew the PDP-7, it got ported to the PDP-11, and gained full typesetting capabilities. Gaining some traction internally, when Bell acquired other PDP-11s, instead of running DEC’s own OS for the machine, they simply ran Unix on it. This has led to the rewrite of the OS in C, a higher level (comparatively, of course) language, which enabled the porting of it to other machines (like the Interdata 7/32 and 8/32). Interest grew, and Bell (not being allowed to turn Unix into a product) simply shipped it at manufacturing cost for the media. Notably, ARPANET used it (see: RFC 681).</p>
<p>In the early 1980s, Unix had become a univeral operating system, used on virtually every serious machine. Then, AT&amp;T got hit by an antitrust suit again. The exact details matter less, but freed it from the old restriction. System V immediately turned into a product, almost killing it. That very year, the GNU project was created, and the BSD project was started in Berkeley. Having grown accustomed to interoperability (since up until that point, there was only really one serious Unix), several standardization attempts were created. The System V Interface Definition was the AT&amp;T one, Europe created the X/Open consortium of Single UNIX Specification fame, and the IEEE put out POSIX. These latter two would eventually merge and become equivalent, developed by the Austin Group, defining the only interface said to be universally interoperable on the OS level that we have to this day.</p>
<p>As of the previous release of POSIX, the Austin Group gained more control over the specification, having it be more working group oriented, and they got to work making the POSIX specification more modern. POSIX 2024 is the first release that bears the fruits of this labor, and as such, the changes made to it are particularly interesting, as they will define the direction of the specification going forwards. This is what this article is about!</p>
<p>Well, mostly. POSIX is composed of a couple of sections. Notably XBD (Base Definitions, which talk about things like what a file is, how regular expressions work, etc), XSH (System Interfaces, the C API that defines POSIX’s internals), and XCU (which defines the shell command language, and the standard utilities available for the system). There’s also XRAT, which explains the rationale of the authors, but it’s less relevant for our purposes today. XBD and XRAT are both interesting as context for XSH and XCU, but those are the real meat of the specification. This article will focus on the XCU section, in particular the utilities part of that section. If you’re more interested in the XSH section, there’s an excellent summary page by <a href="https://sortix.org/" rel="nofollow">sortix’s</a> Jonas Termansen that you can read <a href="https://sortix.org/blog/posix-2024/" rel="nofollow">here</a>.</p>
<h2 id="highlights">Highlights <a href="#highlights" rel="nofollow">#</a></h2>
<h3 id="handling-of-filenames-in-shell">Handling of Filenames in Shell <a href="#handling-of-filenames-in-shell" rel="nofollow">#</a></h3>
<p>One of the most common errors in shell scripts when working with files tends to be the presumption that the newline character (<code>\n</code>) will not be present in the filename. Consider, for example, wanting to do some processing of files in a directory, processing the most recently modified ones first, with some custom break condition. The most common (naive) way of implementing this looks like this<sup id="fnref:1"><a href="#fn:1" rel="nofollow">1</a></sup>:</p>
<pre><code><span><span>1</span><span>ls -t <span>|</span> <span>while</span> <span>read</span> -r f<span>;</span> <span>do</span>
</span></span><span><span>2</span><span>	<span># if my condition; then break; fi</span>
</span></span><span><span>3</span><span>	<span># do something with $f</span>
</span></span><span><span>4</span><span><span>done</span>
</span></span></code></pre><p>After all, <code>read(1p)</code> reads logical lines from stdin into a variable, and <code>ls(1p)</code> outputs one entry per line. The problem is that pathnames<sup id="fnref:2"><a href="#fn:2" rel="nofollow">2</a></sup> (as per section <code>3.254</code> of POSIX 2024) are just strings (meaning they can contain any bytes except the NUL character), meaning it’s incorrect to even treat it as a character string, let alone something you can put in a newline-separated form. As such, the correct solution, historically, has been to loop over the files in some other way (such as wildcards, which aren’t subject to expansion, or using <code>find(1p)</code>), then sort them, then run on the sorted datatype. This question is probably one of the most talked about in shell. POSIX 2024 addresses this issue in two ways.</p>
<h4 id="the-null-option">The Null Option <a href="#the-null-option" rel="nofollow">#</a></h4>
<p><code>find(1p)</code> now supports the <code>-print0</code> primary, which makes <code>find</code> use the NUL character as a separator. To go along with it, <code>xargs(1p)</code> now supports the <code>-0</code> argument, which reads arguments expecting them to be separated with NUL characters. Finally, for (most) other usecases, <code>read(1p)</code> now supports the <code>-d</code> (delimiter) argument, where <code>-d ''</code> means the NUL character is the delimiter. This is a non-ideal resolution though. Previous POSIX releases have considered <code>-print0</code> before, but never ended up adopting it because using a null terminator meant that any utility that would need to process that output would need to have a new option to parse that type of output.</p>
<p>More precisely, this approach does not resolve our original problem. <code>xargs(1p)</code> can’t sort, and therefore we still have to handle that logic separately, unless <code>sort(1p)</code> also grows this support, even after <code>read(1p)</code>. This problem continues with every other type of use-case. Importantly, it breaks the interoperability that POSIX was made to uphold.</p>
<p>Thankfully, there is the second way that they’re fixing this issue.</p>
<h4 id="the-nuclear-option">The Nuclear Option <a href="#the-nuclear-option" rel="nofollow">#</a></h4>
<p>We’ve established that, yes, pathnames can include newlines. We have not established <em>why</em> they can do that. After some deliberation, the Austin Group could not find a single use-case for newlines in pathnames besides breaking naive scripts. Wouldn’t it be nice if the naive scripts were just correct now? Ok, that might be a bit much all at once. We’re heading there though!
A bunch of C functions<sup id="fnref:3"><a href="#fn:3" rel="nofollow">3</a></sup> are now encouraged to report <code>EILSEQ</code> if the last component of a pathname to a file they are to create contains a newline (put differently, they’re to error out instead of creating a filename that contains a newline).</p>
<p>As for the utilities, the following utilities are now either encouraged to error out if they are to create a filename that contains a newline, and/or encouraged to error out if they are about to print a pathname that contains a newline in a context where newlines may be used as a separator: <code>admin(1p)</code>, <code>ar(1p)</code>, <code>awk(1p)</code>, <code>basename(1p)</code>, <code>cd(1p)</code>, <code>cksum(1p)</code>, <code>cmp(1p)</code>, <code>command(1p)</code>, <code>compress(1p)</code>, <code>cp(1p)</code>, <code>csplit(1p)</code>, <code>ctags(1p)</code>, <code>cxref(1p)</code>, <code>dd(1p)</code>, <code>df(1p)</code>, <code>diff(1p)</code>, <code>dirname(1p)</code>, <code>du(1p)</code>, <code>ed(1p)</code>, <code>ex(1p)</code>, <code>file(1p)</code>, <code>find(1p)</code>, <code>fuser(1p)</code>, <code>get(1p)</code>, <code>grep(1p)</code>, <code>hash(1p)</code>, <code>head(1p)</code>, <code>ipcs(1p)</code>, <code>link(1p)</code>, <code>ln(1p)</code>, <code>localedef(1p)</code>, <code>ls(1p)</code>, <code>m4(1p)</code>, <code>mailx(1p)</code>, <code>make(1p)</code>, <code>man(1p)</code>, <code>mkdir(1p)</code>, <code>mkfifo(1p)</code>, <code>mv(1p)</code>,<code>nm(1p)</code>, <code>patch(1p)</code>, <code>pax(1p)</code>, <code>prs(1p)</code>, <code>pws(1p)</code>, <code>rm(1p)</code>, <code>rmdel(1p)</code>, <code>sact(1p)</code>, <code>sccs(1p)</code>, <code>sh(1p)</code>, <code>sort(1p)</code>, <code>split(1p)</code>, <code>tee(1p)</code>, <code>touch(1p)</code>, <code>type(1p)</code>, <code>uncompress(1p)</code>, <code>unget(1p)</code>, <code>uniq(1p)</code>,<code>uucp(1p)</code>, <code>uudecode(1p)</code>, <code>val(1p)</code>, <code>vi(1p)</code>, <code>wc(1p)</code>, <code>what(1p)</code>, <code>yacc(1p)</code>, and <code>zcat(1p)</code>.</p>
<p>Furthermore, <code>sh(1p)</code> talks about future direction, which may require the above to be treated as errors, and <code>pr(1p)</code> has a new section talking about “problematic pathnames” (since, for its use-case, tabs and vertical tabs are also problem-causing).</p>
<p>This is a much better solution, even in its current form. Unless your threat model includes attackers targeting you in particular (which, for example, immediately excludes all “home use” scripts), you can reasonably expect people to be discouraged from creating newline-containing characters, where before it might have been perceived as a “clever hack”. You can’t rely on your system enforcing those files not exist, but this is a major step in that direction.</p>
<h4 id="tldr">TL;DR <a href="#tldr" rel="nofollow">#</a></h4>
<p>While code like <code>ls -t | while read -r f</code> isn’t strictly correct yet, it’s likely to become strictly correct eventually. It’s also much more reasonable to opt into this early, unless you’re writing software with security requirements, are deleting files based on inputs, or similar.</p>
<h3 id="modern-c">Modern C <a href="#modern-c" rel="nofollow">#</a></h3>
<p>C has come a pretty long way in the last half-century, but for most intents and purposes, we haven’t been able to really benefit from it. Did you know that since <code>c11</code> we actually have built-in unicode support via <code>&lt;uchar.h&gt;</code> (ISO/IEC TR 19769:2004)? Most modern programs can’t actually utilize this, because they target <code>c89</code> (often incorrectly called “ANSI C”) or (if you’re lucky) <code>c99</code>. Why does this happen?</p>
<p>Well, when you’re building a new C program, you must decide what version of C to target. Target something too new, and no one will be able to build it. An example of this is <a href="https://github.com/aristocratos/btop" rel="nofollow">btop++</a>, which targetted some newer C++ features (notably <code>&lt;ranges&gt;</code>) that at the time of its publishing LLVM simply did not support: <code>libc++</code> simply didn’t have them yet (at least not in a stable format available on most distributions), and you couldn’t use gcc’s <code>libstdc++</code> because its <code>&lt;ranges&gt;</code> implementation depended on concepts (which LLVM also did not have yet).</p>
<p>As such, what you do, is you look at the platforms you want your program to run on, and try to figure out what the least common denominator would be. It just so happens that for the longest time, that denominator would be <code>c89</code>. For a little while now, it’s been <code>c99</code>. As for why that is, POSIX is a large part. You see, up until POSIX 2024, POSIX required that the <code>c89</code> compiler be present on the system. If you have <code>c89</code> you’re compliant, and if you do not, you are not. Most operating systems try to be POSIX compliant, and so it becomes a typical expectation (so you don’t have to worry about not having C at all, something other languages do have to worry about). This broad presumption of availability also pushes the embedded developers to provide something along those lines as well (setting the expectation of expectations), so most microcontrollers will have a <code>c89</code> (or again, recently, <code>c99</code>) toolchain available for them.</p>
<p>In short, application authors will tend not to target something until it’s fairly common, unless there’s a disproportionate advantage for their specific use-case (such as with <code>c99</code> over <code>c89</code>). What’s fairly common is strongly influenced by what is pseudo-guaranteed by the only portable standard we have.</p>
<p>Anyway, POSIX 2024 now requires <code>c17</code>, and does not require <code>c89</code>. Furthermore, the rationale mentions that future editions will not require <code>c17</code>, but will simply require whatever C specification version is the most modern and already implemented by major toolchains. So going forward, it’ll be much easier to justify using actually modern C for your new projects, and we can expect more and more embedded tools to provide modern C versions (something we’re already seeing, especially on microcontrollers that are based on ARM or RISC-V).</p>
<h3 id="limits--cooperation">Limits &amp; Cooperation <a href="#limits--cooperation" rel="nofollow">#</a></h3>
<p>Operating systems impose limits (often arbitrary) on what runs inside of them, and your applications (and scripts, and interactive usage) may also want to impose some limits and cooperation on what you run. As such, it’s important that you be able to interact with these limits. This is what the <code>nice(1p)</code>, <code>renice(1p)</code>, and <code>ulimit(1p)</code> utilities are meant to do.</p>
<p>Unfortunately, <code>renice(1p)</code> only worked in absolutes, and <code>ulimit(1p)</code> only let you set a maximum write size for files (and didn’t differentiate between hard and soft limits), and was only available as part of the XSI extension.</p>
<p>With POSIX 2024, <code>ulimit(1p)</code> now supports reporting hard and soft limits and defines how those are used and interact. Additionally, the core image size, data segment size, open file descriptor amount, stack size, cpu time<sup id="fnref:4"><a href="#fn:4" rel="nofollow">4</a></sup>, and address space limits now exist. This means that you can now (or rather, in the near future) reasonably rely on those existing and actually make use of them in portable scripts. <code>renice(1p)</code> is also updated to support the <code>-n</code> option (just like <code>nice(1p)</code>) to change the niceness value relatively.</p>
<p>Finally, we get a new utility: <code>timeout(1p)</code>. A lot of tools over the years have added options to handle their own timeouts (<code>curl(1)</code> in particular comes to mind, having several different types of timeouts for various use-cases), but with <code>timeout(1p)</code> you don’t need those (except for the added flexibility) anymore. It even handles child processes (in several implementation defined ways) and (importantly) lets you customize the signal and send a secondary <code>SIGKILL</code> after a secondary timeout.</p>
<h3 id="makefiles">Makefiles <a href="#makefiles" rel="nofollow">#</a></h3>
<p><code>make(1p)</code> remains the default build system to this day. Or at least sort of. Most people tend to write large scripts that wrap around make for various reasons, but in the end they will tend to produce a <code>Makefile</code> (though ninja has been gaining a lot of traction). Let’s take a look at a typical example to explain what the improvements are.</p>
<p>Our use-case is simple: we have a bunch of <code>.c</code> files in <code>./</code>. We want to compile and link them together. We also have a dependency (let’s say it’s libcurl) that requires some additional <code>CFLAGS</code> and <code>LDFLAGS</code>, which we query using <code>pkg-config</code> (well, I’m going to use <code>pkgconf</code>, it’s compatible). Importantly, we’re lazy in that we don’t want to specify every <code>.c</code> file in the directory in our Makefile. We also want to be able to clean our <code>.o</code> files without resorting to something like <code>git clean -fx</code> (that might clean some temporary artifacts that we do want to keep). With GNU Make, that might look something like so:</p>
<pre><code><span><span> 1</span><span><span>SRC</span> <span>:=</span> <span>$(</span>wildcard *.c<span>)</span>
</span></span><span><span> 2</span><span><span>OBJ</span> <span>:=</span> <span>$(</span>SRC:.c<span>=</span>.o<span>)</span>
</span></span><span><span> 3</span><span>
</span></span><span><span> 4</span><span><span>CC</span> <span>?=</span> cc
</span></span><span><span> 5</span><span>
</span></span><span><span> 6</span><span><span>CFLAGS</span>  <span>?=</span> -Os -pipe
</span></span><span><span> 7</span><span><span>LDFLAGS</span> <span>?=</span> -Wl,-O2
</span></span><span><span> 8</span><span><span>LIBS</span>    <span>?=</span>
</span></span><span><span> 9</span><span>
</span></span><span><span>10</span><span><span>PKGCONF</span> <span>?=</span> pkgconf
</span></span><span><span>11</span><span>
</span></span><span><span>12</span><span><span>CURLC</span> <span>!=</span> <span>$(</span>PKGCONF<span>)</span> --cflags libcurl
</span></span><span><span>13</span><span><span>CURLL</span> <span>!=</span> <span>$(</span>PKGCONF<span>)</span> --libs   libcurl
</span></span><span><span>14</span><span>
</span></span><span><span>15</span><span><span>CFLAGS</span>  <span>:=</span> <span>$(</span>CFLAGS<span>)</span>  <span>$(</span>CURLC<span>)</span>
</span></span><span><span>16</span><span><span>LDFLAGS</span> <span>:=</span> <span>$(</span>LDFLAGS<span>)</span> <span>$(</span>CURLL<span>)</span>
</span></span><span><span>17</span><span>
</span></span><span><span>18</span><span><span>myprog</span><span>:</span> <span>$(</span><span>OBJ</span><span>)</span>
</span></span><span><span>19</span><span>	<span>$(</span>CC<span>)</span> -o <span>$@</span> <span>$(</span>LDFLAGS<span>)</span> <span>$(</span>LIBS<span>)</span> $^
</span></span><span><span>20</span><span>
</span></span><span><span>21</span><span><span>.PHONY</span><span>:</span> <span>clean</span>
</span></span><span><span>22</span><span><span>clean</span><span>:</span>
</span></span><span><span>23</span><span>	rm -f <span>$(</span>OBJ<span>)</span> myprog
</span></span></code></pre><p>This will not work on anything but GNU Make. MacOS make<sup id="fnref:5"><a href="#fn:5" rel="nofollow">5</a></sup> won’t be happy with the <code>!=</code> used for <code>CURLC</code>, while bsdmake and bmake won’t be happy with <code>$^</code>. POSIX make would be unhappy with the <code>:=</code>, <code>wildcard</code> and <code>.PHONY</code>. Similarly, if we targetted <code>bmake</code> initially, the result would not properly run on <code>gmake</code>, and so on. The various implementations are mutually incompatible in diverging ways, since the POSIX implementation lacked critical features required for writing such small (and the vast majority of Makefiles <em>should</em> be this small) Makefiles.</p>
<p>While there’s still no good solution for the <code>$(wildcard *.c)</code> portion of this<sup id="fnref:6"><a href="#fn:6" rel="nofollow">6</a></sup>, the following, annotated with comments for changes, should now work in strict POSIX compatibility<sup id="fnref:7"><a href="#fn:7" rel="nofollow">7</a></sup>:</p>
<pre><code><span><span> 1</span><span><span># .POSIX: is meant to make the Make implementation behave as though
</span></span></span><span><span> 2</span><span><span># it is standard POSIX-make, since there may be conflicts
</span></span></span><span><span> 3</span><span><span></span><span>.POSIX</span><span>:</span>
</span></span><span><span> 4</span><span><span># we use ::= here, since POSIX does not define :=.
</span></span></span><span><span> 5</span><span><span># we also strictly enumerate the sources
</span></span></span><span><span> 6</span><span><span></span><span>SRC </span><span>::</span>= <span>one</span>.<span>c</span> <span>two</span>.<span>c</span>
</span></span><span><span> 7</span><span><span>OBJ </span><span>::</span>= <span>$(</span><span>SRC</span>:.<span>c</span>=.<span>o</span><span>)</span>
</span></span><span><span> 8</span><span>
</span></span><span><span> 9</span><span><span>CC</span> <span>?=</span> cc
</span></span><span><span>10</span><span>
</span></span><span><span>11</span><span><span>CFLAGS</span>  <span>?=</span> -Os -pipe
</span></span><span><span>12</span><span><span>LDFLAGS</span> <span>?=</span> -Wl,-O2
</span></span><span><span>13</span><span><span>LIBS</span>    <span>?=</span>
</span></span><span><span>14</span><span>
</span></span><span><span>15</span><span><span>PKGCONF</span> <span>?=</span> pkgconf
</span></span><span><span>16</span><span>
</span></span><span><span>17</span><span><span>CURLC</span> <span>!=</span> <span>$(</span>PKGCONF<span>)</span> --cflags libcurl
</span></span><span><span>18</span><span><span>CURLL</span> <span>!=</span> <span>$(</span>PKGCONF<span>)</span> --libs   libcurl
</span></span><span><span>19</span><span>
</span></span><span><span>20</span><span><span># ditto re: ::=
</span></span></span><span><span>21</span><span><span></span><span>CFLAGS  </span><span>::</span>= <span>$(</span><span>CFLAGS</span><span>)</span>  <span>$(</span><span>CURLC</span><span>)</span>
</span></span><span><span>22</span><span><span>LDFLAGS </span><span>::</span>= <span>$(</span><span>LDFLAGS</span><span>)</span> <span>$(</span><span>CURLL</span><span>)</span>
</span></span><span><span>23</span><span>
</span></span><span><span>24</span><span><span>myprog</span><span>:</span> <span>$(</span><span>OBJ</span><span>)</span>
</span></span><span><span>25</span><span>	<span>$(</span>CC<span>)</span> -o <span>$@</span> <span>$(</span>LDFLAGS<span>)</span> <span>$(</span>LIBS<span>)</span> $^
</span></span><span><span>26</span><span>
</span></span><span><span>27</span><span><span>.PHONY</span><span>:</span> <span>clean</span>
</span></span><span><span>28</span><span><span>clean</span><span>:</span>
</span></span><span><span>29</span><span>	rm -f <span>$(</span>OBJ<span>)</span> myprog
</span></span></code></pre><p>That’s very few changes! Importantly, <code>gmake</code> can already handle this, meaning that by targeting this feature set you are strictly improving compatibility.</p>
<p>To be very specific, POSIX 2024 added support for the <code>$^</code> and <code>$+</code> internal macros, <code>::=</code>, <code>:::=</code>, <code>!=</code>, <code>?=</code>, and <code>+=</code> macro assignment forms, silent includes via <code>-include</code>, <code>.NOTPARALLEL</code>, <code>.PHONY</code>, and <code>.WAIT</code> special targets (of which I did not cover the parallelism ones, as those will typically be mostly useful to meta build systems), and other less important changes that will be listed out in full below.</p>
<h3 id="logging">Logging <a href="#logging" rel="nofollow">#</a></h3>
<p>Our computers have more and more cores. In early 2017 (when the previous version of the standard was being finalized), most consumer grade hardware still maxed out at 4 cores (likely with SMT). This was also the segment of the market most likely to have background batch processing done in shell (as more enterprise-grade uses tend to write in a programming language that can integrate with their numerous external APIs). As such, while background processes were certainly common, it wasn’t as much of a common expectation that one might be doing some major processing (e.g. video re-encoding) in the background while performing other tasks. Of course, right after that point, in March 2017, the first generation of AMD Ryzen CPUs dropped on the scene and put processors with as many as 16 threads into the hands of consumers at more than reasonable prices. Today, in 2024, it’s difficult to buy a new workstation cpu with fewer than 12 threads, making the abovementioned scenarios all the more common.</p>
<p>The original specification of <code>logger(1p)</code> was written with a fairly uncommon, albeit necessary, use-case in mind. Today, such use-cases are much more common, and could be even more common if logging was easier to do correctly<sup id="fnref:8"><a href="#fn:8" rel="nofollow">8</a></sup>. This original specification basically said that <code>logger(1p)</code> takes arguments like echo, but instead of outputting the text into stdout, it does so into syslog. It also means that logging the output of commands is unduly complicated.</p>
<p>In POSIX 2024, <code>logger(1p)</code> becomes a more fully-qualified command, with arguments and stdin interpretation. Notably, if there are no non-option arguments, <code>logger(1p)</code> will read the contents to log from stdin. It is also possible to ask the contents of a specific file to be logged using <code>-f</code>. Additionally, the syslog priority can be specified with <code>-p</code>, the <code>pid</code> of the logger process on each message using <code>-i</code>, and a syslog tag string using <code>-t</code>. Of additional importance, every non-empty line in the input or file shall be logged as a separate message, which means that the <code>-i</code> argument can be used to perform bulk logging where you can differentiate between failed runs.</p>
<h3 id="internationalization">Internationalization <a href="#internationalization" rel="nofollow">#</a></h3>
<p>Different people speak different languages, and it’s important to be able to translate your program for those. While you’re writing a C program or something along those lines, you can always reach for a library that you link into (such as GNU intl). While you’re writing a shell script, however, your options tend to be far more limited, since you can’t distribute it alongside the script very easily. Wouldn’t it be helpful if the standard everyone follows to various degrees actually settled on whatever interface was the most used in practice? Anyway, POSIX 2024 has adopted the <code>gettext</code> suite ala GNU, both as a system interface (<code>gettext(3p)</code> and co) and in the CLI (<code>gettext(1p)</code>, <code>ngettext(1p)</code>, <code>xgettext(1p)</code>, <code>msgmft(1p)</code>).</p>
<p>Since the target audience for this article is primarily shell people and advanced end-users, I’ll quickly go over the utilization in a shell context. <em>If you’re already familiar with the basics of GNU’s implementation, you can skip the rest of this section!</em> Translations are organized by message IDs (<code>msgid</code>) which can then be turned into arbitrary message strings (<code>msgstr</code>). These are encoded in a Portable Object file (<code>.po</code>), which you compile into a Machine Object file (<code>.mo</code>) using <code>msgfmt(1p)</code>. You then place them on your system in such a way that the <code>gettext(1p)</code> utilities will be able to find them, and the typical <code>LANG</code>/<code>LANGUAGE</code>/<code>LC_ALL</code>/<code>LC_MESSAGES</code> mechanism will get you the correct translations.</p>
<p>For the purposes of this minimal example, we’re going to write a very small program that talks about pets. The program will either print <code>you like cats</code>, <code>you like dogs</code>, or <code>you have %d pets</code> (where the <code>%d</code> will be used for <code>printf</code> output). We’ll also demonstrate how special-case plural forms work. We’ll be making a French and an English translation, and everything will be done relative to a directory of your choosing, that I will refer to as <code>$PWD</code> or <code>.</code> interchangeably. We’ll start by writing our two annotated <code>.po</code> files.</p>
<pre><code># ./en/pets.po : the filename and location is arbitrary
# empty messageid and str signals the header
# different languages deal with plural forms differently
# English only has a special case for "one"
# the `plural=` section is a C-like conditional expression
msgid ""
msgstr ""
"Content-Type: text/plain; charset=utf-8\n"
"Plural-Forms: nplurals=2; plural=n != 1;\n"
"Language: en\n"

# the IDs here are identical the messages
# since English is the source language for us
msgid "you like cats"
msgstr "you like cats"

msgid "you like dogs"
msgstr "you like dogs"

# note that if a translation isn't found
# the msgid is used as is
msgid "you have a pet"
msgid_plural "you have %d pets"
msgstr[0] "you have a pet"
msgstr[1] "you have %d pets"
</code></pre>
<pre><code># ./fr/pets.po
# we'll have a special case for 0 (aucun) and 1 (un)
# then the rest will be general case plural
msgid ""
msgstr ""
"Content-Type: text/plain; charset=utf-8\n"
"Plural-Forms: nplurals=3; plural=(n==0)?0: (n==1)?1: 2\n"

msgid "you like cats"
msgstr "vous aimez les chats"

msgid "you like dogs"
msgstr "vous aimez les chiens"

# I translated "pet" as "little companion"
# as there's no satisfactory direct translation
msgid "you have a pet"
msgid_plural "you have %d pets"
msgstr[0] "vous n'avez pas de petits compagnons"
msgstr[1] "vous avez un petit compagnon"
msgstr[2] "vous avez %d petits compagnons"
</code></pre>
<p>These files aren’t usable as-is. We need to compile them into <code>.mo</code> files. We’ll start by compiling them in the same directory: <code>msgfmt en/pets.po -o en/pets.mo; msgfmt fr/pets.po -o fr/pets.mo</code>. We now need to place them in a location that <code>gettext(1p)</code> and co. will be able to find it in. For those specific commands there are numerous special cases, and we’ll take advantage of those via <code>TEXTDOMAINDIR</code>. Under <code>$TEXTDOMAINDIR</code>, the system will try to look for your <code>$LC_MESSAGES</code> locale followed by <code>LC_MESSAGES</code>, then your textdomain. For convenience, we’ll make some symlinks: <code>ln -s . en/LC_MESSAGES; ln -s . fr/LC_MESSAGES</code>. We can now demonstrate the messages manually!</p>
<pre><code><span><span> 1</span><span><span>export</span> <span>TEXTDOMAINDIR</span><span>=</span><span>$PWD</span>
</span></span><span><span> 2</span><span>
</span></span><span><span> 3</span><span><span># the system will access $TEXTDOMAINDIR/${locales…}/$TEXTDOMAIN.mo</span>
</span></span><span><span> 4</span><span><span># you can avoid setting $TEXTDOMAIN if you specify it on the CLI</span>
</span></span><span><span> 5</span><span><span>export</span> <span>TEXTDOMAIN</span><span>=</span>pets
</span></span><span><span> 6</span><span>
</span></span><span><span> 7</span><span><span># we can now translate simple messages!</span>
</span></span><span><span> 8</span><span><span>LC_MESSAGES</span><span>=</span>fr gettext -s <span>'you like cats'</span>
</span></span><span><span> 9</span><span><span># =&gt; "vous aimez les chats"</span>
</span></span><span><span>10</span><span><span>LC_MESSAGES</span><span>=</span>en gettext -s <span>'you like dogs'</span>
</span></span><span><span>11</span><span><span># =&gt; "you like dogs"</span>
</span></span><span><span>12</span><span><span># if you try to access a translation that doesn't exist,</span>
</span></span><span><span>13</span><span><span># it will simply print the ID, thus why it needs to be representative</span>
</span></span><span><span>14</span><span><span>LC_MESSAGES</span><span>=</span>it gettext -s <span>'you like cats'</span>
</span></span><span><span>15</span><span><span># =&gt; "you like cats"</span>
</span></span><span><span>16</span><span>
</span></span><span><span>17</span><span><span># for plural forms, you use ngettext(1p)</span>
</span></span><span><span>18</span><span><span># because we probably also want to show the real number,</span>
</span></span><span><span>19</span><span><span># ngettext can output printf-compatible format strings</span>
</span></span><span><span>20</span><span><span># so we'll write a wrapper</span>
</span></span><span><span>21</span><span><span># $1: locale; $2: msgid; $3: msgid_plural; $4: quantity</span>
</span></span><span><span>22</span><span>plural<span>()</span> <span>{</span>
</span></span><span><span>23</span><span>	<span>printf</span> <span>"</span><span>$(</span><span>LC_MESSAGES</span><span>=</span><span>"</span><span>$1</span><span>"</span> ngettext <span>"</span><span>$2</span><span>"</span> <span>"</span><span>$3</span><span>"</span> <span>"</span><span>$4</span><span>"</span><span>)</span><span>\n"</span> <span>"</span><span>$4</span><span>"</span>
</span></span><span><span>24</span><span><span>}</span>
</span></span><span><span>25</span><span><span># we can now demonstrate how the translation system adapts to plurals</span>
</span></span><span><span>26</span><span><span>for</span> i in <span>$(</span>seq <span>0</span> 2<span>)</span><span>;</span> <span>do</span>
</span></span><span><span>27</span><span>	plural en <span>'you have a pet'</span> <span>'you have %d pets'</span> <span>$i</span>
</span></span><span><span>28</span><span><span>done</span>
</span></span><span><span>29</span><span><span># =&gt; you have 0 pets</span>
</span></span><span><span>30</span><span><span># =&gt; you have a pet</span>
</span></span><span><span>31</span><span><span># =&gt; you have 2 pets</span>
</span></span><span><span>32</span><span>
</span></span><span><span>33</span><span><span># in French, we had a special case for 0, let's see it in action:</span>
</span></span><span><span>34</span><span><span>for</span> i in <span>$(</span>seq <span>0</span> 2<span>)</span><span>;</span> <span>do</span>
</span></span><span><span>35</span><span>	plural fr <span>'you have a pet'</span> <span>'you have %d pets'</span> <span>$i</span>
</span></span><span><span>36</span><span><span>done</span>
</span></span><span><span>37</span><span><span># =&gt; vous n'avez pas de petits compagnons</span>
</span></span><span><span>38</span><span><span># =&gt; vous avez un petit compagnon</span>
</span></span><span><span>39</span><span><span># =&gt; vous avez 2 petits compagnons</span>
</span></span><span><span>40</span><span>
</span></span><span><span>41</span><span><span># if you try to access a translation that doesn't exist,</span>
</span></span><span><span>42</span><span><span># the system will follow typical English rules, as above:</span>
</span></span><span><span>43</span><span><span>for</span> i in <span>$(</span>seq <span>0</span> 2<span>)</span><span>;</span> <span>do</span>
</span></span><span><span>44</span><span>	plural it <span>'you have a pet'</span> <span>'you have %d pets'</span> <span>$i</span>
</span></span><span><span>45</span><span><span>done</span>
</span></span><span><span>46</span><span><span># you have 0 pets</span>
</span></span><span><span>47</span><span><span># you have a pet</span>
</span></span><span><span>48</span><span><span># you have 2 pets</span>
</span></span></code></pre><p>In short, you can now rely on GNU-style <code>gettext</code> and <code>ngettext</code> utilities to be present, and write your script with the presumption that they are there. If the translation files are not installed, the message ID will be used (intelligently, in the case of plural forms), so you don’t need to worry about the possibility of them not being installed.</p>
<h3 id="minor-changes">Minor Changes <a href="#minor-changes" rel="nofollow">#</a></h3>
<p>These are changes that are relatively minor, but I still think deserve a spotlight.</p>
<ul>
<li><code>readlink(1p)</code> and <code>realpath(1p)</code> are now part of the standard, meaning you can reliably find where a symlink points to.</li>
<li><code>rm(1p)</code> takes a <code>-d</code> argument to remove empty directories too, enabling <code>rm -d *</code> and similar use-cases. You also get <code>-v</code>.</li>
<li><code>printf(1p)</code> now supports numbered conversion specs. For example, <code>printf '%2$s%1$s' a b</code> will print out <code>ba</code>.</li>
<li><code>sed(1p)</code> got several interesting upgrades, though for me the highlights are being able to use EREs like in <code>grep(1p)</code> using <code>-E</code>, as well as the <code>i</code> flag on the <code>s</code> command.</li>
<li><code>test(1p)</code> now has <code>-ef</code>, <code>-nt</code>, <code>-ot</code>. String comparisons (<code>&gt;</code> and <code>&lt;</code>) are now affected by collation.</li>
<li>There is a new category of utility, notably intrinsic utilities. These are like a special built-in that can be overridden by a user function, or as a regular built-in that cannot be substituted on the PATH nor need to be possible to exec (except for kill). It’s an important change, but it’s not very relevant for anyone that’s not writing a shell interpreter.</li>
</ul>
<h2 id="changes-index">Changes Index <a href="#changes-index" rel="nofollow">#</a></h2>
<p>If you’re here early, hi! I’ve been working on this piece (I have a good chunk of an MB in plaintext notes) since the middle of the summer. Instead of letting it continue to drag on, I decided to radically reduce the scope just to the highlights, and only the XCU Utilities section. Thanks to sortix (linked above), I feel like I can stick with that latter, but I still plan to actually write out the full changes index here, as well as go over any of the Shell Command Language changes. It’s just going to take a long time still, since I’m not interested in simply dumping out the change notifications, but rather explain every change being made (albeit not as completely as I do in the highlights section). I will update this page in-place and post a second announcement when this section is complete. Don’t expect it any time soon though (probably not until early 2025).</p>



        

        
    </article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Slight Failed: A Slight Post-Mortem (105 pts)]]></title>
            <link>https://www.colmanhumphrey.com/posts/why-slight-failed/</link>
            <guid>41977205</guid>
            <pubDate>Mon, 28 Oct 2024 22:31:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.colmanhumphrey.com/posts/why-slight-failed/">https://www.colmanhumphrey.com/posts/why-slight-failed/</a>, See on <a href="https://news.ycombinator.com/item?id=41977205">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p><span> Back </span> in late 2020, my good friend Raiden and I co-founded Slight. Over the next two and a half years, we got it off the ground, raised a pre-seed round, and got some customers on-board, but in the end we didn’t have enough traction to raise our seed round. We got close with a few investors (after I talked to many…), but getting close still means the company dies.</p><p>I wish I could point to some obvious fuck-up, so in lieu of that I’ll give my best guesses.</p><p>Before we dive in — Slight was a data “switchboard” that turned SQL queries into user-friendly interfaces for anyone who needed data. Click the details below for more.</p><details data-astro-cid-4jmmo4cq=""><summary data-astro-cid-4jmmo4cq=""><span data-astro-cid-4jmmo4cq="">Details</span> | More info on what the product was</summary><div data-astro-cid-4jmmo4cq=""><p>We found a lot of friction within companies doing something that should be simple: getting data to people, and specifically the right data.</p><p>For non-technical users, the typical assumption is that you’d send files or use the <a href="https://en.wikipedia.org/wiki/Business_intelligence" data-astro-cid-lz4bm56f="">BI</a> tool. For technical users, generally they could access the databases themselves but you’d still have to send them the query, they’d have to parameterize it themselves, and there was no central place to put the query to suit both teams. For external users, setting up anything beyond sending files was generally deemed too much hassle.</p><p>Slight solved this: someone on a data team would write a parameterized query, and we’d create the interfaces to suit each user, keeping them in sync regardless of how they wanted to interact with the data. Data teams wouldn’t have to schlep data around, and users could use spreadsheet integrations, a web UI, R/Python SDKs, an API directly, etc., along with appropriate caching and security, for internal or external use.</p><p>It was a “central data dispatcher” that built data apps from parameterized SQL.</p><p>If you’d like to see me blabber on for a while, <a href="https://www.youtube.com/watch?v=cRspkUKUgZQ" data-astro-cid-lz4bm56f="">here’s a demo</a>.</p></div></details><h2 id="did-we-make-something-people-want" data-astro-cid-u4qoyrkz="">Did We Make Something People Want?</h2><!-- { level === 5 && (
     <h5 id={id} class=""><slot /></h5>
     )}

     { level === 6 && (
     <h6 id={id} class=""><slot /></h6>
     )} --><p>I could answer this with a million “Yes, but” paragraphs. Which means the answer is no. Or if I’m being generous: not enough.</p><p><s>Yes, but</s> Large companies didn’t want it enough to deal with our lack of “big company features” (enterprise SSO, compliance certifications, support for legacy databases, or just the long tail of databases). They saw value, but not enough to justify the internal friction.</p><p><s>Yes, but</s> Startups didn’t want it enough, because we were a solution to organizational friction they hadn’t hit or just wasn’t top of mind. Slacking around SQL snippets, schlepping around CSVs, devs having minor inconsistencies when running a bunch of ad-hoc queries: problems for sure, but not major ones for a small company.</p><p><s>Yes, but</s> Not enough in a crowded space. <s>Yes, but</s> Not enough in tough start-up environment. <s>Yes, but</s> Not enough to deal with UX papercuts.</p><p>We were nearly in no-man’s land. Our ideal customer was probably a fast-growing Series B company hitting these pain points for the first time — too narrow a target to systematically find before they settle for good-enough alternatives.</p><p>Maybe we were just a few needed tweaks or features away. Maybe we got the distribution wrong (pretty important in <a href="https://en.wikipedia.org/wiki/Business-to-business" data-astro-cid-lz4bm56f="">B2B</a>). Maybe we just needed another six months. But at some point you have to admit you either didn't build something people wanted enough, or at least not enough to buy time to figure out the rest.</p><h2 id="distribution" data-astro-cid-u4qoyrkz="">Distribution</h2><!-- { level === 5 && (
     <h5 id={id} class=""><slot /></h5>
     )}

     { level === 6 && (
     <h6 id={id} class=""><slot /></h6>
     )} --><p>I think this is where I most clearly dropped the ball. We never fully solved a very important question: <strong>how will your product go from “cool software” etc. to actually being in people’s hands?</strong> I don’t just mean a demo someone can play with, I’m assuming here your product is ready enough to onboard customers. What I mean is: what’s the end-to-end process for a company to get to a place where they’re using your product in a serious way?</p><p>I know I got this badly wrong, because in truth I don’t even know the answer now. If you squint, we looked a little bit like a <a href="https://en.wikipedia.org/wiki/Business_intelligence" data-astro-cid-lz4bm56f="">BI</a> tool, so we mostly followed the BI distribution playbook. This was a mistake: we intentionally avoided some “core” BI features, e.g. dashboards, and so companies couldn’t replace their BI tool with Slight. That would be fine, but we didn’t really offer a vision of how to go from pre-Slight to using Slight in production. No clear wedge, no very obvious first use case, no natural expansion path. I didn’t properly understand which stakeholder needed to see what value.</p><p>When someone asked "how do we get started?", we had a technical answer (“connect your database, write some queries, data for all!”) but no story about which team should champion it first, or which problem to solve first. Data teams? Product teams? Analysts? We had some answers, but not <em>the</em> answer. We had pitches for individual teams that worked well, but we never nailed down the way companies should adopt Slight.</p><p>If you don’t know how your product will be adopted, how would your customers know?</p><p>When things were sort of but not quite working, I made the stupid mistake of just working harder and harder to on-board companies. Instead, we should have sat down and mapped out ways to properly experiment with our approach. Maybe simplifying to a single clear use-case, or finding a completely different initial wedge, or focusing on specific verticals. Near the end I actually did try some of this (specifically around sharing data with customers/externally), but made a hilarious error: I focused the changes on my fundraising efforts instead of actual customer acquisition.</p><h3 id="open-source" data-astro-cid-u4qoyrkz="">Open Source?</h3><!-- { level === 5 && (
     <h5 id={id} class=""><slot /></h5>
     )}

     { level === 6 && (
     <h6 id={id} class=""><slot /></h6>
     )} --><p>Slight could have been open source. One analogy for what we were making is “<a href="https://www.getdbt.com/" data-astro-cid-lz4bm56f="">dbt</a>, but on the other side of the data warehouse”: dbt gets you from raw data to nice datasets, while Slight is a data switchboard that routes datasets to all types of users (technical or not, internal and external).</p><p>Open source might have been a better path to adoption — particularly since we were building (data-)engineer-focused infrastructure. But it would have meant a completely different business and go-to-market strategy.</p><p>I expect something like Slight will eventually exist as an open source tool. The concept is solid — we’re already seeing many data tools expand into this space. After the company shut down, I initially planned to open source Slight, but the reality of post-startup life (including a baby no less!) meant this didn’t happen.</p><h2 id="money" data-astro-cid-u4qoyrkz="">Money…</h2><!-- { level === 5 && (
     <h5 id={id} class=""><slot /></h5>
     )}

     { level === 6 && (
     <h6 id={id} class=""><slot /></h6>
     )} --><p>I think at this stage, there’s enough written about this that many people have been disabused of the notion that “oh shucks, we just happened to run out of money” is a standard risk for startups. It’s a symptom.</p><blockquote><p>The #1 cause of startup death is making something no one wants. The #2 cause is spending too much. Those two account for so many deaths that I'm not even sure what #3 is.</p><p>If you merely make something people want and don't spend too much, you're way ahead. — <a href="https://x.com/paulg/status/1224328241936248832" data-astro-cid-lz4bm56f="">@paulg</a></p></blockquote><p>Whenever I tell people about Slight, they always mention what a dreadful market it was to raise into. This is 100% true: at the time of raising (early 2023) the market was a disaster, and the limited amount of money in play was chasing AI. But the market is what it is, and that’s exactly why you need runway: to survive tough conditions. We did not!</p><p>The market downturn also meant companies were cutting SaaS spend and consolidating tools (very reasonably). Slight was cheap to run, but that’s not a great selling point.</p><p>Blaming the market is cope. Yes, it was tough, but we just didn’t have enough customers. Everything else is context.</p><h2 id="more-guesswork-quickfire" data-astro-cid-u4qoyrkz="">More Guesswork: Quickfire</h2><!-- { level === 5 && (
     <h5 id={id} class=""><slot /></h5>
     )}

     { level === 6 && (
     <h6 id={id} class=""><slot /></h6>
     )} --><p>I doubt anything outside the above makes a huge difference. But you never know.</p><h3 id="moving-too-slowly" data-astro-cid-u4qoyrkz="">Moving Too Slowly?</h3><!-- { level === 5 && (
     <h5 id={id} class=""><slot /></h5>
     )}

     { level === 6 && (
     <h6 id={id} class=""><slot /></h6>
     )} --><p>Eh, probably wasn’t this.</p><p>We had a great team, but they hadn’t all worked in an early-stage startup before, and weren’t necessarily used to “startup trade-offs” — e.g. between quick iterations and robust/polished features. The transition wasn’t too hard though; everyone was very receptive to feedback and adapted quickly.</p><p>Of course aligning the team is one of your roles as founder, but the hard part is coming up with the prioritisation to begin with, not so much in transferring it, especially when the team is small.</p><h3 id="over-engineering" data-astro-cid-u4qoyrkz="">Over-Engineering?</h3><!-- { level === 5 && (
     <h5 id={id} class=""><slot /></h5>
     )}

     { level === 6 && (
     <h6 id={id} class=""><slot /></h6>
     )} --><p>We probably could have put more features on the backburner until after launch, to tighten the feedback loop.</p><p>We spent a bunch of time on multi-tenant infrastructure when we could have just gone with single tenant for a long time. But this is mostly downstream of getting caught in no man’s land with who we were trying to serve.</p><p>We also built a public version of Slight with hosted datasets and queries. I have mixed feelings about this: fantastic for feedback and hiring, but likely more effort than it was worth for customer acquisition.</p><h3 id="co-founder-overlap" data-astro-cid-u4qoyrkz="">Co-Founder Overlap?</h3><!-- { level === 5 && (
     <h5 id={id} class=""><slot /></h5>
     )}

     { level === 6 && (
     <h6 id={id} class=""><slot /></h6>
     )} --><p>Working with Raiden was fantastic, but we both have the same background: stats guys who went into data; solid engineers, but more specialist than generalist. This overlap was great during Slight’s genesis, but it meant we felt compelled to hire both frontend and backend engineers very early on — maybe too early.</p><p>Still, I’d do it again: having the right partner who you can trust matters way more than having the perfect skill mix.</p><h3 id="competitors" data-astro-cid-u4qoyrkz="">Competitors</h3><!-- { level === 5 && (
     <h5 id={id} class=""><slot /></h5>
     )}

     { level === 6 && (
     <h6 id={id} class=""><slot /></h6>
     )} --><p>We did OK here, by which I mean we weren’t consumed by competitor focus (investors on the other hand...).</p><p>However, many potential customers did bring up competitors they used, or ones they were considering — they essentially wanted a comparison. That’s of course reasonable, they’re trying to understand where your product fits in, and what their needs are. So it’s not something you can afford to ignore, unless your product is solving such a unique problem that comparisons are meaningless.</p><h2 id="was-it-all-so-bad" data-astro-cid-u4qoyrkz="">Was It All So Bad?</h2><!-- { level === 5 && (
     <h5 id={id} class=""><slot /></h5>
     )}

     { level === 6 && (
     <h6 id={id} class=""><slot /></h6>
     )} --><p>Oh not at all. Running a start-up was hard work, stressful, but rewarding. We did onboard some actual customers, so clearly we weren’t completely wasting our time. There were plenty of things that went well: hiring (maybe a future post!), the team we built, our customer conversations and understanding of their problems, and technical execution.</p><p>But this post isn’t a Slight retrospective. I'm trying to isolate what went wrong for a business that could execute well — a strong team, solid tech, and good customer engagement. Those weren't our problems.</p><p>We tried, it didn’t work. If the right situation comes along, I’ll probably try again.</p><h2 id="avoid-the-absorbing-barrier" data-astro-cid-u4qoyrkz="">Avoid the Absorbing Barrier</h2><!-- { level === 5 && (
     <h5 id={id} class=""><slot /></h5>
     )}

     { level === 6 && (
     <h6 id={id} class=""><slot /></h6>
     )} --><p>When startups fail, the problems they aimed to solve might still exist. Annoying!</p><p>Startup founders will probably get a lot of shit advice along their journey, or advice that’s not relevant or suitable for their type of startup. But that’s a shite excuse for anything: we are all surely responsible for our own company’s success.</p><p>Everyone who says to move fast is basically right. It’s not about cutting corners, but about giving yourself as many chances as possible to learn, to tweak, to “pivot”, to iterate your way to something people actually want.</p><p>Really know what you’re selling. Then actually sell <span> it. <span></span></span></p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Steve Ballmer was an underrated CEO (335 pts)]]></title>
            <link>https://danluu.com/ballmer/</link>
            <guid>41976754</guid>
            <pubDate>Mon, 28 Oct 2024 21:48:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://danluu.com/ballmer/">https://danluu.com/ballmer/</a>, See on <a href="https://news.ycombinator.com/item?id=41976754">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <p>There's a common narrative that Microsoft was moribund under Steve Ballmer and then later saved by the miraculous leadership of Satya Nadella. This is the dominant narrative in every online discussion about the topic I've seen and it's a commonly expressed belief "in real life" as well. While I don't have anything negative to say about Nadella's leadership in this post, this narrative underrates Ballmer's role in Microsoft's success. Not only did Microsoft's financials, revenue and profit, look great under Ballmer, Microsoft under Ballmer made deep, long-term bets that set up Microsoft for success in the decades after his reign. At the time, the bets were widely panned, indicating that they weren't necessarily obvious, but we can see in retrospect that the company made very strong bets despite the criticism at the time.</p> <p>In addition to overseeing deep investments in areas that people would later credit Nadella for, Ballmer set Nadella up for success by clearing out political barriers for any successor. Much like <a href="https://x.com/danluu/status/1129519029192757249">Gary Bernhardt's talk, which was panned because he made the problem statement and solution so obvious that people didn't realize they'd learned something non-trivial</a>, Ballmer set up Microsoft for future success so effectively that it's easy to criticize him for being a bum because his successor is so successful.</p> <h3 id="criticisms-of-ballmer">Criticisms of Ballmer</h3> <p>For people who weren't around before the turn of the century, in the 90s, Microsoft used to be considered the biggest, baddest, company in town. But it wasn't long before people's opinions on Microsoft changed — by 2007, many people thought of Microsoft as the next IBM and Paul Graham wrote Microsoft is Dead, in which he noted that Microsoft being considered effective was ancient history:</p> <blockquote> <p>A few days ago I suddenly realized Microsoft was dead. I was talking to a young startup founder about how Google was different from Yahoo. I said that Yahoo had been warped from the start by their fear of Microsoft. That was why they'd positioned themselves as a "media company" instead of a technology company. Then I looked at his face and realized he didn't understand. It was as if I'd told him how much girls liked Barry Manilow in the mid 80s. Barry who?</p> <p>Microsoft? He didn't say anything, but I could tell he didn't quite believe anyone would be frightened of them.</p> </blockquote> <p>These kinds of comments often came with comments that Microsoft's revenue was destined to fall, such as these comments by Graham:</p> <blockquote> <p>Actors and musicians occasionally make comebacks, but technology companies almost never do. Technology companies are projectiles. And because of that you can call them dead long before any problems show up on the balance sheet. Relevance may lead revenues by five or even ten years.</p> </blockquote> <p>Graham names Google and the web as primary causes of Microsoft's death, which we'll discuss later. Although Graham doesn't name Ballmer or note his influence in Microsoft is Dead, Ballmer has been a favorite punching bag of techies for decades. Ballmer came up on the business side of things and later became EVP of Sales and Support; techies love belittling non-technical folks in tech<sup id="fnref:P"><a rel="footnote" href="#fn:P">1</a></sup>. A common criticism, then and now, is that Ballmer didn't understand tech and was a poor leader because all he knew was sales and the bottom line and all he can do is copy what other people have done. Just for example, if you look at online comments on tech forums (minimsft, HN, slashdot, etc.) when Ballmer pushed Sinofsky out in 2012, Ballmer's leadership is nearly universally panned<sup id="fnref:H"><a rel="footnote" href="#fn:H">2</a></sup>. Here's a fairly typical comment from someone claiming to be an anonymous Microsoft insider:</p> <blockquote> <p>Dump Ballmer. Fire 40% of the workforce starting with the loser online services (they are never going to get any better). Reinvest the billions in start-up opportunities within the puget sound that can be accretive to MSFT and acquisition targets ... Reset Windows - Desktop and Tablet. Get serious about business cloud (like Salesforce ...)</p> </blockquote> <p>To the extent that Ballmer defended himself, it was by pointing out that <a href="https://x.com/janettu/status/380824535714377728">the market appeared to be undervaluing Microsoft</a>. Ballmer noted that Microsoft's market cap at the time was extremely low relative to its fundamentals/financials relative to Amazon, Google, Apple, Oracle, IBM, and Salesforce. This seems to have been a fair assessment by Ballmer as <abbr title="as of my writing the draft of this post, in July 2024, if you include dividend reinvestment">Microsoft has outperformed all of those companies since then</abbr>.</p> <p>When Microsoft's market cap took off after Nadella became CEO, it was only natural the narrative would be that Ballmer was killing Microsoft and that the company was struggling until Nadella turned it around. You can pick other discussions if you want, but just for example, if we look at the most recent time Microsoft is Dead hit #1 on HN, a quick ctrl+F has Ballmer's name showing up 24 times. Ballmer has some defenders, but the standard narrative that Ballmer was holding Microsoft back is there, and one of the defenders even uses part of the standard narrative: Ballmer was an unimaginative hack, but he at least set up Microsoft well financially. If you look at high ranking comments, they're all dunking on Ballmer.</p> <p>And if you look on less well informed forums, like Twitter or Reddit, you see the same attacks, but Ballmer has fewer defenders. On Twitter, when I search for "Ballmer", the first four results are unambiguously making fun of Ballmer. The fifth hit could go either way, but from the comments, seems to generally be taken as making of Ballmer, and as I far as I scrolled down, all but one of the remaining videos was making fun of Ballmer (the one that wasn't was an interview where Ballmer notes that he offered Zuckerberg "$20B+, something like that" for Facebook in 2009, which would've been the 2nd largest tech acquisition ever at the time, second only to Carly Fiorina's acquisition of Compaq for $25B in 2001). Searching reddit (incognito window with no history) is the same story (excluding the stories about him as an NBA owner, where he's respected by fans). The top story is making fun of him, the next one notes that he's wealthier than Bill Gates and the top comment on his performance as a CEO starts with "The irony is that he is Microsofts [sic] worst CEO" and then has the standard narrative that the only reason the company is doing well is due to Nadella saving the day, that Ballmer missed the boat on all of the important changes in the tech industry, etc.</p> <p>To sum it up, for the past twenty years, people having been dunking on Ballmer for being a buffoon who doesn't understand tech and who was, at best, some kind of bean counter who knew how to keep the lights on but didn't know how to foster innovation and caused Microsoft to fall behind in every important market.</p> <h3 id="ballmer-s-wins">Ballmer's wins</h3> <p>The common view is at odds with what actually happened under Ballmer's leadership. In financially material positive things that happened under Ballmer since Graham declared Microsoft dead, we have:</p> <ul> <li>2009: Bing launched. This is considered a huge failure, but the bar here is fairly high. A quick web search finds that Bing allegedly made $1B in profit in 2015 and $6.4B in FY 2024 on $12.6B of revenue (given Microsoft's PE ratio in 2022, a rough estimate for Bing's value in 2022 would be $240B)</li> <li>2010: Microsoft creates Azure <ul> <li>I can't say that I personally like it as a product, but in terms of running large scale cloud infrastructure, the three companies that are head-and-shoulders ahead of everyone else in the world are Amazon, Google, and Microsoft. From a business standpoint, the worst thing you could say about Microsoft here is that they're a solid #2 in terms of the business and the biggest threat to become the #1</li> <li>The enterprise sales arm, built and matured under Ballmer, was and is critical to the success of Azure and Office</li> </ul></li> <li>2010: Office 365 released <ul> <li>Microsoft transitioned its enterprise / business suite of software from boxed software to subscription-based software with online options <ul> <li>there isn't really a fixed date for this; the official release of Office 365 seems like as good a year as any</li> </ul></li> <li>Like Azure, I don't personally like these products, but if Microsoft were to split up into major business units, the enterprise software suite is the business unit that could possibly rival Azure in market cap</li> </ul></li> </ul> <p>There are certainly plenty of big misses as well. From 2010-2015, HoloLens was one of Microsoft's biggest bets, behind only Azure and then Bing, but no one's big AR or VR bets have had good returns to date. Microsoft failed to capture the mobile market. Although Windows Phone was generally well received by reviewers who tried it, depending on who you ask, Microsoft was either too late or wasn't willing to subsidize Windows Phone for long enough. Although .NET is still used today, in terms of marketshare, .NET and Silverlight didn't live up to early promises and critical parts were hamstrung or killed as a side effect of internal political battles. Bing is, by reputation, a failure and, at least given Microsoft's choices at the time, <a href="https://danluu.com/ftc-google-antitrust/">probably needed antitrust action against Google to succeed</a>, but this failure still resulted in a business unit worth hundreds of billions of dollars. And despite all of the failures, the biggest bet, Azure, is probably worth on the order of a trillion dollars.</p> <p>The enterprise sales arm of Microsoft was built out under Ballmer before he was CEO (he was, for a time, EVP for Sales and Support, and actually started at Microsoft as the first business manager) and continued to get built out when Ballmer was CEO. Microsoft's sales playbook was so effective that, when I was Microsoft, Google would offer some customers on Office 365 Google's enterprise suite (Docs, etc.) for free. Microsoft salespeople noted that they would still usually be able to close the sale of Microsoft's paid product even when competing against a Google that was giving their product away. For the enterprise, the combination of Microsoft's offering and its enterprise sales team was so effective that Google couldn't even give its product away.</p> <p>If you're reading this and you work at a "tech" company, the company is overwhelmingly likely to choose the Google enterprise suite over the Microsoft enterprise suite and the enterprise sales pitch Microsoft sales people have probably sounds ridiculous to you.</p> <p>An acquaintance of mine who ran a startup had a Microsoft Azure salesperson come in and try to sell them on Azure, opening with "You're on AWS, the consumer cloud. You need Azure, the enterprise cloud". For most people in tech companies, enterprise is synonymous with overpriced, unreliable, junk. In the same way it's easy to make fun of Ballmer because he came up on the sales and business side of the house, it's easy to make fun of an enterprise sales pitch when you hear it but, overall, Microsoft's enterprise sales arm does a good job. When I worked in Azure, I looked into how it worked and, having just come from Google, there was a night and day difference. This was in 2015, under Nadella, but the culture and processes that let Microsoft scale this up were built out under Ballmer. I think there were multiple months where Microsoft hired and onboarded more salespeople than Google employed in total and every stage of the sales pipeline was fairly effective.</p> <h3 id="microsoft-s-misses-under-ballmer">Microsoft's misses under Ballmer</h3> <p>When people point to a long list of failures like Bing, Zune, Windows Phone, and HoloLens as evidence that Ballmer was some kind of buffoon who was holding Microsoft back, this demonstrates a lack of understanding of the tech industry. This is like pointing to a list of failed companies a VC has funded as evidence the VC doesn't know what they're doing. But that's silly in a hits based industry like venture capital. If you want to claim the VC is bad, you need to point out poor total return or a lack of big successes, which would imply poor total return. Similarly, a large company like Microsoft has a large portfolio of bets and one successful bet can pay for a huge number of failures. Ballmer's critics can't point to a poor total return because Microsoft's total return was very good under his tenure. Revenue increased from <abbr title="Most sources cite $22B to $78B, which probably stems from not understanding that the fiscal year and the calendar year are not the same. The revenue from the last four quarters Ballmer presided over was 20.403B+24.519B+18.529B+19.114B=82.565B">$14B or $22B to $83B</abbr>, depending on whether you want to count from when Ballmer became President in July 1998 or when Ballmer became CEO in January 2000. The company was also quite profitable when Ballmer left, recording $27B in profit the previous four quarters, more than the revenue of the company he took over. By market cap, Azure alone would be in the top 10 largest public companies in the world and the enterprise software suite minus Azure would probably just miss being in the top 10.</p> <p>As a result, critics also can't point to a lack of hits when Ballmer presided over the creation of Azure, the conversion of Microsoft's enterprise software from set of local desktop apps to Office 365 et al., the creation of the world's most effective enterprise sales org, the creation of Microsoft's video game empire (among other things, Ballmer was CEO when Microsoft acquired Bungie and made Halo the Xbox's flagship game on launch in 2001), etc. Even Bing, widely considered a failure, on last reported revenue and <abbr title="as of when the draft of this post was written in mid-2024">current P/E ratio, would be 12th most valuable tech company in the world</abbr>, between Tencent and ASML. When attacking Ballmer, people cite Bing as a failure that occurred on Ballmer's watch, which tells you something about the degree of success Ballmer had. Most companies would love to have their successes be as successful as Bing, let alone their failures. Of course it would be better if Ballmer was prescient and all of his bets succeeded, making Microsoft worth something like $10T instead of the lowly $3T market cap it has today, but the criticism of Ballmer that says that he had some failures and some $1T successes is a criticism that he wasn't the greatest CEO of all time by a gigantic margin. True, but not much of a criticism.</p> <p>And, unlike Nadella, Ballmer didn't inherit a company that was easily set up for success. As we noted earlier, it wasn't long into Ballmer's tenure that Microsoft was considered a boring, irrelevant company and the next IBM, mostly due to decisions made when Bill Gates was CEO. As a very senior Microsoft employee from the early days, Ballmer was also partially responsible for the state of Microsoft at the time, so Microsoft's problems are also at least partially attributable to him (but that also means he should get some credit for the success Microsoft had through the 90s). Nevertheless, he navigated Microsoft's most difficult problems well and set up his successor for smooth sailing.</p> <p>Earlier, we noted that Paul Graham cited Google and the rise of the web as two causes for Microsoft's death prior to 2007. <a href="https://danluu.com/ftc-google-antitrust/">As we discussed in this look at antitrust action in tech</a>, these both share a common root cause, antitrust action against Microsoft. If we look at <a href="https://danluu.com/us-v-ms/">the documents from the Microsoft antitrust case</a>, it's clear that Microsoft knew how important the internet was going to be and had plans to control the internet. As part of these plans, they used their monopoly power on the desktop to kill Netscape. They technically lost an antirust case due to this, but if you look at the actual outcomes, Microsoft basically got what they wanted from the courts. The remedies levied against Microsoft are widely considered to have been useless (the initial decision involved breaking up Microsoft, but they were able to reverse this on appeal), and the case dragged on for long enough that Netscape was doomed by the time the case was decided, and the remedies that weren't specifically targeted at the Netscape situation were meaningless.</p> <p>A later part of the plan to dominate the web, discussed at Microsoft but never executed, was to kill Google. If we're judging Microsoft by how "dangerous" it is, how effectively it crushes its competitors, <abbr title="Graham notes that Microsoft is dead because it's no longer dangerous">like Paul Graham did when he judged Microsoft to be dead</abbr>, then Microsoft certainly became less dangerous, but the feeling at Microsoft was that their hand was forced due to the circumstances. One part of the plan to kill Google was to redirect users who typed google.com into their address bar to MSN search. This was before Chrome existed and before mobile existed in any meaningful form. Windows desktop marketshare was 97% and IE had between 80% to 95% marketshare depending on the year, with most of the rest of the marketshare belonging to the rapidly declining Netscape. If Microsoft makes this move, Google is killed before it can get Chrome and Android off the ground and, barring extreme antitrust action, such as a breakup of Microsoft, Microsoft owns the web to this day. And then for dessert, it's not clear there wouldn't be a reason not to go after Amazon.</p> <p>After internal debate, Microsoft declined to kill Google not due to fear of antitrust action, but due to fear of bad PR from the ensuing antitrust action. Had Microsoft redirected traffic away from Google, the impact on Google would've been swifter and more severe than their moves against Netscape and in the time it would take for the DoJ to win another case against Microsoft, Google would suffer the same fate as Netscape. It might be hard to imagine this if you weren't around at the time, but the DoJ vs. Microsoft case was regular front-page news in a way that we haven't seen since (in part because companies learned their lesson on this one — Google <a href="https://danluu.com/ftc-google-antitrust/">supposedly killed the 2011-2012 FTC against them with lobbying</a> and has cleverly maneuvered the more recent case so that it doesn't dominate the news cycle in the same way). The closest thing we've seen since the Microsoft antitrust media circus was the media response to the Crowdstrike outage, but that was a flash in the pan compared to the DoJ vs. Microsoft case.</p> <p>If there's a criticism of Ballmer here, perhaps it's something like Microsoft didn't pre-emptively learn the lessons its younger competitors learned from its big antitrust case before the big antitrust case. A sufficiently prescient executive could've advocated for heavy lobbying to head the antitrust case off at pass, like Google did in 2011-2012, or maneuvered to make the antitrust case just another news story, like Google has been doing for the current case. Another possible criticism is that Microsoft didn't correctly read the political tea leaves and realize that there wasn't going to be serious US tech antitrust for at least two decades after the big case against Microsoft. In principle, Ballmer could've overridden the decision to not kill Google if he had the right expertise on staff to realize that the United States was entering a two decade period of reduced antitrust scrutiny in tech.</p> <p>As criticisms go, I think the former criticism is correct, but not an indictment of Ballmer unless you expect CEOs to be <abbr title="in terms of maximizing the bottom line">infallible</abbr>, so as evidence that Ballmer was a bad CEO, this would be a very weak criticism. And it's not clear that the latter criticism is correct. While Google was able to get away with things like <a href="https://x.com/danluu/status/1016164712030134272">hardcoding the search engine in Android to prevent users from changing their search engine setting</a> to <a href="https://x.com/danluu/status/887724695558205440">having badware installers trick users into making Chrome the default browser</a>, they were considered the "good guys" and didn't get much scrutiny for these sorts of actions, Microsoft wasn't treated with kid gloves in the same way by the press or the general public. Google didn't trigger a serious antitrust investigation until 2011, so it's possible the lack of serious antitrust action between 2001 and 2010 was an artifact of Microsoft being careful to avoid antitrust scrutiny and Google being too small to draw scrutiny and that a move to kill Google when it was still possible would've drawn serious antitrust scrutiny and another PR circus. That's one way in which the company Ballmer inherited was in a more difficult situation than its competitors — Microsoft's hands were perceived to be tied and may have actually been tied. Microsoft could and did get severe criticism for taking an action when the exact same action taken by Google would be lauded as clever.</p> <p>When I was at Microsoft, there was a lot of consternation about this. One funny example was when, in 2011, Google officially called out Microsoft for unethical behavior and the media jumped on this as yet another example of Microsoft behaving badly. A number of people I talked to at Microsoft were upset by this because, according to them, Microsoft got the idea to do this when they noticed that Google was doing it, but reputations take a long time to change and actions taken while Gates was CEO significantly reduced Microsoft's ability to maneuver.</p> <p>Another difficulty Ballmer had to deal with on taking over was Microsoft's intense internal politics. Again, as a very senior Microsoft employee going back to almost the beginning, he bears some responsibility for this, but Ballmer managed to clear the board of the worst bad actors so that Nadella didn't inherit such a difficult situation. If we look at why Microsoft didn't dominate the web under Ballmer, in addition to concerns that killing Google would cause a PR backlash, internal political maneuvering killed most of Microsoft's most promising web products and reduced the appeal and reach of most of the rest of its web products. For example, Microsoft had <a href="https://x.com/danluu/status/790599349491212288">a working competitor to Google Docs in 1997</a>, one year before Google was founded and nine years before Google acquired <abbr title="Writely would later become Google Docs">Writely</abbr>, but it was killed for political reasons. And likewise for <a href="https://x.com/danluu/status/1572839354434977795">NetMeeting</a> and other promising products. <a href="https://www.patreon.com/posts/20571244">Microsoft certainly wasn't alone in having internal political struggles</a>, <a href="https://web.archive.org/web/20110911005705/https://www.bonkersworld.net/organizational-charts/">but it was famous for having more brutal politics than most</a>.</p> <p>Although Ballmer certainly didn't do a perfect job at cleaning house, when I was at Microsoft and asked about promising projects that were sidelined or killed due to internal political struggles, <a href="https://www.patreon.com/posts/22091116">the biggest recent sources of those issues</a> were shown the door under Ballmer, leaving a much more functional company for Nadella to inherit.</p> <h3 id="the-big-picture">The big picture</h3> <p>Stepping back to look at the big picture, Ballmer inherited a company that was a financially strong position that was hemmed in by internal and external politics in a way that caused outside observers to think the company was overwhelmingly likely to slide into irrelevance, leading to predictions like Graham's famous prediction that Microsoft is dead, with revenues expected to decline in five to ten years. In retrospect, we can see that moves made under Gates limited Microsoft's ability to use its monopoly power to outright kill competitors, but there was no inflection point at which a miraculous turnaround was mounted. Instead, Microsoft continued its very strong execution on enterprise products and continued making reasonable bets on the future in a successful effort to supplant revenue streams that were internally viewed as long-term dead ends, even if they were going to be profitable dead ends, such as Windows and boxed (non-subscription) software.</p> <p>Unlike most companies in that position, Microsoft was willing to very heavily subsidize a series of bets that leadership thought could power the company for the next few decades, such as Windows Phone, Bing, Azure, Xbox, and HoloLens. From the internal and external commentary on these bets, you can see why it's so hard for companies to use their successful lines of business to subsidize new lines of business when the writing is on the wall for the successful businesses. People panned these bets as stupid moves that would kill the company, saying the company should focus is efforts on its most profitable businesses, such as Windows.</p> <p>Not all of the bets panned out, but if we look at comments from critics who were saying that Microsoft was doomed because it was subsidizing the wrong bets or younger companies would surpass it, well, today, Microsoft is worth 50% more than Google and twice as much as Meta. If we look at the broader history of the tech industry, Microsoft has had sustained strong execution from its founding in 1975 until today, a nearly fifty year run, a run that's arguably been unmatched in the tech industry. Intel's been around as bit longer, but they stumbled very badly around the turn of the century and <a href="https://danluu.com/cpu-bugs/">they've had a number of problems over the past decade</a>. IBM has a long history, but it just wasn't all that big during its early history, e.g., when T.J. Watson renamed Computing-Tabulating-Recording Company to International Business Machines, its revenue was still well under $10M a year (inflation adjusted, on the order of $100M a year). Computers started becoming big and IBM was big for a tech company by the 50s, but the antitrust case brought against IBM in 1969 that dragged on until it was dropped for being "without merit" in 1982 hamstrung the company and its culture in ways that are still visible when you look at, for example, why IBM's various cloud efforts have failed and, in the 90s, the company was on its deathbed and only managed to survive at all due to Gerstner's turnaround. If we look at older companies that had long sustained runs of strong execution, most of them are gone, like DEC and Data General, or had very bad stumbles that nearly ended the company, like IBM and Apple. There are companies that have had similarly long periods of strong execution, like Oracle, but those companies haven't been nearly as effective as Microsoft in expanding their lines of business and, as a result, Oracle is worth perhaps two Bings. That makes Oracle the 20th most valuable public company in the world, which certainly isn't bad, but it's no Microsoft.</p> <p>If Microsoft stumbles badly, a younger company like Nvidia, Meta, or Google could overtake Microsoft's track record, but that would be no fault of Ballmer's and we'd still have to acknowledge that Ballmer was a very effective CEO, not just in terms of bringing the money in, but in terms of setting up a vision that set Microsoft up for success for the next fifty years.</p> <h3 id="appendix-microsoft-s-relevance-under-ballmer">Appendix: Microsoft's relevance under Ballmer</h3> <p>Besides the headline items mentioned above, off the top of my head, here are a few things I thought were interesting that happened under Ballmer since Graham declared Microsoft to be dead</p> <ul> <li>2007: Microsoft releases LINQ, still fairly nice by <a href="https://www.scattered-thoughts.net/writing/against-sql/">in-use-by-practitioners standards today</a></li> <li>2011: Suimt Gulwani, at MSR, publishes "Automating string processing in spreadsheets using input-output examples", named a most influential POPL paper 10 years later <ul> <li>This paper is about using program synthesis for spreadsheet "autocomplete/inference"</li> <li>I'm not a fan of patents, but I would guess that the reason autocomplete/inference works fairly well in Excel and basically doesn't work at all in Google Sheets is that MS has a patent on this based on this work</li> </ul></li> <li>2012: Microsoft releases TypeScript <ul> <li>This has to be the most widely used programming language released this century and it's a plausible candidate for becoming the most widely used language, period (as long as you don't also count TS usage as JS)</li> </ul></li> <li>2012: Microsoft Surface released <ul> <li>Things haven't been looking so good for the Surface line since Panos Panay left in 2022, and this was arguably failure even in 2022, but this was a $7B/yr line of business in 2022, which goes to show you how big and successful Microsoft is — most companies would love to have something doing as well as a failed $7B/yr business</li> </ul></li> <li>2015: Microsoft releases vscode (after the end of Ballmer's tenure in 2014, but this work came out of work under Ballmer's tenure in multiple ways) <ul> <li>This seems like the most widely used editor among programmers today by a very large margin. When I looked at survey data on this a number of years back, I was shocked by how quickly this happened. It seems like vscode has achieved a level of programmer editor dominance that's <abbr title="excluding the very early days, when you could say that there was only one programmer using one thing">never been seen before</abbr>. Probably the closest thing was Visual Studio a decade before Paul declared Microsoft dead, but that never achieved the same level of marketshare due to a combination of effectively being Windows only software and also costing quite a bit of money</li> <li>Heath Borders notes that Erich Gamma, hired in 2011, was highly influential here</li> </ul></li> </ul> <p>One response to Microsoft's financial success, both the direct success that happened under Ballmer as well as later success that was set up by Ballmer, is that Microsoft is financially successful but irrelevant for trendy programmers, like IBM. For one thing, rounded to the nearest Bing, IBM is probably worth either zero or one Bings. But even if we put aside the financial aspect and we just look at how much each $1T tech company (Apple, Nvidia, Microsoft, Google, Amazon, and Meta) has impacted programmers, Nvidia, Apple, and Microsoft all have a lot of programmers who are dependent on the company due to some kind of ecosystem dependence (CUDA; iOS; .NET and Windows, the latter of which is still the platform of choice for many large areas, such as AAA games).</p> <p>You could make a case for the big cloud vendors, but I don't think that companies have a nearly forced dependency on AWS in the same way that a serious English-language consumer app company really needs an iOS app or an AAA game company has to release on Windows and overwhelmingly likely develops on Windows.</p> <p>If we look at programmers who aren't pinned to an ecosystem, Microsoft seems highly relevant to a lot of programmers due to the creation of tools like vscode and TypeScript. I wouldn't say that it's necessarily more relevant than Amazon since so many programmers use AWS, but it's hard to argue that the company that created (among many other things) vscode and TypeScript under Ballmer's watch is irrelevant to programmers.</p> <h3 id="appendix-my-losing-bet-against-microsoft">Appendix: my losing bet against Microsoft</h3> <p>Shortly after joining Microsoft in 2015, I bet Derek Chiou that Google would beat Microsoft to $1T market cap. Unlike most external commentators, I agreed with the bets Microsoft was making, but when I looked around at the kinds of internal dysfunction Microsoft had at the time, I thought that would cause them enough problems that Google would win. That was wrong —&nbsp;Microsoft beat Google to $1T and is now worth $1T more than Google.</p> <p>I don't think I would've made the bet even a year later, after seeing Microsoft from the inside and how effective Microsoft sales was and how good Microsoft was at shipping things that are appealing to enterprises and the comparing that to Google's cloud execution and strategy. But you could say that I made a mistake that was fairly analogous to what external commentators made until I saw how Microsoft operated in detail.</p> <p><i>Thanks to Laurence Tratt, Yossi Kreinin, Heath Borders, Justin Blank, and Fabian Giesen for comments/corrections/discussion</i></p>  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HTML Form Validation is heavily underused (341 pts)]]></title>
            <link>https://expressionstatement.com/html-form-validation-is-heavily-underused</link>
            <guid>41976529</guid>
            <pubDate>Mon, 28 Oct 2024 21:28:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://expressionstatement.com/html-form-validation-is-heavily-underused">https://expressionstatement.com/html-form-validation-is-heavily-underused</a>, See on <a href="https://news.ycombinator.com/item?id=41976529">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article>HTML Forms have powerful validation mechanisms, but they are heavily underused. In fact, not many people even know much about them. Is this because of some flaw in their design? Let’s explore.<!-- -->
<h2 id="attributes-methods-and-properties">Attributes, methods, and properties</h2>
<p>It’s easy to disallow empty inputs by adding
a <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Attributes/required"><code>required</code></a> attribute:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="jsx" data-theme="catppuccin-macchiato catppuccin-frappe"><code data-language="jsx" data-theme="catppuccin-macchiato catppuccin-frappe"><span data-line=""><span>&lt;</span><span>input</span><span> required</span><span>=</span><span>{</span><span>true</span><span>}</span><span> /&gt;</span></span></code></pre></figure>
<p>Beyond that, there is a bunch of other ways that you can add constraints to your input. Precisely, there are three ways to do it:</p>
<ul>
<li>Using specific <code>type</code> attribute values, such as <code>"email"</code>, <code>"number"</code>, or <code>"url"</code></li>
<li>Using other input attributes that create constraints, such as <code>"pattern"</code> or <code>"maxlength"</code></li>
<li><strong>Using the <code>setCustomValidity</code> DOM method of the input</strong></li>
</ul>
<p>The last one is the most powerful as it allows to create arbitrary validation logic and handle
complex cases.
Do you notice how it differs from the first two techniques? The first two are defined with <em>attributes</em>, but <code>setCustomValidity</code>
is a <em>method</em>.</p>
<p>Here’s a great write-up that explains the differences between DOM attributes and properties: <a href="https://jakearchibald.com/2024/attributes-vs-properties/">https://jakearchibald.com/2024/attributes-vs-properties/</a></p>
<h2 id="the-nuance-of-an-imperative-api">The nuance of an imperative API</h2>
<p>The fact that <code>setCustomValidity</code> API is exposed only as a method and doesn’t have an attribute equivalent
leads to some terrible ergonomics. I’ll show you with an example.</p>
<p>But first, a very quick intro to how this API works:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="catppuccin-macchiato catppuccin-frappe"><code data-language="js" data-theme="catppuccin-macchiato catppuccin-frappe"><span data-line=""><span>// Make input invalid</span></span>
<span data-line=""><span>input</span><span>.</span><span>setCustomValidity</span><span>(</span><span>"Any text message"</span><span>)</span><span>;</span></span></code></pre></figure>
<p>This would make input <em>invalid</em> and the browser will show the reason as “Any text message”.</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="catppuccin-macchiato catppuccin-frappe"><code data-language="js" data-theme="catppuccin-macchiato catppuccin-frappe"><span data-line=""><span>// Remove custom constraints and make input valid</span></span>
<span data-line=""><span>input</span><span>.</span><span>setCustomValidity</span><span>(</span><span>""</span><span>)</span><span>;</span></span></code></pre></figure>
<p>Passing an empty string makes the input <em>valid</em> (unless other constraints are applied).</p>
<p>That’s pretty much it! Now let’s apply this knowledge.<br>
Let’s say we want to implement an equivalent of the <code>required</code> attribute.
That means that an empty input must be prevent the form from being submitted.</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="jsx" data-theme="catppuccin-macchiato catppuccin-frappe"><code data-language="jsx" data-theme="catppuccin-macchiato catppuccin-frappe"><span data-line=""><span>&lt;</span><span>input</span></span>
<span data-line=""><span>  name</span><span>=</span><span>"example"</span></span>
<span data-line=""><span>  placeholder</span><span>=</span><span>"..."</span></span>
<span data-line=""><span>  onChange</span><span>=</span><span>{(</span><span>event</span><span>)</span><span> =&gt;</span><span> {</span></span>
<span data-line=""><span>    const</span><span> input </span><span>=</span><span> event</span><span>.</span><span>currentTarget</span><span>;</span></span>
<span data-line="" data-highlighted-line=""><span>    if</span><span> (input</span><span>.</span><span>value </span><span>===</span><span> ""</span><span>) </span><span>{</span></span>
<span data-line="" data-highlighted-line=""><span>      input</span><span>.</span><span>setCustomValidity</span><span>(</span><span>"Custom message: input is empty"</span><span>)</span><span>;</span></span>
<span data-line="" data-highlighted-line=""><span>    }</span><span> else</span><span> {</span></span>
<span data-line="" data-highlighted-line=""><span>      input</span><span>.</span><span>setCustomValidity</span><span>(</span><span>""</span><span>)</span><span>;</span></span>
<span data-line="" data-highlighted-line=""><span>    }</span></span>
<span data-line=""><span>  }}</span></span>
<span data-line=""><span>/&gt;</span></span></code></pre></figure>
<p>This kind of looks like we’re done and this code should be enough to accomplish the task.
But try to see it in action:</p>

<p>It may seem to work, but there’s just one important edge case: the input is
in a valid state <em>initially</em>. If you reset the component and press the “submit”
button, the form submission will go through.
But surely, before we ever touch the input, it is empty, and therefore must be invalid.
But we only ever do something when the input value <em>changes</em>.</p>
<p>How can we fix this?</p>
<p>Let’s execute some code when the component mounts:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="jsx" data-theme="catppuccin-macchiato catppuccin-frappe"><code data-language="jsx" data-theme="catppuccin-macchiato catppuccin-frappe"><span data-line="" data-highlighted-line=""><span>import</span><span> {</span><span> useRef</span><span>,</span><span> useLayoutEffect </span><span>}</span><span> from</span><span> "react"</span><span>;</span></span>
<span data-line=""> </span>
<span data-line=""><span>function</span><span> Form</span><span>()</span><span> {</span></span>
<span data-line="" data-highlighted-line=""><span>  const</span><span> ref </span><span>=</span><span> useRef</span><span>()</span><span>;</span></span>
<span data-line="" data-highlighted-line=""><span>  useLayoutEffect</span><span>(</span><span>()</span><span> =&gt;</span><span> {</span></span>
<span data-line="" data-highlighted-line=""><span>    // Make input invalid on initial render if it's empty</span></span>
<span data-line="" data-highlighted-line=""><span>    const</span><span> input </span><span>=</span><span> ref</span><span>.</span><span>current</span><span>;</span></span>
<span data-line="" data-highlighted-line=""><span>    const</span><span> empty </span><span>=</span><span> input</span><span>.</span><span>value </span><span>!==</span><span> ""</span><span>;</span></span>
<span data-line="" data-highlighted-line=""><span>    input</span><span>.</span><span>setCustomValidity</span><span>(empty </span><span>?</span><span> "Initial message: input is empty"</span><span> :</span><span> ""</span><span>)</span><span>;</span></span>
<span data-line="" data-highlighted-line=""><span>  },</span><span> [])</span><span>;</span></span>
<span data-line=""> </span>
<span data-line=""><span>  return</span><span> (</span></span>
<span data-line=""><span>    &lt;</span><span>form</span><span>&gt;</span></span>
<span data-line=""><span>      &lt;</span><span>input</span></span>
<span data-line=""><span>        ref</span><span>=</span><span>{</span><span>ref</span><span>}</span></span>
<span data-line=""><span>        name</span><span>=</span><span>"example"</span></span>
<span data-line=""><span>        onChange</span><span>=</span><span>{(</span><span>event</span><span>)</span><span> =&gt;</span><span> {</span></span>
<span data-line=""><span>          const</span><span> input </span><span>=</span><span> event</span><span>.</span><span>currentTarget</span><span>;</span></span>
<span data-line=""><span>          if</span><span> (input</span><span>.</span><span>value </span><span>===</span><span> ""</span><span>) </span><span>{</span></span>
<span data-line=""><span>            input</span><span>.</span><span>setCustomValidity</span><span>(</span><span>"Custom message: input is empty"</span><span>)</span><span>;</span></span>
<span data-line=""><span>          }</span><span> else</span><span> {</span></span>
<span data-line=""><span>            input</span><span>.</span><span>setCustomValidity</span><span>(</span><span>""</span><span>)</span><span>;</span></span>
<span data-line=""><span>          }</span></span>
<span data-line=""><span>        }}</span></span>
<span data-line=""><span>      /&gt;</span></span>
<span data-line=""><span>      &lt;</span><span>button</span><span>&gt;</span><span>Submit</span><span>&lt;/</span><span>button</span><span>&gt;</span></span>
<span data-line=""><span>    &lt;/</span><span>form</span><span>&gt;</span></span>
<span data-line=""><span>  )</span><span>;</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>

<p>Great! Now everything works as expected. But at what cost?</p>
<h2 id="the-boilerplate-problem">The boilerplate problem</h2>
<p>Let’s look at our clumsy way to validate the initial value:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="catppuccin-macchiato catppuccin-frappe"><code data-language="js" data-theme="catppuccin-macchiato catppuccin-frappe"><span data-line=""><span>const</span><span> ref </span><span>=</span><span> useRef</span><span>()</span><span>;</span></span>
<span data-line=""><span>useLayoutEffect</span><span>(</span><span>()</span><span> =&gt;</span><span> {</span></span>
<span data-line=""><span>  // Make input invalid on initial render if it's empty</span></span>
<span data-line=""><span>  const</span><span> input </span><span>=</span><span> ref</span><span>.</span><span>current</span><span>;</span></span>
<span data-line=""><span>  const</span><span> empty </span><span>=</span><span> input</span><span>.</span><span>value </span><span>!==</span><span> ""</span><span>;</span></span>
<span data-line=""><span>  input</span><span>.</span><span>setCustomValidity</span><span>(empty </span><span>?</span><span> "Initial message: input is empty"</span><span> :</span><span> ""</span><span>)</span><span>;</span></span>
<span data-line=""><span>},</span><span> [])</span><span>;</span></span></code></pre></figure>
<p>Ugh! Wouldn’t want to write that one each time. Let’s think about what’s wrong with this.</p>
<ul>
<li>The validation logic is duplicated between the onChange handler and the initial render phase</li>
<li>The initial validation is not co-located with the input, so we’re losing code cohesion.
It’s fragile: if you update validation logic, you might forget to update code in both places.</li>
<li>The <code>useRef</code> + <code>useLayouEffect</code> + <code>onChange</code> combo is just too much ceremony,
especially when a form has a lot of inputs. And it gets even more confusing if only some of those inputs use <code>customValidity</code></li>
</ul>
<p>This is what happens when you deal with a purely imperative API in a declarative component.</p>
<blockquote>
<p>Unlike validation attributes, <code>CustomValidity</code> is a purely imperative API.
In other words, there’s no input attribute that we can use to set custom validity.</p>
</blockquote>
<p>In fact, I would argue that this is <strong>the main reason for poor adoption of native form validation</strong>. If the API is cumbersome, sometimes it just does not matter how powerful it is.</p>
<h2 id="the-missing-part">The missing part</h2>
<p>In essence, this is the attribute we need:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="html" data-theme="catppuccin-macchiato catppuccin-frappe"><code data-language="html" data-theme="catppuccin-macchiato catppuccin-frappe"><span data-line=""><span>&lt;</span><span>input</span><span> custom-validity</span><span>=</span><span>"error message"</span><span> /&gt;</span></span></code></pre></figure>
<p>In a declarative framework, this would allow to define input validations in a very powerful way:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="jsx" data-theme="catppuccin-macchiato catppuccin-frappe"><code data-language="jsx" data-theme="catppuccin-macchiato catppuccin-frappe"><span data-line=""><span>function</span><span> Form</span><span>()</span><span> {</span></span>
<span data-line=""><span>  const</span><span> [</span><span>value</span><span>,</span><span> setValue</span><span>]</span><span> =</span><span> useState</span><span>()</span><span>;</span></span>
<span data-line=""><span>  const</span><span> handleChange</span><span> =</span><span> (</span><span>event</span><span>)</span><span> =&gt;</span><span> setValue</span><span>(event</span><span>.</span><span>target</span><span>.</span><span>value)</span><span>;</span></span>
<span data-line=""><span>  return</span><span> (</span></span>
<span data-line=""><span>    &lt;</span><span>form</span><span>&gt;</span></span>
<span data-line=""><span>      &lt;</span><span>input</span></span>
<span data-line=""><span>        name</span><span>=</span><span>"example"</span></span>
<span data-line=""><span>        value</span><span>=</span><span>{</span><span>value</span><span>}</span></span>
<span data-line=""><span>        onChange</span><span>=</span><span>{</span><span>handleChange</span><span>}</span></span>
<span data-line="" data-highlighted-line=""><span>        custom-validity</span><span>=</span><span>{</span><span>value</span><span>.</span><span>length </span><span>?</span><span> "Fill out this field"</span><span> :</span><span> ""</span><span>}</span></span>
<span data-line=""><span>      /&gt;</span></span>
<span data-line=""><span>      &lt;</span><span>button</span><span>&gt;</span><span>Submit</span><span>&lt;/</span><span>button</span><span>&gt;</span></span>
<span data-line=""><span>    &lt;/</span><span>form</span><span>&gt;</span></span>
<span data-line=""><span>  )</span><span>;</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>Pretty cool! In my opinion, at least. Though you can rightfully argue that this accomplishes only what
the existing <code>required</code> attribute is already capable of. Where’s the “power”?</p>
<p>Let me show you, but first, since there’s no actual <code>custom-validity</code> currently
in the <a href="https://html.spec.whatwg.org/multipage/form-control-infrastructure.html#the-constraint-validation-api">HTML Spec</a>,
let’s implement it in userland.</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="jsx" data-theme="catppuccin-macchiato catppuccin-frappe"><code data-language="jsx" data-theme="catppuccin-macchiato catppuccin-frappe"><span data-line=""><span>function</span><span> Input</span><span>({</span><span> customValidity</span><span>,</span><span> ...</span><span>props</span><span> })</span><span> {</span></span>
<span data-line=""><span>  const</span><span> ref </span><span>=</span><span> useRef</span><span>()</span><span>;</span></span>
<span data-line=""><span>  useLayoutEffect</span><span>(</span><span>()</span><span> =&gt;</span><span> {</span></span>
<span data-line=""><span>    if</span><span> (customValidity </span><span>!=</span><span> null</span><span>) </span><span>{</span></span>
<span data-line=""><span>      const</span><span> input </span><span>=</span><span> ref</span><span>.</span><span>current</span><span>;</span></span>
<span data-line=""><span>      input</span><span>.</span><span>setCustomValidity</span><span>(customValidity)</span><span>;</span></span>
<span data-line=""><span>    }</span></span>
<span data-line=""><span>  },</span><span> [customValidity])</span><span>;</span></span>
<span data-line=""> </span>
<span data-line=""><span>  return</span><span> &lt;</span><span>input</span><span> ref</span><span>=</span><span>{</span><span>ref</span><span>}</span><span> {</span><span>...</span><span>props</span><span>}</span><span> /&gt;</span><span>;</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>This will work well for our demo purposes.<br>
For a production-ready component check out
a more <a href="https://gist.github.com/everdimension/a5c1e991a8a6b6aab060ce349b37b825" target="_blank" rel="nooopener">complete implementation</a>.</p>
<h2 id="the-power">The power</h2>
<p>Now we’ll explore which non-trivial cases this design can help solve.</p>
<p>In real-world apps, validation often gets more complex than local checks.
Imagine a username input that should be <strong>valid only if the username is not taken</strong>.
This would require async calls to your server and an intermediary state: the form
should not be valid while the check is in progress.
Let’s see how our abstraction can handle this.</p>

<p>Play around with this example. It uses the <code>required</code> to prevent empty inputs. But then it relies on <code>customValidity</code> to mark input as invalid during the loading state and based on the response.</p>
<h3 id="implementation">Implementation</h3>
<p>First, we create an async function to check whether the username is unique that imitates a server request with a delay.</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="catppuccin-macchiato catppuccin-frappe"><code data-language="js" data-theme="catppuccin-macchiato catppuccin-frappe"><span data-line=""><span>export</span><span> async</span><span> function</span><span> verifyUsername</span><span>(</span><span>userValue</span><span>)</span><span> {</span></span>
<span data-line=""><span>  // imitate network delay</span></span>
<span data-line=""><span>  await</span><span> new</span><span> Promise</span><span>(</span><span>(</span><span>r</span><span>)</span><span> =&gt;</span><span> setTimeout</span><span>(r</span><span>,</span><span> 3000</span><span>))</span><span>;</span></span>
<span data-line=""><span>  const</span><span> value </span><span>=</span><span> userValue</span><span>.</span><span>trim</span><span>()</span><span>.</span><span>toLowerCase</span><span>()</span><span>;</span></span>
<span data-line=""><span>  if</span><span> (value </span><span>===</span><span> "bad input"</span><span>) </span><span>{</span></span>
<span data-line=""><span>    throw</span><span> new</span><span> Error</span><span>(</span><span>"Bad Input"</span><span>)</span><span>;</span></span>
<span data-line=""><span>  }</span></span>
<span data-line=""><span>  const</span><span> validationMessage </span><span>=</span><span> value </span><span>===</span><span> "taken"</span><span> ?</span><span> "Username is taken"</span><span> :</span><span> ""</span><span>;</span></span>
<span data-line=""><span>  return</span><span> {</span><span> validationMessage </span><span>};</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>Next, we’ll create a controlled form component and use <a href="https://tanstack.com/query/latest">react-query</a> to manage to server request when the input value changes:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="jsx" data-theme="catppuccin-macchiato catppuccin-frappe"><code data-language="jsx" data-theme="catppuccin-macchiato catppuccin-frappe"><span data-line=""><span>import</span><span> {</span><span> useState </span><span>}</span><span> from</span><span> "react"</span><span>;</span></span>
<span data-line=""><span>import</span><span> {</span><span> useQuery </span><span>}</span><span> from</span><span> "@tanstack/react-query"</span><span>;</span></span>
<span data-line=""><span>import</span><span> {</span><span> verifyUsername </span><span>}</span><span> from</span><span> "./verifyUsername"</span><span>;</span></span>
<span data-line=""><span>import</span><span> {</span><span> Input </span><span>}</span><span> from</span><span> "./Input"</span><span>;</span></span>
<span data-line=""> </span>
<span data-line=""><span>function</span><span> Form</span><span>()</span><span> {</span></span>
<span data-line=""><span>  const</span><span> [</span><span>value</span><span>,</span><span> setValue</span><span>]</span><span> =</span><span> useState</span><span>(</span><span>""</span><span>)</span><span>;</span></span>
<span data-line=""><span>  const</span><span> {</span><span> data</span><span>,</span><span> isLoading</span><span>,</span><span> isError </span><span>}</span><span> =</span><span> useQuery</span><span>(</span><span>{</span></span>
<span data-line=""><span>    queryKey</span><span>:</span><span> [</span><span>"verifyUsername"</span><span>,</span><span> value]</span><span>,</span></span>
<span data-line=""><span>    queryFn</span><span>:</span><span> ()</span><span> =&gt;</span><span> verifyUsername</span><span>(value)</span><span>,</span></span>
<span data-line=""><span>    enabled</span><span>:</span><span> Boolean</span><span>(value)</span><span>,</span></span>
<span data-line=""><span>  }</span><span>)</span><span>;</span></span>
<span data-line=""> </span>
<span data-line=""><span>  return</span><span> (</span></span>
<span data-line=""><span>    &lt;</span><span>form</span><span>&gt;</span></span>
<span data-line=""><span>      &lt;</span><span>Input</span></span>
<span data-line=""><span>        name</span><span>=</span><span>"username"</span></span>
<span data-line=""><span>        required</span><span>=</span><span>{</span><span>true</span><span>}</span></span>
<span data-line=""><span>        value</span><span>=</span><span>{</span><span>value</span><span>}</span></span>
<span data-line=""><span>        onChange</span><span>=</span><span>{(</span><span>event</span><span>)</span><span> =&gt;</span><span> {</span></span>
<span data-line=""><span>          setValue</span><span>(event</span><span>.</span><span>currentTarget</span><span>.</span><span>value)</span><span>;</span></span>
<span data-line=""><span>        }}</span></span>
<span data-line=""><span>      /&gt;</span></span>
<span data-line=""><span>      &lt;</span><span>button</span><span>&gt;</span><span>Submit</span><span>&lt;/</span><span>button</span><span>&gt;</span></span>
<span data-line=""><span>    &lt;/</span><span>form</span><span>&gt;</span></span>
<span data-line=""><span>  )</span><span>;</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>Great! We have the setup in place. It consists of two crucial parts:</p>
<ul>
<li>Verification request state managed by <code>useQuery</code></li>
<li>Our custom <code>&lt;Input /&gt;</code> component that is capable of taking the <code>customValidity</code> prop</li>
</ul>
<p>Let’s put those pieces together:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="jsx" data-theme="catppuccin-macchiato catppuccin-frappe"><code data-language="jsx" data-theme="catppuccin-macchiato catppuccin-frappe"><span data-line=""><span>import</span><span> {</span><span> useState </span><span>}</span><span> from</span><span> "react"</span><span>;</span></span>
<span data-line=""><span>import</span><span> {</span><span> useQuery </span><span>}</span><span> from</span><span> "@tanstack/react-query"</span><span>;</span></span>
<span data-line=""><span>import</span><span> {</span><span> verifyUsername </span><span>}</span><span> from</span><span> "./verifyUsername"</span><span>;</span></span>
<span data-line=""><span>import</span><span> {</span><span> Input </span><span>}</span><span> from</span><span> "./Input"</span><span>;</span></span>
<span data-line=""> </span>
<span data-line=""><span>function</span><span> Form</span><span>()</span><span> {</span></span>
<span data-line=""><span>  const</span><span> [</span><span>value</span><span>,</span><span> setValue</span><span>]</span><span> =</span><span> useState</span><span>(</span><span>""</span><span>)</span><span>;</span></span>
<span data-line=""><span>  const</span><span> {</span><span> data</span><span>,</span><span> isLoading</span><span>,</span><span> isError </span><span>}</span><span> =</span><span> useQuery</span><span>(</span><span>{</span></span>
<span data-line=""><span>    queryKey</span><span>:</span><span> [</span><span>"verifyUsername"</span><span>,</span><span> value]</span><span>,</span></span>
<span data-line=""><span>    queryFn</span><span>:</span><span> ()</span><span> =&gt;</span><span> verifyUsername</span><span>(value)</span><span>,</span></span>
<span data-line=""><span>    enabled</span><span>:</span><span> Boolean</span><span>(value)</span><span>,</span></span>
<span data-line=""><span>  }</span><span>)</span><span>;</span></span>
<span data-line=""> </span>
<span data-line="" data-highlighted-line=""><span>  const</span><span> validationMessage </span><span>=</span><span> data</span><span>?.</span><span>validationMessage</span><span>;</span></span>
<span data-line=""> </span>
<span data-line=""><span>  return</span><span> (</span></span>
<span data-line=""><span>    &lt;</span><span>form</span><span>&gt;</span></span>
<span data-line=""><span>      &lt;</span><span>Input</span></span>
<span data-line=""><span>        name</span><span>=</span><span>"username"</span></span>
<span data-line=""><span>        required</span><span>=</span><span>{</span><span>true</span><span>}</span></span>
<span data-line="" data-highlighted-line=""><span>        customValidity</span><span>=</span><span>{</span></span>
<span data-line="" data-highlighted-line=""><span>          isLoading</span></span>
<span data-line="" data-highlighted-line=""><span>            ?</span><span> "Verifying username..."</span></span>
<span data-line="" data-highlighted-line=""><span>            :</span><span> isError</span></span>
<span data-line="" data-highlighted-line=""><span>            ?</span><span> "Could not verify"</span></span>
<span data-line="" data-highlighted-line=""><span>            :</span><span> validationMessage</span></span>
<span data-line="" data-highlighted-line=""><span>        }</span></span>
<span data-line=""><span>        value</span><span>=</span><span>{</span><span>value</span><span>}</span></span>
<span data-line=""><span>        onChange</span><span>=</span><span>{(</span><span>event</span><span>)</span><span> =&gt;</span><span> {</span></span>
<span data-line=""><span>          setValue</span><span>(event</span><span>.</span><span>currentTarget</span><span>.</span><span>value)</span><span>;</span></span>
<span data-line=""><span>        }}</span></span>
<span data-line=""><span>      /&gt;</span></span>
<span data-line=""><span>      &lt;</span><span>button</span><span>&gt;</span><span>Submit</span><span>&lt;/</span><span>button</span><span>&gt;</span></span>
<span data-line=""><span>    &lt;/</span><span>form</span><span>&gt;</span></span>
<span data-line=""><span>  )</span><span>;</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>That’s it! We’re describing the whole async validation flow, including
loading, error and success states, <em>in one attribute</em>. You can go back to see <a href="#example-async-username">the result</a> again if you wish</p>
<h3 id="one-more">One more</h3>
<p>This one will be shorter, but also interesting, because it covers dependent input
fields. Let’s implement a form that requires to repeat the entered password:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="jsx" data-theme="catppuccin-macchiato catppuccin-frappe"><code data-language="jsx" data-theme="catppuccin-macchiato catppuccin-frappe"><span data-line=""><span>import</span><span> {</span><span> useState </span><span>}</span><span> from</span><span> "react"</span><span>;</span></span>
<span data-line=""><span>import</span><span> {</span><span> Input </span><span>}</span><span> from</span><span> "./Input"</span><span>;</span></span>
<span data-line=""> </span>
<span data-line=""><span>function</span><span> ConfirmPasswordForm</span><span>()</span><span> {</span></span>
<span data-line=""><span>  const</span><span> [</span><span>password</span><span>,</span><span> setPassword</span><span>]</span><span> =</span><span> useState</span><span>(</span><span>""</span><span>)</span><span>;</span></span>
<span data-line=""><span>  const</span><span> [</span><span>confirmedPass</span><span>,</span><span> setConfirmedPass</span><span>]</span><span> =</span><span> useState</span><span>(</span><span>""</span><span>)</span><span>;</span></span>
<span data-line=""> </span>
<span data-line="" data-highlighted-line=""><span>  const</span><span> matches </span><span>=</span><span> confirmedPass </span><span>===</span><span> password</span><span>;</span></span>
<span data-line=""><span>  return</span><span> (</span></span>
<span data-line=""><span>    &lt;</span><span>form</span><span>&gt;</span></span>
<span data-line=""><span>      &lt;</span><span>Input</span></span>
<span data-line=""><span>        type</span><span>=</span><span>"password"</span></span>
<span data-line=""><span>        name</span><span>=</span><span>"password"</span></span>
<span data-line=""><span>        required</span><span>=</span><span>{</span><span>true</span><span>}</span></span>
<span data-line=""><span>        value</span><span>=</span><span>{</span><span>password</span><span>}</span></span>
<span data-line=""><span>        onChange</span><span>=</span><span>{(</span><span>event</span><span>)</span><span> =&gt;</span><span> {</span></span>
<span data-line=""><span>          setPassword</span><span>(event</span><span>.</span><span>currentTarget</span><span>.</span><span>value)</span><span>;</span></span>
<span data-line=""><span>        }}</span></span>
<span data-line=""><span>      /&gt;</span></span>
<span data-line=""><span>      &lt;</span><span>Input</span></span>
<span data-line=""><span>        type</span><span>=</span><span>"password"</span></span>
<span data-line=""><span>        name</span><span>=</span><span>"confirmedPassword"</span></span>
<span data-line=""><span>        required</span><span>=</span><span>{</span><span>true</span><span>}</span></span>
<span data-line=""><span>        value</span><span>=</span><span>{</span><span>confirmedPass</span><span>}</span></span>
<span data-line="" data-highlighted-line=""><span>        customValidity</span><span>=</span><span>{</span><span>matches </span><span>?</span><span> ""</span><span> :</span><span> "Password must match"</span><span>}</span></span>
<span data-line=""><span>        onChange</span><span>=</span><span>{(</span><span>event</span><span>)</span><span> =&gt;</span><span> {</span></span>
<span data-line=""><span>          setConfirmedPass</span><span>(event</span><span>.</span><span>currentTarget</span><span>.</span><span>value)</span><span>;</span></span>
<span data-line=""><span>        }}</span></span>
<span data-line=""><span>      /&gt;</span></span>
<span data-line=""><span>      &lt;</span><span>button</span><span>&gt;</span><span>Submit</span><span>&lt;/</span><span>button</span><span>&gt;</span></span>
<span data-line=""><span>    &lt;/</span><span>form</span><span>&gt;</span></span>
<span data-line=""><span>  )</span><span>;</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>You can try it out:</p>

<h2 id="conclusion">Conclusion</h2>
<p>I hope I’ve been able to show you how <code>setCustomValidity</code> can cover
validation needs of all kinds.</p>
<p>But the real power comes from great APIs.</p>
<p>And hopefully, you are now equipped with one of those.<br>
And even more hopefully, we will see it natively in the HTML Spec one day.</p>
<!-- -->
<!-- -->
<!-- -->
<!-- -->
<!-- -->
<!-- -->
<!-- -->
<!-- -->
<!-- -->
<!-- -->
<!-- -->
<!-- -->
<!-- -->
<div><p><img src="https://pbs.twimg.com/profile_images/826440416371302400/HosO7Uze_400x400.jpg" alt=""></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A return to hand-written notes by learning to read and write (544 pts)]]></title>
            <link>https://research.google/blog/a-return-to-hand-written-notes-by-learning-to-read-write/</link>
            <guid>41976311</guid>
            <pubDate>Mon, 28 Oct 2024 21:08:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://research.google/blog/a-return-to-hand-written-notes-by-learning-to-read-write/">https://research.google/blog/a-return-to-hand-written-notes-by-learning-to-read-write/</a>, See on <a href="https://news.ycombinator.com/item?id=41976311">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-gt-publish-date="20241028">
                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="">
    




    <p data-block-key="sge1e">Digital note-taking is gaining popularity, offering a durable, editable, and easily indexable way of storing notes in a vectorized form. However, a substantial gap remains between digital note-taking and traditional pen-and-paper note-taking, a practice still favored by a majority of people.</p><p data-block-key="8vfmv">Bridging this gap by converting a note taker’s physical writing into a digital form is a process called derendering. The result is a sequence of strokes, or trajectories of a writing instrument like a pen or finger, recorded as points and stored digitally. This is also known as an “online” representation of writing, or “digital ink”.</p><p data-block-key="7jll9">The conversion to digital ink offers users who still prefer traditional handwritten notes access to their notes in a digital form. Instead of simply using <a href="https://en.wikipedia.org/wiki/Optical_character_recognition" target="_blank" rel="noopener noreferrer">optical character recognition</a> (OCR), which would allow the writing to be transcribed to a text document, by capturing the handwritten documents as a collection of strokes, it's possible to reproduce them in a form that can be edited freely by hand in a way that is more natural. It allows the user to create documents with a realistic look that captures their handwriting style, rather than simply a collection of text. This representation allows the user to later inspect, modify or complete their handwritten notes, which gives their notes enhanced durability, seamless organization and integration with other digital content (images, text, links) or digital assistance.</p><p data-block-key="durd6">For these reasons, this field has gained significant interest in both <a href="https://arxiv.org/abs/2009.04284" target="_blank" rel="noopener noreferrer">academia</a> and <a href="https://support.gingerlabs.com/hc/en-us/articles/5044440428570-Image-to-Ink-Conversion" target="_blank" rel="noopener noreferrer">industry</a>, with software solutions that digitize handwriting and hardware solutions that leverage <a href="https://us.livescribe.com/" target="_blank" rel="noopener noreferrer">smart pens</a> or <a href="https://getrocketbook.com/" target="_blank" rel="noopener noreferrer">special paper</a> for capture. The need for additional hardware and accompanying software stack is, however, an obstacle for wider adoption, as it creates both onboarding friction and carries additional expense for the user.</p><p data-block-key="fdelg">With this in mind, in “<a href="https://arxiv.org/abs/2402.05804" target="_blank" rel="noopener noreferrer">InkSight: Offline-to-Online Handwriting Conversion by Learning to Read and Write</a>”, we propose an approach to derendering that can take a picture of a handwritten note and extract the strokes that generated the writing without the need for specialized equipment. We also remove the reliance on typical geometric constructs, where gradients, contours, and shapes in an image are utilized to extract writing strokes. Instead, we train the model to build an understanding of “reading”, so it can recognize written words, and “writing”, so it can output strokes that resemble handwriting. This results in a more robust model that performs well across diverse scenarios and appearances, including challenging lighting conditions, occlusions, etc. You can access the model and the inference code on our <a href="https://github.com/google-research/inksight/" target="_blank" rel="noopener noreferrer">GitHub repo</a>.</p>
</div>

                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="">
        
            
                <h2>Overview</h2>
            
        
        
            <p data-block-key="69haq">The key goal of this approach is to capture the stroke-level trajectory details of handwriting. The user can then store the resulting strokes in the note taking app of their choice.</p>
        
    </div>

                    
                    
    




                    
                    
    


<div>
        
  <p data-block-key="4vw5g">Under the hood, we apply an off the shelf OCR model to identify handwritten words, then use the model to convert them to strokes. To foster reproducibility, reusability, and ease of adoption, we combine the widely popular and readily available <a href="https://arxiv.org/abs/2010.11929" target="_blank" rel="noopener noreferrer">ViT</a> encoder with an <a href="https://arxiv.org/abs/2010.11934" target="_blank" rel="noopener noreferrer">mT5</a> encoder-decoder.</p>

    </div>

                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="">
    


    <p>
        
            
                <h2>Challenges</h2>
            
        
        
    </p>



    <p data-block-key="4vw5g">While the fundamental concept of derendering appears straightforward — training a model that generates digital ink representations from input images — the practical implementation for arbitrary input images presents two significant challenges:</p><ol><li data-block-key="jn7b"><i>Limited Supervised Data:</i> Acquiring paired data with corresponding images and ground truth digital ink for supervised training can be expensive and time-consuming. To our knowledge, no datasets with sufficient variety exist for this task.</li><li data-block-key="5khqr"><i>Scalability to large images:</i> The model must effectively handle arbitrarily large input images with varying resolutions and amount of content.</li></ol>
</div>

                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="">
    


    <p>
        
            
                <h2>Method</h2>
            
        
        
    </p>



    
</div>

                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="">
    


    <p>
        
            
                <h3>Learning to read and write</h3>
            
        
        
    </p>



    <p data-block-key="4vw5g">To address the first problem while avoiding onerous data collection, we propose a multi-task training setup that combines recognition and derendering tasks. This enables the model to generalize on derendering tasks with various styles of images as input, and injects the model with both semantic understanding and knowledge of the mechanics of writing handwritten text.</p><p data-block-key="9d2mr">This approach thus differs from methods that rely on geometric constructs, where gradients, contours, and shapes in an image are utilized to extract writing strokes. Learning to read enhances the model's capability in precisely locating and extracting textual elements from the images. Learning to write ensures that the resulting vector representation, the digital ink, closely aligns with the typical human approach of writing in terms of physical dynamics and the order of strokes. Combined, these allow us to train a model in the absence of large amounts of paired samples, which are difficult to obtain.</p>
</div>

                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="">
    


    <p>
        
            
                <h3>System workflow</h3>
            
        
        
    </p>



    <p data-block-key="4vw5g">One solution to the problem of scalability is to train a model with very high-resolution input images and very long output sequences. However, this is computationally prohibitive. Instead, we break down the derendering of a page of notes into three steps: (1) OCR to extract word-level bounding boxes, (2) derendering each of the words separately, and (3) replacing the offline (pixel) representation of the words with the derendered strokes using the color coding described above to improve visualization.</p><p data-block-key="eq6q1">To narrow the domain gap between the synthetic images of rendered inks and the real photos, we augment the data in tasks that take rendered ink as input. Data augmentation is done by randomizing the ink angle, color, stroke width, and by adding Gaussian noise and cluttered backgrounds.</p>
</div>

                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="">
    


    <p>
        
            
                <h2>Vision-language model for digital ink</h2>
            
        
        
    </p>



    <p data-block-key="4vw5g">We create a training mixture that comprises five different task types. The first two tasks are derendering tasks (i.e., they generate a digital ink output). One uses only an image as input and the other uses both an image and the accompanying text that has been recognized by the OCR model. The following two tasks are recognition tasks that produce text output, the first of which leverages real images and the latter, synthetic ones. Finally, a fifth task is a combination of recognition and derendering, hence a mixed task with text-and-ink output.</p><p data-block-key="2aaog">Each type of task utilizes a task-specific input text, enabling the model to distinguish between tasks during both training and inference. Below you will find a recognition and a derendering task.</p>
</div>

                    
                    
    




                    
                    
    




                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="">
    




    <p data-block-key="4vw5g">To train the system, we pair images of text and corresponding digital ink. The digital ink is sampled from real-time writing trajectories and subsequently represented as a sequence of strokes. Each stroke is represented by a sequence of points, obtained by sampling from the writing or drawing trajectory at a constant rate (e.g., 50 points per second). The corresponding image is created by rendering the ink - creating a bitmap at a prespecified resolution. This creates a pixel-stroke correspondence, that is a precursor for the model input-output pairs.<br></p><p data-block-key="5caji">A further necessary step, and a unique one for this modality, is the ink tokenizer, which represents the points in a format that is friendly to a large language model (LLM). Each point is converted into two tokens, one each encoding its <i>x</i> and <i>y</i> coordinates. The token sequence for this ink begins with <i>b</i>, signifying the beginning of the stroke, followed by the tokens for the coordinates of the sampled points.</p>
</div>

                    
                    
    




                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="">
    


    <p>
        
            
                <h2>Results</h2>
            
        
        
    </p>



    <p data-block-key="4vw5g">To evaluate the performance of our approach, we first collected an evaluation dataset. We started with OCR data, and then added paired samples that we collected manually by asking people to trace text images they were shown (human-generated traces).</p><p data-block-key="bfisu">We then trained three variants of the model: Small-p (∼340M parameters, “-p” for “public” setup), Small-i (“-i” for “in-house”), and Large-i (∼1B parameters). We compared our approach to a <a href="https://esslab.jp/publications/HaoranSIGRAPH2021.pdf" target="_blank" rel="noopener noreferrer">General Virtual Sketching</a> (GVS) baseline.</p><p data-block-key="eh6dt">We show that the vector representations produced by our system are both semantically and geometrically similar to the input images, and are similar to human-generated digital ink data, as measured by both automatic and human evaluations.</p>
</div>

                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="">
    


    <p>
        
            
                <h3>Qualitative evaluation</h3>
            
        
        
    </p>



    <p data-block-key="4vw5g">We show the performance of our models and GVS compared to two public evaluation datasets, <a href="https://fki.tic.heia-fr.ch/databases/iam-handwriting-database" target="_blank" rel="noopener noreferrer">IAM</a> and <a href="https://paperswithcode.com/dataset/imgur5k" target="_blank" rel="noopener noreferrer">IMGUR5K</a>, and an out of domain dataset of sketches. Our models mostly produce results that accurately reflect the text content, disregarding semantically irrelevant background. They can also handle occlusions, highlighting the benefit of the learned reading prior. In contrast, GVS produces multiple duplicate strokes and has difficulty distinguishing between background and foreground. Our Large-i model is further able to retain more details and accommodate more diverse image styles. See the <a href="https://arxiv.org/abs/2402.05804" target="_blank" rel="noopener noreferrer">paper</a> for more examples.</p>
</div>

                    
                    
    




                    
                    
    




                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="">
    


    <p>
        
            
                <h3>Quantitative evaluation</h3>
            
        
        
    </p>



    <p data-block-key="4vw5g">At present, the field has not established metrics or benchmarks for quantitative evaluation of this task. So, we conduct both human and automated evaluation to compare the similarity of our model output to the original image and to human-generated digital inks.</p><p data-block-key="cf74k">Here we present the human evaluation results, with numerous other results derived from automated evaluations and an ablation study in our <a href="https://arxiv.org/abs/2402.05804" target="_blank" rel="noopener noreferrer">paper</a>. We performed a human evaluation of the quality of the derendered inks produced by the three model variants. We used the “golden” human traced data from the <a href="https://github.com/google-research-datasets/hiertext" target="_blank" rel="noopener noreferrer">HierText dataset</a> as the control group and the output of our model on these samples as the experimental group.</p>
</div>

                    
                    
    




                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="">
    




    <p data-block-key="4vw5g">In the figure above, notice the error in the quote for all models on the top row (the double-quote mark), which the human tracing got correct. On the bottom row the situation is reversed, with the human tracing focusing solely on the main word, missing most other elements. The human tracing is also not perfectly aligned with the underlying image, emphasizing the complexity and tracing difficulty of the handwritten parts of the HierText dataset.</p><p data-block-key="2qlkk">Evaluators were shown the original image alongside a rendered digital ink sample, which was either model-generated or human-traced (unknown to the evaluators). They were asked to answer two questions: (1) Is the digital ink output a reasonable tracing of the input image? (Answers: “Yes, it’s a good tracing,” “It’s an okay tracing, but has some small errors,” “It’s a bad tracing, has some major artifacts.”) (2) Could this digital ink output have been produced by a human? (Answers: “Yes” or “No”.) The evaluation included 16 individuals familiar with digital ink, but not involved in this research. Each sample was evaluated by three raters and aggregated with majority voting.</p>
</div>

                    
                    
    




                    
                    
    


<div>
        
  <p data-block-key="4vw5g">The results show that a majority of derendered inks, generated with the Large-i model perform about as well as human-generated ones. Moreover 87% of the Large-i outputs are marked as good or having only small errors.</p>

    </div>

                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="">
    


    <p>
        
            
                <h2>Conclusion</h2>
            
        
        
    </p>



    <p data-block-key="4vw5g">In this work we present a first-of-its-kind approach to convert photos of handwriting into digital ink. We propose a training setup that works without paired training data. We show that our method is robust to a variety of inputs, can work on full handwritten notes, and generalizes to out-of-domain sketches to some extent. Furthermore, our approach does not require complex modeling and can be constructed from standard building blocks.</p>
</div>

                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="">
    


    <p>
        
            
                <h2>Acknowledgements</h2>
            
        
        
    </p>



    <p data-block-key="4vw5g"><i>We want to thank all the authors of this work,</i> <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Rak,+A" target="_blank" rel="noopener noreferrer"><i>Arina Rak</i></a><i>,</i> <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Schnitzler,+J" target="_blank" rel="noopener noreferrer"><i>Julian Schnitzler</i></a><i>, and</i> <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+C" target="_blank" rel="noopener noreferrer"><i>Chengkun Li</i></a><i>, who formed a student team working with Google Research for the duration of the project, as well as Claudiu Musat, Henry Rowley and Jesse Berent. All authors, with the exception of the student team, are now part of Google Deepmind.</i></p>
</div>

                    
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why so few Matt Levines? (238 pts)]]></title>
            <link>https://gwern.net/matt-levine</link>
            <guid>41975993</guid>
            <pubDate>Mon, 28 Oct 2024 20:37:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gwern.net/matt-levine">https://gwern.net/matt-levine</a>, See on <a href="https://news.ycombinator.com/item?id=41975993">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page-metadata">
        
        <p>Why are popularizing educational newsletter-frequency writers of important fields like Matt Levine for finance so rare? Because most fields are too slow or ambiguous, and writers of the right combination of expertise, obsession, and persistence are also rare.</p>
        
      </div><div id="markdownBody">
        
        <p><a href="https://en.wikipedia.org/wiki/Matt_Levine_(columnist)" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Matt_Levine_(columnist)#bodyContent" title="Matt Levine (columnist)">Matt Levine</a> is the most well-known newslettrist (<a href="https://www.bloomberg.com/opinion/authors/ARbTQlRLRjE/matthew-s-levine" id="levine-2024" data-link-icon="𝐁" data-link-icon-type="text" title="‘Matt Levine, Bloomberg Opinion Columnist’, Levine 2024">“Money Stuff”</a>) in the financial industry, having blogged or written since <span>2011<sub><span title="2011 was 13 years ago.">13ya</span></sub></span>, finding his niche in popularization after stints in Wall Street &amp; law. His commentary is influential, people leak to him, he sometimes interviews major figures (notoriously, Sam Bankman-Fried) or recounts inside information, and a number of phrases like his “laws of insider trading” (specifically, how not to) have gained currency to the point where readers can now do much of the work of sourcing an issue for him.</p>
        <p>He is read by hundreds of thousands of readers (including myself)—everyone from shoeshine boy to billionaire. The size of his audience is respectable, but perhaps its most remarkable feature is that many of those readers have nothing to do with the financial industry. Though his newsletter is officially a Bloomberg News newsletter which he simply writes, many of his readers will visit Bloomberg solely for him, and indeed, might have little idea who or what a Bloomberg is. Nevertheless, readers loyally tune in for each installment every few days to learn about arcane financial instruments they have never heard of before, and (except for Levine) never will again.</p>
        <p>One might ask (and indeed, a billionaire once did), “where are the other Matt Levines?” or <a href="https://www.reddit.com/r/slatestarcodex/comments/hze13t/who_are_the_matt_levines_of_other_fields/" data-link-icon="reddit" data-link-icon-type="svg" data-url-html="https://old.reddit.com/r/slatestarcodex/comments/hze13t/who_are_the_matt_levines_of_other_fields/">“who are the Matt levines of other fields?”</a> That is, where are the Matt Levines of, say, chemistry or drug development<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a>, who explain &amp; popularize other major industries which are vital to modern life, directly or indirectly appear in the news often, and yet people are widely ignorant of it, and deeply misunderstand its fundamental dynamics? Why don’t we have a Matt Levine for every industry? Where is the Levine of, I don’t know, petroleum refining or fracking, of shipping containers? Are we just in need of a good list of recommendations? Or could we just set up a prize to coax out some potential Levines in other industries?</p>
        <div>
          <blockquote>
            <p>When I first met Matt, the first thing I said was “Matt Levine, only you can do what you do!”</p>
            <p><a href="https://marginalrevolution.com/marginalrevolution/2024/09/sunday-assorted-links-488.html" id="cowen-2024" data-link-icon="M𝐑" data-link-icon-type="text" title="'Sunday assorted links', Tyler Cowen 2024-09-29">Tyler Cowen</a></p>
          </blockquote>
        </div>
        <p>My pessimistic conclusion is that Matt Levines are not made, they are born, and that the Matt Levine formula is largely irreproducible: there are few industries where it makes sense, and there are few people suited for this job, and that is the simple answer why there are not many Levines.</p>
        <p>So, what is the formula, exactly? The Matt Levine formula is weighty matters, leavened by humor, with basic explanations of complicated financial matters. As Levine has been doing this online for so long (~13 years), relatively speaking, he can often refer to his previous coverage and comment on how things turned out. Certain themes repeat periodically so often that they receive their own catchphrases, like “worries about bond market liquidity” or the laws of insider trading.</p>
        <p>Many people owe most of what they know about stock trading, bonds, arcane but controversial matters like naked shorts, meme stocks etc to Levine; and I would be embarrassed to admit how much of my economics knowledge comes through Levine rather than some more rigorous source like my old economics textbooks. This is because Levine provides 3 key ingredients which foster learning:</p>
        <ol>
          <li>
            <p>cases with <strong>known outcomes/answers</strong>: to develop expertise in a subject, the subject ideally provides many problems, with known answers, of high accuracy. Most subjects do <em>not</em>. But Levine’s subject (finance &amp; law) does.</p>
            <p>A good subject for developing expertise is something like chess: an endless number of chess games can be played rapidly, they all have a clear outcome (win/draw/loss), and one can study each one carefully to understand what went right or wrong. A bad subject is something like military strategy: there are not many large-scale wars which have been documented adequately, each war is unique and unrepeatable and a general may participate in only a few in a lifetime, and the outcomes (never mind any individual’s contribution) are often difficult or impossible to judge. Many areas are more like military strategy than chess—how do you judge the expertise of a CEO, or a Hollywood director, or a scientist forecasting the distant future?</p>
            <p>Levine works in an area which <em>does</em> provide many clearcut examples, because he focuses on lawsuits, prosecutions, crimes, and deals. These are examples where the outcome will usually be known in a few years, at most, or at least a major update/development, and where the involved parties do all the research necessary, and where the evidence is often completely unambiguous—Levine just has to read their filings, and excerpt the text message where someone boasts about their insider trading in no uncertain terms.</p>
            <p>In relying on reporting &amp; filings for his commentary, Levine is very much like the dying local newspaper crime reporter, who relies on the police blotter &amp; courtroom access to rapidly file their articles. These articles are nearly endless, and mostly forgettable because there are, broadly speaking, not really any <em>principles</em> governing local crime. “Some guy got drunk and into a fight and killed another guy” is something that happens frequently, but it illuminates no universal principle; it sheds no light on anything else. It was just something horrible that often happens at random when people take dangerous drugs like alcohol rather than safe ones like <a href="https://gwern.net/nicotine" id="gwern-nicotine" title="'Nicotine', Gwern 2011">nicotine</a>, and is of no broader importance; true-crime addicts consume it for its entertainment value, like darker versions of Hollywood tabloids—“who murdered who” instead of “who’s sleeping with who”. It’s just “one d—n thing after another”. (The TV show <a href="https://en.wikipedia.org/wiki/Cops_(TV_program)" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Cops_(TV_program)#bodyContent" title="Cops (TV program)"><em>Cops</em></a> has run for 36 years now, and could run for another 36 years without breaking a sweat, but after 72 seasons, what would you have <em>learned</em> that you didn’t learn after the first few?)</p>
            <p>But in Levine’s area, this is not the case. Many of these examples are due to highly intelligent, motivated, competent people and organizations clashing for deep reasons. This means that to understand them, you need…</p>
          </li>
          <li>
            <p><strong>first-principles explanations</strong>: most people experience an “illusion of depth”, in that they believe they understand the causal mechanics of an area far better than they do. But in fact, they have learned only a superficial model of the area. Levine corrects this.</p>
            <p>Particularly in economic matters, people believe many intuitive folk economics like eg. building new houses cannot lower prices, or that businesses raise prices simply “when they feel greedy”, or that voluntary transactions must have a loser &amp; a winner when other people transact (but not themselves personally), or that a policy will have only intended effects because everyone will just do what they are ordered to (even though they personally work around policies or rules all the time for doubtless noble reasons).</p>
            <p>Levine patiently gives from first-principles (supply-and-demand, market efficiency &amp; adverse selection, people following incentives, <a href="https://en.wikipedia.org/wiki/Public_choice" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Public_choice#bodyContent" title="Public choice">public choice theory</a>) explanations of why some thing about markets or contracts is the way it is, how it operated (or failed to operate) in a particular case, and what (and why) the various counterfactual future outcomes are.</p>
            <p>These repeated explanations—however simplified and abstracted—gradually build up genuine knowledge which can transfer to the real world beyond some crammed supply-and-demand schematics in a long-forgotten economics class.</p>
          </li>
          <li>
            <p><a href="https://gwern.net/spaced-repetition" id="gwern-spaced-repetition" title="‘Spaced Repetition for Efficient Learning’, Gwern 2009">spaced repetition</a> enabled by <strong>fast turnover</strong>: a newsletter is inherently spaced in time, and by returning to themes repeatedly, with various twists or instantiations, the reader learns due to the spacing effect.</p>
            <p>In normal news consumption, as opposed to the drip-feed of a columnist on a steady beat, one might read about some instance of financial malfeasance in great depth in the <em>WSJ</em> or <em>NYT</em>, say—but once.</p>
            <p>This coverage might be extremely high quality, but nevertheless, such “massed presentation” is a recipe for forgetting. It is like cramming flashcards the night before the test: no matter how good the flashcards are or how much you remember while taking the test, most of it will be forgotten.</p>
            <p>However, in Levine’s case, even if specific cases or events resolve quickly, the same principle will show up again soon enough.</p>
          </li>
        </ol>
        <p>So, that is the Levine formula: the global economy furnishes him many rapidly-resolved examples which he can use to entertainingly illustrate basic principles of economics, and by doing that so regularly over so long, readers gain a genuine durable education in economics which they will remember and where they can apply those principles on their own.</p>
        <p>Analyzed into parts, we can see why many areas cannot support a Levine: they lack one of the 3 ingredients:</p>
        <ul>
          <li>
            <p>Crime reporting covers crimes which are numerous and rapidly-resolved, but there is not much to learn.</p>
          </li>
          <li>
            <p>Logistics like fracking or oil or containers may cover many cases which have broad principles, but those cases are often resolved in secrecy and due to the extreme boom-bust cycle of those industries, may take decades to ‘mature’ (eg. an over-extended oil company might not go bust for decades depending on how exactly cycles play out).</p>
          </li>
          <li>
            <p>And areas like drug development may be cursed by all 3: drug development often ends in failure for unknown reasons, in the dark, decades later, and what reasons are known may be totally idiosyncratic to a specific drug or disease; what is known may be at best a loose rule of thumb.</p>
            <p>It would be nice to have a blog like Matt Levine covering, say, evolutionary biology or Greco-Roman philosophy, but it’s obvious why that isn’t going to work—you can’t have a very entertaining <em>news</em>letter when it might take centuries for a debate to resolve, if anyone can agree it was resolved at all, and you certainly aren’t going to be able to provide so many clear illustrations of basic principles that reading the newsletter constitutes an education. (You can read &amp; learn about them, but their natural form would be, well, a textbook or a monograph or something like that, and definitely not a newsletter.)</p>
            <p>And most areas are more like drug development than they are like hedge funds suing each other over some contractual gimmick or clerical error.</p>
          </li>
        </ul>
        <p>OK, but surely there are still plenty of areas where the preconditions are met? (Particularly rapidly-developing ones, like cryptocurrency or AI recently?) So where are <em>their</em> Matt Levines?</p>
        <p>This brings us to the second half of the equation of the Matt Levine formula: the Matt Levine part.</p>
        <p>Consider the implication of the 3 requirements for the <em>author</em>, rather than the reader: they are going to see the same human comedies play out, again and again, and have to shout it into the void again, only to watch it happen yet again. The author feels the weight of the repetition far more than any reader does.<a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a> It is like a teacher who must teach the same curriculum for the 30<sup>th</sup> time—and again read &amp; grade each of dozens of assignments on it by hundreds of students.</p>
        <p>It is not every person who can do so well, or at all. Often a great expert will make a terrible teacher, because they are unable to endure the repetition, or understand the ignorance of the beginner, or treat the floundering student with kindness.</p>
        <p>I personally appreciate Levine’s <a href="https://slatestarcodex.com/2013/06/30/the-lottery-of-fascinations/" data-link-icon="SSC" data-link-icon-type="text,tri" title="The Lottery of Fascinations">permanent fascination with finance</a>, his willingness to explain the same things over and over. But I don’t think I (or most people) would be able to do so for a long time without turning it into a dull ticket-punching exercise as part of a mundane job rather than <em>an avocation</em>; and indeed, I have shunned my opportunities to become a Levine of some area, when I felt my interest &amp; patience for fools rapidly waning. (Particularly darknet markets: there was considerable demand for commentators like Eileen Ormsby or DeepDotWeb, but I could see no new principles to maintain my interest and just an endless infosec churn of temporary trivia—a warning that burnout was approaching—and quit the area while I was ahead.)</p>
        <p>Indeed, expertise is a reason to stop teaching entirely: if you are really interested in an area, and good at it (good enough to understand and commentate ongoing events), then even if you are a great teacher who is gifted at explaining the area to laymen, why would you settle for teaching <em>about</em> it instead of <em>doing</em> it? But if you do it, then you will struggle to write about it regularly publicly: actually doing something, instead of reading a court summary of it, can take years of hard work, and prohibit you from writing about it in various ways. (It will also usually pay much worse—Matt Levine is doubtless compensated handsomely by Bloomberg, but perhaps not as handsomely as if he had kept rising through Big Law, and superstar outcomes imply most would-be newslettrists are paid peanuts.)</p>
        <p>So you have a serious problem: anyone good enough to be ‘the Matt Levine of an area’ is also under considerable pressure to <em>not</em> be him.</p>
        <p>Why would anyone want to? Well, if you ask someone like <a href="https://en.wikipedia.org/wiki/Richard_Feynman" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Richard_Feynman#bodyContent" title="Richard Feynman">Richard Feynman</a> or Andrej Karpathy or Tom Lehrer why they pursued pedagogy instead of the professional pursuits that brought them fame &amp; wealth, the answer would have to be that “they love to teach”. Which is a fine reason, but a passion for teaching a particular subject is far from common.</p>
        <p>So you have a filter with many layers: you need areas which fulfill a stringent set of conditions for such an educational newsletter, and you need a very unusual sort of individual, someone who is expert in the area and has preferably gotten their hands dirty, who is good enough to work professionally in it, but who also is capable of explaining it well, at a beginner level, many times, endlessly without burning out or getting bored, because of their intense interest in the area (but again, not quite intense enough to make them go do it instead of write about it).</p>
        <p>Each step here filters out most candidates, and by the end, there’s just not that much left. You can’t fix these filters easily. No prize or Substack tweak will suddenly make drug discovery happen fast and fail for clear reasons, or conjure up a Levine in a specific area when you want one.</p>
        <p>So, that’s why there are so few Matt Levines, and explains where the other Matt Levines are: they don’t, and usually can’t, exist.</p>
        <section id="footnotes" role="doc-endnotes">
          <hr>
          <ol>
            <li id="fn1">
              <p>While I enjoy <a href="https://www.science.org/blogs/pipeline" data-link-icon="S" data-link-icon-type="text">Derek Lowe</a>, the extent to which his posts are inside-baseball and do not repeat themes, or only repeat many years apart, emphasize the contrast with Levine.<a href="#fnref1" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn2">
              <p>I know as an author, one thing that has surprised me is the extent to which you have to repeat something before a reader will remember it, in large part because they never read it in the first place. I often feel exhausted by discussing something, and feel like I must look like a crank ranting on about an obsession and thoroughly worn out my readers’ tolerance, when a reader says they just heard of it for the first time. This is because as an author, you know every time you wrote about something, across all the years, but a reader may well have read <em>none</em> of them. So if you repeat yourself only as often as you can bear to, you have usually fallen short of the mark.<a href="#fnref2" role="doc-backlink">↩︎</a></p>
            </li>
          </ol>
        </section>
        <section id="similars-section">
          <h2><a href="#similars-section" title="Link to section: § 'Similar Links'">Similar Links</a></h2><a id="similars" href="https://gwern.net/metadata/annotation/similar/%252Fmatt-levine.html" title="Similar links for this link (by text embedding). Lazily-transcluded version at footer of page for easier scrolling." data-link-icon="≈" data-link-icon-type="text">[Similar links by topic]</a>
        </section>
        <section id="link-bibliography-section">
          <h2><a href="#link-bibliography-section" title="Link to section: § 'Bibliography'">Bibliography</a></h2><!-- NOTE: In theory, '.collapse' on a '<h1>' is redundant with the '<section>'; but added to parallel Pandoc-generated headers which set all attributes/classes on both. -->
          <a id="link-bibliography" href="https://gwern.net/metadata/annotation/link-bibliography/%252Fmatt-levine.html" title="Bibliography of links cited in this page (forward citations). Lazily-transcluded version at footer of page for easier scrolling." data-link-icon="bibliography" data-link-icon-type="svg">[Bibliography of links/references used in page]</a>
        </section>
      </div></div>]]></description>
        </item>
    </channel>
</rss>