<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 08 Nov 2023 15:00:16 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Major Outage Across ChatGPT and API (102 pts)]]></title>
            <link>https://status.openai.com/incidents/00fpy0yxrx1q</link>
            <guid>38190401</guid>
            <pubDate>Wed, 08 Nov 2023 14:02:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://status.openai.com/incidents/00fpy0yxrx1q">https://status.openai.com/incidents/00fpy0yxrx1q</a>, See on <a href="https://news.ycombinator.com/item?id=38190401">Hacker News</a></p>
Couldn't get https://status.openai.com/incidents/00fpy0yxrx1q: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Wikipedia Became the Last Good Place on the Internet (260 pts)]]></title>
            <link>https://www.cambridge.org/core/journals/american-political-science-review/article/rule-ambiguity-institutional-clashes-and-population-loss-how-wikipedia-became-the-last-good-place-on-the-internet/FC3F7B9CBF951DD30C2648E7DEFB65EE</link>
            <guid>38189878</guid>
            <pubDate>Wed, 08 Nov 2023 13:13:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cambridge.org/core/journals/american-political-science-review/article/rule-ambiguity-institutional-clashes-and-population-loss-how-wikipedia-became-the-last-good-place-on-the-internet/FC3F7B9CBF951DD30C2648E7DEFB65EE">https://www.cambridge.org/core/journals/american-political-science-review/article/rule-ambiguity-institutional-clashes-and-population-loss-how-wikipedia-became-the-last-good-place-on-the-internet/FC3F7B9CBF951DD30C2648E7DEFB65EE</a>, See on <a href="https://news.ycombinator.com/item?id=38189878">Hacker News</a></p>
<div id="readability-page-1" class="page"><div lang="en" data-v-324b68a8="" id="sec0" data-v-cbbc8d8e=""><h2>Abstract</h2>  <div><p>Scholars usually portray institutions as stable, inviting a status quo bias in their theories. Change, when it is theorized, is frequently attributed to exogenous factors. This paper, by contrast, proposes that institutional change can occur endogenously through population loss, as institutional losers become demotivated and leave, whereas institutional winners remain. This paper provides a detailed demonstration of how this form of endogenous change occurred on the English Wikipedia. A qualitative content analysis shows that Wikipedia transformed from a dubious source of information in its early years to an increasingly reliable one over time. Process tracing shows that early outcomes of disputes over rule interpretations in different corners of the encyclopedia demobilized certain types of editors (while mobilizing others) and strengthened certain understandings of Wikipedia’s ambiguous rules (while weakening others). Over time, Wikipedians who supported fringe content departed or were ousted. Thus, population loss led to highly consequential institutional change.</p></div> </div><div id="content-container" data-v-cbbc8d8e=""><div>
<div>
<div data-magellan-destination="sec1" id="sec1">
<h2> INTRODUCTION</h2>
<p> Institutions theorists seek to explain institutional stability and change. However, most accounts have a status quo bias, as institutions are portrayed as stable. When change is observed, it is typically through alterations of the formal rules of the institution. These changes are frequently attributed to easily observable exogenous factors, such as external crises, influxes of new ideas, or alterations in actors’ power. However, endogenous processes may also create change and their neglect biases our accounts of institutions.</p>
<p> This paper advances a theory of endogenous institutional change whereby members of an institution react differently to the outcomes of disputes within institutions. Losers (or those who disagree with the outcomes of the disputes) may become demotivated and disempowered, whereas the winners (or those who agree with the outcomes of the disputes) may become galvanized and empowered. If the winners and losers belong to coherent camps with divergent interests and ideas about the institution, disproportionate exits by the losers can cause drastic institutional changes over time. The contribution of this paper is to show theoretically and empirically that consequential change <em>can occur solely endogenously</em> and that population loss can be the mechanism behind such change.<a href="#fn1"><span>Footnote </span>
<sup>1</sup></a></p>
<p> The paper provides a detailed demonstration of this occurring on the English Wikipedia. Beneath the hood of this popular website exists a large community of volunteers (Wikipedia editors) who collaboratively write all Wikipedia content. This population of volunteers comes together in deliberative and democratic fora where they adjudicate what kind of content belongs on the encyclopedia. This paper shows that the English Wikipedia transformed its content over time through a gradual reinterpretation of its ambiguous Neutral Point of View (NPOV) guideline, the core rule regarding content on Wikipedia. This had meaningful consequences, turning an organization that used to lend credence and false balance to pseudoscience, conspiracy theories, and extremism into a proactive debunker, fact-checker and identifier of fringe discourse. There are several steps to the transformation. First, Wikipedians disputed how to apply the NPOV rule in specific instances in various corners of the encyclopedia. Second, the earliest contentious disputes were resolved against Wikipedians who were more supportive of or lenient toward conspiracy theories, pseudoscience, and conservatism, and in favor of Wikipedians whose understandings of the NPOV guideline were decisively anti-fringe. Third, the resolutions of these disputes enhanced the institutional power of the latter Wikipedians, whereas it led to the demobilization and exit of the pro-fringe Wikipedians. A power imbalance early on deepened over time due to disproportionate exits of demotivated, unsuccessful pro-fringe Wikipedia editors. Fourth, this meant that the remaining Wikipedia editor population, freed from pushback, increasingly interpreted and implemented the NPOV guideline in an anti-fringe manner. This endogenous process led to a gradual but highly consequential reinterpretation of the NPOV guideline throughout the encyclopedia.</p>
<p> The paper demonstrates these processes through qualitative content analysis, archival research, and process tracing. First, to document a transformation in Wikipedia’s content, a qualitative content analysis was conducted on a sample of 63 representative articles. Content on the pages was analyzed across time with a predetermined coding scheme (see Boreus and Bergström <a href="#r6"><span>Reference Boreus and Bergström</span>2017</a>; Elkins, Spitzer, and Tallberg <a href="#r11"><span>Reference Elkins, Spitzer and Tallberg</span>2021</a>; Herrera and Braumoeller <a href="#r17"><span>Reference Herrera and Braumoeller</span>2004</a>). The analysis shows that the content changed over time from lending credence to fringe views to delegitimizing the fringe views. Second, to explain why these content changes occurred, the paper uses process tracing on Wikipedia’s archives, analyzing article talk page discussions about rule interpretations, related discussions on general noticeboards, arbitration rulings, and editor sanctions proceedings, as well as the histories of individual Wikipedia editors. Analyses of debates regarding individual articles lend strong support for the theory of endogenous institutional change. Article-by-article evidence is supplemented by an analysis of a sample of referenda where editors are asked to express their views about the NPOV rule’s application to fringe topics. The analysis shows that the disproportionate population loss is systematic across the encyclopedia, as editors who hold the pro-fringe view exit Wikipedia at a higher rate than anti-fringe editors.</p>
<p> These changes occurred despite structural biases in favor of stability. Even though the rules and content on Wikipedia are constantly subject to change, the organization’s decision-making procedures are biased to a conservative status quo. All changes on Wikipedia must be approved through consensus and editors who act contrary to consensus are punished. Furthermore, the transformation was neither an inevitability nor likely outcome of the original design of the institution. A comparison to other versions of Wikipedia demonstrates the contingent nature of the English Wikipedia’s trajectory. For example, even though the Croatian and English Wikipedia share the same core rules, content on the two versions of Wikipedia looks drastically different, as the Croatian Wikipedia lends credence to anti-LGBT rhetoric and pseudohistory (Sampson <a href="#r29"><span>Reference Sampson</span>2013</a>). These outcomes were not intended by Wikipedia’s founders, as shown by their own delineation of the rules in the early years, and in the case of Wikipedia’s co-founder, a complete disavowal of Wikipedia’s transformation.</p>
<p> This paper uses the understudied politics of Wikipedia as a lens through which to examine institutional theories of change. It has two major contributions. One is theoretical, demonstrating how population loss can be an endogenous mechanism of institutional change. Losses in institutional clashes can be demoralizing and inhibiting for the losers, leading them to abandon the institution and leaving the institution in the hands of their adversaries. The winners subsequently have freer rein to push for changes in the institution. This form of change may potentially have explanatory value regarding the trajectories of bureaucracies, political movements, political parties, and professions, as discontented losers within those institutions opt to leave their institution rather than fight an uphill battle against empowered and emboldened winners.</p>
<p> The other contribution is empirical, as the paper provides a comprehensive study of the politics of Wikipedia, a highly consequential organization in the online political information ecosystem. The paper documents a heretofore undocumented transformation in Wikipedia’s content over its life span. While scholars and commentators have remarked in recent years on Wikipedia’s status as a beacon of information in an online space plagued by misinformation, there is no comprehensive analysis of a transformation over Wikipedia’s life span.<a href="#fn2"><span>Footnote </span>
<sup>2</sup></a></p>
</div>
<div data-magellan-destination="sec2" id="sec2">
<h2> INSTITUTIONS AND ENDOGENOUS CHANGE</h2>
<p> Most scholarly works on institutions have a status quo bias, as the focus is on accounting for the persistence of institutional arrangements over time. To explain change, scholars tend to look for exogenous factors. For rational choice institutionalists, institutions reflect equilibrium solutions to problems of cooperation between different actors.<a href="#fn3"><span>Footnote </span>
<sup>3</sup></a> In most rationalist accounts of institutions, these equilibria do not become unstable unless the external circumstances change (e.g., through alterations in power), and the appearance of new problems that require new solutions. For sociological institutionalists, institutions reflect shared norms and understandings.<a href="#fn4"><span>Footnote </span>
<sup>4</sup></a> Actors that compose the membership of an institution exist in a social environment where institutional practices become taken for granted. Individual actors have limited agency to alter the existing institutional arrangements. These shared norms do not get altered unless by powerful external sources or through the appearance of norm entrepreneurs. For historical institutionalists, institutions reflect decision-making made at critical junctures, temporal sequencing, and path dependency. Past decision-making has a persistent impact on institutions, contributing to stability over time, even if the existing arrangements are suboptimal. The sources of change tend to be external crises or changes in the broader environment that alter the functions and purpose of institutions.<a href="#fn5"><span>Footnote </span>
<sup>5</sup></a></p>
<p> Recent comparative politics scholarship (particularly in the historical institutionalist tradition; see Bleich <a href="#r4"><span>Reference Bleich</span>2018</a>; Mahoney and Thelen <a href="#r25"><span>Reference Mahoney and Thelen</span>2009</a>; Streeck and Thelen <a href="#r34"><span>Reference Streeck and Thelen</span>2005</a>; Thelen <a href="#r35"><span>Reference Thelen</span>2004</a>) and international relations-oriented research on norm contestation (see Dietelhoff and Zimmermann <a href="#r10"><span>Reference Dietelhoff and Zimmermann</span>2020</a>; Sandholtz <a href="#r30"><span>Reference Sandholtz</span>2008</a>; Sandholtz and Stiles <a href="#r31"><span>Reference Sandholtz and Stiles</span>2009</a>; Wiener <a href="#r39"><span>Reference Wiener</span>2009</a>) have identified rule ambiguity and norm ambiguity, respectively, as promising plausible mechanisms for gradual endogenous change.<a href="#fn6"><span>Footnote </span>
<sup>6</sup></a> The seeds of change lie in the intrinsic inability of rules to apply clearly and unambiguously to most situations that confront members of complex institutions. This permits actors to reinterpret rules and norms through their application in specific instances. However, while this literature points to the plausibility of endogenous institutional change through ambiguity in rule application, many cases are prompted by exogenous causes, such as (i) the involvement of new actors, (ii) environmentally driven changes in the balance of power between actors, and (iii) the appearance of new problems that need solving. While these may be gradual processes of change, the underlying causes are frequently exogenous.<a href="#fn7"><span>Footnote </span>
<sup>7</sup></a></p>
<p> A prominent example of this kind of change in the comparative politics literature is Bleich’s (<a href="#r4"><span>Reference Bleich</span>2018</a>) study of the French High Court’s changing interpretation of hate crime laws over time. Bleich’s explanation for the shift in how the court applied the rules focuses on the entry and influence of new actors, as he delineates how activist organizations influenced how the French High Court interpreted hate crime laws. He also shows that the French High Court was influenced by the European Court of Human Rights. In the international relations literature, a prominent example of this kind of change is Sandholtz’s (<a href="#r30"><span>Reference Sandholtz</span>2008</a>) study on the rules regarding wartime plunder of artistic and cultural artifacts. In Sandholtz’s study, rules regarding wartime plunder were reinterpreted after major wars (the Napoleonic Wars and World War II), as the victims of large-scale plunder pressed to regain their property. In both cases, the rules in question (hate crime laws and rules of war) were broad and unspecified, allowing for different application of the rules in practice. In both Bleich (<a href="#r4"><span>Reference Bleich</span>2018</a>) and Sandholtz (<a href="#r30"><span>Reference Sandholtz</span>2008</a>), a lack of specificity in the wording of the rules permitted changes in application over time, but those changes were caused by exogenous factors (new actors or major wars).</p>
<p> My account of institutional change on Wikipedia contrasts with these other accounts in three ways. First, change was not caused by the entry of new actors, but rather the loss of actors. Whereas other approaches to the study of institutions tend to see the relevant population of an institution as being stable or increasing, my account shows that the loss of a particular population contributed to Wikipedia’s shift. Furthermore, other accounts see conflicts within institutions as resulting in winners and losers where the losers typically remain within the institution. As Conran and Thelen (<a href="#r8"><span>Reference Conran, Thelen, Fioretos, Falleti and Sheingate</span>2016</a>) note, losers remobilize and live to fight another day, which may lead them to change the institution in the future. However, the extent to which that is true depends on the nature of the institution, as well as the characteristics of the conflicts and the participants involved. Losses may entrench power advantages that entail feedback effects and are hard to rebalance, thus ensuring that losers cannot return the institution to the status quo. That was certainly the case on Wikipedia.</p>
<p> Second, power asymmetries are formed on Wikipedia. However, unlike many other studies of institutions, the power asymmetry was not due to broader environmental changes that altered the social or material sources of actors’ power. Rather, power asymmetries formed as actors gained power <em>within</em> the rubrics of the institution itself. In the case of Wikipedia, <em>experience</em> provided a potent source of power, which made victories in early disputes consequential. In other organizations, there may be other power dynamics that are made apparent and entrenched through wins and losses in institutional conflicts.</p>
<p> Third, reinterpretation of Wikipedia’s rules was not prompted by the appearance of new problems that required new solutions. Rather, the practical consequence of the rule reinterpretations on Wikipedia entailed fixing old problems (the presence of sources and content that legitimized fringe perspectives) that in large part stemmed from how the rules had been interpreted in the past.</p>
<p> In contrast to much of the existing literature, this paper argues that institutions which are otherwise portrayed as stable are in fact constantly subject to change from within. The change can occur entirely endogenously.<a href="#fn8"><span>Footnote </span>
<sup>8</sup></a> There are four steps to such change. First, consistent with some of the historical institutionalist and norm scholarship, the seeds of change are located in the intrinsic inability of rules and norms to apply clearly and unambiguously to most situations that confront members of the institution. Second, rule ambiguity creates openings for members to impose new meanings on the rules at the micro level, as the rules are applied to specific situations. Since institutions are composed of actors that have different interests and diverse views, rule interpretations can be a potent source of conflict. Third, the conflicts may be resolved, resulting in winners and losers. The resolutions of these conflicts in favor of actors with certain rule interpretations can alter the balance of power within the institution, as the losers of past conflicts get demobilized, sanctioned, or lose status, whereas the winners get mobilized, elevated, and gain institutional power. Fourth, if actors with similar and overlapping viewpoints and interests (coherent camps) win early and frequent victories across an institution, they may shape how the overarching rules of the institution are understood to work in practice.</p>
<p> For the process to play out in this manner, <em>camps with coherent interests and views</em> must exist (A and B in <a href="#fig1">Figure 1</a>). Otherwise, settlements in individual disputes lead to indeterminate long-term results. Additionally, for trajectories to form over time, victories must result in <em>power advantages.</em> In the absence of a meaningful power advantage, losers should be able to regroup and live to fight another day over the interpretation of the rules, with indeterminate long-term results. The relative ease with which actors can exit institutions affects the speed of change. Migrating from a country may entail considerable costs, whereas leaving a voluntary association may be relatively cost-free, which means that rapid change may be more likely in the latter case once a power asymmetry forms. Finally, for the process to play out in this specific manner, it presumes that <em>exogenous factors do not intervene with the process.</em> Both exogenous and endogenous factors can work in tandem, but the contribution of this paper is to show theoretically and empirically that consequential change <em>can occur solely endogenously.</em>
</p><div data-magellan-destination="fig1" id="fig1">
<p><img data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20230308104251995-0268:S0003055423000138:S0003055423000138_fig1.png?pub-status=live" width="1507" height="442" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20230308104251995-0268:S0003055423000138:S0003055423000138_fig1.png" data-zoomable="true" src="https://www.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20230308104251995-0268:S0003055423000138:S0003055423000138_fig1.png"></p>
<div><p><span>Figure 1.</span> How Do Rules Obtain New Meanings?<a href="#fn9"><span>Footnote </span>
<sup>9</sup></a></p>
</div></div>
<p> The remainder of this paper provides a detailed demonstration of how this process of endogenous change plays on the English Wikipedia. The paper explains what Wikipedia is, justifies why Wikipedia is a worthy case of inquiry, documents how Wikipedia’s content has transformed over its life span, and explains how this transformation happened. The penultimate section of the paper examines various alternative explanations, showing that they fail to account for Wikipedia’s transformation.</p>
</div>
<div data-magellan-destination="sec3" id="sec3">
<h2> WIKIPEDIA AS AN IMPORTANT PART OF THE POLITICAL INFORMATION ECOSYSTEM</h2>
<div data-magellan-destination="sec4" id="sec4">
<h3> The Structure of Wikipedia</h3>
<p> Wikipedia is a nonprofit, multilingual, open-access online encyclopedia started by Jimmy Wales and Larry Sanger in 2001. The encyclopedia is user-generated. Anyone is free to edit it. By May 2021, there were more than 40&nbsp;million registered users, of whom nearly 140,000 were active editors. Wikipedians generally edit pseudonymously, but extant data indicate that Wikipedians are disproportionately white males from the Global North. One in four Wikipedians primarily edit the English language version of Wikipedia (see Hill and Shaw <a href="#r18"><span>Reference Hill and Shaw</span>2013</a>; Wikipedia <a href="#r44">2021b</a>; Yasseri, Sumi, and Kertesz <a href="#r45"><span>Reference Yasseri, Sumi and Kertesz</span>2012b</a>).</p>
<p> Editors must comply with three core Wikipedia content guidelines:</p><ol>
<li>
<p><span>1.</span>
<strong>Neutral point of view (WP:NPOV):</strong> “representing fairly, proportionately, and, as far as possible, without editorial bias, all the significant views that have been published by reliable sources on a topic” (Wikipedia <a href="#r40">2020a</a>).</p>
</li>
<li>
<p><span>2.</span>
<strong>No original research (WP:NOR):</strong> “you must be able to cite reliable, published sources that are directly related to the topic of the article, and directly support the material being presented” (Wikipedia <a href="#r41">2020b</a>).</p>
</li>
<li>
<p><span>3.</span>
<strong>Verifiability (WP:V):</strong> “verifiability means other people using the encyclopedia can check that the information comes from a reliable source… All material in Wikipedia mainspace, including everything in articles, lists and captions, must be verifiable” (Wikipedia <a href="#r42">2020c</a>).</p>
</li>
</ol><p>WP:NPOV is the guideline at the heart of most disputes on the encyclopedia. The NPOV guideline affects whether something should be covered, the weight of the coverage, and whether the cited sources are reliable. On subjects where there are diverse and incompatible views, “edit wars” frequently arise. These are situations when a change is made to an article (e.g., removal of text, addition of text, and rewording of text), and the change gets reverted, leading to an unstable cycle of additions and reverts of the same content (see Jemielniak <a href="#r20"><span>Reference Jemielniak</span>2014</a>; Tkacz <a href="#r36"><span>Reference Tkacz</span>2015</a>; Yasseri et al. <a href="#r46"><span>Reference Yasseri, Sumi, Rung, Kornai and Kertesz</span>2012</a>).</p>
<p> How does the encyclopedia deal with disputes like these? One might think that such disputes would lead to an inconsistent product where articles look drastically different from day to day, but Wikipedia produces a very stable and consistent product. This is because there are multilayered dispute settlement mechanisms and elaborate norms regarding editor behavior. Wikipedia’s community reaches decisions about rules and content through a combination of deliberative discussions and referenda. These democratic processes have the goal of determining whether content has “consensus.” If a proposed change does not have consensus, the article experiencing edit wars will be returned to the status quo.</p>
<p> A typical edit war will be resolved in the following manner: an editor makes changes to a page. Other editors disapprove of the change and revert the change. The status quo ante is established until a consensus for inclusion can be found on the talk page of the article. Editors may be able to work out mutually acceptable compromises. If they are not able to work out acceptable compromises among the small subset that are engaged with a single article, they can subject the dispute to input from the broader Wikipedia community. For example, if the editors who edit the Margaret Thatcher page are having a dispute that they cannot resolve among themselves, they may take the dispute to noticeboards that are frequented by large numbers of Wikipedians who do not frequent the Thatcher page.</p>
<p> However, these procedures are not always sufficient to establish stability on a page. This is particularly the case on large high-profile articles with stable and coherent camps of editors, frequent editing, and multiple controversial aspects. When articles experience extraordinary levels of edit-warring, editors may request dispute settlement before administrators on the “Administrators Noticeboard” or arbitrators on the “Arbitration Committee.” These bodies primarily adjudicate behavioral problems among Wikipedians rather than adjudicate content directly (that is something for Wikipedia’s deliberative democratic processes to resolve). The bodies tend to sanction the most active and raucous editors on the dysfunctional pages.</p>
<p> Administrators and arbitrators are elected by the Wikipedia userbase. To become an administrator, an editor goes through the “Request for Adminship” process, which is essentially an election on the suitability of an editor to become an administrator. Any experienced editor can make a request for a position as administrator.<a href="#fn10"><span>Footnote </span>
<sup>10</sup></a> The request is unlikely to be granted unless they have a well-established history as a contributor on the encyclopedia and have demonstrated an ability to get along with other editors. The threshold to become an administrator is high, as editors generally require support by 75% of voters. Editors who are new and who behave in divisive ways are unlikely to get the support needed to become administrators.<a href="#fn11"><span>Footnote </span>
<sup>11</sup></a> The English Wikipedia has approximately one thousand administrators.</p>
<p> The Arbitration Committee is vastly smaller, with membership oscillating between 13 and 18 members. Elections to the Arbitration Committee are more formal and eventful processes than the requests for adminship, as the elections occur annually, eligible registered editors are notified about the elections on their user talk page, and cast secret ballots.<a href="#fn12"><span>Footnote </span>
<sup>12</sup></a> Experienced editors who are not divisive are better poised to garner the votes to become arbitrators.</p>
</div>
<div data-magellan-destination="sec5" id="sec5">
<h3> Case Selection Justification</h3>
<p> There are several motivating factors in choosing the English Wikipedia as a case to study institutional change: (i) it is an important case; (ii) it is an understudied case; (iii) it could be construed as a least-likely case for institutional change; and (iv) it has unique availability of data, which makes it possible to observe slow, gradual, endogenous processes that result in consequential drastic change over time.</p>
<p> First, Wikipedia is an important institution. One that is worthwhile to study in its own right. <em>Wikipedia.org</em> is one of the most popular websites in the world. The English Wikipedia is frequently at the top or near the top of Google searches for a known person or event in the English language (e.g., Vincent and Hecht <a href="#r37"><span>Reference Vincent and Hecht</span>2021</a>). Wikipedia is also widely perceived as a trustworthy and reliable source of information (e.g., Bruckman <a href="#r7"><span>Reference Bruckman</span>2022</a>), giving it considerable power in public discourse and ideational diffusion. Wikipedia’s influence is also boosted by the fact that Wikipedia pages, unlike other forms of content (such as news reports and scholarship), are often written in summary style and in layman’s terms, thus making the information in Wikipedia articles more accessible to readers. Furthermore, tech giants, such as YouTube, Facebook, Google, and Twitter, have incorporated Wikipedia into their own platforms.</p>
<p> Due to its popularity and perceived trustworthiness, Wikipedia has the power to legitimize and delegitimize subjects. This is a website that can declare whether something is a pseudoscience, a falsehood, a conspiracy theory, or bigotry. World leaders can be described as dictators, perpetrators of violence can be identified, and the effects of implementing certain public policies can be characterized as positive or negative. Additionally, in many cases, it seems clear that actors who are covered by Wikipedia believe that Wikipedia matters, as politicians have on many occasions been exposed as having edited their own Wikipedia pages, and authoritarian regimes have blocked Wikipedia in parts or in its entirety.</p>
<p> Wikipedia’s salience has increased over time, as scholars express concern over the intersection of the Internet and politics. The Internet has displaced traditional gatekeepers, and contributed to the wide diffusion of conspiracy theories, pseudoscience, and extremist rhetoric. Whereas the other major online platforms have been criticized for their role in monetizing, inculcating, and diffusing extremism and misinformation, Wikipedia has often been hailed as an exception: a distinctly positive actor in the online political ecosystem,<a href="#fn13"><span>Footnote </span>
<sup>13</sup></a> an actor that serves a proactive gatekeeping role where it outright debunks, fact-checks, and highlights the errors and fringe nature of the very same discourses popularized on the other platforms.</p>
<p> Second, Wikipedia is an understudied institution in an understudied organizational environment. Aside from its importance in politics, there are several things about Wikipedia as an organization that political scientists and organizational scholars should find intriguing. It is an enormous organization that is based on the open-source or commons-based peer production organizational model (e.g., Benkler <a href="#r2"><span>Reference Benkler</span>2002</a>; Reagle <a href="#r27"><span>Reference Reagle</span>2010</a>; Tkacz <a href="#r36"><span>Reference Tkacz</span>2015</a>). Unlike traditional organizations, such as firms and bureaucracies, Wikipedia is characterized by a lack of “formal hierarchy.” Editors on Wikipedia are not managed and instructed by “managers,” but rather self-assign tasks to do. Editors are not motivated by monetary rewards, unlike members of traditional organizations.</p>
<p> Decision-making on Wikipedia is deliberative and democratic. The rules of Wikipedia are always subject to change, which effectively makes all rules, norms, and content on Wikipedia subject to constant plebiscites. Consequently, Wikipedia both reflects and accentuates processes that are analogous to those in other organizations. Scholars have consequentially used Wikipedia to study collaboration, conflict, polarization, and partisanship, as well as politics and organizational dynamics more broadly (Greenstein, Gu, and Zhu <a href="#r14"><span>Reference Greenstein, Gu and Zhu</span>2021</a>; Heaberlin and DeDeo <a href="#r16"><span>Reference Heaberlin and DeDeo</span>2016</a>; Jemielniak <a href="#r20"><span>Reference Jemielniak</span>2014</a>; Konieczsny 2009; Lerner and Lomi <a href="#r24"><span>Reference Lerner and Lomi</span>2019</a>; Reagle <a href="#r27"><span>Reference Reagle</span>2010</a>; Shi et al. <a href="#r33"><span>Reference Shi, Teplitskiy, Duede and Evans</span>2019</a>; Tkacz <a href="#r36"><span>Reference Tkacz</span>2015</a>; Yasseri et al. <a href="#r46"><span>Reference Yasseri, Sumi, Rung, Kornai and Kertesz</span>2012</a>; Yasseri, Sumi, and Kertesz <a href="#r45"><span>Reference Yasseri, Sumi and Kertesz</span>2012</a>). While Wikipedia has been the subject of study by computer scientists, physicists, information scientists, and sociologists, it has been neglected by political scientists.</p>
<p> Third, Wikipedia could be construed as a least-likely case for institutional change and most-likely case for organizational stability, as the organization has several structural biases in favor of stability and the status quo. The requirement that there needs to be a “consensus” among editors in favor of both addition of content and rule changes should make it hard to enact substantial changes.<a href="#fn14"><span>Footnote </span>
<sup>14</sup></a> In the event of disputes, the guiding rule is to retain the status quo unless a consensus can be established for any change. A minority of editors can therefore block controversial changes. Furthermore, the presence of a large and diverse userbase means that ideational changes among individuals and small groups should not result in frequent or sudden changes over time. Wikipedia also strongly enforces compliance with the rules, which means that large-scale rule violations will not be a likely source of change over time (see Piskorski and Gorbatai <a href="#r26"><span>Reference Piskorski and Gorbatai</span>2017</a>). The strong enforcement leads editors to edit within accepted boundaries and within consensuses. Given these institutional characteristics, one might expect Wikipedia to have a conservative status quo bias.<a href="#fn15"><span>Footnote </span>
<sup>15</sup></a></p>
<p> Furthermore, the founders of Wikipedia have not intervened to cause new interpretations of the guidelines among the userbase. Sanger, who crafted the core NPOV rule, has condemned the interpretations of the guideline that emerged over time.<a href="#fn16"><span>Footnote </span>
<sup>16</sup></a> Wales has held a more agnostic view of change on Wikipedia over time, saying in 2006, “One of the great things about NPOV is that it is a term of art and a community fills it with meaning over time” (Reason <a href="#r28">2006</a>).</p>
<p> Fourth, Wikipedia has a unique availability of data. A major problem in most case-specific accounts of institutional change is the inaccessibility of comprehensive data to evaluate causes and effects. Scholars must rely on a sliver of data that are available to piece together what the preferences of various actors might be, what actions these actors took, and how the preferences and actions of these actors led to institutional change or stability. What adds to the problem is that the publicly available data may fail to reflect the actual processes that led to change. For example, debates may take place in front of cameras and voting may be logged into records, but the meaningful negotiations occur behind closed doors.</p>
<p> In comparison with other institutions, Wikipedia has several advantages in terms of studying institutional change. In other largescale institutions, it is not feasible to identify and collect data on every participant, and to track the behavior of every participant over time. Scholars are often forced to fill gaps with theory or by making assumptions. There is a risk that unobserved variables have a significant impact on outcomes, which makes it hard to make robust claims about the causes of institutional change. On Wikipedia, on the other hand, virtually all edits and comments are logged and open to public viewing. This means that it is possible to trace each input in every debate, as well as to sift through the editing history of each Wikipedia editor. Thus, there is an enormity of relevant data on which to test, refine, and build theories of institutional change.</p>
</div>
</div>
<div data-magellan-destination="sec6" id="sec6">
<h2> RESEARCH DESIGN</h2>
<p> The research design of the paper has two components. First, the paper codes content in the lead of Wikipedia articles, showing a pro-to-anti-fringe shift in content. Second, the paper does process-tracing of trends within Wikipedia’s governance to show how this shift happened (George and Bennett <a href="#r12"><span>Reference George and Bennett</span>2005</a>). In terms of analyzing changes in the content of Wikipedia articles, the paper uses nonautomated qualitative content analysis to classify whether Wikipedia pages use language that legitimizes or delegitimizes fringe positions and entities. An advantage of qualitative content analysis is that it permits analysts to observe subtle, yet meaningful nuances in meaning.<a href="#fn17"><span>Footnote </span>
<sup>17</sup></a> It entails human coding and interpretation of textual sources according to structured and systematic coding schemes (see Boreus and Bergström <a href="#r6"><span>Reference Boreus and Bergström</span>2017</a>; Elkins, Spitzer, and Tallberg <a href="#r11"><span>Reference Elkins, Spitzer and Tallberg</span>2021</a>; Herrera and Braumoeller <a href="#r17"><span>Reference Herrera and Braumoeller</span>2004</a>). More specifically, the paper systematically classified a sample of 63 article leads with a predetermined coding scheme.<a href="#fn18"><span>Footnote </span>
<sup>18</sup></a> The 63 pages were chosen because they are representative of the population of relevant cases: all the pages are on topics that have been linked to pseudoscience, conspiracy theories, extremism, and fringe rhetoric in public discourse.</p>
<p> Within the relevant population, the chosen pages reflect diverse topic areas (health, climate, gender, sexuality, race, abortion, religion, politics, international relations, and history). The pages also vary in terms of the time of creation (some pages were created early and others later), and temporal prominence (the topics covered in the pages were more prominent during different time periods). The pages include biographies (which have more restrictive standards for inclusion of pejorative content) and nonbiographies.</p>
<p> The contents of the chosen Wikipedia pages were classified according to a five-category coding scheme. These categories reflect varying degrees of neutrality. On one end of the spectrum, the language lends credence and legitimacy to fringe views. On the other end, the language firmly delegitimizes the fringe views:</p><ol>
<li>
<p><span>1.</span>
<strong>Fringe normalization:</strong> The fringe position/entity is normalized and legitimized. There is an absence of criticism.</p>
</li>
<li>
<p><span>2.</span>
<strong>Teach the controversy:</strong> The fringe position/entity is presented as a matter of active scientific or political dispute (A says X, B says Y).</p>
</li>
<li>
<p><span>3.</span>
<strong>False balance:</strong> The lead places emphasis on the expertise, credibility, evidence, and arguments of the anti-fringe side (e.g., “some scientists say,” “some medical organizations say”), but the pro-fringe side still gets space to rebut.</p>
</li>
<li>
<p><span>4.</span>
<strong>Identification of the fringe view:</strong> The lead places emphasis on the legitimacy and the overwhelming numbers that compose the anti-fringe side (e.g., “scientific consensus,” “the scientific community”), but space is still given to the pro-fringe side.</p>
</li>
<li>
<p><span>5.</span>
<strong>Proactive fringe-busting:</strong> Space is only given to the anti-fringe side whose position is stated as fact in Wikipedia’s own voice.<a href="#fn19"><span>Footnote </span>
<sup>19</sup></a> The evidence that supports the anti-fringe position is presented, whereas the flaws of the pro-fringe perspective are outlined.</p>
</li>
</ol><p>The paper uses a cross-temporal analysis of each article’s lead, thus tracking changes over time. A stable version of each article was analyzed at the end of each year. Articles were checked more frequently if the pages had frequent and erratic editing patterns. Changes on Wikipedia articles can be accessed through archives that show each change, thus making the study replicable. <a href="#tab1">Table 1</a> and the Supplementary Material include examples of language from each article’s lead. Due to space constraints, changes in nine representative article leads are shown in <a href="#tab1">Table 1</a>, whereas 54 additional articles are in the Supplementary Material.</p><div data-magellan-destination="tab1" id="tab1">
<p><span>Table 1.</span> The Lead to Controversial Science- and Politics-Related Wikipedia Pages, January 2001 to June 2020</p>
<p><span>
<p><img data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20230308104251995-0268:S0003055423000138:S0003055423000138_tab1.png?pub-status=live" width="2765" height="4745" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20230308104251995-0268:S0003055423000138:S0003055423000138_tab1.png" data-zoomable="true" src="https://www.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20230308104251995-0268:S0003055423000138:S0003055423000138_tab1.png"></p>
</span>
</p></div>
<div data-magellan-destination="tab2" id="tab2">
<p><span>Table 2.</span> Wikipedia’s List of Deprecated Websites, as of September 4, 2021<a href="#fn23"><span>Footnote </span>
<sup>23</sup></a></p>
<p><span>
<p><img data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20230308104251995-0268:S0003055423000138:S0003055423000138_tab2.png?pub-status=live" width="924" height="1803" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20230308104251995-0268:S0003055423000138:S0003055423000138_tab2.png" data-zoomable="true" src="https://www.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20230308104251995-0268:S0003055423000138:S0003055423000138_tab2.png"></p>
</span>
</p></div>
<div data-magellan-destination="tab3" id="tab3">
<p><span>Table 3.</span> Referenda and Subsequent Exits</p>
<p><span>
<p><img data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20230308104251995-0268:S0003055423000138:S0003055423000138_tab3.png?pub-status=live" width="923" height="1057" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20230308104251995-0268:S0003055423000138:S0003055423000138_tab3.png" data-zoomable="true" src="https://www.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20230308104251995-0268:S0003055423000138:S0003055423000138_tab3.png"></p>
</span>
</p></div>
<p> Per the analysis (see <a href="#tab1">Table 1</a> and the Supplementary Material), content on the English Wikipedia shows a clear trend from language that legitimizes fringe positions to language that delegitimizes fringe positions. Newer articles tend to adopt language from the anti-fringe categories at their creation, whereas older articles tend to adopt language that is more fringe-normalizing at their creation. None of the articles move in a direction where they become more fringe-normalizing.</p>
<p> The analysis shows that in its early years, the English Wikipedia adhered to a “strict” NPOV approach whereby Wikipedia content was open to a diversity of opinions and sources, and where Wikipedians could not state contested views as facts in Wikipedia’s own voice. Thus, a typical page on a subject related to pseudoscience and contested science would adopt a “Some say X, others say Y” style, even on topics where mainstream scientific opinion overwhelmingly favored X.</p>
</div>
<div data-magellan-destination="sec7" id="sec7">
<h2> ENDOGENOUS CHANGE ON WIKIPEDIA</h2>
<div data-magellan-destination="sec8" id="sec8">
<h3> Explaining the Findings</h3>
<p> The paper has demonstrated that English Wikipedia content changed over time. This section seeks to explain <em>why</em> the content changed. In doing so, the paper uses process tracing. The paper systematically analyzes the internal Wikipedia discussion forums where editors duked out content. More specially, the paper analyzes the archived talk pages of the 63 articles and relevant discussions about article content on general noticeboards.<a href="#fn20"><span>Footnote </span>
<sup>20</sup></a> To classify editors into the Anti-Fringe camp (AF) and the Pro-Fringe camp (PF), the paper uses a variety of data sources: the viewpoints expressed by editors on the article talk pages themselves, the views expressed by editors brought up for sanctioning on the Administrators’ noticeboard or the “Requests for Enforcement” page before the Arbitration Committee, and lists of editors brought up in arbitration committee rulings.</p>
<p> These data sources provide article-by-article evidence about relevant individual editors on the pages in question. However, to assess systematically what happens to AF and PF editors across the English Wikipedia (not just the 63 articles), the paper uses a sample of referenda where editors are implicitly asked whether they support a pro- or anti-fringe interpretation of the NPOV guideline. The user histories of the participants in the referenda are then analyzed to uncover whether they are still active or whether they have voluntarily left Wikipedia, substantially reduced their number of contributions, been banned from the relevant topics, or blocked from Wikipedia in its entirety.</p>
<p> The evidence is broadly consistent with the observable implications of the paper’s theory of endogenous institutional change (the processes of the theory are outlined in <a href="#fig2">Figure 2</a>).<a href="#fn21"><span>Footnote </span>
<sup>21</sup></a>
</p><div data-magellan-destination="fig2" id="fig2">
<p><img data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20230308104251995-0268:S0003055423000138:S0003055423000138_fig2.png?pub-status=live" width="1758" height="1185" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20230308104251995-0268:S0003055423000138:S0003055423000138_fig2.png" data-zoomable="true" src="https://www.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20230308104251995-0268:S0003055423000138:S0003055423000138_fig2.png"></p>
<div><p><span>Figure 2.</span> How Did the NPOV Rule Change?</p>
</div></div>
<p> The key piece of evidence is that PF members disappear over time in the wake of losses, both voluntarily and involuntarily. PF members are those who vote affirmatively for policies that normalize or lend credence to fringe viewpoints, who edit such content into articles, and who vote to defend fellow members of PF when there are debates as to whether they engaged in wrongdoing. Members of AF do the opposite.</p>
<p> The causal mechanism for the gradual disappearance is that early losses demotivated members from PF or led to their sanctioning, whereas members of AF were empowered by early victories. As exits of PF members mount across the encyclopedia, the community increasingly adopts AF’s viewpoints as the way that the NPOV guideline should be understood.</p>
<div data-magellan-destination="sec9" id="sec9">
<h4> Step 1: Rule Ambiguity</h4>
<p> The NPOV rule of Wikipedia is ambiguous in its specific application. During the early period of Wikipedia, Wikipedia’s NPOV rule was understood as indicating a “describe-the-controversy” approach to disputes between sources: Wikipedia should not take sides on contested issues, and Wikipedia should be open to a diverse array of sources. Thus, the policy allowed for inclusion of lower-quality sources, so long as they were attributed. This meant that if sources disagreed in terms of how they covered the topic of homeopathy, the Wikipedia article on homeopathy would present both sides of the issue and avoid taking a firm stance. The Wikipedia page on homeopathy included text that effectively said “Advocates for homeopathy say…” and “Critics of homeopathy say…,” whereas it was frowned upon among early Wikipedians to state decisively in Wikipedia’s own voice that “Homeopathy is a pseudoscientific system of alternative medicine” (as the first line of the Wikipedia page stated in 2020). This was largely in line with Larry Sanger’s intentions when he crafted the NPOV rule (ArsTechnica <a href="#r1">2014</a>).</p>
</div>
<div data-magellan-destination="sec10" id="sec10">
<h4> Step 2: Clashes between Camps over Rule Interpretations</h4>
<p> On contentious topics (e.g., American politics, conspiracy theories, and pseudoscience), editors had vast differences in terms of how they understood the application of Wikipedia’s rules. Editors who were anti-conspiracy theories, anti-pseudoscience, and liberal (the AF camp) pushed understandings of NPOV that took a firm anti-conspiracy-theory and anti-pseudoscience stance. Thus, they argued for reliance on strong sources (such as studies and highly reputable mainstream news outlets), nonuse of lower-quality sources (such as partisan outlets and disreputable outlets), stating claims from strong sources in Wikipedia’s own voice (rather than attributing them as a source’s opinion), firmly stating that minority views are fringe, and stating that falsehoods are falsehoods.</p>
<p> Editors who were more supportive of conspiracy theories, pseudoscience, and conservatism (the PF camp) argued for reliance on sources across a broad range of reliability (in part, because they perceived academics and newspapers of record to be biased), stating claims from sources as if they were always an attributed POV, and avoiding firm stances on the state of a controversy.</p>
<p> On the homeopathy page, this meant that PF members raised questions about relying on reporting by the <em>New York Times</em> and <em>Washington Post</em>, insisted that studies skeptical of homeopathy’s efficacy be phrased as opinion, and sought to include rebuttals by pro-homeopathy organizations and pseudoscientists. AF members held the opposite view, as they sought to phrase skeptical content in Wikipedia’s own voice and strongly opposed content sourced to pro-homeopathy organizations.</p>
</div>
<div data-magellan-destination="sec11" id="sec11">
<h4> Step 3: Formation of a Power Asymmetry</h4>
<p> Over the course of years, AF successfully shaped how to understand the practical application of Wikipedia’s NPOV guideline. These early victories gave AF an upper hand in editing disputes on pages related to conspiracy theories, pseudoscience, and American politics. There was no single critical juncture. Rather, there were many gradual mutually reinforcing steps across several spheres of Wikipedia.</p>
<p> One type of change that was important early in Wikipedia’s development was intervention into content disputes by arbitrators on highly dysfunctional pages. Highly dysfunctional pages are those characterized by edit-warring among a multitude of editors to the point that the pages are unstable over extended periods of time, and editors cannot even agree on the status quo version of the pages.</p>
<p> Two particularly important early arbitration rulings in the early years were the arbitration committee cases on climate change (2005) and pseudoscience (2006), which largely reaffirmed some viewpoints held by AF in those specific disputes and led to sanctions that primarily targeted prolific PF editors (although some AF editors were also targeted) for behavioral wrongdoing. The disputes were at their core about edit-warring between different camps as to whether climate change articles should reflect the scientific consensus on climate change or lend weight to those who dispute the scientific consensus, and broadly about how pseudoscientific ideas and minority scientific perspectives should be framed.</p>
<p> A second type of important changes involve the writing of guidelines to supplement the existing NPOV guideline and clarify how the NPOV guideline should be applied. This includes the creation of the Reliable Sources guideline (2005), which introduced a basic framework for evaluating the reliability of sources, the Fringe Theory guideline (2007), which introduced a basic framework for evaluating minority views and fringe views, and the Reliable Sources (Medicine) guideline (2008), which set a higher quality threshold for sources on medicine-related topics out of a concern that poorly sourced content could cause harm to readers.</p>
<p> These guidelines were crafted by a relatively small set of experienced editors, including many from AF who were involved in active content disputes on topics that related to these rules. All editors can participate in processes to change rules and add supplements to rules. These changes must go through the normal Wikipedia consensus decision-making process. While these particular rules were not written to specifically address content in those disputes, the enactment of these rules as supplemental modifications to the NPOV guideline would prove useful in those disputes. Furthermore, discussions related to those rules showed that editors wanted to privilege science and academic expertise in terms of identifying what is fringe, but the Reliable Sources and Fringe Theory guidelines were broad enough in scope that they could also be used to identify fringe discourse and beliefs outside of science, such as with conspiracy theories and extremist rhetoric in politics.</p>
</div>
<div data-magellan-destination="sec12" id="sec12">
<h4> Step 4: Reinterpretation of Wikipedia’s Rules</h4>
<p> The early victories and selective departures had positive feedback effects for three reasons: (i) they enhanced the value of experience, (ii) they created a numerical advantage, and (iii) they spurred a sourcing bias. Consequently, AF editors experienced greater success in editing, whereas PF editors did not, which led to lopsided exits over time. Together, these factors led AF to increasingly get what it wanted and sway the broader Wikipedia userbase to see one interpretation of the rules as the undisputed accurate interpretation of the rules.</p>
<p> First, experience matters a great deal on Wikipedia, which makes disproportionate exits early on highly consequential. Since Wikipedia is notoriously complicated for new editors to maneuver, experienced editors have a decisive advantage in content disputes (Jemielniak <a href="#r20"><span>Reference Jemielniak</span>2014</a>). Experience helps in understanding the rules, norms, and processes of Wikipedia. This leads to greater success in content disputes and edit wars, as well as makes experienced editors able to drive disruptive “newcomers” away from Wikipedia and instill in newcomers’ certain understandings of how rules should be interpreted. Experience also raises the likelihood of becoming an administrator, thereby gaining the power to enforce the rules and sanction editors.<a href="#fn22"><span>Footnote </span>
<sup>22</sup></a> Additionally, experience increases the likelihood of participation in general noticeboard discussions where sanctions of individual editors and specific rule interpretations are discussed in detail. Experienced editors also become aware of administrator elections and arbitration committee elections, which are important levers of power on Wikipedia.</p>
<p> Second, there is power in numbers. It is easier for PF members to fall afoul of the rules if they are frequently at a numerical disadvantage in editing disputes. It makes them less likely to win content disputes (which are often determined by numbers), forces them to spend more time to advance their views, and makes them more likely to have to edit war (make frequent reverts of AF editors). Whereas multiple AF editors can share the burden of doing reverts of PF’s edits and not violate any edit-warring restrictions, a PF editor may be forced to do multiple reverts, thus risking sanction.</p>
<p> Third, the gradual development of a sourcing hierarchy—whereby some sources were deemed reliable, and others were deemed unreliable—created advantages for AF editors. The culmination of a long, gradual conflict over the use of sources on Wikipedia was the 2017 vote to deprecate (ban) the <em>Daily Mail</em>, a British tabloid, from being used as a source for statements of fact. Over the next 4&nbsp;years, 38 additional sources were deprecated.</p>
<p> The gradual development of the sourcing hierarchy reflects how the Wikipedia community shifted its understanding of reliability over time, facilitated by the experience and numerical advantage of AF. An examination of pages in the early years of Wikipedia shows that Wikipedians had very lax standards for sourcing. By the mid-2000s, momentum had formed to privilege scientific publications. This did not mean that other sources were unusable, but that priority and prominence should be afforded to scientific publications. During these early years, Wikipedians did not appear to distinguish between news sources in terms of reliability in any clear manner. Articles into the 2010s show considerable usage of sources that would ultimately by the mid-2010s be deemed unreliable, including sources that were deprecated from 2017 onward.<a href="#fn24"><span>Footnote </span>
<sup>24</sup></a> Discussions about these sources in the previous years had not concluded with support to prohibit them, demonstrating a change in how the community looked at them. The key difference is that the ranks of PF editors who blocked previous attempts to ban sources had been thinned out considerably by 2017.</p>
<p> This hierarchy of sources has implications both for what kind of content can be added to Wikipedia and how it will be phrased. For example, if the <em>New York Times</em> (a source that Wikipedia editors came to recognize as highly reliable) describes something as a conspiracy theory, whereas the <em>New York Post</em> (a source that Wikipedia editors have determined to be unreliable) differs from that description, then Wikipedia content can be added that firmly states in Wikipedia’s voice that something is a conspiracy theory. Under a previous collective understanding of Wikipedia’s rules, Wikipedia’s content would not give a firm statement in Wikipedia’s voice but would rather attribute particular claims to the <em>Times</em> and attribute rebuttal claims to the <em>Post.</em> Thus, over time, Wikipedia has accepted the use of contested labels and taken sides on contested subjects, ultimately producing a type of content that is distinctly anti-pseudoscience and anti-conspiracy theories, and which has the perception of a liberal bent in U.S. politics.</p>
<p> Each shift in policy further weakened the position of PF in editing disputes and made the editing experience less rewarding for those editors because they ended up on the losing end of content disputes. Over time, PF editors responded in three ways<a href="#fn25"><span>Footnote </span>
<sup>25</sup></a>:</p><ol>
<li>
<p><span>1.</span>
<strong>Fight back:</strong> By increasingly editing against consensus and in violation of new interpretations of Wikipedia policy. These editors were subsequently banned.</p>
</li>
<li>
<p><span>2.</span>
<strong>Withdraw:</strong> By leaving Wikipedia or reducing their contributions.</p>
</li>
<li>
<p><span>3.</span>
<strong>Acquiesce:</strong> By gradually adapting to the new interpretations of Wikipedia policy.</p>
</li>
</ol>
<p> Article-by-article evidence substantiates these patterns, with prominent PF editors getting banned, retiring, or adjusting to new interpretations of Wikipedia guidelines.<a href="#fn26"><span>Footnote </span>
<sup>26</sup></a> In explaining their departure on their talk page, retired PF editors frequently decried what they perceived as Wikipedia’s increased bias, hostile editing environment, and the pointlessness of fighting against what they described as a cabal.<a href="#fn27"><span>Footnote </span>
<sup>27</sup></a> This stands in contrast to the explanations offered by non-PF members for retiring. Some PF editors proved more flexible to Wikipedia’s changing environment, acquiescing to new interpretations of Wikipedia policy. For example, a PF editor might affirm the new standards in Wikipedia’s sourcing policy by insisting that content from a source like the <em>New York Times</em> should be stated in Wikipedia’s own voice when a <em>Times</em> story criticizes a left-leaning politician or left-leaning cause. However, in doing so, those PF editors help enshrine the emerging new interpretations of Wikipedia guidelines.</p>
<p> In addition to article-by-article evidence of departures of Wikipedia editors, the paper uses a sample of hotly contested referenda (where editors are asked to express their views about the NPOV rule’s application to fringe topics) to gage whether the disappearance of PF editors (measured by their support or opposition for a fringe position) is systemic across the encyclopedia. This is a unique and useful data source that shows that the relative disappearance of PF editors is systemic.</p>
<p> The raw numbers undersell the importance of those who have departed the encyclopedia. Many of the departees were highly prolific experienced editors from PF, whereas many of the editors who sided with AF and disappeared over time were not highly prolific editors in the first place. These disproportionate exits meant that over time, understandings in line with AF’s interpretation of Wikipedia policy become taken for granted as the way the rules should be interpreted, causing gradual institutional changes that amount to a drastic institutional change over a nearly 20-year period.</p>
</div>
</div>
</div>
<div data-magellan-destination="sec13" id="sec13">
<h2> ALTERNATIVE EXPLANATIONS</h2>
<p> This paper has sought to explain why content on the English Wikipedia transformed drastically over time. The explanation hinges on endogenous factors related to early victories, feedback effects, and population loss. In this section, the paper examines two key alternative explanations, finding that they are inapplicable and generally inconsistent with the data (three additional alternative explanations are addressed in the Supplementary Material).<a href="#fn28"><span>Footnote </span>
<sup>28</sup></a></p>
<div data-magellan-destination="sec14" id="sec14">
<h3> External Events and Processes</h3>
<p> One alternative hypothesis is that external events caused ideational change among Wikipedians. For example, Donald Trump’s 2016 election, the 2016 Brexit referendum, and the emergence of “fake news” websites may have caused Wikipedians to re-evaluate how they understand the rules of Wikipedia and the role of Wikipedia in society. However, as the paper documented, the transformation on Wikipedia has been gradual over time, preceding prominent shocks from 2016. Furthermore, the emergence of “fake news” websites does not fit neatly with Wikipedians’ decision to deprecate long-standing traditional news sources, such as the <em>Daily Mail.</em> The events of 2015 and 2016 did not bring source reliability to the fore in a new way on Wikipedia. Rather, Wikipedians had intensely debated the reliability of sources for nearly a decade prior. It took Wikipedians until 2017 to start deprecating sources because the editors that previously vetoed such attempts were no longer active on the encyclopedia.<a href="#fn29"><span>Footnote </span>
<sup>29</sup></a></p>
<p> Another version of this hypothesis is that slow exogenous processes led Wikipedians to re-evaluate their own attitudes toward the guidelines. For example, Wikipedians may have increasingly come to hold more pro-LGBT views, stronger anti-racism views, and pro-science attitudes. While attitudinal change can certainly be documented among certain Wikipedians, they have remained very stable among many of those belonging to AF and PF, as they vote consistently for and against certain items in predictable ways over long time periods. If a disproportionate number of PF editors had not disappeared over time, they would have been able to block drastic changes.</p>
<p> A third version of this hypothesis is that the sources that Wikipedia relies on for content changed how they cover pseudoscience, conspiracy theories, and extremism. In other words, the news media and the scientific community changed, not Wikipedia. While it is true that Wikipedia is necessarily a reflection of what sources say, it is not correct that news sources and studies have uniformly moved in the same direction on all the subject matters listed in <a href="#tab1">Table 1</a> and the Supplementary Material. Even on subject matters where coverage has changed, such as climate change, climate change denial sources have changed tactics in how they argue against climate change. Rather than deny that any warming has occurred, they dispute the precise role of human activity, emphasize how “alarmist” mainstream climate scientists are, and highlight events that purportedly contradict the scientific consensus. Rather than reflect these updates to climate change denialism in mainstream sources, Wikipedians have simply excluded or debunked climate change denial rhetoric in articles. Furthermore, the particular sources that continued to promote pseudoscience, conspiracy theories, and extremism were over time ultimately deemed unreliable on Wikipedia.</p>
</div>
<div data-magellan-destination="sec15" id="sec15">
<h3> Influx of New Editors</h3>
<p> Anyone can create a Wikipedia account and edit. It is therefore reasonable to query whether Wikipedia experienced an influx of new editors with new ideas, thus causing the transformation over time. This would mean that the old guard of Wikipedia editors were simply replaced or outmaneuvered by a new breed of editors. There are several reasons why this is unlikely to have caused the transformation. Wikipedia has a very rigid and complex structure of rules and norms. New editors that edit in ways that older editors disapprove of often find themselves in trouble. As highlighted above, experience is a source of power of Wikipedia that makes it easier for the old guard to shape the encyclopedia, both by sanctioning disruptive newcomers and by indoctrinating newcomers into a “correct” way of editing. Newcomers, therefore, find themselves forced to assimilate or be booted off the platform. It is also unlikely that the later generation of Wikipedia editors tended to be more likely to be experts and predisposed to mainstream science than the first movers on Wikipedia. Judging by self-described descriptions of themselves, many of the earliest Wikipedians were scientists or had advanced degrees, in particular among editors on pages related to pseudoscience.</p>
</div>
</div>
<div data-magellan-destination="sec16" id="sec16">
<h2> CONCLUSION</h2>
<p> Since its inception in 2001, Wikipedia has transformed from an encyclopedia that adopted a strict “teach the controversy” approach (whereby a diversity of opinions and sources were reflected in articles) to one where Wikipedia takes firm sides on contested subjects. Whereas Wikipedia used to normalize and lend credence to pseudoscience, conspiracy theories, and fringe rhetoric, it has over time become firmly anti-pseudoscience and anti-conspiracy theories.</p>
<p> This transformation occurred through endogenous processes that were ultimately rooted in rule ambiguity, early dispute outcomes, and population loss. The resolution of early disputes in several areas of the encyclopedia demobilized certain types of editors (while mobilizing others) and strengthened certain understandings of Wikipedia’s ambiguous rules (while weakening other understandings of Wikipedia rules). Change occurred endogenously and gradually, as shared meanings from within Wikipedia’s collective about the rules got altered through a combination of compulsory power (sanctioning of dissenters by elite actors) and productive power (collective delegitimization of certain rule interpretations).</p>
<p> This explanation for institutional change on Wikipedia can plausibly help to explain institutional change in other contexts. We might observe in other institutions that institutional change happens as losers become demotivated and sanctioned, and winners become motivated and rewarded. For example, career bureaucrats might leave public service when the bureaucracy shifts toward policies that they disagree with. The bureaucrats could stay in the bureaucracy and make it harder for opponents to transform the bureaucracy, but they might instead leave the bureaucracy because they find it demotivating to fight uphill against other bureaucrats. Rather than obstruct change, the population loss of dissident bureaucrats can propel change.</p>
<p> Within political movements and parties, we can also see how establishment figures who are out of step with newly dominant ideas choose voluntarily to retire rather than obstruct change within the movement. This can plausibly be seen in the Republican Party, as Trump critics have opted to retire rather than use their position to steer the movement in a direction that they find more palatable. Similarly, victories for one side within a movement may energize winners and encourage like-minded actors to jump on the bandwagon in support of that side. This may help to explain how the Tea Party cemented its control of the Republican Party (Blum <a href="#r5"><span>Reference Blum</span>2020</a>). It may also help to explain how the conservative legal movement gradually accepted the legal theory behind the unconstitutionality of the Affordable Care Act (ACA), which was considered fringe and weak in 2010, but grew in support as conservative justices in lower courts ruled the ACA unconstitutional, ultimately almost leading the Supreme Court to rule that the ACA was unconstitutional in 2012 (a narrow 5-4 decision upheld the law while hobbling aspects of it). It may also explain why certain police department cultures form, as some police are driven out of the organization, while others get boosted. It has also been posited that the stability and strength of illiberal regimes within the European Union have gradually been strengthened as dissatisfied citizens migrate from authoritarian states to liberal states (Kelemen <a href="#r22"><span>Reference Kelemen</span>2020</a>). However, more research is needed to assess the generalizability of this endogenous mechanism for institutional change.</p>
</div>
</div>
<div>

<div data-magellan-destination="sec18" id="sec18">
<h2> DATA AVAILABILITY STATEMENT</h2>
<p> Research documentation and data that support the findings of this study are openly available in the American Political Science Review Dataverse at <a href="https://doi.org/10.7910/DVN/JZLTQR">https://doi.org/10.7910/DVN/JZLTQR</a>.</p>
</div>
<div>
<h2> ACKNOWLEDGMENTS</h2>
<p> I am grateful for elaborate feedback from Martha Finnemore and Henry Farrell on earlier versions of the manuscript. This article also benefited from comments by Bit Meehan, Eric Grynaviski, Kendrick Kuo, Michael Miller, Natalie Thompson, Yonatan Lupu, participants at the GWU Political Science Department Graduate Caucus workshop, and three anonymous reviewers in the GWU Political Science Department, as well as the editors and four anonymous reviewers at the <em>APSR.</em></p>
</div>
<div data-magellan-destination="sec19" id="sec19">
<h2> CONFLICT OF INTEREST</h2>
<p> The author declares no ethical issues or conflicts of interest in this research.</p>
</div>
<div data-magellan-destination="sec20" id="sec20">
<h2> ETHICAL STANDARDS</h2>
<p> The author affirms this research did not involve human subjects.</p>
</div>
</div>
</div>    <div id="references-list"><h2>References</h2> <div id="r2" aria-flowto="reference-3-content reference-3-button"><p><span><span>Benkler</span>, <span>Yochai</span></span>. <span>2002</span>. “<span>Coase’s Penguin or, Linux and ‘The Nature of the Firm.’</span>” <span>Yale Law Journal</span> <span>12</span>: <span>369</span>–<span>446</span>.<a target="_blank" aria-label="CrossRef link for Coase’s Penguin or, Linux and ‘The Nature of the Firm.’" href="https://dx.doi.org/10.2307/1562247">CrossRef</a><a target="_blank" aria-label="Google Scholar link for Coase’s Penguin or, Linux and ‘The Nature of the Firm.’" href="https://scholar.google.com/scholar_lookup?title=Coase%E2%80%99s+Penguin+or%2C+Linux+and+%E2%80%98The+Nature+of+the+Firm.%E2%80%99&amp;author=Benkler+Yochai&amp;publication+year=2002&amp;journal=Yale+Law+Journal&amp;volume=12&amp;doi=10.2307%2F1562247&amp;pages=369-446">Google Scholar</a></p></div><div id="r3" aria-flowto="reference-4-content reference-4-button"><p><span><span>Bennett</span>, <span>Andrew</span></span>. <span>2008</span>. “<span>Process Tracing: A Bayesian Perspective</span>.” In <span>The Oxford Handbook of Political Methodology</span>, eds. <span><span>Box-Steffensmeier</span>, <span>Janet</span></span>, <span><span>Brady</span>, <span>Henry</span></span>, and <span><span>Collier</span>, <span>David</span></span>, <span>702</span>–21. <span>New York</span>: <span>Oxford University Press</span>.<a target="_blank" aria-label="Google Scholar link for The Oxford Handbook of Political Methodology" href="https://scholar.google.com/scholar_lookup?title=The+Oxford+Handbook+of+Political+Methodology&amp;author=Bennett+Andrew&amp;author=Box-Steffensmeier+Janet&amp;author=Brady+Henry&amp;author=Collier+David&amp;publication+year=2008">Google Scholar</a></p></div><div id="r4" aria-flowto="reference-5-content reference-5-button"><p><span><span>Bleich</span>, <span>Erik</span></span>. <span>2018</span>. “<span>Historical Institutionalism and Judicial Decision-Making: Ideas, Institutions, and Actors in French High Court Hate Speech Rulings</span>.” <span>World Politics</span> <span>70</span> (<span>1</span>): <span>53</span>–<span>85</span>.<a target="_blank" aria-label="CrossRef link for Historical Institutionalism and Judicial Decision-Making: Ideas, Institutions, and Actors in French High Court Hate Speech Rulings" href="https://dx.doi.org/10.1017/S0043887117000272">CrossRef</a><a target="_blank" aria-label="Google Scholar link for Historical Institutionalism and Judicial Decision-Making: Ideas, Institutions, and Actors in French High Court Hate Speech Rulings" href="https://scholar.google.com/scholar_lookup?title=Historical+Institutionalism+and+Judicial+Decision-Making%3A+Ideas%2C+Institutions%2C+and+Actors+in+French+High+Court+Hate+Speech+Rulings&amp;author=Bleich+Erik&amp;publication+year=2018&amp;journal=World+Politics&amp;volume=70&amp;doi=10.1017%2FS0043887117000272&amp;pages=53-85">Google Scholar</a></p></div><div id="r5" aria-flowto="reference-6-content reference-6-button"><p><span><span>Blum</span>, <span>Rachel</span></span>. <span>2020</span>. <span>How the Tea Party Captured the GOP</span>. Chicago, IL: <span>University of Chicago Press</span>.<a target="_blank" aria-label="CrossRef link for How the Tea Party Captured the GOP" href="https://dx.doi.org/10.7208/chicago/9780226687667.001.0001">CrossRef</a><a target="_blank" aria-label="Google Scholar link for How the Tea Party Captured the GOP" href="https://scholar.google.com/scholar_lookup?title=How+the+Tea+Party+Captured+the+GOP&amp;author=Blum+Rachel&amp;publication+year=2020">Google Scholar</a></p></div><div id="r6" aria-flowto="reference-7-content reference-7-button"><p><span><span>Boreus</span>, <span>Kristina</span></span>, and <span><span>Bergström</span>, <span>Göran</span></span>. <span>2017</span>. <span>Analyzing Text and Discourse</span>. <span>Thousand Oaks, CA</span>: <span>SAGE</span>.<a target="_blank" aria-label="Google Scholar link for Analyzing Text and Discourse" href="https://scholar.google.com/scholar_lookup?title=Analyzing+Text+and+Discourse&amp;author=Boreus+Kristina&amp;author=Bergstr%C3%B6m+G%C3%B6ran&amp;publication+year=2017">Google Scholar</a></p></div><div id="r7" aria-flowto="reference-8-content reference-8-button"><p><span><span>Bruckman</span>, <span>Amy</span></span>. <span>2022</span>. <span>Should You Believe Wikipedia?</span> <span>New York</span>: <span>Cambridge University Press</span>.<a target="_blank" aria-label="CrossRef link for Should You Believe Wikipedia?" href="https://dx.doi.org/10.1017/9781108780704">CrossRef</a><a target="_blank" aria-label="Google Scholar link for Should You Believe Wikipedia?" href="https://scholar.google.com/scholar_lookup?title=Should+You+Believe+Wikipedia%3F&amp;author=Bruckman+Amy&amp;publication+year=2022">Google Scholar</a></p></div><div id="r8" aria-flowto="reference-9-content reference-9-button"><p><span><span>Conran</span>, <span>James</span></span>, and <span><span>Thelen</span>, <span>Kathleen</span></span>. <span>2016</span>. “<span>Institutional Change</span>.” In <span>The Oxford Handbook of Historical Institutionalism</span>, eds. <span><span>Fioretos</span>, <span>Orfeo</span></span>, <span><span>Falleti</span>, <span>Tulia</span></span>, and <span><span>Sheingate</span>, <span>Adam</span></span>, <span>51</span>–<span>70</span>. <span>Oxford</span>: <span>Oxford University Press</span>.<a target="_blank" aria-label="Google Scholar link for The Oxford Handbook of Historical Institutionalism" href="https://scholar.google.com/scholar_lookup?title=The+Oxford+Handbook+of+Historical+Institutionalism&amp;author=Conran+James&amp;author=Thelen+Kathleen&amp;author=Fioretos+Orfeo&amp;author=Falleti+Tulia&amp;author=Sheingate+Adam&amp;publication+year=2016&amp;pages=51-70">Google Scholar</a></p></div><div id="r10" aria-flowto="reference-11-content reference-11-button"><p><span><span>Dietelhoff</span>, <span>Nicole</span></span>, and <span><span>Zimmermann</span>, <span>Lisbeth</span></span>. <span>2020</span>. “<span>Things We Lost in the Fire: How Different Types of Contestation Affect the Robustness of International Norms</span>.” <span>International Studies Review</span> <span>22</span> (<span>1</span>): <span>51</span>–<span>76</span>.<a target="_blank" aria-label="Google Scholar link for Things We Lost in the Fire: How Different Types of Contestation Affect the Robustness of International Norms" href="https://scholar.google.com/scholar_lookup?title=Things+We+Lost+in+the+Fire%3A+How+Different+Types+of+Contestation+Affect+the+Robustness+of+International+Norms&amp;author=Dietelhoff+Nicole&amp;author=Zimmermann+Lisbeth&amp;publication+year=2020&amp;journal=International+Studies+Review&amp;volume=22&amp;pages=51-76">Google Scholar</a></p></div><div id="r11" aria-flowto="reference-12-content reference-12-button"><p><span><span>Elkins</span>, <span>Zachary</span></span>, <span><span>Spitzer</span>, <span>Scott</span></span>, and <span><span>Tallberg</span>, <span>Jonas</span></span>. <span>2021</span>. “<span>Non-Automated Content Analysis</span>.” <span>Perspectives on Politics</span> 19 (1): 198–9. In “The Qualitative Transparency Deliberations: Insights and Implications,” by Alan M. Jacobs, Tim Büthe, Ana Arjona, Leonardo R. Arriola, Eva Bellin, Andrew Bennett, Lisa Björkman, et al. <em>Perspectives on Politics</em> 19 (1): 171–208.<a target="_blank" aria-label="Google Scholar link for Non-Automated Content Analysis" href="https://scholar.google.com/scholar_lookup?title=Non-Automated+Content+Analysis&amp;author=Elkins+Zachary&amp;author=Spitzer+Scott&amp;author=Tallberg+Jonas&amp;publication+year=2021">Google Scholar</a></p></div><div id="r12" aria-flowto="reference-13-content reference-13-button"><p><span><span>George</span>, <span>Alexander</span></span>, and <span><span>Bennett</span>, <span>Andrew</span></span>. <span>2005</span>. <span>Case Studies and Theory Development in the Social Sciences</span>. <span>Cambridge, MA</span>: <span>MIT Press</span>.<a target="_blank" aria-label="Google Scholar link for Case Studies and Theory Development in the Social Sciences" href="https://scholar.google.com/scholar_lookup?title=Case+Studies+and+Theory+Development+in+the+Social+Sciences&amp;author=George+Alexander&amp;author=Bennett+Andrew&amp;publication+year=2005">Google Scholar</a></p></div><div id="r13" aria-flowto="reference-14-content reference-14-button"><p><span><span>Gerschewski</span>, <span>Johannes</span></span>. <span>2021</span>. “<span>Explanations of Institutional Change: Reflecting on a ‘Missing Diagonal.’</span>” <span>American Political Science Review</span> <span>115</span> (<span>1</span>): <span>218</span>–33.<a target="_blank" aria-label="CrossRef link for Explanations of Institutional Change: Reflecting on a ‘Missing Diagonal.’" href="https://dx.doi.org/10.1017/S0003055420000751">CrossRef</a><a target="_blank" aria-label="Google Scholar link for Explanations of Institutional Change: Reflecting on a ‘Missing Diagonal.’" href="https://scholar.google.com/scholar_lookup?title=Explanations+of+Institutional+Change%3A+Reflecting+on+a+%E2%80%98Missing+Diagonal.%E2%80%99&amp;author=Gerschewski+Johannes&amp;publication+year=2021&amp;journal=American+Political+Science+Review&amp;volume=115&amp;doi=10.1017%2FS0003055420000751">Google Scholar</a></p></div><div id="r14" aria-flowto="reference-15-content reference-15-button"><p><span><span>Greenstein</span>, <span>Shane</span></span>, <span><span>Gu</span>, <span>Grace</span></span>, and <span><span>Zhu</span>, <span>Feng</span></span>. <span>2021</span>. “<span>Ideology and Composition Among an Online Crowd: Evidence from Wikipedians</span>.” <span>Management Science</span> <span>67</span> (<span>5</span>): <span>3067</span>–86.<a target="_blank" aria-label="CrossRef link for Ideology and Composition Among an Online Crowd: Evidence from Wikipedians" href="https://dx.doi.org/10.1287/mnsc.2020.3661">CrossRef</a><a target="_blank" aria-label="Google Scholar link for Ideology and Composition Among an Online Crowd: Evidence from Wikipedians" href="https://scholar.google.com/scholar_lookup?title=Ideology+and+Composition+Among+an+Online+Crowd%3A+Evidence+from+Wikipedians&amp;author=Greenstein+Shane&amp;author=Gu+Grace&amp;author=Zhu+Feng&amp;publication+year=2021&amp;journal=Management+Science&amp;volume=67&amp;doi=10.1287%2Fmnsc.2020.3661">Google Scholar</a></p></div><div id="r15" aria-flowto="reference-16-content reference-16-button"><p><span><span>Hannan</span>, <span>Michael</span></span>, and <span><span>Freeman</span>, <span>John</span></span>. <span>1977</span>. “<span>The Population Ecology of Organizations</span>.” <span>American Journal of Sociology</span> <span>82</span> (<span>5</span>): <span>929</span>–64.<a target="_blank" aria-label="CrossRef link for The Population Ecology of Organizations" href="https://dx.doi.org/10.1086/226424">CrossRef</a><a target="_blank" aria-label="Google Scholar link for The Population Ecology of Organizations" href="https://scholar.google.com/scholar_lookup?title=The+Population+Ecology+of+Organizations&amp;author=Hannan+Michael&amp;author=Freeman+John&amp;publication+year=1977&amp;journal=American+Journal+of+Sociology&amp;volume=82&amp;doi=10.1086%2F226424">Google Scholar</a></p></div><div id="r17" aria-flowto="reference-18-content reference-18-button"><p><span><span>Herrera</span>, <span>Yoshiko</span></span>, and <span><span>Braumoeller</span>, <span>Bear</span></span>. <span>2004</span>. “<span>Symposium: Discourse and Content Analysis</span>.” <span>Qualitative Methods</span> <span>2</span> (<span>1</span>): <span>15</span>–<span>19</span>.<a target="_blank" aria-label="Google Scholar link for Symposium: Discourse and Content Analysis" href="https://scholar.google.com/scholar_lookup?title=Symposium%3A+Discourse+and+Content+Analysis&amp;author=Herrera+Yoshiko&amp;author=Braumoeller+Bear&amp;publication+year=2004&amp;journal=Qualitative+Methods&amp;volume=2&amp;pages=15-19">Google Scholar</a></p></div><div id="r19" aria-flowto="reference-20-content reference-20-button"><p><span><span>Hirschman</span>, <span>Albert</span></span>. <span>1970</span>. <span>Exit, Voice, and Loyalty</span>. <span>Cambridge, MA</span>: <span>Harvard University Press</span>.<a target="_blank" aria-label="Google Scholar link for Exit, Voice, and Loyalty" href="https://scholar.google.com/scholar_lookup?title=Exit%2C+Voice%2C+and+Loyalty&amp;author=Hirschman+Albert&amp;publication+year=1970">Google Scholar</a></p></div><div id="r20" aria-flowto="reference-21-content reference-21-button"><p><span><span>Jemielniak</span>, <span>Dariusz</span></span>. <span>2014</span>. <span>Common Knowledge?: An Ethnography of Wikipedia</span>. <span>Redwood City, CA</span>: <span>Stanford University Press</span>.<a target="_blank" aria-label="Google Scholar link for Common Knowledge?: An Ethnography of Wikipedia" href="https://scholar.google.com/scholar_lookup?title=Common+Knowledge%3F%3A+An+Ethnography+of+Wikipedia&amp;author=Jemielniak+Dariusz&amp;publication+year=2014">Google Scholar</a></p></div><div id="r21" aria-flowto="reference-22-content reference-22-button"><p><span><span>Jepperson</span>, <span>Ronald</span></span>, and <span><span>Meyer</span>, <span>John</span></span>. <span>2021</span>. <span>Institutional Theory</span>. <span>New York</span>: <span>Cambridge University Press</span>.<a target="_blank" aria-label="CrossRef link for Institutional Theory" href="https://dx.doi.org/10.1017/9781139939744">CrossRef</a><a target="_blank" aria-label="Google Scholar link for Institutional Theory" href="https://scholar.google.com/scholar_lookup?title=Institutional+Theory&amp;author=Jepperson+Ronald&amp;author=Meyer+John&amp;publication+year=2021">Google Scholar</a></p></div><div id="r22" aria-flowto="reference-23-content reference-23-button"><p><span><span>Kelemen</span>, <span>R. Daniel</span></span>. <span>2020</span>. “<span>The European Union’s Authoritarian Equilibrium</span>.” <span>Journal of European Public Policy</span> <span>27</span> (<span>3</span>): <span>481</span>–99.<a target="_blank" aria-label="CrossRef link for The European Union’s Authoritarian Equilibrium" href="https://dx.doi.org/10.1080/13501763.2020.1712455">CrossRef</a><a target="_blank" aria-label="Google Scholar link for The European Union’s Authoritarian Equilibrium" href="https://scholar.google.com/scholar_lookup?title=The+European+Union%E2%80%99s+Authoritarian+Equilibrium&amp;author=Kelemen+R.+Daniel&amp;publication+year=2020&amp;journal=Journal+of+European+Public+Policy&amp;volume=27&amp;doi=10.1080%2F13501763.2020.1712455">Google Scholar</a></p></div><div id="r25" aria-flowto="reference-25-content reference-25-button"><p><span><span>Mahoney</span>, <span>James</span></span>, and <span><span>Thelen</span>, <span>Kathleen</span></span>, eds. <span>2009</span>. <span>Explaining Institutional Change</span>. <span>New York</span>: <span>Cambridge University Press</span>.<a target="_blank" aria-label="CrossRef link for Explaining Institutional Change" href="https://dx.doi.org/10.1017/CBO9780511806414">CrossRef</a><a target="_blank" aria-label="Google Scholar link for Explaining Institutional Change" href="https://scholar.google.com/scholar_lookup?title=Explaining+Institutional+Change&amp;author=Mahoney+James&amp;author=Thelen+Kathleen&amp;publication+year=2009">Google Scholar</a></p></div><div id="r26" aria-flowto="reference-26-content reference-26-button"><p><span><span>Piskorski</span>, <span>Mikolaj Jan</span></span>, and <span><span>Gorbatai</span>, <span>Sndreea</span></span>. <span>2017</span>. “<span>Testing Coleman’s Social-Norm Enforcement Mechanism: Evidence from Wikipedia</span>.” <span>American Journal of Sociology</span> <span>122</span> (<span>4</span>): <span>1183</span>–222.<a target="_blank" aria-label="CrossRef link for Testing Coleman’s Social-Norm Enforcement Mechanism: Evidence from Wikipedia" href="https://dx.doi.org/10.1086/689816">CrossRef</a><a target="_blank" aria-label="Google Scholar link for Testing Coleman’s Social-Norm Enforcement Mechanism: Evidence from Wikipedia" href="https://scholar.google.com/scholar_lookup?title=Testing+Coleman%E2%80%99s+Social-Norm+Enforcement+Mechanism%3A+Evidence+from+Wikipedia&amp;author=Piskorski+Mikolaj+Jan&amp;author=Gorbatai+Sndreea&amp;publication+year=2017&amp;journal=American+Journal+of+Sociology&amp;volume=122&amp;doi=10.1086%2F689816">Google Scholar</a></p></div><div id="r27" aria-flowto="reference-27-content reference-27-button"><p><span><span>Reagle</span>, <span>Joseph</span></span>. <span>2010</span>. <span>Good Faith Collaboration: The Culture of Wikipedia</span>. <span>Cambridge, MA</span>: <span>MIT Press</span>.<a target="_blank" aria-label="CrossRef link for Good Faith Collaboration: The Culture of Wikipedia" href="https://dx.doi.org/10.7551/mitpress/8051.001.0001">CrossRef</a><a target="_blank" aria-label="Google Scholar link for Good Faith Collaboration: The Culture of Wikipedia" href="https://scholar.google.com/scholar_lookup?title=Good+Faith+Collaboration%3A+The+Culture+of+Wikipedia&amp;author=Reagle+Joseph&amp;publication+year=2010">Google Scholar</a></p></div><div id="r30" aria-flowto="reference-30-content reference-30-button"><p><span><span>Sandholtz</span>, <span>Wayne</span></span>. <span>2008</span>. “<span>Dynamics of International Norm Change: Rules Against Wartime Plunder</span>.” <span>European Journal of International Relations</span> <span>14</span> (<span>1</span>): <span>101</span>–31.<a target="_blank" aria-label="CrossRef link for Dynamics of International Norm Change: Rules Against Wartime Plunder" href="https://dx.doi.org/10.1177/1354066107087766">CrossRef</a><a target="_blank" aria-label="Google Scholar link for Dynamics of International Norm Change: Rules Against Wartime Plunder" href="https://scholar.google.com/scholar_lookup?title=Dynamics+of+International+Norm+Change%3A+Rules+Against+Wartime+Plunder&amp;author=Sandholtz+Wayne&amp;publication+year=2008&amp;journal=European+Journal+of+International+Relations&amp;volume=14&amp;doi=10.1177%2F1354066107087766">Google Scholar</a></p></div><div id="r31" aria-flowto="reference-31-content reference-31-button"><p><span><span>Sandholtz</span>, <span>Wayne</span></span>, and <span><span>Stiles</span>, <span>Kendall</span></span>. <span>2009</span>. <span>International Norms and Cycles of Change</span>. <span>Oxford</span>: <span>Oxford University Press</span>.<a target="_blank" aria-label="Google Scholar link for International Norms and Cycles of Change" href="https://scholar.google.com/scholar_lookup?title=International+Norms+and+Cycles+of+Change&amp;author=Sandholtz+Wayne&amp;author=Stiles+Kendall&amp;publication+year=2009">Google Scholar</a></p></div><div id="r33" aria-flowto="reference-33-content reference-33-button"><p><span><span>Shi</span>, <span>Feng</span></span>, <span><span>Teplitskiy</span>, <span>Misha</span></span>, <span><span>Duede</span>, <span>Eamon</span></span>, and <span><span>Evans</span>, <span>James A.</span></span>. <span>2019</span>. “<span>The Wisdom of Polarized Crowds</span>.” <span>Nature Human Behavior</span> <span>3</span>: <span>329</span>–36.<a target="_blank" aria-label="CrossRef link for The Wisdom of Polarized Crowds" href="https://dx.doi.org/10.1038/s41562-019-0541-6">CrossRef</a><a target="_blank" aria-label="Google Scholar link for The Wisdom of Polarized Crowds" href="https://scholar.google.com/scholar_lookup?title=The+Wisdom+of+Polarized+Crowds&amp;author=Shi+Feng&amp;author=Teplitskiy+Misha&amp;author=Duede+Eamon&amp;author=Evans+James+A.&amp;publication+year=2019&amp;journal=Nature+Human+Behavior&amp;volume=3&amp;doi=10.1038%2Fs41562-019-0541-6">Google Scholar</a><a target="_blank" aria-label="PubMed link for The Wisdom of Polarized Crowds" href="https://www.ncbi.nlm.nih.gov/pubmed/30971793">PubMed</a></p></div><div id="r34" aria-flowto="reference-35-content reference-35-button"><p><span><span>Streeck</span>, <span>Wolfgang</span></span>, and <span><span>Thelen</span>, <span>Kathleen</span></span>. <span>2005</span>. <span>Beyond Continuity</span>. <span>Oxford</span>: <span>Oxford University Press</span>.<a target="_blank" aria-label="Google Scholar link for Beyond Continuity" href="https://scholar.google.com/scholar_lookup?title=Beyond+Continuity&amp;author=Streeck+Wolfgang&amp;author=Thelen+Kathleen&amp;publication+year=2005">Google Scholar</a></p></div><div id="r36" aria-flowto="reference-37-content reference-37-button"><p><span><span>Tkacz</span>, <span>Nathaniel</span></span>. <span>2015</span>. <span>Wikipedia and the Politics of Openness</span>. <span>Chicago, IL</span>: <span>University of Chicago Press</span>.<a target="_blank" aria-label="Google Scholar link for Wikipedia and the Politics of Openness" href="https://scholar.google.com/scholar_lookup?title=Wikipedia+and+the+Politics+of+Openness&amp;author=Tkacz+Nathaniel&amp;publication+year=2015">Google Scholar</a></p></div><div id="r38" aria-flowto="reference-39-content reference-39-button"><p><span><span>Voeten</span>, <span>Erik</span></span>. <span>2020</span>. “<span>Making Sense of the Design of International Institutions</span>.” <span>Annual Review of Political Science</span> <span>22</span>: <span>147</span>–63.<a target="_blank" aria-label="CrossRef link for Making Sense of the Design of International Institutions" href="https://dx.doi.org/10.1146/annurev-polisci-041916-021108">CrossRef</a><a target="_blank" aria-label="Google Scholar link for Making Sense of the Design of International Institutions" href="https://scholar.google.com/scholar_lookup?title=Making+Sense+of+the+Design+of+International+Institutions&amp;author=Voeten+Erik&amp;publication+year=2020&amp;journal=Annual+Review+of+Political+Science&amp;volume=22&amp;doi=10.1146%2Fannurev-polisci-041916-021108">Google Scholar</a></p></div><div id="r39" aria-flowto="reference-40-content reference-40-button"><p><span><span>Wiener</span>, <span>Antje</span></span>. <span>2009</span>. “<span>Enacting Meaning-in-Use: Qualitative Research on Norms and International Relations</span>.” <span>Review of International Studies</span> <span>35</span> (<span>1</span>): <span>175</span>–93.<a target="_blank" aria-label="CrossRef link for Enacting Meaning-in-Use: Qualitative Research on Norms and International Relations" href="https://dx.doi.org/10.1017/S0260210509008377">CrossRef</a><a target="_blank" aria-label="Google Scholar link for Enacting Meaning-in-Use: Qualitative Research on Norms and International Relations" href="https://scholar.google.com/scholar_lookup?title=Enacting+Meaning-in-Use%3A+Qualitative+Research+on+Norms+and+International+Relations&amp;author=Wiener+Antje&amp;publication+year=2009&amp;journal=Review+of+International+Studies&amp;volume=35&amp;doi=10.1017%2FS0260210509008377">Google Scholar</a></p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Banning E2EE is stupid (152 pts)]]></title>
            <link>https://github.com/davidchisnall/banning-e2ee-is-stupid</link>
            <guid>38188938</guid>
            <pubDate>Wed, 08 Nov 2023 11:24:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/davidchisnall/banning-e2ee-is-stupid">https://github.com/davidchisnall/banning-e2ee-is-stupid</a>, See on <a href="https://news.ycombinator.com/item?id=38188938">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" id="user-content-banning-end-to-end-encryption-is-stupid" dir="auto"><a href="#banning-end-to-end-encryption-is-stupid">Banning End-to-End Encryption is Stupid</a></h2>
<p dir="auto">Various lawmakers in different countries are proposing to require messaging services to provide a mechanism for law enforcement to decrypt end-to-end encrypted messages.
This kind of legislation fundamentally misunderstands how easy it is for bad people to build their own end-to-end encryption layers on top of other messaging systems.</p>
<p dir="auto">Requiring Signal, WhatsApp, and so on to introduce vulnerabilities into their products does not make life much harder for criminals.
Criminals can easily build or buy an extra layer of encryption on top and exchange messages that can't be decrypted.</p>
<p dir="auto">It does make everyone else less safe.
If a backdoor exists and is usable by authorised people, it will eventually be exploited and used by malicious people.</p>
<p dir="auto">This repository contains a trivial demonstration of this.
It builds a simple tool that allows sending end-to-end encrypted messages over any messaging service, including plain old SMS (though message-length limits may cause problems there).
It is 186 lines of code (and depends on a load of off-the-shelf open-source libraries) and took about an hour to write.</p>
<p dir="auto">Imagine that Alice wants to send a message to Bob, as she often does in cryptography texts.
She needs a secret passphrase, which will be used to derive some keys:</p>
<div data-snippet-clipboard-copy-content="$ cat pass 
Alice has a totally secret passphrase."><pre><code>$ cat pass 
Alice has a totally secret passphrase.
</code></pre></div>
<p dir="auto">This is the only thing that we need to keep secret to be able to build end-to-end encrypted messaging.
Don't worry about how it's used, just remember that this is some secret that no one should be able to guess.</p>
<p dir="auto">She then runs the following command:</p>
<div data-snippet-clipboard-copy-content="$ banning-e2ee-is-stupid -k pass -u bob"><pre><code>$ banning-e2ee-is-stupid -k pass -u bob
</code></pre></div>
<p dir="auto">The program notices that this is the first time that Alice has sent a message to Bob and so asks for his public key and asks Alice to send Bob her public key.
These are written out as a set of English words:</p>
<div data-snippet-clipboard-copy-content="You have not exchanged keys with this user.  You must send them your public key:
celan fiona tasmanian bloomer terminological elca glamis fenceposts troilus ramapo premeditation meth chairpersons addictiveness bergman beauregard 
Please enter their key:"><pre><code>You have not exchanged keys with this user.  You must send them your public key:
celan fiona tasmanian bloomer terminological elca glamis fenceposts troilus ramapo premeditation meth chairpersons addictiveness bergman beauregard 
Please enter their key:
</code></pre></div>
<p dir="auto">At the same time, Bob is preparing to receive his first message from Alice and so ensures that he has a completely unguessable key phrase and runs the tool to decrypt a message:</p>
<div data-snippet-clipboard-copy-content="$ cat pass 
Bob also has a completely unguessable passphrase
$ ../banning-e2ee-is-stupid -k pass -u alice -d
You have not exchanged keys with this user.  You must send them your public key:
luxuriantly hensel soper chinny kilts esai downpours dissimulation adroitly widmann striven breastbone clonmel forecastle abascal barstools 
Please enter their key:"><pre><code>$ cat pass 
Bob also has a completely unguessable passphrase
$ ../banning-e2ee-is-stupid -k pass -u alice -d
You have not exchanged keys with this user.  You must send them your public key:
luxuriantly hensel soper chinny kilts esai downpours dissimulation adroitly widmann striven breastbone clonmel forecastle abascal barstools 
Please enter their key:
</code></pre></div>
<p dir="auto">Alice and Bob must now send each other their public keys.
The easiest way to do this is to send it as a text message or email and then have a phone call where they read it out.
This isn't secret: it doesn't matter if someone else reads the key (you can put it on your website, Facebook profile, whatever), only if they are able to tamper with it.</p>
<p dir="auto">This key-exchange process is handled by apps like Signal automatically.
Doing it well is the hard part of building an end-to-end encrypted messaging app.
Once Alice and Bob have both pasted each others' public key, they can start exchanging messages.
They won't be asked for keys again.
Alice now sees something like this:</p>
<div data-snippet-clipboard-copy-content="Please enter the message to encrypt:
Hi Bob!

Send the following message to the other person:
anomaly forceful amongst ralphie gia ponds scandalous movies ungracious candidate absolution honan lima lambent cutaways embroider locos computers disqualify boehm naik brimming schrieber glebe "><pre><code>Please enter the message to encrypt:
Hi Bob!

Send the following message to the other person:
anomaly forceful amongst ralphie gia ponds scandalous movies ungracious candidate absolution honan lima lambent cutaways embroider locos computers disqualify boehm naik brimming schrieber glebe 
</code></pre></div>
<p dir="auto">This message is now encrypted in such a way that Bob (and only Bob) can decrypt it.
Alice can now send this message in email, or in her favourite messaging program.
She can even paste it into something public like a GitHub Gist or a Pastebin.
No one else can decrypt it and Bob can detect if it's tampered with.
In fact, Alice can paste a load of random messages in different places and Bob can try decrypting them all to find the one that's intended for him.</p>
<p dir="auto">Bob just needs to paste the message from Alice into the program:</p>
<div data-snippet-clipboard-copy-content="Please enter the message to decrypt:
anomaly forceful amongst ralphie gia ponds scandalous movies ungracious candidate absolution honan lima lambent cutaways embroider locos computers disqualify boehm naik brimming schrieber glebe

Decrypted message:
Hi Bob! "><pre><code>Please enter the message to decrypt:
anomaly forceful amongst ralphie gia ponds scandalous movies ungracious candidate absolution honan lima lambent cutaways embroider locos computers disqualify boehm naik brimming schrieber glebe

Decrypted message:
Hi Bob! 
</code></pre></div>
<p dir="auto">This will report 'Decryption failed' if the message has been tampered with, was not from Alice, or was not intended for Bob (these three conditions are indistinguishable).</p>
<p dir="auto">With this simple program (remember, about an hour's quick coding), it is possible for Alice and Bob to exchange messages over any insecure channel.</p>
<p dir="auto">This is intended as a toy demonstration of how simple it is to build encrypted messaging over an unencrypted messaging service.
Over a decade ago, <a href="https://en.wikipedia.org/wiki/TextSecure" rel="nofollow">TextSecure</a> built a product that did this (using much more clever crypto!) that gave a polished user interface.
Even before this, <a href="https://en.wikipedia.org/wiki/Off-the-record_messaging" rel="nofollow">OTR</a> provided plugins for third-party clients for various unencrypted IM networks that allowed end-to-end encryption (again, with stronger security properties than this demo).
These were polished clients that provided secure end-to-end encrypted messaging used an existing messaging network as an insecure transport.</p>
<h2 tabindex="-1" id="user-content-frequently-asked-questions" dir="auto"><a href="#frequently-asked-questions">Frequently asked questions</a></h2>
<p dir="auto"><em>How do I build this thing?</em></p>
<p dir="auto">You probably shouldn't (see disclaimers below).
If you really want to, run:</p>
<div data-snippet-clipboard-copy-content="$ mkdir build
$ cd build
$ cmake .. -G Ninja
$ ninja"><pre><code>$ mkdir build
$ cd build
$ cmake .. -G Ninja
$ ninja
</code></pre></div>
<p dir="auto">This will give you a program called <code>banning-e2ee-is-stupid</code>.
This will store public keys in a database in the directory that you run it from.</p>
<p dir="auto"><em>Isn't this too complex for end users to use?  It requires using a command line and stuff.</em></p>
<p dir="auto">I wrote this in about an hour, much of which was spent learning how to use libraries.
It is absolutely not a polished end-user product.
Something with a half decent UI, better key storage, and so on, would probably be a whole afternoon's effort.</p>
<p dir="auto"><em>What happens if someone intercepts the key-exchange messages?</em></p>
<p dir="auto">Nothing bad.
Each message is encrypted using both the sender's secret key and the receiver's public key.
Decrypting it requires the sender's public key and the receiver's secret key.
Both encrypting and decrypting a message require that you have a secret key, and these are never sent anywhere.</p>
<p dir="auto"><em>What happens if an attacker uses the attack from <a href="https://xkcd.com/538/" rel="nofollow">XKCD 538</a>?</em></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/e16f7a16fecacb8421126bd8f244d27765f967429a1c09bdbec966078c14be25/68747470733a2f2f696d67732e786b63642e636f6d2f636f6d6963732f73656375726974792e706e67"><img src="https://camo.githubusercontent.com/e16f7a16fecacb8421126bd8f244d27765f967429a1c09bdbec966078c14be25/68747470733a2f2f696d67732e786b63642e636f6d2f636f6d6963732f73656375726974792e706e67" alt="XKCD 538" data-canonical-src="https://imgs.xkcd.com/comics/security.png"></a></p>
<p dir="auto">They get your messages.
Sorry.</p>
<p dir="auto"><em>Isn't it easy to spot messages like "aggarwal ashwell kalter stephenville compounders carleton somatic bks sanada airspaces brees lamb's fossilization wadsworth composit downey's arkansans advanta diffferent hewlitt henne rowed airlifts corba fortune's"?</em></p>
<p dir="auto">Yes.
This doesn't apply any <a href="https://en.wikipedia.org/wiki/Steganography" rel="nofollow">Steganography</a> to the output and so traffic analysis (including scanning in the client device) would probably find it easily.
That said, if your heuristic is 'words used in strange ways' then you will get false positives from all teenagers.</p>
<p dir="auto"><em>Where do all of these words come from?</em></p>
<p dir="auto">The encrypted message is pile of binary data.
A lot of messaging apps will be unhappy if you try to send raw binary data and break it in annoying ways.
This program uses one of <a href="https://www.keithv.com/software/wlist/" rel="nofollow">Keith Vertanen's big word lists</a>, truncated to 2^16 entries.
This means that for every two bytes of binary data, we have one word.
The words are all in the top 84K most commonly used English words and so totally indistinguishable from the kind of piffle that might be generated by the kind of politician that thinks banning encryption is remotely feasible.</p>
<p dir="auto"><em>What does this use?</em></p>
<p dir="auto">This uses libsodium for all of the cryptography.
The passhprase hashed using Argon2id, which is intended to be slow (this is where the startup pause comes from) for a brute force attacker.
The encryption uses libsodium's crypto box construction, which uses X25519, XSalsa20, and Poly1305.
If you know what these are, you understand enough about cryptography to not need to read this page.
If you do not know what these are, you should not be voting on legislation about cryptography without talking to an expert.</p>
<h2 tabindex="-1" id="user-content-do-not-use-this" dir="auto"><a href="#do-not-use-this"><strong>DO NOT USE THIS</strong></a></h2>
<p dir="auto">This code is intended to show that it's easy to write something that does end-to-end encryption without the cooperation of the underlying messaging service.
As such, it is intentionally brief.
It does not follow best practices for encryption, in a number of ways:</p>
<ul dir="auto">
<li>It does not try to make the pass phrase storage secure.
Good code would use the operating system's key storage APIs.</li>
<li>It does not provide a mechanism for rolling over keys.
A good system would periodically re-key to handle cases where the key is leaked.</li>
<li>It does not provide forward security.
If your key is leaked, an attacker can impersonate you and can read all messages that you've received.</li>
<li>It does not protect keys in memory.
Keys may show up in core dumps, swap, and so on.</li>
<li>No one has reviewed the use of crypto.
I am not a cryptographer, I probably did something stupid.</li>
<li>It uses random dependencies.
The SQLite and libsodium wrappers were chosen because they were the first results in a DuckDuckGo search.
This is not how you do supply-chain security.</li>
</ul>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hard-to-swallow truths they won't tell you about software engineer job (375 pts)]]></title>
            <link>https://www.mensurdurakovic.com/hard-to-swallow-truths-they-wont-tell-you-about-software-engineer-job/</link>
            <guid>38188689</guid>
            <pubDate>Wed, 08 Nov 2023 10:41:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mensurdurakovic.com/hard-to-swallow-truths-they-wont-tell-you-about-software-engineer-job/">https://www.mensurdurakovic.com/hard-to-swallow-truths-they-wont-tell-you-about-software-engineer-job/</a>, See on <a href="https://news.ycombinator.com/item?id=38188689">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
                <p>Last weekend I had a chance to talk with some students who just got their degree. They are pursuing their first software engineer job. In conversation with them, I learned that they have a pretty wrong perception of this job.</p><p>This is because the reality for these new kids is so skewed. They only see good pay, remote work, team building, and pizza parties. </p><p>These are all good perks, but no one is talking to them about the real things that we do in this job. </p><p>As someone who spent a lot of years in this industry, I gave them a slap of reality in the face. I told them good things but also some hard-to-swallow truths. </p><p>After reading this article, some people will say I am talking overly negatively about it. but my opinion is that these things go together with the job and you have to accept it.</p><h2 id="1-college-will-not-prepare-you-for-the-job">1) College will not prepare you for the job</h2><p>This is the first thing I explained to these guys. </p><p>To precisely describe how college will prepare you for the job imagine that you are learning how to swim. </p><p>Your instructor spends a huge amount of time to describe you all the moves you need to make. He makes you recite all those moves, asks you questions about it and you have exams about it. But you never touch the water.</p><p>After 5 years, you get a piece of paper that proves your swimming skills. Then the day comes, and you have to swim now. The guys in the swimming place just kick you into the water.</p><p>You have a hard time breathing, you fight for your life. Maybe you will drown, maybe you will manage to swim.</p><p>That's what the first 6 months look like for a freshly graduated student in a software engineer job.</p><p>The college will prepare you for some basics, but what most of the colleges teach is so far away from day-to-day jobs. Most of the professors who teach at universities are not good software engineers. </p><p>Only a small percentage of them even worked as software engineers. Also, the university curriculums are heavily outdated. They trot years behind the software development market needs.</p><p>You have to put in extra work while you are in college. Code more projects besides homework and seminars. Do some volunteering. Learn about business domains to prepare for the job that awaits you. </p><p>Most of the students don't do that. They wait until they get their diploma to start working on their portfolio.</p><h2 id="2-you-will-rarely-get-greenfield-projects">2) You will rarely get greenfield projects</h2><p>In college or boot camps, you get a lot of smaller assignments that you write from scratch. Total freedom to express yourself. You can implement all the fancy stuff you learn, like algorithms or design patterns. </p><p>The time you spend on those assignments is at most a few weeks, but mostly a couple of days of work. Typically those assignments contain at most 500 lines of code.</p><p>In day to day job you are working with projects that contain multiple layers and thousands of lines of code. Multiple people work at the same time on those projects. You have limited freedom, you have to adapt to the project. The time you spend on projects is usually half a year to a couple of years.</p><p>Sometimes you spend a whole week fixing the nasty bug. The fix is just a couple of lines of code. You talk with your colleagues. You exchange information about the project. You collaborate with them to get approval for your solution.</p><p>New projects are rare, and most of the time you work on existing projects. You can consider yourself lucky if you get the normal project and not some old legacy project.</p><figure><img src="https://www.mensurdurakovic.com/content/images/2023/10/image-2-1.png" alt="" loading="lazy" width="768" height="6088" srcset="https://www.mensurdurakovic.com/content/images/size/w600/2023/10/image-2-1.png 600w, https://www.mensurdurakovic.com/content/images/2023/10/image-2-1.png 768w" sizes="(min-width: 720px) 720px"></figure><h2 id="3-nobody-gives-a-f-about-your-clean-code">3) Nobody gives a f*** about your clean code</h2><p>You can forget that your boss will tell you: "Congratulations on writing this elegant and clean code, I am gonna give you a raise!". Quite the opposite, nobody cares about your clean code. </p><p>Don't get me wrong, people will expect you to write good and clean code. Still, you will rarely get any praise for it. Except sometimes from your colleagues who will review your code.</p><p>This may be a shock for some new folks, but it makes perfect sense. As a software engineer, your primary task is to generate value for users. Writing code is just a step that accomplishes that goal. </p><p>You can think of it as the following cycle:</p><ul><li>software engineer writes code</li><li>users get new features</li><li>more users use your products</li><li>company profits from products</li></ul><p>So code is just a tool to get profit. </p><p>I have seen so many graveyards of projects, with horrible legacy codebases. Still, these projects are successful as they have fancy landing pages and solve user's problems. So users are happy to pay for using them.</p><p>The user doesn't know how the codebase looks. The user just sees what features that product is offering. So don't get overly attached to your clean and elegant code. Focus on shipping the feature on time and bug-free.</p><h2 id="4-you-will-sometimes-work-with-incompetent-people">4) You will sometimes work with incompetent people</h2><p>People have prejudices that only smart and competent people work in the IT industry. Especially the software development branch. But this is far from the truth. </p><p>As in every job, you will sometimes have incompetent people in your environment. Working with them is very frustrating. They waste so much time and create a toxic environment. On top of that, they are extremely unproductive. All this reflects on deadlines and produces delays. This costs companies money and resources.</p><p>Unfortunately, I also had experience working with those kinds of people. I have to say, they tested my nerves so well that I spent a good amount of time thinking of ways to get around their incompetence.  </p><p>Here are some advice:</p><ul><li>try to be efficient and productive as much as you can, focus on yourself and not on them</li><li>try other options/solutions that don't involve that person in the process</li><li>document everything you do. If things go wrong, you will have proof of their incompetence</li><li>if you have a blocker because that person didn't do their job, try to ask someone else to unblock you  (if it's possible)</li><li>talk directly to them, be professional but not mean, and tell them what and how they can improve</li></ul><p>Remember that there is no need to be a jerk to them. </p><p>Sometimes you don't know the whole story. I have seen some cases where a person just can't do their job properly. They are burdened with tons of tasks and doing work for 2 people.</p><h2 id="5-get-used-to-being-in-meetings-for-hours">5) Get used to being in meetings for hours </h2><p>Meetings are an important part of the software development job. Some of them are good, but some of them are just time wasters. </p><p>There are recurring meetings scheduled on a daily or weekly basis. Most of these are not productive. The majority of them are forced by a person who is organizing them because that's the only "work" that that person is doing. </p><p>It's just an empty protocol to prove their purpose of existence in the company.</p><figure><iframe width="200" height="113" src="https://www.youtube.com/embed/UDpKWVuBNQE?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" title="Meeting | MonkeyUser 5SP Animation"></iframe></figure><p>On the other hand, there are productive meetings. Those meetings ensure information exchange between team members or different teams. </p><p>The majority of software engineers hate meetings. But remember that your job is also to communicate about things openly and proactively. </p><p>Sharing information is crucial for projects to move forward. When you share information it can help other teams to better understand what you are doing and the opposite. </p><h2 id="6-they-will-ask-you-for-estimates-a-lot-of-times">6) They will ask you for estimates a lot of times</h2><p>Business revolves around numbers. Every project has its cost, and to calculate the cost, management needs to estimate how long it will take to build a certain feature.</p><p>Then, it goes down to software engineers to estimate their work. Usually, estimates are time-based, but sometimes they also ask for complexity estimates. </p><p>In a lot of situations, you will have no clue how long it will take to build something. You read requirements, do some research and you give a number. </p><figure><img src="https://www.mensurdurakovic.com/content/images/2023/10/image-6.png" alt="" loading="lazy" width="1600" height="987" srcset="https://www.mensurdurakovic.com/content/images/size/w600/2023/10/image-6.png 600w, https://www.mensurdurakovic.com/content/images/size/w1000/2023/10/image-6.png 1000w, https://www.mensurdurakovic.com/content/images/2023/10/image-6.png 1600w" sizes="(min-width: 720px) 720px"></figure><p>Later, when you start to work on that feature, you encounter many problems that you weren't aware of when you gave time estimates. Then you need to compensate for the wasted and hope not to break the deadline.</p><p>That's why it's always good to underpromise but overdeliver. </p><p>For example, when your project manager asks you to implement feature X by Friday, you won't say "Oh, I can finish it by Tuesday". Instead, you will say: "Sure, no problem". </p><p>Why?</p><p>Because if you promise to deliver it by Tuesday and you run into some problems, you won't be able to fulfill the promise. Instead, if you accept Friday as a deadline, and you finish it by Wednesday, you can deliver it 2 days earlier. </p><p>There are a lot of formulas on how to do estimates, and everyone has their own rules. I also have my own rules. </p><p>If I need to deliver some feature, and I think it will take 2 days, I add roughly 40% more time to it, just to be safe. So, in this case, the estimate will be 3 days. Later, if I am done in 2 days, I can just deliver it earlier.</p><h2 id="7-bugs-will-be-your-arch-enemy-for-life">7) Bugs will be your arch-enemy for life</h2><p>The more you code, the more you are aware that bugs in the code are everywhere. When you are just starting with programming, you think you will code something, it will work fine and it's the end of the story.</p><p>But in reality, it's a different story. There are countless things that can produce bugs:</p><ul><li>your own code - humans make mistakes, and you should not trust that code is working perfectly. You can write tests, but bugs can occur after that due to various reasons that you aren't even aware of.</li><li>3rd party libraries - those libraries are also written by software engineers like you and me. Always watch for activity and how frequently those libraries are updated.</li><li>hardware failure - software relies on hardware. Mark Hanna explained what your software is without hardware in his quote: "It's fugayzi, fugazi. It's a whazy. It's a woozie. It's fairy dust. It doesn't exist. It's never landed. It is no matter. It's not on the elemental chart. It's not f***ing real."</li></ul><figure><img src="https://media.tenor.com/Zbfjs-GE8p0AAAAC/rookie-numbers.gif" alt="" loading="lazy" width="498" height="289"></figure><ul><li>electricity - yep, hardware needs electric power to run, without it, it's useless. I worked on one project with Raspberry Pi. The client had constant problems with the device turning off at random times. After days of investigation, we finally found out the issue. He used a different power supply than the original one provided. Because of that device was turning off at random times.</li></ul><p>So the truth is you should assume that everything has bugs. That's why experienced devs never trust their code if it runs successfully on the first try. Even if the QA engineer reports a bug, assume that the bug ticket has a "bug" and check for everything.</p><h2 id="8-uncertainty-will-be-your-toxic-friend">8) Uncertainty will be your toxic friend</h2><p>In this job, you will feel uncertainty almost all the time. </p><p>I already explained the estimates example above. That's just one example where you feel uncertainty. You give your best shot but you are not 100% sure you can finish the work in that estimate.</p><p>Besides that, there are countless other things that are uncertain. Here are some examples:</p><ul><li>implementing something in your project you never worked with, eg. 3rd party API - how are you going to implement something you aren't familiar with</li><li>transfer to a new project, with new technologies - you will think about how you are going to be efficient and productive with something you need to learn</li><li>move to a new company - you are unsure how you are going to settle in and vibe with new people</li><li>bug report on the day you need to finish the work - you fear that you are gonna break the deadline</li><li>job security - economic situations, pandemics, wars, and other factors heavily affect this industry which results in layoffs</li><li>the evolution of technology - you are never sure if tomorrow you are gonna be replaced by some new technologies like AI</li></ul><p>The good thing about uncertainty is that drives you to be a better software engineer. It demands improvements and learning if you want to stay in the game.</p><h2 id="9-it-will-be-almost-impossible-to-disconnect-from-your-job">9) It will be almost impossible to disconnect from your job</h2><p>From time to time, I catch myself thinking about my job, problems, and bugs. Or things I have to do tomorrow when I should relax and chill. </p><p>Sometimes, cold water in the shower wakes me up from my thinking about how I am gonna fix the nasty bug I worked on yesterday. I had countless squabbles with my girlfriend about why I am on Slack when we are on the beach. </p><p>So I publicly admit, that I have a hard time disconnecting from work. </p><p>It's especially hard to disconnect when you are working from home. If your laptop is on, you can always check emails or Slack messages.</p><p>So to avoid all this:</p><ul><li>I turn off my laptop after I am done with the work,</li><li>I put quiet hours on my mobile phone for my business emails</li><li>I pause Slack notifications after working hours. I disable them on weekends.</li><li>When my mind gets into this "think about work" loop, I try to immediately cut it out. I remind myself that rest and relaxation are important to be productive. </li><li>I take long walks after work. On some days I do sports like padel or football. </li><li>I try to engage socially as much as I can, avoiding screen time after work.</li></ul><p>Still, with all these steps I do every day, I fail a lot of times.</p><h2 id="10-you-will-profit-more-from-good-soft-skills-than-from-good-technical-skills">10) You will profit more from good soft skills than from good technical skills</h2><p>Technical skills are the ones you can learn easily. With different projects, you can understand a particular programming language. You can learn its syntax, pros and cons. It's just a matter of practice.</p><p>On the other hand, soft skills are much harder to improve. Improvement takes a lot of mental strength. You must do things you are not comfortable with. </p><p>You have to put yourself in situations where you can improve or practice particular soft skills.</p><p>For example, communication is one soft skill that people always talk about. Let's say you suck at public speaking. You have to force yourself into situations where you can practice some public speaking.</p><p>It's very hard to intentionally put yourself into situations where you know you will suck at. Your mind will do everything to avoid those situations. It will bring hundreds of excuses and it's easy to give up.</p><p>Besides communication, there are other <a href="https://www.mensurdurakovic.com/boost-your-career-with-top-ten-soft-skills/" rel="noreferrer">soft skills</a>:</p><ul><li>teamwork</li><li>learning mindset</li><li>organization/time management</li><li>emotional intelligence/empathy</li><li>approachability</li><li>persistence/patience</li><li>confidence</li></ul><p>I have met a lot of folks who are good with technical skills but awful to work with.</p><p>For example, one colleague would ask me for help a lot of times. I helped him a couple of times. Then, I noticed after we fixed his problems, he would come back to me and blame me for messing up other things on the project. Then I had to spend more time with him fixing stuff I wasn't even aware of. And because he was blaming me with such a bad tone, I stopped helping him. I would say that I have a lot on my plate to do, so I can help him tomorrow. </p><p>Another example, I was the new guy on the project. A colleague (let's call him George) was assigned to help me with anything I needed. I set up the project pretty much by myself, but there was one error I was getting when I tried to run the project. I asked George for help. He spent maybe 2 minutes with me in total to solve a problem and said that he didn't know the solution. I thanked him anyway, tried to solve the error on my own but finally succeded with the help of colleague Michael. On daily standup, George said that he spent his whole day supporting me. I never asked George for help, after that. </p><p>One more example, there was one colleague who was the main man on the project. Still, the whole team hated him (other devs, project managers, QA, designers, etc). He was a good software engineer, but a real jerk. Extremely rude in communication with everyone. He never wanted to admit he was wrong or accept constructive criticism of his code. Management tolerated him as he was always the loudest one in the room. When he finally resigned, the whole team was celebrating. </p><p>With good soft skills, people will like you more and you have a better chance of getting a raise or promotion. If you are technically gifted but hard to work with, your chances are slightly reduced.</p><p>Also, with good soft skills, people who know you will spread a good word behind your back. They can recommend you for the job, without even you knowing about it.</p><h2 id="conclusion">Conclusion</h2><p>Software development is not a dream job. </p><p>Working in software engineering often means long working hours. Most of the time, you are glued to a computer screen, with little work-life balance. </p><p>The job demands an online presence, sometimes even after work hours. This often leads to stress and limited personal time. </p><p>Additionally, job satisfaction is frequently hindered by tedious tasks. Depending on the situation you have limited career growth prospects. There is also potential isolation in remote work.  And there is always a threat of job insecurity due to rapidly evolving technology.</p><p>But, there is also positive stuff.</p><p>Software development nurtures continuous innovation. Software engineers can create attractive applications and solve interesting problems.</p><p>The global demand for software solutions across diverse industries is big. This means there is always a demand for good software engineers. Software development careers provide flexibility with remote work options. </p><p>It's one big blessing to work from any location. Flexibility allows you to sleep in the morning without an alarm. You can work from your home in comfy pajamas. Also, you don't waste your precious time and money on commuting.</p>
            </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Chamberlain blocks smart garage door opener from working with smart homes (242 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2023/11/chamberlain-blocks-smart-garage-door-opener-from-working-with-smart-homes/</link>
            <guid>38188614</guid>
            <pubDate>Wed, 08 Nov 2023 10:25:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2023/11/chamberlain-blocks-smart-garage-door-opener-from-working-with-smart-homes/">https://arstechnica.com/gadgets/2023/11/chamberlain-blocks-smart-garage-door-opener-from-working-with-smart-homes/</a>, See on <a href="https://news.ycombinator.com/item?id=38188614">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <header>
            <h4>
      Your phone is our billboard    —
</h4>
            
            <h2 itemprop="description">Chamberlain packed its app with ads while disabling third-party access. </h2>
                    </header>
        <div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/11/2019myQ-LM-Ware233-800x389.jpg" alt="A photo of the myQ app from LiftMaster's website.">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/11/2019myQ-LM-Ware233.jpg" data-height="700" data-width="1440">Enlarge</a> <span>/</span> A photo of the myQ app from LiftMaster's website.</p><p>Liftmaster</p></figcaption>  </figure>

  




<!-- cache hit 679:single/related:6e3ba8fc03d651b12cdeb85680d92238 --><!-- empty -->
<p>Chamberlain Group—the owner of <a href="https://www.lakewoodgaragedoor.biz/blog/the-history-of-garage-door-openers">most</a> of the garage door opener brands like LiftMaster, Chamberlain, Merlin, and Grifco—would like its customers to stop doing smart home things with its "myQ" smart garage door openers. The company recently issued a <a href="https://chamberlaingroup.com/press/a-message-about-our-decision-to-prevent-unauthorized-usage-of-myq">statement</a> decrying "unauthorized usage" of its smart garage door openers. That's "unauthorized usage" by <em>the people who bought the garage door opener</em>, by the way. Basically, Chamberlain's customers want to trigger the garage door and see its status through third-party smart home apps, and Chamberlain doesn't want that.</p>
<p>Here's the statement:</p>
<blockquote><p>Chamberlain Group recently made the decision to prevent unauthorized usage of our myQ ecosystem through third-party apps.</p>
<p>This decision was made so that we can continue to provide the best possible experience for our 10 million+ users, as well as our authorized partners who put their trust in us. We understand that this impacts a small percentage of users, but ultimately this will improve the performance and reliability of myQ, benefiting all of our users.</p>
<p>We encourage those who were impacted to check out our authorized partners here: <a href="https://www.myq.com/works-with-myq" target="_blank" rel="noopener">https://www.myq.com/works-with-myq</a>.</p></blockquote>
<p>We caught wind of this statement through the Home Assistant blog, a popular open source smart home platform. The myQ integration is being stripped from the project because it doesn't work anymore. Allegedly, Chamberlain has been sabotaging Home Assistant support for a while now, with the integration maintainer, Lash-L, telling the Home Assistant blog, "We are playing a game of cat and mouse with MyQ and right now it looks like the cat is winning."</p>
<p>Our immediate question is <em>why</em> would any garage opener company care about customers using its garage door opener. You sell garage door openers—isn't usage the goal? A quick perusal through the app store reviews reveals what's going on. The iOS app is sitting pretty at 4.8 stars, but the Android app has suffered a wave of one-star reviews starting in October.</p>
<p>"Sadly, this app now displays advertisement at the very top and I cannot find a way to disable it," writes one Play Store reviewer (Google doesn't provide links to reviews). "This is very disturbing and on top of it, it moves my garage opening button out of the visible part of the screen. So to use it I now have to first look at the ads, then scroll down and hope to find my button." Another user writes, "I don't want ads in an app that I have already paid for the companion product." Other one-star reviews mention things like, "I clicked door open/close event and it popped up the video storage subscription dialog to ask me to subscribe," and, "Most of the app is dedicated to trying to upsell you on services and devices you don't need."</p>                                            
                                                        
<p><em>Ah</em>, now it makes sense. Your garage door opener app isn't here only to open your garage door; it's here to display ads and upsell you on services. Using third-party apps would get around Chamberlain's hardware-app-as-ad-platform strategy, so they are now banned. Another part of this is probably the plug at the end of Chamberlain's statement to "check out our authorized partners," which includes companies like Amazon and Alarm.com.</p>
<figure><img alt="The logo of the Chamberlain Group, which owns over 60 percent of the garage door market." src="https://cdn.arstechnica.net/wp-content/uploads/2023/11/Chamberlain_Group_Logo.jpg" width="800" height="243"><figcaption><p>The logo of the Chamberlain Group, which owns over 60 percent of the garage door market.</p><p>Chamberlain Group</p></figcaption></figure>
<p>Presumably, these "authorized partners" are paying a fee to work with garage door openers that have already been sold to customers. Home Assistant's founder, Paulus Schoutsen, writes that while Chamberlain Group has never responded to Home Assistant's requests to work together, the open source project "cannot pay a partnership fee. Not only is this financially not viable, it also goes against our values." The integration is being removed in next month's release, though Schoutsen says, "We would happily welcome this integration back if Chamberlain Group would work with us for the good of their customers."</p>
<p>For users stuck with a Chamberlain garage door opener, Home Assistant recommends a little circuit board called a "<a href="https://paulwieland.github.io/ratgdo/">ratgdo</a>," which is specifically meant to hack into Chamberlain/LiftMaster garage door openers. This connects the garage door button wires to your Wi-Fi—something Chamberlain presumably can't break on purpose—and freely communicates with everything. It can even "report back the actual status of the door (closed, opening, open, closing)" somehow.</p>
<p>We'll leave you with some consumer advocacy from Schoutsen and the Home Assistant team: "Once a company decides to be hostile to its customers, the only way we can win is by not playing their game at all. Do not buy products or services from companies that treat their customers this way. Tell your friends not to deal with companies that treat their customers this way. Buy products that work locally and won’t stop functioning when management wants an additional revenue stream."</p>

                                                </div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Home Assistant blocked from integrating with Garage Door opener API (400 pts)]]></title>
            <link>https://www.home-assistant.io/blog/2023/11/06/removal-of-myq-integration/</link>
            <guid>38188162</guid>
            <pubDate>Wed, 08 Nov 2023 09:04:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.home-assistant.io/blog/2023/11/06/removal-of-myq-integration/">https://www.home-assistant.io/blog/2023/11/06/removal-of-myq-integration/</a>, See on <a href="https://news.ycombinator.com/item?id=38188162">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<article>
<header>


</header>
<p><strong>TL;DR:</strong> The MyQ integration will be removed from Home Assistant in release 2023.12 on December 6, 2023. Chamberlain Group, the owners of MyQ, have released a public statement saying they will continue blocking access to third-party apps, like the MyQ integration. For current MyQ users we recommend <a href="https://paulwieland.github.io/ratgdo/" rel="external nofollow">ratgdo</a>, a device that physically connects to your MyQ garage door opener and allows you to control it locally.</p>
<p>If you own a garage door opener from Chamberlain or Liftmaster, you are probably familiar with MyQ. It’s a cloud-based smart home brand owned by Chamberlain Group, best known for its smart garage devices. MyQ is also currently one of the most problematic integrations for Home Assistant users. The MyQ garage door opener integration has, for the past months, been in a state of <a href="https://community.home-assistant.io/t/the-current-state-of-myq-from-the-codeowner/630623">constant repair</a> as the integration breaks, is fixed, and then breaks again. This is a direct result of actions taken by MyQ to block access from third parties.</p>
<a name="read-more"></a>
<p>Last month, Chamberlain Group put out a <a href="https://chamberlaingroup.com/press/a-message-about-our-decision-to-prevent-unauthorized-usage-of-myq" rel="external nofollow">statement</a> by their CTO, Dan Phillips, on this matter:</p>
<blockquote>
<p>Chamberlain Group recently made the decision to prevent unauthorized usage of our myQ ecosystem through third-party apps. This decision was made so that we can continue to provide the best possible experience for our 10 million+ users, as well as our authorized partners who put their trust in us. We understand that this impacts a small percentage of users, but ultimately this will improve the performance and reliability of myQ, benefiting all of our users.</p>
</blockquote>
<p>This <em>‘unauthorized usage’</em> appears to refer to the MyQ integration for Home Assistant which was added to Home Assistant in February, 2017. We have reached out to Chamberlain Group in several ways to see if we can come to an understanding, but we have not received an official response. We can only assume that this means Chamberlain Group has made its decision and will force customers to use only the MyQ app or those of their authorized partners.</p>
<p>You may wonder if Home Assistant could become an authorized partner. In their partner program, the partner companies pay Chamberlain Group for the privilege of letting MyQ owners control their own garage doors. We are open to working together with Chamberlain Group, but as Home Assistant is an open-source project, we cannot pay a partnership fee. Not only is this financially not viable, it also goes against our values. MyQ users should be able to access the devices they paid for and the data they own in any way they want, without a third party having to pay an additional fee.</p>
<p>So, to quote the maintainer of the MyQ integration, <a href="https://github.com/Lash-L" rel="external nofollow">Lash-L</a>:</p>
<blockquote>
<p>We are playing a game of cat and mouse with MyQ and right now it looks like the cat is winning.</p>
</blockquote>
<p>Once a company decides to be hostile to its customers, the only way we can win is by not playing their game at all. Do not buy products or services from companies that treat their customers this way. Tell your friends not to deal with companies that treat their customers this way. Buy products that work locally and won’t stop functioning when management wants an additional revenue stream.</p>
<p>Because we cannot continue to work around Chamberlain Group if they keep blocking access to third parties, the MyQ integration will be removed from Home Assistant in the upcoming 2023.12 release on December 6, 2023. We are very disappointed that it has come to this and sincerely hope that Chamberlain Group is willing to reconsider its position. We would happily welcome this integration back if Chamberlain Group would work with us for the good of their customers.</p>
<p>For now, if you are a MyQ owner, we’re afraid you are in the ‘small percentage of users’ Chamberlain Group refuses to serve. We recommend buying <a href="https://paulwieland.github.io/ratgdo/" rel="external nofollow">ratgdo</a>.</p>
<p>Ratgdo is a fully local, ESPHome-based, solution that is compatible with MyQ’s security+ protocol and can be installed on an existing MyQ system by connecting three wires. It offers the same garage door controls that MyQ does and even adds features that MyQ does not have, like motion events, controlling the light, and locking out wired remotes.</p>
</article>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[When Linux Spooked Microsoft: Remembering 1998's Leaked 'Halloween Documents' (115 pts)]]></title>
            <link>https://linux.slashdot.org/story/23/11/05/046247/when-linux-spooked-microsoft-remembering-1998s-leaked-halloween-documents</link>
            <guid>38187614</guid>
            <pubDate>Wed, 08 Nov 2023 07:23:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://linux.slashdot.org/story/23/11/05/046247/when-linux-spooked-microsoft-remembering-1998s-leaked-halloween-documents">https://linux.slashdot.org/story/23/11/05/046247/when-linux-spooked-microsoft-remembering-1998s-leaked-halloween-documents</a>, See on <a href="https://news.ycombinator.com/item?id=38187614">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="fhbody-172173040"><p>
			
		 	
				It happened a quarter of a century ago.   The <a href="https://archive.nytimes.com/www.nytimes.com/library/tech/98/11/biztech/articles/03memo.html"> <em>New York Times</em> wrote</a> that "An <a href="http://www.catb.org/~esr/halloween/halloween1.html">internal memorandum</a> reflecting the views of some of Microsoft's top executives and software development managers reveals deep concern about the threat of free software and proposes a number of strategies for competing against free programs that have recently been gaining in popularity."

<i> The memo warns that the quality of free software can meet or exceed that of commercial programs and describes it as a potentially serious threat to Microsoft.   The document was sent anonymously last week to Eric Raymond, a key figure in a loosely knit group of software developers who collaboratively create and distribute free programs ranging from operating systems to Web browsers.   Microsoft executives acknowledged that the document was authentic...<p> 

In addition to acknowledging that free programs can compete with commercial software in terms of quality, the memorandum calls the free software movement a "long-term credible" threat and warns that employing a traditional Microsoft marketing strategy known as "FUD," an acronym for "fear, uncertainty and doubt," will not succeed against the developers of free software.   The memorandum also voices concern that Linux is rapidly becoming the dominant version of Unix for computers powered by Intel microprocessors. </p><p> 
 The competitive issues, the note warns, go beyond the fact that the software is free. It is also part of the open-source software, or O.S.S., movement, which encourages widespread, rapid development efforts by making the source code — that is, the original lines of code written by programmers — readily available to anyone. This enables programmers the world over to continually write or suggest improvements or to warn of bugs that need to be fixed.  The memorandum notes that open software presents a threat because of its ability to mobilize thousands of programmers.  "The ability of the O.S.S. process to collect and harness the collective I.Q. of thousands of individuals across the Internet is simply amazing," the memo states. "More importantly, O.S.S. evangelization scales with the size of the Internet much faster than our own evangelization efforts appear to scale."</p></i> <br>


Back in 1998, Slashdot's CmdrTaco <a href="https://slashdot.org/story/98/11/07/1259212/ms-ponders-fighting-linux-with-the-law">covered the whole brouhaha</a> — including <a href="http://www.cnn.com/TECH/computing/9811/06/linux.threat.idg/">this CNN article</a>:

<i>A second internal Microsoft memo on the threat Linux poses to Windows NT calls the operating system "a best-of-breed Unix" and wonders aloud if the open-source operating system's momentum could be slowed in the courts.<p> 

 As with the first "Halloween Document," the memo — written by product manager Vinod Valloppillil and another Microsoft employee, Josh Cohen — was obtained by Linux developer Eric Raymond and posted on the Internet. In it, Cohen and Valloppillil, who also authored the first "Halloween Document," appear to suggest that Microsoft could slow the open-source development of Linux with legal battles.  "The effect of patents and copyright in combating Linux remains to be investigated," the duo wrote.</p></i> <br>

Microsoft's slogain in 1998 was "Where do you want to go today?"  So Eric Raymond published the documents on his web site under the headline  "Where will Microsoft try to drag you today? Do you really want to go there?" </p><p> 

25 years later, and it's all still up there and preserved for posterity on Raymond's web page — a collection of <a href="http://www.catb.org/~esr/halloween/index.html">leaked Microsoft documents and related materials</a> known collectively as "<a href="https://en.wikipedia.org/wiki/Halloween_documents">the Halloween documents</a>." And Raymond made a point of thanking the writers of the documents, "for authoring such remarkable and effective testimonials to the excellence of Linux and open-source software in general."</p><p> 
<em>Thanks to long-time Slashdot reader <a href="https://www.slashdot.org/~mtaht">mtaht</a> for remembering the documents' 25th anniversary...</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stop making every conversation about yourself (162 pts)]]></title>
            <link>https://thoguhts.substack.com/p/stop-making-every-conversation-about</link>
            <guid>38187430</guid>
            <pubDate>Wed, 08 Nov 2023 06:39:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thoguhts.substack.com/p/stop-making-every-conversation-about">https://thoguhts.substack.com/p/stop-making-every-conversation-about</a>, See on <a href="https://news.ycombinator.com/item?id=38187430">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><div><figure><a target="_blank" href="https://images.unsplash.com/photo-1586806974856-c55e8b9364e4?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxNXx8dGFsa2luZ3xlbnwwfHx8fDE2OTgzODk2Mzd8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://images.unsplash.com/photo-1586806974856-c55e8b9364e4?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxNXx8dGFsa2luZ3xlbnwwfHx8fDE2OTgzODk2Mzd8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1586806974856-c55e8b9364e4?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxNXx8dGFsa2luZ3xlbnwwfHx8fDE2OTgzODk2Mzd8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1586806974856-c55e8b9364e4?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxNXx8dGFsa2luZ3xlbnwwfHx8fDE2OTgzODk2Mzd8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1586806974856-c55e8b9364e4?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxNXx8dGFsa2luZ3xlbnwwfHx8fDE2OTgzODk2Mzd8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw"><img src="https://images.unsplash.com/photo-1586806974856-c55e8b9364e4?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxNXx8dGFsa2luZ3xlbnwwfHx8fDE2OTgzODk2Mzd8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" width="1200" height="800" data-attrs="{&quot;src&quot;:&quot;https://images.unsplash.com/photo-1586806974856-c55e8b9364e4?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxNXx8dGFsa2luZ3xlbnwwfHx8fDE2OTgzODk2Mzd8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:false,&quot;imageSize&quot;:&quot;large&quot;,&quot;height&quot;:4000,&quot;width&quot;:6000,&quot;resizeWidth&quot;:1200,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;man in black jacket standing beside body of water during sunset&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="man in black jacket standing beside body of water during sunset" title="man in black jacket standing beside body of water during sunset" srcset="https://images.unsplash.com/photo-1586806974856-c55e8b9364e4?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxNXx8dGFsa2luZ3xlbnwwfHx8fDE2OTgzODk2Mzd8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1586806974856-c55e8b9364e4?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxNXx8dGFsa2luZ3xlbnwwfHx8fDE2OTgzODk2Mzd8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1586806974856-c55e8b9364e4?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxNXx8dGFsa2luZ3xlbnwwfHx8fDE2OTgzODk2Mzd8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1586806974856-c55e8b9364e4?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxNXx8dGFsa2luZ3xlbnwwfHx8fDE2OTgzODk2Mzd8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption></figcaption></figure></div><p>I realized about a year ago, that I’d picked up a trait from my mother, wherein I would not be present in conversations. Instead, I’d be trying to find a way to agree/disagree with the person about a point, and then direct the conversation towards being about MY experience of said thing.</p><p>It’s been hard, like rewiring my brain, but I hope I’ve become more stoic and thoughtful. I was an obnoxious extrovert who had no privacy, shared everything online, and never thought before I spoke.</p><p>This post is actually just me talking about me, but I guess it’s good to share these things. </p><p>If you think you’re too ‘much’ sometimes, it could be because you are seeking approval from external things.&nbsp;</p><p>Whereas true happiness comes from finding the quiet calm, the ‘home’ within you. Once you find this place, you’ll see that your friend list is narrowed down. </p><p>You’ll seek only true friends who align with your values, and experiences that bring you purpose and fulfillment and you’ll find a calm quiet center within&nbsp;:)</p><p><strong>Something extra</strong><span>:</span></p><p>Well, it is important to share information with others, especially those who care about you. However, you should equally give importance to being mindful of how, when, and what you share. If someone shares something with you, Showing empathy or responding with a related story can be a great way to connect. </p><p>Anyhow, it can be offensive and create a negative impression if you overshare or try to one-up the other person’s story. </p><p>Thus, it’s important to strike a balance and be mindful of the impact your words may have on others.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Antidepressants or Tolkien (411 pts)]]></title>
            <link>https://antidepressantsortolkien.vercel.app/</link>
            <guid>38186190</guid>
            <pubDate>Wed, 08 Nov 2023 02:53:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://antidepressantsortolkien.vercel.app/">https://antidepressantsortolkien.vercel.app/</a>, See on <a href="https://news.ycombinator.com/item?id=38186190">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

            <div>
                <p><span>
                        <p>Antidepressants or Tolkien</p>
                    </span><br>
                    <span>
                        <p>Can you guess if the word is an antidepressants drug or a Tolkien character? </p>
                    </span><br>

                    <span>
                        Play
                    </span>
                </p>
            </div>

            <div>
                
                <p><span>

                    </span><br>
                    <span>

                    </span><br>

                    <span>
                    </span><br>

                    <span>
                        <img src="">
                    </span>
                </p>
            </div>


            

            <div>
                <p>
                    Tolkien
                </p>

                <p>
                    Antidepressants
                </p>


                <p>
                    Next
                </p>

                <p>
                    Game ended! See results...
                </p>

                

                <p>
                    New Game
                </p>

                <div>
                    <p>Idea based on <a href="https://twitter.com/checarina/status/977387234226855936" target="_blank">@checarina</a>'s tweet</p>
                    <p>Copyrights and trademarks for the books, films, and other promotional materials are held by their respective owners and their use is allowed under the <a href="http://en.wikipedia.org/wiki/fair_use" title="Fair use" target="_blank">fair use</a> clause of the <a href="http://en.wikipedia.org/wiki/Copyright" target="_blank" title="Copyright">Copyright Law</a>.
            </p></div>
        </div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Interactive examples for learning jq (166 pts)]]></title>
            <link>https://ishan.page/blog/2023-11-06-jq-by-example/</link>
            <guid>38186153</guid>
            <pubDate>Wed, 08 Nov 2023 02:49:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ishan.page/blog/2023-11-06-jq-by-example/">https://ishan.page/blog/2023-11-06-jq-by-example/</a>, See on <a href="https://news.ycombinator.com/item?id=38186153">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    
    
    <blockquote>
<p>Cover Photo by <a href="https://www.pexels.com/photo/airport-bank-board-business-534216/" target="_blank" rel="noopener">Pixabay</a></p>
</blockquote>

<p>Has this ever happened to you?</p>
<p>You’ve just received a massive JSON file that looks like it was designed to confuse you. Or maybe you entered a command, and you got so much JSON that it looks incomprehensible.</p>
<p>The data you need is buried inside, and you’re dreading the hours it’ll take to extract and clean it up.</p>
<p>I’ve been there. I’ve <code>grep</code>ped my way through JSON and written ad-hoc Python scripts to process it for me.</p>
<p>But things don’t have to be like this.</p>
<h2 id="introduction">Introduction</h2>
<p><code>jq</code> is one of the best-kept secrets in the data processing world.</p>
<p>Here are some scenarios where <code>jq</code> could swoop in to save your day (and saves mine regularly):</p>
<ol>
<li>
<p>Integrating with APIs in shell scripts often means handling JSON responses, requiring data extraction and manipulation.</p>
</li>
<li>
<p>Data from different sources may need to be converted to or from JSON format for compatibility.</p>
</li>
<li>
<p>Managing software configuration files in JSON format can be a regular task.</p>
</li>
<li>
<p>Extracting data from websites often results in dealing with JSON data that requires parsing and filtering.</p>
</li>
<li>
<p>Server logs and monitoring data often use JSON, necessitating parsing and analysis.</p>
</li>
<li>
<p>Infra as Code tools like Ansible and Terraform use JSON-like configurations, requiring management. JSON is a subset of YAML, so every valid JSON file is also a valid YAML file.</p>
</li>
</ol>
<p>All examples are <em>✨fully interactive✨</em>, so I encourage you to play around!<br>
In fact, I’ll be downright heartbroken if you don’t, because I put a lot of effort into it. You can edit both the input JSON data, and the jq program as well.</p>
<p>Let’s dive in! We’ll start off easy, and get slowly deeper into the weeds.</p>
<h2 id="basic-operations">Basic Operations</h2>
<h3 id="selecting-values">Selecting values</h3>
<p>Everything in <code>jq</code> is a filter. The dot <code>.</code> is used to select the current object or element, and we can put the property name after it to access a key from an object:</p>


<h3 id="filtering-arrays">Filtering Arrays</h3>
<p>The <code>.[]</code> notation is used to iterate over the elements of an array in a JSON document. It allows you to access each element of an array and perform operations on them.</p>
<p>The <code>select()</code> function is used to filter JSON data based on a specified condition or criteria. It is a powerful tool for extracting specific elements from a JSON document that meet certain conditions.</p>
<p>Similiar to shell scripting, <code>jq</code> works on a pipes-and-filters manner. We use the <code>|</code> to send the data from one filter to the next.
<jq-view name="example2"></jq-view></p>

<h3 id="mapping-arrays">Mapping Arrays</h3>
<p>We can use the <code>map</code> function to run any operation on every element of the array and return a new array containing the outputs of that operation:
<jq-view name="example3"></jq-view></p>

<h3 id="combining-filters">Combining Filters</h3>
<p>The pipe operator <code>|</code> can be used to chain as many filters or functions as we want:
<jq-view name="example4"></jq-view></p>

<h3 id="splitting-strings">Splitting Strings</h3>
<p>We can use the <code>split()</code> function to a split a string on a particular separator character. <br>
Note also the usage of <code>.[0]</code> to select the first index from the split array.</p>


<h3 id="conditional-logic">Conditional Logic</h3>
<p>We can use <code>if</code> to create expressions
<jq-view name="example6"></jq-view></p>

<h3 id="handling-null-values">Handling Null Values</h3>
<p>Null values can often mess up logic in our scripts, so we can filter them all out using <code>map</code> and <code>select</code>
<jq-view name="example17"></jq-view></p>

<h3 id="formatting-output">Formatting Output</h3>
<p>Sometimes we don’t want JSON output. We want it in a particular string format.<br>
Note the use of the <code>-r</code> flag, it makes the output raw. Without it, it would be displayed with quote marks around it.
<jq-view name="example10"></jq-view></p>

<h3 id="multiple-outputs">Multiple Outputs</h3>
<p>Curly braces create a new object, which we can use for multiple outputs:
<jq-view name="example5"></jq-view></p>

<h2 id="dealing-with-nested-items">Dealing with Nested Items</h2>
<p><img src="https://ishan.page/blog/2023-11-06-jq-by-example/russian_dolls.jpg" width="3500" height="2333" srcset="https://ishan.page/blog/2023-11-06-jq-by-example/russian_dolls_hu3d03a01dcc18bc5be0e67db3d8d209a6_252781_480x0_resize_q75_box.jpg 480w, https://ishan.page/blog/2023-11-06-jq-by-example/russian_dolls_hu3d03a01dcc18bc5be0e67db3d8d209a6_252781_1024x0_resize_q75_box.jpg 1024w" loading="lazy" data-flex-grow="150" data-flex-basis="360px"></p>
<blockquote>
<p><a href="https://www.pexels.com/photo/two-yellow-and-red-ceramic-owl-figurines-4966180/" target="_blank" rel="noopener">Photo by cottonbro studio</a></p>
</blockquote>
<p>JSON is very commonly used to store nested objects, and we often need to traverse or manipulate such structures. <code>jq</code> gives us all the tools we need to make it easy:</p>
<h3 id="recursive-descent">Recursive Descent</h3>
<p>We can use <code>..</code> to recursively descend through a tree of an object.</p>


<h3 id="filtering-nested-arrays">Filtering Nested Arrays</h3>


<h3 id="flattening-nested-json-objects">Flattening Nested JSON Objects</h3>
<p>Often, we just want all the key-values, and flattening the object may be the most convenient way to go:</p>


<h3 id="recursive-object-manipulation">Recursive Object Manipulation</h3>
<p>We can use the <code>recurse</code> as well, to traverse a tree.
<jq-view name="example18"></jq-view></p>

<h3 id="complex-object-transformation">Complex Object Transformation</h3>


<h3 id="walk-through-object-and-apply-a-transformation-conditionally">Walk through object and apply a transformation conditionally</h3>
<p>The <code>walk()</code> function provides a convenient way to traverse a nested object and apply some transformation to it.
<jq-view name="example24"></jq-view></p>

<h2 id="statistical-operations">Statistical Operations</h2>
<p><img src="https://ishan.page/blog/2023-11-06-jq-by-example/stats.jpg" width="6016" height="4016" srcset="https://ishan.page/blog/2023-11-06-jq-by-example/stats_hu3d03a01dcc18bc5be0e67db3d8d209a6_2144012_480x0_resize_q75_box.jpg 480w, https://ishan.page/blog/2023-11-06-jq-by-example/stats_hu3d03a01dcc18bc5be0e67db3d8d209a6_2144012_1024x0_resize_q75_box.jpg 1024w" loading="lazy" data-flex-grow="149" data-flex-basis="359px"></p>
<blockquote>
<p>Photo by <a href="https://www.pexels.com/photo/magnifying-glass-on-white-paper-with-statistical-data-5561913/" target="_blank" rel="noopener">Leeloo Thefirst</a></p>
</blockquote>
<p><code>jq</code> is incredibly handy for doing quick and dirty statistical analysis in the field. Here’s most of the common operations related to that</p>
<h3 id="sorting-arrays">Sorting Arrays</h3>
<p>Sorting an array is a basic operation that is useful for many things in statistics.
<jq-view name="example7"></jq-view></p>


<p>Extracting unique values from an array is another fairly basic operation that we need for many things.
<jq-view name="example12"></jq-view></p>

<h3 id="calculating-averages">Calculating Averages</h3>
<p>Calculating the mean or average of a dataset is a common statistical operation we may often need to do
<jq-view name="example20"></jq-view></p>

<h3 id="grouping-and-aggregating">Grouping and Aggregating</h3>
<p>We can group an array of objects by a particular key and get an aggregated value of the other keys fairly easily:
<jq-view name="example11"></jq-view></p>

<h3 id="filtering-after-aggregation">Filtering after Aggregation</h3>


<h3 id="custom-aggregation-with-reduce">Custom Aggregation with reduce</h3>
<p>We can also use <code>reduce</code> to perform a single-output aggregation from an array
<jq-view name="example23"></jq-view></p>

<h3 id="calculating-histogram-bins">Calculating Histogram Bins</h3>
<p>We may want to calculate a histogram from an array of data.
<jq-view name="example22"></jq-view></p>

<h2 id="other-common-operations">Other Common Operations</h2>
<p>These are some other common operations I frequently find myself doing every day, but I couldn’t think of a better way to categorize them.</p>

<p>We can combine multiple conditions in a <code>select</code> call. The <code>test()</code> function is used to check if the passed string contains one of the substrings or not.
<jq-view name="example15"></jq-view></p>

<h3 id="formatting-unix-timestamps">Formatting Unix Timestamps</h3>
<p>Various tools emit Unix Timestamps, and we can use the handy <code>strftime</code> function to format it so it’s easier to understand at a glace.
<jq-view name="example16"></jq-view></p>

<h3 id="enumerating-by-top-level-key-and-value">Enumerating by Top Level Key and Value</h3>


<h2 id="closing-thoughts">Closing Thoughts</h2>
<p>Whew! That’s been a long article 😅 If you’re still here, then I appreciate you staying till the very end.</p>
<p>I hope you’ve learned something new, and that you’ll be able to quickly identify use cases for <code>jq</code> in your current workflow and apply your learnings there.</p>
<h3 id="how-does-this-article-work">How Does This Article Work?</h3>
<ul>
<li>I have used web components to create a custom component <code>&lt;jq-view&gt;</code>.</li>
<li>There is some Javascript in this page which renders all the <code>&lt;jq-view&gt;</code>s when the page is loaded.</li>
<li>The WebAssembly build of <code>jq</code>, as well as all the code for calling out to it is provided by BioWasm.</li>
<li>I have used AlpineJs to make the examples interactive. When the button is clicked, it sends an event to a listener, which makes it run <code>jq</code> and then update the output.</li>
<li>Since I am not good at front-end, this was a substantial learning experience for me.</li>
</ul>
<h3 id="get-in-touch">Get In Touch</h3>
<p>If you have any suggestions on how this may be improved, errors that I might have made, or you just want to discuss any other topic, please feel free to <a href="mailto:ishan.dassharma1@gmail.com">email me</a>. I always love to hear from you.</p>

<p><a href="https://jqlang.github.io/jq/manual/" target="_blank" rel="noopener">JQ Manual</a></p>

</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bing Chat is so hungry for GPUs, Microsoft will rent them from Oracle (127 pts)]]></title>
            <link>https://www.theregister.com/2023/11/07/bing_gpu_oracle/</link>
            <guid>38184925</guid>
            <pubDate>Wed, 08 Nov 2023 00:01:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2023/11/07/bing_gpu_oracle/">https://www.theregister.com/2023/11/07/bing_gpu_oracle/</a>, See on <a href="https://news.ycombinator.com/item?id=38184925">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>Demand for Microsoft's AI services is apparently so great – or Redmond's resources so tight – that the software giant plans to offload some of the machine-learning models used by Bing Search to Oracle's GPU supercluster as part of a multi-year agreement announced Tuesday.</p>
<p>"Our collaboration with Oracle and use of Oracle Cloud infrastructure along with our Microsoft Azure AI infrastructure, will expand access to customers and improve the speed of many of our search results," Divya Kumar, who heads up Microsoft's Search and AI marketing team, explained in a <a href="https://www.oracle.com/apac/news/announcement/oracle-cloud-infrastructure-utilized-by-microsoft-for-bing-conversational-search-2023-11-07/" rel="nofollow">statement</a>.</p>
<p>The partnership essentially boils down to: Microsoft needs more compute resources to keep up with the alleged "explosive growth" of its AI services, and Oracle just happens to have tens of thousands of Nvidia A100s and H100 GPUs available for rent. Far be it from us to suggest the Larry-Ellison-founded database giant doesn't have enough cloud customers to consume its stocks of silicon.</p>

    

<p>Microsoft was among the first to integrate a generative AI chatbot into its search engine with the <a href="https://www.theregister.com/2023/02/07/microsoft_bing_ai/">launch</a> of Bing Chat back in February. You all know the drill by now: you can feed prompts, requests, or queries into Bing Chat, and it will try to look up information, write bad poetry, generate pictures and other content, and so on.</p>

        


        

<p>The large language models that underpin the service not only require massive clusters of GPUs to train, but for inferencing – the process of putting a model to work – to run at scale. It's Oracle's stack of GPUs that will help with this inference work.</p>
<p>The two cloud providers' latest collaboration takes advantage of the Oracle Interconnect for Microsoft Azure, which allows services running in Azure to interact with resources in Oracle Cloud Infrastructure (OCI). The two super-corps have <a href="https://www.theregister.com/2022/07/20/oracle_microsoft_multi_cloud_database/">previously</a> used the service to allow customers to connect workloads running in Azure back to OCI databases.</p>
<ul>

<li><a href="https://www.theregister.com/2023/11/05/biden_ai_reporting_thresholds/">Developing AI models or giant GPU clusters? Uncle Sam would like a word</a></li>

<li><a href="https://www.theregister.com/2023/11/03/microsoft_365_copilot/">Microsoft 365 Copilot 'generally available' – if you can afford 300 seats</a></li>

<li><a href="https://www.theregister.com/2023/10/19/redis_disk_support/">In-memory database Redis wants to dabble in disk</a></li>

<li><a href="https://www.theregister.com/2023/11/06/openai_gpt4_turbo_copyright_shield/">OpenAI hits the GPT-4 Turbo button plus promises copyright shield for fans</a></li>
</ul>
<p>In this case, Microsoft is using the system alongside its Azure Kubernetes Service to orchestrate Oracle's GPU nodes to keep up with what's said to be demand for Bing's AI features.</p>
<p>According to StatCounter, for October 2023, Bing had a <a target="_blank" rel="nofollow" href="https://gs.statcounter.com/search-engine-market-share">3.1 percent</a> global web search market share for all platforms – that's compared to Google's 91.6 percent, but up from 3 percent the month before. On desktop, Bing climbed to 9.1 percent, and 4.6 percent for tablets.</p>

        

<p>Maybe StatCounter is wrong; maybe Microsoft's chatty search engine isn't as staggeringly popular as we're led to believe. Maybe Microsoft just wants to make Bing look like it's in high demand; maybe Redmond really does need the extra compute.</p>
<p>Oracle claims its cloud super-clusters, which presumably Bing will use, can each scale to 32,768 Nvidia A100s or 16,384 H100 GPUs using a ultra-low latency Remote Direct Memory Access (RDMA) network. This is supported by petabytes of high-performance cluster file storage designed to support highly parallel applications.</p>
<p>Microsoft hasn't said just how many of Oracle's GPU nodes it needs for its AI services and apps, and won't say. A spokesperson told us: “Those aren’t details we are sharing as part of this announcement.” We've asked Oracle too for more information and we'll let you know if we hear anything back.</p>

        

<p>This isn't the first time the frenemies have leaned on each other for help. Back in September Oracle <a href="https://www.theregister.com/2023/09/15/oracle_database_at_azure/">announced</a> it would colocate its database systems in Microsoft Azure datacenters. In that case, the collaboration was intended to reduce the latency associated with connecting Oracle databases running in OCI to workloads in Azure. ®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nature retracts controversial superconductivity paper by embattled physicist (177 pts)]]></title>
            <link>https://www.nature.com/articles/d41586-023-03398-4</link>
            <guid>38184750</guid>
            <pubDate>Tue, 07 Nov 2023 23:44:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/d41586-023-03398-4">https://www.nature.com/articles/d41586-023-03398-4</a>, See on <a href="https://news.ycombinator.com/item?id=38184750">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-023-03398-4/d41586-023-03398-4_26239314.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-023-03398-4/d41586-023-03398-4_26239314.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="Ranga Dias working at a desktop computer." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-023-03398-4/d41586-023-03398-4_26239314.jpg">
  <figcaption>
   <p><span>Physicist Ranga Dias is under investigation by his institution, the University of Rochester in New York.</span><span>Credit: Lauren Petracca/New York Times/Redux/eyevine</span></p>
  </figcaption>
 </picture>
</figure><p><i>Nature</i> has retracted a controversial paper<sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup> claiming the discovery of a superconductor — a material that carries electrical currents with zero resistance — capable of operating at room temperature and relatively low pressure.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-023-02733-z" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-023-03398-4/d41586-023-03398-4_26230840.jpg"><p>Why a blockbuster superconductivity claim met a wall of scepticism</p></a>
 </article><p>The <a href="https://www.nature.com/articles/s41586-023-06774-2" data-track="click" data-label="https://www.nature.com/articles/s41586-023-06774-2" data-track-category="body text link">text of the retraction notice</a> states that it was requested by eight co-authors. “They have expressed the view as researchers who contributed to the work that the published paper does not accurately reflect the provenance of the investigated materials, the experimental measurements undertaken and the data-processing protocols applied,” it says, adding that these co-authors “have concluded that these issues undermine the integrity of the published paper”. (The <i>Nature</i> news team is independent from its journals team.)</p><p>It is the third high-profile retraction of a paper by the two lead authors, physicists Ranga Dias at the University of Rochester in New York and Ashkan Salamat at the University of Nevada, Las Vegas (UNLV). <i>Nature</i> withdrew a separate paper last year<sup><a href="#ref-CR2" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">2</a></sup> and <i>Physical Review Letters</i> retracted one this August<sup><a href="#ref-CR3" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">3</a></sup>. It spells more trouble in particular for Dias, <a href="https://www.science.org/content/article/plagiarism-allegations-pursue-physicist-behind-stunning-superconductivity-claims" data-track="click" data-label="https://www.science.org/content/article/plagiarism-allegations-pursue-physicist-behind-stunning-superconductivity-claims" data-track-category="body text link">whom some researchers allege plagiarized portions of his PhD thesis</a>. Dias has objected to the first two retractions and not responded regarding the latest. Salamat approved the two this year.</p><p>“It is at this point hardly surprising that the team of Dias and Salamat has a third high-profile paper being retracted,” says Paul Canfield, a physicist at Iowa State University in Ames and at Ames National Laboratory. Many physicists had seen the <i>Nature</i> retraction as inevitable after the other two — and especially since <a href="https://www.wsj.com/science/room-temperature-superconductor-retract-journal-nature-e554536a" data-track="click" data-label="https://www.wsj.com/science/room-temperature-superconductor-retract-journal-nature-e554536a" data-track-category="body text link"><i>The Wall Street Journal</i></a> and <a href="https://www.science.org/content/article/another-retraction-looms-embattled-physicist-behind-blockbuster-superconductivity" data-track="click" data-label="https://www.science.org/content/article/another-retraction-looms-embattled-physicist-behind-blockbuster-superconductivity" data-track-category="body text link"><i>Science</i></a> reported in September that 8 of the 11 authors of the paper — including Salamat — had requested it in a letter to the journal.</p><p>Dias and Salamat did not respond to a request for comment by <i>Nature</i>’s news team. The retraction states that he and two other co-authors — Nugzari Khalvashi-Sutter and Sasanka Munasinghe, both at Rochester — "have not stated whether they agree or disagree with this retraction".</p><h2>Early scepticism</h2><p>This year’s report by Dias and Salamat is the second significant claim of superconductivity to crash and burn in 2023. In July, a separate team at a start-up company in Seoul described<sup><a href="#ref-CR4" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">4</a></sup><sup>,</sup><sup><a href="#ref-CR5" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">5</a></sup> a crystalline purple material dubbed LK-99 — made of copper, lead, phosphorus and oxygen — that they said showed superconductivity at normal pressures and at temperatures up to at least 127 °C (400 kelvin). There was much online excitement and many attempts to reproduce the results, but researchers quickly reached a consensus that the material <a href="https://www.nature.com/articles/d41586-023-02585-7" data-track="click" data-label="https://www.nature.com/articles/d41586-023-02585-7" data-track-category="body text link">was not a superconductor at all</a>.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-023-02585-7" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-023-03398-4/d41586-023-03398-4_25924888.jpg"><p>LK-99 isn’t a superconductor — how science sleuths solved the mystery</p></a>
 </article><p>Superconductors are important in many applications, from magnetic resonance imaging machines to particle colliders, but their use has been limited by the need to keep them at extremely low temperatures. For decades, researchers have been developing new materials with the dream of finding one that exhibits superconductivity without any refrigeration.</p><p>Specialists in the field have been sceptical since this year’s Dias and Salamat paper was published, says Lilia Boeri, a physicist at the Sapienza University of Rome. This, she says, is in part because of controversies swirling around the team and in part because the latest paper was not written to what she considers a high standard.</p><p>“Virtually every serious condensed-matter physicist I know saw right away that there were serious problems with the work,” says Peter Armitage, an experimental physicist at Johns Hopkins University in Baltimore, Maryland. In particular, members of the community took issue with measurements of the material’s electrical resistance, saying it was not clear whether the property truly dropped to zero, or whether Dias and Salamat had subtracted a background signal from a key plot of resistance to create the appearance that it did. Critics say that it should not be necessary to remove background from this type of measurement. In today's text, the journal stated, "An investigation by the journal and post-publication review have concluded that these concerns are credible, substantial and remain unresolved."</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-022-03066-z" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-023-03398-4/d41586-023-03398-4_23949000.jpg"><p>Stunning room-temperature-superconductor claim is retracted</p></a>
 </article><p>Armitage adds that the publication of the paper also raises questions about the editorial review process at <i>Nature</i>, and why reviewers didn’t catch the issues.</p><p>“The highly qualified expert reviewers we selected raised a number of questions about the original submission, which were largely resolved in later revisions,“ says Karl Ziemelis, chief physical sciences editor at <i>Nature</i>. “What the peer-review process cannot detect is whether the paper as written accurately reflects the research as it was undertaken.”</p><p>“Decisions about what to accept for publication are not always easy to make,” Ziemelis continues. “And there may be conflicts, but we strive to take an unbiased position and to ensure the interests of the community always drive our deliberations.”</p><h2>Audible clamour</h2><p><i>Nature</i> published the now-retracted paper on 8 March. That week, Dias himself presented the results to a standing-room-only audience at a meeting of the American Physical Society in Las Vegas. Over the audible clamour of the crowd assembled outside the room’s doors — where conference staff limited entry to avoid violating fire regulations — Dias briefly described a compound made of hydrogen, lutetium and small amounts of nitrogen that was a superconductor at temperatures up to 21 °C (294 kelvin) when kept at a pressure of around 1 gigapascal (10,000 times atmospheric pressure).</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-023-02401-2" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-023-03398-4/d41586-023-03398-4_25984902.jpg"><p>‘A very disturbing picture’: another retraction imminent for controversial physicist</p></a>
 </article><p>Many teams had already created and experimented with similar hydrogen-rich materials, called hydrides, after a milestone discovery in 2015. A group led by physicist Mikhail Eremets at the Max Planck Institute for Chemistry in Mainz, Germany, reported<sup><a href="#ref-CR6" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">6</a></sup> superconductivity in a hydrogen–sulfur compound at −70 °C (203 kelvin); at the time, this was a record-high operating temperature for a superconductor. But Eremets’s material required a much higher pressure of 145 gigapascals (1.4 million times atmospheric pressure) — comparable to the crushing conditions at the centre of Earth.</p><p>Since then, researchers have made hydride superconductors that push closer and closer to operating at room temperature, but all of them work only under extreme pressures. When Dias and Salamat published their paper in <i>Nature</i> in March, they <a href="https://www.nature.com/articles/d41586-023-02733-z#ref-CR10" data-track="click" data-label="https://www.nature.com/articles/d41586-023-02733-z#ref-CR10" data-track-category="body text link">seemed to have made a significant step</a> towards a material that could find practical applications.</p><p>But some specialists were already wary because of <a href="https://www.nature.com/articles/d41586-022-03066-z" data-track="click" data-label="https://www.nature.com/articles/d41586-022-03066-z" data-track-category="body text link">the first <i>Nature</i> retraction</a>. And some say they immediately found the fresh claims to be improbable. For instance, the material described in the paper was supposed to have around three hydrogen atoms for every lutetium atom. But if so, the lutetium would tend to donate an electron to each hydrogen, resulting in a kind of salt, says Artem Oganov, a materials scientist at the Skolkovo Institute of Science and Technology in Moscow. “You get either an insulator or an extremely poor metal,” he says — not a superconductor.</p><p>One lab says it has partially reproduced Dias and Salamat’s results using a sample provided by the Rochester team<sup><a href="#ref-CR7" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">7</a></sup>. But many others, which tried creating their own samples and running tests, could not. And in the meantime, other causes for concern have arisen. An investigation launched by <i>Physical Review Letters</i> before it retracted its paper by Dias and Salamat found “apparent data fabrication”, <a href="https://www.nature.com/articles/d41586-023-02401-2" data-track="click" data-label="https://www.nature.com/articles/d41586-023-02401-2" data-track-category="body text link">as <i>Nature</i>’s news team reported in July</a>. And an investigation launched by <i>Nature</i>’s journals team after it received an anonymous critique of data in this year’s paper found that “the credibility of the published results are in question”, according to <a href="https://www.science.org/content/article/another-retraction-looms-embattled-physicist-behind-blockbuster-superconductivity" data-track="click" data-label="https://www.science.org/content/article/another-retraction-looms-embattled-physicist-behind-blockbuster-superconductivity" data-track-category="body text link">September’s news story in <i>Science</i></a>.</p><h2>Credibility concerns</h2><p>Armitage does not think that Dias and Salamat will be able to keep doing research, pointing to the investigation findings and allegations of plagiarism in Dias’s PhD thesis. The University of Rochester has confirmed to <i>Nature</i> that it has launched an investigation into the integrity of Dias’s work, which is being conducted now by external experts. The university’s spokesperson did not answer questions about whether the institution has yet disciplined Dias. UNLV did not answer <i>Nature</i>’s queries about whether Salamat is being investigated, saying that “UNLV does not publicly discuss personnel matters”, but that it “is committed to maintaining the highest standards for research integrity campus wide”.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-023-02681-8" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-023-03398-4/d41586-023-03398-4_26007520.jpg"><p>How would room-temperature superconductors change science?</p></a>
 </article><p>Canfield says that the Dias–Salamat collaboration has spread a “foul vapour” over the field, which “is scaring young researchers and funding agencies away”.</p><p>“I have some colleagues who simply are afraid that this case of Dias puts a shadow of doubt on the credibility of our field in general,” Eremets says.</p><p>Ho-Kwang Mao, director of the Center for High Pressure Science and Technology Advanced Research in Beijing, is more sanguine. “I do not think it will affect the funding for superconductivity research other than more careful reviews, which is not necessarily bad,” he says.</p><p>Hai-Hu Wen, director of the Center for Superconducting Physics and Materials at Nanjing University in China, agrees. “Actually, it seems more easy to get funding for the research of superconductivity since some government officials seem to be influenced by the expectation of a room-temperature superconductor,” he says.</p><p>But Boeri says she has heard researchers complain that the controversies — the allegations of PhD thesis plagiarism and the findings of apparent data fabrication — have made it harder to recruit students to work on superconductors. “We face a serious communication problem, to make people understand that the field is healthy — that although there may be some bad apples, the community’s standards are much higher,” she says.</p><p>“Serious people continue to do amazing and interesting work,” Armitage says. “Sure, they can be disheartened by this nonsense, but it won’t stop the science.”</p>
                </div><p>Additional reporting by Lauren Wolf.</p><div id="references" aria-labelledby="Bib1"><h2 id="Bib1">References</h2><div data-container-section="references" id="Bib1-content"><ol data-track-component="outbound reference"><li data-counter="1."><p id="ref-CR1">Dasenbrock-Gammon, N. <i>et al.</i> <i>Nature</i> <b>615</b>, 244–250 (2023); retraction https://doi.org/10.1038/s41586-023-06774-2 (2023).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/s41586-023-05742-0" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41586-023-05742-0" aria-label="Article reference 1" data-doi="10.1038/s41586-023-05742-0">Article</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 1" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Nature&amp;doi=10.1038%2Fs41586-023-05742-0&amp;volume=615&amp;pages=244-250&amp;publication_year=2023&amp;author=Dasenbrock-Gammon%2CN.">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="2."><p id="ref-CR2">Snider, E. <i>et al.</i> <i>Nature</i> <b>586</b>, 373–377 (2022); retraction <b>610</b>, 804 (2022).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/s41586-020-2801-z" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41586-020-2801-z" aria-label="Article reference 2" data-doi="10.1038/s41586-020-2801-z">Article</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 2" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Nature&amp;doi=10.1038%2Fs41586-020-2801-z&amp;volume=586&amp;pages=373-377&amp;publication_year=2022&amp;author=Snider%2CE.">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="3."><p id="ref-CR3">Durkee, D. <i>et al. Phys. Rev. Lett.</i> <b>127</b>, 016401 (2021); retraction <b>131</b>, 079902 (2023).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1103/PhysRevLett.127.016401" data-track-action="article reference" href="https://doi.org/10.1103%2FPhysRevLett.127.016401" aria-label="Article reference 3" data-doi="10.1103/PhysRevLett.127.016401">Article</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 3" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Phys.%20Rev.%20Lett&amp;doi=10.1103%2FPhysRevLett.127.016401&amp;volume=127&amp;publication_year=2021&amp;author=Durkee%2CD.">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="4."><p id="ref-CR4">Lee, S. <i>et al.</i> Preprint at <a href="https://arxiv.org/abs/2307.12037" data-track="click" data-track-action="external reference" data-track-label="https://arxiv.org/abs/2307.12037">https://arxiv.org/abs/2307.12037</a> (2023).</p></li><li data-counter="5."><p id="ref-CR5">Lee, S., Kim, J.-H. &amp; Kwon, Y.-W. Preprint at <a href="https://arxiv.org/abs/2307.12008" data-track="click" data-track-action="external reference" data-track-label="https://arxiv.org/abs/2307.12008">https://arxiv.org/abs/2307.12008</a> (2023).</p></li><li data-counter="6."><p id="ref-CR6">Drozdov, A. P., Eremets, M. I., Troyan, I. A., Ksenofontov, V. &amp; Shylin, S. <i>I. Nature</i> <b>525</b>, 73–76 (2015).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nature14964" data-track-action="article reference" href="https://doi.org/10.1038%2Fnature14964" aria-label="Article reference 6" data-doi="10.1038/nature14964">Article</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 6" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=I.%20Nature&amp;doi=10.1038%2Fnature14964&amp;volume=525&amp;pages=73-76&amp;publication_year=2015&amp;author=Drozdov%2CA.%20P.&amp;author=Eremets%2CM.%20I.&amp;author=Troyan%2CI.%20A.&amp;author=Ksenofontov%2CV.&amp;author=Shylin%2CS.">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="7."><p id="ref-CR7">Salke, N. P., Mark, A. C., Ahart, M. &amp; Hemley, R. J. Preprint at <a href="https://arxiv.org/abs/2306.06301" data-track="click" data-track-action="external reference" data-track-label="https://arxiv.org/abs/2306.06301">https://arxiv.org/abs/2306.06301</a> (2023).</p></li></ol><p><a data-track="click" data-track-action="download citation references" data-track-label="link" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/d41586-023-03398-4?format=refman&amp;flavour=references">Download references</a></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tumble Forth (132 pts)]]></title>
            <link>http://tumbleforth.hardcoded.net/</link>
            <guid>38184539</guid>
            <pubDate>Tue, 07 Nov 2023 23:23:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://tumbleforth.hardcoded.net/">http://tumbleforth.hardcoded.net/</a>, See on <a href="https://news.ycombinator.com/item?id=38184539">Hacker News</a></p>
<div id="readability-page-1" class="page">


<p>
Hello, my name is Virgil Dupras, author of <a href="http://collapseos.org/">Collapse OS</a> and <a href="http://duskos.org/">Dusk OS</a> and I'm starting a series of articles that
aims to hand-hold my former self, a regular web developer, into the rabbit hole
leading to the wonderful world of low level programming. Hopefully, I can
hand-hold you too.
</p>

<p>
The general goal is to broaden your perspectives on the subject of computing. I
intend do to that through story arcs leading, step by step, to some nice and
shiny objective. I also intend to work into a gimmick where in each episode, I
get to tell one corny joke.
</p>

<p>
The target reader is a person who knows their way around programming, but is
inexperienced in the area of low level programming. If you're the target reader
but find some parts of this content difficult to understand, this is not
intentional. In this case, or if you have any question or comment, reach out to
me at hsoft@hardcoded.net.
</p>

<h2>Story arcs</h2>

<h3>Buckle up, Dorothy</h3>

<p>
In my <a href="http://tumbleforth.hardcoded.net/01-duskcc/01-buckleup.html">“pilot” story arc</a>, we peek in
disgust in the abyss of modern software complexity and escape this dystopia by
tumbling down the rabbit hole of low level development.
</p>
<p>
Starting from bare metal on the PC platform, we build a Forth from scratch, then
switch to <a href="http://duskos.org/">Dusk OS</a> and then build a partial C
compiler (just enough to compile our example code), again from scratch.
</p>

<p>Table of Contents</p>
<ol>
    <li><a href="http://tumbleforth.hardcoded.net/01-duskcc/01-buckleup.html">Buckle up, Dorothy</a></li>
    <li><a href="http://tumbleforth.hardcoded.net/01-duskcc/02-baremetal.html">Liberation through bare metal</a></li>
    <li><a href="http://tumbleforth.hardcoded.net/01-duskcc/03-onesector.html">One sector to rule them all</a></li>
    <li><a href="http://tumbleforth.hardcoded.net/01-duskcc/04-wordsshell.html">Words in the shell</a></li>
    <li><a href="http://tumbleforth.hardcoded.net/01-duskcc/05-dolookup.html">Do Look Up</a></li>
    <li><a href="http://tumbleforth.hardcoded.net/01-duskcc/06-taletwostacks.html">A tale of two stacks</a></li>
    <li><a href="http://tumbleforth.hardcoded.net/01-duskcc/07-babywalk.html">Baby's first steps</a></li>
    <li><a href="http://tumbleforth.hardcoded.net/01-duskcc/08-immediate.html">The Unbearable Immediateness of Compiling</a></li>
    <li><a href="http://tumbleforth.hardcoded.net/01-duskcc/09-dusktillc.html">From Dusk Till C</a></li>
    <li><a href="http://tumbleforth.hardcoded.net/01-duskcc/10-beast.html">Feeding the beast</a></li>
    <li><a href="http://tumbleforth.hardcoded.net/01-duskcc/11-eye.html">In the Eye of the Compiler</a></li>
</ol>



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[WasmFX: Effect Handlers for WebAssembly (135 pts)]]></title>
            <link>https://wasmfx.dev/</link>
            <guid>38184339</guid>
            <pubDate>Tue, 07 Nov 2023 23:04:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wasmfx.dev/">https://wasmfx.dev/</a>, See on <a href="https://news.ycombinator.com/item?id=38184339">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        
        <div>
          <div>
            <h3>Efficient and fast</h3>
            <p>WasmFX provides a general and flexible instruction set
            for fast stack switching, paving the way for efficient
            implementations of various non-local control flow
            abstractions, which would otherwise have to be implemented
            using whole-program transformations such as continuation
            passing style. WasmFX is carefully designed to enable Wasm
            to maintain backwards-compatible performance for legacy
            programs.</p>
          </div>
          <div>
            <h3>Modular and composable</h3>
            <p>WasmFX provides a modular and composable basis for
            implementing non-local control flow abstractions via
            delimited control, as the design draws heavily
            on <a href="https://homepages.inf.ed.ac.uk/gdp/publications/Effect_Handlers.pdf">Plotkin
            and Pretnar's handlers for algebraic effects</a>. As such
            the design is based on strong and well-understood
            theoretical foundations; at the same time it also
            incorporates the practical lessons learnt during the
            previous 30 years of research on delimited control.</p>
          </div>
        </div>
        <div>
          <div>
            <h3>Debuggable and profilable </h3>
            <p>WasmFX is designed to preserve the debugging and
            profiling experience of WebAssembly. The structure of
            WasmFX stacks offers excellent compatibility with popular
            formats such as <a href="https://dwarfstd.org/">DWARF
            stack unwind tables</a>, enabling WebAssembly
            implementations to maintain compatibility with standard
            debugging and profiling tools.</p>
          </div>
          <div>
            <h3>Minimal and compatible</h3>
            <p>WasmFX is designed as a minimal and compatible
            extension to WebAssembly for structured non-local control
            flow. The extension is minimal in the sense that it
            leverages WebAssembly's existing instruction set and type
            system. It extends the instruction set with six new
            instructions and the type system with a new reference
            type.</p>
          </div>
        </div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gleam: a type safe language on the Erlang VM (306 pts)]]></title>
            <link>https://gleam.run/</link>
            <guid>38183454</guid>
            <pubDate>Tue, 07 Nov 2023 21:53:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gleam.run/">https://gleam.run/</a>, See on <a href="https://news.ycombinator.com/item?id=38183454">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main">
  <section>
    <p>
        The power of a type system, the expressiveness of functional
        programming, and the reliability of the highly concurrent, fault
        tolerant Erlang runtime, with a familiar and modern syntax.
      </p>
    <pre><code><span>import</span> <span>gleam/io</span>

<span>pub fn</span> <span>main</span>() {
  <span>io.</span><span>println</span>(<span>"hello, friend!"</span>)
}</code></pre>
  </section>

  

  <section>
    <div>
      <h2>Reliable and scalable</h2>
      <p>
        Running on the battle-tested Erlang virtual machine that powers
        planet-scale systems such as WhatsApp and Ericsson, Gleam is ready for
        workloads of any size.
      </p>
      <p>
        Thanks to a multi-core actor based concurrency system that can run
        millions of concurrent tasks, fast immutable data structures, and a
        concurrent garbage collector that never stops the world, your service can
        scale and stay lightning fast with ease.
      </p>
    </div>
    <pre><code><span>fn</span> <span>spawn_task</span>(i) {
  <span>task.</span><span>async</span>(<span>fn</span>() {
    <span>let</span> n = <span>int.</span><span>to_string</span>(i)
    <span>io.</span><span>println</span>(<span>"Hello from "</span> <span>&lt;&gt;</span> n)
  })
}

<span>pub fn</span> <span>main</span>() {
  <span>// Run loads of threads, no problem</span>
  <span>list.</span><span>range</span>(<span>0</span>, <span>200_000</span>)
  <span>|&gt;</span> <span>list.</span><span>map</span>(<span>spawn_task</span>)
  <span>|&gt;</span> <span>list.</span><span>each</span>(<span>task.</span><span>await_forever</span>)
}
</code></pre>
  </section>

  <section>
    <div>
      <h2>Ready when you are</h2>
      <p>
        Gleam comes with compiler, build tool, formatter, editor integrations,
        and package manager all built in, so creating a Gleam project is just
        running <code>gleam new</code>.
      </p>
      <p>
        As part of the wider BEAM ecosystem, Gleam programs can use thousands
        of published packages, whether they are written in Gleam, Erlang, or
        Elixir.
      </p>
    </div>
    <pre><code><span>➜ (main)</span> gleam add gleam_json
<span>  Resolving</span> versions
<span>Downloading</span> packages
<span> Downloaded</span> 2 packages in 0.01s
<span>      Added</span> gleam_json v0.5.0
<span>➜ (main)</span> gleam test
<span> Compiling</span> thoas
<span> Compiling</span> gleam_json
<span> Compiling</span> app
<span>  Compiled</span> in 1.67s
<span>   Running</span> app_test.main
<span>.
1 tests, 0 failures</span>
</code></pre>
  </section>

  <section>
    <div>
      <h2>Here to help</h2>
      <p>
        No null values, no exceptions, clear error messages, and a practical
        type system. Whether you're writing new code or maintaining old code,
        Gleam is designed to make your job as fun and stress-free as possible.
      </p>
    </div>
    <pre><code><span>error:</span> Unknown record field

  ┌─ ./src/app.gleam:8:16
  │
8 │ user.alias
  │ <span>    ^^^^^^ Did you mean `name`?</span>

The value being accessed has this type:
    User

It has these fields:
    .name
</code></pre>
  </section>

  <section>
    <div>
      <h2>Multilingual</h2>
      <p>
        Gleam makes it easy to use code written in other BEAM languages such
        as Erlang and Elixir, so there's a rich ecosystem of thousands of open
        source libraries for Gleam users to make use of.
      </p>
      <p>
        Gleam can additionally compile to JavaScript, enabling you to use your
        code in the browser, or anywhere else JavaScript can run. It also
        generates TypeScript definitions, so you can interact with your Gleam
        code confidently, even from the outside.
      </p>
    </div>
    <pre><code>
      

<span>@external</span>(erlang, <span>"Elixir.HPAX", "new"</span>)
<span>pub fn</span> <span>new</span>(size: <span>Int</span>) -&gt; <span>Table</span>
  


<span>pub fn</span> <span>register_event_handler</span>() {
  <span>let</span> el = <span>document.</span><span>query_selector</span>(<span>"a"</span>)
  <span>element.</span><span>add_event_listener</span>(el, <span>fn</span>() {
    <span>io.</span><span>println</span>(<span>"Clicked!"</span>)
  })
}</code></pre>
  </section>

  <section>
    <!-- TODO: add triangles here -->
    <div>
      <h2>Friendly 💜</h2>
      <p>
        As a community, we want to be friendly too. People from around the
        world, of all backgrounds, genders, and experience levels are welcome
        and respected equally. See our community code of conduct for more.
      </p>
      <p>
        Black lives matter. Trans rights are human rights. No nazi bullsh*t.
      </p>
    </div>
    <!-- TODO: add Lucy here -->
    <img src="https://gleam.run/images/waves.svg" alt="a soft wavey boundary between two sections of the website">
  </section>


  

  <div>
      <h2>You're still here?</h2>
      <p>
        Well, that's all this page has to say. Maybe you should go read the
        language introduction!
      </p>
      <p><a href="https://gleam.run/getting-started">Let's go!</a></p><hr>

      <h3>Wanna keep in touch?</h3>
      <p>
        Subscribe to the Gleam newsletter
      </p>
      
      <p>
        We send emails at most a few times a year, and we'll never share your
        email with anyone else.
      </p>
      <p>
        This site is protected by reCAPTCHA and the Google
        <a href="https://policies.google.com/privacy">Privacy Policy</a> and
        <a href="https://policies.google.com/terms">Terms of Service</a> apply.
      </p>
    </div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Asahi Linux folks are doing us a solid with WPA3 fixes (158 pts)]]></title>
            <link>https://rachelbythebay.com/w/2023/11/07/wpa3/</link>
            <guid>38183394</guid>
            <pubDate>Tue, 07 Nov 2023 21:49:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rachelbythebay.com/w/2023/11/07/wpa3/">https://rachelbythebay.com/w/2023/11/07/wpa3/</a>, See on <a href="https://news.ycombinator.com/item?id=38183394">Hacker News</a></p>
Couldn't get https://rachelbythebay.com/w/2023/11/07/wpa3/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[What the QWAC? an EV Certificate all over again (152 pts)]]></title>
            <link>https://scotthelme.co.uk/what-the-qwac/</link>
            <guid>38183259</guid>
            <pubDate>Tue, 07 Nov 2023 21:40:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://scotthelme.co.uk/what-the-qwac/">https://scotthelme.co.uk/what-the-qwac/</a>, See on <a href="https://news.ycombinator.com/item?id=38183259">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="site-main">
<article>

<section>
<div>
<p>Almost 2 years on from the last time I wrote about QWACs, I'm sadly not here to tell you that things have gone well since then. In fact, I'm actually here to tell you that things are not going well at all...</p><h4 id="qwac">QWAC</h4><p>Back in Jan 2022, I wrote a blog post that went into details on what a QWAC, or Qualified Website Authentication Certificate, actually is: <a href="https://scotthelme.co.uk/looks-like-a-duck-swims-like-a-duck-qwacs-like-a-duck-probably-an-ev-certifiacate/?ref=scotthelme.co.uk" rel="noreferrer">If it looks like a duck, swims like a duck, and QWACs like a duck, then it's probably an EV Certificate</a></p><p>TLDR; It's an EV Certificate all over again 🤷‍♂️</p><figure><img src="https://scotthelme.co.uk/content/images/2023/11/facepalm-crowd.gif" alt="" loading="lazy" width="498" height="274"></figure><p>In all seriousness though, that's actually quite a long and detailed post about the shortcomings of a QWAC and why they're just a terrible, terrible idea. They're only being pushed by organisations that would make $$$ selling them (funny that) and it's like the entire mess of EV has been conveniently forgotten. I'm not here to re-tread the same ground, though, I'm here to talk about something even more concerning. You might think "ok, so we have a new type of pointless certificate available", and if that were the case, I wouldn't be writing about it again and we could all just not buy them. The problem is that there's something bigger lurking that really concerns me.</p><h4 id="my-concerns">My Concerns</h4><p>This isn't all just talk for me, having dedicated a huge portion of my life to working in this industry and being so passionate about it, this worries me. It worries me enough that I've signed multiple open letters speaking out against this, with the most recent <a href="https://last-chance-for-eidas.org/?ref=scotthelme.co.uk" rel="noreferrer">just a few days ago</a>, and I've even travelled to Brussels to sit alongside Member of European Parliament Karen Melchior, and other industry representatives, to <a href="https://securityriskahead.eu/wp-content/uploads/2022/11/Press-release-event-on-cybersecurity-risks-within-the-eIDAS-revision.pdf?ref=scotthelme.co.uk" rel="noreferrer">speak against this</a>. I have absolutely no skin in this game, one way or another, but I've seen something that I believe is just fundamentally wrong, and I feel compelled to speak out against it.</p><figure><img src="https://scotthelme.co.uk/content/images/2023/11/image-17.png" alt="" loading="lazy" width="643" height="382" srcset="https://scotthelme.co.uk/content/images/size/w600/2023/11/image-17.png 600w, https://scotthelme.co.uk/content/images/2023/11/image-17.png 643w"></figure><h4 id="eidas-article-45latest-recitals">eIDAS Article 45 - latest recitals</h4><p>As we come towards the end of the legal process, we're closing in on the final revisions and final draft of some new regulation coming to the EU called <a href="https://digital-strategy.ec.europa.eu/en/policies/eidas-regulation?ref=scotthelme.co.uk" rel="noreferrer">eIDAS</a>. This new regulation contains many things, and it's only one small part of it that I fundamentally oppose, but it will have Global impact, far beyond the borders of any member state of the EU.</p><p>Alongside introducing the concept of a QWAC, discussed in my previous blog post, eIDAS is also going to introduce some very concerning requirements that affect the Internet PKI. At the top of my list of concerns is that browser and client vendors (Root Store Operators) will have a legal obligation to add Government mandated Root Certificate Authorities to their Root Stores, bypassing existing approval mechanisms. </p><p>Yep, you read that right. Government mandated Root Certificate Authorities...</p><p>I could end this blog post right here because anyone reading this will understand the significance of such a statement, and just how much of a catastrophically bad idea that is, but it gets worse. There will also be restrictions placed on Root Store Operators around handling incidents at those Root CAs and possibly removing trust in them for wrongdoing. I cannot stress this enough so I'm going to say it again, this is a terrible idea.</p><h4 id="how-it-works-now">How it works now</h4><p>The system that we have now is not perfect, by any stretch of the imagination, but it has been improved so much over the years with tireless work from the industry, that where we are now, finally, is a good place.</p><p>A browser or device vendor like Apple has a collection of Trusted Root Certificate Authorities that their devices will trust, and in turn, those devices will trust any certificates issued by those Trusted Root CAs. If you want to join this collection of Trusted Root CAs, you have to apply to join the <a href="https://www.apple.com/certificateauthority/ca_program.html?ref=scotthelme.co.uk" rel="noreferrer">Apple Root Certificate Program</a> and pass some very strict requirements. Of course, this makes sense, because being a Trusted Root CA is a massive responsibility that gives you an enormous amount of power, and Apple want to make sure that their customers aren't going to come to any harm because of your actions. The same goes for all such Root Store Operators like <a href="https://www.mozilla.org/en-US/about/governance/policies/security-group/certs/policy/?ref=scotthelme.co.uk" rel="noreferrer">Mozilla</a>, <a href="https://www.chromium.org/Home/chromium-security/root-ca-policy/?ref=scotthelme.co.uk" rel="noreferrer">Chrome</a>, <a href="https://learn.microsoft.com/en-us/security/trusted-root/program-requirements?ref=scotthelme.co.uk" rel="noreferrer">Microsoft</a> and many others that operate Trusted Root Programs for their own devices or software. It is in the interest of the software/device vendor to make sure that a Root CA is capable of operating properly because if not, all of that vendor's customers are at serious risk of having their traffic intercepted and decrypted. So, for Apple, their concern is that if a Root CA makes a mistake, the potential outcome is that everyone using an iPhone could have the security of all of their traffic compromised! That's a serious risk, and it's why organisations like Apple take the process of approving Trusted Root CAs so damn seriously.</p><p>This is the existing approval mechanism that will be bypassed by this new legislation and the Root Store Operators will be required to accept these European Root CAs without the ability to scrutinise them, or, reject their inclusion.</p><figure><img src="https://scotthelme.co.uk/content/images/2023/11/CABForum_logo.png" alt="" loading="lazy" width="426" height="93"></figure><h4 id="how-its-going-to-work">How it's going to work</h4><p>I have access to the near-final text of the regulation, which is not yet public, but was leaked to me by a confidential source. I've been looking through the proposed changes and I still see all of the things that have concerned me throughout this entire process. Here are a few snippets from the hundreds of pages that I've read through that still demonstrate my concerns. These snippets outline the definition of a QWAC and that they must be held against the standards set out in the legislation:</p><blockquote>‘qualified certificate for website authentication’ means a certificate for website authentication, which is issued by a qualified trust service provider and meets the requirements laid down in Annex IV;</blockquote><blockquote>Qualified certificates for website authentication shall meet the requirements laid down in Annex IV.</blockquote><blockquote>Evaluation of compliance with those requirements shall be carried out in accordance with the standards and the specifications referred to in paragraph 3.</blockquote><p>But if that isn't clear enough for you, the legislation goes on to say:</p><blockquote>Qualified certificates for website authentication issued in accordance with paragraph 1 shall be recognised by web-browsers. Web-browsers shall ensure that the identity data attested in the certificate and additional attested attributes are displayed in a user-friendly manner. Web-browsers shall ensure support and interoperability with qualified certificates for website authentication referred to in paragraph 1</blockquote><p>That's pretty clear, and we can still see the same concerns I raised previously about the legislation controlling not only support for, and use of, the Government Mandated Root CAs, but even control over the UI of the browser itself. It goes on:</p><blockquote>National trusted lists should confirm the qualified status of website authentication services and of their trust service providers, including their full compliance with the requirements of this Regulation with regards to the issuance of qualified certificates for website authentication. Recognition of QWACs means that the providers of web-browsers should not deny the authenticity of qualified certificates for website authentication attesting the link between the website domain name and the natural or legal person to whom the certificate is issued and confirming the identity of that person. Providers of web-browsers should display in a user-friendly manner the certified identity data and the other attested attributes to the end-user, in the browser environment, by relying on technical implementations of their choice. To that end, providers of web-browsers should ensure support and interoperability with qualified certificates for website authentication issued in full compliance with the requirement of this Regulation.</blockquote><p>Again, pressing this idea of a list of Trusted Root CAs that the client vendors must accept and "should not deny the authenticity of". Then, with regards to limiting the ability of a Root Store Operator to audit the behaviour of a Trusted Root CA on an ongoing basis:</p><blockquote>In order to contribute to the online security of end-users, providers of web-browsers should be able to take measures, in exceptional circumstances, that are both necessary and proportionate in reaction to substantiated concerns on breaches of security or loss of integrity of an identified certificate or set of certificates. In this case, while taking any such precautionary measures, web browsers should notify without undue delay the national supervisory body and the Commission, the entity to whom the certificate was issued and the qualified trust service provider that issued that certificate or set of certificates of any such concern of a security breach as well as the measures taken relating to a single certificate or a set of certificates. These measures, should be without prejudice to the obligation of the browsers to recognize qualified website authentication certificates in accordance with the national trusted lists.</blockquote><p>Then, just to make sure we don't have any tremendously beneficial technologies like <a href="https://scotthelme.co.uk/tag/certificate-transparency/?ref=scotthelme.co.uk" rel="noreferrer">Certificate Transparency</a> protecting us, it is clarified that:</p><blockquote>Qualified certificates for website authentication shall not be subject to any mandatory requirements other than the requirements laid down in paragraph 1.</blockquote><p>Paragraph 1, of course, does not make any mention of Certificate Transparency. All of these points are then summarised in a newly added section with the title "Cybersecurity precautionary measures": </p><blockquote>1. Web-browsers shall not take any measures contrary to their obligations set out in Art 45, notably the requirement to recognise Qualified Certificates for Web Authentication, and to display the identity data provided in a user friendly manner.</blockquote><blockquote>2. By way of derogation to paragraph 1 and only in case of substantiated concerns related to breaches of security or loss of integrity of an identified certificate or set of certificates, web-browsers may take precautionary measures in relation to that certificate or set of certificates</blockquote><blockquote>3. Where measures are taken, web-browsers shall notify their concerns in writing without undue delay, jointly with a description of the measures taken to mitigate those concerns, to the Commission, the competent supervisory authority, the entity to whom the certificate was issued and to the qualified trust service provider that issued that certificate or set of certificates. Upon receipt of such a notification, the competent supervisory authority shall issue an acknowledgement of receipt to the web-browser in question.</blockquote><blockquote>4. The competent supervisory authority shall consider the issues raised in the notification in accordance with Article 17(3)(c). When the outcome of that investigation does not result in the withdrawal of the qualified status of the certificate(s), the supervisory authority shall inform the web-browser accordingly and request it to put an end to the precautionary measures referred to in paragraph 2.</blockquote><h4 id="the-industry-speaks-out">The industry speaks out</h4><p>It's not just me that thinks this is a bad idea though, of course, I'm just adding my voice to the chorus of other voices from across industry.</p><ol><li>Mozilla set up the <a href="https://securityriskahead.eu/?ref=scotthelme.co.uk" rel="noreferrer">Security Risk Ahead</a> website with lots of details.</li><li>The Chrome Security Team has called for change in <a href="https://security.googleblog.com/2023/11/qualified-certificates-with-qualified.html?ref=scotthelme.co.uk" rel="noreferrer">Qualified certificates with qualified risks</a>.</li><li>You can head over to <a href="https://last-chance-for-eidas.org/?ref=scotthelme.co.uk">https://last-chance-for-eidas.org/</a> to read more about the risks.</li><li>You can read our latest open letter with 400+ signatures so far. <a href="https://eidas-open-letter.org/?ref=scotthelme.co.uk">https://eidas-open-letter.org/</a></li></ol><figure><img src="https://scotthelme.co.uk/content/images/2023/11/zU7C5hOn_400x400.jpg" alt="" loading="lazy" width="400" height="400"></figure><p>The thing that it will always come down to for me, and the thing that you can use to guide your decisions, is to look at the interests of the parties involved. I've long been critical of many CAs for shitty marketing and shady practises, and it seems that's continuing. The organisations and voices speaking out in support of QWACs and Article 45 are those that are going to be able to sell them for $$$ once this comes to pass. The organisations and voices speaking out against QWACs and Article 45 are those with an interest in preserving and improving the security of the ecosystem we've worked so hard to build. I have nothing to gain here by swaying your opinion, but you sure as hell have a lot to lose.</p><h4 id="what-do-we-do-about-it">What do we do about it?</h4><p>I'll quote the following snippet from the 'Last Chance' <a href="https://last-chance-for-eidas.org/?ref=scotthelme.co.uk#what-next" rel="noreferrer">website</a>:</p><blockquote>If you’re a European citizen, you can write to the member of the European Parliament responsible for the <a href="https://oeil.secure.europarl.europa.eu/oeil/popups/ficheprocedure.do?reference=2021%2F0136%28COD%29&amp;l=en&amp;ref=scotthelme.co.uk" rel="noreferrer">eIDAS file</a> - <a href="https://www.europarl.europa.eu/meps/en/112747/ROMANA_JERKOVIC/home?ref=scotthelme.co.uk" rel="noreferrer">Romana JERKOVIĆ</a> - and register your concern.</blockquote><blockquote>If you’re a cybersecurity expert, researcher or represent an NGO, consider signing the open letter at <a href="https://eidas-open-letter.org/?ref=scotthelme.co.uk" rel="noreferrer">https://eidas-open-letter.org</a>.</blockquote><p>In truth, I don't know what else to do next, but I believe we have to do something. If these Qualified Trust Service Providers (QTSP is the name given to a CA that issues QWACs) are all they're cracked up to be, then why can't they just submit to the existing audit/approval process and pass with flying colours?.. That's not too much to ask, is it?</p><h4 id="additional-information-and-reading">Additional information and reading</h4><p><a href="https://sslmate.com/resources/certificate_authority_failures?ref=scotthelme.co.uk" rel="noreferrer">Timeline of Certificate Authority Failures</a> - why Trust Store Operators need the ability to audit and remove Root CAs.</p><p><a href="https://www.european-signature-dialog.eu/ESD_answer_to_Mozilla_misinformation_campaign.pdf?ref=scotthelme.co.uk" rel="noreferrer">Mozilla website pushes serious eIDAS misinformation to political decision makers and public</a> - The ESD (<a href="https://ec.europa.eu/transparencyregister/public/consultation/displaylobbyist.do?id=994150833943-81&amp;ref=scotthelme.co.uk#scrollNav-13" rel="noreferrer">a group of CAs</a>) produced this laughable document. It closes by pointing out that Google and Mozilla are "investors" in Let's Encrypt who are "in competition with all QTSPs" 😂 (a QTSP is a CA that issues QWACs)</p><p>Digital rights organisation epicenter.works had <a href="https://epicenter.works/en/content/eu-digital-identity-reform-the-good-bad-ugly-in-the-eidas-regulation?ref=scotthelme.co.uk#:~:text=to%20this%20information.-,The%20Ugly,-QWACs" rel="noreferrer">this</a> to say about QWACs.</p><p>You should read what <a href="https://alecmuffett.com/article/108139?ref=scotthelme.co.uk" rel="noreferrer">Alec Muffett</a> has to say on eIDAS/QWACs.</p><p>This informative Tweet from <a href="https://x.com/rmhrisk/status/1721329896746848353?s=46&amp;t=Ms_J84N8ypSKLl6M43cBnQ&amp;ref=scotthelme.co.uk" rel="noreferrer">Ryan Hurst</a> is also a great start for info on the Internet PKI.</p><p><em>Update 19:10 UTC 7th Nov</em>: The EFF have just published something on this, <a href="https://www.eff.org/deeplinks/2023/11/article-45-will-roll-back-web-security-12-years?ref=scotthelme.co.uk" rel="noreferrer">Article 45 Will Roll Back Web Security by 12 Years</a>, and as you would expect, it's well written and makes a lot of sense!</p>
</div>
<br>If you want to get notified when I publish a new blog, please consider <a href="https://scotthelme.ghost.io/#/portal/signup">subscribing</a>!<p>
<i></i>
Tags: <a href="https://scotthelme.co.uk/tag/qwac/">QWAC</a>, <a href="https://scotthelme.co.uk/tag/eidas/">eIDAS</a>, <a href="https://scotthelme.co.uk/tag/tls/">TLS</a>, <a href="https://scotthelme.co.uk/tag/pki/">PKI</a>, <a href="https://scotthelme.co.uk/tag/ev/">EV</a></p>
</section>
</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What does and doesn't matter about Apple shooting their October event on iPhone (284 pts)]]></title>
            <link>https://prolost.com/blog/scarybts</link>
            <guid>38182869</guid>
            <pubDate>Tue, 07 Nov 2023 21:08:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://prolost.com/blog/scarybts">https://prolost.com/blog/scarybts</a>, See on <a href="https://news.ycombinator.com/item?id=38182869">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-content-field="main-content" id="pageWrapper" role="main">
        <article id="article-6549717174a74b3d8221d4bd" data-item-id="6549717174a74b3d8221d4bd">

  <!--SPECIAL CONTENT-->

  
     
  

  
  <!--POST HEADER-->
    
  <header>
    
    
  </header>
  
  
  <!--POST BODY-->

  <div data-layout-label="Post Body" data-type="item" data-updated-on="1699312007408" id="item-6549717174a74b3d8221d4bd"><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1699383543319_113472">

      

      
        <figure>
          
        
        

        
          
            
          
        

        
          
          <figcaption>
            <p>Actual footage of me watching the event.</p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="44" id="block-yui_3_17_2_1_1699383543319_260414"><p>Then Apple posts a <a href="https://youtu.be/V3dbG9pAi8I">behind-the-scenes video</a> showing how this was done, revealing a rare and imposing glimpse into the scale and scope of their industry-leading launch videos. At the center of it all, instead of their customary <a href="https://en.wikipedia.org/wiki/Arri_Alexa">Arri Alexa</a> (a digital cinema camera costing $35–150K before you even add a lens, used to shoot everything from <em>Avengers: Endgame</em> to <em>Barbie),</em> was an off-the-shelf iPhone 15 Pro Max, gripped into truckloads of professional support gear.</p></div><div data-block-json="{&quot;thumbnailUrl&quot;:&quot;https://i.ytimg.com/vi/V3dbG9pAi8I/hqdefault.jpg&quot;,&quot;width&quot;:854,&quot;height&quot;:480,&quot;hSize&quot;:null,&quot;html&quot;:&quot;<iframe class=\&quot;embedly-embed\&quot; src=\&quot;//cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FV3dbG9pAi8I%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DV3dbG9pAi8I&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FV3dbG9pAi8I%2Fhqdefault.jpg&amp;key=61d05c9d54e8455ea7a9677c366be814&amp;type=text%2Fhtml&amp;schema=youtube\&quot; width=\&quot;854\&quot; height=\&quot;480\&quot; scrolling=\&quot;no\&quot; title=\&quot;YouTube embed\&quot; frameborder=\&quot;0\&quot; allow=\&quot;autoplay; fullscreen; encrypted-media; picture-in-picture;\&quot; allowfullscreen=\&quot;true\&quot;></iframe>&quot;,&quot;url&quot;:&quot;https://youtu.be/V3dbG9pAi8I&quot;,&quot;resolvedBy&quot;:&quot;youtube&quot;,&quot;floatDir&quot;:null,&quot;providerName&quot;:&quot;YouTube&quot;,&quot;customThumbEnabled&quot;:false}" data-block-type="22" id="block-yui_3_17_2_1_1699312008488_9068"><p>




  <iframe src="//www.youtube.com/embed/V3dbG9pAi8I?wmode=opaque" height="480" width="854" scrolling="no" frameborder="0" allowfullscreen=""></iframe>

</p></div><div data-block-type="44" id="block-yui_3_17_2_1_1699312008488_9121"><p>At this point, some folks felt differently about what was implied by “Shot on iPhone.” There have been <a href="https://www.theverge.com/2023/10/31/23940060/apple-event-shot-on-iphone-behind-the-scenes">bad takes</a> on this, and <a href="https://daringfireball.net/linked/2023/10/31/downplaying-shot-on-iphone">good</a><a href="https://joe-steel.com/2023-11-01-Videography-For-Dummies.html">takes</a> on those bad takes.</p>
<p>Anyone who knows the tiniest bit about video production knows that the camera is a small, but important, but <em>small,</em> part of the overall production picture. “Shot on iPhone” doesn’t promise “and you can do it too” any more than Stanley Kubrick lighting <em>Barry Lyndon</em> with <a href="https://www.criterion.com/current/posts/5059-kubrick-s-candle-tricks-in-barry-lyndon">candlelight</a> means anyone with candles can make <em>Barry Lyndon.</em></p>
<p>But when the camera is the least expensive piece of gear on the set after clothespins and coffee, it does feel strange. I’ve been on a lot of productions like this, having played an active role in the DV filmmaking revolution of the late ’90s-to-early-2000s. It was an odd feeling to scrounge for the adapter plates required to mount a $3,000 DV camcorder purchased at Circuit City to a Fisher dolly that literally has no purchase price.</p>
<p>Apple, of course, has no burden of best-practices logic for their decision to shoot their “Scary Fast” event on iPhone — it’s a marketing ploy, a cool stunt, and a massive flex. A thing to do for its own sake. In the filmmaking community, it was the mic drop of the year. We greedily soaked up all the details in the behind-the-scenes video, and made a hundred tiny calculations about which aspects of this lavish production actually mattered to the question of the iPhone 15’s validity as a professional camera, and which did not.</p>
<p>With all that gear and production support, which aspects of the event really matter to you, the iPhone-curious filmmaker? What can you learn, and which aspects can you safely ignore?</p>
<p>Let’s take it one at a time:</p>
<h2 id="that-they-did-it-does-matter">That They Did It: Does Matter</h2>
<p>As camera features have played a larger and larger role in Apple’s marketing for new iPhones over the years, you might have begun to feel a bit of cognitive dissonance. Apple tells you about how great, and even “pro,” these new iPhone cameras are — but would never have dreamt of using them to shoot their own videos or product stills. Apple was effectively saying “pro enough for you, but not for us.” Valid, but a bit dissatisfying.</p>

</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1699312008488_19622">

      

      
        <figure>
          
        
        

        
          
            
          
        

        
          
          <figcaption>
            <p>A still frame from Apple’s October 30 “Scary Fast” event video, shot on iPhone 15 Pro Max.</p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="44" id="block-yui_3_17_2_1_1699312008488_19960"><p>Apple has set the aesthetic bar impossibly high with these pre-recorded events. They’re not just executives teleprompting in front of Keynote slides — they feature “slice of life” moments shot on huge sets and real locations. Elaborate visual effects transition between locations and settings that might be partially virtual. These videos have looked great ever since Covid pushed Apple to find an alternative to executives-on-stage-in-front-of-slides, and even as Apple is now once again able to welcome guests to in-person product launches, these lavishly-produced videos are the new gold standard in pitching the world on a new iThing.</p>
<p>With “Scary Fast,” Apple repeated their now well-established high-production-value playbook, but yoinked out the professional cameras and lenses, and dropped in a commodity consumer telephone in their place.</p>
<p>And crucially, <em>none of us noticed.</em></p>
<p>It’s a big deal.</p>
<h2 id="they-shot-prores-log-matters-so-much-it-would-be-impossible-without-it">They Shot ProRes Log: Matters So Much It Would Be Impossible Without It</h2>
<p>There is one single feature of the iPhone 15 Pro that made this stunt possible: Log. As I detailed <a href="https://prolost.com/blog/applelog">here in words and video</a>, the “iPhone video look” is designed to win consumer popularity contests, not mimic Apple's own marketing videos, nor plug into professional workflows.</p>
<p>It may be hard to imagine that a slightly different bit of signal processing when recording a video file from a tiny sensor can make the difference between consumer birthday-cam and professional viability, bit that is exactly the power of log. Apple Log has catapulted the iPhone into filmmaking legitimacy.</p>

</div><div data-block-type="44" id="block-yui_3_17_2_1_1699312008488_28200">
<h2 id="they-used-big-lights-does-matter-with-an-asterisk">They Used Big Lights: Does Matter, With An Asterisk</h2>
<p>Apple’s event was set at night, with a dark, Halloween-inspired look. It takes a lot of professional lighting gear to illuminate a wide shot of Apple’s campus, and professional skill to balance this lighting with the practical sources on the building itself.</p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1699312008488_37554">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/a92c31a0-5743-4b57-873d-07115538eb25/BTS_01_iPhone+%2801319%29.jpg" data-image="https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/a92c31a0-5743-4b57-873d-07115538eb25/BTS_01_iPhone+%2801319%29.jpg" data-image-dimensions="3840x2160" data-image-focal-point="0.5,0.5" alt="" data-load="false" src="https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/a92c31a0-5743-4b57-873d-07115538eb25/BTS_01_iPhone+%2801319%29.jpg" width="3840" height="2160" sizes="100vw" srcset="https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/a92c31a0-5743-4b57-873d-07115538eb25/BTS_01_iPhone+%2801319%29.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/a92c31a0-5743-4b57-873d-07115538eb25/BTS_01_iPhone+%2801319%29.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/a92c31a0-5743-4b57-873d-07115538eb25/BTS_01_iPhone+%2801319%29.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/a92c31a0-5743-4b57-873d-07115538eb25/BTS_01_iPhone+%2801319%29.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/a92c31a0-5743-4b57-873d-07115538eb25/BTS_01_iPhone+%2801319%29.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/a92c31a0-5743-4b57-873d-07115538eb25/BTS_01_iPhone+%2801319%29.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/a92c31a0-5743-4b57-873d-07115538eb25/BTS_01_iPhone+%2801319%29.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">
                
            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p>You can do this for cheaper than it looks.</p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="44" id="block-yui_3_17_2_1_1699314604996_9784"><p>Lighting matters more than any camera, more than any lens. As <a href="https://prolost.com/blog/2009/4/13/fact-moment-light.html">I wrote</a> in 2009:</p><blockquote>
<p>Photos are nothing but light — it’s literally all they are made of. Timmy’s birthday and Sally’s wedding are reduced to nothing but photons before they become photographs. So getting the light right is more meaningful to a photo than anything else.</p>
</blockquote><p>Should you look at the giant lights in Apple’s video and feel dejected that your own productions will never afford this level of illumination? I say no, because a) you’re probably not lighting up the whole side of an architectural marvel, and b) you’re probably not designing your production around one of the world’s highest-paid CEOs.</p><p>For Tim Cook’s appearance, Apple’s production had their giant LED light panels on camera dollies, which is not typical. The two reasons I can image they did this are to be low-impact on the campus itself (rubber wheels instead of metal stands), and to be able to adjust the lighting quickly out of respect for Cook’s valuable time. It makes the lighting rigs seem more complex than they really are.</p><p>What they really are is big, bright, and soft. And rather spare — mostly key, a bit of fill.</p><p>Big, soft LED lighting is actually quite affordable these days. I have two <a href="https://www.bhphotovideo.com/c/product/1560654-REG/aputure_light_storm_ls_60x.html/BI/4778/KBID/5292">medium-power bi-color lights from Aputure</a>, and together they cost less than my iPhone 15 Pro Max. I couldn’t cover Cook’s opening wide shot with them, but I could get close.</p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1699331059768_29490">

      

      
        <figure>
          
        
        

        
          
            
          
        

        
          
          <figcaption>
            <p>That big softbox overhead? Now <em>that’s</em> expensive.</p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="44" id="block-yui_3_17_2_1_1699331059768_34898">
<p>I might also be willing to compromise on my ISO settings to work with a smaller lighting package, where Apple seemingly was not. More on this below.</p><p>So the lighting is important, but the quantity of it and the support gear it’s on is specific to this rarified type of time-is-money, night-exterior production. Don’t be distracted by the extra equipment, focus on the fact that the lighting is actually rather spare .</p>
<h2 id="they-attached-the-iphone-to-cranes-and-gimbals-and-drones-and-dollies-does-not-matter-except-for-one-little-thing">They Attached the iPhone to Cranes and Gimbals and Drones and Dollies: Does Not Matter, Except for One Little Thing</h2>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1699312008488_39805">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/d57d0485-4961-446b-93ac-9dfea59024c7/BTS_01_iPhone+%2800742%29.jpg" data-image="https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/d57d0485-4961-446b-93ac-9dfea59024c7/BTS_01_iPhone+%2800742%29.jpg" data-image-dimensions="3840x2160" data-image-focal-point="0.5,0.5" alt="" data-load="false" src="https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/d57d0485-4961-446b-93ac-9dfea59024c7/BTS_01_iPhone+%2800742%29.jpg" width="3840" height="2160" sizes="100vw" srcset="https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/d57d0485-4961-446b-93ac-9dfea59024c7/BTS_01_iPhone+%2800742%29.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/d57d0485-4961-446b-93ac-9dfea59024c7/BTS_01_iPhone+%2800742%29.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/d57d0485-4961-446b-93ac-9dfea59024c7/BTS_01_iPhone+%2800742%29.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/d57d0485-4961-446b-93ac-9dfea59024c7/BTS_01_iPhone+%2800742%29.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/d57d0485-4961-446b-93ac-9dfea59024c7/BTS_01_iPhone+%2800742%29.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/d57d0485-4961-446b-93ac-9dfea59024c7/BTS_01_iPhone+%2800742%29.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/d57d0485-4961-446b-93ac-9dfea59024c7/BTS_01_iPhone+%2800742%29.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">
                
            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="44" id="block-yui_3_17_2_1_1699312008488_40143"><p>The behind-the-scenes video is almost comical in its portrayal of the iPhone gripped into all manner of high-end support gear.</p>
<p>You do not need any of this stuff.</p>
<p>I mean, every filmmaker <em>needs</em> a crane shot — but this is why small cameras are so empowering: everything is a crane when your camera weighs less than a Panavision lens cap!</p>
<p>Check out <a href="https://youtu.be/OnS5sLclqqY">this video</a> from filmmaker Brandon Li. He uses a gimbal on a hand-held pole to create a perfect crane shot for the opening of his action short. Toward the end, he achieves a nifty top-down shot by... standing on a railing. All with a camera substantially more cumbersome than a phone.</p>

</div><div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1699314604996_46552">

      

      
        <figure>
          
        
        

        
          
            
          
        

        
          
          <figcaption>
            <p>Director Brandon Li is his own crane.</p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1699314604996_64587">

      

      
        <figure>
          
        
        

        
          
            
          
        

        
          
          <figcaption>
            <p>Get your kicks without sticks.</p>
          </figcaption>
        
      
        </figure>
      

    </div></div><div data-block-type="44" id="block-yui_3_17_2_1_1699314604996_46890"><p>Apple used cranes and remote heads designed for big cameras because that’s how they know how to shoot these videos. Apple’s marketing department is large, and knows exactly what they need on these productions. One thing they need is for a dozen people to watch the camera feed, making sure everything is committee-approved perfect.</p></div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1699314604996_28757">

      

      
        <figure>
          
        
        

        
          
            
          
        

        
          
          <figcaption>
            <p>This is just the DIT cart, not even the client monitor.</p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="44" id="block-yui_3_17_2_1_1699314604996_29095"><p>This kind of client-driven support gear compounds on its own requirements. As Tyler Stalman points out in his <a href="https://youtu.be/lDqQ_C5BkY4">excellent breakdown video</a>, some of what’s bolted to the iPhones is simply a metal counterweight so that a gimbal head, designed for a much larger camera, can be properly balanced.</p>
<p>You can plug an external drive into the USB-C slot on the iPhone 15 Pro Max, or you can plug in an HDMI adapter for a clean client feed. If you want to do both, you need a USB-C hub, which at that point requires power. So now you’ve got an Anton-Bauer battery pack mounted to this tangle of gear.</p>
<p>When you don’t have clients, you can skip all that and just shoot. This means you can replace most of the gear you see here with a <a href="https://amzn.to/3stXw46">cheap consumer gimbal</a> — or a tripod.</p>
<p>And here’s the key takeaway for this point: Apple achieved optimal image quality from the iPhone in a number of ways, and one, I’m betting, was by turning off image stabilization — which is only advisable when this tiny camera is physically stabilized.</p>
<p>So you don’t need all the stuff Apple used, but if you want comparable results, you need a way to mount your iPhone to something solid. Maybe not a whole powered cage, but certainly a simple tripod mount. Then you can eek out that last bit of extra image quality by turning off image stabilization — which brings us to our next point:</p>
<h2 id="they-used-the-blackmagic-camera-app-matters-as-much-as-log">They Used the Blackmagic Camera App: Matters as Much as Log</h2>

</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1699314604996_70912">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/e02bed03-28ad-4650-b582-c84de4aeaa10/BTS_01_iPhone+%2802096%29.jpg" data-image="https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/e02bed03-28ad-4650-b582-c84de4aeaa10/BTS_01_iPhone+%2802096%29.jpg" data-image-dimensions="3840x2160" data-image-focal-point="0.5,0.5" alt="" data-load="false" src="https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/e02bed03-28ad-4650-b582-c84de4aeaa10/BTS_01_iPhone+%2802096%29.jpg" width="3840" height="2160" sizes="100vw" srcset="https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/e02bed03-28ad-4650-b582-c84de4aeaa10/BTS_01_iPhone+%2802096%29.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/e02bed03-28ad-4650-b582-c84de4aeaa10/BTS_01_iPhone+%2802096%29.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/e02bed03-28ad-4650-b582-c84de4aeaa10/BTS_01_iPhone+%2802096%29.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/e02bed03-28ad-4650-b582-c84de4aeaa10/BTS_01_iPhone+%2802096%29.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/e02bed03-28ad-4650-b582-c84de4aeaa10/BTS_01_iPhone+%2802096%29.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/e02bed03-28ad-4650-b582-c84de4aeaa10/BTS_01_iPhone+%2802096%29.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/e02bed03-28ad-4650-b582-c84de4aeaa10/BTS_01_iPhone+%2802096%29.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">
                
            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p>I don’t think this exact rig was used to capture what we saw in the video, but I believe the settings are representative.</p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="44" id="block-yui_3_17_2_1_1699314604996_71249"><p>The <a href="https://www.blackmagicdesign.com/products/blackmagiccamera">Blackmagic Camera App</a> has the option to turn off image stabilization, yes, and also like a million other features. Manual, locking exposure control is the top of the list, but there’s a ton more. The app includes a false-color mode to help match exposure from shot to shot. It can load a preview LUT, so you can shoot log but view something closer to what the audience will see.</p><p>It’s silly to be grumpy with Apple for not offering this power in their own camera app when they clearly worked with Blackmagic Design to have this app available day-and-date with the iPhone 15.</p><p>Oh, and it’s free.</p><h2 id="they-used-a-180-shutter-matters-more-thank-you-think">They Used a 180º Shutter: Matters More Thank You Think</h2>
</div><div data-block-type="44" id="block-yui_3_17_2_1_1699314604996_101663"><p>One slick feature of the Blackmagic Camera App is that you can choose to express the shutter speed in degrees, like a cinema camera, rather than fractions of a second, which is more typical in stills. A 180º shutter — where the shutter is open for half the duration of a single frame, e.g. 1/60th of a second at 30 fps — is important for a pro look. Anything slower and you get smeary blur and a camcorder look. Anything faster and your footage looks like you shot it on a phone, because 99% of the time our iPhones are using insanely fast shutter speeds to handle typical daylight. Look at any of your own daytime iPhone video — I’d be surprised if you see any motion blur at all.</p></div><div data-block-json="{&quot;blockAnimation&quot;:&quot;none&quot;,&quot;layout&quot;:&quot;caption-below&quot;,&quot;overlay&quot;:false,&quot;description&quot;:{&quot;html&quot;:&quot;<p class=\&quot;\&quot; style=\&quot;white-space:pre-wrap;\&quot;>Compare Apple\u2019s motion blur to mine. 180\u00BA shutter vs. probably something more like 1\u00BA.</p>&quot;},&quot;hSize&quot;:null,&quot;floatDir&quot;:null,&quot;isOldBlock&quot;:false,&quot;settings&quot;:{&quot;muted&quot;:true,&quot;autoPlay&quot;:true,&quot;loop&quot;:true,&quot;controls&quot;:&quot;none&quot;},&quot;url&quot;:&quot;&quot;,&quot;resolvedBy&quot;:&quot;native&quot;,&quot;nativeVideo&quot;:&quot;6549811d14cf17486eb7fe30&quot;}" data-block-type="32" id="block-yui_3_17_2_1_1699314604996_106232"><p>Compare Apple’s motion blur to mine. 180º shutter vs. probably something more like 1º.</p></div><div data-block-type="44" id="block-yui_3_17_2_1_1699314604996_106282"><p>Relatedly:</p>
<h2 id="they-shot-at-iso-55-matters-to-apple-s-goal-of-maximum-image-quality">They Shot at ISO 55: Matters to Apple’s Goal of Maximum Image Quality</h2>
<p>Here’s where the level of professional control over the lighting starts to really matter: If Apple decided that they must shoot at ISO 55 (the lowest, although possibly not the <em>native</em> ISO of the 1x camera) for the highest image quality, and with a 180º shutter for the most pro-camera look, that means they have no other control over exposure. The iPhone 15 Pro 1x lens does not have a variable aperture, so shutter speed and ISO are your only exposure controls.</p>
<p>When shooting in uncontrolled environments, the typical method of limiting the amount of light entering the lens is via ND filters, sometimes <em>variable</em> ND filters. I don’t see any evidence that Apple used filters on this shoot, which would fit with their overall prioritization of image quality over all else. So this goes back to lighting — Apple’s team controlled that lighting perfectly, because they opted out of any exposure control they might have had in-camera.</p>
<p>I'm curious to learn more about this setting though. YouTubers Gerald Undone and Patrick Tomasso <a href="https://youtu.be/RNd74wyVtKw">did some tests</a> and found that the best dynamic range from the iPhone 15 Pro came from ISO 1100–1450, with 1250 being their recommended sweet spot. Did Apple prioritize low noise over dynamic range?</p>
<h2 id="they-shot-30p-doesn-t-matter">They shot 30p: Doesn’t Matter</h2>
<p>Apple has used 30 frames-per-second for these pre-recorded keynotes since they started in September of 2020. They’re not trying to be “cinematic,” they’re trying to make a nice, clean video that can take the place of a live event. 30p is a choice, and a fine one for an on-stage presentation. You might choose 24 or 25 fps for a more narrative look, and that’s great too.</p>
<p>Note that Apple’s native Camera app offers 30.0 and 24.0 fps, but the Blackmagic Camera app adds support for 29.97 and 23.976 fps, which are the actual broadcast frame rates Apple uses for their productions.</p>
<h2 id="they-focused-manually-doesn-t-matter">They Focused Manually: Doesn’t Matter</h2>
<p>The Blackmagic Camera app truly has a dizzying set of features, some seemingly part of an attempt to win some kind of bizarre bet. Like support for wireless external follow-focus controls? I mean, wow, but also, really?</p>
<p>Sure makes for a cool behind-the-scenes shot, but I bet you can live without this.</p>

</div><div data-block-type="44" id="block-yui_3_17_2_1_1699314604996_128955"><h2 id="they-used-a-matte-box-does-matter">They Used a Matte Box: Does Matter</h2>
<p>While Apple did not attach any additional lenses to their production iPhones, they did put stuff in front of the built-in lenses — notably teleprompters, of course, and comically-large matte boxes.</p>

</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1699314604996_132284">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/18eab21f-fa71-4820-8df0-9c51635cf6e9/BTS_01_iPhone+%2801778%29.jpg" data-image="https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/18eab21f-fa71-4820-8df0-9c51635cf6e9/BTS_01_iPhone+%2801778%29.jpg" data-image-dimensions="3840x2160" data-image-focal-point="0.5,0.5" alt="" data-load="false" src="https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/18eab21f-fa71-4820-8df0-9c51635cf6e9/BTS_01_iPhone+%2801778%29.jpg" width="3840" height="2160" sizes="100vw" srcset="https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/18eab21f-fa71-4820-8df0-9c51635cf6e9/BTS_01_iPhone+%2801778%29.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/18eab21f-fa71-4820-8df0-9c51635cf6e9/BTS_01_iPhone+%2801778%29.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/18eab21f-fa71-4820-8df0-9c51635cf6e9/BTS_01_iPhone+%2801778%29.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/18eab21f-fa71-4820-8df0-9c51635cf6e9/BTS_01_iPhone+%2801778%29.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/18eab21f-fa71-4820-8df0-9c51635cf6e9/BTS_01_iPhone+%2801778%29.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/18eab21f-fa71-4820-8df0-9c51635cf6e9/BTS_01_iPhone+%2801778%29.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/18eab21f-fa71-4820-8df0-9c51635cf6e9/BTS_01_iPhone+%2801778%29.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">
                
            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="44" id="block-yui_3_17_2_1_1699314604996_132621"><p>Matte boxes might feel like affectations in this context, but shielding that tiny lens from glare is actually a significant way to improve overall image quality. Luckily, you don’t really need a full-on matte box to do this. A French flag will do it, as will your hand.</p></div><div data-block-json="{&quot;blockAnimation&quot;:&quot;none&quot;,&quot;layout&quot;:&quot;caption-below&quot;,&quot;overlay&quot;:false,&quot;description&quot;:{&quot;html&quot;:&quot;<p class=\&quot;\&quot; style=\&quot;white-space:pre-wrap;\&quot;>If a bright light is dinging your little lens, you\u2019re leaving a ton of image quality on the floor.</p>&quot;},&quot;hSize&quot;:null,&quot;floatDir&quot;:null,&quot;isOldBlock&quot;:false,&quot;settings&quot;:{&quot;muted&quot;:true,&quot;autoPlay&quot;:true,&quot;loop&quot;:true,&quot;controls&quot;:&quot;none&quot;},&quot;url&quot;:&quot;&quot;,&quot;resolvedBy&quot;:&quot;native&quot;,&quot;nativeVideo&quot;:&quot;6549982702e58c5eb5b22795&quot;}" data-block-type="32" id="block-yui_3_17_2_1_1699314604996_437565"><p>If a bright light is dinging your little lens, you’re leaving a ton of image quality on the floor.</p></div><div data-block-type="44" id="block-yui_3_17_2_1_1699314604996_437616"><h2 id="they-exclusively-used-the-1x-camera-does-matter-to-apple">They Exclusively Used the 1x Camera: Does Matter — to Apple</h2>
<p>The 1x camera, with image stabilization turned off, gives the highest-quality image available from the iPhone 15 Pro Max. Are you detecting a theme here? Apple imposed a number of limitations on how they used the iPhone camera, seemingly always in the name of maximizing image quality.</p>
<p>As we’ll discuss below, you may or may not share this priority.</p>
<h2 id="they-edited-in-premiere-pro-doesn-t-matter">They Edited in Premiere Pro? Doesn’t Matter</h2>

</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1699314604996_182614">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/85083792-4dd8-4492-8577-bd5dfb1bb3f5/BTS_01_iPhone+%2803165%29.jpg" data-image="https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/85083792-4dd8-4492-8577-bd5dfb1bb3f5/BTS_01_iPhone+%2803165%29.jpg" data-image-dimensions="3840x2160" data-image-focal-point="0.5,0.5" alt="" data-load="false" src="https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/85083792-4dd8-4492-8577-bd5dfb1bb3f5/BTS_01_iPhone+%2803165%29.jpg" width="3840" height="2160" sizes="100vw" srcset="https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/85083792-4dd8-4492-8577-bd5dfb1bb3f5/BTS_01_iPhone+%2803165%29.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/85083792-4dd8-4492-8577-bd5dfb1bb3f5/BTS_01_iPhone+%2803165%29.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/85083792-4dd8-4492-8577-bd5dfb1bb3f5/BTS_01_iPhone+%2803165%29.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/85083792-4dd8-4492-8577-bd5dfb1bb3f5/BTS_01_iPhone+%2803165%29.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/85083792-4dd8-4492-8577-bd5dfb1bb3f5/BTS_01_iPhone+%2803165%29.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/85083792-4dd8-4492-8577-bd5dfb1bb3f5/BTS_01_iPhone+%2803165%29.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/85083792-4dd8-4492-8577-bd5dfb1bb3f5/BTS_01_iPhone+%2803165%29.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">
                
            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p>Real editor, fake set.</p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="44" id="block-yui_3_17_2_1_1699314604996_182951"><p>Eagle-eyed viewers noticed that an Adobe Premiere Pro timeline appears behind the editor of “Scary Fast.” But we’re not in a real edit suite here — we’re actually on one of the sets from the production. Did Apple <em>really</em> edit in Premiere?</p>
<p>I have a feeling that Stefan Sonnenfeld’s interview was also staged at Apple’s campus rather than filmed on-location at Company 3, to keep the production close to home for cost, control, and secrecy reasons. So let’s assume Apple really did cut in Premiere. This means next to nothing. All editing software does the same job, and it’s unlikely Apple would impose a workflow on the production company they hired to both shoot and post-produce the video.</p>
<p>Other than of course to ensure that it be cut on a Mac. It’s interesting to note that Apple’s Pro Workflows Group, representatives from which are interviewed in the behind-the-scenes video, are a part of the hardware division at Apple. Their charter is to promote and support professional use of Apple <em>devices,</em> regardless of which software they’re running.</p>
<p>Should FCPX users be <a href="https://daringfireball.net/linked/2023/11/02/tobin-bts-scary-fast">nervous</a> that Apple might send it to live on a farm with Shake and Aperture? It’s hard to regain our trust here, but Apple did just release a very nice version for iPad a few months ago, and <a href="https://www.polarpro.com/products/iphone-15-case?variant=iPhone15ProForest">substantial updates</a> to that and the Mac version just yesterday.</p>
<p>So there’s really nothing to see here. Move along.</p>
<h2 id="they-colored-in-resolve-doesn-t-matter">They Colored in Resolve: Doesn’t Matter</h2>
<p>Apple hired <a href="https://www.company3.com/">Company 3</a> to produce this video. Company 3 is best-known as a color house. In my VFX and commercial directing career. I’ve worked with several amazing colorists there, from Dave Hussey to Siggy Ferstl to their CEO, Stefan Sonnenfeld, who is prominently featured in the behind-the-scenes. Stephan is one of the most prolific, talented, and well-known colorists working today. I modeled half the presets in Magic Bullet Looks after his famous grades on films like <em>Transformers, 300, John Wick, Man on Fire,</em> and hundreds more.</p>

</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1699314604996_196734">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/9f3f2f02-c0d2-44c9-a63a-e637fae85939/BTS_01_iPhone+%2802346%29.jpg" data-image="https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/9f3f2f02-c0d2-44c9-a63a-e637fae85939/BTS_01_iPhone+%2802346%29.jpg" data-image-dimensions="3840x2160" data-image-focal-point="0.5,0.5" alt="" data-load="false" src="https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/9f3f2f02-c0d2-44c9-a63a-e637fae85939/BTS_01_iPhone+%2802346%29.jpg" width="3840" height="2160" sizes="100vw" srcset="https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/9f3f2f02-c0d2-44c9-a63a-e637fae85939/BTS_01_iPhone+%2802346%29.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/9f3f2f02-c0d2-44c9-a63a-e637fae85939/BTS_01_iPhone+%2802346%29.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/9f3f2f02-c0d2-44c9-a63a-e637fae85939/BTS_01_iPhone+%2802346%29.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/9f3f2f02-c0d2-44c9-a63a-e637fae85939/BTS_01_iPhone+%2802346%29.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/9f3f2f02-c0d2-44c9-a63a-e637fae85939/BTS_01_iPhone+%2802346%29.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/9f3f2f02-c0d2-44c9-a63a-e637fae85939/BTS_01_iPhone+%2802346%29.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/9f3f2f02-c0d2-44c9-a63a-e637fae85939/BTS_01_iPhone+%2802346%29.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">
                
            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p>Real colorist, not his real office.</p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="44" id="block-yui_3_17_2_1_1699314604996_197072"><p>If you can get Stefan to color your video, <em>that’s</em> what matters — not the tool he uses. Resolve is “free” (with many asterisks), but a session at Company 3 is four-figures per hour.</p>
<p>Whether you use Resolve, Magic Bullet, or whatever else, what matters here is that shooting log means color grading is not just possible, but essential, and great care was taken with this part of the process.</p>
<h2 id="this-all-makes-sense-why-do-i-still-feel-weird-about-it-">This All Makes Sense. Why Do I Still Feel Weird About it?</h2>
<p>As much as I might disagree with an accusation that Apple was disingenuous to say “Shot on iPhone” about a massive production with seemingly unlimited resources, I understand where this feeling comes from. “Shot on iPhone” carries with it the implication of accessibility. We are meant to be inspired by this phrase to use a tool that we already carry with us every day to capture videos that might transcend mere records of our life’s moments, and become part of our artistic pursuits.</p>
<p>And we should absolutely feel that way about the iPhone 15 Pro and Pro Max.</p>
<p>The reason we feel slightly disconnected from Apple’s impressive exercise is not that they were dishonest — it's that their priorities were different from ours. Apple wants to sell iPhones, and to accomplish this, they spared no expense to put the very highest image quality on the screen.</p>
<h4 id="as-a-filmmaker-you-care-about-image-quality-but-you-care-about-other-things-too-probably-more-">As a filmmaker, you care about image quality, but you care about other things too — probably more.</h4>
<p>Sticking to the 1x lens gives the best image quality, sure, but choosing the right lens for the story you’re telling might matter more to a filmmaker. You’ll probably use all the focal lengths Apple supplied.</p>
<p>As much as you might value the clean image that comes from shooting at ISO 55, you might get more production value for your budget by using smaller lights (or available light!) and accepting some noise at higher ISOs.</p>
<p>You might truly appreciate the value of a 180º shutter, and simply not always have a camera case/cage that allows you to mount a variable ND filter to your telephone. I ordered one from <a href="https://proloststore.com/products/applelog">PolarPro</a> the day the iPhone 15 was released and it still hasn’t shipped, so I’ve shot next to no 180º shutter footage so far.</p>
<p>You might well understand that turning off image stabilization will improve your image — unless your shot is now wobbly, because you’re shooting handheld out the window of a moving car. So maybe you’ll leave stabilization on, and be a <a href="https://www.instagram.com/p/BZaekQujG3T/">human crane</a>, or gimbal, or dolly, or all of the above.</p>

</div><div data-block-json="{&quot;blockAnimation&quot;:&quot;none&quot;,&quot;layout&quot;:&quot;caption-below&quot;,&quot;overlay&quot;:false,&quot;description&quot;:{&quot;html&quot;:&quot;<p class=\&quot;\&quot; style=\&quot;white-space:pre-wrap;\&quot;>Never use the 5x lens. Never use image stabilization. Never shoot through a dirty windshield. And never get this dope-ass shot.</p>&quot;},&quot;hSize&quot;:null,&quot;floatDir&quot;:null,&quot;isOldBlock&quot;:false,&quot;settings&quot;:{&quot;muted&quot;:true,&quot;autoPlay&quot;:true,&quot;loop&quot;:true,&quot;controls&quot;:&quot;none&quot;},&quot;resolvedBy&quot;:&quot;native&quot;,&quot;nativeVideo&quot;:&quot;6549c53542010b3a8b88fd32&quot;}" data-block-type="32" id="block-yui_3_17_2_1_1699332889824_51888"><p>Never use the 5x lens. Never use image stabilization. Never shoot through a dirty windshield. And never get this dope-ass shot.</p></div><div data-block-type="44" id="block-yui_3_17_2_1_1699332889824_51940"><p>Let’s be honest: If image quality is your top priority, there are much better options for your next production than a consumer telephone. You’d probably choose the iPhone for accessibility, nimbleness, ubiquity, and cost. Those are great reasons, and when you pair them with image quality that can be mistaken for high-end cinema camera footage by a veteran colorist, you’ve got something magic.</p><h2 id="give-it-to-me-in-bullets-you-long-winded-monster">Give It To Me In Bullets You Long-winded Monster</h2><p>So we may well ignore much of Apple’s implied advice, but we would do well to follow some of it if we can:</p><ul>
<li>Use camera support. Not crazy camera support, but some.</li>
<li>Use lights. Not crazy lights, but some.</li>
<li>Use a camera app that allows manual control, like Blackmagic Camera</li>
<li>Use 180° shutter, if you can (ND filters will help)</li>
<li>Keep light off of the lens using a $8,000 matte box or a bit of black tape</li>
<li>Hire literally the world’s most famous colorist. Or just do some <a href="https://prolost.com/blog/applelog">color correction</a>.</li>
<li>And most importantly, shoot in log, with a good preview <a href="https://prolost.com/blog/applelog">LUT</a></li>
</ul>
</div><div><div data-block-json="{&quot;blockAnimation&quot;:&quot;none&quot;,&quot;layout&quot;:&quot;caption-below&quot;,&quot;overlay&quot;:false,&quot;description&quot;:{&quot;html&quot;:&quot;<p class=\&quot;\&quot; style=\&quot;white-space:pre-wrap;\&quot;>The incredible production apparatus of my helicopter tunnel shot from my last post.</p>&quot;},&quot;hSize&quot;:null,&quot;floatDir&quot;:null,&quot;isOldBlock&quot;:false,&quot;settings&quot;:{&quot;muted&quot;:true,&quot;autoPlay&quot;:true,&quot;loop&quot;:true,&quot;controls&quot;:&quot;minimal&quot;},&quot;url&quot;:&quot;&quot;,&quot;resolvedBy&quot;:&quot;native&quot;,&quot;nativeVideo&quot;:&quot;6549c8ee5cdb130a5993e91d&quot;}" data-block-type="32" id="block-yui_3_17_2_1_1699333890972_22172"><p>The incredible production apparatus of my helicopter tunnel shot from my last post.</p></div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1699333890972_96598">

      

      
        <figure>
          
        
        

        
          <a href="https://prolost.com/blog/applelog">
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/e7641ddb-90bd-4831-b834-9ca32a90d580/thumbnail_05_heloTun_17_A1-SC+%2801007%29.jpg" data-image="https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/e7641ddb-90bd-4831-b834-9ca32a90d580/thumbnail_05_heloTun_17_A1-SC+%2801007%29.jpg" data-image-dimensions="1920x800" data-image-focal-point="0.5,0.5" alt="" data-load="false" src="https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/e7641ddb-90bd-4831-b834-9ca32a90d580/thumbnail_05_heloTun_17_A1-SC+%2801007%29.jpg" width="1920" height="800" sizes="100vw" srcset="https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/e7641ddb-90bd-4831-b834-9ca32a90d580/thumbnail_05_heloTun_17_A1-SC+%2801007%29.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/e7641ddb-90bd-4831-b834-9ca32a90d580/thumbnail_05_heloTun_17_A1-SC+%2801007%29.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/e7641ddb-90bd-4831-b834-9ca32a90d580/thumbnail_05_heloTun_17_A1-SC+%2801007%29.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/e7641ddb-90bd-4831-b834-9ca32a90d580/thumbnail_05_heloTun_17_A1-SC+%2801007%29.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/e7641ddb-90bd-4831-b834-9ca32a90d580/thumbnail_05_heloTun_17_A1-SC+%2801007%29.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/e7641ddb-90bd-4831-b834-9ca32a90d580/thumbnail_05_heloTun_17_A1-SC+%2801007%29.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/53f4e093e4b085e4457080e1/e7641ddb-90bd-4831-b834-9ca32a90d580/thumbnail_05_heloTun_17_A1-SC+%2801007%29.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">
                
            </p>
          </div>
        
          </a>
        

        
      
        </figure>
      

    </div></div><div data-block-type="44" id="block-yui_3_17_2_1_1699333890972_22223"><h2 id="shot-on-iphone-means-what-it-means-to-you">Shot on iPhone Means What it Means to You</h2><p>All of this is academic if you don’t go put it into practice. If you got this far and feel empowered to wring the most out of your iPhone 15 Pro with just the right amount of gear, that’s great. If you actively forget all of this and occasionally flip on the Log switch so you can play with the color of your iPhone videos in post, that’s great too.</p><p>Because here’s the thing: movies have already been shot on phones. No production’s decisions validate any camera for all other production needs. You decide what “Shot on iPhone” means to you, if anything. And the way you decide is by getting out there and shooting something.</p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1699314604996_238460">

      

      
        <figure>
          
        
        

        
          
            
          
        

        
          
          <figcaption>
            <p>I went to Peru with nothing but my iPhone 15 Pro Max. I shot some stuff with zero gear, and I’m having a blast color grading it various ways.</p>
          </figcaption>
        
      
        </figure>
      

    </div></div>
      
  <!--POST FOOTER-->
    
  
  

</article>




<!--PAGINATION-->
  

  




<!-- COMMENTS -->


      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Writing a Compiler is Surprisingly Easy (part 1) (242 pts)]]></title>
            <link>http://sebmestre.blogspot.com/2023/11/en-writing-compiler-is-surprisingly.html</link>
            <guid>38182461</guid>
            <pubDate>Tue, 07 Nov 2023 20:36:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://sebmestre.blogspot.com/2023/11/en-writing-compiler-is-surprisingly.html">http://sebmestre.blogspot.com/2023/11/en-writing-compiler-is-surprisingly.html</a>, See on <a href="https://news.ycombinator.com/item?id=38182461">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-974883917963364008">


<p>
Ever since I was a teenager I wanted to create my own systems programming
language. Such a programming language would certainly
have to be compiled to native code, which meant I'd have to write a compiler.
</p><p>
Even though I managed to write several half-working parsers, I'd always
fail at the stage of generating assembly code, as the task turned too complex.
</p><p>
In this blog I intend to show my teenage self how writing a code generator is, in fact, not
complex at all, and it can be fully done in a couple of weekends. (As long as we
make some simplifying assumptions)
</p>
 <blockquote>I will that assume you already know that <a href="https://sebmestre.blogspot.com/2023/11/en-representing-programs-within.html">representing programs within a
   program is surprisingly easy</a>
</blockquote>
<h2>Where do we even start?</h2>
<p>
The goal today is to translate a high level language to x86-64 machine code. Our
high-level language will have integer literals, variables, negation, and
addition.
</p><p>
To make our job simpler, we will translate the high level code to x86-64
assembly code, and use a pre-existing assembler (like the one inside GCC) to
translate that to machine code.
</p><p>
Let's get started. Here is how we will represent integer literals:
</p>
<pre>struct int_literal {
	int value;
};
</pre>
<p>
Compiling them is straightforward:
</p>
<pre>void compile_int_literal(struct int_literal* e) {
	printf("mov $%d, %%rax\n", e-&gt;value);
}
</pre>

<blockquote><code>printf</code> uses the <code>%</code> character to denote formatting commands, so a double <code>%%</code> is used to print the character verbatim.</blockquote>
<p>
Here I used the x86-64 <code>mov</code> instruction. This instruction moves data from one
place to another. In the notation used in this blog, the flow of data is
  left-to-right (i.e. the instruction is works as <code>mov source, destination</code>)
</p><p>
In this case we are moving an immediate value (a numeric constant written
directly in the code, notated with a <code>$</code> preceding it) into the "a" register (a
little space of data that lives directly inside the CPU, written as <code>%rax</code>)
</p>
<p>
The x86-64 <code>mov</code> instruction can be used in several ways ("addressing modes"):
immediate-to-register (the one we just used), register-to-memory, memory-to-register,
register-to-register, etc. We will see some of these later on.
</p><p>
I want to note that, at this point, many design decisions have already been made.
Most notably:
</p>
<ul>
  <li>Our compiler will output AT&amp;T-style assembly code to <code>stdout</code>, using <code>printf</code></li>
<li>The result of compiling an expression will be assembly code that computes and
  stores the value of that expression in <code>%rax</code></li>
</ul>
<p>
The last point is in no small part responsible for making the compiler so
simple, but it also means that the code we'll generate is not very efficient.
</p><p>
For the initiated, some obvious things that we could add to mitigate this are a
peephole optimizer and a register allocator. But these would force us to
implement some intermediate representation and make our compiler a lot more
complicated, so we are not going to.
</p><p>
Let's move on.
</p>
<h2>How can we implement variables?</h2>
<p>
The way we will represent variables in our compiler is with an integer. Yes, not
with a string that corresponds to the name of the variable, but an index that
represents where that variable will live in memory. Indeed, all variables will
be stored in our computer's main memory. In particular, they will live in a
  place called <i>the stack</i>.
</p>

<p>
The stack is a wonderful mechanism that allows us to quickly store some values
in memory and safely discard them once we don't need them. In memory, it is
simply laid out like an array of 64 bit integers.
</p><p>
The way it works is extremely simple: the CPU has a register -- known as the
stack pointer <code>%rsp</code> -- that always tells us the first slot in the stack that is
available to be used. Every slot after will be free, and every slot before will
be occupied. As with everything low-level, this doesn't happen magically, and we
will be responsible for keeping that register up to date.
</p><p>
We are going to be a bit naughty and not update the stack pointer for now. This
is mostly okay as long as our code does not perform any function calls.
</p><p>
With this in mind, we can represent variables using an index that tells us how
many slots *after* the first available slot the variable is stored.
</p>

<pre>struct variable {
	int slot;
};
</pre>

<blockquote>In a more complete compiler there would be a previous step that takes care of
  assigning a stack slot to each declared variable, and translating variable names to stack
  slots. We wouldn't expect the language user to type in the slots by hand.</blockquote>
<p>
Compiling them is simple enough, but we do need to learn some more x86-64
assembly to fully grasp it.
</p>
<pre>void compile_variable(struct variable* e) {
	int slot = e-&gt;slot;
	printf("mov %d(%%rsp), %%rax\n", -8 * (slot + 1));
}
</pre>
<p>
Here we use a different version of the <code>mov</code> instruction. This one looks like
<code>mov number(register1), register2</code>, and it means "take the value at address
<code>number+register1</code>, and store it in <code>register2</code>". (mem-to-reg mode)
</p><p>
This is how we implement access into the stack. The stack pointer holds the
address of the first free slot, and we add an offset to to access the slot that
holds the desired variable.
</p><p>
Besides that, there is a funny looking bit of math up there. There are two
things to know about the stack:
</p>
<ul>
<li>each slot holds a 64-bit value. This equals 8 bytes, which is why we multiply
  by 8</li>
<li>the stack grows downwards. This means that the occupied slots are at higher
  addresses than the free slots. This explains why the 8 is negative instead of
  positive</li>
<li>there is a +1 in there because I lied. The stack pointer actually points at the last
  occupied slot, instead of the first free slot.</li>
</ul>
<h2> Negation</h2>
<p>
The representation of negation is similarly straightforward, and should be
apparent to those who read the previous blog.
</p>
<pre>struct negation {
	struct expression* target;
};
</pre>

<blockquote>I will omit the definition of <code>struct expression</code> for now, as it would
distract from the main points of this section.</blockquote>
<p>
To compile negations we will take advantage of the fact that compiling an
expression will produce code that stores its result in <code>%rax</code>. This means that
if we emit some code that negates <code>%rax</code> right after the code that computes the
value to be negated, we will have succesfully computed the desired negation.
</p><p>
Luckily, x86-64 has an instruction that does just this.
</p>
<pre>void compile_negation(struct negation* e) {
	compile_expression(e-&gt;target);
	printf("neg %%rax\n");
}
</pre>
<p>
But just to get used to the kind of puzzles we have to solve to compile some
more complicated operations, let's avoid using <code>neg</code>. x86-64 has a <code>sub</code>
instruction that subtracts one operand from another. Let's try using that.
</p><p>
In particular, let's:
</p>
<ul>
  <li>compute the target expression (leaving the result in <code>%rax</code>)</li>
  <li>put a zero into <code>%rcx</code></li>
  <li>substract <code>%rax</code> from <code>%rcx</code> (leaving the negated result in <code>%rcx</code>)</li>
  <li>put the negated result in <code>%rax</code></li>
</ul>
<pre>void compile_negation(struct negation* e) {
	compile_expression(e-&gt;target);
	printf("mov $0, %%rcx\n");
	printf("sub %%rax, %%rcx\n");
	printf("mov %%rcx, %%rax\n");
}
</pre>
<blockquote><p>
We use <code>%rcx</code> instead of <code>%rbx</code> because <code>%rbx</code> is a callee-saved register in
x86-64, meaning we are not allowed to use it without first storing its old
content in the stack, and later restoring its value. This is mandated by the x86-64 calling conventions.
</p><p>
This is too cumbersome, so we just use the nearest non-callee-saved register.
</p></blockquote>
<p>
While not too complicated, this ilustrates the kind of hoops we will sometimes
have to jump through in order to compile more advanced operations. I don't think
this makes compiling hard, but it can get pretty tedious.
</p>
<blockquote>Also, note the use of the reg-to-reg addressing mode</blockquote>

<h2>Addition</h2>
<p>
Like in the previous blog, we will represent additions as follows:
</p>
<pre>struct addition {
	struct expression* left_term;
	struct expression* right_term;
};
</pre>
<p>
The general idea here will be to first compile the left term, then the right
term, then emit an <code>add</code> instruction that adds their results. The main problem
that arises is that the result of the the left term will be lost while we
compute the right term.
</p><p>
A simple fix might be to move the result to a different register, like this:
</p>
<pre>void compile_addition(struct addition* e) {
	compile_expression(e-&gt;left_term);
	printf("mov %%rax, %%rcx\n");
	compile_expression(e-&gt;right_term);
	printf("add %%%rcx, %%rax\n");
}
</pre>


  
<p>
Unfortunately, this will not work when the right term also stores something in
<code>%rcx</code> in an intermediate step. Instead, we will get some help from to our good
friend, the stack.
</p><p>
We will store that intermediate value in the stack, and read it back after the
right term is done computing. If we take care that the code we generate for the
right term doesn't write into the same stack slot, then we can be sure that the
value will be preserved.
</p><p>
The mechanism to prevent re-using the same stack slot is a simple counter. Since
we also store variables in the stack, its value must be greater than any slot
that's been assigned to a variable. For now let's just initialize it with some
large number, like 10.
</p>
<pre>int temp_counter = 10;
void compile_addition(struct addition* e) {
	compile_expression(e-&gt;left_term);
	int slot = temp_counter++; // allocate a new slot
	printf("mov %%rax, %d(%%rsp)\n", -8 * (slot + 1));
	compile_expression(e-&gt;right_term);
	printf("add %d(%%rsp), %%rax\n", -8 * (slot + 1));
	temp_counter--; // restore the counter
}
</pre>

<blockquote>Here we finally see a <code>mov</code> that uses reg-to-mem addressing mode. Also, note that <code>add</code>
can also use mem-to-reg mode.
</blockquote>
<h2>Putting it All Together</h2>
<p>
The final piece of the puzzle, for now, is the <code>struct expression</code> data type,
and its corresponding <code>compile_expression</code> function. These are pretty much
trivial given what we've seen so far, but I'll type them out for completeness'
sake.
</p>
<pre>enum expression_tag {
	EXPRESSION_INT_LITERAL,
	EXPRESSION_VARIABLE,
	EXPRESSION_NEGATION,
	EXPRESSION_ADDITION,
};
struct expression {
	enum expression_tag tag;
	union {
		struct int_literal as_int_literal;
		struct variable as_variable;
		struct negation as_negation;
		struct addition as_addition;
	};
};

void compile_expression(struct expression* e) {
	switch (e-&gt;tag) {
	case EXPRESSION_INT_LITERAL:
		compile_int_literal(&amp;e-&gt;as_int_literal);
		break;
	case EXPRESSION_VARIABLE:
		compile_variable(&amp;e-&gt;as_variable);
		break;
	case EXPRESSION_NEGATION:
		compile_negation(&amp;e-&gt;as_negation);
		break;
	case EXPRESSION_ADDITION:
		compile_addition(&amp;e-&gt;as_addition);
		break;
	}
}
</pre>

<h2>Testing it out</h2>
<p>
At this point, we are capable of compiling simple arithmetic expressions. To
test this out, we can write a small program like this one:
</p>
<pre>int main() {
	// var0 + (-var1 + 42)
	struct expression* e =
		addition(
			variable(0),
			addition(
				negation(variable(1)),
				int_literal(42)));
	compile_expression(e);
}
</pre>

<blockquote><code>int_literal</code>, <code>variable</code>, <code>negation</code> and <code>addition</code> are some helpers that
build up the corresponding expressions.</blockquote>
<p>
Which produces the following output:
</p>
<pre>mov -8(%rsp), %rax
mov %rax, -88(%rsp)
mov -16(%rsp), %rax
neg %rax
mov %rax, -96(%rsp)
mov $42, %rax
add -96(%rsp), %rax
add -88(%rsp), %rax
</pre>

<p>
Then, to be able to run it, we can just add some assembly that will take two
arguments and store them in stack slots 0 and 1, and return after executing the
code.
</p>
<pre>.global foo
foo:
	mov %rdi, -8(%rsp)
	mov %rsi, -16(%rsp)

	mov -8(%rsp), %rax
	mov %rax, -88(%rsp)
	mov -16(%rsp), %rax
	neg %rax
	mov %rax, -96(%rsp)
	mov $42, %rax
	add -96(%rsp), %rax
	add -88(%rsp), %rax

	ret
</pre>

<p>
Finally, we hook into this code from C, and check that it returns the right thing:
</p>
<pre>#include &lt;stdio.h&gt;
#include &lt;stdint.h&gt;
int64_t foo(int64_t a, int64_t b);
int main() {
	for (int i = 0; i &lt; 10; ++i) {
		for (int j = 0; j &lt; 10; ++j) {
			printf("expected: %d, got: %ld\n", i-j+42, foo(i, j));
		}
	}
}
</pre>

<p>
To do this we compile using GCC and run it in a terminal:
</p>
<pre>$ gcc main.c foo.s -o main
$ ./main
expected: 42, got: 42
expected: 41, got: 41
expected: 40, got: 40
... and so on ...
</pre>

<h2> Conclusion</h2>
<p>
Writing a compiler is not as hard as it seems if we are willing to keep it
simple. If we avoid introducing complexity ourselves, its main source is
understanding the target architecture, and not the compilation process itself.
</p><p>
In the next parts we will look at how to compile classic control flow constructs
such as <code>if</code> and <code>while</code>, as well as function calls and pointers.
  </p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA['Ray, this is a religion': How Bridgewater lost two top hires (185 pts)]]></title>
            <link>https://nymag.com/intelligencer/article/ray-dalio-rob-copeland-the-fund-book-excerpt.html</link>
            <guid>38181408</guid>
            <pubDate>Tue, 07 Nov 2023 19:15:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nymag.com/intelligencer/article/ray-dalio-rob-copeland-the-fund-book-excerpt.html">https://nymag.com/intelligencer/article/ray-dalio-rob-copeland-the-fund-book-excerpt.html</a>, See on <a href="https://news.ycombinator.com/item?id=38181408">Hacker News</a></p>
<div id="readability-page-1" class="page"><section data-editable="main" data-track-zone="main">  <article role="main" data-track-type="article-detail" data-uri="nymag.com/intelligencer/_components/article/instances/clon9g6bj001v0pge3mlcivcl@published" data-content-channel="Business" data-crosspost="" data-type="Book Excerpt" data-syndication="original" data-headline="‘Ray, This Is a Religion’" data-authors="Rob Copeland" data-publish-date="2023-11-07" data-tags="chapters, ray dalio, the money game, wall street, money, book excerpt, remove interruptions" data-issue-date="" data-components-count="75" data-canonical-url="http://nymag.com/intelligencer/article/ray-dalio-rob-copeland-the-fund-book-excerpt.html">
    


  
  
  
  <header>
    <div>
      <div>
          
            <h2 data-editable="displayTeaser">How the world’s largest hedge fund lost two top hires — and was paralyzed by puddles of pee.</h2>
            

            
        </div>
          <div>
            <p>
              Ray Dalio is the founder of the Bridgewater Associates, the world’s largest hedge fund.
              <span>Photo-Illustration: Intelligencer; Photos: Getty</span>
            </p>
          </div>
    </div>
      <div data-editable="lede">
          <picture> <source media="(min-resolution: 192dpi) and (min-width: 1180px), (-webkit-min-device-pixel-ratio: 2) and (min-width: 1180px)" srcset="https://pyxis.nymag.com/v1/imgs/31f/e60/adfb533a3933c603006dbe63dd54dc70bd-ray-dalio.2x.rhorizontal.w1100.jpg 2x" width="1100" height="733"> <source media="(min-width: 1180px) " srcset="https://pyxis.nymag.com/v1/imgs/31f/e60/adfb533a3933c603006dbe63dd54dc70bd-ray-dalio.rhorizontal.w1100.jpg" width="1100" height="733"> <source media="(min-resolution: 192dpi) and (min-width: 768px), (-webkit-min-device-pixel-ratio: 2) and (min-width: 768px)" srcset="https://pyxis.nymag.com/v1/imgs/31f/e60/adfb533a3933c603006dbe63dd54dc70bd-ray-dalio.2x.rhorizontal.w1100.jpg 2x" width="1100" height="733"> <source media="(min-width: 768px)" srcset="https://pyxis.nymag.com/v1/imgs/31f/e60/adfb533a3933c603006dbe63dd54dc70bd-ray-dalio.rhorizontal.w1100.jpg" width="1100" height="733"> <source media="(min-resolution: 192dpi), (-webkit-min-device-pixel-ratio: 2)" srcset="https://pyxis.nymag.com/v1/imgs/31f/e60/adfb533a3933c603006dbe63dd54dc70bd-ray-dalio.2x.rhorizontal.w1100.jpg" width="1100" height="733"> <img src="https://pyxis.nymag.com/v1/imgs/31f/e60/adfb533a3933c603006dbe63dd54dc70bd-ray-dalio.rhorizontal.w1100.jpg" data-content-img="" width="1100" height="733" fetchpriority="high"> </picture>
          </div>
        <p>
            Ray Dalio is the founder of the Bridgewater Associates, the world’s largest hedge fund.
          <span>Photo-Illustration: Intelligencer; Photos: Getty</span>
        </p>
  </header>
  <section>
    <div data-editable="content">
      <div>
          <div>
            <picture> <source media="(min-resolution: 192dpi) and (min-width: 1180px), (-webkit-min-device-pixel-ratio: 2) and (min-width: 1180px)" srcset="https://pyxis.nymag.com/v1/imgs/31f/e60/adfb533a3933c603006dbe63dd54dc70bd-ray-dalio.2x.rhorizontal.w1100.jpg 2x" width="1100" height="733"> <source media="(min-width: 1180px) " srcset="https://pyxis.nymag.com/v1/imgs/31f/e60/adfb533a3933c603006dbe63dd54dc70bd-ray-dalio.rhorizontal.w1100.jpg" width="1100" height="733"> <source media="(min-resolution: 192dpi) and (min-width: 768px), (-webkit-min-device-pixel-ratio: 2) and (min-width: 768px)" srcset="https://pyxis.nymag.com/v1/imgs/31f/e60/adfb533a3933c603006dbe63dd54dc70bd-ray-dalio.2x.rhorizontal.w1100.jpg 2x" width="1100" height="733"> <source media="(min-width: 768px)" srcset="https://pyxis.nymag.com/v1/imgs/31f/e60/adfb533a3933c603006dbe63dd54dc70bd-ray-dalio.rhorizontal.w1100.jpg" width="1100" height="733"> <source media="(min-resolution: 192dpi), (-webkit-min-device-pixel-ratio: 2)" srcset="https://pyxis.nymag.com/v1/imgs/31f/e60/adfb533a3933c603006dbe63dd54dc70bd-ray-dalio.2x.rhorizontal.w1100.jpg" width="1100" height="733"> <img src="https://pyxis.nymag.com/v1/imgs/31f/e60/adfb533a3933c603006dbe63dd54dc70bd-ray-dalio.rhorizontal.w1100.jpg" data-content-img="" width="1100" height="733" fetchpriority="high"> </picture>
          </div>
            <div>
              <p>
                  Ray Dalio is the founder of the Bridgewater Associates, the world’s largest hedge fund.
                <span>Photo-Illustration: Intelligencer; Photos: Getty</span>
              </p>
            </div>
              </div>
            <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9g6bj001u0pgetusj4kt7@published" data-word-count="5"><em>There’s piss on the floor</em>.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka2x001s3b82gi87hayl@published" data-word-count="36">So read the email from Ray Dalio, founder of the world’s largest hedge fund, Bridgewater Associates, and a billionaire many times over. It would be read by more than 1,000 of his underlings at the company.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka2x001t3b8268r85lja@published" data-word-count="44">Everyone at Bridgewater would soon learn the backstory. Dalio had excused himself from a meeting and walked to the nearest restroom at Bridgewater’s sprawling, medieval-stone-style headquarters near Westport, Connecticut. After relieving himself at the urinal, Dalio glanced down. There was piss on the floor.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka2y001u3b82qgcq4wfw@published" data-word-count="19">This couldn’t be allowed to go on, Dalio said. Whose was it? And who had permitted it to happen?</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka2y001v3b82l2ejqxw6@published" data-word-count="13">“If people can’t aim their fucking pee, they can’t work here,” Dalio proclaimed.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka2y001w3b82m5a4jrzf@published" data-word-count="135">It seemed inconceivable that it could be an accident. After all, Dalio ruled Bridgewater under what he called his “Principles,” which held that no matter was too small for investigation. There were hundreds of Principles, including one that stated, “People have to value getting at truth so badly that they are willing to humiliate themselves to get it.” Employees were instructed to “probe” one another’s work, as Dalio put it, on a daily basis. That wasn’t all. As part of what Dalio called “radical transparency,” a team of videographers and editors made tapes and videos of Dalio’s many lessons to his staff. These were uploaded into what Bridgewater called its “Transparency Library,” viewable internally. All employees at Bridgewater — from top investment strategists to receptionists — were required to view cases pulled from this library.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka30001x3b82d4s2yjck@published" data-word-count="94">That puddle of urine on the bathroom would be the subject of one of these cases. Dalio summoned the hedge fund’s head of facilities for questioning. Staff were assigned to a rotating guard, standing outside the restroom to take notes on all who entered — and whether they left clean floors. After each visitor, a member of the cleaning crew would rush in to mop. New urinals were brought in for testing. Stickers were applied to the porcelain to give men a more effective target. Then the exact placement of the stickers was probed.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka30001y3b82bw4l6574@published" data-word-count="17">For all that, Dalio was never able to figure out exactly what had happened at that urinal.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka31001z3b82oynyqq0l@published" data-word-count="45">There’s plenty of weird behavior in the business world, from Mark Zuckerberg’s glamour-shot hydrofoiling with an American flag on Independence Day to Goldman Sachs CEO David Solomon’s boasting to his underlings that he — and only he — had received a blowjob the night before.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3100203b825aaxzgm5@published" data-word-count="7">And then there is Dalio and Bridgewater.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3100213b82y3xalxn9@published" data-word-count="86">Since founding Bridgewater nearly 50 years ago, Dalio has become one of the world’s richest investors (Forbes pegs his net worth at roughly $20 billion). He is widely credited with having predicted and profited from the 2008 financial crisis, which made him famous. Yet the Greenwich, Connecticut, resident professes to be seeking something more important than the mere accumulation of wealth. His main interest, as he wrote in his best-selling autobiography-<em>cum–</em>self-help book, <em>Principles: Life and Work,</em> is to lead others toward “meaningful lives” and “meaningful relationships.”</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3100223b820vdv01oe@published" data-word-count="134">Core to that effort is Dalio’s narrative that all Bridgewater employees are on an equal playing field and that any difference in rank or authority is due only to a rigorous system that susses out merit. That system involves employees constantly rating one another, taking real-time polls in meetings on whether a speaker is right or wrong, voting on whom to hire and fire, and being quizzed on internal case studies and investigations. This corporate culture, featuring a strict and overarching dogma created and enforced by a charismatic leader, has been repeatedly compared to a cult. It’s a comparison Dalio rejects. As he told one interviewer, “It’s the opposite of a cult. It’s independent thinking. It’s you knowing that you have the right and obligation to make sense of everything. It is a culture.”</p>

  <div data-uri="nymag.com/intelligencer/_components/image/instances/clonhmnna000o3b807jsx12yb@published" data-editable="settings">
    
    <p>
      Ray Dalio in 2019.
      <span>Photo: David Paul Morris/Bloomberg via Getty Images</span>
    </p>
</div>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3200233b82qk2311t0@published" data-word-count="44">Dalio declined to be interviewed for <a href="https://www.amazon.com/Fund-Bridgewater-Associates-Unraveling-Street-ebook/dp/B0BST4LN26?tag=thestrategistsite-20&amp;ascsubtag=__in1107aam__clon9g6a4001c0pge17h79oez________________" rel="sponsored,nofollow" data-track-type="product-link">the book from which this article is adapted</a>. This excerpt is based on dozens of interviews with current and former Bridgewater employees, as well as on copies of emails, recordings, internal company documents, and published interviews and articles.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3f00243b82y0hxmkn2@published" data-word-count="24">Dalio’s lawyer said the Bridgewater founder “treated all employees equally, giving people at all levels the same respect and extending them the same perks.”</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3f00253b82hbtpad1r@published" data-word-count="103">While Dalio is hardly the first financier to develop a taste for the spotlight, there’s no doubt that he’s been fabulously successful at selling the gospel of Ray. Over the past ten years, he has become world-famous, appearing on major broadcast and cable networks, popular podcasts, and magazine covers and starring in TED Talks. Inside Bridgewater, Dalio has bragged that Bill Gates, Elon Musk, and Jack Dorsey are all interested in his Principles (none of those three would comment). He has said that Sean “Diddy” Combs asked to be personally mentored. He was interviewed by Gwyneth Paltrow, who suggested he run for president.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3g00263b829bci5lt4@published" data-word-count="22">The truth is that at Bridgewater, some are more equal than others. And Ray Dalio is the most equal one of all.</p>

  

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3g00283b82o0vt6o0a@published" data-word-count="99">The piss case, as it was known internally, was merely a small example of the expansive Bridgewater employee-rating system in action. Staffers were given iPads and directed to rank one another on a one-to-ten scale on their performance dozens of times per day in categories derived from Dalio’s Principles, such as “ability to self-assess” and “pushing through to results.” Dalio would often review the results in his office, where the Bridgewater founder would lean back in his chair, chewing on Scotch tape, as was his habit when he was concentrating, and review the ever-changing ratings of those under him.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3h00293b82yeexhqry@published" data-word-count="112">The goal of all the data, Dalio would say, was to sort everyone at Bridgewater on a single scale. At Bridgewater, the most important assessment was “believability,” a score that was applied to each category. If one was judged highly believable in “pushing through to results,” for instance, then his or her ratings of other people in the same category would be counted more heavily. Dalio called this “believability weighting,” and in virtually all important categories, Dalio expected that his rating would be the highest, or close to it, at the firm. That status essentially gave him the final word, no matter how many others at Bridgewater might have disagreed with him.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3h002a3b82xlxllq17@published" data-word-count="112">For more than a decade, at a cost that would balloon to over $100 million, Bridgewater attempted to develop secret software to expand the Principles into what employees widely described as a computerized version of Dalio himself. One iteration carried the nickname “Prince,” short for Principles. Prince was to be the equivalent of Siri, the voice-activated assistant on Apple products. Just as billions of consumers around the world spoke to Siri to get the answers to their queries, so, too, would Prince be the singular source for answers according to the Principles and highly believable employees. The product was first intended for use inside Bridgewater, and then, Dalio hoped, by the world.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3i002b3b822gm0hs62@published" data-word-count="70">For his star creation, Dalio hunted for a star technologist to bring it to life. Near the end of 2012, he found one in computer scientist David Ferrucci, as close to a celebrity as there was at that time in the field of artificial intelligence. Ferrucci had worked at IBM and led the team that created Watson, a question-answering AI system that made headlines by beating human competitors on <em>Jeopardy!.</em></p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3i002c3b82t5prfv5z@published" data-word-count="111">Ferrucci said publicly that he was enthusiastic to join Bridgewater to help predict the direction of financial markets. This made perfect sense — any other hedge fund would have put Ferrucci to work on investments. Instead, Bridgewater put Ferrucci, who reported to Dalio’s deputy Greg Jensen, in charge of a new team that operated in secret and went by a strange name, the Systematized Intelligence Lab. Its main task was to add artificial intelligence to Prince and the other Principles rating tools. Few who weren’t a part of his group were let into the lab and almost no one else at the hedge fund had a clue what was happening there.</p>

  <div data-uri="nymag.com/intelligencer/_components/image/instances/clonhonw1000v3b80iwmhkogb@published" data-editable="settings">
    
    <p>
      David Ferrucci, creator of IBM’s Watson and high-profile Dalio hire, in 2017.
      <span>Photo: Alex Flynn/Bloomberg via Getty Images</span>
    </p>
</div>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3j002d3b82odh5fg4w@published" data-word-count="136">Roughly one year into Ferrucci’s tenure at Bridgewater, Dalio burst into the hedge fund’s headquarters full of excitement. It was just after Christmas 2013, and the night before he had seen a new movie, the Oscar-nominated <em>Her</em>, about a forlorn single man, played by Joaquin Phoenix, who falls in love with his Siri-like virtual assistant, voiced by Scarlett Johansson. Dalio, too, had apparently been taken by Johansson’s sultry work. “Let’s get her,” Dalio said. “We’ll pay her to be the voice of Prince!” But Bridgewater’s staff couldn’t get Johansson’s representatives to return their calls. There was a tantalizing mystery to what was going on inside Ferrucci’s lab. Some of the few with access were consultants from the venture-capital billionaire Peter Thiel’s data-crunching firm, Palantir, famed as the tech spooks who helped hunt down Osama bin Laden.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3k002e3b82kn2jjvnb@published" data-word-count="84">What was becoming obvious to Ferrucci and his staff, however, was that Prince — and the Principles — were a mess. Ferrucci told colleagues he couldn’t figure out where to begin applying hard science to the rating tools. Before Ferrucci’s arrival, it seemed there had been no double-blind tests, no anonymous surveys — not even a simple regression analysis to show that the adoption of the Principles’ methods led to better results. (“I don’t believe in regressions,” Dalio told one employee who suggested it.)</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3k002f3b82mk9js5g7@published" data-word-count="267">Bridgewater’s most important results, the returns of its key hedge funds, seemed to even suggest the opposite: The more time that people spent on the Principles — and its associated arguments, votes, case studies, and rating experiments — the worse the company’s investments seemed to perform. In the six years following the financial crisis, Bridgewater’s main fund, Pure Alpha, turned in two good years and four poor ones.Still, Ferrucci tried to bring Dalio’s vision for the Principles rating tools to life. He modeled his approach on how IBM created Watson. The first step had been to gather a raw trove of real-world knowledge needed to produce answers. IBM engineers seeded Watson with millions of documents, newspaper articles, and books. Ferrucci and his team grappled with what an equivalent process might look like for a program grounded in the Principles. The goal was for Ferrucci’s software to be able to listen in real time to discussions at Bridgewater and identify which Principles needed to be applied at any given moment. He laid out a list of attributes in Dalio’s Principles and looked them up in the dictionary. “What does creativity actually mean?” he asked. The answer, from Merriam-Webster was “the ability to create.” He looked up another attribute: lateral thinking. The definition, “A method for solving problems by making unusual or unexpected connections between ideas.” Ferrucci and his team found that many of the various attributes seemed to bleed into one another. Long periods of time were spent on futile attempts to define them in a manner that was workable in a software program. It was a slog.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3o002g3b82b5awzrh1@published" data-word-count="49">(Ferrucci declined to be interviewed. He denied that he used a dictionary to look up definitions of personality attributes. His spokeswoman wrote in an email about his work for Bridgewater, “Quantitative survey data was used to find statistically significant differentiation using industry-standard statistical methods.” Asked to elaborate, she declined.)</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3p002h3b82mxf91cg1@published" data-word-count="101">Ferrucci tried a different way. Dalio was the inventor of the Principles — surely he could tell the different attributes apart. Ferrucci decided to seed the software system with Dalio’s own definitions of the terms. Ferrucci’s team pulled hundreds of hours of videos from the Transparency Library, Bridgewater’s repository of meeting recordings, and tried to track patterns in when Dalio cited specific Principles. Ferrucci’s lab employees also went through years of Bridgewater’s old management-training videos — the same ones that all staffers were tested on, including the piss case — and painstakingly noted which Principles were used, and in what context.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3p002i3b822zo8sh92@published" data-word-count="86">From all this effort, the researchers created a series of word clouds, intending to show whether employees who used certain language tended to rate highly in one attribute or another. More broadly, Ferrucci hoped to train a computer to read or listen to a passage of text and realize that, if certain words were used in a certain order, the topic at hand dealt with one Principle or another. If the scientists could nail the method, they could essentially create a computerized version of Dalio himself.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3q002j3b82dksxh80b@published" data-word-count="54">The goal proved elusive. Ferrucci’s team could find no pattern to predict when the Bridgewater founder — or anyone else at Bridgewater — would bring up one Principle or another. The employee ratings also showed little underlying logic. Ferrucci shared with colleagues a gradual awakening: Dalio’s system might not be a system at all.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3q002k3b82rxtujb5b@published" data-word-count="32">A few days before Christmas 2014, roughly two years into Ferrucci’s tenure, Dalio called a meeting with him and a host of others from the lab to review progress on their work.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3q002l3b82dv2nvvf6@published" data-word-count="69">The rating system was still a work in progress, but Dalio had access to a prototype that graded more than 1,000 employees in dozens of categories. Dalio began pointing out problems with the ratings of specific Bridgewater employees. Some were rated too highly, Dalio said, which must have been an error in the calculations — the Bridgewater founder knew these individuals to be less competent than the numbers indicated.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3q002m3b82e6zzbdo8@published" data-word-count="20">Ferrucci sat quietly. Midway through his remarks, Dalio stopped and said, “I’m giving you direction. You’re not writing it down.”</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3r002n3b82m2njwp7x@published" data-word-count="7">“I’m taking in your input,” replied Ferrucci.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3r002o3b82q1b8k4y2@published" data-word-count="8">Dalio cocked his head. “You work for me.”</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3r002p3b82ea0qn5lr@published" data-word-count="4">“I work for Greg.”</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3r002q3b82jh05705m@published" data-word-count="38">“No, you work for me.” Dalio waved his hands in a flutter of frustration. He turned to the dozen or so people in the room. “How many people think that Dave is looking at this the right way?”</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3r002r3b828htodoph@published" data-word-count="29">One of Dalio’s assistants typed the prompt into her iPad, creating an instant poll for those in the room. The verdict was that Ferrucci was looking at it wrong.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3r002s3b820m1cmt46@published" data-word-count="35">Dalio continued, pointing out how unlikely it was that certain people could be rated in one way or another in certain attributes. He gave his list of tweaks: “This is what I want to see.”</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3r002t3b82f6ftztik@published" data-word-count="13">Ferrucci’s voice came back, barely above a whisper. “That’s not a valid algorithm.”</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3r002u3b82pl5ve051@published" data-word-count="4">Dalio cocked his head.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3r002v3b82r56tvus2@published" data-word-count="30">Ferrucci, voice quavering, said, “That’s not scientific, Ray.” The team couldn’t just make changes based on Dalio’s whims. “It’s not how I work. I can’t just take direction like this.”</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3r002w3b820fo3u9c4@published" data-word-count="57">The room seemed to shift. The group had seen Dalio tear into countless underlings before, but here he was on unfamiliar ground. He wasn’t in a position to argue computer science with his hand-picked expert. After the meeting, Ferrucci went from the meeting to Bridgewater’s parking lot visibly brimming with anger. Many present assumed he would quit.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3s002x3b82fl6rahz1@published" data-word-count="24">Ferrucci’s spokeswoman said the scientist and Dalio “would regularly have disagreements and would openly debate … always respecting each other and their respective opinions.”</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3s002y3b824dlcxc79@published" data-word-count="42">Bridgewater and Dalio declined to comment on the friction. In a joint statement, they said Dalio “has always believed that there are many dimensions to every person” and wanted “to develop an objective, measurable way to assess individuals’ relative strengths and weaknesses.”</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3s002z3b82tvfo9dmd@published" data-word-count="81">The prospect of Ferrucci leaving the firm was treated as a five-alarm fire. Besides Dalio, he was the best-known Bridgewater employee. Also, quantitative investing was the wave of the future, and any rival hedge fund would have been thrilled to swoop in and hire Ferrucci. Bridgewater didn’t want to have to explain to its clients and the world that it had lost the IBM Watson inventor — particularly if the reason was that Ferrucci judged Dalio’s management system to be hogwash.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3s00303b82hykujvs6@published" data-word-count="54">The day after Christmas, Ferrucci received a note that Dalio wanted to chat again, this time by phone, according to two people briefed on the call afterward. There would be no crowd around a table, and no recording for everyone to listen to. The two men could speak frankly about how to move forward.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3t00313b8260cwjao8@published" data-word-count="10">Ferrucci told Dalio that he couldn’t do the ratings anymore.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3t00323b82bd4br221@published" data-word-count="11">Dalio asked what could get the scientist to change his mind.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3t00333b823n1cvzln@published" data-word-count="102">Ferrucci was prepared for that question. He had an idea for a start-up company, Elemental Cognition, completely unconnected to Bridgewater or investing. Elemental Cognition would use the technology behind Watson to teach computers to understand common sense, human intuition. This could not be achieved merely by trawling the internet for knowledge or reading every book in existence. Elemental Cognition’s goal required the mastery of fundamental concepts such as time, causality, and social interaction. It would require expensive, advanced supercomputing and the services of a slew of Ph.D. researchers. All of this would be expensive, with no guarantee that anything would work out.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3t00343b82h3knbxq9@published" data-word-count="47">Dalio said that he would fund it. He offered tens of millions of dollars from Bridgewater, on one condition. Ferrucci could spend only half his time on his dream company. The other half of the time, Ferrucci would pursue the work that Dalio wanted him to pursue.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3u00353b82aut6anp6@published" data-word-count="4">Ferrucci took the deal.</p>

  

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3u00373b82xvey3jnf@published" data-word-count="61">With Ferrucci now on the equivalent of part-time status, Dalio hunted for more big-name technology help. In May 2016, he hired Steve Jobs’s former lieutenant, Jon Rubinstein, who at Apple had been nicknamed the Podfather for helping create the first iPod. In exchange for compensation of as much as $50 million over the first two years, Rubinstein became Bridgewater’s co-chief executive.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3u00383b828u2dxlp7@published" data-word-count="73">“Technology is pervasively important at Bridgewater, especially since one of our major strategic initiatives in the coming years is to continue building out the systemized decision-making that has been so successful in our investment area and to extend it to our management as well,” Dalio wrote to clients in announcing the new hire. Rubinstein’s phone quickly blew up with well-wishers congratulating him on landing a prestigious gig at the world’s largest hedge fund.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3u00393b82hudv36my@published" data-word-count="74">It didn’t take long for Rubinstein to start having doubts. In his first days, Rubinstein sat, befuddled, as a procession of Bridgewater staffers drilled him on the Principles. They showed him slides and videos of employees being investigated and probed by their superiors, including Dalio. Rubinstein was immediately turned off. His former boss Jobs had a well-earned reputation for being controlling, but the Apple founder never claimed it was due to any high-brow philosophy.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3v003a3b82eskhqbgm@published" data-word-count="61">Rubinstein, who declined to comment, was cognizant of everything he’d heard about the Bridgewater founder’s love of raw honesty and decided to tell Dalio what was on his mind: “You’ve got 375 Principles. Those aren’t principles. Toyota has 14 principles. Amazon has 14 principles. The Bible has ten. Three hundred and seventy-five can’t possibly be principles. They are an instruction manual.”</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3v003b3b82hqpgw0fa@published" data-word-count="12">Dalio told him he simply needed more time to understand it all.</p>

  <div data-uri="nymag.com/intelligencer/_components/image/instances/clonhq8y400123b808nfypnj7@published" data-editable="settings">
    
    <p>
      Jon Rubinstein — aka the Podfather — in 2011.
      <span>Photo: Noah Berger/Bloomberg via Getty Images</span>
    </p>
</div>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3v003c3b82p8i08xh4@published" data-word-count="79">A few months into his gig, Rubinstein was beginning to wonder aloud if there was a Principles system at all. The hedge fund was spending tens of millions of dollars on developing the Principles rating software, but when Rubinstein tried to investigate what it actually did, he was simply told that it measured “believability.” Many told him the systems involved secret calculations from the former IBM scientist Ferrucci and that the researcher told almost no one how it worked.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3w003d3b82uz7tnsx9@published" data-word-count="47">This immediately struck the former Apple executive as off. He had spent his life working with experts like Ferrucci, and in Rubinstein’s experience, corporate scientists tended to overexplain their work to company executives in excruciating, unnecessary detail. The struggle was usually to get them to shut up.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3w003e3b82xasqmu7d@published" data-word-count="29">Rubinstein was no junior hire, so he sought out Ferrucci in person. The two men exchanged pleasantries, then the co-CEO dropped his biggest question: “How do you calculate believability?”</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3w003f3b82fnvixl52@published" data-word-count="11">Ferrucci briefly broke eye contact. “I’m not going to tell you.”</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3w003g3b82loolfv91@published" data-word-count="1">“Why?”</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3w003h3b829x984n1v@published" data-word-count="2">“I’m embarrassed.”</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3w003i3b82d7wt30un@published" data-word-count="8">(Ferrucci’s spokeswoman said he doesn’t recall this conversation.)</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3w003j3b82ah5vytim@published" data-word-count="72">Rubinstein had been vexed from the start by the mysterious, omnipotent metric of “believability.” Ferrucci’s lab, Rubinstein would conclude, was wasting their time. Those at Bridgewater who had been rated highly believable in certain areas didn’t earn those scores through any complicated algorithm of artificial intelligence. They earned it by being an artificial Ray Dalio. The secret to becoming believable at Bridgewater was to model oneself after the only man who mattered.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3x003k3b824z3vvji7@published" data-word-count="9">Bridgewater didn’t run on believability. It ran on believers.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3x003l3b82x1pd4zo7@published" data-word-count="8">“Ray, this is a religion,” Rubinstein told Dalio.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3x003m3b82uctaqzr2@published" data-word-count="70">That would be Rubinstein’s final day at Bridgewater. He agreed to stay for several months longer, in part to save face after his splashy arrival. Rubinstein agreed to keep quiet publicly about his feelings on the firm, and Dalio agreed that the firm would honor paying out the remainder of the co-CEO’s two-year compensation. It added up to tens of millions of dollars for less than a year of work.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3x003n3b829339oaoa@published" data-word-count="110">Much to the surprise of some who worked for him, Ferrucci stayed on for years longer. He split the time between his own company that Dalio funded and project after project at Bridgewater work that Dalio came up with for him. One Bridgewater effort, code-named Allstream, was essentially One Rating to Rule Them All — a metric that would replace all other categories and boil employees down to a single statistic that encompassed their overall worth at the firm. Others on his team worked on a project called “Video Book,” which would package Bridgewater case studies and sell them to the public for $75 apiece. Neither initiative was ever completed.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3y003o3b82h3ws7kkb@published" data-word-count="87">By late 2019, after many years of fruitless toil, Ferrucci had hit his limit. He seemed reluctant to make a big deal about leaving — that might call attention to how little he’d accomplished. Instead, in an agreement with Dalio, he arranged to quit his work at the hedge fund but keep offices on the Bridgewater campus, where he gave interviews about his latest computer-science research. None of them had anything to do with Dalio, Bridgewater, or the Principles. Neither Bridgewater nor Ferrucci publicly acknowledged his departure.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9ka3y003p3b82z2iu6sg0@published" data-word-count="20">Two years later, in late 2021, Bridgewater laid off most of the remaining staff dedicated to building the Principles software.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/clon9nefq005d3b821ntm3ccl@published" data-word-count="22"><strong><em>Excerpted from&nbsp;</em></strong><a href="https://www.amazon.com/Fund-Bridgewater-Associates-Unraveling-Street/dp/1250276934?tag=thestrategistsite-20&amp;ascsubtag=__in1107aam__clon9g6a4001c0pge17h79oez________________" rel="sponsored,nofollow" data-track-type="product-link"><strong>The Fund: Ray Dalio, Bridgewater Associates, and the Unraveling of a Wall Street Legend</strong></a><strong>,<em> by Rob Copeland&nbsp;(St. Martin’s Press, November 7).</em></strong></p>

    </div>

    


          



      <span>‘Ray, This Is a Religion’</span>



  </section>

  
  
    
  

</article>

  

</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Go, Containers, and the Linux Scheduler (327 pts)]]></title>
            <link>https://www.riverphillips.dev/blog/go-cfs/</link>
            <guid>38181346</guid>
            <pubDate>Tue, 07 Nov 2023 19:10:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.riverphillips.dev/blog/go-cfs/">https://www.riverphillips.dev/blog/go-cfs/</a>, See on <a href="https://news.ycombinator.com/item?id=38181346">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-astro-cid-bvzihdzo=""><div data-astro-cid-bvzihdzo=""><div data-astro-cid-bvzihdzo=""><p><time datetime="2023-11-04T00:00:00.000Z">Nov 4, 2023</time></p><p>
Last updated on <time datetime="2023-11-07T00:00:00.000Z">Nov 7, 2023</time></p></div><hr data-astro-cid-bvzihdzo=""></div><p>Like many Go developers my applications are usually deployed in containers.
When running in container orchestrators it’s important to set CPU limits to ensure that the container doesn’t consume all the CPU on the host.
However, the Go runtime is not aware of the CPU limits set on the container and will happily use all the CPU available.
This has bitten me in the past, leading to high latency, in this blog I’ll explain what is going on and how to fix it.</p>
<h2 id="how-the-go-garbage-collector-works">How the Go Garbage Collector works</h2>
<p>This is going to be a pretty high level overview of the Go Garbage Collector (GC).
For a more in depth overview I recommend reading <a href="https://tip.golang.org/doc/gc-guide">the go docs</a>
and this <a href="https://www.ardanlabs.com/blog/2018/12/garbage-collection-in-go-part1-semantics.html">excellent series of blogs</a>
by Will Kennedy.</p>
<p>The vast majority of the time the Go runtime performs garbage collection concurrently with the execution of your program.
This means that the GC is running at the same time as your program. However, there are two points in the GC process where the Go runtime needs to stop every Goroutine.
This is required to ensure data integrity. Before the Mark Phase of the GC the runtime stops every Goroutine to apply the write barrier, this ensures no objects created after this point are garbage collected. This phase is known as Sweep Termination.
After the mark phase has finished there is another stop the world phase, this is known as Mark Termination and the same process happens to remove the write barrier. These usually takes in the order of tens of microseconds.</p>
<p>I created a simple web application that allocates a lot of memory and ran it in a container with a limit of 4 CPU cores with the following command.The Source code for this is available <a href="https://github.com/RiverPhillips/go-cfs-blog">here.</a></p>
<pre tabindex="0" lang="bash"><code><span><span>docker</span><span> run</span><span> --cpus=4</span><span> -p</span><span> 8080</span><span>:8080</span><span> $(</span><span>ko</span><span> build </span><span>-L</span><span> main.go)</span></span>
<span></span></code></pre>
<p>It’s worth noting the docker CPU limit is a soft limit, meaning it’s only enforced when the host is CPU constrained. This means that the container can use more than 4 CPU cores if the host has spare capacity.</p>
<p>You can collect a trace using the <a href="https://golang.org/pkg/runtime/trace/">runtime/trace</a> package then analyze it with <code>go tool trace</code>. The following trace shows a GC cycle captured on my machine. You can see the Sweep Termination and the Mark Termination stop the world phase on <code>Proc 5</code> (They’re labelled STW for stop the world).</p>
<p><a href="https://www.riverphillips.dev/gc_trace.jpg"><img src="https://www.riverphillips.dev/gc_trace.jpg" alt="GC Trace"></a></p>
<p>This GC cycle took just under 2.5ms, but we spent almost 10% of that in a stop the world phase. This is a pretty significant amount of time, especially if you are running a latency sensitive application.</p>
<h2 id="the-linux-scheduler">The Linux Scheduler</h2>
<p>The <a href="https://docs.kernel.org/scheduler/sched-design-CFS.html">Completely Fair Scheduler (CFS)</a> was introduced in Linux 2.6.23 and was the default Scheduler until Linux 6.6 which was released last week. It’s likely you’re using the CFS.</p>
<p>The CFS is a <a href="https://en.wikipedia.org/wiki/Proportional_share_scheduling">proportional share scheduler</a>, this means that the weight of a process is proportional to the number of CPU cores it is allowed to use. For example, if a process is allowed to use 4 CPU cores it will have a weight of 4. If a process is allowed to use 2 CPU cores it will have a weight of 2.</p>
<p>The CFS does this by allocating a fraction of CPU time. A 4 core system has 4 seconds of CPU time to allocate every second. When you allocate a container a number of CPU cores you’re essentially asking the Linux Scheduler to give it <code>n</code> CPUs worth of time.</p>
<p>In the above <code>docker run</code> command I’m asking for 4 CPUs worth of time. This means that the container will get 4 seconds of CPU time every second.</p>
<h2 id="the-problem">The Problem</h2>
<p>When the Go runtime starts it creates an OS thread for each CPU core. This means if you have a 16 core machine the Go runtime will create 16 OS threads - regardless of any CGroup CPU Limits. The Go runtime then uses these OS threads to schedule goroutines.</p>
<p>The problem is that the Go runtime is not aware of the CGroup CPU limits and will happily schedule goroutines on all 16 OS threads. This means that the Go runtime will expect to be able to use 16 seconds of CPU time every second.</p>
<p>Long stop the world durations arise from the Go runtime needing to stop Goroutine on threads that it’s waiting for the Linux Scheduler to schedule. These threads will not be scheduled once the container has used it’s CPU quota.</p>
<h2 id="the-solution">The Solution</h2>
<p>Go allows you to limit the number of CPU threads that the runtime will create using the <code>GOMAXPROCS</code> environment variable.
This time I used the following command to start the container</p>
<pre tabindex="0" lang="bash"><code><span><span>docker</span><span> run</span><span> --cpus=4</span><span> -e</span><span> GOMAXPROCS=</span><span>4</span><span> -p</span><span> 8080</span><span>:8080</span><span> $(</span><span>ko</span><span> build </span><span>-L</span><span> main.go)</span></span></code></pre>
<p>Below is a trace captured from the same application as above, now with the <code>GOMAXPROCS</code> environment variable matching the CPU quota.</p>
<p><a href="https://www.riverphillips.dev/gc_trace_4.jpg"><img src="https://www.riverphillips.dev/gc_trace_4.jpg" alt="GC Trace"></a></p>
<p>In this trace, the garbage collection is much shorter, despite having the exact same load. The GC Cycle took under 1ms and the stop the world phase was 26μs, approximately 1/10 of the time when there was no limit.</p>
<p><code>GOMAXPROCS</code> should be set to the number of CPU cores that the container is allowed to use, if you’re allocating fractional CPU round down, unless you’re allocating less than 1 CPU core in which case round up. <code>GOMAXPROCS=max(1, floor(CPUs))</code> can be used to calculate the value.
If you find it easier Uber has open sourced a library <a href="https://github.com/uber-go/automaxprocs">automaxprocs</a> to calculate this value for you from your container’s cgroups automatically.</p>
<p>There’s an outstanding <a href="https://github.com/golang/go/issues/33803">Github Issue</a> with the Go runtime to support this out the box so hopefully it will be added eventually!</p>
<h2 id="conclusion">Conclusion</h2>
<p>When running Go in a containerised application it’s important to set CPU limits. It’s also important to ensure that the Go runtime is aware of these limits by setting a sensible <code>GOMAXPROCS</code> value or using a library like <a href="https://github.com/uber-go/automaxprocs">automaxprocs</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I Got the Fed to Release Its 2011 "Treasury Default" Playbook (146 pts)]]></title>
            <link>https://www.crisesnotes.com/i-got-the-fed-to-release-its-2011-treasury-default-playbook-heres-what-it-says-and-why-it-matters/</link>
            <guid>38181280</guid>
            <pubDate>Tue, 07 Nov 2023 19:04:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.crisesnotes.com/i-got-the-fed-to-release-its-2011-treasury-default-playbook-heres-what-it-says-and-why-it-matters/">https://www.crisesnotes.com/i-got-the-fed-to-release-its-2011-treasury-default-playbook-heres-what-it-says-and-why-it-matters/</a>, See on <a href="https://news.ycombinator.com/item?id=38181280">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="main">

        <header>
            <span>November 7, 2023</span>
            
        </header>

        <p><img src="https://www.crisesnotes.com/content/images/2023/11/FOIA-2011-memo-success-letter.jpg" alt="I Got the Fed to Release its 2011 “Treasury Default” Playbook. Here’s What it Says and Why it Matters."></p>

        <div>
            <p>Readers may recall that I wrote a Politico Op Ed at a critical moment in the debt ceiling showdown. That piece, was entitled <a href="https://www.politico.com/news/magazine/2023/04/19/powell-debt-ceiling-fed-00092522?ref=crisesnotes.com">“Biden Can Steamroll Republicans on the Debt Ceiling”</a>, and I aimed squarely at debunking the idea that the Federal Reserve would step on any “unilateral actions” to avoid treasury default. My key piece of evidence was a memo that I had not read, and was not publicly available. But I knew the contents of the memo indirectly through the Federal Open Market Committee Meeting transcripts. Those comments were in some ways especially revealing, since they came from the Fed’s three leaders: Ben Bernanke, Janet Yellen and Jerome Powell. It’s worth quoting the key part of <a href="https://www.politico.com/news/magazine/2023/04/19/powell-debt-ceiling-fed-00092522?ref=crisesnotes.com">my Op Ed</a> at length:</p><blockquote>On that call, Powell and most of his colleagues reluctantly endorsed buying defaulted Treasury securities — an unprecedented move to maintain financial stability —<strong> </strong>if a legislative debt ceiling solution did not come in time. Here’s the key exchange between Powell and then-Fed Chair Ben Bernanke (options “8 and 9” in the memo are purchases of defaulted Treasury securities and the Fed “swapping” non-defaulted Treasury securities for defaulted Treasury securities):</blockquote><blockquote><strong>MR. POWELL: </strong>As long as I’m talking, I find 8 and 9 to be loathsome. I hope that gets into the minutes. [Laughter] But I don’t want to say today what I would and wouldn’t do, if we have to actually deal with a catastrophe on this.</blockquote><blockquote></blockquote><blockquote><strong>CHAIRMAN BERNANKE: </strong>So you are willing to accept “loathsome” under some certain circumstances. [Laughter]</blockquote><blockquote></blockquote><blockquote><strong>MR. POWELL: </strong>Yes, under certain circumstances.</blockquote><blockquote>Powell’s willingness to purchase defaulted Treasury securities — however “loathsome” he finds it — casts the entire debate over bypassing Congress on the debt ceiling in a new light. No option under discussion is more extreme, from the Federal Reserve’s point of view, than stepping in and buying compromised securities of uncertain underlying value.</blockquote><p>When I wrote and published this Op Ed, I was actually working on getting hold of that memo. Using the Federal Open Market Committee Freedom of Information Act <a href="https://fomcfoia.federalreserve.gov/?ref=crisesnotes.com">official website portal</a>, I had crafted a request for this memo, as well as some older unreleased materials. As an aside, make sure the website you are on is about the Federal Open Market Committee, if you decide to get into the FOMC Freedom of Information Act Request game yourself. There is a website for the <a href="https://foia.federalreserve.gov/?ref=crisesnotes.com">Board of Governors of the Federal Reserve</a>. If you submit a request to that website when you’re seeking Federal Open Market Committee related material, your request will bounce and will be redirected to the FOMC FOIA website. I personally make sure to resubmit when I make this mistake in case something gets lost in the shuffle between FOIA portals. If you want to request materials related to the discount window and 13(3) “unusual and exigent circumstances” authority, the Board of Governors website is the one you want.</p><p>Anyway, I had not gotten a response at the time of publishing the OpEd — nor frankly did I expect to get one. I thought this material would be considered too sensitive and too recent to release. Happily, I can inform you that I was wrong. On June 30th of this year I received a copy of the memo. It was also quietly linked to in the “2011 memos” section of their website, meaning it has technically been publicly available since I received it. However, it does not show up on google searches (like many other memos do). As far as I can tell, no one has noticed that this memo is now public. Thus, I can confidently say that I am the first to bring the full text of this memo to the public, and analyze its specific contents. The memo is dryly entitled <a href="https://www.federalreserve.gov/monetarypolicy/files/FOMC20110719memo01.pdf">“Potential Policy Responses to the Debt Ceiling”</a>. </p><p>Before getting to the policy options it lays out, it's worth commenting on how it elegantly summarizes “the problem”. The first sentence of the memo claims: “With the federal debt ceiling binding and the Treasury running out of the additional borrowing capacity it can achieve under various accounting procedures, a technical default on Treasury securities cannot be ruled out.”. That summary is notable for a few reasons. First, it recognizes that the problem is not “debt” or “deficits”, but rather the lack of legal authority to issue additional treasury securities. Second, as I’ve hammered many times in this newsletter (as well as the Financial Times and Politico among other outlets) the treasury always already uses accounting gimmicks to continue to fill up its checking account, and make payments. What does that mean?&nbsp; That there is <strong>no debate</strong> about whether the treasury should use accounting gimmicks to circumvent the debt ceiling. It already does that. The real debate is over whether it should use an “accounting gimmick” to permanently and unilaterally avoid default.</p><p>With that out of the way, we move on to their policy options. Let’s start with the five basic options listed first. These are interesting because they were uncontroversial to members of the FOMC when they were discussed in 2011, and again in 2013. Therefore the extent to which these options themselves preserve liquidity for defaulted treasury securities, they emphasize the extent to which the Federal Reserve will strive to contain the economic and financial stability fallout from a treasury default.:</p><blockquote>We begin by describing how defaults could affect five routine policy actions that are permissible under the Federal Reserve Act and fall within the current authorization of the Desk and the authority of the Reserve Banks. The relevant issue for these actions is the treatment of defaulted Treasury securities – that is, securities that are experiencing a delay in principal or interest payments due to the debt ceiling. The staff’s recommendation is to make no changes to our current procedures, thereby treating defaulted Treasury securities in these transactions on the same terms that apply to other (non-defaulted) Treasury securities. This approach would send a signal to the market that we continue to view principal and interest for these securities as backed by the full faith and credit of the U.S. government. Moreover, in many cases, this approach would help the market cope with the pressures that could emerge in such circumstances.&nbsp;</blockquote><p>In other words, there are all sorts of day to day things that the Federal Reserve does that provide liquidity to treasury markets which the Federal Reserve would continue. But what are those options?</p><blockquote>1. Outright purchases of Treasury securities</blockquote><blockquote>2. Rollovers of maturing Treasury securities.</blockquote><blockquote>3. Securities lending activity.</blockquote><blockquote>4. Repurchase agreements.</blockquote><blockquote>5. Discount window lending.</blockquote><p>These tools are very familiar to people who read about monetary policy. The main surprise here is regarding the first “option”. In inferring the contents of this memo from the FOMC transcript, I didn’t realize that the first option included the purchase of defaulted treasury securities. Yet, <strong>it does.</strong> As the document says: “Accordingly, unless otherwise directed by the Committee, the [New York Federal Reserve Trading] Desk intends to accept defaulted securities in these operations in the same manner as other Treasury securities, with the prices determined through competitive bidding.” This means that Federal Reserve officials have across the board found the purchase of defaulted treasury securities by The Fed to be uncontroversial. This is an important and remarkable fact.</p><p>But what of the other options? Option 2 is not really focused on the defaulted securities part of a potential debt ceiling crisis. Instead, it is concerned with how to deal with the Fed’s “reinvestment policy” when the treasury is no longer conductings new auctions. I will skip over that issue to focus on options 3 to 5. These options sound very different, but really they are all about borrowing from the Federal Reserve. They are really concerned with the question of “collateral” for those loans. In option 3, the Federal Reserve is lending securities rather than “cash”. Nevertheless, it wants collateral. The repurchase agreements of option 4 are simply a form of borrowing that has preferential treatment in bankruptcy. Since the collateralized loan is structured like a repurchase agreement, if the borrower fails to “repurchase” the collateral, then the lender gets to keep it. Option 5 is simply the discount window, the most traditional version of Federal Reserve lending.</p><p>So what is the Federal Reserve’s concern? In all these cases, they are thinking through how to treat defaulted treasury securities. An obvious approach would be to not accept defaulted treasury securities as collateral. The justification would be… <strong>they’ve defaulted. </strong>A subtler option is accepting them as collateral, but at their current (depressed) market price, or at some other lower price. The issue with these approaches is that they<strong> worsen liquidity for these securities</strong> — while causing panic about what might happen to currently undefaulted treasury securities. The Memo’s alternative is to accept them <strong>at the same terms they accept undefaulted treasury securities.&nbsp;</strong></p><p>While this is a more technical issue than outright purchases of defaulted treasury securities, <strong>it is not less important. </strong>As we saw earlier this year with the Bank Term Funding Program, changing the collateral policy to treasury securities can have a profound impact. There is no doubt that if the Federal Reserve, for lending purposes, treated defaulted treasury securities like undefaulted securities it would go quite a long way to preserving liquidity for these instruments, and thus the larger treasury market. It is thus notable that Federal Reserve officials treated these options (which are simply three manifestations of the same policy) as uncontroversial&nbsp;</p><p>The next three options are about technical issues related to the repo market, as well as Money Market Mutual Funds. I may write about this section of the memo in a premium piece in the future. But for today I’m keeping my focus on the issue of defaulted treasury securities. It is worth saying though, that this part of the memo illustrates the Federal Reserve’s commitment to preventing a treasury default from having a wider knock-on impact on financial markets. In any case, the remaining part of the memo deals with engaging in purchase and sale operations specifically aimed at absorbing defaulted treasury securities. Now that we have the memo in hand, it's clearer that purchasing defaulted treasury securities was not controversial. Instead, it was specifically aiming at, and designing a comprehensive program to solve, the issue of defaulted treasury securities which made Federal Reserve officials uncomfortable. However, the transcript conversations show far more willingness to use these options if circumstances truly required them than staffers expected. For completeness sake, I will quote this part of the memo below:</p><blockquote><strong>9. Purchase operations to remove defaulted Treasury securities from the market.</strong> To limit the negative impact of defaulted securities on market functioning, the FOMC could decide to purchase a specified amount of these issues from market participants. These purchases would be in addition to those associated with reinvestments (the first policy issue described above). This approach would help market functioning if cash market liquidity were to deteriorate and participants were otherwise unable to sell their Treasury holdings. Moreover, in contrast to the financing operations discussed above, outright purchases remove the securities from firms’ balance sheets, so that the firms no longer have to deal with the operational issues associated with defaulted securities. Unless offset by other actions, such operations would increase the size of the SOMA portfolio and the amount of reserves in the banking system.&nbsp;</blockquote><blockquote></blockquote><blockquote><strong>10. Outright CUSIP swaps to remove defaulted Treasury securities from the market.</strong> One way to avoid the effects of additional purchase operations on the size of the Federal Reserve’s balance sheet and the amount of reserves is to do them as CUSIP swaps. A CUSIP swap would be an operation in which the Desk simultaneously (or at least on the same day) bought a defaulted Treasury security and sold a non-defaulted Treasury security. This operation would be roughly neutral in terms of the size of the SOMA portfolio and the amount of reserves in the banking system. It could also be designed to limit any change in the duration risk of the SOMA. This approach has similarities to securities lending operations against defaulted Treasury collateral (the third policy issue described above), only in this case the operation would remove the defaulted securities from the market permanently. The terms of these operations would be determined through competitive bidding</blockquote><p>Now that we know the Federal Reserve’s playbook in case of a treasury default, we can be much more confident in the future when arguing during the next debt ceiling crisis. Powell, or any future Federal Reserve chair, may deny that this playbook has any implications for how they would respond to unilateral workarounds of the debt ceiling. However, now any reader can plainly see <strong>that that isn’t true.</strong> They are willing to do an incredible amount — if it's necessary to contain the financial instability resulting from a treasury default. They would gladly facilitate an option the Treasury pursues in its role as fiscal agent, since they can always claim that it was the Treasury’s decision and they had no other option as fiscal agent (which<strong> happens to be true</strong>). To deny the Treasury and then step into the breach themselves with a policy that is clearly their own to follow, or not follow, puts them in a far more untenable political position. I’m glad to have this out there, and I look forward to unearthing many more gems through Freedom of Information Act requests in the coming months and years.</p>
        </div>


        

    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Article 45 of eIDAS 2.0 will roll back web security by 12 years (236 pts)]]></title>
            <link>https://www.eff.org/deeplinks/2023/11/article-45-will-roll-back-web-security-12-years</link>
            <guid>38181114</guid>
            <pubDate>Tue, 07 Nov 2023 18:52:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.eff.org/deeplinks/2023/11/article-45-will-roll-back-web-security-12-years">https://www.eff.org/deeplinks/2023/11/article-45-will-roll-back-web-security-12-years</a>, See on <a href="https://news.ycombinator.com/item?id=38181114">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <article role="article">
  
  
  <div><p><span>The EU is poised to </span><a href="https://last-chance-for-eidas.org/"><span>pass a sweeping new regulation</span></a><span>, eIDAS 2.0. Buried deep in the text is Article 45, which returns us to the dark ages of 2011, when certificate authorities (CAs) could collaborate with governments to spy on encrypted traffic—and get away with it. Article 45 forbids browsers from enforcing modern security requirements on certain CAs without the approval of an EU member government. Which CAs? Specifically the CAs that were appointed by the government, which in some cases will be owned or operated by that selfsame government. That means cryptographic keys under one government’s control could be used to intercept HTTPS communication throughout the EU and beyond.</span></p>
<p><span>This is a catastrophe for the privacy of everyone who uses the internet, but particularly for those who use the internet in the EU. Browser makers have not announced their plans yet, but it seems inevitable that they will have to create two versions of their software: one for the EU, with security checks removed, and another for the rest of the world, with security checks intact. We’ve been </span><a href="https://www.nytimes.com/2000/01/31/business/worldbusiness/IHT-us-removes-an-encryption-barrier.html"><span>down this road before</span></a><span>, when export controls on cryptography meant browsers were released in two versions: strong cryptography for US users, and weak cryptography for everyone else. It was a fundamentally inequitable situation and the knock-on effects set back web security by decades.</span></p>
<p><span>The current text of Article 45 requires that browsers trust CAs appointed by governments, and </span><a href="https://blog.mozilla.org/netpolicy/files/2023/11/eIDAS-Industry-Letter-updated.pdf"><span>prohibits browsers from enforcing any security requirements</span></a><span> on those CAs beyond what is approved by </span><a href="https://en.wikipedia.org/wiki/ETSI"><span>ETSI</span></a><span>. In other words, it sets an upper bar on how much security browsers can require of CAs, rather than setting a lower bar. That in turn limits how vigorously browsers can compete with each other on improving security for their users.</span></p>
<p><span>This upper bar on security may even ban browsers from enforcing </span><a href="https://en.wikipedia.org/wiki/Certificate_Transparency"><span>Certificate Transparency</span></a><span>, an IETF technical standard that ensures a CA’s issuing history can be examined by the public in order to detect malfeasance. Banning CT enforcement makes it much more likely for government spying to go undetected.</span></p>
<p><span>Why is this such a big deal? The role of a CA is to bootstrap encrypted </span><a href="https://www.eff.org/encrypt-the-web"><span>HTTPS</span></a><span> communication with websites by issuing certificates. The CA’s core responsibility is to match web site names with customers, so that the operator of a website can get a valid certificate for that website, but no one else can. If someone else gets a certificate for that website, they can use it to intercept encrypted communications, meaning they can read private information like emails.</span><span><br></span><span><br></span><span>We know HTTPS encryption is a barrier to government spying because of the NSA’s famous “</span><a href="https://www.washingtonpost.com/world/national-security/nsa-infiltrates-links-to-yahoo-google-data-centers-worldwide-snowden-documents-say/2013/10/30/e51d661e-4166-11e3-8b74-d89d714ca4dd_story.html"><span>SSL added and removed here</span></a><span>” note. We also know that misissued certificates have been used to spy on traffic in the past. For instance, in 2011 </span><a href="https://en.wikipedia.org/wiki/DigiNotar"><span>DigiNotar was hacked</span></a><span> and the resulting certificates used to intercept emails for people in Iran. In 2015, </span><a href="https://slate.com/technology/2016/12/how-the-2011-hack-of-diginotar-changed-the-internets-infrastructure.html"><span>CNNIC issued an intermediate certificate</span></a><span> used in intercepting traffic to a variety of websites. Each CA was subsequently </span><a href="https://blog.mozilla.org/security/2015/04/02/distrusting-new-cnnic-certificates/"><span>distrusted</span></a><span>.</span></p>
<p><span>Distrusting a CA is just one end of a spectrum of technical interventions browsers can take to improve the security of their users. Browsers operate “root programs” to monitor the security and trustworthiness of CAs they trust. Those root programs impose a number of requirements varying from “how must key material be secured” to “how must validation of domain name control be performed” to “what algorithms must be used for certificate signing.” As one example, certificate security rests critically on the security of the hash algorithm used. The </span><a href="https://en.wikipedia.org/wiki/SHA-1"><span>SHA-1 hash algorithm</span></a><span>, published in 1993, was considered not secure by 2005. NIST disallowed its use in 2013. However, CAs didn't stop using it until 2017, and that only happened because one browser made SHA-1 removal a requirement of its root program. After that, the other browsers followed suit, along with the </span><a href="https://cabforum.org/"><span>CA/Browser Forum</span></a><span>.</span></p>
<p><span>The removal of SHA-1 illustrates the backwards security incentives for CAs. A CA serves two audiences: their customers, who get certificates from them, and the rest of the internet, who trusts them to provide security. When it comes time to raise the bar on security, a CA will often hear from their customers that upgrading is difficult and expensive, as it sometimes is. That motivates the CA to drag their feet and keep offering the insecure technology. But the CA’s other audience, the population of global internet users, needs them to continually improve security. That’s why browser root programs need to (and do) require a steadily increasing level of security of CAs. The root programs advocate for the needs of their users so that they can provide a more secure product. The security of a browser’s root program is, in a very real way, a determining factor in the security of the browser itself.</span></p>
<p><span>That’s why it’s so disturbing that eIDAS 2.0 is poised to prevent browsers from holding CAs accountable. By all means, </span><i><span>raise</span></i><span> the bar for CA security, but permanently lowering the bar means less accountability for CAs and less security for internet users everywhere.</span></p>
<p><span>The text isn't final yet, but is subject to approval behind closed doors in Brussels on November 8.</span></p>

</div>

          </article>
    </div><div>
          <h2>Join EFF Lists</h2>
        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Northlight technology in Alan Wake 2 (412 pts)]]></title>
            <link>https://www.remedygames.com/article/how-northlight-makes-alan-wake-2-shine</link>
            <guid>38180846</guid>
            <pubDate>Tue, 07 Nov 2023 18:32:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.remedygames.com/article/how-northlight-makes-alan-wake-2-shine">https://www.remedygames.com/article/how-northlight-makes-alan-wake-2-shine</a>, See on <a href="https://news.ycombinator.com/item?id=38180846">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>The Northlight team is super-excited to have Alan Wake 2 out there. We've created and polished off a lot of new tech so that Alan Wake 2 looks great and plays great. Here are the key highlights of the new features and tools that we'd like to give a moment in the spotlight. We're keeping the description to a fairly high level here; our engineers will be back with more in-depth topics in due course.</p><h3>Core engine: New data-oriented game object model</h3><p>Northlight switched to a completely new data-oriented game object framework during the development of Alan Wake 2. The new entity component system (ECS) based model enables memory-efficient storage and makes parallel execution efficient and safe. This means that the engine can support a varying number of target hardware cores efficiently, enabling bigger, more dynamic and fuller worlds. ECS also played a supporting role for our tools development, simplifying the building of the new Scattering tool for mass-authoring vegetation - ECS allowed us to simply have a lot more entities without the need to invent any custom solution for scattering objects in the world.</p><p>The ECS framework also made it to our gameplay programmers' "favorite tech" list as it helped implement the Case Board - Saga's visual storyboard for gathering evidence. ECS meant that iteration was quick because adding new or modifying existing systems or game objects was easy, and performance gains were clear when saving and loading the Case Board.</p><figure><p><img src="https://assets-global.website-files.com/64630b03551142e3347ae3da/6544bf00d880ecdd52ccc46e_image-2023-11-3_11-17-22.png" loading="lazy" alt="Picture of Case Board"></p></figure><h3>Voxel-Based Character Controller</h3><p>Alan Wake 2 called forth reimagining character control in Northlight. We built a new Voxel-Based Character Control that enables smooth navigation in cramped, complex and dynamic environments; it makes character movement more natural and fluid. Our marketing folks would say the characters are more responsive and lifelike than ever before; our internal dev notes described it as "the characters won't bump or get stuck into objects in tight spaces".</p><figure><p><iframe allowfullscreen="true" frameborder="0" scrolling="no" src="https://player.vimeo.com/video/880838821" title="VCC_path_demo"></iframe></p></figure><p><figcaption>Character path visualization showing predicted path (white) as the character tries to reach the movement target (yellow).</figcaption></p><h3>Reworked<em> </em>NPC locomotion</h3><p>Our animation tech made some major changes into how Non-Player Characters (NPC) move. The revamped NPC locomotion means that in Alan Wake 2 all NPCs utilize animation-driven movement combined with new distance-based Motion Matching. This new tech improves movement quality and gives us more control over how and when animations are used.</p><figure><p><iframe allowfullscreen="true" frameborder="0" scrolling="no" src="https://player.vimeo.com/video/882017461" title="Alan Wake 2 NPC Locomotion"></iframe></p></figure><h3>Realistic wind</h3><p>More realistic wind was on the must-have tech list for Alan Wake 2 - and that's we have, wind that realistically affects physics, particles, and cloth.</p><p>For example, game designers can now easily define indoor and outdoor areas that have different wind speeds, with smooth, stateless transitions between the two types of areas. The indoor vs. outdoor area definitions are used, for example, to ensure that there's no wind inside the car that Saga and Casey are driving. This is done through defining a moving "wind box" that specifies that the inside of the car is a wind-free indoor area.</p><figure><p><img src="https://assets-global.website-files.com/64630b03551142e3347ae3da/6544bea014d3dfaf2f5c6858_moving_windbox.png" loading="lazy" alt="Northlight Wind Debugger showing wind box that defines indoor vs. outdoor area. Block edge shows the transition."></p><figcaption>Northlight Wind Debugger showing wind box that defines indoor vs. outdoor area. Block edge shows the transition.</figcaption></figure><p>The wind system tech is built on Signed Distance Fields (SDF) methods where wind boxes are used as primitives that define a smooth, global wind strength field. The wind boxes act like basic building blocks that define how strong the wind is in different areas, creating realistic and smoothly varying wind patterns between indoor and outdoor areas.</p><h3>Scattering tool</h3><p>We developed a new Scattering tool to allow for mass-authoring vegetation and propping environments on a grand scale. In Alan Wake 2 it was used to create denser, richer and more life-like environments with the longer-than-before draw distances (i.e. what defines how far the player can see objects and details in the game world).</p><h3>Luau</h3><p>We switched our scripting language from a proprietary language into Luau, embeddable scripting language derived from Lua by our friends at Roblox. It exposes a comprehensive set of engine functionality and supports live-editing. Luau is used for level scripting and also for various gameplay systems such as the weapon upgrade system. Luau allowed the game team to prototype and implement various game features and VFX effects without needing help from engine programmers.</p><p>We've also cooked up our own VS Code language server extension, so that it all integrates seamlessly into Remedy’s development pipelines. And adopting Luau helped us boot out some 80 000 lines of code we no longer need to maintain.</p><figure><p><img src="https://assets-global.website-files.com/64630b03551142e3347ae3da/6544c372589da1e2a2285809_luau_weather_system.png" loading="lazy" alt="VFX team used Luau to implement a weather system - The different weather conditions can be quickly tested through a debug panel,"></p><figcaption>VFX team used Luau to implement a weather system - The different weather conditions can be quickly tested through a debug panel.</figcaption></figure><h2><br>Graphics and rendering</h2><h3>New GPU-driven rendering pipeline</h3><p>Alan Wake 2 showcases Northlight's brand-new GPU-driven rendering pipeline. It allows us to push more geometry into the world without sacrificing performance. With GPU-driven rendering using mesh shaders, we can now do occlusion culling down to a single-pixel precision and use everything in a scene as an occluder. This ability to only draw what is visible means that the world of AW2 has more geometric detail than we’ve ever shipped before.</p><p>Diving a bit deeper into how it works: we also cull meshlets, in addition to the mesh. Meshlets are smaller, more optimized, groups of triangles extracted from the mesh. In the images below you can see what meshlets look like around Cauldron Lake's convenience store location.</p><figure><p><img src="https://assets-global.website-files.com/64630b03551142e3347ae3da/6544d4dc52652337e5f1e521_meshlet_visualization1.jpg" loading="lazy" alt=""></p><figcaption>What the player sees...</figcaption></figure><figure><p><img src="https://assets-global.website-files.com/64630b03551142e3347ae3da/6544d5a842800d0cc9817d14_meshlet_visualization2.jpg" loading="lazy" alt=""></p><figcaption>...vs. what's fed into the renderer (silly amount of geometric detail!). Colors represent clusters.</figcaption></figure><h3>Character-style rigs on foliage - Large scale procedural GPU animation</h3><p>The expansive primordial forest environments of Alan Wake 2 are brought to life through our new shader-based vegetation system. It's based on a new skinning system that runs entirely on GPU and supports "art-driven" bone shader animations. In Alan Wake 2 it enabled us to use full character-style rigs on all the foliage visible in the environments.</p><p>We're calling the new system art-driven here because the bone shaders expose an API with which artists can write their own shader code that hooks into the underlying system. So technically artists could use it to animate anything they'd like: foliage, objects bobbing up and down on water, power cables wobbling in the wind, etc. </p><figure><p><iframe allowfullscreen="true" frameborder="0" scrolling="no" src="https://player.vimeo.com/video/880842428" title="GPU_animation"></iframe></p></figure><p><figcaption>GPU bone visualization - The skeleton rigs animating all vegetation. Each line is one bone, with almost 300,000 bones in Cauldron Lake being processed every frame.</figcaption></p><p>‍</p><figure><p><img src="https://assets-global.website-files.com/64630b03551142e3347ae3da/6544da11f604c7d5e835ae8e_GPU_animation_pic2NoDraw.JPG" loading="lazy" alt=""></p><figcaption>GPU bone visualization OFF</figcaption></figure><figure><p><img src="https://assets-global.website-files.com/64630b03551142e3347ae3da/6544da29c20118c775ce9996_GPU_animation_pic2WDraw.JPG" loading="lazy" alt=""></p><figcaption>GPU bone visualization ON</figcaption></figure><h3>HDR support</h3><p>Alan Wake 2 fully supports HDR. We've made sure that the game looks great out-of-the-box with default settings, regardless of whether your display supports SDR or HDR. </p><p>On the tech side, adding HDR support could almost be said to have been a simple 'let's change the output format' operation; the more significant effort was done on the creative side. Alan Wake 2 is authored in HDR, meaning that the color grading was done by actual human colorists to ensure that the unique art styles and aesthetics, the storytelling and mood of Alan Wake 2 is amplified in both HDR and SDR.</p><h3>Transparency and atmospheric effects</h3><p>The pervasive fog scenes in Alan Wake 2 are built upon improvements into how we render transparency. We're using MBOIT (Moment-Based Order-Independent Transparency) to make see-through surfaces blend together smoothly, even when they have different levels of detail.</p><p>During Alan Wake 2's development, Northlight did a complete overhaul of transparent rendering. We now draw transparency in three resolutions with MBOIT, which makes it possible to blend fog, transparent geometry and effects seamlessly. In addition to better blending of fog and other transparent elements, we’ve improved pipelines to allow more fine-grained control over fog placement in the world. Combining all this to per-pixel transparent lighting and fog-affected reflections makes opaque and transparent elements fit together better than in our previous projects.</p><p>Our fog also approximates multiple light scattering, giving it a thick and realistic look that improves atmosphere.</p><figure><p><img src="https://assets-global.website-files.com/64630b03551142e3347ae3da/6544db9b6fe3aaa039dcc7b8_transparency_boat.jpg" loading="lazy" alt=""></p><figcaption>Note how the water refracts correctly and supports non-uniform blur for realistic looking water.</figcaption></figure><figure><p><img src="https://assets-global.website-files.com/64630b03551142e3347ae3da/6544dba3c20118c775cfbc07_atmospheric_sunset.jpg" loading="lazy" alt=""></p></figure><figure><p><img src="https://assets-global.website-files.com/64630b03551142e3347ae3da/6544dba991ac1ce7f3e43dda_multiple_light_scattering.jpg" loading="lazy" alt=""></p></figure><figure><p><img src="https://assets-global.website-files.com/64630b03551142e3347ae3da/6544dbb6c31507ffce973345_coffee_world_MBOIT.jpg" loading="lazy" alt=""></p><figcaption>Transparent Rendering - A perfect blend of different transparent effects: fog, particles and water all use MBOIT. </figcaption></figure><h3>VFX</h3><p>Effects are an essential part of Alan Wake 2’s visuals and expand on what we achieved in Control. The node-based VFX tools in Northlight have evolved significantly in terms of supported features and runtime performance. Visual effects artists are now able to author complex and dynamic effects like rain, wetness, water simulation, and character wounds. VFX tools also benefit from GPU-driven rendering and can push a lot of geometry through the GPU. This is required, for example, when rendering rain blocker objects to a dynamic mask that prevents rain from appearing indoors or under cover.</p><figure><p><img src="https://assets-global.website-files.com/64630b03551142e3347ae3da/6544dc4a7b3667e57e5d06b7_vfx_off.jpg" loading="lazy" alt=""></p><figcaption>Screenshot of Dark Place with visual effects OFF.</figcaption></figure><figure><p><img src="https://assets-global.website-files.com/64630b03551142e3347ae3da/6544dc4f3df84ea77d6fa679_vfx_on.jpg" loading="lazy" alt=""></p><figcaption>Screenshot of Dark Place with visual effects ON. 'Fade Out' enemies and weather effects are key parts of the visual identity.</figcaption></figure><p>‍</p><h3>Ray tracing</h3><p>We have added support for fully ray-traced direct lighting and combined this with improved denoising and indirect lighting algorithms with Nvidia. Ray tracing in Alan Wake 2 is more accurate and robust than what has been seen in Control. Ray tracing also makes the animated foliage look amazing, now that all the geometry animation (foliage) is authored and simulated using skinning.</p><p>With Alan Wake 2, PC players can experience (GPU and CPU setup permitting) the latest Nvidia DLSS innovations - DLSS Frame Generation, DLSS Ray Reconstruction, Path Traced Indirect Lighting - all the things that enable us to say 'never before' once more.</p><p>‍</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Seeing like a bank (531 pts)]]></title>
            <link>https://www.bitsaboutmoney.com/archive/seeing-like-a-bank/</link>
            <guid>38180477</guid>
            <pubDate>Tue, 07 Nov 2023 18:07:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bitsaboutmoney.com/archive/seeing-like-a-bank/">https://www.bitsaboutmoney.com/archive/seeing-like-a-bank/</a>, See on <a href="https://news.ycombinator.com/item?id=38180477">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>The New York Times recently ran a <a href="https://www.nytimes.com/2023/11/05/business/banks-accounts-close-suddenly.html">piece</a> on a purported sudden spate of banks closing customer accounts. Little of it is surprising if you have read <a href="https://www.bitsaboutmoney.com/archive/money-laundering-and-aml-compliance/">previous</a> <a href="https://www.bitsaboutmoney.com/archive/money-laundering-and-aml-compliance/">issues</a> of Bits about Money. The reported anecdotal user experiences have a common theme to them. Banks frequently present to their users as notably disorganized, discombobulated institutions. This an alarming and surprising fact for the parts of society that are supposed to accurately keep track of all of the money.</p><p>Why does this happen? Why does it happen across issues as diverse as bank-initiated account closures, credit card or Zelle fraud, debit card reissuance, and mortgage foreclosures? Why does it happen in such a similar fashion across many institutions, of all sizes, who exist in vicious competition with each other and who know their customers hate this?</p><p>Banks are extremely good at tracking one kind of truth, ledgers. They are extremely bad at tracking certain other forms of truth, for structural reasons. In pathological cases, which are extremely uncommon relative to all banking activity but which nonetheless happen every day and which will impact some people extremely disproportionately, the bank will appear to lack object permanence. Every interaction of the user with it feels like being Bill Murray in Groundhog Day: the people you’re talking to remember literally nothing of what they’ve promised before, what you’ve told them, and the months or years of history that lead to this moment.</p><p>How did we end up here?</p><h2 id="recordkeeping-systems">Recordkeeping systems</h2><p>Like every bureaucratic system, banks run on a formal system of recordkeeping which requires an unrecognized, illegible shadow system to actually function. The interactions between those systems, and what they are optimized for tracking and not optimized for, cause a lot of the pathologies that people see. The seminal text on this, focused on government bureaucracies, is <a href="https://www.amazon.com/Seeing-like-State-Certain-Condition/dp/0300078153">Seeing like a State</a>.</p><p>Because banks are filled with extremely creative people, we call the primary system banking is conducted on a “<a href="https://www.gartner.com/en/information-technology/glossary/core-banking-systems">core</a>.” The largest banks in the world have complicated bespoke subsystems for this, but most banks are not in the software development business, and instead license a system from a so-called core processor like Jack Henry or Fiserv.</p><p>One could fill a book with architecture diagrams for a mid-sized financial institution. The key thing that non-specialists need to understand is a) the “core” does a lot of what you think of banking as, b) the core interfaces with many other systems which make up a bank, c) in particular, the core interfaces with the ledgers of the bank, and d) all of these systems together cannot represent reality nearly as well as you’d hope.</p><p>They typically grow over the years by accretion, caused by the normal processes of software development, regulatory changes, and competitive pressures. No system will ever be able to answer all interesting questions about a user; that is formally undecidable in computer science. Banks are extremely, painfully aware that the ordinary operation of the business of the bank will occasionally drop things on the floor. They have long-since automated the fat head of customer issues, and the long tail is kicked over to operational and customer support teams.</p><p>Every time responsibility moves between subsystems, be they different organizations, different computer systems, or different groups within the bank, some percentage of cases will simply break. The boundaries of systems are responsible for a huge percentage of all operational issues at banks. (They’re also where most security vulnerabilities live: systems A and B usually agree on reality, but a bad actor can sometimes intentionally get them to disagree, in ways which cause the bad actor to gain value before A and B reconcile their view of reality.)</p><p>A major technological advance over the course of the last few decades has been ticketing systems, which strikes many technologists as being crazy, because they’re almost the simplest software that you can describe as software. All a ticketing system does is enforce an invariant: if there is a problem with a case number assigned to it, and it goes between Group A and Group B, Group A needs to know it no longer is responsible and Group B needs to know it is now responsible. Then you can do can’t-believe-they-pay-us-for-this computing and observe things like “Group B is now working on 10,342 cases”, “There are 76 cases which Group B has not acted on within the last month”, and “Ginger seems to be anomalously unproductive at closing out cases relative to her nearest coworkers.”</p><p>So why didn’t ticketing systems solve this problem? Part of it is that the problem is self-referential: the ticketing system is not the core. The ticketing system is not the subsystem that is directly responsible for anything of interest to you. The ticketing system is an entirely new system, which requires integration with other subsystems and which will frequently need to do handovers to them. This interface is frightening, unexplored territory where new classes of issues that you’ve never seen before can spring up.</p><p>Bank systems are an interesting combination of designed and accidental. The accrete like sedimentary layers. A particular force which affects banks more than most institutions is that the banking industry has undergone <a href="https://www.visualcapitalist.com/the-banking-oligopoly-in-one-chart/">decades of consolidation</a>. When banks merge, one bank doesn’t simply eat the other and digest its balance sheet and people. They end up running their systems in parallel for years while working out an integration plan. That plan will, almost inevitably, cause one of the systems to mostly “win” and the other system to mostly “lose”, but for business reasons, something of the loser will be retained indefinitely. It now has to be grafted onto the winner, despite frequently being itself decades out of date, having its own collection of grafted acquirees partially attached to it, and needing expert input from people who are no longer with the firm.</p><p>Users can watch this play out in real time. For example, I <a href="https://www.bitsaboutmoney.com/archive/requiem-for-a-bank-loan/">banked</a> at First Republic and also bank at Chase, which now owns First Republic. In something which sounds unimpressive and would blow the mind of bank CTOs from as recently as ten years ago, both sides of the bank understand that the same person has an account on the other side. (You wouldn't think testing Social Security Numbers for equality requires any high-tech wizardry, and you'd be right. The thing which was actually hard was building a process to allow complex ad hoc bidirectional synching of systems that were not built in tandem with each other.)</p><figure><img src="https://lh7-us.googleusercontent.com/pw89gj716u9qQuFKG0Et6S09kSO2MfvOjMlxqyZlBanciXztwHW0ctGAWD4AjVMY71ydYW2JhlMDoKlQxfL0tN2EO_MOEobbCg-PmJxqBpoUW6lc7JSPw5y81xAysm51HIkdswJDdBqeQIXHt6qeZcc" alt="A screenshot of a banner in Chase's web application, welcoming First Republic clients." loading="lazy" width="624" height="87"><figcaption><span>Chase paid tens of billions of dollars over the years to get to the point where one engineer could bang this in-app banner out in 30 minutes.</span></figcaption></figure><p>But because that integration is ongoing and will take years to resolve, neither part of the bank knows consequential things the other part knows about me, even where it strikes most people as obvious that they should. Chase eagerly communicates timelines for transitioning the home loan that First Republic very definitely never wrote. It is utterly clueless about the Line of Credit that they factually did extend. And it will require a lot of midnight oil from hundreds or thousands of people for most of another year before I can walk into a Chase branch and ask what the balance is on an account serviced by First Republic's core.</p><h2 id="human-accountability-and-its-malcontents">Human accountability and its malcontents</h2><p>So let’s talk about how banks spackle over the infelicities in their systems.</p><p>First, the bank builds many subsystems which interface with its core processing systems and ledgers. These systems are built so internal bank staff can see what a customer has done in their accounts and, perhaps, act upon those accounts on their behalf.</p><p>For those keeping score: yep, this interface boundary is another place which can cause the bank to fail to agree with reality. Relatively simple programming issues can cause the staff-exposed view of an account to fail to agree with reality known to the bank.</p><p>For example, they not infrequently fail to show some staff transactions which are “pending.” In many cases, “pending” has consequences which are extremely similar to being finalized from the perspective of the user, but a particular system might simply not show them. You’d think that is a confusing choice to make and often underrate the possibility that no one ever made this choice, not really. Sure, it exists (inarguably in this case) in code, and that code might be described in a requirements analysis document that someone handwaved together 18 years ago, but nobody ever said “Nah, exclude pending transactions”. This was a simple oversight, projected into the future indefinitely, to the enduring annoyance of old hands among staff and the continued surprise of non-specialist users. You might assume that senior members of operational staff have the ability to write a memo to engineering or procurement to tell them that the software that makes up the bank is broken. That is a thing which exists at surprisingly few firms.</p><p>(A repeated experience of my time at Stripe was watching engineers embed with Ops teams for a day, then run back to their laptops while saying “I’m so sorry! I can’t believe we did that to you! I will drop everything I am doing and fix it immediately!” In many cases, those bugs had existed for months or years. I watched senior engineering leadership ask senior Ops leadership why they had never been asked to fix them. Ops replied that their long experience in the financial industry had taught them that Ops never gets to use software which isn’t broken and that complaining about this is like complaining about gravity.)</p><p>Banks aggressively partition staff based on job duties and levels within those duties. The most relevant silo for retail consumers is actually a series of parallel silos which staff front-line phone support for the bank. Often, each line of business gets its own silo, which accounts for much of the Your Princess Is In Another Castle that happens when you call a bank with a seemingly straightforward question and then get passed between various departments.</p><p>Most products offered to retail consumers and small businesses are relatively low margin, in absolute dollar terms. To be able to offer these, banks use various methods to cram down their support costs. Offshoring is often the face of these initiatives, but stratification by skill levels and powers granted is probably more important to understand.</p><p>A fairly typical setup for a financial institution will have the retail bank support teams stratified into Tier One, Tier Two, and Tier Three. Each has management located with them, who may or may not be shared across tiers.</p><p>You truly haven’t lived until you’ve tried paying for your college education by being a Tier One customer service representative, like your humble correspondent did. (Not at a bank, thank goodness; I might have stayed in a field with that many interesting problems presented.) The reason Tier One exists is that the median problem, so-to-speak, from a retail user is not actually a problem. That retail user is extremely unsophisticated about the bank account, finance in general, and frequently many other things in life. Tier One exists to handhold this individual in getting something very straightforward done, or to pass the call off to Tier Two.</p><p>Many people in our social class want to be extremely compassionate in explaining challenges that some people endure. Sometimes this compassion extends to believing that people with substantial challenges don’t exist or don’t exist in any large numbers. It is extremely important to understand that those challenges exist and that they will <em>dominate your frequent fliers</em> for support. Some people have only emerging competence in English but will want services from a bank which does business in English. Some people, frequently with large account balances and long successful histories with you, are experiencing age-related decline in their faculties and need to be protected, frequently without that being a capital-F Fact within the system yet. Some people are crooks. Some people have a very interesting relationship with the truth, and say many things to banks which probably felt true to them in the moment. (Is that fraud? Eh, it’s complicated.)</p><p>But to zoom into one particular way people can differ from each other: Some people are not as intelligent as you are. That is uncouth to say. In the United States, almost every large organization will institutionally tamp down on any explicit discussion of it. They all must structure their affairs to deal with the reality of it, though.</p><p>Think of the person from your grade school classes who had the most difficulty at everything. The U.S. expects banks to service people much, much less intelligent than them. Some customers do not understand why a $45 charge and a $32 charge would overdraw an account with $70 in it. The bank will not be more effective at educating them on this than the public school system was given a budget of $100,000 and 12 years to try. This customer calls the bank <em>much more frequently than you do</em>. You can understand why, right? From their perspective, they were just going about their life, doing nothing wrong, and then for some bullshit reason the bank charged them $35.</p><p>The reason you have to “jump through hoops” to “simply talk to someone” (a professional, with meaningful decisionmaking authority) is because the system is set up to a) try to dissuade <em>that guy</em> from speaking to someone whose time is expensive and b) believes, on the basis of voluminous evidence, that you are likely <em>that guy</em> until proven otherwise.</p><p>And so every Tier One rep will talk to dozens of folks a day. Many of those calls are… fairly aggravating, from the perspective of the agent. Tier One has limited ability to do anything useful; this depends on the firm and the silo within the firm, but they are largely read-only interfaces to money. They have a few pre-programmed buttons to push which get 90%+ of people they talk to to not call again. They execute scripts and flowcharts, written by people better paid than them, which gate your access to Tier Two.</p><p>In the best operated systems in the world, Tier One gets about one tweet worth of context to pass over to Tier Two when doing a handoff to them. (“Cust didn’t rec new debit card to Japan plz next day air + waive fee.”) Most financial institutions are not the best operated systems in the world. The bank “forgets” about your issue as soon as you’re off the line with Tier One, and needs to be told it entirely de novo when you speak to Tier Two.</p><p>Tier Two typically spent a few years in Tier One and has begun to specialize in a subfiefdom of banking. They have emerging competence into the nitty gritty of operations at their institution, at least with regards to that subfiefdom. They’re paid more, though not by much. They’re typically given more ability to do what my shop called “accommodations”, which means self-authorizing a resolution for a customer which costs money. Tier Two might be able to, for example, credit an account a small amount of money for an arbitrary reason and have the bank charge it off as an operations loss. </p><p>Your humble correspondent had a soft limit of approximately $200, below which no number was worth trifling my management chain or a specialist about. An interesting observation about the physics of money is that Tier Two could conceivably cost the bank more by authorizing accommodations than they earn in salary. A line manager apprised of this probably will not investigate it for more than five minutes before deciding that the bank is satisfied.</p><p>Then you have Tier Three, which at some firms sits in Customer Service and at some firms sits in Operations. There exist some ambiguity and spectral ranges here, but at some point the job changes in character from “low-wage peon reciting a script” to “professional who has a career doing this and is no longer managed on a tickets-closed-per-hour basis.”</p><p>Tier Three, let’s call them for simplicity, engages in constant firefighting, because at the volume of transactions (and other sources of cases) in all-but-the-tiniest financial firms, something is always on fire. Sometimes you’re covering for hiccups in the technical systems of your institution or counterparties. Sometimes someone has found themselves in an odd edge case or been passed around for ages between departments. Sometimes you’ve received an escalation, about which more later.</p><p>A user of the banking system will often have to redundantly explain themselves when they hit Tier Three, for the same reason as they did when they hit Tier Two. However, because they’re no longer operating in time-starved tickets-per-hour crunch mode, Tier Three has richer access to systems at the bank and more ability to forensically reconstruct procedural history, including history that is not ledgered. They will frequently do this both to do their jobs and to do the legwork for other professionals at the bank who might have decisionmaking authority in some cases but do not have the access or acumen to pull together a view of a case from disparate systems.</p><p>The gaps in experience between getting passed around tiers are replicated for being passed around departments. Say, for example, that the bank owes you a check and you do not receive it in the mail. The vast majority of checks sent through the mail arrive without issue, but the bank knows that it will have to reissue some of them. There is a process for doing this. Unfortunately, because this is a relatively infrequent issue, Tier 2 does not have a Reissue Check button available to them. Instead, their interface to this process is likely “Raise a ticket with Ops and tell the customer someone will call them.” There is no system available to Tier 2 which can verify that that call was actually made. The agent has no basis in their training or experience to know whether the bank routinely makes that call. It is quite possible that success rates on that call being placed are quite low, even if you ask for it three times consecutively, and that the bank is entirely institutionally unaware of this.</p><p>And so the customer will feel frustrated and they have been lied to, Tier 2 certainly doesn’t feel like they’ve lied to anyone (they read the script, it’s a Tuesday), Ops feels like the world is on fire because the world is always on fire, and senior bank management cannot detect this problem because no metric available to them is capable of disaggregating it from the complex monster that is the financial system.</p><h2 id="two-embedded-surprises-about-bank-staffing">Two embedded surprises about bank staffing</h2><p>Many traditionally-minded users of banks assume that someone at their branch can likely help them with issues. Due to the <a href="https://www.bitsaboutmoney.com/archive/branch-banking/">deskilling of the bank branch</a>, the people at a bank branch, including the branch manager in many firms, can only offer solutions to relatively straightforward problems. For the other ones, they also have to call into a support phone tree. Sometimes the bank will have ability to e.g. share context between their screen and the Tier 2 rep; sometimes they’re literally incapable of proving to the bank that they work there. (You might think I’m joking. To beat a drum: the level of technical sophistication across the spectrum of U.S. financial institutions varies wildly.)</p><p>The other surprise is that substantially every financial institution has a parallel way to reach decisionmakers in every area it operates in, which skips most or all of the tiering system and the technical and organizational scar tissue that it carries. This goes by different names in different places but “escalations” is a fairly common one.</p><p>Much like the United States has decided, in its infinite wisdom, that caseworkers for immigration and passport services should be staffed <a href="https://sgp.fas.org/crs/misc/R44726.pdf">in every Congressman’s office</a> and not at the agency that actually handles immigration or passport issuance, there very likely exist people at the bank whose job is working the bank more than it is working for the bank. A number of functions which are not ordinarily customer-facing are given the contact information for that group, with the instruction “In case of emergency, skip Tier Everything and talk immediately to the highly-placed troubleshooting team.”</p><p>If you are a reporter and call a bank for comment about a widow on the cusp of being improperly foreclosed upon, you will (fairly reliably) find your words forwarded to the troubleshooting team within a few minutes. If you’re a regulator and intervene on behalf of an individual, same result. You can absolutely achieve this as a civilian, too; a paper letter to the VP of Retail Banking, Office of the President, or Investor Relations will often cause the bank to swing into motion in the same way. (I wrote a few hundred letters like that <a href="https://www.kalzumeus.com/2017/09/09/identity-theft-credit-reports/#ghostwriting">as a hobby</a> back in the day.)</p><p>These folks are professionals who are capable of keeping paper notes and having day-to-day recollection of things they have done in complex cases. They are managed and incentivized in a way which allows them to have agency. The formal customer support organization is <em>very, very bad at this</em>, at every tier. It is very difficult to do in a model where you’re constantly bouncing cases around individual reps and between departments.</p><p>Is this because banks are malicious? Are they willing to grind retail users to bonemeal in the pursuit of another cent of earnings per share? The truth is a bit more mundane: supporting people with can-do-anything-you-throw-at-them professionals is ridiculously expensive and getting moreso over time. The per-case cost for the troubleshooting team can be more than 100X that of the tiering system.</p><p>Retail customers have relationships where they pay highly-educated high-agency jacks-of-all-trades to provide professional services in arbitrarily complex situations. <em>They hate the experience of those relationships.</em> They hate their medical bills. They are incredulous that lawyers bill hundreds of dollars per hour (in six minute increments).</p><p>You can get something approaching this level of service out of a bank, too, and the private bank generating $150,000+ in annual revenue per client would be happy to make your acquaintance. But if you want phone calls to the bank to be both answered at 2 AM and absolutely free, you want the tiering system. <em>Society</em> wants the tiering system. It is why a high school student with a paper route can open a checking account, get a debit card, and start buying things on Amazon. It is why bank branches can be operated in working class neighborhoods.</p><p>As a sophisticated user of the banking system, a useful skill to have is understanding whether the ultimate solution to an issue facing you is probably available to Tier Two or probably only available to a professional earning six figures a year. You can then route your queries to the bank to get in front of the appropriate person with the minimal amount of effort expended on making this happen.</p><p>You might think bank would hate this, and aggressively direct people who discover side channels to Use The 1-800 Number That Is What It Is For. For better or worse, the side channels are not an accident. They are <em>extremely intentionally designed</em>. Accessing them often requires performance of being a professional-managerial class member or otherwise knowing some financial industry shibboleths. This is <em>not accidental</em>; that greatly cuts down on “misuse” of the side channels by <em>that guy</em>.</p><p>It is also much more institutionally palatable to the bank and other stakeholders like e.g. regulators. No financial institution can say “We offer differential service levels to our community based on their education level, perceived social class, and perceived capability to bring power to bear on their behalf.” <em>Every financial institution factually does that.</em> The successful way to phrase it is “We offer contextually appropriate services to the entire range of customers, who come from all walks of life, <em>and also</em> we respond with alacrity to any issues impacting our important stakeholders via a variety of programs.”&nbsp;</p><h2 id="society-has-goals-which-conflict-with-banks-being-good-at-banking">Society has goals which conflict with banks being good at banking</h2><p>I hate sounding like a conspiracy theorist about banks, which for whatever reason seem to attract a disproportionate amount of attention from people who believe the Illuminati and lizardmen are conspiring to corrupt the free peoples of the world. And so ordinarily I do not want to say crazy things like “Sometimes banks suck because we want them to suck.”</p><p>Sometimes banks suck because we want them to suck.</p><p>In the specific case of “Why did the bank close my account, seemingly for no reason? Why will no one tell me anything about this? Why will no one take responsibility?”, the answer is frequently that the bank is following the law. As we’ve <a href="https://www.bitsaboutmoney.com/archive/money-laundering-and-aml-compliance/">discussed previously</a>, banks will frequently make the “independent” “commercial decision” to “exit the relationship” with a particular customer after that customer has had multiple Suspicious Activity Reports filed. SARs can (and sometimes must!) be filed for innocuous reasons and do not necessarily imply any sort of wrongdoing.</p><p>SARs are secret, by regulation. See <a href="https://www.law.cornell.edu/cfr/text/12/21.11#:~:text=A%20SAR%2C%20and%20any%20information,in%20this%20paragraph%20(k).">12 CFR § 21.11(k)(1)</a> from the Office of Comptroller of the Currency:</p><blockquote>No national <a href="https://www.law.cornell.edu/definitions/index.php?width=840&amp;height=800&amp;iframe=true&amp;def_id=1b4e13db0fedfcf4130540b3d34ef442&amp;term_occur=999&amp;term_src=Title:12:Chapter:I:Part:21:Subpart:B:21.11">bank</a>, and no director, officer, employee, or agent of a national <a href="https://www.law.cornell.edu/definitions/index.php?width=840&amp;height=800&amp;iframe=true&amp;def_id=1b4e13db0fedfcf4130540b3d34ef442&amp;term_occur=999&amp;term_src=Title:12:Chapter:I:Part:21:Subpart:B:21.11">bank</a>, shall disclose a SAR or any information that would reveal the existence of a SAR. Any national <a href="https://www.law.cornell.edu/definitions/index.php?width=840&amp;height=800&amp;iframe=true&amp;def_id=1b4e13db0fedfcf4130540b3d34ef442&amp;term_occur=999&amp;term_src=Title:12:Chapter:I:Part:21:Subpart:B:21.11">bank</a>, and any director, officer, employee, or agent of any national <a href="https://www.law.cornell.edu/definitions/index.php?width=840&amp;height=800&amp;iframe=true&amp;def_id=1b4e13db0fedfcf4130540b3d34ef442&amp;term_occur=999&amp;term_src=Title:12:Chapter:I:Part:21:Subpart:B:21.11">bank</a> that is subpoenaed or otherwise requested to disclose a SAR, or any information that would reveal the existence of a SAR, shall decline to produce the SAR or such information, citing this section and <a href="https://www.law.cornell.edu//uscode/text/31/5318">31 U.S.C. 5318(g)(2)(A)(i)</a>...</blockquote><p>If the United States brings its subpoena power to bear against a bank teller and asks them about a SAR, they’re supposed to say nothing. That is the law! (Regulation, well, if one wants to be technical.) It is designed to be enforced <em>against the interests of the United States of America</em>! Customers have far less access than the U.S. awards to the U.S.! So does the teller, incidentally: to avoid constantly violating this, Compliance at most functioning institutions has long-since decided that SARs will live in their own walled garden of a subsystem, seen only by the people responsible for drafting them and sending them to FinCEN.</p><p>That subsystems’ interactions with every other system are, of course, a site for <em>extremely painful</em> hilarity to happen. If, for example, a SAR is misfiled because that subsystem doesn’t share the same view of account ownership as another part of the overall system, investigating that problem might require telling the customer that they were investigated, which you cannot do. And because this is insufficiently Kafkaesque, at some financial institutions, you can get a SAR filed for knowing what a SAR is, because “advanced knowledge of anti-moneylaundering procedure” is a characteristic only of financial professionals and terrorists. Compliance training can tell e.g. personal bankers to please look at the Know Your Customer questionnaire and see if “Professional background: I work in finance” is bubbled in and then draw the appropriate inference.</p><p>You might think I am joking. I am utterly not joking. Most of the times infelicities in the world have a logical explanation to them, a structural cause where each individual link in the chain sounded good at the time and the result just happens to be suboptimal. And sometimes the world is <em>absolutely batshit insane</em>.</p><h2 id="so-what-can-be-done-about-this">So what can be done about this?</h2><p>Like many structural problems, banks lacking object permanence didn’t happen overnight and can’t be fixed overnight.</p><p>A lot of the fix is technical. In the not-too-distant past, there were zero—<em>zero</em>—financial institutions which were competent at software. There are now a handful of them, after the expenditure of many tens of billions of dollars. That was the price of getting without-loss-of-generality Chase to the point where in-house engineers can cause the retail web app to react to me having an account at First Republic less than a year after their purchase of that bank.</p><p>Although it certainly doesn’t feel like it to people who hit edge cases, the tiered support model is a technology which took us decades to popularize and which <em>made the world much better</em>. It brought down the cost of financial services and supported product innovation which would have been impossible under the mid-century bank staffing model. We could not have credit cards or discount brokerages without the tiered support model. The <a href="https://www.amazon.com/Invested-Changing-Forever-Americans-Invest-ebook/dp/B07MYKVLCL/">biography of Charles Schwab</a> makes this point persuasively at considerable length: competent telephone operations were instrumental to bringing equity ownership to the middle class. You should prefer a world with credit cards and discount brokerages to one which doesn’t have them, even as you listen to hold music occasionally.</p><p>It will similarly take decades to roll out the best-functioning refinements on customer service at scale to the entirety of the financial system. Partly this will happen through continued consolidation; the more banks Chase ends up owning, the higher the average operational competence in the U.S. financial system is. (And that’s… saying something.)</p><p>Partly this will happen as banks increasingly tap external providers for technology where the right things are recorded automatically and actioned appropriately. Partly, I continue to expect Operations to come further into its own as a high-status discipline, and to rewrite the internal structure of banks just as engineering has done over the last two decades.</p><p>Partly this will happen as banks increasingly partner with firms that impose a tech-inflected view of the customer experience. Google is, for example, a legendarily hostile organization to attempt to get customer support from. Google is also beloved by users because of overwhelming competence in shipping products that work almost all of the time. If you think that talking to a compassionate human is a core part of the banking experience, there are many banks in Iowa who will sell that service to you. If you simply want to access your money on your phone and have that almost always work, Cash App will happily operate as a front end over Lincoln Savings Bank to make that happen. (There are <a href="https://www.theinformation.com/articles/tiny-banks-that-powered-cash-app-grew-like-crazy-then-the-feds-came-calling">many layers</a> to that onion. No particular equilibrium is necessarily the “right” one!)</p><p>And partly, we as a society have to make some tradeoffs. We <em>want something</em> from 12 CFR § 21.11(k)(1) . It was not written by accident or because the drafters were stupid. Every future with 12 CFR § 21.11(k)(1) in it will include many Americans whose bank accounts are closed for no reason that can be disclosed to them. Many of them will have done nothing wrong.</p><p>Plausibly, we should decide to stop doing the thing that no one wants us to do. And, as a particular thing which could help unlock that: if one cares a lot about the experience of people at the socioeconomic margins, one should perhaps spend less time fulminating about greedy capitalists and spend more time reading Requests For Public Comment by relatively obscure parts of the administrative state.</p>

        

        <div>
          <h2>Want more essays in your inbox?</h2>
          <p>I write about the intersection of tech and finance, approximately biweekly. It's free.</p>
                  </div>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why only 1% of the Snowden Archive will ever be published (116 pts)]]></title>
            <link>https://www.computerweekly.com/news/366554957/Why-only-1-of-the-Snowden-Archive-will-ever-be-published</link>
            <guid>38180197</guid>
            <pubDate>Tue, 07 Nov 2023 17:49:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.computerweekly.com/news/366554957/Why-only-1-of-the-Snowden-Archive-will-ever-be-published">https://www.computerweekly.com/news/366554957/Why-only-1-of-the-Snowden-Archive-will-ever-be-published</a>, See on <a href="https://news.ycombinator.com/item?id=38180197">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-header">
	
	






	

	<h2>Speaking to Computer Weekly after we published new revelations from the Snowden archive, the Guardian’s Pulitzer Prize winner, Ewen MacAskill, explains why more of the Snowden trove is unlikely to see the light of day</h2>
</div><div id="content-center">
					<!-- EzinePromoController, generated at 13:50:14 Tue Nov 7, 2023, by cds1 -->
<!-- ContentItemController, generated at 12:29:19 Tue Nov 7, 2023, by cds1 -->













<ul>
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	


	
		<li><i data-icon="1"></i></li>
	
	
		<li><i data-icon="2"></i></li>
	
	
		
	
</ul>		<section id="content-body">
<p>Some 10 years after he flew to Hong Kong to meet Edward Snowden with Glenn Greenwald and Laura Poitras, <em>The Guardian’s</em> Pulitzer Prize winner, Ewen MacAskill, talks to Computer Weekly about the Snowden files.</p>
 
<p>MacAskill was speaking after Computer Weekly revealed the <a href="https://www.computerweekly.com/news/366552520/New-revelations-from-the-Snowden-archive-surface">first new facts to emerge from the Snowden files</a> since the archive first made headlines in 2013.</p> 
<p>The three new revelations have surfaced for the first time only thanks to a highly technical publication: a doctoral thesis authored by US investigative journalist and postdoctoral researcher Jacob Appelbaum, as part of his degree in applied cryptography from the Eindhoven University of Technology in the Netherlands.</p> 
<p>Their publication by Computer Weekly has revived the debate as to why the entire Snowden archive has never been published, considering that even after a decade the three revelations remain indisputably in the public interest, and it is reasonable to assume there are many others like them.</p> 
<p>MacAskill, who shared the Pulitzer Prize for Public Service with Glenn Greenwald and Laura Poitras for their journalistic work on the Snowden files, retired from <em>The Guardian</em> in 2018. He told Computer Weekly that:&nbsp;</p> 
<ul> 
 <li>As far as he knows, a copy of the documents is still locked in the <em>New York Times</em> office. Although the files are in the <em>New York Times</em> office, <em>The Guardian</em> retains responsibility for them.</li> 
 <li>As to why the New York Times has not published them in a decade, MacAskill maintains “this is a complicated issue”. “There is, at the very least, a case to be made for keeping them for future generations of historians,” he said.</li> 
 <li>Why was only 1% of the Snowden archive published by the journalists who had full access to it? Ewen MacAskill replied: “The main reason for only a small percentage – though, given the mass of documents, 1% is still a lot – was diminishing interest.”</li> 
</ul> 
<p>The Snowden archive allows exposing and documenting the rise of the mass-surveillance state, a serious threat to democracy. Have the journalists and media with access to the full archive done everything they can to expose this threat? That is the crux of the matter, because even in a democracy bad people can be elected who could use such unprecedented Orwellian control to crush any opposition. Legendary Pentagon Papers whistleblower Daniel Ellsberg said: “As Snowden has put it, we’re a ‘turnkey tyranny’: in other words, turn a switch, and we could be a total police state.”</p> 
<section data-menu-title="Mass surveillance and loss of privacy">
 <h2><i data-icon="1"></i>Mass surveillance and loss of privacy</h2>
 <p>MacAskill tells Computer Weekly: “That is what we did. With hindsight, we could have done some things better. But those stories reverberated around the world and still do today. Snowden wanted to alert the world to the scale of mass surveillance and loss of privacy, and he succeeded in that. He believes that those living in democracies have a right to know.</p>
 <p>“Although the NSA and GCHQ have since developed better tools and surveillance is more intrusive than ever, Snowden has increased public awareness of the threat posed by loss of privacy,” he said. “Much of the public may be apathetic, but at least they know.”</p>
 <p>MacAskill said he only worked on a small selection of documents from the archive, when he met the former CIA whistleblower in Hong Kong. There, Snowden gave him a memory stick with tens of thousands of documents from the National Security Agency (NSA) and its British partner, GCHQ, which formed the basis of the subsequent reporting by <em>The Guardian</em>. <em>The Guardian</em> shared the documents with <em>The New York Times</em> and ProPublica, and were to work alongside journalists from those organisations.</p>
</section>    
<section data-menu-title="Three revelations">
 <h2><i data-icon="1"></i>Three revelations</h2>
 <p><em>The Guardian’s</em> journalist did not recall seeing the three revelations published by Computer Weekly, summarised below:</p>
 <ul> 
  <li>The NSA listed Cavium, an American semiconductor company marketing Central Processing Units (CPUs) – the main processor in a computer which runs the operating system and applications – as a successful example of a “SIGINT-enabled” CPU supplier. Cavium, now owned by Marvell, said it does not implement back doors for any government.</li> 
  <li>The NSA compromised lawful Russian interception infrastructure, SORM. The NSA archive contains slides showing two Russian officers wearing jackets with a slogan written in Cyrillic: “You talk, we listen.” The NSA and/or GCHQ has also compromised key lawful interception systems.</li> 
  <li>Among example targets of its mass-surveillance programme, PRISM, the NSA listed the Tibetan government in exile.</li> 
 </ul>
 <p>“Given the sheer volume of documents, it is possible I and reporters from <em>The Guardian</em>, <em>The New York Times</em> and ProPublica missed them or were more interested in other documents. Or it could be that the documents you refer to are in the main archive, which, as far as I know, only Laura Poitras and Glenn Greenwald had access to.”</p>
 <p>He said he worked on “only a small selection of documents from the archive while in Hong Kong, though these contained the stories that were to have the most impact, such as the mass collection of US phone records and the revelations of the PRISM programme”.</p>
 <p>Why was only 1% of the documents published, in the end? “The documents are not like the WikiLeaks ones from the US state department, which were written by diplomats and, for the most part, easily understandable,” said Ewen MacAskill.</p>
 <p>“The Snowden files are largely technical, with lots of codewords and jargon that is hard to decipher. There are pages and pages of that which the public would not be interested in. There are also documents that relate to operational matters. Snowden said from the start he wanted us to report on issues related to mass surveillance, not operational matters. So we stuck to that.”</p>
</section>       
<section data-menu-title="Snowden did not want documents published en masse">
 <h2><i data-icon="1"></i>Snowden did not want documents published en masse</h2>
 <p><em>The Guardian’s</em> Pulitzer Prize winner said the main reason why only a small percentage was published was due to diminishing interest. “<em>The Guardian</em> published lots of stories from the Snowden files for months and months after Hong Kong,” he said. “But it reached a point where each story attracted smaller and smaller readerships, as interest dwindled.</p>
 <p>“The feeling at <em>The Guardian</em> – and, I assume, at <em>The New York Times</em> and ProPublica – was they had reported on the biggest stories in the documents and there was diminishing interest in publishing more.</p>
 <p>“The feeling, too, at <em>The Guardian</em> was that by continuing to report on stories that attracted less interest, we were in danger of undermining the impact of the initial ones. <em>The Intercept</em>, which had access to more documents than us, continued publishing for a while after us.”</p>
 <p>The three unpublished revelations revealed by Computer Weekly, thanks to Jacob Appelbaum’s doctoral thesis, confirm it is reasonable to assume the archive still contains important information in the public interest. According to Appelbaum: “Even if the privacy-violating intercepts are excluded from publication, there is an entire parallel history in that archive.”</p>
 <p>We asked McAskill why <em>The New York Times</em> hasn’t published them in a decade. “This is a complicated issue,” he said. “Although the files are in the <em>New York Times</em> office, <em>The Guardian</em> retains responsibility for them. Should more journalists be given access to the Snowden documents? In that case, who should decide which journalists get to see them? Should the whole lot just be published for everyone to see? Snowden did not want the documents to be published en masse.</p>
</section>      
<section data-menu-title="Espionage Act">
 <h2><i data-icon="1"></i>Espionage Act</h2>
 <p>“The bottom line is that Snowden is facing charges under the Espionage Act. If he was ever to return to the US and face trial, the documents could be used against him. All journalists have a duty to protect source material. How best to do that? How long would <em>The</em> <em>New York Times</em> be willing to store them? Where else could they be stored? Should the documents be destroyed?”</p>
 <p>MacAskill acknowledges that “there is, at the very least, a case to be made for keeping them for future generations of historians”.</p>
 <p>“Is there a university that would be prepared to take them?” he suggested. “But that would be expensive, and could they ensure they would be secure?”</p>
 <p>MacAskill left the staff of <em>The Guardian</em> in 2018. “I don’t know what discussions, if any, have taken place between <em>The Guardian</em> and <em>The New York Times</em> since then,” he said.&nbsp;</p>
</section></section>










<!-- DownloadOfferController, generated at 13:50:15 Tue Nov 7, 2023, by cds1 -->
<!-- AskAnExpertController, generated at 13:50:15 Tue Nov 7, 2023, by cds1 -->
<!-- DigDeeperController, generated at 12:58:34 Tue Nov 7, 2023, by cds1 -->
<section id="DigDeeperSplash">
		<h4>
			<i data-icon="m"></i>Read more on Privacy and data protection</h4>
		<ul>
			<li><a id="DigDeeperItem-1" href="https://www.computerweekly.com/news/366552520/New-revelations-from-the-Snowden-archive-surface">
					<img data-src="https://cdn.ttgtmedia.com/visuals/ComputerWeekly/Hero Images/US-NSA-aerial-day-hero_searchsitetablet_520X173.jpg" data-srcset="https://cdn.ttgtmedia.com/visuals/ComputerWeekly/Hero%20Images/US-NSA-aerial-day-hero_searchsitetablet_520X173.jpg 960w,https://cdn.ttgtmedia.com/visuals/ComputerWeekly/Hero%20Images/US-NSA-aerial-day-hero.jpg 1280w" alt="" srcset="https://cdn.ttgtmedia.com/visuals/ComputerWeekly/Hero%20Images/US-NSA-aerial-day-hero_searchsitetablet_520X173.jpg 960w,https://cdn.ttgtmedia.com/visuals/ComputerWeekly/Hero%20Images/US-NSA-aerial-day-hero.jpg 1280w">
					<h5>New revelations from the Snowden archive surface</h5>
						
				</a></li>
			<li><a id="DigDeeperItem-2" href="https://www.techtarget.com/searchsecurity/definition/National-Security-Agency">
					<img data-src="https://cdn.ttgtmedia.com/visuals/digdeeper/2.jpg" data-srcset="https://cdn.ttgtmedia.com/visuals/digdeeper/2_searchsitetablet_520X173.jpg 960w,https://cdn.ttgtmedia.com/visuals/digdeeper/2.jpg 1280w" alt="" src="https://cdn.ttgtmedia.com/visuals/digdeeper/2.jpg" srcset="https://cdn.ttgtmedia.com/visuals/digdeeper/2_searchsitetablet_520X173.jpg 960w,https://cdn.ttgtmedia.com/visuals/digdeeper/2.jpg 1280w">
					<h5>National Security Agency (NSA)</h5>
						<div>
							<p><img src="https://cdn.ttgtmedia.com/rms/onlineimages/patrizio_andy.jpg" alt="AndyPatrizio">
									</p>
								<p><span>By: <span>Andy&nbsp;Patrizio</span></span>
							</p></div>
				</a></li>
			<li><a id="DigDeeperItem-3" href="https://www.computerweekly.com/news/252489133/Julian-Assange-held-back-15000-documents-to-prevent-harm-to-US-government">
					<img data-src="https://cdn.ttgtmedia.com/visuals/ComputerWeekly/Hero Images/Julian-Assange-trial-Sept-2020-CREDIT-Jekaterina-Saveljeva-hero_searchsitetablet_520X173.jpg" data-srcset="https://cdn.ttgtmedia.com/visuals/ComputerWeekly/Hero%20Images/Julian-Assange-trial-Sept-2020-CREDIT-Jekaterina-Saveljeva-hero_searchsitetablet_520X173.jpg 960w,https://cdn.ttgtmedia.com/visuals/ComputerWeekly/Hero%20Images/Julian-Assange-trial-Sept-2020-CREDIT-Jekaterina-Saveljeva-hero.jpg 1280w" alt="" srcset="https://cdn.ttgtmedia.com/visuals/ComputerWeekly/Hero%20Images/Julian-Assange-trial-Sept-2020-CREDIT-Jekaterina-Saveljeva-hero_searchsitetablet_520X173.jpg 960w,https://cdn.ttgtmedia.com/visuals/ComputerWeekly/Hero%20Images/Julian-Assange-trial-Sept-2020-CREDIT-Jekaterina-Saveljeva-hero.jpg 1280w">
					<h5>Julian Assange held back 15,000 documents to prevent harm to US government</h5>
						<div>
							<p><img src="https://cdn.ttgtmedia.com/rms/computerweekly/Bill-Goodwin-CW-contributor-2022-140x180px.jpg" alt="BillGoodwin">
									</p>
								<p><span>By: <span>Bill&nbsp;Goodwin</span></span>
							</p></div>
				</a></li>
			<li><a id="DigDeeperItem-4" href="https://www.computerweekly.com/opinion/11-obscure-questions-Facebook-Max-Schrems-and-the-European-Court-of-Justice">
					<img data-src="https://cdn.ttgtmedia.com/rms/computerweekly/Max-Schrems-hero_searchsitetablet_520X173.jpg" data-srcset="https://cdn.ttgtmedia.com/rms/computerweekly/Max-Schrems-hero_searchsitetablet_520X173.jpg 960w,https://cdn.ttgtmedia.com/rms/computerweekly/Max-Schrems-hero.jpg 1280w" alt="" src="https://cdn.ttgtmedia.com/rms/computerweekly/Max-Schrems-hero_searchsitetablet_520X173.jpg" srcset="https://cdn.ttgtmedia.com/rms/computerweekly/Max-Schrems-hero_searchsitetablet_520X173.jpg 960w,https://cdn.ttgtmedia.com/rms/computerweekly/Max-Schrems-hero.jpg 1280w">
					<h5>11 obscure questions, Facebook, Max Schrems and the European Court of Justice</h5>
						<div>
							<p><img src="https://cdn.ttgtmedia.com/rms/computerweekly/Kevin-Cahill-CW-contributor.jpg" alt="KevinCahill">
									</p>
								<p><span>By: <span>Kevin&nbsp;Cahill</span></span>
							</p></div>
				</a></li>
			</ul>
	</section>
<!-- EHandbookController, generated at 12:58:16 Tue Nov 7, 2023, by cds1 -->
<!-- CollectionController, generated at 12:58:16 Tue Nov 7, 2023, by cds1 -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[U.S. military members' personal data being sold by online brokers (154 pts)]]></title>
            <link>https://www.axios.com/2023/11/06/military-data-sold-for-cents-cheap-privacy</link>
            <guid>38180014</guid>
            <pubDate>Tue, 07 Nov 2023 17:39:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.axios.com/2023/11/06/military-data-sold-for-cents-cheap-privacy">https://www.axios.com/2023/11/06/military-data-sold-for-cents-cheap-privacy</a>, See on <a href="https://news.ycombinator.com/item?id=38180014">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-theme="core" data-overlay-container="true" id="__next"><header><nav aria-label="Primary" data-cy="top-nav-header"></nav></header><main id="main-content"><div data-theme="core"><div data-vars-content-id="db774c90-c9b7-4fb2-8014-1ce74aa26dd2" data-vars-headline="Study: U.S. military members' personal data being sold by online brokers" data-vars-category="story" data-vars-sub-category="story"><div><div><p><img alt="headshot" loading="lazy" width="52" height="52" decoding="async" data-nimg="1" srcset="https://www.axios.com/_next/image?url=https%3A%2F%2Fimages.axios.com%2F8DRdJWpR5xDCrc2WmI-xkbYz0ug%3D%2F0x0%3A328x328%2F52x0%2F2020%2F05%2F01%2F1588371366979.jpg&amp;w=320&amp;q=75 1x" src="https://www.axios.com/_next/image?url=https%3A%2F%2Fimages.axios.com%2F8DRdJWpR5xDCrc2WmI-xkbYz0ug%3D%2F0x0%3A328x328%2F52x0%2F2020%2F05%2F01%2F1588371366979.jpg&amp;w=320&amp;q=75"></p></div><div><ul><li data-cy="byline-author"><a href="https://www.axios.com/authors/jknutson"><span>Jacob Knutson</span></a></li></ul></div></div><figure data-cy="au-image"><img data-cy="StoryImage" alt="Members of a military honor guard marching in Washington, D.C., in October 2023." fetchpriority="high" width="1920" height="1080" decoding="async" data-nimg="1" sizes="100vw" srcset="https://images.axios.com/VRvj8WXOIkpPfYSLyXWpppvJRIw=/0x217:8000x4717/320x180/2023/11/06/1699287620638.jpg?w=320 320w, https://images.axios.com/VRvj8WXOIkpPfYSLyXWpppvJRIw=/0x217:8000x4717/320x180/2023/11/06/1699287620638.jpg?w=320 320w, https://images.axios.com/ZY4XOcCtS0XtRmymz2x-6TgPVCA=/0x217:8000x4717/640x360/2023/11/06/1699287620638.jpg?w=640 640w, https://images.axios.com/ZY4XOcCtS0XtRmymz2x-6TgPVCA=/0x217:8000x4717/640x360/2023/11/06/1699287620638.jpg?w=640 640w, https://images.axios.com/Qpz4AXkEJ0FKoglrdTHInSDzzTw=/0x217:8000x4717/768x432/2023/11/06/1699287620638.jpg?w=768 768w, https://images.axios.com/Qpz4AXkEJ0FKoglrdTHInSDzzTw=/0x217:8000x4717/768x432/2023/11/06/1699287620638.jpg?w=768 768w, https://images.axios.com/Dd5PDa9AnD0TjedLbukTVZoFzF8=/0x217:8000x4717/1024x576/2023/11/06/1699287620638.jpg?w=1024 1024w, https://images.axios.com/Dd5PDa9AnD0TjedLbukTVZoFzF8=/0x217:8000x4717/1024x576/2023/11/06/1699287620638.jpg?w=1024 1024w, https://images.axios.com/6Bh9BlqhwhNRYsMOYXHX-jJ1has=/0x217:8000x4717/1366x768/2023/11/06/1699287620638.jpg?w=1366 1366w, https://images.axios.com/6Bh9BlqhwhNRYsMOYXHX-jJ1has=/0x217:8000x4717/1366x768/2023/11/06/1699287620638.jpg?w=1366 1366w, https://images.axios.com/VbfTYCZ6tX51Dn8bDiPttQ2EoB8=/0x217:8000x4717/1600x900/2023/11/06/1699287620638.jpg?w=1600 1600w, https://images.axios.com/VbfTYCZ6tX51Dn8bDiPttQ2EoB8=/0x217:8000x4717/1600x900/2023/11/06/1699287620638.jpg?w=1600 1600w, https://images.axios.com/DXwCc83muwr-D7Y6ZljLbu3v8ss=/0x217:8000x4717/1920x1080/2023/11/06/1699287620638.jpg?w=1920 1920w, https://images.axios.com/DXwCc83muwr-D7Y6ZljLbu3v8ss=/0x217:8000x4717/1920x1080/2023/11/06/1699287620638.jpg?w=1920 1920w" src="https://images.axios.com/DXwCc83muwr-D7Y6ZljLbu3v8ss=/0x217:8000x4717/1920x1080/2023/11/06/1699287620638.jpg?w=1920"><figcaption><p>Members of a military honor guard marching in Washington, D.C., in October 2023. Photo: Drew Angerer/Getty Images</p></figcaption></figure><div><p>Sensitive, highly detailed personal data for thousands of active-duty and veteran U.S. military members can be purchased for as little as one cent per name through data broker websites, according to a <a data-vars-link-text="new study published" data-vars-click-url="https://techpolicy.sanford.duke.edu/wp-content/uploads/sites/4/2023/11/Sherman-et-al-2023-Data-Brokers-and-the-Sale-of-Data-on-US-Military-Personnel.pdf" data-vars-content-id="db774c90-c9b7-4fb2-8014-1ce74aa26dd2" data-vars-headline="Study: U.S. military members' personal data being sold by online brokers" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://techpolicy.sanford.duke.edu/wp-content/uploads/sites/4/2023/11/Sherman-et-al-2023-Data-Brokers-and-the-Sale-of-Data-on-US-Military-Personnel.pdf" target="_blank">new study published</a> on Monday by Duke University researchers.</p><p><strong>Why it matters: </strong>Researchers warned that the data can be easily obtained and used by malicious actors to target current and former military personnel, their families and acquaintances with a myriad of schemes, including blackmail and misinformation campaigns.</p><ul><li>The data about military personnel purchased as part of the study included full names, physical and email addresses, health and financial information and details about their ethnicity, religious practices and political affiliation.</li><li>In some cases, the information also included whether the person owned or rented a home, was married or had children. The children's ages and sexes were accessible, too.</li></ul><p><strong>How it works: </strong>As part of the study, the researchers contacted 12 data brokers about purchasing information on military personnel.</p><ul><li>The researcher found that many of the brokers lacked controls on who could purchase the data or regulations to ascertain the intended uses for the information.</li><li>In making their purchases, the researchers were able to narrow down their data selections to personnel in Maryland, Virginia, or the District of Columbia. </li><li>In one data set, the results showed service members living near military installations including Virginia's Quantico and Fort Walker, formerly known as Fort AP Hill, and North Carolina's Fort Liberty, formerly known as Fort Bragg. </li><li>Thousands of data brokers, many of which are based in the U.S., collect and sell data on millions of people every year.</li><li>The multi-billion-dollar industry collects data on virtually every American, primarily through public records or other businesses — such as mobile app companies and credit reporting agencies — collecting data on their customers and selling it.</li></ul><p><strong>By the numbers: </strong>The researchers bought data on up to around 45,000 military personnel for between $0.12 to $0.32 per record. </p><ul><li>They also bought data belonging to 5,000 friends and family members of military personnel.</li><li>Larger data purchases of over 1.5 million service members were available for as little as $0.01 per record from at least one broker the researchers contacted.</li></ul><p><strong>The big picture:</strong> The researchers called on Congress to pass a comprehensive privacy law and for regulatory agencies like the Federal Trade Commission to develop rules to govern military personnel data purchases.</p><p><strong>Thought bubble</strong>: The report is especially concerning given nation-state adversaries and governments are also interested in buying information from data brokers. </p><ul><li>Although brokers typically collect and sell information about a wide range of Americans, international spies are likely to find unique value in data sets focused on military personnel for their operations targeting classified U.S. sources.</li></ul><p><strong>Go deeper: </strong><a data-vars-link-text="2023 toll of data breaches and leaks already tops 2022" data-vars-click-url="https://www.axios.com/2023/10/13/2023-data-compromises-surpass-2022" data-vars-content-id="db774c90-c9b7-4fb2-8014-1ce74aa26dd2" data-vars-headline="Study: U.S. military members' personal data being sold by online brokers" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.axios.com/2023/10/13/2023-data-compromises-surpass-2022" target="_self">2023 toll of data breaches and leaks already tops 2022</a></p></div></div><h5>Go deeper</h5></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Toyota's new $10k pickup (121 pts)]]></title>
            <link>https://www.motortrend.com/reviews/2025-toyota-imv-0-pickup-truck-first-drive-review-japan-mobility-show/</link>
            <guid>38178727</guid>
            <pubDate>Tue, 07 Nov 2023 16:21:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.motortrend.com/reviews/2025-toyota-imv-0-pickup-truck-first-drive-review-japan-mobility-show/">https://www.motortrend.com/reviews/2025-toyota-imv-0-pickup-truck-first-drive-review-japan-mobility-show/</a>, See on <a href="https://news.ycombinator.com/item?id=38178727">Hacker News</a></p>
Couldn't get https://www.motortrend.com/reviews/2025-toyota-imv-0-pickup-truck-first-drive-review-japan-mobility-show/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Fedora 39 Released (154 pts)]]></title>
            <link>https://fedoramagazine.org/announcing-fedora-linux-39/</link>
            <guid>38178666</guid>
            <pubDate>Tue, 07 Nov 2023 16:17:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fedoramagazine.org/announcing-fedora-linux-39/">https://fedoramagazine.org/announcing-fedora-linux-39/</a>, See on <a href="https://news.ycombinator.com/item?id=38178666">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					
						
<p>On November 6, 2003, the Fedora Project released the Fedora Core 1. One day and twenty years later, we’re pleased to bring you Fedora Linux 39, our complete, community-built operating system for desktops, laptops, servers, the cloud, edge devices — and just about anything else you can think of.</p>



<p>As always, you should make sure your system is fully up-to-date before upgrading from a previous release. Can’t wait to get started? <a href="https://fedoraproject.org/#editions">Download</a> while you read!</p>



<h2><strong>Desktop news</strong></h2>



<p>Fedora Workstation now features GNOME 45, which brings better performance and many usability enhancements, including a new workspace switcher and a much-improved image viewer.</p>



<p>If you’re looking for a different desktop experience, our Budgie Special Interest Group has created Fedora Onyx, a Budgie-based “Atomic” desktop in the spirit of Fedora Silverblue.&nbsp;</p>



<p>Of course, that’s not all — we also have updated desktop flavors featuring KDE Plasma Desktop, Xfce, Cinnamon, and more.</p>



<h2><strong>In the cloud</strong></h2>



<p>Fedora Cloud images will be officially available in Microsoft Azure (in addition to Google Cloud and AWS). Also, our cloud images now are configured so that cloud-init can (at your option) install updates and reboot when first provisioned, so you know you’re running with our latest security updates.</p>



<h2><strong>Other updates</strong></h2>



<p>As always, we’ve updated many, many other packages as we work to bring you the best of everything the free and open source software world has to offer. Fedora Linux 39 includes gcc 13.2, binutils 2.40, glibc 2.38, gdb 13.2, and rpm 4.19. It also has updates to popular programming language stacks, including Python 3.12 and Rust 1.73.</p>



<p>Of particular note, we include the latest version of Inkscape, the popular vector graphics illustration and drawing tool. Inkscape <em>also</em> turned 20 yesterday — we’re digital twins! Congratulations to everyone in that awesome project as well.</p>



<h2><strong>In the unlikely event of a problem…</strong></h2>



<p>If you run into a problem, visit our<a href="https://ask.fedoraproject.org/"> Ask Fedora</a> user support forum. This includes a category for <a href="https://discussion.fedoraproject.org/tags/c/ask/common-issues/82/none/f39">common issues</a>. (There are a few issues with Raspberry Pi in particular which we are still working to resolve. So if you’re planning on updating one of those, make sure to check first.)&nbsp;</p>



<h2>Or if you just want to say “hello”…</h2>



<p>Drop by our <a href="https://discussion.fedoraproject.org/c/fun/8">“virtual watercooler” on Fedora Discussion</a> and join a conversation, share something interesting, and introduce yourself.</p>



<p>You’re also invited to our virtual release party this Friday and Saturday. It’s free! And we’ll have interesting presentations and fun social events. <a href="https://hopin.com/events/fedora-linux-39-release-party/registration">Register here!</a></p>



<h2><strong>Thank you everyone</strong></h2>



<p>Thank you again to the thousands of people who contributed to the Fedora Project in this release cycle. You are amazing!</p>
						
											
					</div><div>
									<p><a href="https://fedoramagazine.org/author/mattdm/"></a>
										<img alt="" src="https://secure.gravatar.com/avatar/2fccc1daa2adc4c5004cbd389e9eecf7?s=96&amp;d=retro&amp;r=g" srcset="https://secure.gravatar.com/avatar/2fccc1daa2adc4c5004cbd389e9eecf7?s=192&amp;d=retro&amp;r=g 2x" height="96" width="96" loading="lazy" decoding="async"></p><!-- .avatar -->
									<h4><a href="https://fedoramagazine.org/author/mattdm/">Matthew Miller</a></h4>

									<p>Matthew is the <a href="https://docs.fedoraproject.org/en-US/council/fpl/">Fedora Project Leader</a>. You can find him on the Fedora mailing lists or Fedora Chat as "mattdm", or <a href="https://fosstodon.org/@mattdm">@mattdm@fosstodon.org </a> on Mastodon.

 Matthew's content on this site is made available under the <a href="https://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International</a> license (or an earlier CC-BY-SA license if you need that for compatibility) — share all you like, give credit, and let others share as well.</p>
								</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A four year plan for async Rust (201 pts)]]></title>
            <link>https://without.boats/blog/a-four-year-plan/</link>
            <guid>38178592</guid>
            <pubDate>Tue, 07 Nov 2023 16:12:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://without.boats/blog/a-four-year-plan/">https://without.boats/blog/a-four-year-plan/</a>, See on <a href="https://news.ycombinator.com/item?id=38178592">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><p>Four years ago today, the Rust async/await feature was released in version 1.39.0. The announcement
<a href="https://blog.rust-lang.org/2019/11/07/Async-await-stable.html">post</a> says that “this work has been a long time in development – the key
ideas for zero-cost futures, for example, were first proposed by Aaron Turon and Alex Crichton in
2016”. It’s now been longer since the release of async/await than the time between the first design
work that underlies async/await. Despite this, and despite the fact that async/await syntax was
explicitly shipped as a “minimum viable product,” the Rust project has shipped almost no extensions
to async/await in the four years since the MVP was released.</p><p>This fact has been noticed, and I contend it is the primary controllable reason that async Rust has
developed a negative reputation (other reasons, like its <a href="https://without.boats/blog/why-async-rust">essential complexity</a>, are
not in the project’s control). It’s encouraging to see project leaders like Niko Matsakis
<a href="https://smallcultfollowing.com/babysteps/blog/2023/10/14/eurorust-reflections/">recognize</a> the problem as well. I want to outline the features that I think async Rust needs
to consider improve its user experience. I’ve organized these features into features that I think
the project could ship in the short term (say, in the next 18 months), to those that will take
longer (up to three years), and finally a section on a potential change to the language that I think
would take years to plan and prepare for.</p><h2 id="near-term-features">Near-term features</h2><p>These features are all features that I believe the Rust project would be able to ship within the
next year or two. They all require relatively small changes to the compiler, because they depend on
abstractive capabilities that are already implemented, and they involve relatively small changes to
the surface syntax, largely new syntax implied already by the existing syntax. I think these are the
things the project should focus its attention on, because they should be easier to ship and easier
to build a consensus around.</p><h2 id="asynciterator-and-async-generators">AsyncIterator and async generators</h2><p>I’ve <a href="https://without.boats/blog/patterns-and-abstractions">harped on</a> the importance of generators to Rust repeatedly in the past, so I won’t
devote a lot of attention here. I’ve also highlighted before that the original plan for iterators
<a href="https://web.archive.org/web/20140716172928/https://mail.mozilla.org/pipermail/rust-dev/2013-June/004599.html">included</a> shipping generator syntax. Briefly, my opinion is that the absence of generators
has left Rust in a confused state, in which the relationship between asynchrony and iteration is
unclear (I elaborate more in my linked blog post). I want to focus specifically on <em>async</em> iterators
and <em>async</em> generators, and the features that are needed to complete these.</p><p>An async generator is a natural transformation from a generator: just like functions, generators
can be marked async, and now you can use the await operator inside of them. Using my preferred
syntax, this would look something like this:</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>async</span><span> </span><span>gen</span><span> </span><span>fn</span> <span>sum_pairs</span><span>(</span><span>rx</span>: <span>Receiver</span><span>&lt;</span><span>i32</span><span>&gt;</span><span>)</span><span> </span><span>yields</span><span> </span><span>i32</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>loop</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>        </span><span>let</span><span> </span><span>left</span><span> </span><span>=</span><span> </span><span>rx</span><span>.</span><span>next</span><span>().</span><span>await</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>let</span><span> </span><span>right</span><span> </span><span>=</span><span> </span><span>rx</span><span>.</span><span>next</span><span>().</span><span>await</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>yield</span><span> </span><span>left</span><span> </span><span>+</span><span> </span><span>right</span><span>;</span><span>
</span></span></span><span><span><span>    </span><span>}</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></div><p>The composition of these features falls out naturally from these syntaxes. Unlike a generator, an
async generator compiles to an <code>AsyncIterator</code>.</p><p>There is one other piece of syntax that is needed: <code>for await</code> loops. These can be called from
within any async context, and consume items from the <code>AsyncIterator</code>, yielding control when the
<code>AsyncIterator</code> yields pending:</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>for</span><span> </span><span>await</span><span> </span><span>item</span><span> </span><span>in</span><span> </span><span>async_iter</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>println!</span><span>(</span><span>"</span><span>{}</span><span>"</span><span>,</span><span> </span><span>item</span><span>);</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></div><p>When I was working on async Rust, this syntax was held up on two different design tangents. On the
one hand, Taylor Cramer thought that the feature was a poor choice because users should instead be
using <code>for_each_concurrent</code>, to get some concurrency. I do not agree with that: it’s not always the
case that users want to use <code>for_each_concurrent</code>, adding more internal concurrency to your async
function is a decision that needs to be considered with care, and there should be an obvious syntax
for when you don’t want that, which <code>for await</code> is. On the other hand, there was some speculation
about making “await patterns” that destructure futures and then somehow making that work here; I
think this would imprudent and leaving await as an expression, and <code>for await</code> as a special
expression for handling <code>AsyncIterator</code>, is the most sensible choice.</p><p>Revisiting the table from my previous blog post, you could add this column for async iteration:</p><table><thead><tr><th></th><th>Asynchronous Iteration</th></tr></thead><tbody><tr><td><strong>Context</strong></td><td><code>async gen</code></td></tr><tr><td><strong>Effect</strong> (iteration)</td><td><code>yield</code></td></tr><tr><td><strong>Forward</strong> (asynchrony)</td><td><code>await</code></td></tr><tr><td><strong>Complete</strong> (iteration)</td><td><code>for await</code></td></tr></tbody></table><p>The biggest thing blocking this is an issue on the library side: how should the <code>AsyncIterator</code>
interface be expressed. I’ve already <a href="https://doc.rust-lang.org/std/async_iter/trait.AsyncIterator.html">written</a> about my preference for stabilizing
<code>AsyncIterator</code> as-is, with the <code>poll_next</code> method. This remains a subject of some controversy,
so I will return to it, but not in this post.</p><p>For now I’ll just say that I think the failure to stabilize <code>AsyncIterator</code> over the past 4 years
(which was absolutely not our intention when we planned the async MVP) has been harmful to async
Rust, because APIs based on async iteration have been relegated to unstable features and
side-libraries, leaving users confused and poorly supported when they need to deal with repetitious
asynchronous events, a very common pattern. The single best thing the Rust project could do for
users is stabilize <code>AsyncIterator</code> so the ecosystem can build on it, and it could do that tomorrow.</p><p>The good news is that work is already <a href="https://github.com/rust-lang/rfcs/pull/3513">underway</a> on reserving the <code>gen</code> keyword in the next
edition, so that generators could be implemented. This feature is using the same state machine
transform that async functions already use, and by analogy should be feasible to implement without
big changes to the compiler. The only big unresolved questions with generators (and which doesn’t
apply to async generators, if <code>AsyncIterator</code> is stabilized as is) is how to make them
self-referential. I’ll return to that question later in this post.</p><h2 id="coroutine-methods">Coroutine methods</h2><p>Orthogonal to the introduction of these additional kinds of coroutines is their integration into the
trait system. Right now, you cannot define an async trait method in stable Rust. The good news is
that this is changing, and in a soon-to-be-released version of Rust, it will be possible to write an
async trait method. As other coroutines, generators and async generators should not require any
special support to use them in traits that wasn’t already implemented for async functions. So when
generators and async generators are implemented and stabilized, they should be supported as methods
out of the box.</p><p>The only thing that remains to be implemented for coroutine methods is the concept of “Return Type
Notation” (or RTN). The problem is that adding a coroutine method to a trait adds an anonymous
associated type to that trait, which is the return type of that method. Sometimes (most importantly:
when spawning that method in a task on a work-stealing executor or otherwise moving it to another
thread) users need to add additional bounds to that anonymous associated type. So Rust needs some
syntax for declaring that. This is RTN. For example:</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>trait</span><span> </span><span>Foo</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>async</span><span> </span><span>fn</span> <span>foo</span><span>(</span><span>&amp;</span><span>self</span><span>);</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>// later:
</span></span></span><span><span><span></span><span>where</span><span> </span><span>F</span>: <span>Foo</span><span> </span><span>+</span><span> </span><span>Send</span><span>,</span><span>
</span></span></span><span><span><span>      </span><span>F</span>::<span>foo</span><span>()</span>: <span>Send</span>
</span></span></code></pre></div><p>In my opinion, it is important to ship RTN because of a design principle I call the “Can you fix
it?” principle. If an upstream dependency of yours has an async method, and you need to add a <code>Send</code>
bound to the return type, can you fix it, or do you need to fork the library? Without the ability to
add RTN bounds to where clauses, you cannot express the bounds that you require without changing the
upstream code, even if your code is all perfectly valid (i.e. even if the async method you want to
call <em>is</em> <code>Send</code>). It’s very frustrating for users to encounter a problem in which their code should
compile fine, but the only way to satisfy the compiler is to fork a dependency.</p><p>Fortunately, the project is already focusing on this feature, and I expect it to be shipped in the
next year. There seems to be some discussion around the exact syntax for this feature: I would
encourage contributors not to be too obstinate over syntax differences that don’t substantially
change the feature.</p><h2 id="coroutine-closures">Coroutine closures</h2><p>Another aspect of Rust’s language design in which coroutines are currently not well-supported is
closures. Niko Matsakis has explored this issue in two recent blog posts, focusing only on async
closures and not on generative or asynchronously generative closures. In the <a href="https://smallcultfollowing.com/babysteps/blog/2023/03/29/thoughts-on-async-closures/">first</a>, he
proposed treating async closures as a new hierarchy of function traits (i.e. adding <code>AsyncFn</code>,
<code>AsyncFnMut</code>, and <code>AsyncFnOnce</code>). In the <a href="https://smallcultfollowing.com/babysteps/blog/2023/05/09/giving-lending-and-async-closures/">second</a>, he instead explores the idea of modeling
async closures as closures returning <code>impl Future</code> (e.g. <code>F: Fn() -&gt; impl Future</code>).</p><p>I prefer the second approach, because it does not result in a proliferation of more traits. This
becomes especially apparent when you consider generative closures and asynchronously generative
closures: if the function trait for each of these things were distinct, instead of 3 function
traits, Rust would have 12. In contrast, by modeling coroutine closures as closures returning an
<code>impl Trait</code>, no new traits are needed. It has the additional benefit that it involves modeling
them in the exact way that Rust already desugars normal async functions.</p><p>As Niko highlights in his blog post, this would require adapting the <code>Fn</code> traits to allow their
return type to capture input lifetimes. There are a few things that Niko calls out in his post that
require changing Rust’s syntax, possibly across an edition boundary:</p><ul><li>Adding a lifetime to the <code>Output</code> parameter of the <code>Fn</code> traits</li><li>Desugaring <code>-&gt; impl Trait</code> to a bound on the associated type projection instead of a new
variable</li></ul><p>Because these may require an edition change, the project should work through the specifics of these
changes immediately. But they do not seem like extremely thorny problems to work out.</p><p>There is one other thing I would add to this feature, though. Once you have <code>Fn() -&gt; impl Future</code>
and so on, it would be natural to extend the syntax to have a kind of “async sugar” (and “gen
sugar”) just like functions do. That is to say, special syntax sugar should be added to the <code>Fn</code>
traits that makes it possible to write closure bounds like this:</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>where</span><span> </span><span>F</span>: <span>async</span><span> </span><span>FnOnce</span><span>()</span><span> </span>-&gt; <span>T</span><span>
</span></span></span><span><span><span></span><span>// equivalent to:
</span></span></span><span><span><span></span><span>where</span><span> </span><span>F</span>: <span>FnOnce</span><span>()</span><span> </span>-&gt; <span>impl</span><span> </span><span>Future</span><span>&lt;</span><span>Output</span><span> </span><span>=</span><span> </span><span>T</span><span>&gt;</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>where</span><span> </span><span>F</span>: <span>gen</span><span> </span><span>FnOnce</span><span>()</span><span> </span><span>yields</span><span> </span><span>T</span><span>
</span></span></span><span><span><span></span><span>// equivalent to:
</span></span></span><span><span><span></span><span>where</span><span> </span><span>F</span>: <span>FnOnce</span><span>()</span><span> </span>-&gt; <span>impl</span><span> </span><span>Iterator</span><span>&lt;</span><span>Item</span><span> </span><span>=</span><span> </span><span>T</span><span>&gt;</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>where</span><span> </span><span>F</span>: <span>async</span><span> </span><span>gen</span><span> </span><span>FnOnce</span><span>()</span><span> </span><span>yields</span><span> </span><span>T</span><span>
</span></span></span><span><span><span></span><span>// equivalent to
</span></span></span><span><span><span></span><span>where</span><span> </span><span>F</span>: <span>FnOnce</span><span>()</span><span> </span>-&gt; <span>impl</span><span> </span><span>AsyncIterator</span><span>&lt;</span><span>Item</span><span> </span><span>=</span><span> </span><span>T</span><span>&gt;</span><span>
</span></span></span></code></pre></div><p>What’s nice about this is that it isn’t some new general-purpose abstractive concept like “trait
transformers” or “effect generics:” it’s just a little bit of sugar that is a natural extension of
sugar that already exists from one place (function declarations) to another (function trait bounds).
And these function traits already have special syntax, because they use parens and arrows for their
parameters and return type. This wouldn’t require a lot of implementation work or consensus on a
controversial new feature.</p><h2 id="medium-term-features">Medium-term features</h2><p>The features in the previous section were all features that I believe could be shipped without a
huge amount of implementation effort, and which don’t have many thorny open questions in their
design. The features in this section, on the other hand, are more difficult. It’s good that people
are already investigating them now, but they don’t seem very close to shipping and I wouldn’t expect
them in the next year or two.</p><h2 id="object-safe-coroutine-methods">Object-safe coroutine methods</h2><p>Though async trait methods will soon be a stable feature, they will not initially be object-safe. I
think this was the right decision, but it would be ideal if someday they could be. The problem with
object-safety is this: each coroutine method implies an anonymous associated type, which would have
a different size and layout in each implementation. In order to erase the static type of the trait
object, you also need to erase the type of that method’s anonymous return type: in other words, it
also needs to somehow be a trait object.</p><p>For our examples, we’ll consider this trait:</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>trait</span><span> </span><span>Foo</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>async</span><span> </span><span>fn</span> <span>foo</span><span>(</span><span>&amp;</span><span>self</span><span>);</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></div><p>If I want to make a trait object of <code>Foo</code>, I need to specify the return type of <code>Foo::foo</code>.
Thankfully, RTN starts to unravel this problem by allowing us this syntax: <code>Box&lt;dyn Foo&lt;foo() = Something&gt;&gt;</code> But what is <code>Something</code>? It can’t be a specific type, or else that limits the trait
object to implementations that return that type: in practice, this means limiting it to a single
specific type, and now it isn’t even a meaningful trait object at all. That’s why it needs to be a
trait object itself.</p><p>For example, that might be <code>Box&lt;dyn Foo&lt;foo() = Pin&lt;Box&lt;dyn Future&lt;Output = ()&gt;&gt;&gt;&gt;&gt;</code>. Of course,
that is incredibly verbose. There are basically two problems at play which shape the design space:</p><ul><li>There needs to be some kind of transformer that takes your implementation of <code>Foo</code>, and includes
the glue to allocate the future in the heap.</li><li>Some members of the project leadership have the very strongly held view that heap allocations
should be “explicit,” where explicit means there should be more syntax required to do it.</li></ul><p>As a result, the project has considered a new wrapper type that would be required, which would
“explicitly” indicate (by virtue of being a different type) that the future type will be heap
allocated. My understanding is that something like what I’ve written above would be <code>Box&lt;Boxed&lt;dyn Foo&gt;&gt;</code>, or maybe just <code>Boxed&lt;dyn Foo&gt;</code> (it’s not clear to me from the material I have available).</p><p>My own opinion is different. I think its reasonable to make the default behavior of a heap allocated
trait object (i.e. <code>Box&lt;dyn Foo&gt;</code>, <code>Rc&lt;dyn Foo&gt;</code> and <code>Arc&lt;dyn Foo&gt;</code>) to allocate the state machine
with the same allocator as that type. For non-owned trait objects, like <code>&amp;mut dyn Foo</code>, I would also
be fine making the default behavior allocating them with the global allocator, though here I see the
point more (especially because this wouldn’ be possible in <code>no_std</code> contexts).</p><p>Regardless, I agree it would be important to allow users to override this behavior with some
alternative glue mechanism. This requires an interface for writing your own glue code, which might
do something else (like use <code>alloca</code> to allocate a dynamically sized type on the stack). I just
think that there should be a reasonable default behavior, which for heap allocated trait objects is
probably heap allocating that state. In my opinion, this is not “implicit” any more than requiring
all users to use an adapter is “implicit,” it just involves setting a reasonable default. Still,
resolving this controversy to everyone’s satisfaction would be a blocker on this feature, as well as
developing the interface for the glue code.</p><p>I want to make one other note in this section: previous discussions of this issue treat the unstable
<a href="https://smallcultfollowing.com/babysteps/blog/2022/03/29/dyn-can-we-make-dyn-sized"><code>dyn*</code></a> feature as a prerequisite for object-safe coroutine methods. I do not believe
this is the case. What <code>dyn*</code> does is create an existential type that all of the different trait
object pointer types would implement, by virtualizing also their destructor code; if you can accept
that trait objects using different allocation strategies for their virtual coroutine methods are
different types, there’s no dependence on <code>dyn*</code> at all. I personally think the <code>dyn*</code> feature is a
questionable direction for the Rust project to pursue.</p><h2 id="async-destructors">Async destructors</h2><p>Another very thorny issue is the problem of async destructors. Sometimes, a destructor might need to
perform some kind of IO operation or otherwise block the current thread; it is desirable to support
non-blocking destructors which instead yield control, so that other tasks can run concurrently.
Unfortunately, there are several problems with this.</p><p>The first problem is that running the async destructor is best effort, even more-so than running any
destructor. This is because if you drop a type with an async destructor in a non-async context,
there’s no possibility of running the destructor because this is not in an async context. There have
been a couple of different ideas about how to solve this, such as using <code>let async</code> bindings to
indicate variables that can’t be moved into a non-async context, or just accepting it and treating
the async destructor as only an optimization over the non-async destructor.</p><p>The second problem is actually very similar to the problem with trait objects: if the async
destructor needs to use some sort of state, where do you store it? One option is to disallow async
destructors from having state, using a poll method. This is simple, but it is problematic for things
like data structures: a <code>Vec</code> for example has no way of storing which items it has polled already,
and has to keep polling their destructors in a loop. This would be pretty unacceptable, probably. But
then dealing with the state raises the same issues as trait objects.</p><p>The third problem with async destructors is how to handle their interaction with unwinding. In
particular, if you are unwinding through an async destructor, which returns <code>Pending</code>, what happens?
There would need to be some kind of asynchronous version of <code>catch_unwind</code> that the pending calls
can jump to, so that other tasks can run. This problem I think is easier to solve than the other
two, but it needs to be specced out.</p><p>I go back and forth between thinking that the difficulty with async destructors is one of the worst
things about async Rust and thinking that maybe async destructors aren’t that useful anyway.
Regardless of where you land, there is a lot of design work needed for this feature to be shippable,
and I don’t think it will come soon.</p><h2 id="long-term-features">Long-term features</h2><p>In contrast to the near-term and medium-term features, there are certain larger problems with the
design of Rust that I think should be considered carefully, such that they could not be addressed in
the next few years. Still, the work of considering them must begin at some point, so that they can
eventually be closed. I’m talking about <a href="https://without.boats/blog/changing-the-rules-of-rust">“changing”</a> the rules of Rust.</p><p>As of right now, there are a few valuable kinds of types that Rust cannot really support:</p><ul><li><strong>Immoveable types:</strong> types which can’t be moved once their address has been witnessed.</li><li><strong>Unforgetable types:</strong> types which can’t go out of scope without running their destructor or
destructuring them.</li><li><strong>Undroppable types:</strong> types which can’t be dropped or forgotten but must be destructured.</li></ul><p>(The latter two are usually grouped together as “linear types” when people talk about them, but
there are very important differences.)</p><p>I think evidence has shown that there is a strong motivation for at least the first two categories.</p><p>To support self-referential coroutines and intrusive data structures, Rust needs some support for
types that are known never to move again. Because Rust doesn’t support immovable types, we added
this functionality using the <code>Pin</code> API. But the <code>Pin</code> API has a few big flaws: one is that the API
is clunky and difficult to work with. More important, though, is that it requires an interface to
explicitly <em>opt in</em> to supporting immovable types; traits that existed before <code>Pin</code> can’t gain the
ability to work with immovable types.</p><p>There are two specific traits for which this is a big problem:</p><ul><li><code>Iterator</code>: because iterator doesn’t support immovable types, the project is at an impasse about
how to support immovable generators.</li><li><code>Drop</code>: because drop doesn’t support immovable types, an arcane implication is that you need
crates like <code>pin-project</code> to access fields of pinned types. This is all very baroque and
confusing, and wouldn’t be necessary if <code>Drop</code> supported immovable types.</li></ul><p>On the other hand, if Rust had the <code>Move</code> trait, these problems would go away. Self-referential
generators would just not implement <code>Move</code>, and work naturally. The <code>Pin</code> type could be completely
deprecated, and a reference to a type that doesn’t implement <code>Move</code> would have the same semantics as
a pinned reference to a type that doesn’t implement <code>Unpin</code>. Of course, this would require pretty
major edition-crossing changes.</p><p>The <a href="https://without.boats/blog/the-scoped-task-trilemma">scoped task trilemma</a> presents a strong argument for types which cannot be forgotten.
Stackless coroutines cannot use the destructor-based concurrent borrow trick: the only way to make
it work is to use a closure-passing “internal” style, which is what Rust opted against when it went
for stackless coroutines. This incompatibility between these two desirable aspects of Rust’s design
makes a strong case that the decision not to support unforgettable types was the wrong decision.</p><p>I titled this post “a four year plan” for a reason: if Rust were to adopt these fundamental changes,
it would have to be done across an edition boundary, and I strongly doubt that it could be done as
part of the 2024 edition. This leaves the 2027 edition, four years from now, as the target for such
a change. But the project should commit to a decision about this change sometime soon, in the next
two years, and that should include a temporary solution for generators, such as requiring them to be
pinned before they can be used as iterators.</p><p>I’ve been exploring what would be required to do this change on my blog this year because I think it
is something the Rust project should seriously consider changing. I intend to continue to focus on
this issue next year, because I think the implications of all of the different options needs to be
fully understood. I’m trying to find ways to make this a collaborative process, but my options are
limited. My goal isn’t really even to make a particular recommendation (though I will surely have
opinions), but just to understand the full space of options for resolving these issues.</p><p>What are the exact trade offs between different options to handle the problem of self-referential
generators? What different requirements would there be to support “unforgettable” types as opposed
to “undroppable” types? If <code>Move</code> were to be added, how could <code>Pin</code> be removed across an edition
boundary? These are the kinds of questions I want to answer.</p><p>However, I recognize that adding support for these kinds of types would be the biggest change to
Rust since it was stabilized in 2015, and that making this change would bring with it enormous costs
for both the project and the community. I also recognize that there are valid arguments why
supporting these kinds of types isn’t really worth it (like the painful interaction with trait
objects). For these reasons, the Rust project should build into its consideration of this idea the
possibility that <em>not doing anything</em> may ultimately be the right outcome.</p><p>In general, my instinct is to doubt big changes to Rust at this point in its design process. What I
think Rust needs is to finish integrating the features it has already committed to - features like
external iterators, stackless coroutines, monomorphized generics, and unsized trait object types. I
specifically feel changing the rules around moveability and linear types is justified because of the
implications for the integration of these existing features.</p><p>This post has once again gotten very long. I decided to focus this post on changes to the language;
in another post to come I will focus my attention on the standard library and the async library
ecosystem, as well as devote a specific post to the <code>AsyncIterator</code> interface. I want to make one
other remark, which I tried to find a place for in this post and the previous one, but couldn’t. It
concerns the controversy around the final syntax for the await operator which played out in 2019.</p><p>For those who don’t know, there was a big debate whether the await operator in Rust should be a
prefix operator (as it is in other languages) or a postfix operator (as it ultimately was). This
attracted an inordinate amount of attention - over 1000 comments. The way it played out was that
almost everyone on the language team had reached a consensus that the operator should be postfix,
but I was the lone hold out. At this point, it was clear that no new argument was going to appear,
and no one was going to change their mind. I allowed this state of affairs to linger for several
months. I regret this decision of mine. It was clear that there was no way to ship except for me to
yield to the majority, and yet I didn’t for some time. In doing so, I allowed the situation to
spiral with more and more “community feedback” reiterating the same points that had already been
made, burning everyone out but especially me.</p><p>The lesson I learned from this experience is to distinguish between factors that are truly critical
and factors that don’t matter. If you’re going to be obstinate about some issue, you’d better be
able to articulate a deep reason why it is important, and it had better be something more pressing
than the slight differences in affordances and aesthetics between syntax options. I’ve tried to
take this to heart in how I engage in technical questions since then.</p><p>I worry that the Rust project took the wrong lesson from this experience. The project continues in
its norm (as Graydon mentioned <a href="https://graydon2.dreamwidth.org/307105.html">here</a>) that with enough ideation and brainstorming,
eventually a win-win solution to every controversy can be discovered. Rather than accepting that
sometimes a hard decision has to be made, the project’s solution to the burnout of that comes from
allowing these controversies to hang open indefinitely has been to turn inward. Design decisions are
now documented primarily in unindexed formats like Zulip threads and HackMD documents. To the extent
that there is a public expression of the design, it is one of a half dozen different blogs belonging
to different contributors. As an outsider, it is nearly impossible to understand what the project
considers a priority, and what the current state of any of these things are.</p><p>I’ve never seen the project’s relationship with its community be in a worse state. But that
community contains invaluable expertise; closing yourselves off is not the solution. I want to see
the relationships of mutual trust and respect rebuilt between project members and community members,
instead of the present situation of hostility and dissatisfaction. To this, I want to thank those
from the project who have reached out and engaged with me on design issues over the last few months.</p></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ICE faces heat after agents install apps, VPNs on official phones (107 pts)]]></title>
            <link>https://www.theregister.com/2023/11/06/ice_device_security/</link>
            <guid>38178582</guid>
            <pubDate>Tue, 07 Nov 2023 16:12:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2023/11/06/ice_device_security/">https://www.theregister.com/2023/11/06/ice_device_security/</a>, See on <a href="https://news.ycombinator.com/item?id=38178582">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>America's immigration cops have pushed back against an official probe that concluded their lax mobile device security potentially put sensitive government information at risk of being stolen by foreign snoops.</p>
<p>Between April 27 and August 17, the US Department of Homeland Security Office of the Inspector General conducted an audit of equipment managed by Immigration and Customs Enforcement (ICE) and the agency's IT policies.</p>
<p>In an October 30 write-up, the inspector general came down hard on the Feds' device management practices, highlighting "urgent issues" on ICE-managed handhelds – including concerns about devices running apps that could be hijacked by foreign adversaries. Think: software linked to or developed within reach of China or Russia, which could be altered or compromised to spy on Uncle Sam.</p>

    

<p>"Specifically, we found mobile device management issues that put ICE mobile devices — and potentially other [Homeland Security] mobile devices demonstrating similar issues — and sensitive data at greater risk of potential espionage, leaks, and attacks from viruses," wrote Inspector General Joseph Cuffari in a redacted report [<a target="_blank" rel="nofollow" href="https://www.oig.dhs.gov/sites/default/files/assets/2023-11/OIG-24-02-Oct23-mgmtalert-Redacted.pdf">PDF</a>].</p>

        


        

<p>The investigation found "thousands" of applications installed on ICE-managed devices that had been installed by employees, contractors, and other DHS staff. This included third-party file sharing services and virtual private networks (VPN), outdated messaging platforms, and apps developed by companies banned from US government IT systems.</p>
<p>While we don't know which of these naughty-list apps auditors found on ICE employees phones, it's probably safe to assume one of the offenders was TikTok — banned from US <a target="_blank" href="https://www.theregister.com/2023/03/01/government_tiktok_ban/">federal government staff</a> and <a target="_blank" href="https://www.theregister.com/2023/06/06/us_contractors_tiktok_ban/">contractors' devices</a> because of <a target="_blank" href="https://www.theregister.com/2023/03/29/china_tiktok_trojan_horse/">espionage concerns</a>, due to the <a target="_blank" href="https://www.theregister.com/2023/03/30/tiktok_ban_register_kettle/">influence</a> the Chinese government can exert over Beijing-based parent ByteDance.</p>

        

<p>The DHS OIG report added the software it found on the handhelds included "applications associated with [redacted] and [redacted]." We're guessing the redacted names are China and Russia. According to the inspector general, these user-installed apps potentially put ICE's operations, employees, and all of DHS at risk.</p>
<p>"These applications introduce the potential for collecting and monitoring user and device information through device sensors such as a camera, microphone, and Global Positioning System," the report stated. "The applications may also collect and distribute information stored on the device (eg, photos, videos, and documents), including potentially sensitive information outside the secure containers."&nbsp;</p>
<ul>

<li><a href="https://www.theregister.com/2023/06/06/us_contractors_tiktok_ban/">US govt now bans TikTok from contractors' work gear</a></li>

<li><a href="https://www.theregister.com/2023/10/25/ice_social_media_surveillance/">Your ex isn't the only one stalking your social media posts. The Feds are, too</a></li>

<li><a href="https://www.theregister.com/2022/12/01/ice_data_dump/">ICE data dump reveals names, locations of 6,000+ asylum seekers</a></li>

<li><a href="https://www.theregister.com/2022/05/14/ice_28bn_domestic_surveillance/">How ICE became a $2.8b domestic surveillance agency</a></li>
</ul>
<p>Because ICE considered these downloads to be "personal applications," it didn't monitor them, we're told, despite their presence on the federal agency's devices. To be fair, some of the ICE-approved apps sound equally concerning, such as "one ICE-owned application allows ICE personnel to capture and search biometric information of people they encounter in real-time."</p>
<p>An ICE spokesperson declined to comment on this scanning app, and also did not answer <em>The Register</em>'s questions about its last personal-use policy update, which happened in 2014, and if it planned to review the policy more frequently from here on out.&nbsp;</p>
<p>In a lengthy statement emailed to <em>The Register</em>, an ICE spokesperson said:</p>

<p>The report acknowledged that ICE has implemented some of the auditors' recommendations already to boost device security, such as blocking and disabling prohibited apps, vulnerable messaging applications, and VPN applications.&nbsp;</p>
<p>"ICE also stated it has taken steps to implement application vetting and is in the process of updating its mobile device use policy," the report says.</p>

        

<p>Additionally, DHS, in its response to the audit, disagreed that ICE security controls did not reduce the risk to federal mobile devices and their sensitive information. Homeland Security also claimed the percentage of ICE-managed devices that did not have mobile threat defense capability installed is significantly lower than the inspector general's audit number.</p>
<p>While ICE's actions "demonstrate progress," the report concludes that ICE still hasn't fully addressed "risks associated with user-installed applications communicated in this alert." ®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Real vs. fake AirPods with industrial CT (132 pts)]]></title>
            <link>https://www.lumafield.com/article/real-vs-fake-apple-products-through-industrial-ct-airpods-pro-macbook-magsafe-charger</link>
            <guid>38178492</guid>
            <pubDate>Tue, 07 Nov 2023 16:06:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.lumafield.com/article/real-vs-fake-apple-products-through-industrial-ct-airpods-pro-macbook-magsafe-charger">https://www.lumafield.com/article/real-vs-fake-apple-products-through-industrial-ct-airpods-pro-macbook-magsafe-charger</a>, See on <a href="https://news.ycombinator.com/item?id=38178492">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Today's counterfeit products are so sophisticated that they often appear visually and functionally identical to the genuine articles—at least initially. For both manufacturers and consumers, counterfeits present a serious challenge: how can you ensure the quality and safety of your products?</p><p>CT scanning, a technique once reserved for medical diagnostics, has found a new purpose in the fight against counterfeit electronics. Industrial CT scanners like the <a href="https://www.lumafield.com/products/neptune-industrial-x-ray-ct-scanner">Neptune</a> allow engineers to inspect and optimize their designs throughout the product development cycle, <a href="https://www.lumafield.com/article/what-is-industrial-ct">from R&amp;D to field support</a>. They’re also the perfect tool for identifying fakes with precision. Along the way, they also reveal the complexity and sophistication of the engineering that goes into genuine products.</p><p>We examined the internal structure of Apple’s AirPods Pro and MagSafe 2 power adapters for MacBook, exposing the shortcuts and compromises made in counterfeit versions that could compromise functionality and user safety.</p><h3>Apple AirPods Pro</h3><h4>Batteries</h4><p>Batteries are the key to the wireless convenience and flexibility of the AirPods design, so we started there. The authentic AirPods house meticulously-engineered button cell batteries in each earbud, designed to fit snugly within the compact form factor and provide optimal power efficiently. In contrast, both specimens of counterfeit AirPods contain lithium-ion pouch cell batteries that are not only less sophisticated in their construction but also potentially less safe. The rectangular pouches are crammed into circular spaces rather than tailored to fit.</p><h4>Circuitry</h4><p>Moving to the internal circuitry, the genuine AirPods are a marvel of miniaturization and precision engineering. They use a combination of rigid and flexible printed circuit boards to pack components densely and ensure that every millimeter of space is used effectively. On the other hand, the counterfeit AirPods reveal much simpler electronics cobbled together from off-the-shelf components. That leaves less room for functionality; the counterfeits have fewer microphones and less control circuitry, compromising their sound quality.</p><h4>Build quality</h4><p>Lastly, the contrast in overall build quality is dramatic between the genuine AirPods and their counterfeit counterparts. One of the fakes doesn't offer wireless charging at all (no coils are visible in the scans), and the other one has wireless charging coils but lacks the magnets that snap the real AirPods case onto Apple's Watch charger. The counterfeit AirPods even resort to using internal weights with no other function than to mimic the heft of the genuine product, a deceptive tactic for making them feel heavier to compensate for poorer materials and less functionality. These fakes may replicate the visual cues of the original, but the use of substandard materials not only affects the tactile experience but also compromises the structural integrity and overall lifespan of the product.</p><h3>Apple MagSafe 2 Power Adapters for MacBook</h3><h4>Power cycling</h4><p>We also CT scanned a genuine and counterfeit 85W MagSafe 2 Power Adapter for MacBook, finding differences in their internal circuitry and power cycling systems. The genuine Apple charger has a sophisticated power management system that includes components for power conditioning and conversion. The counterfeit charger's internal circuitry is far less complex, lacking the filtering features that ensure safety and longevity in Apple's charger. This simplified internal structure not only raises concerns about the counterfeit’s performance but also its ability to safely manage the power supplied to your devices.</p><figure><p><img src="https://assets-global.website-files.com/63e15418201b6e2a5cabb911/6549d5eae6ee48357ab18632_Counterfeit%20charger%201.png" loading="lazy" alt=""></p></figure><h5><a href="https://app.lumafield.com/project/94be4eac-ca9a-426e-8686-92928b2d9efd" target="_blank">Explore the real charger scan</a></h5><h4>Heat sinks</h4><p>Our CT scans reveal an intriguing difference between the heat sinks in the real and counterfeit chargers. The genuine charger uses a relatively thin heat sink that wraps around most of the transformer. The counterfeit uses a heavier, but simpler heat sink design. Apple’s heat sink requires more manufacturing steps to stamp and assemble and likely provides more even distribution of heat. The counterfeit’s design is more likely to lead to dangerous hot spots—especially combined with its less sophisticated transformer, which would tend to generate more heat.</p><figure><p><img src="https://assets-global.website-files.com/63e15418201b6e2a5cabb911/6549d5f8cad14f949bd29e5d_Counterfeit%20charger%202.png" loading="lazy" alt=""></p></figure><h5><a href="https://app.lumafield.com/project/98077f2f-9863-41e5-a6e9-a2c31eba5974" target="_blank">Explore the fake charger scan</a></h5><h4>Build quality</h4><p>The CT scans reveal a dense, well-organized internal structure in the genuine charger and a more haphazard internal layout in the counterfeit. They also reveal that the counterfeit charger has a fake grounding capability: the real charger has a grounded pin that can be connected with an optional 3-prong plug. The fake looks like it has a grounded pin, but the pin isn’t connected to any grounding circuitry inside the charger.</p><h3>Adam Savage compares the scans</h3><figure><p><iframe allowfullscreen="true" frameborder="0" scrolling="no" src="https://www.youtube.com/embed/Db99cXMD780" title="Fake Apple AirPod Pro Exposed!"></iframe></p></figure><h3>Conclusion</h3><p>The differences between these products and their counterfeits are subtle at first glance, but our industrial CT scans reveal significant differences that have implications for performance and safety. In the end, the choice between real and counterfeit may be more than a matter of cost; it’s a decision to invest in reliability and peace of mind.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google ends deal to build 15,000 Bay Area homes due to "market conditions" (165 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2023/11/google-cost-cutting-ends-deal-to-build-thousands-of-affordable-housing-units/</link>
            <guid>38178123</guid>
            <pubDate>Tue, 07 Nov 2023 15:43:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2023/11/google-cost-cutting-ends-deal-to-build-thousands-of-affordable-housing-units/">https://arstechnica.com/tech-policy/2023/11/google-cost-cutting-ends-deal-to-build-thousands-of-affordable-housing-units/</a>, See on <a href="https://news.ycombinator.com/item?id=38178123">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <header>
            <h4>
      Google real estate cuts    —
</h4>
            
            <h2 itemprop="description">15,000-home plan in limbo as Google ends contract with builder after four years.</h2>
                    </header>
        <div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/11/google-housing-800x339.jpg" alt="Illustration of a housing development with apartments and a park area with benches, sidewalks, and a bike lane.">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/11/google-housing.jpg" data-height="906" data-width="2136">Enlarge</a> <span>/</span> Google's conceptual rendering of its housing plan.</p><p>Google</p></figcaption>  </figure>

  




<!-- cache hit 645:single/related:14a669b63c03e8668d4a5d20bab82a1c --><!-- empty -->
<p>Google has ended an agreement with a developer to build 15,000 homes in the San Francisco Bay Area, including affordable housing, as it continues a string of cost-cutting moves to reduce real estate costs. Google said it is "looking at a variety of options" to provide housing despite ending the development deal but didn't offer specific details on its plans.</p>
<p>Google <a href="https://www.lendlease.com/us/media-center/media-releases/lendlease-to-jointly-deliver-new-mixed-use-neighbourhoods-with-google-in-the-san-francisco-bay-area/">partnered</a> with the Australian company Lendlease in 2019 on a $15 billion <a href="https://www.lendlease.com/us/projects/san-francisco-bay-area-project/">plan</a> for "residential, retail, hospitality, and community development space," with an expected completion date of 2038. Lendlease was to be the developer, builder, and owner of the residential, retail, hospitality, and community components, while Google also planned a related office expansion. Including office space, the developments could have covered 15 million square feet.</p>
<p>But on Friday, <a href="https://www.lendlease.com/siteassets/lendlease/shared/investor-centre/announcements/asx/2023/11/2635882.pdf">Lendlease and Google announced</a> that they "mutually reached an agreement to end the Development Services Agreements of the four master-planned districts in San Jose (Downtown West), Sunnyvale (Moffett Park), and Mountain View (Middlefield Park and North Bayshore) in the San Francisco Bay area in California, collectively referred to as the San Francisco Bay Project." Google is making a payment to Lendlease as part of the agreement to part ways.</p>
<p>The Google <a href="https://realestate.withgoogle.com/northbayshore/plan/#housing">plan</a> that Lendlease was involved in calls for up to 7,000 homes in North Bayshore, including 1,050 affordable units; 4,000 homes in <a href="https://realestate.withgoogle.com/sanjose/#section-4">Downtown West</a>, including 1,000 affordable units; and 1,900 homes in <a href="https://realestate.withgoogle.com/middlefieldpark/plan/#housing">Middlefield Park</a>, including 380 affordable units. That would presumably leave 2,100 homes for Sunnyvale, but <a href="https://realestate.withgoogle.com/bay-housing/">Google's description</a> of the project doesn't specify the number of homes in that city.</p>
<p>A Google spokesperson told us today that the company will work with developers and capital partners to move its Bay Area developments forward, despite ending the Lendlease deal. But Google declined to answer our specific questions about the number of homes that will be built now that the original partnership has been scrapped.</p>
<p>"The decision to end these agreements followed a comprehensive review by Google of its real estate investments, and a determination by both organizations that the existing agreements are no longer mutually beneficial given current market conditions... Lendlease will remove the San Francisco Bay Project, which was expected to commence construction in FY26, from its development pipeline," the Lendlease announcement said.</p>                                            
                                                        
<h2>Project was still on track in June</h2>
<p>Lendlease was not the developer or builder in Google's related plan for <a href="https://arstechnica.com/gadgets/2021/05/googles-san-jose-megacampus-will-be-a-mixed-use-neighborhood/">new office buildings</a> on a "megacampus" in San Jose. Google put the office-building plan <a href="https://arstechnica.com/gadgets/2023/04/google-puts-10-to-30-year-campus-construction-project-on-hold-after-2-years/">on hold</a> earlier this year, reports in April said.</p>
<p>In June 2019, <a href="https://blog.google/inside-google/company-announcements/1-billion-investment-bay-area-housing/">Google CEO Sundar Pichai</a> announced a plan to rezone a large part of Google's commercial and office land for residential housing and build "at least 15,000 new homes at all income levels in the Bay Area, including housing options for middle and low-income families." Pichai also announced a $250 million investment fund to "provide incentives to enable developers to build at least 5,000 affordable housing units across the market."</p>
<p>A <a href="https://blog.google/inside-google/company-announcements/google-bay-area-housing-update/">June 2023 update</a> from Google said the company still planned "to support the creation or preservation of 20,000 homes." Google said its "work with local elected officials and residents to rezone $750 million worth of our land has paved the way for up to 12,900 units to be built in Mountain View and San Jose as part of our mixed-use development plans." Google also said it had allocated over $133 million of the $250 million fund to "more than 3,800 units across 29 affordable housing projects."</p>
<p>Exactly how many of the 20,000 planned homes could still be built is unclear. "As we've shared before, we've been optimizing our real estate investments in the Bay Area, and part of that work is looking at a variety of options to move our development projects forward and deliver on our housing commitment. We appreciate Lendlease and the work the team has done to get us to this point," Alexa Arena, senior director of development at Google, said in a statement provided to Ars today.</p>
<p>Google's pullback comes amid a rise in surplus office space.</p>
<p>"Silicon Valley companies are dumping office space at an accelerating pace, as tech leaders such as Google and Facebook parent Meta Platforms close locations and reassess their commitments to the workplace," The Wall Street Journal <a href="https://www.wsj.com/articles/vacant-offices-are-piling-up-in-silicon-valley-68e19f7">wrote in June</a>. "Office-vacancy rates in Silicon Valley, which includes the Northern California communities of San Jose, Palo Alto and Sunnyvale, were up to 17 percent in June from 11 percent in 2019, according to data firm CoStar Group." The vacancy rate had surpassed 20 percent in Menlo Park and Mountain View.</p>

                                                </div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Verizon, AT&T Customers Sue to Undo T-Mobile Merger, Saying It Raised All Prices (257 pts)]]></title>
            <link>https://www.techdirt.com/2023/11/07/verizon-att-customers-sue-to-reverse-t-mobile-merger-saying-it-raised-everybodys-prices/</link>
            <guid>38177930</guid>
            <pubDate>Tue, 07 Nov 2023 15:31:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.techdirt.com/2023/11/07/verizon-att-customers-sue-to-reverse-t-mobile-merger-saying-it-raised-everybodys-prices/">https://www.techdirt.com/2023/11/07/verizon-att-customers-sue-to-reverse-t-mobile-merger-saying-it-raised-everybodys-prices/</a>, See on <a href="https://news.ycombinator.com/item?id=38177930">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="storywrap-424820">


<h3>from the <i>merge-ALL-the-things!</i> dept</h3>

<p>We just got done noting how pretty much all of the criticism of the Sprint T-Mobile merger by economists and consumer advocates <a href="https://www.techdirt.com/2023/10/17/everything-t-mobile-sprint-merger-critics-predicted-has-come-true/" data-type="link" data-id="https://www.techdirt.com/2023/10/17/everything-t-mobile-sprint-merger-critics-predicted-has-come-true/">wound up being true</a>. The deal has resulted in more than 10,000+ eliminated jobs, steady price hikes, annoying new fees, a weaker T-Mobile brand, and a lower quality product overall. It also clearly distracted T-Mobile from <a href="https://www.techdirt.com/2023/01/23/t-mobile-hacked-eighth-time-in-five-years/" data-type="link" data-id="https://www.techdirt.com/2023/01/23/t-mobile-hacked-eighth-time-in-five-years/">competent network security</a>.</p>
<p>T-Mobile’s reddit forums are filled with employees saying the disruptive spirit of the company has been <a href="https://www.reddit.com/r/tmobile/comments/10zsncc/what_changed_about_tmobiles_employee_experience/" data-type="link" data-id="https://www.reddit.com/r/tmobile/comments/10zsncc/what_changed_about_tmobiles_employee_experience/">dead since the merger</a>. T-Mobile customers are annoyed by <a href="https://www.techdirt.com/2023/10/30/t-mobile-backs-off-price-hike-they-pretended-wasnt-a-price-hike/" data-type="link" data-id="https://www.techdirt.com/2023/10/30/t-mobile-backs-off-price-hike-they-pretended-wasnt-a-price-hike/">endless new restrictions and price hikes</a>. </p>
<p>But Verizon and AT&amp;T customers are also pissed, and are part of a new <a href="https://www.reuters.com/legal/litigation/t-mobile-must-face-private-antitrust-lawsuit-over-26-bln-sprint-deal-us-judge-2023-11-03/" data-type="link" data-id="https://www.reuters.com/legal/litigation/t-mobile-must-face-private-antitrust-lawsuit-over-26-bln-sprint-deal-us-judge-2023-11-03/">lawsuit against T-Mobile</a> arguing that the merger raised prices for <strong>everybody</strong> due to the reduction in overall wireless market competition. A federal judge in Chicago last week ruled that plaintiffs made some decent points and the lawsuit should be allowed to proceed:</p>
<blockquote>
<p><em>“U.S. District Judge Thomas Durkin in a&nbsp;<a rel="noreferrer noopener" href="https://fingfx.thomsonreuters.com/gfx/legaldocs/mopajzamwva/Dale%20v%20T%20Mobile%20order%2020231102.pdf" target="_blank">41-page ruling</a>&nbsp;on Thursday said the plaintiffs “plausibly” argued that higher prices “flowed directly” from the $26 billion merger.”</em></p>
</blockquote>
<p>The important time to protect consumers is before these kinds of competition-eroding deals are approved, but that very clearly didn’t happen here. Trump regulators at the FCC <a href="https://www.techdirt.com/2019/10/22/fcc-approved-t-mobile-sprint-merger-without-even-seeing-full-details/" data-type="link" data-id="https://www.techdirt.com/2019/10/22/fcc-approved-t-mobile-sprint-merger-without-even-seeing-full-details/">didn’t even bother to read about the deal’s impact</a> before approving it. Trump “antitrust enforcers” at the FTC actively helped T-Mobile <a href="https://www.techdirt.com/2019/12/30/doj-antitrust-boss-delrahim-ignored-hard-data-as-he-rubber-stamped-t-mobile-merger/" data-type="link" data-id="https://www.techdirt.com/2019/12/30/doj-antitrust-boss-delrahim-ignored-hard-data-as-he-rubber-stamped-t-mobile-merger/">avoid regulatory scrutiny on their personal time</a>, you know, like antitrust enforcers do. </p>
<p>T-Mobile’s response to the lawsuit was expected: to deny everything and insist the U.S. wireless sector is secretly super competitive:</p>
<blockquote>
<p><em>Attorneys for T-Mobile&nbsp;<a rel="noreferrer noopener" href="https://fingfx.thomsonreuters.com/gfx/legaldocs/jnpwwzdgrpw/Softbank%20Tmobile%20-%2079.pdf" target="_blank">called</a>&nbsp;the lawsuit “unprecedented,” and said the plaintiffs’ damages were “speculative.”</em></p>
<p><em>“If plaintiffs are unhappy with Verizon and AT&amp;T, there is a remedy available in the highly competitive market that wireless consumers enjoy today — they should switch to T-Mobile, not sue it,” attorneys for T-Mobile told the court.</em></p>
</blockquote>
<p>The harms of mindless consolidation are not theoretical. They’re clearly documented. Yet we’re dedicated to ignoring those harms because such consolidation is hugely profitable for a handful of over-compensated executives and a few key investors (sometimes). Rinse, wash, repeat, with nobody responsible for the end result getting within a thousand miles of introspection or accountability. </p>
<p>I’d not expect much from the suit in terms of reform. Any payout will be a tiny fraction of the financial harm caused. The real fix lies in more stringent merger review and well funded and staffed regulators; concepts defenders of a broken but profitable status quo have no real interest in. </p>
<p>
Filed Under: <a href="https://www.techdirt.com/tag/antitrust/" rel="tag">antitrust</a>, <a href="https://www.techdirt.com/tag/antitrust-reform/" rel="tag">antitrust reform</a>, <a href="https://www.techdirt.com/tag/competition/" rel="tag">competition</a>, <a href="https://www.techdirt.com/tag/consolidation/" rel="tag">consolidation</a>, <a href="https://www.techdirt.com/tag/high-speed-internet/" rel="tag">high speed internet</a>, <a href="https://www.techdirt.com/tag/mergers/" rel="tag">mergers</a>, <a href="https://www.techdirt.com/tag/prices/" rel="tag">prices</a>, <a href="https://www.techdirt.com/tag/wireless/" rel="tag">wireless</a>
<br>
Companies: <a href="https://www.techdirt.com/company/t-mobile/" rel="category tag">t-mobile</a>
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An update on salary transparency in job posts (122 pts)]]></title>
            <link>https://directlyapply.com/blog/an-update-on-salary-transparency-in-job-posts-november-2023</link>
            <guid>38177702</guid>
            <pubDate>Tue, 07 Nov 2023 15:14:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://directlyapply.com/blog/an-update-on-salary-transparency-in-job-posts-november-2023">https://directlyapply.com/blog/an-update-on-salary-transparency-in-job-posts-november-2023</a>, See on <a href="https://news.ycombinator.com/item?id=38177702">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Salary transparency is a hot topic in today's job market, and for good reason. When job seekers have access to accurate salary information, they can make informed decisions about their careers, negotiate fair compensation, and ensure that their financial needs are met. Employers benefit from salary transparency as well, it can help attract top talent, reduce turnover, build trust with their workforce and crucially, increase efficiency for all parties during the hiring process. </p><p>At DirectlyApply we have always believed that giving job seekers access to as much information up front about potential job opportunities leads to the best outcomes for both them and also employers, not just salary information, but job requirements, benefits offered, working conditions, shifts etc. In order to present this information to job seekers DirectlyApply has developed the most comprehensive rich data extractions for job postings on the market.</p><p><b>44% of job postings across the US contain salary information in November 2023.</b></p><p>From almost 5m active job vacancies on DirectlyApply, we have analysed salary transparency across US states and also job categories. </p><p><b>States:</b></p><p>Louisiana comes in with the lowest level of salary transparency at only 30% of job postings containing salary information, South Dakota tops the charts with 75% of job postings containing salary information.</p><p>While a number of states have enacted salary transparency rules, currently only four states require salary ranges in job postings. Amongst these states, New York is the worse performer with only 44% of job posting containing salary information (though New York only required this from September 2023). Colorado leads with 70% of job postings containing salary information. Washington and California also have job posting requirements with 60%, and 53% respectively. </p><p>The average across the 4 states with enacted laws is 56%, higher than the 44% national average.</p><p>A break down of all states is below: </p><table><tbody><tr><th><p><b>State</b></p></th><th><p><b>Percentage with salary information</b></p></th></tr><tr><td><p>Alabama</p></td><td><p>39%</p></td></tr><tr><td><p>Alaska</p></td><td><p>39%</p></td></tr><tr><td><p>Arizona</p></td><td><p>38%</p></td></tr><tr><td><p>Arkansas</p></td><td><p>37%</p></td></tr><tr><td><p>California</p></td><td><p>53%</p></td></tr><tr><td><p>Colorado</p></td><td><p>70%</p></td></tr><tr><td><p>Connecticut</p></td><td><p>30%</p></td></tr><tr><td><p>Delaware</p></td><td><p>50%</p></td></tr><tr><td><p>District of Columbia</p></td><td><p>34%</p></td></tr><tr><td><p>Florida</p></td><td><p>30%</p></td></tr><tr><td><p>Georgia</p></td><td><p>38%</p></td></tr><tr><td><p>Hawaii</p></td><td><p>33%</p></td></tr><tr><td><p>Idaho</p></td><td><p>47%</p></td></tr><tr><td><p>Illinois</p></td><td><p>33%</p></td></tr><tr><td><p>Indiana</p></td><td><p>48%</p></td></tr><tr><td><p>Iowa</p></td><td><p>59%</p></td></tr><tr><td><p>Kansas</p></td><td><p>44%</p></td></tr><tr><td><p>Kentucky</p></td><td><p>43%</p></td></tr><tr><td><p>Louisiana</p></td><td><p>30%</p></td></tr><tr><td><p>Maine</p></td><td><p>43%</p></td></tr><tr><td><p>Maryland</p></td><td><p>43%</p></td></tr><tr><td><p>Massachusetts</p></td><td><p>36%</p></td></tr><tr><td><p>Michigan</p></td><td><p>35%</p></td></tr><tr><td><p>Minnesota</p></td><td><p>46%</p></td></tr><tr><td><p>Mississippi</p></td><td><p>42%</p></td></tr><tr><td><p>Missouri</p></td><td><p>46%</p></td></tr><tr><td><p>Montana</p></td><td><p>53%</p></td></tr><tr><td><p>Nebraska</p></td><td><p>50%</p></td></tr><tr><td><p>Nevada</p></td><td><p>46%</p></td></tr><tr><td><p>New Hampshire</p></td><td><p>52%</p></td></tr><tr><td><p>New Jersey</p></td><td><p>48%</p></td></tr><tr><td><p>New Mexico</p></td><td><p>50%</p></td></tr><tr><td><p>New York</p></td><td><p>44%</p></td></tr><tr><td><p>North Carolina</p></td><td><p>42%</p></td></tr><tr><td><p>North Dakota</p></td><td><p>50%</p></td></tr><tr><td><p>Ohio</p></td><td><p>36%</p></td></tr><tr><td><p>Oklahoma</p></td><td><p>52%</p></td></tr><tr><td><p>Oregon</p></td><td><p>52%</p></td></tr><tr><td><p>Pennsylvania</p></td><td><p>40%</p></td></tr><tr><td><p>Rhode Island</p></td><td><p>36%</p></td></tr><tr><td><p>South Carolina</p></td><td><p>33%</p></td></tr><tr><td><p>South Dakota</p></td><td><p>75%</p></td></tr><tr><td><p>Tennessee</p></td><td><p>40%</p></td></tr><tr><td><p>Texas</p></td><td><p>35%</p></td></tr><tr><td><p>Utah</p></td><td><p>36%</p></td></tr><tr><td><p>Vermont</p></td><td><p>55%</p></td></tr><tr><td><p>Virginia</p></td><td><p>49%</p></td></tr><tr><td><p>Washington</p></td><td><p>60%</p></td></tr><tr><td><p>West Virginia</p></td><td><p>38%</p></td></tr><tr><td><p>Wisconsin</p></td><td><p>37%</p></td></tr><tr><td><p>Wyoming</p></td><td><p>62%</p></td></tr></tbody></table><p><b>Categories:</b></p><p>When comparing salary transparency by job category Military and IT positions lead with 58% and 57% respectively advertising salary information. At the other end Sanitation, Engineering, Transportation &amp; Logistics and Agriculture all perform well below average with less than 33% of job posting advertising salary ranges. </p><table><tbody><tr><th><p><b>Job Category</b></p></th><th><p><b>Percentage with salary information</b></p></th></tr><tr><td><p>Administration</p></td><td><p>39%</p></td></tr><tr><td><p>Agriculture</p></td><td><p>32%</p></td></tr><tr><td><p>Construction</p></td><td><p>52%</p></td></tr><tr><td><p>Education</p></td><td><p>47%</p></td></tr><tr><td><p>Engineering</p></td><td><p>31%</p></td></tr><tr><td><p>Healthcare</p></td><td><p>46%</p></td></tr><tr><td><p>Hospitality</p></td><td><p>39%</p></td></tr><tr><td><p>Information Technology</p></td><td><p>57%</p></td></tr><tr><td><p>Legal</p></td><td><p>51%</p></td></tr><tr><td><p>Management</p></td><td><p>45%</p></td></tr><tr><td><p>Manufacturing</p></td><td><p>54%</p></td></tr><tr><td><p>Military</p></td><td><p>58%</p></td></tr><tr><td><p>Sales</p></td><td><p>43%</p></td></tr><tr><td><p>Sanitation</p></td><td><p>30%</p></td></tr><tr><td><p>Science</p></td><td><p>39%</p></td></tr><tr><td><p>Security</p></td><td><p>54%</p></td></tr><tr><td><p>Transportation &amp; Logistics</p></td><td><p>32%</p></td></tr></tbody></table><p><b>Notes on the data:</b></p><p>The above data is extracted from 5m active job postings, live on DirectlyApply as of 3rd November 2023. DirectlyApply removes all duplicate postings, postings where our AI systems detect there may be fraud, or job postings that don't meet our standards for advertising.</p><p>If you want more information about DirectlyApply or our features feel free to reach out to&nbsp;&nbsp; <a href="https://www.linkedin.com/in/dylankbuckley/" target="_blank" rel="noopener noreferrer">
          Dylan&nbsp;
        </a>or&nbsp;&nbsp; <a href="https://www.linkedin.com/in/will-capper-a1703045/" target="_blank" rel="noopener noreferrer">
          Will&nbsp;
        </a>on Linkedin or email:&nbsp;&nbsp; <a href="https://directlyapply.com/cdn-cgi/l/email-protection#334756525e73575a415650475f4a5243435f4a1d505c5e" target="_blank" rel="noopener noreferrer">
          <span data-cfemail="057160646845616c77606671697c647575697c2b666a68">[email&nbsp;protected]</span>
        </a></p></div></div>]]></description>
        </item>
    </channel>
</rss>