<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 25 Jul 2023 11:00:06 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Common Pitfalls in Go Benchmarking (1202 pts)]]></title>
            <link>https://eli.thegreenplace.net/2023/common-pitfalls-in-go-benchmarking/</link>
            <guid>36860065</guid>
            <pubDate>Tue, 25 Jul 2023 09:51:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://eli.thegreenplace.net/2023/common-pitfalls-in-go-benchmarking/">https://eli.thegreenplace.net/2023/common-pitfalls-in-go-benchmarking/</a>, See on <a href="https://news.ycombinator.com/item?id=36860065">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                
                <p>Go programmers have the good fortune of excellent testing and benchmarking
tooling built into the standard library - in the <tt>testing</tt> package. However,
<em>benchmarking is hard</em>. This isn't Go specific; it's just one of those things
experienced developers learn over time.</p>
<p>This post lists some common benchmarking pitfalls Go programmers run into. It
assumes basic familiarity with writing Go benchmarks; consult the <a href="https://pkg.go.dev/testing">testing
package documentation</a> if needed. While these
pitfalls are presented in Go, they exist in any programming language or
environment, so the lessons learned here are widely applicable.</p>
<div id="benchmarking-the-wrong-thing">
<h2>Benchmarking the wrong thing</h2>
<p>Let's say we want to benchmark the new sorting functionality available in the
<tt>slices</tt> package starting with Go 1.21 <a href="#footnote-1" id="footnote-reference-1">[1]</a>. Consider the following
benchmark:</p>
<div><pre><span></span><span>const</span><span> </span><span>N</span><span> </span><span>=</span><span> </span><span>100</span><span>_000</span><span></span>

<span>func</span><span> </span><span>BenchmarkSortIntsWrong</span><span>(</span><span>b</span><span> </span><span>*</span><span>testing</span><span>.</span><span>B</span><span>)</span><span> </span><span>{</span><span></span>
<span>  </span><span>ints</span><span> </span><span>:=</span><span> </span><span>makeRandomInts</span><span>(</span><span>N</span><span>)</span><span></span>
<span>  </span><span>b</span><span>.</span><span>ResetTimer</span><span>()</span><span></span>

<span>  </span><span>for</span><span> </span><span>i</span><span> </span><span>:=</span><span> </span><span>0</span><span>;</span><span> </span><span>i</span><span> </span><span>&lt;</span><span> </span><span>b</span><span>.</span><span>N</span><span>;</span><span> </span><span>i</span><span>++</span><span> </span><span>{</span><span></span>
<span>    </span><span>slices</span><span>.</span><span>Sort</span><span>(</span><span>ints</span><span>)</span><span></span>
<span>  </span><span>}</span><span></span>
<span>}</span><span></span>

<span>func</span><span> </span><span>makeRandomInts</span><span>(</span><span>n</span><span> </span><span>int</span><span>)</span><span> </span><span>[]</span><span>int</span><span> </span><span>{</span><span></span>
<span>  </span><span>ints</span><span> </span><span>:=</span><span> </span><span>make</span><span>([]</span><span>int</span><span>,</span><span> </span><span>n</span><span>)</span><span></span>
<span>  </span><span>for</span><span> </span><span>i</span><span> </span><span>:=</span><span> </span><span>0</span><span>;</span><span> </span><span>i</span><span> </span><span>&lt;</span><span> </span><span>n</span><span>;</span><span> </span><span>i</span><span>++</span><span> </span><span>{</span><span></span>
<span>    </span><span>ints</span><span>[</span><span>i</span><span>]</span><span> </span><span>=</span><span> </span><span>rand</span><span>.</span><span>Intn</span><span>(</span><span>n</span><span>)</span><span></span>
<span>  </span><span>}</span><span></span>
<span>  </span><span>return</span><span> </span><span>ints</span><span></span>
<span>}</span><span></span>
</pre></div>
<p>Why is this benchmark wrong? Because it's not really testing what you think it's
testing. <tt>slices.Sort</tt> sorts a slice <em>in-place</em>. After the first iteration,
the <tt>ints</tt> slice is already sorted, so all subsequent iterations "sort"
a sorted slice. This is not what we want to measure <a href="#footnote-2" id="footnote-reference-2">[2]</a>. The
right measurement would create a new random slice for each iteration:</p>
<div><pre><span></span><span>func</span><span> </span><span>BenchmarkSortInts</span><span>(</span><span>b</span><span> </span><span>*</span><span>testing</span><span>.</span><span>B</span><span>)</span><span> </span><span>{</span><span></span>
<span>  </span><span>for</span><span> </span><span>i</span><span> </span><span>:=</span><span> </span><span>0</span><span>;</span><span> </span><span>i</span><span> </span><span>&lt;</span><span> </span><span>b</span><span>.</span><span>N</span><span>;</span><span> </span><span>i</span><span>++</span><span> </span><span>{</span><span></span>
<span>    </span><span>b</span><span>.</span><span>StopTimer</span><span>()</span><span></span>
<span>    </span><span>ints</span><span> </span><span>:=</span><span> </span><span>makeRandomInts</span><span>(</span><span>N</span><span>)</span><span></span>
<span>    </span><span>b</span><span>.</span><span>StartTimer</span><span>()</span><span></span>
<span>    </span><span>slices</span><span>.</span><span>Sort</span><span>(</span><span>ints</span><span>)</span><span></span>
<span>  </span><span>}</span><span></span>
<span>}</span><span></span>
</pre></div>
<p>On my machine, the second benchmark is almost 100x slower; this makes sense,
because it actually sorts something on each iteration.</p>
</div>
<div id="forgetting-to-reset-the-timer">
<h2>Forgetting to reset the timer</h2>
<p>Note how the benchmarks mentioned above are careful to reset or stop/start
the benchmarking timer around certain operations. This is on purpose, because
the benchmark scaffold measures the run-time of the entire <tt>Benchmark*</tt>
function, executing it many times and dividing the total execution time by
<tt>b.N</tt> (much more details on this process later in the post).</p>
<p>Try it! Remove the <tt>b.StopTimer()</tt> and <tt>b.StartTimer()</tt> calls from the
benchmarking loop in <tt>BenchmarkSortInts</tt> and compare the results - you'll
notice the benchmark now reports noticeably slower per-op execution time
because creating the slices of random integers also happens <tt>b.N</tt> times.</p>
<p>This can throw results off significantly in some cases. Even if we use the
same technique to benchmark two approaches, the errors don't necessarily cancel
out. Due to <a href="https://en.wikipedia.org/wiki/Amdahl%27s_law">Amdahl's law</a> we
can easily report the wrong speed-up when not isolating the computation we
care about.</p>
</div>
<div id="fooled-by-the-compiler">
<h2>Fooled by the compiler</h2>
<p>The trickiest benchmarking pitfall is dealing with compiler optimizations
thwarting our results. The Go compiler doesn't give <tt>Benchmark*</tt> functions
any preferential treatment and will try to optimize them and their contents
just as it would any other Go code <a href="#footnote-3" id="footnote-reference-3">[3]</a>. This produces wrong results and it's
not even easy to know they are wrong! Consider this benchmark:</p>
<div><pre><span></span><span>func</span><span> </span><span>isCond</span><span>(</span><span>b</span><span> </span><span>byte</span><span>)</span><span> </span><span>bool</span><span> </span><span>{</span><span></span>
<span>  </span><span>if</span><span> </span><span>b</span><span>%</span><span>3</span><span> </span><span>==</span><span> </span><span>1</span><span> </span><span>&amp;&amp;</span><span> </span><span>b</span><span>%</span><span>7</span><span> </span><span>==</span><span> </span><span>2</span><span> </span><span>&amp;&amp;</span><span> </span><span>b</span><span>%</span><span>17</span><span> </span><span>==</span><span> </span><span>11</span><span> </span><span>&amp;&amp;</span><span> </span><span>b</span><span>%</span><span>31</span><span> </span><span>==</span><span> </span><span>9</span><span> </span><span>{</span><span></span>
<span>    </span><span>return</span><span> </span><span>true</span><span></span>
<span>  </span><span>}</span><span></span>
<span>  </span><span>return</span><span> </span><span>false</span><span></span>
<span>}</span><span></span>

<span>func</span><span> </span><span>BenchmarkIsCondWrong</span><span>(</span><span>b</span><span> </span><span>*</span><span>testing</span><span>.</span><span>B</span><span>)</span><span> </span><span>{</span><span></span>
<span>  </span><span>for</span><span> </span><span>i</span><span> </span><span>:=</span><span> </span><span>0</span><span>;</span><span> </span><span>i</span><span> </span><span>&lt;</span><span> </span><span>b</span><span>.</span><span>N</span><span>;</span><span> </span><span>i</span><span>++</span><span> </span><span>{</span><span></span>
<span>    </span><span>isCond</span><span>(</span><span>201</span><span>)</span><span></span>
<span>  </span><span>}</span><span></span>
<span>}</span><span></span>
</pre></div>
<p>We're trying to benchmark the <tt>isCond</tt> function, but the compiler is
fooling us (at least in recent Go versions):</p>
<div><pre><span></span>BenchmarkIsCondWrong-8        1000000000     0.2401 ns/op
</pre></div>
<p>Sub-nanosecond operation times are always a bit suspect, although not entirely
out of the question given the simplicity of the <tt>isCond</tt> function. There are
two serious errors here, however:</p>
<ol>
<li>We use a constant input to <tt>isCond</tt>; since <tt>isCond</tt> is a simple function
that is likely to be inlined into the benchmark function, the compiler can
theoretically constant-propagate its input through its contents and replace
the code with the compile-time computed answer.</li>
<li>Even if we used a non-constant for the input, the result of <tt>isCond</tt> is
unused, so the compiler may optimize it away.</li>
</ol>
<p>And in fact, in this example the contents of the benchmark loop are optimized
away entirely; looking at the disassembly, we see this:</p>
<div><pre><span></span>    JMP     BenchmarkIsCondWrong_pc7
BenchmarkIsCondWrong_pc4:
    INCQ    CX
BenchmarkIsCondWrong_pc7:
    CMPQ    416(AX), CX
    JGT     BenchmarkIsCondWrong_pc4
    RET
</pre></div>
<p><tt>CX</tt> holds <tt>i</tt>, and <tt>416(AX)</tt> accesses <tt>b.N</tt>. This is just an empty
loop! Now the 0.24 nanoseconds per iteration make sense <a href="#footnote-4" id="footnote-reference-4">[4]</a>.</p>
<p>Here's an even more confounding manifestation of the same problem:</p>
<div><pre><span></span><span>func</span><span> </span><span>countCond</span><span>(</span><span>b</span><span> </span><span>[]</span><span>byte</span><span>)</span><span> </span><span>int</span><span> </span><span>{</span><span></span>
<span>  </span><span>result</span><span> </span><span>:=</span><span> </span><span>0</span><span></span>
<span>  </span><span>for</span><span> </span><span>i</span><span> </span><span>:=</span><span> </span><span>0</span><span>;</span><span> </span><span>i</span><span> </span><span>&lt;</span><span> </span><span>len</span><span>(</span><span>b</span><span>);</span><span> </span><span>i</span><span>++</span><span> </span><span>{</span><span></span>
<span>    </span><span>if</span><span> </span><span>isCond</span><span>(</span><span>b</span><span>[</span><span>i</span><span>])</span><span> </span><span>{</span><span></span>
<span>      </span><span>result</span><span>++</span><span></span>
<span>    </span><span>}</span><span></span>
<span>  </span><span>}</span><span></span>
<span>  </span><span>return</span><span> </span><span>result</span><span></span>
<span>}</span><span></span>

<span>func</span><span> </span><span>BenchmarkCountWrong</span><span>(</span><span>b</span><span> </span><span>*</span><span>testing</span><span>.</span><span>B</span><span>)</span><span> </span><span>{</span><span></span>
<span>  </span><span>inp</span><span> </span><span>:=</span><span> </span><span>getInputContents</span><span>()</span><span></span>
<span>  </span><span>b</span><span>.</span><span>ResetTimer</span><span>()</span><span></span>
<span>  </span><span>for</span><span> </span><span>i</span><span> </span><span>:=</span><span> </span><span>0</span><span>;</span><span> </span><span>i</span><span> </span><span>&lt;</span><span> </span><span>b</span><span>.</span><span>N</span><span>;</span><span> </span><span>i</span><span>++</span><span> </span><span>{</span><span></span>
<span>    </span><span>countCond</span><span>(</span><span>inp</span><span>)</span><span></span>
<span>  </span><span>}</span><span></span>
<span>}</span><span></span>

<span>func</span><span> </span><span>getInputContents</span><span>()</span><span> </span><span>[]</span><span>byte</span><span> </span><span>{</span><span></span>
<span>  </span><span>n</span><span> </span><span>:=</span><span> </span><span>400000</span><span></span>
<span>  </span><span>buf</span><span> </span><span>:=</span><span> </span><span>make</span><span>([]</span><span>byte</span><span>,</span><span> </span><span>n</span><span>)</span><span></span>
<span>  </span><span>for</span><span> </span><span>i</span><span> </span><span>:=</span><span> </span><span>0</span><span>;</span><span> </span><span>i</span><span> </span><span>&lt;</span><span> </span><span>n</span><span>;</span><span> </span><span>i</span><span>++</span><span> </span><span>{</span><span></span>
<span>    </span><span>buf</span><span>[</span><span>i</span><span>]</span><span> </span><span>=</span><span> </span><span>byte</span><span>(</span><span>n</span><span> </span><span>%</span><span> </span><span>32</span><span>)</span><span></span>
<span>  </span><span>}</span><span></span>
<span>  </span><span>return</span><span> </span><span>buf</span><span></span>
<span>}</span><span></span>
</pre></div>
<p>Now there are certainly no constant propagation issues, and the result of
<tt>isCond</tt> is clearly used inside <tt>countCond</tt>. But we've committed the same
error! While the result of <tt>isCond</tt> is used, the result of <tt>countCond</tt> is
not, and therefore the compiler will do something like:</p>
<ul>
<li>Inline <tt>isCond</tt> into <tt>countCond</tt></li>
<li>Inline <tt>countCond</tt> into the benchmark function</li>
<li>Realize that the loop body in <tt>coundCond</tt> isn't producing side effects or
any result used outside it</li>
<li>Hollow out the loop body in <tt>countCond</tt>, leaving only an empty loop</li>
</ul>
<p>This process results in confusing benchmark results because the loop
remains in place, and thus if we grow the input, the benchmarked execution
time will grow accordingly! One of the oldest tricks in the benchmarking book is
to change the input size and observe how the run-time is affected; if it's not
moving, something is wrong. If it's growing roughly in line with the asymptotic
complexity of the code under test, it's at least passing a simple smoke test.
But in this case the heuristic fails due to the specific optimizations
performed.</p>
<div id="keeping-compiler-optimizations-in-benchmarks-under-control">
<h3>Keeping compiler optimizations in benchmarks under control</h3>
<p>There are two main techniques in Go to thwart compiler optimizations where we
don't want them.</p>
<p>The first is using the <a href="https://pkg.go.dev/runtime#KeepAlive">runtime.KeepAlive</a>
function. Originally introduced for fine-grained control of finalizers, it's
also useful as a very-low-overhead way to tell the compiler "I really need this
value, even if you can prove I do not". Here's how we can fix our <tt>countCond</tt>
benchmark with its help:</p>
<div><pre><span></span><span>func</span><span> </span><span>BenchmarkCountKeepAlive</span><span>(</span><span>b</span><span> </span><span>*</span><span>testing</span><span>.</span><span>B</span><span>)</span><span> </span><span>{</span><span></span>
<span>  </span><span>inp</span><span> </span><span>:=</span><span> </span><span>getInputContents</span><span>()</span><span></span>
<span>  </span><span>b</span><span>.</span><span>ResetTimer</span><span>()</span><span></span>
<span>  </span><span>result</span><span> </span><span>:=</span><span> </span><span>0</span><span></span>
<span>  </span><span>for</span><span> </span><span>i</span><span> </span><span>:=</span><span> </span><span>0</span><span>;</span><span> </span><span>i</span><span> </span><span>&lt;</span><span> </span><span>b</span><span>.</span><span>N</span><span>;</span><span> </span><span>i</span><span>++</span><span> </span><span>{</span><span></span>
<span>    </span><span>result</span><span> </span><span>+=</span><span> </span><span>countCond</span><span>(</span><span>inp</span><span>)</span><span></span>
<span>  </span><span>}</span><span></span>
<span>  </span><span>runtime</span><span>.</span><span>KeepAlive</span><span>(</span><span>result</span><span>)</span><span></span>
<span>}</span><span></span>
</pre></div>
<p>Running the benchmark makes it clear that there's more work going into each
iteration now:</p>
<div><pre><span></span>BenchmarkCountWrong-8                 12481       95911 ns/op
BenchmarkCountKeepAlive-8              4143      285527 ns/op
</pre></div>
<p>Another way is without needing a special function, but is slightly more
dangerous; we can use a global exported value to collect the result:</p>
<div><pre><span></span><span>var</span><span> </span><span>Sink</span><span> </span><span>int</span><span></span>

<span>func</span><span> </span><span>BenchmarkCountSink</span><span>(</span><span>b</span><span> </span><span>*</span><span>testing</span><span>.</span><span>B</span><span>)</span><span> </span><span>{</span><span></span>
<span>  </span><span>inp</span><span> </span><span>:=</span><span> </span><span>getInputContents</span><span>()</span><span></span>
<span>  </span><span>b</span><span>.</span><span>ResetTimer</span><span>()</span><span></span>
<span>  </span><span>for</span><span> </span><span>i</span><span> </span><span>:=</span><span> </span><span>0</span><span>;</span><span> </span><span>i</span><span> </span><span>&lt;</span><span> </span><span>b</span><span>.</span><span>N</span><span>;</span><span> </span><span>i</span><span>++</span><span> </span><span>{</span><span></span>
<span>    </span><span>Sink</span><span> </span><span>+=</span><span> </span><span>countCond</span><span>(</span><span>inp</span><span>)</span><span></span>
<span>  </span><span>}</span><span></span>
<span>}</span><span></span>
</pre></div>
<p>Even though our benchmark doesn't use <tt>Sink</tt>, it's very difficult for the
compiler to eliminate because it's a package-level exported value that can be
used pretty much anywhere in the program. I did say it's slightly more
dangerous, though, because theoretically the compiler <em>could</em> prove it's
redundant using more sophisticated cross-package analysis.</p>
<p>Which brings us the to ultimate point: when in doubt, always double check the
assembly generated for the benchmark function, and ensure that the compiler
wasn't too clever.</p>
</div>
<div id="ongoing-work-in-the-go-standard-library">
<h3>Ongoing work in the Go standard library</h3>
<p>Addressing the issue of compiler optimizations in benchmarks is something the Go
team is interested in. There are currently two active proposals in discussion:</p>
<ul>
<li><a href="https://github.com/golang/go/issues/61179">Issue 61179</a>: adding a
low-overhead "keep this value" function in the <tt>testing</tt> package; this would
be similar to <tt>runtime.KeepAlive</tt>, but more targeted at benchmarks with
a better name and clearer guarantees.</li>
<li>A more ambitious proposal in <a href="https://github.com/golang/go/issues/61515">issue 61515</a> discusses changing the main
benchmarking API of the <tt>testing</tt> package to facilitate safer benchmarking.</li>
</ul>
</div>
</div>
<div id="misusing-b-n">
<h2>Misusing <tt>b.N</tt></h2>
<p>There are at least two common ways to misuse the benchmark repetition indicator
<tt>b.N</tt>. Here's the first, completely forgetting the loop:</p>
<div><pre><span></span><span>import</span><span> </span><span>(</span><span></span>
<span>  </span><span>"crypto/rand"</span><span></span>
<span>  </span><span>"testing"</span><span></span>
<span>)</span><span></span>

<span>func</span><span> </span><span>BenchmarkRandPrimeWrongNoLoop</span><span>(</span><span>b</span><span> </span><span>*</span><span>testing</span><span>.</span><span>B</span><span>)</span><span> </span><span>{</span><span></span>
<span>  </span><span>rand</span><span>.</span><span>Prime</span><span>(</span><span>rand</span><span>.</span><span>Reader</span><span>,</span><span> </span><span>200</span><span>)</span><span></span>
<span>}</span><span></span>
</pre></div>
<p>From proper measurements, the <tt>crypto/rand.Prime</tt> invocation with length 200
should take around 1 ms on my machine, but this benchmark reports impossibly
low times - fractions of a nanosecond.</p>
<p>The second is trickier:</p>
<div><pre><span></span><span>func</span><span> </span><span>BenchmarkRandPrimeWrongUseI</span><span>(</span><span>b</span><span> </span><span>*</span><span>testing</span><span>.</span><span>B</span><span>)</span><span> </span><span>{</span><span></span>
<span>  </span><span>for</span><span> </span><span>i</span><span> </span><span>:=</span><span> </span><span>0</span><span>;</span><span> </span><span>i</span><span> </span><span>&lt;</span><span> </span><span>b</span><span>.</span><span>N</span><span>;</span><span> </span><span>i</span><span>++</span><span> </span><span>{</span><span></span>
<span>    </span><span>rand</span><span>.</span><span>Prime</span><span>(</span><span>rand</span><span>.</span><span>Reader</span><span>,</span><span> </span><span>i</span><span>)</span><span></span>
<span>  </span><span>}</span><span></span>
<span>}</span><span></span>
</pre></div>
<p>Note how it's using <tt>i</tt> in each iteration (and thus indirectly <tt>b.N</tt>). On my
machine, this benchmark <strong>doesn't terminate</strong>; what gives?</p>
<p>To understand what's going on here, we should first discuss how benchmarking in
Go works in a bit more detail. The benchmarking harness invokes our benchmark
functions several times. How many? This can be
determined by flags like <tt><span>-benchtime</span></tt>, but the default is for one second.
How does the harness know how many repetitions to pass to our benchmark function
in order to get 1s of execution time? By trying it with low repetitions first,
measuring how long it takes, and then running with increasingly high repetitions
until the intended duration is reached <a href="#footnote-5" id="footnote-reference-5">[5]</a>.</p>
<p>There are some nuances to the process: e.g. the number of repetitions doesn't
grow too fast between attempts (at most 100x, as of Go 1.21), and it is capped
at 1 billion. The key point is that it uses the execution time from the
previous attempt to determine how many repetitions to run next.</p>
<p>With these insights in mind, let's examine the two misuses shown above.</p>
<ol>
<li>Forgetting to loop until <tt>b.N</tt>. Each benchmark invocation only performs the
tested operations once. No matter how many times the benchmark harness
repeats the benchmark with increasing <tt>b.N</tt>, the function takes a
millisecond to run! Therefore, eventually the benchmark harness hits its
<tt>N</tt> limit of a billion invocations, and divides the execution time (1 ms)
by this <tt>N</tt> to get a nonsensical result.</li>
<li>Using the value of <tt>b.N</tt> for something other than "how many times to
repeat the benchmark". <tt>rand.Prime</tt> is very fast when its input length
is small, but gets pretty slow for large inputs. The harness starts by
running the function once to get its bearings, and then 100 times. For size
100 the run-time of <tt>rand.Prime</tt> is moderate, so the next time the harness
can increase <tt>b.N</tt> by another factor of 100. But for higher inputs,
<tt>rand.Prime</tt> also takes much longer. We end up with a quadratic run-time
explosion! Our benchmark function isn't literally hanging - it <em>will</em> finish
eventually, but it may take long minutes or hours.</li>
</ol>
<p>It's worth mentioning that the proposal in <a href="https://github.com/golang/go/issues/61515">issue 61515</a> would address this problem as
well, by exposing a benchmarking API that would be much harder to misuse in this
way. Another proposal that may affect this is <a href="https://github.com/golang/go/issues/61405">add range over int</a>, which will permit us to write
<tt>for range b.N</tt> to iterate exactly <tt>b.N</tt> times, without an explicit
iteration variable.</p>
<hr>
<table id="footnote-1">
<colgroup><col><col></colgroup>
<tbody>
<tr><td><a href="#footnote-reference-1">[1]</a></td><td><p>For background on this work and why the new generic sorting functions
are faster, see <a href="https://eli.thegreenplace.net/2022/faster-sorting-with-go-generics/">this post</a>.</p>
<p>If you're reading this before Go 1.21's final release (in Aug 2023),
you can download the
<a href="https://go.dev/blog/go1.21rc">release candidate</a> or use
<a href="https://pkg.go.dev/golang.org/dl/gotip">gotip</a>.</p>
</td></tr>
</tbody>
</table>
<table id="footnote-2">
<colgroup><col><col></colgroup>
<tbody>
<tr><td><a href="#footnote-reference-2">[2]</a></td><td>To be clear, it's also important to know how well sorting algorithms
perform on sorted or almost-sorted inputs, but here we're talking about
the more general case.</td></tr>
</tbody>
</table>
<table id="footnote-3">
<colgroup><col><col></colgroup>
<tbody>
<tr><td><a href="#footnote-reference-3">[3]</a></td><td>While it's possible to think of ways to treat <tt>Benchmark*</tt> specially,
it's pretty hard to do it correctly because we very much <em>want</em> the
compiler to optimize the functions we're benchmarking. After all, we
want to know what their real run-time is. Drawing the line
between "optimize here" and "don't touch there" is tricky!</td></tr>
</tbody>
</table>
<table id="footnote-4">
<colgroup><col><col></colgroup>
<tbody>
<tr><td><a href="#footnote-reference-4">[4]</a></td><td>The Go compiler doesn't currently optimize loops entirely away, unless
it can prove they terminate.</td></tr>
</tbody>
</table>
<table id="footnote-5">
<colgroup><col><col></colgroup>
<tbody>
<tr><td><a href="#footnote-reference-5">[5]</a></td><td>This also has the side effect of providing whatever <em>warming</em> the
benchmark needs w.r.t. caches (CPU caches, paged memory, file system
caches, DNS caches, etc).</td></tr>
</tbody>
</table>
</div>

            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Parents, environmentalists to Google: stop Chromebooks from expiring this summer (121 pts)]]></title>
            <link>https://pirg.org/edfund/resources/chromebook-expiration-full-letter/</link>
            <guid>36857506</guid>
            <pubDate>Tue, 25 Jul 2023 03:27:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pirg.org/edfund/resources/chromebook-expiration-full-letter/">https://pirg.org/edfund/resources/chromebook-expiration-full-letter/</a>, See on <a href="https://news.ycombinator.com/item?id=36857506">Hacker News</a></p>
Couldn't get https://pirg.org/edfund/resources/chromebook-expiration-full-letter/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Got called to a professor’s office after a complaint his SPARC4 was running slow (446 pts)]]></title>
            <link>https://infosec.exchange/@paco/110772422266480371</link>
            <guid>36857314</guid>
            <pubDate>Tue, 25 Jul 2023 03:00:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://infosec.exchange/@paco/110772422266480371">https://infosec.exchange/@paco/110772422266480371</a>, See on <a href="https://news.ycombinator.com/item?id=36857314">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Mozilla Standards Positions Opposes Web Integrity API (468 pts)]]></title>
            <link>https://github.com/mozilla/standards-positions/issues/852</link>
            <guid>36857032</guid>
            <pubDate>Tue, 25 Jul 2023 02:14:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/mozilla/standards-positions/issues/852">https://github.com/mozilla/standards-positions/issues/852</a>, See on <a href="https://news.ycombinator.com/item?id=36857032">Hacker News</a></p>
<div id="readability-page-1" class="page"><div disabled="" sortable="">
          <p dir="auto">Mozilla opposes this proposal because it contradicts our principles and vision for the Web.</p>
<p dir="auto">Any browser, server, or publisher that implements common standards is <a href="https://www.mozilla.org/en-US/about/webvision/full/#openness" rel="nofollow">automatically part of the Web</a>:</p>
<blockquote>
<p dir="auto">Standards themselves aim to avoid assumptions about the underlying hardware or software that might restrict where they can be deployed. This means that no single party decides which form-factors, devices, operating systems, and browsers may access the Web. It gives people more choices, and thus more avenues to overcome personal obstacles to access. Choices in assistive technology, localization, form-factor, and price, combined with thoughtful design of the standards themselves, all permit a wildly diverse group of people to reach the same Web.</p>
</blockquote>
<p dir="auto">Mechanisms that attempt to restrict these choices are harmful to the openness of the Web ecosystem and are not good for users.</p>
<p dir="auto">Additionally, the use cases listed depend on the ability to “detect non-human traffic” which as described would likely obstruct many existing uses of the Web such as assistive technologies, automatic testing, and archiving &amp; search engine spiders. These depend on tools being able to receive content intended for humans, and then transform, test, index, and summarize that content for humans. The safeguards in the proposal (e.g., “holdback”, or randomly failing to produce an attestation) are unlikely to be effective, and are inadequate to address these concerns.</p>
<p dir="auto">Detecting fraud and invalid traffic is a challenging problem that we're interested in helping address. However this proposal does not explain how it will make practical progress on the listed use cases, and there are clear downsides to adopting it.</p>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Signal walks the line between anarchism and pragmatism (111 pts)]]></title>
            <link>https://www.wired.com/story/signal-politics-software-criticism/</link>
            <guid>36856882</guid>
            <pubDate>Tue, 25 Jul 2023 01:53:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/signal-politics-software-criticism/">https://www.wired.com/story/signal-politics-software-criticism/</a>, See on <a href="https://news.ycombinator.com/item?id=36856882">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p><span>For 20 years,</span> the only way to really communicate privately was to use a widely hated piece of software called Pretty Good Privacy. The software, known as PGP, aimed to make secure communication accessible to the lay user, but it was so poorly designed that even Edward Snowden messed up his first attempt to use PGP to email a friend of Laura Poitras. It also required its users to think like engineers, which included participating in exceptionally nerdy activities like attending real-life “key-signing parties” to verify your identity to other users. Though anyone could technically use PGP, the barrier to entry was so high that only about 50,000 people used it at its peak, meaning that privacy itself was out of reach for most.</p><p>These days, to talk to a friend securely, all you have to do is download a free app. For a certain set, that app will be Signal. Snowden and Elon Musk have recommended it; it’s been name-dropped on big-budget shows like <em>House of Cards, Mr. Robot,</em> and <em>Euphoria,</em> and its users include journalists, members of the White House, NBA players, Black Lives Matters activists, and celebrities trying to get their hands on Ozempic. Its founder has been profiled by <em>The New Yorker</em> and appeared on Joe Rogan’s podcast. A tiny organization with virtually no marketing budget has become synonymous with digital privacy in the public imagination.</p><p>Technology can be deeply shaped by the personal inclinations of a founder. Facebook’s light-fingeredness with user data is inseparable from its roots in Zuckerberg’s dorm room as an app for ranking women by their looks; Apple’s minimalist design was influenced by Jobs’ time spent practicing Zen Buddhism. Signal is no different. During its formative years, the charismatic face of Signal was Moxie Marlinspike, a dreadlocked anarchist who spent his time sailing around the world, living in punk houses, and serving free food to the unhoused. He led every aspect of Signal’s development for almost a decade, <a data-offer-url="https://signal.org/blog/new-year-new-ceo/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://signal.org/blog/new-year-new-ceo/&quot;}" href="https://signal.org/blog/new-year-new-ceo/" rel="nofollow noopener" target="_blank">at one point complaining</a>, &nbsp;“I was writing all the Android code, was writing all of the server code, was the only person on call for the service, was facilitating all product development, and was managing everyone. I couldn’t ever leave cell service.”</p><p>In the field of cryptography, Marlinspike is considered the driving force behind bringing end-to-end encryption—the technology underlying Signal—to the real world. In 2017, Marlinspike and his collaborator, Trevor Perrin, received the Levchin Prize, a prominent prize for cryptographers, for their work on the Signal Protocol. Afterward, Dan Boneh, the Stanford professor who chaired the award committee, <a href="https://www.newyorker.com/magazine/2020/10/26/taking-back-our-privacy">commented</a> that he wasn’t sure that end-to-end encryption would have become widespread without Marlinspike’s work. At the very least, “it would have taken many more decades,” he said.</p><p>The motivations that led to end-to-end encryption going mainstream lie far out on the political fringe. The original impetus for Marlinspike’s entry into cryptography, around 2007, was to challenge existing power structures, particularly the injustice of how (as he put it) “Internet insecurity is used by people I don’t like against people I do: the government against the people.” But sticking to anarchism would imply an almost certain defeat. As Marlinspike once <a data-offer-url="https://moxie.org/stories/promise-defeat/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://moxie.org/stories/promise-defeat/&quot;}" href="https://moxie.org/stories/promise-defeat/" rel="nofollow noopener" target="_blank">noted</a>, the “trail of ideas that disappears into the horizon behind me is completely and utterly mined over with failures … Anarchists are best known for their failures.”</p><p>For an idealistic engineer to succeed, he will have to build something that is useful to many. So there has also been an unusually pragmatic bent to Signal’s approach. Indeed, in many interviews, Marlinspike has taken a mainstream stance, insisting that “Signal is just trying to bring normality to the internet.” Signal’s success depends on maintaining its principled anarchist commitments while finding a wide-ranging appeal to the masses, two goals that might seem at odds. Examining how the app navigates this tension can help us understand what might come next in Signal’s new quest to reach “<a href="https://www.theverge.com/23409716/signal-encryption-messaging-sms-meredith-whittaker-imessage-whatsapp-china">everyone on the planet.</a>”</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p><span>Released after WhatsApp</span> &nbsp;set the standards for messaging, Signal’s problem has always been how to keep up with its competition—a fine dance between mimicry (so as to seem familiar to new users) and innovation (to poach users from its competitors). Signal started off by copying WhatsApp's user experience, while at the same time pioneering end-to-end encryption, a feature that WhatsApp turned around and copied from Signal. Throughout this evolutionary dance, Signal has managed to maintain an unusual focus on the autonomy of the individual, a wariness of state authority, and an aversion to making money, characteristics that are recognizably anarchist.</p><p>Because a small fringe of cypherpunks, Marlinspike included, came to see cryptography as a way to remedy the imbalance of power between the individual and the state, Signal focused on getting end-to-end encryption on messages and calls absolutely right. With Signal, no one can read your messages. Amazon can’t, the US government can’t, Signal can’t. The same is true for voice calls and metadata: A user’s address book and group chat titles are just as safe. Signal knows basically nothing about you, other than your phone number (which is not mapped to your username), the time you created your account, and the time you last used the app. Your data can’t be sold to others or cause ads to follow you around on the internet. Using Signal is just like talking with your friend in the kitchen.</p><p>Because Signal is committed to retaining as little metadata as possible, that makes it hard for it to implement new features that are standard to other apps. Signal is essentially footing the cost of this commitment in engineer-hours, since implementing popular features like group chats, address books, and stickers all required doing novel research in cryptography. That Signal built them anyway is a testament to its desire for mass appeal.</p><p>Signal also pioneered features that gave individuals more autonomy over their information, such as disappearing messages (which WhatsApp later adopted) and a feature that let users blur faces in a photo (which it rapidly rolled out to support the Black Lives Matter protests). At the same time, Signal has garnered users' trust because its code is open source, so that security researchers can verify that its end-to-end encryption is as strong as the organization claims.</p><p>For the ordinary user, though, individual autonomy and privacy may not be as important. On WhatsApp, users accept that it will be very hard to figure out what exactly the app knows about you and who it might be shared with. Users’ information is governed by an ever-shifting labyrinth of grudging caveats and clauses like “we will share your transaction data and IP address with Facebook” and “we can’t see your precise location, but we’ll still try to estimate it as best as we can” and “we <em>will</em> find out if you click on a WhatsApp share button on the web.” WhatsApp is also closed-source, so its code can’t be audited. If using Signal is like talking in a friend’s kitchen, using WhatsApp is like meeting at a very loud bar—your conversation is safe, but you’re exposed, and you’ll have to pay for your place.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>If you’re not an anarchist, you may be less worried about a shadowy state and more worried about actual people you know. People in your community might be harassing you in a group chat, an abusive ex might be searching your chats for old photos to leak, or your child might have gotten access to your unlocked phone. WhatsApp’s features better support a threat model that is sensitive to interpersonal social dynamics: You can leave groups silently, block screenshots for view-once messages, and lock specific chats. WhatsApp can even view the text of end-to-end encrypted messages that have been reported by a user for moderation, whereas Signal has no moderation at all.</p><p>Idealists have called centralization one of the main ills of the internet because it locks users into walled gardens controlled by authoritarian companies. In a great stroke of pragmatism, Signal chose to be centralized anyway. Other encrypted-messaging apps like Matrix offer a federated model akin to email, in which users across different servers can still communicate through a shared protocol. (Someone on Gmail can still email someone on Yahoo, whereas someone on Facebook Messenger can’t contact someone on Signal.) This federated approach more closely mirrors anarchy; it could theoretically be better, because there would be no single point of failure and no single service provider for a government to pressure. But federated software creates a proliferation of different clients and servers for the same protocol, making it hard to upgrade. Users are already used to centralized apps that behave like Facebook or Twitter, and email has already become centralized into a few main service providers. It turns out that being authoritarian is important for maintaining a consistent user experience and a trusted brand, and for rolling out software updates quickly. Even anarchism has its limits.</p><p><span>What Signal has</span> accomplished so far is impressive. But users famously judge software not on how much it can do, but on how much it can’t. In that spirit, it’s time to complain.</p><p>Because of Signal’s small team, limited funding, and the challenges of implementing features under end-to-end encryption, the app bafflingly lacks a number of important features. It doesn’t have encrypted backups for iOS; messages can only be transferred between phones. If you lose your iPhone, you lose all your Signal chat history.</p><p>Signal also doesn’t do a good job serving some of its core users. Activists and organizers deal with huge amounts of messages that involve many people and threads, but Signal’s interface lacks ways to organize all this information. These power users’ group chats become so unwieldy that they migrate to Slack, losing the end-to-end encryption that brought them to Signal in the first place. It’s common to try and make multiple group chats between the same people to manage all their threads. When users are hacking “desire paths” into your interface to create a new feature, or leaving because of the lack of the feature, that’s a strong hint that something is missing.</p><p>WhatsApp and Telegram, on the other hand, are leading the way on defining how group chats can scale up. WhatsApp “communities” gather different private group chats in one place, better mimicking the organization of a neighborhood or school that may be discussing several things at once. Telegram’s social media “channel” features are better for broadcasting info en masse, though Telegram’s lack of moderation has been blamed for attracting the kind of fringe crowd that has been banned from all other platforms.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p><span>It's no exaggeration</span> to say that small features in a chat app encode different visions of how society should be organized. If the first reacji in the palette was a thumbs down rather than a heart, maybe we would all be more negative, cautious people. What kind of social vision did Signal arise from?</p><p>“Looking back, I and everyone I knew was looking for that secret world hidden in this one,” Marlinspike admitted in a 2016 <a href="https://www.wired.com/2016/07/meet-moxie-marlinspike-anarchist-bringing-encryption-us/">interview</a>. A key text in anarchist theory describes the idea of a “temporary autonomous zone,” a place of short-term freedom where people can experiment with new ways to live together outside the confines of current social norms. Originally coined to describe “pirate utopias” that may be apocryphal, the term has <a data-offer-url="https://www.documentjournal.com/2020/08/better-living-through-anarchy-tracking-the-rise-of-the-temporary-autonomous-zone/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.documentjournal.com/2020/08/better-living-through-anarchy-tracking-the-rise-of-the-temporary-autonomous-zone/&quot;}" href="https://www.documentjournal.com/2020/08/better-living-through-anarchy-tracking-the-rise-of-the-temporary-autonomous-zone/" rel="nofollow noopener" target="_blank">since been used</a> to understand the life and afterlife of real-world DIY spaces like communes, raves, seasteads, and protests. And Signal is, unmistakably, a temporary autonomous zone that Marlinspike has spent almost a decade building.</p><p>Because temporary autonomous zones create spaces for the radical urges that society represses, they keep life in the daytime more stable. They can sometimes make money in the way that nightclubs and festivals do. But temporary autonomous zones are temporary for a reason. Over and over, zone denizens make the same mistake: They can’t figure out how to interact productively with the wider society. The zone often runs out of money because it exists in a world where people need to pay rent. Success is elusive; when a temporary autonomous zone becomes compelling enough to threaten daytime stability, it may be violently repressed. Or the attractive freedoms offered by the zone may be taken up in a milder form by the wider society, and eventually the zone ceases to exist because its existence has pressured wider society to be a little more like it. What kind of end might Signal come to?</p><p>There are reasons to think that Signal may not be around for very long. The nonprofit’s blog, meant to convince us of the elite nature of its engineers, has the unintentional effect of conveying the incredible difficulty of building any new software feature under end-to-end encryption. Its team numbers roughly 40; Marlinspike has just <a data-offer-url="https://twitter.com/signalapp/status/1669374296785928193" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://twitter.com/signalapp/status/1669374296785928193&quot;}" href="https://twitter.com/signalapp/status/1669374296785928193" rel="nofollow noopener" target="_blank">left the organization</a>. Achieving impossible feats may be fun for a stunt hacker with something to prove, but competing with major tech companies’ engineering teams may not be sustainable for a small nonprofit with Marlinspike no longer at the helm.</p><p>Fittingly for an organization formerly led by an anarchist, Signal lacks a sustainable business model, to the point where you might almost call it anti-capitalist. It has survived so far in ways that don’t seem replicable, and that may alienate some users. Signal is largely funded by a big loan from a WhatsApp founder, and that loan has already grown to $100 million. It has also accepted funding from the US government through the Open Technology Fund. Because Signal can’t sell its users’ data, it has recently begun developing a business model based on directly providing services to users and encouraging them to donate to Signal in-app. But to get enough donations, the nonprofit must grow from 40 million users to 100 million. The company’s aggressive pursuit of growth, coupled with lack of moderation in the app, has already led Signal employees themselves to <a href="https://www.theverge.com/22249391/signal-app-abuse-messaging-employees-violence-misinformation">publicly question</a> whether growth might come from abusive users, such as far-right groups using Signal to organize.</p><p>But there are also reasons for hope. So far, the most effective change that Signal has created is arguably not the existence of the app itself, but making it easy for WhatsApp to bring Signal-style end-to-end encryption to billions of users. Since WhatsApp’s adoption, Facebook Messenger, Google’s Android Messages, and Microsoft’s Skype have all adopted the open source Signal Protocol, though in milder forms, as the history of temporary autonomous zones would have us guess. Perhaps the existence of the Signal Protocol, coupled with demand from increasingly privacy-conscious users, will encourage better-funded messaging apps to compete against each other to be as encrypted as possible. Then Signal would no longer need to exist. (In fact, this resembles Signal’s original theory of change, before they decided they would rather compete with mainstream tech companies.)</p><p>Now, as the era of the global watercooler ends, small private group chats are becoming the future of social life on the internet. Signal started out a renegade, a pirate utopia encircled by cryptography, but the mainstream has become—alarmingly quickly—much closer to the vision Signal sought. In one form or another, its utopia just might last.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Twitter’s Rebrand to X Could Be a Trademark Nightmare Thanks to Microsoft (190 pts)]]></title>
            <link>https://themessenger.com/tech/twitters-rebrand-to-x-could-be-a-trademark-nightmare-thanks-to-microsoft</link>
            <guid>36856433</guid>
            <pubDate>Tue, 25 Jul 2023 01:01:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://themessenger.com/tech/twitters-rebrand-to-x-could-be-a-trademark-nightmare-thanks-to-microsoft">https://themessenger.com/tech/twitters-rebrand-to-x-could-be-a-trademark-nightmare-thanks-to-microsoft</a>, See on <a href="https://news.ycombinator.com/item?id=36856433">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><p>Elon Musk’s sudden move to rebrand <a href="https://themessenger.com/tech/elon-musks-biographer-hints-why-the-tech-mogul-seems-obsessed-with-the-letter-x">Twitter as X</a> may run into the unyielding wall of trademark law —&nbsp;but not because its logo looks suspiciously familiar to a variety of typeface fonts. Instead, the problem may be in the <a href="https://themessenger.com/business/musk-says-twitter-will-become-x-and-drop-all-the-birds">X brand</a> itself, legal experts told The Messenger.</p><p><strong>X: An Investigation</strong></p><p>While many users quickly noted that the X looks eerily similar to a letter from a typeface made by font producer Monotype, the company was quick to shut down speculation that Musk was using their intellectual property without attribution or payment.</p><p>Monotype Executive Creative Director Phil Garnham Executive told The Messenger in a statement that the company “can confirm that whilst it is similar, this is not the capital X glyph from Monotype’s “Special Alphabets 4.”</p><p>Others have pointed out that the logo is similar if not identical to a Unicode X.&nbsp;</p><p>And a musician, India-based EDM artist Kxlider, said they’re worried about what the new Twitter logo’s resemblance to his own logo will mean for their career.</p><p>However, several trademark lawyers told The Messenger that Musk likely won’t encounter any problems using his new X logo for these reasons.&nbsp;</p><p>“It doesn't matter if it's somebody else's typeface, what matters in the trademark world is whether you're infringing somebody else's preexisting brand,” said <a href="https://www.owe.com/attorneys/linda-joy-kattwinkel/" target="_blank" rel="noreferrer noopener">Linda Joy Kattwinkel</a>, a copyright, trademark and arts lawyer in California who is also a visual artist.&nbsp;</p><p>That, she says, really only comes into play if you are working in a related field. Kxlider is a musician, and X is a social media company.</p><p>“The question there would be, is Twitter stepping on his toes in terms of his business?” Kattwinkel said. “The courts would look into: is Twitter doing anything in the music scene or anything that’s likely to encroach on what he's been doing all along? Probably not, but stranger things have happened in the courts.”</p><p><a href="https://www.google.com/search?q=joel+rothman+miami&amp;oq=joel+rothman+miami&amp;aqs=chrome..69i57j33i160l3.3018j0j7&amp;sourceid=chrome&amp;ie=UTF-8" target="_blank" rel="noreferrer noopener">Joel Rothman</a>, a Miami lawyer who specializes in intellectual property infringement, told The Messenger that font producers usually protect their creations not through any copyright laws but through terms of service that restrict what users can do with them based on the license agreement.&nbsp;</p><p>"The use of a single letter is typically not prohibited, necessarily, by those agreements. Rather, it's the distribution of the entire font family,” Rothman said. Whether or not using single letter as a logo would be a violation would depend on the exact terms of the agreement —&nbsp;but again, that appears not to be a potential problem.</p><figure><p><span><img alt="A worker removes letters from the Twitter sign that is posted on the exterior of Twitter headquarters on July 24, 2023 in San Francisco, California." sizes="100vw" srcset="https://themessenger.com/_next/image?url=https%3A%2F%2Fcms.themessenger.com%2Fwp-content%2Fuploads%2F2023%2F07%2FGettyImages-1568024962-1024x683.jpg&amp;w=640&amp;q=75 640w, https://themessenger.com/_next/image?url=https%3A%2F%2Fcms.themessenger.com%2Fwp-content%2Fuploads%2F2023%2F07%2FGettyImages-1568024962-1024x683.jpg&amp;w=750&amp;q=75 750w, https://themessenger.com/_next/image?url=https%3A%2F%2Fcms.themessenger.com%2Fwp-content%2Fuploads%2F2023%2F07%2FGettyImages-1568024962-1024x683.jpg&amp;w=828&amp;q=75 828w, https://themessenger.com/_next/image?url=https%3A%2F%2Fcms.themessenger.com%2Fwp-content%2Fuploads%2F2023%2F07%2FGettyImages-1568024962-1024x683.jpg&amp;w=1080&amp;q=75 1080w, https://themessenger.com/_next/image?url=https%3A%2F%2Fcms.themessenger.com%2Fwp-content%2Fuploads%2F2023%2F07%2FGettyImages-1568024962-1024x683.jpg&amp;w=1200&amp;q=75 1200w, https://themessenger.com/_next/image?url=https%3A%2F%2Fcms.themessenger.com%2Fwp-content%2Fuploads%2F2023%2F07%2FGettyImages-1568024962-1024x683.jpg&amp;w=1920&amp;q=75 1920w, https://themessenger.com/_next/image?url=https%3A%2F%2Fcms.themessenger.com%2Fwp-content%2Fuploads%2F2023%2F07%2FGettyImages-1568024962-1024x683.jpg&amp;w=2048&amp;q=75 2048w, https://themessenger.com/_next/image?url=https%3A%2F%2Fcms.themessenger.com%2Fwp-content%2Fuploads%2F2023%2F07%2FGettyImages-1568024962-1024x683.jpg&amp;w=3840&amp;q=75 3840w" src="https://themessenger.com/_next/image?url=https%3A%2F%2Fcms.themessenger.com%2Fwp-content%2Fuploads%2F2023%2F07%2FGettyImages-1568024962-1024x683.jpg&amp;w=3840&amp;q=75" decoding="async" data-nimg="fill" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p><figcaption><span>A worker removes letters from the Twitter sign that is posted on the exterior of Twitter headquarters on July 24, 2023 in San Francisco, California.</span><span>Justin Sullivan/Getty Images</span></figcaption></figure><p><strong>A Brand Problem?</strong></p><p>Rather, the biggest obstacle in the way of Twitter's rebrand may be the fact that both Microsoft and Meta hold trademarks on the use of the letter X as a brand identity used for social media and communications purposes.&nbsp;</p><p>Microsoft’s trademark application on the brand mark 'X' was filed <a href="https://tsdr.uspto.gov/#caseNumber=76041368&amp;caseSearchType=US_APPLICATION&amp;caseType=DEFAULT&amp;searchType=statusSearch" target="_blank" rel="noreferrer noopener">all the way back in 2003</a> and last renewed on July 18, 2023.</p><p>In the application, Microsoft stated the X branding would be used for “interactive multiplayer game services for games played over computer networks and global communications networks; providing computer games and video games downloadable over computer global communications networks; providing information on the video game and computer game industries via the Internet; and providing information on computer games, video games, video game consoles and accessories therefor via the Internet.”</p><p>For context, a <a href="https://tsdr.uspto.gov/documentviewer?caseId=sn76041368&amp;docId=SPE20221228152814&amp;linkId=3#docIndex=2&amp;page=1" target="_blank" rel="noreferrer noopener">document</a> filed in December in support of the trademark branding was an advertisement for Xbox Community services.&nbsp;</p><p>Meta’s trademark <a href="https://tsdr.uspto.gov/#caseNumber=87980831&amp;caseType=SERIAL_NO&amp;searchType=statusSearch" target="_blank" rel="noreferrer noopener">use on 'X,'</a> which includes initial documentation from <a href="https://tsdr.uspto.gov/documentviewer?caseId=sn87980831&amp;docId=RFA20190508154130&amp;linkId=31#docIndex=30&amp;page=1" target="_blank" rel="noreferrer noopener">Microsoft filed in 2017</a>, is more encompassing.</p><p>The trademark 'X,' which is stylized as a white and blue symbol, is trademarked to brand a wide array of uses, including but not limited to streaming of video and audio content, “providing online forums for transmission of messages among computer users," and "providing internet chat rooms." These are also services offered by Twitter. The company also claimed the right to use X for audio broadcasting, which is similar to the Twitter Spaces feature.&nbsp;</p><p>It’s unclear when the trademark for the stylized X switched from Microsoft to Meta.</p><p>Copyright lawyer Katttwinkel said the Microsoft trademark filing could prevent Twitter from claiming their services are different enough that they could also successfully trademark 'X' for the brand. Twitter's parent company, X Corp., Kattwinkel said, would have to argue in court that the services their brand offers are actually different, despite both companies being to do with similar technology.</p><p>“In my opinion, [Twitter] won't win such an argument," she added.</p><p>And Ed Timberlake, a former examining attorney at the U.S. Patent &amp; Trademark Office and a practicing trademark lawyer, told The Messenger that Microsoft's claim might be the stronger one if both it and Meta separately decided to take Twitter to court.</p><p>"The strength of [Meta's] registration is it's primarily strong for just that stylization," said Timberlake, who was among the Twitter users who <a href="https://twitter.com/TimberlakeLaw/status/1683522597022236672" target="_blank" rel="noreferrer noopener">spotted</a> the Microsoft trademark registration.</p><p>"The rights are narrower because it's registered in that stylized form."</p><p>"Whereas Microsoft's X is essentially the X registered, without regard to what outfit it shows up in," Timberlake said.</p><p>"So it could show up in blue and yellow, it could show up in blue and white, it could show up as just a typed thing, it could show up in a different font — and the registration rights would be just as powerful," he said.</p><p>Meta and Microsoft did not immediately respond to The Messenger's request for comment. Twitter's autoresponder replied "We'll get back to you soon."</p></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Byron Bay data breach victim told to pay Adidas, NBA $1.2M by US courts (170 pts)]]></title>
            <link>https://www.abc.net.au/news/2023-07-25/byron-bay-data-breach-victim-adidas-nab-us-court-action-damages/102575726</link>
            <guid>36855646</guid>
            <pubDate>Mon, 24 Jul 2023 23:29:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.abc.net.au/news/2023-07-25/byron-bay-data-breach-victim-adidas-nab-us-court-action-damages/102575726">https://www.abc.net.au/news/2023-07-25/byron-bay-data-breach-victim-adidas-nab-us-court-action-damages/102575726</a>, See on <a href="https://news.ycombinator.com/item?id=36855646">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Sarah Luke initially shrugged off a data breach that resulted in&nbsp;her personal details being released onto the dark web.</p><section role="contentinfo" aria-label="key points" data-component="KeyPoints" data-uri="coremedia://teaser/102575910"><h2 data-component="Heading">Key points:</h2><ul data-component="List" role="list"><li data-component="ListItem"><span></span>Hackers traded hundreds of counterfeit goods using Sarah Luke's PayPal account</li><li data-component="ListItem"><span></span>Adidas and the National Basketball Association filed charges relating to trademark infringement</li><li data-component="ListItem"><span></span>The US courts awarded damages against Ms Luke amounting to $US1.2 million ($1.8 million)</li></ul></section><p>But then&nbsp;she was charged in the United States with offences including trademark infringement, and was told to pay damages of $US1.2 million ($1.8 million).&nbsp;</p><p>The first inkling of her impending legal drama came via an email in December, referring to counterfeit Adidas items traded under her name.</p><p>"I thought it was a scam, another hoax, and I deleted the first email," she said.</p><p>"After subsequent emails, I realised, there's something in this, this is real.</p><p>"It was shocking, because this is big.&nbsp;</p><p>"The charges were cybersquatting, trademark infringement, IP infringement, things I don't know anything about."&nbsp;</p><p>Ms Luke said the nightmare began after her information was compromised in the <a href="https://www.abc.net.au/news/2022-12-01/medibank-data-leak-has-everything-been-released-now/101720028" data-component="ContentLink" data-uri="coremedia://article/101720028">Medibank data breach</a>.</p><p>She said this was the only breach of her information she was aware of.</p><p>Medibank released a statement to the ABC saying none of its customers' passwords were compromised in the breach, and it was therefore in no way connected to what unfolded for Ms Luke.</p><figure role="group" data-component="VerticalArticleFigure" aria-labelledby="102575902" data-uri="coremedia://imageproxy/102575902" attribution="[object Object]" figuretype="photo"><img alt="A printed legal document from the US district court in Florida, listing Adidas and others as plaintiffs." sizes="(max-width: 543px) 543px," srcset="https://live-production.wcms.abc-cdn.net.au/db002ef42b9d03fc725099844724dbac?impolicy=wcms_crop_resize&amp;cropH=1017&amp;cropW=1356&amp;xPos=221&amp;yPos=0&amp;width=862&amp;height=647 543w, https://live-production.wcms.abc-cdn.net.au/db002ef42b9d03fc725099844724dbac?impolicy=wcms_crop_resize&amp;cropH=1017&amp;cropW=1526&amp;xPos=136&amp;yPos=0&amp;width=862&amp;height=575" src="https://live-production.wcms.abc-cdn.net.au/db002ef42b9d03fc725099844724dbac?impolicy=wcms_crop_resize&amp;cropH=1017&amp;cropW=1526&amp;xPos=136&amp;yPos=0&amp;width=862&amp;height=575" loading="lazy" data-component="Image" data-lazy="true"><p><figcaption id="102575902" data-component="VerticalArticleFigure__figcaption" aria-live="polite"> <!-- -->Two companies&nbsp;took the action against Sarah Luke through the US court system.<!-- --> <!-- -->(<span data-component="Byline"><span data-component="Text"><span>ABC North Coast: Hannah Ross</span></span></span>)</figcaption></p></figure><p>Ms Luke said&nbsp;hackers took&nbsp;control of her PayPal account, in a credential stuffing attack that affected 35,000 PayPal customers in December.</p><p>Credential stuffing is where hackers access an account by using automation to try out username and password pairs sourced from data leaks on various websites.</p><p>Ms Luke said over the course of two days from December 6 to 8, her PayPal account was used to make hundreds of fraudulent transactions.&nbsp;</p><p>She was then served electronically with papers from the US District Court of Florida outlining Adidas' case against her.</p><p>Similar charges against her were also filed by the National Basketball Association in the District Court of Illinois.&nbsp;</p><p>In both cases, Adidas and the NBA were given leave by the courts to run the cases ex parte —&nbsp;without a requirement for all parties in the case to be present.&nbsp;</p><p>In court documents seen by the ABC, default judgements were handed down by the US courts and damages were awarded against Ms Luke of $US200,000 ($293,000) in the NBA case and $US1million ($1.5 million)&nbsp;in the Adidas matter.</p><figure role="group" data-component="VerticalArticleFigure" aria-labelledby="102575904" data-uri="coremedia://imageproxy/102575904" attribution="[object Object]" figuretype="photo"><img alt="A woman with blonde hair sits at a table with a laptop and documents strewn about. She is reading something on a piece of paper." sizes="(max-width: 543px) 543px," srcset="https://live-production.wcms.abc-cdn.net.au/2b669ffe825a348b6695801a1676ec7c?impolicy=wcms_crop_resize&amp;cropH=1068&amp;cropW=1424&amp;xPos=108&amp;yPos=0&amp;width=862&amp;height=647 543w, https://live-production.wcms.abc-cdn.net.au/2b669ffe825a348b6695801a1676ec7c?impolicy=wcms_crop_resize&amp;cropH=1068&amp;cropW=1602&amp;xPos=19&amp;yPos=0&amp;width=862&amp;height=575" src="https://live-production.wcms.abc-cdn.net.au/2b669ffe825a348b6695801a1676ec7c?impolicy=wcms_crop_resize&amp;cropH=1068&amp;cropW=1602&amp;xPos=19&amp;yPos=0&amp;width=862&amp;height=575" loading="lazy" data-component="Image" data-lazy="true"><p><figcaption id="102575904" data-component="VerticalArticleFigure__figcaption" aria-live="polite"> <!-- -->Sarah Luke says the legal action is taking its toll emotionally and financially.<!-- --> <!-- -->(<span data-component="Byline"><span data-component="Text"><span>&nbsp;ABC North Coast: Hannah Ross</span></span></span>)</figcaption></p></figure><h2 data-component="Heading">Emotional toll</h2><p>Six months on, the Byron Bay woman said she was no closer to clearing her name.</p><p>She has taken the matter to the NSW Police, the Australian Consumer Complaints Authority, the Australian Financial Complaints Commission and the Australian Cyber Security Centre.</p><p>"I've come up against so many barriers trying to sort this out," Ms Luke said.</p><p>"I have felt unheard and unseen by so many organisations and parties.</p><p>"It just goes on and on and I don't know where to go now —&nbsp;I don't know who to turn to."</p><figure role="group" data-component="VerticalArticleFigure" aria-labelledby="102625288" data-uri="coremedia://imageproxy/102625288" attribution="[object Object]" figuretype="photo"><img alt="Adidas shoe being held in woman's hands" sizes="(max-width: 543px) 543px," srcset="https://live-production.wcms.abc-cdn.net.au/670f4e78c7547a7031269806d45888d3?impolicy=wcms_crop_resize&amp;cropH=1115&amp;cropW=1487&amp;xPos=0&amp;yPos=0&amp;width=862&amp;height=647 543w, https://live-production.wcms.abc-cdn.net.au/670f4e78c7547a7031269806d45888d3?impolicy=wcms_crop_resize&amp;cropH=1041&amp;cropW=1562&amp;xPos=0&amp;yPos=0&amp;width=862&amp;height=575" src="https://live-production.wcms.abc-cdn.net.au/670f4e78c7547a7031269806d45888d3?impolicy=wcms_crop_resize&amp;cropH=1041&amp;cropW=1562&amp;xPos=0&amp;yPos=0&amp;width=862&amp;height=575" loading="lazy" data-component="Image" data-lazy="true"><p><figcaption id="102625288" data-component="VerticalArticleFigure__figcaption" aria-live="polite"> <!-- -->Adidas filed a civil suit against Sarah Luke in the District Court of Florida.<!-- --> <!-- -->(<span data-component="Byline"><span data-component="Text"><span>ABC North Coast: Hannah Ross</span></span></span>)</figcaption></p></figure><p>Ms Luke has engaged an intellectual property lawyer in the United States, with an initial engagement fee of $US10,000 ($14,800), in a bid to have the rulings overturned and the damages retracted. &nbsp;</p><p>The single mother of four said the situation was taking its toll.</p><p>"The anxiety that this causes, not knowing if they are going to come and take our house, can they freeze my assets, can they get access to my bank accounts?</p><p>"We just don't know and it really is a case of guilty until I can prove otherwise."</p><h2 data-component="Heading">Legal uncertainty</h2><p>William Gallagher, a professor of Law at the Golden Gate University in San Francisco and director of the Intellectual Property Law Program, said it was possible the companies behind the lawsuits were just flexing their corporate muscles.</p><p>"Maybe just getting the judgement is reward in its own right, it shows they are aggressive enforcers, the message gets out there," he said.</p><p>"There are&nbsp;some benefits to famous trademark owners, even if they never end up enforcing the monetary judgement."</p><p>He said these types of civil suits were relatively common in the US, as corporate entities worked to protect their brands.</p><p>But he said it was not automatic that a US judgement could be enforced in a different jurisdiction, such as Australia.</p><p>Ms Luke said it was not a risk she wanted to take.</p><p>"I've been unable to get any advice as to whether these lawsuits can stick or whether these companies are just trying to prove a point," she said.</p><p>Retired magistrate and dean of law at Southern Cross University, David Heilpern, said if a US company wanted to enforce a judgement in Australia, it would have to register it with the local courts.</p><figure role="group" data-component="VerticalArticleFigure" aria-labelledby="102625490" data-uri="coremedia://imageproxy/102625490" attribution="[object Object]" figuretype="photo"><img alt="A man with glasses places a thick legal book on a shelf in an office. He has a neutral expression on his face." sizes="(max-width: 543px) 543px," srcset="https://live-production.wcms.abc-cdn.net.au/23c301c237d0f93f5a3de9e300f50baa?impolicy=wcms_crop_resize&amp;cropH=1098&amp;cropW=1464&amp;xPos=471&amp;yPos=0&amp;width=862&amp;height=647 543w, https://live-production.wcms.abc-cdn.net.au/23c301c237d0f93f5a3de9e300f50baa?impolicy=wcms_crop_resize&amp;cropH=1098&amp;cropW=1647&amp;xPos=288&amp;yPos=0&amp;width=862&amp;height=575" src="https://live-production.wcms.abc-cdn.net.au/23c301c237d0f93f5a3de9e300f50baa?impolicy=wcms_crop_resize&amp;cropH=1098&amp;cropW=1647&amp;xPos=288&amp;yPos=0&amp;width=862&amp;height=575" loading="lazy" data-component="Image" data-lazy="true"><p><figcaption id="102625490" data-component="VerticalArticleFigure__figcaption" aria-live="polite"> <!-- -->David Heilpern says our digital identities are&nbsp;vulnerable to laws in other jurisdictions.<!-- --> <!-- -->(<span data-component="Byline"><span data-component="Text"><span>ABC North Coast: Hannah Ross</span></span></span>)</figcaption></p></figure><p>Mr Heilpern said even if&nbsp;Adidas and the NBA chose not to&nbsp;pursue the matter here,&nbsp;the judgement could affect Ms Luke's ability to travel to the US.</p><p>He said the case highlighted a problem for lawmakers dealing with the rise of cybercrime.</p><p>"It's really a sign of the times and technology being in advance of the law," he said.</p><p>"This situation would not have occurred 30 years ago before we had digital identities that so easily and seamlessly cross national boundaries.</p><p>"The Australian government and the legal mechanisms need to catch up to ensure that … Australians are protected."</p><p>The Australian Financial Complaints Authority said it was unable to comment on individual cases it was investigating.</p><p>The ABC has also sought comment from the federal Minister for Cyber Security, Clare O'Neil.</p><p>The Department of Home&nbsp;Affairs responded by offering to refer the case to&nbsp;the Australian Federal Police.</p></div><p><span data-component="Text">Posted<!-- -->&nbsp;</span><time data-component="ScreenReaderOnly" datetime="2023-07-24T22:17:24.000Z">6 hours ago</time><time data-component="Text">Mon 24 Jul 2023 at 10:17pm</time>, <span data-component="Text">updated<!-- -->&nbsp;</span><time data-component="ScreenReaderOnly" datetime="2023-07-25T02:55:38.000Z">2 hours ago</time><time data-component="Text">Tue 25 Jul 2023 at 2:55am</time></p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Vscode.dev: Local Development with Cloud Tools (129 pts)]]></title>
            <link>https://vscode.dev/</link>
            <guid>36855517</guid>
            <pubDate>Mon, 24 Jul 2023 23:12:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://vscode.dev/">https://vscode.dev/</a>, See on <a href="https://news.ycombinator.com/item?id=36855517">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[The Fall of Stack Overflow (382 pts)]]></title>
            <link>https://observablehq.com/@ayhanfuat/the-fall-of-stack-overflow</link>
            <guid>36855516</guid>
            <pubDate>Mon, 24 Jul 2023 23:11:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://observablehq.com/@ayhanfuat/the-fall-of-stack-overflow">https://observablehq.com/@ayhanfuat/the-fall-of-stack-overflow</a>, See on <a href="https://news.ycombinator.com/item?id=36855516">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__next" data-reactroot=""><nav><div><a href="https://observablehq.com/"><svg role="img" viewBox="0 0 25 28" width="25" height="28" aria-label="Observable" fill="currentColor" style="width:22px"><path d="M12.5 22.6667C11.3458 22.6667 10.3458 22.4153 9.5 21.9127C8.65721 21.412 7.98339 20.7027 7.55521 19.8654C7.09997 18.9942 6.76672 18.0729 6.56354 17.1239C6.34796 16.0947 6.24294 15.0483 6.25 14C6.25 13.1699 6.30417 12.3764 6.41354 11.6176C6.52188 10.8598 6.72292 10.0894 7.01563 9.30748C7.30833 8.52555 7.68542 7.84763 8.14479 7.27274C8.62304 6.68378 9.24141 6.20438 9.95208 5.87163C10.6979 5.51244 11.5458 5.33333 12.5 5.33333C13.6542 5.33333 14.6542 5.58467 15.5 6.08733C16.3428 6.588 17.0166 7.29733 17.4448 8.13459C17.8969 8.99644 18.2271 9.9103 18.4365 10.8761C18.6448 11.841 18.75 12.883 18.75 14C18.75 14.8301 18.6958 15.6236 18.5865 16.3824C18.4699 17.1702 18.2639 17.9446 17.9719 18.6925C17.6698 19.4744 17.2948 20.1524 16.8427 20.7273C16.3906 21.3021 15.7927 21.7692 15.0479 22.1284C14.3031 22.4876 13.4542 22.6667 12.5 22.6667ZM14.7063 16.2945C15.304 15.6944 15.6365 14.864 15.625 14C15.625 13.1073 15.326 12.3425 14.7292 11.7055C14.1313 11.0685 13.3885 10.75 12.5 10.75C11.6115 10.75 10.8688 11.0685 10.2708 11.7055C9.68532 12.3123 9.36198 13.1405 9.375 14C9.375 14.8927 9.67396 15.6575 10.2708 16.2945C10.8688 16.9315 11.6115 17.25 12.5 17.25C13.3885 17.25 14.124 16.9315 14.7063 16.2945ZM12.5 27C19.4031 27 25 21.1792 25 14C25 6.82075 19.4031 1 12.5 1C5.59687 1 0 6.82075 0 14C0 21.1792 5.59687 27 12.5 27Z" fill="currentColor"></path></svg></a></div></nav><div><div><a href="https://observablehq.com/@ayhanfuat"><picture><source srcset="https://avatars1.githubusercontent.com/u/6108349?v=4&amp;s=60&amp;format=avif 1x, https://avatars1.githubusercontent.com/u/6108349?v=4&amp;s=120&amp;format=avif 2x, https://avatars1.githubusercontent.com/u/6108349?v=4&amp;s=180&amp;format=avif 3x" type="image/avif"><source srcset="https://avatars1.githubusercontent.com/u/6108349?v=4&amp;s=60&amp;format=webp 1x, https://avatars1.githubusercontent.com/u/6108349?v=4&amp;s=120&amp;format=webp 2x, https://avatars1.githubusercontent.com/u/6108349?v=4&amp;s=180&amp;format=webp 3x" type="image/webp"><source srcset="https://avatars1.githubusercontent.com/u/6108349?v=4&amp;s=60&amp;format=jpeg 1x, https://avatars1.githubusercontent.com/u/6108349?v=4&amp;s=120&amp;format=jpeg 2x, https://avatars1.githubusercontent.com/u/6108349?v=4&amp;s=180&amp;format=jpeg 3x" type="image/jpeg"><source srcset="https://avatars1.githubusercontent.com/u/6108349?v=4&amp;s=60&amp;format=png 1x, https://avatars1.githubusercontent.com/u/6108349?v=4&amp;s=120&amp;format=png 2x, https://avatars1.githubusercontent.com/u/6108349?v=4&amp;s=180&amp;format=png 3x" type="image/png"><img alt="@ayhanfuat" src="https://avatars1.githubusercontent.com/u/6108349?v=4&amp;s=60"></picture></a><div><p><span data-state="tooltip-hidden" data-reach-tooltip-trigger=""><a href="https://observablehq.com/@ayhanfuat">Ayhan Fuat Çelik</a></span></p></div></div></div><div data-cy="metadata-bar"><div data-cy="access-level" title="Anyone can see edits to this notebook."><svg viewBox="0 0 16 16" width="16" height="16" fill="none" stroke="currentColor" stroke-width="1.8"><circle cx="8" cy="8" r="7.1"></circle><path d="M1.14285 10.5286 L14.8571 10.5286 M1.14285 5.71429H14.8571 M8 1.14285C7.42857 2.09523 5.14285 4.21428 5.14285 8C5.14285 12.5714 8 14.8571 8 14.8571 M8 1.14285C8.57143 2.09523 10.8571 4.21428 10.8571 8C10.8571 12.5714 8 14.8571 8 14.8571"></path></svg><p>Public</p></div><div title=""><svg width="16" height="17" viewBox="0 0 16 17" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M1 14C1 13.4477 1.44772 13 2 13L4 13C4.55228 13 5 13.4477 5 14C5 14.5523 4.55228 15 4 15L2 15C1.44772 15 1 14.5523 1 14Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M10.884 3.65785L7.78484 10.3041L8.12765 11.9351L9.59745 11.1493L12.6967 4.50309L10.884 3.65785ZM10.823 1.42262C10.3224 1.18921 9.72744 1.40577 9.49404 1.90631L5.83135 9.76098C5.7399 9.95707 5.71453 10.1775 5.75904 10.3893L6.44467 13.6513C6.5818 14.3038 7.30684 14.6418 7.89476 14.3275L10.8344 12.7559C11.0252 12.6539 11.1778 12.4928 11.2692 12.2967L14.9319 4.44202C15.1653 3.94148 14.9487 3.3465 14.4482 3.11309L10.823 1.42262Z" fill="currentColor"></path></svg><p>Edited </p></div><div><svg width="16" height="16" viewBox="0 0 16 16" fill="currentColor" style="flex-shrink:0"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.5 1.75C3.80964 1.75 3.25 2.30964 3.25 3C3.25 3.69036 3.80964 4.25 4.5 4.25C5.19036 4.25 5.75 3.69036 5.75 3C5.75 2.30964 5.19036 1.75 4.5 1.75ZM1.75 3C1.75 1.48122 2.98122 0.25 4.5 0.25C6.01878 0.25 7.25 1.48122 7.25 3C7.25 4.16599 6.52434 5.1625 5.5 5.56253V7H8.5C9.4199 7 10.1947 6.37895 10.4281 5.53327C9.44188 5.11546 8.75 4.13853 8.75 3C8.75 1.48122 9.98122 0.25 11.5 0.25C13.0188 0.25 14.25 1.48122 14.25 3C14.25 4.18168 13.5047 5.18928 12.4585 5.57835C12.1782 7.51343 10.5127 9 8.5 9H5.5V10.4375C6.52434 10.8375 7.25 11.834 7.25 13C7.25 14.5188 6.01878 15.75 4.5 15.75C2.98122 15.75 1.75 14.5188 1.75 13C1.75 11.834 2.47566 10.8375 3.5 10.4375L3.5 9V7V5.56253C2.47566 5.1625 1.75 4.16599 1.75 3ZM4.5 11.75C3.80964 11.75 3.25 12.3096 3.25 13C3.25 13.6904 3.80964 14.25 4.5 14.25C5.19036 14.25 5.75 13.6904 5.75 13C5.75 12.3096 5.19036 11.75 4.5 11.75ZM10.25 3C10.25 2.30964 10.8096 1.75 11.5 1.75C12.1904 1.75 12.75 2.30964 12.75 3C12.75 3.69036 12.1904 4.25 11.5 4.25C10.8096 4.25 10.25 3.69036 10.25 3Z"></path></svg><p><span>1&nbsp;fork</span></p></div><div><svg viewBox="0 0 16 16" fill="var(--white)" stroke="var(--moon-gray)" stroke-width="1.8" stroke-linecap="round" stroke-linejoin="round" width="16" height="16" style="width:16px;height:16px;stroke-width:2"><path d="M13.075 3.925A3.157 3.157 0 0 0 10.842 3c-.838 0-1.641.478-2.233 1.07L8 4.68l-.609-.61c-1.233-1.233-3.233-1.378-4.466-.145a3.158 3.158 0 0 0 0 4.467L3.534 9 8 13.788 12.466 9l.609-.608a3.157 3.157 0 0 0 0-4.467z"></path></svg><p>2</p><!-- --><p>&nbsp;Like</p><!-- --><p>s</p></div></div><div><p>3</p></div><div><div><p><svg width="17" height="15" viewBox="-8.5 -7.5 17 15" style="overflow:visible" role="button" aria-label="Click to insert or merge cells" stroke="currentColor" stroke-width="2" stroke-linecap="round" fill="none" transform=""><title>Insert cell</title><path d="M0,-4.5V4.5M-4.5,0H4.5"></path></svg></p></div><div><p><svg width="17" height="15" viewBox="-8.5 -7.5 17 15" style="overflow:visible" role="button" aria-label="Click to insert or merge cells" stroke="currentColor" stroke-width="2" stroke-linecap="round" fill="none" transform=""><title>Insert cell</title><path d="M0,-4.5V4.5M-4.5,0H4.5"></path></svg></p></div><div><p><svg width="17" height="15" viewBox="-8.5 -7.5 17 15" style="overflow:visible" role="button" aria-label="Click to insert or merge cells" stroke="currentColor" stroke-width="2" stroke-linecap="round" fill="none" transform=""><title>Insert cell</title><path d="M0,-4.5V4.5M-4.5,0H4.5"></path></svg></p></div><div><p><svg width="17" height="15" viewBox="-8.5 -7.5 17 15" style="overflow:visible" role="button" aria-label="Click to insert or merge cells" stroke="currentColor" stroke-width="2" stroke-linecap="round" fill="none" transform=""><title>Insert cell</title><path d="M0,-4.5V4.5M-4.5,0H4.5"></path></svg></p></div><div><p><svg width="17" height="15" viewBox="-8.5 -7.5 17 15" style="overflow:visible" role="button" aria-label="Click to insert or merge cells" stroke="currentColor" stroke-width="2" stroke-linecap="round" fill="none" transform=""><title>Insert cell</title><path d="M0,-4.5V4.5M-4.5,0H4.5"></path></svg></p></div><div><p><svg width="17" height="15" viewBox="-8.5 -7.5 17 15" style="overflow:visible" role="button" aria-label="Click to insert or merge cells" stroke="currentColor" stroke-width="2" stroke-linecap="round" fill="none" transform=""><title>Insert cell</title><path d="M0,-4.5V4.5M-4.5,0H4.5"></path></svg></p></div><div><p><svg width="17" height="15" viewBox="-8.5 -7.5 17 15" style="overflow:visible" role="button" aria-label="Click to insert or merge cells" stroke="currentColor" stroke-width="2" stroke-linecap="round" fill="none" transform=""><title>Insert cell</title><path d="M0,-4.5V4.5M-4.5,0H4.5"></path></svg></p></div><div><p><svg width="17" height="15" viewBox="-8.5 -7.5 17 15" style="overflow:visible" role="button" aria-label="Click to insert or merge cells" stroke="currentColor" stroke-width="2" stroke-linecap="round" fill="none" transform=""><title>Insert cell</title><path d="M0,-4.5V4.5M-4.5,0H4.5"></path></svg></p></div><div><p><svg width="17" height="15" viewBox="-8.5 -7.5 17 15" style="overflow:visible" role="button" aria-label="Click to insert or merge cells" stroke="currentColor" stroke-width="2" stroke-linecap="round" fill="none" transform=""><title>Insert cell</title><path d="M0,-4.5V4.5M-4.5,0H4.5"></path></svg></p></div><div><p><svg width="17" height="15" viewBox="-8.5 -7.5 17 15" style="overflow:visible" role="button" aria-label="Click to insert or merge cells" stroke="currentColor" stroke-width="2" stroke-linecap="round" fill="none" transform=""><title>Insert cell</title><path d="M0,-4.5V4.5M-4.5,0H4.5"></path></svg></p></div><div><p><svg width="17" height="15" viewBox="-8.5 -7.5 17 15" style="overflow:visible" role="button" aria-label="Click to insert or merge cells" stroke="currentColor" stroke-width="2" stroke-linecap="round" fill="none" transform=""><title>Insert cell</title><path d="M0,-4.5V4.5M-4.5,0H4.5"></path></svg></p></div><div><p><svg width="17" height="15" viewBox="-8.5 -7.5 17 15" style="overflow:visible" role="button" aria-label="Click to insert or merge cells" stroke="currentColor" stroke-width="2" stroke-linecap="round" fill="none" transform=""><title>Insert cell</title><path d="M0,-4.5V4.5M-4.5,0H4.5"></path></svg></p></div><div><p><svg width="17" height="15" viewBox="-8.5 -7.5 17 15" style="overflow:visible" role="button" aria-label="Click to insert or merge cells" stroke="currentColor" stroke-width="2" stroke-linecap="round" fill="none" transform=""><title>Insert cell</title><path d="M0,-4.5V4.5M-4.5,0H4.5"></path></svg></p></div><div><p><svg width="17" height="15" viewBox="-8.5 -7.5 17 15" style="overflow:visible" role="button" aria-label="Click to insert or merge cells" stroke="currentColor" stroke-width="2" stroke-linecap="round" fill="none" transform=""><title>Insert cell</title><path d="M0,-4.5V4.5M-4.5,0H4.5"></path></svg></p></div><div><p><svg width="17" height="15" viewBox="-8.5 -7.5 17 15" style="overflow:visible" role="button" aria-label="Click to insert or merge cells" stroke="currentColor" stroke-width="2" stroke-linecap="round" fill="none" transform=""><title>Insert cell</title><path d="M0,-4.5V4.5M-4.5,0H4.5"></path></svg></p></div><div><p><svg width="17" height="15" viewBox="-8.5 -7.5 17 15" style="overflow:visible" role="button" aria-label="Click to insert or merge cells" stroke="currentColor" stroke-width="2" stroke-linecap="round" fill="none" transform=""><title>Insert cell</title><path d="M0,-4.5V4.5M-4.5,0H4.5"></path></svg></p></div><div><p><svg width="17" height="15" viewBox="-8.5 -7.5 17 15" style="overflow:visible" role="button" aria-label="Click to insert or merge cells" stroke="currentColor" stroke-width="2" stroke-linecap="round" fill="none" transform=""><title>Insert cell</title><path d="M0,-4.5V4.5M-4.5,0H4.5"></path></svg></p></div><div><p><svg width="17" height="15" viewBox="-8.5 -7.5 17 15" style="overflow:visible" role="button" aria-label="Click to insert or merge cells" stroke="currentColor" stroke-width="2" stroke-linecap="round" fill="none" transform=""><title>Insert cell</title><path d="M0,-4.5V4.5M-4.5,0H4.5"></path></svg></p></div><div><p><svg width="17" height="15" viewBox="-8.5 -7.5 17 15" style="overflow:visible" role="button" aria-label="Click to insert or merge cells" stroke="currentColor" stroke-width="2" stroke-linecap="round" fill="none" transform=""><title>Insert cell</title><path d="M0,-4.5V4.5M-4.5,0H4.5"></path></svg></p></div></div><div data-cy="welcome-mat-banner"><p><span>Create interactive documents like this one.</span></p></div><div><div data-cy="welcome-mat-footer"><p><span>Learn new data visualization techniques. Perform complex data analysis.<!-- --> <br> Publish your findings in a compelling document. All in the same tool.</span></p></div><div><div><p>More from Observable creators</p></div></div><div><div><a title="Home" href="https://observablehq.com/"><svg role="img" width="173" height="24" viewBox="0 0 173 24" aria-label="Observable" fill="currentColor"><path d="M40.8496 20.6083C39.7407 20.6083 38.6757 20.4234 37.6548 20.0538C36.6339 19.6842 35.7186 19.1209 34.9089 18.364C34.1168 17.607 33.4919 16.6565 33.0343 15.5124C32.5766 14.3683 32.3478 13.0218 32.3478 11.4727C32.3478 9.92375 32.5766 8.57718 33.0343 7.43305C33.5095 6.2889 34.1432 5.33839 34.9353 4.5815C35.7274 3.82462 36.6339 3.26135 37.6548 2.8917C38.6757 2.52205 39.7407 2.33723 40.8496 2.33723C41.9585 2.33723 43.0234 2.52205 44.0444 2.8917C45.0653 3.26135 45.9718 3.82462 46.7639 4.5815C47.556 5.33839 48.1809 6.2889 48.6386 7.43305C49.1138 8.57718 49.3514 9.92375 49.3514 11.4727C49.3514 13.0218 49.1225 14.3683 48.665 15.5124C48.2073 16.6565 47.5736 17.607 46.7639 18.364C45.9718 19.1209 45.0653 19.6842 44.0444 20.0538C43.0234 20.4234 41.9585 20.6083 40.8496 20.6083ZM40.8496 18.9977C41.6065 18.9977 42.2049 18.6896 42.645 18.0736C43.1027 17.4398 43.4283 16.5598 43.622 15.4332C43.8332 14.3067 43.9388 12.9865 43.9388 11.4727C43.9388 9.95894 43.8332 8.63879 43.622 7.51226C43.4283 6.38572 43.1027 5.50562 42.645 4.87193C42.2049 4.23826 41.6065 3.92142 40.8496 3.92142C40.1103 3.92142 39.5118 4.23826 39.0542 4.87193C38.5965 5.50562 38.2621 6.38572 38.0509 7.51226C37.8572 8.63879 37.7604 9.95894 37.7604 11.4727C37.7604 12.9865 37.8572 14.3067 38.0509 15.4332C38.2621 16.5598 38.5965 17.4398 39.0542 18.0736C39.5118 18.6896 40.1103 18.9977 40.8496 18.9977ZM50.5773 20.1066V18.76L51.7655 18.496C51.7831 17.8622 51.7919 17.1846 51.7919 16.4629C51.7919 15.7413 51.7919 15.1076 51.7919 14.5619V3.60459L50.3661 3.41977V2.23162L56.386 1.04348L56.8349 1.33392L56.7293 5.00396V8.33075C57.7678 7.30983 59.0175 6.79937 60.4785 6.79937C61.4466 6.79937 62.3267 7.0546 63.1188 7.56506C63.9109 8.05792 64.5358 8.80601 64.9934 9.80933C65.4687 10.7951 65.7063 12.0448 65.7063 13.5586C65.7063 15.002 65.4335 16.2429 64.8878 17.2814C64.3422 18.3199 63.6293 19.1209 62.7491 19.6842C61.8867 20.2298 60.9626 20.5026 59.9768 20.5026C59.22 20.5026 58.5599 20.3706 57.9966 20.1066C57.4333 19.8601 56.9405 19.4993 56.518 19.0241L55.99 20.5026L50.5773 20.1066ZM58.0494 9.41329C57.8206 9.41329 57.6006 9.44848 57.3893 9.5189C57.1957 9.5893 57.0021 9.68612 56.8085 9.80933V17.5982C57.1605 17.8447 57.5742 17.968 58.0494 17.968C58.7535 17.968 59.3168 17.607 59.7392 16.8853C60.1616 16.1637 60.3729 15.0548 60.3729 13.5586C60.3729 12.0448 60.1616 10.9799 59.7392 10.3638C59.3168 9.73012 58.7535 9.41329 58.0494 9.41329ZM71.7275 20.5026C69.8968 20.5026 68.2775 20.0714 66.8693 19.2089L67.0013 15.8292H69.404L69.8264 18.5752C70.1257 18.6984 70.4337 18.7952 70.7505 18.8657C71.0674 18.9185 71.3931 18.9449 71.7275 18.9449C72.3964 18.9449 72.9156 18.8393 73.2852 18.628C73.6549 18.4168 73.8397 18.0647 73.8397 17.5718C73.8397 17.2198 73.699 16.903 73.4173 16.6213C73.1532 16.3397 72.5812 16.0845 71.7011 15.8556L70.1961 15.4596C69.1575 15.178 68.3654 14.6851 67.8198 13.981C67.2742 13.2594 67.0013 12.3881 67.0013 11.3671C67.0013 10.047 67.503 8.95563 68.5063 8.09312C69.5272 7.23062 70.9882 6.79937 72.8892 6.79937C73.699 6.79937 74.447 6.88738 75.1335 7.06341C75.8376 7.23942 76.5505 7.50345 77.2721 7.8555L77.0609 10.839H74.6318L74.0509 8.48917C73.8749 8.45397 73.6813 8.42757 73.4701 8.40996C73.2765 8.37476 73.0389 8.35715 72.7572 8.35715C72.2644 8.35715 71.8419 8.47157 71.4898 8.70039C71.1554 8.91162 70.9882 9.24606 70.9882 9.70372C70.9882 9.98535 71.1115 10.2582 71.3578 10.5222C71.6043 10.7862 72.1851 11.0414 73.1004 11.2879L74.579 11.6839C75.8112 12.0184 76.6912 12.5377 77.2193 13.2417C77.765 13.9459 78.0379 14.8172 78.0379 15.8556C78.0379 17.4047 77.4658 18.5664 76.3216 19.3409C75.1951 20.1153 73.6637 20.5026 71.7275 20.5026ZM85.8767 8.22514C85.4718 8.22514 85.1286 8.52438 84.847 9.12285C84.5653 9.72132 84.4069 10.839 84.3717 12.476H85.6127C86.2464 12.476 86.6688 12.3528 86.88 12.1064C87.1088 11.8424 87.2233 11.3759 87.2233 10.707C87.2233 9.79173 87.0824 9.14926 86.8008 8.7796C86.5368 8.40996 86.2287 8.22514 85.8767 8.22514ZM85.9559 20.5026C84.6358 20.5026 83.4564 20.2298 82.4179 19.6842C81.3969 19.1384 80.5873 18.3552 79.9887 17.3342C79.3903 16.2957 79.091 15.0548 79.091 13.6114C79.091 12.4673 79.2935 11.4727 79.6983 10.6278C80.1032 9.78293 80.6488 9.07884 81.3353 8.51557C82.0219 7.93471 82.7876 7.50345 83.6325 7.22182C84.4774 6.94018 85.331 6.79937 86.1936 6.79937C87.4609 6.79937 88.5082 7.06341 89.3355 7.59147C90.1804 8.10193 90.8141 8.79721 91.2365 9.67732C91.659 10.5398 91.8702 11.5079 91.8702 12.5816C91.8702 12.8633 91.8614 13.1097 91.8438 13.3209C91.8262 13.5146 91.791 13.7346 91.7382 13.981H84.3981C84.5389 15.266 84.9174 16.1989 85.5335 16.7797C86.1672 17.3606 86.8712 17.651 87.6457 17.651C88.3145 17.651 88.8867 17.5367 89.3619 17.3078C89.8547 17.0614 90.286 16.7622 90.6557 16.4101L91.6854 17.4134C91.1221 18.4872 90.3477 19.2704 89.3619 19.7634C88.3937 20.2562 87.2584 20.5026 85.9559 20.5026ZM92.9624 20.0802V18.7336L94.2298 18.4432C94.2473 17.8094 94.2562 17.1406 94.2562 16.4365C94.2562 15.7149 94.2562 15.0812 94.2562 14.5355V12.8721C94.2562 12.3792 94.2473 11.9832 94.2298 11.6839C94.2298 11.3848 94.221 11.1119 94.2034 10.8654C94.2034 10.6014 94.1945 10.2934 94.177 9.94135L92.7512 9.70372V8.62118L98.5863 6.79937L99.0615 7.08981L99.2727 10.1526C99.6248 9.00844 100.127 8.16353 100.778 7.61787C101.447 7.0722 102.098 6.79937 102.732 6.79937C103.383 6.79937 103.929 6.99299 104.369 7.38024C104.826 7.74988 105.099 8.33956 105.187 9.14925C105.152 9.85333 104.941 10.4078 104.553 10.8126C104.166 11.1998 103.709 11.3935 103.18 11.3935C102.371 11.3935 101.631 10.9094 100.963 9.94135L100.831 9.75652C100.514 10.1086 100.197 10.5662 99.88 11.1295C99.5808 11.6928 99.3784 12.2736 99.2727 12.8721V14.5355C99.2727 15.046 99.2727 15.6444 99.2727 16.3309C99.2727 17.0173 99.2816 17.6687 99.2991 18.2848L101.359 18.7336V20.0802H92.9624ZM114.743 8.46277V7.22182H119.707V8.46277L118.123 8.7268L113.766 20.0802H111.39L106.69 8.7268L105.423 8.46277V7.22182H113.37V8.46277L111.971 8.7796L114.162 14.8523L116.195 8.7532L114.743 8.46277ZM129.974 20.5026C129.147 20.5026 128.478 20.3442 127.967 20.0274C127.475 19.6929 127.114 19.2265 126.885 18.628C126.374 19.1912 125.855 19.6489 125.327 20.001C124.817 20.3354 124.086 20.5026 123.136 20.5026C122.097 20.5026 121.261 20.2034 120.627 19.6049C120.011 19.0064 119.703 18.1703 119.703 17.0966C119.703 16.4101 119.853 15.8028 120.152 15.2747C120.469 14.7291 121.015 14.2363 121.789 13.7962C122.581 13.3386 123.69 12.9249 125.116 12.5552C125.327 12.5024 125.574 12.4409 125.855 12.3704C126.137 12.2825 126.427 12.2033 126.727 12.1328V11.1823C126.727 10.0558 126.594 9.28127 126.33 8.85881C126.084 8.43636 125.547 8.22514 124.72 8.22514C124.649 8.22514 124.579 8.22514 124.509 8.22514C124.456 8.22514 124.394 8.22514 124.324 8.22514V9.12285C124.324 10.1966 124.104 10.9622 123.664 11.4199C123.224 11.86 122.722 12.08 122.159 12.08C121.085 12.08 120.416 11.6047 120.152 10.6542C120.152 9.51009 120.689 8.58599 121.763 7.8819C122.854 7.16021 124.465 6.79937 126.594 6.79937C128.425 6.79937 129.71 7.20422 130.449 8.01391C131.206 8.80601 131.585 10.1086 131.585 11.9216V17.968C131.585 18.2671 131.734 18.4168 132.034 18.4168C132.139 18.4168 132.245 18.3816 132.35 18.3112C132.456 18.2232 132.588 18.0647 132.746 17.8358L133.512 18.2584C133.195 19.0681 132.755 19.6489 132.192 20.001C131.646 20.3354 130.907 20.5026 129.974 20.5026ZM124.245 16.2781C124.245 16.9294 124.377 17.4134 124.641 17.7302C124.905 18.0472 125.23 18.2056 125.618 18.2056C125.741 18.2056 125.873 18.1792 126.014 18.1264C126.172 18.0559 126.41 17.9152 126.727 17.7038V13.3737C126.462 13.4442 126.207 13.541 125.961 13.6642C125.591 13.8403 125.213 14.1394 124.825 14.5619C124.438 14.9843 124.245 15.5564 124.245 16.2781ZM134.041 20.1066V18.76L135.229 18.496C135.247 17.8622 135.256 17.1846 135.256 16.4629C135.256 15.7413 135.256 15.1076 135.256 14.5619V3.60459L133.83 3.41977V2.23162L139.85 1.04348L140.299 1.33392L140.193 5.00396V8.33075C141.232 7.30983 142.481 6.79937 143.942 6.79937C144.91 6.79937 145.791 7.0546 146.583 7.56506C147.375 8.05792 148 8.80601 148.457 9.80933C148.933 10.7951 149.17 12.0448 149.17 13.5586C149.17 15.002 148.898 16.2429 148.352 17.2814C147.806 18.3199 147.093 19.1209 146.213 19.6842C145.351 20.2298 144.426 20.5026 143.441 20.5026C142.684 20.5026 142.024 20.3706 141.461 20.1066C140.897 19.8601 140.405 19.4993 139.982 19.0241L139.454 20.5026L134.041 20.1066ZM141.513 9.41329C141.284 9.41329 141.064 9.44848 140.853 9.5189C140.659 9.5893 140.466 9.68612 140.272 9.80933V17.5982C140.625 17.8447 141.038 17.968 141.513 17.968C142.217 17.968 142.781 17.607 143.203 16.8853C143.626 16.1637 143.837 15.0548 143.837 13.5586C143.837 12.0448 143.626 10.9799 143.203 10.3638C142.781 9.73012 142.217 9.41329 141.513 9.41329ZM150.465 20.0802V18.7336L151.653 18.4696C151.671 17.8007 151.679 17.1406 151.679 16.4893C151.697 15.838 151.707 15.1868 151.707 14.5355V3.6838L150.28 3.41977V2.23162L156.379 1.04348L156.828 1.33392L156.723 5.00396V14.5355C156.723 15.1868 156.723 15.8469 156.723 16.5157C156.74 17.167 156.758 17.8271 156.776 18.496L157.963 18.7336V20.0802H150.465ZM165.938 8.22514C165.533 8.22514 165.189 8.52438 164.908 9.12285C164.626 9.72132 164.467 10.839 164.432 12.476H165.674C166.307 12.476 166.73 12.3528 166.94 12.1064C167.169 11.8424 167.284 11.3759 167.284 10.707C167.284 9.79173 167.143 9.14926 166.861 8.7796C166.597 8.40996 166.289 8.22514 165.938 8.22514ZM166.016 20.5026C164.696 20.5026 163.517 20.2298 162.478 19.6842C161.457 19.1384 160.647 18.3552 160.049 17.3342C159.45 16.2957 159.152 15.0548 159.152 13.6114C159.152 12.4673 159.354 11.4727 159.759 10.6278C160.164 9.78293 160.71 9.07884 161.395 8.51557C162.082 7.93471 162.848 7.50345 163.693 7.22182C164.537 6.94018 165.392 6.79937 166.254 6.79937C167.522 6.79937 168.568 7.06341 169.396 7.59147C170.241 8.10193 170.874 8.79721 171.297 9.67732C171.719 10.5398 171.93 11.5079 171.93 12.5816C171.93 12.8633 171.922 13.1097 171.904 13.3209C171.886 13.5146 171.851 13.7346 171.799 13.981H164.459C164.6 15.266 164.978 16.1989 165.594 16.7797C166.228 17.3606 166.932 17.651 167.706 17.651C168.375 17.651 168.947 17.5367 169.423 17.3078C169.915 17.0614 170.346 16.7622 170.716 16.4101L171.746 17.4134C171.182 18.4872 170.408 19.2704 169.423 19.7634C168.454 20.2562 167.319 20.5026 166.016 20.5026Z"></path><path d="M11.4413 19.7265C10.3849 19.7265 9.46964 19.4977 8.69544 19.04C7.92403 18.5841 7.30727 17.9383 6.91536 17.1761C6.49867 16.3829 6.19365 15.5442 6.00767 14.6801C5.81035 13.743 5.71422 12.7903 5.72068 11.8359C5.72068 11.0802 5.77027 10.3577 5.87038 9.66687C5.96953 8.97688 6.15355 8.27548 6.42147 7.56358C6.68939 6.85167 7.03454 6.23445 7.45501 5.71104C7.89276 5.17482 8.45875 4.73835 9.10924 4.4354C9.79191 4.10837 10.568 3.9453 11.4413 3.9453C12.4977 3.9453 13.4131 4.17413 14.1873 4.63178C14.9587 5.08762 15.5755 5.73343 15.9674 6.49571C16.3811 7.28039 16.6834 8.11242 16.875 8.99178C17.0658 9.87026 17.1621 10.8189 17.1621 11.8359C17.1621 12.5917 17.1125 13.3141 17.0123 14.0049C16.9057 14.7222 16.7171 15.4272 16.4498 16.1082C16.1733 16.8201 15.8301 17.4374 15.4162 17.9608C15.0025 18.4842 14.4552 18.9094 13.7735 19.2364C13.0918 19.5634 12.3147 19.7265 11.4413 19.7265ZM13.4608 13.9249C14.0079 13.3785 14.3122 12.6225 14.3017 11.8359C14.3017 11.0232 14.0281 10.3268 13.4817 9.74686C12.9344 9.1669 12.2547 8.87692 11.4413 8.87692C10.628 8.87692 9.94827 9.1669 9.40099 9.74686C8.86507 10.2993 8.56912 11.0534 8.58102 11.8359C8.58102 12.6486 8.85467 13.3449 9.40099 13.9249C9.94827 14.5049 10.628 14.7949 11.4413 14.7949C12.2547 14.7949 12.9278 14.5049 13.4608 13.9249ZM11.4413 23.6718C17.7599 23.6718 22.8828 18.3723 22.8828 11.8359C22.8828 5.29952 17.7599 0 11.4413 0C5.12287 0 0 5.29952 0 11.8359C0 18.3723 5.12287 23.6718 11.4413 23.6718Z"></path></svg></a><div><p><span><a href="https://www.linkedin.com/company/observable"><svg height="16" style="fill:currentColor" version="1.1" viewBox="0 0 512 512" width="16"><title>LinkedIn</title><path d="M473.305,-1.353c20.88,0 37.885,16.533 37.885,36.926l0,438.251c0,20.393 -17.005,36.954 -37.885,36.954l-436.459,0c-20.839,0 -37.773,-16.561 -37.773,-36.954l0,-438.251c0,-20.393 16.934,-36.926 37.773,-36.926l436.459,0Zm-37.829,436.389l0,-134.034c0,-65.822 -14.212,-116.427 -91.12,-116.427c-36.955,0 -61.739,20.263 -71.867,39.476l-1.04,0l0,-33.411l-72.811,0l0,244.396l75.866,0l0,-120.878c0,-31.883 6.031,-62.773 45.554,-62.773c38.981,0 39.468,36.461 39.468,64.802l0,118.849l75.95,0Zm-284.489,-244.396l-76.034,0l0,244.396l76.034,0l0,-244.396Zm-37.997,-121.489c-24.395,0 -44.066,19.735 -44.066,44.047c0,24.318 19.671,44.052 44.066,44.052c24.299,0 44.026,-19.734 44.026,-44.052c0,-24.312 -19.727,-44.047 -44.026,-44.047Z" style="fill-rule:nonzero"></path></svg></a></span><span><a href="https://twitter.com/observablehq"><svg viewBox="0 0 16 16" width="16" height="16" fill="currentColor"><title>Twitter</title><path d="M15 3.429c-.517.24-1.07.402-1.651.477a3.028 3.028 0 0 0 1.264-1.675 5.761 5.761 0 0 1-1.827.728 2.8 2.8 0 0 0-2.1-.959C9.1 2 7.812 3.355 7.812 5.025c0 .24.027.47.074.691-2.39-.119-4.509-1.327-5.926-3.153-.25.444-.39.96-.39 1.522 0 1.052.509 1.977 1.279 2.52a2.758 2.758 0 0 1-1.302-.379c.66 1.819 1.428 2.821 2.306 3.007a2.783 2.783 0 0 1-1.293.052c.37 1.202 1.43 2.078 2.691 2.102A5.588 5.588 0 0 1 1 12.641 7.886 7.886 0 0 0 5.417 14c5.291 0 8.181-4.611 8.181-8.604 0-.128 0-.258-.008-.387.374-.283.844-.81 1.41-1.58z"></path></svg></a></span><span><a href="https://github.com/observablehq/"><svg viewBox="0 0 16 16" width="16" height="16" fill="currentColor"><title>GitHub</title><path d="M14.0609 4.65755C13.435 3.58505 12.5859 2.73595 11.5135 2.11005C10.4409 1.48413 9.26999 1.17125 7.99989 1.17125C6.72994 1.17125 5.55864 1.48423 4.4863 2.11005C3.4138 2.73591 2.56476 3.58505 1.9388 4.65755C1.31295 5.73002 1 6.90116 1 8.17095C1 9.69625 1.44501 11.0678 2.33526 12.2861C3.22542 13.5044 4.37536 14.3474 5.78501 14.8153C5.94909 14.8457 6.07056 14.8243 6.14954 14.7516C6.22855 14.6787 6.26801 14.5875 6.26801 14.4782C6.26801 14.46 6.26644 14.296 6.26341 13.9861C6.26028 13.6761 6.25881 13.4057 6.25881 13.175L6.04917 13.2113C5.91551 13.2358 5.74689 13.2461 5.54331 13.2432C5.33983 13.2404 5.1286 13.219 4.90989 13.1794C4.69109 13.1401 4.48757 13.0489 4.29919 12.9062C4.11091 12.7634 3.97725 12.5764 3.89823 12.3457L3.80709 12.136C3.74634 11.9963 3.6507 11.8412 3.52004 11.6712C3.38937 11.501 3.25724 11.3856 3.12358 11.3249L3.05977 11.2792C3.01724 11.2488 2.97779 11.2122 2.9413 11.1697C2.90484 11.1273 2.87755 11.0847 2.85932 11.0421C2.84106 10.9995 2.85619 10.9646 2.90487 10.9371C2.95356 10.9097 3.04154 10.8964 3.1692 10.8964L3.35142 10.9236C3.47295 10.948 3.62328 11.0208 3.80259 11.1424C3.98181 11.2639 4.12914 11.4218 4.2446 11.6162C4.38443 11.8654 4.55289 12.0552 4.75046 12.1859C4.94788 12.3166 5.14692 12.3818 5.3474 12.3818C5.54788 12.3818 5.72103 12.3666 5.86692 12.3364C6.01265 12.306 6.14938 12.2603 6.27704 12.1996C6.33173 11.7923 6.48062 11.4794 6.72359 11.2607C6.37728 11.2243 6.06593 11.1695 5.78938 11.0966C5.51299 11.0236 5.22737 10.9052 4.93271 10.741C4.6379 10.577 4.39334 10.3733 4.19895 10.1304C4.00454 9.88734 3.84499 9.56824 3.72052 9.17337C3.59598 8.77835 3.5337 8.32268 3.5337 7.80622C3.5337 7.07086 3.77377 6.4451 4.2538 5.92858C4.02893 5.37573 4.05016 4.75597 4.31755 4.06936C4.49377 4.01461 4.75509 4.05569 5.1014 4.19236C5.44777 4.32909 5.70137 4.44621 5.86245 4.54332C6.02354 4.6404 6.15261 4.72267 6.24984 4.78939C6.81505 4.63147 7.39832 4.55249 7.99982 4.55249C8.60133 4.55249 9.18473 4.63147 9.74996 4.78939L10.0963 4.57075C10.3331 4.42486 10.6128 4.29116 10.9347 4.16963C11.2567 4.04816 11.503 4.0147 11.6732 4.06945C11.9465 4.75609 11.9709 5.37582 11.7459 5.92867C12.2259 6.4452 12.4661 7.07112 12.4661 7.80632C12.4661 8.32277 12.4036 8.77989 12.2793 9.17794C12.1548 9.57606 11.9938 9.89485 11.7964 10.135C11.5988 10.3751 11.3526 10.5771 11.058 10.7411C10.7633 10.9052 10.4776 11.0236 10.2012 11.0966C9.92465 11.1695 9.6133 11.2244 9.26699 11.2608C9.58284 11.5342 9.7408 11.9656 9.7408 12.555V14.478C9.7408 14.5872 9.77879 14.6784 9.85483 14.7513C9.93078 14.8241 10.0507 14.8455 10.2148 14.815C11.6246 14.3472 12.7746 13.5041 13.6647 12.2858C14.5547 11.0676 14.9999 9.69599 14.9999 8.17069C14.9996 6.90106 14.6865 5.73002 14.0609 4.65755Z"></path></svg></a></span><span><a href="https://www.youtube.com/c/Observablehq"><svg height="16" style="fill:currentColor" version="1.1" viewBox="0 0 512 512" width="16"><title>YouTube</title><path d="M501.303,132.765c-5.887,-22.03 -23.235,-39.377 -45.265,-45.265c-39.932,-10.7 -200.038,-10.7 -200.038,-10.7c0,0 -160.107,0 -200.039,10.7c-22.026,5.888 -39.377,23.235 -45.264,45.265c-10.697,39.928 -10.697,123.238 -10.697,123.238c0,0 0,83.308 10.697,123.232c5.887,22.03 23.238,39.382 45.264,45.269c39.932,10.696 200.039,10.696 200.039,10.696c0,0 160.106,0 200.038,-10.696c22.03,-5.887 39.378,-23.239 45.265,-45.269c10.696,-39.924 10.696,-123.232 10.696,-123.232c0,0 0,-83.31 -10.696,-123.238Zm-296.506,200.039l0,-153.603l133.019,76.802l-133.019,76.801Z" style="fill-rule:nonzero"></path></svg></a></span></p></div></div><div><div><div><p><a href="https://observablehq.com/product">Product</a></p></div><ul><li><a href="https://observablehq.com/data-integrations">Integrations</a></li><li><a href="https://observablehq.com/pricing">Pricing</a></li><li><a href="https://observablehq.com/enterprise">Enterprise</a></li></ul></div><div><div><p><a href="https://observablehq.com/explore">Resources</a></p></div><ul><li><a href="https://observablehq.com/customer-stories">Customer stories</a></li><li><a href="https://observablehq.com/community">Community</a></li><li><a href="https://observablehq.com/documentation">Documentation</a></li></ul></div><div><div><p><a href="https://observablehq.com/about">Company</a></p></div><ul><li><a href="https://observablehq.com/blog">Blog</a></li><li><a href="https://observablehq.com/about#jobs">Jobs</a></li><li><a href="https://observablehq.com/collection/@observablehq/newsletters/2">Newsletter</a></li></ul></div></div><div><p><span>© 2023 Observable, Inc.</span><span><a href="https://observablehq.com/privacy-policy">Privacy</a></span><span><a href="https://observablehq.com/terms-of-service">Terms of Service</a></span></p></div></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft Owns the Trademark for “X” (206 pts)]]></title>
            <link>https://tsdr.uspto.gov/#caseNumber=2693757&amp;caseSearchType=US_APPLICATION&amp;caseType=DEFAULT&amp;searchType=statusSearch</link>
            <guid>36855342</guid>
            <pubDate>Mon, 24 Jul 2023 22:50:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tsdr.uspto.gov/#caseNumber=2693757&#x26;caseSearchType=US_APPLICATION&#x26;caseType=DEFAULT&#x26;searchType=statusSearch">https://tsdr.uspto.gov/#caseNumber=2693757&#x26;caseSearchType=US_APPLICATION&#x26;caseType=DEFAULT&#x26;searchType=statusSearch</a>, See on <a href="https://news.ycombinator.com/item?id=36855342">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="contentBoxMain">
            <div id="announcementWrapper"><pre>&lt;b&gt;For assistance with TSDR&lt;/b&gt;, email &lt;a href="mailto:teas@uspto.gov"&gt;teas@uspto.gov&lt;/a&gt;  and include your serial number, the document you are looking for, and a screenshot of any error messages you have received.</pre></div>
            
            
            
            
            
            <div id="multiSearchContainer">
               <table id="multiSerReg">
                  <colgroup>
                     <col width="10%">
                     <col width="10%">
                     <col width="7%">
                     <col width="7%">
                     <col width="7%">
                     <col width="9%">
                     <col width="17%">
                     <col width="10%">
                     <col width="20%">
                  </colgroup>
                  <thead>
                     <tr>
                        <th>Image</th>
                        <th>Mark</th>
                        <th>Ser No</th>
                        <th>Reg No</th>
                        <th>Status</th>
                        <th>Filing</th>
                        <th>Owner</th>
                        <th>Class(es)</th>
                        <th>Goods and Services</th>
                     </tr>
                  </thead>
                  <tfoot>
                     <tr>
                        <th>Image</th>
                        <th>Mark</th>
                        <th>Ser No</th>
                        <th>Reg No</th>
                        <th>Status</th>
                        <th>Filing</th>
                        <th>Owner</th>
                        <th>Class(es)</th>
                        <th>Goods and Services</th>
                     </tr>
                  </tfoot>
               </table>
               <table id="multiIntlReg">
                  <colgroup>
                     <col width="10%">
                     <col width="10%">
                     <col width="7%">
                     <col width="7%">
                     <col width="7%">
                     <col width="9%">
                     <col width="17%">
                     <col width="10%">
                     <col width="20%">
                  </colgroup>
                  <thead>
                     <tr>
                        <th>Image</th>
                        <th>Mark</th>
                        <th>IR No</th>
                        <th>IR Date</th>
                        <th>Ser No</th>
                        <th>Ref No</th>
                        <th>Owner</th>
                        <th>Class(es)</th>
                        <th>Goods and Services</th>
                     </tr>
                  </thead>
                  <tfoot>
                     <tr>
                        <th>Image</th>
                        <th>Mark</th>
                        <th>IR No</th>
                        <th>IR Date</th>
                        <th>Ser No</th>
                        <th>Ref No</th>
                        <th>Owner</th>
                        <th>Class(es)</th>
                        <th>Goods and Services</th>
                     </tr>
                  </tfoot>
               </table>
               <table id="multiRef">
                  <colgroup>
                     <col width="15%">
                     <col width="15%">
                     <col width="15%">
                     <col width="15%">
                     <col width="40%">
                  </colgroup>
                  <thead>
                     <tr>
                        <th>Reference No.</th>
                        <th>Filing Date</th>
                        <th>Intl Reg No.</th>
                        <th>Intl Reg Date</th>
                        <th>Status</th>
                     </tr>
                  </thead>
                  <tfoot>
                     <tr>
                        <th>Reference No.</th>
                        <th>Filing Date</th>
                        <th>Intl Reg No.</th>
                        <th>Intl Reg Date</th>
                        <th>Status</th>
                     </tr>
                  </tfoot>
               </table>
               <table id="multiCerts">
                  <colgroup>
                     <col width="15%">
                     <col width="19%">
                     <col width="19%">
                     <col width="34%">
                     <col width="23%">
                  </colgroup>
                  <thead>
                     <tr>
                        <th><label for="certResultsTbody">Select All
                              </label></th>
                        <th>Number</th>
                        <th>Mail/Create Date</th>
                        <th>Description</th>
                        <th>Type</th>
                     </tr>
                  </thead>
                  <tfoot>
                     <tr>
                        <th>Select All</th>
                        <th>Number</th>
                        <th>Mail/Create Date</th>
                        <th>Description</th>
                        <th>Type</th>
                     </tr>
                  </tfoot>
               </table>
            </div>
         </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cephalopods use RNA editing to modify their nervous sytems (115 pts)]]></title>
            <link>https://pubmed.ncbi.nlm.nih.gov/28388405/</link>
            <guid>36855236</guid>
            <pubDate>Mon, 24 Jul 2023 22:38:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pubmed.ncbi.nlm.nih.gov/28388405/">https://pubmed.ncbi.nlm.nih.gov/28388405/</a>, See on <a href="https://news.ycombinator.com/item?id=36855236">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article-page" data-article-pmid="28388405">
    
<!-- "Filters applied" shows only when page is redirected from search -->
<!-- because search found one result -->

    

    

    <main id="article-details">
  
  

  

  



  


<header id="heading">
  
    
      
      <div id="short-view-heading">
        

        

<h2>
  
    
    
    
    
      
  Trade-off between Transcriptome Plasticity and Genome Evolution in Cephalopods


    
  
</h2>

        

        <p><span>
    
      
        <span><span>Noa Liscovitch-Brauer</span><span>&nbsp;et al.</span></span>
      
    
  </span>
  
    
      <span>
        Cell<span>.</span>
      </span>
      
        <span>
          <time datetime="2017">2017</time><span>.</span>
        </span>
      
    
  
</p>

        
  <p><span>Free PMC article</span></p>
        
      </div>
    
  
</header>

  



  

  



  <div id="abstract">
    
      <h2>
        Abstract
        
      </h2>
      
        
          
            <p>
      
      RNA editing, a post-transcriptional process, allows the diversification of proteomes beyond the genomic blueprint; however it is infrequently used among animals for this purpose. Recent reports suggesting increased levels of RNA editing in squids thus raise the question of the nature and effects of these events. We here show that RNA editing is particularly common in behaviorally sophisticated coleoid cephalopods, with tens of thousands of evolutionarily conserved sites. Editing is enriched in the nervous system, affecting molecules pertinent for excitability and neuronal morphology. The genomic sequence flanking editing sites is highly conserved, suggesting that the process confers a selective advantage. Due to the large number of sites, the surrounding conservation greatly reduces the number of mutations and genomic polymorphisms in protein-coding regions. This trade-off between genome evolution and transcriptome plasticity highlights the importance of RNA recoding as a strategy for diversifying proteins, particularly those associated with neural function. PAPERCLIP.
    </p>
          
        
      

      
    

    

    
      


  

  
    <p>
      
        <strong>
          Keywords:
        </strong>
      
      ADAR; Epitranscriptome; RNA editing; RNA modifications; cephalopods; genome evolution; neural plasticity; proteome diversity.
    </p>
  


    

  </div>


  
  


  <p id="copyright">
    Copyright © 2017 Elsevier Inc. All rights reserved.
  </p>


  
  <div id="conflict-of-interest">
    <h2>
      Conflict of interest statement
    </h2>

    <p xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:p1="http://pubmed.gov/pub-one">The authors declare no conflict of interest.</p>
  </div>


  
  
  
    
      <div id="figures">
        <h2>
          Figures
        </h2>

        <div id="slides-container" itemscope="" itemtype="http://schema.org/ImageGallery">
          
            <figure itemscope="" itemtype="http://schema.org/ImageObject" itemprop="associatedMedia" data-slide-index="0" data-label-slug="figure-1">
              <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/instance/5499236/bin/nihms863911f1.jpg" itemprop="contentUrl" aria-describedby="figure-caption-0" role="button" data-image-width="800" data-image-height="495" data-image-alt="Figure 1" data-pmc-id="PMC5499236" data-figure-id="F1">
                <img itemprop="thumbnail" id="article-image-0" src="https://www.ncbi.nlm.nih.gov/pmc/articles/instance/5499236/bin/nihms863911f1.gif" alt="Figure 1">
              </a>

              <meta itemprop="width" itemtype="http://schema.org/ImageObject" content="800">
              <meta itemprop="height" itemtype="http://schema.org/ImageObject" content="495">

              
                

                

                <figcaption id="figure-caption-0" itemtype="http://schema.org/ImageObject" itemprop="description">
                  
                  



  
    <strong>
      Figure 1. Extensive recoding is an invention of coleoid cephalopods
    </strong>
  

  
    <p><b>(A)</b> The species studied span the cephalopod evolutionary tree, as well as sea hare (<i>Aplysia californica</i>) as an outgroup (top). For comparison, a representative tree for vertebrates is shown (bottom), constructed based on divergence times estimated in (Hedges et al., 2006). <b>(B)</b> Tens-thousands of A-to-I editing sites (identified as A-to-G DNA-RNA mismatches) are detected in squid, sepia and the two octopus species (see Tables S1–S4 for more details). The noise level (estimated by the number of G-to-A mismatches) is rather low. In contrast, in nautilus and sea hare no enrichment of A-to-G mismatches is observed (inset). <b>(C)</b> The nucleotides neighboring the detected editing sites, show a clear pattern consistent with known ADAR preference (Alon et al., 2015; Eggington et al., 2011; Kleinberger and Eisenberg, 2010) for the extensively recoded coleoid species – squid, sepia, and the two octopus species – but not in nautilus or sea hare. The motif is characterized by under-representation of G upstream to the editing site (relative location −1) and over-representation of G in the downstream base (The height of the entire stack of letters represents the information content in bits, the relative height of each letter represents its frequency).</p>
  



                </figcaption>
              

            </figure>
          
            <figure itemscope="" itemtype="http://schema.org/ImageObject" itemprop="associatedMedia" data-slide-index="1" data-label-slug="figure-2">
              <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/instance/5499236/bin/nihms863911f2.jpg" itemprop="contentUrl" aria-describedby="figure-caption-1" role="button" data-image-width="800" data-image-height="312" data-image-alt="Figure 2" data-pmc-id="PMC5499236" data-figure-id="F2">
                <img itemprop="thumbnail" id="article-image-1" src="https://www.ncbi.nlm.nih.gov/pmc/articles/instance/5499236/bin/nihms863911f2.gif" alt="Figure 2">
              </a>

              <meta itemprop="width" itemtype="http://schema.org/ImageObject" content="800">
              <meta itemprop="height" itemtype="http://schema.org/ImageObject" content="312">

              
                

                

                <figcaption id="figure-caption-1" itemtype="http://schema.org/ImageObject" itemprop="description">
                  
                  



  
    <strong>
      Figure 2. Proteomic validation of recoding by RNA editing
    </strong>
  

  
    <p>We analyzed peptides identified by mass spectrometry analysis of two squid tissues, looking for evidence of recoding. For each site covered by one or more peptides, we marked whether the edited, non-edited or both versions of the peptide are observed. The distribution is presented, binned by the predicted RNA editing level (as measured from RNA-seq data). In parentheses are the numbers of recoding sites analyzed in each editing-level bin. The proteomic recoding level follows closely the predicted RNA editing level. Altogether, this experiment validated protein recoding in 432 sites in two tissues: <b>(A)</b> Squid stellate ganglion, where 320 of the 3,204 single-site peptides (10.0%) were shown to be edited. <b>(B)</b> Squid giant axon (giant fiber lobe), where 283 of the 2,741 single-site peptides (10.3%) were shown to be edited.</p>
  



                </figcaption>
              

            </figure>
          
            <figure itemscope="" itemtype="http://schema.org/ImageObject" itemprop="associatedMedia" data-slide-index="2" data-label-slug="figure-3">
              <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/instance/5499236/bin/nihms863911f3.jpg" itemprop="contentUrl" aria-describedby="figure-caption-2" role="button" data-image-width="800" data-image-height="930" data-image-alt="Figure 3" data-pmc-id="PMC5499236" data-figure-id="F3">
                <img itemprop="thumbnail" id="article-image-2" src="https://www.ncbi.nlm.nih.gov/pmc/articles/instance/5499236/bin/nihms863911f3.gif" alt="Figure 3">
              </a>

              <meta itemprop="width" itemtype="http://schema.org/ImageObject" content="800">
              <meta itemprop="height" itemtype="http://schema.org/ImageObject" content="930">

              
                

                

                <figcaption id="figure-caption-2" itemtype="http://schema.org/ImageObject" itemprop="description">
                  
                  



  
    <strong>
      Figure 3. Editing in Octopus bimaculoides
    </strong>
  

  
    <p><b>(A)</b> A-to-I editing sites were found within coding sequences of <i>Octopus bimaculoides</i> using three methods: the genome-free method (alignment to de-novo transcriptome), the genome-dependent approach using REDItools (Picardi and Pesole, 2013), and identification of hyper-edited reads (Porath et al., 2014). Overall, the three methods identified 170,825 unique AG sites in <i>Octopus bimaculoides</i> coding sequences (38,066 hyper-editing sites do not overlap those found by the other methods). See <b>Methods</b> for analysis of the differences between the results of the first two methods. <b>(B)</b> RNA editing levels, measured across the whole transcriptome (see Table S5) by the editing index (weighted average of editing levels over all editing sites identified in the transcriptome, see <b>Methods</b>). Levels vary across tissues and are highest for neural tissues (see Table S6). Unlike mammals, a sizable fraction of editing events (11–13% in neural tissues) results in recoding events. Annotation of transcripts and repeats is based on (Albertin et al., 2015). (CNS= central nervous system; ANC=Axial nerve cord; OL=Optic Lobe; Sub=Subesophageal ganglia; Supra=Supraesophageal ganglia; PSG=posterior salivary gland; ST15=stage 15 embryo) <b>(C)</b> The number of editing sites in coding region is comparable to the number found in introns. <b>(D</b>) Unlike the case in mammals, editing is not exceptionally enriched in specific repeat families in <i>Octopus bimaculoides</i>, as measured by the editing index (here defined as the editing level averaged over all, edited and unedited, adenosines in each specific repeat family). <b>(E)</b> Protocadherins is a gene family known to be principally expressed in the brain, important for mediating combinatorial complexity in neuronal connections and are thought to play a role in diversifying neural circuitry (Chen and Maniatis, 2013). It was impressively expanded in <i>Octopus bimaculoides</i> (Albertin et al., 2015). A large number of protocadherins are found in the assembled transcriptomes for the four coleoid species (127–251 open reading frames), but not in nautilus (28 open reading frames). <b>(F–G)</b> Protocadherins contain significantly higher numbers of AG sites <b>(F)</b> and are edited at higher levels (editing level summed over all sites and normalized by ORF length), in all four coleoid species but not in nautilus <b>(G)</b>.</p>
  



                </figcaption>
              

            </figure>
          
            <figure itemscope="" itemtype="http://schema.org/ImageObject" itemprop="associatedMedia" data-slide-index="3" data-label-slug="figure-4">
              <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/instance/5499236/bin/nihms863911f4.jpg" itemprop="contentUrl" aria-describedby="figure-caption-3" role="button" data-image-width="800" data-image-height="815" data-image-alt="Figure 4" data-pmc-id="PMC5499236" data-figure-id="F4">
                <img itemprop="thumbnail" id="article-image-3" src="https://www.ncbi.nlm.nih.gov/pmc/articles/instance/5499236/bin/nihms863911f4.gif" alt="Figure 4">
              </a>

              <meta itemprop="width" itemtype="http://schema.org/ImageObject" content="800">
              <meta itemprop="height" itemtype="http://schema.org/ImageObject" content="815">

              
                

                

                <figcaption id="figure-caption-3" itemtype="http://schema.org/ImageObject" itemprop="description">
                  
                  



  
    <strong>
      Figure 4. Extensive recoding is conserved across coleoid cephalopods
    </strong>
  

  
    <p><b>(A)</b> Tens-thousands sites are conserved across species (see Table S7). The closer the species are evolutionarily, the higher the number of conserved sites. <b>(B)</b> Virtually all (97.5–99%) mismatches conserved across species are A-to-G, resulting from A-to-I editing. Manual inspection of the few non-A-to-G mismatches appearing in multiple species suggests that they either result from systematic erroneous alignments, or they are actually editing sites that were mistakenly identified as G-to-A mismatches due to insufficient DNA coverage. <b>(C)</b> The majority of editing sites is conserved between the two octopus species, and even the most distant species share a sizable fraction of their sites. <b>(D)</b> In contrast, only 36 human recoding sites (1–2% of human recoding sites) are shared by mouse, and a similar number is shared between <i>Drosophila melanogaster</i> and <i>D. mojavensis</i> (Yu et al., 2016) (diverged at later times than squid-sepia). <b>(E)</b> Interestingly, 1146 AG modification sites (in 443 proteins) are conserved and shared by all four coleoid cephalopod species. Of these, 887 are recoding sites and 705 are highly edited (&gt;=10% editing) recoding sites (in 393 proteins). <b>(F)</b> Some proteins include multiple highly-edited recoding sites (see Table S8). Of note are Uromodulin, α Spectrin (previously reported to harbor the highest number of recoding sites in squid (Alon et al., 2015)), and Calcium-dependent secretion activator 1 (CAPS1) with 14, 8 and 7 strong shared recoding sites, respectively. Recoding in CAPS1 was found to be conserved in vertebrate species from human to zebrafish (Li et al., 2009). <b>(G)</b> Not only are the locations of editing sites conserved, but their editing levels are correlated as well. Editing levels in 887 recoding sites shared by all species are highly, positively and significantly correlated in all pairs of coleoid cephalopod species (p&lt;1e–75 for all pairs; see Supp. Fig. 1 for three additional pairs). Correlation is higher the closer the species are to each other in evolutionary terms, with Pearson rho = 0.95 for the two octopus species.</p>
  



                </figcaption>
              

            </figure>
          
            <figure itemscope="" itemtype="http://schema.org/ImageObject" itemprop="associatedMedia" data-slide-index="4" data-label-slug="figure-5">
              <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/instance/5499236/bin/nihms863911f5.jpg" itemprop="contentUrl" aria-describedby="figure-caption-4" role="button" data-image-width="800" data-image-height="779" data-image-alt="Figure 5" data-pmc-id="PMC5499236" data-figure-id="F5">
                <img itemprop="thumbnail" id="article-image-4" src="https://www.ncbi.nlm.nih.gov/pmc/articles/instance/5499236/bin/nihms863911f5.gif" alt="Figure 5">
              </a>

              <meta itemprop="width" itemtype="http://schema.org/ImageObject" content="800">
              <meta itemprop="height" itemtype="http://schema.org/ImageObject" content="779">

              
                

                

                <figcaption id="figure-caption-4" itemtype="http://schema.org/ImageObject" itemprop="description">
                  
                  



  
    <strong>
      Figure 5. Signs for positive selection of recoding by editing
    </strong>
  

  
    <p><b>(A)</b> The fraction of recoding sites among all editing sites in coding region increases with editing levels (top), as well as the fraction of recoding sites among all conserved sites (bottom). Red horizontal dashed line represents the recoding fraction expected assuming neutrality. <b>(B)</b> Editing levels are higher in conserved recoding sites. Distributions of editing levels in four groups of putative A-to-I editing sites: recoding and conserved (Rec+, Cons+), recoding and non-conserved (Rec+, Cons−), conserved sites that cause a synonymous change (Rec−, Cons+), and non-conserved synonymous sites (Rec−, Cons−). Horizontal red lines mark the median level, and yellow diamonds mark the mean. Conservation and non-synonymity are both positively correlated with higher editing levels, as well as their interaction (ANOVA, p-value&lt;1.0e–162). Data presented here for squid (conserved sites are conserved in sepia), but the results are similar and significant for all species. <b>(C)</b> In contrast with the case in humans, highly edited sites tend to be more conserved: the fraction of conserved sites rises with the editing level for all species pairs, but more dramatically for the closely related octopuses and the sepia-squid pair. <b>(D)</b> Highly conserved regions of the transcriptome are enriched in editing sites, further attesting for positive selection of RNA editing. Density of editing sites (number of AG sites normalized by length) is higher for 112 recoding regions that are highly-conserved across the four species (&gt;95% identity; average length 1382bp), compared with all other, less conserved, regions (Wilcoxon p-value&lt;0.001 for all species). Error bars represent the S.E.M.</p>
  



                </figcaption>
              

            </figure>
          
            <figure itemscope="" itemtype="http://schema.org/ImageObject" itemprop="associatedMedia" data-slide-index="5" data-label-slug="figure-6">
              <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/instance/5499236/bin/nihms863911f6.jpg" itemprop="contentUrl" aria-describedby="figure-caption-5" role="button" data-image-width="800" data-image-height="979" data-image-alt="Figure 6" data-pmc-id="PMC5499236" data-figure-id="F6">
                <img itemprop="thumbnail" id="article-image-5" src="https://www.ncbi.nlm.nih.gov/pmc/articles/instance/5499236/bin/nihms863911f6.gif" alt="Figure 6">
              </a>

              <meta itemprop="width" itemtype="http://schema.org/ImageObject" content="800">
              <meta itemprop="height" itemtype="http://schema.org/ImageObject" content="979">

              
                

                

                <figcaption id="figure-caption-5" itemtype="http://schema.org/ImageObject" itemprop="description">
                  
                  



  
    <strong>
      Figure 6. Conserved and species-specific editing sites affect protein function
    </strong>
  

  
    <p>Unedited (wt) and singly-edited versions of the voltage-dependent K<sup>+</sup> channels of the K<sub>v</sub>2 subfamily were studied under voltage-clamp (see Table S9). <b>(A)</b> (i) Current traces resulting from a voltage step from −80 mV to 40 mV for the wt Sepia K<sub>v</sub>2.1 and the same construct containing the sepia-specific I529V edit, lying within the 4<sup>th</sup> transmembrane domain (green), showing that I529V accelerates the rate of slow inactivation. (ii) Time constants for slow inactivation determined by fitting single exponentials to traces similar to those in panel (i) at different activating voltages (Vm). <b>(B)</b> (i) Tail currents measured at a voltage (Vm) of −80mV, following an activating pulse of +20 mV for 25 ms. Traces are shown for the wt K<sub>v</sub>2.1 channels from squid, sepia and Octopus vulgaris. (ii) Tail currents for the same channels edited at the shared I-to-V site in the 6<sup>th</sup> transmembrane span, following the same voltage protocol. (iii) Time constants from single exponential fits to tail currents obtained at various negative voltages (Vm) (following an activating pulse to 20 mV for 25 ms) show that the unedited channels close at distinct rates, (iv) but the edited versions close at similar rates. N = 5 ± s.e.m. for all data plotted in this figure.</p>
  



                </figcaption>
              

            </figure>
          
            <figure itemscope="" itemtype="http://schema.org/ImageObject" itemprop="associatedMedia" data-slide-index="6" data-label-slug="figure-7">
              <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/instance/5499236/bin/nihms863911f7.jpg" itemprop="contentUrl" aria-describedby="figure-caption-6" role="button" data-image-width="800" data-image-height="735" data-image-alt="Figure 7" data-pmc-id="PMC5499236" data-figure-id="F7">
                <img itemprop="thumbnail" id="article-image-6" src="https://www.ncbi.nlm.nih.gov/pmc/articles/instance/5499236/bin/nihms863911f7.gif" alt="Figure 7">
              </a>

              <meta itemprop="width" itemtype="http://schema.org/ImageObject" content="800">
              <meta itemprop="height" itemtype="http://schema.org/ImageObject" content="735">

              
                

                

                <figcaption id="figure-caption-6" itemtype="http://schema.org/ImageObject" itemprop="description">
                  
                  



  
    <strong>
      Figure 7. RNA editing slows down cephalopod genome evolution
    </strong>
  

  
    <p><b>(A)</b> Inter-species mutations are purified from genome loci surrounding conserved recoding sites (data shown for sites shared by squid and sepia). Depletion of mutations extends up to ~100bp of shared recoding sites (left). As a control, we show the mutations density (mutations/bp) around random non-edited adenosines from the same transcripts (right). Yellow – synonymous change; light green – non-synonymous; dark green – deletions. <b>(B)</b> Genomic polymorphisms are depleted near editing/recoding/conserved-recoding sites in squid, attesting to reduced genome plasticity. Effect is stronger for recoding sites, and even more so for the conserved recoding sites. <b>(C)</b> GC-content is elevated near editing sites in squid, allowing for more stable double-stranded RNA structures. The effect is even stronger in conserved sites. Dashed line represents the baseline GC level in the entire ORFome, and error bars represent the S.E.M. See Supp. Fig. 3 for analyses similar to those presented in panels A–C in other species.</p>
  



                </figcaption>
              

            </figure>
          
        </div>

        
          
        

        <!-- Root element of PhotoSwipe. Must have class pswp. -->




      </div>
    
  


  
  
    
      <div id="linked-commentary">
        <h2>
          Comment in
        </h2>
        <ul>
          
            
              
              
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/28420879/" ref="article_id=28420879&amp;linksrc=comments_link" data-ga-category="comment_correction" data-ga-action="28420879" data-ga-label="linked-commentary">
      
        Evolutionary genetics: Fantastic beasts - cephalopod RNA recoding.
      
    </a></p><p><span>Starling S.</span>
        
      
    
    <span>Starling S.</span>
    <span>Nat Rev Genet. 2017 Jun;18(6):329. doi: 10.1038/nrg.2017.31. Epub 2017 Apr 19.</span>
    <span>Nat Rev Genet. 2017.</span>
  
  <span>PMID: <span>28420879</span></span>
  
  
  
  
  <span>No abstract available.</span>
</p>

  </div>
  
    </li>
  


            
          
            
              
              
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/28716332/" ref="article_id=28716332&amp;linksrc=comments_link" data-ga-category="comment_correction" data-ga-action="28716332" data-ga-label="linked-commentary">
      
        Shaping and Reshaping Transcriptome Plasticity during Evolution.
      
    </a></p><p><span>Hussain S.</span>
        
      
    
    <span>Hussain S.</span>
    <span>Trends Biochem Sci. 2017 Sep;42(9):682-684. doi: 10.1016/j.tibs.2017.06.009. Epub 2017 Jul 14.</span>
    <span>Trends Biochem Sci. 2017.</span>
  
  <span>PMID: <span>28716332</span></span>
  
  
  
  
  
</p>

  </div>
  
    </li>
  


            
          
        </ul>
      </div>
    
  


  
  
    <div id="similar">
      <h2>
        Similar articles
      </h2>
      
        <ul id="similar-articles-list">
          
  
    
      
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/34022057/" ref="article_id=34022057&amp;linksrc=similar_articles_link&amp;ordinalpos=1" data-ga-category="similar_article" data-ga-action="34022057" data-ga-label="">
      
        Adaptive Proteome Diversification by Nonsynonymous A-to-I RNA Editing in Coleoid Cephalopods.
      
    </a></p><p><span>Shoshan Y, Liscovitch-Brauer N, Rosenthal JJC, Eisenberg E.</span>
        
      
    
    <span>Shoshan Y, et al.</span>
    <span>Mol Biol Evol. 2021 Aug 23;38(9):3775-3788. doi: 10.1093/molbev/msab154.</span>
    <span>Mol Biol Evol. 2021.</span>
  
  <span>PMID: <span>34022057</span></span>
  <span>Free PMC article.</span>
  
  
  
  
</p>

  </div>
  
    </li>
  


    
  

  
    
      
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/36790891/" ref="article_id=36790891&amp;linksrc=similar_articles_link&amp;ordinalpos=2" data-ga-category="similar_article" data-ga-action="36790891" data-ga-label="">
      
        Extensive Recoding of the Neural Proteome in Cephalopods by RNA Editing.
      
    </a></p><p><span>Rosenthal JJC, Eisenberg E.</span>
        
      
    
    <span>Rosenthal JJC, et al.</span>
    <span>Annu Rev Anim Biosci. 2023 Feb 15;11:57-75. doi: 10.1146/annurev-animal-060322-114534.</span>
    <span>Annu Rev Anim Biosci. 2023.</span>
  
  <span>PMID: <span>36790891</span></span>
  
  
  <span>Review.</span>
  
  
</p>

  </div>
  
    </li>
  


    
  

  
    
      
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/37295402/" ref="article_id=37295402&amp;linksrc=similar_articles_link&amp;ordinalpos=3" data-ga-category="similar_article" data-ga-action="37295402" data-ga-label="">
      
        Temperature-dependent RNA editing in octopus extensively recodes the neural proteome.
      
    </a></p><p><span>Birk MA, Liscovitch-Brauer N, Dominguez MJ, McNeme S, Yue Y, Hoff JD, Twersky I, Verhey KJ, Sutton RB, Eisenberg E, Rosenthal JJC.</span>
        
      
    
    <span>Birk MA, et al.</span>
    <span>Cell. 2023 Jun 8;186(12):2544-2555.e13. doi: 10.1016/j.cell.2023.05.004. Epub 2023 Jun 8.</span>
    <span>Cell. 2023.</span>
  
  <span>PMID: <span>37295402</span></span>
  
  
  
  
  
</p>

  </div>
  
    </li>
  


    
  

  
    
      
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/33265998/" ref="article_id=33265998&amp;linksrc=similar_articles_link&amp;ordinalpos=4" data-ga-category="similar_article" data-ga-action="33265998" data-ga-label="">
      
        The ADAR Family in Amphioxus: RNA Editing and Conserved Orthologous Site Predictions.
      
    </a></p><p><span>Zawisza-Álvarez M, Pérez-Calles C, Gattoni G, Garcia-Fernàndez J, Benito-Gutiérrez È, Herrera-Úbeda C.</span>
        
      
    
    <span>Zawisza-Álvarez M, et al.</span>
    <span>Genes (Basel). 2020 Nov 30;11(12):1440. doi: 10.3390/genes11121440.</span>
    <span>Genes (Basel). 2020.</span>
  
  <span>PMID: <span>33265998</span></span>
  <span>Free PMC article.</span>
  
  
  
  
</p>

  </div>
  
    </li>
  


    
  

  
    
      
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/32729084/" ref="article_id=32729084&amp;linksrc=similar_articles_link&amp;ordinalpos=5" data-ga-category="similar_article" data-ga-action="32729084" data-ga-label="">
      
        Proteome Diversification by RNA Editing.
      
    </a></p><p><span>Eisenberg E.</span>
        
      
    
    <span>Eisenberg E.</span>
    <span>Methods Mol Biol. 2021;2181:229-251. doi: 10.1007/978-1-0716-0787-9_14.</span>
    <span>Methods Mol Biol. 2021.</span>
  
  <span>PMID: <span>32729084</span></span>
  
  
  <span>Review.</span>
  
  
</p>

  </div>
  
    </li>
  


    
  


        </ul>

        
          

        
      
    </div>
  


  


  
    <div id="citedby">
      <h2>
        Cited by
      </h2>
      
        <ul id="citedby-articles-list">
          
  
    
      
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/37293227/" ref="article_id=37293227&amp;linksrc=citedby_articles_link&amp;ordinalpos=1" data-ga-category="cited_by" data-ga-action="37293227" data-ga-label="">
      
        Adaptation of A-to-I RNA editing in bacteria, fungi, and animals.
      
    </a></p><p><span>Duan Y, Li H, Cai W.</span>
        
      
    
    <span>Duan Y, et al.</span>
    <span>Front Microbiol. 2023 May 24;14:1204080. doi: 10.3389/fmicb.2023.1204080. eCollection 2023.</span>
    <span>Front Microbiol. 2023.</span>
  
  <span>PMID: <span>37293227</span></span>
  <span>Free PMC article.</span>
  
  
  
  <span>No abstract available.</span>
</p>

  </div>
  
    </li>
  


    
  

  
    
      
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/37266373/" ref="article_id=37266373&amp;linksrc=citedby_articles_link&amp;ordinalpos=2" data-ga-category="cited_by" data-ga-action="37266373" data-ga-label="">
      
        Transcriptome-wide selection and validation of a solid set of reference genes for gene expression studies in the cephalopod mollusk <em>Octopus vulgaris</em>.
      
    </a></p><p><span>Imperadore P, Cagnin S, Allegretti V, Millino C, Raffini F, Fiorito G, Ponte G.</span>
        
      
    
    <span>Imperadore P, et al.</span>
    <span>Front Mol Neurosci. 2023 May 17;16:1091305. doi: 10.3389/fnmol.2023.1091305. eCollection 2023.</span>
    <span>Front Mol Neurosci. 2023.</span>
  
  <span>PMID: <span>37266373</span></span>
  <span>Free PMC article.</span>
  
  
  
  
</p>

  </div>
  
    </li>
  


    
  

  
    
      
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/37199468/" ref="article_id=37199468&amp;linksrc=citedby_articles_link&amp;ordinalpos=3" data-ga-category="cited_by" data-ga-action="37199468" data-ga-label="">
      
        The continuing discovery on the evidence for RNA editing in SARS-CoV-2.
      
    </a></p><p><span>Pu X, Xu Q, Wang J, Liu B.</span>
        
      
    
    <span>Pu X, et al.</span>
    <span>RNA Biol. 2023 Jan;20(1):219-222. doi: 10.1080/15476286.2023.2214437.</span>
    <span>RNA Biol. 2023.</span>
  
  <span>PMID: <span>37199468</span></span>
  <span>Free PMC article.</span>
  
  
  
  
</p>

  </div>
  
    </li>
  


    
  

  
    
      
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/36877730/" ref="article_id=36877730&amp;linksrc=citedby_articles_link&amp;ordinalpos=4" data-ga-category="cited_by" data-ga-action="36877730" data-ga-label="">
      
        Identification of exceptionally potent adenosine deaminases RNA editors from high body temperature organisms.
      
    </a></p><p><span>Avram-Shperling A, Kopel E, Twersky I, Gabay O, Ben-David A, Karako-Lampert S, Rosenthal JJC, Levanon EY, Eisenberg E, Ben-Aroya S.</span>
        
      
    
    <span>Avram-Shperling A, et al.</span>
    <span>PLoS Genet. 2023 Mar 6;19(3):e1010661. doi: 10.1371/journal.pgen.1010661. eCollection 2023 Mar.</span>
    <span>PLoS Genet. 2023.</span>
  
  <span>PMID: <span>36877730</span></span>
  <span>Free PMC article.</span>
  
  
  
  
</p>

  </div>
  
    </li>
  


    
  

  
    
      
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/36799984/" ref="article_id=36799984&amp;linksrc=citedby_articles_link&amp;ordinalpos=5" data-ga-category="cited_by" data-ga-action="36799984" data-ga-label="">
      
        Evidence Supporting That C-to-U RNA Editing Is the Major Force That Drives SARS-CoV-2 Evolution.
      
    </a></p><p><span>Wang J, Wu L, Pu X, Liu B, Cao M.</span>
        
      
    
    <span>Wang J, et al.</span>
    <span>J Mol Evol. 2023 Apr;91(2):214-224. doi: 10.1007/s00239-023-10097-1. Epub 2023 Feb 17.</span>
    <span>J Mol Evol. 2023.</span>
  
  <span>PMID: <span>36799984</span></span>
  <span>Free PMC article.</span>
  
  
  
  
</p>

  </div>
  
    </li>
  


    
  


        </ul>

        
          

        
      
    </div>
  


  
  
  
    <div id="publication-types">
      <h2>
        Publication types
      </h2>

      <ul><li></li><li></li><li></li></ul>
    </div>
  


  
  
    <div id="mesh-terms">
      <h2>
        MeSH terms
      </h2>

      <ul><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li></ul>
    </div>
  


  
  
    <div id="substances">
      <h2>
        Substances
      </h2>

      <ul><li></li><li></li></ul>
    </div>
  


  

  

  



  
  

  


  
    <div id="grants">
      <h2>
        Grant support
      </h2>
      <div><ul id="grants-head-list"><li><a title="All articles for grant 311257/ERC_/European Research Council/International" href="https://pubmed.ncbi.nlm.nih.gov/?term=311257%2FERC_%2FEuropean+Research+Council%2FInternational%5BGrant+Number%5D&amp;sort=date&amp;sort_order=desc" ref="linksrc=grant_link" data-ga-category="search" data-ga-action="grant" data-ga-label="311257">
          311257/ERC_/European Research Council/International
        </a></li><li><a title="All articles for grant R01 NS064259/NS/NINDS NIH HHS/United States" href="https://pubmed.ncbi.nlm.nih.gov/?term=R01+NS064259%2FNS%2FNINDS+NIH+HHS%2FUnited+States%5BGrant+Number%5D&amp;sort=date&amp;sort_order=desc" ref="linksrc=grant_link" data-ga-category="search" data-ga-action="grant" data-ga-label="R01 NS064259">
          R01 NS064259/NS/NINDS NIH HHS/United States
        </a></li><li><a title="All articles for grant R01 NS087726/NS/NINDS NIH HHS/United States" href="https://pubmed.ncbi.nlm.nih.gov/?term=R01+NS087726%2FNS%2FNINDS+NIH+HHS%2FUnited+States%5BGrant+Number%5D&amp;sort=date&amp;sort_order=desc" ref="linksrc=grant_link" data-ga-category="search" data-ga-action="grant" data-ga-label="R01 NS087726">
          R01 NS087726/NS/NINDS NIH HHS/United States
        </a></li></ul></div>
    </div>
  


  
  <div id="linkout">
    <h2>
      LinkOut - more resources
    </h2>

    <ul><li><h3>Full Text Sources</h3><ul><li><a href="https://linkinghub.elsevier.com/retrieve/pii/S0092-8674(17)30344-6" target="_blank" rel="noopener" ref="linksrc=linkout_link&amp;PrId=3048&amp;uid=28388405&amp;db=pubmed&amp;itool=Abstract&amp;log$=linkoutlink&amp;nlmid=0413066" data-ga-category="link_out" data-ga-action="Full Text Sources" data-ga-label="Elsevier Science">
                    Elsevier Science
                  </a></li><li><a href="https://europepmc.org/abstract/MED/28388405" target="_blank" rel="noopener" ref="linksrc=linkout_link&amp;PrId=6082&amp;uid=28388405&amp;db=pubmed&amp;itool=Abstract&amp;log$=linkoutlink&amp;nlmid=0413066" data-ga-category="link_out" data-ga-action="Full Text Sources" data-ga-label="Europe PubMed Central">
                    Europe PubMed Central
                  </a></li><li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/28388405/" target="_blank" rel="noopener" ref="linksrc=linkout_link&amp;PrId=3494&amp;uid=28388405&amp;db=pubmed&amp;itool=Abstract&amp;log$=linkoutlink&amp;nlmid=0413066" data-ga-category="link_out" data-ga-action="Full Text Sources" data-ga-label="PubMed Central">
                    PubMed Central
                  </a></li></ul></li><li><h3>Other Literature Sources</h3><ul><li><a href="https://facultyopinions.com/pubmed/28388405" target="_blank" rel="noopener" ref="linksrc=linkout_link&amp;PrId=3533&amp;uid=28388405&amp;db=pubmed&amp;itool=Abstract&amp;log$=linkoutlink&amp;nlmid=0413066" data-ga-category="link_out" data-ga-action="Other Literature Sources" data-ga-label="H1 Connect">
                    H1 Connect
                  </a></li><li><a href="https://www.lens.org/lens/search/patent/list?q=citation_id:28388405" target="_blank" rel="noopener" ref="linksrc=linkout_link&amp;PrId=8681&amp;uid=28388405&amp;db=pubmed&amp;itool=Abstract&amp;log$=linkoutlink&amp;nlmid=0413066" data-ga-category="link_out" data-ga-action="Other Literature Sources" data-ga-label="The Lens - Patent Citations">
                    The Lens - Patent Citations
                  </a></li><li><a href="https://scite.ai/reports/28388405" target="_blank" rel="noopener" ref="linksrc=linkout_link&amp;PrId=10307&amp;uid=28388405&amp;db=pubmed&amp;itool=Abstract&amp;log$=linkoutlink&amp;nlmid=0413066" data-ga-category="link_out" data-ga-action="Other Literature Sources" data-ga-label="scite Smart Citations">
                    scite Smart Citations
                  </a></li></ul></li></ul>
  </div>


</main>

    
  


    

    

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google’s nightmare “Web Integrity API” wants a DRM gatekeeper for the web (847 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2023/07/googles-web-integrity-api-sounds-like-drm-for-the-web/</link>
            <guid>36854114</guid>
            <pubDate>Mon, 24 Jul 2023 20:59:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2023/07/googles-web-integrity-api-sounds-like-drm-for-the-web/">https://arstechnica.com/gadgets/2023/07/googles-web-integrity-api-sounds-like-drm-for-the-web/</a>, See on <a href="https://news.ycombinator.com/item?id=36854114">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2020/07/android-spying-800x450.jpg" alt="A man laughs at his smartphone while a cartoon characters peaks over his shoulder.">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2020/07/android-spying.jpg" data-height="563" data-width="1000">Enlarge</a> <span>/</span> The little Android robot is watching everything you do.</p></figcaption>  </figure>

  




<!-- cache hit 800:single/related:615bdf60c76de8a9103dade2c9b14285 --><!-- empty -->
<p>Google's newest proposed web standard is... DRM? Over the weekend the Internet got wind of <a href="https://github.com/RupertBenWiser/Web-Environment-Integrity/tree/main">this proposal</a> for a "Web Environment Integrity API. " The <a href="https://github.com/RupertBenWiser/Web-Environment-Integrity/blob/main/explainer.md">explainer</a> is authored by four Googlers, including at least one person on Chrome's "<a href="https://arstechnica.com/gadgets/2023/02/googles-privacy-sandbox-advertising-system-arrives-on-android-in-beta/">Privacy Sandbox</a>" team, which is responding to the <a href="https://arstechnica.com/gadgets/2022/07/google-delays-death-of-tracking-cookies-again-wants-more-time-for-testing/">death of tracking cookies</a> by building a user-tracking ad platform right into the browser.</p>
<p>The intro to the Web Integrity API starts out: "Users often depend on websites trusting the client environment they run in. This trust may assume that the client environment is honest about certain aspects of itself, keeps user data and intellectual property secure, and is transparent about whether or not a human is using it."</p>
<p>The goal of the project is to learn more about the person on the other side of the web browser, ensuring they aren't a robot and that the browser hasn't been modified or tampered with in any unapproved ways. The intro says this data would be useful to advertisers to better count ad impressions, stop social network bots, enforce intellectual property rights, stop cheating in web games, and help financial transactions be more secure.</p>                                            
                                                        
<p>Perhaps the most telling line of the explainer is that it "takes inspiration from existing native attestation signals such as [Apple's] <a href="https://developer.apple.com/documentation/devicecheck/validating_apps_that_connect_to_your_server">App Attest</a> and the [Android] <a href="https://developer.android.com/google/play/integrity">Play Integrity API</a>." Play Integrity (formerly called "SafetyNet") is an Android API that lets apps find out if your device has been rooted. Root access allows you full control over the device that you purchased, and a lot of app developers don't like that. So if you root an Android phone and get flagged by the Android Integrity API, several types of apps will just refuse to run. You'll generally be locked out of banking apps, Google Wallet, online games, Snapchat, and some media apps like Netflix. You <em>could</em> be using root access to cheat at games or phish banking data, but you could also just want root to customize your device, remove crapware, or have a viable backup system. Play Integrity doesn't care and will lock you out of those apps either way. Google wants the same thing for the web.</p>
<p>Google's plan is that, during a webpage transaction, the web server could require you to pass an "environment attestation" test before you get any data. At this point your browser would contact a "third-party" attestation server, and you would need to pass some kind of test. If you passed, you would get a signed "IntegrityToken" that verifies your environment is unmodified and points to the content you wanted unlocked. You bring this back to the web server, and if the server trusts the attestation company, you get the content unlocked and finally get a response with the data you wanted.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[People in 1920s Berlin nightclubs flirted via pneumatic tubes (263 pts)]]></title>
            <link>https://www.atlasobscura.com/articles/pneumatic-tube-table-phone-flirting-berlin</link>
            <guid>36854061</guid>
            <pubDate>Mon, 24 Jul 2023 20:54:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.atlasobscura.com/articles/pneumatic-tube-table-phone-flirting-berlin">https://www.atlasobscura.com/articles/pneumatic-tube-table-phone-flirting-berlin</a>, See on <a href="https://news.ycombinator.com/item?id=36854061">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="article-body">
<p><span>You hear it often: dating </span>today doesn’t work like it used to. Or: apps like Tinder have made flirting more distant.</p>
<p>But the process of staring, judging, and messaging potential suitors from afar—hallmarks of modern dating apps—is not new. Beginning in the 1920s, nightclub-goers in Berlin who feared face-to-face encounters could communicate with beautiful strangers from across the room.</p>
<p>All they needed to do? Turn to the nearest pneumatic tube.</p>
<p>Two nightclubs in particular—the Resi and the Femina—pioneered the trend. At the Resi (also called the Residenz-Casino), a large nightclub with a live band and a dance floor that held 1,000 people, an elaborate system of table phones and pneumatic tubes allowed for anonymous, late-night flirtation between complete strangers.</p>
<figure><img src="https://img.atlasobscura.com/LpS7oAs99gAjNXH7kMcvkohGLmNUem_T-asmE7IkKzU/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy80ZjE4ZDQzMGE4/NzAwZmI0M2RfUmVz/aV8xLnBuZw.png" alt="Resi nightclub tables awaiting enigmatic and flirtatious strangers." width="auto" data-kind="article-image" id="article-image-42702" data-src="https://img.atlasobscura.com/LpS7oAs99gAjNXH7kMcvkohGLmNUem_T-asmE7IkKzU/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy80ZjE4ZDQzMGE4/NzAwZmI0M2RfUmVz/aV8xLnBuZw.png"><figcaption>Resi nightclub tables awaiting enigmatic and flirtatious strangers. <a target="_blank" href="http://liquidfireworks.com/about-us/">Courtesy of Michael Przystawik</a></figcaption></figure>
<p>A <em><a href="http://archives.chicagotribune.com/1968/10/30/page/48/article/the-tribune-travelers-guide">Chicago Tribune</a></em> article describes the Resi’s “nightly ‘spectacular’—‘a dancing water ballet’ with jets of water rising and falling to a recorded symphony while colored lights flash.” The water-jet ballet, now known as a “<a href="http://liquidfireworks.com/video-gallery/">Waltzing Water</a>,” began in 1928 and drew in many visitors.</p>
<p>But the <em>Tribune</em> article refers to the system of phones and pneumatic tubes at each table as the Resi’s “big lure.”</p>
<p>Phones were fixed to individual tables, and above many was a lighted number. Singles needed only to look around the room until a fetching stranger caught their eye, note the number, and then direct a message to that table. “Lonesome Americans, and others, can call or send a note to equally lonesome women who look like they would enjoy company,” the article noted.</p>
<p>In 1931, during the heyday of this across-the-nightclub flirtation, <em><a href="https://books.google.com/books?id=nlpBDAAAQBAJ&amp;pg=PA164&amp;lpg=PA164&amp;dq=The+Residenz-Casino&amp;source=bl&amp;ots=v3JYahpG1R&amp;sig=BeO7Wv1zKw2FmCrhC0hUYqbZ5ZA&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiUor2wh5vUAhUo9YMKHVpMA38Q6AEIWTAJ#v=onepage&amp;q=The%20Residenz-Casino&amp;f=false">The Berliner Herold</a></em> described the process of receiving a call from an amorous stranger: “the tabletop telephones buzzed, and the acquaintance with the blonde, raven-haired or redheaded, monocle-wearing beauty was made, one was no longer alone, and had twice as much fun.” (At the Ballhaus Berlin, this numbered phone system still lives today—check out photos <a href="http://www.ballhaus-berlin.de/first-gallery/">here</a>.)</p>
<figure><img src="https://img.atlasobscura.com/vsmYxzi0VnQuRhaU1hq-j6i0wls1sSIsuRyRLenFV3k/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy9lMTFhZTA1ODkz/N2M0NDZkODVfUmVz/aS0yLmpwZWc.jpg" alt="The Resi floor plan." width="auto" data-kind="article-image" id="article-image-42689" data-src="https://img.atlasobscura.com/vsmYxzi0VnQuRhaU1hq-j6i0wls1sSIsuRyRLenFV3k/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy9lMTFhZTA1ODkz/N2M0NDZkODVfUmVz/aS0yLmpwZWc.jpg"><figcaption>The Resi floor plan. <a target="_blank" href="http://www.cabaret-berlin.com/?p=491">Courtesy of Brendan Nash </a></figcaption></figure>
<p>Similar systems thrived at the Femina, the larger of the two nightclubs, which <a href="https://books.google.com/books?id=nlpBDAAAQBAJ&amp;q=two+large+bars+and+a+smaller+one+in+the+vestibule#v=snippet&amp;q=two%20large%20bars%20and%20a%20smaller%20one%20in%20the%20vestibule&amp;f=false">boasted</a> more than 2,000 seats, “two large bars and a smaller one in the vestibule, in addition to three orchestras, a hydraulic dance floor,” and over 225 table telephones, which were accompanied by instructions in both German and English.</p>
<p>But for those who were too shy to pick up the phone, the pneumatic tubes offered a perfect alternative. The tubes were built into the handrails, and one was located at each table. The nightclub provided paper on which to scrawl notes. Patrons only had to specify where they wanted their missives sent. Like messaging on a dating app, but with—you know—tubes.</p>
<p>At the Resi, many provocative notes were passed around, but eager flirters needed to be careful—“<a href="http://archives.chicagotribune.com/1968/10/30/page/48/article/the-tribune-travelers-guide">messages sent by tube [were] checked by female ‘censors’ in the switchboard room</a>” in an early form of comment moderation.</p>
<figure><img src="https://img.atlasobscura.com/ryYDtpazWBELSiSJ2X06o4FRiUPMURueJ5JStWcJhAY/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy80ZjE4ZDQzMGE4/NzAwZmI0M2RfUmVz/aV8yLnBuZw.png" alt="The lavish Resi nightclub." width="auto" data-kind="article-image" id="article-image-42703" data-src="https://img.atlasobscura.com/ryYDtpazWBELSiSJ2X06o4FRiUPMURueJ5JStWcJhAY/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy80ZjE4ZDQzMGE4/NzAwZmI0M2RfUmVz/aV8yLnBuZw.png"><figcaption>The lavish Resi nightclub. <a target="_blank" href="http://liquidfireworks.com/about-us/">Courtesy of Michael Przystawik</a></figcaption></figure>
<p>The pneumatic tube system existed for decades, and Americans who visited Berlin after World War II <a href="http://www.cabaret-berlin.com/?p=491#comments">remember it fondly</a>.</p>
<p>Today, many fictionalized accounts memorialize it: <em>Then We Take Berlin</em> by John Lawton <a href="https://books.google.com/books?id=m6vGAgAAQBAJ&amp;pg=PA246&amp;lpg=PA246&amp;dq=pneumatic+tube+nightclub&amp;source=bl&amp;ots=7IP9C488gY&amp;sig=jUynGQMdbt4bihAh2QQ3NC_TmJ4&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjhq6vpwZ3UAhWBaxQKHRvcA-k4ChDoAQhMMAg#v=onepage&amp;q=pneumatic%20tube%20nightclub&amp;f=false">describes</a> how visitors could “write a message, stick it in the snake’s head, yank on the handle and the pneumatic tube would whisk it up to the top gallery and they’d redirect it to the right table.” (<em>Cabaret</em>, meanwhile, tributes the table phone system in “<a href="https://www.youtube.com/watch?v=X5SLGipH-cA">The Telephone Song</a>.”) Ian McEwan’s novel <em>The Innocent</em> also offers an evocative tribute. When his main character, Leonard, visits a fictionalized version of the Resi nightclub, the protagonist finds a pamphlet that boasts the establishment’s “Modern Table-Phone-System” and “Pneumatic-Table-Mail-Service,” which sends “every night thousands of letters or little presents from one visitor to another.”</p>
<p>This “table mail service” was real, and allowed patrons to send more than just a handwritten note to that handsome stranger across the way. The Resi offered a long menu of gifts that visitors could dispatch via pneumatic tube—including <a href="https://books.google.com/books?id=81FjCwAAQBAJ&amp;q=pneumatic#v=snippet&amp;q=pneumatic&amp;f=false">perfume bottles, cigar cutters, travel plans</a>, and, <a href="https://www.roundabouttheatre.org/Roundabout/media/Roundabout/PDF/UPSTAGE/Cabaret_UpstageGuide_R3.pdf">according to one source, cocaine</a>.</p>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Wind and Solar Will Be 25% of Total U.S. Generating Capacity Within Three Years (192 pts)]]></title>
            <link>https://electricenergyonline.com/article/energy/category/climate-change/82/1033617/wind-solar-will-be-25-of-total-u-s-generating-capacity-within-three-years-as-they-grow-by-over-100-gw-.html</link>
            <guid>36853565</guid>
            <pubDate>Mon, 24 Jul 2023 20:11:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://electricenergyonline.com/article/energy/category/climate-change/82/1033617/wind-solar-will-be-25-of-total-u-s-generating-capacity-within-three-years-as-they-grow-by-over-100-gw-.html">https://electricenergyonline.com/article/energy/category/climate-change/82/1033617/wind-solar-will-be-25-of-total-u-s-generating-capacity-within-three-years-as-they-grow-by-over-100-gw-.html</a>, See on <a href="https://news.ycombinator.com/item?id=36853565">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="description_news"><p>Based upon a review by the SUN DAY Campaign of data newly released by the Federal Energy Regulatory Commission (FERC), utility-scale solar and wind appear to be on track to provide a quarter of the nation's installed electrical generating capacity within three years.&nbsp;</p>



<p>In its latest monthly "Energy Infrastructure Update" report (with data through May 31, 2023) FERC notes that wind is now 11.63% of total installed generating capacity while utility-scale solar provides another 6.86%. &nbsp;</p>



<p>However, over the next three years (i.e., by May 2026), FERC anticipates "high probability additions" of solar to provide another 80,087-MW while wind is expected to expand by 19,816-MW. Assuming that materializes, in three years, wind would account for 12.43% of installed capacity while utility-scale solar would provide another 12.41%. And that does not include generating capacity provided by small-scale, distributed (e.g., rooftop) solar. [1]&nbsp;</p>



<p>Factoring in FERC's forecasts for hydropower, geothermal, and biomass, renewable energy sources would expand from today's 28.01% of installed generating capacity to 33.85% - i.e., over a third - by May 2026. &nbsp;&nbsp;</p>



<p>Solar and wind's share of U.S. generating capacity could actually be substantially higher if new capacity exceeds FERC's forecast of "high probability additions." The agency indicates that the amount of solar and wind in the three-year pipeline could be nearly three times higher than the total of the "high probability additions". Solar could add 214,022-MW while wind could grow by 66,065-MW.&nbsp;</p>



<p>Moreover, recent history suggests that solar and wind growth is outpacing FERC's predictions for "high probability additions." A year ago, FERC reported "high-probability additions" for wind and solar within three years of 18,711-MW and 62,835-MW respectively.[2] FERC's latest 3-year forecast for those sources is now 22.5% higher. &nbsp;&nbsp;</p>



<p>Signs of that prospective growth may already be evident. For the first five months of 2023, wind and solar accounted for more than half of the new capacity additions this year - 51.07% - comprised of 4,460-MW of solar and 2.645-MW of wind. New capacity provided by hydropower (254-MW), geothermal (37-MW), and biomass (29-MW) brought renewables' combined share of new capacity up to 53.38%. The balance (other than 2-MW from oil) was provided by natural gas. &nbsp;</p>



<p>Notwithstanding the recent natural gas additions, FERC foresees a net decline of 1,564-MW in natural gas generating capacity over the next three years in addition to a drop of 19,966-MW in coal capacity. Oil and nuclear capacity are also foreseen falling - by 569-MW and 123-MW respectively.&nbsp;</p>



<p>Should just FERC's "high probability" forecasts materialize, by May 2026, installed U.S. fossil fuels' share of total capacity will drop over the next three years: natural gas - 41.67% (from 44.37% in May 2023), coal - 14.05% (from 16.49%), and oil - 2.68% (from 2.89%). Nuclear power will also fall from 8.07% today to 7.60% in May 2026.&nbsp;</p>



<p>"Wind and solar are now poised to each provide an eighth of the nation's installed generating capacity within three years while all renewables combined will account for over a third," noted the SUN DAY Campaign's executive director Ken Bossong. "But in light of renewable energy growth rates of recent years, those numbers may very well prove to be an under-estimate."</p>

<p><b>Sources:&nbsp;</b></p>

<p>FERC's 7-page "Energy Infrastructure Update for May 2023" was released on July 17, 2023, and can be found at:&nbsp;<a href="https://cms.ferc.gov/media/energy-infrastructure-update-may-2023">https://cms.ferc.gov/media/energy-infrastructure-update-may-2023</a>. For the information cited in this update, see the tables entitled "New Generation In-Service (New Build and Expansion)," "Total Available Installed Generating Capacity," and "Generation Capacity Additions and Retirements." &nbsp;</p>



<p><b>Notes:</b></p>

<p>[1] FERC generally only reports data for utility-scale facilities (i.e., those rated 1-MW or greater) and therefore its data do not reflect the capacity of distributed renewables, notably rooftop solar PV which - according to the U.S. Energy Information Administration (EIA) - accounts for approximately 30% of the nation's electrical generation by solar. That would imply that the total of distributed and utility-scale solar capacity combined is significantly more than the solar capacity of 6.86% reported by FERC for the first five months of 2023 and is perhaps closer to 9.0% or 10.0%.&nbsp;</p>



<p>[2] FERC's 6-page "Energy Infrastructure Update for May 2022" was released on July 12, 2022, and can be found at:&nbsp;<a href="https://cms.ferc.gov/media/energy-infrastructure-update-may-2022">https://cms.ferc.gov/media/energy-infrastructure-update-may-2022</a>&nbsp;</p>



<p>The SUN DAY Campaign is a non-profit research and educational organization founded in 1992 to support a rapid transition to 100% reliance on sustainable energy technologies as a cost-effective alternative to nuclear power and fossil fuels and as a solution to climate change. Follow on&nbsp;Twitter: @SunDayCampaign&nbsp;&nbsp;</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Python: Overlooked core functionalities (190 pts)]]></title>
            <link>https://erikvandeven.medium.com/python-uncovering-the-overlooked-core-functionalities-54590420c225</link>
            <guid>36853495</guid>
            <pubDate>Mon, 24 Jul 2023 20:06:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://erikvandeven.medium.com/python-uncovering-the-overlooked-core-functionalities-54590420c225">https://erikvandeven.medium.com/python-uncovering-the-overlooked-core-functionalities-54590420c225</a>, See on <a href="https://news.ycombinator.com/item?id=36853495">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><a rel="noopener follow" href="https://erikvandeven.medium.com/?source=post_page-----54590420c225--------------------------------"><div aria-hidden="false"><p><img alt="Erik van de Ven" src="https://miro.medium.com/v2/resize:fill:88:88/1*fwRvhhQzroW7HLMQ2MJN5w.jpeg" width="44" height="44" loading="lazy"></p></div></a></div><figure><figcaption>Photo by <a href="https://unsplash.com/fr/@usinglight?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Stefan Steinbauer</a> on <a href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="1076">What Kyle Simpson mentions about JavaScript in his books, Luciano Ramalho mentions in his book ‘Fluent Python’ about Python. They basically address the same issue of both languages. To put it in my own words:</p><blockquote><p id="dccc">Because the language is so easy to learn, many practitioners only scratch the surface of its full potential, neglecting to delve into the more advanced and powerful aspects of the language which makes it so truly unique and powerful</p></blockquote><p id="abb4">So let’s discuss briefly all functionalities you might haven’t heard of, but you definitely want to know if you aim to become a truly seasoned Pythonista.</p><h2 id="e247">Evaluation of Default Arguments</h2><p id="2954">Python arguments are evaluated when the function definition is encountered. Good to remember that! This means that each time the <strong>fib_memo </strong>function (which is mentioned below) is called without explicitly providing a value for the memo argument, it will use the same dictionary object that was created when the function was defined.</p><pre><span id="d90b">def fib_memo(n, memo={0:0, 1:1}):<br>  """ <br>  n is the number nth number <br>  you would like to return in the sequence <br>  """<br>  if not n in memo:<br>    memo[n] = fib_memo(n-1) + fib_memo(n-2)<br>  return memo[n]<p># 6th Fibonacci (including 0 as first number)<br>fib_memo(6) # should return 8</p></span></pre><p id="6d25">So this code works, in Python. This also means that you could execute the <strong>fib_memo </strong>function in a single script multiple times, like in a for loop, with each execution increasing the fibonacci number to be computed, without hitting the “maximum recursion depth exceeded” limit, as the memo will keep expanding. <a href="https://medium.com/@erikvandeven/memoization-done-right-in-python-96d3527a43f6" rel="noopener">More information can be found in my other article</a>.</p><h2 id="34c6">Walrus Operator</h2><p id="08b5">The walrus operator (<code>:=</code> ), introduced in Python 3.8, allows you to assign a value to a variable within an expression. This way, you can assign the value to a variable and check its value in one expression:</p><pre><span id="ce90">import random<br>some_value = random.randint(0,100) # return a number between 0 and, including, 100<p>if((below_ten := some_value) &lt; 10):<br>    print(f"{below_ten} is smaller than 10")</p></span></pre><p id="d177">Obviously, it’s also easy to assign and check whether the returned value contains a truthy value or not:</p><pre><span id="fbac">if(result := some_method()): # If result is not Falsy<br>    print(result)</span></pre><h2 id="3b91">*args and **kwargs</h2><p id="2eeb">With the asterisk (<code>*</code> ) you can unpack the arguments or keyword arguments (with <code>**</code> ) before passing them to the function. For example, let’s consider the following code:</p><pre><span id="9ecb">my_numbers = [1,2]<p>def sum_numbers(first_number, second_number):<br>    return first_number + second_number</p><p># This will return a TypeError.<br># TypeError: sum() missing 1 required positional argument: 'second_number'<br>sum_numbers(my_numbers)</p><p># This will return the expected result, 3<br>sum_numbers(*my_numbers)</p></span></pre><p id="c430">When we call the <code>sum_numbers</code> function without unpacking <code>my_numbers</code>, it raises a <em>TypeError </em>because the function expects two separate arguments. However, by using the asterisk (*), we can unpack the values from <code>my_numbers</code> and pass them as individual arguments, resulting in the correct output.</p><p id="4790">This unpacking technique works not only with tuples and lists, but also with dictionaries (though it will pass the keys as arguments). But what about keyword arguments? For that, we can utilize the double asterisk (**). Take the following code as an example:</p><pre><span id="70c5">def greet_person(last_name, first_name):<br>  print(f"Hello {first_name} {last_name}")<p>data = {"first_name": "John", "last_name": "Doe"}<br>greet_person(**data)</p></span></pre><p id="275b">Besides unpacking a sequence to pass them as arguments to a function, you could also use it to create a new sequence, for example:</p><pre><span id="ec87">numbers = [1, 2, 3, 4, 5]<br>new_list_numbers = [*numbers]</span></pre><p id="8f8f">The original numbers list remains unaffected, and you have a <code>new_list_numbers</code> variable which contains a copy of the same list. Be careful with links containing objects, though:</p><pre><span id="16bb">numbers = [[1, 2], [3, 4], [5, 6]]<br>packed_numbers = [*numbers]<p>numbers[0].append(10)  # Modify the nested list within the original list</p><p>print(numbers)          # Output: [[1, 2, 10], [3, 4], [5, 6]]<br>print(packed_numbers)   # Output: [[1, 2, 10], [3, 4], [5, 6]]</p></span></pre><h2 id="ffdb">any and all</h2><p id="bae3"><code>any</code> and <code>all</code> are built-in functions that operate on iterable objects (such as lists, tuples, or sets) and return a Boolean value based on the elements in the iterable. An example:</p><pre><span id="26e3">some_booleans = [True, False, False, False]<p>any(some_booleans) # returns True<br>all(some_booleans) # returns</p></span></pre><p id="cec9">You could use the <code>all</code> and <code>any</code> functions in combination with list comprehensions, which return an iterable and pass it as argument to the <code>all</code> functions:</p><pre><span id="e6d1">numbers = [5, 10, 3, 8, -2]<br>all_positive = all(num &gt; 0 for num in numbers)</span></pre><p id="499e">… or any functions:</p><pre><span id="c007">fruits = ['apple', 'banana', 'cherry', 'durian']<p># Check if all fruits start with 'a'<br>result = all(fruit.startswith('a') for fruit in fruits)<br>print(result)  # Output: False</p></span></pre><p id="5547">A table which shows the differences of outputs depending on the values in the iterable, is shown below.</p><figure></figure><h2 id="648b">Swapping Variables</h2><p id="95bd">You can combine tuple packing (what is happening on the right of the equal (=) sign) and unpacking (what is happening on the left of the equal(=) sign) and leverage this functionality for swapping variables:</p><pre><span id="9ac7">a = 10<br>b = 5<p># Swap the values of b and a by packing and unpacking<br>a, b = b, a</p><p>print(a) # 5<br>print(b) # 10</p></span></pre><h2 id="0cd5">str vs repr</h2><p id="12d6">We are used to converting some variable or value to a string, using<code>str(some_value)</code>, so we can print it for debugging purposes. I would like to make you aware of <code>repr(some_value)</code>. The main difference is that <code>repr</code> tries to return a printable representation of the object, while <code>str</code> just tries to return a string representation. A better example is shown below:</p><pre><span id="e33c">import datetime<p>today = datetime.datetime.now()</p><p>print(str(today))   # Output: 2023-07-20 15:30:00.123456<br>print(repr(today))  # Output: datetime.datetime(2023, 7, 20, 15, 30, 0, 123456)</p></span></pre><p id="2e4c">As you can see, <code>str()</code> simply returns the <em>datetime </em>as a string representation. If you want to determine whether the variable <strong>today </strong>contains a string or a <em>datetime </em>object, you wouldn’t be able to discern that information from this alone. On the other hand, ⁣<code>repr()</code> provides information about the actual object that the variable holds. This information is significantly more valuable during debugging.</p><h2 id="4869">Extended Iterable Unpacking</h2><p id="22ad">We can keep this simple: if you would like to get the first and last value of a sequence in a single command:</p><pre><span id="4ede">first, *middle, last = [1, 2, 3, 4, 5]<p>print(first)  # 1<br>print(middle) # [2, 3, 4]<br>print(last)   # 5</p></span></pre><p id="50aa">But this works as well</p><pre><span id="f1df">*the_first_three, second_last, last = [1, 2, 3, 4, 5]<p>print(the_first_three) # [1, 2, 3]<br>print(second_last)     # 4<br>print(last)            # 5</p></span></pre><p id="516a">Or other combinations.</p><h2 id="326c">Multiple Context Managers</h2><p id="d74c">We are used to using one context manager at a time, like opening a file:</p><pre><span id="734d">with open('file.txt', 'r') as file:<br>    # Code that uses the file<br>    # The file will be automatically closed at the end of the block<br>    # even if an exception occurs<p>    # Example: reading lines from the file<br>    for line in file:<br>        print(line.strip())</p><p>with open('file_2.txt', 'r') as other_file:<br>    # Second context manager </p><p>    for line in other_file:<br>        print(line.strip())</p></span></pre><p id="a93d">But we could easily open multiple files in a single statement. Easy if you would like to write lines to the other file for example:</p><pre><span id="e21f">with open('file1.txt') as file1, open('file2.txt') as file2:<br>    # Code that uses both file1 and file2<br>    # The files will be automatically closed at the end of the block<br>    # even if an exception occurs<p>    # Example: reading lines from file1 and writing them to file2<br>    for line in file1:<br>        file2.write(line)</p></span></pre><h2 id="6f7b">The Python Debugger</h2><p id="aedc">We could just print a ton of variables in our file, for debugging purposes, or we could simply use the Python Debugger (pdb), which helps us to set breakpoints which makes it so much easier:</p><pre><span id="2d39">import pdb<p># Set this breakpoint somewhere in your code<br>pdb.set_trace()</p></span></pre><p id="e97d">What makes this so much more valuable is that the program will stop at the breakpoint at which you could print any variable to check its value or existence at that specific breakpoint. Try it! These are several commands you could use when the program hits a breakpoint:</p><ul><li id="6187"><code>n</code> or <code>next</code>: Execute the next line.</li><li id="cfb5"><code>s</code> or <code>step</code>: Step into a function call.</li><li id="0f52"><code>c</code> or <code>continue</code>: Continue execution until the next breakpoint.</li><li id="b58f"><code>l</code> or <code>list</code>: Show the current code context.</li><li id="1c34"><code>p &lt;expression&gt;</code> or <code>pp &lt;expression&gt;</code>: Print the value of an expression.</li><li id="48c2"><code>b &lt;line&gt;</code> or <code>break &lt;line&gt;</code>: Set a new breakpoint at the specified line.</li><li id="c763"><code>h</code> or <code>help</code>: Get help on using <code>pdb</code>.</li><li id="455f"><code>q</code> or <code>quit</code>: Quit the debugger and terminate the program.</li></ul><h2 id="fffc">collections.Counter</h2><p id="3fd8">The <code>Counter</code> class from the <code>collections</code> module provides a convenient way to count elements in an iterable:</p><pre><span id="7d70">from collections import Counter<p>my_list = [1, 2, 3, 1, 2, 1, 3, 4, 5]<br>counts = Counter(my_list)<br>print(counts)  # Output: Counter({1: 3, 2: 2, 3: 2, 4: 1, 5: 1})</p></span></pre><h2 id="ed15">Combinations Using Itertools</h2><p id="f5b8">We could combine different for loops to create permutations, combinations or a cartesian product, or we could simply use the built in itertools.</p><h2 id="0cdd">Permutations</h2><pre><span id="0079">import itertools<p># Generating permutations<br>perms = itertools.permutations([1, 2, 3], 2)<br>print(list(perms))  # Output: [(1, 2), (1, 3), (2, 1), (2, 3), (3, 1), (3, 2)]</p></span></pre><h2 id="0c28">Combinations</h2><pre><span id="6457">import itertools<p># Generating combinations<br>combs = itertools.combinations('ABC', 2)<br>print(list(combs))  # Output: [('A', 'B'), ('A', 'C'), ('B', 'C')]</p></span></pre><h2 id="380c">Cartesian product</h2><pre><span id="992e">import itertools<p># Generating Cartesian product<br>cartesian = itertools.product('AB', [1, 2])<br>print(list(cartesian))  # Output: [('A', 1), ('A', 2), ('B', 1), ('B', 2)]</p></span></pre><h2 id="662b">Two Ways of Using Underscore</h2><p id="823c">These are two ways to use the underscore in Python: as a separater for large numbers or as a throwaway variable.</p><h2 id="0db3">Throwaway Variable</h2><p id="7870">The underscore <code>_</code> can be used as a throwaway variable to discard unwanted values:</p><pre><span id="0672"># Ignoring the first return value of a function<br>_, result = some_function()<p># Looping without using the loop variable<br>for _ in range(5):<br>    do_something()</p><p># You just need the first and the last<br>first, *_, last = [1, 2, 3, 4, 5]</p></span></pre><h2 id="a32f">Separater for Large Numbers</h2><p id="81aa">You can use underscores (<code>_</code>) as visual separators to enhance readability when working with large numeric values. This feature was introduced in Python 3.6 and is known as "underscore literals."</p><pre><span id="6f99">population = 7_900_000_000<br>revenue = 3_249_576_382.50<p>print(population)  # Output: 7900000000<br>print(revenue)  # Output: 3249576382.5</p></span></pre></div><div><p id="ac5b">Liked this article?</p><p id="135d">A few <strong>claps 👏 </strong>is very much appreciated (Just hold that clap button and never let go 😜)! And if you want to read more, make sure to give me a <strong>follow. 🙏</strong></p><p id="803d">Thanks so much for reading!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Algorithmic Beauty of Plants [pdf] (117 pts)]]></title>
            <link>http://algorithmicbotany.org/papers/abop/abop.pdf</link>
            <guid>36853206</guid>
            <pubDate>Mon, 24 Jul 2023 19:39:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://algorithmicbotany.org/papers/abop/abop.pdf">http://algorithmicbotany.org/papers/abop/abop.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=36853206">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Advanced Performance Extensions (APX) (129 pts)]]></title>
            <link>https://www.intel.com/content/www/us/en/developer/articles/technical/advanced-performance-extensions-apx.html</link>
            <guid>36853166</guid>
            <pubDate>Mon, 24 Jul 2023 19:35:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.intel.com/content/www/us/en/developer/articles/technical/advanced-performance-extensions-apx.html">https://www.intel.com/content/www/us/en/developer/articles/technical/advanced-performance-extensions-apx.html</a>, See on <a href="https://news.ycombinator.com/item?id=36853166">Hacker News</a></p>
<div id="readability-page-1" class="page">


















    
    
    
        















    
    
        
        
        
        
        
        
        
    




    
    
        
    




    
    
    
    
        
    


    
    
        
    


    
    
        
    


    
    
        
    






















    
    
    
    
    



    


    

































    
        <div data-component="global-nav-redesign" data-component-id="1">
            <header role="banner">
                <nav role="navigation" aria-label="main navigation" data-igm="">
                    <!-- Brand and toggle get grouped for better mobile display -->
                    <div>

                        <div>
                            <a href="https://www.intel.com/content/www/us/en/homepage.html" alt="Intel homepage">
                                    
                                <img src="https://www.intel.com/content/dam/logos/intel-header-logo.svg" height="300" width="118" alt="Intel logo - Return to the home page">
                            </a>
                        </div>

                        

                        <!-- START MOBLE TOGGLE buttons -->
                        <div>


                            <!-- START: NON-signed in panel -->
                            <span id="not-logged-in-scenario">

    
        </span>


                            


















































<span id="logged-in-scenario">






</span>

                            
















    




<div id="panel-language-selector" aria-expanded="false" aria-selected="false">
                <h2>
                    
                        
                            Select Your Language
                        
                        
                    
                </h2>
            </div>


                            <!-- END: NON-sign in panel -->
                            

























    

    



    








    











    
    
    
        
    


    
    <div aria-live="off" id="simplify-search" data-component="wa_skip_track" data-igm-search-content="" aria-expanded="false" aria-selected="false" document-height="true">
                             
                                 
                            
                            <!-- Search Result Typeahead -->
                            <div id="igm-search-result" data-igm-search-results="">
                                    
                                        Sign In to access restricted content
                                </div>
                            <!-- Recent Searches: 1) display default search info if no search terms is available  -->
                            <!-- Recent Searches: 2) display recenter terms when available and hide default search info  -->
                            <div data-igm-search-related="">
                                <div>
                                    <!-- default search info -->
                                    <div>
                                        <h3>Using Intel.com Search</h3>
                                        <p>You can easily search the entire Intel.com site in several ways.</p>
                                        <ul>
                                            <li>
                                                Brand Name:
                                                <strong>
                                                    Core i9
                                                </strong>
                                            </li>
                                            <li>
                                                Document Number:
                                                <strong>
                                                    123456
                                                </strong>
                                            </li>
                                            <li>
                                                Code Name:
                                                <strong>
                                                    Alder Lake
                                                </strong>
                                            </li>
                                            <li>
                                                Special Operators:
                                                <strong>
                                                    “Ice Lake”, Ice AND Lake, Ice OR Lake, Ice*
                                                </strong>
                                            </li>
                                        </ul>
                                    </div>
                                    <!-- quick links is always visible on the recents overlay -->
                                    <div>
                                        <h3>Quick Links</h3>
                                        <p>You can also try the quick links below to see results for most popular searches.</p>
                                        <ul>
                                            <!--<li>
                                                <a class="quick-link" rel="noopener noreferrer"
                                                   href=https://ark.intel.com?wapkw=quicklink:product-specifications>
                                                    Product Specifications
                                                </a>
                                            </li>-->
                                            <li>
                                                <a rel="noopener noreferrer" href="https://www.intel.com/content/www/us/en/products/overview.html?wapkw=quicklink:products">
                                                    Product Information
                                                </a>
                                            </li>
                                            <li><a rel="noopener noreferrer" href="https://www.intel.com/content/www/us/en/support.html?wapkw=quicklink:support">
                                                Support
                                            </a>
                                            </li>
                                            <li>
                                                <a rel="noopener noreferrer" href="https://downloadcenter.intel.com/?wapkw=quicklink:download-center">
                                                    Drivers &amp; Software
                                                </a>
                                            </li>
                                        </ul>
                                    </div>
                                    <!-- recent search terms -->
                                    <div data-component="wa_skip_track" data-component-id="1">
                                            <h3>Recent Searches</h3>
                                        </div>
                                </div>
                                <div>
                                    
                                        Sign In to access restricted content
                                </div>
                            </div>
                            
                                 <div data-igm-advanced-search="">
											<div data-component="wa_skip_track" data-component-id="1">
													<h3>Advanced Search</h3>
													<div>
															<h3>Only search in</h3>
															<div aria-label="Only Search In">
																<label for="search_title">
																	
																	Title</label>

																	<label for="search_description">
																	
																Description</label>

																	<label for="search_id">
																	Content ID</label>
															</div>

															
														</div>
												</div>
											<div>
												Sign in to access
												restricted content.
											</div>
										</div>
                                    
                        </div>


                        </div>
                        <!-- END MOBILE TOGGLE buttons -->
                    </div>
                </nav>
            </header>
        </div>

    
    


<div id="alertMsg" data-scroll-track="false">
                        <p>The browser version you are using is not recommended for this site.<br>Please consider upgrading to the latest version of your browser by clicking one of the following links.</p>
                        <div>
                            <ul>
                                
                                    <li><a href="https://support.apple.com/downloads/safari">Safari</a></li>
                                
                                    <li><a href="https://support.google.com/chrome/answer/95346?hl=en">Chrome</a></li>
                                
                                    <li><a href="https://www.microsoft.com/en-us/edge">Edge</a></li>
                                
                                    <li><a href="https://www.mozilla.org/en-US/firefox/new/">Firefox</a></li>
                                
                            </ul>
                        </div>
                    </div>


<main id="primary-content">


























    
        
        
        <main id="primary-content-main">
            

            <div id="articleHero-1" data-component="articleHero" data-component-id="1">
                                        <h2>Introducing Intel® Advanced Performance Extensions (Intel® APX)</h2>
                                    </div>

            
            <article data-component-id="1" data-component="articletemplate">
                <div>
                            <div>
                                        
                                        <img src="" alt="author-image">
                                    </div>

                            <div>
                                            <!-- BEGIN NEW BUILD IN - Article Actions - DESKTOP -->
                                            
                                            <div id="articleintro" data-component="articleintro" data-component-id="1">
                    <span>Sebastian Winkel </span>
                    <span></span>
                
                    <span>Jason Agron</span>
                    <span></span>
                </div>

                                            <div id="articleparagraph-1" data-component="articleparagraph" data-component-id="1">
                    
                    


<commons:articleparagraphtag>


    <div>
                                
                                
                                    
                                    
                                        <p>Intel® architecture powers datacenters and personal computers around the world. Since its introduction by Intel® in 1978, the architecture has continuously evolved to take advantage of emerging workloads and the relentless pace of Moore’s law. The original instruction set defined only eight 16-bit general-purpose registers, which over the years were doubled in number and quadrupled in size. A large set of vector registers was added, and most recently Intel® AMX introduced two-dimensional matrix registers, providing a big jump in AI performance.<a href="https://www.intel.com/content/www/us/en/products/docs/accelerator-engines/advanced-matrix-extensions/overview.html"><sup>1</sup></a></p>

<p>Today, we are introducing the next major step in the evolution of Intel® architecture. Intel® Advanced Performance Extensions (Intel® APX) expands the entire x86 instruction set with access to more registers and adds various new features that improve general-purpose performance. The extensions are designed to provide efficient performance gains across a variety of workloads – without significantly increasing silicon area or power consumption of the core.</p>

<p>Intel® APX doubles the number of general-purpose registers (GPRs) from 16 to 32. This allows the compiler to keep more values in registers; as a result, APX-compiled code contains 10% fewer loads and more than 20% fewer stores than the same code compiled for an Intel® 64 baseline.<sup>2</sup>&nbsp;Register accesses are not only faster, but they also consume significantly less dynamic power than complex load and store operations.</p>

<p>Compiler enabling is straightforward – a new REX2 prefix provides uniform access to the new registers across the legacy integer instruction set. Intel® AVX instructions gain access via new bits defined in the existing EVEX prefix. In addition, legacy integer instructions now can also use EVEX to encode a dedicated destination register operand – turning them into three-operand instructions and reducing the need for extra register move instructions. While the new prefixes increase average instruction length, there are 10% fewer instructions in APX-compiled code<sup>2</sup>, resulting in similar code density as before.</p>

<p>The new GPRs are XSAVE-enabled, which means that they can be automatically saved and restored by XSAVE/XRSTOR sequences during context switches. They do not change the size and layout of the XSAVE area as they take up the space left behind by the deprecated Intel® MPX registers.</p>

<p>We propose to define the new GPRs as caller-saved (volatile) state in application binary interfaces (ABIs), facilitating interoperability with legacy binaries. Optimized calling conventions can be introduced where legacy compatibility requirements are relaxed. Generally, more register state will need to be managed at function boundaries. In order to reduce the associated overhead, we are adding PUSH2/POP2 instructions that transfer two register values within a single memory operation. The processor tracks these new instructions internally and fast-forwards register data between matching PUSH2 and POP2 instructions without going through memory.</p>

<p>The performance features introduced so far will have limited impact in workloads that suffer from a large number of conditional branch mispredictions. As out-of-order CPUs continue to become deeper and wider, the cost of mispredictions increasingly dominates performance of such workloads. Branch predictor improvements can mitigate this to a limited extent only as data-dependent branches are fundamentally hard to predict.</p>

<p>To address this growing performance issue, we significantly expand the conditional instruction set of x86, which was first introduced with the Intel® Pentium® Pro in the form of CMOV/SET instructions. These instructions are used quite extensively by today’s compilers, but they are too limited for broader use of if-conversion (a compiler optimization that replaces branches with conditional instructions).</p>

<p>Intel® APX adds conditional forms of load, store, and compare/test instructions, and it also adds an option for the compiler to suppress the status flags writes of common instructions. These enhancements expand the applicability of if-conversion to much larger code regions, cutting down on the number of branches that may incur misprediction penalties. All these conditional ISA improvements are implemented via EVEX prefix extensions of existing legacy instructions.</p>

<p>Application developers can take advantage of Intel® APX by simple recompilation – source code changes are not expected to be needed. Workloads written in dynamic languages will automatically benefit as soon as the underlying runtime system has been enabled.</p>

<p>Intel® APX demonstrates the advantage of the variable-length instruction encodings of x86 – new features enhancing the entire instruction set can be defined with only incremental changes to the instruction-decode hardware. This flexibility has allowed Intel® architecture to adapt and flourish over four decades of rapid advances in computing – and it enables the innovations that will keep it thriving into the future.</p>

<h2>References</h2>

<ul>
	<li><a href="http://cdrdv2.intel.com/v1/dl/getContent/784266">Intel® Advanced Performance Extensions (Intel® APX) Architecture Specification</a>&nbsp;- specifies the Intel® APX extension of the encodings and the semantics of Intel architecture.</li>
	<li><a href="https://cdrdv2.intel.com/v1/dl/getContent/784265">Intel® Advanced Performance Extensions (Intel® APX) Software Enabling Introduction</a>&nbsp;-&nbsp;outlines the changes needed to enable Intel® APX in compilers, ABIs, operating systems, and hypervisors.</li>
	<li><a href="https://cdrdv2.intel.com/v1/dl/getContent/784267">Intel® Advanced Vector Extensions 10 (Intel® AVX10) Architecture Specification</a>&nbsp;- describes the Intel® Advanced Vector Extensions 10 Instruction Set Architecture.</li>
	<li><a href="https://cdrdv2.intel.com/v1/dl/getContent/784343">The Converged Vector ISA: Intel® Advanced Vector Extensions 10 Technical Paper</a>&nbsp;- provides introductory information regarding the converged vector ISA: Intel® Advanced Vector Extensions 10.</li>
</ul>

<h2>Footnotes</h2>

<ol>
	<li><a href="https://www.intel.com/content/www/us/en/products/docs/accelerator-engines/advanced-matrix-extensions/overview.html">intel.com/content/www/us/en/products/docs/accelerator-engines/advanced-matrix-extensions/overview.html</a></li>
	<li>This projection is based on a prototype simulation of the SPEC CPU® 2017 Integer benchmark suite.</li>
</ol>

                                    
                                
                            </div>

                    



                </commons:articleparagraphtag></div>


                                        </div>
                        </div>

                
                    
                

                
                
                
                
                    
                        
                    
                

            </article>
        </main>
    
    





    <div>
                    <h4>Product and Performance Information</h4>
                    
                    
                    
                        
                            
                        
                    
                    
                    
                </div>




    


</main>












































    







</div>]]></description>
        </item>
        <item>
            <title><![CDATA[A simple guide to fine-tuning Llama 2 (256 pts)]]></title>
            <link>https://brev.dev/blog/fine-tuning-llama-2</link>
            <guid>36852971</guid>
            <pubDate>Mon, 24 Jul 2023 19:18:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://brev.dev/blog/fine-tuning-llama-2">https://brev.dev/blog/fine-tuning-llama-2</a>, See on <a href="https://news.ycombinator.com/item?id=36852971">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><strong>In this guide, I show how you can fine-tune Llama 2 to be a dialog summarizer!</strong></p><p>Last weekend, I wanted to finetune Llama 2 (which now reigns supreme in the <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">Open LLM leaderboard</a>) on a dataset of my own collection of Google Keep notes; each one of my notes has both a title and a body so I wanted to train Llama to generate a body from a given title.</p><p>This first part of the tutorial covers finetuning Llama 2 on the <a href="https://huggingface.co/datasets/samsum">samsum dialog summarization dataset</a> using Huggingface libraries. I tend to find that while Huggingface has built a superb library in transformers, their guides tend to overcomplicate things for the average joe. The second part, fine-tuning on custom data, is coming at the end of the week!</p><p>To get started, get yourself either an A10, A10G, A100 (or any GPU with &gt;24GB GPU memory). If you're not sure where to start, the <a href="https://console.brev.dev/environment/new?instance=gpu_1x_a10">Brev Cloud</a> makes it easy to access each of these GPUs!</p><h2 id="1-download-the-model">1. Download the model</h2><p>Clone Meta's Llama inference repo (which contains the download script):</p><pre><code><span>git</span><span> clone https://github.com/facebookresearch/llama.git</span>
</code></pre><p>Then run the download script:</p><pre><code><span>bash</span><span> download.sh</span>
</code></pre><p>It'll prompt you to enter the URL you got sent by Meta in an email. If you haven't signed up, do it <a href="https://ai.meta.com/llama/">here</a>. They are surprisingly quick at sending you the email!</p><p>For this guide, you only need to download the 7B model.</p><h2 id="2-convert-model-to-hugging-face-format">2. Convert model to Hugging Face format</h2><pre><code><span>pip </span><span>install</span><span> git+https://github.com/huggingface/transformers</span>
<span></span><span>cd</span><span> transformers</span>
<!-- -->
<span>python convert_llama_weights_to_hf.py </span><span>\</span><span></span>
<span>    --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir models_hf/7B</span>
</code></pre><p>This now gives us a Hugging Face model that we can fine-tune leveraging Huggingface libraries!</p><h2 id="3-run-the-fine-tuning-notebook">3. Run the fine-tuning notebook:</h2><p>Clone the Llama-recipies repo:</p><pre><code><span>git</span><span> clone https://github.com/facebookresearch/llama-recipes.git</span>
</code></pre><p>Then open the <strong>quickstart.ipynb</strong> file in your preferred notebook interface:</p><p>(I use Jupyter lab like so):</p><pre><code><span>pip </span><span>install</span><span> jupyterlab</span>
<span>jupyter lab </span><span># in the repo you want to work in</span>
</code></pre><p>Then just run the whole notebook.</p><p>Make sure you change the line:</p><pre><code><span>model_id</span><span>=</span><span>"./models_hf/7B"</span>
</code></pre><p>to your actual model path that you converted. And that's that! You will end up with a Lora fine-tuned.</p><h2 id="4-run-inference-on-your-fine-tuned-model">4. Run inference on your fine-tuned model</h2><p>The issue here is that Huggingface only saves the adapter weights and not the full model. So we need to load the adapter weights into the full model. I struggled for a bit finding the right documentation to do this...But eventually worked it out!</p><p>Import libraries:</p><pre><code><span>import</span><span> torch</span>
<span></span><span>from</span><span> transformers </span><span>import</span><span> LlamaForCausalLM</span><span>,</span><span> LlamaTokenizer</span>
<span></span><span>from</span><span> peft </span><span>import</span><span> PeftModel</span><span>,</span><span> PeftConfig</span>
</code></pre><p>Load the tokenizer and model:</p><pre><code><span>model_id</span><span>=</span><span>"./models_hf/7B"</span><span></span>
<span>tokenizer </span><span>=</span><span> LlamaTokenizer</span><span>.</span><span>from_pretrained</span><span>(</span><span>model_id</span><span>)</span><span></span>
<span>model </span><span>=</span><span>LlamaForCausalLM</span><span>.</span><span>from_pretrained</span><span>(</span><span>model_id</span><span>,</span><span> load_in_8bit</span><span>=</span><span>True</span><span>,</span><span> device_map</span><span>=</span><span>'auto'</span><span>,</span><span> torch_dtype</span><span>=</span><span>torch</span><span>.</span><span>float16</span><span>)</span>
</code></pre><p>Load the adapter from where you saved it post-train:</p><pre><code><span>model </span><span>=</span><span> PeftModel</span><span>.</span><span>from_pretrained</span><span>(</span><span>model</span><span>,</span><span> </span><span>"/root/llama-recipes/samsungsumarizercheckpoint"</span><span>)</span>
</code></pre><p>Run inference:</p><pre><code><span>eval_prompt </span><span>=</span><span> </span><span>"""</span>
<span>Summarize this dialog:</span>
<span>A: Hi Tom, are you busy tomorrow’s afternoon?</span>
<span>B: I’m pretty sure I am. What’s up?</span>
<span>A: Can you go with me to the animal shelter?.</span>
<span>B: What do you want to do?</span>
<span>A: I want to get a puppy for my son.</span>
<span>B: That will make him so happy.</span>
<span>A: Yeah, we’ve discussed it many times. I think he’s ready now.</span>
<span>B: That’s good. Raising a dog is a tough issue. Like having a baby ;-)</span>
<span>A: I'll get him one of those little dogs.</span>
<span>B: One that won't grow up too big;-)</span>
<span>A: And eat too much;-))</span>
<span>B: Do you know which one he would like?</span>
<span>A: Oh, yes, I took him there last Monday. He showed me one that he really liked.</span>
<span>B: I bet you had to drag him away.</span>
<span>A: He wanted to take it home right away ;-).</span>
<span>B: I wonder what he'll name it.</span>
<span>A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))</span>
<span>---</span>
<span>Summary:</span>
<span>"""</span><span></span>
<!-- -->
<span>model_input </span><span>=</span><span> tokenizer</span><span>(</span><span>eval_prompt</span><span>,</span><span> return_tensors</span><span>=</span><span>"pt"</span><span>)</span><span>.</span><span>to</span><span>(</span><span>"cuda"</span><span>)</span><span></span>
<!-- -->
<span>model</span><span>.</span><span>eval</span><span>(</span><span>)</span><span></span>
<span></span><span>with</span><span> torch</span><span>.</span><span>no_grad</span><span>(</span><span>)</span><span>:</span><span></span>
<span>    </span><span>print</span><span>(</span><span>tokenizer</span><span>.</span><span>decode</span><span>(</span><span>model</span><span>.</span><span>generate</span><span>(</span><span>**</span><span>model_input</span><span>,</span><span> max_new_tokens</span><span>=</span><span>100</span><span>)</span><span>[</span><span>0</span><span>]</span><span>,</span><span> skip_special_tokens</span><span>=</span><span>True</span><span>)</span><span>)</span>
</code></pre><p>Next in this series, I'll show you how you can format your own dataset to train Llama 2 on a custom task! Message me on <a href="https://twitter.com/samlhuillier_">Twitter</a> if you want to get me to hurry up on this!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Elixir is still safe (220 pts)]]></title>
            <link>https://paraxial.io/blog/still-safe</link>
            <guid>36852231</guid>
            <pubDate>Mon, 24 Jul 2023 18:22:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://paraxial.io/blog/still-safe">https://paraxial.io/blog/still-safe</a>, See on <a href="https://news.ycombinator.com/item?id=36852231">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><img src="https://paraxial.io/blog/cheetah.jpeg" width="700"></p>
<p>
In March 2021 Nathan Long published <a href="https://dockyard.com/blog/2021/03/30/elixir-is-safe">Elixir is Safe</a>, a post about the security benefits of using Elixir, which focused on memory and thread safety. It is an excellent article for programmers and executives about the security benefits of Elixir. In July 2023 I googled “Elixir is Safe”, and the first result was a snippet from the paper, “Vision for a Secure Elixir Ecosystem: An Empirical Study of Vulnerabilities in Elixir Programs”, which was <a href="https://dl.acm.org/doi/10.1145/3476883.3520204">published by the ACM in April 2022.</a></p>
<p><img src="https://paraxial.io/blog/google.png" width="550"></p><p>
Several of the paper’s authors are students, and this post is not meant to criticize them as researchers. The paper is written as a reaction to Nathan’s article, which accurately describes the security benefits of Elixir. The paper’s claims are misleading, and the fact that it currently ranks above the original article on Google, and was published in a reputable journal, warrants a response.</p>
<p>
<em>“Practitioners perceive Elixir to be a ‘safe’ language, as the language allows practitioners to write fast software programs without introducing vulnerabilities, unlike other languages, such as C[16]. Positive perception of practitioners about the safety and security Elixir programs is subject to empirical validation. Practitioner perceptions are formed through personal experience, and not based on empirical evidence [9]. A systematic empirical investigation that quantifies reported security vulnerabilities can shed light on the state of security of Elixir programs. Such empirical investigation can also yield recommendations for practitioners and researchers on how to securely develop Elixir programs.”</em></p>
<p>
<em>[16] Nathan Long. 2021. Elixir is Safe. <a href="https://dockyard.com/blog/2021/03/30/elixir-is-safe">https://dockyard.com/blog/2021/03/30/elixir-is-safe</a></em></p>
<p>
Nobody thinks Elixir “allows practitioners to write fast software programs without introducing vulnerabilities”. The article “Elixir is Safe” never makes that claim, rather it mentions that Elixir is a memory safe language, which C is not, and discusses the thread safety benefits of Elixir. These are both important topics, for example Elixir completely eliminates <a href="https://paraxial.io/blog/data-race">security issues due to data races</a>, a severe vulnerability that can lead to users being logged into the wrong account, a disaster for banking and medical portals. </p>
<p>
An empirical study showing that Elixir programs contain vulnerabilities is not surprising, because software written in every language has them. Imagine Elixir as a type of asphalt which reduces the rate of potholes in roads, and Nathan’s article is explaining this benefit. Then a paper is published saying, “roads paved with Elixir still have potholes”, ignoring that it results in fewer potholes. The paper does not mention memory or thread safety at all, which are the specific security benefits Nathan discusses. Rather, it surveys popular open-source Elixir projects for vulnerability related commits. </p>
<p><img src="https://paraxial.io/blog/repo_set.png" width="500"></p><p>
Which repos were selected for the dataset? The paper does not mention them, below is a list pulled from the <a href="https://figshare.com/articles/dataset/Elixir_Vulnerability_Dataset/15078573/1">dataset</a>. The unix command <code>awk -F "," '{print $1}' ELIXIR_FINAL_SECURITY_BUG_DATASET.csv | sort | uniq</code> was used to get this list.</p>
<pre><code>1. 30-days-of-elixir
2. absinthe
3. analytics
4. awesome-elixir
5. credo
6. curlconverter
7. distillery
8. edeliver
9. elixir
10. elixir-koans
11. guardian
12. httpoison
13. papercups
14. phoenix
15. phoenix-trello
16. poison
17. quantum-core
18. realtime
19. rustler</code></pre>
<p>
It is surprising to only see 19 repos, when the paper says 25. The inclusion of <code>30-days-of-elixir</code>, <code>awesome-elixir</code>, and <code>elixir-koans</code> is debatable, considering these repos do not store projects intended to be run in production. The counts for “Elixir Files” and “Elixir-related Commits” in the dataset do match the paper. </p>
<p><img src="https://paraxial.io/blog/table2.png" width="500"></p><p>
These numbers do match the dataset, but the definition of “program” is misleading. For example, commit <code>2d68e9c</code> maps to two files in the dataset:</p>
<pre><code>absinthe/test/absinthe/phase/document/complexity_test.exs
absinthe/lib/absinthe/phase/document/complexity/result.ex</code></pre>
<p>
<a href="https://github.com/absinthe-graphql/absinthe/pull/800/commits/2d68e9c4d6540a200cd134661c23400d26631f75">View this commit on Github.</a> Both of these files are considered a “program” in the paper. It is misleading to say the dataset has “319 programs with vulnerability related commits”, the paper came to that number by counting files per commit classified as vulnerable. </p>
<p>
In the paper’s dataset, the repo for <a href="https://plausible.io/">Plausible Analytics</a> is included, with commit <code>f7b37fe</code>. You can <a href="https://github.com/plausible/analytics/commit/f7b37fe9eac54336f7a61a6b4d2378a381637b5b">view this commit on Github</a>, but it is for a PR with 32 changed files, so the commit alone is not enough information to determine what vulnerability the dataset is referring to. The paper mentions that commits were found via a keyword search for terms often used to describe vulnerabilities, one of which is “sql”.</p>
<pre><code>lib/plausible_web/controllers/api/external_controller.ex</code></pre>
<pre><code><span>postgres_health</span><span> </span><span>=</span><span>
  </span><span>case</span><span> </span><span>Ecto.Adapters.SQL</span><span>.</span><span>query</span><span data-group-id="7394294953-1">(</span><span>Plausible.Repo</span><span>,</span><span> </span><span>"SELECT 1"</span><span>,</span><span> </span><span data-group-id="7394294953-2">[</span><span data-group-id="7394294953-2">]</span><span data-group-id="7394294953-1">)</span><span> </span><span data-group-id="7394294953-3">do</span><span>
    </span><span data-group-id="7394294953-4">{</span><span>:ok</span><span>,</span><span> </span><span>_</span><span data-group-id="7394294953-4">}</span><span> </span><span>-&gt;</span><span> </span><span>"ok"</span><span>
    </span><span>e</span><span> </span><span>-&gt;</span><span> </span><span>"error: </span><span data-group-id="7394294953-5">#{</span><span>inspect</span><span data-group-id="7394294953-6">(</span><span>e</span><span data-group-id="7394294953-6">)</span><span data-group-id="7394294953-5">}</span><span>"</span><span>
  </span><span data-group-id="7394294953-3">end</span></code></pre>
<p>
Sobelow will flag this code as vulnerable, even though it is not, because no user input is being passed to the SQL query. </p>
<pre><code>lib/plausible_release.ex</code></pre>
<pre><code><span>  </span><span>defp</span><span> </span><span>do_create_ch_db</span><span data-group-id="8989898227-1">(</span><span data-group-id="8989898227-1">)</span><span> </span><span data-group-id="8989898227-2">do</span><span>
    </span><span>db_to_create</span><span> </span><span>=</span><span> </span><span>Keyword</span><span>.</span><span>get</span><span data-group-id="8989898227-3">(</span><span>Application</span><span>.</span><span>get_env</span><span data-group-id="8989898227-4">(</span><span>:plausible</span><span>,</span><span> </span><span>:clickhouse</span><span data-group-id="8989898227-4">)</span><span>,</span><span> </span><span>:database</span><span data-group-id="8989898227-3">)</span><span>
    </span><span>IO</span><span>.</span><span>puts</span><span data-group-id="8989898227-5">(</span><span>"create </span><span data-group-id="8989898227-6">#{</span><span>inspect</span><span data-group-id="8989898227-7">(</span><span>db_to_create</span><span data-group-id="8989898227-7">)</span><span data-group-id="8989898227-6">}</span><span> clickhouse database/tables if it doesn't exist"</span><span data-group-id="8989898227-5">)</span><span>
    </span><span>Clickhousex</span><span>.</span><span>query</span><span data-group-id="8989898227-8">(</span><span>:clickhouse</span><span>,</span><span> </span><span>"CREATE DATABASE IF NOT EXISTS </span><span data-group-id="8989898227-9">#{</span><span>db_to_create</span><span data-group-id="8989898227-9">}</span><span>"</span><span>,</span><span> </span><span data-group-id="8989898227-10">[</span><span data-group-id="8989898227-10">]</span><span data-group-id="8989898227-8">)</span><span>
  </span><span data-group-id="8989898227-2">end</span></code></pre>
<p>
The string interpolation in a Clickhouse DB query looks suspicious, but the <code>db_to_create</code> value is not set by user input, so there is no vulnerability. The keywords “security” and “vulnerability” also appear in the PR. Link to sub-commits:</p>
<p>
<a href="https://github.com/plausible/analytics/pull/209/commits/b3f783edb698c273cfa13271be3a74c9f226d65a"><code>b3f783e</code></a> - Adds a container security tool, not related to Elixir. </p>
<p>
<a href="https://github.com/plausible/analytics/pull/209/commits/57171f893aa762d02eeea5130f6e84f739f447a3"><code>57171f8</code></a> - The text of this comment says, “fixing some vulnerabilities identified by the scanning tools”, however it involved a Debian upgrade, not related to Elixir.</p>
<p>
The paper counts <code>f7b37fe</code> as “28 programs with vulnerability related commits”, because it involves 28 Elixir source code files, which is not correct. The lack of clarity around the vulnerable code in the dataset is concerning, making it impossible to truly replicate the paper’s findings, because you have to guess which specific lines the dataset is referring to as vulnerable. The dataset contains no labels for the type of security issue found (DoS, XSS, CSRF, etc), no information on the severity of each issue, and no discussion on how an attacker would use each vulnerability. This limits its utility in discussing the security of Elixir applications.</p>
<p>
It is highly unlikely that the dataset from this paper includes vulnerabilities related to thread or memory safety, which is the main point Nathan correctly makes in his article <a href="https://dockyard.com/blog/2021/03/30/elixir-is-safe">Elixir is Safe</a>.</p>
<hr>
<p>
<em><a href="https://paraxial.io/">Paraxial.io</a> secures Elixir and Phoenix applications. Professional services, including Elixir developer security training and penetration testing, are also available. <a href="https://calendly.com/paraxial/paraxial-io">Schedule a call today</a>.</em></p>
<meta property="og:image" content="https://paraxial.io/blog/cheetah.jpeg">
<meta name="twitter:image" content="https://paraxial.io/blog/cheetah.jpeg">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:title" content="Elixir is (Still) Safe">
<meta name="twitter:description" content="Examining the misleading claims in a published paper about Elixir security">
<meta name="twitter:site" content="@paraxialio">

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Have Attention Spans Been Declining? – Yes, 65% (558 pts)]]></title>
            <link>https://slimemoldtimemold.com/2023/07/24/your-mystery-have-attention-spans-been-declining/</link>
            <guid>36851644</guid>
            <pubDate>Mon, 24 Jul 2023 17:43:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://slimemoldtimemold.com/2023/07/24/your-mystery-have-attention-spans-been-declining/">https://slimemoldtimemold.com/2023/07/24/your-mystery-have-attention-spans-been-declining/</a>, See on <a href="https://news.ycombinator.com/item?id=36851644">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
		<main id="main" role="main">

		
			
<article id="post-3310">
	<!-- .entry-header -->

	<div>
		
<p>[<em>This is one of the finalists in the SMTM Mysteries Contest, by a reader writing under the pseudonym&nbsp;</em>Cennfaeladh<em>. We’ll be posting about one of these a week until we have gotten through all the finalists. At the end, we’ll ask you to vote for a favorite, so remember which ones you liked</em>.]</p>







<p><strong>I investigate whether the attention span of individual humans has been falling over the last two decades (prompted by curiosity about whether the introduction of the internet may be harmful to cognitive performance). I find little direct work on the topic, despite its wide appeal. Reviewing related research indicates that individual attention spans might indeed have been declining<sub>65%</sub>.</strong></p>



<p>In what might be just the age-old regular<a href="https://en.wikipedia.org/wiki/Ephebiphobia"> ephebiphobia</a>, claims have been raised that individual attention spans<a href="https://docs.google.com/document/d/1HgV-GYVzALKj3ykZAqZD1HGWUKLgUopFMfOPHx-t0uI/edit#heading=h.pmepesu94ht5"> have been declining</a>—not just among adolescents, but among the general population. If so, this would be quite worrying: Much of the economy in industrialized societies is comprised of knowledge work, and knowledge work depends on attention to the task at hand: switching between tasks too often might prevent progress on complicated and difficult problems.</p>



<p>I became interested in the topic after<a href="https://edition.cnn.com/2023/01/11/health/short-attention-span-wellness/index.html"> seeing</a><a href="https://eu.usatoday.com/story/life/health-wellness/2021/12/22/covid-attention-span-exhaustion/8926439002/"> several</a><a href="https://twitter.com/amix011/status/1603927882459672576"> claims</a> that e.g.<a href="https://en.wikipedia.org/wiki/Generation_Z"> Generation Z</a> allegedly has lower attention spans, observing myself and how I struggled to get any work done when connected to the internet, and hearing reports from others online and in person having the same problem.</p>



<p>The exact question being asked is:</p>



<p><strong>“Have the attention spans of individuals on neutral tasks (that is, tasks that are not specifically intended to be stimulating) declined from 2000 to the present?”</strong></p>



<p>(One might also formulate it as “Is there an equivalent of the “<a href="https://en.wikipedia.org/wiki/Flynn_Effect#Possible_end_of_progression">Reversed Flynn Effect</a>” for attention span?”) I am not particularly wedded to the specific timeframe, though the worries mentioned above assert that this has become most stark during the last decade or so, attributing the change to widespread social media/smartphone/internet usage. Data from before 2000 or just the<a href="https://en.wikipedia.org/wiki/Aughts"> aughts</a> would be less interesting. The near-global<a href="https://en.wikipedia.org/wiki/COVID-19_lockdowns"> COVID-19 lockdows</a> could provide an especially enlightening<a href="https://en.wikipedia.org/wiki/Natural_experiment"> natural experiment</a>: Did social media usage increase (my guess: yes<sub>90%</sub>), and if so, did attention spans decrease at the same time (or with a lag) (my guess: also yes<sub>75%</sub>), but I don’t think anyone has the data on that <em>and</em> wants to share it.</p>



<p>Ideally want to have experiments from ~2000 up to 2019: close enough to the present to see whether there is a downward trend (a bit more than a decade after the introduction of the<a href="https://en.wikipedia.org/wiki/iPhone_(1st_generation)"> iPhone in 2007</a>), but before the<a href="https://en.wikipedia.org/wiki/COVID-19_pandemic"> COVID-19 pandemic</a> which might be a huge confounder, or just have accelerated existing trends (which we can probably check in another 2 years).</p>



<p>I am mostly interested in the attention span of individual humans and not groups:<a href="https://www.nature.com/articles/s41467-019-09311-w"> Lorenz-Spreen et al. 2019</a> investigate the development of a construct they call “collective attention” (and indeed find a decline), but that seems less economically relevant than individual attention span. I am also far less interested in self-perception of attention span, give me data from a proper<a href="http://www.careergym.com/psychometric_glossary/power_test"> power-</a> or<a href="http://www.careergym.com/psychometric_glossary/speed_test"> speed-test</a>, cowards!</p>



<p>So the question I am asking is not any of the following:</p>



<ul>
<li>“Does more social media/internet usage cause decreased attention spans?”</li>



<li>“Does more social media/internet usage correlate with decreased attention spans?”</li>



<li>“Does more social media/internet usage correlate with people reporting having shorter attention spans?”</li>



<li>“Did collective attention spans decrease?”</li>



<li>“Are people on average spending less time on webpages than they used to?”</li>
</ul>







<h2><strong>How Is Attention Span Defined?</strong></h2>



<p><a href="https://en.wikipedia.org/wiki/Attention">Attention</a> is generally divided into three distinct categories: <strong>sustained attention</strong>, which is the consistent focus on a specific task or piece of information over time (Wikipedia states that the span for sustained attention has a<a href="https://www.gwern.net/Leprechauns"> leprechaun</a> figure of 10 minutes floating around, elaborated on in<a href="https://psycnet.apa.org/record/2007-09934-002"> Wilson &amp; Korn 2007</a>); <strong>selective attention</strong>, which is the ability to resist distractions while focusing on important information while performing on a task (the thing trained during<a href="https://en.wikipedia.org/wiki/Mindfulness"> mindfulness meditation</a>); and <strong>alternating</strong> or <strong>divided attention</strong>, also known as the ability to<a href="https://en.wikipedia.org/wiki/Human_multitasking"> multitask</a>.</p>



<p>When asking the question “have attention spans been declining”, we’d ideally want the same test measuring all those three aspects of attention (and not just asking people about their<a href="https://guzey.com/statistics/dont-believe-self-reported-data/"> perception via surveys</a>), performed annually on large random samples of humans over decades, ideally with additional information such as age, sex, intelligence (or alternatively educational attainment), occupation etc. I’m personally most interested in the development of sustained attention, and less so in the development of selective attention. But I have not been able to find such research, and in fact there is apparently no agreed upon test for measuring attention span in the first place:</p>



<blockquote>
<p>She studies attention in drivers and witnesses to crime and says the idea of an “average attention span” is pretty meaningless. “It’s very much task-dependent. How much attention we apply to a task will vary depending on what the task demand is.”</p>




<cite><em>— Simon Maybin quoting Dr. Gemma Briggs,</em><a href="https://www.bbc.com/news/health-38896790"><em> “Busting the attention span myth”</em></a><em>, 2017</em></cite></blockquote>



<p>So,<a href="https://slatestarcodex.com/2014/08/11/does-the-glasgow-coma-scale-exist-do-comas/"> similar to comas</a>, attention span doesn’t exist…sure,<a href="https://unremediatedgender.space/2019/Dec/on-the-argumentative-form-super-proton-things-tend-to-come-in-varieties/index.html"> super-proton things come in varieties</a>, but <strong><em>which varieties</em></strong>?? And how??? Goddamn, psychologists, do your job and don’t just<a href="https://www.lesswrong.com/rationality/explain-worship-ignore"> worship</a> complexity.</p>



<p>Perhaps I should soften my tone, as this perspective appears elsewhere:</p>



<blockquote>
<p>[…] Gould suggests the metaphor of a dense bush whose branches are periodically pruned by nature. This allows for parallel evolutionary sequences, some of which are adaptive and others not — at any moment in time only the tips of aseledaptive branches are in evidence, the pruned ones cannot be seen. Thus rather than being direct descendants of primitive hominids, for example, huankind would have evolved along a separate but parallel line from other primates.</p>



<p>Might the ontogeny of selective attention recapitulate this theme? That is, rather than selective attention comprising a single construct with a fixed ontogenic plan, might it be better conceptualized as a multidimensional construct with separat, parallel developmental trajectories for different components. To carry the analogy still further, might the specific developmental progression for a particular component of selective attention be determined by the adaptive fit of that component with the individual’s ‘environmental press’? Although such a conjecture rekindles the tened of <em>ontogeny recapitulates phylogney</em> long since abandoned in physiological development (e.g., Dixon and Lerner, 1985), we suggest that it may nonetheless provide an overarching framework within which to cast life-span research and theory on the development of selective attention.</p>




<cite><em>— Plude et al.,</em><a href="https://pubmed.ncbi.nlm.nih.gov/7976468/"><em> “The development of selective attention: A life-span overview”</em></a><em> p. 31, 1994</em></cite></blockquote>







<h2><strong>How Do We Measure Attention Span?</strong></h2>



<p>One of my hopes was that there is a canonical and well-established (and therefore, ah, <em>tested</em>) test for attention span (or just attention) à la the IQ test for <em>g</em>: If so, I would be able to laboriously go through the literature on attention, extract the individual measurements (and maybe even acquire some datasets) and perform a meta-analysis.</p>



<h3><strong>Continuous Performance Tests</strong></h3>



<p>For measuring sustained and selective attention, I found the family of<a href="https://en.wikipedia.org/wiki/Continuous_performance_task"> continuous performance tests</a>, including the Visual and Auditory CPT (IVA-2), the<a href="https://en.wikipedia.org/wiki/Test_of_variables_of_Attention"> Test of Variables of Attention</a> (T.O.V.A.), <a href="https://storefront.mhs.com/collections/conners-cpt-3">Conners’ CPT-III</a>, the <a href="https://pubmed.ncbi.nlm.nih.gov/23299180/">gradCPT</a> and the <a href="https://www.qbtech.com/adhd-tests/">QbTest</a>, some of which are described<a href="https://en.wikipedia.org/wiki/Continuous_performance_task#Test_administration"> here</a>. These tests usually contain two parts: a part with low stimulation and rare changes of stimuli, which tests for lack of attention, and a part with high stimulation and numerous changes of stimuli, which tests for impulsivity/self control.</p>



<p>Those tests<a href="https://en.wikipedia.org/wik/Continuous_performance_task#Test_scoring"> usually report four different scores</a>:</p>



<ol>
<li><strong>Correct detection:</strong> This indicates the number of times the client responded to the target stimulus. Higher rates of correct detections indicate better attentional capacity.</li>



<li><a href="https://en.wikipedia.org/wiki/Reaction_time"><strong>Reaction times</strong></a><strong>:</strong> This measures the amount of time between the presentation of the stimulus and the client’s response.</li>



<li><strong>Omission errors:</strong> This indicates the number of times the target was presented, but the client did not respond/click the mouse. High omission rates indicate that the subject is either not paying attention (distractibility) to stimuli or has a sluggish response.</li>



<li><strong>Commission errors:</strong> This score indicates the number of times the client responded but no target was presented. A fast reaction time and high commission error rate points to difficulties with impulsivity. A slow reaction time with high commission and omission errors, indicates inattention in general.</li>
</ol>



<p>I’m currently unsure about two crucial points:</p>



<ul>
<li>How much does any CPT measure the concept we naively call attention span? The papers I’ve read don’t refer to attention span per se, but a general capability of sustained and selective attention.</li>



<li>Are there any time-series analyses or longitudinal studies using a CPT, or alternatively meta-analyses using data collected from existing studies? I have not been able to find any.</li>
</ul>



<h3><strong>Other Heterogenous Metrics</strong></h3>



<p>I also attempted to find a survey or review paper on attention span, but was unsuccessful in my quest, so I fell back to collecting metrics for attention span from different papers:</p>



<ul>
<li><a href="https://dl.motamem.org/microsoft-attention-spans-research-report.pdf">Gausby 2015</a>
<ul>
<li>Three online tests (probably devised by the authors (?), since no source is given) (n≈2000 Canadians). Very little information about the exact nature of the tests.
<ul>
<li>Sustained attention span: “Counting the number of times responds correctly identified an X occurring after an A.”</li>



<li>Selective attention span: “Counting the number of times respondents correctly identified a change in the orientation of the rectangles”</li>



<li>Alternating attention span: “Calculating the difference in the time lapsed to perform a series of consecutive number or letter classification, compared to a mixture of number and letter classifications.”</li>
</ul>
</li>



<li>Neurological research: The same games/tests as above with the participants being measured with an<a href="https://en.wikipedia.org/wiki/Electroencephalography"> EEG</a> (“Results were reported as ACE (Attention, Connectivity, Encoding) scores, as well as the number of attention bursts”) (n=112 Canadians)</li>
</ul>
</li>



<li><a href="https://www.proquest.com/openview/4d7e41eb019558871327638cf5234990/1?pq-origsite=gscholar&amp;cbl=716332">Carstens et al. 2018</a> (n=209 American respondents to a survey)
<ul>
<li>Questionnaire developed by the authors based on Conners 2004 (reliability: α=0.786)</li>
</ul>
</li>



<li><a href="https://psycnet.apa.org/record/2007-09934-002">Wilson &amp; Korn 2007</a> report several different measures of attention span during lectures: the amount of notes taken over time, observation of the students by an author of one study or two independent observers in another study, retention of material after the lecture, self-report in 5-minute intervals during the lecture, and heart rate. They also note that “Researchers use behaviors such as fidgeting, doodling, yawning, and looking around as indicators of inattentiveness (e.g., Frost, 1965; Johnstone &amp; Percival, 1976).”</li>



<li><a href="https://pubmed.ncbi.nlm.nih.gov/7976468/">Plude et al. 1994</a> review how selective attention develops during a human life. For measuring attention, they mainly focus on studies using reaction time as a metric—the speed at which an action occurs as a result of a changing stimulus: eye movement patterns of infants, simple tests such as pressing a button on a changing (often visual) stimulus, the influence of irrelevant visual stimuli at the periphery on a task performed at the centre of the visual field, judging similarity of stimuli at various distances in the visual field, responding to a target stimulus surrounded by interfering distractor stimuli, and determining whether a visual target item is present or absent. They also mention skin conductance (measuring arousal).
<ul>
<li>They also mention studies investigating the time required for attentional switching in acoustic contexts: “Pearson and Lane (1991a) studied the time course of the attention-shifting process between lists and also found large age-related improvements between 8 and 11 years. Whereas 8-year-olds required more than 3.5 s to completely switch from monitoring one list to another, 11-year-olds and adults appeared to complete the switch in less than 2.5 seconds.”</li>
</ul>
</li>



<li><a href="https://www.digitalinformationworld.com/2020/02/report-shows-that-attention-spans-are-shortening.html">Muhammad 2020</a>
<ul>
<li>Time spent on websites on average.
<ul>
<li>This is not an adequate metric, I believe: It would also decline if people would become better at prioritising on which websites are more worthy of their attention.</li>
</ul>
</li>
</ul>
</li>



<li><a href="https://www.nature.com/articles/s41467-019-09311-w">Lorenz-Spreen et al. 2019</a>
<ul>
<li>Time that specific pieces of information (hashtags/n-grams/Reddit submissions &amp;c) were popular</li>
</ul>
</li>
</ul>



<hr>



<p>As it stands, I think there’s a decent chance<sub>60%</sub> that one or several tests from the CPT family can be used as tests for attention span without much of a problem.</p>



<p>I don’t think a separate dedicated test for attention span exists<sub>45%</sub>: The set of listed measures I found (apart from the CPT) appears to be too heterogenous, idiosyncratic, mostly not quantitative enough and measuring slightly different things to be robustly useful for a meta-analysis.</p>







<h2><strong>What Are the Existing Investigations?</strong></h2>



<blockquote>
<p>A lack of long-term studies means we can’t tell whether attention spans have actually declined.</p>




<cite><em>—Bobby Duffy &amp; Marion Thain,</em><a href="https://www.kcl.ac.uk/policy-institute/assets/how-people-focus-and-live-in-the-modern-information-environment.pdf"><em> “Do we have your attention”</em></a><em> p. 5, 2022</em></cite></blockquote>







<ul>
<li><a href="https://dl.motamem.org/microsoft-attention-spans-research-report.pdf">Gausby 2015</a>
<ul>
<li>Questions answered:
<ul>
<li>Sustained attention:
<ul>
<li><em>Do younger people perform worse on the sustained attention span test?</em>, Yes (31% high sustained attention for group aged 18-34, 34% for group aged 35-54, and 35% group aged 55+) (the methodology is wholly unclear here, though: how do we determine the group that has “high sustained attention span”? Did they perform any statisitical tests? If yes, which?).</li>



<li><em>Do people who report more technology usage (web browsing/multi-screen usage while online/social media usage/tech adoption) perform worse on the sustained attention span test?</em>, Yes. Light:medium:heavy usage for web browsing has 39%:33%:27% users with high sustained attention span, 36%:33%:27% for light:medium:heavy multi-screen usage, 36%:29%:23% for light:medium:heavy social media usage and 35%:31%:25% for light:medium:heavy tech adoption (though these numbers are basically not elaborated on).</li>
</ul>
</li>



<li>Selective attention:
<ul>
<li><em>Do younger people perform worse on the selective attention span test?</em> No (34% high selective attention for group aged 18-34, 30% for group aged 35-54, and 35% group aged 55+).</li>



<li><em>Do people with high selective attention use fewer devices at the same time?</em> Yes (details p. 31).</li>
</ul>
</li>



<li>Alternating attention:
<ul>
<li><em>Do younger people perform worse on the alternating attention span test?</em> No (36% high selective attention for group aged 18-34, 28% for group aged 35-54, and 36% group aged 55+).</li>



<li><em>Do people who report more technology usage (tech adoption/web browsing/multi-screen usage while online) perform worse on the alternating attention span test?</em> No, they seem to perform better: Light:medium:heavy tech adoption corresponds to 31%:39%:40% having high alternating attention spans, light:medium:heavy web browsing to 29%:34%:37% and multi-screening while online to 27%:32%:37%.</li>



<li><em>Do people who use social media more have higher Attention/Connection/Encoding scores on EEG measurements?</em>, Not quite: “Moderate users of social media are better at multi-tasking than lower users. But, when crossing into the top quartile of social media usage, scores plummet.”</li>
</ul>
</li>
</ul>
</li>



<li>This is a marketing statement wearing the skinsuit of a previously great paper, it would be awesome if they released their exact methodology (tests performed, data collected, exact calculations &amp; code written). I can smell that they actually put effort into the research: Creating an actual test instead of just asking respondents about their attention spans, doing EEG measurements of over 100 people, for 3 different types of attention…come on! Just put out there what you did!</li>
</ul>
</li>



<li><a href="https://www.proquest.com/openview/4d7e41eb019558871327638cf5234990/1?pq-origsite=gscholar&amp;cbl=716332">Carstens et al. 2018</a> (n=209 American respondents to a survey)
<ul>
<li>Questions answered:
<ul>
<li><em>Is self-reported attention span related to the number of social media accounts?</em>, No, not statistically significant (F(2, 206)=0.1223, p&gt;0.05) (via a one-way<a href="https://en.wikipedia.org/wiki/ANOVA"> ANOVA</a>)</li>



<li><em>Is self-reported attention span related to whether a respondent mainly uses a mobile phone or a computer?</em>, No, not statistically significant (P(2,713)=0.923, p&gt;0.05) (via a one-way ANOVA)</li>
</ul>
</li>



<li>I do <strong>not</strong> trust this paper: Calling (what I think is) Generation Z “Generation D” (without source for the term), being clearly written in Word, and confusing grammar (I <em>think</em> the authors are all Americans, so no excuse here):</li>
</ul>
</li>
</ul>



<blockquote>
<p>Users that are older such as late adolescents and emerging adults average approximately 30-minutes daily for just Facebook that does not calculate the time spent on all social media networks</p>




<cite><em>—Carstens et al.,</em><a href="https://www.proquest.com/openview/4d7e41eb019558871327638cf5234990/1?pq-origsite=gscholar&amp;cbl=716332"><em> “Social Media Impact on Attention Span”</em></a><em> p. 2, 2018</em></cite></blockquote>







<blockquote>
<p>Bakardjieva and Gaden (2012) examined the field of social interaction in general to the everyday chatter of unstructured and spontaneous interactions among individuals to highly structured and regulated interaction consisting of the military or the stock exchange.</p>




<cite><em>—Carstens et al.,</em><a href="https://www.proquest.com/openview/4d7e41eb019558871327638cf5234990/1?pq-origsite=gscholar&amp;cbl=716332"><em> “Social Media Impact on Attention Span”</em></a><em> p. 3, 2018</em></cite></blockquote>







<ul>
<li><a href="https://www.digitalinformationworld.com/2020/02/report-shows-that-attention-spans-are-shortening.html">Muhammad 2020</a>
<ul>
<li>Question answered: <em>How much time do people spend on a website, on average?</em>, “if you look at the trend for mobile browsing between the years 2017 and 2019 you would see that there is a drop of about 11 seconds in the average time spent on a website.” and “The data suggests that the average amount of time spent on websites before navigating away for all devices has gone down by 49 seconds which is a pretty huge reduction all things considered.”</li>



<li>The data is from the right timeframe (up to but not including 2020), but the linked<a href="https://www.similarweb.com/corp/reports/2020-digital-trends-lp/"> SimilarWeb report</a> is behind a paywall, so I can’t confirm the numbers. Furthermore, the time spent on websites is a weak proxy: Perhaps people simply have become better at prioritising information sources.</li>
</ul>
</li>



<li><a href="https://www.nature.com/articles/s41467-019-09311-w">Lorenz-Spreen et al. 2019</a>
<ul>
<li>Questions answered:
<ul>
<li><em>How long does any particular hashtag stay in the group of the top 50 most used hashtags? Specifically, how has that number developed from 2013 to 2016?</em>, “in 2013 a hashtag stayed within the top 50 for 17.5 hours on average, a number which gradually decreases to 11.9 hours in 2016”, and “The average maximum popularity ⟨L(tpeak)⟩</li>
</ul>
</li>
</ul>
</li>
</ul>



<p>on one day tpeak stays relatively constant, while the average gradients ⟨ΔL⟩</p>



<ul>
<li>in positive and negative direction become steeper over the years.”</li>



<li><em>Do things become more popular faster over time? That is, when e.g. a movie is gaining popularity, did it take longer to become popular in 1985 than it did in 2018?</em>, Broadly yes (the trends holds for popularity of hashtags in tweets (2013-2016)/<a href="https://en.wikipedia.org/wiki/n-gram">n-grams</a> in books (1900-2004)/number of theaters that movies were screened in (1985-2018)/topics for search queries on Google (2010-2017)/Reddit comments on posts (2010-2015)/citations of publications (1990-2015)/daily traffic for Wikipedia articles (2012-2017)). Again the length of the time at the peak mostly didn’t change (except in the case of Wikipedia articles, where the time at the peak <em>shrunk</em>)</li>



<li>While it investigates a question different from the one I have, this paper seems good and trustworthy to me, while supporting a suspicion I’ve had (observing that the lifecycle of e.g. memes has apparently sped up significantly). I’d be interested in seeing whether the same process holds for internet communities I’m part of (for example on votes<a href="https://www.lesswrong.com/"> LessWrong</a> and the<a href="https://forum.effectivealtruism.org/"> EA Forum</a> or forecasts on<a href="https://www.metaculus.com/"> Metaculus</a>).</li>
</ul>


<div>
<figure><img data-attachment-id="3324" data-permalink="https://slimemoldtimemold.com/2023/07/24/your-mystery-have-attention-spans-been-declining/image-8-3/" data-orig-file="https://slimemoldtimemold.files.wordpress.com/2023/07/image-8.png" data-orig-size="841,559" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-8" data-image-description="" data-image-caption="" data-medium-file="https://slimemoldtimemold.files.wordpress.com/2023/07/image-8.png?w=300" data-large-file="https://slimemoldtimemold.files.wordpress.com/2023/07/image-8.png?w=660" src="https://slimemoldtimemold.files.wordpress.com/2023/07/image-8.png" alt="Chart indicating how the speed at which hashtags become popular changed over the years. Four plots (yellow, green, blue and purple) which form a peak in the middle and fall off at the sides. The yellow line is highest around the peak, the green one is lower, blue even lower and purple the lowest." title="Chart indicating how the speed at which hashtags become popular changed over the years. Four plots \(yellow, green, blue and purple\) which form a peak in the middle and fall off at the sides. The yellow line is highest around the peak, the green one is lower, blue even lower and purple the lowest."></figure></div>






<p><a href="https://www.goodreads.com/en/book/show/60795084">Mark 2023</a> is a recent book about attention spans, which I was excited to read and find the important studies I’d missed. Unfortunately, it is quite thin on talking about the development of attention span over time. It states that</p>



<blockquote>
<p>My own research, as well as those of others, has shown that over the last fifteen years, our attention spans have declined in duration when we use our devices. Our attention spans while on our computers and smartphones have become short—crazily short—as we now spend about forty-seven seconds on any screen on average.</p>




<cite><em>—Gloria Mark, “Attention Span” p. 13/14, 2023</em></cite></blockquote>







<p>which is not quite strong enough a measurement for me.</p>



<blockquote>
<p>In 2004, in our earliest study, we found that people averaged about one hundred fifty seconds (two and a half minutes) on a computer screen before switching their attention to another screen; in 2012, the average went down to seventy-five seconds before switching. In later years, from 2016 to 2021, the average amount of time on any screen before switching was found to be relatively consistent between forty-four and fifty seconds. Others replicated our results, also with computer logging. seconds. Others replicated our results, also with computer logging. André Meyer and colleagues at Microsoft Research found the average attention span of twenty software developers over eleven workdays to be fifty seconds.⁹ For her dissertation, my student Fatema Akbar found the average attention span of fifty office workers in various jobs over a period of three to four weeks to be a mere forty-four seconds.¹⁰ In other words, in the last several years, every day and all day in the workplace, people switch their attention on computer screens about every forty-seven seconds on average. In fact, in 2016 we found the median (i.e., midpoint) for length of attention duration to be forty seconds.¹¹ This means that half the observations of attention length on any screen were shorter than forty seconds. </p>




<cite><em>—Gloria Mark, “Attention Span” p. 74/75, 2023</em></cite></blockquote>



<p><img width="555" height="356" src="https://lh5.googleusercontent.com/bdlZsd-IZ76TjJJ776QzZQ0Ftu2GLD2KxQWz-cXycXNPvdEJ-3PM25Cukktv8clkkVva7-s2vJGktWKQxh53AYLobCOVJ4DyZgXUFDrATpJ9GRPc-4OMa5mm_GcPGDo-AjBTqz-Nmh-y7yK0_d9uoaA"></p>







<p>She doesn’t mention the hypothesis that this could be the symptom of a higher ability to prioritize tasks, although she is adamant that multi-tasking is bad.</p>



<p>Furthermore, this behavior displays only a decrease in the <em>propensity</em> of attention, but not necessarily one of <em>capacity</em>: Perhaps people could concentrate more, if they wanted to/were incentivized to, but they don’t, because there is no strong intent to or reward for doing so. Admittedly, this is less of an argument in the workplace where these studies were conducted, but perhaps people just care not as much about their jobs (or so I’ve heard).</p>



<blockquote>
<p>when email was cut off, people’s attention spans were significantly longer while working on their computers—in other words, they switched their attention less frequently.</p>




<cite><em>—Gloria Mark, “Attention Span” p. 97, 2023</em></cite></blockquote>







<p>She gives some useful statistics about time spent on screens:</p>



<blockquote>
<p>Nielsen reports that Americans spend on average five hours and thirty minutes daily of screen time on their computers, tablets and phones<a href="https://www.nielsen.com/us/en/insights/report/2021/total-audience-advertising-across-todays-media/"><sup>8</sup></a>. […] But what is really astonishing is that when we add in the time watching other media like TV and films to this, then we see that our attention is fixated on some form of screen, in some type of mediated environment, nearly ten hours a day<a href="https://www.nielsen.com/us/en/insights/report/2021/total-audience-advertising-across-todays-media/"><sup>8</sup></a>.</p>




<cite><em>—Gloria Mark, “Attention Span” p. 180, 2023</em></cite></blockquote>







<p>She connects attention span to shot-length in movies:</p>



<blockquote>
<p>The type of motion within shots has been changing. According to film scholar James Cutting and his colleagues at Cornell, shots containing the onset of motion (like a standing person who then runs) have increased because filmmakers believe that it will better attract viewers’ attention. […] The average film shot length in 1930 was twelve seconds, but then began to shorten, reaching an average of less than four seconds after the year 2010, as measured by James Cutting and colleagues.<sup>12</sup> Interestingly, the shot length for film sequels also decreased. For example, the shot length of the first Iron Man film averaged about 3.7 seconds; for Iron Man 2, 3.0 seconds; and for Iron Man 3, about 2.4 seconds.<sup>13</sup></p>




<cite><em>—Gloria Mark, “Attention Span” p. 180/181, 2023</em></cite></blockquote>






<div>
<figure><img data-attachment-id="3322" data-permalink="https://slimemoldtimemold.com/2023/07/24/your-mystery-have-attention-spans-been-declining/image-7-3/" data-orig-file="https://slimemoldtimemold.files.wordpress.com/2023/07/image-7.png" data-orig-size="839,448" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-7" data-image-description="" data-image-caption="" data-medium-file="https://slimemoldtimemold.files.wordpress.com/2023/07/image-7.png?w=300" data-large-file="https://slimemoldtimemold.files.wordpress.com/2023/07/image-7.png?w=660" src="https://slimemoldtimemold.files.wordpress.com/2023/07/image-7.png" alt=""></figure></div>






<blockquote>
<p>Like in TV and film, shot lengths in television commercials also shortened over time. The average shot length of commercials in 1978 was 3.8 seconds, dropping down to an average of 2.3 seconds in 1991. […] It’s not just the shot lengths, though, that are short—the overall length of advertisements on TV has also decreased. The majority of ads started out as sixty seconds in length in the 1950s,<sup>26</sup> but that length comprised only 5 percent of ads shown in 2017. In the 1980s, advertisers started experimenting with showing fifteen-second ads instead of thirty-second ads. They discovered that fifteen seconds was even more persuasive than thirty seconds, especially when the ads used elements expressing cuteness and humor.<sup>27</sup> In 2014, 61 percent of ads were thirty seconds in length, but three years later, that percentage decreased to 49 percent.<sup>28</sup></p>




<cite><em>—Gloria Mark, “Attention Span” p. 189, 2023</em></cite></blockquote>











<h2><strong>Do People Believe Attention Spans Have Declined?</strong></h2>



<blockquote>
<p>Half of the public feel their attention span is shorter than it used to be, compared with around a quarter (23%) who believe they are just attentive [sic] as they’ve always been.</p>



<p>Again, the feeling of is not just reported by the young — it’s also the dominant feeling among the middle aged too, with 56% of 35- to 54-year-olds thinking their attention spans have worsened.</p>




<cite><em>—Bobby Duffy &amp; Marion Thain,</em><a href="https://www.kcl.ac.uk/policy-institute/assets/how-people-focus-and-live-in-the-modern-information-environment.pdf"><em> “Do we have your attention”</em></a><em> p. 6, 2022</em></cite></blockquote>







<blockquote>
<p>Even more widespread is the belief that young people’s attention spans in particular are worse than they were in the past—two-thirds of people think this is the case (66%).</p>



<p>Perhaps unsurprisingly, this belief is most common among the oldest age group surveyed, of those aged 55 or over — however, young people themselves also feel this way, with a majority of 18- 34-year-olds holding this view.</p>




<cite><em>—Bobby Duffy &amp; Marion Thain,</em><a href="https://www.kcl.ac.uk/policy-institute/assets/how-people-focus-and-live-in-the-modern-information-environment.pdf"><em> “Do we have your attention”</em></a><em> p. 7, 2022</em></cite></blockquote>







<p>Note that<a href="https://pubmed.ncbi.nlm.nih.gov/7976468/"> selective attention mostly improves with age</a>, so the older age-groups might be comparing themselves now to the younger age groups now (as opposed to remembering back at their own attention spans).</p>



<blockquote>
<p>The absence of long-term research means it remains unknown whether technology has caused a deterioration in the country’s ability to concentrate — but comparisons with survey data from previous decades indicate that, on some measures the public feel more pressured than they did in the past.</p>




<cite><em>—Bobby Duffy &amp; Marion Thain,</em><a href="https://www.kcl.ac.uk/policy-institute/assets/how-people-focus-and-live-in-the-modern-information-environment.pdf"><em> “Do we have your attention”</em></a><em> p. 18, 2022</em></cite></blockquote>







<p>In response to the questions (n=2093 UK adults aged 18+ in 2021):</p>



<ul>
<li>“To what extent do you agree or disagree with the following statement? <strong>The pace of life is too much for me these days</strong>” (1983: 30% agree, 2021: 41% agree)</li>



<li>“To what extent do you agree or disagree with the following statement? <strong>I wish I could slow down the pace of my life</strong>” (1997: 47% agree, 1999: 51% agree, 2008: 45% agree, 2021: 54% agree)</li>
</ul>







<h2><strong>What About Rates of ADHD?</strong></h2>



<p><a href="https://www.cdc.gov/ncbddd/adhd/timeline.html">Data from the CDC</a> shows a clear increase in the percentage of children with a parent-reported ADHD diagnosis:</p>


<div>
<figure><img data-attachment-id="3323" data-permalink="https://slimemoldtimemold.com/2023/07/24/your-mystery-have-attention-spans-been-declining/image-27/" data-orig-file="https://slimemoldtimemold.files.wordpress.com/2023/07/image.jpeg" data-orig-size="900,450" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://slimemoldtimemold.files.wordpress.com/2023/07/image.jpeg?w=300" data-large-file="https://slimemoldtimemold.files.wordpress.com/2023/07/image.jpeg?w=660" src="https://slimemoldtimemold.files.wordpress.com/2023/07/image.jpeg" alt=""></figure></div>






<p>There has been a similar increase in the diagnosis of<a href="https://www.psychologytoday.com/us/blog/balanced/202302/why-is-the-prevalence-of-adhd-increasing"> ADHD among adults</a>, “from 0.43 to 0.96 percent” between 2007 and 2016.</p>



<p>However, this does not necessarily mean that the <em>rate</em> of ADHD has increased, if e.g. awareness of ADHD has increased and therefore leads to more diagnoses.</p>







<h2><strong>What Could A Study Look Like?</strong></h2>



<p>Compared to other feats that psychology is accomplishing, finding out whether individual attention spans are declining appears to be of medium difficulty, so I’ll try to outline how this could be accomplished in three different ways:</p>



<ol>
<li>Develop a good instrument for measuring attention span (optionally just use a<a href="https://en.wikipedia.org/wiki/Continuous_performance_task"> continuous performance test</a>). Once one has a suitable instrument for measuring attention span, one can every year (or every second year) for a couple of years pick a random sample from the population (not of the same set of people, though, since attention span<a href="https://www.kcl.ac.uk/policy-institute/assets/how-people-focus-and-live-in-the-modern-information-environment.pdf"> increases with age</a>), e.g. via the internet if the test can be done online. One could then apply a<a href="https://en.wikipedia.org/wiki/Linear_trend_estimation"> linear trend estimation</a> or a fancier statistical technique I don’t know to find out whether attention spans have declined between the measurements.
<ol>
<li>This could be done relatively cheaply: Let’s say we collect 50 datapoints a year, from Mechanical Turk workers at $10/hr. A conservative estimate is that the test takes ~30 minutes to complete, so for three years the cost of the data would be 50⋅3⋅10$/h⋅0.5h=$750. It looks like there <a href="https://gitlab.pavlovia.org/demos/continuous_performance_test">are</a> <a href="https://github.com/search?q=continuous+performance+task">open-source implementations</a> <a href="https://github.com/NeuroanatomyAndConnectivity/ConjunctiveContinuousPerformanceTask">of the test</a> available (Conners’ CPT 3 costs <a href="https://storefront.mhs.com/collections/conners-cpt-3">$1.5k</a>), so the additional cost is for the researcher setting up the test and recruiting the participants, which could take ~30 hours, and another ~30 hours for analysing the data. So the total cost of the experiment would be, at an hourly wage of $15 for the researcher (come on, we can let a grad student do it), $750+60h⋅15$/h=$1650</li>
</ol>
</li>



<li>. Fudging upwards by taking the<a href="https://en.wikipedia.org/wik/Planning_fallacy"> planning fallacy</a> into account gives $2k for the experiment.</li>
</ol>



<ol start="2">
<li>Find someone who <a href="https://www.braintrain.com/cpt/">has been collecting data on attention span</a>, ask them for it nicely, and analyse that data.</li>



<li>Use the<a href="https://en.wikipedia.org/wiki/Control_group"> control groups</a> from studies testing the effect of interventions on attention as data and then perform a<a href="https://en.wikipedia.org/wiki/Meta-analysis"> meta-analysis</a>. A lot of studies use some variant of the CPT, I started collecting such studies in Appendix B.</li>
</ol>







<h2><strong>Conclusion</strong></h2>



<p>Given the amount of interest the question about shrinking attention spans has received, I was surprised to not find a knockdown study of the type I was looking for, and instead many different investigations that were either not <em>quite</em> answering the question I was asking or too shoddy (or murky) to be trusted. It seems likely to me that individual attention spans have declined (I’d give it ~70%), but I wouldn’t be surprised if the decline was relatively small, noisy &amp; dependent on specific tests.</p>



<p>So—why hasn’t anyone investigated this question to satisfaction yet? After all, it doesn’t seem to me to be extremely difficult to do (compared to<a href="https://en.wikipedia.org/wiki/Large_Hadron_Collider"> other</a><a href="https://en.wikipedia.org/wiki/Smallpox#Eradication"> things</a><a href="https://en.wikipedia.org/wiki/Apollo_11"> science</a><a href="https://en.wikipedia.org/wiki/Dolly_(sheep)"> has</a><a href="https://en.wikipedia.org/wiki/AlphaFold"> accomplished</a>), there is pretty clearly a lot of media attention on the question (so much so that<a href="https://www.bbc.com/news/health-38896790"> a likely incorrect number proliferates</a> far &amp; wide), it appears economically and strategically relevant to me (especially sustained attention is <em>probably</em> an important factor in knowledge work, I’d guess?) and it slots more or less into cognitive psychology.</p>



<p>I’m not sure why this hasn’t happened yet (and consider this text evidence for a partial violation of<a href="https://marginalrevolution.com/marginalrevolution/2015/04/tyler-cowens-three-laws.html"> Cowen’s 2nd law</a>—although, to be fair, the law doesn’t specify there needs to be a <em>good</em> literature on everything…). The reasons I can think of is that one would need to first develop a good test for determining attention span, which is some work in itself (or use the CPT); be relatively patient (since the test would need to be re-run at least twice with a &gt;1 year pause, for which the best grant structure might not exist); there are many partial investigations into the topic, making it appear like it’s solved; and perhaps there just aren’t enough cognitive psychologists around to investigate all the interesting questions that come up.</p>



<p>So I want to end with a call to action: If you have the capacity to study this problem, there is room for improvement in the existing literature! Attention spans could be important, it’s probably not hard to measure them, and many people claim that they’re declining, but are way too confident about it given the state of the evidence. False numbers are widely circulated, meaning that correct numbers might be cited even more widely. And it’s probably not even (that) hard!</p>



<p>Consider your incentives :-).</p>







<h2><strong>Appendix A: Claims That Attention Spans Have Been Declining</strong></h2>



<p>Most of these are either unsourced or cite<a href="https://dl.motamem.org/microsoft-attention-spans-research-report.pdf"> Gausby 2015</a><a href="https://www.bbc.com/news/health-38896790"> fallaciously</a> (which<a href="https://www.archemedx.com/learning-resources/learning-science/bradbury-attention-span-during-lectures-8-seconds-10-minutes-or-more/"> Bradbury 2016</a> conjectures to be the number of seconds spent on websites on average).</p>



<blockquote>
<p>Today, individuals are constantly on an information overload from both the quantity of information available and the speed of which information gets into the hands of individuals through advertising and multimedia. Attention deficits tend to be increasing as it is challenging to attract individuals and hold their attention long enough for people to read or watch messages such as work memos, advertisements, etc.</p>
<cite><em>—Carstens et al.,</em><a href="https://www.proquest.com/openview/4d7e41eb019558871327638cf5234990/1?pq-origsite=gscholar&amp;cbl=716332"><em> “Social Media Impact on Attention Span”</em></a><em> p. 2, 2018</em></cite></blockquote>







<blockquote>
<p>Big data plays an important role in the development of microlearning. In the age of big data, human’s attention span is decreasing. As per Hebert (1971), “what information consumes is rather obvious: it consumes the attention of its recipients. Hence a wealth of information creates a poverty of attention and a need to allocate that attention efficiently among the overabundance of information sources that might consume it” (p. 41). An example of short attention span in the age of big data can be found in the music industry, as per (Gauvin, 2017), the average time that passed before the audience would hear the vocals on any radio song was 23 s, today the average intro is just 5 s long. Wertz (2017) also suggested that 40% of users are likely to abandon a website if it does not load within three seconds or less. Furthermore, a survey (Gausby, 2015) conducted by Microsoft indicated that the average attention span of a human dropped from 12 to eight seconds, which means shorter than a goldfish. Given the average human attention span is decreasing, microlearning becomes more and more important because it emphasises short learning duration.</p>
<cite><em>—Leong et al., </em><a href="https://www.emerald.com/insight/content/doi/10.1108/JWAM-10-2020-0044/full/html"><em>“A review of the trend of microlearning”</em></a><em> p. 2, 2020</em></cite></blockquote>







<blockquote>
<p>Unfortunately, all too many of us are having “squirrel” days, according to Dr. Gloria Mark, a professor of informatics at the University of California, Irvine, who studies how digital media impacts our lives. In her new book,<a href="https://www.amazon.com/Attention-Span-Finding-Fighting-Distraction/dp/1335449418"> “Attention Span: A Groundbreaking Way to Restore Balance, Happiness and Productivity,”</a> Mark explained how decades of research has tracked the decline of the ability to focus.</p>



<p>“In 2004, we measured the average attention on a screen to be 2½ minutes,” Mark said. “Some years later, we found attention spans to be about 75 seconds. Now we find people can only pay attention to one screen for an average of 47 seconds.”</p>



<p>Not only do people concentrate for less than a minute on any one screen, Mark said, but when attention is diverted from an active work project, it also takes about 25 minutes to refocus on that task.</p>
<cite><em>—Sandee LaMotte,</em><a href="https://edition.cnn.com/2023/01/11/health/short-attention-span-wellness/index.html"><em> “Your attention span is shrinking, studies say. Here’s how to stay focused”</em></a><em>, 2023</em></cite></blockquote>







<blockquote>
<p>Tech-savvy users often say that the way the modern internet works has made it so that people’s attention spans are getting shorter every single day but the truth behind this story is rather tough to ascertain. However, recent data from<a href="https://www.similarweb.com/corp/reports/2020-digital-trends-lp/"> SimilarWeb</a> indicates that people definitely are suffering from shorter attention spans, and what’s more is that these attention spans are shortening at a pretty rapid pace when you take into account the numerous factors that are coming into play, all of which serve some kind of purpose in this trend.</p>



<p>If you look at the data for how long users spend on websites before navigating away, for the most part the trend has been that these times are remaining more or less stable on web based browsing, but if you look at the trend for mobile browsing between the years 2017 and 2019 you would see that there is a drop of about 11 seconds in the average time spent on a website. When you take into account the fact that mobile browsing is starting to become a lot more popular and in many ways has become the preferred form of browsing for people on the internet, the change is a lot more drastic.</p>
<cite><em>—Zia Muhammad,</em><a href="https://www.digitalinformationworld.com/2020/02/report-shows-that-attention-spans-are-shortening.html"><em> “Research Indicates That Attention Spans Are Shortening”</em></a><em>, 2020</em></cite></blockquote>







<blockquote>
<p>However, as much as technology can be used as an effective learning tool inside and outside the classroom, there’s no denying that one of the biggest challenges faced by educators today is the distraction posed by social media. <strong>Students are distracted by their phones during class, and even without that distraction, the time they spend on social media outside the classroom has an impact on their attention spans</strong>.</p>
<cite><em>—EU Business School,</em><a href="https://www.euruni.edu/blog/the-truth-about-decreasing-attention-spans-in-university-students/"><em> “The Truth about Decreasing Attention Spans in University Students”</em></a><em>, 2022</em></cite></blockquote>







<p>(No link given.)</p>



<blockquote>
<p>In 2015, a study commissioned by Microsoft and discussed in Time magazine found that the average attention span was in fact only 8 s. If indeed this is the case, then even participating in a 15-min lecture would be positively heroic. To place this in perspective, it was reported in the same Time article, that goldfish, of the piscine rather than snack variety, have an attention span of 9 s, one whole second greater than humans! It is perhaps rather premature to opt for an 8-s lecture format, as there are many caveats to the Time article, not the least of which is that no one knows how to actually measure a goldfish’s attention span. What has been measured is goldfish memory, which, according to researchers in the School of Psychology at the University of Plymouth, is actually quite good (7). Similarly the 8-s attention span for humans actually reflects the average time a person will spend on a web page before looking somewhere else.</p>
<cite><em>—Neil A. Bradbury,</em><a href="https://www.archemedx.com/learning-resources/learning-science/bradbury-attention-span-during-lectures-8-seconds-10-minutes-or-more/"><em> “Attention span during lectures: 8 seconds, 10 minutes, or more?”</em></a><em>, 2016</em></cite></blockquote>











<h2><strong>Appendix B: Studies for a Meta-Analysis</strong></h2>



<p>I’ll list the closest thing those studies have to a control group, list sorted by year.</p>



<h3><strong>Studies Using the CPT</strong></h3>



<ul>
<li><a href="https://academic.oup.com/schizophreniabulletin/article-pdf/22/4/643/5462174/22-4-643.pdf">What Determines Continuous Performance Task Performance?</a> (Maria J. O. van Asma/René P. Rombouts/Robert J. van den Bosch, 1996): Patient controls n=19 (mean age 29.5±8.1 years), normal controls n=20 (mean age 32.2±8.3 years). Tested with an unspecified digit related <em>CPT</em>.</li>



<li><a href="https://academic.oup.com/schizophreniabulletin/article-pdf/24/1/163/5309286/24-1-163.pdf">Performance of the Continuous Performance Test Among Community Samples</a> (Chuhsing K. Hsiao/Hai-Gwo Hwu/Li-Ling Hsiao/Wei J. Chen, 1998): n=115 students (aged 14±0.8 years) and n=345 adults (aged 41.3±13 years), randomly selected, from the<a href="https://en.wikipedia.org/wiki/Jinshan_District,_New_Taipei"> Jinshan District</a>. Tested using the <em>CPT 1-9</em>, both degraded and undegraded.</li>



<li><a href="https://www.academia.edu/download/47902451/s0924-9338_2899_2980711-120160808-9487-4azvq5.pdf">Decreased frontal activation in schizophrenics during stimulation with the Continuous Performance Test — a functional magnetic resonance imaging study</a> (H.-P. Volz/C. Gaser/F. Häger/R. Rzanny/J. Pönisch/H.-J. Mentzel/W.A. Kaiser/H. Sauer, 1999): n=20 German volunteers (28.2±5.7 years), tested using the <em>CPT-double-T-version</em>.</li>



<li><a href="https://www.researchgate.net/profile/John-Klaric/publication/304425329_Continuous_Performance_Test_Performance_in_a_Normative_Epidemiological_Sample/links/598261b9a6fdcc8b56f57e5c/Continuous-Performance-Test-Performance-in-a-Normative-Epidemiological-Sample.pdf">Continuous Performance Test Performance in a Normative Epidemiological</a> Sample (C. Keith Conners/Jeffery N. Epstein/Adrian Angold/John Klaric, 2002): n=816 children from North Carolina (9-17 years), tested using <em>Conners’ CPT</em>.</li>



<li><a href="https://www.researchgate.net/profile/Elizabeth-Costello/publication/9049493_Relations_Between_Continuous_Performance_Test_Performance_Measures_and_ADHD_Behaviors/links/551ac1520cf2fdce84373c26/Relations-Between-Continuous-Performance-Test-Performance-Measures-and-ADHD-Behaviors.pdf">Relations Between Continuous Performance Test Performance Measures and ADHD Behaviors</a> (Jeffery N. Epstein/Alaatin Erkanli/C. Keith Conners/John Klaric/Jane E. Costello/Adrian Angold, 2003): Epidemiological sample n=817 North Carolina children. Administered the Conners’ CPT.</li>



<li><a href="https://www.academia.edu/download/54248709/j.ics.2004.04.03220170825-21101-1dvtxr7.pdf">Longitudinal change of ERP during cued continuous performance test in child with attention-deficit/hyperactivity disorder</a> (Hisaki Ozaki/Hisao Maekawa/Satoshi Futakami/Shinji Okazaki, 2004): n=1 (yes, really), male child with ADHD, measurements at 9.6/10.6/11.6 years old with and without medication, tested using the <em>CPT-AX</em>.</li>



<li><a href="https://labs.psych.ucsb.edu/schooler/jonathan/sites/labs.psych.ucsb.edu.schooler.jonathan/files/pubs/stream_of_consciousness.pdf">Segmenting the stream of consciousness: The psychological correlates of temporal structures in the time series data of a continuous performance task</a> (Jonathan Smallwood/Merrill McSpadden/Bryan Luus/Jonathan Schooler, 2007): n=23 Canadian undergraduate students (aged 19-22). Tested using an unspecified <em>CPT</em> with the task of recognising specific digits as target stimuli. Probably not useful.</li>



<li><a href="https://www.pnas.org/content/106/37/15583.full/">Cognitive control in media multitaskers</a> (Anthony D. Wagner/Clifford Nass/Eyal Ophir, 2009): n=28 (?), tested with the <em>CPT-AX</em>.</li>



<li><a href="https://journals.sagepub.com/doi/pdf/10.1177/1087054708323019?casa_token=oYYJVOVBfmwAAAAA:QUTP5af1t1LHYdCmJoIvsIc_yDTX8l8L5QnL1-x9miVWEbwM02W-4fgpq4yGofQlF2zl5sh4-YgpXek">Measuring Several Aspects of Attention in One Test</a> (Iwona Kovalik-Gran/Jens Egeland, 2010): n=376 Norwegian patients, aged 14-77 (mean 32.9 years, standard deviation 13.8), either referred to Egeland or to the Vestfold Mental Health Care Trust, with various psychological disorders (57 without such disorders). Tested using <em>Conners’ CPT</em>. Scores for normal group not reported independently.</li>



<li><a href="https://bktimes.net/data/board_notice/1340624308-42.pdf">Association between urine cotinine levels, continuous performance test variables, and attention deficit hyperactivity disorder and learning disability symptoms in school-aged children</a> (B.-N. Kim/E.-J. Park/H.-J.Yoo/I.-H. Cho/J.-H. Lee/J. Hur/J.-W. Kim/M.-H. Park/M.-S. Shin/S.-B. Hong/S.-C. Cho/S.-K. Han/S. Park/S.-Y. Bhang/Y.-C. Hong, 2012): n=989 Korean children (mean age 9.1±0.7 years), tested with an unspecified <em>CPT</em>.</li>



<li><a href="https://link.springer.com/article/10.3758/s13414-012-0413-x">Sustaining visual attention in the face of distraction: a novel gradual-onset continuous performance task</a> (Joseph DeGutis/Michael Esterman/Monica Rosenberg/Sarah Noonan, 2013): n=29 healthy US-American participants (18-26 years old), tested with the therein developed <em>gradCPT</em>.</li>



<li><a href="https://journals.sagepub.com/doi/pdf/10.1177/0956797615594896?casa_token=_fGih5qwpdYAAAAA:PogTmgY1Kg2L-glPuM8RXl_4VNhfoESvGBYDWhWO0OznTktyHaFEpUqkwYawh8JotSdChvFdhFrHHKQ">Sustained attention across the lifespan in a sample of 10,000: Dissociating ability and strategy</a> (Francesca C. Fortenbaugh/Jeremy Wilmer/Joseph DeGutes/Kathryn Russo/Laura Germine/Mallory Grosso/Michael Esterman, 2015): n=10430 random internet sample, aged 10 to 70 (mean 26.07±11.77 years). Tested using the <em>gradCPT</em>.</li>



<li><a href="https://documentserver.uhasselt.be/bitstream/1942/22633/1/main-article-cognac.pdf">Recent <em>versus</em> chronic exposure to particulate matter air pollution in association with neurobehavioral performance in a panel study of primary schoolchildren</a> (Charlotte Vanpoucke/Eline B. Provost/Harry A. Roels/Karen Vrijens/Mineke K. Viaene/Nelly D. Saenen/Tim S. Nawrot/Wouter Lefebvre, 2016): n=310 Belgian school children (aged 10.2±1.3 years), tested using an unspecified <em>CPT</em> (reporting 593±51.2 msec as the outcome of the test (?)).</li>



<li><a href="https://link.springer.com/article/10.1186/s12888-016-0993-4">Linear and non-linear analyses of Conner’s Continuous Performance Test-II discriminate adult patients with attention deficit hyperactivity disorder from patients with mood and anxiety disorders</a> (Anita L. Hansen/Jan Øystein Berle/Ketil J. Oedegaard/Kristin Mjeldheim/Ole Bernt Fasmer/Vigdis Elin Giæver Syrstad/Wenche Førland, 2016): n=99 adult (aged 18-65 years) Norwegian patients in need of a diagnostic evaluation of ADHD, mood or anxiety disorders, tested with <em>Conners’ CPT II</em>.</li>



<li><a href="https://www.researchgate.net/profile/Esteban-Vaucheret-Paz/publication/323374989_Continuous_performance_test_in_children_with_intellectual_disability_and_attention_deficit_hyperactivity_disorder/links/61a50d90ee3e086e3d3a72a8/Continuous-performance-test-in-children-with-intellectual-disability-and-attention-deficit-hyperactivity-disorder.pdf">Continuous performance test in children with intellectual disability and attention deficit hyperactivity disorder</a> (Puga María Celeste/Vaucheret Paz Esteban/Leist Mariana/García Basalo María José/Baliarda Florencia/Ekonen Christy/Lascombes María IVsabel/Agostau Guillermo, 2018): n=122 Argentinian children (mean age 10 years) with ADHD and IQ&gt;80, tested with the <em>CPT II</em>.</li>



<li><a href="https://www.tandfonline.com/doi/abs/10.1080/23279095.2019.1570199">Reliability and validity of the Conners’ Continuous Performance Test</a> (Danielle Shaked/Lauren M. D. Faulkner/Kathryn Tolle/Carrington R. Wendell/Shari R. Waldstein/Robert J. Spencer, 2019): n=91 undergraduate psychology students (20.01±1.68 years), tested using <em>Conners’ CPT II</em>. Page 3 lists more retest studies for Conners’ CPT II.</li>



<li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8244585/">Poor Sleep Hygiene is Associated with Decreased Discrimination and Inattention on Continuous Performance Task in Doctor of Physical Therapy Students: A Cross-sectional Study</a> (Catherine F Siengsukon/Stacy Coffyn, 2020): n=50 Kansas students of Physical Therapy (age 24.18±1.6 years), tested with <em>Conners CPT 3</em>. (Raw CPT results not reported as far as I can tell, therefore probably useless :-|. But perhaps the authors can be contacted for the raw data…)</li>



<li><a href="https://link.springer.com/article/10.1186/s41235-021-00303-3">Do concerns about COVID-19 impair sustained attention</a>? (Jihyang Jun/Yi Ni Toh/Caitlin A. Sisk/Roger W. Remington/Vanessa G. Lee, 2021): n=161 participants (23±5.2 years), recruited online. No control group, but instead a correlational study. Tested using the <em>scene CPT</em>.</li>



<li><a href="https://journals.sagepub.com/doi/pdf/10.1177/1087054719829822?casa_token=hEKiWfMs9QkAAAAA:9u3lUp2R-Pu7v9pKLVHzkXpDiZRegEYVvUQMdBl8ATuVPAlQPoNUWRDYnbJ5bZfID1k5V6rzEpX_rjc">Individual Variability in Reaction Time and Prediction of Clinical Response to Methyphenidate in Adult ADHD: A Prospective Open Label Study Using Conners’ Continuous Performance Test II</a> (Jan Haavik/Jens Egeladn/Mats Fredriksen/Ole Bernt Fasmer, 2021): n=123 Norwegian participants with ADHD, tested using <em>Conners’ CPT II</em>.</li>
</ul>



<p>Furthermore, “<a href="https://acamh.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-7610.1993.tb01784.x?casa_token=2lrnDpDqclUAAAAA:OYObW0UahohP9yeg0XI8QDDWGO64t7oZEbPdynEkA_8J1HgD2XwWETWpFppQzyuBGRx5LXok7WZDn0xfFA">Is the Continuous Performance Task a Valuable Research Tool for use with Children with Attention-Deficit-Hyperactivity Disorder</a>” (Linda S. Siegel/Penny V. Corkum, 1993) p. 8-9 contains references to several studies from before 1993 using the CPT on children with ADHD.</p>







<h2><strong>Appendix C: How I Believed One Might Measure Attention Span Before I Found Out About The CPT</strong></h2>



<p>Before I found out about the Continuous Performance Test, I speculated about how to measure attention span:</p>



<p>(Note that I’m not a psychometrician, but I like speculating about things, so the ideas below might contain subtle and glaring mistakes. Noting them down here anyway because I might want to implement them at some point.)</p>



<p>It seems relatively easy to measure attention span with a power- or speed-test, via one of three methods:</p>



<ol>
<li>Present a stimulus, change the stimulus and let the test subject report the change in stimulus; this results in two numbers: the time between the stimulus being initially being presented and the time it was changed (let’s call this value t_change), and the time between the change of the stimulus and the reporting of the change (calling this value t_report). Performing this test with different value of t_change should result in different values of t_report. There is a t_change for which t_report falls over a threshold value, that t_change can be called the attention span.
<ol>
<li>This method has some disadvantages:
<ol>
<li>It needs a change in stimulus that requires selective attention to notice, but changing e.g. visual stimuli involves motion, which direct attention. (Idea: have a colored stimulus continuously changing color, and a reference color, once the stimulus has the reference color, the subject is supposed to report; avoiding sudden changes in visual stimuli.)</li>



<li>The method would require many samples to find the t_change for which t_report falls over the threshold value.</li>



<li>Performing the test multiple times in a row might induce mental fatigue, decreasing attention span</li>
</ol>
</li>
</ol>
</li>



<li>Let the test subject engage in a mentally draining exercise like the<a href="https://en.wikipedia.org/wiki/Stroop_test"> Stroop test</a> with some performance measure. I would the performance to decline over time, and one could define a threshold value at which the subject “is no longer paying attention”.</li>



<li>Let the subject observe a neutral stimulus while measuring some indicator of attention (such as arousal via skin conductivity or the<a href="https://en.wikipedia.org/wiki/Default_mode_network"> default mode network</a> being inactive), when the measured value falls under/over a threshold the subject has “lost attention”.
<ol>
<li>This method has the major disadvantage that it requires special equipment to perform.</li>
</ol>
</li>
</ol>



<p>Such an instrument would of course need to have different forms of<a href="https://en.wikipedia.org/wiki/Reliability_(psychometrics)"> reliability</a> and<a href="https://en.wikipedia.org/wiki/Validity_(psychometrics)"> validity</a>, and I think it would probably work best as a power test or a speed test.</p>



<p>I’m not sure how such a test would relate to standard<a href="https://en.wikipedia.org/wiki/Intelligence_quotient"> IQ tests</a>: would it simply measure a subpart of <em>g</em>, completely independent or just partially related to it?</p>
			</div><!-- .entry-content -->

</article><!-- #post-## -->

				<!-- .navigation -->
	
			
<!-- #comments -->

		
		</main><!-- #main -->
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple Vision Pro developer kit (158 pts)]]></title>
            <link>https://developer.apple.com/visionos/developer-kit/</link>
            <guid>36851535</guid>
            <pubDate>Mon, 24 Jul 2023 17:36:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://developer.apple.com/visionos/developer-kit/">https://developer.apple.com/visionos/developer-kit/</a>, See on <a href="https://news.ycombinator.com/item?id=36851535">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main" role="main">

		<section>
			<div>

						<h2>Apple&nbsp;Vision&nbsp;Pro developer&nbsp;kit</h2>
						<p>Have an innovative idea for an app or game for visionOS that requires building and testing on Vision&nbsp;Pro? Apply for a Vision&nbsp;Pro developer kit. This kit will help you deliver amazing spatial experiences by letting you quickly build, iterate, and&nbsp;test&nbsp;on&nbsp;Vision&nbsp;Pro.</p>
						
					</div>
			<div>
					<picture>
						<source srcset="https://developer.apple.com/visionos/developer-kit/images/vision-side_2x.webp" type="image/webp">
						<img src="https://developer.apple.com/visionos/developer-kit/images/vision-side_2x.png" width="100%" alt="">
					</picture>
				</div>
		</section>

		<div>

						<h3>How it works</h3>
						<p>We’ll loan you a Vision&nbsp;Pro developer kit to prepare your app for the launch of the new App&nbsp;Store on Vision&nbsp;Pro.</p>
						<p>You’ll also receive:</p>
						<ul>
							<li>Help setting up the device and onboarding.</li>
							<li>Check-ins with Apple experts for UI design and development guidance, and help refining your app.</li>
							<li>Two additional code-level support requests, so you can troubleshoot any issues with your code.</li>
						</ul>
						<p>This Apple-owned development device needs to be returned upon request.</p>

						<h3>How to apply</h3>
						<p>Submit a brief application for a Vision&nbsp;Pro developer kit. You’ll need to be an Account&nbsp;Holder in the Apple&nbsp;Developer Program, provide details about your team’s development skills and existing apps, and agree to the terms and conditions. Applications will be reviewed and priority will be given to applicants creating an app that takes advantage of visionOS features and&nbsp;capabilities.</p>
						
						
					</div>
		
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Transformer Attention is off by one (851 pts)]]></title>
            <link>https://www.evanmiller.org/attention-is-off-by-one.html</link>
            <guid>36851494</guid>
            <pubDate>Mon, 24 Jul 2023 17:33:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.evanmiller.org/attention-is-off-by-one.html">https://www.evanmiller.org/attention-is-off-by-one.html</a>, See on <a href="https://news.ycombinator.com/item?id=36851494">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">

<p>By <a href="https://www.evanmiller.org/">Evan Miller</a></p>

<p><em>July 24, 2023</em></p>





<blockquote>
    <hr>
<p>
About which one cannot speak, one must pass over in silence. –Wittgenstein
</p>

    <hr>
</blockquote>

<p>
Do you see the off-by-one error in this formula?
</p><p>

\[
\textrm{Attention}(Q, K, V) = \textrm{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V
\]

</p><p>
The attention formula is the central equation of modern AI,
but there’s a bug in it that has been driving me nuts the last week. I tried
writing a serious-looking research paper about the bug and my proposed fix, but
I lost a series of pitched battles against Pytorch and <code>biblatex</code>,
so I figured I’d just write a blog post instead. (History is written by the
winners; blogs are written by…)
</p>

<p>
In this post I’ll explain how (IMHO) the current generation of AI models have
an off-by-one error in a crucial place, and it’s making everyone’s Transformer
models needlessly difficult to compress and deploy. Consider this an opinion
piece, but if anyone out there on the Internet wants to run some experiments and
prove me right, we can collaborate and call ourselves scientists.
</p>

<h2>
It’s All About Outliers
</h2>

<p>
First let’s talk about why this off-by-one error matters. <em>ChatGPT works
great, what’s your problem bro?</em> I first caught the scent of something
amiss whilst minding my business and perusing research papers on quantization,
the technique by which LLM Edgers <a href="https://justine.lol/mmap/">cram</a>
big models onto Mac Minis and <a href="https://github.com/ggerganov/llama.cpp/issues/2164">Rasberry Pis</a> and
jailbroken home thermostats. 
    Everybody in AI is limited by
    RAM, so the less RAM you use the more cool stuff you can do, both <a href="https://arxiv.org/abs/2211.05102">Up
        There In The Cloud</a> and down here on the edge. LLMs have billions of weights,
and if we can make these weights tighten their haunches and suck in their
paunches, we can compose better sonnets or plagiarize superior essays or
accelerate the end of days, whatever your personal motivation for using
language may be.
</p>

<p>
RAM stores information. This sounds like a tautology, but hang with me.
<a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">Information is negative log-probability</a>,
and is how many bits we need to store things. If a stream of numbers is highly
predictable, for example is always contained in a limited range, we need fewer
bits to store them. If a stream of numbers is not predictable, like once in a
blue moon a mega-number shows up, we need more binary digits to encode the
Colossus.
</p>

<p>
This is what’s been happening in LLMs – for reasons that are only partially
understood, Transformer models contain these outlier weights and are
emitting Black Swan mega-activations that are much, much, much larger, like
orders of magnitude larger, than their peers. <em>But no one can get rid of them</em>;
the megalodons seem to be critical to the operation of these models, and their
existence is contrary to everything we thought we knew about neural networks
prior to building ones that worked so well. <a href="https://arxiv.org/abs/2105.06990">Many</a> <a href="https://arxiv.org/abs/2109.12948">papers</a> have been written about
these outlier values, and people have been <a href="https://arxiv.org/abs/1909.05840">cooking</a> <a href="https://arxiv.org/abs/2101.01321">up</a> <a href="https://arxiv.org/abs/2208.07339">all</a> <a href="https://arxiv.org/abs/2211.10438">kinds</a> <a href="https://arxiv.org/abs/1910.06188">of</a> bit-burning
schemes to encode them with fewer ones and zeroes, because right now we get
pretty gnarly performance degradation with vanilla scale-and-bias integer
quantization. <a href="https://github.com/ggerganov">Georgi</a> can tell you more; I’m getting ahead of myself.
</p>

<p>
The best analysis of all of this comes from a team at Qualcomm AI Research in this paper:
<strong><a href="https://arxiv.org/abs/2306.12929">Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing</a></strong>. The authors traced the existence of these outlier values to the
attention mechanism’s 
    softmax function,
the seemingly innocent exponentiator that no one thought capable of such
kurtotic barbarities. The researchers came <em>this close</em> to finding the
off-by-one error, like killer-in-the-closet close, but they must all be on
summer vacation in Italy as none of them are responding to my email overtures, and so I
must appeal to the international community of scholars the old-fashioned way.
</p>

<p>
If you read the linked paper, just ignore their proposals. Sounds harsh, but
hear me out. The clipped softmax comes with a wheel-spinning zero gradient, and
their gated attention proposal, while workable, introduces millions of new
parameters to solve what is really just a failure to increment. There’s a
simple and hindsight-obvious solution here that, from all of my reading, no one
has thought to try.
</p>

<p>
All right, I’ve talked up this silly error enough. I’ve alluded to its
location. Let’s talk about the 
    <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax function</a>
and why it’s
not-quite-the-right tool for the job when it comes to attention.
</p>


<h2>
The Trouble With Softmax
</h2>

<p>
Now to explain the bug you really have to understand what the attention
mechanism is trying to do. Most numerical bugs are programmers implementing
equations incorrectly. However, when you’re dealing not with bad code, but with
bad math, you need to understand where that equation comes from and what it’s
supposed to be doing before you have any hope of fixing it.
</p>

<p>
I had to read like 50 arXiV papers to understand all this, and probably I
should have taken a Udemy course or binge-watched Andrej Karpathy’s YouTube
<a href="https://www.youtube.com/c/AndrejKarpathy">channel</a> instead, but
let’s start with what is called the input embedding, which is a floating-point
vector that represents a word in the input string.
</p>

<p>
This vector seems to get taller every model year, for example the recent <a href="https://ai.meta.com/llama/">LLaMA
    2 model from Meta</a> uses an embedding vector of length 3,204, which works out to
6KB+ in half-precision floating-point, just to represent <em>one word</em> in
the vocabulary, which typically contains 30,000 - 50,000 entries.
</p>

<p>
Now if you’re a memory-miserly C programmer <a href="https://www.evanmiller.org/you-cant-dig-upwards.html">like me</a>, you might wonder, why in
the world are these AI goobers using 6KB to represent something that ought to
take, like 2 bytes tops? If their vocabulary is less than \(2^{16}\)=65,384, we
only need 16 bits to represent an entry, yeah?
</p>

<p>
Well, here is what the Transformer is actually doing: it <em>transforms</em>
(eh?) that input vector to an output vector <em>of the same size</em>, and that
final 6KB output vector needs to encode <em>absolutely everything needed to
predict the token after the current one</em>. The job of each layer of the
Transformer is quite literally adding information to the original, single-word
vector. This is where the residual (née skip) connections come in: all of the
attention machinery is just adding supplementary material to that original two
bytes’ worth of information, analyzing the larger context to indicate, for
instance, that the word <em>pupil</em> is referring to a student, and not to the
hole in your eye. Repeat the attention mechanism a few dozen times and you’ve mastered
the English language and all its vasty contents.
</p>

<p>
Now the final step of Transformer is to multiply this output vector by a
rectangular matrix, and cram the resulting vocabulary-length vector into a
softmax, treating those exponentiated outputs as next-token probabilities. This
is reasonable, but everyone knows it’s not quite right, as no sane and
respected model out there regards those output probabilities as correct, but
instead every implementation and its sister uses a <a href="https://arxiv.org/abs/2007.14966">sampling mechanism</a> to hide
the fact that softmax is over-representing low probabilities.
</p>

<p>
That’s all fine and workable; softmax in the output step gives us a gradient
for every word in the vocabulary, and it’s a reasonable choice until something
better comes along.
</p>

<p>
But what I want to argue is that sauce for the goose shouldn’t be slathered on
the gander; the Transformer’s <em>output</em> softmax serves a very different
purpose from the attention mechanism’s <em>internal</em> softmax, and we’d all
do well to get rid of the latter, or at least prop up its denominator with something
handy ⛱️.
</p>

<h3>An exponential peg…</h3>

<p>
So what is softmax? It started out in statistical mechanics as a way to
predict the distribution of states based on their energy levels:
</p><p>

\[
p_i \propto \exp\left(-\frac{\varepsilon_i}{kT}\right)
\]

</p><p>
Then the economists got a hold of it and realized that if the noise term in
people’s otherwise linear utility functions happened to follow a <a href="https://en.wikipedia.org/wiki/Gumbel_distribution">Gumbel
distribution</a> (doesn’t yours?), then the probability someone will choose an
item will be proportional to the exponent of the utility inputs:
</p><p>

\[
\Pr(Y_i=k)=\frac{\exp(X_i \beta_k)}{\sum_j \exp(X_i \beta_j)}
\]

</p><p>
This gave softmax a life in multinomial logistic functions; this is where I got
to know the old chap, as in I hand-derived the Hessian of this sucker in grad
school and coded it up to run on GPUs using linear fixed effects, which to my
knowledge no one else has been foolish enough to attempt before or since. I
just mention this because I know the softmax function better than I know many
of my humanoid friends, and I can recognize when it’s being used for things it
oughtn’t.
</p>

<p>
Softmax is a kind of cheat code to map real-valued numbers to probabilities
that sum to one. In physics, it works quite well; in economics, it’s a little
bit of a lie, but by the time it got to machine learning, it just became a
thing that seemed to work whenever a discrete choice was involved. Softmax
the vector and pick something, all right?
</p><p>

\[
(\textrm{softmax(x)})_i =\frac{\exp(x_i)}{\sum_j \exp(x_j)}
\]

</p><p>
Here’s the core mechanic of softmax: <em>it forces a choice among competing
alternatives</em>, whether it’s particles picking an energy state or consumers
choosing a car. That is, if a softmax mechanism <em>doesn’t want to choose
anything at all</em>, softmax will require modification, or else we would
expect the softmax to produce distortions once it encounters actual data.
</p>

<p>
In the context of LLMs, one of those distortions is to heavily weight
non-semantic tokens (commas and such), and those beefy weights become those
difficult-to-compress outlier values that make Life on the Edge harder
than it should be. <em>The Qualcomm AI researchers found that 97%+ of outlier
activations in LLMs occur in whitespace and punctuation positions.</em>
Something’s fishy around here… and I thought I ordered the chicken.
</p>

<h3>
    …in a linear hole
</h3>

<p>
Let’s dive in to how softmax is used inside of attention and see exactly where
it goes wrong. Here’s the equation again:
</p><p>

\[
\textrm{Attention}(Q, K, V) = \textrm{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V
\]

</p><p>
Breaking it down: in decoder-only models (i.e., everything since ChatGPT), \(Q\),
\(K\), and \(V\) all originate from the same input sequence. They’re not identical,
as they’ve been projected in different ways along the way, but in each layer
they all start with the same annotated (added-to) embedding vector.
</p>

<p>
Now: \(QK^T\) is looking for correlations between token (embedding) vectors in
different positions, essentially building up a square matrix of correlation
(dot-product scaled by \(1/\sqrt{d}\)) values, where each column and row corresponds to a token
position. Then each row of this square matrix is softmaxed, with the
resulting probabilities used as a mixing function for the value vectors in the
\(V\) matrix. The probability-mixed \(V\) gets added to the input vector, with the
sum passed on down the neural network for further processing.
</p>

<p>
Multi-head attention goes through this process several times, in parallel, per
layer. Essentially it divides up the embedding vector, and each head uses
information in the entire vector to annotate one (non-overlapping) segment of
the output vector. If you were confused by the Concatenation operation in the
<a href="https://arxiv.org/abs/1706.03762">original Transformer paper</a>, that’s all that’s happening: Head 1 adds
information to Segment 1, Head 2 adds information to Segment 2, and so on.
</p>

<p>
<em>The problem with using softmax is that it forces each attention head to
make an annotation, even if it has no information to add to the output
vector.</em> Using softmax to choose among discrete alternatives is great;
using it for optional annotation (i.e. as input into addition) is, like,
not cool, man. The problem here is exacerbated with multi-head attention, as a
specialized head is more likely to want to “pass” than a general-purpose one.
These attention heads are needlessly noisy, a deafening democracy where
abstention is disallowed.
</p>

<p>
Now it’s possible that softmax should be replaced wholesale, but it’s worked
pretty well for the most part, except for this one wee little bug that prevents
attention heads from saying nothing. So I propose a very small tweak on which I
am willing to stake all future Internet claims to being correct. The tweak
is so small, yet so obvious, and it’s been sitting here under everyone’s noses
ever since attention was <a href="https://arxiv.org/abs/1409.0473">invented</a> (2014).
</p><p>
Are you ready?
</p>

<p>
Are you <em>ready-ready</em>?
</p>

<p>
I can’t hear you.
</p>


<h2>
Softmax One and Quiet Attention
</h2>

<p>
All right, here it is and you are welcome, the long-awaited Softmax Super-Mod that
is soon to set the LLM hacker channels aflame:
</p><p>

\[
(\textrm{softmax}_1(x))_i = \frac{\exp(x_i)}{1+\sum_j \exp(x_j)}
\]

</p><p>
Bit of a let-down, eh? All I did was added one to the denominator. This lets
the vector as a whole tend to zero if it wants, but otherwise just shrinks the
values by a small amount, a shrinkage which will be made up for during
normalization, which happens right after attention.
</p>

<p>
The key difference is in the negative limit, when entries in \(x\) are significantly
less than zero and the model is trying to avoid an annotation altogether.
Compare the limiting behavior of the original softmax
</p><p>

\[
\lim_{x_1 \to -\infty} \ldots
\lim_{x_k \to -\infty} 
(\textrm{softmax}(x))_i = \frac{1}{k} \gt 0
\]

</p><p>
to that of the new and improved softmax<sub>1</sub>:
</p><p>

\[
\lim_{x_1 \to -\infty} \ldots
\lim_{x_k \to -\infty} 
(\textrm{softmax}_1(x))_i = 0
\]

</p><p>
Vanilla softmax will always emit the same total weight; softmax<sub>1</sub>
mostly looks the same, but comes with an escape hatch in the negative orthant.
To be clear: the core issue here is <em>mathematical</em> and not
<em>numerical</em> in nature. Extra precision won’t save the softmax; all
Transformers are affected.
</p>

<p>
You’ll notice a couple of other things about softmax<sub>1</sub>. The
derivative is positive, so we always have a non-zero gradient, and its sum is
between zero and one, so the output isn’t out of control. The function maintains the property
</p><p>

\[
\frac{(\textrm{softmax}_1(x))_i}{(\textrm{softmax}_1(x))_j} 
= \frac{(\textrm{softmax}(x))_i}{(\textrm{softmax}(x))_j} 
= \frac{\exp(x_i)}{\exp(x_j)} \quad \forall \ i, j
\]

</p><p>i.e. relative values in the output vector are unchanged.</p>

<p>
Originally I wanted to call this
function <em>ghostmax</em>, as you can think of there being an extra
zero-valued entry in \(x\) (as \(\exp(0)=1\)), as well as a zero vector in the
\(V\) matrix that attenuates the result. But I didn’t want to scare anyone.
</p>

<p>
Even though softmax<sub>1</sub> is facially quite boring, I’m 99.44% sure that it will
resolve the outlier feedback loop that’s making quantization the subject of
cascades of research. If you want to run some experiments and prove me right,
<a href="https://twitter.com/EvMill">DM me on Twitter</a> and we’ll get a
paper going.
</p>

<p>
We can call the improved mechanism <em>QuietAttention</em>, as it allows
attention heads to keep their yaps shut. So may I propose to a receptive public
</p><p>

\[
\textrm{QuietAttention}(Q, K, V) := \textrm{softmax}_1 \left(\frac{QK^T}{\sqrt{d}}\right)V
\]

</p><p>
I think a test could be hacked together fairly quickly: if you prefix every
input context with a zero vector, and ensure that your neural network of choice
doesn’t add any bias (including with the positional encoding), then the zero
should pass through unaltered and will have the effect of adding unity to every
subsequent softmax denominator; that way you won’t lose your mind mucking with 
gradient code. I <em>think</em> this could be done with a LLaMA model that uses
fixed embeddings and a special prefix token, but 
I’d very much like to get
back to my non-existent hobbies, and
I’ve already spent more time
on this problem than my therapist needs to know about.
</p>

<p>
You’d still have to re-train the model, so don’t try this on an RPi just yet.
But do let me know how those weight kurtoses and activation infinity norms are
looking after a few runs. I’m thinking those numbers will make for a handsome
table in a soon-to-be influential arXiV paper, either when those Qualcomm AI
researchers step off the plane from Italy, or someone in an LLM hacker channel
figures out <code>biblatex</code>, whichever happens first.
</p>



<hr>

<p><em>You’re reading <a href="https://www.evanmiller.org/">evanmiller.org</a>, a random collection of math, tech, and musings. If you liked this you might also enjoy:
    </em></p><ul><em>
        <li><a href="https://www.evanmiller.org/how-not-to-sort-by-average-rating.html">How Not To Sort By Average Rating</a>
        </li><li><a href="https://www.evanmiller.org/winkel-tripel-warping-trouble.html">Winkel Tripel Warping Trouble</a>
        </li><li><a href="https://www.evanmiller.org/you-cant-spell-cuped-without-frisch-waugh-lovell.html">You Can’t Spell CUPED Without Frisch-Waugh-Lovell</a>
        </li><li><a href="https://www.evanmiller.org/evaluating-splatoons-ranking-system.html">Evaluating Splatoon’s Ranking System</a>
    </li></em></ul>

<hr>
<p><em>Get new articles as they’re published, via <a href="https://www.linkedin.com/in/evanmmiller/">LinkedIn</a>, <a href="https://twitter.com/EvMill">Twitter</a>, or <a href="https://www.evanmiller.org/news.xml">RSS</a>.</em></p>

<hr>

<p><em>Want to look for statistical patterns in your MySQL, PostgreSQL, or SQLite database? My desktop statistics software <strong><a href="https://www.wizardmac.com/">Wizard</a></strong> can help you analyze <strong>more data in less time</strong> and <strong>communicate discoveries visually</strong> without spending days struggling with pointless command syntax. Check it out!</em></p>
<p><a href="https://www.wizardmac.com/"><img height="128" width="128" src="https://www.evanmiller.org/images/index/wizard2.png"></a><br>
<strong><a href="https://www.wizardmac.com/">Wizard</a></strong><br><span>Statistics the Mac way</span>
</p>


<hr>

<p><a href="https://www.evanmiller.org/">Back to Evan Miller’s home page</a> 
– <a href="https://www.evanmiller.org/news.xml">Subscribe to RSS</a>
– <a href="https://www.linkedin.com/in/evanmmiller/">LinkedIn</a>
– <a href="https://twitter.com/EvMill">Twitter</a> 
</p>

<hr>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[40 years ago yesterday Air Canada Flight 143 ran out of fuel mid-flight (613 pts)]]></title>
            <link>https://www.damninteresting.com/the-gimli-glider/</link>
            <guid>36850111</guid>
            <pubDate>Mon, 24 Jul 2023 16:03:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.damninteresting.com/the-gimli-glider/">https://www.damninteresting.com/the-gimli-glider/</a>, See on <a href="https://news.ycombinator.com/item?id=36850111">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

                    

                    <p><span>

                                                	<span>Long-Form:</span>
                            <span>When a botched imperial-to-metric conversion left a commercial jet with insufficient fuel, pilots had to improvise.<br></span>
                            <span>Written by <a href="https://www.damninteresting.com/contributors/alan-bellows/">Alan Bellows</a></span>
                        
                                                	•
                        	<span>Non-Fiction</span>
                        
		        					        		•
			        		<span>November 2007</span>
		        		                    </span>


                </p></div><div>

		            				<article>
											<p>
			© 2007 All Rights Reserved. Do not distribute or repurpose this work without written permission from the copyright holder(s).
	</p>
<p>
	Printed from https://www.damninteresting.com/the-gimli-glider/<br>
</p>
												
		                
						
						
											
					<p>“Holy shit.”</p>
<p>Inside the cockpit of the cruising airliner, Captain Bob Pearson was understandably alarmed at the out-of-the-ordinary beeps that were chiming from his flight computer.  On the control panel, an amber <i>low fuel pressure</i> warning lamp lit up to punctuate the audio alarm.  </p>
<p>First Officer Maurice Quintal, copilot of Air Canada Flight 143, checked the indicator light to determine the cause of the computer’s complaints.  “Something’s wrong with the fuel pump,” he reported.</p>
<p>The mustachioed Captain Pearson pulled out the trusty Boeing handbook, his fingers dashing through the pages to find the specifics of the warning.  To his relief, the troubleshooting chart indicated that the situation was not as perilous as it might seem: the fuel pump in the left wing tank was signaling a problem, a minor issue considering that gravity would continue to feed the engines even if the pump failed.  </p>
<p>“You know,” he commented to Copilot Quintal, “I would not take this air…”  He trailed off as the computer blurted out another four beeps, and the indicator panel lit up like a Christmas tree decorated with bad news. “Oh <i>fuck</i>,” Pearson lamented, “we’ve got to go to Winnipeg.”
</p>
<p>
The date was 23 July 1983, and although the fuel pressure warnings were not the flight’s first mechanical frustrations, they were certainly the most distressing so far.  When pilots Pearson and Quintal had arrived for their shift earlier that day, they had been notified that the plane’s fuel gauges were non-functional due to a fault in the Fuel Quantity Indicator System (FQIS).  Even worse, the component required to repair it could not be delivered until later that evening.  </p>
<p>Rather than canceling the flight, Captain Pearson instructed the engineers to check the fuel level manually.  The four-month-old 767 was a state-of-the-art machine with state-of-the-art glitches, and FQIS issues were becoming a common complaint.  Several independent dripstick checks later, the fuel hosers were satisfied that sufficient fuel was loaded, and they advised Air Canada Flight 143 to take off.  The airliner departed from Montreal at 5:48pm eastern time with their sixty-one passengers. At 6:58pm they made a brief scheduled stopover in Ottawa, where engineers once again checked the fuel dripsticks⁠— just to be safe.  </p>
<p>It was just after 8:00pm central time that the cockpit computer began its string of inexplicable beeps and warning lights.  As the jumbo jet crossed the Canadian countryside at 41,000 feet, Copilot Quintal thumbed through the 767 handbook to ascertain the nature of the airplane’s problem.  “They don’t say anything if you’ve got more than one though, main tank, eh?” he said to Captain Pearson, as well as the flight engineer who had joined them.  “Like there’s two pumps, they don’t say anything about only one, eh?”  According to the computer’s calculations there should have been plenty of fuel remaining, but multiple fuel pumps were indicating pressure problems.  The flummoxed flight crew decided to divert to the nearby Winnipeg airport as a precaution, and alerted Air Traffic Control (ATC) of their intent.</p>
<figure><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAMAAAAoyzS7AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAAZQTFRFgYGBAAAAqeg2zgAAAAxJREFUeNpiYAAIMAAAAgABT21Z4QAAAABJRU5ErkJggg==" data-height-ratio="0.66451612903226" data-lazy-load-src="//damn-8791.kxcdn.com/wp-content/uploads/2007/11/767_cockpit.jpg" alt="The cockpit of a typical Boeing 767" title="The cockpit of a typical Boeing 767"></p><figcaption>The cockpit of a typical Boeing 767</figcaption></figure>
<p>“Air Canada 143 cleared present position direct Winnipeg,” the tower responded.  “We’re landing runway 31.  You’re cleared to maintain six thousand descent your discretion.”  Pearson and Quintal updated their flight computer with the new heading and destination.  “Air Canada 143 did you want any <i>assistance</i>?” the traffic controller inquired, where “assistance” is an aeronautic euphemism for a reception from the fire brigade.</p>
<p>“For the moment we won’t require any assistance,” Pearson responded. </p>
<p>The flight engineer struggled to assess the situation.  “You’ve got nothing in the center tanks, eh?” he inquired of the captain.</p>
<p>“No, we ran the pumps,” the captain replied, referring to an earlier attempt to transfer fuel from another tank.  “Uh, let’s put them back on again.”  Within moments, several more warning lights snapped on in quick succession.  “Holy shit.” </p>
<p>“God damn,” Quintal remarked, “they’re all going out, eh?  How about uh…”</p>
<p>“All the lights are on,” Pearson observed soberly, as the array of low fuel pressure indicators glowed with incandescent urgency.  The captain summoned the in-charge flight attendant to the cockpit and apprised him of the situation, but his summary was outdated mere moments later.  The flight computer bellowed out a flamboyant <i>BONG!</i> which none of the men present could recall having heard before.  </p>
<p>“Okay,” the captain observed upon examining the instruments, “We’ve lost the left engine.”</p>
<p>“Okay, what…will we do?” Quintal replied.  “Want the checklist now?” </p>
<p>“Checklist, yeah.”</p>
<p>The pilots began preparations for a delicate-but-very-doable single-engine landing, and Copilot Quintal contacted Winnipeg tower to request the previously offered “assistance”.  It was becoming increasingly clear that the plane’s problems lay not in its machinery, but in its fuel.  The men, however, were unsure of exactly what was amiss.</p>
<p>Following two minutes of uneventful descent, the ever-present vibrations in the deck were disrupted by an almost imperceptible shudder, and  the white-noise hum of the remaining jet engine faded away with a long and melancholy mechanical sigh.  The gauges and monitors of the control panel⁠— which had been so animated with anxiety mere moments before⁠— fell dark.  Absent the usual murmur of the twin turbofans, an unsettling silence hung heavy in the air.</p>
<p>“How come I have no instruments?” Captain Pearson wondered aloud, though the answer lingered mockingly in the cockpit’s uncharacteristic quiet.  The airliner’s generators and hydraulic systems required at least a single functioning engine in order to operate, without which there was no electricity for the computer, and no power to manipulate the ailerons, rudder, and elevator.  In effect, the highly advanced flying machine had roughly the maneuverability of a flying brick, with barely enough instrumentation to monitor its slow dive towards the Earth.  After a few ponderous moments, however, the automatic emergency systems twitched into action.  Onboard batteries revived a few of the most critical instruments, and a door popped open on the plane’s underbelly to expose a ram-air turbine (RAT) designed to provide limited emergency hydraulic support.</p>
<p>“143,” the radio crackled, “We have lost your transponder return right now.”</p>
<figure><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAMAAAAoyzS7AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAAZQTFRFgYGBAAAAqeg2zgAAAAxJREFUeNpiYAAIMAAAAgABT21Z4QAAAABJRU5ErkJggg==" data-height-ratio="1.4622641509434" data-lazy-load-src="//damn-8791.kxcdn.com/wp-content/uploads/2007/11/bob_pearson.jpg" alt="Captain Bob Pearson" title="Captain Bob Pearson"></p><figcaption>Captain Bob Pearson</figcaption></figure>
<p>Captain Pearson was beginning to grasp the true gravity of the situation.  “Center, one-four-three, this is a mayday and we require a vector onto the closest available runway.  We are out of 22,000 feet on… both engines have failed due to looks like fuel starvation and we are on emergency instruments and can only give you limited headings.  Information⁠— we are heading two five zero now, please give us a vector to the nearest runway.”</p>
<p>“143 we copy all that okay.  We have lost your transponder return and attempting to pick up your target now… we have it now, just stand by on the two fifty heading.”</p>
<p>“Ah, roger.”</p>
<p>After repeated unsuccessful attempts to restart the stalled engines, Pearson and Quintal once again consulted the 767 emergency manual, this time for advice on an unpowered landing.  Much to their dismay, no such section existed, presumably because a simultaneous engine failure had been too ridiculous for Boeing engineers to contemplate.  The pilots sat anxiously in their darkened cockpit and monitored the plane’s slow and silent descent using a handful of analog instruments based on pre-WW2 technology: a magnetic compass, an artificial horizon, an airspeed indicator, and an altimeter.  </p>
<p>The traffic controller in the tower at Winnipeg advised the flight officers of their options.  “143 we show you at sixty-five miles from Winnipeg and approximately forty-five miles from Gimli.”</p>
<p>“Okay,” Pearson responded, “is there emergency equipment at Gimli?</p>
<p>“Negative emergency equipment at all.  Just one runway available I believe and no control and no information on it.”</p>
<p>“We’d prefer Winnipeg then.”  </p>
<p>In a stroke of profound luck, Captain Pearson was an accomplished glider pilot, a skill which afforded him with some sense of the vehicle’s glide capabilities.  He applied his expertise to estimate the plane’s best glide ratio speed, but having neither a vertical speed indicator nor a view of the landscape through the clouds, he was unaware that Winnipeg was well beyond the reach of their gravity-gripped flight equipment.</p>
<p>Back in the passenger compartment, the in-charge flight attendant radiated counterfeit calm as he informed the plane’s sixty-one passengers of the situation, and instructed them in the subtle art of not freaking out during an in-flight emergency.  In the meantime, crew members directed able-bodied men to move into the rows alongside the exit doors, then solemnly buckled into their own seats.  Many of the crew members were keenly aware that jumbo jets such as theirs were not designed for dead-stick flight⁠— let alone dead-stick landings.  In all probability, their inevitable confrontation with the Earth would not be an improvement on their current situation.</p>
<p>As the impromptu glider emerged from the ceiling of clouds and obtained a view of the landscape, the pilots quickly realized that the plane was shedding altitude far too quickly to have any chance of reaching Winnipeg.  Copilot Quintal confirmed this conclusion using radar data from Air Traffic Control.  </p>
<p>“How far are we from Gimli?” Pearson inquired of the Winnipeg tower.</p>
<figure><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAMAAAAoyzS7AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAAZQTFRFgYGBAAAAqeg2zgAAAAxJREFUeNpiYAAIMAAAAgABT21Z4QAAAABJRU5ErkJggg==" data-height-ratio="0.75806451612903" data-lazy-load-src="//damn-8791.kxcdn.com/wp-content/uploads/2007/11/gimli_glider_map.jpg" alt="" title=""></p></figure>
<p>“You are approximately twelve miles from Gimli right now.”</p>
<p>Air Traffic Control had no specific data on the remote airstrip, but in another stroke of luck, First Officer Quintal had been stationed there during his time in the Royal Canadian Air Force.  Lacking any feasible alternative, the copilot recommended they drop in on his old friends from the service.  He was not aware, however, that the facility had since been converted into a public airport; nor did any of the men know that one of its two runways had been decommissioned and carved up for use as a racetrack.</p>
<p>As Flight 143 fell below the Air Traffic Control radar range, the tower grimly requested a count of the souls on board.  As Pearson began his long final approach, he scraped up a bit of optimism as he updated Winnipeg tower on their status.  “We have the field in sight,” he reported, “and we feel we’re in good shape.”  </p>
<p>On the ground at Gimli, it was Family Day at the local racetrack.  Sports Racers buzzed along the decommissioned runway as spectators cheered from the sidelines.  A collection of campers at the end of the airstrip soaked up the summer Saturday evening as their dinners sizzled on assorted barbecues.  Without the jet engines to announce the airliner’s approach, the people were oblivious of the 132-ton Boeing behemoth which was bearing down on them.</p>
<p>In the cockpit, Copilot Quintal activated the manual landing gear controls, and the two main gears lowered and locked.  The nose gear, however, dangled limply from its housing.  For Captain Pearson, the flight controls were becoming increasingly difficult to operate.  The effectiveness of the emergency RAT was governed by the speed of the wind slipping around the fuselage, so as the plane gradually slowed, the hydraulic assistance was diminishing.  Nevertheless, Pearson needed to sharply reduce the speed and altitude of his approach, otherwise the 767 would overshoot the tarmac; and without engines there would be no opportunity for a second try.  Ordinarily an airline pilot would apply some combination of flaps and aerobrakes, but none of these systems were functioning on Pearson’s crippled craft.  </p>
<p>Lacking a more orthodox option, Captain Pearson cranked the control wheel to the right and gave the left rudder pedal a firm stomp.  The criss-crossed controls tilted the deck to the right as one wing dipped toward the ground, providing the passengers with a lovely view of the golf course on one side, and nothing but blue sky on the other.  The fuselage also rotated its heading to the left, becoming diagonal relative to its direction of travel.  Such <i>forward-slip</i> maneuvers were sometimes used on small planes and gliders, but the curve-ballish air acrobatics were unheard of with a jumbo jet.  The airplane indeed decelerated, but the reduced airspeed robbed the controls of even more precious hydraulic pressure, requiring Pearson to apply monumental force to try to straighten the slip.  </p>
<figure data-embiggen="true"><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAMAAAAoyzS7AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAAZQTFRFgYGBAAAAqeg2zgAAAAxJREFUeNpiYAAIMAAAAgABT21Z4QAAAABJRU5ErkJggg==" data-height-ratio="0.8" data-lazy-load-src="https://damn-8791.kxcdn.com/wp-content/uploads/2007/11/forward_slip1.jpg" alt="Orientation of the aircraft during forward slip maneuver" title="Orientation of the aircraft during forward slip maneuver"></p><figcaption>Orientation of the aircraft during forward slip maneuver</figcaption></figure>
<p>At the opposite end of the runway, the Family Day campers and spectators had finally spotted the silent and oddly-angled incoming aircraft, and they were scrambling from its path with appropriate levels of panic.  First Officer Quintal caught sight of the fleeing families, but it was far too late to revise their landing plans, so he opted not to distract the captain with the unsettling discovery.</p>
<p>Forty feet above the ground⁠— mere seconds before contact⁠— Captain Pearson managed to wrestle Flight 143 back to a straight and level approach.  At 8:38pm central time, the rear landing gears grabbed the tarmac at Gimli airport, and Bob Pearson stood on the brake pedals as the airplane skidded towards the scattering bystanders.  A few of the loudly protesting tires finally succumbed to the abuse and blew out with adequate force to shimmy the fuselage.  As some of the weight shifted forward, the unsecured front landing gear buckled, dumping the nose section onto the pavement and spraying a three-hundred foot shower of sparks.  </p>
<p>After sledding across the asphalt for 2,900 feet, Air Canada Flight 143 ground to a halt just a few hundred yards from the shocked onlookers.  There was a moment of stupefied contemplation within the passenger cabin, followed by an eruption of cheering and applause.  Meanwhile several astute racetrack workers dashed to the nose of Flight 143 and doused a small friction-induced fire using hand-held extinguishers.  Within a few minutes the inflatable rubber escape chutes plopped from the sides of the plane, and the sixty-nine frazzled occupants disembarked.  </p>
<p>A crew of engineers from Winnipeg airport clambered into a van and headed for Gimli to assess the damage.  During transit, however, their vehicle unexpectedly ran out of fuel, nearly ripping a hole in the delicate space-irony continuum.  When airline mechanics finally arrived at the landing site, they found all three of the 767’s fuel tanks completely dry, with no evidence of a fuel leak.  A review of the day’s events traced the problem back to the manual dripstick checks in Montreal and Ottawa.  In order to maintain awareness of the overall weight of the aircraft, flight crews kept track of fuel quantity based on kilograms rather than the fuel company’s liter-based measurements.  Pearson and Quintal had determined the fuel weight by multiplying the the number of dripsticked liters by 1.77, as indicated by the documentation.  However, unbeknownst to the pilots and the fuel crew, this multiplier provided the weight in imperial pounds;  the new, all-metric 767 was based on kilograms, and required a multiplier of 0.8.  As a consequence of this documentation disconnect, Flight 143 had left Montreal with roughly half the necessary fuel.</p>
<p>Because the rear escape slides were excessively steep due to the buckled front gear, a few bumps and bruises were sustained on egress; but no one was seriously injured in the Gimli incident.  Had it not been for Pearson’s capable captaining and glider experience, as well as Quintal’s cucumber-cool support, the outcome of the metric mixup might have been considerably less pleasant.  In addition, had it not been for the drag created by the collapsed front gear, the powerless plane would have plunged into the crowd of spectators, sowing destruction and death in its wake.  All told, the soon-to-be-dubbed “Gimli Glider” was a nearly perfect demonstration of dead-stick flying, accompanied by an extra-large portion of good fortune.</p>
<figure data-embiggen="true"><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAMAAAAoyzS7AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAAZQTFRFgYGBAAAAqeg2zgAAAAxJREFUeNpiYAAIMAAAAgABT21Z4QAAAABJRU5ErkJggg==" data-height-ratio="0.70967741935484" data-lazy-load-src="https://damn-8791.kxcdn.com/wp-content/uploads/2007/11/gimlix.jpg" alt="The Gimli after landing (photo by Wayne Glowacki, Winnipeg Free Press)" title="The Gimli after landing (photo by Wayne Glowacki, Winnipeg Free Press)"></p><figcaption>The Gimli after landing (photo by Wayne Glowacki, Winnipeg Free Press)</figcaption></figure>
<p>With just two days of mechanical jiggering, Captain Pearson’s wounded 767 was patched up sufficiently to fly it out for repairs elsewhere.  The Gimli Glider officially rejoined the Air Canada fleet after a bit of body work, a new front gear, a new wiring harness, a repaired Fuel Quantity Indicator System, and a full load of jet fuel.  The internal investigation into the incident laid the blame partially upon Captain Bob Pearson and First Officer Maurice Quintal, who should have observed the Minimum Equipment List (MEL) and grounded the aircraft since it lacked functioning fuel gauges.  Some of the responsibility was also assigned to the maintenance workers, and to “corporate deficiencies.”  As a consequence Pearson was briefly demoted, and Quintal was suspended for two weeks.  Nonetheless both pilots continued to work for Air Canada, and in 1985 they received the well-deserved <i>Fédération Aéronautique Internationale</i> Diploma for Outstanding Airmanship for their handling of the unusual landing.</p>
<p>As for the Gimli Glider herself, the twenty-four year old 767 remains an active part of the Air Canada fleet to this very day.  Some grizzled old pilots swear that sometimes, when the wind is just right on a quiet night, you can just about make out the double-engine-failure <i>BONG!</i> as the old girl is flying by; and if you’re very lucky, you might catch the faint odor of damp pilots in the air.</p>
				

											<p><b>Update:</b> The Gimli Glider–Air Canada Flight 143–was retired from service on 24 January 2008 in a ceremony involving Captain Robert Pearson, First Officer Maurice Quintal, and three of the six flight attendants who had been aboard Flight 143 during its unscheduled glide and rough landing.</p>

										
										
						<p>
			© 2007 All Rights Reserved. Do not distribute or repurpose this work without written permission from the copyright holder(s).
	</p>
<p>
	Printed from https://www.damninteresting.com/the-gimli-glider/<br>
</p>
						<p>
							<i>Since you enjoyed our work enough to print it out, and read it clear to the end, would you consider donating a few dollars at https://www.damninteresting.com/donate</i> ?
						</p>
									</article>
			
		            	            		
	            	
		            			

				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Zenbleed (308 pts)]]></title>
            <link>https://lock.cmpxchg8b.com/zenbleed.html</link>
            <guid>36848680</guid>
            <pubDate>Mon, 24 Jul 2023 14:34:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lock.cmpxchg8b.com/zenbleed.html">https://lock.cmpxchg8b.com/zenbleed.html</a>, See on <a href="https://news.ycombinator.com/item?id=36848680">Hacker News</a></p>
Couldn't get https://lock.cmpxchg8b.com/zenbleed.html: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Which recently research paper blow your mind? (186 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=36846965</link>
            <guid>36846965</guid>
            <pubDate>Mon, 24 Jul 2023 12:34:25 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=36846965">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="36848605"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36848605" href="https://news.ycombinator.com/vote?id=36848605&amp;how=up&amp;goto=item%3Fid%3D36846965"></a></center>    </td><td><p><span>"Overview of SHARD: A System for Highly Available Replicated Data" it's the first paper to introduce the concept of database sharding. It was published in 1988 by the Computer Corporation of America.<p>It is referenced hundreds of times in many classic papers.</p><p>But, here's the thing.  <i>It doesn't exist</i>.</p><p>Everyone cites Sarin, DeWitt &amp; Rosenb[e|u]rg's paper but none have ever seen it. I've emailed dozens of academics, libraries, and archives - none of them have a copy.</p><p>So it blows my mind that something so influential is, effectively, a myth.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36849591"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36849591" href="https://news.ycombinator.com/vote?id=36849591&amp;how=up&amp;goto=item%3Fid%3D36846965"></a></center>    </td><td><p><span>I found LinkedIn profiles for Sunil Sarin, Mark Dewitt, and Ronni Rosenberg who all worked at CCA during this time period.<p>I've gone ahead and sent them each a message asking if they might be able to make the paper available.</p><p>If you'd like to get in contact with them yourself and are having trouble finding their LinkedIn, shoot me an email and I'll be happy to provide you links.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36849144"><td></td></tr>
            <tr id="36849692"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36849692" href="https://news.ycombinator.com/vote?id=36849692&amp;how=up&amp;goto=item%3Fid%3D36846965"></a></center>    </td><td><br><div>
                  <p><span>Going through the bibliography of other people's papers and theses, looking for papers that you better cite "for good luck", or because "you gotta cite that one" is a classic PhD student behavior (I've done it) and it's not terribly surprising that something like this can happen. In fact I'd expect it to be much more widespread...</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36849912"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36849912" href="https://news.ycombinator.com/vote?id=36849912&amp;how=up&amp;goto=item%3Fid%3D36846965"></a></center>    </td><td><p><span>Funny, in my high school literature class I clearly remember being chastised for having sources in my works cited; but not warranting their inclusion with an actual reference in the work.<p>It's kind of wild that LLMs and other models/sequences will be able to quickly suss out which papers have high levels of referential integrity.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="36848908"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36848908" href="https://news.ycombinator.com/vote?id=36848908&amp;how=up&amp;goto=item%3Fid%3D36846965"></a></center>    </td><td><p><span>I am infinitely disappointed to discover you are also the only person who seems to care about this online. I found a website, but it’s you apparently.<p>Now I’m going to be bugged by this too! Great trivia also a heck of a mystery
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36849317"><td></td></tr>
                  <tr id="36849618"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36849618" href="https://news.ycombinator.com/vote?id=36849618&amp;how=up&amp;goto=item%3Fid%3D36846965"></a></center>    </td><td><br><div>
                  <p><span>I guess I'm not too surprised, this seems like a corporate tech report.  Some companies were good at having public archives of these (like Bell Labs) but I'm sure it takes a lot of resources to keep that up.  It's essentially some company's internal Wiki page.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36848690"><td></td></tr>
                <tr id="36849286"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36849286" href="https://news.ycombinator.com/vote?id=36849286&amp;how=up&amp;goto=item%3Fid%3D36846965"></a></center>    </td><td><br><div>
                  <p><span>If it did exist, there's some delicious irony in an original paper on replicating data in a highly-available manner being lost.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="36849634"><td></td></tr>
            <tr id="36849220"><td></td></tr>
            <tr id="36849565"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36849565" href="https://news.ycombinator.com/vote?id=36849565&amp;how=up&amp;goto=item%3Fid%3D36846965"></a></center>    </td><td><br><div>
                  <p><span>Do Sarin, DeWitt &amp; Rosenb[e|u]rg exist? Are they still alive? Tracking them down and going directly to the source would seem to be the way to go. Perhaps even enlisting some "big names" in the industry to ask around?</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36849442"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36849442" href="https://news.ycombinator.com/vote?id=36849442&amp;how=up&amp;goto=item%3Fid%3D36846965"></a></center>    </td><td><br><div>
                  <p><span>That's pretty interesting actually. Someone must have a copy somewhere. Seems like a real failure of scholarship if it's truly lost, and a serious argument against walled gardens-style publishing.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36849097"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36849097" href="https://news.ycombinator.com/vote?id=36849097&amp;how=up&amp;goto=item%3Fid%3D36846965"></a></center>    </td><td><br><div>
                  <p><span>This is wild if true. Surely <i>someone</i> has to have a copy of this. How is it even being referenced if it is non existent?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36849673"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36849673" href="https://news.ycombinator.com/vote?id=36849673&amp;how=up&amp;goto=item%3Fid%3D36846965"></a></center>    </td><td><p><span>I don't know why parent comment is stirring up drama but:<p>1. Not available online doesn't mean the paper's existence is made up. It's a very bold claim to make for the authors that they cite work that is fabricated.</p><p>From the available information, this looks like a technical report by a, probably now defunct, company back in the 80s. If this was its only form of publication, and not on some conference proceedings for example, it would be only found available on select university libraries as a physical copy. But most important,</p><p>2. This isn't even as an impactful paper as the parent comment states. Or if its proposed concept is, the original idea is probably derived from some other paper that is indeed the one that is highly cited and most definitely available online.</p><p>Accumulative citations number from Google Scholar and IEEEXplore doesn't exceed fifteen for the particular paper though.</p><p><a href="https://scholar.google.com/scholar?cites=14914487445955020264" rel="nofollow noreferrer">https://scholar.google.com/scholar?cites=1491448744595502026...</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36849998"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36849998" href="https://news.ycombinator.com/vote?id=36849998&amp;how=up&amp;goto=item%3Fid%3D36846965"></a></center>    </td><td><p><span><i>Not available online doesn't mean the paper's existence is made up.</i><p>True, but note that the post you're referring to does say:</p><p><i>I've emailed dozens of academics, libraries, and archives - none of them have a copy.</i></p><p>So this isn't somebody just saying "I couldn't find it with Google, therefore it doesn't exist."</p><p><i>From the available information, this looks like a technical report by a, probably now defunct, company back in the 80s.</i></p><p>Yeah, I think that's the key point. An internal technical memo from a private company, from that far back, isn't likely to be easy to find. It's quite possible that it's never been digitized and put on the 'net, and it it wasn't published in a journal, it may never have been archived by any university libraries or such-like.</p><p>That said, I'd be a little surprised if a copy didn't turn up <i>somewhere</i>, even if it means a former employee of CCA finding a copy in a desk drawer and providing it. But who knows?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="36849214"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36849214" href="https://news.ycombinator.com/vote?id=36849214&amp;how=up&amp;goto=item%3Fid%3D36846965"></a></center>    </td><td><p><span>Which papers cite it?  Are they old papers, when perhaps it still existed, or recent ones?<p>Very interesting either way!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36848963"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36848963" href="https://news.ycombinator.com/vote?id=36848963&amp;how=up&amp;goto=item%3Fid%3D36846965"></a></center>    </td><td><p><span>&gt; It is referenced hundreds of times in many classic papers.<p>Wait, you mean people include papers they haven't even <i>opened</i> in their references?!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36849086"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36849086" href="https://news.ycombinator.com/vote?id=36849086&amp;how=up&amp;goto=item%3Fid%3D36846965"></a></center>    </td><td><br><div>
                  <p><span>Happens far more than you think. It can be an innocent (sort of) mistake, where authors see the citation in a previous paper and simply copy it into their own.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36849711"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36849711" href="https://news.ycombinator.com/vote?id=36849711&amp;how=up&amp;goto=item%3Fid%3D36846965"></a></center>    </td><td><br><div>
                  <p><span>This feels like something that would, at large scale, be unhealthy for science as a whole.  While existing papers have already gone through their own quality checks, this enables bad, misleading, or false statements to propagate which can end up being a blow to the credibility of the entire model.  Shouldn't there be an ethical duty to due one's due diligence?</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36849289"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36849289" href="https://news.ycombinator.com/vote?id=36849289&amp;how=up&amp;goto=item%3Fid%3D36846965"></a></center>    </td><td><p><span>&gt; where authors see the citation in a previous paper and simply copy it into their own.<p>Without opening the paper to even read the abstract? To me it doesn't sound like “innocent” at all, and borderline malpractice…
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36849452"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_36849452" href="https://news.ycombinator.com/vote?id=36849452&amp;how=up&amp;goto=item%3Fid%3D36846965"></a></center>    </td><td><br><div>
                  <p><span>There's a crowd who tilt the other way -- if I might possibly have hinted at the idea before you then it's borderline malpractice to not reference me. In many fields it's common to directly reference what appear to be the bigger transitive references then, even if they didn't directly influence this work in particular. I'd personally want to see a twidge more evidence before bringing out the pitchforks.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36849530"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_36849530" href="https://news.ycombinator.com/vote?id=36849530&amp;how=up&amp;goto=item%3Fid%3D36846965"></a></center>    </td><td><br><div>
                  <p><span>It's innocent in the sense that it's (usually) not checked out of laziness/complacency as opposed to malicious and intentional citation fraud.</span></p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="36849803"><td></td></tr>
            <tr id="36849564"><td></td></tr>
            <tr id="36849118"><td></td></tr>
                  <tr id="36847752"><td></td></tr>
                <tr id="36848597"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36848597" href="https://news.ycombinator.com/vote?id=36848597&amp;how=up&amp;goto=item%3Fid%3D36846965"></a></center>    </td><td><br><div>
                  <p><span>Is this really "mind blowing"? Knowing the competency of the average police department, I'd consider it more par for the course.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36848940"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36848940" href="https://news.ycombinator.com/vote?id=36848940&amp;how=up&amp;goto=item%3Fid%3D36846965"></a></center>    </td><td><br><div>
                  <p><span>The result is obvious, but the question is demonstrably not. Good researchers know how to ask interesting questions that no one had bothered to ask before. Seeing clever work like this makes me reflect and continually ask myself "What cool angles am I missing?"</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36849135"><td></td></tr>
                        <tr id="36849011"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36849011" href="https://news.ycombinator.com/vote?id=36849011&amp;how=up&amp;goto=item%3Fid%3D36846965"></a></center>    </td><td><p><span>Integral Neural Networks (CVPR 2023 Award Candidate), a nifty way of building resizable networks.<p>My understanding of this work: A forward pass for a (fully-connected) layer of a neural network is just a dot product of the layer input with the layer weights, followed by some activation function. Both the input and the weights are vectors of the same, fixed size.</p><p>Let's imagine that the discrete values that form these vectors happen to be samples of two different continuous univariate functions. Then we can view the dot product as an approximation to the value of integrating the multiplication of the two continuous functions.</p><p>Now instead of storing the weights of our network, we store some values from which we can reconstruct a continuous function, and then sample it where we want (in this case some trainable interpolation nodes, which are convoluted with a cubic kernel). This gives us the option to sample different-sized networks, but they are all performing (an approximation to) the same operation. After training with samples at different resolutions, you can freely pick your network size at inference time.</p><p>You can also take pretrained networks, reorder the weights to make the functions as smooth as possible, and then compress the network, by downsampling. In their experiments, the networks lose much less accuracy when being downsampled, compared to common pruning approaches.</p><p>Paper: <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Solodskikh_Integral_Neural_Networks_CVPR_2023_paper.pdf" rel="nofollow noreferrer">https://openaccess.thecvf.com/content/CVPR2023/papers/Solods...</a></p><p>Code: <a href="https://github.com/TheStageAI/TorchIntegral">https://github.com/TheStageAI/TorchIntegral</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36849191"><td></td></tr>
            <tr id="36849437"><td></td></tr>
                  <tr id="36849684"><td></td></tr>
            <tr id="36848847"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36848847" href="https://news.ycombinator.com/vote?id=36848847&amp;how=up&amp;goto=item%3Fid%3D36846965"></a></center>    </td><td><p><span>I thought the AlphaZero paper was pretty cool: <a href="https://arxiv.org/abs/1712.01815" rel="nofollow noreferrer">https://arxiv.org/abs/1712.01815</a><p>Not only did we get a whole new type of Chess engine, it was also interesting to see how the engine thought of different openings at various stages in its training. For instance, the Caro-Kann, which is my weapon of choice, was favored quite heavily by it for several hours and then seemingly rejected (perhaps it even refuted it?!) near the end.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36848249"><td></td></tr>
            <tr id="36848188"><td></td></tr>
                <tr id="36848967"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36848967" href="https://news.ycombinator.com/vote?id=36848967&amp;how=up&amp;goto=item%3Fid%3D36846965"></a></center>    </td><td><p><span>&gt; ... 2010. Before nobody believed that backprop can be GPU-accelerated.<p>When I was doing my master's in 2004-06, I talked to a guy whose MSc thesis was about running NNs with GPUs. My thought was: you're going to spend a TON of time fiddling with hacky systems code like CUDA, to get basically a minor 2x or 4x improvement in training time, for a type of ML algorithm that wasn't even that useful: in that era the SVM was generally considered to be superior to NNs.</p><p>So it wasn't that people thought it couldn't be done, it's that nobody saw why this would be worthwhile. Nobody was going around saying, "IF ONLY we could spend 20x more compute training our NNs, then they would be amazingly powerful".
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36849976"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36849976" href="https://news.ycombinator.com/vote?id=36849976&amp;how=up&amp;goto=item%3Fid%3D36846965"></a></center>    </td><td><p><span>Exactly.<p>It's easy to see in retrospect, but hard in prospect: the original paper [1] on GPU acceleration of NNs  20x. Assuming a bit of cherry-picking to make get the paper published, the 'real-world speedup' will have been assumed by the readership to be less. But this triggered a virtuos cycle of continuous improvements at all levels that has been dubbed "winning the hardware lottery" [2].</p><p>[1] K.-S. Oh, K. Jung, <i>GPU implementation of neural networks.</i></p><p>[2] S. Hooker, <i>The Hardware Lottery.</i> <a href="https://arxiv.org/abs/2009.06489" rel="nofollow noreferrer">https://arxiv.org/abs/2009.06489</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="36848724"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36848724" href="https://news.ycombinator.com/vote?id=36848724&amp;how=up&amp;goto=item%3Fid%3D36846965"></a></center>    </td><td><p><span>&gt; First time for ML that is not deep learning<p>What do you mean by this? Virtually all "classic" or "shallow" ML can be GPU-accelerated, from linear regression to SVM to GBM.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36849110"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36849110" href="https://news.ycombinator.com/vote?id=36849110&amp;how=up&amp;goto=item%3Fid%3D36846965"></a></center>    </td><td><p><span>Can you point me to papers with reproducible benchmarking that achieves big speedups on those?<p>Modern GPUs are <i>GP</i>-GPUs: where GP means <i>"general purpose"</i>: you can run any code on GPGPUs. But if you want to gain real speed-ups you will have to program in an
awkward style ("data parallel"). I am not aware of GPU acceleration of the work-horses of symbolic AI, such as Prolog, or SMT solving. There has been a lot of work on running SAT-solvers on GPUs, but I don't think this has really succeeded so far.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36849838"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36849838" href="https://news.ycombinator.com/vote?id=36849838&amp;how=up&amp;goto=item%3Fid%3D36846965"></a></center>    </td><td><p><span>I think we're conflating two things: shallow/classic ML is not symbolic AI. I'm not sure "ML" even encompasses anything "symbolic"; I see symbolic AI and ML as subfields with little overlap.<p>I'm not saying symbolic AI has been GPU accelerated in the past, but that non-deep ML has been.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="36849915"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36849915" href="https://news.ycombinator.com/vote?id=36849915&amp;how=up&amp;goto=item%3Fid%3D36846965"></a></center>    </td><td><br><div>
                  <p><span>computing a single matrix right divide is probably faster done in place with the cpu using vector math instructions.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="36848587"><td></td></tr>
                <tr id="36848834"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36848834" href="https://news.ycombinator.com/vote?id=36848834&amp;how=up&amp;goto=item%3Fid%3D36846965"></a></center>    </td><td><p><span>Good question. All supervised learning is a form of search with three components:<p>- <i>Specification:</i> what are you are looking for?</p><p>- <i>Search space:</i> were are you looking?</p><p>- <i>Search mechanism:</i> how are you going through the search space?</p><p>Program synthesis is simply learning where the search space is syntax. 
In deep learning, taking the ImageNet paper as an example, the specification a bunch of photos with annotations, the search space is multi-variate real functions (encoded as matrix of floats) and the search mechanisms is 
gradient descent (implemented as backprop) via a loss function.</p><p>I think this paper uses regular expressions  an example of how to search fast over syntax. It claims not to be tied to regular expressions.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="36849137"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36849137" href="https://news.ycombinator.com/vote?id=36849137&amp;how=up&amp;goto=item%3Fid%3D36846965"></a></center>    </td><td><br><div>
                  <p><span>I recently read "Enabling tabular deep learning when d ≫ n with an auxiliary knowledge graph" (<a href="https://arxiv.org/pdf/2306.04766.pdf" rel="nofollow noreferrer">https://arxiv.org/pdf/2306.04766.pdf</a>) for one of my graduate classes.  Essentially, when there are significantly more data points than features (n &gt;&gt; d), machine learning usually works fine (assuming data quality, an underlying relationship, etc.).  But, for sparse datasets where there are fewer data points than features (d &gt;&gt; n), most machine learning methods fail.  There's just not enough data to learn all the relationships.  This paper builds a knowledge graph based on relationships and other pre-existing knowledge of data features to improve model performance in this case.  It's really interesting - I hadn't realized there were ways to get better performance in this case.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36849088"><td></td></tr>
            <tr id="36849108"><td></td></tr>
            <tr id="36847713"><td></td></tr>
                <tr id="36848411"><td></td></tr>
                <tr id="36849797"><td></td></tr>
                        <tr id="36849620"><td></td></tr>
            <tr id="36849769"><td></td></tr>
            <tr id="36847919"><td></td></tr>
            <tr id="36847803"><td></td></tr>
            <tr id="36848217"><td></td></tr>
            <tr id="36847912"><td></td></tr>
                <tr id="36847924"><td></td></tr>
                  <tr id="36847916"><td></td></tr>
            <tr id="36849349"><td></td></tr>
            <tr id="36847864"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36847864" href="https://news.ycombinator.com/vote?id=36847864&amp;how=up&amp;goto=item%3Fid%3D36846965"></a></center>    </td><td><p><span>“A classification of endangered high-THC cannabis (Cannabis sativa subsp. indica) domesticates and their wild relatives”<p>By McPartland and Small.</p><p>Moving on from cannabis sativa indica and cannabis sativa sativa to cannabis sativa indica Himalayansis and cannabis sativa indica asperrima depending on distribution from the original location of the extinct ancient cannabis wildtype.</p><p>Following this new classification, I believe there’s a third undocumented variety in North East Asia.</p><p>If anyone else has noticed the samesameification of cannabis strains and is wondering what the path forward is, this may be illuminating.</p><p><a href="https://phytokeys.pensoft.net/article/46700/" rel="nofollow noreferrer">https://phytokeys.pensoft.net/article/46700/</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Driver.js: Product tours, highlights, contextual help and more (209 pts)]]></title>
            <link>https://driverjs.com/</link>
            <guid>36846520</guid>
            <pubDate>Mon, 24 Jul 2023 12:02:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://driverjs.com/">https://driverjs.com/</a>, See on <a href="https://news.ycombinator.com/item?id=36846520">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  
      <h2 data-examples-heading="">Examples</h2>
<p data-examples-tagline="">Here are just a few examples; find more <a href="https://driverjs.com/docs/installation">in the documentation</a>.</p>

<p>
  
  
  
  
  
  
  
  
  
  
  
  
  <a href="https://driverjs.com/docs/installation">
    and much more ...
  </a>
</p>
      <p>Due to it's extensive API, driver.js can be used for a wide range of use
  cases.</p>
<div>
  <div>
  <h3>
    Onboard Users
  </h3>
  <p>
    Onboard your users by explaining how to use your product and answer common questions.
  </p>
</div>
  <div>
  <h3>
    Remove Distractions
  </h3>
  <p>
    With highlight feature, you can remove distractions and focus your users attention on what matters.
  </p>
</div>
  <div>
  <h3>
    Contextual Help
  </h3>
  <p>
    Provide contextual help for your users, explain how to use your product and answer common questions.
  </p>
</div>
  <div>
  <h3>
    Feature Adoption
  </h3>
  <p>
    Highlight new features, explain how to use them and make sure your users don't miss them.
  </p>
</div>
</div>
    
</div><div>
      <div>
        <h2>Loved by Many</h2>
        <p>With millions of downloads, Driver.js is an <span>MIT licensed</span>
          opensource
          project and is used by
          thousands of companies around the world.</p>

        
      </div>
      <p><img src="https://driverjs.com/thumbs.svg" alt="Hero Image">
    </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Researchers Find ‘Backdoor’ in Encrypted Police and Military Radios (103 pts)]]></title>
            <link>https://www.vice.com/en/article/4a3n3j/backdoor-in-police-radios-tetra-burst</link>
            <guid>36846242</guid>
            <pubDate>Mon, 24 Jul 2023 11:38:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.vice.com/en/article/4a3n3j/backdoor-in-police-radios-tetra-burst">https://www.vice.com/en/article/4a3n3j/backdoor-in-police-radios-tetra-burst</a>, See on <a href="https://news.ycombinator.com/item?id=36846242">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component="BodyComponentRenderer"><div><div><p><a href="https://www.vice.com/en/topic/cyber"><img src="https://video-images.vice.com/topics/57a205628cb727dec795a6b1/callout_logo/1614199980283-screen-shot-2021-02-24-at-34918-pm.png?resize=240:*" alt="Screen Shot 2021-02-24 at 3"></a></p><p>Hacking. Disinformation. Surveillance. CYBER is Motherboard's podcast and reporting on the dark underbelly of the internet.</p></div><p><span data-component="TextBlock"><p>A group of cybersecurity researchers has uncovered what they believe is an intentional backdoor in encrypted radios used by police, military, and critical infrastructure entities around the world. The backdoor may have existed for decades, potentially exposing a wealth of sensitive information transmitted across them, according to the researchers.</p></span><span data-component="TextBlock"><p>While the researchers frame their discovery as a backdoor, the organization responsible for maintaining the standard pushes back against that specific term, and says the standard was designed for export controls which determine the strength of encryption. The end result, however, are radios with traffic that can be decrypted using consumer hardware like an ordinary laptop in under a minute.</p></span></p></div><div><p><span data-component="TextBlock"><p>“There's no other way in which this can function than that this is an intentional backdoor,” Jos Wetzels, one of the researchers from cybersecurity firm Midnight Blue, told Motherboard in a phone call.</p></span></p><blockquote data-component="QuoteBlock"><p><em><strong>Do you know about other vulnerabilities in communications networks? We'd love to hear from you. Using a non-work phone or computer, you can contact Joseph Cox securely on Signal on +44 20 8133 5190, Wickr on josephcox, or email&nbsp;joseph.cox@vice.com.</strong></em></p></blockquote><p><span data-component="TextBlock"><p>The research is the first public and in-depth analysis of the <a href="https://www.etsi.org/technologies/tetra#:~:text=Standards-,Introduction,Public%20Safety" target="_blank">TErrestrial Trunked RAdio (TETRA) standard</a> in the more than 20 years the standard has existed. Not all users of TETRA-powered radios use the specific encryption algorithim called TEA1 which is impacted by the backdoor. TEA1 is part of the TETRA standard approved for export to other countries. But the researchers also found other, multiple vulnerabilities across TETRA that could allow historical decryption of communications and deanonymization. TETRA-radio users in general include national police forces and emergency services in Europe; military organizations in Africa; and train operators in North America and critical infrastructure providers elsewhere.&nbsp;</p></span><span data-component="TextBlock"><p>Midnight Blue will be presenting their findings at the upcoming Black Hat cybersecurity conference in August. The details of the talk have been closely under wraps, with the Black Hat website simply <a href="https://www.blackhat.com/us-23/briefings/schedule/index.html#redacted-telecom-talk-31807" target="_blank">describing the briefing as a “Redacted Telecom Talk.”</a> That reason for secrecy was in large part due to the unusually long disclosure process. Wetzels told Motherboard the team has been disclosing these vulnerabilities to impacted parties so they can be fixed for more than a year and a half. That included an initial meeting with Dutch police in January 2022, a meeting with the intelligence community later that month, and then the main bulk of providing information and mitigations being distributed to stakeholders. NLnet Foundation, an organization which funds “those with ideas to fix the internet,” financed the research. </p></span></p></div><p><span data-component="TextBlock"><p>The European Telecommunications Standards Institute (ETSI), an organization that standardizes technologies across the industry, first created TETRA in 1995. Since then, TETRA has been used in products, including radios, sold by Motorola, Airbus, and more. Crucially, TETRA is not open-source. Instead, it relies on what the researchers describe in their presentation slides as “secret, proprietary cryptography,” meaning it is typically difficult for outside experts to verify how secure the standard really is.</p></span><span data-component="TextBlock"><p>The researchers said they worked around this limitation by purchasing a TETRA-powered radio from eBay. In order to then access the cryptographic component of the radio itself, Wetzels said the team found a vulnerability in an interface of the radio. From there, they achieved code execution on the main application processor; they then jumped to the signals processor, which Wetzels described as something equivalent to a wifi or 3G chip, which handles the radio’s signals. On that chip, a secure enclave held the cryptographic ciphers themselves. The team finally found vulnerabilities in that which allowed them to extract the cryptography and perform their analysis. The team then reverse-engineered how TETRA implemented its cryptography, which led to the series of vulnerabilities that <a href="http://tetraburst.com/" target="_blank">they have called TETRA:BURST</a>. “It took less time than we initially expected,” Wetzels said.</p></span></p><p><span data-component="TextBlock"><p>Most interestingly is the researchers’ findings of what they describe as the backdoor in TEA1. Ordinarily, radios using TEA1 used a key of 80-bits. But Wetzels said the team found a “secret reduction step” which dramatically lowers the amount of entropy the initial key offered. An attacker who followed this step would then be able to decrypt intercepted traffic with consumer-level hardware and a cheap software defined radio dongle.</p></span><span data-component="TextBlock"><p>“This is a trivial type of attack that fully breaks the algorithm. That means an attacker can passively decrypt everything in almost real time. And it's undetectable, if you do it passively, because you don't need to do any weird interference stuff,” Wetzels said.</p></span><span data-component="TextBlock"><p>Not all current TETRA-radio customers will use TEA1, and some may have since moved onto TETRA’s other encryption algorithms. But given TETRA’s long life span, its existence still means there may have been room for exploitation if another party was aware of this issue.</p></span><span data-component="TextBlock"><p>“There's bigger fish who likely found this much earlier,” Wetzels said, referring to other third parties who may have discovered the issue.&nbsp;</p></span><span data-component="TextBlock"><p>The researchers say they identified multiple entities that they believe may have used TEA1 products at some point. They include U.S. Africom, <a href="https://www.africom.mil/" target="_blank">a part of the U.S. military which focuses on the continent</a>. Multiple military agencies did not respond to Motherboard’s request for comment.</p></span></p><p><span data-component="TextBlock"><p>“In the interest of public safety, we do not share detailed information on our cybersecurity infrastructure,” Lenis Valens, a spokesperson for PANYNJ which manages JFK airport, said in a statement when asked if the organization used TETRA radios when contacted by Motherboard. “The agency has robust protocols in place and employs the latest technologies and best practices. Safety for our passengers and customers always comes first,” the statement said.</p></span><span data-component="TextBlock"><p>Most law enforcement agencies contacted by Motherboard did not respond to a request for comment. Swedish authorities declined to comment.</p></span><span data-component="TextBlock"><p>Several radio manufacturers directed Motherboard to ETSI for comment. Claire Boyer, press and media officer for ETSI, told Motherboard in an email that “As the authority on the ETSI TETRA technology standard, we welcome research efforts that help us further develop and strengthen the security of the standard so that it remains safe and resilient for decades to come. We will respond to the report when it has been published.”</p></span><span data-component="TextBlock"><p>Specifically on the researchers’ claims of a backdoor in TEA1, Boyer added “At this time, we would like to point out that the research findings do not relate to any backdoors. The TETRA security standards have been specified together with national security agencies and are designed for and subject to export control regulations which determine the strength of the encryption.”</p></span><span data-component="TextBlock"><p>The researchers stressed that the key reduction step they discovered is not advertised publicly.</p></span></p><p><span data-component="TextBlock"><p>“‘Intentional weakening’ without informing the public seems like the definition of a backdoor,” Wouter Bokslag from Midnight Blue told Motherboard in an email.</p></span><span data-component="TextBlock"><p>In ETSI’s statement to Motherboard, Boyer said “there have not been any known exploitations on operational networks” of the vulnerabilities the researchers disclosed.</p></span><span data-component="TextBlock"><p>Bokslag from Midnight Blue said in response that “There is no reason ETSI would be aware of exploitations in the wild, unless customers reach out to ETSI after detecting anomalies in their network traffic.” Then with the TEA1 issues specifically, “since it can be passively intercepted and decrypted, there is no detectable interference, and ETSI not knowing any concrete cases seems like a bit of a meaningless statement with this regard.”</p></span><span data-component="TextBlock"><p>In response to some of the researchers’ findings, radio manufacturers have developed firmware updates for their products. For TEA1, however, the researchers recommend users migrate to another TEA cipher or apply additional end-to-end encryption to their communications. Wetzels said that such an add-on does exist, but that hasn’t been vetted by outside experts at this time.</p></span><span data-component="TextBlock"><p>Bart Jacobs, a professor of security, privacy and identity, who did not work on the research itself but says he was briefed on it, said he hopes “this really is the end of closed, proprietary crypto, not based on open, publicly scrutinised standards.”</p></span><span data-component="TextBlock"><p><em><strong>Subscribe to our cybersecurity podcast,&nbsp;<a href="https://itunes.apple.com/gb/podcast/cyber/id1441708044?mt=2" target="_blank">CYBER</a>. Subscribe to&nbsp;<a href="https://m.twitch.tv/vice" target="_blank">our Twitch channel</a>.</strong></em></p></span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The IBM mainframe: How it runs and why it survives (139 pts)]]></title>
            <link>https://arstechnica.com/information-technology/2023/07/the-ibm-mainframe-how-it-runs-and-why-it-survives/</link>
            <guid>36846195</guid>
            <pubDate>Mon, 24 Jul 2023 11:35:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/information-technology/2023/07/the-ibm-mainframe-how-it-runs-and-why-it-survives/">https://arstechnica.com/information-technology/2023/07/the-ibm-mainframe-how-it-runs-and-why-it-survives/</a>, See on <a href="https://news.ycombinator.com/item?id=36846195">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <header>
            <h4>
      big iron    —
</h4>
            
            <h2 itemprop="description">In this deep-dive explainer, we look at a big-business mainstay.</h2>
                    </header>
        <section>
            <div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/06/new_IBM_z16-800x452.webp" alt="A Z16 Mainframe.">
      <figcaption><div><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/06/new_IBM_z16.webp" data-height="690" data-width="1220">Enlarge</a> <span>/</span> A Z16 Mainframe.</p></div></figcaption>  </figure>

  




<!-- cache hit 757:single/related:a5a1b6c312432e311134d2370d1d0b3a --><!-- empty -->
<p>Mainframe computers are often seen as ancient machines—practically dinosaurs. But mainframes, which are purpose-built to process enormous amounts of data, are still extremely relevant today. If they’re dinosaurs, they’re T-Rexes, and desktops and server computers are puny mammals to be trodden underfoot.</p>
<p>It’s estimated that there are 10,000 mainframes in use today. They’re used almost exclusively by the largest companies in the world, including two-thirds of Fortune 500 companies, 45 of the world’s top 50 banks, eight of the top 10 insurers, seven of the top 10 global retailers, and eight of the top 10 telecommunications companies. And most of those mainframes come from IBM.</p>
<p>In this explainer, we’ll look at the IBM mainframe computer—what it is, how it works, and why it’s still going strong after over 50 years.</p>
<h2>Setting the stage</h2>
<p>Mainframes descended directly from the technology of the first computers in the 1950s. Instead of being streamlined into low-cost desktop or server use, though, they evolved to handle massive data workloads, like bulk data processing and high-volume financial transactions.</p>
<p>Vacuum tubes, magnetic core memory, magnetic drum storage, tape drives, and punched cards were the foundation of the IBM 701 in 1952, the IBM 704 in 1954, and the IBM 1401 in 1959. Primitive by today’s standards, these machines provided the functions of scientific calculations and data processing that would otherwise have to be done by hand or mechanical calculators. There was a ready market for these machines, and IBM sold them as fast as it could make them.</p>
<p>In the early years of computing, IBM had many competitors, including Univac, Rand, Sperry, Amdahl, GE, RCA, NEC, Fujitsu, Hitachi, Unisys, Honeywell, Burroughs, and CDC. At the time, all of these other companies combined accounted for about 20 percent of the mainframe market, and IBM claimed the rest. Today, IBM is the only mainframe manufacturer that matters and that does any kind of business at scale. Its de facto competitors are now the cloud and clusters, but as we'll see, it's not always cost-effective to switch to those platforms, and they're not able to provide the reliability of the mainframe.</p>                                            
                                                        
<h2>Built-in redundancy</h2>
<p>By any standard, mainframes are enormous. Today’s mainframe can have up to 240 server-grade CPUs, 40TB of error-correcting RAM, and many petabytes of redundant flash-based secondary storage. They’re designed to process large amounts of critical data while maintaining a 99.999 percent uptime—that’s a bit over five minutes' worth of outage per year. A medium-sized bank may use a mainframe to run 50 or more separate financial applications and supporting processes and employ thousands of support personnel to keep things running smoothly.</p>
<p>Most mainframes process high-volume financial transactions, which include things like credit card purchases at a cash register, withdrawals from an ATM, or stock purchases on the Internet.</p>
<p>A bank’s lifeblood isn't money—it’s data. Every transaction a bank makes involves data that must be processed. A debit card transaction, for instance, involves the following data that must be processed:</p>
<ul>
<li>Retrieving a user’s debit account info</li>
<li>Validating the user ID and PIN</li>
<li>Checking the availability of funds</li>
<li>Debiting the user’s account for the transaction amount</li>
<li>Crediting the seller’s account</li>
</ul>
<p>All this must happen in seconds, and banks have to ensure they can maintain a rapid response even during high-volume events such as shopping holidays. Mainframes are designed from the ground up to provide both redundancy and high throughput for these purposes. High-speed processing is no good if processing stops during business hours, and reliable processing is no good if people have to wait minutes for a transaction to process.</p>
<p>When you process a financial transaction, it means you’re making money. If you’re processing a lot of transactions, you need to spend a lot of money on redundancy to keep things running smoothly. When parts inevitably fail, the show must go on. That’s where mainframes’ built-in redundant processing comes in.</p>

                                                </div>

            
                            <nav>Page: <span>1 <a href="https://arstechnica.com/information-technology/2023/07/the-ibm-mainframe-how-it-runs-and-why-it-survives/2/">2</a> <a href="https://arstechnica.com/information-technology/2023/07/the-ibm-mainframe-how-it-runs-and-why-it-survives/3/">3</a> <a href="https://arstechnica.com/information-technology/2023/07/the-ibm-mainframe-how-it-runs-and-why-it-survives/4/">4</a> <a href="https://arstechnica.com/information-technology/2023/07/the-ibm-mainframe-how-it-runs-and-why-it-survives/5/">5</a> <a href="https://arstechnica.com/information-technology/2023/07/the-ibm-mainframe-how-it-runs-and-why-it-survives/6/">6</a> <a href="https://arstechnica.com/information-technology/2023/07/the-ibm-mainframe-how-it-runs-and-why-it-survives/7/">7</a> <a href="https://arstechnica.com/information-technology/2023/07/the-ibm-mainframe-how-it-runs-and-why-it-survives/2/"><span>Next <span>→</span></span></a></span></nav>
            
        </section>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Unicode Character “𝕏” (U+1D54F) (301 pts)]]></title>
            <link>https://www.compart.com/en/unicode/U+1D54F</link>
            <guid>36846076</guid>
            <pubDate>Mon, 24 Jul 2023 11:22:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.compart.com/en/unicode/U+1D54F">https://www.compart.com/en/unicode/U+1D54F</a>, See on <a href="https://news.ycombinator.com/item?id=36846076">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p><a data-keep-scroll="true" href="https://www.compart.com/en/unicode/U+1D550" onclick="charNext()">Next char</a></p></div><div><p><a data-keep-scroll="true" href="https://www.compart.com/en/unicode/U+1D54E" onclick="charPrev()">Previous char</a></p></div><h2 id="gen-h1-1-00000001">Unicode Character “<span>𝕏</span>” (U+1D54F)</h2><p><span>𝕏</span></p><div data-keep-scroll=""><table><tbody><tr><td>Name:</td><td>Mathematical Double-Struck Capital X<sup><a href="#UNC_DB">[1]</a></sup></td></tr><tr><td>Unicode Version:</td><td>3.1 (March 2001)<sup><a href="#UNC_VERSION">[2]</a></sup></td></tr><tr><td>Block:</td><td><a href="https://www.compart.com/en/unicode/block/U+1D400">Mathematical Alphanumeric Symbols, U+1D400 - U+1D7FF</a><sup><a href="#UNC_BLOCKS">[3]</a></sup></td></tr><tr><td>Plane:</td><td><a href="https://www.compart.com/en/unicode/plane/U+10000">Supplementary Multilingual Plane, U+10000 - U+1FFFF</a><sup><a href="#UNC_BLOCKS">[3]</a></sup></td></tr><tr><td>Script:</td><td><a href="https://www.compart.com/en/unicode/scripts/Zyyy">Code for undetermined script</a> (Zyyy) <sup><a href="#UNC_SCRIPTS">[4]</a></sup></td></tr><tr><td>Category:</td><td><a href="https://www.compart.com/en/unicode/category/Lu">Uppercase Letter</a> (Lu) <sup><a href="#UNC_DB">[1]</a></sup></td></tr><tr><td>Bidirectional Class:</td><td><a href="https://www.compart.com/en/unicode/bidiclass/L">Left To Right</a> (L) <sup><a href="#UNC_DB">[1]</a></sup></td></tr><tr><td>Combining Class:</td><td><a href="https://www.compart.com/en/unicode/combining/0">Not Reordered</a> (0) <sup><a href="#UNC_DB">[1]</a></sup></td></tr><tr><td>Character is Mirrored:</td><td>No <sup><a href="#UNC_DB">[1]</a></sup></td></tr><tr><td>HTML Entity:</td><td><ul><li><code>&amp;#120143;</code></li><li><code>&amp;#x1D54F;</code></li><li><code>&amp;Xopf;</code></li></ul></td></tr><tr><td>UTF-8 Encoding:</td><td><code>0xF0 0x9D 0x95 0x8F</code></td></tr><tr><td>UTF-16 Encoding:</td><td><code>0xD835 0xDD4F</code></td></tr><tr><td>UTF-32 Encoding:</td><td><code>0x0001D54F</code></td></tr><tr><td>Decomposition:</td><td><a href="https://www.compart.com/en/unicode/U+0058">X (U+0058)</a><sup><a href="#UNC_DB">[1]</a></sup></td></tr></tbody></table></div><h2 id="gen-h2-2-00000002">Based on "<span>X</span>" (U+0058) <span></span></h2><div><p>Unicode</p><p>Character</p><p>Name</p></div><h2 id="gen-h2-2-00000003">References</h2><div><ol><li id="UNC_DB"><a href="ftp://ftp.unicode.org/Public/UNIDATA/UnicodeData.txt" target="_blank" rel="noopener">Unicode Database - UnicodeData</a></li><li id="UNC_VERSION"><a href="http://www.unicode.org/Public/UCD/latest/ucd/DerivedAge.txt" target="_blank" rel="noopener">Unicode Database - Derived Age</a></li><li id="UNC_BLOCKS"><a href="ftp://ftp.unicode.org/Public/UNIDATA/Blocks.txt" target="_blank" rel="noopener">Unicode Database - Blocks</a></li><li id="UNC_SCRIPTS"><a href="ftp://ftp.unicode.org/Public/UNIDATA/Scripts.txt" target="_blank" rel="noopener">Unicode Database - Scripts</a></li></ol></div></div></div>]]></description>
        </item>
    </channel>
</rss>