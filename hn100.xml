<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 06 Jun 2025 21:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Breakthrough in search for HIV cure leaves researchers 'overwhelmed' (147 pts)]]></title>
            <link>https://www.theguardian.com/global-development/2025/jun/05/breakthrough-in-search-for-hiv-cure-leaves-researchers-overwhelmed</link>
            <guid>44202664</guid>
            <pubDate>Fri, 06 Jun 2025 16:44:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/global-development/2025/jun/05/breakthrough-in-search-for-hiv-cure-leaves-researchers-overwhelmed">https://www.theguardian.com/global-development/2025/jun/05/breakthrough-in-search-for-hiv-cure-leaves-researchers-overwhelmed</a>, See on <a href="https://news.ycombinator.com/item?id=44202664">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>A cure for HIV could be a step closer after researchers found a new way to force the virus out of hiding inside human cells.</p><p>The virus’s ability to conceal itself inside certain white blood cells has been one of the main challenges for scientists looking for a cure. It means there is a reservoir of the HIV in the body, capable of reactivation, that neither the immune system nor drugs can tackle.</p><p>Now researchers from the Peter Doherty Institute for Infection and Immunity in Melbourne, have demonstrated a way to make the virus visible, paving the way to fully clear it from the body.</p><p>It is based on mRNA technology, which came to prominence during the Covid-19 pandemic <a href="https://www.theguardian.com/science/2021/nov/01/flu-cancer-hiv-after-covid-success-what-next-for-mrna-vaccines" data-link-name="in body link">when it was used in vaccines</a> made by Moderna and Pfizer/BioNTech.</p><p>In a <a href="https://www.nature.com/articles/s41467-025-60001-2" data-link-name="in body link">paper published in Nature Communications</a>, the researchers have shown for the first time that mRNA can be delivered into the cells where HIV is hiding, by encasing it in a tiny, specially formulated fat bubble. The mRNA then instructs the cells to reveal the virus.</p><p>Globally, there are <a href="https://www.unaids.org/en/resources/fact-sheet" data-link-name="in body link">almost 40 million people</a> living with HIV, who must take medication for the rest of their lives in order to suppress the virus and ensure they do not develop symptoms or transmit it. For many it remains deadly, with UNAids figures suggesting one person died of HIV every minute in 2023.</p><p>It was “previously thought impossible” to deliver mRNA to the type of white blood cell that is home to HIV, said Dr Paula Cevaal, research fellow at the Doherty Institute and co-first author of the study, because those cells did not take up the fat bubbles, or lipid nanoparticles (LNPs), used to carry it.</p><p>The team have developed a new type of LNP that those cells will accept, known as LNP X. She said: “Our hope is that this new nanoparticle design could be a new pathway to an HIV cure.”</p><p>When a colleague first presented test results at the lab’s weekly meeting, Cevaal said, they seemed too good to be true.</p><p>“We sent her back into the lab to repeat it, and she came back the next week with results that were equally good. So we had to believe it. And of course, since then, we’ve repeated it many, many, many more times.</p><p>“We were overwhelmed by how [much of a] night and day difference it was – from not working before, and then all of a sudden it was working. And all of us were just sitting gasping like, ‘wow’.”</p><p>Further research will be needed to determine whether revealing the virus is enough to allow the body’s immune system to deal with it, or whether the technology will need to be combined with other therapies to eliminate HIV from the body.</p><p>The study is laboratory based and was carried out in cells donated by HIV patients. The path to using the technology as part of a cure for patients is long, and would require successful tests in animals followed by safety trials in humans, likely to take years, before efficacy trials could even begin.</p><p>“In the field of biomedicine, many things eventually don’t make it into the clinic – that is the unfortunate truth; I don’t want to paint a prettier picture than what is the reality,” stressed Cevaal. “But in terms of specifically the field of HIV cure, we have never seen anything close to as good as what we are seeing, in terms of how well we are able to reveal this virus.</p><p>“So from that point of view, we’re very hopeful that we are also able to see this type of response in an animal, and that we could eventually do this in humans.”</p><p>Dr Michael Roche of the University of Melbourne and co-senior author of the research, said the discovery could have broader implications beyond HIV, with the relevant white blood cells also involved in other diseases including cancers.</p><p>Dr Jonathan Stoye, a retrovirologist and emeritus scientist at the Francis Crick Institute, who was not involved in the study, said the approach taken by the Melbourne team appeared be a major advance on existing strategies to force the virus out of hiding, but further studies would be needed to determine how best to kill it after that.</p><p>He added: “Ultimately, one big unknown remains. Do you need to eliminate the entire reservoir for success or just the major part? If just 10% of the latent reservoir survives will that be sufficient to seed new infection? Only time will tell.</p><p>“However, that does not detract from the significance of the current study, which represents a major potential advance in delivery of mRNA for therapeutic purposes to blood cells.”</p><p>Prof Tomáš Hanke of the Jenner Institute, University of Oxford, disputed the idea that getting RNA into white blood cells had been a significant challenge. He said the hope that all cells in the body where HIV was hiding could be reached in this way was “merely a dream”.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Decreasing Gitlab repo backup times from 48 hours to 41 minutes (247 pts)]]></title>
            <link>https://about.gitlab.com/blog/2025/06/05/how-we-decreased-gitlab-repo-backup-times-from-48-hours-to-41-minutes/</link>
            <guid>44201975</guid>
            <pubDate>Fri, 06 Jun 2025 15:43:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://about.gitlab.com/blog/2025/06/05/how-we-decreased-gitlab-repo-backup-times-from-48-hours-to-41-minutes/">https://about.gitlab.com/blog/2025/06/05/how-we-decreased-gitlab-repo-backup-times-from-48-hours-to-41-minutes/</a>, See on <a href="https://news.ycombinator.com/item?id=44201975">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-v-67d56f21="" data-v-53094866="" data-v-74bd29c6=""><p>Repository backups are a critical component of any robust disaster recovery strategy. However, as repositories grow in size, the process of creating reliable backups becomes increasingly challenging.  Our own <a href="https://gitlab.com/gitlab-org/gitlab">Rails repository</a> was taking 48 hours to back up — forcing impossible choices between backup frequency and system performance. We wanted to tackle this issue for our customers and for our own users internally.</p>
<p>Ultimately, we traced the issue to a 15-year-old Git function with O(N²) complexity and fixed it with an algorithmic change, <strong>reducing backup times exponentially</strong>. The result: lower costs, reduced risk, and backup strategies that actually scale with your codebase.</p>
<p>This turned out to be a Git scalability issue that affects anyone with large repositories. Here's how we tracked it down and fixed it.</p>
<h2 id="backup-at-scale" tabindex="-1">Backup at scale <a href="#backup-at-scale"><svg width="24" height="24" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M12.2426 3.75736C11.4615 2.97631 10.1952 2.97631 9.41416 3.75736L7.99995 5.17157C7.60942 5.56209 6.97626 5.56209 6.58573 5.17157C6.19521 4.78105 6.19521 4.14788 6.58573 3.75736L7.99995 2.34314C9.56205 0.781046 12.0947 0.781046 13.6568 2.34314C15.2189 3.90524 15.2189 6.4379 13.6568 8L12.2426 9.41421C11.8521 9.80473 11.2189 9.80473 10.8284 9.41421C10.4379 9.02369 10.4379 8.39052 10.8284 8L12.2426 6.58578C13.0236 5.80473 13.0236 4.5384 12.2426 3.75736Z" fill="#333333"></path><path d="M10.5355 5.4645C10.926 5.85502 10.926 6.48819 10.5355 6.87871L6.87863 10.5356C6.4881 10.9261 5.85494 10.9261 5.46441 10.5356C5.07389 10.145 5.07389 9.51188 5.46441 9.12135L9.12127 5.4645C9.51179 5.07397 10.145 5.07397 10.5355 5.4645Z" fill="#333333"></path><path d="M3.75742 9.41422C2.97637 10.1953 2.97637 11.4616 3.75742 12.2426C4.53847 13.0237 5.8048 13.0237 6.58584 12.2426L8.00006 10.8284C8.39058 10.4379 9.02375 10.4379 9.41427 10.8284C9.8048 11.219 9.8048 11.8521 9.41427 12.2426L8.00006 13.6569C6.43796 15.219 3.9053 15.219 2.3432 13.6569C0.781107 12.0948 0.781107 9.56211 2.3432 8.00001L3.75742 6.5858C4.14794 6.19527 4.78111 6.19527 5.17163 6.5858C5.56216 6.97632 5.56215 7.60948 5.17163 8.00001L3.75742 9.41422Z" fill="#333333"></path></svg></a></h2>
<p>First, let's look at the problem. As organizations scale their repositories and backups grow more complex, here are some of the challenges they can face:</p>
<ul>
<li><strong>Time-prohibitive backups:</strong> For very large repositories, creating a repository backup could take several hours, which can hinder the ability to schedule regular backups.</li>
<li><strong>Resource intensity:</strong> Extended backup processes can consume substantial server resources, potentially impacting other operations.</li>
<li><strong>Backup windows:</strong> Finding adequate maintenance windows for such lengthy processes can be difficult for teams running 24/7 operations.</li>
<li><strong>Increased failure risk:</strong> Long-running processes are more susceptible to interruptions from network issues, server restarts, and system errors, which can force teams to restart the entire very long backup process from scratch.</li>
<li><strong>Race conditions:</strong> Because it takes a long time to create a backup, the repository might have changed a lot during the process, potentially creating an invalid backup or interrupting the backup because objects are no longer available.</li>
</ul>
<p>These challenges can lead to compromising on backup frequency or completeness – an unacceptable trade-off when it comes to data protection. Extended backup windows can force customers into workarounds. Some might adopt external tooling, while others might reduce backup frequency, resulting in potential inconsistent data protection strategies across organizations.</p>
<p>Now, let's dig into how we identified a performance bottleneck, found a resolution, and deployed it to help cut backup times.</p>
<h2 id="the-technical-challenge" tabindex="-1">The technical challenge <a href="#the-technical-challenge"><svg width="24" height="24" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M12.2426 3.75736C11.4615 2.97631 10.1952 2.97631 9.41416 3.75736L7.99995 5.17157C7.60942 5.56209 6.97626 5.56209 6.58573 5.17157C6.19521 4.78105 6.19521 4.14788 6.58573 3.75736L7.99995 2.34314C9.56205 0.781046 12.0947 0.781046 13.6568 2.34314C15.2189 3.90524 15.2189 6.4379 13.6568 8L12.2426 9.41421C11.8521 9.80473 11.2189 9.80473 10.8284 9.41421C10.4379 9.02369 10.4379 8.39052 10.8284 8L12.2426 6.58578C13.0236 5.80473 13.0236 4.5384 12.2426 3.75736Z" fill="#333333"></path><path d="M10.5355 5.4645C10.926 5.85502 10.926 6.48819 10.5355 6.87871L6.87863 10.5356C6.4881 10.9261 5.85494 10.9261 5.46441 10.5356C5.07389 10.145 5.07389 9.51188 5.46441 9.12135L9.12127 5.4645C9.51179 5.07397 10.145 5.07397 10.5355 5.4645Z" fill="#333333"></path><path d="M3.75742 9.41422C2.97637 10.1953 2.97637 11.4616 3.75742 12.2426C4.53847 13.0237 5.8048 13.0237 6.58584 12.2426L8.00006 10.8284C8.39058 10.4379 9.02375 10.4379 9.41427 10.8284C9.8048 11.219 9.8048 11.8521 9.41427 12.2426L8.00006 13.6569C6.43796 15.219 3.9053 15.219 2.3432 13.6569C0.781107 12.0948 0.781107 9.56211 2.3432 8.00001L3.75742 6.5858C4.14794 6.19527 4.78111 6.19527 5.17163 6.5858C5.56216 6.97632 5.56215 7.60948 5.17163 8.00001L3.75742 9.41422Z" fill="#333333"></path></svg></a></h2>
<p>GitLab's repository backup functionality relies on the <a href="https://git-scm.com/docs/git-bundle"><code>git bundle create</code></a> command, which captures a complete snapshot of a repository, including all objects and references like branches and tags. This bundle serves as a restoration point for recreating the repository in its exact state.</p>
<p>However, the implementation of the command suffered from poor scalability related to reference count, creating a performance bottleneck. As repositories accumulated more references, processing time increased exponentially. In our largest repositories containing millions of references, backup operations could extend beyond 48 hours.</p>
<h3 id="root-cause-analysis" tabindex="-1">Root cause analysis <a href="#root-cause-analysis"><svg width="24" height="24" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M12.2426 3.75736C11.4615 2.97631 10.1952 2.97631 9.41416 3.75736L7.99995 5.17157C7.60942 5.56209 6.97626 5.56209 6.58573 5.17157C6.19521 4.78105 6.19521 4.14788 6.58573 3.75736L7.99995 2.34314C9.56205 0.781046 12.0947 0.781046 13.6568 2.34314C15.2189 3.90524 15.2189 6.4379 13.6568 8L12.2426 9.41421C11.8521 9.80473 11.2189 9.80473 10.8284 9.41421C10.4379 9.02369 10.4379 8.39052 10.8284 8L12.2426 6.58578C13.0236 5.80473 13.0236 4.5384 12.2426 3.75736Z" fill="#333333"></path><path d="M10.5355 5.4645C10.926 5.85502 10.926 6.48819 10.5355 6.87871L6.87863 10.5356C6.4881 10.9261 5.85494 10.9261 5.46441 10.5356C5.07389 10.145 5.07389 9.51188 5.46441 9.12135L9.12127 5.4645C9.51179 5.07397 10.145 5.07397 10.5355 5.4645Z" fill="#333333"></path><path d="M3.75742 9.41422C2.97637 10.1953 2.97637 11.4616 3.75742 12.2426C4.53847 13.0237 5.8048 13.0237 6.58584 12.2426L8.00006 10.8284C8.39058 10.4379 9.02375 10.4379 9.41427 10.8284C9.8048 11.219 9.8048 11.8521 9.41427 12.2426L8.00006 13.6569C6.43796 15.219 3.9053 15.219 2.3432 13.6569C0.781107 12.0948 0.781107 9.56211 2.3432 8.00001L3.75742 6.5858C4.14794 6.19527 4.78111 6.19527 5.17163 6.5858C5.56216 6.97632 5.56215 7.60948 5.17163 8.00001L3.75742 9.41422Z" fill="#333333"></path></svg></a></h3>
<p>To identify the root cause of this performance bottleneck, we analyzed a flame graph of the command during execution.</p>
<p><img src="https://images.ctfassets.net/r9o86ar0p03f/7IS0wTEb44v1DTFKdklt1s/f8c724b1578ef0c8f550d9237f03dbeb/image1.jpg" alt="Flame graph showing command during execution"></p>
<p>A flame graph displays the execution path of a command through its stack trace. Each bar corresponds to a function in the code, with the bar's width indicating how much time the command spent executing within that particular function.</p>
<p>When examining the flame graph of <code>git bundle create</code> running on a repository with 10,000 references, approximately 80% of the execution time is consumed by the <code>object_array_remove_duplicates()</code> function. This function was introduced to Git in the <a href="https://gitlab.com/gitlab-org/git/-/commit/b2a6d1c686">commit b2a6d1c686</a> (bundle: allow the same ref to be given more than once, 2009-01-17).</p>
<p>To understand this change, it's important to know that <code>git bundle create</code> allows users to specify which references to include in the bundle. For complete repository bundles, the <code>--all</code> flag packages all references.</p>
<p>The commit addressed a problem where users providing duplicate references through the command line – such as <code>git bundle create main.bundle main main</code> - would create a bundle without properly handling the duplicated main reference. Unbundling this bundle in a Git repository would break, because it tries to write the same ref twice. The code to avoid duplication uses nested <code>for</code> loops that iterate through all references to identify duplicates. This O(N²) algorithm becomes a significant performance bottleneck in repositories with large reference counts, consuming substantial processing time.</p>
<h3 id="the-fix-from-o(n%C2%B2)-to-efficient-mapping" tabindex="-1">The fix: From O(N²) to efficient mapping <a href="#the-fix-from-o(n%C2%B2)-to-efficient-mapping"><svg width="24" height="24" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M12.2426 3.75736C11.4615 2.97631 10.1952 2.97631 9.41416 3.75736L7.99995 5.17157C7.60942 5.56209 6.97626 5.56209 6.58573 5.17157C6.19521 4.78105 6.19521 4.14788 6.58573 3.75736L7.99995 2.34314C9.56205 0.781046 12.0947 0.781046 13.6568 2.34314C15.2189 3.90524 15.2189 6.4379 13.6568 8L12.2426 9.41421C11.8521 9.80473 11.2189 9.80473 10.8284 9.41421C10.4379 9.02369 10.4379 8.39052 10.8284 8L12.2426 6.58578C13.0236 5.80473 13.0236 4.5384 12.2426 3.75736Z" fill="#333333"></path><path d="M10.5355 5.4645C10.926 5.85502 10.926 6.48819 10.5355 6.87871L6.87863 10.5356C6.4881 10.9261 5.85494 10.9261 5.46441 10.5356C5.07389 10.145 5.07389 9.51188 5.46441 9.12135L9.12127 5.4645C9.51179 5.07397 10.145 5.07397 10.5355 5.4645Z" fill="#333333"></path><path d="M3.75742 9.41422C2.97637 10.1953 2.97637 11.4616 3.75742 12.2426C4.53847 13.0237 5.8048 13.0237 6.58584 12.2426L8.00006 10.8284C8.39058 10.4379 9.02375 10.4379 9.41427 10.8284C9.8048 11.219 9.8048 11.8521 9.41427 12.2426L8.00006 13.6569C6.43796 15.219 3.9053 15.219 2.3432 13.6569C0.781107 12.0948 0.781107 9.56211 2.3432 8.00001L3.75742 6.5858C4.14794 6.19527 4.78111 6.19527 5.17163 6.5858C5.56216 6.97632 5.56215 7.60948 5.17163 8.00001L3.75742 9.41422Z" fill="#333333"></path></svg></a></h3>
<p>To resolve this performance issue, we contributed an upstream fix to Git that replaces the nested loops with a map data structure. Each reference is added to the map, which automatically ensures only a single copy of each reference is retained for processing.</p>
<p>This change dramatically enhances the performance of <code>git bundle create</code> and enables much better scalability in repositories with large reference counts. Benchmark testing on a repository with 10,000 references demonstrates a 6x performance improvement.</p>
<pre><code>Benchmark 1: bundle (refcount = 100000, revision = master)
  Time (mean ± σ): 	14.653 s ±  0.203 s	[User: 13.940 s, System: 0.762 s]
  Range (min … max):   14.237 s … 14.920 s	10 runs

Benchmark 2: bundle (refcount = 100000, revision = HEAD)
  Time (mean ± σ):  	2.394 s ±  0.023 s	[User: 1.684 s, System: 0.798 s]
  Range (min … max):	2.364 s …  2.425 s	10 runs

Summary
  bundle (refcount = 100000, revision = HEAD) ran
	6.12 ± 0.10 times faster than bundle (refcount = 100000, revision = master)
</code></pre>
<p>The patch was accepted and <a href="https://gitlab.com/gitlab-org/git/-/commit/bb74c0abbc31da35be52999569ea481ebd149d1d">merged</a> into upstream Git. At GitLab, we backported this fix to ensure our customers could benefit immediately, without waiting for the next Git release.</p>
<h2 id="the-result-dramatically-decreased-backup-times" tabindex="-1">The result: Dramatically decreased backup times <a href="#the-result-dramatically-decreased-backup-times"><svg width="24" height="24" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M12.2426 3.75736C11.4615 2.97631 10.1952 2.97631 9.41416 3.75736L7.99995 5.17157C7.60942 5.56209 6.97626 5.56209 6.58573 5.17157C6.19521 4.78105 6.19521 4.14788 6.58573 3.75736L7.99995 2.34314C9.56205 0.781046 12.0947 0.781046 13.6568 2.34314C15.2189 3.90524 15.2189 6.4379 13.6568 8L12.2426 9.41421C11.8521 9.80473 11.2189 9.80473 10.8284 9.41421C10.4379 9.02369 10.4379 8.39052 10.8284 8L12.2426 6.58578C13.0236 5.80473 13.0236 4.5384 12.2426 3.75736Z" fill="#333333"></path><path d="M10.5355 5.4645C10.926 5.85502 10.926 6.48819 10.5355 6.87871L6.87863 10.5356C6.4881 10.9261 5.85494 10.9261 5.46441 10.5356C5.07389 10.145 5.07389 9.51188 5.46441 9.12135L9.12127 5.4645C9.51179 5.07397 10.145 5.07397 10.5355 5.4645Z" fill="#333333"></path><path d="M3.75742 9.41422C2.97637 10.1953 2.97637 11.4616 3.75742 12.2426C4.53847 13.0237 5.8048 13.0237 6.58584 12.2426L8.00006 10.8284C8.39058 10.4379 9.02375 10.4379 9.41427 10.8284C9.8048 11.219 9.8048 11.8521 9.41427 12.2426L8.00006 13.6569C6.43796 15.219 3.9053 15.219 2.3432 13.6569C0.781107 12.0948 0.781107 9.56211 2.3432 8.00001L3.75742 6.5858C4.14794 6.19527 4.78111 6.19527 5.17163 6.5858C5.56216 6.97632 5.56215 7.60948 5.17163 8.00001L3.75742 9.41422Z" fill="#333333"></path></svg></a></h2>
<p>The performance gains from this improvement have been nothing short of transformative:</p>
<ul>
<li><strong>From 48 hours to 41 minutes:</strong> Creating a backup of our largest repository (<code>gitlab-org/gitlab</code>) now takes just 1.4% of the original time.</li>
<li><strong>Consistent performance:</strong> The improvement scales reliably across repository sizes.</li>
<li><strong>Resource efficiency:</strong> We significantly reduced server load during backup operations.</li>
<li><strong>Broader applicability:</strong> While backup creation sees the most dramatic improvement, all bundle-based operations that operate on many references benefit.</li>
</ul>
<h2 id="what-this-means-for-gitlab-customers" tabindex="-1">What this means for GitLab customers <a href="#what-this-means-for-gitlab-customers"><svg width="24" height="24" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M12.2426 3.75736C11.4615 2.97631 10.1952 2.97631 9.41416 3.75736L7.99995 5.17157C7.60942 5.56209 6.97626 5.56209 6.58573 5.17157C6.19521 4.78105 6.19521 4.14788 6.58573 3.75736L7.99995 2.34314C9.56205 0.781046 12.0947 0.781046 13.6568 2.34314C15.2189 3.90524 15.2189 6.4379 13.6568 8L12.2426 9.41421C11.8521 9.80473 11.2189 9.80473 10.8284 9.41421C10.4379 9.02369 10.4379 8.39052 10.8284 8L12.2426 6.58578C13.0236 5.80473 13.0236 4.5384 12.2426 3.75736Z" fill="#333333"></path><path d="M10.5355 5.4645C10.926 5.85502 10.926 6.48819 10.5355 6.87871L6.87863 10.5356C6.4881 10.9261 5.85494 10.9261 5.46441 10.5356C5.07389 10.145 5.07389 9.51188 5.46441 9.12135L9.12127 5.4645C9.51179 5.07397 10.145 5.07397 10.5355 5.4645Z" fill="#333333"></path><path d="M3.75742 9.41422C2.97637 10.1953 2.97637 11.4616 3.75742 12.2426C4.53847 13.0237 5.8048 13.0237 6.58584 12.2426L8.00006 10.8284C8.39058 10.4379 9.02375 10.4379 9.41427 10.8284C9.8048 11.219 9.8048 11.8521 9.41427 12.2426L8.00006 13.6569C6.43796 15.219 3.9053 15.219 2.3432 13.6569C0.781107 12.0948 0.781107 9.56211 2.3432 8.00001L3.75742 6.5858C4.14794 6.19527 4.78111 6.19527 5.17163 6.5858C5.56216 6.97632 5.56215 7.60948 5.17163 8.00001L3.75742 9.41422Z" fill="#333333"></path></svg></a></h2>
<p>For GitLab customers, this enhancement delivers immediate and tangible benefits on how organizations approach repository backup and disaster recovery planning:</p>
<ul>
<li><strong>Transformed backup strategies</strong>
<ul>
<li>Enterprise teams can establish comprehensive nightly schedules without impacting development workflows or requiring extensive backup windows.</li>
<li>Backups can now run seamlessly in the background during nightly schedules, instead of needing to be dedicated and lengthy.</li>
</ul>
</li>
<li><strong>Enhanced business continuity</strong>
<ul>
<li>With backup times reduced from days to minutes, organizations significantly minimize their recovery point objectives (RPO). This translates to reduced business risk – in a disaster scenario, you're potentially recovering hours of work instead of days.</li>
</ul>
</li>
<li><strong>Reduced operational overhead</strong>
<ul>
<li>Less server resource consumption and shorter maintenance windows.</li>
<li>Shorter backup windows mean reduced compute costs, especially in cloud environments, where extended processing time translates directly to higher bills.</li>
</ul>
</li>
<li><strong>Future-proofed infrastructure</strong>
<ul>
<li>Growing repositories no longer force difficult choices between backup frequency and system performance.</li>
<li>As your codebase expands, your backup strategy can scale seamlessly alongside it</li>
</ul>
</li>
</ul>
<p>Organizations can now implement more robust backup strategies without compromising on performance or completeness. What was once a challenging trade-off has become a straightforward operational practice.</p>
<p>Starting with the <a href="https://about.gitlab.com/releases/2025/05/15/gitlab-18-0-released/">GitLab 18.0</a> release, all GitLab customers regardless of their license tier can already fully take advantage of these improvements for their <a href="https://docs.gitlab.com/administration/backup_restore/backup_gitlab/">backup</a> strategy and execution. There is no further change in configuration required.</p>
<h2 id="what's-next" tabindex="-1">What's next <a href="#what's-next"><svg width="24" height="24" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M12.2426 3.75736C11.4615 2.97631 10.1952 2.97631 9.41416 3.75736L7.99995 5.17157C7.60942 5.56209 6.97626 5.56209 6.58573 5.17157C6.19521 4.78105 6.19521 4.14788 6.58573 3.75736L7.99995 2.34314C9.56205 0.781046 12.0947 0.781046 13.6568 2.34314C15.2189 3.90524 15.2189 6.4379 13.6568 8L12.2426 9.41421C11.8521 9.80473 11.2189 9.80473 10.8284 9.41421C10.4379 9.02369 10.4379 8.39052 10.8284 8L12.2426 6.58578C13.0236 5.80473 13.0236 4.5384 12.2426 3.75736Z" fill="#333333"></path><path d="M10.5355 5.4645C10.926 5.85502 10.926 6.48819 10.5355 6.87871L6.87863 10.5356C6.4881 10.9261 5.85494 10.9261 5.46441 10.5356C5.07389 10.145 5.07389 9.51188 5.46441 9.12135L9.12127 5.4645C9.51179 5.07397 10.145 5.07397 10.5355 5.4645Z" fill="#333333"></path><path d="M3.75742 9.41422C2.97637 10.1953 2.97637 11.4616 3.75742 12.2426C4.53847 13.0237 5.8048 13.0237 6.58584 12.2426L8.00006 10.8284C8.39058 10.4379 9.02375 10.4379 9.41427 10.8284C9.8048 11.219 9.8048 11.8521 9.41427 12.2426L8.00006 13.6569C6.43796 15.219 3.9053 15.219 2.3432 13.6569C0.781107 12.0948 0.781107 9.56211 2.3432 8.00001L3.75742 6.5858C4.14794 6.19527 4.78111 6.19527 5.17163 6.5858C5.56216 6.97632 5.56215 7.60948 5.17163 8.00001L3.75742 9.41422Z" fill="#333333"></path></svg></a></h2>
<p>This breakthrough is part of our ongoing commitment to scalable, enterprise-grade Git infrastructure. While the improvement of 48 hours to 41 minutes for backup creation time represents a significant milestone, we continue to identify and address performance bottlenecks throughout our stack.</p>
<p>We're particularly proud that this enhancement was contributed upstream to the Git project, benefiting not just GitLab users but the broader Git community. This collaborative approach to development ensures that improvements are thoroughly reviewed, widely tested, and available to all.</p>
<blockquote>
<p>Deep infrastructure work like this is how we approach performance at GitLab. Join the GitLab 18 virtual launch event to see what other fundamental improvements we're shipping. <a href="https://about.gitlab.com/eighteen/">Register today!</a></p>
</blockquote>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[4-7-8 Breathing (158 pts)]]></title>
            <link>https://www.breathbelly.com/exercises/4-7-8-breathing</link>
            <guid>44201901</guid>
            <pubDate>Fri, 06 Jun 2025 15:36:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.breathbelly.com/exercises/4-7-8-breathing">https://www.breathbelly.com/exercises/4-7-8-breathing</a>, See on <a href="https://news.ycombinator.com/item?id=44201901">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Meta: Shut Down Your Invasive AI Discover Feed. Now (389 pts)]]></title>
            <link>https://www.mozillafoundation.org/en/campaigns/meta-shut-down-your-invasive-ai-discover-feed-now/</link>
            <guid>44201872</guid>
            <pubDate>Fri, 06 Jun 2025 15:33:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mozillafoundation.org/en/campaigns/meta-shut-down-your-invasive-ai-discover-feed-now/">https://www.mozillafoundation.org/en/campaigns/meta-shut-down-your-invasive-ai-discover-feed-now/</a>, See on <a href="https://news.ycombinator.com/item?id=44201872">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            
    <p data-block-key="ox7g0"><b>Meta is quietly turning private AI chats into public content — and too many people don’t realize it’s happening.</b></p><p data-block-key="a6hs9">That’s why the Mozilla community is demanding that Meta:</p><ol><li data-block-key="f4j6n"><b>Shut down the Discover feed</b> until real privacy protections are in place.</li><li data-block-key="3q7uu"><b>Make all AI interactions private by default</b> with no public sharing option unless explicitly enabled through informed consent.</li><li data-block-key="ddf8c"><b>Provide full transparency</b> about how many users have unknowingly shared private information.</li><li data-block-key="cfiao"><b>Create a universal, easy-to-use opt-out system</b> for all Meta platforms that prevents user data from being used for AI training.</li><li data-block-key="8j7da"><b>Notify all users whose conversations may have been made public</b>, and allow them to delete their content permanently.</li></ol><p data-block-key="3i8bo">Meta is blurring the line between private and public — and it’s happening at the cost of our privacy. People have the right to know when they’re speaking in public, especially when they believe they’re speaking in private.</p><p data-block-key="9cait">If you agree, add your name to demand Meta shut down its invasive AI feed — and guarantee that no private conversations are made public without clear, explicit, and informed opt-in consent.</p>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sandia turns on brain-like storage-free supercomputer (118 pts)]]></title>
            <link>https://blocksandfiles.com/2025/06/06/sandia-turns-on-brain-like-storage-free-supercomputer/</link>
            <guid>44201812</guid>
            <pubDate>Fri, 06 Jun 2025 15:24:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blocksandfiles.com/2025/06/06/sandia-turns-on-brain-like-storage-free-supercomputer/">https://blocksandfiles.com/2025/06/06/sandia-turns-on-brain-like-storage-free-supercomputer/</a>, See on <a href="https://news.ycombinator.com/item?id=44201812">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <!-- image --><div><figure><a href="https://blocksandfiles.com/wp-content/uploads/2025/06/CFA8505la-1.jpg" data-caption="Brad Theilman facing camera, and Felix Wang, background, unpack a new shipment of the SpiNNaker2 computing core.  It is a collaboration between Sandia and SpiNNcloud and will be the first in world commercial product from the cooperation. 

Funded through NNSA’s Advanced Simulation and Computing (ASC) program, this work will explore how neuromorphic computing can be leveraged for the nation’s nuclear deterrence missions.

SpiNNaker is a contraction of ‘Spiking Neural Network Architecture,’ which is a brain inspired neuromorphic computer for large-scale, real-time modeling of brain-like applications. This technology can simulate large brain-like networks to enhance researchers’ understanding of the brain, as well as provide a framework to test the boundaries of current computing capabilities.

Photo by Craig Fritz"><img width="696" height="464" src="https://blocksandfiles.com/wp-content/uploads/2025/06/CFA8505la-1-696x464.jpg" srcset="https://blocksandfiles.com/wp-content/uploads/2025/06/CFA8505la-1-696x464.jpg 696w, https://blocksandfiles.com/wp-content/uploads/2025/06/CFA8505la-1-300x200.jpg 300w, https://blocksandfiles.com/wp-content/uploads/2025/06/CFA8505la-1-1024x683.jpg 1024w, https://blocksandfiles.com/wp-content/uploads/2025/06/CFA8505la-1-768x512.jpg 768w, https://blocksandfiles.com/wp-content/uploads/2025/06/CFA8505la-1-1068x712.jpg 1068w, https://blocksandfiles.com/wp-content/uploads/2025/06/CFA8505la-1-630x420.jpg 630w, https://blocksandfiles.com/wp-content/uploads/2025/06/CFA8505la-1.jpg 1500w" sizes="(max-width: 696px) 100vw, 696px" alt="" title="SpiNNaker Computer"></a><figcaption>Brad Theilman facing camera, and Felix Wang, background, unpack a new shipment of the SpiNNaker2 computing core.  It is a collaboration between Sandia and SpiNNcloud and will be the first in world commercial product from the cooperation. 

Funded through NNSA’s Advanced Simulation and Computing (ASC) program, this work will explore how neuromorphic computing can be leveraged for the nation’s nuclear deterrence missions.

SpiNNaker is a contraction of ‘Spiking Neural Network Architecture,’ which is a brain inspired neuromorphic computer for large-scale, real-time modeling of brain-like applications. This technology can simulate large brain-like networks to enhance researchers’ understanding of the brain, as well as provide a framework to test the boundaries of current computing capabilities.

Photo by Craig Fritz</figcaption></figure></div>
            <!-- content -->
<p>Sandia National Labs has flipped the switch on its SpiNNaker 2 “brain-inspired” supercomputer that eschews both GPUs and internal storage.</p>



<p>The system, supplied by Germany-based SpiNNcloud, will rank amongst the top five “brain inspired” platforms, mimicking between 150 and 180 million neurons.</p>



<p>The architecture was initially <a href="https://www.theregister.com/2017/10/19/steve_furber_arm_brain_interview/">developed </a>by Arm pioneer Steve Furber, though it still falls somewhat short of the human brain’s 100 billion neurons.</p>



<p>As SpiNNcloud explains it, the SpiNNaker 2’s highly parallel architecture has 48 SpiNNaker 2 chips per server board, each of which in turn carries 152 based cores and specialized accelerators.</p>



<p>Each of the 48 chips packs 20 MB of SRAM, with each board carrying 96 GB of external LPDDR4 external memory. So, with 90 boards, that amounts to 8640 GB of DRAM, while a 1440 board system carries 69,120 chips and 138240 TB of DRAM.</p>



<p>Needless to say, the system uses high-speed chip to chip communication. And this, SpiNNcloud says, eliminates the need for centralized storage. That, and the vast amount of memory.</p>



<h2>Speeding on DRAM</h2>



<p>In Sandia’s case, it has taken delivery of a 24 board, 175,000 core system. At <a href="https://blocksandfiles.com/2022/09/05/doe-data-management-research/">Sandia</a>, according to <a href="https://spinncloud.com/">SpiNNcloud</a>, “The Supercomputer is hooked in to existing HPC systems and does not contain any OS or disks. The speed is generated by keeping data in the SRAM and DRAM.”</p>



<p>Standard parallel ethernet ports are “sufficient for loading/saving the data.” &nbsp;The “current maximum system” is over 10.5m cores, which SpiNNcloud says means it can maintain biological real time.</p>



<p>Moreover, it allows complex event-driven compute and simulations with more power efficiency compared to GPU systems.</p>



<p>Hector A. Gonzalez, co-founder&nbsp;and CEO of SpiNNcloud, said the system would be targeted at problems in “next generation defense and beyond. The SpiNNaker2’s efficiency gains make it particularly well-suited for the demanding computational needs of national security applications.”</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI is retaining all ChatGPT logs "indefinitely." Here's who's affected (111 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2025/06/openai-confronts-user-panic-over-court-ordered-retention-of-chatgpt-logs/</link>
            <guid>44201785</guid>
            <pubDate>Fri, 06 Jun 2025 15:21:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2025/06/openai-confronts-user-panic-over-court-ordered-retention-of-chatgpt-logs/">https://arstechnica.com/tech-policy/2025/06/openai-confronts-user-panic-over-court-ordered-retention-of-chatgpt-logs/</a>, See on <a href="https://news.ycombinator.com/item?id=44201785">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
          
<p>In the copyright fight, Magistrate Judge Ona Wang granted the order within one day of the NYT's request. She agreed with news plaintiffs that it seemed likely that ChatGPT users may be spooked by the lawsuit and possibly set their chats to delete when using the chatbot to skirt NYT paywalls. Because OpenAI wasn't sharing deleted chat logs, the news plaintiffs had no way of proving that, she suggested.</p>
<p>Now, OpenAI is not only asking Wang to reconsider but has "also appealed this order with the District Court Judge," the Thursday statement said.</p>
<p>"We strongly believe this is an overreach by the New York Times," Lightcap said. "We’re continuing to appeal this order so we can keep putting your trust and privacy first."</p>
<h2>Who can access deleted chats?</h2>
<p>To protect users, OpenAI provides an <a href="https://openai.com/index/response-to-nyt-data-demands/">FAQ</a> that clearly explains why their data is being retained and how it could be exposed.</p>
<p>For example, the statement noted that the order doesn’t impact OpenAI API business customers under Zero Data Retention agreements because their data is never stored.</p>
<p>And for users whose data is affected, OpenAI noted that their deleted chats could be accessed, but they won't "automatically" be shared with The New York Times. Instead, the retained data will be "stored separately in a secure system" and "protected under legal hold, meaning it can’t be accessed or used for purposes other than meeting legal obligations," OpenAI explained.</p>
<p>Of course, with the court battle ongoing, the FAQ did not have all the answers.</p>
<p>Nobody knows how long OpenAI may be required to retain the deleted chats. Likely seeking to reassure users—some of which appeared to be considering switching to a rival service until the order lifts—OpenAI noted that "only a small, audited OpenAI legal and security team would be able to access this data as necessary to comply with our legal obligations."</p>


          
                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An Interactive Guide to Rate Limiting (104 pts)]]></title>
            <link>https://blog.sagyamthapa.com.np/interactive-guide-to-rate-limiting</link>
            <guid>44201583</guid>
            <pubDate>Fri, 06 Jun 2025 14:58:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.sagyamthapa.com.np/interactive-guide-to-rate-limiting">https://blog.sagyamthapa.com.np/interactive-guide-to-rate-limiting</a>, See on <a href="https://news.ycombinator.com/item?id=44201583">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-content-parent"><h2 id="heading-introduction">Introduction</h2>
<p>Rate limiting is a must have strategy in every back-end app. It prevent one user from overusing a resource and degrading the quality of service for other users. Here are some benefits of rate limiting</p>
<ul>
<li><p>It presents resource starvation</p>
</li>
<li><p>Reduces server hosting cost</p>
</li>
<li><p>Provides basic protection against <a target="_blank" href="https://en.wikipedia.org/wiki/Denial-of-service_attack">DDoS</a></p>
</li>
</ul>
<p>I have made four interactive app that let’s you play around with common rate limiting algorithms.</p>
<h2 id="heading-token-bucket">Token bucket</h2>
<h3 id="heading-working">Working:</h3>
<ul>
<li><p>A bucket holds fixed number tokens</p>
</li>
<li><p>Tokens are added to bucket at fixed rate</p>
</li>
<li><p>When a request comes in:</p>
<ul>
<li><p>If a token is available, it’s removed from the bucket and the request is allowed.</p>
</li>
<li><p>If no tokens are available, the request is rejected or delayed.</p>
</li>
</ul>
</li>
<li><p>Allows for occasional short burst if tokens are available</p>
</li>
</ul>
<p>I have created an <a target="_blank" href="https://tools.sagyamthapa.com.np/token-bucket">app</a> that let’s you play with leaky bucket algorithm.</p>

<h2 id="heading-leaky-bucket">Leaky bucket</h2>
<h3 id="heading-working-1">Working</h3>
<ul>
<li><p>Think of it as a bucket leaking at a fixed rate</p>
</li>
<li><p>Incoming requests are added to the bucket</p>
</li>
<li><p>Requests are processed (or "leak") at a <strong>constant rate</strong></p>
</li>
<li><p>If the bucket is full when a new request arrives, the request is dropped</p>
</li>
<li><p>Smooths out bursts; outputs requests at a steady rate</p>
<p>  I have made an <a target="_blank" href="https://tools.sagyamthapa.com.np/leaky-bucket">app</a> that let’s you play with leaky bucket algorithm.</p>
</li>
</ul>

<h2 id="heading-fixed-window-counter">Fixed window counter</h2>
<h3 id="heading-working-2">Working:</h3>
<ul>
<li><p>Time is divided into fixed size windows (e.g., 1 minute)</p>
</li>
<li><p>A counter tracks the number of requests per client/IP in the current window</p>
</li>
<li><p>If the count exceeds the limit, further requests are rejected until the next window</p>
</li>
<li><p>Simple and efficient, but allows burst traffic spike at end/start</p>
<p>  I have created an <a target="_blank" href="https://tools.sagyamthapa.com.np/fixed-window">app</a> that let’s you play with fixed bucket algorithm.</p>
</li>
</ul>

<h2 id="heading-sliding-window-counter">Sliding window counter</h2>
<h3 id="heading-working-3">Working:</h3>
<ul>
<li><p>Keeps a timestamped log of each request</p>
</li>
<li><p>When a request comes in, logs are checked to count how many requests were made in the last <code>X</code> seconds</p>
</li>
<li><p>If under the limit, the request is allowed and logged; otherwise, it’s rejected</p>
<p>  I have created an <a target="_blank" href="https://tools.sagyamthapa.com.np/sliding-window">app</a> that let’s you play with sliding bucket algorithm.</p>
</li>
</ul>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Top researchers leave Intel to build startup with 'the biggest, baddest CPU' (124 pts)]]></title>
            <link>https://www.oregonlive.com/silicon-forest/2025/06/top-researchers-leave-intel-to-build-startup-with-the-biggest-baddest-cpu.html</link>
            <guid>44201072</guid>
            <pubDate>Fri, 06 Jun 2025 14:07:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.oregonlive.com/silicon-forest/2025/06/top-researchers-leave-intel-to-build-startup-with-the-biggest-baddest-cpu.html">https://www.oregonlive.com/silicon-forest/2025/06/top-researchers-leave-intel-to-build-startup-with-the-biggest-baddest-cpu.html</a>, See on <a href="https://news.ycombinator.com/item?id=44201072">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p id="SZALMQPN25DQXLA42EDYATXDTU">Together, the four founders of Beaverton startup AheadComputing spent nearly a century at Intel.</p><p id="7DQCWW5M6FCYHGBBFPKHQXK4JY">They were among Intel’s top chip architects, working years in advance to develop new generations of microprocessors to power the computers of the future. </p><p id="LZDFRYI46NAPVL4BRWZ7BECEZY">Now they’re on their own, flying without a net, building a new class of microprocessor on an entirely different architecture from Intel’s. Founded a year ago, AheadComputing is trying to prove there’s a better way to design computer chips.</p><p id="KZAFSZ7L4NGABHBUFUJRIIPDSY">“AheadComputing is doing the biggest, baddest CPU in the world,” said Debbie Marr, the company’s CEO.</p><p id="3BBPK4LLVBD45LBEIE7XIP4ZRQ">A CPU is a central processing unit, the brain inside a computer. Intel has dominated the CPU market for decades, selling processors based on an architecture the chipmaker owns, called x86.</p><p id="OH4FNLKKOVGHPJV4QDYNBHFTNQ">Today, the computing ecosystem is fragmenting as new standards emerge. That’s one of the reasons Intel is struggling, as computing mainstays like Apple and Google use rival architectures to design their own chips for PCs, smartphones and data centers. </p><p id="HPECJENTQBAWRB4AWMGKF3NCLQ">AheadComputing is betting on an open architecture called RISC-V — RISC stands for “reduced instruction set computer.” The idea is to craft a streamlined microprocessor that works more efficiently by doing fewer things, and doing them better than conventional processors. </p><p id="MJOKIYKAMBFJBH6L2G6WGXJKH4">For AheadComputing’s founders and 80 employees, many of them also Intel alumni, it’s a major break from the kind of work they’ve been doing all their careers. They’ve left a company with more than 100,000 workers to start a business with fewer than 100.</p><p id="ILTV5O7QBJBOTJEOFGZQE3MZLI">“Every person in this room,” Marr said, looking across a conference table at her colleagues, “we could have stayed at Intel. We could have continued to do very exciting things at Intel.” </p><p id="Q5CNYQ7T5VFRDPL4DIWETUUI4M">They decided they had a better chance at leading a revolution in semiconductor technology at a startup than at a big, established company like Intel. And AheadComputing could be at the forefront of renewal in Oregon’s semiconductor ecosystem.</p><p id="WQTZVEPSNZECLK6KTHTKXCF3RY">“We see this opportunity, this light,” Marr said. “We took our chances.”</p><p id="N7SOASVO45EHPERYNSS4BFW6X4"><b>Big players lose clout</b></p><p id="HCKLZMD2FVHVTETV7IRCDQSE3M">Intel has been operating in Oregon since the 1970s and has long been Oregon’s largest corporate employer. It has more than 20,000 employees in Washington County, including some of the world’s top semiconductor researchers. </p><p id="ONOR6PP4RNAJRLZVXPC2KIXG5A">Very rarely, though, have any of those thousands of workers left to start their own chip business.</p><p id="4AHCXCVNAJGNJAVTERAAIN3KUY">That’s partly because it’s prohibitively expensive for a new company to build a semiconductor factory, and partly because Intel and other tech giants controlled the essential pieces around the CPU. It was hard for newcomers to break in.</p><p id="RISUTA4HUJDQHKDW72ZCA6E6JE">Today’s chip startups, however, don’t need their own factories. So-called fabless semiconductor designers like AheadComputing can outsource manufacturing to contractors like Taiwan Semiconductor Manufacturing Co. The graphics and AI chip designer Nvidia has become a $3 trillion company — the world’s most valuable business — doing exactly that.</p><p id="53UVBLCX3ZFM3G3SYL74CD7C44">And AheadComputing cofounder Jonathan Pearce says the reign of the chip titans is fading as the systems that run computers break up into a series of “chiplets” from various companies, optimized for specific tasks.</p><p id="GBQR6PXGGJF6FFG2EICXU2T7YU">“You get the opportunity for a company like AheadComputing to provide that piece of the overall system,” Pearce said. “As opposed to the past 20 years where it was just one tech giant.”</p><p id="D3PIRDSVKZA7ZJXRSTPT27KJ3U">RISC-V, pronounced “Risk-five,” is especially inviting to startups because it’s an open technology, overseen by a consortium of tech firms. Unlike processors based on x86 or ARM designs, there’s no licensing fee to use RISC-V.</p><p id="BY7NDKNCC5FPFDKSSA3VVISY2I">AheadComputing maintains that open standards are the future for microprocessors, giving technology companies more opportunity to customize chip designs to suit their specific needs. </p><p id="2BXOY3ZH3VCDVJ647GT3UJ2UQQ">It will be years before AheadComputing’s designs are on the market, but it envisions its chips will someday power PCs, laptops and data centers. Possible clients could include Google, Amazon, Samsung or other large computing companies. </p><figure><div><picture><source media="(max-width: 500px)" srcset="https://www.oregonlive.com/resizer/v2/TWYNPIP45ZH7NID424ARQ37OEU.jpg?auth=04f5165a951bcd601cae3bc30cdb1cd3c55fb4ded8b6b6b2ffd23140ec990dcd&amp;width=500&amp;quality=90 500w"><source media="(max-width: 800px)" srcset="https://www.oregonlive.com/resizer/v2/TWYNPIP45ZH7NID424ARQ37OEU.jpg?auth=04f5165a951bcd601cae3bc30cdb1cd3c55fb4ded8b6b6b2ffd23140ec990dcd&amp;width=800&amp;quality=90 800w"><source srcset="https://www.oregonlive.com/resizer/v2/TWYNPIP45ZH7NID424ARQ37OEU.jpg?auth=04f5165a951bcd601cae3bc30cdb1cd3c55fb4ded8b6b6b2ffd23140ec990dcd&amp;width=1280&amp;quality=90 1280w"><img src="https://www.oregonlive.com/resizer/v2/TWYNPIP45ZH7NID424ARQ37OEU.jpg?auth=04f5165a951bcd601cae3bc30cdb1cd3c55fb4ded8b6b6b2ffd23140ec990dcd&amp;width=500&amp;quality=90" sizes="50vw" fetchpriority="low" loading="lazy" alt="AheadComputing" decoding="async" height="600" width="300"></picture><figcaption><p>Members of AheadComputing's executive team. From left to right: Mark Dechene (co-founder), Alon Mahl (vice president), Jonathan Pearce (co-founder), Srikanth Srinivasan (co-founder), Debbie Marr (co-founder, CEO)<span>Mike Rogoway/The Oregonian</span></p></figcaption></div></figure><p id="FW5OQ4UJ7JCXZAERWP22SQOC6E">That vision, and the pedigree of the Intel alumni leading the Beaverton startup, has attracted a lot of attention within the industry. AheadComputing <a href="https://www.oregonlive.com/silicon-forest/2025/02/intel-alumni-raise-215-million-for-portland-computing-startup.html" target="_blank" rel="">raised $22 million in venture capital</a> in February. Prominent semiconductor engineer Jim Keller — a superstar in the rarified field of chip architecture, with stints at Intel, AMD and Apple — turned heads when he joined AheadComputing’s board a month later.</p><p id="FZUCJEBS3RFLDKHPRXWZ2GDBS4">The semiconductor industry is undergoing a transformation as new chips emerge for artificial intelligence and chipmakers test the laws of physics with radical innovations like quantum computing. </p><p id="BOTIYCVVIFDGDP7FPS4M3RO56U">“This is where the future is. It’s not about x86. It’s not traditional computer architecture. That’s pretty much a dead field,” said Christof Teuscher, a Portland State University professor who teaches microprocessor design and computer architecture. </p><p id="L6Q3Z2UAB5GGJA24C6HRUXAACM">AheadComputing is arriving at the right moment, Teuscher said, positioning itself to capitalize on upheaval in the industry — and the ongoing exodus of Intel employees. At this point, though, he said no one can know which new chip architectures will prevail.</p><p id="CX3FWJ5PJBCJROADK7DMOEYOIE">Traditionally, Teuscher said RISC-V has been used for embedded systems and academic research. He said he’s skeptical that it can prevail as the industry moves toward high-performance designs for artificial intelligence and supercomputing.</p><p id="JXX3ZXH345HYVKF7MRJTHOYWSE">“There’s high risk, but potentially high payoff,” Teuscher said. “And that’s definitely what I would describe their approach.”</p><p id="OWDPDKTWMJEHREWQITVDAOATPQ"><b>Silicon Forest’s new growth</b></p><p id="C3PWNDVWVJALJGH4LLGTACPP3I">At Intel, AheadComputing’s founders were accustomed to working with cutting-edge manufacturing tools and enormous research labs. Now they’re doing everything themselves — setting up their Wi-Fi network, adding memory to desktop computers and ordering snacks and coffee for the break room. </p><p id="TWQTCO3V3JHFFEU5H3UYENS224">Alon Mahl left Intel to be AheadComputing’s vice president of design verification, a key role ensuring the company’s chips work as expected. But one of his first tasks was to find and lease an office. He picked a spot on the top floor of The Round development near downtown Beaverton. </p><p id="GNVICN7BLJFB3H5OXPYXH4PNJQ">“That’s what’s exciting, is that you’re not doing only one thing. You’re doing everything,” Mahl said. “And each one is doing whatever he can to make this company a success. So that’s why I feel energized.”</p><p id="UFBBQY76EJAPPDSEJNXVSQH4F4">Intel provided the energy and innovation that fueled several generations of new chip technologies. It spent tens of billions to stay ahead of rivals, growing into Oregon’s largest corporate employer with more than 20,000 workers in Washington County.</p><p id="ZRXZQQNSQBHRVLJTTVSTBV6NUE">The cutting edge became increasingly tenuous, though, as the features on computer chips approached the atomic scale. Intel suffered a series of manufacturing setbacks, the result of bad bets on production technology and an utter failure to break into new sectors, like smartphones and artificial intelligence. </p><p id="ER5XEJUQLJCG7D4NVUUU64D7YE">Intel’s stumbles risk destabilizing Oregon’s semiconductor industry, one of the state’s economic pillars. The company <a href="https://www.oregonlive.com/silicon-forest/2025/01/intel-shed-3000-oregon-jobs-last-year-through-layoffs-buyouts-and-attrition.html" target="_blank" rel="">shed 3,000 jobs last year</a>, and <a href="https://www.oregonlive.com/silicon-forest/2025/04/intel-says-it-will-cut-costs-jobs-and-warns-sales-will-keep-falling.html" target="_blank" rel="">more layoffs are expected</a> in the coming weeks. </p><p id="ZIFJ3I5LDNDBNPVFQQD3T6K3GU">There’s no replacing the outsized impact of Intel’s factories. But new players could help keep Oregon in the game as the semiconductor industry evolves.</p><p id="LCDPDFPZPBHFHDSVFQYJ4XTOUI">Ampere Computing, founded by former Intel President Renée James, sold to Japanese investment bank SoftBank in March for $6.5 billion. It’s <a href="https://www.oregonlive.com/silicon-forest/2025/03/ampere-chip-designer-with-big-portland-office-sells-for-65-billion.html" target="_blank" rel="">retaining its offices in Portland’s Pearl District</a>, where it designs chips used in data centers.</p><p id="L2FPJZCR2FDCLJXDXB7MY6PEUU">Other Intel alumni have started a handful of other Oregon chip businesses in recent months. AheadComputing says it has already outgrown its offices at The Round and is looking for more space somewhere in Washington County.</p><p id="I4IXMNW4ZNG45KIGLH5HFC6SAE">Big companies are designed to meet many corporate needs, in the same way conventional CPUs are built to handle all computing workloads. AheadComputing wants to be more like its processors, moving faster because it’s dedicated to specific tasks. </p><p id="R76FH6I4CVDIVIWI7I4OWWVFQQ">“It’s very hard to disrupt an industry from inside,” said Mark Dechene, who spent 16 years as an Intel CPU architect before co-founding AheadComputing. </p><p id="ZSTGW2CHIFDSVOO4SZLMHR27HQ">At Intel, he said, researchers would consistently underestimate how long a project would take. He said AheadComputing’s engineers guess wrong about the timeframe, too, but now it’s because they’re moving much more quickly than they anticipated. </p><p id="BLIT55W24BGZBK6TIIZN5Q3KKY">“With a small, focused team of very capable people,” Dechene said, “you can get stuff done incredibly fast.” </p></div><p>If you purchase a product or register for an account through a link on our site, we may receive compensation.<span> By using this site, you consent to our <a href="https://www.advancelocal.com/advancelocalUserAgreement/user-agreement.html" target="_blank" rel="noopener noreferrer">User Agreement</a> and agree that your clicks, interactions, and personal information may be collected, recorded, and/or stored by us and social media and other third-party partners in accordance with our <a href="https://www.advancelocal.com/advancelocalUserAgreement/privacy-policy.html" target="_blank" rel="noopener noreferrer">Privacy Policy.</a></span></p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A masochist's guide to web development (147 pts)]]></title>
            <link>https://sebastiano.tronto.net/blog/2025-06-06-webdev/</link>
            <guid>44200895</guid>
            <pubDate>Fri, 06 Jun 2025 13:48:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sebastiano.tronto.net/blog/2025-06-06-webdev/">https://sebastiano.tronto.net/blog/2025-06-06-webdev/</a>, See on <a href="https://news.ycombinator.com/item?id=44200895">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<h2 id="table-of-contents">Table of contents</h2>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#setting-things-up">Setting things up</a></li>
<li><a href="#hello-world">Hello world</a></li>
<li><a href="#intermezzo-i-what-is-webassembly">Intermezzo I: What is WebAssembly?</a></li>
<li><a href="#building-a-library">Building a library</a></li>
<li><a href="#intermezzo-ii-javascript-and-the-dom">Intermezzo II: JavaScript and the DOM</a></li>
<li><a href="#loading-the-library-and-making-it-a-module">Loading the library and making it a module</a></li>
<li><a href="#multithreading">Multithreading</a></li>
<li><a href="#intermezzo-iii-web-workers-and-spectre">Intermezzo III: Web Workers and Spectre</a></li>
<li><a href="#dont-block-the-main-thread">Don’t block the main thread!</a></li>
<li><a href="#callback-functions">Callback functions</a></li>
<li><a href="#persistent-storage">Persistent storage</a></li>
<li><a href="#closing-thoughts">Closing thoughts</a></li>
</ul>
<h2 id="introduction">Introduction</h2>
<p>I have recently worked on making a web application out of
<a href="https://git.tronto.net/nissy-core/file/README.md.html">my latest Rubik’s cube optimal solver</a>.
This involved building a rather complex C code base (with
multithreading, SIMD, callback functions and whatnot) to
<a href="https://en.wikipedia.org/wiki/WebAssembly">WebAssembly</a> via
<a href="https://emscripten.org/">Emscripten</a>, and writing a minimal amount of
JavaScript and HTML for the frontend.</p>
<p>This whole process was complex, tiring and at times frustrating -
but eventually <a href="https://tronto.net:48/">it was a success</a>! Not only
I accomplished my goal, but I have learnt a lot along the way. After
finishing the work, I decided to write down all that I have learnt and
share it with the world with this post.</p>
<p>You may be wondering why one should do such a thing instead of either
rewriting their code base in a more web-friendly language, or distributing
their app using a native GUI framework. The main reason to use WebAssembly
is that it can provide near-native performance (or so they claim) while
running inside a web browser; this gives you all the portability of a
web app without too much of a performance drawback, something that would
not be possible with an interpreted language such as JavaScript.</p>
<p>So, what is this blog post? A tutorial for web development? I am not sure
about this, but if it is, it is definitely not a normal one. As the title
suggests, you should not start from this guide unless you just <em>love</em>
banging your head against the wall.  If you are looking for a <em>sane</em>
guide to web development, I strongly advise you head on to the
<a href="https://developer.mozilla.org/en-US/docs/MDN/Tutorials">Mozilla Developer Network tutorials page</a>
and start from there.</p>
<p>But if you are a C or C++ developer looking to port a program or library
to the web, then you are in the right place. With this post I am going
to walk you through the process of building an increasingly complex
library that can run in a web browser.  Make sure you are
sitting comfortably and be ready to sweat, because I am not going to
shy away from the hard stuff and the complicated details.</p>
<p>To follow this tutorial you won’t need much experience with web
development, but some familiarity with HTML and an idea of what JavaScript
will be useful. It will also help to know that you can access your
browser’s JavaScript console and other developer tools by pressing F12,
at least on Firefox or Chrome - but I guess I have literally just taught
you that, if you did not already know it. For all the rest, I’ll make
sure to add many hyperlinks throughout the text, so you can follow them
if something is new to you.</p>
<p>A little disclaimer: although I am a somewhat experienced C developer,
I had very little web development experience before embarking in
this adventure.  If you are a web developer, you may find errors in
this post that are going to make you laugh at my ignorance. If you do,
I’d appreciate it if you could report them to me by sending an email to
<code>sebastiano@tronto.net</code>!</p>
<p>With this out of the way, let’s get started!</p>
<h2 id="setting-things-up">Setting things up</h2>
<p>The examples used in this tutorial are all contained in a git repository,
which you can find either on
<a href="https://git.tronto.net/emscripten-tutorial/file/README.md.html">my git page</a> or
<a href="https://github.com/sebastianotronto/emscripten-tutorial">on github</a>.</p>
<p>In order to follow them you are going to need:</p>
<ul>
<li>A working installation of <a href="https://emscripten.org/">Emscripten</a>
(which also includes Node.js). Refer to the official website for
installation instructions.</li>
<li>A web server such <a href="https://github.com/emikulic/darkhttpd">darkhttpd</a>
or the Python <code>http.server</code> package; the examples will use darkhttpd.</li>
</ul>
<p>I have only tested all of this on Linux, but everything should work
exactly the same on any UNIX system. If you are a Windows user, you can
either run everything inside
<a href="https://learn.microsoft.com/en-us/windows/wsl/">WSL</a>, or you can try and
adjust the examples to your system - if you choose this second option,
I’ll happily accept patches or pull requests :)</p>
<h2 id="hello-world">Hello world</h2>
<p>Let’s start with the classic Hello World program:</p>
<pre><code>#include &lt;stdio.h&gt;

int main() {
    printf("Hello, web!\n");
}
</code></pre>
<p>You can compile the code above with</p>
<pre><code>emcc -o index.html hello.c
</code></pre>
<p>And if you now start a web server in the current folder, for example with
<code>darkhttpd .</code> (the dot at the end is important), and open a web browser to
<a href="http://localhost:8080/">localhost:8080</a> (or whatever port your web server
uses), you should see something like this:</p>
<p><img src="https://sebastiano.tronto.net/blog/2025-06-06-webdev/hello.png" alt="Hello world in a browser"></p>
<p>As you can see, the compiler generated a bunch of extra stuff around
you print statement. You may or may not want this, but for now we can
take it as a convenient way to check that our program works as expected.</p>
<p>There are other ways to run this compiled code. With the command above,
the compiler should have generated for you 3 files:</p>
<ul>
<li><code>index.html</code> - the web page in the screenshot above.</li>
<li><code>index.wasm</code> - the actual compiled code of your program; this file contains
WebAssembly bytecode.</li>
<li><code>index.js</code> - some JavaScript <em>glue code</em> to make it possible for <code>index.wasm</code>
to actually run in a browser.</li>
</ul>
<p>If you don’t specify <code>-o index.html</code>, or if your specify <code>-o</code> followed
by a filename ending in <code>.js</code>, the <code>.html</code> page is not going to be
generated. In this case (but also if you <em>do</em> generate the html page),
you can run the JavaScript code in your terminal with:</p>
<pre><code>node index.js
</code></pre>
<p>In later examples, the same code may not work seamlessly in both a web
browser and in Node.js - for example, when dealing with persistent data
storage. But until then, we can generate all three files with a single
command and run our code in either way.</p>
<p>It is also possible to ask Emscripten to generate only the <code>.wasm</code> file,
in case you want to write the JavaScript glue code by yourself. To do
this, you can pass the <code>-sSTANDALONE_WASM</code> option to <code>emcc</code>. However,
in some cases the <code>.js</code> file is going to be generated even when this
option is used, for example when building a source file without a <code>main()</code>
entry point. Since this is something we’ll do soon, we can forget about
this option and just take it as a fact that the <code>.wasm</code> files generated
by emscripten require some glue JavaScript code to actually run,
but in case you are interested you can check out
<a href="https://emscripten.org/docs/tools_reference/settings_reference.html#standalone-wasm">the official documentation</a>.</p>
<p>You can find the code for this example, as well as scripts to
build it and run the web server, in the directory <code>00_hello_world</code>
of the git repository
(<a href="https://git.tronto.net/emscripten-tutorial/file/README.md.html">git.tronto.net</a>,
<a href="https://github.com/sebastianotronto/emscripten-tutorial">github</a>).</p>
<p>Anyway, now we can build our C code to run in a web page.  But this is
probably not the way we want to run it. First of all, we don’t want to
use the HTML template provided by Emscripten; but more importantly, we
probably don’t want to write a program that just prints stuff to standard
output. More likely, we want to write some kind of library of functions
that can be called from the front-end, so that the user can interact with
our program via an HTML + JavaScript web page.  Before going into that,
let’s take a break to discuss what we are actually compiling our code to.</p>
<h2 id="intermezzo-i-what-is-webassembly">Intermezzo I: What is WebAssembly?</h2>
<p><img src="https://sebastiano.tronto.net/blog/2025-06-06-webdev/wasm.png" alt="The logo of WebAssembly"></p>
<p><a href="https://en.wikipedia.org/wiki/WebAssembly">WebAssembly</a> is a low-level
language meant to run in a virtual machine inside a web browser. The main
motivation behind it is running higher-performance web applications compared
to JavaScript; this is made possible, by its
compact bytecode and its stack-based virtual machine.</p>
<p>WebAssembly (or WASM for short) is supported by all major browsers
since around 2017.  Interestingly, Emscripten, the compiler we are
using to translate our C code to WASM, first appeared in 2011,
predating WASM by a few years. Early on, Emscripten would compile
C and C++ code into JavaScript, or rather a subset thereof called
<a href="https://en.wikipedia.org/wiki/Asm.js">asm.js</a>.</p>
<p>Just like regular
<a href="https://en.wikipedia.org/wiki/Assembly_language">assembly</a>, WASM
also has a text-based representation. This means that one could write
WASM code directly, assemble it to bytecode, and then run it.  We are
not going to do it, but if you are curious here is a simple example
(computing the factorial of a number, taken from Wikipedia):</p>
<pre><code>(func (param i64) (result i64)
    local.get 0
    i64.eqz
    if (result i64)
        i64.const 1
    else
        local.get 0
        local.get 0
        i64.const 1
        i64.sub
        call 0
        i64.mul
    end)
</code></pre>
<p>As you can see, it looks like a strange mix of assembly and
<a href="https://en.wikipedia.org/wiki/Lisp_(programming_language)">Lisp</a>.
If you want to try and run WASM locally, outside of a web browser,
you could use something like <a href="https://wasmtime.dev/">Wasmtime</a>.</p>
<p>Until early 2025, the WASM “architecture” was 32-bit only. One big
limitation that this brings is that you cannot use more that 4GB
(2<sup>32</sup> bytes) of memory, because pointers are only 32 bits
long; moreover, your C / C++ code may need some adjustments if it
relied on the assumption that e.g. <code>sizeof(size_t) == 8</code>. At the
time writing a new standard that enables 64 bit pointers, called
WASM64, is supported on Firefox and Chrome, but not on Webkit-based
browsers such as Safari yet. Depending on when you are reading this,
this may have changed - you can check the status of WASM64 support
<a href="https://webassembly.org/features/">here</a>.</p>
<h2 id="building-a-library">Building a library</h2>
<p>Back to the main topic. Where were we? Oh yes, we wanted to build
a C <em>library</em> to WASM and call it from JavaScript. Our complex,
high-performance, math-heavy library probably looks something like this:</p>
<p>library.h (actually, we are not going to need this):</p>
<pre><code>int multiply(int, int);
</code></pre>
<p>library.c:</p>
<pre><code>int multiply(int a, int b) {
    return a * b;
}
</code></pre>
<p>Or maybe it is a bit more complicated than that. But we said we are
going to build up in complexity, and this is just the beginning, so
let’s stick to <code>multiply()</code>.</p>
<p>To build this library you can use:</p>
<pre><code>emcc -o library.js library.c
</code></pre>
<p>As we saw before, this is going to generate both a <code>library.js</code> and a
<code>library.wasm</code> file. Now we would like to call our library function
with something like this</p>
<p>program.js:</p>
<pre><code>var library = require("./library.js");
const result = library.multiply(6, 7);
console.log("The answer is " + result);
</code></pre>
<p><em>(The <code>require()</code> syntax above is valid when running this code in Node.js,
but not, for example when running in a browser. We’ll see in the next
session what to do in that case, but for now let’s stick to this.)</em></p>
<p>Unfortunately, this will not work for a couple of reasons. The reason
first is that Emscripten is going to add an underscore <code>_</code> to all our
function names; so we’ll have to call <code>library._multiply()</code>. But this
still won’t work, because by default the compiler does not <em>export</em> all
the functions in your code - that is, it does not make them visible to
the outside. To specify which functions you want to
export, you can use the <code>-sEXPORTED_FUNCTIONS</code> flag, like so:</p>
<pre><code>emcc -sEXPORTED_FUNCTION=_multiply -o library.js library.c
</code></pre>
<p>And now we finally have access to our <code>multiply()</code> function…</p>
<pre><code>$ node program.js
Aborted(Assertion failed: native function `multiply` called before runtime initialization)
</code></pre>
<p>…or maybe not. If you are new to JavaScript like I was a few weeks
ago, you may find this error message surprising. Some runtime must be
initialized, but can’t it just, like… initialize <em>before</em> trying to
run the next instruction?</p>
<p>Things are not that simple. A lot of things in JavaScript happen
<em>asynchronously</em>, and in these situations you’ll have to either use
<a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/await"><code>await</code></a>
or a
<a href="https://developer.mozilla.org/en-US/docs/Glossary/Callback_function"><em>callback function</em></a>.
So we’ll have to do something like this:</p>
<pre><code>var library = require("./build/library.js");

library.onRuntimeInitialized = () =&gt; {
    const result = library._multiply(6, 7);
    console.log("The answer is " + result);
};
</code></pre>
<p>And now we can finally run our program:</p>
<pre><code>$ node program.js 
The answer is 42
</code></pre>
<p>The code for this example can be found in the <code>01_library</code> folder in
the git repository
(<a href="https://git.tronto.net/emscripten-tutorial/file/README.md.html">git.tronto.net</a>,
<a href="https://github.com/sebastianotronto/emscripten-tutorial">github</a>).</p>
<h2 id="intermezzo-ii-javascript-and-the-dom">Intermezzo II: JavaScript and the DOM</h2>
<p><img src="https://sebastiano.tronto.net/blog/2025-06-06-webdev/logos.png" alt="The logos of HTML, CSS and JavaScript"></p>
<p>If we want to build an interactive web page using JavaScript, we’ll
need a way for our script to communicate with the page, i.e. a way
to access the HTML structure from JavaScript code. What we are looking
for is called
<em><a href="https://developer.mozilla.org/en-US/docs/Web/API/Document_Object_Model">Document Object Model</a></em>,
or DOM for short.</p>
<p>For example, if you have a paragraph with some text in your HTML:</p>
<pre><code>&lt;p id="myParagraph"&gt;Hello!&lt;/p&gt;
</code></pre>
<p>you can access this text from JavaScript like this:</p>
<pre><code>var paragraph = document.getElementById("myParagraph");
paragraph.innerText = "New text!";
</code></pre>
<p>Here we are selecting the paragraph HTML element using its ID, and we
are changing its text via its <code>innerText</code> property, all from JavaScript.</p>
<p>Let’s see a more complex example:</p>
<p>HTML:</p>
<pre><code>&lt;button id="theButton"&gt;Press me!&lt;/button&gt;
</code></pre>
<p>JS:</p>
<pre><code>var button = document.getElementById("theButton");
var counter = 0;

button.addEventListener("click", () =&gt; {
    counter++;
    button.innerText = "I have been pressed " + counter + " times!";
});
</code></pre>
<p>In the example above we add an
<em><a href="https://developer.mozilla.org/en-US/docs/Web/API/EventTarget/addEventListener">event listener</a></em>
to a button: the (anonymous) function we defined is going to be called
every time the button is clicked. And since this is a web page, I guess
I can show you what this actually looks like.</p>
<p>Behold, the dynamic button:</p>


<p>If you are completely new to web development, you may be wondering
where you should write this JavaScript code. One option is to write it
in the same HTML file as the rest of the page, inside a <code>&lt;script&gt;</code> tag;
this is how I did it in the example above, as you can check by viewing
the source of this page (press Ctrl+U, or right-click and select
“view source”, or prepend <code>view-source:</code> to this page’s URL; hopefully
at least one of these methods should work in your browser).</p>
<p>However, if the script gets too large you may want to split it off in
a separate file, which we’ll demonstrate in this next example.</p>
<p>Let’s now make a template web page for using our powerful library. Let’s
start with the HTML, which is in large part boilerplate:</p>
<p>index.html:</p>
<pre><code>&lt;!doctype html&gt;
&lt;html lang="en-US"&gt;
&lt;head&gt;
    &lt;meta charset="utf-8" /&gt;
    &lt;meta name="viewport" content="width=device-width" /&gt;
    &lt;title&gt;Multiply two numbers&lt;/title&gt;
    &lt;script src="./script.js" defer&gt;&lt;/script&gt;
&lt;/head&gt;

&lt;body&gt;
    &lt;p&gt;
    &lt;input id="aInput" /&gt; x &lt;input id="bInput" /&gt;
    &lt;button id="goButton"&gt;=&lt;/button&gt;
    &lt;span id="resultText"&gt;&lt;/span&gt;
    &lt;/p&gt;
&lt;/body&gt;

&lt;/html&gt;
</code></pre>
<p>Besides the <code>&lt;body&gt;</code> element, the only important line for us is line
7, which loads the script from a file. Notice that we use the <code>defer</code>
keyword here: this is telling the browser to wait until the whole page
has been loaded before executing the script. If we did not do this, we
could run in the situation where we <code>document.getElementById()</code> returns
<code>null</code>, because the element we are trying to get is not loaded yet (yes,
this happened to me while I was writing this post). If you want to know
more, check out this
<a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Reference/Elements/script#defer">MDN page</a>.</p>
<p>Now to the JavaScript code. For now we are going to use the built-in
<code>*</code> operator to multiply the two numbers, but in the next section we
are going to replace it with our own library.</p>
<p>script.js (in the same folder as index.html):</p>
<pre><code>var aInput = document.getElementById("aInput");
var bInput = document.getElementById("bInput");
var button = document.getElementById("goButton");
var resultText = document.getElementById("resultText");

button.addEventListener("click", () =&gt; {
    var a = Number(aInput.value);
    var b = Number(bInput.value);
    resultText.innerText = a * b;
});
</code></pre>
<p>The final result will look something like this:</p>
<p>
 x 

<span id="resultText"></span>
</p>

<p>In a real-world scenario you would probably want to check that the text
provided in the input fields is actually a number, or perhaps use the
<a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Reference/Elements/input/number"><code>type="number"</code></a>
attribute for the input fields. But we’ll ignore these issues here -
we are going to have more serious problems to deal with.</p>
<h2 id="loading-the-library-and-making-it-a-module">Loading the library and making it a module</h2>
<p>With what we have learned in the previous intermezzo (you are not skipping
those, right?) we can finally run our library code in a real web page. The
code is pretty much the same as above; we just need to include both the
library and the script file in the HTML:</p>
<pre><code>    &lt;script src="./library.js" defer&gt;&lt;/script&gt;
    &lt;script src="./script.js" defer&gt;&lt;/script&gt;
</code></pre>
<p>and of course we have to change the line where we perform the multiplication:</p>
<pre><code>    resultText.innerText = Module._multiply(a, b);
</code></pre>
<p>Here <code>Module</code> is the default name given to our library by
Emscripten. Apart from being too generic a name, this leads to another
problem: we can’t include more than one Emscripten-built library in our
page in this way - otherwise, both are going to be called <code>Module</code>.</p>
<p>Luckily, there is another way: we can build a
<a href="https://emscripten.org/docs/compiling/Modularized-Output.html">modularized</a>
library, i.e. obtain a
<a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Modules">JavaScript Module</a>.
This may sound a bit strange, because the name <code>Module</code> kind of implies
there is already a module. The way I understand it is that by default
Emscripten produces a <em>script</em> that <em>contains</em> a module named <code>Module</code>;
when building a modularized library, the whole resulting file is a module.</p>
<p>Modularizing our build is not necessary right now, but
there are a couple of other advantages to it:</p>
<ul>
<li>As mentioned above, we can change the name of our module and include
more than one Emscripten-built library, if we want.</li>
<li>We will be able to use the module in the same way in Node.js and in
our web page script. This way we can minimize the differences between
the two versions of our code, which can be useful for testing.</li>
<li>In case we want to build a more complex layer of JavaScript between
our library and our web page, with a modularized build we can easily
include the module in another file, which can then be included in the
main script.</li>
</ul>
<p>So let’s go ahead and build our library like so:</p>
<pre><code>emcc -sEXPORTED_FUNCTION=_multiply -sMODULARIZE -sEXPORT_NAME=MyLibrary \
     -o library.mjs library.c
</code></pre>
<p>Notice I have changed the extension from <code>.js</code> to <code>.mjs</code>. Don’t worry,
either extension can be used. And you are going to run into issues with
either choice:</p>
<ul>
<li>If you run your code in Node.js, it will understand that the library
file is a module only if you use the <code>.mjs</code> extension. Alternatively,
you can change some settings in a local configuration file to
enforce this.</li>
<li>If you run your code in a web page, your web server may not be
configured to serve <code>.mjs</code> files as JavaScript files. This can
easily be changed by adding a configuration line somewhere.</li>
</ul>
<p>In my examples I chose to use the <code>.mjs</code> extensions to make Node.js
happy, and I changed the configuration of my web servers as needed. For
example, for darkhttpd I added a file called <code>mime.txt</code> with a single
line <code>text/javascript mjs</code>, and launched the server with the
<code>--mimetypes mime.txt</code> option.</p>
<p>Now we have to make a couple of changes. Our <code>program.js</code>, for running
in node, becomes:</p>
<pre><code>import MyLibrary from "./library.mjs"

var myLibraryInstance = await MyLibrary();

const result = myLibraryInstance(6, 7);
console.log("The answer is " + result);
</code></pre>
<p>By the way, I have renamed this file to <code>program.mjs</code>. This is because
only modules can use the
<a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/import">static <code>import</code></a>
statement; alternatively, I could have used the
<a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/import">dynamic <code>import()</code></a>
and kept the <code>.js</code> extension.</p>
<p>Similary, we have to update our <code>script.js</code> (or <code>script.mjs</code>) to import
the module and create an instance. Moreover, we have to specify in the
HTML that the script is now a module:</p>
<pre><code>    &lt;script src="./script.mjs" type="module" defer&gt;&lt;/script&gt;
</code></pre>
<p>And we can get rid of the other <code>&lt;script&gt;</code> tag, since now the library
is included directly in <code>script.mjs</code>.</p>
<p>You can find the full the code for this example the folder
<code>02_library_modularized</code> in the git repository
(<a href="https://git.tronto.net/emscripten-tutorial/file/README.md.html">git.tronto.net</a>,
<a href="https://github.com/sebastianotronto/emscripten-tutorial">github</a>).</p>
<h2 id="multithreading">Multithreading</h2>
<p><img src="https://sebastiano.tronto.net/blog/2025-06-06-webdev/threads.jpg" alt="&quot;Gotta go fast&quot; meme"></p>
<p>Let’s move on to a more interesting example. If one of the goals of
WebAssembly is performance, there is no point in using only 1/16th of
your CPU - let’s port a multithreaded application to the web!</p>
<p>As a more complicated example, let’s write a function that counts how
many prime numbers there are in a given range. This function takes two
integers as input and returns a single integer as output, but it does
a non-trivial amount of work under the hood. A simple implementation of
this routine would be something like this:</p>
<pre><code>bool isprime(int n) {
    if (n &lt; 2)
        return false;

    for (int i = 2; i*i &lt;= n; i++)
        if (n % i == 0)
            return false;
    return true;
}

int primes_in_range(int low, int high) {
    if (low &lt; 0 || high &lt; low)
        return 0;

    int count = 0;
    for (int i = low; i &lt; high; i++)
        if (isprime(i))
            count++;

    return count;
}
</code></pre>
<p>This algorithm is
<a href="https://en.wikipedia.org/wiki/Embarrassingly_parallel">embarassingly parallelizable</a>:
we can split the interval <code>[low, high)</code> into smaller sub-intervals and
process each one of them in a separate thread; then we just need to add
up the results of the sub-intervals.</p>
<p>For the actual implementation, we are going to use
<a href="https://en.wikipedia.org/wiki/Pthreads">pthreads</a>, for the simple reason
that it is
<a href="https://emscripten.org/docs/porting/pthreads.html">supported by Emscripten</a>.
In practice, assuming we are working on a UNIX platform, we could also
use C11’s <a href="https://en.cppreference.com/w/c/header/threads">threads.h</a> or
C++’s <a href="https://en.cppreference.com/w/cpp/thread/thread.html">std::thread</a>,
but only because they happen to be wrappers around pthreads. On other
platforms, or in other implementations of the C and C++ standard library,
this may not be the case; so we’ll stick to old-school pthreads.</p>
<p>This is my parallel version of <code>primes_in_range()</code>:</p>
<p>primes.c:</p>
<pre><code>#include &lt;stdbool.h&gt;
#include &lt;pthread.h&gt;

#define NTHREADS 16

bool isprime(int);
void *pthread_routine(void *);

struct interval { int low; int high; int count; };

int primes_in_range(int low, int high) {
    pthread_t threads[NTHREADS];
    struct interval args[NTHREADS];

    if (low &lt; 0 || high &lt; low)
        return 0;

    int interval_size = (high-low)/NTHREADS + 1;
    for (int i = 0; i &lt; NTHREADS; i++) {
        args[i].low = low + i*interval_size;
        args[i].high = args[i].low + interval_size;
        pthread_create(&amp;threads[i], NULL, pthread_routine, &amp;args[i]);
    }

    int result = 0;
    for (int i = 0; i &lt; NTHREADS; i++) {
        pthread_join(threads[i], NULL);
        result += args[i].count;
    }

    return result;
}

bool isprime(int n) {
    if (n &lt; 2)
        return false;

    for (int i = 2; i*i &lt;= n; i++)
        if (n % i == 0)
            return false;
    return true;
}

void *pthread_routine(void *arg) {
    struct interval *interval = arg;

    interval-&gt;count = 0;
    for (int i = interval-&gt;low; i &lt; interval-&gt;high; i++)
        if (isprime(i))
            interval-&gt;count++;

    return NULL;
}
</code></pre>
<p><em>(Pro tip: if you take the number of threads as an extra parameter for
your function, you can pass to it the value
<a href="https://developer.mozilla.org/en-US/docs/Web/API/Navigator/hardwareConcurrency"><code>navigator.hardwareConcurrency</code></a>
from the JavaScript front-end and use exactly the maximum number of
threads that can run in parallel on the host platform.)</em></p>
<p>To build this with Emscripten we’ll have to pass the <code>-pthread</code> option and,
optionally, a suitable value for
<a href="https://emscripten.org/docs/tools_reference/settings_reference.html#pthread-pool-size"><code>-sPTHREAD_POOL_SIZE</code></a>.</p>
<p>If we want to run our multithreaded code in an actual browser, we’ll
have to scratch our head a bit harder. The code we are supposed to
write is exactly what we expect, but once again we have to tinker with
our web server configuration. For technical reasons that we’ll cover in
the next intermezzo, in order to run multithreaded code in a browser
we must add a couple of HTTP headers:</p>
<pre><code>Cross-Origin-Opener-Policy: same-origin
Cross-Origin-Embedder-Policy: require-corp
</code></pre>
<p>These headers are part of the response your browser will receive when
it requests any web page from the server. The way you set these depends on
the server you are using; with darkhttpd you can use the <code>--header</code> option.</p>
<p>With your server correctly set up, you can enjoy a multithreaded program
running in your browser!  As always, you can check out this example from
the <code>03_threads</code> folder of the git repository
(<a href="https://git.tronto.net/emscripten-tutorial/file/README.md.html">git.tronto.net</a>,
<a href="https://github.com/sebastianotronto/emscripten-tutorial">github</a>).</p>
<h2 id="intermezzo-iii-web-workers-and-spectre">Intermezzo III: Web Workers and Spectre</h2>
<p><img src="https://sebastiano.tronto.net/blog/2025-06-06-webdev/spectre.png" alt="The logo of the Spectre vulnerability"></p>
<p>On a low level, threads are implemented by Emscripten using
<a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API">web workers</a>,
which are processes separated from the main web page process and
communicate with it and with each other by
<a href="https://developer.mozilla.org/en-US/docs/Web/API/Worker/postMessage">passing messages</a>.
Web workers are commonly used to run slow operations in the background
without blocking the UI threads, so the web page remains responsive
while these operations run - we’ll do this in the next section.</p>
<p>Web workers do not have regular access to the same memory as the main
process, and this is something that will give us some issues in later
sections. However, there are ways around this limitation. One of these
ways is provided by
<a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/SharedArrayBuffer">SharedArrayBuffer</a>,
which we won’t use directly in this tutorial, but is used by
Emscripten under the hood.</p>
<p>And this is why we had to set the <code>Cross-Origin-*</code> headers. In 2018, a
CPU vulnerability called <a href="https://spectreattack.com/">Spectre</a> was found,
and it was shown that an attacker could take advantage of shared memory
between the main browser thread and web workers to
<a href="https://en.wikipedia.org/wiki/Spectre_(security_vulnerability)#Remote_exploitation">execute code remotely</a>.
As a counter-measure, most browsers now require your app to be in a
<a href="https://developer.mozilla.org/en-US/docs/Web/Security/Secure_Contexts">secure context</a>
and
<a href="https://developer.mozilla.org/en-US/docs/Web/API/Window/crossOriginIsolated">cross-origin isolated</a>
to allow using <code>SharedArrayBuffer</code>s.</p>
<p>Even if you do not plan to use web workers directly, it is still good to
have a rough idea of how they work, because of the
<a href="https://en.wikipedia.org/wiki/Leaky_abstraction">law of leaky abstractions</a>:
<em>all abstractions are leaky</em>.
The fact that we had to mess around with our <code>Cross-Origin-*</code> headers
despite not caring at all about <code>SharedArrayBuffer</code>s is a blatant example
of this.</p>
<h2 id="dont-block-the-main-thread">Don’t block the main thread!</h2>
<p>If you have run the previous example, may have noticed a scary warning
like this in your browser’s console:</p>
<p><img src="https://sebastiano.tronto.net/blog/2025-06-06-webdev/blocking.png" alt="A warning saying &quot;Blocking on the main thread is very dangerous, see [link]&quot;"></p>
<p><em>The link points to
<a href="https://emscripten.org/docs/porting/pthreads.html#blocking-on-the-main-browser-thread">this page</a>
in Emscripten’s documentation.</em></p>
<p>The issue here is that our heavy computation is not running “in the
background”, but its main thread (the one spawning the other threads)
coincides with the browser’s main thread, the one that is responsible
for drawing the UI and handling user interaction. So if our computation
really takes long, the browser is going to freeze - and after a few
seconds it will ask us if we want to kill this long-running script.</p>
<p>As we anticipated in the previous intermezzo, we are going to solve this
with a web worker. We will structure this solution as follows:</p>
<ul>
<li>The main script will be responsible for reading the user input, sending
a message to the worker to ask it to compute the result, and handling
the result that the worker is going to send back once it is done. No
slow operation is performed by this script, so that it won’t block
the main thread.</li>
<li>The worker will be responsible for receiving mesages from the main
script, handling them by calling the library, and sending a message
with the response back once it is done computing.</li>
</ul>
<p>In practice, this will look like this:</p>
<p>script.mjs:</p>
<pre><code>var aInput = document.getElementById("aInput");
var bInput = document.getElementById("bInput");
var button = document.getElementById("goButton");
var resultText = document.getElementById("resultText");

var worker = new Worker("./worker.mjs", { type: "module" });

button.addEventListener("click", () =&gt; worker.postMessage({
    a: Number(aInput.value),
    b: Number(bInput.value)
}));

worker.onmessage = (e) =&gt; resultText.innerText = "There are " +
    e.data.result + " primes between " + e.data.a + " and " + e.data.b;
</code></pre>
<p>worker.mjs:</p>
<pre><code>import Primes from "./build/primes.mjs";

var primes = await Primes();

onmessage = (e) =&gt; {
    const count = primes._primes_in_range(e.data.a, e.data.b);
    postMessage({ result: count, a: e.data.a, b: e.data.b });
};
</code></pre>
<p>More complicated than before, but nothing crazy. Notice how we are using
<a href="https://developer.mozilla.org/en-US/docs/Web/API/Worker/postMessage"><code>postMessage()</code></a>
and
<a href="https://developer.mozilla.org/en-US/docs/Web/API/Worker/message_event"><code>onmessage()</code></a>
to pass events back and forth. The argument of <code>postMessage()</code> is the
actual data we want to send in
<a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/JSON">JSON</a>
format, while the argument of <code>onmessage()</code> is an
<a href="https://developer.mozilla.org/en-US/docs/Web/API/Event">event</a>
whose <code>data</code> property contains the object that was sent with <code>postMessage()</code>.</p>
<p>You can check out this example in the directory <code>04_no_block</code> in the
repository
(<a href="https://git.tronto.net/emscripten-tutorial/file/README.md.html">git.tronto.net</a>,
<a href="https://github.com/sebastianotronto/emscripten-tutorial">github</a>).
Try also large numbers, in the range of millions or tens of millions, and
compare it with the previous example - but not don’t go too large, we
only support 32-bit integers for now.  Notice how, with this new setup,
the browser remains responsive while it is loading the response.</p>
<p>Oh and by the way, a nice exercise for you now could be making the
script show some kind of <code>"Loading result..."</code> message while the worker
is working. This is not hard to do, but a huge improvement in user
experience!</p>
<h2 id="callback-functions">Callback functions</h2>
<p><img src="https://sebastiano.tronto.net/blog/2025-06-06-webdev/callback.jpg" alt="The hand of a person using a phone"></p>
<p>For one reason or another, your library function may take as parameter
another function. For example, you may use this other function to print
log messages regardless of where your library code is run: a command-line
tool may pass <code>printf()</code> to log to console, while a GUI application
may want to show these messages to some text area in a window, and it
will pass the appropriate function pointer parameter. This is the use case
that we are going to take as an example here, but it is not the only one.</p>
<p>Implementing this was probably the step that took me the longest in my
endeavor to port my Rubik’s cube solver to the web. Luckily for you,
when writing this post I found a simpler method, so you won’t have to
endure the same pain.</p>
<p>First, we’ll have to adapt our library function like this:</p>
<pre><code>int primes_in_range(int low, int high, void (*log)(const char *)) {
    /* The old code, with calls to log() whenever we want */
};
</code></pre>
<p><em>Tip: when using callback functions like this, it is good practice
to have them accept an extra <code>void *</code> parameter, and the library
function should also accept an extra <code>void *</code> parameter that it then
passes on to the callback. So our function would look something like
this: <code>int primes_int_range(int low, int high, void (*log)(const char *, void *), void *log_data)</code>.
This makes the setup extremely flexible, and allows passing callback
functions in situation where this may be tricky. For example, this
way you could pass a C++ member function by passing an object as
<code>log_data</code> and a function that call <code>log_data</code>’s member function
as <code>log</code>. Since we are not going to use this in this example, I’ll stick
to the simpler setup.</em></p>
<p>Now, to call our function from the JavaScript side we would like
to do something like this:</p>
<pre><code>int result = primes_in_range(a, b, console.log); // Logging to console
</code></pre>
<p>Unfortunately, this will not work, because <code>console.log</code>, a JavaScript
<a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Function">function object</a>,
does not get automatically converted to a function <em>pointer</em>, which is
what C expects. So we’ll have to do something slightly more complicated:</p>
<pre><code>import Primes from "./build/primes.mjs"

var primes = await Primes();
const logPtr = primes.addFunction((cstr) =&gt; {
    console.log(primes.UTF8ToString(cstr));
}, "vp");

const count = primes._primes_in_range(1, 100, logPtr);
</code></pre>
<p>Here <code>addFunction()</code> is a function generated by Emscripten.  Notice also
that we are wrapping our <code>console.log()</code> in a call to <code>UTF8ToString()</code>,
an Emscripten utility to convert C strings to JavaScript strings, and
that we are passing the function’s signature <code>"vp"</code> (returns <code>void</code>,
takes a <code>pointer</code>) as an argument; see
<a href="https://emscripten.org/docs/porting/connecting_cpp_and_javascript/Interacting-with-code.html#function-signatures">here</a>
for more information.</p>
<p>Other than that, you just need to add a couple of compiler flags:</p>
<ul>
<li><code>-sEXPORTED_RUNTIME_METHODS=addFunction,UTF8ToString</code> to tell the
compiler to make these two methods available.</li>
<li><code>-sALLOW_TABLE_GROWTH</code> to make it possible to add functions to
out module at runtime with <code>addFunction()</code>.</li>
</ul>
<p>And as you can check by running the example <code>05_callback</code> from the repo
(<a href="https://git.tronto.net/emscripten-tutorial/file/README.md.html">git.tronto.net</a>,
<a href="https://github.com/sebastianotronto/emscripten-tutorial">github</a>),
everything works as expected, both in Node.js and in a web page.  To make
the examples more interesting, the web page one is not only not logging the
messages to console, but it also shows them as text in the web page.</p>
<p><em>Note: you must be careful where you call this callback function from.
If you try to call it from outside the main thread - for example, in one
of the threads that are spawned to count the primes in the sub-intervals
- you’ll get a horrible crash. This is because web workers do not have
access to the functions that reside in another worker’s memory.</em></p>
<h2 id="persistent-storage">Persistent storage</h2>
<p><img src="https://sebastiano.tronto.net/blog/2025-06-06-webdev/storage.png" alt="3D rendering of a spinning hard drive"></p>
<p>Our multithreaded implementation of <code>primes_in_range()</code> is not slow, but
it could be faster. One possible way to speed it up is to use a look-up
table to make <code>is_prime()</code> run in constant time; for this we’ll need to
memorize which numbers below 2<sup>31</sup> (the maximum value of 32-bit
signed integer) are prime. This will require 2<sup>31</sup> bits of data,
or 256MB.  It would be nice if we could store this data persistently in
the user’s browser, so that if they use our app again in the future we
won’t need to repeat expensive calculations or re-download large files.</p>
<p>Putting aside the question of whether any of the above is a good idea,
and assuming you know how to generate such a table, in C you would
read and store the data like this:</p>
<pre><code>#include &lt;stdio.h&gt;

#define FILENAME "./build/primes_table"

void read_table(unsigned char *table) {
    FILE *f = fopen(FILENAME, "rb");
    fread(table, TABLESIZE, 1, f);
    fclose(f);
}

void store_table(const unsigned char *table) {
    FILE *f = fopen(FILENAME, "wb");
    fwrite(table, TABLESIZE, 1, f);
    fclose(f);
}
</code></pre>
<p><em>Note: the code snippet above is extremely simplified, you probably want
to add some error-handling code if you implement something like this.</em></p>
<p>The good news is that we can use the same code when building with
Emscripten! The bad news is that… well, it’s a bit more complicated
than that.</p>
<p>First of all, it is important to know that
<a href="https://emscripten.org/docs/api_reference/Filesystem-API.html">Emscripten’s File System API</a>
supports different “backends”, by which I mean ways of translating the
C / C++ file operations to WASM / JavaScript. I am not going to discuss
all of them here, but I want to highlight a few key points:</p>
<ul>
<li>The default backend is called <code>MEMFS</code>. It is a virtual file system
that resides in RAM, and all data written to it is lost when the
application is closed.</li>
<li>Only one of these backends (<code>NODERAWFS</code>) gives access to the actual
local file system, and it is only usable when running your app with
Node.js. Browsers are <em>sandboxed</em>, and the filesystem is not normally
accessible to them. There are ways, such as the
<a href="https://developer.mozilla.org/en-US/docs/Web/API/File_System_API">File System API</a>,
to access files, but as far as I understand each file you want to
access requires explicit actions from the user. We would like to manage
our data automatically, so we are not going to use this API.</li>
<li>The backend we are going to use is called <code>IDBFS</code>. It provides access
to the <a href="https://developer.mozilla.org/en-US/docs/Web/API/IndexedDB_API">IndexedDB API</a>,
which allows to persistently store large quantities of data in the
browser’s cache. The data is only removed if the user asks for it,
for example by cleaning it from the browser’s settings page.</li>
</ul>
<p>To activate the <code>IDBFS</code> backend, we are going to add <code>--lidbfs.js</code>
to our compiler options.  The Indexed DB is not the only way to store
data persistently in the browser. For an overview of all the options,
you can take a look at
<a href="https://developer.mozilla.org/en-US/docs/Learn_web_development/Extensions/Client-side_APIs/Client-side_storage">this page on MDN</a>.</p>
<p>The compiler flag is not enough, however. We also need to:</p>
<ol>
<li>Create a directory (for the virtual file system) where our data file
is going to be stored. We are going to call this directory <code>assets</code>,
but you can pick any other name; it does not have to coincide with the
name of a directory that exists on your local file system.</li>
<li>Mount the directory we have just created in the indexed DB.</li>
<li>Synchronize the virtual file system, so that our script is able to
read pre-existing files.</li>
</ol>
<p>All of the above has to be done from JavaScript, which makes things a
little bit complicated, because we are reading our files from C code.
We have a couple of ways to work around this issue:</p>
<ul>
<li>Using
<a href="https://emscripten.org/docs/porting/connecting_cpp_and_javascript/Interacting-with-code.html#interacting-with-code-call-javascript-from-native">inline JavaScript</a>
in our C code with the <code>EM_JS()</code> or <code>EM_ASYNC_JS()</code> Emscripten macros.</li>
<li>Setting up the indexed DB file system when the module loads using
the <code>--pre-js</code> compiler option.</li>
</ul>
<p>Here we are going to use the second solution, but the first option is
good to keep in mind, because it allows us to call JavaScript code at
any point rather than just at startup.</p>
<p><em>Note: if you do end up using <code>EM_ASYNC_JS()</code> to make asynchronous
JavasScript functions callable from C, keep in mind that any C
function that, directly or indirectly, calls an async JavaScript
function, will now return a
<a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise">promise</a>
when called from JavaScript. But wether an async function is called is
determined at runtime, so you C function may return a value one time
and a promise another time, depending on how exactly it runs!</em></p>
<p>So we are going to add <code>--pre-js init_idbfs.js</code> to our compiler options,
with <code>init_idbfs.js</code> containing the following:</p>
<pre><code>Module['preRun'] = [
    async () =&gt; {
        const dir = "/assets";

        FS.mkdir(dir);
        FS.mount(IDBFS, { autoPersist: true }, dir);

        Module.fileSystemLoaded = new Promise((resolve, reject) =&gt; {
            FS.syncfs(true, (err) =&gt; {
                if (err) reject(err);
                else resolve(true);
            });
        });

    }
];
</code></pre>
<p>As you can see, the syncing operation is more complicated, the main
reason being that it is an
<a href="https://developer.mozilla.org/en-US/docs/Learn_web_development/Extensions/Async_JS">asynchronous operation</a>.
For this reason, we are wrapping it in a
<a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise">Promise</a>,
so we can detect when this operation is done and react accordingly.
We are going to do so from our worker script, which will send a message to
the main script to communicate that the file system is ready to go:</p>
<pre><code>primes.fileSystemLoaded.then(() =&gt; {
    postMessage({ type: "readySignal" });
});
</code></pre>
<p>The main script can then handle this signal as it prefers, for example by
enabling the <code>Compute</code> button, if it was previously marked as <code>disabled</code>.</p>
<p>One last thing: since we are now using a large amount of memory and
loading the virtual file system at the start, the compiler will complain
that we are not reserving enough memory for our application. Adding a
<code>-sINITIAL_MEMORY=272629760</code> compiler flag will do the trick (watch out:
the number you provide must be a multiple of 2<sup>16</sup>). I am not
entirely sure why this is the case, since we are not loading the file in
memory statically, but only at runtime, and only when the
<code>primes_in_range()</code> function is called. I would expect that using
<a href="https://emscripten.org/docs/tools_reference/settings_reference.html#allow-memory-growth"><code>-sALLOW_MEMORY_GROWTH</code></a>
would be enough - and indeed this is the case if we use the <code>EM_ASYNC_JS()</code>
macro to load the file system on-demand.</p>
<p>And with all this, we are ready to run our optimized version of the
<code>primes_in_range()</code> algorithm, all from within our browser! As always,
you can check out the complete code in the folder <code>06_storage</code> of
the repository
(<a href="https://git.tronto.net/emscripten-tutorial/file/README.md.html">git.tronto.net</a>,
<a href="https://github.com/sebastianotronto/emscripten-tutorial">github</a>).</p>
<p>If generating this data on the user’s side seems redundant, you can
also have it downloaded from the server. I won’t explain how to it here,
since there are many possible ways to achieve this - after all, the indexed
DB is also accessible from JavaScript. If you want to experiment more
with Emscripten you can try to use the
<a href="https://emscripten.org/docs/api_reference/fetch.html">Fetch API</a>; in my
project I was not able to make its synchronous version work together with
<code>-sMODULARIZE</code>, so I ended up using
<a href="https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API"><code>fetch()</code></a>
directly from within an <code>EM_ASYNC_JS()</code> function. This tutorial is already
too long, so I am going to leave this as an exercise for the reader.</p>
<h2 id="closing-thoughts">Closing thoughts</h2>
<p>I have discussed almost everything that I have learned about building a
webapp in C / C++ with Emscripten. I ended up using C, not C++, for all
of my example, so I did not have a chance to discuss some neat C++-specific
features such as
<a href="https://emscripten.org/docs/porting/connecting_cpp_and_javascript/embind.html"><code>EMBIND()</code></a>
and
<a href="https://emscripten.org/docs/api_reference/val.h.html"><code>emscripten::val</code></a>
- do check them out if you plan to use C++ for your web app!</p>
<p>Even if this page is structured like a tutorial, this is probably better
described as a collection of personal notes, a “brain dump” that I wrote
for myself as is the case with many of my blog posts. Writing this piece
was a great occasion for me to review the work that I have done and the
things I have learned. And while reflecting on all of this I was able to
isolate a specific impression that I had while working on this,
and I summarized it in on sentence:</p>
<center><strong><i>
It’s leaky abstractions all the way down.
</i></strong></center>
<p>If you have not encountered this term before (but you should, I have already
used it in this post), <em>leaky abstraction</em> is a term used to describe the
failure of an abstraction to hide the low-level details it is abstracting.
The so-called
<a href="https://www.joelonsoftware.com/2002/11/11/the-law-of-leaky-abstractions/">law of leaky abstractions</a>
says that all abtractions are leaky. But, in my opinion not all
abstractions leak in the same way - some leak way more than others.</p>
<p>Emscripten is a great project that tries to abstract away all the web
(JavaScript, WASM, web workers, local storage…) so that you can build
and run your C / C++ code in a web browser. Frankly, this is mind-blowing,
and I have mad respect for the Emscripten developers.</p>
<p>But as soon as the complexity of your codebase bumps up a notch, you
immediately find out that the abstractions don’t hold anymore. If yor
app is multithreaded, you have to learn what a web worker is. If you
want to read some data from a file, welcome to the world of client-side
storage. You need 64-bit memory support because you are processing more
than 2GB of data? Sure, but first make sure that your users are not
using Safari.</p>
<p>But I am not complaining about this. A browser is a very different beast
from a bare-metal operating system, and it is to be expected that you
have to know something about the system you are deploying to. I am happy
that I could learn about all of this, and I believe this knowledge is
going to give me an extra edge whenever I’ll work on the web again.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Odyc.js – A tiny JavaScript library for narrative games (150 pts)]]></title>
            <link>https://odyc.dev</link>
            <guid>44200866</guid>
            <pubDate>Fri, 06 Jun 2025 13:46:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://odyc.dev">https://odyc.dev</a>, See on <a href="https://news.ycombinator.com/item?id=44200866">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <p>Play to create Games</p> <p>A small javascript library that lets you code video games even without programming experience.</p> <div><p><a href="https://odyc.dev/playground/">Create a game</a> <a href="https://odyc.dev/doc/getting-started/intro">Learn</a></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Small Programs and Languages (101 pts)]]></title>
            <link>https://ratfactor.com/cards/pl-small</link>
            <guid>44200797</guid>
            <pubDate>Fri, 06 Jun 2025 13:38:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ratfactor.com/cards/pl-small">https://ratfactor.com/cards/pl-small</a>, See on <a href="https://news.ycombinator.com/item?id=44200797">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
        <header>
            
                
            
            
            
            
            
            
                <p>Page started: <span>2025-06-02</span></p>
            
            
                <p>Page published: <span>2025-06-04</span></p>
            
            
        </header>

        
        <p>My <a href="https://ratfactor.com/forth/implementing">Implementing a Forth</a> article got some
great feedback, particularly around the subject of tiny Forth implementations.
(And it was an excuse to list some of the tiniest Forths I’ve seen.)</p>
<p>I know I’m not alone in seeing the appeal of tiny Forths, tiny languages, tiny
programs, and just small stuff in general. What’s up with that, anyway?</p>
<div>
<h2 id="_the_appeal_of_tiny_programs">The appeal of tiny programs</h2>
<div>
<p>Tiny programs are approachable.</p>
<p>If I see that something is 200 lines of code, I’m <em>much</em> more likely to
read it than if it’s 2,000 or 20,000 lines.</p>
<p>What’s more, if it’s a program that does something interesting in 20 lines when
I was <em>expecting</em>  2,000, then I’m going to be very, very interested to read
those 20 lines!</p>
<p>When I was researching DOM diffing libraries
prior to creating <a href="https://ratfactor.com/retrov/">RetroV</a>, one of
the repos I looked at was <strong>ijk</strong> by Luke Jackson.</p>
<p>I was pretty used to looking at fat JavaScript
libraries at that point, so when I cracked open
the source file for ijk, I was really surprised.
Here’s the source: <a href="https://raw.githubusercontent.com/lukejacksonn/ijk/refs/heads/master/index.js">index.js</a> (githubusercontent.com).</p>
<p><strong>Just 25 lines!</strong> I was instantly intrigued.</p>
<p>More recently, one of the responses to my aforementioned Forth article was
Philippe Brochard with a 46 byte "Forth".</p>
<p><strong>46 bytes!</strong></p>
<p>It’s <em>so</em> small, I can just put the hexdump right here:</p>
<div>
<pre>50b8 8e00 31d8 e8ff 0017 003c 0575 00ea
5000 3c00 7401 eb02 e8ee 0005 0588 eb47
b8e6 0200 d231 14cd e480 7580 c3f4</pre>
</div>

<p>Sure, very few people jump at a chance to read raw machine code. But that’s
exactly my point.  Something <em>this</em> small doesn’t feel so scary, right? I mean,
46 bytes. If you put the effort into it, you <em>know</em> you can figure out how it
works eventually.</p>
<p>This next one almost serves as a counter-example because the code is so dense
and frightening, but I can’t help mentioning it anyway.</p>
<p>One of my favorite software stories is Roger K.W. Hui’s Appendix A "Incunabulum"
from <em>An Implementation of J</em> in which he describes Arthur Whitney writing
an array-based language interpreter,</p>
<div>
<blockquote>
<p>"…​on <strong>one page</strong> and in one afternoon — an interpreter fragment
on the AT&amp;T 3B1 computer. I studied this interpreter for about a week for its
organization and programming style…​"</p>
</blockquote>
</div>
<p>Hui can list the entire interpreter in the appendix because it’s just that
single page, <strong>122 lines</strong> of incredibly cryptic C, which you can view here:</p>

<p>I haven’t personally tackled that puzzle yet, but it’s in my unwritten list of
dense computer things that I would print out and take with me if I suddenly
found myself cast out of society and forced to go live alone up on a mountain
or something.</p>
<p>(By the way, J is in the family of array languages which include APL.
If you enjoy brevity, cryptic programming puzzles, and high performance
numerical computing, these languages have got you covered!)</p>
<p>Something else that would certainly need to be in the collection I take to that
lonely mountain would have to be <em>The Most Beautiful Program Ever Written</em>,
according to the title of a talk by William Byrd.  Spoiler alert: It’s this
Lisp interpreter in Lisp:</p>
<div>
<pre>(define eval-expr
  (lambda (expr env)
    (pmatch expr
      [`,x (guard (symbol? x))
        (env x)]
      [`(lambda (,x) ,body)
        (lambda (arg)
          (eval-expr body (lambda (y)
                            (if (eq? x y)
                                arg
                                (env y)))))]
      [`(,rator ,rand)
       ((eval-expr rator env)
        (eval-expr rand env))])))</pre>
</div>

</div>
</div>
<div>
<h2 id="_recreation_or_fundamental_truths">Recreation or fundamental truths?</h2>
<div>
<p>Now, we all know that
<a href="https://en.wikipedia.org/wiki/Code_golf">code golfing</a> (wikipedia.org) is
a programmer’s recreational game, and just for fun, right?</p>
<p>But is any of this actually useful?</p>
<p>Actually, I’ll go one step further and say that these examples are
more than just useful. They are <em>meaningful</em>.</p>
<p>When you see a
<a href="https://fabiensanglard.net/rayTracing_back_of_business_card/">ray tracer that fits on a business card</a> (fabiensanglard.net), it makes you realize that a ray tracer is
<em>something that can fit on a business card</em>!</p>
<p>That’s an actual <em>fact</em> about the minimum complexity of ray tracers.</p>

<div>
<blockquote>
<p>"…​the length of a shortest computer program (in a predetermined programming language) that produces the object as output."</p>
</blockquote>
</div>
<p>(I learned about Kolmogorov complexity in a book about Kurt Gödel and it was my
favorite thing in what was otherwise a pretty dry and depressing book.)</p>
<p>The meaning I get from this is that code golfers, for the benefit of the whole
human race, produce versions of programs that represent the minimum
<em>complexity</em> of those programs. Way to go code golfers!</p>
<p>What do we know about the fundamental complexity of the Lisp programming language?</p>

<p>That’s a <em>fact</em> about Lisps with garbage collection. When you strip everything
else away, you can get the essentials down to <strong>436 bytes!</strong></p>
<p>I take comfort in this because that means the concept can be boiled down to
this hand-held size. And <em>that</em> means I can probably fit it in my head.</p>
<p>Thanks to Frank Force, we know that a 3D browser game can fit
in 2Kb of (insanely compressed) JavaScript source:
<a href="https://frankforce.com/how-i-made-a-3d-game-in-only-2k-of-javascript/">A 3D game in 2Kb of JS</a> (frankforce.com)</p>

<p>And we haven’t even scraped the <em>surface</em> of what the demoscene has to offer
for computers going back to the 1980s.
Here’s a good place to start: <a href="https://demoscene.assembly.org/">https://demoscene.assembly.org/</a></p>
<p>Mini-things are fun <em>and</em> useful <em>and</em> meaningful.</p>
<p>My point here is that you don’t have to actually study the intricate
clockwork puzzle of a miniaturized/golfed program to benefit from it.  The
miniature simply serves to prove that the concept <em>can</em> fit on a business card
and it <em>can</em> fit in your head.  Go ahead and find a readable version to study
instead. (The miniature will likely have <em>many</em> things to teach, but some of
them will be about how to make miniature programs.)</p>
</div>
</div>
<div>
<h2 id="_small_languages">Small languages</h2>
<div>
<p>Small programs are interesting. But I’m also interested in small programming
languages.</p>
<p><em>Generally</em> speaking, the smaller the language, the less expressive it is.</p>
<p>One of the most well-known, least expressive languages is assembly.</p>
<p>Assembly languages are <em>very</em> syntactically and conceptually simple.  There’s
not much language at all: it’s mostly a rigid sequence of opcode mnemonics.
Even with macros, writing in assembly is mostly about understanding the puzzle
of a particular CPU’s instruction set architecture, or ISA. (You can prove this
by comparing the syntax of assembly for CISC and RISC processors. The puzzle is
totally different, while the language remains practically unchanged.)</p>
<p>An honorable mention must be made here for <a href="https://ratfactor.com/snobol/">SNOBOL</a>, which is
kind of like assembly language both for the rigid syntactic reasons listed above and
<em>also</em> because it presents a similar control flow challenge. Like a CPU, it
only understands jumps and call/return. (In every other way, SNOBOL is
wildly different, being a string matching and replacing language. It’s weird
and fun and highly effective.)</p>
<p>Moving on to syntactically tiny "high-level" languages, these three are
brilliant:</p>
<div>
<ul>
<li>
<p><strong>Forths</strong> win just about any "smallness" contest. The only syntactical construct
is the space character. Forths are mind-bendingly flexible and powerful in a way that is downright disturbing.</p>
</li>
<li>
<p><strong>Lisps</strong>, (especially Scheme?), have small core languages with very simple syntax and enormous expressive power and flexibility.</p>
</li>
<li>
<p><strong>Tcl</strong> deserves to be mentioned with Forth and Lisp. Tcl is wild because <em>everything</em> is a string in a way that is hard to appreciate at first. You can construct your own language in Tcl just as you can with Forth or Lisp.</p>
</li>
</ul>
</div>
<p>Forth, Lisp, and Tcl only require you to understand a few core ideas and give
you limitless power in return. You could fit the syntactic core of these
languages on an index card or less.</p>
<p>And yet, to wield these tools effectively requires a massive shift in thinking
if you’re used to "normal" Algol-like procedural languages.</p>
<p>So how about something that is both small and simple and doesn’t require a
blood sacrifice?</p>
<p>I think the Lua programming language fits the bill. Check out how small the
core language is:</p>

<p>I’ve got the printed book version of this document and it’s a thin book! The
Lua language is covered in the beginning and it is just 27 pages. <strong>27
pages!</strong> I have game manuals longer than that.</p>
<p>The C language is pretty small. I don’t think it’s super easy to program
effectively and safely with C, but there’s no denying it’s compact and
pretty expressive for its size.</p>
<p>Newcomer Zig is C-like. But I think it’s actually quite large and you need to
know a lot of it to use it effectively.  (I’m also a fan of Zig and think it’s
<a href="https://ratfactor.com/zig/hard">totally worth it</a>. The size is there for a reason.)</p>
<p>JavaScript has a core language that is quite small. Probably on par with Lua,
actually. Functions, arrays, and objects (used as dictionaries) are pretty much
all the JS I use and I like it that way!</p>
<p>As with the natural human languages, the trade-off with a bigger programming
language is, hopefully, greater expressivity.</p>
<p>Small or expressive. Which way should you lean?</p>
<p>In a wonderful talk by
<a href="https://en.wikipedia.org/wiki/David_Ungar">David Ungar</a> (wikipedia.org)
titled <em>Self and Self: Whys and Wherefores</em>, he laments the mistake
of Self’s multiple inheritance:</p>
<div>
<blockquote>
<p>"It’s far better to make a system a little too simple so a person has to write extra stuff, even write it twice with a comment that says, 'Look over here,' than to have a system that acts in strange ways and nobody can understand why. So, simplicity trumps expressiveness."</p>
</blockquote>
</div>
<p>Got that? Here’s the bullet from the slide to help you remember:</p>
<div>
<p><img src="https://ratfactor.com/cards/images/david_ungar_self_and_self2.jpg" alt="snippet of a slide that reads Simplicity trumps expressiveness">
</p>
</div>
<p>I think small languages can <em>also</em> be fairly expressive, but only with very,
very careful design. Recall Forth, Lisp, and Tcl.</p>
<p><strong>Microworlds:</strong> The idea of small, self-contained, learnable programming
environments <em>also</em> has a David Ungar connection I discovered while watching
his talk.  See "ARK" on my <a href="https://ratfactor.com/cards/microworlds">Microworlds</a> card. (Smith and Ungar
co-created the <a href="https://selflanguage.org/">Self language</a> (selflanguage.org),
which is a notable milestone in programming and still under active development
to this day.)</p>
<p><strong>Libraries:</strong>
Another easily overlooked aspect of a programming language’s apparent size is
its "standard library" of functions, data structures, and methods.</p>
<p>I think function libraries are subject to the exact same size versus
expressiveness challenge.</p>
<p>For example, learning the
<a href="https://ramdajs.com/">Ramda</a> (ramdajs.com)
functional programming library for JavaScript was very much like learning a new
programming language and I never managed to learn all of it. The composability
of Ramda and the consistency of its curried functions was extremely eye-opening
to me. <strong>A little bit of Ramda goes a long way!</strong></p>
<p><strong>The feels:</strong> Concision in our tools has a certain quality that goes beyond
utility. Little things just <em>spark joy</em>, you know?</p>
<p>Why are small things so delightful?</p>
</div>
</div>
<div>
<h2 id="_the_appeal_of_tiny_things_in_general">The appeal of tiny things in general</h2>
<div>
<p>I’ve always had a fascination with miniatures, dioramas, and tiny scale models.</p>
<p>I know I’m not alone!</p>
<p><strong>Stephanie M. Langin-Hooper</strong>, author of <em>Figurines in Hellenistic Babylonia</em>
writes,</p>
<div>
<blockquote>
<p>"…​the act of shrinking a life-size thing to a smaller scale has an influential effect. The object is now cute, more accessible, more delicate, more 'squee'. It is also more helpless, and thus controllable – comforting and completely non-threatening due to its diminutive size.</p>
</blockquote>
</div>
<p>Which is exactly the reaction I have when I see a tiny
bit of code that does something useful.</p>
<p>(Also, I’m super okay with "controllable" in the context of a computer, but the
feeling of that paragraph changes a bit if we’re talking about, say, dolls.)</p>
<div>
<blockquote>
<p>"What might be threatening to think about in the real-scale world can be more manageable and more conceivable in miniature."</p>
</blockquote>
</div>
<p>Small things, "allow low-stakes experimentation," which
is why we often build tiny models of things before we scale them up.</p>

<p><strong>Simon Garfield</strong>, author of <em>In Miniature: How Small Things Illuminate the World</em>
has many of the same answers. He writes,</p>
<div>
<blockquote>
<p>"The answer lies in our desire for mastery and elucidation. The ability to enhance a life by bringing scaled-down order and illumination to an otherwise chaotic world – a world over which we may otherwise feel we have little control – cannot be overvalued. […​] At its simplest, the miniature shows us how to see, learn and appreciate more with less."</p>
</blockquote>
</div>
<p>I think this is a <strong>big</strong> part of what I get out of
<a href="https://ratfactor.com/assembly-nights">Assembly Nights</a> with the small scale and orderly logic
and internal consistency. Unlike the vast, fathomless Real World, an offline
computer is a fantasy place where I can believe I could understand <em>everything</em>
if I just tried hard enough for long enough.</p>
<div>
<blockquote>
<p>"The satisfaction of observing small things becomes a desire to make small things, and both stages address the human need for comprehension and order. We live in a huge and doomy universe, and controlling just a tiny scaled-down part of it restores our sense of order and worth."</p>
</blockquote>
</div>
<p>I’ve been studying computer history for a little while and I’ve only
scratched the surface. But one of the things I’ve been finding is that
a lot of the "big" stuff in our field are just accumulations of lots of
"little" victories as individuals have solved "little" problems in new and
different ways.</p>
<div>
<blockquote>
<p>"We create small universes in which we may bury ourselves to the exclusion of all else. Blocking out real life for a while – always the prerequisite of the dedicated domestic hobby, from doll’s house modeller to jigsaw enthusiast to adult book colourist – may be contemplative, meditative, blinkered and essential. The people crouching over tiny details as if the world depended on it are only doing it because their world does depend on it."</p>
</blockquote>
</div>


</div>
</div>

        
    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dystopian tales of that time when I sold out to Google (191 pts)]]></title>
            <link>https://wordsmith.social/elilla/deep-in-mordor-where-the-shadows-lie-dystopian-stories-of-my-time-as-a-googler</link>
            <guid>44200773</guid>
            <pubDate>Fri, 06 Jun 2025 13:36:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wordsmith.social/elilla/deep-in-mordor-where-the-shadows-lie-dystopian-stories-of-my-time-as-a-googler">https://wordsmith.social/elilla/deep-in-mordor-where-the-shadows-lie-dystopian-stories-of-my-time-as-a-googler</a>, See on <a href="https://news.ycombinator.com/item?id=44200773">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I will do something I normally never do here, and make my first ever blog post on the topic of, long sigh: tech. I’ve already talked about Google a number of times on Mastodon which is, blessedly, by design, not discoverable; but I’ve decided to commit the full story to print.  Hopefully this won't come back to bite me in the ass but eh it’s the apocalypse, who cares at this point. At least Wordsmith Dot Social is half-abandoned and has no comment system, so I won’t have to deal with techbros batting for billionaires or preaching the power of Open Source (™&nbsp;Open&nbsp;Source&nbsp;Initiativeⓡ).</p>

<p>But if you clicked this, Dear Reader, then you wanted the tea; and I am nothing if not forthcoming with tea-spilling. The fact that Google fired me with shut-up money only makes it more fun to do it. So go get some chamomile and sit comfortably, for this is an old woman reminiscing; let’s talk about capitalism and anarchism, about the precariat and surveillance, plush dolls and churrascarias and gay argots; let us go back in time and space, and journey to tropical Brazil in the distant time of 2007…</p>

<p><img src="https://files.transmom.love/google/camera.jpeg" alt="Black-white photo of a camera installed on an architectural detail like a geometric series of parallel slats. Taken 2008."></p>

<h2 id="1-treason">1. Treason</h2>

<p><img src="https://files.transmom.love/google/paranoia.jpeg" alt="Cover to the Brazilian edition of the pen-and-paper RPG called Paranoia, second edition from the nineties. In a comedic cartoon style, it shows a man in a red suit walking down a corridor that's heavily monitored, and about to be ambushed by an assortment of robots, agents, clones of himself, and random weapons coming out of the walls, including one cartoonish round bomb on one hand, being lit by a different hand coming from another hole of the futuristic wall."></p>

<p>2007 wasn’t a good year to start a new career, as it turned out.</p>

<p>Google back then prided itself on broadcasting its Best Place To Work award, won year after year after year.  Younger people will have trouble picturing this, but Google used to nurture an image of being the “good one” among megacorps; they championed open standards (except when they didn’t), supported open source projects (until they backstabbed them), and used language that corporate wasn’t supposed to use, like “don’t be evil” (until they, infamously and in a true dark comedy move, retracted that motto). The work environment was all colourful, nerdy cool, not a single necktie in sight—this was seen as brave and refreshing rather than cringe and tired, you see.  And they made a big deal out of something called “20% time”: Every engineer was promised 1/5 of their work time for themselves, to do anything they want. (Google owners will still own whatever you create during your 20% time, natürlich).  Famously, Gmail came out of someone exploring their interests during 20% time.</p>

<p>I don’t think much of anything else came out of it, though.</p>

<p>I found out I was always overworked on drudgery; my main job was to fix boring bugs on the Ruby on Rails internal user accounting system that someone else had developed.  When I complained that this was a far cry from the academia-like, exciting research environment I had been promised, and asked to be assigned to a more challenging project, I was told the following rationale against it: “no”.   Moreover the deadlines and expectations were such that even if I worked (unpaid) overtime every day, I was still was at risk of a performance review.  Making actual use of the “20% time” felt like a pipe dream.</p>

<p>And all that with wages well below even the local market in our crumbling Third World economy. With no exciting research positions nor self-managed time nor compensation, what was the advantage over a high-paying job at Microsoft or IBM? A bright blue vinyl floor and WarioWare Wii in the cafeteria? Well we were the hip tech vanguard, we were all geniuses, we were paid in prestige and promises and ego massages.  Perform good enough and you might be awarded a smattering of shares at some point, get some crumbs from the bountiful capitalists' table.</p>

<p>Like most employees I blamed myself for not working hard enough to get good compensation—or to have time to exercise my right of 20% free time… Until I saw in the “Googlegeist” statistics that some 95% of employees never use their “20% time” at all, being trapped under the same pressures as I was.</p>

<p>I started a discussion about how the recruiter's promise of “20% free time” could maybe be this little thing that the forgotten priestesses of ancient Samarkand called a “lie”.  There was an internal Blogger system, only available for other employees and bosses; and I wrote a post arguing that if no one feels able to use their 20% time, then it’s not much of a perk, is it.</p>

<p>The result of this was my boss having a fit over me “backstabbing” him. See, me complaining about the unfulfilled recruiter promises marked me as an Unhappy Googler. And Google, if you remember, was the Best Place To Work.  It was very important that every promising young engineer thought of Google as the dream job where everyone is happy.  Unhappiness isn't allowed.  My manager was severely scolded by <em>his</em> manager for having <em>dissatisfaction</em> (gasp) within his team.</p>

<p>I said, “But the issue is real and not my fault, don’t you agree? I just used the data to bring it to attention.  Didn't you say we operate under 'radical transparency'?” (I was young and believed in this kind of slogan. Yes, I was a sitting duck and didn’t stand a chance.)</p>

<p>Boss replied,</p>

<blockquote><p>Radical transparency doesn't mean you get to say negative things.</p></blockquote>

<p>Exact quote, I remember every word in that backroom in Phoenix, AZ.</p>

<hr>

<p>Ever heard of the 1984 dark comedy RPG «Paranoia»? It takes place in a dystopian future society called the Alpha Complex. The Complex is ruled by the Computer, which is perfect and makes no mistakes, guaranteeing the best possible life for all humans.  If someone commits a crime, for example, the Computer will always know—every wall has cameras—and instantly disintegrate the offender—every wall is packed with death lasers.</p>

<p>Of course, if you suggested that this ever happens, you would be implying that the Computer can raise a criminal human. It would logically follow that you’re holding the Computer to be less than perfect.  That accusation is a crime of treason.  Treason is punished with death.  Nobody ever complains about life in the Alpha Complex, which goes to show how perfect the Computer is.  In fact, everyone in the Alpha Complex is perfectly happy with the Computer's rule.  To feel unhappy is equivalent to accusing the Computer of making mistakes, and it therefore constitutes treason.  Happiness is mandatory.  Are you happy, citizen?</p>

<h2 id="2-the-google-precariat-part-i-dictbot">2. The Google Precariat, Part I: <code>dictbot</code></h2>

<p><img src="https://files.transmom.love/google/balaclava.jpeg" alt="Photo of the anti-racism protests in reaction to the murder of the Black teenager Nahel in France. While most people are dressed in normal riot gear, with black hoodies and COVID masks, the photo focus on a protester who is wearing a red net over their had that covers their entire face, eyes and nose and all.  Underneath they wear a striped yellow-white T-shirt and large, hip-hop style chains.">
<em>(Photo by Stephane DUPRAT / Hans Lucas.)</em></p>

<p>When you joined Google, you were quickly overwhelmed by massive amounts of corporate jargon—a hundred opaque project names, TLAs for everything etc.  To help new Googlers settle in, the Intranet had an online glossary.</p>

<p>Now in the spirit of “20% time”, we were encouraged to tinker with pet projects, or so they told me.  And we used to hang out in IRC chatrooms back then.  So I made a little IRC bot that would fetch definitions from the glossary.  Very basic stuff, if someone said “wtf is Chrome” in the channel, the bot would dump the summary paragraph, “Project Chrome is an initiative to develop a Google web browser, based on KHTML…”</p>

<p>I then got scolded for it, because I was leaking private information into a space that could be accessed by “temps, part-timers, and contractors”—Google's sprawling precariat (put a pin on that, more on that later).  As I alluded to, we Googlers were pampered with prestige; but the “temps, part-timers and contractors”—no fun name for them, they were always called “temps, part-timers, and contractors”—were second-class in Google Nation, had to be constantly put in their place in a myriad ways.  How else would the Engineers feel like geniuses, if there wasn’t a “normie” class to be treated worse than them?</p>

<p>One of the barbed-wire fences around temps, part-timers and contractors is that they should not have access to inside info, e.g. what is Project Chrome. My bot was, allegedly, violating that norm.  I pointed out that all that my bot did was to fetch info from the glossary page, and that anyone with access to the IRC channels already had access to the glossary page.</p>

<p>Dear Reader, this is how I became responsible for provoking the Computer into fencing away the glossary website from temps, part-timers, and contractors.</p>

<h2 id="3-a-lament-from-project-android">3. A lament from Project Android</h2>

<p><img src="https://files.transmom.love/google/nightingale.jpeg" alt="A colourful illustration of a clockwork bird, made of gold and jewels, singing. From a Russian edition of Andersen's &quot;Nightingale&quot; story.">
<em>Illustration: Джона Пейшенса, ISBN:  5-232-00383-6.</em></p>

<p>The Reader might well imagine how I had become persona non grata to my boss after the “backstabbing” episode.  When I wrote that blog post, I had gotten a number of emails from employees thanking me for talking about it, saying they’re glad someone is finally taking a stand,  praising me for my bravery.</p>

<p>Now my posture back then will feel very natural for those of you who only met me post-transition, and knew me from the start as this like, badass nazi-punching antifa thug with no filter and no sense of consequences.  But you have to understand: back then I was a shy little nerd terrified of everything. I wasn't brave; I was incredibly, <em>magnificently</em> naïve.  I was maybe the only person in the world who believed Google’s corporate kool-aid; I bit it hook, line and sinker, I really did believe we were some sort of new, dynamic academia, we didn't work in offices we worked in “campi”, the company was a way to fund exciting new research and we were there to improve the world by organising its information.  At least I thought I was.</p>

<blockquote><p>Interviewer: What attracted you to Google?
me: I agree with the Ten Principles of the company.
Interviewer: The what now?
Me: The Ten Principles? Google's Principles? In the 'about' page?
Interviewer: Uuh, sure…</p></blockquote>

<p>It did not even <em>occur</em> to me that it was all a scam, that everyone else knew it was all a scam and the actual point was to get rich.  In retrospect I should have read the undertones in early Paul Graham essays; I was a literary girl, I'm good at undertones; but I only read what I wanted to be true.</p>

<hr>

<p>Not long after my post in the Intranet Blogger, there was a post by some engineer I didn't know; a core programmer from the secret Project Android.  Out of the three big ones I had to stay quiet about, Project Android and Project Chrome got finished and became highly successful—only to immediately turn into world-wrecking monstrosities that we, low-level grunts, would never have imagined.  The third project, a physical-layer broadcast technology for the Internet—Youtube with HD quality if you logged in at showtime—never went forward.</p>

<p>But this insider, they were venting about how disappointed they had become with the directions that Project Android was taking.  They were losing their motivation, this is not what they thought the “Linux phone” would be about, this wasn’t what they signed up for.  The blogger was silent on any tech details, or what exactly was so disappointing; but with the benefit of hindsight it's easy to imagine.</p>

<p>A few days later, the same person posted something like “haha disregard that, I was having personal mental health issues and wrote a ill-conceived rant but it's all my fault really, of course there's a always few bumps but Project Android is amazing actually!! Y'all are going to love this, it's going to change everything!! We're organising the world's information and making a difference”, etc. etc.</p>

<p>Like, conspicuously back-to-back, the two posts.</p>

<p>I’m an airheaded bimbo but at some point the lesson will penetrate even my smooth silly brain.  This time, I was observant.</p>

<h2 id="4-mona-entendida-odara-elza">4. Mona, entendida, odara… 🤔 elza</h2>

<p><img src="https://files.transmom.love/google/travestis2.jpeg" alt="Police photo of a group of Brazilian travestis—a local transfeminine culture—detained in skimpy clothing, their photos blurred."></p>

<p>I wasn't out as trans yet, but I was already proud to be queer.  Showed up first day with neon orange hair, unicorn T-shirt, the works.  That made of me a Gaygler™, and Google Belo Horizonte was always happy to have me on team photos to add some colour and progressiveness to the image.</p>

<p>Now even though Google is fundamentally a spyware advertising company (some 80% of its revenue is advertising; the proportion was even higher back then), we Engineers were kept carefully away from that reality, as much as meat eaters are kept away from videos of the meat industry: don't think about it, just enjoy your steak.  If you think about it it will stop being enjoyable, so we just churned along, pretending to work for an engineering company rather than for a giant machine with the sole goal of manipulating people into buying cruft.  The ads and business teams were on different floors, and we never talked to them.</p>

<p>One day one of the AdSense people asked me for a little meeting.  They sat right by my desk, all sleek and confident, and said that they had heard I was a Gaygler™ and were wondering if I could help with one of their clients.  “Can you tell me some words that the Brazilian gay community uses? like slang, popular media you like, names of parties, that kind of thing?”</p>

<p>Caught off-guard and unsure how to react, I struggled to think of gay-coded speech, and I was expertly mined for pajubá terms to be fed into the machine.  Whole interaction took maybe ten minutes.  The AdSense goon left, never to be seen again, leaving me feeling violated in ways I couldn't articulate.</p>

<p>Google supported its queer employees.</p>

<hr>

<p>After I got marked as a troublemaker and put into the inevitable performance review, one of the items raised against me was that my company profile page was 'too personal'.  the extent of personal information in my profile was this sentence: “I am a nerd, a bisexual polyamorist, and a parent.”¹</p>

<h2 id="5-the-google-precariat-part-ii-a-water-purifier-s-salary">5. The Google Precariat, Part II: A water purifier’s salary</h2>

<p><img src="https://files.transmom.love/google/2klassenpunsch_karl_berge.jpeg" alt="Cartoon of a homeless person being rejected at the communal kitchen for lacking documents.">
<em>“Sure, we give aid to the poor! We’ll only need your registration forms, bank statement, and certificate of good conduct!” Cartoon by Karl Berger for Augustin.</em></p>

<p>You might have noticed, Dear Reader, that I have made somewhat contradictory claims: 1) that we Engineers were pampered, and 2) that we Engineers were underpaid, pressured to do unpaid overtime at salaries low even for the Brazilian market.  Such was the carrot and the stick.  We all were told that if we performed just a bit better we would get higher pay, shares, positions at cool projects, and the biggest carrot of all: a relocation to the magical Global North where human rights are real.  A way through the wall.</p>

<p>We trudged on, with little more than promises and hope.  But we trudged on with <em>style.</em>  The offices were all gaudy in Google colours with vinyl flooring, full of fridges with free snacks; the break room had the latest Playstation with brand-new high-tech Rock Band controllers; when you joined in you got a small bonus to buy toys for your desk (most Engineers got legos, I got a pink Kirby plushie I would dress up).  This was unheard of; companies at the time were all Microsoft, all performative professionalism, Google was fun! Google gave you Perks, gods, so so many Perks. the Lumon motivation baubles from “Severance” gave me heavy Google flashbacks.  We were periodically treated to dinner with the managers at the most expensive churrascarias.  Master let us eat right there with him, inside the big house.</p>

<p>I will be honest and say that most of my fellow programmers ate that shit up, we had all been gold-star kids and here was the hottest company in the world constantly massaging our egos, telling us we were better than everyone for being geniuses. I would have loved to feel the same, I <em>tried</em> to feel the same, but I came from poverty and I could not stop noticing the precariat: temps, part-timers, and contractors, an entire layer of the company who did the brunt of work without being Googlers.  No toy budget for kitchen staff.</p>

<p>It's the little things that bugged me, how people would eat the free candy or have a bowl of cereal and just leave trash and dirty dishes everywhere for the cleaning ladies (contractors) to deal with; more than that the way nobody looked at them or said “thank you”.  We Brazilians have a social class for that, a social code underlying that studied invisibility, I knew what this was: these were maids.²  Servants.  The women in my family, my friends at school.  The “campus” was pretty open and my then-wife visited it a few times; it creeped the&nbsp;Fuck out of her, the distinction between people and non-people.</p>

<hr>

<p>We had those expensive, high-tech water purifiers, several on each floor. One day there was a discussion on the topic of cost savings, and I suggested the traditional Brazilian solution—the well-known ceramic filters in terracotta jars; they're consistently rated among the safest, need no electricity, make the water cool even in summer without spending any energy, cost little and last a long time before you need to replace the charcoal element, which is anyway inexpensive.  The idea was dismissed out of hand.  Too low-tech, I suppose.³</p>

<p>The fancy water purifiers weren't owned by Google; they were leased, at a high cost.  Somehow it bothered me a lot that each of those excessively technological water monsters got more money per month than any temp, part-timer, or contractor.</p>

<p>The water purifiers were never fired.</p>

<h2 id="6-cathy-don-t-send-that-email-today">6. Cathy don't send that email today</h2>

<p><img src="https://files.transmom.love/google/cathy.jpeg" alt="A still from the music video for &quot;Cathy don't go to the supermarket today&quot; (1985), by the extremely abusive sex cult Family International. The song is about how paying with barcodes and cards was an implementation of the Mark of the Beast and will cost your soul.  In the still, a large, creepy, Terminator-like enforcer in a trenchcoat is stopping a woman from paying groceries in cash. The number &quot;666&quot; is partly visible on the walls."></p>

<p>Google was my first taste of smartphones, back when that meant a Blackberry (delightful, sturdy little corporate toys with pleasantly clicky, full-QWERTY thumb-keyboards). Mobile data plans were prohibitively expensive for anyone on wage labour, but I was graciously allowed to use my company phone for private purposes; and I delighted in the novelty of not getting lost for once, walking up and down the hills of Belo Horizonte with futuristic, always-on Google Maps under, whoa, unlimited data.</p>

<p>Which is to say, Google was my first taste of the surveillance society that has now become the new normal.</p>

<p>The Reader will remember our big carrot; all of us at Google Brazil worked hard to get the job because it meant a ticket to the Global North (potentially). Now I had been a weeb from an early age, and back then I was already like, intermediate to advanced in Japanese.  So of course my dream was to move to Japan.  But when I talked about it with my boss—a disembodied face from Phoenix to whom I would report under a giant monitor; this too felt very new, very high-tech, and very dystopic at the time—he dismissed the idea out of hand, saying my Japanese wasn't fluent and that this would make me a poor fit.</p>

<p>I talked to my colleagues about it and someone said, wtf girl no, most international engineers brought to Shibuya cannot even say konnichiwa, if anything your language ability and cultural experience with the diaspora make you the ideal candidate.  We had a relevant contact in Google Sweden, and my mates said I should talk to them about contacting Shibuya directly regarding relocation.</p>

<p>And there I was after putting a target on my back as a troublemaker, about to directly contradict my boss and look for a way into Japan behind his back.  My colleagues <em>sternly</em> advised me to <em>never</em> mention any of this by email, and also not call from my desk. “You really think they would do that? Just go on my email inbox and breach privacy? :O” International calls were very expensive those days and I didn’t have a landline, so I ended up calling Sweden from a company line inside a little cleaning closet, between brooms and bottles of disinfectant, in the dark, after everyone was gone from the office.  Sorry, “campus”.</p>

<p>The Sweden contact told me they knew people in Tōkyō and were sure they would be happy to have me.  A couple weeks after that, I was fired.  (Mid-economic crisis, in the 3rd world, with one 2-year-old kid and another about to be born.)</p>

<p>And it was <em>so</em> weird and surreal to be in that little locker room, afraid of every whisper, aware that every communication was being spied on.  And when I tell this story to my now adult children, I struggle to convey how weird it was.  I realised belatedly that they never <em>experienced</em> existing with technology without it being the default expectation that it's hostile to you and it's spying on you all the time.  For them this has been the case <em>all of their lives</em>.</p>

<p>Today, the concept of “spyware” has been obsoleted because every software is spyware.  Google's “organising the information of the world” turned out to be indexing which Gaza families to bomb, children and all; “making money in the free market to invest in social change” was about bankrolling literal, textbook fascism.  Today, for us Latinx to even briefly step in the USA, if we don't have an always-on handheld device with spyware “social media”, its absence is taken as proof of criminality.  I will never visit Arizona again, and my kids will never know a world that's not like this; but for me I saw this world being forged up close and personal, deep in Mordor where the shadows lie.</p>

<h2 id="7-the-google-precariat-part-iii-without-a-heart-to-guide-them-the-other-powers-are-useless">7. The Google Precariat, Part III: Without a Heart to Guide them, the Other Powers are Useless</h2>

<p><img src="https://files.transmom.love/google/captain_planet.jpeg" alt="Fanart of the Eco-Villains from children's cartoon Captain Planet, coloured by hand. Villains include Hoggish Greedly, a rich man in a short mohawk and green suit; Looten PLunder, a sleazy-looking hunter in noveau-rich furs; Sly Sludge, looking less like an oil magnate and more like an operative in working overalls; Duke Nukem, a radioactive monster who looks like a stony yellow humanoid; Dr. Blight, a sexy mad scientist grinning evily, and her AI, MAL, shown as a digital green face; and Verminous Skumm, a rat-human hybrid.">
<em><a href="https://www.deviantart.com/vultureclaw/art/Captain-Planet-Eco-Villains-303582092" rel="nofollow">Art: Vultureclaw</a></em></p>

<p>I was always an anarchist, abstractly, but in many ways Google was my political awakening.</p>

<p>We had an office party every Friday evening.  Every single Friday.  It was called TGIF, “thanks God it's Friday”, and involved fancy finger food, drinks, and more of those dystopic heads on monitors talking to us of all the great things Google was doing to revolutionise the world.  Thanks to TGIFs, we all could leave work early on Friday afternoons.</p>

<p>I was such a sucker for things like this, I was so entranced by the food variety and the socialisation and the festive atmosphere, that it took me a long time to think of Bretch's question (“All those feasts—who did the dishes?”).⁴  Belatedly I realised that none of the dishwashers would think of Friday afternoons like, “graças a Deus é sexta-feira”.  My privilege of working less and partying every week was paid by them staying <em>late</em> every Friday, dealing with the aftermath of our juvenile entitlement.  Most of these women will never step inside a Fogo de Chão restaurant in their lives; while we were taken on fancy dinners at a whim by the bosses, when they wanted to reassure us of our specialness.</p>

<hr>

<p>One day, shortly before I was fired, the 2008 crisis had hit full force, the Phoenix office that managed us got shut, and Google had fired 70% of the South American precariat, in one fell swoop.  Then, during one of my last TGIFs, I accidentally listened to two high-level managers talking about it, two white male gringos in expensive business-casual. They were commenting on how the company was still doing perfectly fine without all that weight.</p>

<p>And that's not what stuck with me, the arguments, no.  I understood the incentives to do layoffs, and the human need to rationalise them.  What stuck with me was their happy smiling faces.  Their <em>laughing</em>.</p>

<p>Yes they laughed about it.  Out loud.</p>

<p>I had full awareness of what it meant for Third World people to be fired under the crisis, what it was about to be like for the Argentinians, for our families—but <em>so did they</em>, they were down here, they knew the reality.  They talked to us every day, they had their spreadsheets handled by temps and were now here eating food prepared by contractors.  Yet here they were, in tailored clothes that cost more than a cafeteria lady's living expenses, partying happily without even bothering to <em>pretend</em> to be sad about all those families.  Not caring enough about us to <em>even bullshit.</em></p>

<p>Any sympathies I might have had about the simplistic logic of free-market liberalism evaporated under that laugh.</p>

<p>As a little girl I used to despise cartoons like Captain Planet, whose devilish, paper-thin villains destroyed the world with manic laughs for nothing but the thrill of power, polluting for the sake of polluting.  I thought that was deeply unrealistic, and condescending too; I felt talked down to.  I cherished nuanced villains like Lady Eboshi from Mononoke-hime, the leader of Irontown who was destroying the ancient forest—but with the goal of liberating women from violent patriarchy and poverty; Irontown was a refuge for outcasts, its mining economy a ticket out of male domination, and Lady Eboshi would give her own life for her girls.  Complexity! Humanity!</p>

<p>It was at Google that I learned that no, capitalists are actually literally the same as Captain Planet villains.  We are not blessed enough to live in Ghibli reality, capital owners built us a 90s trashy USA cartoon reality. What is crypto mining if not a textbook Captain Planet villain scheme—to kill and raze and destroy for nothing but imaginary tokens proving that you did lots of killing and razing and destroying? What is GenAI if not stealing energy and water and even art itself, only to syphon it all into a grinder, producing no benefit but the hoarding of even more money away from the poors—when you already have more money than a human being could possibly ever spend? What is this all-encompassing addiction to “number go up” if not Sly Sludge, dripping happily with pollutants, going “Aloha suckers! I'll miss this profit paradise but I have a souvenir to remember it by”, as he picks a briefcase full of money and leaves the island to explode?</p>

<hr>

<p>My experience at Google drove me to want to understand capitalism, and I would eventually find in Malatesta the answer as to why capital owners cannot help but be cruel, revel in cruelty, performatively broadcast cruelty; why the cruelty is indeed almost a side effect, a corollary to what it means to <em>do</em> capitalism.  A mould that grows inevitable on material that’s inherently rotten. Every action you take has consequences not just for the world but for your psyche; you cannot avoid being affected by your decisions, anymore than you can avoid the third law of motion when you punch a wall.  You cannot make people work for you and hoard all the profits while they are stuck with fixed salaries, without in the process developing strong feelings on why you're entitled to do that and how they deserve it actually.</p>

<p>But before I got into political theory, it was Google who <em>demonstrated</em> to me what is capitalism, firsthand up close. I wouldn't say that this was worth working there, but I benefited from the lived experience; from that part of it, and nothing else.</p>


<ol><li><p>That was me in egg state beating around the bush; I am now fully out as a jock, a lesbian relationship-anarchist, and a mother.  I added this footnote so that men stop hitting on me because I wrote the b-word once in this text about capitalism.</p></li>

<li><p>Anyone interested in the Latina “maid” as a social class is encouraged to watch <em>Que horas ela volta? (2015)</em> (English title: <em>The Second Mother</em>). It’s an engaging and heartwrenching film but keep in mind: everything it portrays about the social othering of maids is factually true, and happening today.</p></li>

<li><p>If this kind of thing appeals to you <em>and</em> you haven’t heard of it yet, I am pleased to introduce you to <a href="https://solar.lowtechmagazine.com/" rel="nofollow">the low tech magazine</a>.</p></li>

<li></li></ol>

<blockquote><p>Who built Thebes of the Seven Gates?
All articles name the names of kings; I gather the kings
brought those boulders on their royal backs?
And great Babylon, who fell and fell again,
Who put'er back together, every time? In which flats
of gold-paved Lima lived the road-pavers?
The night the Great Wall of China was finished, where did
the construction crew hang out? Awesome Rome
is full of triumphal archs.  Who arched them up? Also—
who did the Caesars triumph over? We sing the palaces
    of Byzantium—
the whole thing was just palaces? Even Atlantis of tall tales
shouted, choking, as the seas swallowed it whole—</p>

<p>for its slaves.</p>

<p>Young Alexander conquered India.
All by himself then?
Caesar defeated the Gauls.
Did he bring along a cook at least?
Felipe de España, el Prudente, cried when his Armada
sunk into the sea.  And nobody else cried that day?
In the Seven Years' War, Federico Secondo grasped
victory.  Who else grasped it with him?</p>

<p>All these pages, all these conquests.
All those feasts—who did the dishes?
Every ten years a new Great Man.
Who covered the budget?</p>

<p>So many headlines.
So many questions.</p></blockquote>

<p><img src="https://files.transmom.love/google/zapatista.jpeg" alt="Art from the Zapatista revolutionaries. It shows a crowd of women and children in colourful indigenous clothes, all wearing red bandanas or black balaclavas, armed wtih sticks—children inclusive—and staring at the viewer between curly yellow-green leaves."></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Being fat is a trap (145 pts)]]></title>
            <link>https://federicopereiro.com/fat-trap/</link>
            <guid>44200199</guid>
            <pubDate>Fri, 06 Jun 2025 12:32:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://federicopereiro.com/fat-trap/">https://federicopereiro.com/fat-trap/</a>, See on <a href="https://news.ycombinator.com/item?id=44200199">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

			
<p>This has been a difficult article to write. Partly because this is a painful topic for many, and partly because I have to be completely open and vulnerable to write about it. I only write this in the hope it can help others in similar situations. Nothing in this article is meant to judge you. If someone judges you for being or feeling fat, they should go f…figure out themselves. Judging people inside the fat trap just intensifies their misery and reduces the odds they can get out of it.</p>



<p>Let’s start with my story. I have been overweight (occasionally bordering on obese) from about age 8 to 23. At 23 I found out I was pre-diabetic and went on a strict three month diet that put me back on a healthy weight. I’ve maintained a healthy weight since then till the present (I’m 40 now). However, I’ve spent most of that time focused (sometimes obsessed) with not becoming fat again and generally struggling with food. It is only in the past couple of years that I am finally starting to trust myself with food and giving up the feelings of worry and insecurity about how my body looks and feels.</p>



<p>The notion of two traps, a physical trap and a mental trap, comes from Allen Carr’s fantastic <a href="https://en.wikipedia.org/wiki/The_Easy_Way_to_Stop_Smoking">Easy Way To Stop Smoking</a>. He states, quite convincingly, that there is a physical aspect to smoking (nicotine addiction) and a mental aspect to smoking (feeling that you need it).</p>



<p>I believe that the fat trap is also physical and mental. Let’s start with the physical, which is the easiest to understand. There is a range of body fat percentage that is healthy. If you are outside (probably above) this percentage, you are damaging your health. The science is unequivocal on this. What the range is is a bit harder to ascertain, but, from what I’ve read, it is about 10-20% for men and 15-25% for women.</p>



<p>The physical trap of being fat is being physically laden with just too much fat. Besides the overall damage to your health, it is less fun to be in an overweight body: you have less energy; you have more aches; it is harder to move, your sleep is worse. As painful and sad this is, there is no denying it.</p>



<p>Now, I believe the body positivity movement is a great step forward. Body positivity is about accepting others’ bodies, as well as your own, without regard to size, shape and gender. For those inside the fat trap, this brings tremendous relief. Being judged for being fat, or for being obsessed about fat, is almost always extremely counterproductive. It is harmful. If you are a non-fat person that goes around judging fat people, it might astonish you to find out that <strong>most fat people are painfully and constantly aware that they are fat</strong>, as well as the fact that that’s bad for them and they should make a change. There is nothing to be gained and everything to lose by judging someone for being fat. Eating disorders have extremely high mortality rates among psychiatric disorders: <em>if you’re making someone feel bad about their weight, you may well be increasing the likelihood they will die sooner.</em></p>



<p>And while self-acceptance is essential to get out of the mental fat trap, I don’t think you should accept that you are going to be overweight or even obese for the rest of your life. I might be wrong, but I believe you have a choice. It might be a very difficult choice, and it may take years, but you have the choice to step out of the fat trap. Both of them.</p>



<p>Getting out of the physical fat trap, strictly speaking, is simple:</p>



<ul>
<li>Develop basic sleep habits so that you get decent sleep.</li>



<li>Exercise daily or almost daily.</li>



<li>Walk 7500 steps a day.</li>



<li>Reduce refined carbs, unhealthy fats and alcohol from your diet. Focus on getting enough vegetables, fruits, complex carbs and healthy fats.</li>



<li>In some cases, you’ll also need medical advice, particularly if you have hormonal issues.</li>
</ul>



<p>If you stick to the above, you’ll be out of the physical fat trap in a few months — if you are morbidly obese, it might take a couple of years. But it doesn’t really matter how long it will take. What matters is that you start the journey.</p>



<p>Simple, however, is not easy. And what really makes it hard to stick with a healthy lifestyle is the mental fat trap.</p>



<p>The mental fat trap is the most difficult one to escape. For me, it manifests in the following thoughts:</p>



<ul>
<li>I want food but I can’t have it.</li>



<li>If I have what I want, I will be fat again.</li>



<li>I cannot trust myself with food.</li>



<li>I don’t look good enough to feel good about myself.</li>



<li>When I manage to achieve X target weight, <em>then</em> I will feel good.</li>



<li>Counting the days or weeks until you reach “your goal”.</li>



<li>I’m too old not to be fat.</li>



<li>I don’t have the genes not to be fat.</li>



<li>I don’t have the genes to eat healthily.</li>



<li>Let’s read about this diet.</li>
</ul>



<p>These thoughts are the walls of the mental fat trap. It is an obsession with the problem of being or feeling fat, even if you’re objectively not overweight. A typical pattern (at least for me) is to fixate on “when will I get there”. The answer is: you are there now. It is only through daily acceptance and work that you can be free. This point comes straight from Carr’s insight: that the mental trap is a waiting-for-nothing. When you focus on the present, the trap dissolves.</p>



<p>If getting out of the physical trap is to switch to a healthy lifestyle and stick with it, getting out of the mental trap is to leave behind the thoughts that keep you obsessed with food and dieting. Getting out is learning to identify as someone that no longer has issues with food, and that is healing every day from the wounds of being in the fat trap.</p>



<p>How to get out of the mental fat trap? I know three ways only.</p>



<ul>
<li>Therapy.</li>



<li>Meditation.</li>



<li>Read books on the subject.</li>
</ul>



<p>Therapy is going to reveal painful, uncomfortable truths that you’ve been avoiding — or rather, confronting indirectly through your obsession with food. Therapy can also give you tools to replace your harmful thoughts with better alternatives.</p>



<p>Meditation will give you space to detach from your thoughts, so that you can identify them and be able to start letting some of them go.</p>



<p>Reading is probably the easiest starting point. I can highly recommend <a href="https://federicopereiro.com/notes-roth-breaking-free/">Geneen Roth’s Breaking Free from Emotional Eating</a>. This book taught me about how some of us use food as a way to comfort ourselves, numb ourselves, and generally avoid or withstand painful aspects of our lives. Another great recommendation is <a href="https://www.amazon.com/Let-Go-story-about-weight/dp/9081958437">Andrew Dasselaar’s Let Go</a>. Both books are based on harrowing personal trials by the authors. Geneen has been helping people out of the fat trap for decades.</p>



<p>I’m still not out of the mental fat trap, at least not yet. I’m mostly out, and very grateful to have made it this far, but I still worry about food every other day or so. For me, getting out of this trap is a process of letting go of those thoughts that make food the main problem of my life, as well as the go-to solution for any uncomfortable feelings that arise.</p>



<p>I hope to have helped you, and not hurt you. If you think any of this is wrong or counterproductive, please let me know (fpereiro@gmail.com) and I will carefully consider your opinion.</p>

		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Doge Developed Error-Prone AI Tool to "Munch" Veterans Affairs Contracts (135 pts)]]></title>
            <link>https://www.propublica.org/article/trump-doge-veterans-affairs-ai-contracts-health-care</link>
            <guid>44199887</guid>
            <pubDate>Fri, 06 Jun 2025 11:49:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.propublica.org/article/trump-doge-veterans-affairs-ai-contracts-health-care">https://www.propublica.org/article/trump-doge-veterans-affairs-ai-contracts-health-care</a>, See on <a href="https://news.ycombinator.com/item?id=44199887">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-pp-location="article body">

        
                    <div data-pp-location="top-note">
                

                                                
            <p>ProPublica is a nonprofit newsroom that investigates abuses of power. Sign up to receive <a href="https://www.propublica.org/newsletters/the-big-story?source=www.propublica.org&amp;placement=top-note&amp;region=national">our biggest stories</a> as soon as they’re published.</p>

                

            </div><!-- end .article-body__top-notes -->
        
        
        




                    

<figure data-pp-id="1" data-pp-blocktype="embed">

    


                        
            
    
<figcaption>
    
    
    
    </figcaption>


</figure>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="2.0">As the Trump administration prepared to cancel contracts at the Department of Veteran Affairs this year, officials turned to a software engineer with no health care or government experience to guide them.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="3.0">The engineer, working for the Department of Government Efficiency, quickly built an artificial intelligence tool to identify which services from private companies were not essential. He labeled those contracts “MUNCHABLE.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="4.0">The code, using outdated and inexpensive AI models, produced results with glaring mistakes. For instance, it hallucinated the size of contracts, frequently misreading them and inflating their value. It concluded more than a thousand were each worth $34 million, when in fact some were for as little as $35,000.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="6.0">The DOGE AI tool flagged more than 2,000 contracts for “munching.” It’s unclear how many have been or are on track to be canceled — the Trump administration’s decisions on VA contracts have largely been a black box. The VA uses contractors for many reasons, including to support hospitals, research and other services aimed at caring for ailing veterans.</p>
        
    
                        
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="8.0">VA officials have said they’ve killed nearly 600 contracts overall. Congressional Democrats have been pressing VA leaders for specific details of what’s been canceled without success.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="9.0">We identified at least two dozen on the DOGE list that have been canceled so far. Among the canceled contracts was one to maintain a gene sequencing device used to develop better cancer treatments. Another was for blood sample analysis in support of a VA research project. Another was to provide additional tools to measure and improve the care nurses provide.</p>
        
    
                    
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="11.0">ProPublica obtained the code and the contracts it flagged from a source and shared them with a half dozen AI and procurement experts. All said the script was flawed. Many criticized the concept of using AI to guide budgetary cuts at the VA, with one calling it “deeply problematic.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="12.0">Cary Coglianese, professor of law and of political science at the University of Pennsylvania who studies the governmental use and regulation of artificial intelligence, said he was troubled by the use of these general-purpose large language models, or LLMs. “I don’t think off-the-shelf LLMs have a great deal of reliability for something as complex and involved as this,” he said.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="13.0">Sahil Lavingia, the programmer enlisted by DOGE, which was then run by Elon Musk, acknowledged flaws in the code.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="14.0">“I think that mistakes were made,” said Lavingia, who worked at DOGE for nearly two months. “I’m sure mistakes were made. Mistakes are always made. I would never recommend someone run my code and do what it says. It’s like that ‘Office’ episode where Steve Carell drives into the lake because Google Maps says drive into the lake. Do not drive into the lake.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="15.0">Though Lavingia has talked about his time at DOGE previously, this is the first time his work has been examined in detail and the first time he’s publicly explained his process, down to specific lines of code.</p>
        
    
                        
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="17.0">Lavingia has nearly 15 years of experience as a software engineer and entrepreneur but no formal training in AI. He briefly worked at Pinterest before starting Gumroad, a small e-commerce company that nearly collapsed in 2015. “I laid off 75% of my company — including many of my best friends. It really sucked,” he said. Lavingia kept the company afloat by “replacing every manual process with an automated one,” according to <a href="https://sahillavingia.com/god">a post on his personal blog</a>.</p>
        
    
                    

<figure data-pp-id="18" data-pp-blocktype="image">

    


                    
    


    <img alt="A man wearing khakis, a blue jacket and white sneakers sits in a leather armchair in an office in a renovated loft, decorated with deflated pink balloons, some paper streamers and a potted artificial tree." width="3000" height="2001" loading="lazy" js-autosizes="" src="https://img.assets-d.propublica.org/v5/images/sahil_brs4154_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=534&amp;q=75&amp;w=800&amp;s=684cbd8f717d254a1cd5ab606983bf89" srcset="https://img.assets-d.propublica.org/v5/images/sahil_brs4154_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=267&amp;q=75&amp;w=400&amp;s=cabcfd67565f1f031df5d6a0650bb5e2 400w, https://img.assets-d.propublica.org/v5/images/sahil_brs4154_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=534&amp;q=75&amp;w=800&amp;s=684cbd8f717d254a1cd5ab606983bf89 800w, https://img.assets-d.propublica.org/v5/images/sahil_brs4154_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=800&amp;q=75&amp;w=1200&amp;s=626caeef3003d72b1fee76ad7c13101b 1200w, https://img.assets-d.propublica.org/v5/images/sahil_brs4154_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=867&amp;q=75&amp;w=1300&amp;s=5233281dd1fbc9d33f09311e437983fa 1300w, https://img.assets-d.propublica.org/v5/images/sahil_brs4154_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=967&amp;q=75&amp;w=1450&amp;s=ad8135856b060cdba0760a73b537742a 1450w, https://img.assets-d.propublica.org/v5/images/sahil_brs4154_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1067&amp;q=75&amp;w=1600&amp;s=a43a48c8965cafb5a7b3028827c8564a 1600w, https://img.assets-d.propublica.org/v5/images/sahil_brs4154_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1334&amp;q=75&amp;w=2000&amp;s=74c8ff0acf368e50b4cd2d992c7b3f8a 2000w">

            
    
<figcaption>
        <span>Sahil Lavingia at his office in Brooklyn</span>
    
        <span>
        <span>Credit: </span>
        Ben Sklar for ProPublica
    </span>
    
    
    </figcaption>

</figure>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="19.0">Lavingia did not have much time to immerse himself in how the VA handles veterans’ care between starting on March 17 and writing the tool on the following day. Yet his experience with his own company aligned with the direction of the Trump administration, which has embraced the use of AI across government to streamline operations and save money.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="20.0">Lavingia said the quick timeline of <a href="https://www.whitehouse.gov/presidential-actions/2025/02/implementing-the-presidents-department-of-government-efficiency-cost-efficiency-initiative/">Trump’s February executive order</a>, which gave agencies 30 days to complete a review of contracts and grants, was too short to do the job manually. “That’s not possible — you have 90,000 contracts,” he said. “Unless you write some code. But even then it’s not really possible.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="21.0">Under a time crunch, Lavingia said he finished the first version of his contract-munching tool on his second day on the job — using AI to help write the code for him. He told ProPublica he then spent his first week downloading VA contracts to his laptop and analyzing them.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="22.0">VA press secretary Pete Kasperowicz lauded DOGE’s work on vetting contracts in a statement to ProPublica. “As far as we know, this sort of review has never been done before, but we are happy to set this commonsense precedent,” he said.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="23.0">The VA is reviewing all of its 76,000 contracts to ensure each of them benefits veterans and is a good use of taxpayer money, he said. Decisions to cancel or reduce the size of contracts are made after multiple reviews by VA employees, including agency contracting experts and senior staff, he wrote.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="24.0">Kasperowicz said that the VA will not cancel contracts for work that provides services to veterans or that the agency cannot do itself without a contingency plan in place. He added that contracts that are “wasteful, duplicative or involve services VA has the ability to perform itself” will typically be terminated.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="25.0">Trump officials have said they are working toward a <a href="https://www.nytimes.com/2025/05/06/us/politics/doug-collins-veterans-affairs-job-cuts.html">“goal” of cutting</a> around 80,000 people from the VA’s workforce of nearly 500,000. Most employees work in one of the VA’s 170 hospitals and nearly 1,200 clinics.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="26.0">The VA has said it would avoid cutting contracts that directly impact care out of fear that it would cause harm to veterans. ProPublica recently reported that relatively small cuts at the agency have already <a href="https://www.propublica.org/article/trump-veterans-affairs-budget-staff-cuts-jeopardize-cancer-research">been jeopardizing veterans’ care</a>.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="27.0">The VA has not explained how it plans to simultaneously move services in-house, as Lavingia’s code suggested was the plan, while also slashing staff.</p>
        
    
                                  
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="29.0">Many inside the VA told ProPublica the process for reviewing contracts was so opaque they couldn’t even see who made the ultimate decisions to kill specific contracts. Once the “munching” script had selected a list of contracts, Lavingia said he would pass it off to others who would decide what to cancel and what to keep. No contracts, he said, were terminated “without human review.”</p>
        
    
                        
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="31.0">“I just delivered the [list of contracts] to the VA employees,” he said. “I basically put munchable at the top and then the others below.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="32.0">VA staffers told ProPublica that when DOGE identified contracts to be canceled early this year — before Lavingia was brought on — employees sometimes were given little time to justify retaining the service. One recalled being given just a few hours. The staffers asked not to be named because they feared losing their jobs for talking to reporters.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="33.0">According to one internal email that predated Lavingia’s AI analysis, staff members had to respond in 255 characters or fewer — just shy of the 280 character limit on Musk’s X social media platform.</p>
        
    
                    

<figure data-pp-id="34" data-pp-blocktype="image">

    


                    
    


    <img alt="A portion of an email to VA staffers reads: “I was just informed subject is on a list that they are considering directing us to terminate. I am advising you, if this is not acceptable, please go up your chain to submit an appropriate justification. On previous requests we have been informed the one sentence justification/statement should be no more than 255 characters in length. The statement should explain why the contract is essential for VHA to fulfill its statutory purposes including statute support, healthcare support, or any critical mission requirement, where applicable.”" width="3000" height="644" loading="lazy" js-autosizes="" src="https://img.assets-d.propublica.org/v5/images/20250604_doge-va-ai_255-characters_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=172&amp;q=75&amp;w=800&amp;s=49192739da6cdf7af94cd841344ccc19" srcset="https://img.assets-d.propublica.org/v5/images/20250604_doge-va-ai_255-characters_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=86&amp;q=75&amp;w=400&amp;s=3f4e3e2eb9d041b6b5e579b1ac3563fe 400w, https://img.assets-d.propublica.org/v5/images/20250604_doge-va-ai_255-characters_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=172&amp;q=75&amp;w=800&amp;s=49192739da6cdf7af94cd841344ccc19 800w, https://img.assets-d.propublica.org/v5/images/20250604_doge-va-ai_255-characters_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=258&amp;q=75&amp;w=1200&amp;s=9caafad40e8eef41e482dc69458dda95 1200w, https://img.assets-d.propublica.org/v5/images/20250604_doge-va-ai_255-characters_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=279&amp;q=75&amp;w=1300&amp;s=934a4aa534e9a89f2b5355118cc51e74 1300w, https://img.assets-d.propublica.org/v5/images/20250604_doge-va-ai_255-characters_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=311&amp;q=75&amp;w=1450&amp;s=3df53eb353b97c524e95f08232287b0c 1450w, https://img.assets-d.propublica.org/v5/images/20250604_doge-va-ai_255-characters_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=343&amp;q=75&amp;w=1600&amp;s=62d7b2434ad6fc9b1b8bdf8171f0f664 1600w, https://img.assets-d.propublica.org/v5/images/20250604_doge-va-ai_255-characters_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=429&amp;q=75&amp;w=2000&amp;s=78f1e620bc43260b9add32cf76c0594e 2000w">

            
    
<figcaption>
        <span>A VA email tells staffers that the justification of contracts targeted by DOGE must be limited to 255 characters.</span>
    
        <span>
        <span>Credit: </span>
        Obtained by ProPublica
    </span>
    
    
    </figcaption>

</figure>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="35.0">Once he started on DOGE’s contract analysis, Lavingia said he was confronted with technological limitations. At least some of the errors produced by his code can be traced to using older versions of OpenAI models available through the VA — models not capable of solving complex tasks, according to the experts consulted by ProPublica.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="36.0">Moreover, the tool’s underlying instructions were deeply flawed. Records show Lavingia programmed the AI system to make intricate judgments based on the first few pages of each contract — about the first 2,500 words — which contain only sparse summary information.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="37.0">“AI is absolutely the wrong tool for this,” said Waldo Jaquith, a former Obama appointee who oversaw IT contracting at the Treasury Department. “AI gives convincing looking answers that are frequently wrong. There needs to be humans whose job it is to do this work.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="38.0">Lavingia’s prompts did not include context about how the VA operates, what contracts are essential or which ones are required by federal law. This led AI to determine a core piece of the agency’s own contract procurement system was “munchable.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="39.0">At the core of Lavingia’s prompt is the direction to spare contracts involved in “direct patient care.”</p>
        
    
                    
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="41.0">Such an approach, experts said, doesn’t grapple with the reality that the work done by doctors and nurses to care for veterans in hospitals is only possible with significant support around them.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="42.0">Lavingia’s system also used AI to extract details like the contract number and “total contract value.” This led to avoidable errors, where AI returned the wrong dollar value when multiple were found in a contract. Experts said the correct information was readily available from public databases.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="43.0">Lavingia acknowledged that errors resulted from this approach but said those errors were later corrected by VA staff.</p>
        
    
                        
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="45.0">In late March, Lavingia published a version of the “munchable” script <a href="https://github.com/slavingia/va">on his GitHub account</a> to invite others to use and improve it, he told ProPublica. “It would have been cool if the entire federal government used this script and anyone in the public could see that this is how the VA is thinking about cutting contracts.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="46.0">According <a href="https://sahillavingia.com/doge">to a post on his blog</a>, this was done with the approval of Musk before he left DOGE. “When he asked the room about improving DOGE’s public perception, I asked if I could open-source the code I’d been writing,” Lavingia said. “He said yes — it aligned with DOGE’s goal of maximum transparency.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="47.0">That openness may have eventually led to Lavingia’s dismissal. Lavingia confirmed he was <a href="https://sahillavingia.com/doge">terminated from DOGE</a> after giving <a href="https://www.fastcompany.com/91330297/doge-sahil-lavignia-gumroad">an interview to Fast Company magazine</a> about his work with the department. A VA spokesperson declined to comment on Lavingia’s dismissal.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="48.0">VA officials have declined to say whether they will continue to use the “munchable” tool moving forward. But the administration may deploy AI to help the agency replace employees. Documents previously obtained by ProPublica show DOGE officials proposed in March consolidating the benefits claims department by relying more on AI.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="49.0">And the government’s contractors are paying attention. After Lavingia posted his code, he said he heard from people trying to understand how to keep the money flowing.</p>

<p data-pp-blocktype="copy" data-pp-id="49.1">“I got a couple DMs from VA contractors who had questions when they saw this code,” he said. “They were trying to make sure that their contracts don’t get cut. Or learn why they got cut.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="50.0">“At the end of the day, humans are the ones terminating the contracts, but it is helpful for them to see how DOGE or Trump or the agency heads are thinking about what contracts they are going to munch. Transparency is a good thing.”</p>
        
    
                    <div data-pp-location="bottom-note" data-pp-view="" data-pp-category="get involved">
                        <p>If you have any information about the misuse or abuse of AI within government agencies, Brandon Roberts is an investigative journalist on the news applications team and has a wealth of experience using and dissecting artificial intelligence. He can be reached on Signal @brandonrobertz.01 or by email <a href="https://www.propublica.org/cdn-cgi/l/email-protection#88eafae9e6ece7e6a6fae7eaedfafcfbc8f8fae7f8fdeae4e1ebe9a6e7faef"><span data-cfemail="6d0f1f0c03090203431f020f081f191e2d1d1f021d180f01040e0c43021f0a">[email&nbsp;protected]</span></a>.</p>
<p>If you have information about the VA that we should know about, contact reporter Vernal Coleman on Signal, vcoleman91.99, or via email, <a href="https://www.propublica.org/cdn-cgi/l/email-protection#1f697a6d717e73317c70737a727e715f6f6d706f6a7d73767c7e31706d78"><span data-cfemail="86f0e3f4e8e7eaa8e5e9eae3ebe7e8c6f6f4e9f6f3e4eaefe5e7a8e9f4e1">[email&nbsp;protected]</span></a>, and Eric Umansky on Signal, Ericumansky.04, or via email, <a href="https://www.propublica.org/cdn-cgi/l/email-protection#6b0e190208451e060a051800122b1b19041b1e090702080a4504190c"><span data-cfemail="21445348420f544c404f524a586151534e5154434d4842400f4e5346">[email&nbsp;protected]</span></a>.</p>

        </div><!-- end .article-body__bottom-notes -->
        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Jepsen: TigerBeetle 0.16.11 (209 pts)]]></title>
            <link>https://jepsen.io/analyses/tigerbeetle-0.16.11</link>
            <guid>44199592</guid>
            <pubDate>Fri, 06 Jun 2025 10:53:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jepsen.io/analyses/tigerbeetle-0.16.11">https://jepsen.io/analyses/tigerbeetle-0.16.11</a>, See on <a href="https://news.ycombinator.com/item?id=44199592">Hacker News</a></p>
<div id="readability-page-1" class="page"><p><a href="https://tigerbeetle.com/">TigerBeetle</a> is a distributed OLTP database oriented towards financial transactions. We tested TigerBeetle 0.16.11 through 0.16.30. We discovered seven client and server crashes, including a segfault on client close and several panics during server upgrades. Single-node failures could cause significantly elevated latencies for the duration of the fault, and requests were intentionally retried forever, which complicates error handling. We found only two safety issues: missing results for queries with multiple predicates, and a minor issue with a debugging API returning incorrect timestamps. TigerBeetle offered exceptional resilience to disk corruption, including damage to every replica’s files. However, it lacked a way to handle the total loss of a node’s data. As of version 0.16.30, TigerBeetle appeared to meet its promise of Strong Serializability. As of 0.16.45, TigerBeetle had addressed every issue we found, with the exception of indefinite retries. TigerBeetle has written a <a href="https://tigerbeetle.com/blog/2025-06-06-fuzzer-blind-spots-meet-jepsen/">companion blog post</a> to this work. This report was funded by TigerBeetle, Inc., and conducted in accordance with the <a href="https://jepsen.io/analyses/ethics">Jepsen ethics policy</a>.</p><article>
  <div>
<h2 data-number="1" id="background"> Background</h2>
<p>TigerBeetle is an <a href="https://tigerbeetle.com/blog/2024-07-23-rediscovering-transaction-processing-from-history-and-first-principles">Online Transactional Processing (OLTP)</a> database built for double-entry accounting with a strong emphasis on safety and speed. It builds on the <a href="https://pmg.csail.mit.edu/papers/vr-revisited.pdf">Viewstamped Replication (VR)</a> consensus protocol to offer <a href="https://jepsen.io/consistency/models/strong-serializable">Strong Serializable</a> consistency. Unlike general-purpose databases, TigerBeetle stores only accounts and transfers between them. This data model is well-suited for financial transactions, inventory, ticketing, or utility metering. To store other kinds of information, users typically pair TigerBeetle with other databases, linking them through user-defined identifiers.</p>
<p>TigerBeetle optimizes for <a href="https://web.archive.org/web/20250126010247/https://docs.tigerbeetle.com/about/oltp#business-transactions-dont-shard-well">high-contention and high-throughput workloads</a>, such as central bank switches or brokerages. A central bank exchange might have only a half-dozen to a few hundred account records—one for each partner bank—and process <a href="https://www.bcb.gov.br/en/statistics/graphicdetail/graficospix/PixTransactionsAmount">hundreds of millions</a> of <a href="https://www.npci.org.in/what-we-do/upi/product-statistics">transactions per day</a> between 647 banks. A large brokerage, after the trading day closes, might need to settle the entire day’s trades as quickly as possible.<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a> These trades also tend to be concentrated on a small number of popular stocks. Under high contention, per-object concurrency control mechanisms can be the limiting factor in throughput. Instead, TigerBeetle funnels all writes through <a href="https://web.archive.org/web/20250126010247/https://docs.tigerbeetle.com/about/performance#single-core-by-design">a single core on the primary VR node</a>. This limits throughput to whatever a single node can execute: TigerBeetle is firmly scale-up, not scale-out. To make that single node as fast as possible, TigerBeetle makes extensive use of batching, IO parallelization, a fixed schema, and hardware-friendly optimizations—such as fixed-size, cache-aligned data structures.</p>
<p>Refreshingly, TigerBeetle stresses fault tolerance in their marketing and documentation. They offer <a href="https://web.archive.org/web/20250126010247/https://docs.tigerbeetle.com/about/safety">explicit models</a> for memory, process, clock, storage, and network faults. ECC RAM is assumed to be correct. Processes may pause or crash. Clocks may jump forward and backward in time. Disks are assumed to not only fail completely, but to tear individual writes or corrupt data. Networks may delay, drop, duplicate, misdirect, and corrupt messages. To mitigate these faults, TigerBeetle combines Viewstamped Replication with techniques from <a href="https://www.usenix.org/system/files/conference/fast18/fast18-alagappan.pdf">Protocol-Aware Recovery</a>, uses extensive checksums stored separately from data blocks, and for critical data, writes and reads multiple copies. TigerBeetle also makes extensive use of runtime correctness assertions to identify and limit the damage from faults and bugs alike.</p>
<p>Unlike most distributed systems, TigerBeetle claims to keep running without data loss <a href="https://web.archive.org/web/20241213103525/docs.tigerbeetle.com/about/safety#durability">if even a single replica retains a copy</a> of a record:</p>
<blockquote>
<p>A record would need to get corrupted on all replicas in a cluster to get lost, and even in that case the system would safely halt.</p>
</blockquote>
<p>To test safety under faults, TigerBeetle employs <a href="https://notes.eatonphil.com/2024-08-20-deterministic-simulation-testing.html">deterministic simulation testing</a>: tests which perform reproducible, pseudo-random operations against the system and ensure that some property holds.<a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a> The <a href="https://web.archive.org/web/20250126010247/https://docs.tigerbeetle.com/about/vopr/">Viewstamped Operation Replicator (VOPR)</a> test simulates an entire TigerBeetle cluster, including clock, disk, and network interfaces. It simulates clock skew, corrupts reads and writes, loses and reorders network messages, and so on. There are other simulation tests which stress specific subsystems, as well as <a href="https://github.com/tigerbeetle/tigerbeetle/blob/092713075e797e633b6c306fb6d32b2523475f30/docs/internals/HACKING.md">a variety of more traditional integration and unit tests</a>.</p>
<p>TigerBeetle also offers a noteworthy approach to upgrades. Each TigerBeetle binary includes the code not just for that particular version, but several previous versions. For example, the 0.16.21 binary can run 0.16.17, 0.16.18, and so on through 0.16.21. To upgrade, one simply <a href="https://docs.tigerbeetle.com/operating/upgrading/">replaces the binary on disk</a>. TigerBeetle loads the new binary, but continues running with the current version. It then coordinates across the cluster to smoothly roll out each successive version, until all nodes are running the latest version available. This approach does not require operators to carefully sequence the upgrade process. Instead, upgrades are performed automatically, and coupled to the replicated state machine. This also allows TigerBeetle to ensure that an operation which commits on version <span><em>x</em></span> will never commit on any other version—guarding against state divergence.</p>
<h2 data-number="1.1" id="time"> Time</h2>
<p>TigerBeetle defines an <a href="https://docs.tigerbeetle.com/coding/time/">explicit model of time</a>. Viewstamped Replication forms a totally ordered sequence of state transitions, and its view and op numbers can be used as a totally ordered logical clock. Financial systems usually prefer wall clocks, so most TigerBeetle timestamps are in “physical time,” which, like <a href="https://cse.buffalo.edu/tech-reports/2014-04.pdf">Hybrid Logical Clocks</a>, approximate POSIX time with stronger ordering guarantees. Specifically, TigerBeetle leaders <a href="https://github.com/tigerbeetle/tigerbeetle/blob/5aa44f58870a510155cab8cb0ff56e629724bc74/src/vsr/clock.zig#L23">collect POSIX timestamps</a> from all replicas<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a> and try to find a time which falls within a reasonable margin of error across a quorum of nodes.<a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a> Those timestamps are incorporated into the VR-replicated state machine, and constrained to be strictly monotonic. When no quorum of clocks falls within a <a href="https://github.com/tigerbeetle/tigerbeetle/blob/5aa44f58870a510155cab8cb0ff56e629724bc74/src/config.zig#L122">twenty-second window</a> for longer than sixty seconds, the cluster refuses requests until clocks come back in sync.</p>
<p>As of October 2024, TigerBeetle’s documentation <a href="http://web.archive.org/web/20240823231836/https://docs.tigerbeetle.com/reference/account/#timestamp">described TigerBeetle timestamps</a> as “nanoseconds since UNIX epoch”. This is <a href="https://aphyr.com/posts/378-seconds-since-the-epoch">not quite true</a>: POSIX time is presently twenty-seven seconds less than the actual number of seconds since the epoch. During leap seconds or other negative time adjustments, TigerBeetle’s clock slows to a crawl until values from <code>CLOCK_REALTIME</code> catch up.</p>
<h2 data-number="1.2" id="data-model"> Data Model</h2>
<p><a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/coding/#data-model-overview">TigerBeetle’s data model</a> is specifically intended for <a href="https://en.wikipedia.org/wiki/Double-entry_bookkeeping">double-entry bookkeeping</a>. It has no way to represent arbitrary rows, objects, graphs, blobs, and so on.<a href="#fn5" id="fnref5" role="doc-noteref"><sup>5</sup></a> Instead TigerBeetle stores two types of data: <em>accounts</em>, and <em>transfers</em> between them. All fields are fixed-size,<a href="#fn6" id="fnref6" role="doc-noteref"><sup>6</sup></a> and numbers are generally unsigned integers. All values are, with limited exceptions, immutable.</p>
<p>An <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/account/">account</a> represents an entity which sends and receives something. For example, a “Gross Revenues” account might accrue dollars, “Meadow Lake Wind Farm” might generate kilowatt-hours of electricity, and “Beyoncé” would obviously hold an ever-growing number of Grammy awards. Accounts are uniquely identified by a user-defined 128-bit <code>id</code>, a <code>ledger</code> which determines which accounts can interact with each other, a bitfield of <code>flags</code> controlling <a href="https://docs.tigerbeetle.com/reference/account/#flags">various behaviors</a>, a creation <code>timestamp</code>, a user-defined <code>code</code>, and three custom fields of varying sizes: <code>user_data_32</code>, <code>user_data_64</code>, and <code>user_data_128</code>. There are also four derived fields which represent the current sum of transfers into (<em>credits</em>) and out of (<em>debits</em>) the account: <code>debits_pending</code>, <code>debits_posted</code>, <code>credits_pending</code>, and <code>credits_posted</code>.</p>
<p>A <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/transfer/">transfer</a> is an immutable record which represents an integer quantity moving from one account to another. Like accounts, transfers have a unique, user-specified, 128-bit <code>id</code>, a <code>code</code>, a <code>ledger</code>, a bitfield of <a href="https://docs.tigerbeetle.com/reference/transfer/#flags"><code>flags</code></a>, and three custom fields: <code>user_data_32</code>, <code>user_data_64</code>, and <code>user_data_128</code>. Transfers also include the <code>debit_account_id</code> and <code>credit_account_id</code> of the two accounts involved, and the integer <code>amount</code> transferred between them.</p>
<p>A single-phase transfer takes effect, or <em>posts</em>, immediately. A transfer can also be executed in <a href="https://docs.tigerbeetle.com/coding/two-phase-transfers/">two phases</a>, represented by two transfer records. The first phase, <em>pending</em>, reserves capacity in the debit and credit account for the given amount. The second phase posts the pending transfer, transferring at most the pending amount. Pending transfers can be explicitly <em>voided</em>, which cancels them, or automatically <em>expire</em>, which is controlled by a <code>timeout</code> field. Posting and voiding transfers use a flag and a <code>pending_id</code> field to indicate which pending transfer they resolve. Pending transfers resolve <a href="https://web.archive.org/web/20240828091507/https://docs.tigerbeetle.com/reference/transfer">at most once</a>.</p>
<p>A special kind of transfer can <a href="https://web.archive.org/web/20250128232234/https://docs.tigerbeetle.com/coding/recipes/close-account/"><em>close</em></a> an account, preventing it from participating in later transfers. Closing transfers are always pending. Account closures can be “un-done” by voiding the closing transfer.</p>
<p>Accounts are immutable with five exceptions: a <code>closed</code> flag, which is derived from closing and re-opening transfers, and four balance fields, which are derived from the sum of pending and posted transfers. Transfers are always immutable. One alters or un-does a transfer by creating a new, compensating transfer.</p>
<h2 data-number="1.3" id="operations"> Operations</h2>
<p>TigerBeetle clients make <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/"><em>requests</em></a> to update or query database state. Each request represents a single kind of logical operation, like <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/create_accounts">creating accounts</a> or <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/query_transfers">querying transfers</a>. Requests and their corresponding responses usually involve a batch of <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/#batching-events">up to</a> 8190 <em>events</em>, all of the same type. For example, a <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/create_transfers">create-transfers</a> request includes a batch of transfers to create, and logically<a href="#fn7" id="fnref7" role="doc-noteref"><sup>7</sup></a> returns a batch of results, one per transfer. Read operations generally take a list of IDs, or a query predicate, and return a batch of matching records.</p>
<p>From a database perspective, each TigerBeetle request is a single transaction: an ordered group of micro-operations which execute atomically. Events within a request <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/#guarantees">are executed in order</a>. Each event observes a unique, strictly increasing timestamp.<a href="#fn8" id="fnref8" role="doc-noteref"><sup>8</sup></a> There are no interactive transactions, mixed read-write transactions, or indeed any kind of multi-request transactions.</p>
<p>TigerBeetle’s <a href="https://tigerbeetle.com/">home page</a> promises <a href="https://jepsen.io/consistency/models/strong-serializable">Strong Serializability</a>, and the documentation is consistent with this promise. Requests execute <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/#guarantees">at most once</a>, and events within a request “do not interleave with events from other requests.” TigerBeetle also promises several <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/sessions/#guarantees">session safety properties</a>: a session “reads its own writes” and “observes writes in the order that they occur on the cluster.” These are guaranteed by <a href="https://cs.uwaterloo.ca/~kmsalem/pubs/DaudjeeICDE04.pdf">Strong Session Serializability</a>, which in turn is implied by Strong Serializability.</p>
<p>There are two kinds of write requests. The <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/create_accounts"><code>create_accounts</code></a> and <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/create_transfers"><code>create_transfers</code></a> requests add a series of accounts or transfers to the database. There are also six read requests. Users look up specific accounts or transfers by ID using <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/lookup_accounts"><code>lookup_accounts</code></a> and <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/lookup_transfers"><code>lookup_transfers</code></a>. To query accounts or transfers matching a predicate, one uses <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/query_accounts"><code>query_accounts</code></a>, <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/query_transfers"><code>query_transfers</code></a>, and <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/get_account_transfers"><code>get_account_transfers</code></a>. Finally, <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/get_account_balances"><code>get_account_balances</code></a> reads historical balance information.</p>
<p>Requests are atomic in the sense that either all or none of a request’s events execute. However, specific events in a committed request can <em>logically</em> fail, returning error codes. For example, a <code>create_transfers</code> request might try to create two transfers, the first of which fails due to a balance constraint, and the second of which succeeds. This request can still commit, even though only one of its two transfers was added to the database.</p>
<p>To make one event contingent on another, TigerBeetle offers a sort of logical sub-transaction within a request, called a <em>chain</em>. Each event in a chain succeeds if and only if all others succeed. This allows users to express <a href="https://docs.tigerbeetle.com/coding/recipes/multi-debit-credit-transfers">complex, multi-step transfers</a> that succeed or fail atomically.</p>
<h2 data-number="2" id="test-design"> Test Design</h2>
<p>We built a <a href="https://github.com/jepsen-io/tigerbeetle/">test suite</a> for TigerBeetle using the <a href="https://github.com/jepsen-io/jepsen">Jepsen testing library</a>, which combines property-based testing with fault injection. We tested TigerBeetle versions 0.16.11 through 0.16.30, including several development builds. Our tests ran on clusters of three to six<a href="#fn9" id="fnref9" role="doc-noteref"><sup>9</sup></a> Debian nodes, both in LXC containers and on EC2 VMs.</p>
<p>TigerBeetle offers only a “smart” client which connects to every node in the cluster. These clients can mask concurrency errors by routing all requests to a single server.<a href="#fn10" id="fnref10" role="doc-noteref"><sup>10</sup></a> In addition to testing this smart-client behavior, we also ran tests with each client restricted to a single node, by passing invalid addresses for the other nodes. Since TigerBeetle followers do not proxy <code>register</code> requests to the leader, most clients spend their time attempting futile requests against inexorable followers. This is fine for safety testing, so long as they time out quickly enough to keep up with leader elections.</p>
<p>TigerBeetle’s domain-specific data model poses a challenge for validation. Jepsen has <a href="https://github.com/jepsen-io/elle">well-established tricks</a> for checking Strict Serializability of lists, sets, and registers, but TigerBeetle has no direct analogue to these structures.</p>
<p>As in our 2022 work on <a href="https://jepsen.io/analyses/radix-dlt-1.0-beta.35.1">Radix DLT</a>, we considered interpreting each account as a list of transfers. Creating a transfer would be interpreted as a pair of appends to the debit and credit accounts. A balance read could, with the help of a constraint solver, often be mapped to a read of a specific set of transfers. However, this leaves account creation and most queries untested. It also makes it difficult to validate the rich semantics of TigerBeetle transfers. For example, TigerBeetle supports balancing transfers, which adjusts the amount of a transfer to ensure the debit (and/or credit) account maintains certain invariants, like a positive or negative balance.</p>
<p>Instead, we decided to take advantage of TigerBeetle’s explicit total order of transactions. In broad strokes, <a href="https://github.com/jepsen-io/tigerbeetle/blob/815e32d20185db00b4b72bbfd8c1e43e2dff5e8b/src/jepsen/tigerbeetle/checker.clj">our checker</a> splits the problem into two interlocking parts. First, we check that the apparent timestamps of operations are Strong Serializable. Second, we check that the <em>semantics</em> of those operations, when executed in timestamp order, make sense.</p>
<h2 data-number="2.1" id="timestamp-order"> Timestamp Order</h2>
<p>Verifying timestamp order was relatively straightforward. TigerBeetle added a <a href="https://javadoc.io/doc/com.tigerbeetle/tigerbeetle-java/latest/com.tigerbeetle/com/tigerbeetle/Batch.Header.html">new client API</a> which allowed us to read the timestamp assigned to every successful request. For operations which failed or timed out, we inferred their timestamps from the timestamp assigned to any of their effects. For instance, if the creation of account 3 timed out, but we later read account 3 with timestamp 72, we assumed that write executed at timestamp 72. TigerBeetle’s promise that timestamps are strictly ordered both within and between requests means that this inference should yield an order compatible with the request timestamps. We ignored any failed reads, whether definite or indefinite—this is safe as reads have no semantic side effects.</p>
<p>Timestamp inference required that we eventually observe the effect of every attempted write. We divided the test into two phases: a <em>main phase</em> involving writes and reads, and a <em>final read phase</em> where we tried to read any unseen writes until TigerBeetle definitively responded “yes, this write exists”, or “no, it does not (yet) exist.” Our goal was to infer exactly which operations executed during the main phase, and the timestamps of those operations.<a href="#fn11" id="fnref11" role="doc-noteref"><sup>11</sup></a></p>
<p>If a write was observed, and its inferred timestamp fell before the timestamp of the last successfully acknowledged write,<a href="#fn12" id="fnref12" role="doc-noteref"><sup>12</sup></a> we <a href="https://github.com/jepsen-io/tigerbeetle/blob/815e32d20185db00b4b72bbfd8c1e43e2dff5e8b/src/jepsen/tigerbeetle/checker.clj#L269-L310">inferred that it executed</a> during the main phase. If a write was <em>not</em> observed, we assumed that it did not execute during the main phase. There are two possible scenarios:</p>
<ul>
<li><p>TigerBeetle is Strong Serializable. If the write had executed during the main phase, Strong Serializability would have ensured its visibility during the final read phase. Our inference is correct.</p></li>
<li><p>TigerBeetle is not Strong Serializable. If the write did not execute during the main phase, our inference is correct. If it <em>did</em> execute during the main phase, our inference is incorrect. We might encounter false positives or false negatives—but in either case, TigerBeetle has failed to maintain a promised invariant.</p></li>
</ul>
<p>If TigerBeetle were Strong Serializable, our checker would not falsely report an error. If TigerBeetle were to (e.g.) exhibit a <a href="https://jepsen.io/consistency/phenomena/stale-read">stale read</a> or another violation of Strong Serializability, we might detect it indirectly. It could, for example, manifest as a model-checker error on a different operation much earlier in the history. This non-locality is not ideal, but we found it an acceptable tradeoff in exchange for excellent coverage.</p>
<p>Having inferred a set of operations executed during the main phase, and timestamps for each, we used <a href="https://github.com/jepsen-io/elle">Elle</a> to construct a graph over operations. We linked operations by real-time edges when operation A ended before operation B began.<a href="#fn13" id="fnref13" role="doc-noteref"><sup>13</sup></a> We also linked operations in ascending timestamp order. A violation of Strong Serializability (as far as timestamps were concerned) would manifest as a cycle in this graph. Elle checks for cycles in roughly linear time, and constructs compact exemplars of consistency violations.</p>
<h2 data-number="2.2" id="model-checking"> Model Checking</h2>
<p>To verify that the semantics of TigerBeetle’s requests and responses were correct, we built a detailed, <a href="https://github.com/jepsen-io/tigerbeetle/blob/815e32d20185db00b4b72bbfd8c1e43e2dff5e8b/src/jepsen/tigerbeetle/model.clj">single-threaded model</a> of the TigerBeetle state machine based on the documentation. This model is essentially a datatype with an initial state <span><em>i</em><em>n</em><em>i</em><em>t</em></span> and a transition function <span><em>s</em><em>t</em><em>e</em><em>p</em>(<em>s</em><em>t</em><em>a</em><em>t</em><em>e</em>,&nbsp;<em>i</em><em>n</em><em>v</em><em>o</em><em>k</em><em>e</em>,&nbsp;<em>c</em><em>o</em><em>m</em><em>p</em><em>l</em><em>e</em><em>t</em><em>e</em>) → <em>s</em><em>t</em><em>a</em><em>t</em><em>e</em>′</span>, which takes a state, the invocation of a request, and the completion of that request, and returns a new state. Illegal transitions (for instance, a read whose completion value does not agree with the state) returned a special <em>invalid</em> state. Given the inferred list of timestamp-sorted operations from the main phase, we stepped through each operation in order. Any invalid state was reported as an error.</p>
<p>We <a href="https://github.com/jepsen-io/tigerbeetle/blob/815e32d20185db00b4b72bbfd8c1e43e2dff5e8b/src/jepsen/tigerbeetle/model.clj">modeled the state</a> as an immutable data structure including the current timestamps,<a href="#fn14" id="fnref14" role="doc-noteref"><sup>14</sup></a> maps of IDs to accounts and transfers, transient errors<a href="#fn15" id="fnref15" role="doc-noteref"><sup>15</sup></a>, a set of indices to support efficient querying, and a few internal statistics. To model the flow of clocks, we provided each state with a pre-computed map of IDs to timestamps, derived from the reads performed during the test. Whenever one of those IDs was created, we <a href="https://github.com/jepsen-io/tigerbeetle/blob/815e32d20185db00b4b72bbfd8c1e43e2dff5e8b/src/jepsen/tigerbeetle/model.clj">advanced the clock</a> to that timestamp.</p>
<p>The state machine is surprisingly complex, involving over 1,600 lines of Clojure and an <a href="https://github.com/jepsen-io/tigerbeetle/blob/815e32d20185db00b4b72bbfd8c1e43e2dff5e8b/src/jepsen/tigerbeetle/model.clj">extensive test suite</a>. A <a href="https://github.com/jepsen-io/tigerbeetle/blob/815e32d20185db00b4b72bbfd8c1e43e2dff5e8b/src/jepsen/tigerbeetle/model.clj">broad array of error conditions</a> had to be handled, including duplicate IDs, non-monotonic timestamps, balance constraints, incompatible flags, and more. Linked chains of events required <a href="https://github.com/jepsen-io/tigerbeetle/blob/815e32d20185db00b4b72bbfd8c1e43e2dff5e8b/src/jepsen/tigerbeetle/model.clj#L392-L402">speculative execution and rollback</a> of the state—made simpler by our pure, functional approach. We made extensive use of Zach Tellman’s <a href="https://github.com/lacuna/bifurcan">Bifurcan</a>, a thoroughly tested library of high performance persistent data structures.</p>
<p>Modeling the full state machine takes time, but allows extraordinarily detailed verification of correctness. To make checking computationally tractable, typical Jepsen tests use only a handful of carefully selected data types and operations on them. The implicit assumption is that if the database’s concurrency control protocol handles that selected example correctly, it is likely correct for other workloads as well. Modeling the state machine in detail allowed us to check almost<a href="#fn16" id="fnref16" role="doc-noteref"><sup>16</sup></a> every<a href="#fn17" id="fnref17" role="doc-noteref"><sup>17</sup></a> operation TigerBeetle can perform. We verified that observed results of queries matched exactly, down to specific error codes. As discussed in this report, this approach found bugs we would have otherwise missed.</p>
<h2 data-number="2.3" id="generating-operations"> Generating Operations</h2>
<p>The downside of testing so much of TigerBeetle’s state machine is that we must then generate requests which <em>exercise</em> it. Generating syntactically valid requests is easy, but generating requests which often succeed, or queries which return non-empty results, is surprisingly hard.</p>
<p>Our <a href="https://github.com/jepsen-io/tigerbeetle/blob/815e32d20185db00b4b72bbfd8c1e43e2dff5e8b/src/jepsen/tigerbeetle/workload/generator.clj">generator</a> maintained extensive in-memory state throughout each test, including probabilistic models of which account and transfer IDs were likely to exist, which transfers were likely pending, what timestamps were likely extant, and what each worker process was currently doing. The generator updated this state with each operation’s invocation and completion.</p>
<p>We selected Zipfian distributed IDs, ledgers, codes, and so on, ensuring a mix of very hot and very cool objects. We used a broad set of parameters to guide stochastic choices of request types, account and transfer IDs, chain lengths, flags, queries, and probabilistic state updates. These parameters were carefully tuned across a variety of concurrencies, request rates, hardware environments, and fault conditions to find a reasonable balance of successes and failures, non-empty query results, attempted invariant violations, and so on.</p>
<h2 data-number="2.4" id="fault-injection"> Fault Injection</h2>
<p>Jepsen provides several kinds of faults “out of the box.” We <a href="https://github.com/jepsen-io/tigerbeetle/blob/815e32d20185db00b4b72bbfd8c1e43e2dff5e8b/src/jepsen/tigerbeetle/nemesis.clj">stressed TigerBeetle</a> with process crashes (<code>SIGKILL</code>), pauses (<code>SIGSTOP</code>), a variety of transitive and non-transitive network partitions, and clock changes ranging from milliseconds to hundreds of seconds, as well as strobing the clock rapidly back and forth. We also <a href="https://github.com/jepsen-io/tigerbeetle/blob/815e32d20185db00b4b72bbfd8c1e43e2dff5e8b/src/jepsen/tigerbeetle/nemesis.clj#L395-L421">upgraded nodes through several versions</a> during tests.</p>
<p>We also introduced a variety of storage faults via a <a href="https://github.com/jepsen-io/jepsen/blob/8de1c9c9c1f7ac08fdf5c1eae3f709aa19cc3f9b/jepsen/resources/corrupt-file.c">new file corruption nemesis</a>. We flipped random bits to simulate (e.g.) cosmic ray interference. We replaced chunks of the file with other chunks, in an attempt to simulate <a href="https://pages.cs.wisc.edu/~remzi/OSTEP/file-integrity.pdf">misdirected writes</a>. We also took snapshots of chunks of the file, then restored them later, to simulate lost writes.</p>
<p>Each TigerBeetle node has a single data file, which is divided into <em>zones</em> at predictable offsets. Each zone stores a single kind of fixed-sized record. We scoped our faults to specific zones—corrupting, for example, only the write-ahead-log (WAL)’s headers, or restoring a snapshot of just one of the four redundant copies in the superblock zone. In many tests we corrupted multiple zones, or the entire file.</p>

<p>We also targeted a variety of nodes for file corruption. In one scenario, we corrupted data throughout (e.g.) the superblock, but only on a minority of nodes. In a second scenario, we corrupted every node’s data, but selected different chunks of the file for each node. For example, one node in a three-node cluster might corrupt the first, fourth, seventh, and tenth chunks of the grid; another would corrupt the second, fifth, eighth, and so on. We called this a <em>helical</em> disk fault. If you imagine arranging the cluster’s nodes into a ring, and drawing their file offsets along the ring’s symmetry axis, the corrupted chunks “spin” around the ring, forming a helix. Because TigerBeetle’s file layout is (generally speaking) bit-for-bit identical between up-to-date replicas, this avoids corrupting any single record in the database beyond repair.<a href="#fn18" id="fnref18" role="doc-noteref"><sup>18</sup></a></p>
<h2 data-number="3" id="results"> Results</h2>
<h2 data-number="3.1" id="requests-never-time-out-206"> Requests Never Time Out (#206)</h2>
<p>Our first tests of TigerBeetle routinely stalled forever. For example, in <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/first-request-stalls.zip">this test run</a> the very first request never returned, which prevented the test from ever completing. This turned out to be a consequence of an unusual design decision: TigerBeetle <a href="https://web.archive.org/web/20241222161312/https://docs.tigerbeetle.com/reference/requests/#guarantees">actually guaranteed</a> that requests would never time out:</p>
<blockquote>
<p>Requests do not time out. Clients will continuously retry requests until they receive a reply from the cluster. This is because in the case of a network partition, a lack of response from the cluster could either indicate that the request was dropped before it was processed or that the reply was dropped after the request was processed.</p>
</blockquote>
<p>The <a href="https://web.archive.org/web/20240712120653/https://docs.tigerbeetle.com/reference/sessions/#retries">session documentation</a> reaffirmed this stance: a TigerBeetle client “will never time out” and “does not surface network errors”. This is particularly surprising since most systems do expose network errors, whether Strong Serializable or otherwise.</p>
<blockquote>
<p>With TigerBeetle’s strict consistency model, surfacing these errors at the client/application level would be misleading. An error would imply that a request did not execute, when that is not known[.]</p>
</blockquote>
<p>There are, broadly speaking, two classes of errors in distributed systems. A <em>definite</em> error, like a constraint violation, signifies that an operation has not and will never happen. An <em>indefinite</em> error, like a timeout, signifies that the operation may have already happened, might happen later, or might never happen.<a href="#fn19" id="fnref19" role="doc-noteref"><sup>19</sup></a> Consistent with the documentation, TigerBeetle tries to conceal both kinds of error through an unbounded internal retry loop.</p>
<p>However, TigerBeetle clients actually <em>can</em> produce timeout errors. The Java client’s asynchronous methods, like <a href="https://javadoc.io/doc/com.tigerbeetle/tigerbeetle-java/latest/com.tigerbeetle/com/tigerbeetle/Client.html"><code>createTransfersAsync</code></a>, return <a href="https://javadoc.io/doc/com.tigerbeetle/tigerbeetle-java/latest/com.tigerbeetle/com/tigerbeetle/Client.html"><code>CompletableFutures</code></a>. <code>CompletableFuture</code> usually represents operations, like network requests, which are subject to indefinite failures. Indeed, timeouts are integral to the datatype: one awaits a future using <code>.get(timeout, timeUnit)</code>, or wraps it in <code>.orTimeout(seconds, timeUnit)</code> to throw a timeout automatically. Likewise, the .Net client’s <a href="https://github.com/tigerbeetle/tigerbeetle/blob/274de35df357b790f0ccddded3531bb2592fbe2f/src/clients/dotnet/TigerBeetle/Client.cs#L65"><code>createTransferAsync</code></a> and friends return <a href="https://learn.microsoft.com/en-us/dotnet/api/system.threading.tasks.task.wait?view=net-9.0#system-threading-tasks-task-wait(system-int32)"><code>Task</code></a> objects which offer timeout-driven <code>Wait()</code> methods.</p>
<p>Even if users constrain themselves to synchronous calls, applications rarely have unbounded time to run. It seems likely that applications will wrap TigerBeetle calls in their own timeouts. If they do not, the application may eventually terminate, which is a worse kind of indefinite failure. Even when applications can wait, their clients (or the human beings waiting for an operation), may give up at any time. The challenge of indefinite errors is intrinsic to asynchronous networks and cannot be eliminated.</p>
<p>Because TigerBeetle clients handle all failures through a silent internal retry mechanism, they unnecessarily convert definite errors into indefinite ones. For example, imagine a common fault: a TigerBeetle server has crashed. An application makes a <code>createTransfer</code> request. Its client attempts to open a TCP connection to submit the request, and receives <code>ECONNREFUSED</code>. The client knows internally that this request cannot possibly have executed: it has a definite failure. However, it refuses to inform the caller, and instead retries again and again. The caller’s only sign of an error is that the client appears to have stalled. When the caller times out or eventually shuts down, that definite failure becomes indefinite. Instead of making indefinite errors impossible, TigerBeetle’s client design <em>proliferates</em> them.</p>
<p>This is an ongoing discussion within TigerBeetle (#<a href="https://github.com/tigerbeetle/tigerbeetle/issues/206">206</a>). Jepsen recommended that TigerBeetle develop a first-class representation for definite and indefinite errors, and return those errors to callers when problems occur. It is perfectly fine to keep automatic retries—perhaps even unbounded ones—but this behavior should be configurable. TigerBeetle clients should take options controlling the maximum time allowed for opening a connection, and for awaiting responses from a submitted request. Users can request unbounded timeouts if desired.</p>
<h2 data-number="3.2" id="client-uninitialized-memory-access-2435"> Client Uninitialized Memory Access (#2435)</h2>
<p>Because synchronous client operations never timed out, our early tests generally failed to terminate. To avoid this problem, we tried wrapping calls to TigerBeetle clients in two kinds of timeouts. In the first, we spawned a new thread to make a synchronous call, and interrupted that thread if it did not complete within a few seconds.<a href="#fn20" id="fnref20" role="doc-noteref"><sup>20</sup></a></p>
<div id="cb1"><pre><code><span id="cb1-1">(<span>let</span> [worker (<span>future</span> (.createAccounts</span>
<span id="cb1-2">                        client accounts))</span>
<span id="cb1-3">      ret    (<span>deref</span> worker <span>5000</span> <span>::timeout</span>)]</span>
<span id="cb1-4">  (<span>if</span> (<span>=</span> ret <span>::timeout</span>)</span>
<span id="cb1-5">    (<span>do</span> (<span>future-cancel</span> worker)</span>
<span id="cb1-6">        (<span>throw</span> {<span>:type</span> <span>:timeout</span>}))</span>
<span id="cb1-7">    retval))</span></code></pre></div>
<p>In 0.16.11, this immediately <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/interrupt-segfault.log">segfaulted the entire JVM</a>. TigerBeetle’s Java client is implemented via a <a href="https://en.wikipedia.org/wiki/Java_Native_Interface">JNI</a> binding to a client library written in <a href="https://ziglang.org/">Zig</a>, and a Zig panic crashes the JVM as well. Concerned that our use of multiple threads or a thread interrupt might be at fault, we tried an alternate approach, using the client’s asynchronous methods which returned a <code>CompletableFuture</code>. If that future did not produce a result within a few seconds, we closed the client.</p>
<div id="cb2"><pre><code><span id="cb2-1">(<span>let</span> [<span>future</span> (.createAccountsAsync</span>
<span id="cb2-2">                c (account-batch accounts))</span>
<span id="cb2-3">       ret   (<span>deref</span> <span>future</span> <span>5000</span> <span>::timeout</span>)]</span>
<span id="cb2-4">    (<span>if</span> (<span>=</span> ret <span>::timeout</span>)</span>
<span id="cb2-5">      (<span>do</span> (close! client)</span>
<span id="cb2-6">          (throw+ {<span>:type</span> <span>:timeout</span>}))</span>
<span id="cb2-7">      ret))</span></code></pre></div>
<p>—this too <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/async-close-segfault.log">segfaulted the JVM</a>.</p>
<p>In a closely related issue, calling <code>client.close()</code> in 0.16.11, on a freshly opened client, <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/close-unreachable.log">caused the JVM to panic</a> with a <code>reached unreachable code</code> error in <code>tb_client.zig:122</code>.</p>
<p>TigerBeetle traced these problems to uninitialized fields in the client’s request data structure (<a href="https://github.com/tigerbeetle/tigerbeetle/pull/2435">#2435</a>). These fields were normally filled in during request submission. However, if the client was closed between request creation and submission, it would dereference uninitialized pointers from that structure, jumping to random offsets in memory. This issue was fixed in 0.16.12.</p>
<h2 data-number="3.3" id="client-crash-on-eviction-2484"> Client Crash on Eviction (#2484)</h2>
<p>The official TigerBeetle clients crashed the entire process when a server informed them that their session had been evicted. TigerBeetle allows only <a href="https://web.archive.org/web/20241222161312/https://docs.tigerbeetle.com/reference/sessions/#eviction">64 concurrent sessions</a> by default, making it relatively easy to hit this limit.<a href="#fn21" id="fnref21" role="doc-noteref"><sup>21</sup></a> TigerBeetle also evicts clients which use a newer client version than the server.</p>
<p>This behavior made it impossible for clients to cleanly recover from eviction, or to back off under load. TigerBeetle changed this behavior in <a href="https://github.com/tigerbeetle/tigerbeetle/pull/2484">#2484</a>. As of 0.16.13, clients return errors to their callers on eviction, rather than crashing the entire process.</p>
<h2 data-number="3.4" id="elevated-latencies-on-single-node-faults-2739"> Elevated Latencies on Single-Node Faults (#2739)</h2>
<p>When a single node failed, we often saw client latencies jump by three to five orders of magnitude. In <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/0241210T170510.624-single-node-latency-jump.zip">this test of a five-node cluster, with clients constrained to a single node each</a>, killing a single node caused minimum latencies to rise from less than one millisecond to ten seconds. There were fluctuations down to one second, but in general elevated latencies lasted for the full duration of a fault.</p>
<p><img src="https://jepsen.io/analyses/tigerbeetle-0.16.11/single-node-latency-jump-2.png" alt="A time-series plot of latencies. A red bar shows the period where n3 was dead. Latencies jump dramatically."><br>
</p>
<p>Under higher load, the situation could become considerably worse. Consider <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/20250214T110802.529-single-node-latency-jump-3.zip">this test run</a> with a three-node cluster, where each client was allowed to connect to all three nodes. A few seconds into the test, we killed node <code>n3</code>. This drove latencies on every client from between 1–50 milliseconds to roughly a hundred seconds per request. This situation persisted for almost a thousand seconds, until we restarted <code>n3</code>.</p>
<p>In the original <a href="https://pmg.csail.mit.edu/papers/vr.pdf">Viewstamped Replication</a> and <a href="https://www.pmg.csail.mit.edu/papers/vr-revisited.pdf">Viewstamped Replication Revisited</a>, a primary sends a <em>prepare</em> message to every secondary when it wants to perform an operation. The secondaries send acknowledgements back to the primary, which can commit once <span><em>f</em></span> replicas have acknowledged. The failure of any single node (shown in red) causes a single acknowledgement to be lost, but does not affect any of the other nodes or their acknowledgements. The system as a whole is relatively insensitive to single-node failures.</p>

<p>TigerBeetle approaches prepares differently. Nodes are arranged in a ring, and the primary sends a single prepare message to the next secondary in the ring. That secondary sends a prepare to the following secondary, and so on, until all nodes have received the message. Acknowledgements are sent directly to the primary. This approach reduces the bandwidth requirements for any single node, but creates a weakness: if the primary must receive <span><em>f</em></span> acknowledgements to commit, the failure of any one of the next <span><em>f</em></span> replicas in the ring will prevent commit entirely. The effect of a single-node failure cascades through the rest of the ring. We opened issue <a href="https://github.com/tigerbeetle/tigerbeetle/issues/2739">#2739</a> to track this issue.</p>
<p>Version 0.16.30 includes a new tactic to <a href="https://github.com/tigerbeetle/tigerbeetle/pull/2761">mitigate this problem</a>. Sending every other prepare message <em>backwards</em> around the ring allows half of prepares to bypass the faulty node. Since prepares must be processed in order, the replicas which receive these counter-ringward prepares must repair the ringward prepare messages they missed. Repairing takes time, but the overall effect is significant. Rather than hundred-second latencies during a single-node fault, 0.1.16 delivered 1–30 second latencies in our tests.</p>
<p>After our collaboration, TigerBeetle continued work on single-node fault tolerance, adding a new series of deterministic performance tests to their simulation framework. As of version 0.16.43, TigerBeetle includes <a href="https://github.com/tigerbeetle/tigerbeetle/issues/2739#issuecomment-2919767029">a host of performance improvements</a>. Nodes replicate in both directions around the ring, which reduces latencies and the impact of single failures. The ring topology is now dynamic: and the cluster continually adjusts the order of nodes to minimize latency based on network conditions and faults.</p>

<p>To support Jepsen’s tests, TigerBeetle added a <a href="https://javadoc.io/doc/com.tigerbeetle/tigerbeetle-java/0.16.13/com.tigerbeetle/com/tigerbeetle/Batch.Header.html">new, experimental API</a> in 0.16.13 for obtaining the execution timestamp for each request from a header included in response messages. In the Java client, this API routinely returned <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/20250106T115221.401-duplicate-timestamps.zip">incorrect and duplicate timestamps</a>. For example, both of these <code>create-transfers</code> operations returned identical timestamps.</p>
<div id="cb3"><pre><code><span id="cb3-1">{<span>:index</span>     <span>5827</span>,</span>
<span id="cb3-2"> <span>:type</span>      <span>:ok</span>,</span>
<span id="cb3-3"> <span>:process</span>   <span>1</span>,</span>
<span id="cb3-4"> <span>:f</span>         <span>:create-transfers</span>,</span>
<span id="cb3-5"> <span>:value</span>     [<span>:ok</span>],</span>
<span id="cb3-6"> <span>:timestamp</span> <span>1736185975365035812</span>}</span>
<span id="cb3-7">{<span>:index</span>     <span>5829</span>,</span>
<span id="cb3-8"> <span>:type</span>      <span>:ok</span>,</span>
<span id="cb3-9"> <span>:process</span>   <span>11</span>,</span>
<span id="cb3-10"> <span>:f</span>         <span>:create-transfers</span>,</span>
<span id="cb3-11"> <span>:value</span>     [<span>:ok</span> <span>:ok</span> <span>:ok</span> <span>:ok</span>],</span>
<span id="cb3-12"> <span>:timestamp</span> <span>1736185975365035812</span>}</span></code></pre></div>
<p>TigerBeetle traced this bug to a <a href="https://github.com/tigerbeetle/tigerbeetle/pull/2495">mutable singleton response object</a> in the Java client: <code>Batch.EMPTY</code>. Every empty response used the same instance of this object, updating its header to reflect that response’s timestamp. As responses overwrote each other’s headers, callers observed incorrect timestamps. Since TigerBeetle represents entirely-successful responses as empty batches, this happened quite often.</p>
<p>This bug (<a href="https://github.com/tigerbeetle/tigerbeetle/pull/2495">#2495</a>) was fixed in 0.16.14, just seven days after 0.6.13’s release. It did not affect the correctness of the actual data involved, only request timestamps from the Java client’s header API. We believe the impact to users was likely nil.</p>
<h2 data-number="3.6" id="missing-query-results-2544"> Missing Query Results (#2544)</h2>
<p>In version 0.16.13, responses for <code>query_accounts</code>, <code>query_transfers</code>, and <code>get_account_transfers</code> routinely omitted some or all results.<a href="#fn22" id="fnref22" role="doc-noteref"><sup>22</sup></a> Missing results were always at the end: each response was a (possibly empty) prefix of the correct results. This behavior occurred frequently in healthy clusters. For example, take <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/20241205T192824.267-missing-query-results.zip">this test run</a>, where 281 seconds into the test, a client called <code>query_transfers</code> with the following filter:</p>
<div id="cb4"><pre><code><span id="cb4-1">{<span>:flags</span>  #{<span>:reversed</span>}</span>
<span id="cb4-2"> <span>:limit</span>  <span>9</span></span>
<span id="cb4-3"> <span>:ledger</span> <span>3</span></span>
<span id="cb4-4"> <span>:code</span>   <span>289</span>}</span></code></pre></div>
<p>This query returned a single result:</p>
<div id="cb5"><pre><code><span id="cb5-1">[{<span>:amount</span>            34N,</span>
<span id="cb5-2">  <span>:ledger</span>            <span>3</span>,</span>
<span id="cb5-3">  <span>:debit-account-id</span>  3137N,</span>
<span id="cb5-4">  <span>:pending-id</span>        0N,</span>
<span id="cb5-5">  <span>:credit-account-id</span> 1483N,</span>
<span id="cb5-6">  <span>:user-data</span>         <span>9</span>,</span>
<span id="cb5-7">  <span>:id</span>                327610N,</span>
<span id="cb5-8">  <span>:code</span>              <span>289</span>,</span>
<span id="cb5-9">  <span>:timeout</span>           <span>0</span>,</span>
<span id="cb5-10">  <span>:timestamp</span>         <span>1733448783658756894</span>,</span>
<span id="cb5-11">  <span>:flags</span>             #{<span>:linked</span>}}]</span></code></pre></div>
<p>However, our model expected eight additional transfers which TigerBeetle omitted. For instance, transfer <code>326112</code> had ledger 3 and code 289, and was successfully acknowledged five seconds before this query began. It should have been included in these results, but was not.</p>
<div id="cb6"><pre><code><span id="cb6-1">{<span>:amount</span>            <span>21</span>,</span>
<span id="cb6-2"> <span>:ledger</span>            <span>3</span>,</span>
<span id="cb6-3"> <span>:debit-account-id</span>  123076N,</span>
<span id="cb6-4"> <span>:pending-id</span>        0N,</span>
<span id="cb6-5"> <span>:credit-account-id</span> 51358N,</span>
<span id="cb6-6"> <span>:user-data</span>         <span>2</span>,</span>
<span id="cb6-7"> <span>:id</span>                326112N,</span>
<span id="cb6-8"> <span>:code</span>              <span>289</span>,</span>
<span id="cb6-9"> <span>:timeout</span>           <span>0</span>,</span>
<span id="cb6-10"> <span>:timestamp</span>         <span>1733448782536800935</span>,</span>
<span id="cb6-11"> <span>:flags</span>             #{}}</span></code></pre></div>
<p>Note that this query asked for transfers matching both <code>ledger = 3</code> and <code>code = 289</code>. Queries which filtered on only a single field did not exhibit this problem. TigerBeetle traced the cause to a bug in the zig-zag merge join between multiple indices (<a href="https://github.com/tigerbeetle/tigerbeetle/pull/2544">#2544</a>). When traversing an index, a bounds check prevented scanning the same chunk of records twice. During a join between two indices, the scans informed one another that some records can be safely skipped. This process could push the highest (or lowest) key outside the bounds check in the wrong direction, causing the scan to terminate early. The issue was fixed in version 0.16.17.</p>
<p>This bug went undetected by all four TigerBeetle fuzzers which perform index scans. Two fuzzers, <code>fuzz_lsm_tree</code> and <code>fuzz_lsm_forest</code>, did not perform joins. The other two, <code>vopr</code> and <code>fuzz_lsm_scan</code>, generated objects which happened to appear consecutively in each index—the “zig-zag” part of the merge join was never executed. <a href="https://github.com/tigerbeetle/tigerbeetle/commit/73f13ee2d092fe6998c315b0c4ae5fbc2983154a">Rewriting the scan fuzzer</a> to generate unpredictable objects helped it reproduce this bug quickly.</p>
<h2 data-number="3.7" id="panic-at-the-disk-0-2681a"> Panic! At the Disk 0 (#2681a)</h2>
<p>Occasionally, tests with single-bit file corruption in the superblock, WAL, or grid zones caused TigerBeetle 0.16.20 to <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/20250127T183251.857-bitflip-padding-panic.zip">crash on startup</a>. The process would print <code>panic: reached unreachable code</code>, then exit.<a href="#fn23" id="fnref23" role="doc-noteref"><sup>23</sup></a></p>
<p>These crashes were caused by three near-identical bugs in checking sector padding. For example, each superblock header in TigerBeetle’s data file contains an unused padding region normally filled with zeroes. Similarly, entries in the WAL and grid blocks may have zero padding bytes at the end. TigerBeetle’s checksums cover the data stored in each chunk, but exclude the padding. If a bit in the padding flipped from zero to one, the chunk’s checksum would still pass. Then, TigerBeetle <a href="https://github.com/tigerbeetle/tigerbeetle/blob/9e754a3dbddfbd6b51ce44805b2718061f799a97/src/vsr/grid.zig#L1080">checked to make sure</a> the padding bytes were still zeroed. When it encountered the flipped bit, that failed assertion caused the server to crash. This is perhaps worth logging, but damage to padding bytes does not compromise safety. The corrupted padding could be re-zeroed or repaired from other replicas.</p>
<p>TigerBeetle’s internal testing with the VOPR did not discover this bug because it corrupted entire sectors, rather than single bits. Corrupting a sector caused the checksum to fail and triggered the repair process. The zero-padding assertion was never reached! TigerBeetle <a href="https://github.com/tigerbeetle/tigerbeetle/pull/2681">revised the VOPR (#2681)</a> to introduce single-byte errors, which reproduced the bugs. As of 0.16.26, TigerBeetle repairs sectors with corrupt padding, instead of crashing.</p>
<h2 data-number="3.8" id="panic-due-to-superblock-bitflips-2681b"> Panic Due to Superblock Bitflips (#2681b)</h2>
<p>In a closely related bug, TigerBeetle could crash with an identical <code>panic: reached unreachable code</code> error, when we flipped bits in the superblock’s region rather than padding. Each of the superblock’s four copies includes a unique two-byte <code>copy</code> number, so that TigerBeetle can detect if a write or read of the superblock was misdirected by the disk. However, each copy of the superblock was supposed to have an identical checksum. Those checksums therefore skipped over the copy number.</p>
<p>When writing a superblock back to disk, TigerBeetle <a href="https://github.com/tigerbeetle/tigerbeetle/pull/2681/commits/f38f2921435c71ad1eb20369bc52674edb1dcfe6">checked to make sure (#2681)</a> that the copy number was between 0 and 3. If the copy number had been corrupted on disk, and that corrupted version read into memory, that assertion would fail at write time, causing a panic. TigerBeetle resolved this in version 0.16.26 by resetting the copy number, rather than crashing.</p>
<h2 data-number="3.9" id="checkpoint-divergence-on-upgrade-2745"> Checkpoint Divergence on Upgrade (#2745)</h2>
<p>When testing upgrades from 0.16.25 and before to 0.16.26 and higher, we observed repeated TigerBeetle crashes with log messages like <code>panic: checkpoint diverged</code>. For example, this one-minute test <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/20250213T152133.089-0.16.26-upgrade-diverge.zip">upgraded a five-node cluster from 0.16.25 to 0.16.26</a>, with no other faults. Node n5 detected the new binary at 21:48, and switched to executing 0.16.26 at 22:01. Immediately after starting, it panicked at <code>replica.zig:1766</code>:</p>
<pre><code>2025-02-13 21:22:06.159Z error(replica):
4: on_prepare: checkpoint diverged (op=23040
expect=3779fc8a6a13bf5cf9f995b8895c2609
received=05383d884c680d15e726071358854f67
from=2)
thread 227936 panic: checkpoint diverged</code></pre>
<p>TigerBeetle traced this crash to a change in the <code>CheckpointState</code> structure in 0.16.26. Between checkpoints, TigerBeetle tracks a set of released blocks in the file. In 0.16.26, TigerBeetle changed when that set was flipped to empty. The old version of <code>CheckpointState</code> did not need to track released blocks, because that set was always empty at checkpoint time. The new version included released blocks. To ensure older replicas could still sync state from newer ones, nodes running 0.16.26 could send both the old and new versions of <code>CheckpointState</code>. This allowed a node running 0.16.26 to send a backwards-compatible <code>CheckpointState</code> with an empty set of released blocks to a node running 0.16.25. If that node then restarted on 0.16.26, it would be missing the released blocks which other replicas knew about. Thankfully, the assertion detected this divergence and crashed the node, rather than allowing clients to observe inconsistent data.</p>
<p>We found this bug after several later versions had already been released. Because it requires state synchronization, rather than the normal replication path, we believe healthy and non-lagging clusters shouldn’t encounter this bug. The impact should also be limited to a minority of nodes. Based on these factors, and a lack of test coverage for upgrades in general, TigerBeetle opted not to release a patched version of 0.16.26. Instead, the team updated the changelog (<a href="https://github.com/tigerbeetle/tigerbeetle/pull/2745">#2745</a>) to inform customers of the hazard. Operators should pause all clients and wait for replicas to catch up before upgrading to (or past) 0.16.26.</p>
<h2 data-number="3.10" id="panic-in-release_transition-on-multiple-upgrades-2758"> Panic in <code>release_transition</code> on Multiple Upgrades (#2758)</h2>
<p>In upgrade tests from 0.16.16 to 0.16.28, we found that TigerBeetle could crash with an assertion failure in <code>replica.zig</code>’s <code>release_transition</code> function. This happened when we executed multiple upgrades within ~20 seconds of one another, or when nodes crashed or paused during the upgrade process. We could reproduce this bug reliably–with process pauses, it manifested <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/20250220T153519-release-transition-panic.zip">a few times per minute</a>.</p>
<p>TigerBeetle traced this problem to <a href="https://github.com/tigerbeetle/tigerbeetle/pull/2758">an over-zealous assertion</a> in the upgrade code (#2758).</p>
<p>Imagine a node currently runs version <span><em>A</em></span>, and an operator replaces its binary with version <span><em>B</em></span>. The node detects that version <span><em>B</em></span> is available, opens the new binary with a <a href="https://man7.org/linux/man-pages/man2/memfd_create.2.html"><code>memfd</code></a>, and uses <code>exec()</code> to replace the running process with that new code. Meanwhile, an operator replaces the binary with version <span><em>C</em></span>. The replica starts up with <span><em>B</em></span>, and as a safety check, asserts that the binary on disk (not the <code>memfd</code>!) has version header <span><em>B</em></span>. This assertion fails, since the binary is actually version <span><em>C</em></span>.</p>
<p>TigerBeetle resolved this issue in 0.16.29 by replacing the assertion with a warning message; running a different version than the binary on disk does not actually break safety.</p>
<h2 data-number="3.11" id="panic-on-deprecated-message-types-2763"> Panic on Deprecated Message Types (#2763)</h2>
<p>We encountered another occasional crash in the upgrade from 0.16.26 to 0.16.27. In <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/20250221T134853-deprecated-panic.zip">this test run</a>, two nodes crashed shortly after the upgrade. Both logged <code>panic: switch on corrupt value</code>, originating in <code>message_header.zig</code>’s <code>into_any</code> function. The crashed nodes recovered after a restart.</p>
<p>This crash was caused by a switch expression which <a href="https://github.com/tigerbeetle/tigerbeetle/pull/2763">dispatched based on the type of a message</a>. Prior to 0.16.28, Tigerbeetle removed deprecated message types from these switch expressions. An older node could send a network message of a type that the newer node would have no corresponding <code>switch</code> case for. TigerBeetle resolved the issue in version 0.16.29 by adding the deprecated message types back into the switch statements, and simply ignoring them.</p>
<h2 data-number="3.12" id="no-recovery-path-for-single-node-disk-failure-2767"> No Recovery Path for Single-Node Disk Failure (#2767)</h2>
<p>TigerBeetle offers exceptional resilience to data file corruption. However, disk failure, fires, EBS volume errors, operator errors, and more can cause a node to lose its entire data file, or to corrupt that data beyond repair. TigerBeetle is fault tolerant and can continue running safely with a minority of nodes offline. However, failed nodes do need to be replaced eventually, and most distributed systems have a mechanism for doing so. In systems which support membership changes, the best path is often to add a new, replacement node to the cluster, and to remove the failed node. Others have dedicated replacement procedures.</p>
<p>TigerBeetle, surprisingly, has no story for replacing a failed node. The documentation says nothing on the matter. There is an undocumented recovery procedure: users can run <code>tigerbeetle format</code> to re-initialize the node with an empty data file, and allow TigerBeetle’s automatic repair mechanisms to transfer the data from other nodes. Since our tests often damaged data files beyond repair, we used this reformat approach regularly.</p>
<p>Reformatting nodes works most of the time, but as TigerBeetle explained to Jepsen, it may be unsafe. For example, imagine a committed operation <code>op</code> is present on two out of three nodes, and one of those nodes is reformatted. The cluster now has a two-thirds majority which can execute a view change <em>without</em> observing <code>op</code>; the operation is then lost. In our testing, data loss was infrequent, and limited to just a few operations. For example, <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/20250226T161747-upgrade-reformat.zip">this run</a> lost five acknowledged transfers which were created in two separate requests. Another problem arises when nodes are upgraded. If a node is formatted using a newer binary, but the cluster has not yet completed the transition to that version, the node will <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/20250225T043429-upgrade-reformat.zip">crash on startup</a> during <code>replica.zig/open</code>.</p>
<p>TigerBeetle had been aware of <a href="https://github.com/tigerbeetle/tigerbeetle/issues/2767">this issue #2767</a> for some time, and planned to add a safe recovery path for the loss of a node. However, it took time to design, build, and document. After our collaboration, TigerBeetle completed this work. Version 0.16.43 incorporates a new <a href="https://docs.tigerbeetle.com/operating/recovering/"><code>tigerbeetle recover</code></a> command to recovera node which has suffered catastrophic data loss.</p>
<table>
<thead>
<tr>
<th>№</th>
<th>Summary</th>
<th>Event Required</th>
<th>Fixed in</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/tigerbeetle/tigerbeetle/issues/206">206</a></td>
<td>Requests never time out</td>
<td>None</td>
<td>Unresolved</td>
</tr>
<tr>
<td><a href="https://github.com/tigerbeetle/tigerbeetle/pull/2435">2435</a></td>
<td>Client uninitialized memory access on close</td>
<td>Interrupt or close a client</td>
<td>0.16.12</td>
</tr>
<tr>
<td><a href="https://github.com/tigerbeetle/tigerbeetle/pull/2484">2484</a></td>
<td>Client crash on eviction</td>
<td>Newer, or too many, clients</td>
<td>0.16.13</td>
</tr>
<tr>
<td><a href="https://github.com/tigerbeetle/tigerbeetle/issues/2739">2739</a></td>
<td>Elevated latencies on single-node fault</td>
<td>Pause or crash</td>
<td>0.16.43</td>
</tr>
<tr>
<td><a href="https://github.com/tigerbeetle/tigerbeetle/pull/2495">2495</a></td>
<td>Incorrect header timestamp from Java client</td>
<td>None</td>
<td>0.16.14</td>
</tr>
<tr>
<td><a href="https://github.com/tigerbeetle/tigerbeetle/pull/2544">2544</a></td>
<td>Missing query results</td>
<td>None</td>
<td>0.16.17</td>
</tr>
<tr>
<td><a href="https://github.com/tigerbeetle/tigerbeetle/pull/2681">2681a</a></td>
<td>Panic on bitflips in chunk padding</td>
<td>Bitflip and restart</td>
<td>0.16.26</td>
</tr>
<tr>
<td><a href="https://github.com/tigerbeetle/tigerbeetle/pull/2681/commits/f38f2921435c71ad1eb20369bc52674edb1dcfe6">2681b</a></td>
<td>Panic on bitflips in superblock copy number</td>
<td>Bitflip and restart</td>
<td>0.16.26</td>
</tr>
<tr>
<td><a href="https://github.com/tigerbeetle/tigerbeetle/pull/2745">2745</a></td>
<td>Checkpoint divergence</td>
<td>0.16.26 upgrade during sync</td>
<td>Documented</td>
</tr>
<tr>
<td><a href="https://github.com/tigerbeetle/tigerbeetle/pull/2758">2758</a></td>
<td>Panic in <code>release_transition</code> on upgrades</td>
<td>Upgrades in quick succession</td>
<td>0.16.29</td>
</tr>
<tr>
<td><a href="https://github.com/tigerbeetle/tigerbeetle/pull/2763">2763</a></td>
<td>Panic on deprecated message types</td>
<td>Upgrade</td>
<td>0.16.29</td>
</tr>
<tr>
<td><a href="https://github.com/tigerbeetle/tigerbeetle/issues/2767">2767</a></td>
<td>No recovery path for single-node disk failure</td>
<td>Single-node disk failure</td>
<td>0.16.43</td>
</tr>
</tbody>
</table>
<h2 data-number="4" id="discussion"> Discussion</h2>
<p>We found two safety issues in TigerBeetle.<a href="#fn24" id="fnref24" role="doc-noteref"><sup>24</sup></a><a href="#fn25" id="fnref25" role="doc-noteref"><sup>25</sup></a> Prior to version 0.16.17, TigerBeetle often omitted results from queries with multiple filters, even in healthy clusters. We also found a very minor issue in which a debugging API in the Java client, added specifically for our tests, returned incorrect and duplicate timestamps for operations. As of 0.16.26 and higher, our findings were consistent with TigerBeetle’s claims of <a href="https://jepsen.io/consistency/models/strong-serializable">Strong Serializability</a>—one of the strongest consistency models for concurrent systems. TigerBeetle preserved this property through various combinations of process pauses, crashes, network partitions, clock errors, disk corruption, and upgrades.</p>
<p>We also found seven crashes in TigerBeetle. Two affected the Java client: an uninitialized memory access caused by a shared mutable data structure, and a design choice to crash the entire process when a server evicted a client. Both were fixed by 0.16.13. Five involved servers: two panics on disk corruption, and three more involving upgrades. All crashes were resolved by 0.16.29, with the exception of #2745, which is now documented.</p>
<p>We found some surprising performance and availability issues in TigerBeetle. Server latencies jumped dramatically when even a single node was unavailable. This behavior is unusual—most consensus systems are relatively insensitive to single-node failures—and stemmed from a design choice to replicate data in a ring, rather than broadcasting from the primary to all backups directly. This behavior was somewhat improved in 0.16.30, but still quite noticeable. After our collaboration, TigerBeetle extended their simulation tests to measure performance under various faults, and used those tests to drive <a href="https://github.com/tigerbeetle/tigerbeetle/issues/2739#issuecomment-2919767029">extensive improvements</a> in 0.16.43. The ring topology now continually adapts to observed latencies, and messages are broadcast in both directions around the ring. We believe these improvements should significantly mitigate the latency impact of failures.</p>
<p>TigerBeetle also lacked a safe path to recover a node which had suffered catastrophic disk failure. After our collaboration, TigerBeetle <a href="https://github.com/tigerbeetle/tigerbeetle/pull/2996">built</a> a new <a href="https://docs.tigerbeetle.com/operating/recovering/">recovery command</a>, which is available as of 0.16.43.</p>
<p>Only one issue remains unresolved. By design, client requests are retried forever, which complicates error handling. TigerBeetle plans to address this, but the work will take some time.</p>
<p>We recommend users upgrade to 0.16.43, which addresses all but one of the issues reported here. Users should exercise particular caution during the upgrade to (or past) 0.16.26; consult <a href="https://github.com/tigerbeetle/tigerbeetle/blob/main/CHANGELOG.md#tigerbeetle-01626">the release notes</a>. We also suggest that users simulate single-node failures in a testing environment, and measure how their application responds to elevated latencies.</p>
<p>TigerBeetle exhibits a refreshing dedication to correctness. The architecture appears sound: Viewstamped Replication is a well-established consensus protocol, and TigerBeetle’s integration of flexible quorums and protocol-aware recovery seem to have allowed improved availability and extreme resilience to data file corruption. Integrating these protocols does not appear to have compromised the key invariant of Strong Serializability. Most of our findings involved crashes or performance degradation, rather than safety errors. Moreover, several of those crashes were due to overly cautious assertions.</p>
<p>We attribute this robustness in large part to TigerBeetle’s extensive simulation, integration, and property-based tests, which caught a broad range of safety bugs both before and during our engagement. As we brought new issues to the TigerBeetle team, they quickly expanded their internal test suite to reproduce them. We are confident that TigerBeetle’s investment in careful engineering and rigorous testing will continue to pay off, and we’re excited to see these techniques adopted by more databases in the future.</p>
<p>As always, we caution that Jepsen takes an experimental approach to safety verification: we can prove the presence of bugs, but not their absence. While we make extensive efforts to find problems, we cannot prove correctness.</p>
<h2 data-number="4.1" id="disk-faults"> Disk Faults</h2>
<p>TigerBeetle offers exceptional resilience to disk faults. In our tests, it recovered from bitflips and other kinds of file corruption in almost every part of a node’s data file, so long as corruption was limited to a minority of nodes. In some file zones, like the grid, TigerBeetle tolerated the loss or corruption of all but one copy. In the superblock, client replies, and grid zones of the data file, TigerBeetle could recover our “helical” faults, in which every node experienced data corruption spread across disjoint regions of the file.</p>
<p>As previously noted, bitflips in the superblock copy number, or in various zero-padding regions, could cause TigerBeetle to crash. These issues have been resolved as of 0.16.26.</p>
<p>Rolling back all four copies of a node’s superblock to an earlier version can permanently disable a TigerBeetle node; it will crash upon detecting WAL entries newer than the superblock. TigerBeetle considers this outside their fault model, and we concur. Given that TigerBeetle performs four separate, sequential writes of the superblock and reads them back to confirm their presence, it seems remarkably unlikely to encounter this by accident.</p>
<p>Helical faults in the WAL can permanently disable a TigerBeetle cluster. The most recent “head” entry of the WAL is critical, and since some nodes may lag behind others, they may have heads at different file offsets. In our tests, helical faults often corrupted the head of the WAL on a majority of nodes, rendering the entire cluster unusable.</p>
<p>When a node’s disk file is lost or corrupted beyond repair, TigerBeetle currently has no safe path for recovery. We recommend users exercise caution when reformatting a failed node. Avoid upgrades when a node is down, and try to reformat a node only when the remainder of the cluster appears healthy. If possible, pause clients and check node logs before upgrading: none should be logging sync-related messages.</p>
<h2 data-number="4.2" id="retries"> Retries</h2>
<p>Users should think carefully about the official TigerBeetle clients’ retry behavior. By default, clients retry operations forever. Synchronous operations will never time out; you may need to implement your own timeouts. The futures returned by asynchronous calls do offer APIs with timeouts, but the client will continue retrying those operations forever. Long-lasting unavailability could cause TigerBeetle clients to consume unbounded memory as they attempt to buffer and retry an ever-growing set of requests.</p>
<p>This retry behavior flattens definite and indefinite failures into indefinite ones: <em>everything</em> becomes a timeout. Contrary to TigerBeetle’s documentation, indefinite network errors are very much possible. Indeed, they are more likely in TigerBeetle than in systems which return distinct network errors! Moreover, TigerBeetle users may find it more difficult to implement (e.g.) exponential backoff or load-shedding circuit breakers: in order to abandon a single request, the entire client must be torn down.</p>
<p>Jepsen recommends that users carefully consider and test their timeout behavior during faults. We also suggest TigerBeetle introduce at least two kinds of error, so users can distinguish between definite and indefinite faults. Finally, clients should take configurable timeouts, so users can bound their time and memory consumption.</p>
<h2 data-number="4.3" id="crashing-as-a-way-of-life"> Crashing as a Way of Life</h2>
<p>TigerBeetle prizes safety, and employs defensive programming techniques to ensure it. In addition to carefully designed algorithms and extensive testing, both client and server code are full of assertions which double-check that intended invariants have been preserved. Assertion failures crash the entire program to preserve safety. When this happens, clients or servers may be partially or totally unavailable, sometimes for minutes, sometimes permanently. Many of our findings involved an assertion which turned what <em>would</em> have been a safety hazard into a simple crash: a welcome tradeoff for safety-critical systems.</p>
<p>This is a sensible approach. Complex systems <a href="https://how.complexsystems.fail/">ensure safety</a> through an interlocking set of guards: each guard screens out errors the others might have missed. Abandoning possibly-incorrect execution is also a core tenet of Erlang/OTP’s <a href="https://erlang.org/pipermail/erlang-questions/2003-March/007870.html">“let it crash”</a> ethos. However, TigerBeetle’s approach is not without drawbacks.</p>
<p>First, several of the crashes we found in this report were due to overly conservative assertions. For instance, prior to 0.16.26, TigerBeetle crashed on encountering non-zero bytes in unused padding regions on disk. Safety was never endangered by these errors, but the crashes compromised availability—and could push users into the dangerous recovery path of reformatting.</p>
<p>Second, in 0.16.11, TigerBeetle’s client library forcibly crashed the entire application process when a client used a newer version than the server, or when the server simply had too many connections. These are errors that a well-designed application can and should recover from—for instance, through an exponential backoff and retry system, or by coordinating with other clients. Crashing the process, instead of returning an error code or throwing an exception, denies the application the ability to make these mitigations.</p>
<p>In Erlang, “let it crash” means more than simply abandoning computation early. It is integrally linked with Erlang’s actor model, which allows actors to crash independently of one another. It also relies on Erlang’s <em>supervisor trees</em>: every actor has a supervisor which is notified of a crash and can restart the failed computation. In TigerBeetle, the failure domain is the entire POSIX process, and the supervisor, where one exists, is something like the init system or Kubernetes. These supervisors generally lack visibility into <em>why</em> the crash happened or how to recover, and they are often not equipped to adapt to changing circumstances. They may restart the process over and over again, crashing every time. On repeated crashes, they may give up on the process forever.</p>
<p>Despite these limitations, we feel TigerBeetle makes a reasonable compromise. TigerBeetle is intended for financial systems of record where integrity is key, and overly-cautious assertions can be fixed easily as they arise. Those assertions also help to experimentally validate and guide the mental models of engineers working on TigerBeetle. TigerBeetle’s clients have shifted more towards returning error codes, rather than crashing outright.</p>
<p>More generally, we encourage engineers to think about fault domains when designing error paths. Ask, “If we must crash, how can we keep a part of the system running?” And after a crash, “How will that part recover?” This is especially important for client libraries, which are guests in another system’s home.</p>
<h2 data-number="4.4" id="future-work"> Future Work</h2>
<p>TigerBeetle includes a <a href="https://docs.tigerbeetle.com/reference/transfer/#timeout">timeout mechanism</a> for pending transfers. We do not know how to robustly test this system, since timeouts may, by design, not void a transfer until some time after their deadline has passed. We would like to revisit timeout semantics with an eye towards establishing quantitative bounds.</p>
<p>During the course of this research, Jepsen, TigerBeetle, and <a href="https://antithesis.com/">Antithesis</a> collaborated to run Jepsen’s TigerBeetle test suite within Antithesis’s environment—taking advantage of Antithesis’ deterministic simulation, fault injection, and time-travel debugging capabilities. These experiments are still in the early stages, but could lay the groundwork for a powerful, complementary analysis of distributed systems.</p>
<p>Multi-version systems are also devilishly hard to pull off. While TigerBeetle already had excellent test coverage for single versions, they lacked fuzz tests for cross-version upgrades. Our tests found several issues in the upgrade process, and TigerBeetle plans to expand their testing of upgrades in the future. Similarly, membership changes in distributed systems are notoriously challenging, and currently unimplemented in TigerBeetle. As TigerBeetle builds support for adding and removing nodes, we anticipate a rich opportunity for further testing.</p>
<p>Finally, TigerBeetle’s approach to retries has been the subject of ongoing discussion, and redesigning their approch will take time. We anticipate further work towards a robust client representation of errors.</p>
<p><em>This work would not have been possible without the invaluable assistance of the TigerBeetle team, including Fabio Arnold, Rafael Batiati, Chaitanya Bhandari, Lewis Daly, Joran Dirk Greef, djg, Alex Kladov, Federico Lorenzi, and Tobias Ziegler. Our thanks to <a href="https://github.com/duckinator">Ellen Marie Dash</a> for helping write the new file-corruption nemesis used in this research. We are grateful to <a href="https://www.irenekannyo.com/">Irene Kannyo</a> for her editorial support. This report was funded by TigerBeetle, Inc.&nbsp;and conducted in accordance with the <a href="https://jepsen.io/analyses/ethics">Jepsen ethics policy</a>.</em></p>
<section role="doc-endnotes">
<hr>
<ol>
<li id="fn1" role="doc-endnote"><p>To give some idea of the rates involved—India’s Unified Payments Interface <a href="https://www.npci.org.in/what-we-do/upi/product-statistics">processes roughly 16 billion transfers per month</a>, which works out to about 6,000 per second, on average. <a href="https://www.clearstreet.io/">Clear Street</a>, a brokerage in New York, indicates that they process on the order of 30,000 debit-credit transfers per second after the market closes.<a href="#fnref1" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>Deterministic simulation testing is essentially <a href="https://dl.acm.org/doi/pdf/10.1145/3597503.3639581">property-based testing</a> with techniques to turn non-deterministic systems into deterministic ones. The clock, disk state, scheduler, network delivery, external services, and so on are controlled to ensure reproducibility. For more on this approach, you might start with <a href="https://smallbone.se/papers/finding-race-conditions.pdf">PULSE</a>, <a href="https://www.youtube.com/watch?v=N5HyVUPuU0E">Simulant</a>, <a href="https://www.foundationdb.org/files/fdb-paper.pdf">FoundationDB</a>, and <a href="https://antithesis.com/solutions/problems_we_solve/">Antithesis</a>.<a href="#fnref2" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>Timestamps are derived from <code>CLOCK_REALTIME</code>, which is presumably synchronized via NTP, PTP, etc. The primary uses <a href="https://tigerbeetle.com/blog/2021-08-30-three-clocks-are-better-than-one"><code>CLOCK_BOOTTIME</code></a> to estimate and compensate for network latency in clock messages.<a href="#fnref3" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>Many consensus systems use a majority of nodes as a quorum. As Heidi Howard <a href="http://hh360.user.srcf.net/blog/2016/08/majority-agreement-is-not-necessary/">showed in 2016</a>, Paxos can use <em>different</em> quorums for its leader election and replication phases; these two quorums must intersect, but one may be less than a majority. TigerBeetle applies this “flexible quorum” approach to Viewstamped Replication. It requires only half, not a majority, of clocks to agree.<a href="#fnref4" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p>TigerBeetle’s core is designed to replicate arbitrary state machines, so this may change in the future.<a href="#fnref5" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p>This representation is unusual: most databases allow user-defined schemas, a variety of types, and variable-size data. However, TigerBeetle’s domain is well-understood: the broad shape of financial record-keeping has not changed in centuries. Moreover, a rigid, fixed-size schema provides significant performance advantages: efficient encoding and decoding, zero-copy transfer of structures between network and disk, prefetcher/branch prediction friendliness, cache line alignment, and so on.<a href="#fnref6" role="doc-backlink">↩︎</a></p></li>
<li id="fn7" role="doc-endnote"><p>For efficiency, TigerBeetle omits successful results from the actual response messages, and returns only errors, if present.<a href="#fnref7" role="doc-backlink">↩︎</a></p></li>
<li id="fn8" role="doc-endnote"><p><a href="https://www.datomic.com/">Datomic</a>, <a href="https://fauna.com/">FaunaDB</a>, and TigerBeetle are all Strong Serializable temporal databases. However, they choose varying semantics for the flow of time and effects within a transaction. Datomic evaluates the parts of a transaction <a href="https://jepsen.io/analyses/datomic-pro-1.0.7075#intra-transaction-semantics">concurrently</a>, and assigns them all a single timestamp. Fauna executes them sequentially, but all operations <a href="https://docs.fauna.com/fauna/current/reference/fql-api/time/now/">observe a single timestamp</a>. TigerBeetle executes sequentially <em>and</em> gives each micro-operation a distinct timestamp.<a href="#fnref8" role="doc-backlink">↩︎</a></p></li>
<li id="fn9" role="doc-endnote"><p>Unlike most consensus systems, which use majority quorums and work best with an odd number of nodes, TigerBeetle uses flexible quorums which allows some operations to commit with (e.g.) just three out of six nodes.<a href="#fnref9" role="doc-backlink">↩︎</a></p></li>
<li id="fn10" role="doc-endnote"><p>Imagine a write <span><em>w</em></span> is acknowledged by node <span><em>a</em></span>, but node <span><em>b</em></span> lags behind, such that a read sent to <span><em>b</em></span> would not observe <span><em>w</em></span>. This would be a <a href="https://jepsen.io/consistency/phenomena/stale-read">stale read</a>—a violation of Strong Serializability. Smart clients tend to route all requests to <em>either</em> <span><em>a</em></span> or <span><em>b</em></span>, rather than balancing requests between them. A test suite using such a client would likely miss the stale read.<a href="#fnref10" role="doc-backlink">↩︎</a></p></li>
<li id="fn11" role="doc-endnote"><p>This technique does not work for imported events, where reads tell us the <em>imported</em> timestamp, rather than the <em>execution</em> timestamp. When testing imports, we used very long timeouts, and required that every operation succeed in order to check the history.<a href="#fnref11" role="doc-backlink">↩︎</a></p></li>
<li id="fn12" role="doc-endnote"><p>We used the timestamp of the last successful write as the upper bound on the main phase. Writes may have been executed during the final read phase (e.g.&nbsp;due to network delays), but we ignored them for safety checking.<a href="#fnref12" role="doc-backlink">↩︎</a></p></li>
<li id="fn13" role="doc-endnote"><p>For efficiency, we actually computed a transitive reduction of the real-time dependency graph.<a href="#fnref13" role="doc-backlink">↩︎</a></p></li>
<li id="fn14" role="doc-endnote"><p>TigerBeetle has three internal timestamps that constrain clock values: the “current” timestamp, and two separate clocks for imported accounts and transfers, whose timestamps are still monotonic, but lag behind the current time.<a href="#fnref14" role="doc-backlink">↩︎</a></p></li>
<li id="fn15" role="doc-endnote"><p>TigerBeetle guarantees that certain classes of errors, called <a href="https://web.archive.org/web/20241222161312/https://docs.tigerbeetle.com/reference/requests/create_transfers/#id_already_failed"><em>transient errors</em></a>, ensure that a transfer will <em>always</em> fail, even if resubmitted under conditions where it would otherwise succeed. These errors are transient in the sense that they are caused by (potentially) short-lived conditions in the database state, but persistent in the sense that the database must remember them for all time.<a href="#fnref15" role="doc-backlink">↩︎</a></p></li>
<li id="fn16" role="doc-endnote"><p>Our strategy does require that a single ID is never written twice. We complemented the main workload with a dedicated <a href="https://github.com/jepsen-io/tigerbeetle/blob/815e32d20185db00b4b72bbfd8c1e43e2dff5e8b/src/jepsen/tigerbeetle/workload/idempotence.clj">idempotence</a> workload which verifies that duplicate attempts to write the same data never succeed, and never lead to divergent values for the same ID.<a href="#fnref16" role="doc-backlink">↩︎</a></p></li>
<li id="fn17" role="doc-endnote"><p>TigerBeetle includes an automatic timeout mechanism for pending writes, but timeouts are not exactly deterministic, which makes it difficult to model-check.<a href="#fnref17" role="doc-backlink">↩︎</a></p></li>
<li id="fn18" role="doc-endnote"><p>One notable exception to this rule is the WAL. The write-ahead log is built as a ring buffer. The head of the write-ahead log is critical: if the head of the WAL is corrupted on one node, that node cannot trust its own data file and must ask the other nodes to help it repair the damage. Because some nodes may lag behind others, it is possible that the head of the WAL could be at different file offsets on different nodes. A helical fault could corrupt the head on a majority of nodes, preventing the cluster from recovering.<a href="#fnref18" role="doc-backlink">↩︎</a></p></li>
<li id="fn19" role="doc-endnote"><p>To be clear: a definite failure can be retried, and that retry operation might succeed. When we say a definite error means an operation will “never happen,” we refer to the original operation, not retries.<a href="#fnref19" role="doc-backlink">↩︎</a></p></li>
<li id="fn20" role="doc-endnote"><p>This is Jepsen’s <a href="https://github.com/jepsen-io/jepsen/blob/ce07779ca3b81feb8553276581faa73d963a95b7/jepsen/src/jepsen/util.clj#L471-L482">standard timeout macro</a>, used when clients don’t time out reliably on their own. For clarity, we’ve omitted some error handling.<a href="#fnref20" role="doc-backlink">↩︎</a></p></li>
<li id="fn21" role="doc-endnote"><p>This low session limit is an intentional design choice: TigerBeetle benefits from large batches of requests, and enforcing a smaller number of clients nudges users towards designs which can perform client-side batching efficiently. One imagines a <a href="https://www.pgbouncer.org/">PgBouncer-style</a> proxy might also come in handy here.<a href="#fnref21" role="doc-backlink">↩︎</a></p></li>
<li id="fn22" role="doc-endnote"><p>This also occurred with <code>get_account_balances</code>, but our test harness didn’t yet cover that API call.<a href="#fnref22" role="doc-backlink">↩︎</a></p></li>
<li id="fn23" role="doc-endnote"><p>All TigerBeetle assertion failures printed <code>reached unreachable code</code> then exited—there was no error message to tell them apart. Debugging builds offered a stacktrace.<a href="#fnref23" role="doc-backlink">↩︎</a></p></li>
<li id="fn24" role="doc-endnote"><p>Issue 2745 is in some sense both a safety and liveness issue. Replicas disagree on which blocks are free, violating a key safety property in TigerBeetle’s design: replicas should have identical on-disk state. However, a defensive assertion converts this safety violation to a crash, which prevents users from observing the divergence. In this sense it is a liveness issue, and we report it as such.<a href="#fnref24" role="doc-backlink">↩︎</a></p></li>
<li id="fn25" role="doc-endnote"><p>For the formal verification enthusiasts in the crowd: yes, recoverable crashes, transient availability issues, and high latency are all technically safety issues, in that they involve <a href="https://www.hillelwayne.com/post/safety-and-liveness">finite counterexamples</a>.<a href="#fnref25" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
  </div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Infomaniak comes out in support of controversial Swiss encryption law (181 pts)]]></title>
            <link>https://www.tomsguide.com/computing/vpns/infomaniak-breaks-rank-and-comes-out-in-support-of-controversial-swiss-encryption-law</link>
            <guid>44199377</guid>
            <pubDate>Fri, 06 Jun 2025 10:13:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tomsguide.com/computing/vpns/infomaniak-breaks-rank-and-comes-out-in-support-of-controversial-swiss-encryption-law">https://www.tomsguide.com/computing/vpns/infomaniak-breaks-rank-and-comes-out-in-support-of-controversial-swiss-encryption-law</a>, See on <a href="https://news.ycombinator.com/item?id=44199377">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">
<section>
<div>
<div>
<picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/xAw9pt6qKPwTyceQerobem-1280-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/xAw9pt6qKPwTyceQerobem-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/xAw9pt6qKPwTyceQerobem-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/xAw9pt6qKPwTyceQerobem-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/xAw9pt6qKPwTyceQerobem-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/xAw9pt6qKPwTyceQerobem-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/xAw9pt6qKPwTyceQerobem-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/xAw9pt6qKPwTyceQerobem.jpg" alt="Swiss flag, red with white cross, flying in front of a building on sunny day" srcset="https://cdn.mos.cms.futurecdn.net/xAw9pt6qKPwTyceQerobem-1280-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/xAw9pt6qKPwTyceQerobem-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/xAw9pt6qKPwTyceQerobem-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/xAw9pt6qKPwTyceQerobem-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/xAw9pt6qKPwTyceQerobem-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/xAw9pt6qKPwTyceQerobem-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/xAw9pt6qKPwTyceQerobem-320-80.jpg 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/xAw9pt6qKPwTyceQerobem.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/xAw9pt6qKPwTyceQerobem.jpg" data-pin-nopin="true" fetchpriority="high">
</picture>
</div>
<figcaption>
<span>(Image credit: Sunphol Sorakul / Getty Images)</span>
</figcaption>
</div>

<div id="article-body">
<p>In Switzerland, some of the <a data-analytics-id="inline-link" href="https://www.tomsguide.com/best-picks/best-vpn" data-before-rewrite-localise="https://www.tomsguide.com/best-picks/best-vpn">best VPNs</a> are in the firing line as a result of the country's <a data-analytics-id="inline-link" href="https://www.tomsguide.com/computing/vpns/proposed-swiss-encryption-laws-may-have-a-severe-impact-on-vpns-what-you-need-to-know" data-before-rewrite-localise="https://www.tomsguide.com/computing/vpns/proposed-swiss-encryption-laws-may-have-a-severe-impact-on-vpns-what-you-need-to-know">proposed changes to encryption laws</a>.</p><p>The law's revision would extend surveillance obligations and require companies to collect information and identification on their users – a move that would significantly impact online privacy.</p><p>Swiss-based VPNs <a data-analytics-id="inline-link" href="https://www.tomsguide.com/reviews/protonvpn-review" data-before-rewrite-localise="https://www.tomsguide.com/reviews/protonvpn-review">Proton VPN</a> and <a data-analytics-id="inline-link" href="https://www.tomsguide.com/computing/vpns/introducing-nymvpn-could-this-be-the-worlds-most-secure-vpn" data-before-rewrite-localise="https://www.tomsguide.com/computing/vpns/introducing-nymvpn-could-this-be-the-worlds-most-secure-vpn">NymVPN</a> would be affected, and Proton CEO <a data-analytics-id="inline-link" href="https://www.tomsguide.com/computing/vpns/proton-vpn-boss-compares-switzerland-to-russia-and-claims-it-could-leave-the-country-over-proposed-law" data-before-rewrite-localise="https://www.tomsguide.com/computing/vpns/proton-vpn-boss-compares-switzerland-to-russia-and-claims-it-could-leave-the-country-over-proposed-law">Andy Yen said the privacy-focused company would rather leave its Swiss base</a> than risk the privacy of its users.</p><p>The <a data-analytics-id="inline-link" href="https://www.tomsguide.com/best-picks/most-private-vpn" data-before-rewrite-localise="https://www.tomsguide.com/best-picks/most-private-vpn">most private VPNs</a> uphold strict <a data-analytics-id="inline-link" href="https://www.tomsguide.com/computing/vpns/why-windscribes-court-case-proves-how-important-vpn-no-logging-policies-are" data-before-rewrite-localise="https://www.tomsguide.com/computing/vpns/why-windscribes-court-case-proves-how-important-vpn-no-logging-policies-are">no-logs policies</a> and collect very little information about users. This law would see these policies undermined.</p><p>Despite widespread opposition from across the country, Swiss cloud security company Infomaniak is supporting the law.</p><p>Infomaniak describes itself as an "ethical cloud" company and one that doesn't compromise on "ecology, privacy, or people." It's surprising, then, that they are seemingly the only privacy-focused company in Switzerland supporting the law change.</p><h2 id="infomaniak-opposes-anonymity-3">Infomaniak opposes anonymity</h2><p>In a <a data-analytics-id="inline-link" href="https://www.rts.ch/play/tv/les-beaux-parleurs/video/faire-bonne-figure?urn=urn:rts:video:f357b48c-e80a-3330-b5c6-972e254d401f" target="_blank" rel="nofollow" data-url="https://www.rts.ch/play/tv/les-beaux-parleurs/video/faire-bonne-figure?urn=urn:rts:video:f357b48c-e80a-3330-b5c6-972e254d401f" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">debate on Radio Télévision Suisse</a> (RTS), and <a data-analytics-id="inline-link" href="https://www.clubic.com/actualite-566385-infomaniak-tacle-proton-et-introduira-le-chiffrement-des-mails-sans-anonymat.html" target="_blank" rel="nofollow" data-url="https://www.clubic.com/actualite-566385-infomaniak-tacle-proton-et-introduira-le-chiffrement-des-mails-sans-anonymat.html" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">reported in Clubic</a>, Infomaniak spokesperson Thomas Jacobsen addressed Andy Yen's comments on the law.</p><p>Jacobsen believed Yen showed a "lack of knowledge of Swiss political institutions" and called for finding the right balance, not looking for extremes.</p><p>Infomaniak argued that anonymity prevents justice, saying there must be a "happy medium" to prevent the digital landscape becoming a "Wild West."</p><p>Proton was cited as a company that advocates for anonymity, but this isn't technically the case. Proton, and Proton VPN, advocates for privacy – and there is a subtle but important difference between the two.</p><p>Confusing privacy and anonymity is common – a Tom's Guide VPN survey found that <a data-analytics-id="inline-link" href="https://www.tomsguide.com/computing/vpns/we-surveyed-toms-guide-readers-about-vpns-and-i-need-to-bust-some-myths" data-before-rewrite-localise="https://www.tomsguide.com/computing/vpns/we-surveyed-toms-guide-readers-about-vpns-and-i-need-to-bust-some-myths">29% of readers think VPNs make you anonymous</a> – but they don't mean the same thing. Anonymity is when your identity isn't known and no trace of your activity is left behind, with the Tor Network being an example.</p><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/2kLGpu5zaMJ9qcC8sDqNhk-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/2kLGpu5zaMJ9qcC8sDqNhk-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/2kLGpu5zaMJ9qcC8sDqNhk-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/2kLGpu5zaMJ9qcC8sDqNhk-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/2kLGpu5zaMJ9qcC8sDqNhk-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/2kLGpu5zaMJ9qcC8sDqNhk-1200-80.jpg.webp 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)"><img src="https://cdn.mos.cms.futurecdn.net/2kLGpu5zaMJ9qcC8sDqNhk.jpg" alt="Shadowy hooded figure stood crossed armed in front of red and dark blue background" srcset="https://cdn.mos.cms.futurecdn.net/2kLGpu5zaMJ9qcC8sDqNhk-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/2kLGpu5zaMJ9qcC8sDqNhk-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/2kLGpu5zaMJ9qcC8sDqNhk-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/2kLGpu5zaMJ9qcC8sDqNhk-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/2kLGpu5zaMJ9qcC8sDqNhk-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/2kLGpu5zaMJ9qcC8sDqNhk-1200-80.jpg 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-original-mos="https://cdn.mos.cms.futurecdn.net/2kLGpu5zaMJ9qcC8sDqNhk.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/2kLGpu5zaMJ9qcC8sDqNhk.jpg"></picture></p></div><figcaption itemprop="caption description"><span itemprop="copyrightHolder">(Image credit: urbazon / Getty Images)</span></figcaption></figure><p>VPNs protect your data and your privacy. Many can still see some identifiable information and most don't claim to offer anonymity. Although your data is encrypted and reputable VPN providers can't see your internet activity, they can still see your connecting IP address and your payment information. The key point is that they never log or share it.</p><p>Hackers, third-parties, or your ISP can't see what you're doing, and that is the privacy VPNs offer. Infomaniak is incorrect in saying Proton advocates for anonymity.</p><h2 id="calling-out-free-vpns-3">Calling out free VPNs</h2><p>Infomaniak also took issue with free services, such as free VPNs. In the debate, Jacobsen said how these free services allow anyone to hide from the law by enabling anonymity.</p><p>While VPNs can be misused by bad actors for criminal endeavours, something all reputable VPNs and Tom's Guide opposes, this doesn't mean they should be taken away or targeted.</p><p>Almost every kind of technology and device can be used for illicit purposes. We have to accept that not everything can, or should, be controlled in order to target a small minority.</p><p>This trade-off would take away the right to privacy of millions of genuine users.</p><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/atrrkzhMVawuwLjXF7bHVL-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/atrrkzhMVawuwLjXF7bHVL-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/atrrkzhMVawuwLjXF7bHVL-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/atrrkzhMVawuwLjXF7bHVL-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/atrrkzhMVawuwLjXF7bHVL-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/atrrkzhMVawuwLjXF7bHVL-1200-80.jpg.webp 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)"><img src="https://cdn.mos.cms.futurecdn.net/atrrkzhMVawuwLjXF7bHVL.jpg" alt="Broken speech bubble on red background" srcset="https://cdn.mos.cms.futurecdn.net/atrrkzhMVawuwLjXF7bHVL-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/atrrkzhMVawuwLjXF7bHVL-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/atrrkzhMVawuwLjXF7bHVL-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/atrrkzhMVawuwLjXF7bHVL-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/atrrkzhMVawuwLjXF7bHVL-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/atrrkzhMVawuwLjXF7bHVL-1200-80.jpg 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-original-mos="https://cdn.mos.cms.futurecdn.net/atrrkzhMVawuwLjXF7bHVL.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/atrrkzhMVawuwLjXF7bHVL.jpg"></picture></p></div><figcaption itemprop="caption description"><span itemprop="copyrightHolder">(Image credit: rob dobi / Getty Images)</span></figcaption></figure><p>VPNs, and especially the <a data-analytics-id="inline-link" href="https://www.tomsguide.com/best-picks/best-free-vpn" data-before-rewrite-localise="https://www.tomsguide.com/best-picks/best-free-vpn">best free VPNs</a>, are a lifeline for people living under censorship and internet restrictions. Without them, they would be unable to access a free and open internet and would suffer at the hands of authoritarian regimes.</p><p>Proton VPN has a host of <a data-analytics-id="inline-link" href="https://www.tomsguide.com/computing/vpns/what-are-anti-censorship-features-and-how-is-proton-vpn-leading-the-way" data-before-rewrite-localise="https://www.tomsguide.com/computing/vpns/what-are-anti-censorship-features-and-how-is-proton-vpn-leading-the-way">dedicated anti-censorship features</a>, aimed at protecting the privacy of those who need it most – including the free service <a data-analytics-id="inline-link" href="https://www.tomsguide.com/reviews/proton-vpn-free-review" data-before-rewrite-localise="https://www.tomsguide.com/reviews/proton-vpn-free-review">Proton VPN Free</a>. Many VPNs also offer <a data-analytics-id="inline-link" href="https://www.tomsguide.com/computing/vpns/world-press-freedom-day-2025-how-vpns-are-helping-fight-for-a-free-and-open-internet" data-before-rewrite-localise="https://www.tomsguide.com/computing/vpns/world-press-freedom-day-2025-how-vpns-are-helping-fight-for-a-free-and-open-internet">free emergency VPNs</a> for journalists or activists.</p><p>VPNs cannot just be for those who can afford them, so Infomaniak's targeting of free privacy services fails to consider the appropriate repercussions.</p><p>The article quotes Infomaniak's founder, Boris Siegenthaler, as saying "the answer is clear: the day activists for important climate, humanitarian, or democratic causes are in the crosshairs, we will oppose this request."</p><p>However, many argue that the Swiss government's request <em>would</em> put those people in the crosshairs, and they wouldn't be protected.</p><p>Infomaniak doesn't advocate for widespread surveillance, but that would not be needed under these plans. Metadata collection could form a large part of the new surveillance law, and it's seemingly something Infomaniak supports.</p><p>In a separate <a data-analytics-id="inline-link" href="https://www.rts.ch/play/tv/forum/video/succes-dinfomaniak-le-fournisseur-suisse-de-services-cloud-interview-de-thomas-jacobsen?urn=urn:rts:video:1e5a3af0-bb0b-3ba2-9d56-34842591ee9c" target="_blank" rel="nofollow" data-url="https://www.rts.ch/play/tv/forum/video/succes-dinfomaniak-le-fournisseur-suisse-de-services-cloud-interview-de-thomas-jacobsen?urn=urn:rts:video:1e5a3af0-bb0b-3ba2-9d56-34842591ee9c" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">interview with RTS</a>, Jacobsen argues that metadata collection is acceptable in order to help prosecute individuals who "carry out illicit activities" anonymously. He says how in other aspects of life, we don't accept that, saying you need ID for taking out a phone number and SIM card.</p><p>"The outside of the package is enough to bring justice," he said – referring to metadata. The contents of messages or communications will remain encrypted, but the metadata will be seen and collected.</p><p>Metadata can include geolocation, date and time, IP addresses, file size, device identifiers, plus who sent and received the message. So, even though the actual content of the message remains encrypted and hidden, you can identify and subsequently prosecute individuals based on analysis of metadata.</p><p>The opposition claims this is a fundamental privacy risk if handled in the wrong way, and something that should be opposed, not lauded.</p><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/vHTTmCZU5J94BTnGKziW33-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/vHTTmCZU5J94BTnGKziW33-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/vHTTmCZU5J94BTnGKziW33-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/vHTTmCZU5J94BTnGKziW33-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/vHTTmCZU5J94BTnGKziW33-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/vHTTmCZU5J94BTnGKziW33-1200-80.jpg.webp 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)"><img src="https://cdn.mos.cms.futurecdn.net/vHTTmCZU5J94BTnGKziW33.jpg" alt="Floating eyeballs watching a red laptop" srcset="https://cdn.mos.cms.futurecdn.net/vHTTmCZU5J94BTnGKziW33-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/vHTTmCZU5J94BTnGKziW33-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/vHTTmCZU5J94BTnGKziW33-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/vHTTmCZU5J94BTnGKziW33-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/vHTTmCZU5J94BTnGKziW33-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/vHTTmCZU5J94BTnGKziW33-1200-80.jpg 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-original-mos="https://cdn.mos.cms.futurecdn.net/vHTTmCZU5J94BTnGKziW33.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/vHTTmCZU5J94BTnGKziW33.jpg"></picture></p></div><figcaption itemprop="caption description"><span itemprop="copyrightHolder">(Image credit: J Studios / Getty Images)</span></figcaption></figure><p>Infomaniak's approach to metadata has also received backlash from others in the industry. A <a data-analytics-id="inline-link" href="https://www.linkedin.com/posts/boris-siegenthaler-7808431_infomaniak-tacle-proton-et-introduira-le-activity-7331320049992888320-ha3X/?rcm=ACoAACTv6S0B-_lKpcaXH3R2J0865yGHmmFsV20" target="_blank" rel="nofollow" data-url="https://www.linkedin.com/posts/boris-siegenthaler-7808431_infomaniak-tacle-proton-et-introduira-le-activity-7331320049992888320-ha3X/?rcm=ACoAACTv6S0B-_lKpcaXH3R2J0865yGHmmFsV20" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">LinkedIn post by founder Boris Siegenthaler</a> saw disagreement with Infomaniak's position in the comments.</p><p>A journalist said he was concerned about metadata, should it be collected, falling into the wrong hands. He claimed certain people and sources would be at risk and communication methods must preserve their safety.</p><p>One comment argued that "metadata protection is important to avoid profiling," and another said a middle ground between Proton and Infomaniak's position was needed.</p><p>Infomanaik has said it's moving to an encrypted email service. Jacobsen said the content of emails will be protected, "but without anonymity." Infomaniak's own service would therefore appear to be affected by the law change and they'd be required to collect and store the metadata of its users and their emails.</p><p>The Swiss government's consultation on the proposed law change ended on May 6 2025. Its findings are still not known, but we will monitor its progress closely.</p><p><em>Quotes in this article have been translated from French to English.</em></p><div id="slice-container-freeText-vcmpQU2qC7th8tP5LMQF6K-6O8wESLvgW7uppVg8yrcy8O70avqNoWI"><p>We test and review VPN services in the context of legal recreational uses. For example: 1. Accessing a service from another country (subject to the terms and conditions of that service). 2. Protecting your online security and strengthening your online privacy when abroad. We do not support or condone the illegal or malicious use of VPN services. Consuming pirated content that is paid-for is neither endorsed nor approved by Future Publishing.</p></div>
</div>



<!-- Drop in a standard article here maybe? -->

<div id="slice-container-authorBio-vcmpQU2qC7th8tP5LMQF6K"><p>George is a Staff Writer at Tom's Guide, covering&nbsp;VPN, privacy, and cybersecurity news. He is especially interested in digital rights and censorship, and its interplay with politics. Outside of work, George is passionate about music, Star Wars, and Karate.</p></div>
</section>




</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Czech Republic: Petition for open source in public administration (177 pts)]]></title>
            <link>https://portal.gov.cz/e-petice/1205-petice-za-povinne-zverejneni-zdrojovych-kodu-softwaru-pouzitych-ve-verejne-sprave</link>
            <guid>44199299</guid>
            <pubDate>Fri, 06 Jun 2025 09:57:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://portal.gov.cz/e-petice/1205-petice-za-povinne-zverejneni-zdrojovych-kodu-softwaru-pouzitych-ve-verejne-sprave">https://portal.gov.cz/e-petice/1205-petice-za-povinne-zverejneni-zdrojovych-kodu-softwaru-pouzitych-ve-verejne-sprave</a>, See on <a href="https://news.ycombinator.com/item?id=44199299">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <section>
                <div>
                    <h3>Informace</h3>

                    <ul>
                        
                        <li>
                            <a href="https://portal.gov.cz/tiraz/osobni-udaje-a-cookies">Zpracování osobních údajů a cookies</a>
                        </li>
                        
                        <li>
                            <a href="https://portal.gov.cz/tiraz/pro-media">Kontakty pro média</a>
                        </li>
                        
                        <li>
                            <a href="https://portal.gov.cz/tiraz/mapa-webu">Mapa webu </a>
                        </li>
                        
                        <li>
                            <a href="https://portal.gov.cz/tiraz/prohlaseni-o-pristupnosti">Prohlášení o přístupnosti</a>
                        </li>
                        
                        <li>
                            <a href="https://postaticfiles.z6.web.core.windows.net/PVS_prirucka_uzivatel.pdf" target="_blank">Uživatelská příručka</a>
                        </li>
                        
                    </ul>
                </div>
                <div>
                    <h3>Máte dotaz? Napište nám</h3>

                    <ul>
                        <li>
                            <a href="mailto:portalobcana@dia.gov.cz">portalobcana@dia.gov.cz</a>
                            <a title="Zkopírovat do schránky" onclick="navigator.clipboard.writeText('portalobcana@dia.gov.cz')">
                                ⧉
                            </a>
                        </li>
                    </ul>

                    <h3>Sledujte český eGovernment</h3>
                    <ul>
                        <li>
                            <a href="https://www.facebook.com/cz.eGovernment/" onclick="window.open(this.href); return false">
                                <span></span>
                            </a>
                        </li>
                        <li>
                            <a href="https://x.com/gov_cz" onclick="window.open(this.href); return false">
                                <svg style="vertical-align: middle" width="23.25" height="21.25" viewBox="0 0 1200 1227" fill="none" xmlns="http://www.w3.org/2000/svg">
                                    <path d="M714.163 519.284L1160.89 0H1055.03L667.137 450.887L357.328 0H0L468.492 681.821L0 1226.37H105.866L515.491 750.218L842.672 1226.37H1200L714.137 519.284H714.163ZM569.165 687.828L521.697 619.934L144.011 79.6944H306.615L611.412 515.685L658.88 583.579L1055.08 1150.3H892.476L569.165 687.854V687.828Z" fill="white"></path>
                                </svg>
                            </a>
                        </li>
                        <li>
                            <a href="https://www.youtube.com/channel/UCCEISjcwdeqghfa4Z2mWMOg" onclick="window.open(this.href); return false">
                                <span></span>
                            </a>
                        </li>
                    </ul>

                </div>
                <div>
                    <h3>Portál veřejné správy vám přináší</h3>
                    <ul>
                        <li>
                            <a href="https://www.dia.gov.cz/" onclick="window.open(this.href); return false">
                                <img src="https://portal.gov.cz/static/images/logo-dia.svg" alt="Logo DIA">
                            </a>
                        </li>
                        <li>
                            <a href="https://nakit.cz/" onclick="window.open(this.href); return false">
                                <img src="https://portal.gov.cz/static/images/logo-nakit.svg" alt="Logo NAKIT">
                            </a>
                        </li>
                    </ul>
                    <br>
                    <ul>
                        <li>
                            <a href="https://european-union.europa.eu/live-work-study/funding-grants-subsidies_cs" onclick="window.open(this.href); return false">
                                <img src="https://portal.gov.cz/static/images/logo-eu.svg" alt="Logo NextGeneration EU">
                            </a>
                        </li>
                        <li>
                            <a href="https://www.planobnovycr.cz/" onclick="window.open(this.href); return false">
                                <img src="https://portal.gov.cz/static/images/logo-npo.svg" width="120" alt="Logo Národní plán obnovy">
                            </a>
                        </li>
                    </ul>

                </div>
            </section>

            <hr>

        <section>
            <p>
                2025 © Digitální a informační agentura • Informace jsou poskytovány v&nbsp;souladu se zákonem č.&nbsp;106/1999 Sb., o&nbsp;svobodném přístupu k&nbsp;informacím.
            </p>

            <p>
                Verze 4.2.200
            </p>
        </section>

        <!-- Pixel: "egov all" -->



        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Self-hosting your own media considered harmful according to YouTube (1484 pts)]]></title>
            <link>https://www.jeffgeerling.com/blog/2025/self-hosting-your-own-media-considered-harmful</link>
            <guid>44197932</guid>
            <pubDate>Fri, 06 Jun 2025 04:59:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jeffgeerling.com/blog/2025/self-hosting-your-own-media-considered-harmful">https://www.jeffgeerling.com/blog/2025/self-hosting-your-own-media-considered-harmful</a>, See on <a href="https://news.ycombinator.com/item?id=44197932">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I just received my <a href="https://github.com/geerlingguy/youtube/issues/12">second community guidelines violation</a> for my video demonstrating the use of LibreELEC on a Raspberry Pi 5, for 4K video playback.</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/community-guidelines-strike.jpg" width="700" height="469" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-876a116a-6da8-4656-bf14-863c5958ed02" data-insert-attach="{&quot;id&quot;:&quot;876a116a-6da8-4656-bf14-863c5958ed02&quot;,&quot;attributes&quot;:{&quot;alt&quot;:[&quot;alt&quot;,&quot;description&quot;],&quot;title&quot;:[&quot;title&quot;]}}" alt="Community Guidelines Strike - YouTube"></p>

<p>I purposefully avoid demonstrating any of the tools (with a suffix that rhymes with "car") that are popularly used to circumvent purchasing movie, TV, and other media content, or any tools that automatically slurp up YouTube content.</p>

<p>In fact, in my own house, for multiple decades, I've purchased physical media (CDs, DVDs, and more recently, Blu-Rays), and only have legally-acquired content on my NAS. Streaming services used to be a panacea but are now fragmented and mostly full of garbage—and lots of ads. We just wanted to be able to watch TV shows and movies without hassle (and I'm happy to pay for physical media that I want to watch).</p>

<p>But this morning, as I was finishing up work on a video about a new mini Pi cluster, I got a cheerful email from YouTube saying my video on LibreELEC on the Pi 5 was removed because it promoted:</p>

<blockquote>
  <p><strong>Dangerous or Harmful Content</strong><br>
  Content that describes how to get unauthorized or free access to audio or audiovisual content, software, subscription services, or games that usually require payment isn't allowed on YouTube.</p>
</blockquote>

<p>I never described any of that stuff, only how to self-host your own media library.</p>

<p>This wasn't my first rodeo—in October last year, I got a <a href="https://github.com/geerlingguy/youtube/issues/13">strike for showing people how to install Jellyfin</a>!</p>

<p>In <em>that</em> case, I was happy to see my appeal granted within an hour of the strike being placed on the channel. (Nevermind the fact the video had been live for <em>over two years</em> at that point, with nary a problem!)</p>

<p>So I thought, this case will be similar:</p>

<ul>
<li>The video's been up for over a year, without issue</li>
<li>The video's had over half a million views</li>
<li>The video doesn't promote or highlight any tools used to circumvent copyright, get around paid subscriptions, or reproduce any content illegally</li>
</ul>

<p>Slam-dunk, right? Well, not according to whomever reviewed my appeal. Apparently self-hosted open source media library management is harmful.</p>

<p>Who knew open source software could be so <em>subversive</em>?</p>

<h2>The video</h2>

<p>So along that theme, I've re-uploaded the video to Internet Archive, free for anyone to download and view at their leisure.</p>

<p><em>Yes, even those rebels running LibreELEC on their Raspberry Pis!</em></p>

<p>Here it is: <a href="https://archive.org/details/libreelec-raspberry-pi-5">LibreELEC on the Raspberry Pi 5 - Internet Archive</a>.</p>

<p><a href="https://archive.org/details/libreelec-raspberry-pi-5"><img src="https://www.jeffgeerling.com/sites/default/files/images/jeff-geerling-video-libreelec-pi-5.jpg" width="400" height="auto" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-906ddc3d-671e-4d19-9597-36ab08016d49" data-insert-attach="{&quot;id&quot;:&quot;906ddc3d-671e-4d19-9597-36ab08016d49&quot;,&quot;attributes&quot;:{&quot;alt&quot;:[&quot;alt&quot;,&quot;description&quot;],&quot;title&quot;:[&quot;title&quot;]}}" alt="LibreELEC on Pi 5 video thumbnail with play button"></a></p>

<p>I've also uploaded it <a href="https://www.floatplane.com/post/bNx4Mhzqvu">on Floatplane</a>, for subscribers.</p>

<h2>Alternatives</h2>

<p>I've been slowly uploading my back catalog to <a href="https://www.floatplane.com/channel/JeffGeerling/home">my channel on Floatplane</a>, though not all my content is there yet.</p>

<p>Some in the fediverse ask why I'm not on Peertube. Here's the problem (and it's not insurmountable): <em>right now</em>, there's no easy path towards sustainable content production when the audience for the content is 100x smaller, and the number of patrons/sponsors remains proportionally the same.</p>

<p>I was never able to sustain my open source work based on patronage, and content production is the same—just more expensive to maintain to any standard (each video takes between 10-300 hours to produce, and I have a family to feed, and <a href="https://www.jeffgeerling.com/tags/crohns">US health insurance companies to fund</a>).</p>

<p>YouTube was, and still is, a creative anomaly. I'm hugely thankful to my <a href="https://www.patreon.com/c/geerlingguy">Patreon</a>, <a href="https://github.com/sponsors/geerlingguy">GitHub</a>, and <a href="https://www.floatplane.com/channel/JeffGeerling/home">Floatplane</a> supporters—and I hope to have direct funding fully able to support my work someday. But until that time, YouTube's AdSense revenue and vast reach is a kind of 'golden handcuff.'</p>

<p>The handcuff has been a bit tarnished of late, however, with Google recently adding AI summaries to videos—which <em>seems</em> to indicate maybe <a href="https://www.msn.com/en-us/news/technology/google-gemini-s-ai-video-summary-implies-youtube-doesn-t-care-about-content-creators/ar-AA1rMsoy">Gemini is slurping up my content and using it in their AI models</a>?</p>

<p>Maybe the handcuffs are fools-gold, and I just don't see it yet.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Test Postgres in Python Like SQLite (142 pts)]]></title>
            <link>https://github.com/wey-gu/py-pglite</link>
            <guid>44196945</guid>
            <pubDate>Fri, 06 Jun 2025 00:56:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/wey-gu/py-pglite">https://github.com/wey-gu/py-pglite</a>, See on <a href="https://news.ycombinator.com/item?id=44196945">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">py-pglite</h2><a id="user-content-py-pglite" aria-label="Permalink: py-pglite" href="#py-pglite"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/1651790/451981298-3c6ef886-5075-4d82-a180-a6b1dafe792b.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDkyMDk3MDIsIm5iZiI6MTc0OTIwOTQwMiwicGF0aCI6Ii8xNjUxNzkwLzQ1MTk4MTI5OC0zYzZlZjg4Ni01MDc1LTRkODItYTE4MC1hNmIxZGFmZTc5MmIucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDYwNiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA2MDZUMTEzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YmI0YmFlYjMwNDEwMjQ1YTIxY2YzYmEwOTE2N2EyMjAwZmI1MGU5ZmIxNjU3NDgxNjc0MDZmNDY5N2Y2NmVmMiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.Y7StF80nv5tw4wN8UZ35RnkDmE65iPkS1MMjCQBh7Nw"><img src="https://private-user-images.githubusercontent.com/1651790/451981298-3c6ef886-5075-4d82-a180-a6b1dafe792b.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDkyMDk3MDIsIm5iZiI6MTc0OTIwOTQwMiwicGF0aCI6Ii8xNjUxNzkwLzQ1MTk4MTI5OC0zYzZlZjg4Ni01MDc1LTRkODItYTE4MC1hNmIxZGFmZTc5MmIucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDYwNiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA2MDZUMTEzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YmI0YmFlYjMwNDEwMjQ1YTIxY2YzYmEwOTE2N2EyMjAwZmI1MGU5ZmIxNjU3NDgxNjc0MDZmNDY5N2Y2NmVmMiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.Y7StF80nv5tw4wN8UZ35RnkDmE65iPkS1MMjCQBh7Nw" alt="image"></a></p>
<p dir="auto"><a href="https://github.com/wey-gu/py-pglite/actions/workflows/ci.yml"><img src="https://github.com/wey-gu/py-pglite/actions/workflows/ci.yml/badge.svg" alt="CI"></a>
<a href="https://codecov.io/gh/wey-gu/py-pglite" rel="nofollow"><img src="https://camo.githubusercontent.com/137353bbd51cf4cb94d56c7f9288a47c40c9564dd00c5901064fba896672b855/68747470733a2f2f636f6465636f762e696f2f67682f7765792d67752f70792d70676c6974652f6272616e63682f6d61696e2f67726170682f62616467652e7376673f746f6b656e3d594f55525f434f4445434f565f544f4b454e" alt="codecov" data-canonical-src="https://codecov.io/gh/wey-gu/py-pglite/branch/main/graph/badge.svg?token=YOUR_CODECOV_TOKEN"></a>
<a href="https://github.com/astral-sh/ruff"><img src="https://camo.githubusercontent.com/e9e401f08622ada6a3147f8bb1ba4ee849b38c7f123199ce3886ef05658299e6/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7374796c652d727566662d626c75653f6c6f676f3d72756666266c6f676f436f6c6f723d7768697465" alt="Ruff" data-canonical-src="https://img.shields.io/badge/style-ruff-blue?logo=ruff&amp;logoColor=white"></a>
<a href="https://mypy.readthedocs.io/en/stable/introduction.html" rel="nofollow"><img src="https://camo.githubusercontent.com/c85115d7ffc8f4ff2c49c03cb9d2973ddc81a476bb4c06c9427848f918916f03/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f747970655f636865636b65642d6d7970792d696e666f726d6174696f6e616c2e737667" alt="MyPy" data-canonical-src="https://img.shields.io/badge/type_checked-mypy-informational.svg"></a>
<a href="https://badge.fury.io/py/py-pglite" rel="nofollow"><img src="https://camo.githubusercontent.com/6d08914750fc8f1671c104bb431d8bd17d8b46cb54d00364ca0e7b7365267ef7/68747470733a2f2f62616467652e667572792e696f2f70792f70792d70676c6974652e737667" alt="PyPI version" data-canonical-src="https://badge.fury.io/py/py-pglite.svg"></a>
<a href="https://pypi.org/project/py-pglite/" rel="nofollow"><img src="https://camo.githubusercontent.com/337ddbbe5fa88fd4643f6421c27687d2d1c74a97536f3b0b12a872bdcbca29d2/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f70792d70676c6974652e737667" alt="Python" data-canonical-src="https://img.shields.io/pypi/pyversions/py-pglite.svg"></a>
<a href="https://github.com/wey-gu/py-pglite/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/d3171074e95a4a5adb658c6465e6162b2c23a96a3a1408a73b0a5fb93d1db34d/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f70792d70676c6974652e737667" alt="License" data-canonical-src="https://img.shields.io/pypi/l/py-pglite.svg"></a></p>
<p dir="auto">A Python testing library that provides seamless integration between <a href="https://github.com/electric-sql/pglite">PGlite</a> and Python test suites. Get the full power of PostgreSQL in your tests without the overhead of a full PostgreSQL installation.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🎯 Why py-pglite?</h2><a id="user-content--why-py-pglite" aria-label="Permalink: 🎯 Why py-pglite?" href="#-why-py-pglite"></a></p>
<ul dir="auto">
<li><strong>⚡ Blazing Fast</strong>: In-memory PostgreSQL for ultra-quick test runs</li>
<li><strong>🛠️ Effortless Setup</strong>: No PostgreSQL install needed—just Node.js(I know)!</li>
<li><strong>🐍 Pythonic</strong>: Native support for SQLAlchemy &amp; SQLModel in your tests</li>
<li><strong>🧊 Fully Isolated</strong>: Every test module gets its own fresh database</li>
<li><strong>🦾 100% Compatible</strong>: True PostgreSQL features via <a href="https://pglite.dev/" rel="nofollow">PGlite</a></li>
<li><strong>🧩 Pytest Plug-and-Play</strong>: Ready-to-use fixtures for instant productivity</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">📦 Installation</h2><a id="user-content--installation" aria-label="Permalink: 📦 Installation" href="#-installation"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Basic Installation</h3><a id="user-content-basic-installation" aria-label="Permalink: Basic Installation" href="#basic-installation"></a></p>

<p dir="auto"><h3 tabindex="-1" dir="auto">With Optional Dependencies</h3><a id="user-content-with-optional-dependencies" aria-label="Permalink: With Optional Dependencies" href="#with-optional-dependencies"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# For SQLModel support
pip install &quot;py-pglite[sqlmodel]&quot;

# For FastAPI integration
pip install &quot;py-pglite[fastapi]&quot;

# For development
pip install &quot;py-pglite[dev]&quot;"><pre><span><span>#</span> For SQLModel support</span>
pip install <span><span>"</span>py-pglite[sqlmodel]<span>"</span></span>

<span><span>#</span> For FastAPI integration</span>
pip install <span><span>"</span>py-pglite[fastapi]<span>"</span></span>

<span><span>#</span> For development</span>
pip install <span><span>"</span>py-pglite[dev]<span>"</span></span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Requirements</h3><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<ul dir="auto">
<li><strong>Python</strong>: 3.10+</li>
<li><strong>Node.js</strong>: 18+ (for PGlite)</li>
<li><strong>SQLAlchemy</strong>: 2.0+</li>
</ul>
<p dir="auto">The library automatically manages PGlite npm dependencies.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🚀 Quick Start</h2><a id="user-content--quick-start" aria-label="Permalink: 🚀 Quick Start" href="#-quick-start"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Basic Usage with Pytest</h3><a id="user-content-basic-usage-with-pytest" aria-label="Permalink: Basic Usage with Pytest" href="#basic-usage-with-pytest"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="import pytest
from sqlmodel import Session, SQLModel, Field, select
from py_pglite import pglite_session

# Your models
class User(SQLModel, table=True):
    id: int | None = Field(default=None, primary_key=True)
    name: str
    email: str

# Test with automatic PGlite management
def test_user_creation(pglite_session: Session):
    user = User(name=&quot;Alice&quot;, email=&quot;alice@example.com&quot;)
    pglite_session.add(user)
    pglite_session.commit()
    
    # Query back
    users = pglite_session.exec(select(User)).all()
    assert len(users) == 1
    assert users[0].name == &quot;Alice&quot;"><pre><span>import</span> <span>pytest</span>
<span>from</span> <span>sqlmodel</span> <span>import</span> <span>Session</span>, <span>SQLModel</span>, <span>Field</span>, <span>select</span>
<span>from</span> <span>py_pglite</span> <span>import</span> <span>pglite_session</span>

<span># Your models</span>
<span>class</span> <span>User</span>(<span>SQLModel</span>, <span>table</span><span>=</span><span>True</span>):
    <span>id</span>: <span>int</span> <span>|</span> <span>None</span> <span>=</span> <span>Field</span>(<span>default</span><span>=</span><span>None</span>, <span>primary_key</span><span>=</span><span>True</span>)
    <span>name</span>: <span>str</span>
    <span>email</span>: <span>str</span>

<span># Test with automatic PGlite management</span>
<span>def</span> <span>test_user_creation</span>(<span>pglite_session</span>: <span>Session</span>):
    <span>user</span> <span>=</span> <span>User</span>(<span>name</span><span>=</span><span>"Alice"</span>, <span>email</span><span>=</span><span>"alice@example.com"</span>)
    <span>pglite_session</span>.<span>add</span>(<span>user</span>)
    <span>pglite_session</span>.<span>commit</span>()
    
    <span># Query back</span>
    <span>users</span> <span>=</span> <span>pglite_session</span>.<span>exec</span>(<span>select</span>(<span>User</span>)).<span>all</span>()
    <span>assert</span> <span>len</span>(<span>users</span>) <span>==</span> <span>1</span>
    <span>assert</span> <span>users</span>[<span>0</span>].<span>name</span> <span>==</span> <span>"Alice"</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Manual Management</h3><a id="user-content-manual-management" aria-label="Permalink: Manual Management" href="#manual-management"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="from py_pglite import PGliteManager, PGliteConfig

# Custom configuration
config = PGliteConfig(
    timeout=30,
    cleanup_on_exit=True,
    log_level=&quot;DEBUG&quot;
)

# Manual management
with PGliteManager(config) as manager:
    engine = manager.get_engine()
    SQLModel.metadata.create_all(engine)
    
    with Session(engine) as session:
        # Your database operations here
        pass"><pre><span>from</span> <span>py_pglite</span> <span>import</span> <span>PGliteManager</span>, <span>PGliteConfig</span>

<span># Custom configuration</span>
<span>config</span> <span>=</span> <span>PGliteConfig</span>(
    <span>timeout</span><span>=</span><span>30</span>,
    <span>cleanup_on_exit</span><span>=</span><span>True</span>,
    <span>log_level</span><span>=</span><span>"DEBUG"</span>
)

<span># Manual management</span>
<span>with</span> <span>PGliteManager</span>(<span>config</span>) <span>as</span> <span>manager</span>:
    <span>engine</span> <span>=</span> <span>manager</span>.<span>get_engine</span>()
    <span>SQLModel</span>.<span>metadata</span>.<span>create_all</span>(<span>engine</span>)
    
    <span>with</span> <span>Session</span>(<span>engine</span>) <span>as</span> <span>session</span>:
        <span># Your database operations here</span>
        <span>pass</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔧 Features</h2><a id="user-content--features" aria-label="Permalink: 🔧 Features" href="#-features"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Pytest Fixtures</h3><a id="user-content-pytest-fixtures" aria-label="Permalink: Pytest Fixtures" href="#pytest-fixtures"></a></p>
<ul dir="auto">
<li><strong><code>pglite_engine</code></strong>: SQLAlchemy engine connected to PGlite</li>
<li><strong><code>pglite_session</code></strong>: Database session with automatic cleanup</li>
<li><strong><code>pglite_manager</code></strong>: Direct access to PGlite process management</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Automatic Management</h3><a id="user-content-automatic-management" aria-label="Permalink: Automatic Management" href="#automatic-management"></a></p>
<ul dir="auto">
<li>✅ Process lifecycle management</li>
<li>✅ Socket cleanup and health checks</li>
<li>✅ Graceful shutdown and error handling</li>
<li>✅ Per-test isolation with automatic cleanup</li>
<li>✅ Node.js dependency management</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Configuration</h3><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="from py_pglite import PGliteConfig

config = PGliteConfig(
    timeout=30,               # Startup timeout in seconds
    cleanup_on_exit=True,     # Auto cleanup on exit
    log_level=&quot;INFO&quot;,         # Logging level (DEBUG/INFO/WARNING/ERROR)
    socket_path=&quot;/tmp/.s.PGSQL.5432&quot;,  # Custom socket path
    work_dir=None,            # Working directory (None = temp dir)
    node_modules_check=True,  # Verify node_modules exists
    auto_install_deps=True,   # Auto-install npm dependencies
)"><pre><span>from</span> <span>py_pglite</span> <span>import</span> <span>PGliteConfig</span>

<span>config</span> <span>=</span> <span>PGliteConfig</span>(
    <span>timeout</span><span>=</span><span>30</span>,               <span># Startup timeout in seconds</span>
    <span>cleanup_on_exit</span><span>=</span><span>True</span>,     <span># Auto cleanup on exit</span>
    <span>log_level</span><span>=</span><span>"INFO"</span>,         <span># Logging level (DEBUG/INFO/WARNING/ERROR)</span>
    <span>socket_path</span><span>=</span><span>"/tmp/.s.PGSQL.5432"</span>,  <span># Custom socket path</span>
    <span>work_dir</span><span>=</span><span>None</span>,            <span># Working directory (None = temp dir)</span>
    <span>node_modules_check</span><span>=</span><span>True</span>,  <span># Verify node_modules exists</span>
    <span>auto_install_deps</span><span>=</span><span>True</span>,   <span># Auto-install npm dependencies</span>
)</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Utility Functions</h3><a id="user-content-utility-functions" aria-label="Permalink: Utility Functions" href="#utility-functions"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="from py_pglite import utils

# Database cleanup utilities
utils.clean_database_data(engine)                    # Clean all data
utils.clean_database_data(engine, exclude_tables=[&quot;users&quot;])  # Exclude tables
utils.reset_sequences(engine)                        # Reset auto-increment sequences
utils.verify_database_empty(engine)                  # Check if database is empty

# Schema operations
utils.create_test_schema(engine, &quot;test_schema&quot;)      # Create test schema
utils.drop_test_schema(engine, &quot;test_schema&quot;)        # Drop test schema

# Get table statistics
row_counts = utils.get_table_row_counts(engine)      # Dict of table row counts"><pre><span>from</span> <span>py_pglite</span> <span>import</span> <span>utils</span>

<span># Database cleanup utilities</span>
<span>utils</span>.<span>clean_database_data</span>(<span>engine</span>)                    <span># Clean all data</span>
<span>utils</span>.<span>clean_database_data</span>(<span>engine</span>, <span>exclude_tables</span><span>=</span>[<span>"users"</span>])  <span># Exclude tables</span>
<span>utils</span>.<span>reset_sequences</span>(<span>engine</span>)                        <span># Reset auto-increment sequences</span>
<span>utils</span>.<span>verify_database_empty</span>(<span>engine</span>)                  <span># Check if database is empty</span>

<span># Schema operations</span>
<span>utils</span>.<span>create_test_schema</span>(<span>engine</span>, <span>"test_schema"</span>)      <span># Create test schema</span>
<span>utils</span>.<span>drop_test_schema</span>(<span>engine</span>, <span>"test_schema"</span>)        <span># Drop test schema</span>

<span># Get table statistics</span>
<span>row_counts</span> <span>=</span> <span>utils</span>.<span>get_table_row_counts</span>(<span>engine</span>)      <span># Dict of table row counts</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">📚 Examples</h2><a id="user-content--examples" aria-label="Permalink: 📚 Examples" href="#-examples"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">FastAPI Integration</h3><a id="user-content-fastapi-integration" aria-label="Permalink: FastAPI Integration" href="#fastapi-integration"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="from fastapi import Depends, FastAPI
from fastapi.testclient import TestClient
from sqlmodel import Session
from py_pglite import pglite_engine

app = FastAPI()

def get_db():
    # Production database dependency
    pass

@app.post(&quot;/users/&quot;)
def create_user(user_data: dict, db: Session = Depends(get_db)):
    # Your endpoint logic
    pass

# Test with PGlite
def test_create_user_endpoint(pglite_engine):
    # Override database dependency
    def override_get_db():
        with Session(pglite_engine) as session:
            yield session
    
    app.dependency_overrides[get_db] = override_get_db
    
    with TestClient(app) as client:
        response = client.post(&quot;/users/&quot;, json={&quot;name&quot;: &quot;Bob&quot;})
        assert response.status_code == 200"><pre><span>from</span> <span>fastapi</span> <span>import</span> <span>Depends</span>, <span>FastAPI</span>
<span>from</span> <span>fastapi</span>.<span>testclient</span> <span>import</span> <span>TestClient</span>
<span>from</span> <span>sqlmodel</span> <span>import</span> <span>Session</span>
<span>from</span> <span>py_pglite</span> <span>import</span> <span>pglite_engine</span>

<span>app</span> <span>=</span> <span>FastAPI</span>()

<span>def</span> <span>get_db</span>():
    <span># Production database dependency</span>
    <span>pass</span>

<span>@<span>app</span>.<span>post</span>(<span>"/users/"</span>)</span>
<span>def</span> <span>create_user</span>(<span>user_data</span>: <span>dict</span>, <span>db</span>: <span>Session</span> <span>=</span> <span>Depends</span>(<span>get_db</span>)):
    <span># Your endpoint logic</span>
    <span>pass</span>

<span># Test with PGlite</span>
<span>def</span> <span>test_create_user_endpoint</span>(<span>pglite_engine</span>):
    <span># Override database dependency</span>
    <span>def</span> <span>override_get_db</span>():
        <span>with</span> <span>Session</span>(<span>pglite_engine</span>) <span>as</span> <span>session</span>:
            <span>yield</span> <span>session</span>
    
    <span>app</span>.<span>dependency_overrides</span>[<span>get_db</span>] <span>=</span> <span>override_get_db</span>
    
    <span>with</span> <span>TestClient</span>(<span>app</span>) <span>as</span> <span>client</span>:
        <span>response</span> <span>=</span> <span>client</span>.<span>post</span>(<span>"/users/"</span>, <span>json</span><span>=</span>{<span>"name"</span>: <span>"Bob"</span>})
        <span>assert</span> <span>response</span>.<span>status_code</span> <span>==</span> <span>200</span></pre></div>
<p dir="auto">See also <a href="https://github.com/wey-gu/py-pglite/blob/main/examples/test_fastapi_auth_example.py">examples/test_fastapi_auth_example.py</a> for an example of how to use py-pglite with FastAPI e2e test that includes authentication.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Complex Testing Scenario</h3><a id="user-content-complex-testing-scenario" aria-label="Permalink: Complex Testing Scenario" href="#complex-testing-scenario"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="def test_complex_operations(pglite_session: Session):
    # Create related data
    user = User(name=&quot;Alice&quot;, email=&quot;alice@example.com&quot;)
    pglite_session.add(user)
    pglite_session.commit()
    pglite_session.refresh(user)
    
    # Create dependent records
    orders = [
        Order(user_id=user.id, amount=100.0),
        Order(user_id=user.id, amount=250.0),
    ]
    pglite_session.add_all(orders)
    pglite_session.commit()
    
    # Complex query with joins
    result = pglite_session.exec(
        select(User.name, func.sum(Order.amount))
        .join(Order)
        .group_by(User.name)
    ).first()
    
    assert result[0] == &quot;Alice&quot;
    assert result[1] == 350.0"><pre><span>def</span> <span>test_complex_operations</span>(<span>pglite_session</span>: <span>Session</span>):
    <span># Create related data</span>
    <span>user</span> <span>=</span> <span>User</span>(<span>name</span><span>=</span><span>"Alice"</span>, <span>email</span><span>=</span><span>"alice@example.com"</span>)
    <span>pglite_session</span>.<span>add</span>(<span>user</span>)
    <span>pglite_session</span>.<span>commit</span>()
    <span>pglite_session</span>.<span>refresh</span>(<span>user</span>)
    
    <span># Create dependent records</span>
    <span>orders</span> <span>=</span> [
        <span>Order</span>(<span>user_id</span><span>=</span><span>user</span>.<span>id</span>, <span>amount</span><span>=</span><span>100.0</span>),
        <span>Order</span>(<span>user_id</span><span>=</span><span>user</span>.<span>id</span>, <span>amount</span><span>=</span><span>250.0</span>),
    ]
    <span>pglite_session</span>.<span>add_all</span>(<span>orders</span>)
    <span>pglite_session</span>.<span>commit</span>()
    
    <span># Complex query with joins</span>
    <span>result</span> <span>=</span> <span>pglite_session</span>.<span>exec</span>(
        <span>select</span>(<span>User</span>.<span>name</span>, <span>func</span>.<span>sum</span>(<span>Order</span>.<span>amount</span>))
        .<span>join</span>(<span>Order</span>)
        .<span>group_by</span>(<span>User</span>.<span>name</span>)
    ).<span>first</span>()
    
    <span>assert</span> <span>result</span>[<span>0</span>] <span>==</span> <span>"Alice"</span>
    <span>assert</span> <span>result</span>[<span>1</span>] <span>==</span> <span>350.0</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">🤝 Contributing</h2><a id="user-content--contributing" aria-label="Permalink: 🤝 Contributing" href="#-contributing"></a></p>
<p dir="auto">Contributions welcome! Please read our <a href="https://github.com/wey-gu/py-pglite/blob/main/CONTRIBUTING.md">Contributing Guide</a>.</p>
<ol dir="auto">
<li>Fork the repository</li>
<li>Create a feature branch</li>
<li>Make your changes</li>
<li>Add tests for new functionality</li>
<li>Run the development workflow: <code>python hacking.py</code></li>
<li>Submit a pull request</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">📄 License</h2><a id="user-content--license" aria-label="Permalink: 📄 License" href="#-license"></a></p>
<p dir="auto">Apache 2.0 License - see <a href="https://github.com/wey-gu/py-pglite/blob/main/LICENSE">LICENSE</a> file.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🙏 Acknowledgments</h2><a id="user-content--acknowledgments" aria-label="Permalink: 🙏 Acknowledgments" href="#-acknowledgments"></a></p>
<ul dir="auto">
<li><a href="https://github.com/electric-sql/pglite">PGlite</a> - The amazing in-memory PostgreSQL</li>
<li><a href="https://www.sqlalchemy.org/" rel="nofollow">SQLAlchemy</a> - Python SQL toolkit</li>
<li><a href="https://sqlmodel.tiangolo.com/" rel="nofollow">SQLModel</a> - Modern Python SQL toolkit</li>
<li><a href="https://pytest.org/" rel="nofollow">Pytest</a> - Testing framework</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Best Practices</h2><a id="user-content-best-practices" aria-label="Permalink: Best Practices" href="#best-practices"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Multiple Database Sessions</h3><a id="user-content-multiple-database-sessions" aria-label="Permalink: Multiple Database Sessions" href="#multiple-database-sessions"></a></p>
<p dir="auto">For multiple database connections, use <strong>multiple sessions with the same engine</strong> rather than multiple engines:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# ✅ Recommended: Multiple sessions with same engine
with PGliteManager() as manager:
    engine = manager.get_engine()
    
    # Multiple sessions work perfectly
    session1 = Session(engine)
    session2 = Session(engine)
    session3 = Session(engine)

# ❌ Not recommended: Multiple engines from same manager
with PGliteManager() as manager:
    engine1 = manager.get_engine()  # Can cause connection conflicts
    engine2 = manager.get_engine()  # when used simultaneously"><pre><span># ✅ Recommended: Multiple sessions with same engine</span>
<span>with</span> <span>PGliteManager</span>() <span>as</span> <span>manager</span>:
    <span>engine</span> <span>=</span> <span>manager</span>.<span>get_engine</span>()
    
    <span># Multiple sessions work perfectly</span>
    <span>session1</span> <span>=</span> <span>Session</span>(<span>engine</span>)
    <span>session2</span> <span>=</span> <span>Session</span>(<span>engine</span>)
    <span>session3</span> <span>=</span> <span>Session</span>(<span>engine</span>)

<span># ❌ Not recommended: Multiple engines from same manager</span>
<span>with</span> <span>PGliteManager</span>() <span>as</span> <span>manager</span>:
    <span>engine1</span> <span>=</span> <span>manager</span>.<span>get_engine</span>()  <span># Can cause connection conflicts</span>
    <span>engine2</span> <span>=</span> <span>manager</span>.<span>get_engine</span>()  <span># when used simultaneously</span></pre></div>
<p dir="auto"><strong>Why?</strong> Creating multiple SQLAlchemy engines from the same PGlite manager can cause connection pool conflicts since they all connect to the same Unix socket.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Performance Tips</h3><a id="user-content-performance-tips" aria-label="Permalink: Performance Tips" href="#performance-tips"></a></p>
<ul dir="auto">
<li>Use <code>pglite_session</code> fixture for automatic cleanup between tests</li>
<li>Use <code>pglite_engine</code> fixture when you need direct engine access</li>
<li>Use utility functions for efficient database operations</li>
<li>Consider custom configurations for specific test requirements</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Testing Patterns</h3><a id="user-content-testing-patterns" aria-label="Permalink: Testing Patterns" href="#testing-patterns"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Pattern 1: Simple CRUD testing
def test_user_crud(pglite_session):
    # Create
    user = User(name=&quot;Test&quot;, email=&quot;test@example.com&quot;)
    pglite_session.add(user)
    pglite_session.commit()
    
    # Read
    found_user = pglite_session.get(User, user.id)
    assert found_user.name == &quot;Test&quot;
    
    # Update
    found_user.name = &quot;Updated&quot;
    pglite_session.commit()
    
    # Delete
    pglite_session.delete(found_user)
    pglite_session.commit()

# Pattern 2: Custom cleanup
def test_with_custom_cleanup(pglite_engine):
    SQLModel.metadata.create_all(pglite_engine)
    
    with Session(pglite_engine) as session:
        # Your test logic
        pass
    
    # Custom cleanup if needed
    utils.clean_database_data(pglite_engine)"><pre><span># Pattern 1: Simple CRUD testing</span>
<span>def</span> <span>test_user_crud</span>(<span>pglite_session</span>):
    <span># Create</span>
    <span>user</span> <span>=</span> <span>User</span>(<span>name</span><span>=</span><span>"Test"</span>, <span>email</span><span>=</span><span>"test@example.com"</span>)
    <span>pglite_session</span>.<span>add</span>(<span>user</span>)
    <span>pglite_session</span>.<span>commit</span>()
    
    <span># Read</span>
    <span>found_user</span> <span>=</span> <span>pglite_session</span>.<span>get</span>(<span>User</span>, <span>user</span>.<span>id</span>)
    <span>assert</span> <span>found_user</span>.<span>name</span> <span>==</span> <span>"Test"</span>
    
    <span># Update</span>
    <span>found_user</span>.<span>name</span> <span>=</span> <span>"Updated"</span>
    <span>pglite_session</span>.<span>commit</span>()
    
    <span># Delete</span>
    <span>pglite_session</span>.<span>delete</span>(<span>found_user</span>)
    <span>pglite_session</span>.<span>commit</span>()

<span># Pattern 2: Custom cleanup</span>
<span>def</span> <span>test_with_custom_cleanup</span>(<span>pglite_engine</span>):
    <span>SQLModel</span>.<span>metadata</span>.<span>create_all</span>(<span>pglite_engine</span>)
    
    <span>with</span> <span>Session</span>(<span>pglite_engine</span>) <span>as</span> <span>session</span>:
        <span># Your test logic</span>
        <span>pass</span>
    
    <span># Custom cleanup if needed</span>
    <span>utils</span>.<span>clean_database_data</span>(<span>pglite_engine</span>)</pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How we’re responding to The NYT’s data demands in order to protect user privacy (263 pts)]]></title>
            <link>https://openai.com/index/response-to-nyt-data-demands/</link>
            <guid>44196850</guid>
            <pubDate>Fri, 06 Jun 2025 00:35:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/index/response-to-nyt-data-demands/">https://openai.com/index/response-to-nyt-data-demands/</a>, See on <a href="https://news.ycombinator.com/item?id=44196850">Hacker News</a></p>
Couldn't get https://openai.com/index/response-to-nyt-data-demands/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Anthropic co-founder on cutting access to Windsurf (107 pts)]]></title>
            <link>https://techcrunch.com/2025/06/05/anthropic-co-founder-on-cutting-access-to-windsurf-it-would-be-odd-for-us-to-sell-claude-to-openai/</link>
            <guid>44196807</guid>
            <pubDate>Fri, 06 Jun 2025 00:24:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2025/06/05/anthropic-co-founder-on-cutting-access-to-windsurf-it-would-be-odd-for-us-to-sell-claude-to-openai/">https://techcrunch.com/2025/06/05/anthropic-co-founder-on-cutting-access-to-windsurf-it-would-be-odd-for-us-to-sell-claude-to-openai/</a>, See on <a href="https://news.ycombinator.com/item?id=44196807">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p id="speakable-summary">Anthropic co-founder and Chief Science Officer Jared Kaplan said his company cut Windsurf’s direct access to Anthropic’s Claude AI models largely because of rumors and reports that OpenAI, its largest competitor, is acquiring the AI coding assistant.</p>

<p>“We really are just trying to enable our customers who are going to sustainably be working with us in the future,” said Kaplan during an onstage interview Thursday with TechCrunch at <a href="https://techcrunch.com/events/tc-sessions-ai/">TC Sessions: AI 2025</a>.</p>







<p>“I think it would be odd for us to be selling Claude to OpenAI,” Kaplan said.</p>

<p>The comment comes just a few weeks after Bloomberg reported that <a href="https://www.bloomberg.com/news/articles/2025-05-06/openai-reaches-agreement-to-buy-startup-windsurf-for-3-billion" target="_blank" rel="noreferrer noopener nofollow">OpenAI was acquiring Windsurf for $3 billion</a>. Earlier this week, Windsurf said that <a href="https://techcrunch.com/2025/06/03/windsurf-says-anthropic-is-limiting-its-direct-access-to-claude-ai-models/">Anthropic cut its direct access to Claude 3.5 Sonnet and Claude 3.7 Sonnet</a>, two of the more popular AI models for coding, forcing the startup to find third-party computing providers on relatively short notice. Windsurf said it was disappointed in Anthropic’s decision and that it might cause short-term instability for users trying to access Claude via Windsurf.</p>

<p>Windsurf declined to comment on Kaplan’s remarks, and an OpenAI spokesperson did not immediately respond to TechCrunch’s request. The companies have not confirmed the acquisition rumors.</p>

<p>Part of the reason Anthropic cut Windsurf’s access to Claude, according to Kaplan, is because the company is quite computing-constrained today. Anthropic would like to reserve its computing for what Kaplan characterized as “lasting partnerships.”</p>

<p>However, Kaplan said the company hopes to greatly increase the availability of models it can offer users and developers in the coming months. He added that Anthropic has just started to unlock capacity on a new computing cluster from its partner, Amazon, which he says is “really big and continues to scale.”</p>


<p>As Anthropic pulls away from Windsurf, Kaplan said he’s collaborating with other customers building AI coding tools, such as Cursor — a company Kaplan said Anthropic expects to work with for a long time. Kaplan rejected the idea that Anthropic was in competition with companies like Cursor, which is developing its own AI models.</p>

<p>Meanwhile, Kaplan says Anthropic is increasingly focused on developing its own agentic coding products, such as Claude Code, rather than AI chatbot experiences. While companies like OpenAI, Google, and Meta are competing for the most popular AI chatbot platform, Kaplan said the chatbot paradigm was limiting due to its static nature, and that AI agents would in the long run be much more helpful for users.</p>


</div><div>
	
	
	
	

	
<div>
	<p>
		Maxwell Zeff is a senior reporter at TechCrunch specializing in AI and emerging technologies. Previously with Gizmodo, Bloomberg, and MSNBC, Zeff has covered the rise of AI and the Silicon Valley Bank crisis. He is based in San Francisco. When not reporting, he can be found hiking, biking, and exploring the Bay Area’s food scene.	</p>
</div>


	
	<p>
		<a data-ctatext="View Bio" data-destinationlink="https://techcrunch.com/author/maxwell-zeff/" data-event="button" href="https://techcrunch.com/author/maxwell-zeff/">View Bio <svg style="width: 1em;" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24"><path fill="var(--c-svg, currentColor)" d="M16.5 12 9 19.5l-1.05-1.05L14.4 12 7.95 5.55 9 4.5z"></path></svg></a>
	</p>
	
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I do not remember my life and it's fine (269 pts)]]></title>
            <link>https://aethermug.com/posts/i-do-not-remember-my-life-and-it-s-fine</link>
            <guid>44196576</guid>
            <pubDate>Thu, 05 Jun 2025 23:26:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aethermug.com/posts/i-do-not-remember-my-life-and-it-s-fine">https://aethermug.com/posts/i-do-not-remember-my-life-and-it-s-fine</a>, See on <a href="https://news.ycombinator.com/item?id=44196576">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main components="[object Object]"><p><em>This post is part of the <a href="https://aethermug.com/posts/a-list-of-introspective-descriptions">List of Introspective Descriptions</a>.</em></p>

<p>I've <a href="https://aethermug.com/posts/aphantasia">written</a> <a href="https://aethermug.com/posts/aphantasia-is-no-creativity-killer">about</a> <a href="https://aethermug.com/posts/new-aphantasia-article-on-nautilus">aphantasia</a> <a href="https://aethermug.com/posts/reading-blood-meridian-with-aphantasia">several</a> <a href="https://aethermug.com/posts/normality-and-surprise-in-an-image-free-mind">times</a> <a href="https://aethermug.com/posts/an-aphantasic-s-observations-on-the-imagination-of-shapes">before</a> on this blog, and many people have shown interest in the topic. Most readers are simply curious when I say that I can't form any kind of image, sound, or other sensation in my mind. Occasionally, someone shows pity or commiseration towards me, as if I were in constant, daily suffering from a crippling disability. Nothing could be further from the truth, of course. I've been successful at most of what I've tried to accomplish in my life until now, and never had to battle with a sense of being disadvantaged. On top of that, even aphantasia experts generally agree that <a href="https://onlinelibrary.wiley.com/doi/10.1111/sjop.12887" rel="nofollow noopener noreferrer" target="_blank">it is not a disorder</a>.</p>
<p>That is not to say that I feel as capable as anyone else at everything. In particular, there is an area in which I <em>do</em> feel—all too well—that I am weaker than most: my memory of past episodes.</p>
<p>For obvious reasons, my recollections lack a visual component, but that is only part of the story. I seem to have an extremely poor ability to "relive" past events mentally. In fact, my condition is accurately described as</p>
<blockquote>
<p>a mnemonic syndrome that is confined to an inability to mentally travel backwards in time in the absence of detectable neuropathology or significant daily handicap,</p>
</blockquote>
<p>which is the definition of a trait called SDAM, for <em>Severely Deficient Autobiographical Memory</em>.</p>
<p>SDAM was only <a href="https://www.sciencedirect.com/science/article/pii/S002839321500158X" rel="nofollow noopener noreferrer" target="_blank">discovered</a> in 2015, and it is still poorly understood. Yet there is mounting evidence that it has deep links with aphantasia: about half of the people with SDAM also <a href="https://www.youtube.com/watch?v=Zvam_uoBSLc" rel="nofollow noopener noreferrer" target="_blank">report</a> having aphantasia, and many people with aphantasia claim to have difficulties with recalling past episodes from their own lives. For these reasons, I believe I have SDAM or something closely resembling it.</p>
<p>What does that imply, though? That is what I aim to clarify with this post. I always find it very difficult to tell how much of my subjective experience is rare and how much of it is normal for most of humanity. I've never swapped brains with anyone to find out. The only solution <a href="https://aethermug.com/posts/normality-and-surprise-in-an-image-free-mind">I've found</a> is to try my best at explaining what the inner experience is like for me, and hope to receive comments from readers who have similar—or entirely different—experiences.</p>
<p>Below are some brief observations about the way my episodic memory works, based on notes I took over the past couple of years.</p>
<h2>Recalling Specific Episodes</h2>
<p>When I was looking for my first job, a Japanese company I applied to had me fill in a screening questionnaire. One question was something along these lines: "Write about a time during your university studies in which you faced a difficult problem, and what you did to overcome it." A perfectly reasonable question to ask a potential recruit with no employment history, but an impossibly hard question for me to answer.</p>
<p>I was completely stumped. In my university years, I worked on many research projects, and it wasn't always easy. I <em>knew</em> I had faced various kinds of problems during my graduate studies, and I assumed I had overcome them all before getting my degree. Why couldn't I come up with a single example?</p>
<p>This was the first time I noticed that something was off. Those questions about relevant episodes are pretty standard in certain industries, and I had never heard anyone complain about them specifically. Yet they were anathema for me.</p>
<figure><img src="https://aethermug.com/assets/posts/i-do-not-remember-my-life-and-it-s-fine/kolar.webp" alt="Original photo by Jan Antonin Kolar, Unsplash (modified)" title="A row of wooden drawers with metal handles and blotted-out labels."><figcaption>Original photo by Jan Antonin Kolar, Unsplash (modified)</figcaption></figure>
<p>My memory feels like a file cabinet without labels, a database without an index, a dictionary of randomly-ordered words without a table of contents. There are many memories there, but most of them can't be retrieved with convenient keywords like "a time when X happened".</p>
<p>Only with very specific cues and external help am I able to, sometimes, recall the events I'm looking for. In the case of the job application questionnaire, I struggled with it for several days, asked a friend for advice, and eventually managed to put together a lame but passable answer based on my research notes. Still, I was left with the nagging feeling that more fitting and relevant examples remained buried away in my psyche, somewhere out of reach.</p>
<p>I felt the same limitation very strongly again last year, when my grandfather passed away. I determined to sit down and write everything I could remember about him, and my relationship with him. I went back in my mind to his house in the Roman countryside and wrote things as they came to me.</p>
<p>He was kind and jovial with us grandchildren. He often involved us in making bread and pizza together in his stone oven, and I liked that. I could even write a general visual description (not by putting into words what I saw in my mind at the time of writing, but by recalling what I "knew" about his looks). And so on, I could muster a good number of generic, timeless <em>facts</em> about him, including my feelings related to him, but I soon realized that episodes and conversations were sorely absent: alright, he used to keep bees, and took me to see them more than once; but how many times? What did we say during those visits? What happened specifically?</p>
<p>Nothing that resembles a "scene" or sequence of events resurfaced in my memory. Everything I wrote was in the past progressive tense: "he used to be like this", "we would often do that", "more than once we did so".</p>
<p>Nowhere in what I wrote was any sense of sequential events, nor any specific conversation, not to mention specific utterances. I could write a good deal about him, but I had to rely on educated guesses in order to put together a coherent description of things that happened.</p>
<p>I wanted to bring back specific episodes, one-time events that we had experienced together, but I could find very little. He was there in my mind, no mistake about that, but in an intangible, elusive way. That day I felt disheartened, and dropped the project in the middle.</p>

<p>Most of the time this weakness in recollecting specific life episodes doesn't have major practical consequences. If necessary, others can help me bring back a memory, and I can remember the most consequential information as facts rather than episodes. In fact, a <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC10598423/" rel="nofollow noopener noreferrer" target="_blank">recent study</a> has found that aphantasics aren't any worse than non-aphantasics as eyewitnesses: although the participants "recalled 30% less correct information and accounts were less complete," "they made no more errors and were as accurate as typical imagers."</p>
<p>The downsides, then, seem to be mostly emotional, not pragmatic.</p>
<p>(If you're wondering how I can remember this "episode" of me trying to write about my grandfather, it's because I have the unfinished text saved, and wrote my reflections about the attempt soon afterwards. I have what I need to reconstruct the episode without leaning very much on my remembering powers.)</p>
<h2>Memory Voids</h2>
<hr><figure id="floating-1"><img src="https://aethermug.com/assets/posts/i-do-not-remember-my-life-and-it-s-fine/512200ldsdl.webp" alt="A painting of a desert scene with ruins of ancient buildings."><figcaption>A Bedouin encampment surrounded by ruins, Charles Théodore Frère</figcaption></figure>
<p>The blank I drew with my grandfather is just an example of what you might call a "memory void." It's not that I tend to forget people. Indeed, my loved ones are safe in my mind, albeit in that intangible and elusive form, with no risk of being forgotten—more on this later. My memory voids are specifically about <em>the concrete things I did in my life.</em></p>
<p>Ask me how my childhood was, or if I had fun in my twenties, and all I can answer is "I think so." Not because I wasn't sure about it at the time, but because I don't remember what I thought of it. With such broad and general questions, I have almost no hope of coming up with representative memories to help me answer those questions. No flashbacks to times I thought "this is great!" nor to moments of sadness. Again, many such events are buried as facts and observations somewhere in my memory, but that's not how I can recover them.</p>
<p>My past feels like someone else's. I know a great deal about it, more than anyone else in fact, yet I don't remember <em>being</em> in it. I can create a year-by-year history of my whole life with information such as the places I lived in, the schools I went to, the major turning points in my life; I can explain many facts about the key people and events of each time period; I can even arrange many of these in the form of stories or ordered stages of growth—yet none of this feels like things <em>I</em> did. It's like being the world's top expert about a stranger's life.</p>
<p>To be clear, this is not dissociative amnesia, trauma-induced selective forgetting, or anything like that. I know I had a good, sheltered childhood and early adulthood, with a caring and kind family, good friends, no financial difficulties, no scarring or traumatizing events. I was, by all measures, a happy and privileged kid. But I know that as a dry fact, not as a rush of nostalgic emotions.</p>
<p>Why aphantasia would do that to one's memory is still unclear. The topic itself hasn't been studied much yet, but this is slowly changing. In a very recent <a href="https://www.researchsquare.com/article/rs-6675918/v1" rel="nofollow noopener noreferrer" target="_blank">experiment</a>, Boere et al. (2025) used EEG (electroencephalography) to show that there might be fundamental differences in neural activity at the time of <em>forming</em> new episodic memories, rather than at the time of retrieving them. Aphantasics, they found, have lower levels of the kind of brain waves associated with attention and, crucially, memory updating.</p>
<p>This is very interesting in itself, but the following observation in their abstract is arguably even more important:</p>
<blockquote>
<p>Despite these neural differences, behavioral performance remained comparable, indicating possible compensatory strategies.</p>
</blockquote>
<p>In other words, people with aphantasia don't fare significantly worse in their practical use of memory—they just use it in a different way.</p>
<p>Whenever I think about a period of my life, all the "situational" and somewhat "concrete" memories I get are <em>averaged out</em>, all similarities between separate days and recurrences overlapping each other and blending together, while all the deviations from routine are washed away into oblivion: everything in the past progressive.</p>
<h2>Semantic and Spatial Memory Are Fine</h2>
<hr><figure id="floating-2"><img src="https://aethermug.com/assets/posts/i-do-not-remember-my-life-and-it-s-fine/535271ldsdl.webp" alt="A painting of a group of people with turbans in a middle-eastern bazaar."><figcaption>A Bazaar in Cairo, Charles Théodore Frère</figcaption></figure>
<p>If the results of Boere et al. are confirmed, this could shed some fascinating light on how different non-episodic, or "semantic" memory is from the episodic kind. In my case, semantic memory seems to be perfectly intact, and only the episodic, autobiographical kind is impaired.</p>
<p>From the observations above, it's as if my memory-encoding neural circuits work by comparing new experiences with pre-existing <a href="https://aethermug.com/posts/a-framing-and-model-about-framings-and-models">mental models</a>, tweaking and tuning those mental models of the world with each new sensory input, rather than collecting separate instances of similar but slightly different situations.</p>
<p>This would explain why the important, recurrent facts remain, while all the fickle details are washed away as if by an averaging operation. Perhaps this is what goes on in everyone's brains, except that in most people the episode-storing circuits are also working at the same time, and the two processes feel inseparable.</p>
<p>The interpretation above would also explain why my mental models—the <a href="https://aethermug.com/posts/embedded-prophesy-devices">embedded prophesy devices</a> I rely on to predict the future and function in everyday life—are as good as anyone else's. Indeed, it might even explain why I seem to care and think about mental models, and about cognition in general, more than the average person. For me, mental models are <em>the main way</em> I benefit from my memory. They help me not only to form reasonable expectations about what might happen in the future, but also to "reconstruct" my past—the educated guesses about my own past I referred to before.</p>
<p>And we shouldn't forget spatial awareness. This "sense of space and location" plays a major role in my thinking system.</p>
<hr><figure id="floating-3"><img src="https://aethermug.com/assets/posts/i-do-not-remember-my-life-and-it-s-fine/heidi-kaden-TJrorizZcPA-unsplash.webp" alt="A view of Florence's Duomo, seen from a small street."><figcaption>Photo by Heidi Kaden, Unsplash</figcaption></figure>
<p>For as long as I can remember, I've been very good at understanding maps and not getting lost. When I lived in Florence, at the age of nine or ten, I loved to be the one guiding my family through the city's meandering side-streets with the help of a paper map. I would choose (past progressive!) new and unknown routes every time, just for the fun of exploration, but we always ended up emerging into the square or courtyard I had intended.</p>
<p>For all the difficulties I have in recalling scenes that unfolded on specific days of my life, I have no trouble at all remembering the spatial layout of the places where those scenes took place. I can draw the floor map of any house I've lived in or spent more than a couple of days in the past 30 or more years. When I visit Rome, a city I haven't lived in for more than a decade, the routes to get anywhere familiar come back to me as clearly as if I had learned them the previous day. This is clearly another kind of memory, quite distinct from both the episodic and the semantic kinds.</p>
<p>As a matter of fact, spatial memory is the closest thing I have to an "index" for the musty file cabinet of my episodic memories. If I can remember <em>where</em> something happened, there is a good chance I can remember many more details about <em>what</em> happened.</p>
<p>This is a recent realization of mine, and I've taken to calling it the <em>Swoosh Effect</em>. Often my wife mentions an event or the name of a shop, saying something like "I miss the Flavor Savor hamburgers we used to go to when we lived in Nagareyama!" Usually, to her unconcealed dismay, I draw a complete blank: "what's the Flavor Savor?" We used to go there all the time, she says, and it wasn't even that long ago.</p>
<p>I get absolutely nothing. I frantically try to think of hamburger joints in Nagareyama: zero hits.</p>
<p>Then she adds some spatial information, like "it's on the last floor of the XYZ building in front of the station" and suddenly I'm transported there in a roller-coaster instant and it all comes back to me clearly. I almost <em>feel</em> the swooshing movement of going from the station to the entrance of XYZ building, then to the escalators, then up to the last floor, and finally homing into the entrance of the Flavor Savor, all in less than a second. Now all the semantic information pours out: "of course, the Flavor Savor! We went there, like, six times in a year. They have great avocado burgers and a tasty homemade sauce there!" If not too averaged-out, even some fragments of Flavor Savor episodes might come back to me at that point.</p>
<p>In short, I use my semantic and spatial memory to fill in what my episodic memory is unable to recover (or store). Most of the time this works fine, but in some cases that way of compensating doesn't work.</p>
<p>For example, I think I may also have mild face-blindness, the difficulty in recognizing faces and linking them with names. Usually, it doesn't cause major issues, and with some effort and repetition, I can learn to recognize people. But the face-blindness really rears its head when I meet someone not-so-familiar in an unexpected place, like random encounters on a train. Since I don't have the usual contextual cues to help me, in these cases I find it very hard to pin down who they are. They go "hey Marco, what's up?" and all I get is the vague sense that I know this person from <em>somewhere</em>. Only when they mention names or other contextual information do I have a chance of allocating them in their rightful place in my mental social network.</p>
<h2>Not Bad, All in All</h2>
<hr><figure id="floating-4"><img src="https://aethermug.com/assets/posts/i-do-not-remember-my-life-and-it-s-fine/535272ldsdl.webp" alt="A painting of a man in a boat on a peaceful middle-eastern river."><figcaption>Dhows on the Nile, Charles Théodore Frère</figcaption></figure>
<p>If you have intact episodic memory, some of the descriptions above might sound entirely alien to you. You might have many questions, and I don't know how to answer them all. Before concluding, though, I will try to address two of what might be your biggest doubts.</p>
<p>First, does my lack of remembered episodes and nostalgic flashbacks mean that the people in my life don't really exist as people in my mind, and that my forgotten experiences taught me nothing? No, and no.</p>
<p>It is hard to explain, but the things that matter do stay with me, even if I can't reminisce about the specific times they happened to me. I may not be able to play back fond memories of distinct interactions with my late grandfather, for instance, but I internalized them all. Intangible and invisible as he may be, he is there in my mind and will always be, and thinking about him does evoke many emotions in me—emotions I feel <em>now</em>, not replicas of past emotions. Something invisible can never fade.</p>
<p>More broadly, my mind's constant "averaging" work makes it very hard for me to build an encyclopedic memory—the minor details quickly escape me—but the <em>understanding</em> remains. The important insights stick with me, and my learning takes the form of better and better mental models: more sophisticated, more inclusive of many factors, more widely applicable or abstract. This, even when the specifics of how I obtained those insights refuse to be summoned back. Those vague "problems I had to overcome in university" that the job screening question wanted from me had happened, and I <em>had</em> learned my lessons from them, even though I forgot how they unfolded.</p>
<p>I consider this to be a key component of my intelligence because it allows me to concentrate on what is important. My experience is distilled directly into wisdom. Which brings me to the second doubt you might have: is SDAM a detestable handicap?</p>
<p>More so than with aphantasia, I can see how one might want to call SDAM a disorder. It does sound like a net negative, the removal of an ability that cannot be fully replaced, at least when your goal is to be "reunited" with a lost or distant loved one. Unlike aphantasia, I do often feel my weakness on this front, and I understand the many people with SDAM who bemoan their condition.</p>
<p>But—call me an indefatigable optimist—I also see benefits to having SDAM.</p>
<p>By doing away with reminiscences, flashbacks, and graphic visions of possible futures, I can stay focused on the now, and on what I can do now to improve tomorrow. I don't get intrusive scenes to distract me and sway me with sudden emotions.</p>
<p>Perhaps SDAM pushes me to work harder at interpreting the new information <em>as I perceive it</em>, because I know, deep down, that I will either "get" it now—updating a mental model—or I risk forever missing the opportunity to "get" it. This commitment to immediate understanding, in turn, helps me improve as a rational thinker.</p>
<p>And, once again, there is still no empirical proof that these memory "deficits" bring significant disadvantages <em>in practice</em>. The paper I mentioned at the beginning—the one that found no difference in eyewitness accuracy between people with and without aphantasia—makes this conclusion:</p>
<blockquote>
<p>Our pattern of results indicates reduced mental imagery ability might be compensated for by alternative self-initiated cognitive strategies.</p>
</blockquote>
<p>That is one fair way to put it. But here is another one, from my point of view: <em>having strong mental imagery and episodic memory doesn't seem to help much in practice.</em> It is just an alternative way to experience the world. ●</p>
<div><p>Cover image:</p><p><em>Caravane Au Coucher Du Soleil, Charles Théodore Frère</em></p></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Ask-human-mcp – zero-config human-in-loop hatch to stop hallucinations (105 pts)]]></title>
            <link>https://masonyarbrough.com/blog/ask-human</link>
            <guid>44196433</guid>
            <pubDate>Thu, 05 Jun 2025 22:57:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://masonyarbrough.com/blog/ask-human">https://masonyarbrough.com/blog/ask-human</a>, See on <a href="https://news.ycombinator.com/item?id=44196433">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Claude Composer (145 pts)]]></title>
            <link>https://github.com/possibilities/claude-composer</link>
            <guid>44196417</guid>
            <pubDate>Thu, 05 Jun 2025 22:53:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/possibilities/claude-composer">https://github.com/possibilities/claude-composer</a>, See on <a href="https://news.ycombinator.com/item?id=44196417">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Claude Composer CLI</h2><a id="user-content-claude-composer-cli" aria-label="Permalink: Claude Composer CLI" href="#claude-composer-cli"></a></p>
<blockquote>
<p dir="auto">A tool for enhancing Claude Code with automation, configuration, and better UX</p>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li><strong>Reduced interruptions</strong>: Auto-handles permission dialogs based on configurable rules</li>
<li><strong>Flexible control</strong>: Rulesets define which actions to allow automatically</li>
<li><strong>Tool management</strong>: Toolsets configure which tools Claude can use</li>
<li><strong>Enhanced visibility</strong>: System notifications keep you informed</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick Start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick Start" href="#quick-start"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Install
npm install -g claude-composer

# Initialize configuration
claude-composer cc-init

# Run with default settings
claude-composer

# Use different rulesets
claude-composer --ruleset internal:yolo  # Accept all prompts
claude-composer --ruleset internal:safe  # Manual confirmation only"><pre><span><span>#</span> Install</span>
npm install -g claude-composer

<span><span>#</span> Initialize configuration</span>
claude-composer cc-init

<span><span>#</span> Run with default settings</span>
claude-composer

<span><span>#</span> Use different rulesets</span>
claude-composer --ruleset internal:yolo  <span><span>#</span> Accept all prompts</span>
claude-composer --ruleset internal:safe  <span><span>#</span> Manual confirmation only</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><strong>Prerequisites</strong>: Node.js 18+, npm/yarn/pnpm, Claude Code installed</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Global install
pnpm add -g claude-composer
# or
yarn global add claude-composer
# or
npm install -g claude-composer"><pre><span><span>#</span> Global install</span>
pnpm add -g claude-composer
<span><span>#</span> or</span>
yarn global add claude-composer
<span><span>#</span> or</span>
npm install -g claude-composer</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration</h2><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto">Run <code>claude-composer cc-init</code> to create configuration:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Global config (default)
claude-composer cc-init

# Project-specific config
claude-composer cc-init --project"><pre><span><span>#</span> Global config (default)</span>
claude-composer cc-init

<span><span>#</span> Project-specific config</span>
claude-composer cc-init --project</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Configuration Structure</h3><a id="user-content-configuration-structure" aria-label="Permalink: Configuration Structure" href="#configuration-structure"></a></p>
<div data-snippet-clipboard-copy-content="~/.claude-composer/          # Global
├── config.yaml
├── rulesets/*.yaml         # Custom rulesets
└── toolsets/*.yaml         # Custom toolsets

.claude-composer/           # Project-specific
├── config.yaml
├── rulesets/*.yaml
└── toolsets/*.yaml"><pre><code>~/.claude-composer/          # Global
├── config.yaml
├── rulesets/*.yaml         # Custom rulesets
└── toolsets/*.yaml         # Custom toolsets

.claude-composer/           # Project-specific
├── config.yaml
├── rulesets/*.yaml
└── toolsets/*.yaml
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Basic Configuration</h3><a id="user-content-basic-configuration" aria-label="Permalink: Basic Configuration" href="#basic-configuration"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# config.yaml
rulesets:
  - internal:cautious
  - my-custom-rules

toolsets:
  - internal:core
  - my-tools

roots:
  - ~/projects/work
  - ~/projects/personal

show_notifications: true
sticky_notifications: false"><pre><span><span>#</span> config.yaml</span>
<span>rulesets</span>:
  - <span>internal:cautious</span>
  - <span>my-custom-rules</span>

<span>toolsets</span>:
  - <span>internal:core</span>
  - <span>my-tools</span>

<span>roots</span>:
  - <span>~/projects/work</span>
  - <span>~/projects/personal</span>

<span>show_notifications</span>: <span>true</span>
<span>sticky_notifications</span>: <span>false</span></pre></div>
<p dir="auto">See <a href="https://github.com/possibilities/claude-composer/blob/main/docs/configuration.md">docs/configuration.md</a> for details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Rulesets</h2><a id="user-content-rulesets" aria-label="Permalink: Rulesets" href="#rulesets"></a></p>
<p dir="auto">Control which permission dialogs are automatically accepted.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Built-in Rulesets</h3><a id="user-content-built-in-rulesets" aria-label="Permalink: Built-in Rulesets" href="#built-in-rulesets"></a></p>
<ul dir="auto">
<li><strong><code>internal:safe</code></strong>: All dialogs require manual confirmation</li>
<li><strong><code>internal:cautious</code></strong>: Auto-accepts project operations, confirms global ones</li>
<li><strong><code>internal:yolo</code></strong>: Accepts all operations without confirmation</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Using Rulesets</h3><a id="user-content-using-rulesets" aria-label="Permalink: Using Rulesets" href="#using-rulesets"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Built-in
claude-composer --ruleset internal:cautious

# Custom global
claude-composer --ruleset my-workflow

# Project-specific
claude-composer --ruleset project:backend

# Chain multiple
claude-composer --ruleset internal:cautious --ruleset my-overrides"><pre><span><span>#</span> Built-in</span>
claude-composer --ruleset internal:cautious

<span><span>#</span> Custom global</span>
claude-composer --ruleset my-workflow

<span><span>#</span> Project-specific</span>
claude-composer --ruleset project:backend

<span><span>#</span> Chain multiple</span>
claude-composer --ruleset internal:cautious --ruleset my-overrides</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Custom Ruleset Example</h3><a id="user-content-custom-ruleset-example" aria-label="Permalink: Custom Ruleset Example" href="#custom-ruleset-example"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# .claude-composer/rulesets/backend.yaml
name: backend
description: Backend development rules

accept_project_edit_file_prompts:
  paths:
    - 'src/**/*.js'
    - 'test/**'
    - '!**/*.env'

accept_project_bash_command_prompts: true
accept_fetch_content_prompts: false"><pre><span><span>#</span> .claude-composer/rulesets/backend.yaml</span>
<span>name</span>: <span>backend</span>
<span>description</span>: <span>Backend development rules</span>

<span>accept_project_edit_file_prompts</span>:
  <span>paths</span>:
    - <span><span>'</span>src/**/*.js<span>'</span></span>
    - <span><span>'</span>test/**<span>'</span></span>
    - <span><span>'</span>!**/*.env<span>'</span></span>

<span>accept_project_bash_command_prompts</span>: <span>true</span>
<span>accept_fetch_content_prompts</span>: <span>false</span></pre></div>
<p dir="auto">See <a href="https://github.com/possibilities/claude-composer/blob/main/docs/rulesets.md">docs/rulesets.md</a> for complete documentation.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Toolsets</h2><a id="user-content-toolsets" aria-label="Permalink: Toolsets" href="#toolsets"></a></p>
<p dir="auto">Configure which tools Claude can use and MCP server connections.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Built-in Toolsets</h3><a id="user-content-built-in-toolsets" aria-label="Permalink: Built-in Toolsets" href="#built-in-toolsets"></a></p>
<ul dir="auto">
<li><strong><code>internal:core</code></strong>: Provides Context7 documentation tools</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Using Toolsets</h3><a id="user-content-using-toolsets" aria-label="Permalink: Using Toolsets" href="#using-toolsets"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Built-in
claude-composer --toolset internal:core

# Custom
claude-composer --toolset my-tools

# Multiple
claude-composer --toolset internal:core --toolset project:dev-tools"><pre><span><span>#</span> Built-in</span>
claude-composer --toolset internal:core

<span><span>#</span> Custom</span>
claude-composer --toolset my-tools

<span><span>#</span> Multiple</span>
claude-composer --toolset internal:core --toolset project:dev-tools</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Custom Toolset Example</h3><a id="user-content-custom-toolset-example" aria-label="Permalink: Custom Toolset Example" href="#custom-toolset-example"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# .claude-composer/toolsets/dev-tools.yaml
allowed:
  - Read
  - Write
  - Edit
  - Bash

disallowed:
  - WebSearch

mcp:
  my-server:
    type: stdio
    command: node
    args: [./tools/mcp-server.js]"><pre><span><span>#</span> .claude-composer/toolsets/dev-tools.yaml</span>
<span>allowed</span>:
  - <span>Read</span>
  - <span>Write</span>
  - <span>Edit</span>
  - <span>Bash</span>

<span>disallowed</span>:
  - <span>WebSearch</span>

<span>mcp</span>:
  <span>my-server</span>:
    <span>type</span>: <span>stdio</span>
    <span>command</span>: <span>node</span>
    <span>args</span>: <span>[./tools/mcp-server.js]</span></pre></div>
<p dir="auto">See <a href="https://github.com/possibilities/claude-composer/blob/main/docs/toolsets.md">docs/toolsets.md</a> for details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Command Line Options</h2><a id="user-content-command-line-options" aria-label="Permalink: Command Line Options" href="#command-line-options"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Core Options</h3><a id="user-content-core-options" aria-label="Permalink: Core Options" href="#core-options"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Configuration
--ruleset <name...>              # Use specified rulesets
--toolset <name...>              # Use specified toolsets
--ignore-global-config           # Ignore global config

# Safety
--dangerously-allow-in-dirty-directory
--dangerously-allow-without-version-control
--dangerously-suppress-automatic-acceptance-confirmation

# Notifications
--show-notifications / --no-show-notifications
--sticky-notifications / --no-sticky-notifications

# Debug
--quiet                          # Suppress preflight messages
--allow-buffer-snapshots         # Enable Ctrl+Shift+S snapshots
--log-all-pattern-matches        # Log to ~/.claude-composer/logs/"><pre><span><span>#</span> Configuration</span>
--ruleset <span>&lt;</span>name...<span>&gt;</span>              <span><span>#</span> Use specified rulesets</span>
--toolset <span>&lt;</span>name...<span>&gt;</span>              <span><span>#</span> Use specified toolsets</span>
--ignore-global-config           <span><span>#</span> Ignore global config</span>

<span><span>#</span> Safety</span>
--dangerously-allow-in-dirty-directory
--dangerously-allow-without-version-control
--dangerously-suppress-automatic-acceptance-confirmation

<span><span>#</span> Notifications</span>
--show-notifications / --no-show-notifications
--sticky-notifications / --no-sticky-notifications

<span><span>#</span> Debug</span>
--quiet                          <span><span>#</span> Suppress preflight messages</span>
--allow-buffer-snapshots         <span><span>#</span> Enable Ctrl+Shift+S snapshots</span>
--log-all-pattern-matches        <span><span>#</span> Log to ~/.claude-composer/logs/</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Subcommands</h3><a id="user-content-subcommands" aria-label="Permalink: Subcommands" href="#subcommands"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Initialize configuration
claude-composer cc-init [options]
  --project                      # Create in current directory
  --use-yolo-ruleset            # Use YOLO ruleset
  --use-cautious-ruleset        # Use cautious ruleset
  --use-safe-ruleset            # Use safe ruleset
  --use-core-toolset            # Enable core toolset"><pre><span><span>#</span> Initialize configuration</span>
claude-composer cc-init [options]
  --project                      <span><span>#</span> Create in current directory</span>
  --use-yolo-ruleset            <span><span>#</span> Use YOLO ruleset</span>
  --use-cautious-ruleset        <span><span>#</span> Use cautious ruleset</span>
  --use-safe-ruleset            <span><span>#</span> Use safe ruleset</span>
  --use-core-toolset            <span><span>#</span> Enable core toolset</span></pre></div>
<p dir="auto">All unrecognized options pass through to Claude Code.</p>
<p dir="auto">See <a href="https://github.com/possibilities/claude-composer/blob/main/docs/cli-reference.md">docs/cli-reference.md</a> for complete reference.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Environment Variables</h2><a id="user-content-environment-variables" aria-label="Permalink: Environment Variables" href="#environment-variables"></a></p>
<ul dir="auto">
<li><code>CLAUDE_COMPOSER_CONFIG_DIR</code> - Override config directory</li>
<li><code>CLAUDE_COMPOSER_NO_NOTIFY</code> - Disable notifications</li>
<li><code>FORCE_COLOR</code> - Control color output</li>
</ul>
<p dir="auto">See <a href="https://github.com/possibilities/claude-composer/blob/main/docs/environment-variables.md">docs/environment-variables.md</a> for details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Roots Configuration</h2><a id="user-content-roots-configuration" aria-label="Permalink: Roots Configuration" href="#roots-configuration"></a></p>
<p dir="auto">Define trusted parent directories to auto-accept initial trust prompts:</p>
<div dir="auto" data-snippet-clipboard-copy-content="roots:
  - ~/projects # Trusts ~/projects/my-app, not ~/projects/my-app/src
  - $WORK_DIR/repos # Environment variable expansion supported"><pre><span>roots</span>:
  - <span>~/projects </span><span><span>#</span> Trusts ~/projects/my-app, not ~/projects/my-app/src</span>
  - <span>$WORK_DIR/repos </span><span><span>#</span> Environment variable expansion supported</span></pre></div>
<p dir="auto">See <a href="https://github.com/possibilities/claude-composer/blob/main/docs/roots-config.md">docs/roots-config.md</a> for details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Development</h2><a id="user-content-development" aria-label="Permalink: Development" href="#development"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Contributing</h3><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<ol dir="auto">
<li>Fork the repository</li>
<li>Create feature branch</li>
<li>Commit changes</li>
<li>Push branch</li>
<li>Open Pull Request</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Release</h3><a id="user-content-release" aria-label="Permalink: Release" href="#release"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="npm run release:patch  # Bug fixes
npm run release:minor  # New features
npm run release:major  # Breaking changes"><pre>npm run release:patch  <span><span>#</span> Bug fixes</span>
npm run release:minor  <span><span>#</span> New features</span>
npm run release:major  <span><span>#</span> Breaking changes</span></pre></div>
<p dir="auto">See <a href="https://github.com/possibilities/claude-composer/blob/main/docs/release-process.md">docs/release-process.md</a> for details.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What a developer needs to know about SCIM (139 pts)]]></title>
            <link>https://tesseral.com/blog/what-a-developer-needs-to-know-about-scim</link>
            <guid>44196393</guid>
            <pubDate>Thu, 05 Jun 2025 22:48:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tesseral.com/blog/what-a-developer-needs-to-know-about-scim">https://tesseral.com/blog/what-a-developer-needs-to-know-about-scim</a>, See on <a href="https://news.ycombinator.com/item?id=44196393">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2>Why SCIM exists</h2>
<p>Imagine you’re running a relatively large company, one with a few thousand employees. All of those employees use at least some software to do their jobs. It’s probably safe to assume that you’re dealing with hundreds of different SaaS applications across the company. You’ll have an app for approving expenses, an app for managing salespeople’s compensation, an app for piping data into your data warehouse, and much more. There’s an awfully long list of stuff.</p>
<p>Every employee needs access to some subset of apps. They need to do their jobs, after all. But you can’t give everyone access to everything. That’d cause all kinds of security, compliance, and practical problems. You need a way to assign different permissions to different people.</p>
<p>To handle access and permissions all in one centralized place, companies tend to use IT management software like Entra, Okta, or OneLogin (among many others); people tend to describe these tools as identity providers.</p>
<p>An identity provider (IDP) behaves a bit like a database. It maintains a list of employees along with a bunch of information about each person. Similarly, it maintains a list of different software applications. It keeps track of the mappings between people and applications. It’s very easy for the IT team to modify and create relationships between records.</p>
<p>Simply having a list of users and their access privileges in a database doesn’t help anyone much, though. The identity provider also needs to communicate information about users with other software.</p>
<p>The identity provider basically needs to communicate three kinds of changes to other software:</p>
<ol>
<li>The addition of new users (e.g. new hires)</li>
<li>The change of any existing user’s attributes (e.g. name, job title, etc.)</li>
<li>The removal of any existing users (e.g. departing employees)</li>
</ol>
<p>Identity providers typically rely on a standard called SCIM (the System for Cross-domain Identity Management) for these three communication tasks. They use SCIM to make every integration with other software look roughly the same, which eliminates the need for complicated bespoke integrations with the myriad applications they need to support.</p>
<h2>What SCIM (basically) does</h2>
<p>At a certain level of abstraction, a SCIM implementation looks a bit like a CS101 problem set. All we’re doing is making one list in your software look like another list in your customer’s software.</p>
<p>Put very simply, SCIM just defines some rules for the JSON that the identity provider sends and the JSON that the identity provider expects to receive in response. The JSON we’re trading with the identity provider exists solely to help us perform matching CRUD operations.</p>
<p>Let’s go through a conceptual example.</p>
<p>Suppose you’re selling a new inventory management system to Dunder Mifflin. Their IT team wants you to provision users programmatically from their identity provider. They have a list of users that need access to the inventory software:</p>
<ul>
<li>Kevin Malone, Accountant</li>
<li>Darryl Philbin, Shipping Manager</li>
<li>Creed Bratton, Quality Assurance Representative</li>
</ul>
<p>Great – via SCIM, you’ll get a pretty standardized block of JSON telling you which users to create. With that block of JSON, you’ll modify your Users table to include records for Kevin, Darryl, and Creed.</p>
<p>Suppose Darryl gets transitioned to a new role as Marketing Director. Great, IT updates his title in the identity provider. Within a few moments, you’ll get a standard block of JSON telling you to change Darryl’s job title. You’ll again just modify your Users table. Nothing too crazy.</p>
<p>Given Darryl’s job change, the Dunder Mifflin IT team decides he doesn’t need access to his account in the inventory management system anymore. Great, they just remove the mapping between Darryl and your software in their identity provider. Before long, you receive a standard block of JSON that tells you to deprovision – remove – Darryl’s account. You’ll make the corresponding changes in your Users table.</p>
<p>That’s all that SCIM is trying to do.</p>
<h2>What SCIM isn’t</h2>
<p>We often meet people who think SCIM support means that they need to make major changes to their software. This really should not be the case. Here, I’ll go through a few common misconceptions.</p>
<p>SCIM doesn’t really have anything to do with compliance. SCIM isn’t really related to SOC 2, even though there’s probably overlap in the kinds of customers who care about SCIM and the customers who care about SOC 2.</p>
<p>SCIM doesn’t really have anything to do with data retention. This is usually a separate conversation you might need to have with your customer.</p>
<p>SCIM doesn’t have any direct effect on your single sign-on implementation. Although it’s typical for people to use SCIM in combination with SAML SSO, these two standards actually don’t need to exist together or interact at all. It would be unusual, but you could use SCIM to manage users that use vanilla passwords to access your software.</p>
<p>SCIM doesn’t have any direct effect on how you manage your sessions. You can use whatever session management tools you’d like. Relatedly, SCIM doesn’t require single log-out support. If you receive an instruction from your customer to deprovision a user, you don’t typically need to revoke any active sessions belonging to that user.</p>
<p>SCIM doesn’t require major changes to your users schema. As long as you have a way of translating the JSON you receive into the desired CRUD operations,</p>
<p>SCIM doesn’t even really mean you have to support hard-deleting users or their data. You should set clear expectations with your customers, but it’s usually sufficient from your customer’s perspective that de-provisioning a user results in their account appearing no longer to exist. If that simply means adding some boolean column in your database that looks like IS_DELETED, that’s probably fine.</p>
<p>SCIM doesn’t require real-time updates. In practice, it’s usually fine if you’re processing updates every few hours. Many companies’ identity providers can’t support real-time updates anyway.</p>
<h2>How SCIM works at a (minimally) technical level</h2>
<p>Earlier, I mentioned that SCIM just performs CRUD operations via JSON. We should take that relatively literally. SCIM will only ever handle creating, reading, updating, and deleting records.</p>
<p>To do so, SCIM maps pretty familiar HTTP verbs onto these CRUD tasks. We’ll need to support GET and POST, as you would expect. We’ll also need to spend time thinking about PUT, PATCH, and DELETE, which can get annoying.</p>
<p>There’s something really important to bear in mind as we venture a little deeper into SCIM here. If your customer wants your software to hook into their identity provider, your software becomes the server – and your customer’s identity provider becomes the client. This might feel a little weird at first. After all, your customer’s identity provider stores the data you need. But it should make sense as we go.</p>
<h2>Client/server relationship and authentication in SCIM</h2>
<p>For any given SCIM operation, your customer’s identity provider will send one or more requests to your software. You need to process the request correctly, then you need to respond in a manner that the identity provider expects and understands.</p>
<p>Your software will play a passive role in SCIM. It will stay online and process requests as they roll in. The customer’s identity provider is the client. Your software is the server.</p>
<p>Given its role as the client, your customer’s identity provider needs to authenticate itself to you. It needs to prove that it’s actually the identity provider – and not some attacker – to make the changes it’s requesting. (It would be really bad if anyone could come along and provision users!)</p>
<p>We have a few different ways of authenticating SCIM clients, but bearer tokens are the most widely-supported option. You as the server need to generate a secret, share it with your customer, and have the customer’s identity provider present that secret in HTTP headers when it makes requests. You'll consider the presentation of a valid bearer token to be sufficient proof of identity and, consequently, you'll honor any HTTP request that correctly presents a valid bearer token.</p>
<h2>The data that we handle in SCIM operations</h2>
<p>SCIM’s creators built it around a generic concept that they call a <em>resource</em>. In this context, we should understand a resource to mean a particular kind of record.</p>
<p>In principle, we can represent basically anything as a SCIM resource. In practice, though, we pretty much only care about two of them. SCIM has users, and it has groups. People don’t tend to use SCIM for anything else.</p>
<p>Users in SCIM work pretty much like you’d expect. They’re just records of the people who use your software. You can think of groups as lists of users. (We can also have groups of groups.)</p>
<h2>Read operations in SCIM</h2>
<p>Let’s say you’ve just correctly configured a client/server trust relationship with your customer’s identity provider.</p>
<p>Before anything else happens, your customer’s identity provider will send you an HTTP GET. It will look basically like this:</p>
<p><img src="https://images.ctfassets.net/336y06gasq8n/2Pc9062CO8ZZ9oP9jeZFhJ/e686a5d948168d2bb541d08544d3ed38/scimget.png" alt="scimget"></p>
<p>All the identity provider does here is to hit a given <em>/Users</em> endpoint – in this case with a <em>userName</em> query parameter. It’s effectively asking here, <em>tell me what data you have for creed.bratton@example.com</em>. You’ll reply with a standard response code and some JSON.</p>
<p>In this case, we don’t have any data for Creed, so we’ll respond with a status code 200 and the following JSON:</p>
<pre tabindex="0"><code><span><span>{</span></span>
<span><span>    "itemsPerPage"</span><span>: </span><span>1</span><span>,</span></span>
<span><span>    "resources"</span><span>: [],</span></span>
<span><span>    "schemas"</span><span>: [</span></span>
<span><span>        "urn:ietf:params:scim:schemas:core:2.0:User"</span></span>
<span><span>    ],</span></span>
<span><span>    "startIndex"</span><span>: </span><span>1</span><span>,</span></span>
<span><span>    "totalResults"</span><span>: </span><span>0</span></span>
<span><span>}</span></span></code></pre>
<p>Notice that we’re using this schemas property here to reference <code>urn:ietf:params:scim:schemas:core:2.0:User</code>. We’re just referencing the built-in concept of users from the SCIM spec.</p>
<h2>Creation operations in SCIM</h2>
<p>The identity provider sees that we don’t have any data for Creed yet – based on the JSON we sent back – and so it instructs us to create a record.</p>
<p>To do so, it will use an HTTP POST with the following request body:</p>
<pre tabindex="0"><code><span><span>{</span></span>
<span><span>    "name"</span><span>: {</span></span>
<span><span>        "familyName"</span><span>: </span><span>"Bratton"</span><span>,</span></span>
<span><span>        "givenName"</span><span>: </span><span>"Creed"</span></span>
<span><span>    },</span></span>
<span><span>    "userName"</span><span>: </span><span>"creed.bratton@example.com"</span></span>
<span><span>}</span></span></code></pre>
<p>We just take this JSON and use our preferred method of creating a user record for Creed in our backend.</p>
<h2>Update operations in SCIM</h2>
<p>Updates in SCIM will use either HTTP PUT or HTTP PATCH, depending on the identity provider. Some identity providers tend only to use PUT. Others tend only to use PATCH. It’s kind of chaotic.</p>
<p>PUT operations in SCIM are pretty simple. They basically communicate: replace this user’s data with the data I’m showing you here. For example, Okta might send the following PUT request:</p>
<pre tabindex="0"><code><span><span>PUT</span><span> /scim/v2/Users/23a35c27-23d3-4c03-b4c5-6443c09e7173 </span><span>HTTP</span><span>/</span><span>1.1</span></span>
<span><span>User-Agent</span><span>:</span><span> Okta SCIM Client 1.0.0</span></span>
<span><span>Authorization</span><span>:</span><span> &lt;Authorization credentials&gt;</span></span>
<span></span>
<span><span>{</span></span>
<span><span>    "schemas"</span><span>: [</span><span>"urn:ietf:params:scim:schemas:core:2.0:User"</span><span>],</span></span>
<span><span>    "id"</span><span>: </span><span>"23a35c27-23d3-4c03-b4c5-6443c09e7173"</span><span>,</span></span>
<span><span>    "userName"</span><span>: </span><span>"test.user@okta.local"</span><span>,</span></span>
<span><span>    "name"</span><span>: {</span></span>
<span><span>        "givenName"</span><span>: </span><span>"Another"</span><span>,</span></span>
<span><span>        "middleName"</span><span>: </span><span>"Excited"</span><span>,</span></span>
<span><span>        "familyName"</span><span>: </span><span>"User"</span></span>
<span><span>    },</span></span>
<span><span>    "emails"</span><span>: [{</span></span>
<span><span>        "primary"</span><span>: </span><span>true</span><span>,</span></span>
<span><span>        "value"</span><span>: </span><span>"test.user@okta.local"</span><span>,</span></span>
<span><span>        "type"</span><span>: </span><span>"work"</span><span>,</span></span>
<span><span>        "display"</span><span>: </span><span>"test.user@okta.local"</span></span>
<span><span>    }],</span></span>
<span><span>    "active"</span><span>: </span><span>true</span><span>,</span></span>
<span><span>    "groups"</span><span>: [],</span></span>
<span><span>    "meta"</span><span>: {</span></span>
<span><span>        "resourceType"</span><span>: </span><span>"User"</span></span>
<span><span>    }</span></span>
<span><span>}</span></span></code></pre>
<p>You as the server will just look for the test.user@okta.local user data and replace it. Not too bad!</p>
<p>PATCH requests in SCIM can be a little more complicated. PATCH represents a partial update, whereas PUT represents a complete replacement. Here’s an example PATCH from the SCIM specification:</p>
<pre><code>PATCH /Groups/acbf3ae7-8463-...-9b4da3f908ce
Host: example.com
Accept: application/scim+json
Content-Type: application/scim+json
Authorization: Bearer h480djs93hd8
If-Match: W/"a330bc54f0671c9"

{
    "schemas": ["urn:ietf:params:scim:api:messages:2.0"], 
    "Operations":[{ 
            "Op":"add", 
            "Path":"members", 
            "Value":[{ 
                    "display": "Babs Jensen",
                    "$ref": "https://example.com/v2/Users/2819c223...413861904646",
                    "Value": "2819c223-7f76-453a-919d-413861904646" 
                }
            ]
        }
    ]
} 

</code></pre>
<p>Notice the <code>Operations</code> array here. The SCIM specification allows PATCH operations to add, remove, or replace data. This turns out to be a little complicated. (More on this later).</p>
<h2>Delete operations in SCIM</h2>
<p>According to the SCIM specification, the identity provider should send an HTTP DELETE request, something like the following:</p>
<pre><code>DELETE /Users/2819c223-7f76-453a-919d-413861904646
Host: example.com
Authorization: Bearer h480djs93hd8
If-Match: W/"c310cd84f0281b7"
</code></pre>
<p>This is sometimes how things work, but it’s not how things always work in practice.</p>
<p>For example, Okta doesn’t want to use DELETE. Instead, they PUT or PATCH a record such that the active property gets set to False.</p>
<h2>Is it a good idea to build SCIM?</h2>
<h2>The SCIM specification is basically good, but has some subtle details</h2>
<p>We really like the SCIM specification. Compared to other standards (looking at you, SAML), SCIM makes a lot of sense. It’s conceptually pretty simple, and it doesn’t come with major design flaws.</p>
<p>It does have some subtle quirks, though. I’ll mention just a few here.</p>
<p>As I mentioned earlier, PATCH can get a bit complicated. You have to handle a bunch of different kinds of operations to support PATCH. Don’t worry about the content – but here’s an excerpt from the SCIM spec on how PATCH’s add operation is supposed to work:</p>
<blockquote>
<p>The result of the add operation depends upon what the target location
indicated by "path" references:</p>
</blockquote>
<blockquote>
<p>If omitted, the target location is assumed to be the resource
itself.  The "value" parameter contains a set of attributes to be
added to the resource.</p>
</blockquote>
<blockquote>
<p>If the target location does not exist, the attribute and value are
added.</p>
</blockquote>
<blockquote>
<p>If the target location specifies a complex attribute, a set of
sub-attributes SHALL be specified in the "value" parameter.</p>
</blockquote>
<blockquote>
<p>If the target location specifies a multi-valued attribute, a new
value is added to the attribute.</p>
</blockquote>
<blockquote>
<p>If the target location specifies a single-valued attribute, the
existing value is replaced.</p>
</blockquote>
<blockquote>
<p>If the target location specifies an attribute that does not exist
(has no value), the attribute is added with the new value.</p>
</blockquote>
<blockquote>
<p>If the target location exists, the value is replaced.</p>
</blockquote>
<blockquote>
<p>If the target location already contains the value specified, no
changes SHOULD be made to the resource, and a success response
SHOULD be returned.  Unless other operations change the resource,
this operation SHALL NOT change the modify timestamp of the
resource.</p>
</blockquote>
<p>That’s a decent number of <em>if</em>s to support! Individually, none of these is too bad. But taken as a whole, they represent a decent chunk of code that you’ll have to write. More importantly, they represent an awful lot of tests that you’ll have to write. Yuck.</p>
<p>I alluded indirectly to this other quirk earlier. SCIM stakes some strange opinions, including the expected instant messaging providers (e.g. Yahoo) associate with a built-in user attribute. It does not, however, stake a strong opinion at all regarding the meaning of <em>active</em>. Whether <em>active</em> represents a concept like <em>is_deleted</em> or something else altogether is left up to you and the identity provider. That just creates some weird, needless ambiguity.</p>
<p>Open source repos like Authentik’s are just <a href="https://github.com/goauthentik/authentik/issues/6695">littered with weird issues</a> resulting from ambiguity. No one seems to have all of the answers.</p>
<h2>Identity providers don’t always implement the spec</h2>
<p>Some identity providers do some really weird stuff in non-compliance with the SCIM spec. And then they don’t document their weird choices properly. (I’m looking at you, Microsoft!) You just have to collide with their weird SCIM APIs in the real world and tweak your implementation as you go.</p>
<p><img src="https://images.ctfassets.net/336y06gasq8n/2osXgz5SVQf4hQuaPJn5oV/2c7408109a4790e2b97c575406d543ec/dog_at_computer.jpg" alt="dog at computer"></p>
<p>Microsoft keeps a list of SCIM non-compliance issues <a href="https://learn.microsoft.com/en-us/entra/identity/app-provisioning/application-provisioning-config-problem-scim-compatibility">here</a>. You may notice that an issue called <em>Update PATCH behavior to ensure compliance (such as active as boolean and proper group membership removals)</em> still has a planned fix date of TBD ... on an article last updated in October 2023.</p>
<p>It turns out that Microsoft’s default behavior sends a boolean value as a string:</p>
<pre tabindex="0"><code><span><span>{</span></span>
<span><span>  "schemas"</span><span>: [</span></span>
<span><span>      "urn:ietf:params:scim:api:messages:2.0:PatchOp"</span></span>
<span><span>  ],</span></span>
<span><span>  "Operations"</span><span>: [</span></span>
<span><span>      {</span></span>
<span><span>          "op"</span><span>: </span><span>"Replace"</span><span>,</span></span>
<span><span>          "path"</span><span>: </span><span>"active"</span><span>,</span></span>
<span><span>          "value"</span><span>: </span><span>"False"</span></span>
<span><span>      }</span></span>
<span><span>  ]</span></span>
<span><span>}</span></span></code></pre>
<p>It should really be sending the below modified JSON,</p>
<pre tabindex="0"><code><span><span>{</span></span>
<span><span>  "schemas"</span><span>: [</span></span>
<span><span>      "urn:ietf:params:scim:api:messages:2.0:PatchOp"</span></span>
<span><span>  ],</span></span>
<span><span>  "Operations"</span><span>: [</span></span>
<span><span>      {</span></span>
<span><span>          "op"</span><span>: </span><span>"replace"</span><span>,</span></span>
<span><span>          "path"</span><span>: </span><span>"active"</span><span>,</span></span>
<span><span>          "value"</span><span>: </span><span>false</span></span>
<span><span>      }</span></span>
<span><span>  ]</span></span>
<span><span>}</span></span></code></pre>
<p>You can force Microsoft to send you the proper JSON if you use a certain feature flag (<code>aadOptscim062020</code>), but that’s really not an obvious solution! You really have to dig. It's more practical just to accept that Microsoft misbehaves and modify your code accordingly.</p>
<p>This sort of stuff is very time-consuming and demoralizing to resolve.</p>
<h2>You probably shouldn’t implement SCIM from scratch</h2>
<p>I really do not recommend building SCIM in-house. To be clear, you absolutely <em>could</em>. We're not exactly splitting the atom here.</p>
<p>It's just that you definitely have better things to spend your time on – things that are closer to the problems your customers care most about.</p>
<p>Although SCIM lacks much conceptual complexity, it comes with a bunch of annoying baggage. If you build SCIM yourself, it will become someone’s de facto job to babysit the SCIM API and debug unforeseen subtleties. Your customers will have a not-so-great experience. You will likely be very unhappy and distracted.</p>
<p>This is a case where you should probably look for an off-the-shelf solution and move on.</p></div></div>]]></description>
        </item>
    </channel>
</rss>