<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 02 Apr 2025 07:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Where does air pollution come from? (114 pts)]]></title>
            <link>https://ourworldindata.org/air-pollution-sources</link>
            <guid>43553663</guid>
            <pubDate>Wed, 02 Apr 2025 04:25:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ourworldindata.org/air-pollution-sources">https://ourworldindata.org/air-pollution-sources</a>, See on <a href="https://news.ycombinator.com/item?id=43553663">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><header><h2>A breakdown of the sources of many air pollutants that damage our health and ecosystems.</h2></header><p><span>Millions of people </span><a href="https://ourworldindata.org/data-review-air-pollution-deaths" target="_blank" rel="noopener"><span>die prematurely</span></a><span> from air pollution every year. </span><a href="https://ourworldindata.org/energy-poverty-air-pollution" target="_blank" rel="noopener"><span>This problem has existed</span></a><span> since humans started burning materials for fuel — first wood and biomass, then fossil fuels.</span></p><p><span>But it’s an environmental and public health problem that we can make progress on. We know this because the world </span><em><span>has already</span></em><span> </span><a href="https://ourworldindata.org/cleanest-air-lessons" target="_blank" rel="noopener"><span>been successful</span></a><span> in reducing air pollutants, and many countries that used to be highly polluted now have much cleaner air than they used to.</span></p><p><span>To tackle air pollution effectively — to&nbsp;focus our efforts on the interventions that will have the biggest impact — we need to understand where it’s coming from.</span></p><p><span>That’s why we wrote this article.</span><a href="#note-1"><sup><span>1</span></sup></a></p><div inert="true"><h2 id="a-note-on-data-and-definitions"><span>A note on data and definitions</span><a href="#a-note-on-data-and-definitions"></a></h2><p><span>The main data source we rely on is the Community Emissions Data System (CEDS). There are a couple of reasons why we think it’s an incredibly valuable resource:</span></p><ul><li><span>It has long-term global and national data extending back to the 18th century and is frequently updated with the latest estimates for 2022.</span></li><li><span>It’s published with an open-access license and transparent methodology and inputs, which you can find </span><a href="https://github.com/JGCRI/CEDS/tree/master" target="_blank" rel="noopener"><span>on GitHub</span></a><span>. The peer-reviewed paper describing the methodology is </span><a href="https://gmd.copernicus.org/articles/11/369/2018/gmd-11-369-2018.html" target="_blank" rel="noopener"><span>here</span></a><span>.</span><a href="#note-2"><sup><span>2</span></sup></a></li><li><span>This online data resource is open to user comments and feedback, so errors or issues can be easily reported.</span></li><li><span>CEDS provides clear documentation of data improvements and detailed comparisons of recent updates against previous versions.</span></li></ul><p><span>To be clear, CEDS does not have high-quality </span><em><span>measurements</span></em><span> of emissions of air pollutants — certainly not dating back to the 18th century. These figures are calculated and modeled based on inputs, such as the quantity of different fuels that were burned, technological advancements and pollution controls, fertilizer use, and agricultural production. You can, for example, estimate the amount of sulfur dioxide produced from burning one tonne of coal in a power plant (with or without pollution filters).</span></p><p><span>Of course, this means the data comes with some uncertainty, especially in earlier periods. However, it gives us a reasonable and consistent global dataset to understand how trends in emissions of air pollutants have changed over time.</span></p><p><span>This article will focus on the breakdown of pollutants by their source. For this, we’ll use a categorization based on CEDS’s classification. In the table below, we summarize what is included in each category.</span></p><div><table><tbody><tr><th scope="col"><p><strong><span>Category</span></strong></p></th><th scope="col"><p><strong><span>Sub-categories</span></strong></p></th></tr><tr><td><p><span>Agriculture</span></p></td><td><p><span>Enteric fermentation</span></p><p><span>Fuel use in agriculture, forestry, and fishing</span></p><p><span>Indirect N₂O emissions</span></p><p><span>Manure management</span></p><p><span>Rice cultivation</span></p><p><span>Soil emissions</span></p><p><span>Other agricultural emissions</span></p></td></tr><tr><td><p><span>Buildings</span></p></td><td><p><span>Commercial and institutional buildings</span></p><p><span>Residential buildings</span></p></td></tr><tr><td><p><span>Domestic aviation</span></p></td><td><p><span>Domestic aviation</span></p></td></tr><tr><td><p><span>Energy</span></p></td><td><p><span>Electricity production (autoproducer)</span></p><p><span>Electricity production (public)</span></p><p><span>Fossil fuel fires</span></p><p><span>Fugitive emissions from natural gas distribution</span></p><p><span>Fugitive emissions from natural gas production</span></p><p><span>Fugitive emissions from other energy sources</span></p><p><span>Fugitive emissions from petroleum</span></p><p><span>Fugitive emissions from solid fuels</span></p><p><span>Heat production</span></p><p><span>Other energy transformation</span></p><p><span>Other fuel use (unspecified)</span></p></td></tr><tr><td><p><span>Industry</span></p></td><td><p><span>Adipic acid production</span></p><p><span>Aluminum production</span></p><p><span>Cement production</span></p><p><span>Chemical industry</span></p><p><span>Industrial combustion (chemicals)</span></p><p><span>Industrial combustion (construction)</span></p><p><span>Industrial combustion (food and tobacco)</span></p><p><span>Industrial combustion (iron and steel)</span></p><p><span>Industrial combustion (machinery)</span></p><p><span>Industrial combustion (mining and quarrying)</span></p><p><span>Industrial combustion (non-ferrous metals)</span></p><p><span>Industrial combustion (non-metallic minerals)</span></p><p><span>Industrial combustion (other)</span></p><p><span>Industrial combustion (pulp and paper)</span></p><p><span>Industrial combustion (textile and leather)</span></p><p><span>Industrial combustion (transport equipment)</span></p><p><span>Industrial combustion (wood products)</span></p><p><span>Iron and steel alloy production</span></p><p><span>Lime production</span></p><p><span>Nitric acid production</span></p><p><span>Other mineral production</span></p><p><span>Other non-ferrous metal production</span></p><p><span>Pulp and paper, food, beverage, and wood processing</span></p></td></tr><tr><td><p><span>International aviation</span></p></td><td><p><span>International aviation</span></p></td></tr><tr><td><p><span>International shipping</span></p></td><td><p><span>International shipping</span></p><p><span>Oil tanker loading</span></p></td></tr><tr><td><p><span>Solvents</span></p></td><td><p><span>Chemical products manufacture and processing</span></p><p><span>Degreasing and cleaning</span></p><p><span>Other product use</span></p><p><span>Paint application</span></p></td></tr><tr><td><p><span>Transport</span></p></td><td><p><span>Domestic navigation</span></p><p><span>Rail transportation</span></p><p><span>Road transportation</span></p><p><span>Other transport</span></p></td></tr><tr><td><p><span>Waste</span></p></td><td><p><span>Solid waste disposal</span></p><p><span>Waste combustion</span></p><p><span>Wastewater handling</span></p><p><span>Other waste handling</span></p><p><span>Other waste sources</span></p><p><span>Unspecified waste sources</span></p></td></tr></tbody></table></div></div><h2 id="how-air-pollution-damages-our-health"><span>How air pollution damages our health</span><a href="#how-air-pollution-damages-our-health"></a></h2><p><span>Before we discuss the sources of some of the key air pollutants, we should briefly explain how air pollution affects human health and how each of these pollutants contributes to this.</span></p><p><span>There are three key pathways by which a pollutant can cause harm</span><a href="#note-3"><sup><span>3</span></sup></a><span>:</span></p><ul><li><strong><span>Direct exposure:</span></strong><span> Some gases are toxic and can have an acute effect on health. These acute impacts are more common for people with existing respiratory conditions such as asthma or chronic obstructive pulmonary disease (COPD). This direct exposure does cost lives, but the total number is relatively small compared to the millions that die from chronic exposure to air pollution.</span></li><li><strong><span>Formation of particulate matter: </span></strong><span>Many of the pollutants we’ll look at contribute to health impacts </span><em><span>indirectly</span></em><span> by breaking down to form secondary smaller particles. These particles are called “particulate matter”. Typically, the smaller the particles are, the worse they can be for human health because they can enter our lungs and airways — and, in some cases, the bloodstream. Particulate matter can cause respiratory and cardiovascular problems, including cancer, strokes, and heart attacks.</span></li><li><strong><span>Formation of ozone: </span></strong><span>Another </span><em><span>indirect</span></em><span> way these pollutants can affect our health is by forming a gas called ozone (O</span><sub><span>3</span></sub><span>). Ozone can cause breathing problems and worsen acute conditions like asthma and COPD</span><a href="#note-4"><sup><span>4</span></sup></a><span>. However, it also affects our health through chronic exposure by causing inflammation of the lungs, increasing the risk of respiratory diseases, and reducing our cardiovascular health.</span></li></ul><p><span>As we discuss each pollutant, we’ll briefly explain how it affects health through one or several pathways. While it’s difficult to pinpoint exactly how many deaths each pollutant causes, wherever possible, we’ll also try to give a rough order of magnitude estimate.</span></p><p><span>To give some sense of scale, </span><a href="https://ourworldindata.org/explorers/global-health?tab=chart&amp;Health+Area=Risk+factors&amp;Indicator=Air+pollution&amp;Metric=Number+of+deaths&amp;Source=IHME&amp;country=OWID_WRL~CHN~ZAF~BRA~USA~GBR~IND~RWA"><span>here</span></a><span> is the Global Burden of Disease’s breakdown of global deaths from air pollution.</span><a href="#note-5"><sup><span>5</span></sup></a></p><p><span>In 2021, this totaled around</span><strong><span> 8 million deaths.</span></strong><span> Note that there are also natural sources of particulate matter, so not </span><em><span>all</span></em><span> of these pollution deaths resulted from human emissions. But most did. For more on this, see our colleague Max Roser’s </span><a href="https://ourworldindata.org/data-review-air-pollution-deaths" target="_blank" rel="noopener"><span>article</span></a><span>, which looks at estimates from various sources.</span></p><p><strong><span>3.1 million</span></strong><span> came from </span><em><span>household</span></em><span> air pollution, a combination of direct toxicity and particulate matter. </span><strong><span>4.7 million</span></strong><span> came from outdoor particulate matter, and another </span><strong><span>half a million</span></strong><span> from outdoor ozone pollution.</span><a href="#note-6"><sup><span>6</span></sup></a></p><div><figure data-grapher-src="https://ourworldindata.org/grapher/deaths-from-household-and-outdoor-air-pollution"><a href="https://ourworldindata.org/grapher/deaths-from-household-and-outdoor-air-pollution" target="_blank" rel="noopener"><picture><source id="grapher-preview-source" srcset="https://ourworldindata.org/grapher/deaths-from-household-and-outdoor-air-pollution.png?imWidth=850 850w, https://ourworldindata.org/grapher/deaths-from-household-and-outdoor-air-pollution.png?imWidth=1700 1700w" sizes="(max-width: 850px) 100vw, 850px"><img src="https://ourworldindata.org/grapher/deaths-from-household-and-outdoor-air-pollution.png" width="850" height="600" loading="lazy" data-no-lightbox="true"></picture></a></figure></div><h2 id="breaking-down-the-sources-of-different-air-pollutants"><span>Breaking down the sources of different air pollutants</span><a href="#breaking-down-the-sources-of-different-air-pollutants"></a></h2><h2 id="sulfur-dioxide-the-source-of-acid-rain"><span>Sulfur dioxide: the source of acid rain</span><a href="#sulfur-dioxide-the-source-of-acid-rain"></a></h2><p><span>Sulfur dioxide (SO</span><sub><span>2</span></sub><span>) is the main pollutant that causes </span><a href="https://worksinprogress.co/issue/the-end-of-acid-rain/" target="_blank" rel="noopener"><span>acid rain</span></a><span>. This has been a major environmental problem because acid rain can change the chemistry of rivers and lakes, affecting fish populations, soils, and the extent and quality of forests. You can also see the effects of acid rain on older limestone and marble buildings and statues, where the acidity dissolves parts of the structure.</span></p><p><span>There are two ways that SO</span><sub><span>2</span></sub><span> </span><a href="https://www.eea.europa.eu/en/topics/in-depth/air-pollution/eow-it-affects-our-health" target="_blank" rel="noopener"><span>can threaten</span></a><span> human health. First, direct inhalation of SO</span><sub><span>2</span></sub><span> can exacerbate respiratory problems such as asthma and bronchitis. But its main contribution is by breaking down to form small particulate matter. While an exact figure is hard to pin down, given that sulfur dioxide is a substantial contributor to particulate matter and that at least 4 million deaths are linked to these small particles yearly, we would estimate that </span><strong><span>hundreds of thousands</span></strong><span> of deaths per year are linked to SO</span><sub><span>2</span></sub><span>.</span></p><p><span>SO</span><sub><span>2</span></sub><span> is formed when we burn fuels that contain sulfur.</span></p><p><span>The charts below show where global emissions come from and how these sectors have changed over time. In 2022, energy production was the biggest contributor by far. This is predominantly due to power from coal, which has sulfur impurities that are released when it’s burned.</span></p><p><span>The main contribution of the industry is the metal smelting process.</span><a href="#note-7"><sup><span>7</span></sup></a><span> This is because many of the ores that are used to produce metals – such as pyrite – contain large amounts of sulfur, which is released when they are roasted at high temperatures.</span></p><p><span>Oil also contains sulfur, which is why road transport, shipping, and aviation all contribute. Shipping emissions have received a lot of attention in the last few years because they </span><a href="https://ourworldindata.org/explorers/air-pollution?country=~OWID_WRL&amp;Pollutant=Sulfur+dioxide&amp;Sector=International+shipping&amp;Per+capita=false"><span>dropped by more than 70%</span></a><span> in 2020 after the introduction of tight regulations on maritime fuels.</span></p><p><span>You’ll notice that global emissions of SO</span><sub><span>2</span></sub><span> peaked in 1979 and have almost halved since then, thanks to the introduction of pollution controls, particularly in Europe, North America, and China. SO</span><sub><span>2 </span></sub><span>can be removed from smokestacks in coal plants using technologies that filter or “scrub” the sulfur away before it’s emitted into the atmosphere. This, combined with a </span><a href="https://ourworldindata.org/explorers/energy?tab=chart&amp;country=USA~GBR~OWID_EU27&amp;Total+or+Breakdown=Select+a+source&amp;Energy+or+Electricity=Electricity+only&amp;Metric=Share+of+total&amp;Select+a+source=Coal"><span>move away from coal</span></a><span> in Europe and North America, has led to a rapid reduction in emissions.</span><sub><span> </span></sub></p><h2 id="nitrogen-oxides-n-ox-the-reason-car-exhaust-fumes-are-so-damaging"><span>Nitrogen oxides (NOₓ): the reason car exhaust fumes are so damaging</span><a href="#nitrogen-oxides-n-ox-the-reason-car-exhaust-fumes-are-so-damaging"></a></h2><p><span>Nitrogen oxides (NOₓ) are a group of gases, mostly made up of nitric oxide (NO) and nitrogen dioxide (NO</span><sub><span>2</span></sub><span>). They are formed when fuels containing nitrogen (again, mostly fossil fuels) are burned, causing them to react with oxygen.</span></p><p><span>Like sulfur dioxide, which we just looked at, NOₓ can cause acid rain, threatening wildlife and ecosystems. NOₓ has a particularly large impact on human health because it acts through all three mechanisms we looked at earlier. It can be acutely toxic, inflaming the lungs. It reacts with other gases to form particulate matter, and it also forms ozone. NOₓ, therefore, causes smog and the thick haze you often see in highly polluted cities.</span></p><p><span>Again, we don’t have exact estimates for how many deaths it contributes to. But, given that it’s a main source of ozone (which kills around half a million) and a substantial fraction of particulate matter (which kills several million), it’s reasonable to expect that NOₓ is linked to </span><strong><span>over a million deaths yearly</span></strong><span>.</span></p><p><span>Since coal, oil, and gas all contain nitrogen, NOₓ is produced in various sectors, as the chart below shows. The biggest source is transport — mostly from road vehicles — where NOₓ is emitted from the exhaust of cars and trucks. This is almost matched by the burning of coal and gas for electricity production, shown as “energy” in the chart. Like road transport, burning fuel for shipping emits significant amounts of NOₓ, making it a leading source, too.</span></p><p><span>And industrial processes such as metal smelting, cement production, and petroleum refining contribute a lot.</span></p><p><span>A smaller but still important source is agriculture. When nitrogen is applied to crops as synthetic fertilizer or manure, some of this nitrogen is converted to nitrogen oxides (and ammonia, which we’ll come on to later) in the soil.</span><a href="#note-8"><sup><span>8</span></sup></a><span> While emissions of NOₓ from processes such as transport and electricity production have declined a lot globally, progress on agriculture has been much slower: emissions have flatlined but have not fallen much.</span></p><p><span>Some countries have been successful in drastically reducing emissions of NOₓ – with huge benefits for human health. Moving away from fossil fuels – particularly coal in electricity production – has led to a large decline in “energy” emissions (have a look at </span><a href="https://ourworldindata.org/explorers/air-pollution?time=1950..2022&amp;Pollutant=Nitrogen+oxides&amp;Sector=All+sectors+%28total%29&amp;Per+capita=false&amp;country=~GBR"><span>the United Kingdom</span></a><span> as an example). Setting pollution control standards for automakers has also played a crucial role in reducing emissions from road transport. NOₓ emissions from exhausts can be dramatically reduced through the use of </span><a href="https://en.wikipedia.org/wiki/Catalytic_converter" target="_blank" rel="noopener"><span>catalytic converters</span></a><span>, which are devices that split the NOₓ compounds into nitrogen and oxygen before they are released into the atmosphere.</span></p><h2 id="black-carbon-the-soot-that-fills-our-skies-and-lungs"><span>Black carbon: the soot that fills our skies and lungs</span><a href="#black-carbon-the-soot-that-fills-our-skies-and-lungs"></a></h2><p><span>Black carbon (BC) are the small particles that many of us know as “soot”.</span></p><p><span>As most of us know from experience — such as lighting a bonfire — soot is formed when we burn materials such as wood and biomass or fossil fuels like coal. It’s the </span><em><span>incomplete</span></em><span> combustion of these materials that leads to the formation of these BC particles.</span></p><p><span>These particles are black because they absorb light, and this absorption of sunlight contributes to climate change. However, when it comes to health, it contributes through its direct toxicity and the formation of small particulates.</span></p><p><span>Black carbon can be a major issue for household and outdoor particulate matter and </span><a href="https://drawdown.org/publications/reducing-black-carbon" target="_blank" rel="noopener"><span>probably contributes to</span></a><span> </span><strong><span>several million deaths per year</span></strong><span>.</span></p><p><span>In the charts below, you can see the sources of black carbon globally.</span></p><p><span>Unsurprisingly, the biggest contributor to BC is energy production, mostly coal and biomass burning for electricity and heat. Black carbon is a big problem in industrialized countries with large-scale electricity production, but also in lower-income countries where people rely on burning biomass and charcoal for cooking and heating.</span></p><p><span>Road transport is another major contributor since black carbon is also formed from diesel engines and exhausts. The open burning of waste also plays a surprisingly large role, particularly in low-to-middle-income countries, where this is often used for waste disposal.</span></p><p><span>Some countries — particularly richer ones — have seen </span><a href="https://ourworldindata.org/explorers/air-pollution?time=1950..2022&amp;Pollutant=Black+carbon&amp;Sector=All+sectors+%28total%29&amp;Per+capita=false&amp;country=GBR~FRA~DEU~USA"><span>a huge drop</span></a><span> in black carbon emissions over the last 50 years due to moving away from biomass and coal burning and introducing cleaner cars.</span></p><h2 id="methane-burping-cows-rice-paddies-and-gas-leaks"><span>Methane: burping cows, rice paddies, and gas leaks</span><a href="#methane-burping-cows-rice-paddies-and-gas-leaks"></a></h2><p><span>Methane (CH</span><sub><span>4</span></sub><span>) is a greenhouse gas, so it’s </span><a href="https://ourworldindata.org/co2-and-greenhouse-gas-emissions" target="_blank" rel="noopener"><span>mostly discussed</span></a><span> regarding contributions to climate change. However, methane can also affect health when it breaks down to form ozone, a gas that’s hazardous to human health. In fact, methane is the biggest precursor to ozone in many places.</span></p><p><span>It’s </span><a href="https://www.eea.europa.eu/en/analysis/publications/methane-climate-change-and-air-quality-in-europe-exploring-the-connections" target="_blank" rel="noopener"><span>estimated that</span></a><span> methane can lead to up to </span><strong><span>half a million premature deaths</span></strong><span> a year.</span><a href="#note-9"><sup><span>9</span></sup></a></p><p><span>The charts below show where it comes from.</span></p><p><span>Agriculture, specifically livestock and rice production, is the biggest source of methane. Ruminant livestock — mostly cows — produce methane in their digestive systems and release it into the atmosphere by burping. That’s why beef and lamb </span><a href="https://ourworldindata.org/less-meat-or-sustainable-meat" target="_blank" rel="noopener"><span>tend to have</span></a><span> a high carbon footprint.</span><a href="#note-10"><sup><span>10</span></sup></a><span> Rice also produces methane because it’s often grown in flooded paddy fields with low oxygen levels. This means methane is produced rather than carbon dioxide. Eating less beef, lamb, and dairy could </span><a href="https://ourworldindata.org/food-choice-vs-eating-local" target="_blank" rel="noopener"><span>dramatically reduce</span></a><span> emissions from agriculture. Finding innovative ways to reduce the amount of methane produced per cow by changing their diets could also help.</span></p><p><span>Energy generation is another large source of methane. Most of it comes from leaks — which we call “fugitive emissions” — from oil and gas wells. If these are not properly managed, some methane escapes into the atmosphere. Another key source is coal mining. Monitoring oil and gas wells for methane leaks and enforcing regulations to ensure that limits are not breached can reduce these emissions. New drone and satellite technologies are </span><a href="https://www.catf.us/2023/02/guide-monitoring-quantifying-methane-emissions-oil-gas-sector/" target="_blank" rel="noopener"><span>already being developed</span></a><span> to provide a global map of where these leaks are coming from.</span></p><p><span>The third sector that contributes a lot is waste. Methane is produced when organic material — like food waste or paper — rots in conditions without much oxygen (like in a landfill). Securely sealing landfills or capturing this methane for energy can effectively reduce these emissions. Methane from waste has been falling in many richer countries — like </span><a href="https://ourworldindata.org/explorers/air-pollution?time=1990..2022&amp;country=~GBR&amp;Pollutant=Methane&amp;Sector=Waste&amp;Per+capita=false"><span>the United Kingdom</span></a><span> — that have implemented these strategies.</span></p><h2 id="ammonia-nh3-it-s-all-about-farming"><span>Ammonia (NH</span><sub><span>3</span></sub><span>): it’s all about farming</span><a href="#ammonia-nh3-it-s-all-about-farming"></a></h2><p><span>As the chart below shows, nearly all human emissions of ammonia (NH</span><sub><span>3</span></sub><span>) come from agriculture. When we add nitrogen to crops as synthetic fertilizers or manure, some of this nitrogen is converted into NH</span><sub><span>3</span></sub><span> in the soil. Other smaller sources include decomposing organic waste in landfills and energy production.</span></p><p><span>Although NH</span><sub><span>3 </span></sub><span>doesn’t stay in the atmosphere for long — typically hours to days — it can react with other gases to form small particulates that harm human health.</span></p><p><span>Some studies suggest ammonia could drive </span><strong><span>several hundred thousand (up to 385,000) premature deaths</span></strong><span> from particulate matter.</span><a href="#note-11"><sup><span>11</span></sup></a></p><p><span>In the charts below, you can see that unlike most other air pollutants, where emissions have peaked globally, emissions of NH</span><sub><span>3 </span></sub><span>have continued to rise as </span><a href="https://ourworldindata.org/meat-production" target="_blank" rel="noopener"><span>livestock production</span></a><span> and the use of synthetic </span><a href="https://ourworldindata.org/fertilizers" target="_blank" rel="noopener"><span>fertilizers</span></a><span> have increased.</span></p><p><span>Some countries — particularly </span><a href="https://ourworldindata.org/explorers/air-pollution?time=1950..2022&amp;Pollutant=Ammonia&amp;Sector=All+sectors+%28total%29&amp;Per+capita=false&amp;country=GBR~FRA~DEU"><span>those in Europe</span></a><span> — have achieved small reductions in emissions because they </span><a href="https://ourworldindata.org/effective-policies-reducing-environmental-impacts-agriculture" target="_blank" rel="noopener"><span>use less fertilizers</span></a><span> than a few decades ago.</span></p><h2 id="non-methane-volatile-organic-compounds-nmvo-cs"><span>Non-methane volatile organic compounds (NMVOCs)</span><a href="#non-methane-volatile-organic-compounds-nmvo-cs"></a></h2><p><span>Non-methane volatile organic compounds (NMVOCs) can threaten human health through all three of the pathways we looked at earlier: they can be directly toxic in high concentrations and mix with other gases to form ozone and small particulates.</span></p><p><span>You can see the global sources of NMVOCs in the chart below.</span></p><p><span>NMVOCs are produced by traditional pollution sources like burning fossil fuels and car exhausts. However, unlike most other pollutants, solvents such as paints, cleaning products, and chemical plants are also major sources.</span></p><p><span>In addition to switching to low-carbon energy and phasing out gasoline cars, we also need to reduce the use of volatile organic compounds (VOCs) in personal care products and solvents. Setting emission limits on the chemicals industry will also be key to lowering our exposure to non-methane VOCs (NMVOCs).</span></p><h2 id="while-pollution-sources-are-diverse-the-solutions-are-often-not"><span>While pollution sources are diverse, the solutions are often not</span><a href="#while-pollution-sources-are-diverse-the-solutions-are-often-not"></a></h2><p><span>Going through so many pollutants, one by one — as we just did — can seem overwhelming. We don’t need to just tackle one or two; we need to tackle more than six.</span><a href="#note-12"><sup><span>12</span></sup></a></p><p><span>The good news is that the solutions we need often cut across several gases at the same time.</span></p><p><span>Burning stuff for energy — whether that’s fossil fuels or biomass — is the root source for many of these gases. Moving to clean energy — deploying renewable or nuclear electricity, </span><a href="https://ourworldindata.org/electric-car-sales" target="_blank" rel="noopener"><span>electrifying our cars</span></a><span>, our industry, and home heating — and ensuring that people worldwide </span><a href="https://ourworldindata.org/worlds-energy-problem" target="_blank" rel="noopener"><span>have access</span></a><span> to </span><em><span>modern</span></em><span> energy sources would simultaneously cut many of these pollutants.</span></p><p><span>Reducing meat production and consumption </span><a href="https://ourworldindata.org/food-choice-vs-eating-local" target="_blank" rel="noopener"><span>by shifting to</span></a><span> more plant-based diets would reduce methane and ammonia emissions at the same time, too.</span></p><p><span>These transitions come with large health benefits, just from reducing air pollution alone.</span></p><p><span>And we know that it can be done. Many countries </span><em><span>have</span></em><span> dramatically reduced levels of air pollution, and as you can see in the chart below, they’ve prevented hundreds of thousands of early deaths as a result.</span></p><figure><figcaption><span>The total number of deaths has declined in these countries despite much larger older populations. The </span><a href="https://ourworldindata.org/grapher/death-rate-from-air-pollution-per-100000?tab=chart&amp;country=GBR~USA~NOR~DEU~SWE"><span>decline in</span></a><span> death </span><em><span>rates</span></em><span> has been even larger.</span></figcaption></figure><div><figure data-grapher-src="https://ourworldindata.org/grapher/deaths-from-household-and-outdoor-air-pollution?country=GBR~USA~CAN~DEU~NOR~SWE"><a href="https://ourworldindata.org/grapher/deaths-from-household-and-outdoor-air-pollution?country=GBR~USA~CAN~DEU~NOR~SWE" target="_blank" rel="noopener"><picture><source id="grapher-preview-source" srcset="https://ourworldindata.org/grapher/deaths-from-household-and-outdoor-air-pollution.png?country=GBR~USA~CAN~DEU~NOR~SWE&amp;imWidth=850 850w, https://ourworldindata.org/grapher/deaths-from-household-and-outdoor-air-pollution.png?country=GBR~USA~CAN~DEU~NOR~SWE&amp;imWidth=1700 1700w" sizes="(max-width: 850px) 100vw, 850px"><img src="https://ourworldindata.org/grapher/deaths-from-household-and-outdoor-air-pollution.png?country=GBR~USA~CAN~DEU~NOR~SWE" width="850" height="600" loading="lazy" data-no-lightbox="true"></picture></a></figure></div><div><h4>Acknowledgments</h4><p><span>Many thanks to Max Roser and Edouard Mathieu for their feedback and comments on this article.</span></p></div><h2 id="continue-reading-on-our-world-in-data"><span>Continue reading on Our World in Data</span><a href="#continue-reading-on-our-world-in-data"></a></h2><a href="https://ourworldindata.org/cleanest-air-lessons"><div><picture><source srcset="https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/2634177d-6b60-4df5-3152-c38263399f00/w=48 48w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/2634177d-6b60-4df5-3152-c38263399f00/w=100 100w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/2634177d-6b60-4df5-3152-c38263399f00/w=350 350w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/2634177d-6b60-4df5-3152-c38263399f00/w=850 850w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/2634177d-6b60-4df5-3152-c38263399f00/w=1350 1350w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/2634177d-6b60-4df5-3152-c38263399f00/w=2400 2400w" type="image/png" sizes="350px"><img src="https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/2634177d-6b60-4df5-3152-c38263399f00/w=2400" alt="Featured image" loading="lazy" data-filename="reducing-air-pollution-featured.png" width="2400" height="1260"></picture></div><div><p><h3>In many countries, people breathe the cleanest air in centuries. What can the rest of the world learn from this?</h3></p><p>Air pollution tends to get worse before it gets better, but how can we accelerate this transition?</p></div></a><a href="https://ourworldindata.org/explorers/air-pollution"><div><p><h3> Data Explorer</h3></p></div></a><a href="https://ourworldindata.org/data-review-air-pollution-deaths"><div><picture><source srcset="https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/02062e42-9ceb-45b2-bad7-aed8d5169d00/w=48 48w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/02062e42-9ceb-45b2-bad7-aed8d5169d00/w=100 100w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/02062e42-9ceb-45b2-bad7-aed8d5169d00/w=350 350w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/02062e42-9ceb-45b2-bad7-aed8d5169d00/w=850 850w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/02062e42-9ceb-45b2-bad7-aed8d5169d00/w=1200 1200w" type="image/png" sizes="350px"><img src="https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/02062e42-9ceb-45b2-bad7-aed8d5169d00/w=1200" loading="lazy" data-filename="air-pollution-deaths-thumbnail.png" width="1200" height="630"></picture></div><div><p><h3>Data review: how many people die from air pollution?</h3></p><p>This data review presents published estimates of the global death toll from air pollution and provides the context that makes them understandable.</p></div></a><div><h3 id="article-endnotes">Endnotes</h3><ol><li id="note-1"><p><span>Here, we’re not talking about greenhouse gases that drive climate change — which we cover in great detail elsewhere — although we will include a few greenhouse gases, such as methane, which can also act as a precursor to local air pollutants.</span></p></li><li id="note-2"><p><span>Hoesly, R. M., Smith, S. J., Feng, L., Klimont, Z., Janssens-Maenhout, G., Pitkanen, T., ... &amp; Zhang, Q. (2018). </span><a href="https://gmd.copernicus.org/articles/11/369/2018/gmd-11-369-2018.html" target="_blank" rel="noopener"><span>Historical (1750–2014) anthropogenic emissions of reactive gases and aerosols from the Community Emissions Data System (CEDS)</span></a><span>. Geoscientific Model Development, 11(1), 369-408.</span></p></li><li id="note-3"><p><span>World Health Organization (2021). </span><a href="https://www.who.int/publications/i/item/9789240034228" target="_blank" rel="noopener"><span>WHO global air quality guidelines: particulate matter (‎PM2.5 and PM10)‎, ozone, nitrogen dioxide, sulfur dioxide and carbon monoxide</span></a><span>.</span></p></li><li id="note-4"><p><span>Note that we’re talking about ground-level, or tropospheric, ozone in the lower atmosphere. At this level, it’s considered a pollutant. This differs from </span><em><span>stratospheric</span></em><span> ozone, which is high in the atmosphere and crucial for protecting us from ultraviolet radiation. We cover this in our work on the </span><a href="https://ourworldindata.org/ozone-layer" target="_blank" rel="noopener"><span>Ozone Layer</span></a><span>.</span></p><p><span>* Mar, K. A., Unger, C., Walderdorff, L., &amp; Butler, T. (2022). Beyond CO2 equivalence: The impacts of methane on climate, ecosystems, and health. Environmental science &amp; policy.</span></p></li><li id="note-5"><p><span>The Global Burden of Disease is published by the Institute for Health Metrics and Evaluation (IHME).</span></p></li><li id="note-6"><p><span>Note that when we add all of these </span><em><span>individual</span></em><span> risk factors — indoor particulates, outdoor particulates, and outdoor ozone — the total comes to 8.3 million, which is higher than the Global Burden of Disease reports on aggregate. This is because different risk factors can combine to increase health problems and the risk of premature death. In </span><a href="https://ourworldindata.org/how-do-researchers-estimate-the-death-toll-caused-by-each-risk-factor-whether-its-smoking-obesity-or-air-pollution" target="_blank" rel="noopener"><span>this article</span></a><span>, our colleague, Saloni Dattani, examines how risk factors are estimated and why they can’t be summed up to give the total number of premature deaths.</span></p></li><li id="note-7"><p><span>Fioletov, V. E., McLinden, C. A., Krotkov, N., Li, C., Joiner, J., Theys, N., ... &amp; Moran, M. D. (2016). </span><a href="https://acp.copernicus.org/articles/16/11497/2016/" target="_blank" rel="noopener"><span>A global catalogue of large SO2 sources and emissions derived from the Ozone Monitoring Instrument</span></a><span>. Atmospheric Chemistry and Physics, 16(18), 11497-11519.</span></p></li><li id="note-8"><p><span>Pan, S. Y., He, K. H., Lin, K. T., Fan, C., &amp; Chang, C. T. (2022). </span><a href="https://www.nature.com/articles/s41612-022-00265-3" target="_blank" rel="noopener"><span>Addressing nitrogenous gases from croplands toward low-emission agriculture</span></a><span>. Npj Climate and Atmospheric Science.</span></p></li><li id="note-9"><p><span>This estimate comes from the UN Environment Programme and Climate and Clean Air Coalition:UNEP and Climate and Clean Air Coalition (2021) </span><a href="https://www.unep.org/resources/report/global-methane-assessment-benefits-and-costs-mitigating-methane-emissions" target="_blank" rel="noopener"><span>Global Methane Assessment: Benefits and Costs of Mitigating Methane Emissions</span></a><span>.</span></p></li><li id="note-10"><p><span>Although this is not the only reason they have a high carbon footprint, even </span><a href="https://ourworldindata.org/carbon-footprint-food-methane" target="_blank" rel="noopener"><span>when we ignore methane</span></a><span>, they still emit much carbon through land use and manure.</span></p></li><li id="note-11"><p><span>Wyer, K. E., Kelleghan, D. B., Blanes-Vidal, V., Schauberger, G., &amp; Curran, T. P. (2022). Ammonia emissions from agriculture and their contribution to fine particulate matter: A review of implications for human health. Journal of Environmental Management, 323, 116285.</span></p></li><li id="note-12"><p><span>We included six of the big ones in this article, but it’s not a complete list.</span></p></li></ol></div><div id="article-citation"><h3>Cite this work</h3><p>Our articles and data visualizations rely on work from many different people and organizations. When citing this article, please also cite the underlying data sources. This article can be cited as:</p><div><pre><code>Hannah Ritchie and Pablo Rosado (2025) - “Air pollution kills millions every year&nbsp;— where does it come from?” Published online at OurWorldinData.org. Retrieved from: 'https://ourworldindata.org/air-pollution-sources' [Online Resource]</code></pre></div><p>BibTeX citation</p><div><pre><code>@article{owid-air-pollution-sources,
    author = {Hannah Ritchie and Pablo Rosado},
    title = {Air pollution kills millions every year&nbsp;— where does it come from?},
    journal = {Our World in Data},
    year = {2025},
    note = {https://ourworldindata.org/air-pollution-sources}
}</code></pre></div></div><div id="article-licence"><p><img src="https://ourworldindata.org/owid-logo.svg" alt="Our World in Data logo" width="104" height="57"></p><h3>Reuse this work freely</h3><p>All visualizations, data, and code produced by Our World in Data are completely open access under the <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank" rel="noopener">Creative Commons BY license</a>. You have the permission to use, distribute, and reproduce these in any medium, provided the source and authors are credited.</p><p>The data produced by third parties and made available by Our World in Data is subject to the license terms from the original third-party authors. We will always indicate the original source of the data in our documentation, so you should always check the license of any such third-party data before use and redistribution.</p><p>All of <a href="https://ourworldindata.org/faqs#how-can-i-embed-one-of-your-interactive-charts-in-my-website">our charts can be embedded</a> in any site.</p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Val Kilmer has passed away (110 pts)]]></title>
            <link>https://www.nytimes.com/2025/04/01/movies/val-kilmer-dead.html</link>
            <guid>43553573</guid>
            <pubDate>Wed, 02 Apr 2025 04:04:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2025/04/01/movies/val-kilmer-dead.html">https://www.nytimes.com/2025/04/01/movies/val-kilmer-dead.html</a>, See on <a href="https://news.ycombinator.com/item?id=43553573">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2025/04/01/movies/val-kilmer-dead.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[The state of binary compatibility on Linux and how to address it (123 pts)]]></title>
            <link>https://jangafx.com/insights/linux-binary-compatibility</link>
            <guid>43551934</guid>
            <pubDate>Tue, 01 Apr 2025 22:22:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jangafx.com/insights/linux-binary-compatibility">https://jangafx.com/insights/linux-binary-compatibility</a>, See on <a href="https://news.ycombinator.com/item?id=43551934">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="linuxBinaryCompatibilityPage"><h2>The Atrocious State Of Binary Compatibility on Linux and How To Address It.</h2>
<p>By Dale Weiler <a href="https://github.com/graphitemaster/">GitHub</a></p>
<ul>
<li>Time To Read: ~30 Minutes</li>
<li>Last Updated: Monday, March 17th 2025</li>
</ul>
<h2>Summary</h2>
<p>Linux binary compatibility is plagued by one thing that is often overlooked when evaluating shipping software on Linux. This article will deconstruct how to arrive to that conclusion, how to address it when shipping software today and what needs to be done to actually fix it.</p>
<p>Table of contents</p>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#containers">Containers</a></li>
<li><a href="#versioning">Versioning</a></li>
<li><a href="#system-libraries">System Libraries</a></li>
<li><a href="#our-approach">Our Approach</a></li>
<li><a href="#fixing-it">Fixing It</a></li>
<li><a href="#questioning-it">Questioning It</a></li>
</ul>
<h2>Introduction</h2>
<p>At JangaFX, we make several products that run natively on Linux. We love the flexibility and power that Linux offers our developers, but shipping software on it is a whole different challenge.</p>
<p>Linux is an incredibly powerful platform, but when it comes to shipping software, it can feel like a minefield. Unlike other operating systems, Linux isn’t just one system—it’s a chaotic mix of different services, libraries, and even philosophies. Every distribution does things a little differently, and that means the same executable that works flawlessly on one system might completely break on another.</p>
<p>This shouldn’t even be a problem. The Linux kernel itself has maintained relatively stable system calls. But everything built on top of it changes constantly in ways that break compatibility, making it incredibly frustrating to ship software that "just works." If you’re developing for Linux, you’re not targeting a single platform—you’re navigating an ecosystem that has evolved without much concern for binary compatibility.</p>
<p>Some of us, coming from the game industry before moving into VFX have dealt with this problem before. Shipping games on Linux has always been a nightmare, and the same issues persists regardless of industry. In this article, We're going to explain why we think containers are <strong>the wrong approach</strong>, how we build and ship Linux software in a way that actually works, what we think is responsible for Linux's binary compatibility problem and what needs to change to fix it.</p>
<h4>The latter part of this article will get deeply technical about what exactly the problem is and how it can be fixed.</h4>
<h2>Containers</h2>
<p>Tools like <a href="https://flatpak.org/">Flatpak</a>, <a href="https://appimage.org/">AppImage</a>, and similar solutions attempt to simplify shipping executables by creating "containers"—or as as we've recently taken to calling them, <strong>"a Linux Environment inside a Linux"</strong> Using Linux features like <a href="https://en.wikipedia.org/wiki/Linux_namespaces">namespaces</a> and <a href="https://en.wikipedia.org/wiki/Chroot">chroots</a>, these solutions package an entire Linux environment, complete with all required dependencies, into a single self-contained bundle. In extreme cases, this means shipping an entire Linux user-space just for one application.</p>
<p>One of the major challenges with these containerized solutions is that they
often don’t work well with applications that need to interact with the rest of
the system. To access hardware-accelerated APIs like
<a href="https://en.wikipedia.org/wiki/OpenGL">OpenGL</a>,
<a href="https://en.wikipedia.org/wiki/Vulkan_(API)">Vulkan</a>,
<a href="https://en.wikipedia.org/wiki/VDPAU">VDPAU</a> or
<a href="https://developer.nvidia.com/cuda-zone">CUDA</a>, an application must dynamically
link against the system's graphics driver libraries. Since these libraries exist
outside the container and cannot be shipped with the application, various
"pass-through" techniques have been developed to work around this, some of which
introduce runtime overhead (e.g., shimming libraries). Because containerized applications are isolated from the system, they often <strong>feel</strong> isolated too. This creates consistency issues, where the application may not recognize the user’s name, home directory, system settings, desktop environment preferences, or even have proper access to the filesystem.</p>
<p>To work around these limitations, many containerized environments rely on the
<a href="https://wiki.archlinux.org/title/XDG_Desktop_Portal"><strong>XDG Desktop Portal</strong></a>
protocol, which introduces yet another layer of complexity. This system requires
IPC (inter-process communication) through
<a href="https://en.wikipedia.org/wiki/D-Bus"><strong>DBus</strong></a> just to grant applications
access to basic system features like file selection, opening URLs, or reading
system settings—problems that wouldn’t exist if the application weren’t
artificially sandboxed in the first place.</p>
<p>We don’t believe that piling on more layers is an acceptable solution. As engineers, we need to stop and ask ourselves: "should we keep adding to this tower of Babel?", or is it time to peel back some of these abstractions and reevaluate them? At some point, the right solution isn’t more complexity—it’s less.</p>
<p>While containerized solutions can work under certain conditions, we believe that shipping lean, native executables—without containers—provides a more seamless and integrated experience that better aligns with user expectations.</p>
<h2>Versioning</h2>
<p>When you compile your application, it links against the specific library versions present on the build machine. This means that by default, the versions on a user's system may not match, causing compatibility issues. Let’s assume the user has all the necessary libraries installed, but the versions don’t match what your application was built against. This is where the real problem begins.
Short of shipping the exact machine used to deploy your application, how do you ensure compatibility with the versions installed on a user’s system?</p>
<p>We believe there are two ways to solve this problem, and we've given them our own names:</p>
<ol>
<li><strong>Replication Approach</strong> – This means bundling all the libraries from the build machine and shipping them with your application. This is the philosophy behind Flatpak and AppImage. We do <strong>not</strong> use this approach at JangaFX.</li>
<li><strong>Relaxation Approach</strong> – Instead of relying on specific or newer library versions, you link against versions that are so old they’re almost guaranteed to be compatible everywhere. This minimizes the risk of incompatibility on a user’s system.</li>
</ol>
<p>The first approach works well in cases where the necessary libraries may not exist on a user’s machine, but it fails for libraries that cannot be shipped (we call these "system libraries") The second approach is particularly effective for system libraries and is the approach we use at JangaFX.</p>
<h2>System Libraries</h2>
<p>There are various libraries present on a Linux machine that cannot be shipped because they are system libraries. These are libraries tied to the system itself and cannot be provided in a container. Typically these include things like user-space drivers for the GPU, enterprise installed components for security, and of course, <a href="https://en.wikipedia.org/wiki/C_standard_library">libc</a> itself.</p>
<p>If you’ve ever tried to distribute Linux binaries, you may have encountered an error message like this:</p>
<pre><code>/lib64/libc.so.6: version `GLIBC_2.18' not found
</code></pre>
<blockquote>
<p>For those unaware, <strong>glibc</strong> (GNU C Library) provides the C standard library, POSIX APIs, and the dynamic linker responsible for loading shared libraries, and itself.</p>
</blockquote>
<p>GLIBC is an example of a "system library" that cannot be bundled with your
application because it includes the dynamic linker itself. This linker is
responsible for loading other libraries, some of which may also depend on
GLIBC—but not always. Complicating matters further, since GLIBC is a dynamic
library, it must also load itself. This self-referential, chicken-and-egg
problem highlights GLIBC’s complexity and monolithic design, as it attempts to
fulfill multiple roles simultaneously. A large downside to this monolithic
design is that upgrading GLIBC often requires upgrading the entire system. Later in this article, we will
explain why this structure needs to change to truly solve Linux’s binary
compatibility problem.</p>
<p>Before you suggest statically linking GLIBC—<strong>that’s not an option.</strong> GLIBC relies on dynamic linking for features like <a href="https://en.wikipedia.org/wiki/Name_Service_Switch">NSS</a> modules, which handle hostname resolution, user authentication, and network configuration, among other dynamically loaded components. Static linking breaks this because it does not include the dynamic linker, which is why GLIBC does not officially support it. Even if you managed to statically link GLIBC—or used an alternative like <a href="https://musl.libc.org/">musl</a>—your application would be unable to load any dynamic libraries at runtime. Static linking the dynamic linker itself is not possible, for reasons that will be explained later. In short, this would prevent your application from dynamically linking against any system libraries at all.</p>
<h2>Our Approach</h2>
<p>Since our application relies on many non-system libraries that may not be installed on the user’s system, we need a way to include them. The most straightforward approach is the <strong>Replication Approach</strong>, where we ship these libraries alongside our application. However, this negates the benefits of dynamic linking, such as shared memory usage and system-wide updates. In such cases, statically linking these libraries into the application is a better choice, as it eliminates dependency issues entirely. It also enables additional optimizations, such as <a href="https://en.wikipedia.org/wiki/Interprocedural_optimization">LTO</a>, and results in a smaller package by stripping unused components from the included libraries.</p>
<p>Instead, we take a different approach: <strong>statically linking everything we can</strong>. When doing so, special care is needed if a dependency embeds another dependency within its static library. We've encountered static libraries that include object files from other static libraries (e.g., libcurl), but we still need to link them separately. This duplication is conveniently avoided with dynamic libraries, but with static libraries, you may need to extract all object files from the archive and remove the embedded ones manually. Similarly, compiler runtimes like <code>libgcc</code> default to dynamic linking. We recommend using <code>-static-libgcc</code>.</p>
<p>Finally, when it comes to dealing with system libraries, we use the <strong>Relaxation Approach</strong>. Rather than requiring exact or newer versions of system libraries, we link against versions that are old enough to be nearly universally compatible. This increases the likelihood that the user’s system libraries will work with our application, reducing dependency issues without the need for containerization or bundling system components and shims.</p>
<p>The method we suggest when linking against older system libraries is to obtain a corresponding older Linux environment. You don’t need to install an old Linux version on physical hardware or even set up a full virtual machine. Instead, a <strong>chroot</strong> provides a lightweight, isolated environment within an existing Linux installation, allowing you to build against an older system without the overhead of full virtualization. Ironically, this suggests that containers were the right solution all along—just not at runtime, but at build time.</p>
<p>To achieve this, we use <a href="https://salsa.debian.org/installer-team/debootstrap">debootstrap</a>, an excellent script that creates a minimal Debian installation from scratch. Debian is particularly suited for this approach due to its stability and long-term support for older releases, making it a great choice for ensuring compatibility with older system libraries.</p>
<p>Of course, once you have an older Linux setup, you may find that its binary package toolchains are too outdated to build your software. To address this, we compile a modern LLVM toolchain from source and use it to build both our dependencies and our software. The details of this process are beyond the scope of this article.</p>
<p>Finally, we automate the entire debootstrap process with a Python script, which we've included here for reference.</p>
<pre><p><code><span>#!/bin/env python3</span><span>
</span><span></span><span>import</span><span> os</span><span>,</span><span> subprocess</span><span>,</span><span> shutil</span><span>,</span><span> multiprocessing
</span>
<span>PACKAGES </span><span>=</span><span> </span><span>[</span><span> </span><span>'build-essential'</span><span> </span><span>]</span><span>
</span><span>DEBOOSTRAP </span><span>=</span><span> </span><span>'https://salsa.debian.org/installer-team/debootstrap.git'</span><span>
</span><span>ARCHIVE </span><span>=</span><span> </span><span>'http://archive.debian.org/debian'</span><span>
</span><span>VERSION </span><span>=</span><span> </span><span>'jessie'</span><span> </span><span># Released in 2015</span><span>
</span>
<span></span><span>def</span><span> </span><span>chroot</span><span>(</span><span>pipe</span><span>)</span><span>:</span><span>
</span><span>  </span><span>try</span><span>:</span><span>
</span><span>    os</span><span>.</span><span>chroot</span><span>(</span><span>'chroot'</span><span>)</span><span>
</span><span>    os</span><span>.</span><span>chdir</span><span>(</span><span>'/'</span><span>)</span><span>
</span>
<span>    </span><span># Setup an environment for the chroot</span><span>
</span><span>    env </span><span>=</span><span> </span><span>{</span><span>
</span><span>      </span><span>'HOME'</span><span>:</span><span> </span><span>'/root'</span><span>,</span><span>
</span><span>      </span><span>'TERM'</span><span>:</span><span> </span><span>'xterm'</span><span>,</span><span>
</span><span>      </span><span>'PATH'</span><span>:</span><span> </span><span>'/bin:/usr/bin:/sbin:/usr/sbin'</span><span>
</span><span>    </span><span>}</span><span>
</span>
<span>    </span><span># The Debian is going to be quite old and so the keyring keys will likely be</span><span>
</span><span>    </span><span># expired. To work around this we will replace the sources.list to contain</span><span>
</span><span>    </span><span># '[trusted=yes]'</span><span>
</span><span>    </span><span>with</span><span> </span><span>open</span><span>(</span><span>'/etc/apt/sources.list'</span><span>,</span><span> </span><span>'w'</span><span>)</span><span> </span><span>as</span><span> fp</span><span>:</span><span>
</span><span>      fp</span><span>.</span><span>write</span><span>(</span><span>f'deb [trusted=yes] http://archive.debian.org/debian </span><span>{</span><span>VERSION</span><span>}</span><span> main\n'</span><span>)</span><span>
</span>
<span>    </span><span># Update and install packages</span><span>
</span><span>    subprocess</span><span>.</span><span>run</span><span>(</span><span>[</span><span>'apt'</span><span>,</span><span> </span><span>'update'</span><span>]</span><span>,</span><span> env</span><span>=</span><span>env</span><span>)</span><span>
</span><span>    subprocess</span><span>.</span><span>run</span><span>(</span><span>[</span><span>'apt'</span><span>,</span><span> </span><span>'install'</span><span>,</span><span> </span><span>'-y'</span><span>,</span><span> </span><span>*</span><span>PACKAGES</span><span>]</span><span>,</span><span> env</span><span>=</span><span>env</span><span>)</span><span>
</span>
<span>    </span><span>#</span><span>
</span><span>    </span><span># Script your Linux here, remember to pass `env=env` to subprocess.run.</span><span>
</span><span>    </span><span>#</span><span>
</span><span>    </span><span># We suggest downloading GCC 7.4.0, compiling from source, and installing</span><span>
</span><span>    </span><span># it since it's the minimum version required to compile the latest LLVM from</span><span>
</span><span>    </span><span># source. We then suggest downloading, compiling from source, and installing</span><span>
</span><span>    </span><span># the latest LLVM, which as of time of writing is 20.1.0.</span><span>
</span><span>    </span><span>#</span><span>
</span><span>    </span><span># You can then compile and install all other source packages your software</span><span>
</span><span>    </span><span># requires from source using this modern LLVM toolchain.</span><span>
</span><span>    </span><span>#</span><span>
</span><span>    </span><span># You can also enter the chroot with an interactive shell from this script</span><span>
</span><span>    </span><span># by uncommenting the following and running this script as usual.</span><span>
</span><span>    </span><span>#  subprocess.run(['bash'])</span><span>
</span><span>    </span><span>#</span><span>
</span>
<span>    </span><span># You can send messages to the parent with pipe.send()</span><span>
</span><span>    pipe</span><span>.</span><span>send</span><span>(</span><span>'Done'</span><span>)</span><span> </span><span># This one has special meaning in main</span><span>
</span><span>  </span><span>except</span><span> Exception </span><span>as</span><span> exception</span><span>:</span><span>
</span><span>    pipe</span><span>.</span><span>send</span><span>(</span><span>exception</span><span>)</span><span>
</span>
<span></span><span>def</span><span> </span><span>main</span><span>(</span><span>)</span><span>:</span><span>
</span><span>  </span><span># We need to run as root to use 'mount', 'umount', and 'chroot'</span><span>
</span><span>  </span><span>if</span><span> os</span><span>.</span><span>geteuid</span><span>(</span><span>)</span><span> </span><span>!=</span><span> </span><span>0</span><span>:</span><span>
</span><span>    </span><span>print</span><span>(</span><span>'Script must be run as root'</span><span>)</span><span>
</span><span>    </span><span>return</span><span> </span><span>False</span><span>
</span>
<span>  </span><span>with</span><span> multiprocessing</span><span>.</span><span>Manager</span><span>(</span><span>)</span><span> </span><span>as</span><span> manager</span><span>:</span><span>
</span><span>    mounts </span><span>=</span><span> manager</span><span>.</span><span>list</span><span>(</span><span>)</span><span>
</span><span>    pipe </span><span>=</span><span> multiprocessing</span><span>.</span><span>Pipe</span><span>(</span><span>)</span><span>
</span><span>    </span><span>def</span><span> </span><span>mount</span><span>(</span><span>parts</span><span>)</span><span>:</span><span>
</span><span>      subprocess</span><span>.</span><span>run</span><span>(</span><span>[</span><span>'mount'</span><span>,</span><span> </span><span>*</span><span>parts</span><span>]</span><span>)</span><span>
</span><span>      mounts</span><span>.</span><span>append</span><span>(</span><span>parts</span><span>[</span><span>-</span><span>1</span><span>]</span><span>)</span><span>
</span>
<span>    </span><span># Ensure we have a fresh chroot and clone of debootstrap</span><span>
</span><span>    shutil</span><span>.</span><span>rmtree</span><span>(</span><span>'chroot'</span><span>,</span><span> ignore_errors</span><span>=</span><span>True</span><span>)</span><span>
</span><span>    shutil</span><span>.</span><span>rmtree</span><span>(</span><span>'debootstrap'</span><span>,</span><span> ignore_errors</span><span>=</span><span>True</span><span>)</span><span>
</span><span>    os</span><span>.</span><span>mkdir</span><span>(</span><span>'chroot'</span><span>)</span><span>
</span>
<span>    </span><span># Clone debootstrap</span><span>
</span><span>    subprocess</span><span>.</span><span>run</span><span>(</span><span>[</span><span>'git'</span><span>,</span><span> </span><span>'clone'</span><span>,</span><span> DEBOOSTRAP</span><span>]</span><span>)</span><span>
</span><span>    subprocess</span><span>.</span><span>run</span><span>(</span><span>[</span><span>'debootstrap'</span><span>,</span><span> </span><span>'--arch'</span><span>,</span><span> </span><span>'amd64'</span><span>,</span><span> VERSION</span><span>,</span><span> </span><span>'../chroot'</span><span>,</span><span> ARCHIVE</span><span>]</span><span>,</span><span>
</span><span>                    env</span><span>=</span><span>{</span><span>**</span><span>os</span><span>.</span><span>environ</span><span>,</span><span> </span><span>'DEBOOTSTRAP_DIR'</span><span>:</span><span> </span><span>'.'</span><span>}</span><span>,</span><span>
</span><span>                    cwd</span><span>=</span><span>'debootstrap'</span><span>)</span><span>
</span>
<span>    </span><span># Mount nodes needed for the chroot</span><span>
</span><span>    mount</span><span>(</span><span>[</span><span>'-t'</span><span>,</span><span> </span><span>'proc'</span><span>,</span><span> </span><span>'/proc'</span><span>,</span><span> </span><span>'chroot/proc'</span><span>]</span><span>)</span><span>
</span><span>    mount</span><span>(</span><span>[</span><span>'--rbind'</span><span>,</span><span> </span><span>'/sys'</span><span>,</span><span> </span><span>'chroot/sys'</span><span>]</span><span>)</span><span>
</span><span>    mount</span><span>(</span><span>[</span><span>'--make-rslave'</span><span>,</span><span> </span><span>'chroot/sys'</span><span>]</span><span>)</span><span>
</span><span>    mount</span><span>(</span><span>[</span><span>'--rbind'</span><span>,</span><span> </span><span>'/dev'</span><span>,</span><span> </span><span>'chroot/dev'</span><span>]</span><span>)</span><span>
</span><span>    mount</span><span>(</span><span>[</span><span>'--make-rslave'</span><span>,</span><span> </span><span>'chroot/dev'</span><span>]</span><span>)</span><span>
</span>
<span>    </span><span># Setup the chroot in a separate process</span><span>
</span><span>    process </span><span>=</span><span> multiprocessing</span><span>.</span><span>Process</span><span>(</span><span>target</span><span>=</span><span>chroot</span><span>,</span><span> args</span><span>=</span><span>(</span><span>pipe</span><span>[</span><span>1</span><span>]</span><span>,</span><span>)</span><span>)</span><span>
</span><span>    process</span><span>.</span><span>start</span><span>(</span><span>)</span><span>
</span><span>    </span><span>try</span><span>:</span><span>
</span><span>      </span><span>while</span><span> </span><span>True</span><span>:</span><span>
</span><span>        data </span><span>=</span><span> pipe</span><span>[</span><span>0</span><span>]</span><span>.</span><span>recv</span><span>(</span><span>)</span><span>
</span><span>        </span><span>if</span><span> </span><span>isinstance</span><span>(</span><span>data</span><span>,</span><span> Exception</span><span>)</span><span>:</span><span>
</span><span>          </span><span>raise</span><span> data
</span><span>        </span><span>else</span><span>:</span><span>
</span><span>          </span><span>print</span><span>(</span><span>data</span><span>)</span><span>
</span><span>          </span><span>if</span><span> data </span><span>==</span><span> </span><span>'Done'</span><span>:</span><span>
</span><span>            </span><span>break</span><span>
</span><span>    </span><span>finally</span><span>:</span><span>
</span><span>      process</span><span>.</span><span>join</span><span>(</span><span>)</span><span>
</span><span>      </span><span>for</span><span> umount </span><span>in</span><span> </span><span>reversed</span><span>(</span><span>list</span><span>(</span><span>set</span><span>(</span><span>mounts</span><span>)</span><span>)</span><span>)</span><span>:</span><span>
</span><span>        subprocess</span><span>.</span><span>run</span><span>(</span><span>[</span><span>'umount'</span><span>,</span><span> </span><span>'-R'</span><span>,</span><span> umount</span><span>]</span><span>)</span><span>
</span><span>        subprocess</span><span>.</span><span>run</span><span>(</span><span>[</span><span>'sync'</span><span>]</span><span>)</span><span>
</span>
<span></span><span>if</span><span> __name__ </span><span>==</span><span> </span><span>'__main__'</span><span>:</span><span>
</span><span>  </span><span>try</span><span>:</span><span>
</span><span>    main</span><span>(</span><span>)</span><span>
</span><span>  </span><span>except</span><span> KeyboardInterrupt</span><span>:</span><span>
</span><span>    </span><span>print</span><span>(</span><span>'Cancelled'</span><span>)</span></code></p></pre>
<h2>Fixing it</h2>
<p>Generally, most applications do not link directly against system libraries, and instead load which ever is present on the user's machine already at runtime. So while these libraries are considered system components, they typically have few system dependencies beyond libc itself. This is what makes libc—specifically <strong>GLIBC</strong>—the real source of compatibility issues, it's essentially the only system component directly linked against.</p>
<p>In just the past two years, our team has encountered three separate <strong>GLIBC-specific</strong> compatibility issues, each directly impacting our products:</p>
<ol>
<li><a href="https://sourceware.org/bugzilla/show_bug.cgi?id=29456">https://sourceware.org/bugzilla/show_bug.cgi?id=29456</a></li>
<li><a href="https://sourceware.org/bugzilla/show_bug.cgi?id=32653">https://sourceware.org/bugzilla/show_bug.cgi?id=32653</a></li>
<li><a href="https://sourceware.org/bugzilla/show_bug.cgi?id=32786">https://sourceware.org/bugzilla/show_bug.cgi?id=32786</a></li>
</ol>
<p>From our perspective, the core issue with <strong>GLIBC</strong> is that it tries to do far too much. It’s a massive, monolithic system that handles everything—from system calls and memory management to threading and even the dynamic linker. This tight coupling is why upgrading <strong>GLIBC</strong> often means upgrading the entire system; everything is intertwined. If it were broken into smaller, more focused components, users could update only the parts that change, rather than dragging their whole system along with it.</p>
<p>More importantly, separating the dynamic linker from the <strong>C library</strong> itself would allow multiple versions of <strong>libc</strong> to coexist, eliminating a major source of compatibility issues. This is exactly how <strong>Windows</strong> handles it, which is one of the reasons Windows maintains such strong binary compatibility. You can still run decades-old Windows software today because Microsoft doesn’t force everything to be tied to a single, ever-changing libc.</p>
<p>Of course, this isn’t as simple as just splitting things apart. GLIBC has deep cross-cutting concerns, particularly with threading, TLS (Thread-Local Storage), and global memory management.</p>
<p>For example, even if you managed to get two versions of GLIBC to coexist, returning allocated memory from one and attempting to free it in another would likely lead to serious issues. Their heaps would be unaware of each other, potentially using different allocation strategies, causing unpredictable failures. To avoid this, even the heap would likely need to be separated into its own dedicated <strong>libheap</strong>.</p>
<p>We think a better approach would be breaking GLIBC into distinct libraries, something like this:</p>
<ul>
<li><strong>libsyscall</strong> – Handles making system calls and nothing else. This is
provided as a static library only. Used by <code>libheap</code>, <code>libthread</code> and <code>libc</code> to gain access to shared system call code. Since it's static it's embedded in all three. You can pretend this library otherwise does not exist.</li>
<li><strong>libdl (Dynamic Linker)</strong> – A standalone linker that loads shared libraries.
Links only against <code>libsyscall</code> statically. Is a true, free-standing library, depending on nothing. Provided as both a static and dynamic library. When you link against it statically you can still load things with it dynamically. You just end up with a dynamic linker inside your executable.</li>
<li><strong>libheap</strong> - The single heap shared by all below. Links against <code>libsyscall</code>
statically. Provided only as a dynamic library. Cannot ever be linked against statically.</li>
<li><strong>libthread</strong> – Deals with threading and TLS, links against
<code>libheap</code>. Provided only as a dynamic library. Cannot ever be linked against statically.</li>
<li><strong>libc</strong> – Links against <code>libthread</code>, and thus <code>libheap</code>, and <code>libdl</code> transitively. Provided as both a static and dynamic library. When linking statically it links <code>libdl</code> statically. The linking of <code>libthread</code> and <code>libheap</code> is always done dynamically though through the included <code>libdl</code> if linked statically or through <code>libdl</code> program loader if linked dynamically.</li>
</ul>
<p>These libraries would be aware of each other and allow multiple versions to
coexist in the same address space. That way, we don’t end up with this brittle
mess where upgrading GLIBC can break everything. The actual structure would look
something like</p>
<blockquote>
<p>This architecture is quite similar to Windows, where the equivalents of
<code>libsyscall</code>, <code>libdl</code>, <code>libheap</code>, and <code>libthread</code> are all bundled within a single
<code>kernel32.dll</code>. This DLL is pre-mapped and automatically loaded into the
address space of every executable on Windows.</p>
</blockquote>
<h3><strong>Statically Linked <code>libc</code></strong></h3>
<ul>
<li>Application statically links <code>libc</code> and <code>libdl</code> (it is not the program loader).</li>
<li>Application starts execution and dynamically loads <code>libheap</code> and <code>libthread</code> using embedded <code>libdl</code>.</li>
</ul>
<pre><code>[Application]
   │
   ▼
[libc (static)]
   │
   ▼
[libdl (static)]
   ├── [libheap (dynamic)]
   └── [libthread (dynamic)]
          └── [libheap (dynamic)]
</code></pre>
<ul>
<li><code>libc</code> and <code>libdl</code> are embedded in the executable, meaning the application itself starts execution.</li>
<li>The embedded <code>libdl</code> dynamically loads <code>libthread</code> and <code>libheap</code>.</li>
</ul>
<hr>
<h3><strong>Dynamically Linked <code>libc</code></strong></h3>
<ul>
<li>Application starts execution via the program interpreter (<code>libdl</code>).</li>
<li><code>libdl</code> (program loader) loads the application and resolves dependencies.</li>
<li>Application dynamically links <code>libc</code>, <code>libheap</code>, and <code>libthread</code>.</li>
</ul>
<pre><code>[Application (interpreter entry)]
   │
   ▼
[libdl (program loader)]
   │
   ▼
[libc (dynamic)]
   ├── [libheap (dynamic)]
   ├── [libthread (dynamic)]
   │      └── [libheap (dynamic)]
   ▼
[Application (regular entry)]
</code></pre>
<h3><strong>Comparison Table</strong></h3>
<table><thead><tr><th>Scenario</th><th><code>libdl</code> (included by)</th><th><code>libc</code> (how it loads)</th><th><code>libthread</code> (via <code>libdl</code>)</th><th><code>libheap</code> (via <code>libdl</code>)</th></tr></thead><tbody><tr><td><strong>Static <code>libc</code></strong></td><td>Static linking</td><td>Static linking</td><td><strong>Linked by <code>libdl</code></strong></td><td><strong>Linked by <code>libdl</code></strong></td></tr><tr><td><strong>Dynamic <code>libc</code></strong></td><td>Program interpreter</td><td><strong>Linked by <code>libdl</code></strong></td><td><strong>Linked by <code>libdl</code></strong></td><td><strong>Linked by <code>libdl</code></strong></td></tr></tbody></table>
<p>This architecture effectively reduces the binary compatibility problem to two key <strong>system libraries</strong>: <code>libheap</code> and <code>libthread</code>. These cannot be statically linked because they manage shared resources critical to the entire system.</p>
<p>The reason is straightforward—heap memory must be shared across all components, ensuring compatibility between allocations and deallocations. Similarly, TLS and threading require a unified system-wide approach, as they involve complex initialization and finalization logic, particularly for global constructors and destructors. However, these components are relatively small and stable, meaning they undergo fewer changes that would necessitate version updates.</p>
<h2>Questioning it</h2>
<p>This is, of course, a non-trivial amount of rearchitecting, which naturally raises the question: why is libc implemented the way it is instead of this alternative approach?</p>
<p>Setting historical reasons aside, attempting to solve this problem quickly
becomes difficult the moment you start writing any code using libc. Here is a trivial example of the issues that arise when trying to support multiple
versions of libc.</p>
<p>Suppose you have a dynamic library that contains the following C code.</p>
<pre><p><code><span>#</span><span>include</span><span> </span><span>&lt;stdio.h&gt;</span><span>
</span><span>FILE</span><span>*</span><span> </span><span>open_thing</span><span>(</span><span>)</span><span> </span><span>{</span><span>
</span><span>  </span><span>return</span><span> </span><span>fopen</span><span>(</span><span>"thing.bin"</span><span>,</span><span> </span><span>"r"</span><span>)</span><span>;</span><span>
</span><span></span><span>}</span></code></p></pre>
<p>And your application links against this library and calls <code>open_thing</code>. Your application would be responsible for calling <code>fclose</code> on the returned <code>FILE*</code>. If your code links against a different version of libc from the version the library links against, then it would be calling the wrong implementation of <code>fclose</code>!</p>
<p>Suppose though, that libc was written in such a way that the <code>FILE*</code> returned always required a version field or a pointer to a <a href="https://en.wikipedia.org/wiki/Virtual_method_table">vtable</a> containing the implementation of <code>fclose</code> (and other functions) and every version of libc agreed on this so that it could always call the correct one across this ABI boundary. This would solve this compatibility issue, but now lets say your code calls <code>fflush</code>.</p>
<pre><p><code><span>// Defined in header &lt;stdio.h&gt;</span><span>
</span><span></span><span>int</span><span> </span><span>fflush</span><span>(</span><span>FILE </span><span>*</span><span>fp</span><span>)</span><span>;</span></code></p></pre>
<p>Except it doesn't flush the file, and instead passes <code>NULL</code>.</p>
<pre><p><code><span>fflush</span><span>(</span><span>NULL</span><span>)</span><span>;</span></code></p></pre>
<p>If you're not familiar with C's <code>fflush</code> function, passing <code>NULL</code> to it requires flushing all open files (every <code>FILE*</code>). However, in this scenario, it would only flush files seen by the libc version your application is linked against, not those opened by other libc versions (such as the one used by <code>open_thing</code>).</p>
<p>To handle this correctly, each libc version would need a way to enumerate files across all other libc instances, including dynamically loaded ones, ensuring that every file is visited exactly once without forming cycles. This enumeration must also be thread-safe. Additionally, while enumeration is in progress, another libc could be dynamically loaded (e.g., via <code>dlopen</code>) on a separate thread, or a new file could be opened (e.g., a global constructor in a dynamically loaded library calling <code>fopen</code>).</p>
<p>This global list of things owned by libc shows up in multiple places. Take for instance:</p>
<pre><p><code><span>// Defined in header &lt;stdlib.h&gt;</span><span>
</span><span></span><span>int</span><span> </span><span>atexit</span><span>(</span><span>void</span><span> </span><span>(</span><span>*</span><span>func</span><span>)</span><span>(</span><span>void</span><span>)</span><span>)</span><span>;</span></code></p></pre>
<blockquote>
<p>Registers the function pointed to by <em>func</em> to be called on normal program termination (via <code>exit()</code> or returning from <code>main()</code>). The functions will be called in reverse order they were registered, i.e. the function registered last will be executed first.</p>
</blockquote>
<blockquote>
<p>There is also another variant of this called <code>at_quick_exit</code>.</p>
</blockquote>
<p>This implies that somewhere within libc, there must be a list of functions registered via <code>atexit</code> that need to be executed in reverse order. For multiple libc implementations to coexist, any system handling <code>atexit</code> must not only enumerate and call all registered functions, but also establish a total order for how they were inserted across all instances of libc.</p>
<p>Essentially, any resource owned by one libc would need to be sharable and accessible from any other version of libc. This turns out to be quite a lot of things. For the sake of our argument we've actually gone through all of the standard C (not POSIX) functions which produce or operate on a resource which has an opaque implementation where careful considerations would need to be made.</p>
<table><thead><tr><th>Header</th><th>Function</th><th>Resource</th><th>Notes</th></tr></thead><tbody><tr><td><code>&lt;fenv.h&gt;</code></td><td>N/A</td><td><code>fexcept_t</code></td><td>Float environment exceptions need to be stable across libc.</td></tr><tr><td><code>&lt;fenv.h&gt;</code></td><td><code>*</code></td><td><code>fexcept_t</code></td><td>Any functions using this type</td></tr><tr><td><code>&lt;fenv.h&gt;</code></td><td><code>fegetenv</code></td><td><code>fenv_t</code></td><td>Float environment needs to be stable across libc.</td></tr><tr><td><code>&lt;fenv.h&gt;</code></td><td><code>*</code></td><td><code>fenv_t</code></td><td>Any functions using this type</td></tr><tr><td><code>&lt;locale.h&gt;</code></td><td><code>localeconv</code></td><td><code>struct lconv</code></td><td>Common initial sequence needs to be stable across libc.</td></tr><tr><td><code>&lt;math.h&gt;</code></td><td>N/A</td><td><code>int</code></td><td>Math defines need to have a stable set of integer values across libc.</td></tr><tr><td><code>&lt;setjmp.h&gt;</code></td><td>N/A</td><td><code>jmp_buf</code></td><td>Usually defined by compiler</td></tr><tr><td><code>&lt;setjmp.h&gt;</code></td><td><code>*</code></td><td><code>jmp_buf</code></td><td>Usually defined by compiler intrinsic</td></tr><tr><td><code>&lt;signal.h&gt;</code></td><td>N/A</td><td><code>int</code></td><td>Signal defines need to have a stable set of integer values across libc.</td></tr><tr><td><code>&lt;signal.h&gt;</code></td><td>N/A</td><td><code>sig_atomic_t</code></td><td>Stable type across libc</td></tr><tr><td><code>&lt;stdarg.h&gt;</code></td><td>N/A</td><td><code>va_list</code></td><td>Usually defined by compiler</td></tr><tr><td><code>&lt;stdarg.h&gt;</code></td><td><code>va_start</code></td><td><code>va_list</code></td><td>Usually defined by compiler intrinsic</td></tr><tr><td><code>&lt;stdarg.h&gt;</code></td><td><code>*</code></td><td><code>va_list</code></td><td>Any functions or macros using this type</td></tr><tr><td><code>&lt;stdatomic.h&gt;</code></td><td><code>*</code></td><td><code>_Atomic T</code></td><td>Stable type across libc</td></tr><tr><td><code>&lt;stdatomic.h&gt;</code></td><td>N/A</td><td><code>int</code></td><td>Atomic defines need to have a stable set of integer values across libc.</td></tr><tr><td><code>&lt;stdatomic.h&gt;</code></td><td>N/A</td><td><code>typedef</code></td><td>Many typedefs need to have a stable set of types across libc.</td></tr><tr><td><code>&lt;stddef.h&gt;</code></td><td>N/A</td><td><code>typedef</code></td><td>Many typedefs need to have a stable set of types across libc.</td></tr><tr><td><code>&lt;stdint.h&gt;</code></td><td>N/A</td><td><code>typedef</code></td><td>Many typedefs need to have a stable set of types across libc.</td></tr><tr><td><code>&lt;stdint.h&gt;</code></td><td>N/A</td><td><code>int</code></td><td>Many defines need to have a stable set of types across libc.</td></tr><tr><td><code>&lt;stdio.h&gt;</code></td><td><code>*</code></td><td><code>FILE</code></td><td>Many functions (any taking <code>FILE*</code> or returning <code>FILE*</code>)</td></tr><tr><td><code>&lt;stdio.h&gt;</code></td><td>N/A</td><td><code>typedef</code></td><td>Many types need to have a stable set of types across libc.</td></tr><tr><td><code>&lt;stdio.h&gt;</code></td><td>N/A</td><td><code>int</code></td><td>Many defines need to have a stable set of integer values across libc.</td></tr><tr><td><code>&lt;stdio.h&gt;</code></td><td>N/A</td><td>N/A</td><td>Locale for string formatting needs to be shared across libc</td></tr><tr><td><code>&lt;stdio.h&gt;</code></td><td><code>stderr</code></td><td>N/A</td><td>Needs to be a macro that expands to a function call like <code>__stdio(STDERR_FILENO)</code></td></tr><tr><td><code>&lt;stdio.h&gt;</code></td><td><code>stdout</code></td><td>N/A</td><td>Needs to be a macro that expands to a function call like <code>__stdio(STDOUT_FILENO)</code></td></tr><tr><td><code>&lt;stdio.h&gt;</code></td><td><code>stdin</code></td><td>N/A</td><td>Needs to be a macro that expands to a function call like <code>__stdio(STDIO_FILENO)</code></td></tr><tr><td><code>&lt;stdlib.h&gt;</code></td><td>N/A</td><td><code>div_t</code>,</td><td>Needs to have a stable definition across libc.</td></tr><tr><td><code>&lt;stdlib.h&gt;</code></td><td>N/A</td><td><code>ldiv_t</code>,</td><td>Needs to have a stable definition across libc.</td></tr><tr><td><code>&lt;stdlib.h&gt;</code></td><td>N/A</td><td><code>lldiv_t</code></td><td>Needs to have a stable definition across libc.</td></tr><tr><td><code>&lt;stdlib.h&gt;</code></td><td>N/A</td><td><code>int</code></td><td>Many defines need to have a stable set of integer values across libc.</td></tr><tr><td><code>&lt;stdlib.h&gt;</code></td><td><code>call_once</code></td><td><code>once_flag</code></td><td>Needs to be stable across libc and also libthread</td></tr><tr><td><code>&lt;stdlib.h&gt;</code></td><td><code>rand</code></td><td>N/A</td><td>Global PRNG needs to be shared across libc.</td></tr><tr><td><code>&lt;stdlib.h&gt;</code></td><td><code>srand</code></td><td>N/A</td><td>Global PRNG needs to be shared across libc.</td></tr><tr><td><code>&lt;stdlib.h&gt;</code></td><td><code>aligned_alloc</code></td><td><code>void*</code></td><td>Shared heap</td></tr><tr><td><code>&lt;stdlib.h&gt;</code></td><td><code>calloc</code></td><td><code>void*</code></td><td>Shared heap</td></tr><tr><td><code>&lt;stdlib.h&gt;</code></td><td><code>free</code></td><td><code>void*</code></td><td>Shared heap</td></tr><tr><td><code>&lt;stdlib.h&gt;</code></td><td><code>free_sized</code></td><td><code>void*</code></td><td>Shared heap</td></tr><tr><td><code>&lt;stdlib.h&gt;</code></td><td><code>free_aligned_size</code></td><td><code>void*</code></td><td>Shared heap</td></tr><tr><td><code>&lt;stdlib.h&gt;</code></td><td><code>malloc</code></td><td><code>void*</code></td><td>Shared heap</td></tr><tr><td><code>&lt;stdlib.h&gt;</code></td><td><code>realloc</code></td><td><code>void*</code></td><td>Shared heap</td></tr><tr><td><code>&lt;stdlib.h&gt;</code></td><td><code>atexit</code></td><td>N/A</td><td>Global list needs to be shared across libc.</td></tr><tr><td><code>&lt;stdlib.h&gt;</code></td><td><code>at_quick_exit</code></td><td>N/A</td><td>Global list needs to be shared across libc.</td></tr><tr><td><code>&lt;string.h&gt;</code></td><td><code>strcoll</code></td><td>N/A</td><td><code>LC_COLLATE</code> locale needs to be shared across libc</td></tr><tr><td><code>&lt;threads.h&gt;</code></td><td>N/A</td><td><code>cnd_t</code></td><td>Any opaque method</td></tr><tr><td><code>&lt;threads.h&gt;</code></td><td>N/A</td><td><code>thrd_t</code></td><td>Any opaque method</td></tr><tr><td><code>&lt;threads.h&gt;</code></td><td>N/A</td><td><code>tss_t</code></td><td>Any opaque method</td></tr><tr><td><code>&lt;threads.h&gt;</code></td><td>N/A</td><td><code>mtx_t</code></td><td>Any opaque method</td></tr><tr><td><code>&lt;threads.h&gt;</code></td><td><code>*</code></td><td><code>cnd_t</code></td><td>Many functions using this type</td></tr><tr><td><code>&lt;threads.h&gt;</code></td><td><code>*</code></td><td><code>thrd_t</code></td><td>Many functions using this type</td></tr><tr><td><code>&lt;threads.h&gt;</code></td><td><code>*</code></td><td><code>tss_t</code></td><td>Many functions using this type</td></tr><tr><td><code>&lt;threads.h&gt;</code></td><td><code>*</code></td><td><code>mtx_t</code></td><td>Many functions using this type</td></tr><tr><td><code>&lt;threads.h&gt;</code></td><td><code>*</code></td><td><code>typedef</code></td><td>Many types need to have a stable set of types across libc.</td></tr><tr><td><code>&lt;threads.h&gt;</code></td><td>N/A</td><td><code>int</code></td><td>Many defines need to have a stable set of integer values across libc.</td></tr><tr><td><code>&lt;threads.h&gt;</code></td><td><code>call_once</code></td><td><code>once_flag</code></td><td>See <code>&lt;stdlib.h&gt;</code> above</td></tr><tr><td><code>&lt;time.h&gt;</code></td><td>N/A</td><td><code>typedef</code></td><td>Many types need to have a stable set of types across libc.</td></tr><tr><td><code>&lt;time.h&gt;</code></td><td>N/A</td><td><code>struct tm</code></td><td>Common initial sequence needs to be stable across libc.</td></tr><tr><td><code>&lt;uchar.h&gt;</code></td><td>N/A</td><td><code>char8_t</code></td><td>Needs to be same across libc</td></tr><tr><td><code>&lt;uchar.h&gt;</code></td><td>N/A</td><td><code>char16_t</code></td><td>Needs to be same across libc</td></tr><tr><td><code>&lt;uchar.h&gt;</code></td><td>N/A</td><td><code>char32_t</code></td><td>Needs to be same across libc</td></tr><tr><td><code>&lt;uchar.h&gt;</code></td><td><code>*</code></td><td><code>char8_t</code></td><td>Many functions using this type</td></tr><tr><td><code>&lt;uchar.h&gt;</code></td><td><code>*</code></td><td><code>char16_t</code></td><td>Many functions using this type</td></tr><tr><td><code>&lt;uchar.h&gt;</code></td><td><code>*</code></td><td><code>char32_t</code></td><td>Many functions using this type</td></tr><tr><td><code>&lt;uchar.h&gt;</code></td><td>N/A</td><td><code>mbstate_t</code></td><td>Any opaque method</td></tr><tr><td><code>&lt;uchar.h&gt;</code></td><td><code>*</code></td><td><code>mbstate_t</code></td><td>Many functions using this type.</td></tr><tr><td><code>&lt;wchar.h&gt;</code></td><td><code>*</code></td><td><code>*</code></td><td>Repeat of <code>&lt;uchar.h&gt;</code> essentially</td></tr><tr><td><code>&lt;wctype.h&gt;</code></td><td>N/A</td><td><code>wctrans_t</code></td><td>Need to be same across libc</td></tr><tr><td><code>&lt;wctype.h&gt;</code></td><td>N/A</td><td><code>wctype_t</code></td><td>Need to be same across libc</td></tr><tr><td><code>&lt;wctype.h&gt;</code></td><td><code>*</code></td><td><code>wctrans_t</code></td><td>Many functions using this type</td></tr><tr><td><code>&lt;wctype.h&gt;</code></td><td><code>*</code></td><td><code>wctype_t</code></td><td>Many functions using this type</td></tr></tbody></table>
<p>Most defines (of constants), ABI-exposed types (and typedefs) should just be stable across libc implementations for this to work reliably. Since these are baked into executables there's no real way to modify or change them without breaking stuff anyways. When discussing stuff that is opaque (listed as <code>Any opaque method</code>) we propose affixing a pointer to a vtable containing the implementation as the first value in the type, this way functions operating on it can always recover the correct implementation and dispatch it indirectly through the vtable. Other methods like using a version field can also work here too.</p>
<p>Regardless, certain aspects of libc introduce complexity, particularly global and thread-local elements like <a href="https://en.wikipedia.org/wiki/Errno.h">errno</a> and <a href="https://en.wikipedia.org/wiki/Locale_(computer_software)">locale</a>. However, with a well-designed architecture, these challenges can be effectively addressed.</p>
<p>Memory allocation functions—<code>calloc</code>, <code>malloc</code>, <code>aligned_alloc</code>, <code>realloc</code>, and <code>free</code>—from <code>&lt;stdlib.h&gt;</code> pose another difficulty. Since they can return any pointer, tracking them is non-trivial. One possible approach is to store a pointer to a vtable in the allocation header, allowing each allocation to reference its implementation. However, this method incurs significant performance and memory overhead. Instead, we propose centralizing heap management in a dedicated <code>libheap</code>. This would also contain implementation of the POSIX extensions like <code>posix_memalign</code>.</p>
<p>Things get even more interesting when moving from standard C to POSIX, which introduces unique challenges that require libc support. Some of these functionalities might be better split into separate libraries (for example, why is the DNS resolver in libc?). Among these challenges though, <code>setxid</code> stands out.</p>
<p>For those unfamiliar, permissions in POSIX—such as the <a href="https://en.wikipedia.org/wiki/User_identifier">real, effective, and saved user/group IDs</a>—apply at the process level. However, Linux treats threads as independent processes that share memory, meaning these permissions are managed per thread rather than per process. To comply with POSIX semantics, libc must interrupt every thread, forcing them to execute code that invokes the system call to modify their thread-local permissions. This must be done atomically, without failure, and while remaining <a href="https://man7.org/linux/man-pages/man7/signal-safety.7.html">async-signal safe</a>. This is a nightmare to implement and has proven to be <a href="https://ewontfix.com/17/">a major challenge</a>. More importantly, getting it right is crucial for security.</p>
<p>Ultimately, this means libc must track every thread and provide a way to synchronously execute code across all threads. To address this, we propose consolidating threading, TLS, and the necessary POSIX compliance mechanisms within a single <code>libthread</code>.</p>
<p>There are many additional complexities we’ve glossed over and many alternative ways this can be implemented. The key takeaway is that these issues are solvable—they just require significant architectural changes. This necessitates reevaluating this aspect of the Linux userspace from the ground up with binary compatibility as a core design principle. GLIBC has never seriously attempted this. Until someone decides, "enough is enough, let’s fix this," binary compatibility on Linux will remain an unsolved problem; we strongly believe this is a problem worth solving.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DEDA – Tracking Dots Extraction, Decoding and Anonymisation Toolkit (204 pts)]]></title>
            <link>https://github.com/dfd-tud/deda</link>
            <guid>43551397</guid>
            <pubDate>Tue, 01 Apr 2025 21:11:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/dfd-tud/deda">https://github.com/dfd-tud/deda</a>, See on <a href="https://news.ycombinator.com/item?id=43551397">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">DEDA - tracking Dots Extraction, Decoding and Anonymisation toolkit</h2><a id="user-content-deda---tracking-dots-extraction-decoding-and-anonymisation-toolkit" aria-label="Permalink: DEDA - tracking Dots Extraction, Decoding and Anonymisation toolkit" href="#deda---tracking-dots-extraction-decoding-and-anonymisation-toolkit"></a></p>
<p dir="auto">Document Colour Tracking Dots, or yellow dots, are small systematic dots which encode information about the printer and/or the printout itself. This process is integrated in almost every commercial colour laser printer. This means that almost every printout contains coded information about the source device, such as the serial number.</p>
<p dir="auto">On the one hand, this tool gives the possibility to read out and decode these forensic features and on the other hand, it allows anonymisation to prevent arbitrary tracking.</p>
<p dir="auto">If you use this software, please cite the paper:
Timo Richter, Stephan Escher, Dagmar Schönfeld, and Thorsten Strufe. 2018. Forensic Analysis and Anonymisation of Printed Documents. In Proceedings of the 6th ACM Workshop on Information Hiding and Multimedia Security (IH&amp;MMSec '18). ACM, New York, NY, USA, 127-138. DOI: <a href="https://doi.org/10.1145/3206004.3206019" rel="nofollow">https://doi.org/10.1145/3206004.3206019</a></p>
<hr>
<p dir="auto"><h4 tabindex="-1" dir="auto">Installation</h4><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<ul dir="auto">
<li>Install Python 3</li>
<li>Install Deda</li>
</ul>
<p dir="auto">From PyPI:
<code>$ pip3 install --user deda</code></p>
<p dir="auto">Or from current directory:
<code>$ pip3 install --user .</code></p>
<ul dir="auto">
<li>Optional requirement by deda_anonmask_apply (Unix and GNU/Linux only):
<code>$ pip3 install --user wand</code></li>
</ul>
<p dir="auto">Without Wand, pages containing white areas on images cannot be anonymised.</p>
<hr>
<p dir="auto"><h4 tabindex="-1" dir="auto">Graphical User Interface</h4><a id="user-content-graphical-user-interface" aria-label="Permalink: Graphical User Interface" href="#graphical-user-interface"></a></p>
<ul dir="auto">
<li>To open the GUI type:
<code>$ deda_gui</code></li>
</ul>
<hr>
<p dir="auto"><h4 tabindex="-1" dir="auto">Terminal Application</h4><a id="user-content-terminal-application" aria-label="Permalink: Terminal Application" href="#terminal-application"></a></p>
<p dir="auto"><h5 tabindex="-1" dir="auto">1. Reading tracking data</h5><a id="user-content-1-reading-tracking-data" aria-label="Permalink: 1. Reading tracking data" href="#1-reading-tracking-data"></a></p>
<p dir="auto">Tracking data can be read and sometimes be decoded from a scanned image. For good results the input shall use a lossless compression (e.g. png) and 300 dpi. Make sure to set a neutral contrast
<code>$ deda_parse_print INPUTFILE</code></p>
<p dir="auto"><h5 tabindex="-1" dir="auto">2. Find a divergent printer in a set of scanned documents</h5><a id="user-content-2-find-a-divergent-printer-in-a-set-of-scanned-documents" aria-label="Permalink: 2. Find a divergent printer in a set of scanned documents" href="#2-find-a-divergent-printer-in-a-set-of-scanned-documents"></a></p>
<p dir="auto"><code>$ deda_compare_prints INPUT1 INPUT2 [INPUT3] ...</code></p>
<p dir="auto"><h5 tabindex="-1" dir="auto">3. Analysing an unknown tracking pattern</h5><a id="user-content-3-analysing-an-unknown-tracking-pattern" aria-label="Permalink: 3. Analysing an unknown tracking pattern" href="#3-analysing-an-unknown-tracking-pattern"></a></p>
<p dir="auto">New patterns might not be recognised by parse_print. The dots can be extracted
for further analysis.<br>
<code>$ deda_extract_yd INPUTFILE</code></p>
<p dir="auto"><h5 tabindex="-1" dir="auto">4. Create your own tracking dots</h5><a id="user-content-4-create-your-own-tracking-dots" aria-label="Permalink: 4. Create your own tracking dots" href="#4-create-your-own-tracking-dots"></a></p>
<p dir="auto">If you want to create your own tracking dots matrix and add it to a pdf
document, pass the contents as parameters (see <code>deda_create_dots -h</code>).
<code>$ deda_create_dots PDFINPUT</code></p>
<p dir="auto">The calibration page (<code>$ deda_anonmask_create -w</code>) may be used as an input.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">5. Anonymise a scanned image</h5><a id="user-content-5-anonymise-a-scanned-image" aria-label="Permalink: 5. Anonymise a scanned image" href="#5-anonymise-a-scanned-image"></a></p>
<p dir="auto">This (mostly) removes tracking data from a scan:<br>
<code>$ deda_clean_document INPUTFILE OUTPUTFILE</code></p>
<p dir="auto"><h5 tabindex="-1" dir="auto">6. Anonymise a document for printing</h5><a id="user-content-6-anonymise-a-document-for-printing" aria-label="Permalink: 6. Anonymise a document for printing" href="#6-anonymise-a-document-for-printing"></a></p>
<ul dir="auto">
<li>
<p dir="auto">Save your document as a PDF file and call it DOCUMENT.PDF.</p>
</li>
<li>
<p dir="auto">Print the testpage.pdf file created by<br>
<code>$ deda_anonmask_create -w</code><br>
without any page margin.</p>
</li>
<li>
<p dir="auto">Scan the document (300 dpi) and pass the lossless file to<br>
<code>$ deda_anonmask_create -r INPUTFILE</code><br>
This creates 'mask.json', the individual printer's anonymisation mask.</p>
</li>
<li>
<p dir="auto">Now apply the anonymisation mask:<br>
<code>$ deda_anonmask_apply mask.json DOCUMENT.PDF</code>
This creates 'masked.pdf', the anonymised document. It may be printed with a
zero page margin setting.</p>
</li>
</ul>
<p dir="auto">Check whether a masked page covers your printer's tracking dots by using a
microscope. The mask's dot radius, x and y offsets can be customised and
passed to <code>deda_anonmask_apply</code> as parameters.</p>
<p dir="auto">Note that if DOCUMENT.PDF contains graphics with white or light coloured parts, these can only be masked if "wand" is installed (see above).</p>
<hr>
<p dir="auto"><h4 tabindex="-1" dir="auto">Troubleshooting</h4><a id="user-content-troubleshooting" aria-label="Permalink: Troubleshooting" href="#troubleshooting"></a></p>
<p dir="auto"><h5 tabindex="-1" dir="auto">deda_parse_print: command not found</h5><a id="user-content-deda_parse_print-command-not-found" aria-label="Permalink: deda_parse_print: command not found" href="#deda_parse_print-command-not-found"></a></p>
<p dir="auto">Possible solutions:</p>
<ul dir="auto">
<li>Install deda accordig to chapter 0</li>
<li>Execute
<code>$ export PATH="$PATH:$(python -c 'import site,os; print(os.path.join(site.USER_BASE, "bin"))')"</code></li>
</ul>
<p dir="auto"><h5 tabindex="-1" dir="auto">Deda does not recognise my tracking dots</h5><a id="user-content-deda-does-not-recognise-my-tracking-dots" aria-label="Permalink: Deda does not recognise my tracking dots" href="#deda-does-not-recognise-my-tracking-dots"></a></p>
<p dir="auto">Set up your scan program so that it does not eliminate the paper structure nor tracking dots by some threshold and check again. Remember that monochrome pages as well as inkjet prints might not contain tracking dots.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">My printer does not print tracking dots. Can I hide this fact?</h5><a id="user-content-my-printer-does-not-print-tracking-dots-can-i-hide-this-fact" aria-label="Permalink: My printer does not print tracking dots. Can I hide this fact?" href="#my-printer-does-not-print-tracking-dots-can-i-hide-this-fact"></a></p>
<p dir="auto">If there are really no tracking dots, you can either create your own ones (<code>deda_create_dots</code>) or print the calibration page (<code>deda_anonmask_create -w</code>) with another printer and use the mask for your own printer. You can use the anonymised version of the tracking dots or just copy them (<code>deda_anonmask_create --copy</code>). See chapters "Anonymise a document for printing" and "Create your own tracking dots".</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">Install Error: command 'x86_64-linux-gnu-gcc' failed with exit status 1</h5><a id="user-content-install-error-command-x86_64-linux-gnu-gcc-failed-with-exit-status-1" aria-label="Permalink: Install Error: command 'x86_64-linux-gnu-gcc' failed with exit status 1" href="#install-error-command-x86_64-linux-gnu-gcc-failed-with-exit-status-1"></a></p>
<p dir="auto">This may be caused by the eel dependency which is needed for the GUI. Try
<code>$ sudo apt-get install build-essential autoconf libtool pkg-config python3.6-dev gcc &amp;&amp; pip3 install --user eel</code></p>
<p dir="auto"><h5 tabindex="-1" dir="auto">wand.exceptions.PolicyError: attempt to perform an operation not allowed by the security policy PDF' @ error/constitute.c/IsCoderAuthorized/408</h5><a id="user-content-wandexceptionspolicyerror-attempt-to-perform-an-operation-not-allowed-by-the-security-policy-pdf--errorconstituteciscoderauthorized408" aria-label="Permalink: wand.exceptions.PolicyError: attempt to perform an operation not allowed by the security policy PDF' @ error/constitute.c/IsCoderAuthorized/408" href="#wandexceptionspolicyerror-attempt-to-perform-an-operation-not-allowed-by-the-security-policy-pdf--errorconstituteciscoderauthorized408"></a></p>
<p dir="auto">This is being caused by ImageMagick. Either remove Wand (<code>pip3 uninstall wand</code>) or add <code>&lt;policy domain="coder" rights="read | write" pattern="PDF" /&gt;</code> just before <code>&lt;/policymap&gt;</code> in /etc/ImageMagick-*/policy.xml. See also <a href="https://stackoverflow.com/questions/52998331/imagemagick-security-policy-pdf-blocking-conversion" rel="nofollow">https://stackoverflow.com/questions/52998331/imagemagick-security-policy-pdf-blocking-conversion</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Shared DNA in Music (158 pts)]]></title>
            <link>https://pudding.cool/2025/04/music-dna/</link>
            <guid>43551352</guid>
            <pubDate>Tue, 01 Apr 2025 21:07:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pudding.cool/2025/04/music-dna/">https://pudding.cool/2025/04/music-dna/</a>, See on <a href="https://news.ycombinator.com/item?id=43551352">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2 tabindex="0" data-svelte-h="svelte-1tbat7j">This is a project about shared DNA in music.</h2> <div role="status" aria-live="polite" aria-busy="true" data-svelte-h="svelte-1h77645"> <p><span>Loading visualization data...</span></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How Silica Gel Took Over the World (109 pts)]]></title>
            <link>https://www.scopeofwork.net/silica-gel/</link>
            <guid>43550556</guid>
            <pubDate>Tue, 01 Apr 2025 19:33:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.scopeofwork.net/silica-gel/">https://www.scopeofwork.net/silica-gel/</a>, See on <a href="https://news.ycombinator.com/item?id=43550556">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
          <div>
            
            <p>I find them stuffed into the toes of a new pair of sneakers. I find them wedged into a sheaf of seaweed snacks. I find them in the over-inflated bag that contains my new inhaler, and in the vacuum-sealed one puckered around my kids’ 3D printing filament. “DO NOT EAT,” they all admonish me, and I find myself slipping them under the top layer of the garbage already in the trash can, as if my kids wouldn’t be able to control their urge to taste whatever is inside these tiny white pouches. Silica gel packets are <em>everywhere</em>, their presence seemingly the only thing keeping our packaged food crispy and our belongings free of mildew. How on earth did they all get here? <em>Is silica gel taking over the world?</em></p><figure><img src="https://www.scopeofwork.net/content/images/2025/03/silica-2---1.jpeg" alt="" loading="lazy" width="2000" height="1125" srcset="https://www.scopeofwork.net/content/images/size/w600/2025/03/silica-2---1.jpeg 600w, https://www.scopeofwork.net/content/images/size/w1000/2025/03/silica-2---1.jpeg 1000w, https://www.scopeofwork.net/content/images/size/w1600/2025/03/silica-2---1.jpeg 1600w, https://www.scopeofwork.net/content/images/size/w2400/2025/03/silica-2---1.jpeg 2400w" sizes="(min-width: 720px) 720px"></figure><p>Tear its little Tyvek wrapping, and spill a packet of glassy silica gel beads into the palm of your hand; they won’t hurt you. They are made of the same stuff as sand: “Silica” means “silicon dioxide,” which is the primary component of most drinkware, windshields, and the screen of whatever electronic device you’re reading this on. But glass has a density of around 2500 kilograms per cubic meter, and crystalline silicon dioxide (quartz) is around 2650. Silica gel, on the other hand, is more like 700 kilograms per cubic meter. It may look fully dense, but in fact it’s shot through with countless tiny pores. If your windowpane is like a thin sheet of solid ice, then a silica gel bead is like a tiny snowball. </p><figure><img src="https://www.scopeofwork.net/content/images/2025/03/Screenshot-2025-03-29-at-19.10.13-1.png" alt="" loading="lazy" width="1437" height="1174" srcset="https://www.scopeofwork.net/content/images/size/w600/2025/03/Screenshot-2025-03-29-at-19.10.13-1.png 600w, https://www.scopeofwork.net/content/images/size/w1000/2025/03/Screenshot-2025-03-29-at-19.10.13-1.png 1000w, https://www.scopeofwork.net/content/images/2025/03/Screenshot-2025-03-29-at-19.10.13-1.png 1437w" sizes="(min-width: 720px) 720px"><figcaption><i><em>Silica gel beads, as seen by a scanning electron microscope. Image credit: Dusan Berek, via </em></i><a href="https://www.researchgate.net/publication/229181928_Structural_inhomogeneis_in_wide-pore_silica_gels?ref=scopeofwork.net" rel="noreferrer"><i><em>Structural inhomogeneities in wide-pore silica gels</em></i></a><i><em>.</em></i></figcaption></figure><p>Zoom in on a silica gel bead with a scanning electron microscope, and its smooth surface turns discontinuous, riddled with voids about 2.5 nanometers across (roughly the diameter of a strand of DNA). This microstructure gives silica gel radical properties. The silica gel packets in my kids’ seaweed snacks are just a little bit bigger than postage stamps, and have a total mass of about a gram. That single gram of silica gel could have an internal surface area of eight hundred square meters—the size of <em>almost two basketball courts</em>. </p><p>These factors allow silica gel to adsorb up to 40% of its own weight in water vapor, through a process called <a href="https://en.wikipedia.org/wiki/Capillary_condensation?ref=scopeofwork.net">capillary condensation</a>. When humid air migrates into the pores of a silica gel bead, its vapor pressure increases, causing water to condense onto the silica gel’s internal surfaces. At the risk of anthropomorphizing water, it’s as if it prefers to be a liquid, stuck within the silica gel’s tiny capillaries, than a vapor, carried along with whatever else is in the air.</p><p>If you find a packet of silica gel in an imported snack or the pocket of a new jacket, it’s probably there to filter water vapor out of air. Like most filters, there is a limit to how much water a piece of silica gel can hold; its internal surfaces are finite in size, and as a result there is a finite amount of humidity that a given packet of silica gel can adsorb. Luckily, silica gel vendors offer nifty calculators so that their customers can size their silica gel packets to the volumes of air they wish to dry out—and how dry they want the air to be. If you wanted to desiccate the air inside of a child’s balloon, you’d calculate its volume (let’s call it 14 liters), then make some assumptions about its temperature and humidity (around 35°C, 75% RH), then decide what you want its final relative humidity to be (let’s bring it down to 20% RH). Run these numbers and you’d find you needed to pull about 0.3 grams of water out of the air in your balloon. This can be done, pretty reliably, with a one- or two-gram packet of silica gel, slipped into the balloon just before you blow it up.</p><p>Of course, different applications will entail different goals. The bag that holds your potato chips is not <em>completely</em> impermeable to moisture, and if you’re going to ship it across an ocean and then let it sit on a shelf for a few months, you should expect some additional water vapor to find its way in. There are also applications in which silica gel is used not so much to dry something out but to maintain a particular equilibrium humidity. In the art world, silica gel packets are slipped into exhibit cases and <a href="https://resources.culturalheritage.org/wp-content/uploads/sites/8/2015/02/osg009-12.pdf?ref=scopeofwork.net">used for their “buffering capacity,”</a> helping maintain a stable (and relatively high) humidity inside the case while conditions in the rest of the gallery might vary.</p><figure><img src="https://www.scopeofwork.net/content/images/2025/03/Screenshot-2025-03-29-at-19.13.50.png" alt="" loading="lazy" width="1432" height="883" srcset="https://www.scopeofwork.net/content/images/size/w600/2025/03/Screenshot-2025-03-29-at-19.13.50.png 600w, https://www.scopeofwork.net/content/images/size/w1000/2025/03/Screenshot-2025-03-29-at-19.13.50.png 1000w, https://www.scopeofwork.net/content/images/2025/03/Screenshot-2025-03-29-at-19.13.50.png 1432w" sizes="(min-width: 720px) 720px"><figcaption><i><em>Silica gel is commonly placed inside museum display cases, in hidden compartments that ventilate to the display chamber. Image via the National Park Service's *very* thorough </em></i><a href="https://imlive.s3.amazonaws.com/Federal%20Government/ID196793570404309568859839779465274805789/Attachment2_Exhibit_Conservation_Guidelines.pdf?ref=scopeofwork.net" rel="noreferrer"><i><em>Exhibit Conservation Guidelines, Second Edition</em></i></a><i><em>. </em></i></figcaption></figure><p>While glass and glassy substances have been used by humans for many thousands of years, it wasn’t until the early twentieth century that Walter Patrick, a researcher at Johns Hopkins, developed <a href="https://patentimages.storage.googleapis.com/c5/7d/08/1ba3c5281606de/US1297724.pdf?ref=scopeofwork.net" rel="noreferrer">and patented</a> an efficient way to create silica gel. He did this by mixing a substance called “water glass” with an acid. <a href="https://en.wikipedia.org/wiki/Sodium_silicate?ref=scopeofwork.net">Water glass</a>—a fascinating material in itself, as it is essentially water-soluble glass—is an alkaline compound containing sodium oxide and silicon dioxide. When mixed with an acid its silicon dioxide precipitates, linking into the matrix now known as silica gel. The silica gel—glassy and hard—is then washed and dried to remove excess acid, salt, and water.</p><p>Silica gel became commercially available within five years of Patrick’s invention, through a deal with the Davison Chemical company. But according to <a href="https://web.archive.org/web/20051112233703/http://www.rotman.utoronto.ca/feldman/papers/Research%20Universities%20and%20Local%20Economic%20Development.pdf">this academic paper</a>, which analyzes how discoveries at Johns Hopkins impacted the local economy, “the transition from the laboratory to the commercial world was a long and arduous one... Much development work had been needed to develop the original academic breakthrough, and new applications had to be found for the product before it became a commercial success.”</p><figure><img src="https://www.scopeofwork.net/content/images/2025/03/Screenshot-2025-03-29-at-19.38.31.png" alt="" loading="lazy" width="1844" height="1030" srcset="https://www.scopeofwork.net/content/images/size/w600/2025/03/Screenshot-2025-03-29-at-19.38.31.png 600w, https://www.scopeofwork.net/content/images/size/w1000/2025/03/Screenshot-2025-03-29-at-19.38.31.png 1000w, https://www.scopeofwork.net/content/images/size/w1600/2025/03/Screenshot-2025-03-29-at-19.38.31.png 1600w, https://www.scopeofwork.net/content/images/2025/03/Screenshot-2025-03-29-at-19.38.31.png 1844w" sizes="(min-width: 720px) 720px"><figcaption><i><em>A 1930 pamphlet, published by Davison's silica gel subsidiary, advertises this silica-gel based air conditioning unit. Image </em></i><a href="https://www.nypl.org/research/research-catalog/bib/hb990032798230203941?ref=scopeofwork.net"><i><em>via</em></i></a><i><em> the New York Public Library.</em></i></figcaption></figure><p>But a commercial success it eventually became. In 1927, Davison built a silica gel factory in Baltimore, and by 1930 they claimed to have applied for or received over three hundred patents on the stuff. The company was acquired by W.R. Grace in 1954, and Grace continues to manufacture specialty silica gel products at that same location.</p><p>To learn about the silica gel industry today, I spoke with Demetrius Michos, a PhD chemist who has worked at Grace for over thirty years. He assured me that I “could not go about my day without touching Grace’s products,” and went on to repeatedly surprise me with obscure and curious applications for silica gel. But they've mostly moved up-market from the little envelopes in our consumer packaged goods. Grace is big in the <em>silica gel</em> business, and they sell a lot of other silica products, but they don’t seem very interested in <em>silica gel desiccant packets </em>specifically. </p><figure><img src="https://www.scopeofwork.net/content/images/2025/03/20230913_Grace_01383-smaller.jpg" alt="" loading="lazy" width="2000" height="2390" srcset="https://www.scopeofwork.net/content/images/size/w600/2025/03/20230913_Grace_01383-smaller.jpg 600w, https://www.scopeofwork.net/content/images/size/w1000/2025/03/20230913_Grace_01383-smaller.jpg 1000w, https://www.scopeofwork.net/content/images/size/w1600/2025/03/20230913_Grace_01383-smaller.jpg 1600w, https://www.scopeofwork.net/content/images/size/w2400/2025/03/20230913_Grace_01383-smaller.jpg 2400w" sizes="(min-width: 720px) 720px"><figcaption><i><em>An operator tends the wash pots at WR Grace's Curtis Bay, Maryland factory. The silica gel manufacturing process includes multiple washing steps. Image via W.R. Grace &amp; Co.</em></i></figcaption></figure><p>So I went looking for imports. The Census Bureau’s import data goes back to 1992, and shows rising—and accelerating—imports of both silicon dioxide and silica gel. Total imports peak in 2022, and are currently about ten times their 1992 levels. This is interesting, but it does not answer my question about desiccant packets. Sure, we may be importing more silica gel today than we did in the early nineties. But we’re also importing a lot more shoes than we did in the nineties, and it's not like I find pairs of shoes slipped into every third thing I buy.</p><figure><img src="https://www.scopeofwork.net/content/images/2025/03/silica-gel-imports.png" alt="" loading="lazy" width="672" height="504" srcset="https://www.scopeofwork.net/content/images/size/w600/2025/03/silica-gel-imports.png 600w, https://www.scopeofwork.net/content/images/2025/03/silica-gel-imports.png 672w"><figcaption><i><em>US Census data on imports for silicon dioxide (a precursor for silica gel—and many other things) and silica gel itself. Image via </em></i><a href="https://usatrade.census.gov/?ref=scopeofwork.net"><i><em>USA Trade Online</em></i></a><i><em>.</em></i></figcaption></figure><p>After a round of mostly fruitless phone calls, I finally spoke to John Perona, a sales rep at a company that sells, among other things, silica gel desiccant packets. He started by pointedly questioning my intention of writing anything about silica gel at all, then told me that I was “getting into the weeds. Just look at how things ship around the world, and you can understand the problems that silica gel packets are trying to address.”</p><p>The reasons behind the increase in silica gel <em>imports</em>, he went on somewhat reluctantly, were simple: “No one wants a silica gel factory in their backyard.” Only specialty silica gel products are manufactured in the US today; the packets that I find in my snacks and pharmaceuticals are either made overseas or, in some cases, assembled in the US from imported silica gel beads.</p><p>Then John reminded me that a couple decades ago there were hundreds of thousands of manufacturing jobs on a single mile of road, a half-hour from my house. If I lived in Brooklyn then, I could have purchased goods from those factories. They would experience few, if any, swings in temperature and pressure on their short journey from factory to home, and even if they did, their packaging probably would have let excess humidity ventilate off.</p><p>The farther you ship a product—the longer it takes to go from the factory to the customer’s hands, and the more temperature and pressure cycles it experiences during that time—the more you need to control humidity inside of its airtight packaging. Silica gel is a cheap, easy, and reliable way to do so. In this sense silica gel sits alongside containerized shipping, and <a href="https://www.scopeofwork.net/stretch-wrap/">stretch wrap</a>, and bills of lading: It is a technology without which we’d have a <em>much</em> harder time maintaining global supply chains. </p><p>Desiccant packets haven’t actually taken over the world—<em>globalization</em> has.</p><p>So we seal our seaweed snacks, and our inhalers, and our 3D printer filament inside airtight plastic bags—then ship them across the world. We could, as one engineer I spoke to suggested, fill all these packages with dry, inert gas first. But little Tyvek bags, slipped inside right before they’re sealed up, are quite a bit easier.</p><hr><p><em>Update: </em><a href="https://www.scopeofwork.net/2025-04-01/" rel="noreferrer"><em>We published a follow-up to this piece here</em></a><em>; it contains a bunch of fascinating miscellany about silica gel. </em></p><hr><p>Thanks! To Nick Fountain for suggesting the topic, and to Demetrius Michos, Sharyn Nerenberg, John Perona for educating me on it. Thanks to Brad Avenson for suggesting (facetiously) that we fill our products &amp; packaging with “dry, inert gas,” and a <em>big</em> thanks to the Supporters and Members of Scope of Work, who <a href="https://scopeofwork.net/membership?ref=scopeofwork.net" rel="noreferrer">make it possible for me to spend a week immersing myself in a little corner of the infrastructural world</a>.</p>
          </div>
        </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dave Täht has died (203 pts)]]></title>
            <link>https://libreqos.io/2025/04/01/in-loving-memory-of-dave/</link>
            <guid>43550098</guid>
            <pubDate>Tue, 01 Apr 2025 18:40:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://libreqos.io/2025/04/01/in-loving-memory-of-dave/">https://libreqos.io/2025/04/01/in-loving-memory-of-dave/</a>, See on <a href="https://news.ycombinator.com/item?id=43550098">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-id="21df1089" data-element_type="widget" data-widget_type="text-editor.default">
									<p>04/01/2025</p><p>We are devastated to report that Dave Täht has passed away.</p><p>Dave was an amazing man, helping the world with FQ-CoDel and CAKE, fighting bufferbloat and trying to make the world a better place. Always willing to help, and without him – LibreQoS (and the other QoE solutions out there) wouldn’t exist.</p><p>Dave was an inspiration, and we all miss him. We’re reaching out to family and close friends to see if there’s anything we can do to help.</p><p>Dave was an inspiration to us. Dave’s contributions to Linux, FQ-CoDel, and CAKE improved internet connectivity around the world for millions of people. Because of him, millions of people now have access to reliable video calls – and in turn, access to loved ones, healthcare, and community. One of Robert’s ISP customers is a kind paraplegic woman who lives in a far-flung rural Colonia near El Paso, Texas. Her reliable access to her doctors through telemedicine, and to her family through FaceTime, was only made possible because of his algorithms. There are millions of cases like hers, where Dave’s contributions have silently enabled human connection and safety. Everything Dave contributed to the world of technology was free and open source, for the betterment of humanity.</p><p>Dave is the reason that Starlink was able to tackle its latency issues – enabling a generation of young entrepreneurs across the developing world, such as these young folks pictured in the Phillipines, to start their own ISPs to expand internet access to their communities. Dave started work on FQ-CoDel in part because of his own journey working to expand internet access in Nicaragua, so we know he saw that his work had come full-circle and helped so many.</p><p>We’re incredibly grateful to have Dave as our friend, mentor, and as someone who continuously inspired us – showing us that we could do better for each other in the world, and leverage technology to make that happen. He will be dearly missed.</p><p><strong>PS:</strong> Dave is forever in our hearts and souls, in our routers and… in production. <strong><a href="https://github.com/LibreQoE/LibreQoS/pull/684" rel="nofollow">https://github.com/LibreQoE/LibreQoS/pull/684</a> </strong>We will miss you so much, Dave.</p><p>– Robert, Herbert, and Frank with LibreQoS</p>								</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Myst Graph: A New Perspective on Myst (197 pts)]]></title>
            <link>https://glthr.com/myst-graph-1</link>
            <guid>43549293</guid>
            <pubDate>Tue, 01 Apr 2025 17:19:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://glthr.com/myst-graph-1">https://glthr.com/myst-graph-1</a>, See on <a href="https://news.ycombinator.com/item?id=43549293">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-content-parent"><div data-node-type="callout">
<p>👋</p>
<p><em>Hi HackerNews! Please note that </em><a target="_blank" href="https://glthr.com/myst-graph-2"><em>a second article</em></a><em> uses graph analysis to reveal new findings about Myst (e.g., unreachable views, most connected locations).</em></p>
</div>

<p>Upon reflection, <a target="_blank" href="https://en.wikipedia.org/wiki/Myst">Myst</a> has long been more analogous to a graph than a traditional linear game, owing to the relative freedom it affords players. This is particularly evident in its first release (Macintosh, 1993), which was composed of interconnected HyperCard cards.</p>
<p>It is now literally one. Here is Myst as a graph:</p>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1743209037941/20772834-e8ee-4c58-9fad-660cfe6b3f09.png?auto=compress,format&amp;format=webp" alt=""></p>
<p>The Myst graph, a comprehensive representation of the game’s structure that maps the connections between various views and locations, <strong>is downloadable</strong> <a target="_blank" href="https://github.com/glthr/DeMystify/blob/054bcb5de98e977bd5635b2a2664bebf343b56ea/graphs/myst_graph.pdf"><strong>as a poster-size PDF</strong></a>.  </p>
<h2 id="heading-introduction">Introduction</h2>
<h2 id="heading-origins-of-the-myst-graph">Origins of the Myst Graph</h2>
<p>I fondly recall creating, just like many others (including <a target="_blank" href="http://www.imajeenyus.com/computer/20120814_selenitic_age_map/selenitic_age_maze_map.pdf">more skilled cartographers</a>), a rudimentary <em>topological map</em> of <a target="_blank" href="https://dni.fandom.com/wiki/Mazerunner">the infamous Myst Mazerunner puzzle</a> when I was young. The idea recently struck me: could this approach be extended to the entire game? The HyperCard implementation’s deterministic connection of cards seemed tailor-made for an even more abstract map: a <em>graph</em>.  After all, graphs are a powerful tool for information analysis. By having the ability to represent the original Myst as a network of interconnected nodes and edges, we could gain a deeper understanding of how the different aspects of the game are related and uncover new insights into its underlying mechanics.</p>
<p>However, I did not see myself creating it manually; it would have been a monumental and error-prone task. Fortunately, I soon discovered, thanks to the efforts of <a target="_blank" href="https://mastodon.social/@uliwitness@chaos.social">Uli Kusterer</a> (Youtube channel: <a target="_blank" href="https://www.youtube.com/watch?v=CjrBwRnIVG4&amp;list=PLZjGMBjt_VVCl3cftfjeO8qQsSxgDQYO1">“Masters of the Void”</a>) and the <a target="_blank" href="https://www.youtube.com/watch?v=lacwEuMaQvQ">“Reverse Engineering Myst”</a> presentation at <a target="_blank" href="https://mysterium.net/">Mysterium Con</a> 2024, that the game’s source code was accessible. That changed everything: how about generating the graph programmatically?</p>
<h2 id="heading-roadmap">Roadmap</h2>
<p><em>(1)</em> In this article, I introduce the concept of the Myst graph, explaining why it is an important tool for analyzing the game, and walk readers through how to interpret the graph, highlighting key concepts and insights. <em>(2)</em> <strong>A second article,</strong> <a target="_blank" href="https://glthr.com/myst-graph-2"><strong>already published</strong></a><strong>, explores new findings that have emerged from analyzing the Myst graph. This article builds on the present article, providing additional context and explanations for the insights gained from the graph.</strong> <em>(3)</em> A third article, not published yet, will detail the technical approach and open source “DeMystify,” the program I created to generate the graph. This will provide readers with hands-on experience exploring the Myst graph using code. <em>(4)</em> Last, a fourth article will discuss how this graph can be used as a starting point for new projects and speculate on what other insights might be gained by further analysis.</p>
<h2 id="heading-minor-terminology-clarifications">Minor Terminology Clarifications</h2>
<p>Before diving into the presentation of the graph, let’s clarify some words. “<em>Dunny</em>” refers to D’ni: this is how the D’ni stack was initially named. Also, I may refer to the Myst Age as the “<em>Myst Island</em>” for variation. When discussing graph theory, I will use the following terms: <em>nodes</em> will refer to the vertices, while <em>edges</em> represent the connections between them. For those unfamiliar with graph theory, just remember that edges are arrows pointing from one box (or view, or card) to another, the nodes, illustrating a relationship between the two.</p>
<h2 id="heading-disclaimer">Disclaimer</h2>
<p>This project is a personal initiative to analyze and understand the classic Myst game. It is an unofficial and nonlucrative open-source effort created for educational purposes only. This project is not affiliated with Cyan Worlds, Inc. or the original creators of Myst. Any opinions, insights, or analyses presented on this blog are my personal views and do not reflect any official positions or statements of Cyan Worlds, Inc., its affiliates, or any organizations I might be affiliated with.</p>
<p>Before analyzing this graph, please be aware that this series of articles will contain spoilers for the game. If you are new to Myst or plan to play it soon, I recommend skipping these articles and experiencing it firsthand: Myst remains such a fantastic game!</p>
<h2 id="heading-basic-properties-of-the-myst-graph">Basic Properties of the Myst Graph</h2>
<h2 id="heading-nodes-and-edges">Nodes and Edges</h2>
<p>The game comprises 6 HyperCard stacks, one for each Age, totaling 1,355 cards. The graph abstracts these into 1,364 nodes connected by 3,189 edges.</p>
<p>The number of nodes does not coincide with the number of cards and stacks because 3 nodes are <em>virtual</em>. They represent cards that were not shipped in the release. They only exist because they are referred to by actual cards. These virtual nodes are Mechanical Age 17673 and 20348, and Selenitic Age 31832.</p>
<p>In this version of the graph, edges do not encode gameplay constraints. A card is considered connected to another if it references that card; the specifics of how the player interacts with the card to reach the other card—whether by simply clicking or solving a complex puzzle—are left implicit. (This choice is discussed later.)</p>
<h2 id="heading-paths">Paths</h2>
<p>Paths are sequences of edges from node <em>A</em> to node <em>B</em>. Because the gameplay has not been integrated into the graph, they do not necessarily represent feasible direct paths in the game. For instance, a puzzle on the path may require performing actions outside the path. In other words, the paths evoked in these articles abstract away all the constraints of the game, except for the connections between cards.</p>
<p>Theoretically, the <em>shortest path</em> between the starting point and the (good) ending consists of 24 edges. From a pure graph distance point of view, reaching the beginning of the game and its ending only takes 24 movements or a change of direction. The <em>most distant nodes</em> (in other words, the longest shortest path in the graph) are separated by 130 edges. In the context of the game, <a target="_blank" href="https://glthr.com/myst-graph-2">these paths are detailed in the second article</a>.</p>
<h2 id="heading-clusters">Clusters</h2>
<p>Interestingly, while the file that encodes the graph does not contain any information about clusters (except for isolated clusters, as explained in the second article, and for the colors of the nodes, for rendering purposes), the rendering engine that transforms it into a PDF, creates an organic (albeit imperfect) formation of clusters: the Myst Island in the middle, and the Ages at the periphery. This is explainable: Myst Island refers to all Ages; each Age only refers to Myst Island.</p>
<p>There is one disconnected cluster outside a few isolated nodes—that, consequently, cannot be reached from within the game. This isolated cluster comprises 3 nodes, including 2 virtual ones, from the Mechanical Age: originally, 17552 had directed edges with 20348 and 17673, both absent from the Mechanical Age stack.</p>
<h2 id="heading-legend-how-to-read-the-graph">Legend: How to Read the Graph</h2>
<h2 id="heading-nodes">Nodes</h2>
<h3 id="heading-colors">Colors</h3>
<p>Each Age has its own color to facilitate the identification of stack-based clusters.</p>
<h3 id="heading-nodes-labels">Nodes Labels</h3>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1743209347182/07be4e6f-ef5a-423a-b356-707561202f3f.png?auto=compress,format&amp;format=webp" alt=""></p>
<p>When a node abstracts a stack, the name of the stack (corresponding to the Age) is prefixed with “[Stack]”. For instance, “[Stack] Myst” is the stack containing the cards belonging to the Myst Island.</p>
<p>Nodes representing cards are labeled following this pattern: “{Stack}:{ID} [({Original name})] \ {Image name}”.</p>
<p>The cards are identified by an ID (see <a target="_blank" href="https://hypercard.org/HyperTalk%20Reference%202.4.pdf#page=280">page 280 of this reference</a>), a HyperCard-generated integer. Example: “Mechanical Age:12345” refers to card ID 12345 belonging to the Mechanical Age stack. These IDs are unique <em>per stack</em>. Since each stack has its own set of unique IDs, there can be collisions across stacks where two or more IDs are identical between different stacks. This is evident in the Myst graph, where some IDs appear twice across Ages (8059, 8794, 9075, 18304, 19870, 21064, 32302, 33578, 34421, 39710, 52457, 55809, and 74269). As HyperCard automatically increments these IDs within each stack, one can reasonably infer that the greater an ID value is, the closer in time it was created relative to the initial release. However, this relationship may not hold for the underlying assets, as they were rendered separately through a different process. In other words, a “recent” (relative to the release) card does not necessarily mean that the asset it represents was also rendered close to the release.</p>
<p>Cards can have been voluntarily named. When that occurs, the original name is specified between parentheses after the ID. For example, “Myst:81655 (AchenarLose)” was named “AchenarLose” by the Myst creators.</p>
<p>Last, cards represent a view or an image, so they need a reference to an asset filename. This information is displayed just below the label. So “Myst:8336 (dock) \ <em>Dock1-E</em>” means that card ID 8336, belonging to the Myst Island and named initially “dock” features an image called “<em>Dock1-E</em>”. This specific node plays an important role in the game, as it is where the player starts.</p>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1743209447972/35be9198-659d-4aba-8134-e326d8c41f65.png?auto=compress,format&amp;format=webp" alt=""></p>
<h3 id="heading-special-nodes">Special Nodes</h3>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1743209475311/765207ee-1398-4fb2-a176-fba8c7b3a2c5.png?auto=compress,format&amp;format=webp" alt=""></p>
<p><strong>Sink nodes</strong> serve as destinations, receiving connections but not providing any exit paths. They can be reached within the game, contrary to the following categories of nodes. <strong>Source nodes</strong> act as starting points, sending out connections and initiating new paths without incoming edges. They are unreachable in the game. <strong>Isolated nodes</strong> are unreachable from the rest of the graph, having no incoming or outgoing edges that could be followed.</p>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1743209487825/0f17f99e-67a5-4b9b-8983-b8f2d4e19427.png?auto=compress,format&amp;format=webp" alt=""></p>
<p>More specific to the game, nodes with <strong>blue borders</strong> represent views containing blue pages, while those with <strong>red borders</strong> show views having red pages. The unique, <strong>purple-bordered</strong> node denotes a view encompassing blue and red pages.</p>
<h2 id="heading-edges">Edges</h2>
<h3 id="heading-directions">Directions</h3>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1743209522084/2b46ae44-812d-4996-ab70-286d53a18dbe.png?auto=compress,format&amp;format=webp" alt=""></p>
<p>The Myst graph is a directed graph. The quasi-totality of the edges is either <strong>directed</strong> (they connect two nodes in one direction) or <strong>bidirectional</strong> (they connect two nodes in both directions). Rarely (3 occurrences), some edges are <strong>self-loops</strong>, connecting a node to itself.</p>
<p>In the game, a <em>bidirectional</em> edge typically represents a direct change of orientation (cardinal points, looking up and down). Movements are usually represented by <em>directed</em> edges, as walking backward is impossible, except for some exceptions (backtracking edges; see below).</p>
<p>Here is a concrete demonstration. In the village of the Channelwood Age, changing between one of the staircase ascending views (17225) and its corresponding pathway view (73038) can be done in a single click as the edge is bidirectional. However, once the player takes the stairs to go up (73346), it is impossible to backtrack directly to the original staircase ascending view without first turning around to look down (73587), then reaching the original pathway view (73038) and last turning around again (17225).</p>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1743209541672/1d322a52-aec4-436e-97d2-7b3482a33c9b.png?auto=compress,format&amp;format=webp" alt=""></p>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1743209546442/b749a6d0-83bf-422d-b064-b604212c0dbc.png?auto=compress,format&amp;format=webp" alt=""></p>
<p>To summarize, and with some exceptions, from a navigational perspective, while bidirectional edges can be interpreted as walking, directed edges can be interpreted as changing directions.</p>
<h3 id="heading-special-edges">Special Edges</h3>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1743209571761/5a268f32-8899-4ff6-afc8-1fec1a61fb1e.png?auto=compress,format&amp;format=webp" alt=""></p>
<p>Edges connecting two nodes from different Ages (<strong>cross-Ages edges</strong>) are rendered with a thick line. Edges that connect exclusively two nodes are <strong>backtracking edges</strong>. They act as a device to zoom in/zoom out (one click to enter the next node, one click to go back to the previous node) and are represented by edges with inversed arrow heads and tails. Last, some of the connections are commented out in the source code of Myst: they characterize <strong>disabled edges</strong>, represented by dashed lines with a tee arrow shape.</p>
<h3 id="heading-transitivity">Transitivity</h3>
<p>In Myst, there are three forms of transitivity.</p>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1743209612487/02eb6814-2c5c-4d6f-93eb-58fd0e203c22.png?auto=compress,format&amp;format=webp" alt=""></p>
<p>First, by default, intermediary nodes are <strong>fully transitive</strong> (edges directions notwithstanding, obviously). If node B can access A, and C and D can access B, then C and D can access A. This is the common navigational mechanism.</p>
<p>Second, some edges are <strong>intransitive</strong>: while B can access A and, reciprocally, A can access B, C and D can access B too, but they cannot access A. This relationship characterizes the backtracking edges.</p>
<p>Lastly, some edges are <strong>partially transitive</strong>: if node B can access A, C, and D can access B, only D can access A. In the realm of Myst, this is only true of the edges making the player return to Myst after completing an Age. Upon returning to Myst Island with the book, they first systematically see the ceiling of the library (Myst:44018). When they click the ceiling, they automatically face the bookshelf (Myst:46439). (The mechanism by which this is done with HyperTalk, the scripting language of HyperCard, will be detailed in the third article.)</p>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1743209635499/d455df43-c947-45e0-9150-5e2d22912715.png?auto=compress,format&amp;format=webp" alt=""></p>
<h2 id="heading-limitations">Limitations</h2>
<p>First, as explained in the previous section, the graph does not capture the gameplay mechanics. Although this approach has its limitations, it provides a significant advantage. By separating the gameplay from the graph structure, we can already gain novel insights into Myst without dealing with the complexities of puzzle design. Of course, nothing prevents future versions of the graph from including this logic.</p>
<p>Second, the rendered clusters are not entirely separate from each other. They overlap in their boundaries. For example, Myst:38896 and Myst:30143 encroach on the D’ni Age. While it would be possible to constrain the graph so there is no overlapping, I made the rendering engine as Age agnostic as possible for this initial version. As “DeMystify”, the graph generator, will be released, readers can try different arrangements; I would be very interested in seeing their results.</p>
<p>Third, the graph only captures straightforward relationships between cards and stacks. Analyzing the scripts more granularly and identifying complex relationships, such as image substitutions, would be interesting. This lack of granularity is evident for the books from the bookshelf: the current graph does not sufficiently capture the pages. A future version may improve this aspect.</p>
<h2 id="heading-next-steps">Next Steps</h2>
<p>Now that we have had a chance to familiarize ourselves with the Myst graph, its structure, and how it relates to the game, l<strong>et’s dive deeper</strong> <a target="_blank" href="https://glthr.com/myst-graph-2"><strong>with a second article</strong></a><strong>, which explores some of the insights and discoveries made possible through this new conceptual tool</strong>. The article will take us through the hidden connections and relationships between various game elements, revealing new aspects of Myst that were previously unknown or unseen.</p>
<p>The graph will make one with the game.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Qwen-2.5-32B is now the best open source OCR model (196 pts)]]></title>
            <link>https://github.com/getomni-ai/benchmark/blob/main/README.md</link>
            <guid>43549072</guid>
            <pubDate>Tue, 01 Apr 2025 17:00:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/getomni-ai/benchmark/blob/main/README.md">https://github.com/getomni-ai/benchmark/blob/main/README.md</a>, See on <a href="https://news.ycombinator.com/item?id=43549072">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true" aria-labelledby="file-name-id-wide file-name-id-mobile"><article itemprop="text"><p dir="auto"><a href="https://getomni.ai/ocr-benchmark" rel="nofollow"><img src="https://camo.githubusercontent.com/628b0587ccd21fe4c90b6d165c5ab572fd0a2a1c78d59c4ce1587d2cc6f79d17/68747470733a2f2f6f6d6e6961692d696d616765732e73332e75732d656173742d312e616d617a6f6e6177732e636f6d2f6f6d6e692d6f63722d62656e63686d61726b2e706e67" alt="Omni OCR Benchmark" data-canonical-src="https://omniai-images.s3.us-east-1.amazonaws.com/omni-ocr-benchmark.png"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Omni OCR Benchmark</h2><a id="user-content-omni-ocr-benchmark" aria-label="Permalink: Omni OCR Benchmark" href="#omni-ocr-benchmark"></a></p>
<p dir="auto">A benchmarking tool that compares OCR and data extraction capabilities of different large multimodal models such as gpt-4o, evaluating both text and json extraction accuracy. The goal of this benchmark is to publish a comprehensive benchmark of OCRaccuracy across traditional OCR providers and multimodal Language Models. The evaluation dataset and methodologies are all Open Source, and we encourage expanding this benchmark to encompass any additional providers.</p>
<p dir="auto"><a href="https://getomni.ai/blog/benchmarking-open-source-models-for-ocr" rel="nofollow"><strong>Open Source LLM Benchmark Results (Mar 2025)</strong></a> | <a href="https://huggingface.co/datasets/getomni-ai/ocr-benchmark" rel="nofollow"><strong>Dataset</strong></a></p>
<p dir="auto"><a href="https://getomni.ai/ocr-benchmark" rel="nofollow"><strong>Benchmark Results (Feb 2025)</strong></a> | <a href="https://huggingface.co/datasets/getomni-ai/ocr-benchmark" rel="nofollow"><strong>Dataset</strong></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/30934424/429220141-2be179ad-0abd-4f0e-b73a-7d5a70390367.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDM1NTA1MDMsIm5iZiI6MTc0MzU1MDIwMywicGF0aCI6Ii8zMDkzNDQyNC80MjkyMjAxNDEtMmJlMTc5YWQtMGFiZC00ZjBlLWI3M2EtN2Q1YTcwMzkwMzY3LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA0MDElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNDAxVDIzMzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTE2OWRhMDYyMTJkY2Y0Y2IyNTNjZGRhM2I0ZGI4ZGJjMzUxNGFiY2UzZTU4MmMzYTc3ZjA2NDc5NmU5YzMwN2MmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.2QjIz9LPDIfSr2nqi-Sm9fshhY-h3mhkhQfh9BFDfyI"><img src="https://private-user-images.githubusercontent.com/30934424/429220141-2be179ad-0abd-4f0e-b73a-7d5a70390367.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDM1NTA1MDMsIm5iZiI6MTc0MzU1MDIwMywicGF0aCI6Ii8zMDkzNDQyNC80MjkyMjAxNDEtMmJlMTc5YWQtMGFiZC00ZjBlLWI3M2EtN2Q1YTcwMzkwMzY3LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA0MDElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNDAxVDIzMzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTE2OWRhMDYyMTJkY2Y0Y2IyNTNjZGRhM2I0ZGI4ZGJjMzUxNGFiY2UzZTU4MmMzYTc3ZjA2NDc5NmU5YzMwN2MmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.2QjIz9LPDIfSr2nqi-Sm9fshhY-h3mhkhQfh9BFDfyI" alt="image"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Methodology</h2><a id="user-content-methodology" aria-label="Permalink: Methodology" href="#methodology"></a></p>
<p dir="auto">The primary goal is to evaluate JSON extraction from documents. To evaluate this, the Omni benchmark runs <strong>Document ⇒ OCR ⇒ Extraction</strong>. Measuring how well a model can OCR a page, and return that content in a format that an LLM can parse.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/3888f1759b84adb355b4f91d0da2ee6fc772fc7b44fed778aff22d8e6b8cb0f1/68747470733a2f2f6f6d6e6961692d696d616765732e73332e75732d656173742d312e616d617a6f6e6177732e636f6d2f6d6574686f646f6c6f67792d6469616772616d2e706e67"><img src="https://camo.githubusercontent.com/3888f1759b84adb355b4f91d0da2ee6fc772fc7b44fed778aff22d8e6b8cb0f1/68747470733a2f2f6f6d6e6961692d696d616765732e73332e75732d656173742d312e616d617a6f6e6177732e636f6d2f6d6574686f646f6c6f67792d6469616772616d2e706e67" alt="methodology" data-canonical-src="https://omniai-images.s3.us-east-1.amazonaws.com/methodology-diagram.png"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Evaluation Metrics</h2><a id="user-content-evaluation-metrics" aria-label="Permalink: Evaluation Metrics" href="#evaluation-metrics"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">JSON accuracy</h3><a id="user-content-json-accuracy" aria-label="Permalink: JSON accuracy" href="#json-accuracy"></a></p>
<p dir="auto">We use a modified <a href="https://github.com/zgrossbart/jdd">json-diff</a> to identify differences between predicted and ground truth JSON objects. You can review the <a href="https://github.com/getomni-ai/benchmark/blob/main/src/evaluation/json.ts">evaluation/json.ts</a> file to see the exact implementation. Accuracy is calculated as:</p>
<math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="48ab4fa3f8d3dd346b396ed5891458ab">$$\text{Accuracy} = 1 - \frac{\text{number of difference fields}}{\text{total fields}}$$</math-renderer>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/1cb5a1143e4200a729f6c296a5306caceb6de7dec623d0f59da135530a4fa6d6/68747470733a2f2f6f6d6e6961692d696d616765732e73332e75732d656173742d312e616d617a6f6e6177732e636f6d2f6a736f6e5f61636375726163792e706e67"><img src="https://camo.githubusercontent.com/1cb5a1143e4200a729f6c296a5306caceb6de7dec623d0f59da135530a4fa6d6/68747470733a2f2f6f6d6e6961692d696d616765732e73332e75732d656173742d312e616d617a6f6e6177732e636f6d2f6a736f6e5f61636375726163792e706e67" alt="json-diff" data-canonical-src="https://omniai-images.s3.us-east-1.amazonaws.com/json_accuracy.png"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Text similarity</h3><a id="user-content-text-similarity" aria-label="Permalink: Text similarity" href="#text-similarity"></a></p>
<p dir="auto">While the primary benchmark metric is JSON accuracy, we have included <a href="https://en.wikipedia.org/wiki/Levenshtein_distance" rel="nofollow">levenshtein distance</a> as a measurement of text similarity between extracted and ground truth text.
Lower distance indicates higher similarity. Note this scoring method heavily penalizes accurate text that does not conform to the exact layout of the ground truth data.</p>
<p dir="auto">In the example below, an LLM could decode both blocks of text without any issue. All the information is 100% accurate, but slight rearrangements of the header text (address, phone number, etc.) result in a large difference on edit distance scoring.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/d1d49357f37a1fa8e1ba6673a550fbbcc70d4245bf41c6be215d8ce25927dbc2/68747470733a2f2f6f6d6e6961692d696d616765732e73332e75732d656173742d312e616d617a6f6e6177732e636f6d2f656469745f64697374616e63652e706e67"><img src="https://camo.githubusercontent.com/d1d49357f37a1fa8e1ba6673a550fbbcc70d4245bf41c6be215d8ce25927dbc2/68747470733a2f2f6f6d6e6961692d696d616765732e73332e75732d656173742d312e616d617a6f6e6177732e636f6d2f656469745f64697374616e63652e706e67" alt="text-similarity" data-canonical-src="https://omniai-images.s3.us-east-1.amazonaws.com/edit_distance.png"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Running the benchmark</h2><a id="user-content-running-the-benchmark" aria-label="Permalink: Running the benchmark" href="#running-the-benchmark"></a></p>
<ol dir="auto">
<li>Clone the repo and install dependencies: <code>npm install</code></li>
<li>Prepare your test data
<ol dir="auto">
<li>For local data, add individual files to the <code>data</code> folder.</li>
<li>To pull from a DB, add <code>DATABASE_URL</code> in your <code>.env</code></li>
</ol>
</li>
<li>Copy the <code>models.example.yaml</code> file to <code>models.yaml</code>. Set up API keys in <code>.env</code> for the models you want to test. Check out the <a href="#supported-models">supported models</a> here.</li>
<li>Run the benchmark: <code>npm run benchmark</code></li>
<li>Results will be saved in the <code>results/&lt;timestamp&gt;/results.json</code> file.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Supported models</h2><a id="user-content-supported-models" aria-label="Permalink: Supported models" href="#supported-models"></a></p>
<p dir="auto">To enable specific models, create a <code>models.yaml</code> file in the <code>src</code> directory. Check out the <a href="https://github.com/getomni-ai/benchmark/blob/main/src/models.example.yaml">models.example.yaml</a> file for the required variables.</p>
<div dir="auto" data-snippet-clipboard-copy-content="models:
  - ocr: gemini-2.0-flash-001 # The model to use for OCR
    extraction: gpt-4o # The model to use for JSON extraction

  - ocr: gpt-4o
    extraction: gpt-4o
    directImageExtraction: true # Whether to use the model's native image extraction capabilities"><pre><span>models</span>:
  - <span>ocr</span>: <span>gemini-2.0-flash-001 </span><span><span>#</span> The model to use for OCR</span>
    <span>extraction</span>: <span>gpt-4o </span><span><span>#</span> The model to use for JSON extraction</span>

  - <span>ocr</span>: <span>gpt-4o</span>
    <span>extraction</span>: <span>gpt-4o</span>
    <span>directImageExtraction</span>: <span>true </span><span><span>#</span> Whether to use the model's native image extraction capabilities</span></pre></div>
<p dir="auto">You can view configuration for each model in the <a href="https://github.com/getomni-ai/benchmark/blob/main/src/models">src/models/</a> folder.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Closed-source LLMs</h3><a id="user-content-closed-source-llms" aria-label="Permalink: Closed-source LLMs" href="#closed-source-llms"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model Provider</th>
<th>Models</th>
<th>OCR</th>
<th>JSON Extraction</th>
<th>Required ENV Variables</th>
</tr>
</thead>
<tbody>
<tr>
<td>Anthropic</td>
<td><code>claude-3-5-sonnet-20241022</code></td>
<td>✅</td>
<td>✅</td>
<td><code>ANTHROPIC_API_KEY</code></td>
</tr>
<tr>
<td>OpenAI</td>
<td><code>gpt-4o</code></td>
<td>✅</td>
<td>✅</td>
<td><code>OPENAI_API_KEY</code></td>
</tr>
<tr>
<td>Gemini</td>
<td><code>gemini-2.0-flash-001</code>, <code>gemini-1.5-pro</code>, <code>gemini-1.5-flash</code></td>
<td>✅</td>
<td>✅</td>
<td><code>GOOGLE_GENERATIVE_AI_API_KEY</code></td>
</tr>
<tr>
<td>Mistral</td>
<td><code>mistral-ocr</code></td>
<td>✅</td>
<td>❌</td>
<td><code>MISTRAL_API_KEY</code></td>
</tr>
<tr>
<td>OmniAI</td>
<td><code>omniai</code></td>
<td>✅</td>
<td>✅</td>
<td><code>OMNIAI_API_KEY</code>, <code>OMNIAI_API_URL</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Open-source LLMs</h3><a id="user-content-open-source-llms" aria-label="Permalink: Open-source LLMs" href="#open-source-llms"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model Provider</th>
<th>Models</th>
<th>OCR</th>
<th>JSON Extraction</th>
<th>Required ENV Variables</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gemma 3</td>
<td><code>google/gemma-3-27b-it</code></td>
<td>✅</td>
<td>❌</td>
<td></td>
</tr>
<tr>
<td>Qwen 2.5</td>
<td><code>qwen2.5-vl-32b-instruct</code>, <code>qwen2.5-vl-72b-instruct</code></td>
<td>✅</td>
<td>❌</td>
<td></td>
</tr>
<tr>
<td>Llama 3.2</td>
<td><code>meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo</code>, <code>meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo</code></td>
<td>✅</td>
<td>❌</td>
<td></td>
</tr>
<tr>
<td>ZeroX</td>
<td><code>zerox</code></td>
<td>✅</td>
<td>✅</td>
<td><code>OPENAI_API_KEY</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Cloud OCR Providers</h3><a id="user-content-cloud-ocr-providers" aria-label="Permalink: Cloud OCR Providers" href="#cloud-ocr-providers"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model Provider</th>
<th>Models</th>
<th>OCR</th>
<th>JSON Extraction</th>
<th>Required ENV Variables</th>
</tr>
</thead>
<tbody>
<tr>
<td>AWS</td>
<td><code>aws-text-extract</code></td>
<td>✅</td>
<td>❌</td>
<td><code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code>, <code>AWS_REGION</code></td>
</tr>
<tr>
<td>Azure</td>
<td><code>azure-document-intelligence</code></td>
<td>✅</td>
<td>❌</td>
<td><code>AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT</code>, <code>AZURE_DOCUMENT_INTELLIGENCE_KEY</code></td>
</tr>
<tr>
<td>Google</td>
<td><code>google-document-ai</code></td>
<td>✅</td>
<td>❌</td>
<td><code>GOOGLE_LOCATION</code>, <code>GOOGLE_PROJECT_ID</code>, <code>GOOGLE_PROCESSOR_ID</code>, <code>GOOGLE_APPLICATION_CREDENTIALS_PATH</code></td>
</tr>
<tr>
<td>Unstructured</td>
<td><code>unstructured</code></td>
<td>✅</td>
<td>❌</td>
<td><code>UNSTRUCTURED_API_KEY</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<ul dir="auto">
<li>LLMS are instructed to use the following <a href="https://github.com/getomni-ai/benchmark/blob/main/src/models/shared/prompt.ts">system prompts</a> for OCR and JSON extraction.</li>
<li>For Google Document AI, you need to include <code>google_credentials.json</code> in the <code>data</code> folder.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Benchmark Dashboard</h2><a id="user-content-benchmark-dashboard" aria-label="Permalink: Benchmark Dashboard" href="#benchmark-dashboard"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/getomni-ai/benchmark/blob/main/assets/dashboard-gif.gif"><img src="https://github.com/getomni-ai/benchmark/raw/main/assets/dashboard-gif.gif" alt="dashboard" data-animated-image=""></a></p>
<p dir="auto">You can use benchmark dashboard to easily view the results of each test run. Check out the <a href="https://github.com/getomni-ai/benchmark/blob/main/dashboard/README.md">dashboard documentation</a> for more details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project is licensed under the MIT License - see the LICENSE file for details.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Tell HN: Camelgate NPM Outage (Cloudflare) (108 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=43548589</link>
            <guid>43548589</guid>
            <pubDate>Tue, 01 Apr 2025 16:19:28 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=43548589">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>To be fair to WAFs, most are more than just a pile of regexes. Things like detecting bot traffic - be it spammers or AI scrapers - are valuable (ESPECIALLY the AI scraper detection, because unlike search engines these things have zero context recognition or respect for robots.txt and will just happily go on and ingest very heavy endpoints), and the large CDN/WAF providers can do it even better because they can spot shit like automated port scanners, Metasploit or similar skiddie tooling across all the services that use them.</p><p>Honestly what I'd _love_ to see is AWS, GCE, Azure, Fastly, Cloudflare and Akamai band together and share information about such bad actors, compile evidence lists and file abuse reports against their ISP - or in case the ISP is a "bulletproof hoster" or certain enemy states, initiate enforcement actors like governments to get these bad ISPs disconnected from the Internet.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A man powers home for eight years using a thousand old laptop batteries (366 pts)]]></title>
            <link>https://techoreon.com/a-man-powers-home-8-years-laptop-batteries/</link>
            <guid>43548217</guid>
            <pubDate>Tue, 01 Apr 2025 15:49:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techoreon.com/a-man-powers-home-8-years-laptop-batteries/">https://techoreon.com/a-man-powers-home-8-years-laptop-batteries/</a>, See on <a href="https://news.ycombinator.com/item?id=43548217">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
<p><strong>A man has managed to power his home for eight years with a system using more than 1,000 recycled laptop batteries</strong>. This ingenious project, based on the use of electronic waste, has proven to be an environmentally friendly and economical solution, without the need to even replace batteries over the years.</p>



<p>This system also uses solar panels, which were the origin of his renewable energy project that he started a long time ago and which has been enough for him to live during this time.</p>



<h3><mark>How Does This DIY Power System Work?</mark></h3>



<p>The project began in November 2016, when the creator, known with the alias <em>Glubux</em> on online forums, began sharing his plans in the <a href="https://secondlifestorage.com/index.php?threads%2Fglubuxs-powerwall.126%2F=" target="_blank" rel="noopener"><em>Second Life Storage community</em></a>. From the outset, his goal was clear: to generate energy for his home without relying on the electrical grid, through a combination of solar panels and recycled batteries.</p>




<p>In its early stages, it used a basic 1.4 kW solar panel system, along with an old 24V 460Ah forklift battery, charge controllers, and a 3 kVA inverter. However, its vision was to <strong>expand the system and take it beyond</strong> what it had initially achieved.</p>



<p>The centerpiece of their system is more than 1,000 secondhand laptop batteries. For many, old computer batteries are considered waste, but for Glubux, they represented an opportunity to create a completely independent as well as renewable energy source.</p>



<p>Reusing these batteries is a great idea and is an example of how it’s possible to give a second life to electronic waste, a sector in which the UN has noted that <strong>less than a quarter</strong> of the e-waste generated globally is properly collected and recycled.</p>




<p>The system was initially modest, but over time, Glubux began adding more and more recycled batteries. Soon,&nbsp;his installation grew from a small setup to a self-powered system consisting of 650 batteries.</p>



<p>This growth forced the creator to build a separate warehouse, located about 50 meters from his home, to store the batteries and the new charge controllers and inverters. The warehouse became a workshop where he assembled the battery packs, grouping them together to create blocks with a capacity of approximately 100 Ah each.</p>



<p>At first, he faced some obstacles. The battery discharge rates were uneven due to differences in the battery cells used, <strong>causing some to drain faster than others</strong>. However, the solution came with rearranging and adjusting the cells to ensure the packs worked more efficiently.</p>



<p>Glubux even began disassembling entire laptop batteries, removing individual cells and organizing them into custom racks. This task, which likely required a great deal of manual labor and technical knowledge, was key to making the system work effectively and sustainably.</p>



<h3><mark>How this System Has Lasted Eight Years?</mark></h3>



<p><strong>The most amazing thing about this project</strong> is that, despite the initial difficulties and the experimental nature of the system, it has continued to operate uninterruptedly today. In its eight years of operation, not a single battery cell has needed to be replaced, a remarkable achievement considering the operating conditions and the nature of the recycled batteries.</p>



<p>In addition, over the years, Glubux has improved and expanded its solar panel system. Currently, its installation features 24 solar panels, each measuring 440W, allowing it to generate sufficient power even during the coldest months.</p>




<p><strong>Despite being an unusual system</strong>, with recycled and homemade components, no major problems have been reported, such as fires or swollen batteries, which is a common issue with some second-hand electronic devices.</p>



<p>Glubux, for its part, continues to operate with complete confidence in its installation, which has not only been able to supply all of its home’s electricity, but also allows the operation of equipment such as the washing machine.</p>



<hr>







<!-- CONTENT END 1 -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[RubyUI (Former PhlexUI): Ruby Gem for RubyUI Components (142 pts)]]></title>
            <link>https://github.com/ruby-ui/ruby_ui</link>
            <guid>43548108</guid>
            <pubDate>Tue, 01 Apr 2025 15:39:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ruby-ui/ruby_ui">https://github.com/ruby-ui/ruby_ui</a>, See on <a href="https://news.ycombinator.com/item?id=43548108">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">RubyUI (former PhlexUI) 🚀</h2><a id="user-content-rubyui-former-phlexui-" aria-label="Permalink: RubyUI (former PhlexUI) 🚀" href="#rubyui-former-phlexui-"></a></p>
<p dir="auto">Beautifully designed components that you can copy and paste into your apps. Accessible. Customizable. Open Source.</p>
<p dir="auto">This is NOT a component library. It's a collection of re-usable components that you can generate or copy and paste into your apps.</p>
<p dir="auto">Pick the components you need. Copy and paste the code into your project and customize to your needs. The code is yours.</p>
<p dir="auto">Use this as a reference to build your own component libraries.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Key Features:</h3><a id="user-content-key-features" aria-label="Permalink: Key Features:" href="#key-features"></a></p>
<ul dir="auto">
<li><strong>Built for Speed</strong> ⚡: RubyUI leverages Phlex, which is up to 12x faster than traditional Rails ERB templates.</li>
<li><strong>Stunning UI</strong> 🎨: Design beautiful, streamlined, and customizable UIs that sell your app effortlessly.</li>
<li><strong>Stay Organized</strong> 📁: Keep your UI components well-organized and easy to manage.</li>
<li><strong>Customer-Centric UX</strong> 🧑‍💼: Create memorable app experiences for your users.</li>
<li><strong>Completely Customizable</strong> 🔧: Full control over the design of all components.</li>
<li><strong>Minimal Dependencies</strong> 🍃: Uses custom-built Stimulus.js controllers to keep your app lean.</li>
<li><strong>Reuse with Ease</strong> ♻️: Build components once and use them seamlessly across your project.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">How to Use:</h3><a id="user-content-how-to-use" aria-label="Permalink: How to Use:" href="#how-to-use"></a></p>
<ol dir="auto">
<li><strong>Find the perfect component</strong> 🔍: Browse live-embedded components on our documentation page.</li>
<li><strong>Copy the snippet</strong> 📋: Easily copy code snippets for quick implementation.</li>
<li><strong>Make it yours</strong> 🎨: Customize components using Tailwind utility classes to fit your specific needs.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation 🚀</h2><a id="user-content-installation-" aria-label="Permalink: Installation 🚀" href="#installation-"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">1. Install the gem</h3><a id="user-content-1-install-the-gem" aria-label="Permalink: 1. Install the gem" href="#1-install-the-gem"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="bundle add ruby_ui --group development --require false"><pre>bundle add ruby_ui --group development --require <span>false</span></pre></div>
<p dir="auto">or add it to your Gemfile:</p>
<div dir="auto" data-snippet-clipboard-copy-content="gem &quot;ruby_ui&quot;, group: :development, require: false"><pre><span>gem</span> <span>"ruby_ui"</span><span>,</span> <span>group</span>: <span>:development</span><span>,</span> <span>require</span>: <span>false</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">2. Run the installer:</h3><a id="user-content-2-run-the-installer" aria-label="Permalink: 2. Run the installer:" href="#2-run-the-installer"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="bin/rails g ruby_ui:install"><pre>bin/rails g ruby_ui:install</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">3. Done! 🎉</h3><a id="user-content-3-done-" aria-label="Permalink: 3. Done! 🎉" href="#3-done-"></a></p>
<p dir="auto">You can generate your components using <code>ruby_ui:component</code> generator.</p>
<div dir="auto" data-snippet-clipboard-copy-content="bin/rails g ruby_ui:component Accordion"><pre>bin/rails g ruby_ui:component Accordion</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Documentation 📖</h2><a id="user-content-documentation-" aria-label="Permalink: Documentation 📖" href="#documentation-"></a></p>
<p dir="auto">Visit <a href="https://rubyui.com/docs/introduction" rel="nofollow">https://rubyui.com/docs/introduction</a> to view the full documentation, including:</p>
<ul dir="auto">
<li>Detailed component guides</li>
<li>Themes</li>
<li>Lookbook</li>
<li>Getting started guide</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Speed Comparison 🏎️</h2><a id="user-content-speed-comparison-️" aria-label="Permalink: Speed Comparison 🏎️" href="#speed-comparison-️"></a></p>
<p dir="auto">RubyUI, powered by Phlex, outperforms alternative methods:</p>
<ul dir="auto">
<li>Phlex: Baseline 🏁</li>
<li>ViewComponent: ~1.5x slower 🚙</li>
<li>ERB Templates: ~5x slower 🐢</li>
</ul>
<p dir="auto">See the original <a href="https://github.com/KonnorRogers/view-layer-benchmarks">view layers benchmark</a> by @KonnorRogers and its <a href="https://github.com/KonnorRogers/view-layer-benchmarks/forks">variations</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Importmap notes:</h2><a id="user-content-importmap-notes" aria-label="Permalink: Importmap notes:" href="#importmap-notes"></a></p>
<p dir="auto">If you run into importmap issues this stackoverflow question might help:
<a href="https://stackoverflow.com/questions/70548841/how-to-add-custom-js-file-to-new-rails-7-project/72855705" rel="nofollow">https://stackoverflow.com/questions/70548841/how-to-add-custom-js-file-to-new-rails-7-project/72855705</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License 📜</h2><a id="user-content-license-" aria-label="Permalink: License 📜" href="#license-"></a></p>
<p dir="auto">Licensed under the <a href="https://github.com/shadcn/ui/blob/main/LICENSE.md">MIT license</a>.</p>
<hr>
<p dir="auto">© 2024 RubyUI. All rights reserved. 🔒</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[We can, must, and will simulate nematode brains (106 pts)]]></title>
            <link>https://asteriskmag.com/issues/09/we-can-must-and-will-simulate-nematode-brains</link>
            <guid>43547813</guid>
            <pubDate>Tue, 01 Apr 2025 15:16:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://asteriskmag.com/issues/09/we-can-must-and-will-simulate-nematode-brains">https://asteriskmag.com/issues/09/we-can-must-and-will-simulate-nematode-brains</a>, See on <a href="https://news.ycombinator.com/item?id=43547813">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	
	<div data-mode="add-marker">
		<p><img id="marker" src="https://asteriskmag.com/assets/img/asterisk_mark.png" title="save highlight"></p><!-- <a href="https://asteriskmag.com/about/#highlights"><img id="help" src="https://asteriskmag.com/assets/img/asterisk_help.png" title="about highlights"></a> -->
		
	</div>

	<section>
				
		 			<h2>
				   
					<span>Michael Skuhersky</span>
							</h2>
			</section>
	
			<section id="rangyscope">
					<p>Scientists have spent over 25 years trying — and failing — to build computer simulations of the smallest brain we know. Today, we finally have the tools to pull it off.</p>
				<div>
											<div><p>A near-perfect simulation of the human brain would have profound implications for humanity. It could offer a pathway for us to transcend the biological limitations that have constrained human potential, and enable unimaginable new forms of intelligence, creativity, and exploration. This represents the next phase in human evolution, freeing our cognition and memory from the limits of our organic structure.</p><p>Unfortunately, it’s also a long way off. The human brain contains on the order of one hundred billion neurons — interconnected by up to a quadrillion synapses. Reverse-engineering this vast network would require computational resources far exceeding what’s currently available. Scientists seeking a proof of concept for whole brain emulation have had to turn to simpler model organisms. And by far the simplest available brain — at just 300 neurons — belongs to the nematode <em>Caenorhabditis elegans</em>.&nbsp;</p><p>Scientists have been working on the problem of simulating <em>C. elegans </em>in some form or another for over 25 years. So far, they’ve been met with little success. But with today’s technology, the task is finally possible, and —&nbsp;as I’ll argue —&nbsp;necessary.&nbsp;</p></div>
											<div><figure>
      <p><img src="https://asteriskmag.com/media/pages/issues/09/we-can-must-and-will-simulate-nematode-brains/6edcf61517-1737579357/skuhersky-04.png" alt="">
  </p>
    <p>
    Motion patterns of <em>C. elegans</em>. Credit: Hiroshima University, Osaka University  </p>
  </figure>
</div>
											<p><h2><strong>A brief history of worm brains</strong></h2>
</p>
											<div><p>The biologist Sydney Brenner became interested in <em>C. elegans </em>as a model organism for developmental biology in the 1970s. Its simplicity and small size made it an ideal lab subject. In 1986, John C. White, a scientist in Brenner’s research group, produced a nearly complete map of the neural connections that make up the <em>C. elegans</em> brain — what scientists now call the connectome. As computers became more accessible, other scientists started building on Brenner’s work. Ernst Neibur and Paul Erdös kicked things off with a <a href="https://www.cell.com/biophysj/fulltext/S0006-3495(91)82149-X">biophysical model</a> of nematode locomotion in 1991. Two different teams (one at <a href="https://web.archive.org/web/20110817114324/http://www.csi.uoregon.edu/projects/celegans/">the University of Oregon</a> and the other in <a href="https://pubmed.ncbi.nlm.nih.gov/9847421/">Japan</a>) published plans for building more ambitious models in the late 1990s. Both would have utilized White’s work on neural circuitry.&nbsp; Unfortunately, neither got off the ground.&nbsp;</p><p>In 2004, the Virtual <em>C. elegans </em>project at Hiroshima University got somewhat farther: they released <a href="http://www.bsys.hiroshima-u.ac.jp/pub/pdf/J/J_152.pdf">two</a> <a href="http://www.bsys.hiroshima-u.ac.jp/pub/pdf/J/J_153.pdf">papers</a> describing their model, which simulated the nematode’s motor control circuits. The simulated nematode could respond to virtual pokes on its head, but it didn’t do much else. And even this was, arguably, not a true simulation. Although the researchers had a map of the nematode’s neurons, they didn’t know their innate biophysical parameter — that is, the precise electrical characteristics of the connections between them. Instead, the researchers used machine learning to produce a set of values for each neuron that made their simulated nematode respond to a poke like a real one would. As a result, this approach was not entirely grounded in biological reality — a recurring theme that would surface in several future simulation attempts.</p><p>That is where things stood at the dawn of the 2010s. While <a href="https://etheses.whiterose.ac.uk/1377/">work</a> <a href="https://www.researchgate.net/publication/228374526_A_Biologically_Accurate_3D_Model_of_the_Locomotion_of_Caenorhabditis_Elegans">continued</a> on simulating nematode locomotion, there was no progress on simulating a nematode’s brain —&nbsp;let alone a realistic one. Then, on January 1st, 2010, the engineer Giovanni Idili tweeted at the official account of the Whole Brain Catalogue, a project to consolidate data from mouse brains: <a href="https://x.com/John_Idol/status/7279117575">“new year's resolution: simulate the whole C.Elegans brain (302 neurons)!”</a> U.C. San Diego neuroscience grad student Stephen Larson noticed the tweet and, by August, Larson was pitching the idea at conferences. By early 2011, Larson and Idili had put together a team to start work on what would become the OpenWorm project —&nbsp;the efforts of a decentralized group of academics with the goal of creating a complete, realistic, and open source model of <em>C. elegans.&nbsp;</em></p><p>This was a heady time to be interested in simulating extremely tiny brains. Over the next few years, OpenWorm published a series of papers and model updates. In 2013, they hosted their first conference in Paris and landed an optimistic story in <em>The Atlantic </em>(title: “<a href="https://www.theatlantic.com/technology/archive/2013/05/is-this-virtual-worm-the-first-sign-of-the-singularity/275715/">Is This Virtual Worm The First Sign of the Singularity?</a>”). Meanwhile, the researcher David Dalrymple was working on a parallel project at MIT, which he dubbed Nemaload. OpenWorm scientists largely used data from dead nematodes but Dalrymple wanted to use the then-new technique of optogenetics to study living specimens. Optogenetics allows scientists to control neurons and other cells with light. In this case, the technique could be used to collect data on how a nematode’s brain responds to different states by perturbing it thousands-upon-thousands of times. In a 2011 <a href="https://www.lesswrong.com/posts/XhHetxjWxZ6b85HK9/whole-brain-emulation-looking-at-progress-on-c-elgans?commentId=wwwhhRufNfuNTSmQy">comment </a>on LessWrong, Dalrymple wrote “I would be Extremely Surprised, for whatever that's worth, if this is still an open problem in 2020.”&nbsp;</p><p>It’s now 2025, and nematode simulation remains an open problem. Dalrymple abandoned Nemaload in 2012. OpenWorm still exists but has not made substantial progress over the past ten years towards creating a truly scientific whole brain simulation, due to a lack of available data<em>. </em>Occasionally, <a href="https://www.frontiersin.org/journals/computational-neuroscience/articles/10.3389/fncom.2019.00008/full">more modern (though still heavily assumption-based) simulations</a> are published, including <a href="https://www.nature.com/articles/s43588-024-00738-w%23MOESM1">integrative models that strive to make fewer assumptions</a>. We’re not quite back where we were in the 2010s: we have much better data on the <em>C. elegans</em> nervous system and — as I’ll discuss later — much better tools to study it. But we aren’t much closer to simulating a whole brain.&nbsp;</p><p>What went wrong? Why has it taken over 25 years to build a working computer simulation of one of the simplest brains known to mankind? And, more importantly, why do I think that this time we can actually pull it off?</p></div>
											<p><h2><strong>Why we got stuck</strong></h2>
</p>
											<div><p>Before explaining what happened, we should ask a more fundamental question: what does it mean to successfully simulate a brain? This is a topic where it’s important to be specific. The term "simulation" in academic neuroscience often evokes the <a href="https://www.scientificamerican.com/article/why-the-human-brain-project-went-wrong-and-how-to-fix-it/">notorious failures</a> of the Human Brain Project. In 2013, neuroscientist Henry Markram secured about 1 billion euros from the European Union to "simulate the human brain" — a proposal widely deemed unrealistic even at the time. The project faced significant challenges and ultimately did not meet its ambitious yet vague goals. These events cast something of a stigma on brain simulation research, making it especially important for those in the field to set clearer, more realistic goals with concrete milestones along the way.&nbsp;</p><p>What makes a good simulation is a debate in itself, so I’ll just share my view: a good simulation of a nervous system is one that both accurately replicates its functionality and reliably predicts the future activity of a real system under the same initial conditions. That is, a simulated nematode in a simulated plate of agar should behave the same way as a real nematode in a real plate of agar. If we disturb the simulation —&nbsp;say, by poking or shining a light on it — it should respond the same way the real nematode would. And it should keep acting like a real nematode over time, instead of accumulating more error as time goes on.&nbsp;</p><p>This definition can help us clarify what is and isn’t simulation. Last October, a consortium of scientists across 127 institutions published the <a href="https://www.nature.com/immersive/d42859-024-00053-4/index.html">complete connectome</a> of the fruit fly, <em>Drosophila melanogaster. </em>This is a massive accomplishment by any objective standard: it is only the second complete connectome assembled, after that of <em>C. elegans</em>, and contains over 140,000 neurons (as compared to <em>C. elegans</em>’s 300). The success of the project, called FlyWire, has rekindled interest in brain simulation. And, in a sense, the FlyWire connectome can be used to simulate a fruit fly. When Philip Shiu, a researcher on the project, test-‘fired’ the neurons responsible for sensing sugar, the model predicted that other neurons that extend the fly’s proboscis would fire, as they would in a real fly. Other researchers have since used Shiu’s model to accurately predict neural patterns involved in the fly’s sense of taste, grooming, and locomotion.&nbsp;</p><p>Shiu’s model represents an important advance in our understanding of fruit fly brains, but it isn’t really a simulation. (Nor is it trying to be;&nbsp;Shiu himself has been <a href="https://news.berkeley.edu/2024/10/02/researchers-simulate-an-entire-fly-brain-on-a-laptop-is-a-human-brain-next/">clear</a> that the model is extremely simplified and makes assumptions about key parameters governing how neurons behave). While the model can successfully predict the behavior of particular groups of neurons, it cannot mimic the exact functionality of an entire fly brain. That’s because the FlyWire model is missing the same thing as OpenWorm (and other attempts to simulate nematodes) did: good data on the relationship between neural structure and neural function.&nbsp;</p><p>Think of the connectome as a map of the brain. It can tell us how neurons connect to each other through electrical and chemical synapses. But despite revealing which neurons connect to one another, it doesn’t tell us anything about how those connections work. To fully model a brain, we need to understand the biophysical parameters governing each neuron’s behavior. This includes not only the variable strength of synapses (in neuroscience, these are called weights) but also the cells’ membrane properties, such as capacitance and the shapes of dendrites and axons, which affect how electrical signals propagate. We need to know both a neuron’s firing threshold as well as how that threshold changes as the animal learns new things (learning involves shifts in both synaptic weights and the intrinsic properties of neurons themselves). A simulation based only on a static connectome can’t learn —&nbsp;so it won’t behave very much like the real creature it’s trying to simulate.&nbsp;</p><p>Unfortunately, learning the dynamic biophysical features of a living brain is much harder than understanding its structure (which, as we’ve seen, is hard enough). The primary technique used to map a connectome is electron microscopy. Because electrons have a wavelength up to one hundred thousand times smaller than that of visible light, they can be used to produce images at a much higher resolution than light microscopes. But electron microscopy has a serious disadvantage. It can only be used on sliced brain tissue, so it can’t tell us how a living brain responds to stimuli or changes over time. The technique can give us extremely detailed, high quality images, but can’t tell us a neuron’s electrical characteristics, like the strength of its synapses or how its membranes store electrical charge.</p><p>For decades, the only way to learn such things was through a technique called patch clamping. The advantage of patch clamping is that it is highly accurate. The disadvantage is that it requires the painstaking placement of electrodes on each individual neuron. With effort, it’s feasible to patch clamp about three neurons at once, making it a less-than-ideal choice for capturing information about neural activity throughout the whole brain. &nbsp;</p><p>This is where things stood when earlier attempts to simulate <em>C. elegans </em>stalled out. It was a problem of timing:&nbsp; In 2013, the tools that would let us understand what happens inside neurons either didn’t exist, or weren’t ready for practical use.</p></div>
											<p><h2><strong>New ways to see</strong></h2>
</p>
											<div><p>As <em>C. elegans </em>simulation research was losing steam, other researchers pushed forward in advancing the ability to observe cells. First, advances in optical microscopy made it possible to capture fast, relatively sharp images of living cells without destroying them. Since the late 1950s, biologists have relied on confocal microscopes, which use a tiny pinhole to block out-of-focus light. This creates higher resolution images, but the method is also slow, since capturing a whole sample means scanning it point-by-point. This is a serious problem for studying traits that change rapidly (like neuronal activity). This is where modern techniques like light sheet microscopy prove particularly useful. Instead of focusing light through a point, light sheet microscopes use a laser sheet to illuminate an entire 2D cross-section of a sample.The process is dramatically faster and gentler on tissue than traditional confocal methods.&nbsp;</p><p>Light sheet microscopes have existed since the 1990s, but early versions of the technology struggled to capture fast intracellular processes. That changed with a series of innovations in the early 2010s. First, new techniques were developed to allow optical microscopy below the diffraction limit (the smallest distance between two points at which they can still be distinguished by an optical system). For visible light, this distance is between 200 and 250 nanometers — too big to distinguish most cellular features. That changed with the introduction of <a href="https://pubmed.ncbi.nlm.nih.gov/20643879/">super-resolution microscopy</a> which featured resolutions of 100 nanometers and below. Another major advance was DiSPIM,<sup>
    <!-- <a id="fnref-1" href="#fn-1"> -->
    <span id="fnref-1">
        1    </span>
    <!-- </a> -->
</sup>
 invented in 2014. In light sheet microscopes, the light illuminating an image has to be perpendicular to the camera picking it up. Originally, this meant that the camera and the light sheet were part of separate assemblies. DiSPIM microscopes use two perpendicular lens assemblies, each equipped with a light source and a camera. This approach doubled the speed with which the microscope could capture images of living samples, and ensured that images could be reconstructed at the same resolution across all three dimensions. In 2015, a group at Columbia University developed a method called SCAPE,<sup>
    <!-- <a id="fnref-2" href="#fn-2"> -->
    <span id="fnref-2">
        2    </span>
    <!-- </a> -->
</sup>
 which used an oblique sheet of light to scan and image a sample using a single lens assembly. SCAPE is even faster than earlier light sheet techniques, making it particularly useful for tracking rapid neuronal activity. &nbsp;</p><p>Another set of innovations has to do with what the microscopes are looking at. All the methods we’ve discussed depend on fluorescent reporters — engineered proteins that fluoresce under certain conditions, such as the presence of a specific protein or the expression of a particular gene. In our case, that trigger is calcium. When a neuron fires, calcium ions flood into the cell, making calcium influx a reliable proxy for neuronal activity. The key breakthrough here was the development of the GCaMP6 family of reporters by a team at the Janelia Research Campus between 2013 and 2015. This new generation of calcium indicators were brighter and more sensitive than earlier versions, quickly becoming the go-to tool for imaging neuronal circuits in living organisms. While GCaMP6 revolutionized calcium-based imaging, even more precise measurements could come from fluorescent reporters that respond to voltage directly. These already exist for larger organisms and are actively being developed for use in <em>C. elegans</em>.</p><p>Today, the combination of calcium imaging and microscopy techniques like DiSPIM and SCAPE means that we can see how neurons behave throughout the entire <em>C. elegans </em>brain — in real time. The next challenge is to actually do it. And to do it a lot. Our understanding of the <em>C. elegans </em>connectome has improved significantly since White’s groundbreaking work in 1986. White’s connectome was a mosaic of five individual worms. However, the same neuron in different animals might differ in size or capacity for electric charge. To fully understand the <em>C. elegans </em>brain and its operation during a broad range of behaviors, we need to collect data from thousands of individuals.&nbsp;</p><p>There’s the question of what to do with the data once we have it. This is another area where recent advances –&nbsp;this time, in machine learning — make the process much more feasible. For all its biological complexity, the <em>C. elegans </em>brain still consists of just 300 neurons — tiny compared to state-of-the-art large language models. Using <a href="https://arxiv.org/abs/2006.10782">symbolic regression</a>, a machine learning technique for discovering mathematical formulas that explain observed data, we can take our data on neuronal activity and use it to derive key parameters like capacitance and synaptic strength for every single neuron and every single neuronal connection. These equations would likely resemble the biophysical models that scientists have already derived from patch-clamp experiments, but inferred directly from whole-brain data.</p></div>
											<p><h2><strong>Fish, flies, and beyond</strong></h2>
</p>
											<div><p>I don’t mean to suggest that building an accurate <em>C. elegans </em>simulation will be easy. There are many considerations that the technologies I’ve described may not account for, from extra-synaptic signalling to the role of specific neuron morphology (not to mention the fact that neurons and synapses change over the course of a nematode’s life). But with modern techniques, which continue to rapidly improve, I do believe that it is possible.&nbsp;</p><p>And if we want to one day build simulations of larger animals — including humans — I also believe that it is necessary. The optical microscopy techniques that let us observe the neural activity of living organisms have one key limitation: depth. Light can only penetrate so far into tissue. With current techniques, that limit is roughly 750 microns, a bit less than a millimeter. To build an accurate whole brain simulation, we need activity data from a whole brain — which means that we’re currently limited to brains less than a millimeter deep. In other words, <em>C. elegans</em>, larval zebrafish, and fly brains are our only options. By investigating small organisms, we can develop new methods that allow us to predict neural activity by looking at the brain’s structure and other indirect forms of data. These techniques will make it possible for us to model more complex brains, including those that are too large for us to image their activity directly.&nbsp;</p><p>My research focuses on creating a scientifically-grounded simulation of <em>C. elegans</em> by integrating these recently developed microscopy, fluorescent reporter, and machine learning methods into a cohesive pipeline and methodological framework. The idea is to create a proven simulation creation blueprint that can then be applied to more complex brains. But achieving a successful simulation of <em>C. elegans</em> would be a remarkable scientific accomplishment on its own. More importantly, it would help us begin to decipher how the structure of a brain relates to the dynamic processes unfolding within it. Over time, this understanding will open the doors to simulating more complex organisms, ultimately including humans. We have a long journey ahead of us, but now is the best time to begin — expeditiously, and with tractable, well-defined milestones along the way.</p></div>
										 
				</div>
		
	</section>
	 	<section>
		 		 <p><strong>Michael Skuhersky</strong> holds a PhD in neuroscience from MIT and is currently founding a nonprofit research institute focused on brain simulation.</p>		 		 		 </section>
	 	<section>            
		<p>
			Published March 2025		</p>
		
		<p>Have something to say? Email us at <a href="mailto:letters@asteriskmag.com">letters@asteriskmag.com</a>.</p>		                        
	</section>	
	
	
	<!--end published content, not coming soon-->

	<!--tags-->
		<section>
		<h4>Further Reading</h4>                
			<p>
				More:  
									<span data-no="tag-1">science</span>
									<span data-no="tag-2">technology</span>
							</p>
			<!--related articles-->
			             
	</section>
	 
	
	  
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Systems Correctness Practices at AWS: Leveraging Formal and Semi-Formal Methods (145 pts)]]></title>
            <link>https://queue.acm.org/detail.cfm?id=3712057</link>
            <guid>43547593</guid>
            <pubDate>Tue, 01 Apr 2025 14:59:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://queue.acm.org/detail.cfm?id=3712057">https://queue.acm.org/detail.cfm?id=3712057</a>, See on <a href="https://news.ycombinator.com/item?id=43547593">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	<div>
		
		<p><a href="https://queue.acm.org/"><img src="https://queue.acm.org/img/acmqueue_logo.gif"></a>

	</p></div>

<!--
<p style='text-align:center;'>
<a href='/app/' target='_new'><img src='/app/2021_03-04_lrg.png' with=90 height=120 style='float:right;width:90px;height:120px;' alt='March/April 2021 issue of acmqueue' /></a>
<b><a href='/app/'>The March/April 2021 issue of acmqueue is out now</a></b>
<br />
<br />
<a href='https://cdn.coverstand.com/3rd_pty/acm/login.html?&btx_i=705849'>Subscribers and ACM Professional members login here</a>
<br clear=all />
<hr style='display:block;color:red;margin:5px;' />
</p>
-->




<p><label>February 4, 2025<br><b><a href="https://queue.acm.org/issuedetail.cfm?issue=3714453">Volume 22, issue 6 </a></b></label></p><p>
<!-- // Check for existence of associated MP3 file-->

 &nbsp;
	
			<a href="https://portal.acm.org/citation.cfm?id=3712057">
				<img src="https://queue.acm.org/img/icon_pdf.png" alt="Download PDF version of this article">
				PDF
			</a>
		
</p>


 
   
  <h2>Leveraging Formal and Semi-formal Methods</h2> 
  <h3>Marc Brooker and Ankush Desai</h3> 
  <p>AWS (Amazon Web Services) strives to deliver reliable services that customers can trust completely. This demands maintaining the highest standards of security, durability, integrity, and availability—with systems correctness serving as the cornerstone for achieving these priorities. An April 2015 paper published in <i>Communications of the ACM</i>, titled "How Amazon Web Services Uses Formal Methods," highlighted the approach for ensuring the correctness of critical services that have since become among the most widely used by AWS customers.<sup>21</sup> </p> 
  <p>Central to this approach was TLA+,<sup>14</sup> a formal specification language developed by Leslie Lamport. Our experience at AWS with TLA+ revealed two significant advantages of applying formal methods in practice. First, we could identify and eliminate subtle bugs early in development—bugs that would have eluded traditional approaches like testing. Second, we gained the deep understanding and confidence needed to implement aggressive performance optimizations while maintaining systems correctness. </p> 
  <p>Moreover, 15 years ago, AWS's software testing practice relied primarily on build-time unit testing, often against mocks, and limited deployment-time integration testing. Since then, we have significantly evolved our correctness practices, integrating both formal and semi-formal approaches into the development process. As AWS has grown, formal methods have become increasingly valuable—not only for ensuring correctness but also for performance improvements, particularly in verifying the correctness of both low- and high-level optimizations. This systematic approach toward systems correctness has become a force multiplier at AWS's scale, enabling faster development cycles through improved developer velocity while delivering more cost-effective services to customers.</p> 
  <p>This article surveys the portfolio of formal methods used across AWS to deliver complex services with high confidence in its correctness. We consider an umbrella definition of formal methods that encompasses these rigorous techniques—from traditional formal approaches such as theorem proving,<sup>7,10</sup> deductive verification,<sup>18</sup> and model checking<sup>8,14</sup> to more lightweight semi-formal approaches such as property-based testing,<sup>6,19</sup> fuzzing,<sup>9</sup> and runtime monitoring.<sup>11</sup></p> 
   
  <h3>The P Programming Language</h3> 
  <p>As the use of formal methods was expanded beyond the initial teams at AWS in the early 2010s, we discovered that many engineers struggled to learn and become productive with TLA+. This difficulty seemed to stem from TLA+'s defining feature: It is a high-level, abstract language that more closely resembles mathematics than the imperative programming languages most developers are familiar with. While this mathematical nature is a significant strength of TLA+, and we continue to agree with Lamport's views on the benefits of mathematical thinking,<sup>15</sup> we also sought a language that would allow us to model check (and later prove) key aspects of systems designs while being more approachable to programmers.</p> 
  <p>We found this balance in the P programming language.<sup>8</sup> P is a state-machine-based language for modeling and analysis of distributed systems. Using P, developers model their system designs as communicating state machines, a mental model familiar to Amazon's developer population, most of whom develop systems based on microservices and SOAs (service-oriented architectures). P has been developed at AWS since 2019 and is maintained as a strategic open-source project.<sup>22</sup> Teams across AWS that build some of its flagship products—from storage (e.g., Amazon S3, EBS), to databases (e.g., Amazon DynamoDB, MemoryDB, Aurora), to compute (e.g., EC2, IoT)—have been using P to reason about the correctness of their system designs.</p> 
  <p>For example, P was used in migrating S3 (Simple Storage Service) from eventual to strong read-after-write consistency.<sup>1</sup> A key component of S3 is its index subsystem, an object metadata store that enables fast data lookups. To achieve strong consistency, the S3 team had to make several nontrivial changes to the S3 index protocol stack.<sup>25</sup> Because these changes were difficult to get right at S3 scale, and the team wanted to deliver strong consistency with high confidence in correctness, they used P to formally model and validate the protocol design. P helped eliminate several design-level bugs early in the development process and allowed the team to deliver risky optimizations with confidence, as they could be validated using model checking.</p> 
  <p>In 2023, the P team at AWS built PObserve, which provides a new tool for validating the correctness of distributed systems both during testing and in production. With PObserve, we take structured logs from the execution of distributed systems and validate post-hoc that they match behaviors allowed by the formal P specification of the system. This allows for bridging the gap between the P specification of the system design and the production implementation (typically in languages like Rust or Java). While there are significant benefits from verifying protocols at design time, runtime monitoring of the same properties for the implementation makes the investment in formal specification much more valuable and addresses classic concerns with the deployment of formal methods in practice (i.e., connecting design-time validation with system implementation).</p> 
   
  <h3>Lightweight Formal Methods</h3> 
  <p>Another way that AWS has brought formal methods closer to its engineering teams is through the adoption of <i>lightweight formal methods</i>.</p> 
   
  <h4>Property-based testing</h4> 
  <p>The most notable single example of leveraging light-weight formal method is in Amazon S3's ShardStore, where the team used property-based testing throughout the development cycle both to test correctness and to speed up the development (described in detail by Bornholt, et al.<sup>4</sup>). The key idea in their approach was combining property-based testing with developer-provided correctness specifications, coverage-guided fuzzing (an approach where the distribution of inputs is guided by code coverage metrics), failure injection (where hardware and other system failures are simulated during testing), and minimization (where counterexamples are automatically reduced to aid human-guided debugging).</p> 
   
  <h4>Deterministic simulation </h4> 
  <p>Another lightweight method widely used at AWS is deterministic simulation testing, in which a distributed system is executed on a single-threaded simulator with control over all sources of randomness such as thread scheduling, timing, and message delivery order. Tests are then written for particular failure or success scenarios, such as the failure of a participant at a particular stage in a distributed protocol. The nondeterminism in the system is controlled by the test framework, allowing developers to specify orderings that they believe are interesting (such as ones that have caused bugs in the past). The scheduler in the testing framework can also be extended for fuzzing of orderings or exploring all possible orderings to be tested. </p> 
  <p>Deterministic simulation testing moves testing of system properties, like behavior under delay and failure, closer to build time instead of integration testing. This accelerates development and provides for more complete behavioral coverage during testing. Some of the work done at AWS on build-time testing of thread ordering and systems failures has been open-sourced as part of the shuttle (<a href="https://github.com/awslabs/shuttle">https://github.com/awslabs/shuttle</a>) and turmoil (<a href="https://github.com/tokio-rs/turmoil">https://github.com/tokio-rs/turmoil</a>) projects.</p> 
   
  <h4>Continuous fuzzing or random test-input generation</h4> 
  <p>Continuous fuzzing, especially coverage-guided scalable test-input generation, is also effective for testing systems correctness at integration time. During the development of Amazon Aurora's data-sharding feature (Aurora Limitless Database<sup>3),</sup> for example, we made extensive use of fuzzing to test two key properties of the system. First, by fuzzing SQL queries (and entire transactions), we validated that the logic partitioning SQL execution over shards is correct. Large volumes of random SQL schemas, datasets, and queries are synthesized and run through the engines under test, and the results compared with an oracle based on the nonsharded version of the engine (as well as other approaches to validation, like those pioneered by SQLancer<sup>23</sup>). </p> 
  <p>Fuzzing, combined with fault injection testing, is also useful for testing other aspects of database correctness such as atomicity, consistency, and isolation. In database testing, transactions are automatically generated, their correct behavior is defined using a formally specified correctness oracle, and then all possible interleaving of transactions and statements within the transaction are executed against the system under test. We also use post-hoc validation of properties like isolation (following approaches such as Elle<sup>13).</sup></p> 
   
  <h3>Fault Injection as a Service</h3> 
  <p>In early 2021 AWS launched FIS (Fault Injection Service)<sup>2</sup> with the goal of making testing based on fault injection accessible to a wide range of AWS customers. FIS allows customers to inject simulated faults, from API errors to I/O pauses and failed instances, into test or production deployments of their infrastructure on AWS. Injecting faults allows customers to validate that the resiliency mechanisms they have built into their architectures (such as failovers and health checks) actually work to improve availability and do not introduce correctness problems. Fault-injection testing based on FIS is widely used by AWS customers, and internally within Amazon. For example, Amazon.com ran 733 FIS-based fault-injection experiments in preparation for Prime Day 2024.</p> 
  <p>In 2014, Yuan, et al. found that 92 percent of catastrophic failures in tested distributed systems were triggered by incorrect handling of nonfatal errors. Many distributed-systems practitioners who were told about this research were surprised the percentage wasn't higher. Happy-case catastrophic failures are rare simply because the happy case of systems is executed often, tested better (both implicitly and explicitly), and is significantly simpler than the error cases. Fault-injection testing and FIS make it much easier for practitioners to test the behavior of their systems under faults and failures, closing the gap between happy-case and error-case bug density.</p> 
  <p>While fault injection is not considered a formal method, it can be combined with formal specifications. Defining the expected behavior using a formal specification, and then comparing results during and after fault injection to the specified behavior, allows for catching a lot more bugs than simply checking for errors in metrics and logs (or having a human look and say, "Yup, that looks about right").</p> 
   
  <h3>Metastability and Emergent System Behavior</h3> 
  <p>Over the past decade, there has been an emerging interest in a particular class of systems failure: those where some triggering event (like an overload or a cache emptying) causes a distributed system to enter a state where it doesn't recover without intervention (such as reducing load below normal). This class of failures, dubbed <i>metastable failures</i>,<sup>5</sup> is one of the most important contributors to unavailability in cloud systems. Figure 1 (adapted from Bronson, et al.<sup>5</sup>) illustrates a common type of metastable behavior: Load increases on the system are initially met with increasing goodput, followed by saturation, followed by congestion and goodput dropping to zero (or near zero). From there, the system cannot return to healthy state by slightly reducing load. Instead, it must follow the dotted line and may not recover until load is significantly reduced. This type of behavior is present in even simple systems. For example, it can be triggered in most systems with timeout-and-retry client logic.</p> 
  <p><img src="https://dl.acm.org/cms/attachment/html/10.1145/3712057/assets/html/brooker1.png" alt="Systems Correctness Practices at AWS"></p> 
  <p>Traditional formal approaches to modeling distributed systems typically focus on <i>safety</i> (nothing bad happens) and <i>liveness</i> (something good eventually happens), but metastable failures remind us that systems have a variety of behaviors that cannot be neatly categorized this way. We have increasingly turned to discrete-event simulation to understand the emergent behavior of systems, investing both in custom-built systems simulations and tooling that allow the use of existing system models (built in languages like TLA+ and P) to simulate system behavior. Extending exhaustive model checkers, like TLA+'s TLC with probabilistic simulations, also allows for the generation of statistical results such as posterior latency distributions, making model checking useful for tasks like understanding the achievability of latency SLAs (service-level agreements).</p> 
   
  <h3>Formal Proof</h3> 
  <p>In some cases, the formal methods enumerated so far in this article are not sufficient. For critical security boundaries such as authorization and virtualization, for example, proofs of correctness can be both desirable and worth the significant investment needed to create them.</p> 
  <p>In 2023, AWS introduced the Cedar authorization policy language for writing policies that specify fine-grained permissions. Cedar was designed for automated reasoning and formal proof.<sup>12,24</sup> The language was designed to be well-suited for proof, and the implementation was built in the verification-aware programming language Dafny. Using Dafny, the team was able to prove that the implementation satisfies a variety of security properties. This type of proof goes beyond testing. It is a proof in the mathematical sense. The team also applied a differential testing approach using the Dafny code as a correctness oracle to verify the correctness of the production-ready Rust implementation. Publishing the Dafny code and test procedures as open source, along with the Cedar implementation, allows Cedar users to check the team's work on correctness.</p> 
  <p>Another example is the Firecracker VMM (virtual machine monitor). Firecracker uses a low-level protocol called <i>virtio</i> to expose emulated hardware devices (such as a network card or solid-state drive) to guest kernels running inside the VM. This emulated device is a critical security boundary because it is the most complex interaction between the untrusted guest and trusted host. The Firecracker team used a tool called Kani<sup>20</sup> that is able to reason formally about Rust code to prove key properties of this security boundary. Again, proof here goes beyond testing and ensures that the critical properties of this boundary are held no matter what the guest attempts to do.</p> 
  <p>Proofs around the behaviors of programs are an important part of AWS's software correctness program, and so we support development on tools such as Kani, Dafny,<sup>18</sup> and Lean,<sup>16</sup> and the underlying tools—like SMT (satisfiability modulo theories) solvers—that power them.</p> 
  <p>The ability to use formal models and specifications—for model-checking systems at design time, for validating in-production behavior using runtime monitoring by serving as a correctness oracle, for simulating emergent systems behavior, and for building proofs of critical properties—allows AWS to amortize the engineering effort of developing these specifications over a larger amount of business and customer value.</p> 
   
  <h3>Benefits Beyond Correctness</h3> 
  <p>Finally, as discussed in the aforementioned 2015 paper, formal methods are a crucial part of safely improving the performance of cloud systems. Modeling a key commit protocol for the Aurora relational database engine in P and TLA+ allowed us to identify an opportunity to reduce the cost of distributed commits from two to 1.5 network roundtrips without sacrificing any safety properties. These kinds of stories are usual for teams that adopt formal methods, driven by at least two different dynamics. </p> 
  <p>First, the act of deeply thinking about and formally writing down distributed protocols forces a structured way of thinking that leads to deeper insights about the structure of protocols and the problem to be solved. </p> 
  <p>Second, having the ability to formally check (and, in some cases, prove) that proposed design optimizations are correct allows naturally conservative distributed-systems engineers to be bolder in their protocol design choices without increasing risk and boosting the developer velocity toward delivering reliable services.</p> 
  <p>These productivity and cost benefits are limited not only to high-level design optimizations but also to low-level code that normally gets ignored. In one example, the AWS team identified optimizations to the implementation of the RSA (Rivest-Shamir-Adleman) public-key encryption scheme on our ARM-based Graviton 2 processor, which could improve throughput by up to 94 percent.<sup>17</sup> </p> 
  <p>Using the HOL Light interactive theorem prover, the team was able to prove the correctness of these optimizations. Given the high percentage of cloud CPU cycles spent on cryptography, this type of optimization can significantly reduce infrastructure costs and aid sustainability while at the same time improving customer-visible performance.</p> 
   
  <h3>Challenges and Opportunities for the Future</h3> 
  <p>Despite significant success in scaling formal and semi-formal testing methods across AWS over the past 15 years, several challenges persist, particularly in industrial adoption of formal methods. The primary barriers for formal methods tools include their steep learning curve and the specialized domain expertise required. Additionally, many of these tools remain academic in nature and lack user-friendly interfaces.</p> 
  <p>Even well-established semi-formal approaches face adoption challenges. For example, deterministic simulation, a key distributed-systems testing technique used successfully at AWS and in projects like FoundationDB, remains unfamiliar to many experienced distributed-systems developers joining AWS. Similar gaps exist in the adoption of other proven methodologies such as fault-injection testing, property-based testing, and fuzzing. The challenge is educating distributed-systems developers about these testing methods and tools, teaching the art of rigorous thinking.</p> 
  <p>The education gap begins at the academic level, where even basic formal reasoning approaches are rarely taught, making it difficult for graduates from top institutions to adopt these tools. Although formal methods and automated reasoning are crucial for industry applications, they continue to be viewed as niche fields. We anticipate that increased industry adoption of formal methods and automated reasoning will attract more talent to this domain.</p> 
  <p>Metastability and other emergent properties of large-scale systems represent another critical research area facing similar awareness challenges. Common practices that lead to metastable system behavior, such as "retry N times on timeout," continue to be widely recommended despite their known issues. Current tools and techniques for understanding emergent system behavior are still in their early stages, making system stability modeling both expensive and complex. The ongoing research in this area holds promising potential for advancement.</p> 
  <p>Looking ahead, we believe large language models and AI assistants will significantly help address the adoption challenges of formal methods in practice. Just as AI-assisted unit testing has gained popularity, these tools are expected soon to help developers create formal models and specifications, making these advanced techniques more accessible to the broader developer community.</p> 
   
  <h3>Conclusion</h3> 
  <p>Building reliable and secure software requires a range of approaches to reason about systems correctness. Alongside industry-standard testing methods (such as unit and integration testing), AWS has adopted model checking, fuzzing, property-based testing, fault-injection testing, deterministic simulation, event-based simulation, and runtime validation of execution traces. Formal methods have been an important part of the development process—perhaps most importantly, formal specifications as test oracles that provide the correct answers for many of AWS's testing practices. Correctness testing and formal methods remain key areas of investment at AWS, accelerated by the excellent returns seen on investments in these areas already.</p> 
   
  <h4>References</h4> 
  <p>1. Amazon Web Services. 2020. Amazon S3 now delivers strong read-after-write consistency automatically for all applications; <a href="https://aws.amazon.com/about-aws/whats-new/2020/12/amazon-s3-now-delivers-strong-read-after-write-consistency-automatically-for-all-applications">https://aws.amazon.com/about-aws/whats-new/2020/12/amazon-s3-now-delivers-strong-read-after-write-consistency-automatically-for-all-applications</a>. (Announces automatic strong read-after-write consistency for all S3 operations across all AWS regions.)</p> 
  <p>2. Amazon Web Services. 2021. Announcing general availability of AWS Fault Injection Simulator, a fully managed service to run controlled experiments; <a href="https://aws.amazon.com/about-aws/whats-new/2021/03/aws-announces-service-aws-fault-injection-simulator/">https://aws.amazon.com/about-aws/whats-new/2021/03/aws-announces-service-aws-fault-injection-simulator/</a>.</p> 
  <p>3. Amazon Web Services. 2023. Announcing Amazon Aurora Limitless Database; <a href="https://aws.amazon.com/about-aws/whats-new/2023/11/amazon-aurora-limitless-database/">https://aws.amazon.com/about-aws/whats-new/2023/11/amazon-aurora-limitless-database/</a>.</p> 
  <p>4. Bornholt, J., Joshi, R., Astrauskas, V., Cully, B., Kragl, B., Markle, S., Sauri, K., Schleit, D., Slatton, G., Tasiran, S., Van Geffen, J., Warfield, A. 2021. Using lightweight formal methods to validate a key-value storage node in Amazon S3. In <i>Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles</i>, 836–850; <a href="https://dl.acm.org/doi/10.1145/3477132.3483540">https://dl.acm.org/doi/10.1145/3477132.3483540</a>.</p> 
  <p>5. Bronson, N., Aghayev, A., Charapko, A., Zhu, T. 2021. Metastable failures in distributed systems. In <i>Proceedings of the Workshop on Hot Topics in Operating Systems</i>, 221–227; <a href="https://dl.acm.org/doi/10.1145/3458336.3465286">https://dl.acm.org/doi/10.1145/3458336.3465286</a>.</p> 
  <p>6. Claessen, K., Hughes, J. 2000. QuickCheck: A lightweight tool for random testing of Haskell programs. In <i>Proceedings of the Fifth ACM SIGPLAN International Conference on Functional Programming</i>, 268–279; <a href="https://dl.acm.org/doi/10.1145/351240.351266">https://dl.acm.org/doi/10.1145/351240.351266</a>. </p> 
  <p>7. de Moura, L., Ullrich, S. 2021. The Lean 4 theorem prover and programming language. In <i>Automated Deduction – CADE 28</i> (28th International Conference on Automated Deduction, volume 12699 of <i>Lecture Notes in Computer Science</i>, 625–635. Springer; <a href="https://dl.acm.org/doi/10.1007/978-3-030-79876-5_37">https://dl.acm.org/doi/10.1007/978-3-030-79876-5_37</a>.</p> 
  <p>8. Desai, A., Gupta, V., Jackson, E., Qadeer, S., Rajamani, S., Zufferey, D. 2013. P: safe asynchronous event-driven programming. <i>ACM SIGPLAN Notices</i> 48(6), 321–332; <a href="https://dl.acm.org/doi/10.1145/2499370.2462184">https://dl.acm.org/doi/10.1145/2499370.2462184</a>.</p> 
  <p>9. Fioraldi, A., Maier, D., Eißfeldt, H., Heuse, M. 2020. AFL++: Combining incremental steps of fuzzing research. In <i>14th Usenix Workshop on Offensive Technologies</i>; <a href="https://www.usenix.org/conference/woot20/presentation/fioraldi">https://www.usenix.org/conference/woot20/presentation/fioraldi</a>.</p> 
  <p>10. Harrison, J. 2009. HOL Light: an overview. In <i>Proceedings of the 22nd International Conference on Theorem Proving in Higher Order Logics</i>, Munich, Germany, ed., S. Berghofer, T. Nipkow, C. Urban, and M. Wenzel, 60–66, volume 5674 of <i>Lecture Notes in Computer Science</i>. Springer-Verlag; <a href="https://link.springer.com/chapter/10.1007/978-3-642-03359-9_4">https://link.springer.com/chapter/10.1007/978-3-642-03359-9_4</a>.</p> 
  <p>11. Havelund, K., Rosu, G. 2019. Runtime verification – 17 years later. In <i>Runtime Verification</i>, 18th International Conference, RV 2018, Limassol, Cyprus, ed., C. Colombo and M. Leucker, 3–17, volume 11237 of <i>Lecture Notes in Computer Science</i>. Springer; <a href="https://link.springer.com/book/10.1007/978-3-030-03769-7">https://link.springer.com/book/10.1007/978-3-030-03769-7</a>.</p> 
  <p>12. Hicks, M. 2023. How we built Cedar with automated reasoning and differential testing. Amazon Science; <a href="https://www.amazon.science/blog/how-we-built-cedar-with-automated-reasoning-and-differential-testing">https://www.amazon.science/blog/how-we-built-cedar-with-automated-reasoning-and-differential-testing</a>.</p> 
  <p>13. Kingsbury, K., Alvaro, P. 2020. Elle: inferring isolation anomalies from experimental observations. <i>Proceedings of the VLDB Endowment</i> 14(3), 268–280; <a href="https://dl.acm.org/doi/10.14778/3430915.3430918">https://dl.acm.org/doi/10.14778/3430915.3430918</a>.</p> 
  <p>14. Lamport, L. 2002. <i>Specifying Systems: The TLA+ Language and Tools for Hardware and Software Engineers</i>. Addison-Wesley Professional.</p> 
  <p>15. Lamport, L. 2015. Who builds a house without drawing blueprints? <i>Communications of the ACM</i> 58(4), 38–41; <a href="https://dl.acm.org/doi/10.1145/2736348">https://dl.acm.org/doi/10.1145/2736348</a>.</p> 
  <p>16. Lean Prover Community. 2024. Lean 4. GitHub; <a href="https://github.com/leanprover/lean4">https://github.com/leanprover/lean4</a>.</p> 
  <p>17. Lee, J., Becker, H., Harrison, J. 2024. Formal verification makes RSA faster—and faster to deploy. Amazon Science; <a href="https://www.amazon.science/blog/formal-verification-makes-rsa-faster-and-faster-to-deploy">https://www.amazon.science/blog/formal-verification-makes-rsa-faster-and-faster-to-deploy</a>.</p> 
  <p>18. Leino, K. R. M. 2010. Dafny: an automatic program verifier for functional correctness. In <i>Proceedings of the 16th International Conference on Logic for Programming, Artificial Intelligence, and Reasoning</i>, 348–370. Springer-Verlag; <a href="https://dl.acm.org/doi/10.5555/1939141.1939161">https://dl.acm.org/doi/10.5555/1939141.1939161</a>.</p> 
  <p>19. MacIver, D. R., Hatfield-Dodds, Z., et al. 2019. Hypothesis: a new approach to property-based testing. <i>Journal of Open Source Software</i> 4(43), 1891; <a href="https://joss.theoj.org/papers/10.21105/joss.01891.pdf">https://joss.theoj.org/papers/10.21105/joss.01891.pdf</a>.</p> 
  <p>20. Monteiro, F., Roy, P. 2023. Using Kani to validate security boundaries in AWS Firecracker. Technical report, Amazon Web Services; <a href="https://model-checking.github.io/kani-verifier-blog/2023/08/31/using-kani-to-validate-security-boundaries-in-aws-firecracker.html">https://model-checking.github.io/kani-verifier-blog/2023/08/31/using-kani-to-validate-security-boundaries-in-aws-firecracker.html</a>.</p> 
  <p>21. Newcombe, C., Rath, T., Zhang, F., Munteanu, B., Brooker, M., Deardeuff, M. 2015. How Amazon Web Services uses formal methods. <i>Communications of the ACM </i>58(4), 66–73. <a href="https://dl.acm.org/doi/10.1145/2699417">https://dl.acm.org/doi/10.1145/2699417</a>.</p> 
  <p>22. P Team. 2024. P: a programming language for formal specification of distributed systems. <a href="https://github.com/p-org/P">https://github.com/p-org/P</a>. (Used extensively in AWS and Microsoft for formal verification of distributed systems.)</p> 
  <p>23. Rigger, M., Su, Z. 2020. Testing database engines via pivoted query synthesis. In 14th Usenix Symposium on Operating Systems Design and Implementation, Article 38, 667–682; <a href="https://dl.acm.org/doi/10.5555/3488766.3488804">https://dl.acm.org/doi/10.5555/3488766.3488804</a>. </p> 
  <p>24. Rungta, N. 2024. Trillions of formally verified authorizations a day! Splash keynote; <a href="https://2024.splashcon.org/details/splash-2024-keynotes/3/Trillions-of-Formally-Verified-Authorizations-a-day-">https://2024.splashcon.org/details/splash-2024-keynotes/3/Trillions-of-Formally-Verified-Authorizations-a-day-</a>.</p> 
  <p>25. Vogels, W. 2021. Diving deep on S3 consistency. All Things Distributed; <a href="https://www.allthingsdistributed.com/2021/04/s3-strong-consistency.html">https://www.allthingsdistributed.com/2021/04/s3-strong-consistency.html</a>. (Blog post describing the implementation of strong consistency in Amazon S3.)</p> 
   
  <p><b>Marc&nbsp;Brooker&nbsp;</b>is a Distinguished Engineer at Amazon Web Services, where he focusses on AI and databases. He's interested in distributed systems, serverless, systems correctness, and formal methods. Marc holds a Ph.D. in Electrical Engineering from the University of Cape Town.</p> 
  <p><b>Ankush Desai </b>is a Principal Applied Scientist at Amazon Web Services, where he focusses on building tools and techniques that help developers deliver distributed services with high assurance of correctness. He's interested in improving developer productivity, formal methods, systematic testing, fuzzing, and distributed systems. Ankush holds a PhD in Computer Science from the University of California, Berkeley.</p> 
  <p>Copyright © 2024 held by owner/author. Publication rights licensed to ACM.</p>  
  

	<div>
	
		<p><img src="https://queue.acm.org/img/q%20stamp_small.jpg" width="26" height="45" alt="acmqueue"></p><p>
	
	<em>Originally published in Queue vol. 22, no. 6</em>—
 	<br>
	Comment on this article in the <a href="http://portal.acm.org/citation.cfm?id=3712057">ACM Digital Library</a></p></div>
	



<br>
<!--
<a href="https://twitter.com/share" class="twitter-share-button" data-via="ACMQueue">Tweet</a>
-->


<br>

<!--
<fb:like></fb:like>
-->

<br>



<!-- these get hooked up to js events -->


<!-- FB Like -->
<!--
<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) return;
  js = d.createElement(s); js.id = id;
  js.src = "connect.facebook.net/en_US/all.js#xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>

<div id="fb-root"></div>
-->

<!-- Place this tag after the last +1 button tag. -->

<!--
<script type="text/javascript">
  (function() {
    var po = document.createElement('script'); po.type = 'text/javascript'; po.async = true;
    po.src = 'https://apis.google.com/js/plusone.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  })();
</script>

<br />
<script src="https://connect.facebook.net/en_US/all.js#xfbml=1"></script>

<script>
FB.Event.subscribe('edge.create', function(targetUrl) {
  _gaq.push(['_trackSocial', 'facebook', 'like', targetUrl]);
});
</script>
-->



<hr noshade="" size="1"><p>




More related articles:

	  </p><p>
	  <span>Achilles Benetopoulos</span> - <a href="https://queue.acm.org/detail.cfm?id=3712258"><b>Intermediate Representations for the Datacenter Computer</b></a>
	  <br>
	  We have reached a point where distributed computing is ubiquitous. In-memory application data size is outstripping the capacity of individual machines, necessitating its partitioning over clusters of them; online services have high availability requirements, which can be met only by deploying systems as collections of multiple redundant components; high durability requirements can be satisfied only through data replication, sometimes across vast geographical distances.
	  </p>
	  

	  <p>
	  <span>David R. Morrison</span> - <a href="https://queue.acm.org/detail.cfm?id=3711677"><b>Simulation: An Underutilized Tool in Distributed Systems</b></a>
	  <br>
	  Simulation has a huge role to play in the advent of AI systems: We need an efficient, fast, and cost-effective way to train AI agents to operate in our infrastructure, and simulation absolutely provides that capability.
	  </p>
	  

	  <p>
	  <span>Matt Fata, Philippe-Joseph Arida, Patrick Hahn, Betsy Beyer</span> - <a href="https://queue.acm.org/detail.cfm?id=3264508"><b>Corp to Cloud: Google’s Virtual Desktops</b></a>
	  <br>
	  Over one-fourth of Googlers use internal, data-center-hosted virtual desktops. This on-premises offering sits in the corporate network and allows users to develop code, access internal resources, and use GUI tools remotely from anywhere in the world. Among its most notable features, a virtual desktop instance can be sized according to the task at hand, has persistent user storage, and can be moved between corporate data centers to follow traveling Googlers. Until recently, our virtual desktops were hosted on commercially available hardware on Google’s corporate network using a homegrown open-source virtual cluster-management system called Ganeti. Today, this substantial and Google-critical workload runs on GCP (Google Compute Platform).
	  </p>
	  

	  <p>
	  <span>Pat Helland</span> - <a href="https://queue.acm.org/detail.cfm?id=3025012"><b>Life Beyond Distributed Transactions</b></a>
	  <br>
	  This article explores and names some of the practical approaches used in the implementation of large-scale mission-critical applications in a world that rejects distributed transactions. Topics include the management of fine-grained pieces of application data that may be repartitioned over time as the application grows. Design patterns support sending messages between these repartitionable pieces of data.
	  </p>
	  <br>


<hr noshade="" size="1">





<hr noshade="" size="1">

	<p>
	<a href="#"><img src="https://queue.acm.org/img/logo_acm.gif"></a>
	<br>
	© ACM, Inc. All Rights Reserved.
	</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Why hasn’t AMD made a viable CUDA alternative? (169 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=43547309</link>
            <guid>43547309</guid>
            <pubDate>Tue, 01 Apr 2025 14:37:54 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=43547309">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="43547461"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547461" href="https://news.ycombinator.com/vote?id=43547461&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>There is more than one way to answer this.</p><p>They have made an alternative to the CUDA language with HIP, which can do most of the things the CUDA language can.</p><p>You could say that they haven't released supporting libraries like cuDNN, but they are making progress on this with AiTer for example.</p><p>You could say that they have fragmented their efforts across too many different paradigms but I don't think this is it because Nvidia also support a lot of different programming models.</p><p>I think the reason is that they have not prioritised support for ROCm across all of their products. There are too many different architectures with varying levels of support. This isn't just historical. There is no ROCm support for their latest AI Max 395 APU. There is no nice cross architecture ISA like PTX. The drivers are buggy. It's just all a pain to use. And for that reason "the community" doesn't really want to use it, and so it's a second class citizen.</p><p>This is a management and leadership problem. They need to make using their hardware easy. They need to support all of their hardware. They need to fix their driver bugs.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547568"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547568" href="https://news.ycombinator.com/vote?id=43547568&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>This ticket, finally closed after being open for <i>2 years</i>, is a pretty good micocosm of this problem:</p><p><a href="https://github.com/ROCm/ROCm/issues/1714" rel="nofollow">https://github.com/ROCm/ROCm/issues/1714</a></p><p>Users complaining that the docs don't even specify which cards work.</p><p>But it goes deeper - a valid complaint is that "this only supports one or two consumer cards!" A common rebuttal is that it works fine on lots of AMD cards if you set some environment flag to force the GPU architecture selection.  The fact that this is <i>so close</i> to working on a wide variety of hardware, and yet doesn't, is exactly the vibe you get with the whole ecosystem.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43549097"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43549097" href="https://news.ycombinator.com/vote?id=43549097&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>What I don't get is why they don't at least assign a dev or two to make the poster child of this work: llama.cpp</p><p>It's the first thing anyone tries when trying to dabble in AI or compute on the gpu, yet it's a clusterfuck to get to work. A few blessed cards work, with proper drivers and kernel; others just crash, perform horribly slow, or output GGGGGGGGGGGGGG to every input (I'm not making this up!) Then you LOL, dump it and go buy nvidia et voila, stuff works first try.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43548203"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43548203" href="https://news.ycombinator.com/vote?id=43548203&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I suspect part of it is also that Nvidia actually does a lot of things in firmware that can be upgraded. The new Nvidia Linux drivers (the "open" ones) support Turing cards from 2018. That means chips that old already do much of the processing in firmware.</p><p>AMD keeps having issues because their drivers talk to the hardware directly so their drivers are massive bloated messes, famous for pages of auto-generated register definitions. Likely it's much more difficult to fix anything.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547940"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547940" href="https://news.ycombinator.com/vote?id=43547940&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>Geez. If I were Berkshire Hathaway looking to invest in the GPU market, this would be a major red flag in my fundamentals analysis.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547700"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547700" href="https://news.ycombinator.com/vote?id=43547700&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I had a similar (I think) experience when building LLVM from source a few years ago.</p><p>I kept running into some problem with LLVM's support for HIP code, even though I had not interest in having that functionality.</p><p>I realize this isn't exactly an AMD problem, but IIRC it was they were who contributed the troublesome code to LLVM, and it remained unfixed.</p><p>Apologies if there's something unfair or uninformed in what I wrote, it's been a while.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547988"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547988" href="https://news.ycombinator.com/vote?id=43547988&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>That reeks of gross incompetence somewhere in the organization. Like a hosting company that has a customer dealing with very poor performance, over pays greatly to avoid it while the whole time nobody even thinks to check what the linux swap file is doing.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43547799"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547799" href="https://news.ycombinator.com/vote?id=43547799&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>&gt;This is a management and leadership problem.</p><p>It's easy (and mostly correct) to blame management for this, but it's such a foundational issue that even if everyone up to the CEO pivoted on every topic, it wouldn't change anything. They simply don't have the engineering talent to pull this off, because they somehow concluded that making stuff open source means someone else will magically do the work for you. Nvidia on the other hand has accrued top talent for more than a decade and carefully developed their ecosystem to reach this point. And there are only so many talented engineers on the planet. So even if AMD leadership wakes up tomorrow, they won't go anywhere for a looong time.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547827"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547827" href="https://news.ycombinator.com/vote?id=43547827&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>&gt; This is a management and leadership problem. They need to make using their hardware easy. They need to support all of their hardware. They need to fix their driver bugs.</p><p>Yes. This kind of thing is unfortunately endemic in hardware companies, which don't "get" software. It's cultural and requires (a) a leader who does Get It and (b) one of those Amazon memos stating "anyone who does not Get With The Program will be fired".</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547675"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547675" href="https://news.ycombinator.com/vote?id=43547675&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>It is a little bit more complicated than ROCm simply not having support because ROCm has at a point claimed support, and they've had to walk it back painfully (multiple times). Its not a driver issue, nor a hardware issue on their side.</p><p>There has been a long-standing issue between AMD and its mainboard manufacturers. The issue has to do with features required for ROCm, namely PCIe Atomics. AMD has been unable or unwilling to hold the mainboard manufacturers to account for advertising features the mainboard does not support.</p><p>The CPU itself must support this feature, but the mainboard must as well (in firmware).</p><p>One of the reasons why ROCm hasn't worked in the past is because the mainboard manufacturers have claimed and advertised support for PCIe Atomics, and the support they've claimed has been shown to be false, and the software fails in non-deterministic ways when tested. This is nightmare fuel for the few AMD engineers tasked with ROCm.</p><p>PCIe Atomics requires non-translated direct IO to operate correctly, and in order to support the same CPU models from multiple generations they've translated these IO lines in firmware.</p><p>This has left most people that query their system to check this showing PCIAtomics is supported, while when actual tests that rely on that support are done they fail, in chaotic ways. There is no technical specification or advertising that the mainboard manufacturers provide showing whether this is supported. Even the boards with multiple x16 slots and the many technologies related to it such as Crossfire/SLI/mGPU brandings these don't necessarily show whether PCIAtomics is properly supported.</p><p>In other words, the CPU is supported, the firmware/mainboard fail with no way to differentiate between the two at the upper layers of abstraction.</p><p>All in all. You shouldn't be blaming AMD for this. You should be blaming the three mainboard manufacturers who chose to do this. Some of these manufacturers have upper end boards where they actually did do this right they just chose to not do this for any current gen mainboard costing less than ~$300-500.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43549200"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43549200" href="https://news.ycombinator.com/vote?id=43549200&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>There are so many hardware certification programs out there, why doesn't AMD run one to fix this?</p><p>Create a "ROCm compatible" logo and a list of criteria. Motherboard manufacturers can send a pre-production sample to AMD along with a check for some token amount (let's say $1000). AMD runs a comprehensive test suite to check actual compatibility, if it passes the mainboard is allowed to be advertised and sold with the previously mentioned logo. Then just tell consumers to look for that logo if they want to use ROCm. If things go wrong on a mainboard without the certification, communicate that it's probably the mainboard's fault.</p><p>Maybe add some kind of versioning scheme to allow updating requirements in the future</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43549341"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43549341" href="https://news.ycombinator.com/vote?id=43549341&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>AIUI, AMD documentation claims that the requirement for PCIe Atomics is due to ROCm being based on Heterogeneous System Architecture, <a href="https://en.wikipedia.org/wiki/Heterogeneous_System_Architecture" rel="nofollow">https://en.wikipedia.org/wiki/Heterogeneous_System_Architect...</a> which allows for a sort of "unified memory" (strictly speaking, a unified address space) across CPU and GPU RAM.  Other compute API's such as CUDA, OpenCL, SYCL or Vulkan Compute don't have HSA as a strict requirement but ROCm apparently does.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547796"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547796" href="https://news.ycombinator.com/vote?id=43547796&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>Look, this sounds like a frustrating nightmare, but the way it seems to us consumers is that AMD chose to rely on poorly implemented and supported technology, and Nvidia didn't. I can't blame AMD for the poor support by motherboards manufacturers but I can and will blame AMD for relying on it.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43548795"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43548795" href="https://news.ycombinator.com/vote?id=43548795&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>While we won't know for sure, unless someone from AMD comments on this; in fairness there may not have been any other way.</p><p>Nvidia has a large number of GPU related patents.</p><p>The fact that AMD chose to design their system this way, in such a roundabout and brittle manner, which is contrary to how engineer's approach things, may have been a direct result of being unable to design such systems any other way because of broad patents tied to the interface/GPU.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43549136"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43549136" href="https://news.ycombinator.com/vote?id=43549136&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I feel like this issue is to at least some extent a red herring. Even accepting that  ROCm doesn't work on some motherboards, this can't explain why so few of AMD's GPUs have official ROCm support.</p><p>I notice that at one point there was a ROCm release which said it didn't require atomics for gfx9 GPUs, but the requirement was reintroduced in a later version of ROCm. Not sure what happened there but this seems to suggest AMD might have had a workaround at some point (though possibly it didn't work).</p><p>If this really is due to patent issues AMD can likely afford to licence or cross-license the patent given potential upside.</p><p>It would be in line with other decisions taken by AMD if they took this decision because it works well with their datacentre/high-end GPUs, and they don't (or didn't) really care about offering GPGPU to the mass/consumer GPU market.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43549331"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_43549331" href="https://news.ycombinator.com/vote?id=43549331&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>&gt; I feel like this issue is to at least some extent a red herring.</p><p>I don't see that, these two issues adequately explain why so few GPUs have official support. They don't want to get hit with a lawsuit, as a result of issues outside their sphere of control.</p><p>&gt; If this really is due to patent issues AMD can likely afford to license or cross-license the patent given potential upside.</p><p>Have you ever known any company willing to cede market dominance and license or cross-license a patent letting competition into a market that they hold an absolute monopoly over, let alone in an environment where antitrust is non-existent and fang-less?</p><p>There is no upside for NVIDIA to do that. If you want to do serious AI/ML work you currently need to use NVIDIA hardware, and they can charge whatever they want for that.</p><p>The moment you have a competitor, demand is halved at a bare minimum depending on how much the competitor undercuts you by. Any agreement on coordinating prices leads to price-fixing indictments.</p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="43547751"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547751" href="https://news.ycombinator.com/vote?id=43547751&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>How does NVIDIA manage this issue? I wonder whether they have a very different supply chain or just design software that puts less trust in the reliability of those advertised features.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43549279"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43549279" href="https://news.ycombinator.com/vote?id=43549279&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I should point out here, if nobody has already; Nvidia's GPU designs are <i>extremely</i> complicated compared to what AMD and Apple ship. The "standard" is to ship a PCIe card with display handling drivers and some streaming multiprocessor hardware to process your framebuffers. Nvidia goes even further by adding additional accelerators (ALUs by way of CUDA core and tensor cores), onboard RTOS management hardware (what Nvidia calls GPU System Processor), and more complex userland drivers that very well might be able to manage atomics without any PCIe standards.</p><p>This is also one of the reasons AMD and Apple can't simply turn their ship around right now. They've both invested heavily in simplifying their GPU and removing a lot of the creature-comforts people pay Nvidia for. 10 years ago we could at least all standardize on OpenCL, but these days it's all about proprietary frameworks and throwing competitors under the bus.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547816"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43547816" href="https://news.ycombinator.com/vote?id=43547816&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>Its an open question they have never answered afaik.</p><p>I would speculate that their design is self-contained in hardware.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43547777"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547777" href="https://news.ycombinator.com/vote?id=43547777&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>So .. how's Nvidia dealing with this? Or do they benefit from motherboard manufacturers doing preferential integration testing?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43549371"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43549371" href="https://news.ycombinator.com/vote?id=43549371&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I made a post here a while back suggesting an investment strategy of spending one billion on AMD shares and one billion on software developers to 3rd party write a quality support stack for their hardware. I'm still not sure if its a crazy idea.</p><p>Actually it might be better to spend 1B on shares and 10x 100M on development and take ten attempts in parallel and use the best of them.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547919"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547919" href="https://news.ycombinator.com/vote?id=43547919&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I want to argue that graphics cards are really 3 markets: integrated, gaming (dedicated), and compute. Not only do these have different hardware (fixed function, ray tracing cores, etc.) but also different programming and (importantly) distribution models. NVIDIA went from 2 to 3. Intel went from 1 to 2, and bought 3 (trying to merge). AMD started with 2 and went to 1 (around Llano) and attempted the same thing as NVIDIA via GCN (please correct me if I'm wrong).</p><p>My understanding is that the reason is that the real market for 3 (GPUs for compute) didn't show up until very late, so AMD's GCN bet didn't pay off. Even in 2021, NVIDIA's revenue from gaming was above data center revenue (a segment they basically had no competition in, and 100% of their revenue was from CUDA). AMD meanwhile won the battle for Playstation and Xbox consoles, and was executing a turnaround in data centers with EPYC and CPUs (with Zen). So my guess as to why they might have underinvested is basically: for much of the 2010s they were just trying to survive, so they focused on battles they could win that would bring them revenue.</p><p>This high level prioritization would explain a lot of "misexecution", e.g. if they underhired for ROCm, or prioritized APU SDK experience over data center, their testing philosophy ("does this game work ok? great").</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547462"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547462" href="https://news.ycombinator.com/vote?id=43547462&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>They likely haven't put even close to enough money behind it. This isn't a unique situation - you'll see in corporate america a lot of CEOs who say "we are investing in X" and they really believe they are. But the required size is billions (like, hundreds of really insanely talented engineers being paid 500k-1m, lead by a few being paid $3-10m), and they are instead investing low 10's of millions.</p><p>They can't bring themselves to put so much money into it that it would be an obvious fail if it didn't work.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547563"><td></td></tr>
                <tr id="43547645"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547645" href="https://news.ycombinator.com/vote?id=43547645&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>The big players are all investing in building chips themselves.</p><p>And probably not putting enough money behind it... it takes enormous courage as a CEO to walk into a boardroom and say "I'm going to spend $50 billion, I think it will probably work, I'm... 60% certain".</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547778"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43547778" href="https://news.ycombinator.com/vote?id=43547778&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>You're probably correct, but I feel like I have to raise the issue of Zuckerberg spending a comparable amount on VR which was much more speculative.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43548018"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43548018" href="https://news.ycombinator.com/vote?id=43548018&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>Zuck is founder and owner. So is Huang (Nvidia CEO). They call all the shots.</p><p>Whereas AMD's CEO was appointed, and can be fired. Huge difference in their risk appetite.</p><p>I'm reminded of pg's article "founder mode": <a href="https://paulgraham.com/foundermode.html" rel="nofollow">https://paulgraham.com/foundermode.html</a></p><p>I think some companies simply aren't capable of taking big risks and innovating in big ways, for this reason.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547864"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43547864" href="https://news.ycombinator.com/vote?id=43547864&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>Zuckerberg owns Facebook though. It’s a lot easier to make bold decisions when you’re the majority shareholder.</p><p>Edit: though emphasis should be put on “easIER” because it’s still far from easy.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547935"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_43547935" href="https://news.ycombinator.com/vote?id=43547935&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>This. Without knowing the guy, he seems to be a) very comfortable taking a lot of risk and b) it's actually not that risky for him to blow $20 billion.</p><p>There aren't many cases like this. Larry/Sergey were more than comfortable risking $10 billion here and there.</p></div></td></tr>
        </tbody></table></td></tr>
                                    <tr id="43547934"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547934" href="https://news.ycombinator.com/vote?id=43547934&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>It amazes me how much these companies make actually gets spent on R&amp;D, you see the funnel charts on reddit and I am like what the hell. Microsoft only spends ~6bn USD on R&amp;D with a total 48bn of revenue and 15bn in profits?</p><p>What the hell is going on, they should be able to keep an army of PhDs doing pointless research even if only one paper in 10 years comes to a profitable product. But instead they are cutting down workforce like there is no tomorrow...</p><p>(I know, I know, market dynamics, value extraction, stock market returns)</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43549373"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43549373" href="https://news.ycombinator.com/vote?id=43549373&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>R and D in the financial statements I've seen basically covers the entire product, engineering etc org. Lots and lots of people, but not what regular people consider RnD.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43548046"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43548046" href="https://news.ycombinator.com/vote?id=43548046&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>well, look at Meta... they're spending Billions with a capital B on stuff and they get slaughtered every earnings call because it hasn't paid off yet. if Zuckerberg wasn't the majority share holder it probably wouldn't be sustainable.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43549332"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43549332" href="https://news.ycombinator.com/vote?id=43549332&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I've been telling people for years that NVIDIA is actually a software company, but nobody ever listens.  My argument is that their silicon is nothing special and could easily be replicated by others, and therefore their real value is in their driver+CUDA layer.</p><p>(Maybe "nothing special" is a little bit strong, but as a chip designer I've never seen the actual NVIDIA chips as all that much of a moat.  What makes it hard to find alternatives to NVIDIA is their driver and CUDA stack.)</p><p>Curious to hear others' opinions on this.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43549300"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43549300" href="https://news.ycombinator.com/vote?id=43549300&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>If AMD developers use AI deployed on nvidia hardware to create tools that complete against nvidia as a company but overall improves outcomes because of competition, would this be an example of co evolution observable in human time standards... I feel like ai is evolving, taking a stable form in this complex multi dimension multi paradigm sweet spot of an environment we have created, on top of this technical, social and governmental infrastructure and we're watching it live on discovery tech filtered into a 2d video narrated by some idiot who has no right to be as confident as he sounds. I'm sorry I'm on withdrawal from quitting mass media and I'm very bored.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43549395"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43549395" href="https://news.ycombinator.com/vote?id=43549395&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>&gt;I'm sorry I'm on withdrawal from quitting mass media and I'm very bored.</p><p>Good choice!  So many people doing that these days.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43547586"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547586" href="https://news.ycombinator.com/vote?id=43547586&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>CUDA is an entire ecosystem - not a single programming language extension (C++) or a single library, but a collection of libraries &amp; tools for specific use cases and optimizations (cuDNN, CUTLASS, cuBLAS, NCCL, etc.). There is also tooling support that Nvidia provides, such as profilers, etc. Many of the libraries build on other libraries. Even if AMD had the decent, reliable language extensions for general-purpose GPU programming, they still don't have the libraries and the supporting ecosystem to provide anything to the level that CUDA provides today, which is a decade plus of development effort from Nvidia to build.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43548095"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43548095" href="https://news.ycombinator.com/vote?id=43548095&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>The counter point is they could make a higher level version of CUDA which wouldn't necessitate all the other supporting libraries. The draw of cuBLAS is that CUDA is a confusing pain. It seems reasonable to think they could write a better, higher level language (in the same vein as triton) and not have to write as many support libraries</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43548849"><td></td></tr>
                        <tr id="43547594"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547594" href="https://news.ycombinator.com/vote?id=43547594&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>Back in 2015, they were a quarter or two from bankruptcy, saved by the XBOX and Playstation contracts.  Those years saw several significant layoffs, and talent leaving for greener pastures.    Lisa Su has done a great job at rebuilding the company.   But not in a position to hire 2000 engineers x few million comp (~$4 billion annually) even if there were people readily available.</p><p>"it'd still be a good investment." - that's definitely not a sure thing.  Su isn't a risk taker, seems to prefer incremental growth, mainly focused on the CPU side.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43548052"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43548052" href="https://news.ycombinator.com/vote?id=43548052&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>Where does the idea that engineers cost "a few million" come from? You might pay that much to senior engineering management, big names who can attract other talent, but normal engineers cost much less than a million dollars a year.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43548486"><td></td></tr>
            <tr id="43547672"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547672" href="https://news.ycombinator.com/vote?id=43547672&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>This is the difference between Jensen and Su. It’s not that Jensen is a risk taker. No. Jensen focused on incremental growth of the core business while slowly positioning the company for growth in other verticals as well should the landscape change.</p><p>Jensen never said… hey I’m going to bet it all on AI and cuda. Let’s go all in. This never happened. Both Jensen and Su are not huge risk takers imo.</p><p>Additionally there’s a lot of luck involved with the success of NVIDIA.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43549302"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43549302" href="https://news.ycombinator.com/vote?id=43549302&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I think this broaches the real matter, which is that nVidia's core business is GPUs while AMD's core business is CPUs. And, frankly, AMD has lately been doing a great job at its core business. The problem is that GPUs are now much more profitable than CPUs, both in terms of unit economics and growth potential. So they are winning a major battle (against Intel) even as they are losing a different one (against nVidia). I'm not sure there's a strategy they could have adopted to win both at the same time.</p><p>However, the next big looming problem for them is likely to be the shrinking market for x86 vs. the growing market for Arm etc. So they might very well have demonstrated great core competence, that ends up being completely swept away by not just one but two major industry shifts.</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43549345"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43549345" href="https://news.ycombinator.com/vote?id=43549345&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>In turn I will raise you the following: Why are GPU ISA trade secrets at all? Why not open them up like CPU ISAs, get rid of specialized cores and let compiler writers port their favorite languages to compile into native GPU programs? Everyone will be happy. Game devs will be happy with more control over the hardware, Compiler devs will be happy to run haskell or prolog natively on GPUs, ML devs will be happier, NVIDIA/AMD will be happier with taking the MainStage.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43548054"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43548054" href="https://news.ycombinator.com/vote?id=43548054&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>CUDA isn't the moat people think it is. NVIDIA absolutely has the best dev ergonomics for machine learning, there's no question about that. Their driver is also far more stable than AMD's. But AMD is also improving, they've made some significant strides over the last 12-18 months.</p><p>But I think more importantly, what is often missed in this analysis is that most programmers doing ML work aren't writing their own custom kernels. They're just using pytorch (or maybe something even more abstracted/multi-backend like keras 3.x) and let the library deal with implementation details related to their GPU.</p><p>That doesn't mean there aren't footguns in that particular land of abstraction, but the delta between the two providers is not nearly as stark as its often portrayed. At least not for the average programmer working with ML tooling.</p><p>(EDIT: also worth noting that the work being done in the MLIR project has a role to play in closing the gap as well for similar reasons)</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43548642"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43548642" href="https://news.ycombinator.com/vote?id=43548642&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>&gt; But I think more importantly, what is often missed in this analysis is that most programmers doing ML work aren't writing their own custom kernels. They're just using pytorch (or maybe something even more abstracted/multi-backend like keras 3.x) and let the library deal with implementation details related to their GPU.</p><p>That would imply that AMD could just focus on implementing good PyTorch support on their hardware and they would be able to start taking market share. Which doesn't sound like much work compared with writing a full CUDA competitor. But that does not seem to be the strategy, which implies it is not so simple?</p><p>I am not an ML engineer so don't have first hand experience, but those I have talked to say they depend on a lot more than just one or two key libraries. But my sample size is small. Interested in other perspectives...</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43549088"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43549088" href="https://news.ycombinator.com/vote?id=43549088&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>&gt; But that does not seem to be the strategy, which implies it is not so simple?</p><p>That is exactly what has been happening [1], and not just in pytorch. Geohot has been very dedicated in working with AMD to upgrade their station in this space [2]. If you hang out in the tinygrad discord, you can see this happening in real time.</p><p>&gt; those I have talked to say they depend on a lot more than just one or two key libraries.</p><p>Theres a ton of libraries out there yes, but if we're talking about python and the libraries in question are talking to GPUs its going to be exceedingly rare that theyre not using one of these under the hood: pytorch, tensorflow, jax, keras, et al.</p><p>There are of course exceptions to this, particularly if you're not using python for your ML work (which is actually common for many companies running inference at scale and want better runtime performance, training is a different story). But ultimately the core ecosystem does work just fine with AMD GPUs, provided you're not doing any exotic custom kernel work.</p><p>(EDIT: just realized my initial comment unintentionally borrowed the "moat" commentary from geohot's blog. A happy accident in this case, but still very much rings true for my day to day ML dev experience)</p><p>[1] <a href="https://github.com/pytorch/pytorch/pulls?q=is%3Aopen+is%3Apr+label%3A%22module%3A+rocm%22" rel="nofollow">https://github.com/pytorch/pytorch/pulls?q=is%3Aopen+is%3Apr...</a></p><p>[2] <a href="https://geohot.github.io//blog/jekyll/update/2025/03/08/AMD-YOLO.html" rel="nofollow">https://geohot.github.io//blog/jekyll/update/2025/03/08/AMD-...</a></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43547535"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547535" href="https://news.ycombinator.com/vote?id=43547535&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>I can't contribute much to this discussion due to bias and NDAs, but I just wanted to mention, technically HIP is our CUDA competitor. ROCm is the foundation that HIP is being built on.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547554"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547554" href="https://news.ycombinator.com/vote?id=43547554&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>I wonder what the purpose is behind creating a whole new API? Why not just focus on getting Vulkan compute on AMD GPUs to have the data throughput of CUDA?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43548619"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43548619" href="https://news.ycombinator.com/vote?id=43548619&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I don’t know answer to your question, but I recalled something relevant. Some time ago, Microsoft had a tech which compiled almost normal looking C++ into Direct3D 11 compute shaders: <a href="https://learn.microsoft.com/en-us/cpp/parallel/amp/cpp-amp-overview?view=msvc-170" rel="nofollow">https://learn.microsoft.com/en-us/cpp/parallel/amp/cpp-amp-o...</a> The compute kernels are integrated into CPU-running C++ in the similar fashion CUDA does.</p><p>As you see, the technology deprecated in Visual Studio 2022. I don’t know why but I would guess people just didn’t care. Maybe because it only run on Windows.</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43547492"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547492" href="https://news.ycombinator.com/vote?id=43547492&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>AMD have actually made several attempts at it.</p><p>The first time, they went ahead and killed off their effort to consolidate on OpenCL. OpenCL went terribly (in no small part because NVIDIA held out on OpenCL 2 support) and that set AMD back a long ways.</p><p>Beyond that, AMD does not have a strong software division or one with the teeth to really influence hardware to their needs . They have great engineers but leadership doesn’t know how to get them to where they need to be.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547900"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547900" href="https://news.ycombinator.com/vote?id=43547900&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>This is it, it's an organisational skill issue. To be fair, being a HW company and a SW company at the same time is very difficult.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43548029"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43548029" href="https://news.ycombinator.com/vote?id=43548029&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>It is but you have to be.</p><p>It’s been key to the success of their peers. NVIDIA and Apple are the best examples but even Intel to a smaller degree.</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43547545"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547545" href="https://news.ycombinator.com/vote?id=43547545&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>The idea that CUDA is the main reason behind Nvidia dominance seems strange to me. If most of the money is coming from Facebook and Microsoft they have their own teams writing code at a lower level than CUDA anyway. Even deepseek was writing stuff lower than that.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43549250"><td></td></tr>
                  <tr id="43547853"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547853" href="https://news.ycombinator.com/vote?id=43547853&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>AMD <i>was</i> investing in a drop-in CUDA compatibility layer &amp; cross-compiler!</p><p>Perhaps in keeping with the broader thread here, they had only ever funded a single contract developer working on it, and then discontinued the project (for who-knows-what legal or political reasons). But the developer had specified that he could open-source the pre-AMD state if the contract was dissolved, and he did exactly that! The project is active with an actively contributing community, and is rapidly catching up to where it was.</p><p><a href="https://www.phoronix.com/review/radeon-cuda-zluda" rel="nofollow">https://www.phoronix.com/review/radeon-cuda-zluda</a></p><p><a href="https://vosen.github.io/ZLUDA/blog/zludas-third-life/" rel="nofollow">https://vosen.github.io/ZLUDA/blog/zludas-third-life/</a></p><p><a href="https://vosen.github.io/ZLUDA/blog/zluda-update-q4-2024/" rel="nofollow">https://vosen.github.io/ZLUDA/blog/zluda-update-q4-2024/</a></p><p>IMO it's vital that even if NVIDIA's future falters in some way, the (likely) collective millennia of research built on top of CUDA will continue to have a path forward on other constantly-improving hardware.</p><p>It's frustrating that AMD will benefit from this without contributing - but given the entire context of this thread, maybe it's best that they <i>aren't</i> actively managing the thing that gives their product a future!</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547906"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547906" href="https://news.ycombinator.com/vote?id=43547906&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>ZLUDA is built on HIP which is built on ROCm.   Both of the latter are significant efforts that AMD is pouring significant resources into.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43548633"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43548633" href="https://news.ycombinator.com/vote?id=43548633&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>CUDA is over a decade of investment. I left CUDA toolkit team in 2014 and it was probably around 10 years old back then. Can't build something comparable fast.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547506"><td></td></tr>
            <tr id="43547335"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547335" href="https://news.ycombinator.com/vote?id=43547335&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>The answer is in the question, because if they had the foresight to do such a thing the tech would already be here, instead they thought 1 dimensionally about their product, were part of the group that fumbled OpenCL and now they're a decade behind playing catch up.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547379"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547379" href="https://news.ycombinator.com/vote?id=43547379&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>A good group can catch up significantly in 2 years. They will still be behind, but if they are cheaper (or just you can buy them) that would still go a long way.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547589"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547589" href="https://news.ycombinator.com/vote?id=43547589&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>I think even with the trashy api and drivers if they release graphic cards with 4x the memory of the nvidia equivalents the community would put the effort to make them work.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547800"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43547800" href="https://news.ycombinator.com/vote?id=43547800&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>Yeah. Easier said than done, I know, but they need to not just catch up to nVidia but leapfrog them somehow.</p><p>I would have said that releasing cards with 32GB+ of onboard RAM, or better yet 128GB, would have gotten things moving. They'd be able to run/train models that nVidia's consumer cards couldn't.</p><p>But I think nVidia closed that gap with their "Project Digits" (or whatever the final name is) PCs.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43547724"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547724" href="https://news.ycombinator.com/vote?id=43547724&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>I think that it is really hard to be cheaper in the ways that really matter. Performance per watt matters a lot here, and NVidia is excellent at this. It doesn't seem like anyone else will be able to compete within at least the next couple of years.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547858"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547858" href="https://news.ycombinator.com/vote?id=43547858&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>"good group" is carrying a lot of weight here.   You can't buy that.    You can buy good small groups, but AMD needs a good large group, and that can't be bought.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43547882"><td></td></tr>
                  <tr id="43547416"><td></td></tr>
            <tr id="43548746"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43548746" href="https://news.ycombinator.com/vote?id=43548746&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>Another possible reason might be outreach. NVIDIA spends big money on getting people to use their products. I have worked at two HPC centers and at both we had NVIDIA employees stationed there, whose job it was to help us get the most out of the hardware. Besides that, they also organize Hackatrons and they have dedicated software developer programs for each common application, be it LLMs, Weather Prediction or Protein Folding, not to mention dedicated libraries for pretty much every domain.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43549019"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43549019" href="https://news.ycombinator.com/vote?id=43549019&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>Maybe this is an overly cynical response but the answer is simply that they cannot (at least not immediately). They have not invested enough into engineering talent with this specific goal in mind.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547831"><td></td></tr>
            <tr id="43547510"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547510" href="https://news.ycombinator.com/vote?id=43547510&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>&gt; The delta between NVIDIA's value and AMD's is bigger than the annual GDP of Spain.</p><p>Nvidia is massively overvalued right now. AI has rocketed them into absolute absurdity, and it's not sustainable. Put aside the actual technology for a second and realize that public image of AI is at rock bottom. Every single time a company puts out AI-generated materials, they receive immense public backlash. That's not going away any time soon and it's only likely to get worse.</p><p>Speaking as someone that's not even remotely anti-AI, I wouldn't touch the shit with a 10 foot pole because of how bad the public image is. The moment that capital realizes this, that bubble is going to pop and it's going to pop hard.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547590"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547590" href="https://news.ycombinator.com/vote?id=43547590&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>Interesting perspective, I haven't noticed much if any public backlash against AI generation. What are some examples?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547761"><td></td></tr>
                <tr id="43548218"><td></td></tr>
                  <tr id="43547884"><td></td></tr>
                  <tr id="43547711"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547711" href="https://news.ycombinator.com/vote?id=43547711&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>&gt; AI has rocketed them into absolute absurdity, and it's not sustainable</p><p>Why isn't it sustainable? Their biggest customers all have strong finances and legitimate demand. Google and Facebook would happily run every piece of user generated content through an LLM if they had enough GPUs. Same with Microsoft and every enterprise document.</p><p>The VC backed companies and Open AI are more fragile, but they're comparatively small customers.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43548050"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43548050" href="https://news.ycombinator.com/vote?id=43548050&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>IMO the closest analogue for Nvidia now is Cisco during the dot-com boom. Cisco sold the physical infrastructure required for Internet companies to operate. Investors all bought in because they figured it was a safe bet. Individual companies may come and go, but if the Internet keeps growing, companies will always need to buy networking equipment. Despite the Internet being way bigger than it was in 2000, and Cisco being highly profitable, Cisco's share price has never exceeded the peak it was at during the dot-com boom.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43548097"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43548097" href="https://news.ycombinator.com/vote?id=43548097&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>Google may well want to run more of their content through an LLM, but they will not be using Nvidia hardware to do it, they'll be using their TPUs.</p><p>Amazon are on their third generation of in-house AI chips and Anthropic will be using those chips to train the next generation of Claude.</p><p>In other words, their biggest customers are looking for cheaper alternatives and are already succeeding in finding them.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547801"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547801" href="https://news.ycombinator.com/vote?id=43547801&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>&gt; Google and Facebook would happily run every piece of user generated content through an LLM if they had enough GPUs. Same with Microsoft and every enterprise document.</p><p>.. But how much actual value derives from this?</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547917"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43547917" href="https://news.ycombinator.com/vote?id=43547917&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>Youtube could conceivably put multi language subtitles on every video. Potentially even dub them.</p><p>But the "real value" would come from making adverts better targeted and more interactive. It's hard to quantity as a person outside of the companies, but the intuition for a positive value is pretty strong.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43548093"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43548093" href="https://news.ycombinator.com/vote?id=43548093&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>&gt; Youtube could conceivably put multi language subtitles on every video.</p><p>They already do this, it's opt-in.</p><p>&gt; But the "real value" would come from making adverts better targeted and more interactive.</p><p>Is there any evidence to suggest that a transformer would be better at collaborative filtering than the current deep learning system that was custom engineered and built for this?</p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="43547559"><td></td></tr>
                <tr id="43547716"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547716" href="https://news.ycombinator.com/vote?id=43547716&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I see this form of argument sometimes here but I really don’t get it.</p><p>Lots of people don’t play the stock market or just invest in funds. It seems like just a way of challenging somebody that looks vaguely clever, or calls them out in a “put your money where your mouth is” sense, but actually presents no argument.</p><p>Anyway, if you want to short Nvidia you have to know when their bubble is going to pop to get much benefit out of it, right? The market can remain stupid for longer than you can remain solvent or whatever.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547942"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43547942" href="https://news.ycombinator.com/vote?id=43547942&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>Spot on on the timing being important. I don't think you need to fine-tune it that much; short and hold until the pop happens. If you hold off for a <i>the pop could happen at an indefinite time; maybe very far from now</i>, then I think that invalidates the individual prediction.</p><p>One frustrating aspect of investing is that confident information is tough to come by. It's my take that if you have any (I personally rarely do), you should act on it. So, when someone claims confidently (e.g. with adjectives that imply confidence) that something's going to happen, then that's better than the default.</p><p>I don't have the insight the claimer does; my thought is: "I am jealous. I with I could be that confident about a stock's trajectory. I would act on it."</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43548338"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43548338" href="https://news.ycombinator.com/vote?id=43548338&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I was a student up until 2009; watching people talk about buying houses for 50K and selling them for 100K, everyone talking about easy money.</p><p>I knew things were bad when a friend of my sister was complaining that her father(a building framer) was not able to get a loan for a 500K house, something that his colleagues had been able to get. It took another 6 months before the collapse started to hit and the banks when up.</p><p>Timing is hard.</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43547659"><td></td></tr>
                <tr id="43547867"><td></td></tr>
                <tr id="43548013"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43548013" href="https://news.ycombinator.com/vote?id=43548013&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>&gt; But they didn't say soon.</p><p>Setting an indefinite timeline devalues any claim. You could prove this to yourself using <i>Reductio ad absurdum</i>, or by applying it to various general cases.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547984"><td></td></tr>
                        <tr id="43547619"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547619" href="https://news.ycombinator.com/vote?id=43547619&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>You imply you either would believe his word or would short nvidia yourself if he said he was.  If not, why not?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547856"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43547856" href="https://news.ycombinator.com/vote?id=43547856&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>Close - If I had the degree of confidence that post implies about Nvidia being overvalued, I would take an aggressive short position.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547996"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43547996" href="https://news.ycombinator.com/vote?id=43547996&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>&gt; Lots of very smart people have lost a lot of money by being completely right about the destination, but wrong about the path and how long it will take to get there.</p><p>If you make a habit of this and still lose money, then either you statistically were very unlucky, or did not have a history of being right.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547944"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43547944" href="https://news.ycombinator.com/vote?id=43547944&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p><i>I would take an aggressive short position.</i></p><p>Lots of very smart people have lost a lot of money by being completely right about the destination, but wrong about the path and how long it will take to get there.</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43547847"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547847" href="https://news.ycombinator.com/vote?id=43547847&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I forbid myself from speculative trading as a consequence of idiosyncratic principles that I live my life by. One of many symbolic rejections of toxic profiteering that infests our neo-mercantile society. I have enough digits in my bank account that adding any more would be unambiguously greedy and distasteful, so in the end it would be violating my principles simply to debase myself. No thanks.</p><p>Anyways you'd need some kind of window of when a stock is going to collapse to short it. Good luck predicting this one.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547885"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43547885" href="https://news.ycombinator.com/vote?id=43547885&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I respect, and adore your philosophy.</p><p>For a short, I think you don't need that strong of a window. For an options combination, yes.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43547803"><td></td></tr>
                  <tr id="43547630"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547630" href="https://news.ycombinator.com/vote?id=43547630&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>One thing I've learned the hard way is that industry trends -- and the stock valuations that go with them -- can stay irrational far longer than you can imagine.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43548927"><td></td></tr>
                  <tr id="43547728"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547728" href="https://news.ycombinator.com/vote?id=43547728&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>HIP is definitely a viable option. In fact with some effort you can port large CUDA projects to be compilable with the HIP/ AMD-clang toolchain. This way you don’t have to rewrite the world from scratch in a new language but still be able to run GPU workloads on AMD hardware.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547450"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547450" href="https://news.ycombinator.com/vote?id=43547450&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I don't think its that bad. The focus will turn to inference going forward and that eventually means a place for AMD and maybe even Intel. Eventually it will be all about the efficiency of inference in watts.</p><p>That switch will reduce the NVIDIA margins by a lot. NVIDIA probably has 2 years left of being the only one with golden shovels.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547549"><td></td></tr>
                  <tr id="43547627"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547627" href="https://news.ycombinator.com/vote?id=43547627&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>What about Rust for GPU programming? I wonder why AMD doesn't back such kind of effort as an alternative.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547340"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547340" href="https://news.ycombinator.com/vote?id=43547340&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>Leadership. At the end of the day, the buck stops with leadership.</p><p>If they wanted to prioritize this, they would. They're simply not taking it seriously.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547363"><td></td></tr>
                <tr id="43547469"><td></td></tr>
            <tr id="43547604"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547604" href="https://news.ycombinator.com/vote?id=43547604&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>Why, because 90% of her job is talking to and appeasing shareholders, grand standing with fat whales, and what else.. what do you think a CEO at these companies actually does? They aren't in the trenches of each subdivision nurturing and cracking whips. She likely attends a 2 hour briefing with a line item: CUDA parity project: on schedule release date not set</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43547560"><td></td></tr>
            <tr id="43547520"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547520" href="https://news.ycombinator.com/vote?id=43547520&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>HIP is now somewhat viable (and ROCm is now all HIP).</p><p>But — too late. First versions of ROCm were terrible. Too much boilerplate. 1200 lines of template-heavy C++ for a simple FFT. Can't just start hacking around.</p><p>Since then, the CUDA way is cemented in minds of developers. Intel now has oneAPI, and it is not too bad, and hackable, but there is no hardware and no one will learn it. And HIP is "CUDA-like", so why not CUDA, unless you _have to_ use AMD hardware.</p><p>Tl;dr first versions of ROCm were bad. Now they are better, but it is too late.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547893"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547893" href="https://news.ycombinator.com/vote?id=43547893&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>AMD's CEO is the cousin of Nvidias CEO.</p><p>Neither will encroach too much on the others turf.    The two companies don't want to directly compete on the things that really drive the share price.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547347"><td></td></tr>
                <tr id="43547359"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547359" href="https://news.ycombinator.com/vote?id=43547359&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>The question was why don't they have anything as good as CUDA, not why don't they adopt CUDA itself.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547402"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547402" href="https://news.ycombinator.com/vote?id=43547402&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>Isn't the "goodness" of CUDA really down to its mass adoption -- and therefore its community and network effects -- not strictly its technical attributes?</p><p>If I recall, there are various "GPU programming" and "AI" efforts that have existed for AMD GPUs, but none of them have had the same success in large part because they're simply non-"standard?"</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547524"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43547524" href="https://news.ycombinator.com/vote?id=43547524&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>I thought OpenCL was supposed to be the "standard"?  From the Wikipedia page, it's largely vendor neutral and not that much younger than CUDA (initial release Aug 2009 vs Feb 2007).  Maybe some more knowledgeable people can comment why it seems to have been outcompeted by the proprietary CUDA?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43548063"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43548063" href="https://news.ycombinator.com/vote?id=43548063&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>CUDA has a comparatively nicer user experience. If you would like to understand tacitly and have an nvidia GPU, try writing a simple program using both. (Something highly parallel, like nbody, for example)</p>
              </div></td></tr>
        </tbody></table></td></tr>
                                    </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bletchley code breaker Betty Webb dies aged 101 (399 pts)]]></title>
            <link>https://www.bbc.com/news/articles/c78jd30ywv8o</link>
            <guid>43546236</guid>
            <pubDate>Tue, 01 Apr 2025 12:55:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/c78jd30ywv8o">https://www.bbc.com/news/articles/c78jd30ywv8o</a>, See on <a href="https://news.ycombinator.com/item?id=43546236">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div data-testid="byline-new" data-component="byline-block"><p><span>Aida Fofana</span></p><p><span>BBC News, West Midlands<!-- --></span></p></div><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/ae8b/live/9ed85c70-0ee3-11f0-bb66-1953ea6c7fa4.png.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/ae8b/live/9ed85c70-0ee3-11f0-bb66-1953ea6c7fa4.png.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/ae8b/live/9ed85c70-0ee3-11f0-bb66-1953ea6c7fa4.png.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/ae8b/live/9ed85c70-0ee3-11f0-bb66-1953ea6c7fa4.png.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/ae8b/live/9ed85c70-0ee3-11f0-bb66-1953ea6c7fa4.png.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/ae8b/live/9ed85c70-0ee3-11f0-bb66-1953ea6c7fa4.png.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/ae8b/live/9ed85c70-0ee3-11f0-bb66-1953ea6c7fa4.png.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/ae8b/live/9ed85c70-0ee3-11f0-bb66-1953ea6c7fa4.png.webp" loading="eager" alt="BBC Pictured is Betty Webb with short curly white hair and in a purple coast with army pin badges on the right lapel. She is slightly smiling and looking towards the camera."><span>BBC</span></p></div><p data-component="caption-block"><figcaption>Bletchley Park code breaker Betty Webb has died at the age of 101<!-- --></figcaption></p></figure><div data-component="text-block"><p>A decorated World War Two code breaker who spent her youth deciphering enemy messages at Bletchley Park has died at the age of 101.<!-- --></p><p>Charlotte "Betty" Webb MBE - who was among the last surviving Bletchley code breakers - died on Monday night, the Women's Royal Army Corps Association confirmed.<!-- --></p><p>Mrs Webb, from Wythall in Worcestershire, joined operations at the Buckinghamshire base at the age of 18, later going on to help with Japanese codes at The Pentagon in the US. She was awarded France's highest honour - the Légion d'Honneur - in 2021. <!-- --></p><p>The Women's Royal Army Corps Association described Mrs Webb as a woman who "inspired women in the Army for decades".<!-- --></p></div><div data-component="text-block"><p>Bletchley Park Trust CEO Iain Standen said Mrs Webb will not only be remembered for her work but "also for her efforts to ensure that the story of what she and her colleagues achieved is not forgotten."<!-- --></p><p>"Betty's passion for preserving the history and legacy of Bletchley Park has undoubtedly inspired many people to engage with the story and visit the site," he said in a statement.<!-- --></p><p>Tributes to Mrs Webb have begun to be posted on social media, including one from historian and author Dr Tessa Dunlop who said she was with her in her final hours.<!-- --></p><p>Describing Mrs Webb as "the very best", she said on X: "She is one of the most remarkable woman I have ever known."<!-- --></p><p>Mrs Webb told the BBC in 2020 that she had "never heard of Bletchley", Britain's wartime code-breaking centre, before starting work there as a member of the ATS, the Auxiliary Territorial Service.<!-- --></p><p>She had been studying at a college near Shrewsbury, Shropshire, when she volunteered as she said she and others on the course felt they "ought to be serving our country rather than just making sausage rolls".<!-- --></p><p>Her mother had taught her to speak German as a child and ahead of her posting remembered being "taken into the mansion [at Bletchley] to read the Official Secrets Act".<!-- --></p><p>"I realised that from then on there was no way that I was going to be able to tell even my parents where I was and what I was doing until 1975 [when restrictions were lifted]," she recalled.<!-- --></p><p>She would tell the family with whom she lodged that she was a secretary.<!-- --></p></div><p data-component="caption-block"><figcaption>Listen on BBC Sounds: Mrs Webb went to work at Bletchley Park when she was 18<!-- --></figcaption></p><div data-component="text-block"><p>When the War ended in Europe in May of 1945, she went to work at the Pentagon after spending four years at Bletchley, which with its analysis of German communications had served as a vital cog in the Allies' war machine.<!-- --></p><p>At the Pentagon she would paraphrase and transcribe already-decoded Japanese messages. She said she was the only member of the ATS to be sent to Washington, describing it as a "tremendous honour".<!-- --></p><p>Mrs Webb, in 2020, recalled she had had no idea the Americans planned to end the conflict by dropping atomic weapon on Japanese cities, describing the weapons' power as "utterly awful"<!-- --></p><p>After the Allies' final victory, it took Mrs Webb several months to organise return passage to the UK, where she worked as a secretary at a school in Shropshire.<!-- --></p><p>The head teacher there had also worked at Bletchley so knew of her professionalism, whereas other would-be employers, she recalled, were left stumped by her being unable to explain - due to secrecy requirements - her previous duties.<!-- --></p><p>More than half a century later, in 2021, Mrs Webb was one of 6,000 British citizens to receive the Légion d'Honneur, following a decision by President François Hollande in 2014 to recognise British veterans who helped liberate France.<!-- --></p></div><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/87d9/live/c8583c40-0ee9-11f0-ba12-8d27eb561761.png.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/87d9/live/c8583c40-0ee9-11f0-ba12-8d27eb561761.png.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/87d9/live/c8583c40-0ee9-11f0-ba12-8d27eb561761.png.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/87d9/live/c8583c40-0ee9-11f0-ba12-8d27eb561761.png.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/87d9/live/c8583c40-0ee9-11f0-ba12-8d27eb561761.png.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/87d9/live/c8583c40-0ee9-11f0-ba12-8d27eb561761.png.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/87d9/live/c8583c40-0ee9-11f0-ba12-8d27eb561761.png.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/87d9/live/c8583c40-0ee9-11f0-ba12-8d27eb561761.png.webp" loading="lazy" alt="PA Media Mrs Webb sat in the front row in a red skirt suit surrounded by other people in large hats, floral print dresses and trouser suits at the Kings Coronation."><span>PA Media</span></p></div><p data-component="caption-block"><figcaption>Betty Webb, seen in the front row in a red suit, was invited to the Coronation<!-- --></figcaption></p></figure><div data-component="text-block"><p>In 2023, she and her niece were among 2,200 people from 203 countries invited to Westminster Abbey to see King Charles III's coronation.<!-- --></p><p>The same year she celebrated her 100th birthday at Bletchley Park with a party. <!-- --></p><p>She and her guests were treated to a fly-past by a Lancaster bomber. She said at the time: "It was for me - it's unbelievable isn't it? Little me."<!-- --></p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why F#? (321 pts)]]></title>
            <link>https://batsov.com/articles/2025/03/30/why-fsharp/</link>
            <guid>43546004</guid>
            <pubDate>Tue, 01 Apr 2025 12:34:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://batsov.com/articles/2025/03/30/why-fsharp/">https://batsov.com/articles/2025/03/30/why-fsharp/</a>, See on <a href="https://news.ycombinator.com/item?id=43546004">Hacker News</a></p>
<div id="readability-page-1" class="page"><section itemprop="text">
        
        <p>If someone had told me a few months ago I’d be playing with .NET again after a
15+ years hiatus I probably would have laughed at this.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup> Early on in my
career I played with .NET and Java, and even though .NET had done some things
better than Java (as it had the opportunity to learn from some early Java
mistakes), I quickly settled on Java as it was a truly portable environment.</p>

<p>I guess everyone who reads my blog knows that in the past few years I’ve been
playing on and off with OCaml and I think it’s safe to say that it has become
one of my favorite programming languages - alongside the likes of Ruby and
Clojure. My work with OCaml drew my attention recently to F#, an ML targeting
.NET, developed by Microsoft. The functional counterpart of the
(mostly) object-oriented C#. The newest ML language created…</p>

<h2 id="what-is-f">What is F#?</h2>

<blockquote>
  <p>Unfortunately, no one can be told what the Matrix is. You have to see it for yourself.</p>

  <p>– Morpheus, The Matrix</p>
</blockquote>

<p>Before we start discussing F#, I guess we should answer first the question
“What is F#?”. I’ll borrow a bit from the <a href="https://learn.microsoft.com/en-us/dotnet/fsharp/what-is-fsharp">official page</a> to answer it.</p>

<p>F# is a universal programming language for writing succinct, robust and performant code.</p>

<p>F# allows you to write uncluttered, self-documenting code, where your focus remains on your problem domain, rather than the details of programming.</p>

<p>It does this without compromising on speed and compatibility - it is open-source, cross-platform and interoperable.</p>

<div><pre><code><span>open</span> <span>System</span> <span>// Gets access to functionality in System namespace.</span>

<span>// Defines a list of names</span>
<span>let</span> <span>names</span> <span>=</span> <span>[</span> <span>"Peter"</span><span>;</span> <span>"Julia"</span><span>;</span> <span>"Xi"</span> <span>]</span>

<span>// Defines a function that takes a name and produces a greeting.</span>
<span>let</span> <span>getGreeting</span> <span>name</span> <span>=</span> <span>$</span><span>"Hello, {name}"</span>

<span>// Prints a greeting for each name!</span>
<span>names</span>
<span>|&gt;</span> <span>List</span><span>.</span><span>map</span> <span>getGreeting</span>
<span>|&gt;</span> <span>List</span><span>.</span><span>iter</span> <span>(</span><span>fun</span> <span>greeting</span> <span>-&gt;</span> <span>printfn</span> <span>$</span><span>"{greeting}! Enjoy your F#"</span><span>)</span>
</code></pre></div>

<p><strong>Trivia:</strong> F# is the language that made the pipeline operator (<code>|&gt;</code>) popular.</p>

<p>F# has numerous features, including:</p>

<ul>
  <li>Lightweight syntax</li>
  <li>Immutable by default</li>
  <li>Type inference and automatic generalization</li>
  <li>First-class functions</li>
  <li>Powerful data types</li>
  <li>Pattern matching</li>
  <li>Async programming</li>
</ul>

<p>A full set of features are documented in the <a href="https://learn.microsoft.com/en-us/dotnet/fsharp/language-reference/">F# language guide</a>.</p>

<p>Looks pretty promising, right?</p>

<p>F# 1.0 was officially released in May 2005 by Microsoft Research. It was
initially developed by Don Syme at Microsoft Research in Cambridge and evolved
from an earlier research project called “Caml.NET,” which aimed to bring OCaml
to the .NET platform.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" rel="footnote">2</a></sup> F# was officially moved from Microsoft Research to
Microsoft (as part of their developer tooling division) in 2010 (timed
with the release of F# 2.0).</p>

<p>F# has been steadily evolving since those early days and the most recent release
<a href="https://learn.microsoft.com/en-us/dotnet/fsharp/whats-new/fsharp-9">F# 9.0</a> was
released in November 2024.  It seems only appropriate that F# would come to my
attention in the year of its 20th birthday!</p>

<p>There were several reasons why I wanted to try out F#:</p>

<ul>
  <li>.NET became open-source and portable a few years ago and I wanted to check the progress on that front</li>
  <li>I was curious if F# offers any advantages over OCaml</li>
  <li>I’ve heard good things about the F# tooling (e.g. Rider and Ionide)</li>
  <li>I like playing with new programming languages</li>
</ul>

<p>Below you’ll find my initial impressions for several areas.</p>

<h2 id="the-language">The Language</h2>

<p>As a member of the ML family of languages, the syntax won’t surprise
anyone familiar with OCaml. As there are quite few people familiar with
OCaml, though, I’ll mention that Haskell programmers will also feel right at
home with the syntax. And Lispers.</p>

<p>For everyone else - it’d be fairly easy to pick up the basics.</p>

<div><pre><code><span>// function application</span>
<span>printfn</span> <span>"Hello, World!"</span>

<span>// function definition</span>
<span>let</span> <span>greet</span> <span>name</span> <span>=</span>
    <span>printfn</span> <span>"Hello, %s!"</span> <span>name</span>

<span>greet</span> <span>"World"</span>

<span>// whitespace is significant, like in Python</span>
<span>let</span> <span>foo</span> <span>=</span>
    <span>let</span> <span>i</span><span>,</span> <span>j</span><span>,</span> <span>k</span> <span>=</span> <span>(</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>)</span>

    <span>// Body expression:</span>
    <span>i</span> <span>+</span> <span>2</span> <span>*</span> <span>j</span> <span>+</span> <span>3</span> <span>*</span> <span>k</span>

<span>// conditional expressions</span>
<span>let</span> <span>test</span> <span>x</span> <span>y</span> <span>=</span>
  <span>if</span> <span>x</span> <span>=</span> <span>y</span> <span>then</span> <span>"equals"</span>
  <span>elif</span> <span>x</span> <span>&lt;</span> <span>y</span> <span>then</span> <span>"is less than"</span>
  <span>else</span> <span>"is greater than"</span>

<span>printfn</span> <span>"%d %s %d."</span> <span>10</span> <span>(</span><span>test</span> <span>10</span> <span>20</span><span>)</span> <span>20</span>

<span>// Looping over a list.</span>
<span>let</span> <span>list1</span> <span>=</span> <span>[</span> <span>1</span><span>;</span> <span>5</span><span>;</span> <span>100</span><span>;</span> <span>450</span><span>;</span> <span>788</span> <span>]</span>
<span>for</span> <span>i</span> <span>in</span> <span>list1</span> <span>do</span>
   <span>printfn</span> <span>"%d"</span> <span>i</span>

<span>// Looping over a sequence of tuples</span>
<span>let</span> <span>seq1</span> <span>=</span> <span>seq</span> <span>{</span> <span>for</span> <span>i</span> <span>in</span> <span>1</span> <span>..</span> <span>10</span> <span>-&gt;</span> <span>(</span><span>i</span><span>,</span> <span>i</span><span>*</span><span>i</span><span>)</span> <span>}</span>
<span>for</span> <span>(</span><span>a</span><span>,</span> <span>asqr</span><span>)</span> <span>in</span> <span>seq1</span> <span>do</span>
  <span>printfn</span> <span>"%d squared is %d"</span> <span>a</span> <span>asqr</span>

<span>// A simple for...to loop.</span>
<span>let</span> <span>function1</span> <span>()</span> <span>=</span>
  <span>for</span> <span>i</span> <span>=</span> <span>1</span> <span>to</span> <span>10</span> <span>do</span>
    <span>printf</span> <span>"%d "</span> <span>i</span>
  <span>printfn</span> <span>""</span>

<span>// A for...to loop that counts in reverse.</span>
<span>let</span> <span>function2</span> <span>()</span> <span>=</span>
  <span>for</span> <span>i</span> <span>=</span> <span>10</span> <span>downto</span> <span>1</span> <span>do</span>
    <span>printf</span> <span>"%d "</span> <span>i</span>
  <span>printfn</span> <span>""</span>

<span>// Records</span>

<span>// Labels are separated by semicolons when defined on the same line.</span>
<span>type</span> <span>Point</span> <span>=</span> <span>{</span> <span>X</span><span>:</span> <span>float</span><span>;</span> <span>Y</span><span>:</span> <span>float</span><span>;</span> <span>Z</span><span>:</span> <span>float</span> <span>}</span>

<span>// You can define labels on their own line with or without a semicolon.</span>
<span>type</span> <span>Customer</span> <span>=</span>
    <span>{</span> <span>First</span><span>:</span> <span>string</span>
      <span>Last</span><span>:</span> <span>string</span>
      <span>SSN</span><span>:</span> <span>uint32</span>
      <span>AccountNumber</span><span>:</span> <span>uint32</span> <span>}</span>

<span>let</span> <span>mypoint</span> <span>=</span> <span>{</span> <span>X</span> <span>=</span> <span>1</span><span>.</span><span>0</span><span>;</span> <span>Y</span> <span>=</span> <span>1</span><span>.</span><span>0</span><span>;</span> <span>Z</span> <span>=</span> <span>-</span><span>1</span><span>.</span><span>0</span> <span>}</span>

<span>// Discriminated Union</span>
<span>type</span> <span>Shape</span> <span>=</span>
    <span>|</span> <span>Circle</span> <span>of</span> <span>radius</span><span>:</span> <span>float</span>
    <span>|</span> <span>Rectangle</span> <span>of</span> <span>width</span><span>:</span> <span>float</span> <span>*</span> <span>height</span><span>:</span> <span>float</span>

<span>// Functing using pattern matching</span>
<span>let</span> <span>area</span> <span>shape</span> <span>=</span>
    <span>match</span> <span>shape</span> <span>with</span>
    <span>|</span> <span>Circle</span> <span>radius</span> <span>-&gt;</span> <span>System</span><span>.</span><span>Math</span><span>.</span><span>PI</span> <span>*</span> <span>radius</span> <span>*</span> <span>radius</span>
    <span>|</span> <span>Rectangle</span> <span>(</span><span>width</span><span>,</span> <span>height</span><span>)</span> <span>-&gt;</span> <span>width</span> <span>*</span> <span>height</span>

<span>let</span> <span>circle</span> <span>=</span> <span>Circle</span> <span>5</span><span>.</span><span>0</span>
<span>let</span> <span>rectangle</span> <span>=</span> <span>Rectangle</span><span>(</span><span>4</span><span>.</span><span>0</span><span>,</span> <span>3</span><span>.</span><span>0</span><span>)</span>

<span>printfn</span> <span>"Circle area: %f"</span> <span>(</span><span>area</span> <span>circle</span><span>)</span>
<span>printfn</span> <span>"Rectangle area: %f"</span> <span>(</span><span>area</span> <span>rectangle</span><span>)</span>
</code></pre></div>

<p>Nothing shocking here, right?</p>

<p>Here’s another slightly more involved example:</p>

<div><pre><code><span>open</span> <span>System</span>

<span>// Sample data - simple sales records</span>
<span>type</span> <span>SalesRecord</span> <span>=</span> <span>{</span> <span>Date</span><span>:</span> <span>DateTime</span><span>;</span> <span>Product</span><span>:</span> <span>string</span><span>;</span> <span>Amount</span><span>:</span> <span>decimal</span><span>;</span> <span>Region</span><span>:</span> <span>string</span> <span>}</span>

<span>// Sample dataset</span>
<span>let</span> <span>sales</span> <span>=</span> <span>[</span>
    <span>{</span> <span>Date</span> <span>=</span> <span>DateTime</span><span>(</span><span>2023</span><span>,</span> <span>1</span><span>,</span> <span>15</span><span>);</span> <span>Product</span> <span>=</span> <span>"Laptop"</span><span>;</span> <span>Amount</span> <span>=</span> <span>1200</span><span>m</span><span>;</span> <span>Region</span> <span>=</span> <span>"North"</span> <span>}</span>
    <span>{</span> <span>Date</span> <span>=</span> <span>DateTime</span><span>(</span><span>2023</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>);</span>  <span>Product</span> <span>=</span> <span>"Phone"</span><span>;</span>  <span>Amount</span> <span>=</span> <span>800</span><span>m</span><span>;</span>  <span>Region</span> <span>=</span> <span>"South"</span> <span>}</span>
    <span>{</span> <span>Date</span> <span>=</span> <span>DateTime</span><span>(</span><span>2023</span><span>,</span> <span>1</span><span>,</span> <span>20</span><span>);</span> <span>Product</span> <span>=</span> <span>"Tablet"</span><span>;</span> <span>Amount</span> <span>=</span> <span>400</span><span>m</span><span>;</span>  <span>Region</span> <span>=</span> <span>"North"</span> <span>}</span>
    <span>{</span> <span>Date</span> <span>=</span> <span>DateTime</span><span>(</span><span>2023</span><span>,</span> <span>2</span><span>,</span> <span>18</span><span>);</span> <span>Product</span> <span>=</span> <span>"Laptop"</span><span>;</span> <span>Amount</span> <span>=</span> <span>1250</span><span>m</span><span>;</span> <span>Region</span> <span>=</span> <span>"East"</span> <span>}</span>
    <span>{</span> <span>Date</span> <span>=</span> <span>DateTime</span><span>(</span><span>2023</span><span>,</span> <span>1</span><span>,</span> <span>5</span><span>);</span>  <span>Product</span> <span>=</span> <span>"Phone"</span><span>;</span>  <span>Amount</span> <span>=</span> <span>750</span><span>m</span><span>;</span>  <span>Region</span> <span>=</span> <span>"West"</span> <span>}</span>
    <span>{</span> <span>Date</span> <span>=</span> <span>DateTime</span><span>(</span><span>2023</span><span>,</span> <span>2</span><span>,</span> <span>12</span><span>);</span> <span>Product</span> <span>=</span> <span>"Tablet"</span><span>;</span> <span>Amount</span> <span>=</span> <span>450</span><span>m</span><span>;</span>  <span>Region</span> <span>=</span> <span>"North"</span> <span>}</span>
    <span>{</span> <span>Date</span> <span>=</span> <span>DateTime</span><span>(</span><span>2023</span><span>,</span> <span>1</span><span>,</span> <span>28</span><span>);</span> <span>Product</span> <span>=</span> <span>"Laptop"</span><span>;</span> <span>Amount</span> <span>=</span> <span>1150</span><span>m</span><span>;</span> <span>Region</span> <span>=</span> <span>"South"</span> <span>}</span>
<span>]</span>

<span>// Quick analysis pipeline</span>
<span>let</span> <span>salesSummary</span> <span>=</span>
    <span>sales</span>
    <span>|&gt;</span> <span>List</span><span>.</span><span>groupBy</span> <span>(</span><span>fun</span> <span>s</span> <span>-&gt;</span> <span>s</span><span>.</span><span>Product</span><span>)</span>                          <span>// Group by product</span>
    <span>|&gt;</span> <span>List</span><span>.</span><span>map</span> <span>(</span><span>fun</span> <span>(</span><span>product</span><span>,</span> <span>items</span><span>)</span> <span>-&gt;</span>                          <span>// Transform each group</span>
        <span>let</span> <span>totalSales</span> <span>=</span> <span>items</span> <span>|&gt;</span> <span>List</span><span>.</span><span>sumBy</span> <span>(</span><span>fun</span> <span>s</span> <span>-&gt;</span> <span>s</span><span>.</span><span>Amount</span><span>)</span>
        <span>let</span> <span>avgSale</span> <span>=</span> <span>totalSales</span> <span>/</span> <span>decimal</span> <span>(</span><span>List</span><span>.</span><span>length</span> <span>items</span><span>)</span>
        <span>let</span> <span>topRegion</span> <span>=</span>
            <span>items</span>
            <span>|&gt;</span> <span>List</span><span>.</span><span>groupBy</span> <span>(</span><span>fun</span> <span>s</span> <span>-&gt;</span> <span>s</span><span>.</span><span>Region</span><span>)</span>                   <span>// Nested grouping</span>
            <span>|&gt;</span> <span>List</span><span>.</span><span>maxBy</span> <span>(</span><span>fun</span> <span>(_,</span> <span>regionItems</span><span>)</span> <span>-&gt;</span>
                <span>regionItems</span> <span>|&gt;</span> <span>List</span><span>.</span><span>sumBy</span> <span>(</span><span>fun</span> <span>s</span> <span>-&gt;</span> <span>s</span><span>.</span><span>Amount</span><span>))</span>
            <span>|&gt;</span> <span>fst</span>

        <span>(</span><span>product</span><span>,</span> <span>totalSales</span><span>,</span> <span>avgSale</span><span>,</span> <span>topRegion</span><span>))</span>
    <span>|&gt;</span> <span>List</span><span>.</span><span>sortByDescending</span> <span>(</span><span>fun</span> <span>(_,</span> <span>total</span><span>,</span> <span>_,</span> <span>_)</span> <span>-&gt;</span> <span>total</span><span>)</span>      <span>// Sort by total sales</span>

<span>// Display results</span>
<span>salesSummary</span>
<span>|&gt;</span> <span>List</span><span>.</span><span>iter</span> <span>(</span><span>fun</span> <span>(</span><span>product</span><span>,</span> <span>total</span><span>,</span> <span>avg</span><span>,</span> <span>region</span><span>)</span> <span>-&gt;</span>
    <span>printfn</span> <span>"%s: $%M total, $%M avg, top region: %s"</span>
        <span>product</span> <span>total</span> <span>avg</span> <span>region</span><span>)</span>
</code></pre></div>

<p>Why don’t you try saving the snippet above in a file called <code>Sales.fsx</code> and running it like this:</p>



<p>Now you know that F# is a great choice for ad-hoc scripts! Also, running <code>dotnet fsi</code> by itself
will pop an F# REPL where you can explore the language at your leisure.</p>

<p>I’m not going to go into great details here, as much of what I wrote about OCaml
<a href="https://batsov.com/articles/2022/08/29/ocaml-at-first-glance/">here</a> applies to F# as well.
I’d also suggest this quick <a href="https://learn.microsoft.com/en-us/dotnet/fsharp/tour">tour of F#</a>
to get a better feel for its syntax.</p>

<p><strong>Tip:</strong> Check out the <a href="https://fsprojects.github.io/fsharp-cheatsheet/">F# cheatsheet</a>
if you’d like to see a quick syntax reference.</p>

<p>One thing that made a good impression to me is the focus of the language designers on
making F# approachable to newcomers, by providing a lot of small quality of life improvements
for them. Below are few examples, that probably don’t mean much to you, but would mean something
to people familiar with OCaml:</p>

<div><pre><code><span>// line comments</span>
<span>(* the classic ML comments are around as well *)</span>

<span>// mutable values</span>
<span>let</span> <span>mutable</span> <span>x</span> <span>=</span> <span>5</span>
<span>x</span> <span>&lt;-</span> <span>6</span>

<span>// ranges and slices</span>
<span>let</span> <span>l</span> <span>=</span> <span>[</span><span>1</span><span>..</span><span>2</span><span>..</span><span>10</span><span>]</span>
<span>name</span><span>[</span><span>5</span><span>..]</span>

<span>// C# method calls look pretty natural</span>
<span>let</span> <span>name</span> <span>=</span> <span>"FOO"</span><span>.</span><span>ToLower</span><span>()</span>

<span>// operators can be overloaded for different types</span>
<span>let</span> <span>string1</span> <span>=</span> <span>"Hello, "</span> <span>+</span> <span>"world"</span>
<span>let</span> <span>num1</span> <span>=</span> <span>1</span> <span>+</span> <span>2</span>
<span>let</span> <span>num2</span> <span>=</span> <span>1</span><span>.</span><span>0</span> <span>+</span> <span>2</span><span>.</span><span>5</span>

<span>// universal printing</span>
<span>printfn</span> <span>"%A"</span> <span>[</span><span>1</span><span>..</span><span>2</span><span>..</span><span>100</span><span>]</span>
</code></pre></div>

<p>I guess some of those might be controversial, depending on whether you’re a ML
language purist or not, but in my book anything that makes ML more popular is a
good thing.</p>

<p>Did I also mention it’s easy to work with unicode strings and regular expressions?</p>

<p>Often people say that F# is mostly a staging ground for future C# features, and perhaps that’s true.
I haven’t observed both languages long enough to have my own opinion on the subject, but I was impressed
to learn that <code>async/await</code> (of C# and later JavaScript fame) originated in… F# 2.0.</p>

<blockquote>
  <p>It all changed in 2012 when C#5 launched with the introduction of what has now
become the popularized <code>async/await</code> keyword pairing. This feature allowed you to
write code with all the benefits of hand-written asynchronous code, such as not
blocking the UI when a long-running process started, yet read like normal
synchronous code. This <code>async/await</code> pattern has now found its way into many
modern programming languages such as Python, JS, Swift, Rust, and even C++.</p>

  <p>F#’s approach to asynchronous programming is a little different from <code>async/await</code>
but achieves the same goal (in fact, <code>async/await</code> is a cut-down version of F#’s
approach, which was introduced a few years previously, in F#2).</p>

  <p>– Isaac Abraham, F# in Action</p>
</blockquote>

<p>Time will tell what will happen, but I think it’s unlikely that C# will ever be able to fully replace F#.</p>

<p>I’ve also found this <a href="https://www.reddit.com/r/fsharp/comments/xlegmc/why_doesnt_microsoft_use_f/">encouraging comment from 2022</a> that Microsoft might be willing to invest more in F#:</p>

<blockquote>
  <p>Some good news for you. After 10 years of F# being developed by 2.5 people
internally and some random community efforts, Microsoft has finally decided to
properly invest in F# and created a full-fledged team in Prague this
summer. I’m a dev in this team, just like you I was an F# fan for many years
so I am happy things got finally moving here.</p>
</blockquote>

<p>Looking at the changes in F# 8.0 and F 9.0, it seems the new full-fledged team
has done some great work!</p>

<h2 id="ecosystem">Ecosystem</h2>

<p>It’s hard to assess the ecosystem around F# after such a brief period, but overall it seems to
me that there are fairly few “native” F# libraries and frameworks out there and most people
rely heavily on the core .NET APIs and many third-party libraries and frameworks geared towards C#.
That’s a pretty common setup when it comes to hosted languages in general, so nothing surprising here as well.</p>

<p>If you’ve ever used another hosted language (e.g. Scala, Clojure, Groovy) then you probably know what
to expect.</p>

<p><a href="https://github.com/fsprojects/awesome-fsharp">Awesome F#</a> keeps track of popular F# libraries, tools and frameworks. I’ll highlight here the web development and data science libraries:</p>

<p><strong>Web Development</strong></p>

<ul>
  <li><strong>Giraffe</strong>: A lightweight library for building web applications using ASP.NET Core. It provides a functional approach to web development.</li>
  <li><strong>Suave</strong>: A simple and lightweight web server library with combinators for routing and task composition. (Giraffe was inspired by Suave)</li>
  <li><strong>Saturn</strong>: Built on top of Giraffe and ASP.NET Core, it offers an MVC-style framework inspired by Ruby on Rails and Elixir’s Phoenix.</li>
  <li><strong>Bolero</strong>: A framework for building client-side applications in F# using WebAssembly and Blazor.</li>
  <li><strong>Fable</strong>: A compiler that translates F# code into JavaScript, enabling integration with popular JavaScript ecosystems like React or Node.js.</li>
  <li><strong>Elmish</strong>: A model-view-update (MVU) architecture for building web UIs in F#, often used with Fable.</li>
  <li><strong>SAFE Stack</strong>: An end-to-end, functional-first stack for building cloud-ready web applications. It combines technologies like Saturn, Azure, Fable, and Elmish for a type-safe development experience.</li>
</ul>

<p><strong>Data Science</strong></p>

<ul>
  <li><strong>Deedle</strong>: A library for data manipulation and exploratory analysis, similar to pandas in Python.</li>
  <li><strong>DiffSharp</strong>: A library for automatic differentiation and machine learning.</li>
  <li><strong>FsLab</strong>: A collection of libraries tailored for data science, including visualization and statistical tools.</li>
</ul>

<p>I haven’t played much with any of them at this point yet, so I’ll reserve any
feedback and recommendations for some point in the future.</p>

<h2 id="documentation">Documentation</h2>

<p>The official documentation is pretty good, although I find it kind of weird that
some of it is hosted on <a href="https://learn.microsoft.com/en-us/dotnet/fsharp/what-is-fsharp">Microsoft’s site</a>
and the rest is on <a href="https://fsharp.org/">https://fsharp.org/</a> (the site of the F# Software Foundation).</p>

<p>I really liked the following parts of the documentation:</p>

<ul>
  <li><a href="https://learn.microsoft.com/en-us/dotnet/fsharp/style-guide/">F# Style Guide</a></li>
  <li><a href="https://github.com/fsharp/fslang-design">F# Design</a> - a repository of RFCs (every language should have one of those!)</li>
  <li><a href="https://fsharp.github.io/fsharp-core-docs/">F# Standard Library API</a></li>
</ul>

<p><a href="https://fsharpforfunandprofit.com/">https://fsharpforfunandprofit.com/</a> is another good learning resource. (even if it seems a bit dated)</p>



<p>F# has a somewhat troubled dev tooling story, as historically
support for F# was great only in Visual Studio, and somewhat subpar
elsewhere. Fortunately, the tooling story has improved a lot in the past
decade:</p>

<blockquote>
  <p>In 2014 a technical breakthrough was made with the creation of the
FSharp.Compiler.Service (FCS) package by Tomas Petricek, Ryan Riley, and Dave
Thomas with many later contributors. This contains the core implementation of
the F# compiler, editor tooling and scripting engine in the form of a single
library and can be used to make F# tooling for a wide range of
situations. This has allowed F# to be delivered into many more editors,
scripting and documentation tools and allowed the development of alternative
backends for F#. Key editor community-based tooling includes Ionide, by
Krzysztof Cieślak and contributors, used for rich editing support in the
cross-platform VSCode editor, with over 1M downloads at time of writing.</p>

  <p>– Don Syme, The Early History of F#</p>
</blockquote>

<p>I’ve played with the F# plugins for several editors:</p>

<ul>
  <li>Emacs (<code>fsharp-mode</code>)</li>
  <li>Zed (third-party plugin)</li>
  <li>Helix (built-in support for F#)</li>
  <li>VS Code (<a href="https://ionide.io/">Ionide</a>)</li>
  <li>Rider (JetBrains’s .NET IDE)</li>
</ul>

<p>Overall, Rider and VS Code provide the most (and the most polished) features,
but the other options were quite usable as well.  That’s largely due to the fact
that the F# LSP server <code>fsautocomplete</code> (naming is hard!) is quite robust and
any editor with good LSP support gets a lot of functionality for free.</p>

<p>Still, I’ll mention that I found the tooling lacking in some regards:</p>

<ul>
  <li><code>fsharp-mode</code> doesn’t use TreeSitter (yet) and doesn’t seem to be very actively developed (looking at the code - it seems it was derived from <code>caml-mode</code>)</li>
  <li>Zed’s support for F# is quite spartan</li>
  <li>In VS Code shockingly the expanding and shrinking selection is broken, which is quite odd for what is supposed to be the flagship editor for F#</li>
</ul>

<p>I’m really struggling with VS Code’s keybindings (too many modifier keys and functions keys for my taste) and editing model, so I’ll likely stick with Emacs going forward. Or I’ll finally spend more quality time with neovim!</p>

<p>It seems that everyone is using the same code formatter (<code>Fantomas</code>), including the F# team, which is great!
The linter story in F# is not as great (seems the only popular linter <a href="https://fsprojects.github.io/FSharpLint/">FSharpLint</a> is abandonware these days), but when your
compiler is so good, you don’t really need a linter as much.</p>

<p>Oh, well… It seems that Microsoft are not really particularly invested in
supporting the tooling for F#, as pretty much all the major projects in this
space are community-driven.</p>

<p>Using AI coding agents (e.g. Copilot) with F# worked pretty well, but I didn’t
spend much time on this front.</p>

<p>In the end of the day any editor will likely do, as long as you’re using LSP.</p>

<p>By the way, I had an interesting observation while programming in F# (and OCaml for that matter) -
that when you’re working with a language with a really good type system you don’t really need that much
from your editor. Most the time I’m perfectly happy with just some inline type information (e.g. something like CodeLenses), auto-completion and the ability to easily send code to <code>fsi</code>. Simplicity continues
to be the ultimate sophistication…</p>

<p>Other tools that should be on your radar are:</p>

<ul>
  <li><a href="https://fsprojects.github.io/Paket/">Paket</a> - Paket is a dependency manager for .NET projects. Think of it as something like <code>bundler</code>, <code>npm</code> or <code>pip</code>, but for .NET’s NuGet package ecosystem.</li>
  <li><a href="https://fake.build/">FAKE</a> -  A DSL for build tasks and more, where you can use F# to specify the tasks. Somewhat similar to Ruby’s <code>rake</code>. Some people claim that’s the easiest way to sneak F# into an existing .NET project.</li>
</ul>

<h2 id="use-cases">Use Cases</h2>

<p>Given the depth and breath of .NET - I guess that sky is the limit for you!</p>

<p>Seems to me that F# will be a particularly good fit for data analysis and manipulation, because
of features like <a href="https://learn.microsoft.com/en-us/dotnet/fsharp/tutorials/type-providers/">type providers</a>.</p>

<p>Probably a good fit for backend services and even full-stack apps, although I haven’t really played
with the F# first solutions in this space yet.</p>

<p>Fable and Elmish make F# a viable option for client-side programming and might offer
another easy way to sneak F# into your day-to-day work.</p>

<p><strong>Note:</strong> Historically, Fable has been used to target JavaScript but since Fable
4, you can also target other languages such as TypeScript, Rust, Python, and
more.</p>

<p>Here’s how easy it is to transpile an F# codebase into something else:</p>

<div><pre><code><span># If you want to transpile to JavaScript</span>
dotnet fable

<span># If you want to transpile to TypeScript</span>
dotnet fable <span>--lang</span> typescript

<span># If you want to transpile to Python</span>
dotnet fable <span>--lang</span> python
</code></pre></div>

<p>Cool stuff!</p>



<p>My initial impression of the community is that it’s fairly small, perhaps even
smaller than that of OCaml.  The F# Reddit and Discord (the one listed on
Reddit) seem like the most active places for F# conversations. There’s supposed
to be some F# Slack as well, but I couldn’t get an invite for it. (seems the
automated process for issuing those invites has been broken for a while)</p>

<p>I’m still not sure what’s the role Microsoft plays in the community, as I
haven’t seen much from them overall.</p>

<p>For a me a small community is not really a problem, as long as the community is
vibrant and active. Also - I’ve noticed I always feel more connected to smaller
communities. Moving from Java to Ruby back in the day felt like night and day as
far as community engagement and sense of belonging go.</p>

<p>I didn’t find many books and community sites/blogs dedicated to F#, but I didn’t
really expect to in the first place.</p>

<p>The most notable community initiatives I discovered were:</p>

<ul>
  <li><a href="https://amplifyingfsharp.io/">Amplifying F#</a> - an effort to promote F# and to get more businesses involved with it</li>
  <li><a href="https://fsharpforfunandprofit.com/">F# for Fun and Profit</a> - a collection of tutorials and essays on F#</li>
  <li><a href="https://fslab.org/">F# Lab</a> - The community driven toolkit for datascience in F#</li>
  <li><a href="https://sergeytihon.com/category/f-weekly/">F# Weekly</a> - a weekly newsletter about the latest developments in the world of F#</li>
</ul>

<p>Seems to me that more can be done to promote the language and engage new programmers and businesses
with it, although that’s never easy 20 years into the existence of some project. I continue to be
somewhat puzzled as to why Microsoft doesn’t market F# more, as I think it could be a great
marketing vehicle for them.</p>

<p>All in all - I don’t feel qualified to comment much on the F# community at this point.</p>

<h2 id="the-popularity-contest">The Popularity Contest</h2>

<p>Depending on the type of person you are you may or may not care about a a programming language’s
“popularity”. People often ask my why I spent a lot of time with languages that are unlikely to
ever result in job opportunities for me, e.g.:</p>

<ul>
  <li>Emacs Lisp</li>
  <li>Clojure</li>
  <li>OCaml</li>
  <li>F#</li>
</ul>

<p>Professional opportunities are important, of course, but so are:</p>

<ul>
  <li>having fun (and the F in F# stands for “fun”)</li>
  <li>learning new paradigms and ideas</li>
  <li>challenging yourself to think and work differently</li>
</ul>

<p>That being said, F# is not a popular language by most conventional metrics. It’s not highly ranked
on TIOBE, StackOverflow or most job boards. But it’s also not less popular than most “mainstream”
functional programming languages. The sad reality is that functional programming is still not
mainstream and perhaps it will never be.</p>

<p>A few more resources on the subject:</p>

<ul>
  <li><a href="https://medium.com/@lanayx/about-f-popularity-c9b78ed89252">About F#’s popularity</a></li>
  <li><a href="https://hamy.xyz/blog/2024-11_fsharp-popularity">How Popular is F# in 2024</a>
    <ul>
      <li>Here’s also a <a href="https://www.youtube.com/watch?v=JioaHcy_QE0&amp;t=1s">video</a> for the article above</li>
    </ul>
  </li>
</ul>

<h2 id="f-vs-ocaml">F# vs OCaml</h2>

<blockquote>
  <p>The early conception of F# was simple: to bring the benefits of OCaml to .NET and .NET to OCaml: a
marriage between strongly typed functional programming and .NET. Here “OCaml” meant both the
core of the language itself, and the pragmatic approach to strongly-typed functional programming
it represented. The initial task was relatively well-defined: I would re-implement the core of the
OCaml language and a portion of its base library to target the .NET Common Language Runtime.
The implementation would be fresh, i.e. not using any of the OCaml codebase, for legal clarity.</p>

  <p>– Don Syme, creator of F#, The Early History of F#</p>
</blockquote>

<p>F# was derived from OCaml, so the two languages share a lot of DNA. Early on
F# made some efforts to support as much of OCaml’s syntax as possible, and it
even allowed the use of <code>.ml</code> and <code>.mli</code> file extensions for F# code. Over time
the languages started to diverge a bit, though.<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" rel="footnote">3</a></sup></p>

<p>Creating a language that’s independent from OCaml, of course, was something
intended from the very beginning. That’s also reflected in the decision
to chose the name F#, even if early versions of the language were called “Caml.NET”:</p>

<blockquote>
  <p>Although the first version of F# was initially presented as “Caml-for-.NET”,
in reality it was always a new language, designed for .NET from day 1. F# was
never fully compatible with any version of OCaml, though it shared a compatible
subset, and it took Caml-Light and OCaml as its principal sources of design
guidance and inspiration.</p>

  <p>– Don Syme, The Early History of F#</p>
</blockquote>

<p>If you ask most people about the pros and cons of F# over OCaml you’ll probably
get the following answers.</p>

<p><strong>F# Pros</strong></p>

<ul>
  <li>Runs on .NET
    <ul>
      <li>Tons of libraries are at disposal</li>
    </ul>
  </li>
  <li>Backed by Microsoft</li>
  <li>Arguably it’s a bit easier to learn by newcomers (especially those who have only experience with OO programming)
    <ul>
      <li>The syntax is slightly easier to pick up (I think)</li>
      <li>The compiler errors and warnings are “friendlier” (easier to understand)</li>
      <li>It’s easier to debug problems (partially related to the previous item)</li>
    </ul>
  </li>
  <li>Strong support for <a href="https://learn.microsoft.com/en-us/dotnet/fsharp/tutorials/async">async programming</a></li>
  <li>Has some cool features, absent in OCaml, like:
    <ul>
      <li>Anonymous Records</li>
      <li>Active Patterns</li>
      <li>Computational expressions</li>
      <li>Sequence comprehensions</li>
      <li>Type Providers</li>
      <li>Units of measure</li>
    </ul>
  </li>
</ul>

<p><strong>F# Cons</strong></p>

<ul>
  <li>Runs on .NET
    <ul>
      <li>The interop with .NET influenced a lot of language design decisions (e.g. allowing <code>null</code>)</li>
    </ul>
  </li>
  <li>Backed by Microsoft
    <ul>
      <li>Not everyone likes Microsoft</li>
      <li>Seems the resources allocated to F# by Microsoft are modest</li>
      <li>It’s unclear how committed Microsoft will be to F# in the long run</li>
    </ul>
  </li>
  <li>Naming conventions: I like <code>snake_case</code> way more than <code>camelCase</code> and <code>PascalCase</code></li>
  <li>Misses some cool OCaml features
    <ul>
      <li>First-class modules and functors</li>
      <li>GADTs</li>
    </ul>
  </li>
  <li>Doesn’t have a friendly camel logo</li>
  <li>The name F# sounds cool, but is a search and filename nightmare (and you’ll see FSharp quite often in the wild)</li>
</ul>

<p>Both F# and OCaml can also target JavaScript runtimes as well - via <a href="https://fable.io/">Fable</a> on
the F# side and Js_of_ocaml and Melange on the OCaml side. Fable seems like a
more mature solution at a cursory glance, but I haven’t used any of the three
enough to be able to offer an informed opinion.</p>

<p>In the end of the day both remain two fairly similar robust, yet niche,
languages, which are unlikely to become very popular in the future. I’m guessing
working professionally with F# is more likely to happen for most people, as .NET
is super popular and I can imagine it’d be fairly easy to sneak a bit of F# here
in there in established C# codebases.</p>

<p>One weird thing I’ve noticed with F# projects is that they still use XML project
manifests, where you have to list the source files manually in the order in
which they should be compiled (to account for the dependencies between them). I
am a bit shocked that the compiler can’t handle the dependencies automatically,
but I guess that’s because in F# there’s not direct mapping between source files
and modules. At any rate - I prefer the OCaml compilation process (and Dune) way
more.</p>

<p>As my interest in MLs is mostly educational I’m personally leaning towards OCaml, but if I had to build
web services with an ML language I’d probably pick F#. I also have a weird respect for every language
with its own runtime, as this means that it’s unlikely that the runtime will force some compromises
on the language.</p>

<h2 id="closing-thoughts">Closing thoughts</h2>

<p>All in all I liked F# way more than I expected to! In a way it reminded me of my
experience with Clojure back in the day in the sense that Clojure was the most
practical Lisp out there when it was released, mostly because of its great
interop with Java.</p>

<p>I have a feeling that if .NET was portable since day 1 probably ClojureCLR would have become
as popular as Clojure, and likely F# would have developed a bigger community and
broader usage by now. I’m fairly certain I would have never dabbled in .NET again
if it hadn’t been for .NET Core, and I doubt I’m the only one.</p>

<p>Learning OCaml is definitely not hard, but I think that people interested to learn some ML
dialect might have an easier time with F#. And, as mentioned earlier, you’ll probably have an
easier path to “production” with it.</p>

<p>I think that everyone who has experience with .NET will benefit from learning F#.
Perhaps more importantly - everyone looking to do more with an ML family language
should definitely consider F#, as it’s a great language in its own right, that gives
you access to one of the most powerful programming platforms out there.</p>

<p>Let’s not forget about <a href="https://fable.io/">Fable</a>, which makes it possible for you leverage
F# in JavaScript, Dart, Rust and Python runtimes!</p>

<p>So, why F#? In the F# community there’s the saying that the “F” in F# stands for
“Fun”. In my brief experience with F# I found this to be very true! I’ll go a
step further and make the claim that F# is both seriously <strong>fun</strong> and seriously
practical!</p>

<p>Also if your code compiles - it will probably work the way you expect it to. I
hear that’s generally considered a desirable thing in the world of programming!</p>

<p>That’s all I have for you today. Please, share in the comments what do you love about F#!</p>

<p>In sane type systems we trust!</p>

<h2 id="discussions">Discussions</h2>

<ul>
  <li><a href="https://news.ycombinator.com/item?id=43546004">Hacker News</a></li>
  <li><a href="https://lobste.rs/s/kubso9/why_f">Lobsters</a></li>
</ul>



        
      </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Electron Band Structure in Germanium, My Ass (790 pts)]]></title>
            <link>https://pages.cs.wisc.edu/~kovar/hall.html</link>
            <guid>43545917</guid>
            <pubDate>Tue, 01 Apr 2025 12:25:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pages.cs.wisc.edu/~kovar/hall.html">https://pages.cs.wisc.edu/~kovar/hall.html</a>, See on <a href="https://news.ycombinator.com/item?id=43545917">Hacker News</a></p>
<div id="readability-page-1" class="page">
<base target="top">
<basefont size="3">

<b></b>
<p><b> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Abstract: The exponential dependence of resistivity on temperature 
in germanium is found to be a great big lie.  My careful theoretical modeling and painstaking experimentation reveal 1) that my equipment 
is crap, as are all the available texts on the subject and 2) that this whole exercise was a complete waste of my 
time.
</b>
 </p>

<h3><u>Introduction</u></h3> 

<p> 
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  Electrons in germanium are confined to 
well-defined energy bands that are separated by "forbidden regions" of zero charge-carrier density.  You can 
read about it yourself if you want to, although I don't recommend it.  You'll have to wade through an obtuse, convoluted discussion about considering an arbitrary number of 
non-coupled harmonic-oscillator potentials and taking limits and so on.  The upshot is that if you heat up a sample of germanium, electrons will jump from a
non-conductive energy band to a conductive one, thereby creating a measurable change in resistivity.  This 
relation between temperature and resistivity can be shown to be exponential in certain temperature regimes 
by waving your hands and chanting "to first order".  
 </p>
<h3><u>Experiment procedure</u></h3> 

<p>
&nbsp; &nbsp; &nbsp; I sifted through the box of germanium crystals and chose the one that appeared 
to be the least cracked.  Then I soldered wires onto the crystal in the spots shown in figure 2b of Lab Handout 
32.  Do you have any idea how hard it is to solder wires to germanium?  I'll tell you: real goddamn hard.  The 
solder simply won't stick, and you can forget about getting any of the grad students in the solid state labs to 
help you out. 
<br> 
&nbsp; &nbsp; &nbsp;  Once the wires were in place, I attached them as appropriate to the second-rate  
equipment I scavenged from the back of the lab, none of which worked properly.  I soon wised up and swiped 
replacements from the well-stocked research labs.   This is how they treat undergrads around here: they give 
you broken tools and then don't understand why  you don't get any results.
 <br>

<table>
<tbody><tr>
<td rowspan="3">&nbsp;</td>
<td>
<img src="https://pages.cs.wisc.edu/~kovar/fittedHall.gif" width="351" height="285">
</td></tr>
<tr><td></td></tr>
<tr><td>
<b><span face="Arial, Geneva, Helvetica, sans-serif" size="2">
Fig. 1: Check this shit out.
</span></b>
</td></tr></tbody></table>


&nbsp; &nbsp; &nbsp;  In order to control the temperature of the germanium, I attached the crystal to a 
copper rod, the upper end of which was attached to a heating coil and the lower end of which was dipped in 
a thermos of liquid nitrogen.   Midway through the project, the thermos began leaking.  That's right: I pay a cool ten grand a quarter to 
come here, and yet they can't spare the five bucks to ensure that I have a working thermos.  
</p>
<h3><u>Results</u></h3> 

<p>
&nbsp; &nbsp; &nbsp; Check this shit out (Fig. 1).  That's bonafide, 100%-real data, my friends.  I took it 
myself over the course of two weeks.  And this was not a leisurely two weeks, either; I busted my
ass day and night in order to provide you with nothing but the best data possible.   Now, let's look a bit more closely
at this data, remembering that it is absolutely first-rate.  Do you see the exponential dependence?  I sure don't.  I see a bunch of crap.<br>
&nbsp; &nbsp; &nbsp; Christ, this was such a waste of my time. <br>
&nbsp; &nbsp; &nbsp;  Banking on my hopes that whoever grades this will just look at the pictures, I drew an 
exponential through my noise.  I believe the apparent legitimacy is enhanced by the fact that I used a complicated computer program
to make the fit.  I understand this is the same process by which the top quark was discovered.
</p>
<h3><u>Conclusion</u></h3> 

<p>
&nbsp; &nbsp; &nbsp; Going into physics was the biggest mistake of my life.  I should've declared CS.  I still 
wouldn't have any women, but at least I'd be rolling in cash.
</p>
</div>]]></description>
        </item>
        <item>
            <title><![CDATA[CERN scientists find evidence of quantum entanglement in sheep (314 pts)]]></title>
            <link>https://home.cern/news/news/physics/cern-scientists-find-evidence-quantum-entanglement-sheep</link>
            <guid>43545349</guid>
            <pubDate>Tue, 01 Apr 2025 11:08:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://home.cern/news/news/physics/cern-scientists-find-evidence-quantum-entanglement-sheep">https://home.cern/news/news/physics/cern-scientists-find-evidence-quantum-entanglement-sheep</a>, See on <a href="https://news.ycombinator.com/item?id=43545349">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <figure id="CERN-PHOTO-201706-157-2"><a href="https://cds.cern.ch/images/CERN-PHOTO-201706-157-2" title="View on CDS"><img alt="Life at CERN" src="https://cds.cern.ch/images/CERN-PHOTO-201706-157-2/file?size=large"></a><figcaption>The CERN flock of sheep on site in 2017.<span> (Image: CERN)</span></figcaption></figure>
<p>Quantum entanglement is a fascinating phenomenon where two particles’ states are tied to each other, no matter how far apart the particles are. In 2022, the <a href="https://home.cern/news/news/knowledge-sharing/cern-congratulates-winners-2022-nobel-prize-physics">Nobel Prize in Physics</a> was awarded to Alain Aspect, John F. Clauser and Anton Zeilinger for groundbreaking experiments involving entangled photons. These experiments confirmed the predictions for the manifestation of entanglement that had been made by the <a href="https://home.cern/news/news/physics/fifty-years-bells-theorem">late CERN theorist John Bell</a>. This phenomenon has so far been observed in a wide variety of systems, such as in top quarks at <a href="https://home.cern/news/press-release/physics/lhc-experiments-cern-observe-quantum-entanglement-highest-energy-yet">CERN’s Large Hadron Collider</a> (LHC) in 2024. Entanglement has also found several important societal applications, such as quantum cryptography and quantum computing. Now, it also explains the famous herd mentality of sheep.</p>

<p>A flock of sheep (ovis aries) has roamed the CERN site during the spring and summer months <a href="https://cds.cern.ch/record/970008?ln=en">for over 40 years</a>. Along with the CERN shepherd, they help to maintain the vast expanses of grassland around the LHC and are part of the Organization’s long-standing <a href="https://home.cern/news/news/cern/environmental-awareness-exploring-cerns-biodiversity">efforts to protect the site’s biodiversity</a>. In addition, their <a href="https://www.nature.com/articles/s41567-022-01769-8">flocking behaviour</a> has been of great interest to CERN's physicists. It is well known that sheep <a href="https://physicsworld.com/a/field-work-the-physics-of-sheep-from-phase-transitions-to-collective-motion/">behave like particles</a>: their stochastic behaviour has been studied by zoologists and physicists alike, who noticed that a flock’s ability to quickly change phase is similar to that of atoms in a solid and a liquid. Known as the Lamb Shift, this can cause them to get themselves into bizarre situations, such as walking in a circle for <a href="https://www.abc.net.au/news/science/2022-11-22/sheep-circling-mystery-could-have-simple-explanation/101682672">days on end.</a></p>

<p>Now, new research has shed light on the reason for these extraordinary abilities. Scientists at CERN have found evidence of quantum entanglement in sheep. Using sophisticated modelling techniques and specialised trackers, the findings show that the brains of individual sheep in a flock are quantum-entangled in such a way that the sheep can move and vocalise simultaneously, no matter how far apart they are. The evidence has several ramifications for ovine research and has set the baa for a new branch of quantum physics.</p>

<p>“The fact that we were having our lunch next to the flock was a shear coincidence,” says Mary Little, leader of the <a href="https://greybook.cern.ch/experiment/detail?id=HERD">HERD collaboration</a>, describing how the project came about. “When we saw and herd their behaviour, we wanted to investigate the movement of the flock using the technology at our disposal at the Laboratory.”</p>

<p>Observing the sheep’s ability to simultaneously move and vocalise together caused one main question to aries: since the sheep behave like subatomic particles, could quantum effects be the reason for their behaviour?</p>

<p>“Obviously, we couldn’t put them all in a box and see if they were dead or alive,” said Beau Peep, a researcher on the project. “However, by assuming that the sheep were spherical, we were able to model their behaviour in almost the exact same way as we model subatomic particles.”</p>

<p>Using sophisticated trackers, akin to those in the LHC experiments, the physicists were able to locate the precise particles in the sheep’s brains that might be the cause of this entanglement. Dubbed “moutons” and represented by the Greek letter lambda, l, these particles are leptons and are close relatives of the muon, but fluffier.</p>

<p>The statistical significance of the findings is 4 <a href="https://home.cern/resources/faqs/five-sigma">sigma</a>, which is enough to show evidence of the phenomenon. However, it does not quite pass the baa to be classed as an observation.</p>

<p>“More research is needed to fully confirm that this was indeed an observation of ovine entanglement or a statistical fluctuation,” says Ewen Woolly, spokesperson for the HERD collaboration. “This may be difficult, as we have found that the research makes physicists become inexplicably drowsy.”</p>

<p>“While entanglement is now the leading theory for this phenomenon, we have to take everything into account,” adds Dolly Shepherd, a CERN theorist. “Who knows, maybe further variables are hidden beneath their fleeces. Wolves, for example.”</p>

<figure id="CERN-HOMEWEB-PHO-2025-028-1"><a href="https://cds.cern.ch/images/CERN-HOMEWEB-PHO-2025-028-1" title="View on CDS"><img alt="home.cern,Life at CERN" src="https://cds.cern.ch/images/CERN-HOMEWEB-PHO-2025-028-1/file?size=large"></a>
<figcaption>Theoretical physicist John Ellis, pioneer of the penguin diagram, with its updated sheep version. Scientists at CERN find evidence of quantum entanglement in sheep in 2025, the year declared by the United Nations as the <a href="https://home.cern/news/news/knowledge-sharing/official-launch-quantum-year">International Year of Quantum Science and Technology.</a>&nbsp;<span>(Image: CERN)</span></figcaption></figure>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Self-Hosting like it's 2025 (207 pts)]]></title>
            <link>https://kiranet.org/self-hosting-like-its-2025/</link>
            <guid>43544979</guid>
            <pubDate>Tue, 01 Apr 2025 10:11:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kiranet.org/self-hosting-like-its-2025/">https://kiranet.org/self-hosting-like-its-2025/</a>, See on <a href="https://news.ycombinator.com/item?id=43544979">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[US accidentally sent Maryland father to Salvadorian prison, can't get him back (207 pts)]]></title>
            <link>https://www.independent.co.uk/news/world/americas/us-politics/trump-el-salvador-abrego-garcia-b2725002.html</link>
            <guid>43544534</guid>
            <pubDate>Tue, 01 Apr 2025 09:14:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.independent.co.uk/news/world/americas/us-politics/trump-el-salvador-abrego-garcia-b2725002.html">https://www.independent.co.uk/news/world/americas/us-politics/trump-el-salvador-abrego-garcia-b2725002.html</a>, See on <a href="https://news.ycombinator.com/item?id=43544534">Hacker News</a></p>
Couldn't get https://www.independent.co.uk/news/world/americas/us-politics/trump-el-salvador-abrego-garcia-b2725002.html: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA['A hostile state': Why some travellers are avoiding the US (121 pts)]]></title>
            <link>https://www.bbc.com/travel/article/20250328-the-people-boycotting-travel-to-the-us</link>
            <guid>43544293</guid>
            <pubDate>Tue, 01 Apr 2025 08:39:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/travel/article/20250328-the-people-boycotting-travel-to-the-us">https://www.bbc.com/travel/article/20250328-the-people-boycotting-travel-to-the-us</a>, See on <a href="https://news.ycombinator.com/item?id=43544293">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/images/ic/160xn/p0l11gb2.jpg.webp 160w,https://ichef.bbci.co.uk/images/ic/240xn/p0l11gb2.jpg.webp 240w,https://ichef.bbci.co.uk/images/ic/320xn/p0l11gb2.jpg.webp 320w,https://ichef.bbci.co.uk/images/ic/480xn/p0l11gb2.jpg.webp 480w,https://ichef.bbci.co.uk/images/ic/640xn/p0l11gb2.jpg.webp 640w,https://ichef.bbci.co.uk/images/ic/800xn/p0l11gb2.jpg.webp 800w,https://ichef.bbci.co.uk/images/ic/1024xn/p0l11gb2.jpg.webp 1024w,https://ichef.bbci.co.uk/images/ic/1376xn/p0l11gb2.jpg.webp 1376w,https://ichef.bbci.co.uk/images/ic/1920xn/p0l11gb2.jpg.webp 1920w" src="https://ichef.bbci.co.uk/images/ic/480xn/p0l11gb2.jpg.webp" loading="eager" alt="Getty Images A line of American flags on poles through a wire fence (Credit: Getty Images)"><span>Getty Images</span></p></div><p data-component="caption-block"><figcaption>(Credit: Getty Images)<!-- --></figcaption></p></figure><p><b id="as-the-list-of-nations-issuing-travel-warnings-to-the-us-grows,-some-visitors-are-opting-to-boycott-it-entirely.-here's-why-many-foreigners-are-changing-their-travel-plans-and-what-this-could-mean-for-americans.">As the list of nations issuing travel warnings to the US grows, some visitors are opting to boycott it entirely. Here's why many foreigners are changing their travel plans and what this could mean for Americans.<!-- --></b></p><p>The cold shoulder has been particularly noticeable from the US's northern neighbour, Canada, which sends more than <!-- --><a target="_blank" href="https://www.ustravel.org/press/potential-results-decline-canadian-travel-united-states#:~:text=Canada%20is%20the%20top%20source,spending%20and%2014%2C000%20job%20losses.">20 million visitors<!-- --></a> to the country per year – more than any other nation. In response to Trump's proposed <!-- --><a target="_self" href="https://www.bbc.com/news/articles/cn93e12rypgo">tariffs<!-- --></a> and repeated threats to <!-- --><a target="_self" href="https://www.bbc.com/news/articles/czx82j5wd8vo">annex the nation<!-- --></a>, former prime minister Justin Trudeau recently urged his fellow Canadians: "Now is the time to choose Canada," adding, "it might mean changing your summer vacation plans to stay here in Canada."<!-- --></p><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/images/ic/160xn/p0l11ghw.jpg.webp 160w,https://ichef.bbci.co.uk/images/ic/240xn/p0l11ghw.jpg.webp 240w,https://ichef.bbci.co.uk/images/ic/320xn/p0l11ghw.jpg.webp 320w,https://ichef.bbci.co.uk/images/ic/480xn/p0l11ghw.jpg.webp 480w,https://ichef.bbci.co.uk/images/ic/640xn/p0l11ghw.jpg.webp 640w,https://ichef.bbci.co.uk/images/ic/800xn/p0l11ghw.jpg.webp 800w,https://ichef.bbci.co.uk/images/ic/1024xn/p0l11ghw.jpg.webp 1024w,https://ichef.bbci.co.uk/images/ic/1376xn/p0l11ghw.jpg.webp 1376w,https://ichef.bbci.co.uk/images/ic/1920xn/p0l11ghw.jpg.webp 1920w" src="https://ichef.bbci.co.uk/images/ic/480xn/p0l11ghw.jpg.webp" loading="lazy" alt="Alamy Justin Trudeau recently urged Canadians to stay in Canada instead of travelling to the US (Credit: Alamy)"><span>Alamy</span></p></div><p data-component="caption-block"><figcaption>Justin Trudeau recently urged Canadians to stay in Canada instead of travelling to the US (Credit: Alamy)<!-- --></figcaption></p></figure><p>The appeal seems to have caught on, as many infuriated Canadians are now <!-- --><a target="_blank" href="https://www.wsj.com/lifestyle/travel/trump-canada-vacation-travel-plans-1428d574">boycotting US holidays<!-- --></a>. In February, border crossings were down by more than 20%, <!-- --><a target="_blank" href="https://www150.statcan.gc.ca/n1/daily-quotidien/250310/dq250310d-eng.htm">according to Statistics Canada<!-- --></a>. The US Travel Association estimates that even a 10% reduction in Canadian visitors could result in $2.1bn in lost spending and 14,000 job losses.<!-- --></p><p>While some Canadians are snubbing the US because of policy changes, others say they simply don't feel as safe as they once did.<!-- --></p><p>"My partner and I decided not to go ahead with our planned vacations to the US this year," said Canadian travel journalist Kate Dingwall. "I worry about the border and getting stuck somehow, especially with how prickly Trump is to Canada. There's just a sense of uneasiness around visiting America at the moment."&nbsp;<!-- --></p><p>Keith Serry, a writer and comedian based in Montreal, Quebec, cancelled five April appearances in New York City (including four shows at the upcoming <!-- --><a target="_blank" href="https://frigid.nyc/new-york-city-fringe/">New York City Fringe<!-- --></a> festival) due to the tense political situation.<!-- --></p><p>"This decision will, of course, rob me of the opportunity to share my art with many of you in New York I've grown to know and love," he wrote on his <!-- --><a target="_blank" href="https://www.facebook.com/volumeknob/posts/pfbid0eCymJvBJNoTt2D5zM45upv4RznMsqN6REKag4eEdE4h3umj7CVPhVuZ65KVxJmBpl?rdid=aN7hg2gEiAusfGtb">Facebook page<!-- --></a>. "That said, the honest truth is that I just don't feel safe travelling to the States right now. In addition, I feel a powerful disinclination to spending my money in any way that might aid the economy of a hostile state."<!-- --></p><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/images/ic/160xn/p0l11gmr.jpg.webp 160w,https://ichef.bbci.co.uk/images/ic/240xn/p0l11gmr.jpg.webp 240w,https://ichef.bbci.co.uk/images/ic/320xn/p0l11gmr.jpg.webp 320w,https://ichef.bbci.co.uk/images/ic/480xn/p0l11gmr.jpg.webp 480w,https://ichef.bbci.co.uk/images/ic/640xn/p0l11gmr.jpg.webp 640w,https://ichef.bbci.co.uk/images/ic/800xn/p0l11gmr.jpg.webp 800w,https://ichef.bbci.co.uk/images/ic/1024xn/p0l11gmr.jpg.webp 1024w,https://ichef.bbci.co.uk/images/ic/1376xn/p0l11gmr.jpg.webp 1376w,https://ichef.bbci.co.uk/images/ic/1920xn/p0l11gmr.jpg.webp 1920w" src="https://ichef.bbci.co.uk/images/ic/480xn/p0l11gmr.jpg.webp" loading="lazy" alt="Alamy Some international travellers have been inexplicably detained by US officials (Credit: Alamy)"><span>Alamy</span></p></div><p data-component="caption-block"><figcaption>Some international travellers have been inexplicably detained by US officials (Credit: Alamy)<!-- --></figcaption></p></figure><p>In recent months, some Canadians have faced tougher border security than ever before – even those with clean records and valid documents. One woman made international headlines as <!-- --><a target="_blank" href="https://www.theguardian.com/us-news/2025/mar/19/canadian-detained-us-immigration-jasmine-mooney">she was detained by ICE for two weeks<!-- --></a> in reportedly bleak conditions after her visa was revoked.<!-- --></p><p>"We're a country that prides itself, imperfectly but intentionally, on values like inclusion, equity and human rights. When those values feel out of step with what's happening across the border, it becomes harder to justify participation," said Amar Charles Marouf, a Canadian citizen who works as growth strategist at the international law firm Gowling WLG. "The political climate raises broader questions. What kind of treatment are we normalising? What assumptions are being made about who is welcome and who isn’t?"<!-- --></p><p>It's not that Canadians aren't traveling internationally; many are simply swapping their traditional destinations. Marouf says Mexico, South America and Europe all feel more welcoming at the moment. "California will be there when this is all over, but I just feel like there's less risk going to Portugal instead," added Dingwall.<!-- --></p><p>Some international properties have seen a bump in traffic, directly attributable to Canadians now spurning the US. "In the last several weeks, we've received 10-plus leads from Canadians seeking to relocate events from the United States this summer," said Diarmaid O'Sullivan, director of sales &amp; marketing at <!-- --><a target="_blank" href="https://www.thehamiltonprincess.com/">Hamilton Princess Hotel &amp; Beach Club Bermuda<!-- --></a>. "This is a mix of leisure travellers, including those planning weddings, and companies relocating business events. This represents an approximately 20% increase in forecasted revenue from the Canadian market."<!-- --></p><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/images/ic/160xn/p0l11gtf.jpg.webp 160w,https://ichef.bbci.co.uk/images/ic/240xn/p0l11gtf.jpg.webp 240w,https://ichef.bbci.co.uk/images/ic/320xn/p0l11gtf.jpg.webp 320w,https://ichef.bbci.co.uk/images/ic/480xn/p0l11gtf.jpg.webp 480w,https://ichef.bbci.co.uk/images/ic/640xn/p0l11gtf.jpg.webp 640w,https://ichef.bbci.co.uk/images/ic/800xn/p0l11gtf.jpg.webp 800w,https://ichef.bbci.co.uk/images/ic/1024xn/p0l11gtf.jpg.webp 1024w,https://ichef.bbci.co.uk/images/ic/1376xn/p0l11gtf.jpg.webp 1376w,https://ichef.bbci.co.uk/images/ic/1920xn/p0l11gtf.jpg.webp 1920w" src="https://ichef.bbci.co.uk/images/ic/480xn/p0l11gtf.jpg.webp" loading="lazy" alt="Alamy Ironically, the US had recently been touted as the world's top tourism market (Credit: Alamy)"><span>Alamy</span></p></div><p data-component="caption-block"><figcaption>Ironically, the US had recently been touted as the world's top tourism market (Credit: Alamy)<!-- --></figcaption></p></figure><p>Interestingly, all of this is happening just as the US had been experiencing record growth in the travel and tourism sector. Marta Soligo, professor at UNLV and director of Tourism Research at the UNLV Office of economic development, points to The World Travel &amp; Tourism Council (WTTC)'s <!-- --><a target="_blank" href="https://wttc.org/research/economic-impact">2024 Economic Impact Trends Report<!-- --></a>, which showed <!-- --><a target="_self" href="https://www.bbc.com/travel/article/20240617-why-the-us-is-the-top-country-for-tourism-in-2024">the US as the world's top travel and tourism market<!-- --></a>. Not only did the industry achieve an unprecedented economic impact of $2.36tn in the US, but the US Bureau of Labor Statistics projections indicate that the US economy will add more than 800,000 jobs in the leisure and hospitality sector.<!-- --></p><p>"Such numbers help us understand the importance of the sector in the US, showing that a decrease in tourism from top-origin countries could significantly damage the US economy," said Soligo. "The consequences of these issues can impact both large US-based corporations, such as hotel chains, and small businesses alike."&nbsp;<!-- --></p><p>The change in travel patterns is already having a tangible effect on the US economy. <!-- --><a target="_blank" href="https://www.tourismeconomics.com/press/latest-research/escalating-trade-war-threatens-us-travel-sector/">Tourism Economics<!-- --></a> recently updated its inbound US travel forecast from a predicted 8.8% growth, to a 5.1% decline, attributing the change to "strained" travel sentiment, "sweeping tariffs" and "exchange rate shifts" making travel to the US more expensive.&nbsp;<!-- --></p><p>"The combination of travel bans and a reduction of US travel could have a material impact on tourism and economic development," said Jeff Le, former deputy cabinet secretary for the State of California and the state's federal coordinator during the first Trump Administration.<!-- --></p><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/images/ic/160xn/p0l11gzq.jpg.webp 160w,https://ichef.bbci.co.uk/images/ic/240xn/p0l11gzq.jpg.webp 240w,https://ichef.bbci.co.uk/images/ic/320xn/p0l11gzq.jpg.webp 320w,https://ichef.bbci.co.uk/images/ic/480xn/p0l11gzq.jpg.webp 480w,https://ichef.bbci.co.uk/images/ic/640xn/p0l11gzq.jpg.webp 640w,https://ichef.bbci.co.uk/images/ic/800xn/p0l11gzq.jpg.webp 800w,https://ichef.bbci.co.uk/images/ic/1024xn/p0l11gzq.jpg.webp 1024w,https://ichef.bbci.co.uk/images/ic/1376xn/p0l11gzq.jpg.webp 1376w,https://ichef.bbci.co.uk/images/ic/1920xn/p0l11gzq.jpg.webp 1920w" src="https://ichef.bbci.co.uk/images/ic/480xn/p0l11gzq.jpg.webp" loading="lazy" alt="Alamy International travellers to California contributed $24bn in 2023 (Credit: Alamy)"><span>Alamy</span></p></div><p data-component="caption-block"><figcaption>International travellers to California contributed $24bn in 2023 (Credit: Alamy)<!-- --></figcaption></p></figure><p>In California alone, international visitors provided more than $24bn in local economies in 2023, reports Le. "Losing this would likely hurt both people and local government funds," he said. "It is harder to measure reputational loss, but I cannot imagine active boycotts help."<!-- --></p><p>While the broad impact of boycotting travel to the US could have substantial national and governmental effects, some advocacy organisations also warn that the loss of tourism dollars is most likely to be felt by individuals.<!-- --></p><p>"It's US workers and small businesses who get affected by US travel boycotts," said Thomas F Goodwin, leader of the <!-- --><a target="_blank" href="https://www.exhibitionsconferencesalliance.org/">Exhibitions and Conferences Alliance<!-- --></a>. Goodwin notes that more than 99% of the US business and professional events industry is made up of small businesses, and more than 80% of all exhibitors are US and international small businesses. "When international business travellers forgo coming to the US, everyone from exposition booth builders and general service contractors to venue caterers and individual skilled labourers suffer – not politicians or the government. Boycotts have a knock-on effect on hotels, taxis, restaurants, local entertainers, high street commerce and more."<!-- --></p><p>There is also a cost that is harder to quantify: the global influence of American culture.<!-- --></p><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/images/ic/160xn/p0l11h6t.jpg.webp 160w,https://ichef.bbci.co.uk/images/ic/240xn/p0l11h6t.jpg.webp 240w,https://ichef.bbci.co.uk/images/ic/320xn/p0l11h6t.jpg.webp 320w,https://ichef.bbci.co.uk/images/ic/480xn/p0l11h6t.jpg.webp 480w,https://ichef.bbci.co.uk/images/ic/640xn/p0l11h6t.jpg.webp 640w,https://ichef.bbci.co.uk/images/ic/800xn/p0l11h6t.jpg.webp 800w,https://ichef.bbci.co.uk/images/ic/1024xn/p0l11h6t.jpg.webp 1024w,https://ichef.bbci.co.uk/images/ic/1376xn/p0l11h6t.jpg.webp 1376w,https://ichef.bbci.co.uk/images/ic/1920xn/p0l11h6t.jpg.webp 1920w" src="https://ichef.bbci.co.uk/images/ic/480xn/p0l11h6t.jpg.webp" loading="lazy" alt="Alamy Hotels, restaurants and local businesses are likely to feel the effect of travel boycotts (Credit: Alamy)"><span>Alamy</span></p></div><p data-component="caption-block"><figcaption>Hotels, restaurants and local businesses are likely to feel the effect of travel boycotts (Credit: Alamy)<!-- --></figcaption></p></figure><p>"Visitor numbers declining in the US sends a signal that the US is losing its soft power – the influence it once held through openness, cultural leadership and global goodwill," said Neri Karra Sillaman, entrepreneurship expert at the University of Oxford and author of the forthcoming book <!-- --><a target="_blank" href="https://www.pioneersbook.com/">Pioneers: 8 Principles of Business Longevity from Immigrant Entrepreneurs<!-- --></a>. "If this trend continues, it may force tourism boards or even local governments to create counter narratives to win back trust. I am not sure if it will be effective, however, since at the end of the day, it is the policies of the government that give the ultimate direction and influence perception."&nbsp;<!-- --></p><p>Sillaman notes that travel boycotts also have a long-term cost to culture, the economy and overall innovation. "When academics, scientists, artists, designers and entrepreneurs start to choose other countries instead of US, the US is going to lose more than just visitors," she said. "In the long term, it's going to lose its competitiveness, goodwill and it will turn into a closed society that stifles growth and innovation."<!-- --></p><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/images/ic/160xn/p0l11hgh.jpg.webp 160w,https://ichef.bbci.co.uk/images/ic/240xn/p0l11hgh.jpg.webp 240w,https://ichef.bbci.co.uk/images/ic/320xn/p0l11hgh.jpg.webp 320w,https://ichef.bbci.co.uk/images/ic/480xn/p0l11hgh.jpg.webp 480w,https://ichef.bbci.co.uk/images/ic/640xn/p0l11hgh.jpg.webp 640w,https://ichef.bbci.co.uk/images/ic/800xn/p0l11hgh.jpg.webp 800w,https://ichef.bbci.co.uk/images/ic/1024xn/p0l11hgh.jpg.webp 1024w,https://ichef.bbci.co.uk/images/ic/1376xn/p0l11hgh.jpg.webp 1376w,https://ichef.bbci.co.uk/images/ic/1920xn/p0l11hgh.jpg.webp 1920w" src="https://ichef.bbci.co.uk/images/ic/480xn/p0l11hgh.jpg.webp" loading="lazy" alt="Alamy Fewer travellers suggest the US may be losing its power and influence (Credit: Alamy)"><span>Alamy</span></p></div><p data-component="caption-block"><figcaption>Fewer travellers suggest the US may be losing its power and influence (Credit: Alamy)<!-- --></figcaption></p></figure><p>This human impact of boycotts also shows up in a myriad of ways, noted Missouri lawyer John Beck. "I've worked with hundreds of non-citizens over the last 15 years and seen firsthand how policies between governments can result in individuals – regular travellers, students, families – getting caught in the middle," he said. "It's rarely about the politics. It's about missed weddings, lost business deals or being unable to visit a dying parent."<!-- --></p><p>Beck says he has worked with at least 80 clients who have either delayed or cancelled US travel due to real or perceived hostility over the past few years. This causes complications with split families, international companies who can't move talent quickly and lost business due to the lack of ease of movement. Even though the geopolitical stakes are high, there is a human aspect on both sides of the border.<!-- --></p><p>"Most Americans have little to do with the policies that frustrate global travellers," he said. "They just want safety, fairness and dignity. That's what we need to protect."<!-- --></p><p>--<!-- --></p><p><i id="if-you-liked-this-story,">If you liked this story, <!-- --></i><a target="_self" href="https://cloud.email.bbc.com/SignUp10_08?&amp;at_bbc_team=studios&amp;at_medium=Onsite&amp;at_objective=acquisition&amp;at_ptr_name=bbc.com&amp;at_link_origin=featuresarticle&amp;at_campaign=essentiallist&amp;at_campaign_type=owned&amp;&amp;&amp;&amp;&amp;"><b id="sign-up-for-the-essential-list-newsletter"><i id="sign-up-for-the-essential-list-newsletter">sign up for The Essential List newsletter<!-- --></i></b></a><i id="–-a-handpicked-selection-of-features,-videos-and-can't-miss-news,-delivered-to-your-inbox-twice-a-week."> – a handpicked selection of features, videos and can't-miss news, delivered to your inbox twice a week.<!-- --></i>&nbsp;<!-- --></p><p><i id="for-more-travel-stories-from-the-bbc,-follow-us-on">For more Travel stories from the BBC, follow us on <!-- --></i><a target="_blank" href="https://www.facebook.com/BBCTravel/"><b id="facebook"><i id="facebook">Facebook<!-- --></i></b></a><i id=",">, <!-- --></i><a target="_blank" href="https://twitter.com/BBC_Travel"><b id="x"><i id="x">X<!-- --></i></b></a><i id="and"> and <!-- --></i><a target="_blank" href="https://www.instagram.com/bbc_travel/"><b id="instagram"><i id="instagram">Instagram<!-- --></i></b></a><i id=".">.<!-- --></i>&nbsp;<!-- --></p></article></div>]]></description>
        </item>
    </channel>
</rss>