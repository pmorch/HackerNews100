<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 28 Jan 2025 05:30:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Machine Learning in Production (CMU Course) (149 pts)]]></title>
            <link>https://mlip-cmu.github.io/s2025/</link>
            <guid>42847834</guid>
            <pubDate>Tue, 28 Jan 2025 01:18:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mlip-cmu.github.io/s2025/">https://mlip-cmu.github.io/s2025/</a>, See on <a href="https://news.ycombinator.com/item?id=42847834">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="container">

<h2>Machine Learning in Production (17-445/17-645/17-745) / AI Engineering (11-695)</h2>
<h3>Spring 2025</h3>
<p><em>CMU course that covers how to build, deploy, assure, and maintain software products with machine-learned models. Includes the entire lifecycle from a prototype ML model to an entire system deployed in production. Covers also <strong>responsible AI</strong> (including safety, security, fairness, explainability) and <strong>MLOps</strong>. For earlier offerings see websites for&nbsp;<a href="https://ckaestne.github.io/seai/F2019">Fall 2019</a>,&nbsp;<a href="https://ckaestne.github.io/seai/S2020">Summer 2020</a>, <a href="https://ckaestne.github.io/seai/F2020/">Fall 2020</a>, <a href="https://ckaestne.github.io/seai/S2021/">Spring 2021</a>&nbsp;&nbsp;<a href="https://ckaestne.github.io/seai/S2022/">Spring 2022</a>, <a href="https://ckaestne.github.io/seai/F2022/">Fall 2022</a>,&nbsp;<a href="https://github.com/mlip-cmu/s2023">Spring 2023</a>, <a href="https://github.com/mlip-cmu/s2024">Spring 2024</a>, and <a href="https://github.com/mlip-cmu/f2024">Fall 2024</a>. This Spring 2025 offering is designed for students with some data science experience (e.g., has taken a machine learning course, has used sklearn) and basic programming skills (e.g., basic Python programming with libraries, can navigate a Unix shell), but will not expect a software engineering background (i.e., experience with testing, requirements, architecture, process, or teams is not required). Going forward we expect to offer this course at least every spring semester and possibly some fall semesters (not summer semesters).</em></p>
<p><strong>Waitlist: we often cannot accommodate all interested students in Spring semesters, though we expect there to be waitlist movement.  We encourage students who are able to move to alternative labs with space, to do so.  For students enrolled in 17-XXX numbers, contact Jenni Cooper (<a href="mailto:cooperj@andrew.cmu.edu">cooperj@andrew.cmu.edu</a>) for assistance; for students enrolled in 11-XXX numbers, contact Amber Vivis (<a href="mailto:albrown@andrew.cmu.edu">albrown@andrew.cmu.edu</a>) and Karen Kirk (<a href="mailto:karensuk@andrew.cmu.edu">karensuk@andrew.cmu.edu</a>).  Note that the instructors cannot help with waitlist/registration movement, please contact the course admins instead!</strong>  </p>
<hr>
<p>For researchers, educators, or others interested in this topic, we share all course material, including slides and assignments, under a creative commons license on GitHub (<a href="https://github.com/mlip-cmu">https://github.com/mlip-cmu</a>) and have also published a <a href="https://mlip-cmu.github.io/book/">textbook</a> with chapters corresponding to almost every lecture. A while ago we also wrote an article describing the rationale and the initial design of this course: <a href="https://arxiv.org/abs/2001.06691">Teaching Software Engineering for AI-Enabled Systems</a>. Video recordings of the Summer 2020 offering are online on the <a href="https://ckaestne.github.io/seai/S2020/#course-content">course page</a>, though they are a bit outdated by now. We would be happy to see this course or a similar version taught at other universities. See also an <a href="https://github.com/ckaestne/seaibib">annotated bibliography</a> on research in this field.</p>
<h2>Course Description</h2>
<p>This is a course for those who want to build <strong>software products</strong> with <strong>machine learning</strong>, not just models and demos. We assume that you can train a model or build prompts to make predictions, but what does it take to turn the model into a product and actually deploy it, have confidence in its quality, and successfully operate and maintain it at scale? </p>
<p>The course is designed to establish a working relationship between <strong>software engineers</strong> and <strong>data scientists</strong>: both contribute to building ML-enabled systems but have different expertise and focuses. To work together they need a mutual understanding of their roles, tasks, concerns, and goals and build a working relationship. This course is aimed at <strong>software engineers</strong> who want to build robust and responsible products meeting the specific challenges of working with ML components and at <strong>data scientists</strong> who want to understand the requirements of the model for production use and want to facilitate getting a prototype model into production; it facilitates communication and collaboration between both roles. The course is a good fit for student looking at a career as an <strong>ML engineer</strong>. <em>The course focuses on all the steps needed to turn a model into a production system in a responsible and reliable manner.</em></p>
<p><img src="https://mlip-cmu.github.io/s2025/overview.svg" alt="Course overview"></p>
<p>It covers topics such as:</p>
<ul>
<li><strong>How to design for wrong predictions the model may make?</strong> How to assure <em>safety</em> and <em>security</em> despite possible mistakes? How to design the <em>user interface</em> and the entire system to operate in the real world?</li>
<li><strong>How to reliably deploy and update models in production?</strong> How can we <em>test</em> the entire machine learning pipeline? How can <em>MLOps</em> tools help to automate and scale the deployment process? How can we <em>experiment in production</em> (A/B testing, canary releases)? How do we detect <em>data quality</em> issues, <em>concept drift</em>, and <em>feedback loops</em> in production?</li>
<li><strong>How to scale production ML systems?</strong> How do we design a system to process huge amounts of training data, telemetry data, and user requests? Should we use stream processing, batch processing, lambda architecture, or data lakes?</li>
<li><strong>How to test and debug production ML systems?</strong> How can we <em>evaluate</em> the quality of a model’s predictions in production? How can we <em>test</em> the entire ML-enabled system, not just the model? What lessons can we learn from <em>software testing</em>, <em>automated test case generation</em>, <em>simulation</em>, and <em>continuous integration</em> for testing for production machine learning?</li>
<li><strong>Which qualities matter beyond a model’s prediction accuracy?</strong> How can we identify and measure important quality requirements, including <em>learning and inference latency, operating cost, scalability, explainablity, fairness, privacy, robustness</em>, and <em>safety</em>? Does the application need to be able to <em>operate offline</em> and how often do we need to update the models? How do we identify what’s important in a ML-enabled product in a production setting for a business? How do we resolve <em>conflicts</em> and <em>tradeoffs</em>?</li>
<li><strong>How to work effectively in interdisciplinary teams?</strong> How can we bring data scientists, software engineers, UI designers, managers, domain experts, big data specialists, operators, legal council, and other roles together and develop a <em>shared understanding</em> and <em>team culture</em>?</li>
</ul>
<p><strong>Examples and case studies</strong> of ML-driven products we discuss include automated audio transcription; distributed detection of missing children on webcams and instant translation in augmented reality; cancer detection, fall detection, COVID diagnosis, and other smart medical and health services; automated slide layout in Powerpoint; semi-automated college admissions; inventory management; smart playlists and movie recommendations; ad fraud detection; delivery robots and smart driving features; and many others.</p>
<p>An extended group project focuses on building, deploying, evaluating, and maintaining a robust and scalable <em>movie recommendation service</em> under somewhat realistic “production” conditions with 1 million users.</p>
<h3>Learning Outcomes</h3>
<p>After taking this course, among others, students should be able to</p>
<ul>
<li>analyze tradeoffs for designing production systems with ML-components, analyzing various qualities beyond accuracy such as operation cost, latency, updateability, and explainability</li>
<li>plan for mistakes in ML components and implement production-quality systems that are robust to those mistakes</li>
<li>design fault-tolerant and scalable data infrastructure for learning models, serving models, versioning, and experimentation</li>
<li>ensure quality of the entire machine learning pipeline with test automation and other quality assurance techniques, including automated checks for data quality, data drift, feedback loops, and model quality</li>
<li>build systems that can be tested and monitored in production and build robust deployment pipelines</li>
<li>consider system-level requirements such as safety, security, privacy, fairness, and usability when building complex ML-enabled products</li>
<li>communicate effectively in interdisciplinary teams</li>
</ul>
<p>In addition, students will gain familiarity with production-quality infrastructure tools, including stream processing with Apache Kafka, test automation with Jenkins, monitoring with Prometheus and Grafana, and deployment with Docker and various MLOps tools.</p>
<h2>Logistics and People</h2>
<p>17-445/17-645/17-745, 12 Units</p>
<p>The course is the same under all course numbers, except for the PhD-level 17-745 number, which replaces two homework assignments with a mandatory <a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/research_project.md">research project</a>.</p>
<p>Open to all undergraduate and graduate students meeting the prerequisites.</p>
<h3>Spring 2025</h3>
<p>Lectures Monday/Wednesday 2:00-3:20pm, in person, PH 100</p>
<p>Labs Friday 9:30-10:50am in PH 226C (A) and SH 236 (B) and 11-12:20pm in PH A22 (C) and PH 226A (D) and 2-3:20 in PH 226C (E) and TEP 1308 (F). There is also a remote only lab (G), Friday 11:00-12:20 pm. </p>
<p>Instructors: <a href="https://www.cs.cmu.edu/~clegoues">Claire Le Goues</a> and <a href="https://austinhenley.com/">Austin Henley</a></p>
<p>TAs: </p>
<h3>Coordination</h3>
<p>We are happy to answer questions by email and over Slack, meet in person, and will jump on a quick Zoom call if you ask us. We also always arrive 5 to 10 min early to class and stay longer for discussions and questions. If you have questions about assignments and logistics, we prefer that you ask them publicly on Slack.</p>
<h2>Course content</h2>
<p>The general course content has been fairly stable over the last few years, though specific topics and tools are constantly updated with new research and tooling. Our list of learning goals under <a href="https://github.com/mlip-cmu/s2025/blob/main/learning_goals.md">Learning Goals</a> describes what we aim to cover. Below is a table of a preliminary schedule. This is subject to change and will be updated as the semester progresses, especially to help focus on requested topics or support learning.</p>
<p>[Schedule]</p>
<table>
<thead>
<tr>
<th>Date</th>
<th>Topic</th>
<th><a href="https://mlip-cmu.github.io/book/">Book Chapter</a></th>
<th>Reading</th>
<th>Assignment due</th>
</tr>
</thead>
<tbody><tr>
<td>Mon, Jan 13</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/01_introduction/intro.html">Introduction and Motivation </a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/01_introduction/intro.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/01_introduction/intro.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/01/">1</a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Wed, Jan 15</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/02_systems/systems.html">From Models to AI-Enabled Systems</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/02_systems/systems.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/02_systems/systems.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/02/">2</a>,<a href="https://mlip-cmu.github.io/book/04/">4</a>,<a href="https://mlip-cmu.github.io/book/05/">5</a></td>
<td><a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019649190004436">Building Intelligent Systems</a>, Ch. 4, 5, 7, 8</td>
<td></td>
</tr>
<tr>
<td>Fri, Jan 17</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab01.md">Calling, securing, and creating APIs: Flask</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Jan 20</td>
<td><img src="https://img.shields.io/badge/-break-red.svg" alt="Break"> MLK Day, no classes</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Wed, Jan 22</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/03_requirements/requirements.html">Gathering Requirements </a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/03_requirements/requirements.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/03_requirements/requirements.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/06/">6</a></td>
<td><a href="https://scholar.google.com/scholar?cluster=1090758480873197042">The World and the Machine</a></td>
<td></td>
</tr>
<tr>
<td>Fri, Jan 24</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab02.md">Stream processing: Apache Kafka</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Jan 27</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/04_mistakes/mistakes.html">Planning for Mistakes</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/04_mistakes/mistakes.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/04_mistakes/mistakes.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/07/">7</a></td>
<td><a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019649190004436">Building Intelligent Systems</a>, Ch. 6, 8, 24</td>
<td><a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/I1_mlproduct.md">I1: ML Product</a></td>
</tr>
<tr>
<td>Wed, Jan 29</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/05_modelaccuracy/modelquality1.html">Model Quality</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/05_modelaccuracy/modelquality1.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/05_modelaccuracy/modelquality1.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/15/">15</a></td>
<td><a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019649190004436">Building Intelligent Systems</a>, Ch. 19</td>
<td></td>
</tr>
<tr>
<td>Fri, Jan 31</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab03.md">Collaboration with git</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Feb 3</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/06_teamwork/teams.html">Fostering Interdisciplinary (Student) Teams</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/06_teamwork/teams.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/06_teamwork/teams.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td><a href="https://third-bit.com/2018/05/11/meetings/">Meetings</a></td>
<td><a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/I2_requirements.md">I2: Requirements</a></td>
</tr>
<tr>
<td>Wed, Feb 5</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/07_modeltesting/modelquality2.html">Behavioral Model Testing</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/07_modeltesting/modelquality2.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/07_modeltesting/modelquality2.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/15/">15</a></td>
<td><a href="https://aclanthology.org/2020.acl-main.442.pdf">Beyond Accuracy: Behavioral Testing of NLP Models with CheckList</a></td>
<td></td>
</tr>
<tr>
<td>Fri, Feb 7</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab04.md">Model testing</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Feb 10</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/08_architecture/tradeoffs.html">Toward Architecture and Design </a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/08_architecture/tradeoffs.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/08_architecture/tradeoffs.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/08/">8</a>,<a href="https://mlip-cmu.github.io/book/09/">9</a>,<a href="https://mlip-cmu.github.io/book/11/">11</a></td>
<td><a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019649190004436">Building Intelligent Systems, Ch. 18</a> and <a href="https://hackernoon.com/choosing-the-right-machine-learning-algorithm-68126944ce1f">Choosing the Right Machine Learning Algorithm</a></td>
<td></td>
</tr>
<tr>
<td>Wed, Feb 12</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/09_deploying_a_model/deployment.html">Deploying a Model</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/09_deploying_a_model/deployment.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/09_deploying_a_model/deployment.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/10/">10</a></td>
<td><a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019649190004436">Building Intelligent Systems</a>, Ch. 13 and <a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/1feg4j8/alma991019735160604436">Machine Learning Design Patterns</a>, Pat. 16</td>
<td></td>
</tr>
<tr>
<td>Fri, Feb 14</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab05.md">Containers: Docker</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Feb 17</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/10_qainproduction/qainproduction.html">Testing and Experimenting in Production</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/10_qainproduction/qainproduction.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/10_qainproduction/qainproduction.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/19/">19</a></td>
<td><a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019649190004436">Building Intelligent Systems</a>, Chs. 14 and 15</td>
<td><a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/project.md">M1: Modeling and First Deployment</a></td>
</tr>
<tr>
<td>Wed, Feb 19</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/11_dataquality/dataquality.html">Data Quality</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/11_dataquality/dataquality.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/11_dataquality/dataquality.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/16/">16</a></td>
<td><a href="https://dl.acm.org/doi/abs/10.1145/3411764.3445518">Data Cascades in High-Stakes AI</a></td>
<td></td>
</tr>
<tr>
<td>Fri, Feb 21</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab06.md">Continuous Integration: Jenkins</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Feb 24</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/12_pipelinequality/pipelinequality.html">Automating and Testing ML Pipelines</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/12_pipelinequality/pipelinequality.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/12_pipelinequality/pipelinequality.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/11/">11</a>,<a href="https://mlip-cmu.github.io/book/18/">18</a>,<a href="https://mlip-cmu.github.io/book/19/">19</a></td>
<td><a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46555.pdf">The ML Test Score</a></td>
<td></td>
</tr>
<tr>
<td>Wed, Feb 26</td>
<td><img src="https://img.shields.io/badge/-midterm-blue.svg" alt="Midterm"> Midterm 1</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Fri, Feb 28</td>
<td><img src="https://img.shields.io/badge/-break-red.svg" alt="Break"> No lab (happy spring break)</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Mar 3</td>
<td><img src="https://img.shields.io/badge/-break-red.svg" alt="Break"> Spring break, no classes</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Wed, Mar 5</td>
<td><img src="https://img.shields.io/badge/-break-red.svg" alt="Break"> Spring break, no classes</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Fri, Mar 7</td>
<td><img src="https://img.shields.io/badge/-break-red.svg" alt="Break"> Spring break, no classes</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Mar 10</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/13_dataatscale/dataatscale.html">Scaling the System</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/13_dataatscale/dataatscale.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/13_dataatscale/dataatscale.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/12/">12</a></td>
<td><a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019577936304436">Big Data: Principles and best practices of scalable realtime data systems</a>, Ch. 1</td>
<td></td>
</tr>
<tr>
<td>Wed, Mar 12</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/14_operations/operations.html">Planning for Operations</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/14_operations/operations.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/14_operations/operations.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/13/">13</a></td>
<td><a href="https://arxiv.org/abs/2209.09125">Operationalizing machine learning: An interview study</a></td>
<td></td>
</tr>
<tr>
<td>Fri, Mar 14</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab07.md">Monitoring: Prometheus, Grafana</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Mar 17</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/15_provenance/provenance.html">Versioning, Provenance, and Reproducability</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/15_provenance/provenance.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/15_provenance/provenance.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/24/">24</a></td>
<td><a href="http://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf">Hidden Technical Debt in Machine Learning Systems</a></td>
<td><a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/project.md">M2: Infrastructure Quality</a></td>
</tr>
<tr>
<td>Wed, Mar 19</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/16_process/process.html">Process &amp; Technical Debt</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/16_process/process.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/16_process/process.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/20/">20</a></td>
<td><a href="https://arxiv.org/pdf/2110.10234.pdf">Collaboration Challenges in Building ML-Enabled Systems</a></td>
<td></td>
</tr>
<tr>
<td>Fri, Mar 21</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab08.md">Pipeline automation: MLFlow</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Mar 24</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/17_intro_ethics_fairness/intro-ethics-fairness.html">Intro to Ethics + Fairness</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/17_intro_ethics_fairness/intro-ethics-fairness.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/17_intro_ethics_fairness/intro-ethics-fairness.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/23/">23</a>,<a href="https://mlip-cmu.github.io/book/26/">26</a></td>
<td><a href="https://datasociety.net/wp-content/uploads/2018/04/Data_Society_Algorithmic_Accountability_Primer_FINAL-4.pdf">Algorithmic Accountability: A Primer</a></td>
<td><a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/I3_mlops_tools.md">I3: MLOps Tools</a></td>
</tr>
<tr>
<td>Wed, Mar 26</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/18_fairness_measures/model_fairness.html">Measuring Fairness</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/18_fairness_measures/model_fairness.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/18_fairness_measures/model_fairness.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/26/">26</a></td>
<td><a href="https://dl.acm.org/doi/pdf/10.1145/3178876.3186138">Human Perceptions of Fairness in Algorithmic Decision Making</a></td>
<td></td>
</tr>
<tr>
<td>Fri, Mar 28</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab09.md">Container orchestration: Kubernetis</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Mar 31</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/19_system_fairness/system_fairness.html">Building Fairer Systems</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/19_system_fairness/system_fairness.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/19_system_fairness/system_fairness.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/26/">26</a></td>
<td><a href="http://users.umiacs.umd.edu/~hal/docs/daume19fairness.pdf">Improving Fairness in Machine Learning Systems</a></td>
<td></td>
</tr>
<tr>
<td>Wed, Apr 2</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/20_explainability/explainability.html">AVAILABLE</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/20_explainability/explainability.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/20_explainability/explainability.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/25/">25</a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Fri, Apr 4</td>
<td><img src="https://img.shields.io/badge/-break-red.svg" alt="Break"> Carnival, no classes</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Apr 7</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/21_transparency/transparency.html">Explainability</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/21_transparency/transparency.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/21_transparency/transparency.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/29/">29</a></td>
<td><a href="https://dataskeptic.com/blog/episodes/2020/black-boxes-are-not-required">Interpretability Podcast</a> or <a href="https://arxiv.org/abs/1811.10154">equivalent artice</a></td>
<td><a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/project.md">M3: Monitoring and CD</a></td>
</tr>
<tr>
<td>Wed, Apr 9</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/22_security/security.html">Transparency &amp; Accountability</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/22_security/security.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/22_security/security.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/28/">28</a></td>
<td><a href="https://pair.withgoogle.com/chapter/explainability-trust/">Google chapter on Explainability and Trust</a></td>
<td></td>
</tr>
<tr>
<td>Fri, Apr 11</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab10.md">Model Explainability Tools</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Apr 14</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/23_safety/safety.html">Security and Privacy</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/23_safety/safety.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/23_safety/safety.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/27/">27</a></td>
<td><a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019649190004436">Building Intelligent Systems</a>, Ch. 25, and <a href="https://canvas.cmu.edu/courses/45008/files/12156923/download?wrap=1">The Top 10 Risks of Machine Learning Security</a></td>
<td><a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/I4_explainability.md">I4: Explainability</a></td>
</tr>
<tr>
<td>Wed, Apr 16</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/24_summary/all.html">Safety</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/24_summary/all.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/24_summary/all.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td><a href="http://ceur-ws.org/Vol-2560/paper40.pdf">Practical Solutions for Machine Learning Safety in Autonomous Vehicles</a></td>
<td></td>
</tr>
<tr>
<td>Fri, Apr 18</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab11.md">LLM Jailbreaking</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Apr 21</td>
<td>Explainability Discussion / Summary / Review</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Wed, Apr 23</td>
<td><img src="https://img.shields.io/badge/-midterm-blue.svg" alt="Midterm"> Midterm 2</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Fri, Apr 25</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> No lab</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td><a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/project.md">M4: Fairness, Security and Feedback Loops</a></td>
</tr>
<tr>
<td>TBD</td>
<td>Final Project Presentations (5:30-8:30pm in GHC 4401)</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td><a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/project.md">Final report</a></td>
</tr>
</tbody></table>
<h2>Course Syllabus and Policies</h2>
<p>The course uses Canvas and Gradescope for homework submission, grading, discussion, questions, announcements, and supplementary documents; slides will be posted here; Slack is used for communication around homework and projects; Github is used to coordinate group work. All public course material (assignments, slides, syllabus) can be found in the course’s <a href="https://github.com/mlip-cmu/s2025">GitHub repository</a>; announcements and all <em>private</em> material (e.g., grades, passwords) will be shared through Canvas.</p>
<p><strong>Prerequisites:</strong> The course does not have formal prerequisites, but we describe background knowledge that will help you be successful in the course. In a nutshell, we expect basic exposure to machine learning and basic programming skills, but do not require software engineering experience. </p>
<p><em>Machine learning (some experience recommended):</em> We suggest that you have basic familiarity with the process of extracting features, building and evaluating models, and a basic understanding of how and when different kinds of learning techniques work. Familiarity with Python and Jupyter notebooks is helpful. Courses such as 10-301, 10-315, and 05-434 will prepare you well, but project experience or self-learning from books or online courses will likely be sufficient for our purposes. For example, if you have no prior experience, we recommend the book <a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019665684604436">Hands-On Machine Learning</a> to get practical experience in building and evaluating models prior to taking this course. We have set up a <em><a href="https://forms.gle/JcS61Uao7wHSFQen8">prerequisite knowledge check</a></em> as a Google Form, where we ask 10 questions on machine learning, which help you assess your background – this is set up as an anonymous and ungraded quiz, where you can compare your knowledge against what we believe is useful for you to be successful in this course (click on <em>“view score”</em> after submitting your answer). After submitting your answers, the system will give specific pointers to readings and exercises that may help you fill gaps in background knowledge. </p>
<p><em>Programming (basic proficiency required):</em> The course has a substantial programming component, especially in the first assignment and the team project, so basic programming skills will be needed. If you take the course without programming experience, you will significantly struggle and it may cause conflicts within the group project. We expect that you meet the following criteria: (1) basic fluency in a programming language like Python, (2) ability to install and learn libraries in that language, (3) ability to ssh into a Unix machine and perform basic command line operations, and (4) ability to install and learn new tools like Docker. We do not prescribe a programming language, but almost all student teams decide to work primarily in Python. We will provide some introductions and examples for essential tools like Git, Docker, Grafana, and Jenkins in labs, but we expect that you will be able to pick up new tools and libraries on your own. For example, we expect that you will be able, on your own, to learn basic use of a library like <a href="https://flask.palletsprojects.com/en/2.1.x/">Flask</a> to write a web service. Throughout the semester, expect to read lots of documentation and tutorials to learn various libraries and tools on your own. If you are worried whether your technical background is sufficient, we recommend that you look at (or even try) <a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/I1_mlproduct.md">homework I1</a> before the semester.</p>
<p><em>Software engineering (no experience required):</em> Many students will have some software engineering experience beyond basic programming skills from software engineering courses, from internships, or from working in industry, for example experience with requirements engineering, software design, software testing, distributed systems, continuous deployment, or managing teams. No such experience is expected as a prerequisite; we will cover these topics in the course.</p>
<p>Email the instructors if you would like to further talk to us about prerequisites.</p>
<p><strong>In-person teaching and lecture recordings:</strong> The course will be taught in person.  We consider in-class participation an important part of the learning experience. We <em>do</em> make <em>best effort</em> lecture recordings, which will be available in Canvas.  We do <em>not</em> provide a synchronous remote option, and we do not record labs.  You are welcome to use recordings to make up missed lectures and review material. However, absent extenuating circumstances (see below), viewing the recording will not make up for missed in-class activities.  </p>
<p>We regularly use Slack for in-class activities. Please make sure that you have access to Slack on a laptop, tablet, or mobile phone during class.</p>
<p>If you cannot attend class due to a medical issue, family emergency, interview, or other unforeseeable reason, please contact us about possible accommodations. We try to be as flexible as we can, but will handle these cases individually.</p>
<p><strong>Exams:</strong> The course has two midterms and a final project presentation, but no final exam. We typically use the registrar-assigned final exam timeslot (to be announced about halfway through the semester <a href="https://www.cmu.edu/hub/docs/final-exams.pdf">here</a>) for the final project presentation. The midterms are during the normal class period as per schedule. The second midterm is not comprehensive, and only covers material after the first midterm. Examples of past midterms can be found in the <a href="https://github.com/mlip-cmu/s2025/tree/main/exams">course repository</a>.</p>
<p><strong>Grading:</strong> Evaluation will be based on the following distribution: 35% individual assignments, 30% group project, 15% midterms, 5% participation, 10% labs, 5% reading quizzes. No final exam.</p>
<p>We strive to provide clear specifications and clear point breakdowns for all homework to set clear expectations and take the guessing out of homework. We often give you choices to self-direct your learning, deciding what to work on and how to address a problem (e.g., we never prescribe a programming language and often give choices to answer a subset of possible questions). Clear specifications and point breakdowns allow you to intentionally decide to skip parts of assignments with clear upfront consequences. All parts will be graded pass/fail, no partial credit. For opportunities to redo work, see <em>resubmissions</em> below. For grading participation and quizzes see below. Some assignments have a small amount of bonus points. </p>
<p>Since we give flexibility to resubmit assignments, we set grade boundaries fairly high. We expect the following grade boundaries:</p>
<table>
<thead>
<tr>
<th>Grade</th>
<th>Cutoff</th>
</tr>
</thead>
<tbody><tr>
<td>A+</td>
<td>&gt;99%</td>
</tr>
<tr>
<td>A</td>
<td>&gt;96%</td>
</tr>
<tr>
<td>A-</td>
<td>&gt;94%</td>
</tr>
<tr>
<td>B+</td>
<td>&gt;91%</td>
</tr>
<tr>
<td>B</td>
<td>&gt;86%</td>
</tr>
<tr>
<td>B-</td>
<td>&gt;82%</td>
</tr>
<tr>
<td>C</td>
<td>&gt;75%</td>
</tr>
<tr>
<td>D</td>
<td>&gt;60%</td>
</tr>
</tbody></table>
<p><strong>Participation:</strong> Design and engineering content requires active engagement with the material and discussions of judgment decisions on specific scenarios and cases. We strongly believe in in-class discussions and in-class exercises and want all students to participate, e.g., answering or asking questions in class, sharing own experiences, presenting results, or participating in in-class votes and surveys. We will give many opportunities for participation in every lecture and lab. We note student engagement with in-class activities to include as a component in grading.  We will provide feedback at mid-semester so that you can check in on how you’re doing. Again, please talk to us if you need accommodations.</p>
<p>We assign participation grades as follows:</p>
<ul>
<li>100%: Participates actively at least once in most lectures (4 lectures waived, no questions asked)</li>
<li>90%: Participates actively at least once in two thirds of the lectures</li>
<li>75%: Participates actively at least once in over half of the lectures</li>
<li>50%: Participates actively at least once in one quarter of the lectures</li>
<li>20%: Participates actively at least once in at least 3 lectures.</li>
<li>0%: Participation in less than 3 lectures.</li>
</ul>
<p><strong>Labs:</strong> Labs typically introduce tools and have a task with one or more clear deliverables. Lab assignments are designed to take about 1h of work and can be completed before or during the lab session. Each deliverable is graded pass/fail at any time during that week's lab session by showing your work to the TA. Typically showing your work involves showing source code, demoing executions, and (verbally) answering a few questions. The TA may ask a few questions about your implementation to probe that you understand your work.</p>
<p>We intend labs to be very low stakes – this is your first practical engagement with the material and mistakes are a normal part of the learning process. Deliverables are graded pass/fail on whether they meet the stated expectations for the deliverables. If your solution does not meet the expectations you can continue working on it during the lab session until it does. Outside of explicit accommodations (e.g., medical issues) or using tokens (see below), we do not accept lab solutions after the end of the lab session.</p>
<p>We encourage collaboration on labs: You can work together with other students both before the lab session and during the lab session. While we do not recommend it, you may look at other students’ solutions and reference solutions and even copy them. However, you will have to present and explain your solution to the TA on your own.</p>
<p><strong>Textbook, reading assignments, and reading quizzes:</strong> We will be using Goeff Hulten's <em>"Building Intelligent Systems: A Guide to Machine Learning Engineering"</em> (ISBN: 1484234316) throughout much of the course. The library provides an <a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019649190004436">electronic copy</a>. In addition, we will provide various additional readings, including blog posts and academic papers, throughout the semester.</p>
<p>We also wrote our own textbook "<a href="https://mlip-cmu.github.io/book/">Machine Learning in Production</a>" that aligns closely with the lecture content. The book will be published by MIT Press and is additionally available under a creative commons license online. We will not assign chapters from our own textbook, but we always point to the corresponding chapter for each lecture, which we suggest as supplementary reading.</p>
<p>We will assign readings for most classes and post a corresponding quiz on Canvas that is due before class. Each quiz contains an open-ended question that relates to the reading. Reading quizzes are intended to be low-stakes assessments and are graded pass/fail for a good-faith effort to engage with the question. </p>
<p><strong>Teamwork:</strong> Teamwork is an essential part of this course. The course contains a multi-milestone group project to be done in teams of 3-5 students. Teams will be assigned by the instructor. A TA will serve as a mentor for each team. We will help teams throughout the semester and cover some specific content on teamwork as part of the course. Peer rating will be performed for team assignments with regard to <em>team citizenship</em> (i.e., being active and cooperative members), following a procedure adapted from <a href="https://www.cs.tufts.edu/~nr/cs257/archive/teaching/barbara-oakley/JSCL-collaboration.pdf">this article</a>, which we will further explain in an early lecture. Use <a href="https://mlip-cmu.github.io/s2025/assignments/peergrading.html">this form</a> to preview the expected adjustments for peer ratings. The team's mentor will also debrief with the team after every milestone and discuss possible strategies to improve teamwork. </p>
<p><strong>Late work policy and resubmissions:</strong> We understand that students will always have competing deadlines, unusual events, interviews for job searches, and other activities that compete with coursework. We therefore build flexibility and a safety net directly into the rubric. If you need additional accommodations, please contact us.</p>
<p>In addition, we expect that the past/fail grading scheme without partial credit, may lead to harsh point deductions for missing small parts of the requirements, so we provide a mechanism to resubmit work with a short reflection to regain lost points.</p>
<p>Every student receives <em>8 individual tokens</em> that they can spend throughout the semester in the following ways:</p>
<ul>
<li>For each token, a student can submit a homework assignment 1 day late (with 2 tokens a student can submit two homeworks one day late each or a single homework up to two days late).</li>
<li>For <em>three</em> tokens, a student can improve or redo an individual homework assignment and resubmit together with a short reflection. The earlier submission is discarded and the regraded assignment counts toward the final grade. Resubmissions can be made at any time in the semester up to the final project presentation (see schedule). – Note that this technically allows a student to blow the original deadline (no submission necessary, receiving 0 points initially) and then resubmit the homework arbitrarily late for three tokens.</li>
<li>For one token, a student can submit a reading quiz late (any time before the final presentation) or resubmit a graded reading quiz.</li>
<li>For one token, a student can complete a lab late or redo a lab (any time before the final presentation) by showing the work to a TA during office hours.</li>
<li>Remaining individual tokens at the end of the semester are counted as one participation day each.</li>
</ul>
<p>If a student runs out of tokens, late individual assignments receive a penalty of 15% per started day. Late team formation survey and teamwork peer assessment surveys do not receive any points.</p>
<p>Every team independently receives <em>8 team tokens</em> that they can spend for extensions of any milestone deadline (1 token per day per milestone, except final presentation deadline) or to resubmit any milestone with a reflection (3 tokens each, resubmitted any time before the final presentation). If a team runs out of tokens, late submissions in group assignments receive a penalty of 15% per started day.</p>
<p>Individual tokens and team tokens are entirely separate; it is not possible to use individual tokens for teamwork or vice versa. The team should make collective decisions about how to use team tokens.</p>
<p>In general, late submissions and resubmissions can be done at any point in the semester before the final presentations. Late submissions that are 1-3 days late can be made directly to Gradescope; for everything else see instructions and forms on Canvas.</p>
<p>Exceptions to this policy will be made at the discretion of the instructor in important circumstances, almost always involving a family or medical emergency and an email from your advisor — you can ask your academic advisor or the Dean of Student Affairs requesting the exception on your behalf. Where issues affect teamwork, please communicate proactively with your team.</p>
<p><strong>Communication:</strong> We make important announcements on Slack; we recommend to enable Slack notifications. We answer email and monitor Slack, which may all be used for clarifying homework assignments and other interactions. We strongly recommend to ask questions publicly on Slack if others might have similar questions. Email or slack us if you would like to make an appointment.</p>
<p><strong>Auditing:</strong> Due to the high demand for this course, we do <em>not</em> allow auditing. If you like to self-study, all course materials are online. We welcome interested students and visitors to sit in for lectures as long as the room capacity allows it. </p>
<p><strong>Time management:</strong> This is a 12-unit course, and it is our intention to manage it so that you spend close to 12 hours a week on the course, on average. In general, 3 hours/week will be spent in class, about 1 hour for the labs, 1-2 hours on readings and reading quizzes, and 6-7 hours on assignments. Notice that much homework is done in groups, so please account for the overhead and decreased time flexibility that comes with groupwork. Please give the course staff feedback if the time the course is taking for you differs significantly from our intention.</p>
<p><strong>Writing:</strong> Describing tradeoffs among decisions and communication with stakeholders from other backgrounds are key aspects of this class. Many homework assignments have a component that requires discussing issues in written form or reflecting about experiences. To practice writing skills, the Global Communications Center (GCC) offers one-on-one help for students, along with workshops. The instructors are also happy to provide additional guidance if requested.</p>
<p><strong>Use of content generation AI tools and external sources:</strong> Given the nature of this course, we are open to using AI tools for completing work. We place no restrictions on the use of content generation tools, such as ChatGPT, Bard, Co-Pilot, or Stable Diffusion. You may also reuse code from external sources, such as StackOverflow or tutorials. In any case, you will be solely responsible for the correctness of the solution. Note that content generation tools often create plausible-looking but incorrect answers, which will not receive credit. You are also responsible for complying with any applicable licenses. If you use content generation tools, we encourage you to share your experience with the course staff or the entire class.</p>
<p><strong>Academic honesty and collaboration:</strong> The usual policies apply, especially the <a href="https://www.cmu.edu/policies/student-and-student-life/academic-integrity.html">University Policy on Academic Integrity</a>. Many parts of the work will be done in groups. We expect that group members collaborate with one another, but that groups work independently from other groups, not exchanging results with other groups. Within groups, we expect that you are honest about your contribution to the group's work. This implies not taking credit for others' work and not covering for team members that have not contributed to the team. This also applies to in-class discussions, where indicating working with others who did not participate in the discussion is considered an academic honesty violation. Otherwise, our expectations regarding academic honestly and collaboration for group and pair work are the same as for individual work, substituting elevated to the level of "group."</p>
<p>Beyond that, the key guiding principle of academic honesty in this course is: <em>"You may not copy any part of a solution to a problem that was written by another student (in this or prior iterations of the class), or was developed together with another student, or was delegated to another person. You may not look at another student's solution, even if you have completed your own, nor may you knowingly give your solution to another student or leave your solution where another student can see it.</em>" Note that this implies that you cannot publicly post your solutions on GitHub (e.g., as part of a portfolio during job applications). While the use of AI content generation tools is okay (see above) using the work from other students is not. Discussing challenges and solution strategies with others at a high level is okay, sharing code or text is not.</p>
<p>You may collaborate with other students on labs, but not on reading quizzes, homeworks, and exams.</p>
<p>We also expect and respect honesty when communicating with the course staff.</p>
<p>Any violation of this policy is cheating. The minimum penalty for cheating will be a zero grade for the whole assignment. Cheating incidents will also be reported through University channels, with possible additional disciplinary action (see the University Policy on Academic Integrity). There is no statute of limitations for violations of the collaboration policy; penalties may be assessed (and referred to the university disciplinary board) after you have completed the course, and some requirements of the collaboration policy (such as restrictions on you posting your solutions) extend beyond your completion of the course.</p>
<p>If you have any question about how this policy applies in a particular situation, ask the instructors for clarification.</p>
<p><strong>Research in this Course:</strong> We are conducting academic research in this course. This research will involve analyzing student work of assignment. You will not be asked to do anything above and beyond the normal learning activities and assignments that are part of this course. You are free not to participate in this research, and your participation will have no influence on your grade for this course or your academic career at CMU. If you do not wish to participate, please send an email to Nadia Nahar (<a href="mailto:nadian@andrew.cmu.edu">nadian@andrew.cmu.edu</a>). Participants will not receive any compensation or extra credit. The data collected as part of this research will not include student grades. All analyses of data from participants’ coursework will be conducted after the course is over and final grades are submitted -- instructors will not know who chooses not to participate before final grades are submitted. All data will be analyzed in de-identified form and presented in the aggregate, without any personal identifiers. If you have questions pertaining to your rights as a research participant, or to report concerns to this study, please contact Nadia Nahar (<a href="mailto:nadian@andrew.cmu.edu">nadian@andrew.cmu.edu</a>) or the Office of Research Integrity and Compliance at Carnegie Mellon University (<a href="mailto:irb-review@andrew.cmu.edu">irb-review@andrew.cmu.edu</a>; phone: 412-268-4721).</p>
<p><strong>Accommodations for students with disabilities:</strong> If you have a disability with an accommodations letter from the Disability Resources office, we encourage you to discuss your accommodations and needs with us as early in the semester as possible. We will work with you to ensure that accommodations are provided as appropriate. If you suspect that you may have a disability and would benefit from accommodations but are not yet registered with the Office of Disability Resources, we encourage you to contact them at <a href="mailto:access@andrew.cmu.edu">access@andrew.cmu.edu</a>.</p>
<p><strong>Respect for diversity:</strong> It is our intent that students from all diverse backgrounds and perspectives be well served by this course, that students’ learning needs be addressed both in and out of class, and that the diversity that students bring to this class be viewed as a resource, strength and benefit. It is my intent to present materials and activities that are respectful of diversity: gender, sexuality, disability, age, socioeconomic status, ethnicity, race, and culture. Especially in lectures on fairness we will also cover diversity discussions, typically through a lens of the contemporary discourse in the US. Your suggestions are encouraged and appreciated. Please let us know ways to improve the effectiveness of the course for you personally or for other students or student groups. </p>
<p><strong>A note on self care.</strong> Please take care of yourself. Do your best to maintain a healthy lifestyle this semester by eating well, exercising, avoiding drugs and alcohol, getting enough sleep and taking some time to relax. This will help you achieve your goals and cope with stress. All of us benefit from support during times of struggle. You are not alone. There are many helpful resources available on campus and an important part of the college experience is learning how to ask for help. Asking for support sooner rather than later is often helpful.
If you or anyone you know experiences any academic stress, difficult life events, or feelings like anxiety or depression, we strongly encourage you to seek support. Counseling and Psychological Services (CaPS) is here to help: call 412-268-2922 and visit their website at <a href="http://www.cmu.edu/counseling/">http://www.cmu.edu/counseling/</a>. Consider reaching out to a friend, faculty or family member you trust for help getting connected to the support that can help.</p>

        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I trusted an LLM, now I'm on day 4 of an afternoon project (209 pts)]]></title>
            <link>https://nemo.foo/blog/day-4-of-an-afternoon-project</link>
            <guid>42845933</guid>
            <pubDate>Mon, 27 Jan 2025 21:37:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nemo.foo/blog/day-4-of-an-afternoon-project">https://nemo.foo/blog/day-4-of-an-afternoon-project</a>, See on <a href="https://news.ycombinator.com/item?id=42845933">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><strong>TLDR - AI isn’t a co-pilot; it’s a junior dev faking competence. Trust it at your own risk.</strong></p>
<p>I’m 4 days into an afternoon project. I was so sure I’d crush this one. I had a good plan and the stoke was high. Let me introduce <em>Deskthang</em>. It’s a thang for your desk. When I work, I want to put my phone in the other room, and only get the important notifications (thangs) in a different way. If my deployment pipeline fails, I want a globe on my desk to turn red and show me a gitlab logo. I do not want to check my phone or email or anywhere a distraction might find me.</p>
<blockquote>
<p>Quick backstory: I work full time++ doing boring enterprise software dev and rarely get to flex my engineering skills. While my title says engineer, I’d disagree.</p>
</blockquote>
<h2>The Problem I’m Trying to Solve</h2>
<p>I always try to align multiple interests for a side project. I wanted to pull my electronics hardware box out of storage, I wanted to solve the notifications and focus issue for myself, and I wanted to see how scared I should be about AI taking my job. As they say, I was trying to get a few birds stoned at once.</p>
<p><img src="https://nemo.foo/blog-content/deskthang/two_birds_stoned.gif" alt=""></p>
<h3>1. I miss working with hardware.</h3>
<p>During COVID lock-downs I landed an R&amp;D contract for a IoT Prototype. That R&amp;D job was the most fulfilling work of my career. I worked with a small, scrappy team with some of my best friends. I was 3D printing models, soldering components, writing embedded C, and field-testing with mechanical engineers… Real engineers. We worked hard, often late into the night, and the collaboration felt more like playing StarCraft with the boiz than a 9-to-5. I’ve missed that deeply ever since. Recently, I’ve been inspired recently by <a href="https://x.com/_MaxBlade">@_MaxBlade and DeskHub</a> and wanted to brush the dust off my electronics skills.</p>
<h3>2. I hate the UX of MFA (Multi Factor Authentication).</h3>
<p>I use GitLab heavily for CI/CD with my personal Kubernetes projects. Knowing the status of my pipelines is crucial… broken builds could disrupt all 7 of my users! Logging into GitLab feels like getting stabbed in the spleen. Every time I log in (multiple times a day), I face captchas, authenticator apps, or waiting for email codes, followed by yet another captcha. I’ve tried pipeline notifications through Slack, Discord, and Telegram, but those apps are like productivity black holes. I don’t want my phone near me while working, or to open chat apps that derail my focus. Removing these distractions keeps me locked in.</p>
<h3>3. I want to see how good these AI tools are.</h3>
<p>I want to figure out if AI is going to take my job. I’m skeptical it can replace what I do, but I like testing my assumptions. Sometimes AI surprises me; other times, it’s just a rabbit hole of wasted hours when I avoid doing real thinking.</p>
<p>Recently, I used Claude Sonnet 3.5 to brute-force hundreds of React compile errors while upgrading a project from React 15 to 18. I threw <code>package.json</code> updates, deleted <code>node_modules</code>, and burned through a small fortune in AI tokens. To my surprise, we had a passing build by the end of the day. Work has been encouraging us to adopt an AI-first workflow and giving us unlimited tokens. It’s a wild experiment.</p>
<p>This happened on a Friday. I wiped the sweat off my brow after a hard day’s prompting, and headed home early to start on my side project…</p>
<h2>The Plan</h2>
<p>Unlike me, my wife likes to leave the house and do things. I’ve spent a few years turning my garage into my favorite place to be. My wife and I have a deal where 1 day a month, She takes the kiddo and I am absolved of all responsibilities. I get a full day to lock in and build projects. From her perspective, I order doordash and turn into a degen who is unfit to father. From my perspective, I get to enjoy my favorite place and just tinker or play games or do whatever. These are the days I get to play mad scientist and feel most like myself. I look forward to it every month. My plan was to learn zig, brush off my hardware skills, build this project, write a blog post and make a video about it. Totally achievable.</p>
<p>I wanted to wire up a Raspberry Pi Pico, a small 240x240 LCD display and some RGB LEDs. I was going to learn Zig and use it to send image data over USB to the pico which will put an image on the screen and change the LED color. I would set up webhooks from GitLab to call an API in my Kube cluster and setup my host Zig app to poll that same API for changes and send updates to the Pico. I really wanted to transmit the data over USB because I’ve never done that before. I’ve already used Bluetooth, LTE and Wifi and just wanted to do something new.</p>
<p>The wiring is simple. Common patterns I was familiar with like <a href="https://en.wikipedia.org/wiki/Serial_Peripheral_Interface">SPI (Serial Peripheral Interface)</a> for the display + some RGB leds. The <a href="https://itsfoss.com/what-is-tty-in-linux/?utm_source=chatgpt.com">TTY (TeleTYpewriter)</a> serial data port on Linux <code>/dev/ttyACM0</code> for USB communication with the Pico felt familiar because of how I had setup debug logging in the past. It looked like I had enough example repos collected that I could stitch a solution together. I did a little research each day and felt a little more sure each time. I’ve been using ChatGPT and Claude more and more to do initial research. I was at an AI hype peak and was bold enough to trust it…</p>
<p><img src="https://nemo.foo/blog-content/deskthang/excalidraw.png" alt=""></p>
<p>Since I do full stack web stuff on the daily, the api, webhooks and postgres are out of scope for the degen day. I was scoping the day’s work to Zig -&gt; Pico image transfer.</p>
<h3>1. Setup Pico</h3>
<ul>
<li>Organize the workspace</li>
<li>Find a micro usb cable that supports data and not just charging… really why are they all power only?!</li>
<li>Wire up a breadboard with Pico &amp; display and LED</li>
<li>Setup the C SDK for the raspi pico and a repo <a href="https://gitlab.com/nemofoo/deskthang">gitlab</a>, <a href="https://github.com/nemofoo/deskthang">gihub-mirror</a></li>
<li>Push a build and see logs on <code>cat /dev/ttyACM0</code></li>
</ul>
<p><img src="https://nemo.foo/blog-content/deskthang/first_assembly.jpg" alt=""></p>
<h3>2. Setup Pico Display</h3>
<ul>
<li>Put something on the screen during the boot loop.</li>
</ul>
<p><img src="https://nemo.foo/blog-content/deskthang/first_test_pattern.jpg" alt=""></p>
<p><a href="https://youtube.com/shorts/H6b64PJI40o">youtube link (12 sec)</a></p>
<h3>3. Setup Host Zig Project</h3>
<ul>
<li>Setup a host directory in repo</li>
<li>Init zig project</li>
<li>Send a message and see something on the screen</li>
</ul>
<p><a href="https://youtu.be/Y0wkzbwGWJc">youtube link (9 sec)</a></p>
<h3>4. Image Transfer</h3>
<ul>
<li>Yeet the raw rgb image data over USB</li>
<li>It’s bidirectional safe right</li>
<li>USB CDC is bidirectional safe
<ul>
<li>TTY interface is built on top of USB CDC
<ul>
<li>TTY is bidirectional safe because CDC is (no it’s not… thanks gpt)</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="https://nemo.foo/blog-content/deskthang/gptlies.png" alt=""></p>
<blockquote>
<p>In the above image you can see the outright lie that broke me… USB CDC has separate TX (Transmit) and RX (Receive) buffers so it’s bidirectional safe. The same is not true for TTY which is bidirectional but less safe with a single buffer for TX and RX data.</p>
</blockquote>
<h2>Timeline of Actuality (AI Woes)</h2>
<p>After a dozen duds, I found a data capable usb micro cable and everything went smoothly until the image transfer. I used Claude, Cline, and ChatGPT to AI-max my way to a buggy but working implementation. I sent commands from my terminal with Zig over USB to the Pico which read them and changed the screen. This only took a few hours and I was excited that the AI assisted dream was real. It’s not complex but I think it was faster than I could have done alone. I have no experience with zig besides hearing ThePrimeagen yap about it. I haven’t even read the docs.</p>
<p>Multiple times, I found myself stopping Cline from starting completely new implementations of already solved issues. I didn’t catch everything though. When Cline blew through my API limits, I added Claude to my harem and ran both in parallel when possible.</p>
<blockquote>
<p>As I look through the code now, I realize that I’m lousy at multitasking and was gaslighting myself.</p>
</blockquote>
<h3>The Image Transfer Disaster</h3>
<p>This is where my hubris came into play. In my mind, I pictured sending all the image data in one go, like an S3 upload. I imagined clean, raw data streaming over <code>/dev/ttyACM0</code>. It wasn’t clean. It wasn’t raw. It was chaos.</p>
<p>I expected to see:</p>
<pre><code>pico - heartbeat
zig - start image transfer
zig - [240x240 COLOR PIXELS]
zig - end image transfer
pico - heartbeat
</code></pre>
<p>What I actually saw looked like this, but worse:</p>
<pre><code>pico - heartbeat
zig - start ima%
pico - heage transfert
pico - heartbeat
zig - [240x240 COL
pico - heartbea
OR PIXELtS]
pico - heartbeat
zig - end image transfer
pico - heartbeat
</code></pre>
<p><a href="https://youtu.be/jnmqlsdD6oU">It’s just like that interrupting cow knock knock joke.</a> Completely unfunny and day ruining.</p>
<p><strong>Key Problems:</strong></p>
<p><strong>1. Buffer Conflicts:</strong> <code>/dev/ttyACM0</code> was the battlefield. The same buffer was used for both logging and image transfer. If a log slipped in during the data stream… well good luck figuring out what the hell just happened.</p>
<p><strong>2. Noise:</strong> Some weird corruption was happening. Maybe I wasn’t clearing buffers properly. Maybe the gods of USB communication just hate me.</p>
<p>The bottom line? Neither the Pico nor my laptop could trust the data. Each system needed to learn to yield, and I needed to build the round-a-bout to force them to be polite and wait their turn.</p>
<h3>Packets, Protocols &amp; State Machines, Oh my…</h3>
<p>I needed to get serious. So, naturally, I let Claude write some docs:</p>
<ul>
<li>Detailed a packet shape.</li>
<li>Documented a checksum verification plan.</li>
<li>Described data format for transfer.</li>
<li>Denoted how to chunk and rebuild the image.</li>
<li>Depicted the state machine transitions.</li>
<li>Demonstrated command system.</li>
<li>Designed a logging system that doesn’t break incoming commands.</li>
</ul>
<p>After delving down dem docs, I let AI run with the actual implementations. At this point, my “degen hat” came off, and I resumed my dad duties while letting Cursor and Cline play StarCraft with my codebase. This is the dream use case for AI, right? Just let it rip and come back to a perfectly functioning system. Let’s see just how close we get to the sun.</p>
<p>Reality Check: AI tools are like interns who know how to Google really fast but don’t understand context. Cursor started changing core implementations for unrelated edits. Cline would randomly rewrite half the system without asking. By the time I noticed, my codebase looked like the aftermath of a spaghetti fight at a junior developer convention. Most of the codebase was actually unreachable.</p>
<h2>What did I learn?</h2>
<p>Like Icarus, my codebase is irrecoverable. A tangled heap of wing fragments and melted wax, dripping with half-baked ideas and unsupervised AI chaos. My grand vision of outsourcing grunt work to AI had sent me soaring, but the sun of reality burned away any hope of landing gracefully. Here’s what I’m taking away from this flaming descent.</p>
<h3>1. AI is a tool, not a co-pilot</h3>
<p>AI is great for generating ideas or drafting code, but it doesn’t understand. It’s like giving a junior developer a chainsaw instead of a scalpel—it might finish the job, but you’ll spend twice as long cleaning up the mess. I learned that I need to stay firmly in the driver’s seat when tackling new tech.</p>
<h3>2. Friction forces focus</h3>
<p>Having AI directly in my editor felt like playing with infinite cheat codes. It was too easy to let it run wild and harder to maintain control. Moving forward, I’m introducing deliberate friction. I will be using AI only in web interfaces or as a brainstorming tool. If I have to paste its suggestions into my code manually, I’ll be more mindful of the process and less likely to reach for it.</p>
<h3>3. Mistakes teach better than shortcuts</h3>
<p>When I make mistakes, I learn. Debugging my own failures has always been one of the best ways to understand a new language or concept. Relying on AI to “fix” things for me short-circuited that learning process. As a result, I’m left with no deeper understanding of Zig than when I started.</p>
<h3>4. Patience beats hubris</h3>
<p>Building something new, with unfamiliar tools takes time. The idea that I could fully implement my vision in a single “degen day” was overly optimistic, bordering on foolish. Sometimes, you have to respect the complexity of what you’re trying to achieve.</p>
<h2>Moving Forward</h2>
<p>Deskthang has grown from a casual afternoon project into a saga of overconfidence, AI misadventures, and lessons learned the hard way. For now, I’m shelving the AI driven shortcuts and committing to a rewrite on my next no-responsibilities day.</p>
<p>I’ve picked up a pen and started writing docs by hand like it’s the stone age. I plan to work through some Advent of Code problems in Zig to actually learn the language before taking another crack at this project.</p>
<p>Want to see if Deskthang ever works, or just enjoy the chaos as I fail forward? Subscribe below for a monthly email drop of my latest misadventures and skill issues. Together, we’ll learn how to coexist with AI, relearn the lessons I forget, and hopefully build something worthwhile in the process.</p>
<p>LFG 🚀</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nvidia sheds almost $600B in market cap, biggest one-day loss in US history (235 pts)]]></title>
            <link>https://www.cnbc.com/2025/01/27/nvidia-sheds-almost-600-billion-in-market-cap-biggest-drop-ever.html</link>
            <guid>42845681</guid>
            <pubDate>Mon, 27 Jan 2025 21:13:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2025/01/27/nvidia-sheds-almost-600-billion-in-market-cap-biggest-drop-ever.html">https://www.cnbc.com/2025/01/27/nvidia-sheds-almost-600-billion-in-market-cap-biggest-drop-ever.html</a>, See on <a href="https://news.ycombinator.com/item?id=42845681">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="SpecialReportArticle-ArticleBody-6" data-module="ArticleBody" data-test="articleBody-2" data-analytics="SpecialReportArticle-articleBody-6-2"><div id="ArticleBody-InlineImage-108082997" data-test="InlineImage"><p>Nvidia CEO Jensen Huang holds a Blackwell GeForce RTX 50 Series GPU (L) and a RTX 5000 laptop as he delivers a keynote address at the Consumer Electronics Show (CES) in Las Vegas, Nevada on January 6, 2025.&nbsp;</p><p>Patrick T. Fallon | Afp | Getty Images</p></div><div><p><span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-1"><a href="https://www.cnbc.com/quotes/NVDA/">Nvidia</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> lost close to $600 billion in market cap on Monday, the biggest drop for any company on a single day in U.S. history.</p><p>The chipmaker's stock price plummeted 17% to close at $118.58. It was Nvidia's worst day on the market since March 16, 2020, which was early in the Covid pandemic. After <a href="https://www.cnbc.com/2025/01/21/nvidia-passes-apple-again-to-become-worlds-most-valuable-company-.html">Nvidia surpassed</a> <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-3"><a href="https://www.cnbc.com/quotes/AAPL/">Apple</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> last week to become the most valuable publicly traded company, the stock's drop Monday led a 3.1% slide in the tech-heavy Nasdaq.</p><p>The sell-off was <a href="https://www.cnbc.com/2025/01/27/nvidia-falls-10percent-in-premarket-trading-as-chinas-deepseek-triggers-global-tech-sell-off.html">sparked</a> by concerns that Chinese <a href="https://www.cnbc.com/ai-artificial-intelligence/">artificial intelligence</a> lab DeepSeek is presenting increased competition in the global AI battle. In late December, <a href="https://www.cnbc.com/2025/01/24/how-chinas-new-ai-model-deepseek-is-threatening-us-dominance.html">DeepSeek unveiled</a> a free, open-source large language model that <a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf" target="_blank">it&nbsp;said</a>&nbsp;took only two months and less than $6 million to build, using reduced-capability chips from&nbsp;<a href="https://www.cnbc.com/quotes/NVDA/">Nvidia</a>&nbsp;called H800s.&nbsp;</p><p>Nvidia's graphics processing units, or GPUs, dominate the market for AI data center chips in the U.S., with tech giants such as <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-9"><a href="https://www.cnbc.com/quotes/GOOGL/">Alphabet</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>, <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-10"><a href="https://www.cnbc.com/quotes/META/">Meta</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> and <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-11"><a href="https://www.cnbc.com/quotes/AMZN/">Amazon</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> spending billions of dollars on the processors to train and run their AI models. </p><p>Analysts at Cantor wrote in a report Monday that the release of DeepSeek's latest technology has caused "great angst as to the impact for compute demand, and therefore, fears of peak spending on GPUs."</p></div><div id="SpecialReportArticle-RelatedContent-1"><h2>Read more DeepSeek coverage</h2><div><ul><li><a href="https://www.cnbc.com/2025/01/27/chinas-deepseek-ai-tops-chatgpt-app-store-what-you-should-know.html">China's DeepSeek AI dethrones ChatGPT on App Store</a></li><li><a href="https://www.cnbc.com/2025/01/27/deepseek-hit-with-large-scale-cyberattack-says-its-limiting-registrations.html">DeepSeek hit with large-scale cyberattack, says it's limiting registrations</a></li><li><a href="https://www.cnbc.com/2025/01/24/how-chinas-new-ai-model-deepseek-is-threatening-us-dominance.html">How China's new AI model DeepSeek is threatening U.S. dominance</a></li><li><a href="https://www.cnbc.com/2025/01/27/nvidia-falls-10percent-in-premarket-trading-as-chinas-deepseek-triggers-global-tech-sell-off.html">Nvidia hits new low for session on threat from DeepSeek AI model</a></li><li><a href="https://www.cnbc.com/2025/01/27/how-the-buzz-around-chinese-ai-model-deepseek-sparked-a-massive-nasdaq-sell-off.html">Buzz around Chinese AI model DeepSeek sparks massive Nasdaq sell-off</a></li><li><a href="https://www.cnbc.com/2025/01/27/the-key-chart-levels-to-watch-on-nvidia-tech-stocks-on-deepseek-fear.html">Pro: The key chart levels to watch on Nvidia and other tech stocks amid DeepSeek rout</a></li></ul></div></div><div><p>The analysts said they "think this view is farthest from the truth" and that advancements in AI will most likely lead to "the AI industry wanting more compute, not less." They recommend buying Nvidia shares.</p><p>But after Nvidia's huge run-up — the stock soared 239% in <a href="https://www.cnbc.com/2023/05/30/nvidia-on-track-to-hit-1-trillion-market-cap-when-market-opens.html">2023</a> and 171% in <a href="https://www.cnbc.com/2024/12/25/ai-crypto-top-tech-stocks-applovin-microstrategy-palantir-nvidia.html">2024</a> — the market is on edge about any possible pullback in spending. <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-14"><a href="https://www.cnbc.com/quotes/AVGO/">Broadcom</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>, the other big U.S. chipmaker to see giant valuation gains from AI, fell 17% on Monday, pulling its market cap down by $200 billion.</p><p>Data center companies reliant on Nvidia's GPUs for their hardware sales saw big sell-offs as well. <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-15"><a href="https://www.cnbc.com/quotes/DELL/">Dell</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>, <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-16"><a href="https://www.cnbc.com/quotes/HPE/">Hewlett Packard Enterprise</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> and <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-17"><a href="https://www.cnbc.com/quotes/SMCI/">Super Micro Computer</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> dropped at least 5.8%. <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-18"><a href="https://www.cnbc.com/quotes/ORCL/">Oracle</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>, a part of President<a href="https://www.cnbc.com/donald-trump/"> Donald Trump's</a> latest AI initiative, fell 14%.</p><p>For Nvidia, the loss was more than <a href="https://www.cnbc.com/2024/09/04/asian-chip-stocks-fall-after-nvidia-sell-off-on-wall-street-overnight.html">double the $279 billion drop</a> the company saw in September, which was the biggest one-day market value loss in history at the time, unseating Meta's <a href="https://www.cnbc.com/2022/02/03/facebooks-232billion-drop-in-value-sets-all-time-record.html">$232 billion loss</a> in 2022. Before that, the steepest drop was $182 billion by Apple in 2020.</p><p>Nvidia's decline is more than double the market cap of <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-22"><a href="https://www.cnbc.com/quotes/KO/">Coca-Cola</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> and <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-23"><a href="https://www.cnbc.com/quotes/CVX/">Chevron</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> and exceeds the market value of both Oracle and <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-24"><a href="https://www.cnbc.com/quotes/NFLX/">Netflix</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>.</p><p>CEO Jensen Huang's net worth also took a massive hit, declining roughly $21 billion, according to <a href="https://www.forbes.com/real-time-billionaires/#458f33ab3d78" target="_blank">Forbes' real-time billionaires list</a>. The move demoted Huang to 17th on the richest-person list.</p><p>The sudden excitement around DeepSeek over the weekend pushed its app <a href="https://www.cnbc.com/2025/01/27/chinas-deepseek-ai-tops-chatgpt-app-store-what-you-should-know.html">past OpenAI's ChatGPT</a> as the most-downloaded free app in the U.S. on Apple's app store. The model's development comes despite a slew of recent curbs on U.S. chip exports to China.</p><p>Venture capitalist David Sacks, who was <a href="https://www.cnbc.com/2024/12/05/trump-david-sacks-billionaire-ai-crypto.html">tapped</a> by Trump to be the White House's AI and crypto czar, <a href="https://x.com/DavidSacks/status/1883935713877782884" target="_blank">wrote on X</a> that DeepSeek's model "shows that the AI race will be very competitive" and that Trump was right to rescind President <a href="https://www.cnbc.com/video/2019/04/25/joe-biden-enters-2020-presidential-race.html">Joe Biden</a>'s executive order last week on AI safety.</p><p>"I'm confident in the U.S. but we can't be complacent," Sacks wrote.</p><p>Nvidia is now the third most-valuable public company, behind Apple and <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-30"><a href="https://www.cnbc.com/quotes/MSFT/">Microsoft</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>.</p><p><strong>WATCH:</strong> <a href="https://www.cnbc.com/video/2025/01/27/pro-watch-cnbcas-full-interview-with-bernsteins-stacy-rasgon-trivariateas-adam-parker-and-payne-capitalas-courtney-garcia.html">CNBC's full interview with Bernstein's Stacy Rasgon</a></p></div><div id="Placeholder-ArticleBody-Video-108093000" data-test="VideoPlaceHolder" role="region" tabindex="0" data-vilynx-id="7000364746" aria-labelledby="Placeholder-ArticleBody-Video-108093000"><p><img src="https://image.cnbcfm.com/api/v1/image/108093001-17380094731738009466-38180086461-1080pnbcnews.jpg?v=1738009472&amp;w=750&amp;h=422&amp;vtcrop=y" alt="Watch CNBC’s full interview with Bernstein's Stacy Rasgon, Trivariate’s Adam Parker and Payne Capital’s Courtney Garcia"></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Illustrated DeepSeek-R1 (206 pts)]]></title>
            <link>https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1</link>
            <guid>42845488</guid>
            <pubDate>Mon, 27 Jan 2025 20:51:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1">https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1</a>, See on <a href="https://news.ycombinator.com/item?id=42845488">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><em><span>[Draft post, updates to come, please let me know if you have any suggestions or feedback here or on </span><a href="https://bsky.app/profile/jayalammar.bsky.social" rel="">Bluesky</a><span> or </span><a href="https://x.com/JayAlammar" rel="">X/Twitter</a><span>]</span></em></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98138856-a4de-45e3-ad08-1434378127c2_1130x408.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98138856-a4de-45e3-ad08-1434378127c2_1130x408.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98138856-a4de-45e3-ad08-1434378127c2_1130x408.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98138856-a4de-45e3-ad08-1434378127c2_1130x408.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98138856-a4de-45e3-ad08-1434378127c2_1130x408.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98138856-a4de-45e3-ad08-1434378127c2_1130x408.png" width="1130" height="408" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/98138856-a4de-45e3-ad08-1434378127c2_1130x408.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:408,&quot;width&quot;:1130,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98138856-a4de-45e3-ad08-1434378127c2_1130x408.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98138856-a4de-45e3-ad08-1434378127c2_1130x408.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98138856-a4de-45e3-ad08-1434378127c2_1130x408.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98138856-a4de-45e3-ad08-1434378127c2_1130x408.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>DeepSeek-R1 is the latest resounding beat in the steady drumroll of AI progress. For the ML R&amp;D community, it is a major release for reasons including: </p><ol><li><p>It is an open weights model with smaller, distilled versions and </p></li><li><p>It shares and reflects upon a training method to reproduce a reasoning model like OpenAI O1. </p></li></ol><p>In this post, we’ll see how it was built.</p><p>Contents:</p><ul><li><p>Recap: How LLMs are trained</p></li><li><p>DeepSeek-R1 Training Recipe</p></li><li><p>1- Long chains of reasoning SFT Data</p></li><li><p>2- An interim high-quality reasoning LLM (but worse at non-reasoning tasks).</p></li><li><p>3- Creating reasoning models with large-scale reinforcement learning (RL) </p><ul><li><p>3.1- Large-Scale Reasoning-Oriented Reinforcement Learning (R1-Zero)</p></li><li><p>3.2- Creating SFT reasoning data with the interim reasoning model</p></li><li><p>3.3- General RL training phase </p></li></ul></li><li><p>Architecture</p></li></ul><p><span>Most of the foundational knowledge you need to understand how such a model works is available in our book, </span><a href="https://github.com/handsOnLLM/Hands-On-Large-Language-Models" rel="">Hands-On Large Language Models</a><span>.</span></p><p>Just like most existing LLMs, DeepSeek-R1 generates one token at a time, except it excels at solving math and reasoning problems because it is able to spend more time processing a problem through the process of generating thinking tokens that explain its chain of thought.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5280089e-8989-45d7-8194-93396b25557d_613x152.gif" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5280089e-8989-45d7-8194-93396b25557d_613x152.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5280089e-8989-45d7-8194-93396b25557d_613x152.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5280089e-8989-45d7-8194-93396b25557d_613x152.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5280089e-8989-45d7-8194-93396b25557d_613x152.gif 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5280089e-8989-45d7-8194-93396b25557d_613x152.gif" width="613" height="152" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5280089e-8989-45d7-8194-93396b25557d_613x152.gif&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:152,&quot;width&quot;:613,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:3441758,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/gif&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5280089e-8989-45d7-8194-93396b25557d_613x152.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5280089e-8989-45d7-8194-93396b25557d_613x152.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5280089e-8989-45d7-8194-93396b25557d_613x152.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5280089e-8989-45d7-8194-93396b25557d_613x152.gif 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The following figure, from Chapter 12 of our book shows the general recipe of creating a high-quality LLM over three steps:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa354473-6ae0-4ae7-a20c-e858c804d6c4_1600x477.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa354473-6ae0-4ae7-a20c-e858c804d6c4_1600x477.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa354473-6ae0-4ae7-a20c-e858c804d6c4_1600x477.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa354473-6ae0-4ae7-a20c-e858c804d6c4_1600x477.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa354473-6ae0-4ae7-a20c-e858c804d6c4_1600x477.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa354473-6ae0-4ae7-a20c-e858c804d6c4_1600x477.png" width="1456" height="434" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/aa354473-6ae0-4ae7-a20c-e858c804d6c4_1600x477.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:434,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa354473-6ae0-4ae7-a20c-e858c804d6c4_1600x477.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa354473-6ae0-4ae7-a20c-e858c804d6c4_1600x477.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa354473-6ae0-4ae7-a20c-e858c804d6c4_1600x477.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa354473-6ae0-4ae7-a20c-e858c804d6c4_1600x477.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>1) The language modeling step where we train the model to predict the next word using a massive amount of web data. This step results in a base model.</p><p>2) a supervised fine-tuning step that makes the model more useful in following instructions and answering questions. This step results in an instruction tuned model or a supervised fine -tuning / SFT model.</p><p>3) and finally a preference tuning step which further polishes its behaviors and aligns to human preferences, resulting in the final preference-tuned LLM which you interact with on playgrounds and apps.</p><p><span>DeepSeek-R1 follows this general recipe. The details of that first step come from a </span><a href="https://arxiv.org/pdf/2412.19437v1" rel="">previous paper for the DeepSeek-V3 model</a><span>. R1 uses the </span><em>base</em><span> model (not the final DeepSeek-v3 model) from that previous paper, and still goes through an SFT and preference tuning steps, but the details of how it does them are what's different.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc66dff5b-8332-4696-b484-b2ddb029b78c_854x234.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc66dff5b-8332-4696-b484-b2ddb029b78c_854x234.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc66dff5b-8332-4696-b484-b2ddb029b78c_854x234.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc66dff5b-8332-4696-b484-b2ddb029b78c_854x234.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc66dff5b-8332-4696-b484-b2ddb029b78c_854x234.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc66dff5b-8332-4696-b484-b2ddb029b78c_854x234.png" width="854" height="234" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c66dff5b-8332-4696-b484-b2ddb029b78c_854x234.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:234,&quot;width&quot;:854,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:30102,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc66dff5b-8332-4696-b484-b2ddb029b78c_854x234.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc66dff5b-8332-4696-b484-b2ddb029b78c_854x234.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc66dff5b-8332-4696-b484-b2ddb029b78c_854x234.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc66dff5b-8332-4696-b484-b2ddb029b78c_854x234.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>There are three special things to highlight in the R1 creation process.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26136780-897d-4f64-b1e5-45936b6078dd_854x434.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26136780-897d-4f64-b1e5-45936b6078dd_854x434.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26136780-897d-4f64-b1e5-45936b6078dd_854x434.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26136780-897d-4f64-b1e5-45936b6078dd_854x434.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26136780-897d-4f64-b1e5-45936b6078dd_854x434.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26136780-897d-4f64-b1e5-45936b6078dd_854x434.png" width="854" height="434" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/26136780-897d-4f64-b1e5-45936b6078dd_854x434.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:434,&quot;width&quot;:854,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:43757,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26136780-897d-4f64-b1e5-45936b6078dd_854x434.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26136780-897d-4f64-b1e5-45936b6078dd_854x434.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26136780-897d-4f64-b1e5-45936b6078dd_854x434.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26136780-897d-4f64-b1e5-45936b6078dd_854x434.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>This is a large number of long chain-of-thought reasoning examples (600,000 of them). These are very hard to come by and very expensive to label with humans at this scale. Which is why the process to create them is the second special thing to highlight</p><p><span>This data is created by a precursor to R1, an unnamed sibling which specializes in reasoning. This sibling is inspired by a third model called </span><em>R1-Zero </em><span>(that we’ll discuss shortly). It is significant not because it’s a great LLM to use, but because creating it required so little labeled data alongside large-scale reinforcement learning resulting in a model that excels at solving reasoning problems. </span></p><p>The outputs of this unnamed specialist reasoning model can then be used to train a more general model that can also do other, non-reasoning tasks, to the level users expect from an LLM.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4caea6a5-52a1-4651-8c71-4586c0637f3e_924x427.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4caea6a5-52a1-4651-8c71-4586c0637f3e_924x427.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4caea6a5-52a1-4651-8c71-4586c0637f3e_924x427.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4caea6a5-52a1-4651-8c71-4586c0637f3e_924x427.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4caea6a5-52a1-4651-8c71-4586c0637f3e_924x427.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4caea6a5-52a1-4651-8c71-4586c0637f3e_924x427.png" width="924" height="427" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4caea6a5-52a1-4651-8c71-4586c0637f3e_924x427.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:427,&quot;width&quot;:924,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:50317,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4caea6a5-52a1-4651-8c71-4586c0637f3e_924x427.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4caea6a5-52a1-4651-8c71-4586c0637f3e_924x427.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4caea6a5-52a1-4651-8c71-4586c0637f3e_924x427.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4caea6a5-52a1-4651-8c71-4586c0637f3e_924x427.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>This happens in two steps:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png" width="1456" height="629" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:629,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:176092,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Here, RL is used to create the interim reasoning model. The model is then used to  generate the SFT reasoning examples. But what makes creating this model possible is an earlier experiment creating an earlier model called </span><em>DeepSeek-R1-Zero</em><span>.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b9f117-caa3-42fd-a949-dc6433990d26_1526x506.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b9f117-caa3-42fd-a949-dc6433990d26_1526x506.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b9f117-caa3-42fd-a949-dc6433990d26_1526x506.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b9f117-caa3-42fd-a949-dc6433990d26_1526x506.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b9f117-caa3-42fd-a949-dc6433990d26_1526x506.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b9f117-caa3-42fd-a949-dc6433990d26_1526x506.png" width="1456" height="483" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/69b9f117-caa3-42fd-a949-dc6433990d26_1526x506.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:483,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:103072,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b9f117-caa3-42fd-a949-dc6433990d26_1526x506.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b9f117-caa3-42fd-a949-dc6433990d26_1526x506.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b9f117-caa3-42fd-a949-dc6433990d26_1526x506.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b9f117-caa3-42fd-a949-dc6433990d26_1526x506.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>R1-Zero is special because it is able to excel at reasoning tasks without having a labeled SFT training set. Its training goes directly from a pre-trained base model through a RL training process (no SFT step). It does this so well that it’s competitive with o1.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5c964f-b654-49b2-ab5a-5618b256ef99_1588x418.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5c964f-b654-49b2-ab5a-5618b256ef99_1588x418.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5c964f-b654-49b2-ab5a-5618b256ef99_1588x418.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5c964f-b654-49b2-ab5a-5618b256ef99_1588x418.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5c964f-b654-49b2-ab5a-5618b256ef99_1588x418.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5c964f-b654-49b2-ab5a-5618b256ef99_1588x418.png" width="1456" height="383" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7b5c964f-b654-49b2-ab5a-5618b256ef99_1588x418.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:383,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:106577,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5c964f-b654-49b2-ab5a-5618b256ef99_1588x418.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5c964f-b654-49b2-ab5a-5618b256ef99_1588x418.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5c964f-b654-49b2-ab5a-5618b256ef99_1588x418.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5c964f-b654-49b2-ab5a-5618b256ef99_1588x418.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>This is significant because data has always been the fuel for ML model capability. How can this model depart from that history? This points to two things:</p><p>1- Modern base models have crossed a certain threshold of quality and capability (this base model was trained on 14.8 trillion high-quality tokens).</p><p>2- Reasoning problems, in contrast to general chat or writing requests, can be automatically verified or labeled. Let’s show this with an example. This can be a prompt/question that is a part of this RL training step:</p><blockquote><p>Write python code that takes a list of numbers, returns them in a sorted order, but also adds 42 at the start.</p></blockquote><p>A question like this lends itself to many ways of automatic verification. Say we present this this to the model being trained, and it generates a completion:</p><ul><li><p>A software linter can check if the completion is proper python code or not</p></li><li><p>We can execute the python code to see if it even runs</p></li><li><p>Other modern coding LLMs can create unit tests to verify the desired behavior (without being reasoning experts themselves). </p></li><li><p>We can go even one step further and measure execution time and make the training process prefer more performant solutions over other solutions — even if they’re correct python programs that solve the issue.</p></li></ul><p>We can present a question like this to the model in a training step, and generate multiple possible solutions.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8edd9db2-a071-4bba-9d14-bbdb076d6355_798x444.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8edd9db2-a071-4bba-9d14-bbdb076d6355_798x444.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8edd9db2-a071-4bba-9d14-bbdb076d6355_798x444.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8edd9db2-a071-4bba-9d14-bbdb076d6355_798x444.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8edd9db2-a071-4bba-9d14-bbdb076d6355_798x444.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8edd9db2-a071-4bba-9d14-bbdb076d6355_798x444.png" width="798" height="444" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8edd9db2-a071-4bba-9d14-bbdb076d6355_798x444.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:444,&quot;width&quot;:798,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:60456,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8edd9db2-a071-4bba-9d14-bbdb076d6355_798x444.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8edd9db2-a071-4bba-9d14-bbdb076d6355_798x444.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8edd9db2-a071-4bba-9d14-bbdb076d6355_798x444.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8edd9db2-a071-4bba-9d14-bbdb076d6355_798x444.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>We can automatically check (with no human intervention) and see that the first completion is not even code. The second one is indeed python code but does not solve the problem. The third is a possible solution, but fails the unit tests, and the forth is a correct solution.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f9645a0-b1fb-4753-942c-583504297c25_972x517.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f9645a0-b1fb-4753-942c-583504297c25_972x517.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f9645a0-b1fb-4753-942c-583504297c25_972x517.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f9645a0-b1fb-4753-942c-583504297c25_972x517.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f9645a0-b1fb-4753-942c-583504297c25_972x517.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f9645a0-b1fb-4753-942c-583504297c25_972x517.png" width="972" height="517" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1f9645a0-b1fb-4753-942c-583504297c25_972x517.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:517,&quot;width&quot;:972,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:74268,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f9645a0-b1fb-4753-942c-583504297c25_972x517.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f9645a0-b1fb-4753-942c-583504297c25_972x517.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f9645a0-b1fb-4753-942c-583504297c25_972x517.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f9645a0-b1fb-4753-942c-583504297c25_972x517.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>These are all signals that can be directly use to improve the model. This is of course done over many examples (in mini-batches) and over successive training steps.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b595e04-bd57-4f78-8c9b-ab37797e9b66_955x543.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b595e04-bd57-4f78-8c9b-ab37797e9b66_955x543.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b595e04-bd57-4f78-8c9b-ab37797e9b66_955x543.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b595e04-bd57-4f78-8c9b-ab37797e9b66_955x543.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b595e04-bd57-4f78-8c9b-ab37797e9b66_955x543.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b595e04-bd57-4f78-8c9b-ab37797e9b66_955x543.png" width="955" height="543" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5b595e04-bd57-4f78-8c9b-ab37797e9b66_955x543.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:543,&quot;width&quot;:955,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:92262,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b595e04-bd57-4f78-8c9b-ab37797e9b66_955x543.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b595e04-bd57-4f78-8c9b-ab37797e9b66_955x543.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b595e04-bd57-4f78-8c9b-ab37797e9b66_955x543.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b595e04-bd57-4f78-8c9b-ab37797e9b66_955x543.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>These reward signals and model updates are how the model continues improving on tasks over the RL training process as seen in Figure 2 from the paper.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe48af6fa-8956-44b0-84cf-915e607f3b5e_1546x884.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe48af6fa-8956-44b0-84cf-915e607f3b5e_1546x884.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe48af6fa-8956-44b0-84cf-915e607f3b5e_1546x884.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe48af6fa-8956-44b0-84cf-915e607f3b5e_1546x884.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe48af6fa-8956-44b0-84cf-915e607f3b5e_1546x884.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe48af6fa-8956-44b0-84cf-915e607f3b5e_1546x884.png" width="1456" height="833" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e48af6fa-8956-44b0-84cf-915e607f3b5e_1546x884.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:833,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:211203,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe48af6fa-8956-44b0-84cf-915e607f3b5e_1546x884.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe48af6fa-8956-44b0-84cf-915e607f3b5e_1546x884.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe48af6fa-8956-44b0-84cf-915e607f3b5e_1546x884.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe48af6fa-8956-44b0-84cf-915e607f3b5e_1546x884.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Corresponding with the improvement of this capability is the length of the generated response, where the model generates more thinking tokens to process the problem.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd2b7d78-62ac-408c-8bd7-e14053bb8a46_1518x912.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd2b7d78-62ac-408c-8bd7-e14053bb8a46_1518x912.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd2b7d78-62ac-408c-8bd7-e14053bb8a46_1518x912.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd2b7d78-62ac-408c-8bd7-e14053bb8a46_1518x912.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd2b7d78-62ac-408c-8bd7-e14053bb8a46_1518x912.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd2b7d78-62ac-408c-8bd7-e14053bb8a46_1518x912.png" width="1456" height="875" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/cd2b7d78-62ac-408c-8bd7-e14053bb8a46_1518x912.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:875,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:250596,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd2b7d78-62ac-408c-8bd7-e14053bb8a46_1518x912.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd2b7d78-62ac-408c-8bd7-e14053bb8a46_1518x912.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd2b7d78-62ac-408c-8bd7-e14053bb8a46_1518x912.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd2b7d78-62ac-408c-8bd7-e14053bb8a46_1518x912.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>This process is useful, but the R1-Zero model, despite scoring high on these reasoning problems, confronts other issues that make it less usable than desired. </p><blockquote><p>Although DeepSeek-R1-Zero exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability, and language mixing.</p></blockquote><p>R1 is meant to be a more usable model. So instead of relying completely on the RL process, it is used in two places as we’ve mentioned earlier in this section:</p><p>1- creating an interim reasoning model to generate SFT data points</p><p>2- Training the R1 model to improve on reasoning and non-reasoning problems (using other types of verifiers)</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png" width="1456" height="629" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:629,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>To make the interim reasoning model more useful, it goes through an supervised fine-tuning (SFT) training step on a few thousand examples of reasoning problems (some of which are generated and filtered from R1-Zero). The paper refers to this as cold start data”</p><blockquote><p><strong>2.3.1. Cold Start</strong><br><span>Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1- Zero outputs in a readable format, and refining the results through post-processing by human annotators.</span></p></blockquote><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a89a9a0-c08f-430d-b135-7f012c2810ba_1824x586.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a89a9a0-c08f-430d-b135-7f012c2810ba_1824x586.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a89a9a0-c08f-430d-b135-7f012c2810ba_1824x586.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a89a9a0-c08f-430d-b135-7f012c2810ba_1824x586.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a89a9a0-c08f-430d-b135-7f012c2810ba_1824x586.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a89a9a0-c08f-430d-b135-7f012c2810ba_1824x586.png" width="1456" height="468" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1a89a9a0-c08f-430d-b135-7f012c2810ba_1824x586.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:468,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:160514,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a89a9a0-c08f-430d-b135-7f012c2810ba_1824x586.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a89a9a0-c08f-430d-b135-7f012c2810ba_1824x586.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a89a9a0-c08f-430d-b135-7f012c2810ba_1824x586.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a89a9a0-c08f-430d-b135-7f012c2810ba_1824x586.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>But wait, if we have this data, then why are we relying on the RL process? It’s because of the scale of the data. This dataset might be 5,000 examples (which is possible to source), but to train R1, 600,000 examples were needed. This interim model bridges that gap and allows to synthetically generate that extremely valuable data.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F857e61c8-03e7-4bc7-bcbe-ca182f60a70e_3300x1170.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F857e61c8-03e7-4bc7-bcbe-ca182f60a70e_3300x1170.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F857e61c8-03e7-4bc7-bcbe-ca182f60a70e_3300x1170.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F857e61c8-03e7-4bc7-bcbe-ca182f60a70e_3300x1170.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F857e61c8-03e7-4bc7-bcbe-ca182f60a70e_3300x1170.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F857e61c8-03e7-4bc7-bcbe-ca182f60a70e_3300x1170.png" width="1456" height="516" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/857e61c8-03e7-4bc7-bcbe-ca182f60a70e_3300x1170.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:516,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:244148,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F857e61c8-03e7-4bc7-bcbe-ca182f60a70e_3300x1170.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F857e61c8-03e7-4bc7-bcbe-ca182f60a70e_3300x1170.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F857e61c8-03e7-4bc7-bcbe-ca182f60a70e_3300x1170.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F857e61c8-03e7-4bc7-bcbe-ca182f60a70e_3300x1170.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>If you’re new to the concept of Supervised Fine-Tuning (SFT), that is the process that presents the model with training examples in the form of prompt and correct completion. This figure from chapter 12 shows a couple of SFT training examples:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b630dbc-aaa4-4c27-804b-542055b0f298_2264x1324.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b630dbc-aaa4-4c27-804b-542055b0f298_2264x1324.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b630dbc-aaa4-4c27-804b-542055b0f298_2264x1324.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b630dbc-aaa4-4c27-804b-542055b0f298_2264x1324.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b630dbc-aaa4-4c27-804b-542055b0f298_2264x1324.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b630dbc-aaa4-4c27-804b-542055b0f298_2264x1324.png" width="1456" height="851" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8b630dbc-aaa4-4c27-804b-542055b0f298_2264x1324.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:851,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:575809,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b630dbc-aaa4-4c27-804b-542055b0f298_2264x1324.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b630dbc-aaa4-4c27-804b-542055b0f298_2264x1324.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b630dbc-aaa4-4c27-804b-542055b0f298_2264x1324.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b630dbc-aaa4-4c27-804b-542055b0f298_2264x1324.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>This enables R1 to excel at reasoning as well as other non-reasoning tasks. The process is similar to the the RL process we’ve seen before. But since it extends to non-reasoning applications, it utilizes a helpfulnes and a safety reward model (not unlike the Llama models) for prompts that belong to these applications.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5f9acf-b4ca-4ec4-9731-4845c8fc5515_902x394.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5f9acf-b4ca-4ec4-9731-4845c8fc5515_902x394.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5f9acf-b4ca-4ec4-9731-4845c8fc5515_902x394.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5f9acf-b4ca-4ec4-9731-4845c8fc5515_902x394.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5f9acf-b4ca-4ec4-9731-4845c8fc5515_902x394.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5f9acf-b4ca-4ec4-9731-4845c8fc5515_902x394.png" width="902" height="394" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0e5f9acf-b4ca-4ec4-9731-4845c8fc5515_902x394.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:394,&quot;width&quot;:902,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:72511,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5f9acf-b4ca-4ec4-9731-4845c8fc5515_902x394.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5f9acf-b4ca-4ec4-9731-4845c8fc5515_902x394.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5f9acf-b4ca-4ec4-9731-4845c8fc5515_902x394.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5f9acf-b4ca-4ec4-9731-4845c8fc5515_902x394.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Just like previous models from the dawn of </span><a href="https://jalammar.github.io/illustrated-gpt2/" rel="">GPT2</a><span> and </span><a href="https://jalammar.github.io/how-gpt3-works-visualizations-animations/" rel="">GPT 3</a><span>, DeepSeek-R1 is a stack of </span><a href="https://jalammar.github.io/illustrated-transformer/" rel="">Transformer</a><span> decoder blocks. It’s made up 61 of them. The first three are dense, but the rest are mixture-of-experts layers (See my co-author Maarten’s incredible intro guide here: </span><a href="https://substack.com/home/post/p-148217245" rel="">A Visual Guide to Mixture of Experts (MoE)</a><span>).</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F199f326e-9a8d-4a95-8574-4778d5b7657b_538x413.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F199f326e-9a8d-4a95-8574-4778d5b7657b_538x413.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F199f326e-9a8d-4a95-8574-4778d5b7657b_538x413.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F199f326e-9a8d-4a95-8574-4778d5b7657b_538x413.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F199f326e-9a8d-4a95-8574-4778d5b7657b_538x413.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F199f326e-9a8d-4a95-8574-4778d5b7657b_538x413.png" width="538" height="413" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/199f326e-9a8d-4a95-8574-4778d5b7657b_538x413.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:413,&quot;width&quot;:538,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:39245,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F199f326e-9a8d-4a95-8574-4778d5b7657b_538x413.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F199f326e-9a8d-4a95-8574-4778d5b7657b_538x413.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F199f326e-9a8d-4a95-8574-4778d5b7657b_538x413.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F199f326e-9a8d-4a95-8574-4778d5b7657b_538x413.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>In terms of model dimension size and other hyperparameters, they look like this:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ee664ae-a544-4e19-a145-0ae87acc43fa_916x481.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ee664ae-a544-4e19-a145-0ae87acc43fa_916x481.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ee664ae-a544-4e19-a145-0ae87acc43fa_916x481.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ee664ae-a544-4e19-a145-0ae87acc43fa_916x481.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ee664ae-a544-4e19-a145-0ae87acc43fa_916x481.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ee664ae-a544-4e19-a145-0ae87acc43fa_916x481.png" width="916" height="481" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2ee664ae-a544-4e19-a145-0ae87acc43fa_916x481.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:481,&quot;width&quot;:916,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:63869,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ee664ae-a544-4e19-a145-0ae87acc43fa_916x481.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ee664ae-a544-4e19-a145-0ae87acc43fa_916x481.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ee664ae-a544-4e19-a145-0ae87acc43fa_916x481.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ee664ae-a544-4e19-a145-0ae87acc43fa_916x481.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>More details about the model architecture are presented in their two earlier papers:</p><ul><li><p><a href="https://arxiv.org/pdf/2412.19437v1" rel="">DeepSeek-V3 Technical Report</a></p></li><li><p><a href="https://arxiv.org/pdf/2401.06066" rel="">DeepSeekMoE: Towards Ultimate Expert Specialization in</a></p><p><a href="https://arxiv.org/pdf/2401.06066" rel="">Mixture-of-Experts Language Models</a></p></li></ul><p>With this, you should now have the main intuitions to wrap your head around the DeepSeek-R1 model. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed7fd8c3-7654-497c-a8e2-1f2e7930992e_3302x1438.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed7fd8c3-7654-497c-a8e2-1f2e7930992e_3302x1438.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed7fd8c3-7654-497c-a8e2-1f2e7930992e_3302x1438.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed7fd8c3-7654-497c-a8e2-1f2e7930992e_3302x1438.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed7fd8c3-7654-497c-a8e2-1f2e7930992e_3302x1438.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed7fd8c3-7654-497c-a8e2-1f2e7930992e_3302x1438.png" width="1456" height="634" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ed7fd8c3-7654-497c-a8e2-1f2e7930992e_3302x1438.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:634,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:341594,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed7fd8c3-7654-497c-a8e2-1f2e7930992e_3302x1438.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed7fd8c3-7654-497c-a8e2-1f2e7930992e_3302x1438.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed7fd8c3-7654-497c-a8e2-1f2e7930992e_3302x1438.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed7fd8c3-7654-497c-a8e2-1f2e7930992e_3302x1438.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>If you felt needed a little more foundational information to understand this post, I’d suggest you pick up a copy of </span><a href="https://www.llm-book.com/" rel="">Hands-On Large Language Models</a><span> or read it online on </span><a href="https://learning.oreilly.com/library/view/hands-on-large-language/9781098150952/" rel="">O’Reilly</a><span> and check it out on </span><a href="https://github.com/handsOnLLM/Hands-On-Large-Language-Models" rel="">Github</a><span>.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7beb5f-e943-4d2d-8b4c-eb1e80231670_582x768.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7beb5f-e943-4d2d-8b4c-eb1e80231670_582x768.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7beb5f-e943-4d2d-8b4c-eb1e80231670_582x768.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7beb5f-e943-4d2d-8b4c-eb1e80231670_582x768.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7beb5f-e943-4d2d-8b4c-eb1e80231670_582x768.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7beb5f-e943-4d2d-8b4c-eb1e80231670_582x768.png" width="158" height="208.49484536082474" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bd7beb5f-e943-4d2d-8b4c-eb1e80231670_582x768.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:768,&quot;width&quot;:582,&quot;resizeWidth&quot;:158,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Book Cover&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Book Cover" title="Book Cover" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7beb5f-e943-4d2d-8b4c-eb1e80231670_582x768.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7beb5f-e943-4d2d-8b4c-eb1e80231670_582x768.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7beb5f-e943-4d2d-8b4c-eb1e80231670_582x768.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7beb5f-e943-4d2d-8b4c-eb1e80231670_582x768.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Other suggested resources are:</p><ul><li><p><a href="https://www.interconnects.ai/p/deepseek-r1-recipe-for-o1" rel="">DeepSeek R1's recipe to replicate o1 and the future of reasoning LMs</a><span> by </span></p><span> </span></li><li><p><a href="https://substack.com/home/post/p-148217245" rel="">A Visual Guide to Mixture of Experts (MoE)</a><span> by </span></p><span> </span></li><li><p><span>Sasha Rush’s YouTube video </span><a href="https://www.youtube.com/watch?v=6PEJ96k1kiw" rel="">Speculations on Test-Time Scaling (o1)</a><span> </span></p></li><li><p><span>Yannis Kilcher’s </span><a href="https://www.youtube.com/watch?v=bAWV_yrqx4w" rel="">DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models (Paper Explained)</a></p></li><li><p><a href="https://github.com/huggingface/open-r1" rel="">Open R1</a><span> is the HuggingFace project to openly reproduce DeepSeek-R1</span></p></li><li><p><a href="https://huggingface.co/blog/putting_rl_back_in_rlhf_with_rloo" rel="">Putting RL back in RLHF</a></p></li></ul></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Go 1.24's go tool is one of the best additions to the ecosystem in years (168 pts)]]></title>
            <link>https://www.jvt.me/posts/2025/01/27/go-tools-124/</link>
            <guid>42845323</guid>
            <pubDate>Mon, 27 Jan 2025 20:33:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jvt.me/posts/2025/01/27/go-tools-124/">https://www.jvt.me/posts/2025/01/27/go-tools-124/</a>, See on <a href="https://news.ycombinator.com/item?id=42845323">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>For those that aren't aware, one of the big changes in February's upcoming Go 1.24 release is the new <a href="https://tip.golang.org/doc/go1.24#tools"><code>go tool</code></a> command, and <code>tool</code> directive in the <code>go.mod</code> to manage any tools your project uses. I'm <em>incredibly excited</em> about this, and in my opinion, this is one of the best changes we've had in recent years in the ecosystem as a whole.</p><p>I've been meaning to write this post since the first release candidate for Go 1.24 landed, but after reading Howard John's <a href="https://blog.howardjohn.info/posts/go-tools-command/">Exploring the new "go tool" support in Go 1.24</a> this morning, I thought I should write my thoughts up.</p><h2 id="what-is-it">What is it?</h2><p>Within your Go codebases, there's often some additional tools that you need to have installed to be able to build/test/deploy the project.</p><p>Sometimes this will a dependency that's needed for <code>go generate</code>ing, or it may be that you want to pipe your <code>go test</code> output into a JUnit-compatible format, so your CI platform can provide more useful metadata.</p><p>For each of these, you have two choices:</p><ul><li>require that the user knows how to install them, i.e. by knowing to run <code>make deps</code> or <code>just setup</code> before building anything on the project (which will then i.e. <code>go install</code> the commands)</li><li>use the <a href="https://www.jvt.me/posts/2022/06/15/go-tools-dependency-management/"><code>tools.go</code> pattern</a> to make it so you can <em>just</em> run <code>go generate</code>, and that'll call the right dependency via <code>go run</code></li></ul><p>My preference is <a href="https://www.jvt.me/posts/2022/06/15/go-tools-dependency-management/"><code>tools.go</code> pattern</a>, but there are two key problems with this approach.</p><p>Firstly, there's a performance hit of using a <code>tools.go</code>. It's something that is <em>slightly</em> noticeable, moreso if your project relies upon a lot of <code>go run</code> i.e. with lots of <code>go generate</code>s, because prior to Go 1.24, the <code>go run</code> invocations were not cached.</p><p>Secondly, it also leads to dependency tree bloat, because you have to record your dependency on i.e. <code>github.com/sqlc-dev/sqlc/cmd/sqlc</code> which then gets recorded in your <code>go.mod</code>, and then anyone using <em>your module</em> will then see that as an indirect (transitive) dependency.</p><p>This was something we <a href="https://www.jvt.me/posts/2023/10/23/oapi-codegen-v2-decrease/">worked on for <code>oapi-codegen</code>'s v2 release</a> to further reduce unnecessary dependencies, and make things a bit cleaner for our consumers. This is somewhat mitigated by Go's <a href="https://go.dev/ref/mod#graph-pruning">module graph pruning</a> which won't download dependencies that aren't used, but consumers may still see the dependencies coming in as an indirect dependency, which may not be ideal (especially as it can then bloat their indirect dependencies, which then gets passed on to their consumers and so on .</p><p>Dependency tree bloat can also be further mitigated by splitting your <a href="https://www.jvt.me/posts/2024/09/30/go-tools-module/"><code>tools.go</code> into a separate module</a>, which makes it more awkward to invoke dependencies but makes sure that none of your consumers will be seeing any tool-related dependencies.</p><p>For those who know me as co-maintainer of <a href="https://github.com/oapi-codegen/oapi-codegen">oapi-codegen</a>, you'll know that the <code>tools.go</code> pattern is our <a href="https://github.com/oapi-codegen/oapi-codegen#install">explicit recommendation</a> and we believe is better than installing it as a binary, so it's probably unsurprising that I'm very excited about this as an option to manage dependencies.</p><h2 id="how-does-it-work">How does it work?</h2><p>I've started playing around with this <a href="https://gitlab.com/tanna.dev/dependency-management-data/-/commits/spike/go-tools-124">on a branch</a> of the <a href="https://dmd.tanna.dev/">dependency-management-data</a> project, where I've got a mix of different tools that need to be installed and used.</p><p>Let's take a worked example of how we'd move over calls to <code>oapi-codegen</code> to the new <code>go tool</code> pattern.</p><h3 id="existing-state">Existing state</h3><p>For instance let's say that we have the following <code>tools.go</code> in its own module:</p><pre tabindex="0"><code data-lang="gomod"># tools/go.mod
module dmd.tanna.dev/tools

go 1.22.0

require (
	github.com/99designs/gqlgen v0.17.49
	github.com/oapi-codegen/oapi-codegen/v2 v2.4.1
	github.com/sqlc-dev/sqlc v1.26.0
)
</code></pre><p>We can then see that we invoke this via <code>go run</code>:</p><div><pre tabindex="0"><code data-lang="go"><span><span><span>// internal/ecosystems/generate.go
</span></span></span><span><span><span>//go:generate go run -modfile=../../tools/go.mod github.com/oapi-codegen/oapi-codegen/v2/cmd/oapi-codegen --config=config.yaml openapi.yaml
</span></span></span></code></pre></div><h3 id="migrating">Migrating</h3><p>To start migrating over to <code>go tool</code>, we need to make sure that we've first pulled in the new version of Go in our top-level Go module:</p><div><pre tabindex="0"><code data-lang="diff"><span><span> module dmd.tanna.dev
</span></span><span><span>
</span></span><span><span><span>-go 1.22.7
</span></span></span><span><span><span></span><span>+go 1.24
</span></span></span><span><span><span></span>
</span></span><span><span><span>-toolchain go1.23.2
</span></span></span><span><span><span></span><span>+toolchain go1.24rc2
</span></span></span></code></pre></div><p>Next, we need to pull in a <code>tool</code> dependency on <code>oapi-codegen</code>'s CLI tool - notice that you need <strong>the full path</strong> to the command that's being invoked:</p><div><pre tabindex="0"><code data-lang="sh"><span><span><span># NOTE the full import path</span>
</span></span><span><span>% go get -tool github.com/oapi-codegen/oapi-codegen/v2/cmd/oapi-codegen@v2.4.1
</span></span></code></pre></div><p>We could also do this by hand, but doing it via <code>go get</code> simplifies this a little.</p><p>From here, we'll notice that our <code>go.mod</code> has a few other changes:</p><div><pre tabindex="0"><code data-lang="diff"><span><span><span>@@ -57,12 +57,16 @@ require (
</span></span></span><span><span><span></span>        github.com/cenkalti/backoff/v4 v4.3.0 // indirect
</span></span><span><span>        github.com/cespare/xxhash/v2 v2.3.0 // indirect
</span></span><span><span>        github.com/charmbracelet/lipgloss v0.10.0 // indirect
</span></span><span><span><span>+       github.com/dprotaso/go-yit v0.0.0-20220510233725-9ba8df137936 // indirect
</span></span></span><span><span><span></span>        github.com/dustin/go-humanize v1.0.1 // indirect
</span></span><span><span>        github.com/felixge/httpsnoop v1.0.4 // indirect
</span></span><span><span><span>+       github.com/getkin/kin-openapi v0.127.0 // indirect
</span></span></span><span><span><span></span>        github.com/go-ini/ini v1.67.0 // indirect
</span></span><span><span>        github.com/go-logfmt/logfmt v0.6.0 // indirect
</span></span><span><span>        github.com/go-logr/logr v1.4.2 // indirect
</span></span><span><span>        github.com/go-logr/stdr v1.2.2 // indirect
</span></span><span><span><span>+       github.com/go-openapi/jsonpointer v0.21.0 // indirect
</span></span></span><span><span><span>+       github.com/go-openapi/swag v0.23.0 // indirect
</span></span></span><span><span><span></span>        github.com/gobwas/glob v0.2.3 // indirect
</span></span><span><span>        github.com/google/go-querystring v1.1.0 // indirect
</span></span><span><span>        github.com/gorilla/mux v1.8.1 // indirect
</span></span><span><span><span>@@ -72,16 +76,22 @@ require (
</span></span></span><span><span><span></span>        github.com/hashicorp/go-retryablehttp v0.7.5 // indirect
</span></span><span><span>        github.com/hashicorp/golang-lru/v2 v2.0.7 // indirect
</span></span><span><span>        github.com/inconshreveable/mousetrap v1.1.0 // indirect
</span></span><span><span><span>+       github.com/invopop/yaml v0.3.1 // indirect
</span></span></span><span><span><span>+       github.com/josharian/intern v1.0.0 // indirect
</span></span></span><span><span><span></span>        github.com/klauspost/compress v1.17.11 // indirect
</span></span><span><span>        github.com/lucasb-eyer/go-colorful v1.2.0 // indirect
</span></span><span><span><span>+       github.com/mailru/easyjson v0.7.7 // indirect
</span></span></span><span><span><span></span>        github.com/mattn/go-isatty v0.0.20 // indirect
</span></span><span><span>        github.com/mattn/go-runewidth v0.0.15 // indirect
</span></span><span><span>        github.com/mitchellh/mapstructure v1.5.0 // indirect
</span></span><span><span><span>+       github.com/mohae/deepcopy v0.0.0-20170929034955-c48cc78d4826 // indirect
</span></span></span><span><span><span></span>        github.com/muesli/reflow v0.3.0 // indirect
</span></span><span><span>        github.com/muesli/termenv v0.15.2 // indirect
</span></span><span><span>        github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822 // indirect
</span></span><span><span>        github.com/ncruces/go-strftime v0.1.9 // indirect
</span></span><span><span><span>+       github.com/oapi-codegen/oapi-codegen/v2 v2.4.1 // indirect
</span></span></span><span><span><span></span>        github.com/olekukonko/tablewriter v0.0.5 // indirect
</span></span><span><span><span>+       github.com/perimeterx/marshmallow v1.1.5 // indirect
</span></span></span><span><span><span></span>        github.com/prometheus/client_golang v1.20.5 // indirect
</span></span><span><span>        github.com/prometheus/client_model v0.6.1 // indirect
</span></span><span><span>        github.com/prometheus/common v0.60.1 // indirect
</span></span><span><span><span>@@ -91,8 +101,10 @@ require (
</span></span></span><span><span><span></span>        github.com/rivo/uniseg v0.4.7 // indirect
</span></span><span><span>        github.com/sirupsen/logrus v1.9.4-0.20230606125235-dd1b4c2e81af // indirect
</span></span><span><span>        github.com/sosodev/duration v1.3.1 // indirect
</span></span><span><span><span>+       github.com/speakeasy-api/openapi-overlay v0.9.0 // indirect
</span></span></span><span><span><span></span>        github.com/spf13/pflag v1.0.5 // indirect
</span></span><span><span>        github.com/tchap/go-patricia/v2 v2.3.1 // indirect
</span></span><span><span><span>+       github.com/vmware-labs/yaml-jsonpath v0.3.2 // indirect
</span></span></span><span><span><span></span>        github.com/xeipuuv/gojsonpointer v0.0.0-20190905194746-02993c407bfb // indirect
</span></span><span><span>        github.com/xeipuuv/gojsonreference v0.0.0-20180127040603-bd5ef7bd5415 // indirect
</span></span><span><span>        github.com/yashtewari/glob-intersection v0.2.0 // indirect
</span></span><span><span><span>@@ -110,11 +122,13 @@ require (
</span></span></span><span><span><span></span>        go.opentelemetry.io/otel/metric v1.32.0 // indirect
</span></span><span><span>        go.opentelemetry.io/proto/otlp v1.3.1 // indirect
</span></span><span><span>        golang.org/x/exp v0.0.0-20231108232855-2478ac86f678 // indirect
</span></span><span><span><span>+       golang.org/x/mod v0.18.0 // indirect
</span></span></span><span><span><span></span>        golang.org/x/net v0.30.0 // indirect
</span></span><span><span>        golang.org/x/oauth2 v0.23.0 // indirect
</span></span><span><span>        golang.org/x/sys v0.27.0 // indirect
</span></span><span><span>        golang.org/x/term v0.25.0 // indirect
</span></span><span><span>        golang.org/x/time v0.5.0 // indirect
</span></span><span><span><span>+       golang.org/x/tools v0.22.0 // indirect
</span></span></span><span><span><span></span>        google.golang.org/genproto/googleapis/api v0.0.0-20241104194629-dd2ea8efbc28 // indirect
</span></span><span><span>        google.golang.org/genproto/googleapis/rpc v0.0.0-20241104194629-dd2ea8efbc28 // indirect
</span></span><span><span>        google.golang.org/grpc v1.68.0 // indirect
</span></span><span><span><span>@@ -128,3 +142,5 @@ require (
</span></span></span><span><span><span></span>        modernc.org/token v1.1.0 // indirect
</span></span><span><span>        sigs.k8s.io/yaml v1.4.0 // indirect
</span></span><span><span> )
</span></span><span><span><span>+
</span></span></span><span><span><span>+tool github.com/oapi-codegen/oapi-codegen/v2/cmd/oapi-codegen
</span></span></span></code></pre></div><p>From here, we can see:</p><ul><li>there is a <code>tool</code> directive for <code>github.com/oapi-codegen/oapi-codegen/v2/cmd/oapi-codegen</code></li><li>the containing Go module for the CLI, <code>github.com/oapi-codegen/oapi-codegen/v2</code>, is now an <code>indirect</code> dependency</li><li>any other required dependencies of <code>github.com/oapi-codegen/oapi-codegen/v2/cmd/oapi-codegen</code> are now <code>indirect</code> dependencies</li></ul><p>Now we've done this, we could run:</p><div><pre tabindex="0"><code data-lang="sh"><span><span>% go tool github.com/oapi-codegen/oapi-codegen/v2/cmd/oapi-codegen --help
</span></span><span><span>Usage of /home/jamie/.cache/go-build/0e/0e04736601c8bbef785d372de02859bf8f39405aae9ccbf371477b0f2d8df755-d/oapi-codegen:
</span></span><span><span><span># ...</span>
</span></span></code></pre></div><p>With this tool set up, we can now modify i.e. <code>internal/ecosystems/generate.go</code> like so to use the new <code>go tool</code>:</p><div><pre tabindex="0"><code data-lang="diff"><span><span> package ecosystems
</span></span><span><span>
</span></span><span><span><span>-//go:generate go run -modfile=../../tools/go.mod github.com/oapi-codegen/oapi-codegen/v2/cmd/oapi-codegen --config=config.yaml openapi.yaml
</span></span></span><span><span><span></span><span>+//go:generate go tool github.com/oapi-codegen/oapi-codegen/v2/cmd/oapi-codegen --config=config.yaml openapi.yaml
</span></span></span></code></pre></div><p>Then running <code>go generate ./internal/ecosystems</code> works as it did before 🚀</p><h2 id="performance-implications">Performance implications</h2><p>A less scientific view than Howard John's article above, but we can see a slight improvement in performance:</p><div><pre tabindex="0"><code data-lang="sh"><span><span><span># first time using `go tool`, from a fresh cache directory</span>
</span></span><span><span>% <span>time</span> go generate ./internal/ecosystems
</span></span><span><span>go generate ./internal/ecosystems  55.05s user 4.57s system 531% cpu 11.220 total
</span></span><span><span><span># a subsequent call</span>
</span></span><span><span>% <span>time</span> go generate ./internal/ecosystems
</span></span><span><span>go generate ./internal/ecosystems  0.59s user 0.18s system 424% cpu 0.181 total
</span></span><span><span><span># another just to see</span>
</span></span><span><span>% <span>time</span> go generate ./internal/ecosystems
</span></span><span><span>go generate ./internal/ecosystems  0.57s user 0.25s system 404% cpu 0.202 total
</span></span></code></pre></div><p>Compare this to the previous implementation:</p><div><pre tabindex="0"><code data-lang="sh"><span><span><span># first time using `go run`, from a fresh cache directory</span>
</span></span><span><span>% <span>time</span> go generate ./internal/ecosystems
</span></span><span><span>go generate ./internal/ecosystems  50.29s user 3.67s system 536% cpu 10.063 total
</span></span><span><span><span># a subsequent call</span>
</span></span><span><span>% <span>time</span> go generate ./internal/ecosystems
</span></span><span><span>go generate ./internal/ecosystems  1.04s user 0.21s system 185% cpu 0.677 total
</span></span><span><span><span># another just to see</span>
</span></span><span><span>% <span>time</span> go generate ./internal/ecosystems
</span></span><span><span>go generate ./internal/ecosystems  1.02s user 0.26s system 191% cpu 0.669 total
</span></span></code></pre></div><p>Notice that the first call is similar in speed, but the use of <code>go tool</code>'s subsequent calls are still faster.</p><p>I'm a big fan of the fact that as of Go 1.24+ the <code>go run</code>s will be cached, so even if you don't move over to <code>go tool</code>, you'll get a performance boost!</p><h2 id="concerns">Concerns</h2><p>Now, there are still a few things I've noticed while doing the migration that aren't necessarily what I expected.</p><h3 id="gomod-implications"><code>go.mod</code> implications</h3><p>Something interesting is that the usage of the <code>tool</code> dependencies being treated as an <code>indirect</code> dependency is that they're present in the dependency tree, and treated like any other <code>indirect</code> dependency.</p><p>I'd also have preferred that we had just used <code>// tool</code> instead of <code>// indirect</code>, but I can see why this is likely the choice that's made - so they're treated like any other dependency - but making them less clear as only being required for tools could lead to issues with clashing dependencies, or where you upgrade an <code>indirect</code> dependency and then that breaks other things.</p><p>This means that tools such as Renovate need to be a little more involved in how to do the updates, but <a href="https://github.com/renovatebot/renovate/discussions/33867">that's all in hand</a>.</p><h2 id="gqlgen-fails-to-run-with-go-124rc2"><code>gqlgen</code> fails to run with Go 1.24rc2</h2><p>Something I've noticed while playing around with this is that <a href="https://github.com/99designs/gqlgen/issues/3505"><code>gqlgen</code> struggles to run with Go 1.24rc2</a>, which <a href="https://github.com/golang/go/issues/71448">feels like an upstream Go issue</a>, but it looks like that may be related to the use of <code>/x/tools</code> 🤔</p><p>It may be interesting to find out what else gets affected by this - please give the RC a test!</p><h2 id="closing">Closing</h2><p>Overall, I'm feeling very positive about it, and improving the way that dependencies get installed <em>if they should be built from source</em>, but there are dependencies such as <code>golangci-lint</code> which <a href="https://golangci-lint.run/welcome/install/#install-from-sources">don't recommend building from source</a> and instead using their pre-built binaries, which is fair, and is unlikely to change here.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[We're bringing Pebble back (1568 pts)]]></title>
            <link>https://repebble.com/</link>
            <guid>42845091</guid>
            <pubDate>Mon, 27 Jan 2025 20:11:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://repebble.com/">https://repebble.com/</a>, See on <a href="https://news.ycombinator.com/item?id=42845091">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="text-container">
            
            
            
            <div>
                <div>
                    <h2>We're making new Pebble watches</h2>
                    <p>I've tried pretty much every other smartwatch on Earth, yet I still wear my Pebble every day—nothing else matches its features and long battery life. I really, <em>really</em>, <strong><em>really</em></strong> hoped someone else would create a proper replacement, but no one has stepped up, and my stash of old Pebbles is dwindling!</p>
                    <p>It's time to take matters into my own hands. A small team and I are working on a new Pebble-like smartwatch that runs open source PebbleOS, has the same beloved features (plus some fun new stuff), and stays true to the core Pebble vision. If enough people are interested, we'll build it. <a href="https://repebble.com/signup.html">Sign up</a> to get one!</p>
                    <a href="https://ericmigi.com/blog/why-were-bringing-pebble-back" target="_blank">
                        <h3>Read the full blog post</h3>
                        <p>Why We're Bringing Pebble Back</p>
                    </a>
                </div>

                <div>
                    <h2>PebbleOS is now open source</h2>
                    <p>Google (which purchased Fitbit, which had bought Pebble) still owns PebbleOS. Over the last year, a team inside Google (including some amazing ex-Pebblers turned Googlers) has been working on open sourcing the OS! The source code for PebbleOS is now available at <a href="https://github.com/google/pebble" target="_blank">github.com/google/pebble</a>. Read more on their <a href="https://opensource.googleblog.com/2025/01/see-code-that-powered-pebble-smartwatches.html" target="_blank">blog</a>.</p>
                    <p>Thank you so much, Google! I can't stress how thankful I am to the individuals who did the heavy lifting. This was also made possible by the <a href="https://rebble.io/" target="_blank">Rebble</a> team and community, who have supported Pebble since it shut down. Check out the vibrant <a href="https://reddit.com/r/pebble" target="_blank">r/Pebble</a> and <a href="https://discordapp.com/invite/aRUAYFN" target="_blank">Discord</a>.</p>
                </div>
                
                <div>
                    <h2>What happens now?</h2>
                    <p>The source code that powers each Pebble smartwatch is now freely available to download, modify and improve on <a href="https://github.com/google/pebble" target="_blank">Github</a>. Want a reminder of how awesome Pebble OS is? Dive back into the <a href="https://ericmigi.com/blog/pebbleos-is-awesome" target="_blank">beautiful, retro, pixelated world</a> of Pebble.</p>
                    <p>Anyone can use PebbleOS in any way they want. You can get it working on existing Pebble watches, emulate it, run it on other embedded devices, or create new hardware specifically for it. <a href="https://github.com/pebble-dev/pebbleos" target="_blank">Learn more</a> about PebbleOS.</p>
                    <p>We're setting out to bring Pebble back, we'd love for you to join the fun!</p>
                </div>
            </div>

            <h2 id="do-you-want-one">Do you want a new Pebble?</h2>
            
            <div>
                <p>Eric Migicovsky<br>Founder of Pebble</p>
                <div>
                    <h3>Wait, what is Pebble again?</h3>
                    <p>Pebble is an e-paper smartwatch with simple functionality, long battery life, and fun, quirky design. It first launched on <a href="https://www.kickstarter.com/projects/getpebble/pebble-e-paper-watch-for-iphone-and-android" target="_blank">Kickstarter</a> in 2012 and sold over 2 million watches before the company's IP was sold to Fitbit in 2016.</p>
                </div>
                <p id="smallText">© Copyright <span id="year">----</span> Core Devices LLC. All Rights Reserved.<br><a href="https://repebble.com/privacy.html">Privacy</a> · <a href="https://repebble.com/terms.html">Terms</a> · <a href="https://twitter.com/pebble" target="_blank">Twitter</a></p>
            </div>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google open-sources the Pebble OS (912 pts)]]></title>
            <link>https://opensource.googleblog.com/2025/01/see-code-that-powered-pebble-smartwatches.html</link>
            <guid>42845070</guid>
            <pubDate>Mon, 27 Jan 2025 20:09:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://opensource.googleblog.com/2025/01/see-code-that-powered-pebble-smartwatches.html">https://opensource.googleblog.com/2025/01/see-code-that-powered-pebble-smartwatches.html</a>, See on <a href="https://news.ycombinator.com/item?id=42845070">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-5561126018697023258" itemprop="articleBody">
<meta name="twitter:image" content="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjHCX-mb_DqHgkNn1By45jRl-t4yGY82D79aFivyvhLIjiW9oglYr2fu7qOXFTEPj4sg-18anq6Aydli437ogx_AfTNI4V8Kq9Wjm1pPpOpqsSG1aiTwNLURTHgzFTeND8VuCxmndTLxT48Hr5RQgWvilKyeI9ORfoRNE40ZyqV49xuxTNarCAIoErsYbw/s1600/Pebble-Smartwatch%20%281%29.png">
<p>

<a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjHCX-mb_DqHgkNn1By45jRl-t4yGY82D79aFivyvhLIjiW9oglYr2fu7qOXFTEPj4sg-18anq6Aydli437ogx_AfTNI4V8Kq9Wjm1pPpOpqsSG1aiTwNLURTHgzFTeND8VuCxmndTLxT48Hr5RQgWvilKyeI9ORfoRNE40ZyqV49xuxTNarCAIoErsYbw/s1600/Pebble-Smartwatch%20%281%29.png" imageanchor="1"><img data-original-height="800" data-original-width="100%" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjHCX-mb_DqHgkNn1By45jRl-t4yGY82D79aFivyvhLIjiW9oglYr2fu7qOXFTEPj4sg-18anq6Aydli437ogx_AfTNI4V8Kq9Wjm1pPpOpqsSG1aiTwNLURTHgzFTeND8VuCxmndTLxT48Hr5RQgWvilKyeI9ORfoRNE40ZyqV49xuxTNarCAIoErsYbw/s1600/Pebble-Smartwatch%20%281%29.png"></a></p><p>We are excited to announce that the source code that powered Pebble smartwatches is now <a href="https://github.com/google/pebble" target="_blank">available for download</a>.</p>

<p>This is part of an effort from Google to help and support the <a href="https://rebble.io/" target="_blank">volunteers</a> who have come together to maintain functionality for Pebble watches after the original company ceased operations in 2016.</p><br>

<h3><b>A quick look back</b></h3>

<p>Pebble was initially launched through a very successful <a href="https://www.kickstarter.com/projects/getpebble/pebble-e-paper-watch-for-iphone-and-android" target="_blank">Kickstarter project</a>. Pebble’s first Kickstarter was the single most funded at the time, and its successor Kickstarter for the <a href="https://www.kickstarter.com/projects/getpebble/pebble-time-awesome-smartwatch-no-compromises" target="_blank">Pebble Time</a> repeated that feat – and remains the second most funded today! Over the course of four years, Pebble sold over two million smartwatches, cultivating a thriving community of thousands of developers who created over ten thousand Pebble apps and watchfaces.</p>

<p>In 2016, Fitbit acquired Pebble, including Pebble’s intellectual property. Later on, Fitbit itself was acquired by Google, taking the Pebble OS with it.</p>

<p>Despite the Pebble hardware and software support being discontinued eight years ago, Pebble still has thousands of dedicated fans.</p><br>

<h3><b>What is being released</b></h3>

<p>We are releasing most of the source code for the Pebble operating system. This repository contains the entire OS, which provides all the standard smartwatch functionality – notifications, media controls, fitness tracking, and support for custom apps and watchfaces – on tiny ARM Cortex-M microcontrollers. Built with <a href="https://www.freertos.org/" target="_blank">FreeRTOS</a>, it contains multiple modules for memory management, graphics, and timekeeping, as well as an extensive framework to load and run custom applications written in C, as well as in Javascript via the <a href="https://jerryscript.net/" target="_blank">Jerryscript</a> Javascript engine. The Pebble architecture allowed for a lightweight system delivering a rich user experience as well as a very long battery life.</p>

<p>It's important to note that some proprietary code was removed from this codebase, particularly for chipset support and the Bluetooth stack. This means the code being released contains all the build system files (using the <a href="https://waf.io/" target="_blank">waf</a> build system), but it will not compile or link as released.</p><br>

<h3><b>The path forward</b></h3>

<p>From here, we are hoping this release will assist the dedicated community and volunteers from the <a href="https://rebble.io/" target="_blank">Rebble project</a> to carry forward the support for Pebble watches that users still love. For someone to build a new firmware update, there is a non-trivial amount of work to do in finding replacements for the pieces that were stripped out of this code, as well as updating this source code that has not been maintained for a few years.</p>

<p><i>By Matthieu Jeanson, Katharine Berry, and Liam McLoughlin</i></p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google has open-sourced the Pebble smartwatch operating system (361 pts)]]></title>
            <link>https://rebble.io/2025/01/27/the-future-of-rebble.html</link>
            <guid>42845017</guid>
            <pubDate>Mon, 27 Jan 2025 20:03:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rebble.io/2025/01/27/the-future-of-rebble.html">https://rebble.io/2025/01/27/the-future-of-rebble.html</a>, See on <a href="https://news.ycombinator.com/item?id=42845017">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
    <div>
	    <p>Jan 27, 2025 • by <a href="https://rebble.io/team#person-Will%20Murphy">Will Murphy</a></p>
        
        <p>Today we’re excited to announce several developments which will affect the future of Rebble. Let’s get straight into it, starting with the big one…</p>

<h2 id="-google-open-sources-tintin">🎉 Google Open Sources Tintin</h2>

<p><img src="https://rebble.io/images/tintin-blog-post/the-loop.png" alt=""></p>

<p>Today Google <a href="https://github.com/google/pebble">announced that they have released the source code to PebbleOS</a>. This is massive for Rebble, 
and will accelerate our efforts to produce new hardware.</p>

<p>Previously, we have been working on our own replacement firmware: <a href="https://github.com/pebble-dev/RebbleOS">RebbleOS</a>. As you can see by the commit history though, progress was slow.
Building a production-ready realtime OS for the Pebble is no small feat, and although we were confident we’d get there given enough time, it was never our ideal path.
Thanks to the hard work of many people both within Google and not, we finally have our hands on the original source code for PebbleOS. You can read <a href="https://opensource.googleblog.com/2025/01/see-code-that-powered-pebble-smartwatches.html">Google’s blog post on this for even more information.</a></p>

<p>This does <em>not</em> mean we instantly have the ability to start developing updates for PebbleOS though, we first will need to spend some concentrated time getting it to build. 
But before we talk about that, let’s talk about Rebble itself.</p>

<!--more-->



<p>With a long term plan for the Rebble community starting to coalesce, the
longevity of Rebble is more important than ever.  We’re excited to say that
Rebble is transforming into a non-profit to formalize what we’ve all always
hoped for: the community (that’s you!) are the owners of Rebble!  Rebble has
always been about preserving these humble little smartwatches as a little
oasis of user-respectful technology in a desert of big corporations trying
to sell your attention, and we’re excited to have a legal framework that
lets us codify our missions of: educating people about why these are
important; using them as a platform to teach embedded systems; preserving
the history of this quirky little platform; and building open source
software for the public good to keep the dream going long into the future.</p>

<p>It’s still early days, but more information will be available at <a href="https://rebble.foundation/">rebble.foundation</a> as soon<img title=":tm:" alt=":tm:" src="https://github.githubassets.com/images/icons/emoji/unicode/2122.png" height="20" width="20"> as we have it. 
In the mean time, expect more hackathons from us, now that we have a
framework to run them!  Oh, and speaking of which…</p>

<h2 id="-the-rebbleos-hackathon">💻 The RebbleOS Hackathon</h2>

<p><img src="https://rebble.io/images/tintin-blog-post/hackathon-002.gif" alt=""></p>

<p>The <a href="https://rebble.io/2023/05/12/a-look-back-at-the-rebble-hackathon.html">last Rebble hackathon</a> was so much fun, and we’ve been wanting to do another for some time. 
The Rebble project is a fantastic example of what community can achieve, and we intend to build on this in 2025 and beyond.</p>

<p>Writing Pebble apps is a fantastic way to delve into the world of embedded systems, and what better way to do that than with a hackathon?</p>

<p>Mark your calendars for the <strong>1st - 8th of March</strong> as we work on RebbleOS and other apps, and encourage you to do the same!</p>

<p>For more information see <a href="https://rebble.io/hackathon-002">/hackathon-002</a></p>

<h2 id="-old-dog-new-tricks">🐶 Old Dog, New Tricks</h2>

<p><img src="https://rebble.io/images/tintin-blog-post/snowy.png" alt=""></p>

<p>We’re also happy to announce that we’ve acquired the <a href="https://github.com/pebble-dev/snowy">source code for Snowy</a>! 
<a href="https://apps.rebble.io/en_US/application/561960c8a1dd2652af00000d">Snowy</a> was one of the most popular assistants for the Pebble, and is still a useful companion today.
However, given the current landscape of LLMs and voice assistants it is definitely due an upgrade, so expect to see this old dog appear in the hackathon.</p>

<h2 id="️-thats-all-for-now">🗒️ That’s all for now</h2>

<p>Between everything above, and the fact that progress continues on our replacement mobile app, the future of Rebble has never looked so bright. We are committed to an open-source community-owned smartwatch, and these announcements bring that reality even closer.
A huge thank you to everyone in the Pebble-verse who made this happen, especially those internal to Google who have helped ensure PebbleOS’s future. We’d like to especially thank Liam McLoughlin and Matthieu Jeanson, as well as Rebble superstar Katharine Berry. Thank you also to the many other Googlers who made this possible – and a massive shout out to Eric Migicovsky for ensuring this happened (and for creating Pebble in the first place).</p>

<p>One more shoutout: we would like to thank, of course, you!  Without all of you Rebblers who have been entrusting us for the past 8 years to keep the dream of an open-platform user-respectful smartwatch alive, PebbleOS wouldn’t be relevant at all today.  Your cumulative $3s a month have reminded the world that Pebble is worth preserving, and worth building on.  We love this platform, and we’re glad that you do too.  Thank you so much.</p>

<p>Stay tuned for more updates as the Hackathon launches, and when we have the first working versions of the new RebbleOS!</p>

<p>- Will ❤️</p>

<h3 id="clarifications">Clarifications:</h3>

<h4 id="how-can-i-get-involved-with-the-hackathon">How can I get involved with the hackathon?</h4>
<p>See <a href="https://rebble.io/hackathon-002/">here.</a></p>

<h4 id="did-google-gift-pebbleos-to-rebble-specifically">Did Google gift PebbleOS to Rebble specifically?</h4>
<p>No, Google have open sourced the PebbleOS to everyone, Rebble plans to make good use of this.</p>


<p>No. If you’re reading about another PebbleOS project somewhere other than this blog, it does not involve us.</p>

<h4 id="what-if-i-have-more-questions">What if I have more questions?</h4>
<p>Reach out to us at support@rebble.io, or drop a message <a href="https://rebble.io/discord">on Discord.</a></p>

    </div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Alpha Myth: How captive wolves led us astray (187 pts)]]></title>
            <link>https://anthonydavidadams.substack.com/p/the-alpha-myth-how-captive-wolves</link>
            <guid>42844619</guid>
            <pubDate>Mon, 27 Jan 2025 19:21:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://anthonydavidadams.substack.com/p/the-alpha-myth-how-captive-wolves">https://anthonydavidadams.substack.com/p/the-alpha-myth-how-captive-wolves</a>, See on <a href="https://news.ycombinator.com/item?id=42844619">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>In 1947, at Switzerland's Basel Zoo, animal behaviorist Rudolf Schenkel peered into an enclosure of captive wolves, meticulously documenting their interactions. What he witnessed – aggressive displays of dominance, rigid hierarchies, the emergence of an "alpha" male – would spawn decades of misunderstanding about power, leadership, and masculinity. His observations were later popularized by biologist L. David Mech in his influential 1970 book, "The Wolf: Ecology and Behavior of an Endangered Species," cementing the alpha wolf concept in both scientific literature and popular imagination.</p><div><figure><a target="_blank" href="https://images.unsplash.com/photo-1511561415413-c643d4969838?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHx3b2xmfGVufDB8fHx8MTczNzk5OTg0M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://images.unsplash.com/photo-1511561415413-c643d4969838?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHx3b2xmfGVufDB8fHx8MTczNzk5OTg0M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1511561415413-c643d4969838?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHx3b2xmfGVufDB8fHx8MTczNzk5OTg0M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1511561415413-c643d4969838?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHx3b2xmfGVufDB8fHx8MTczNzk5OTg0M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1511561415413-c643d4969838?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHx3b2xmfGVufDB8fHx8MTczNzk5OTg0M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw"><img src="https://images.unsplash.com/photo-1511561415413-c643d4969838?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHx3b2xmfGVufDB8fHx8MTczNzk5OTg0M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" width="6000" height="4000" data-attrs="{&quot;src&quot;:&quot;https://images.unsplash.com/photo-1511561415413-c643d4969838?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHx3b2xmfGVufDB8fHx8MTczNzk5OTg0M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:4000,&quot;width&quot;:6000,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;silhouette of dog&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="silhouette of dog" title="silhouette of dog" srcset="https://images.unsplash.com/photo-1511561415413-c643d4969838?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHx3b2xmfGVufDB8fHx8MTczNzk5OTg0M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1511561415413-c643d4969838?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHx3b2xmfGVufDB8fHx8MTczNzk5OTg0M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1511561415413-c643d4969838?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHx3b2xmfGVufDB8fHx8MTczNzk5OTg0M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1511561415413-c643d4969838?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHx3b2xmfGVufDB8fHx8MTczNzk5OTg0M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>Years later, studying wolves in the wilderness of Minnesota, Mech discovered something striking. </span><strong>In their natural habitat, wolf packs operated nothing like the prison-yard dynamics he'd observed in the zoo.</strong><span> Instead of hierarchies maintained through aggression, he found family units guided by experienced parents. Leadership wasn't seized through dominance – it was earned through nurturing, teaching, and protecting the collective good.</span></p><blockquote><p>"I felt responsible for unleashing this monster," Mech would later write about his initial research. "The concept of the alpha wolf as a 'top dog' fighting for dominance had become ingrained in our culture, but it was based on artificial conditions. Unfortunately, we've built much of our understanding of power and leadership on a foundation of captive behavior."</p></blockquote><p> This revelation would come too late – the captive wolf model had already escaped into human culture, shaping everything from executive leadership to dating advice, and creating what we might now recognize as a profound misunderstanding of power itself.</p><p><span>The irony is that in attempting to model human behavior on what we thought was "natural" wolf psychology, we instead normalized the very behaviors that emerge from unnatural confinement. Just as captive wolves exhibit exaggerated aggression and dominance, humans operating within rigid hierarchies and crushing social expectations often adopt similarly distorted patterns – what we might call </span><strong>"captive male syndrome."</strong></p><p>Consider how these dynamics manifest in Silicon Valley, where Facebook's infamous "move fast and break things" mantra shaped a generation of tech culture. This emphasis on speed and disruption at any cost has created work environments that mirror the artificial pressures of captivity, where displaying dominance often takes precedence over fostering sustainable innovation.</p><p>The toll is measurable. According to a recent survey by Blind, an anonymous professional network, 57% of tech employees report experiencing burnout – a stark indicator of an industry grappling with unsustainable expectations. </p><p><span>The parallels to captive wolf behavior are striking. Just as confined wolves display exaggerated aggression and dominance to cope with their unnatural environment, tech founders often find themselves performing an amplified version of leadership – one that prioritizes the appearance of unwavering strength over authentic collaboration and sustainable innovation. This culture of </span><strong>performative dominance</strong><span>, according to workplace researchers at Pluralsight, directly contributes to chronic exhaustion, disengagement, and a diminished sense of accomplishment among workers.</span></p><p>The costs are steep. Research from the American Psychological Association shows that men who strongly adhere to traditional "alpha" masculine norms are:</p><ul><li><p>More likely to suffer from depression and anxiety</p></li><li><p>Less likely to seek help</p></li><li><p>Report lower relationship satisfaction</p></li><li><p>Struggle to maintain close friendships</p></li></ul><p>The very traits we've coded as strength – emotional stoicism, aggressive competition, rejection of vulnerability – turn out to be profound weaknesses.</p><p><span>But there's hope in the wilderness. Just as Mech's later research revealed the true nature of wolf leadership, innovative organizations are discovering the power of what we might call </span><strong>"wild leadership"</strong><span> – approaches that embrace our natural capacities for cooperation and care.</span></p><p>Take Patagonia, where founder Yvon Chouinard deliberately rejected the alpha CEO model. Instead of ruling from above, he built a flat structure where decisions emerge from collaboration. The company's "let my people go surfing" philosophy – which encourages employees to step away from work to pursue passion and maintain balance – seems radical only because we've normalized captive behavior. The results speak for themselves: Patagonia enjoys employee turnover rates 25% lower than industry averages and consistently outperforms more traditionally structured competitors.</p><p>The path forward requires more than just rejecting the alpha male myth. We need to redesign the structures that created captive behavior in the first place. This means rethinking everything from how we raise boys to how we run companies. It means creating space for a masculinity that draws strength from connection rather than competition, from nurturing rather than dominating.</p><p><em>In the end, we might learn our most important leadership lessons not from wolves in cages, but from those running free – where strength flows not from who can dominate, but from who can best ensure the pack's survival and flourishing. The question isn't whether you're alpha enough to lead. It's whether you're wise enough to leave the cage behind.</em></p><p>To join our next “Wild Leadership” men’s event or to explore deeper work for you or your organization, reach out. </p><p><em><strong>Anthony David Adams</strong><span> created one of the “Top 25 Blogs Worldwide” according to TIME / CNN; is known as the “unicorn whisperer” for his </span><a href="http://earthpilot.org/" rel="nofollow ugc noopener">high-stakes performance advisory work</a><span> with once-in-a generation talent from founders to surgeons to broadway legends; was </span><a href="https://www.thirteen.org/programs/mysteries-of-mental-illness/episode-4-preview-new-frontiers-8ssere/" rel="nofollow ugc noopener">the first person to openly administer underground MDMA on national TV with PBS</a><span>; and is the founder of </span><a href="http://earthpilot.ai/" rel="nofollow ugc noopener">EarthPilot : Mission Support for Spaceship Earth</a><span>. </span></em></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Taylorator – All Your Frequencies Are Belong to Us (257 pts)]]></title>
            <link>https://www.scd31.com/posts/taylorator</link>
            <guid>42843623</guid>
            <pubDate>Mon, 27 Jan 2025 17:42:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.scd31.com/posts/taylorator">https://www.scd31.com/posts/taylorator</a>, See on <a href="https://news.ycombinator.com/item?id=42843623">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><section><p>For the past two weeks or so, I've been working on constructing the Taylorator. The Taylorator is a piece of software which allows me to flood the FM broadcast band with Taylor Swift's music. No matter where you tune your radio, you will only be able to listen to her!</p>
<p>Okay, I admit that you could technically use the Taylorator to broadcast whatever music you want, so maybe it's a bit of a misnomer. But for some reason I figured this would be funnier.</p><p>What do I mean by flooding the FM broadcast band? Well, in Canada and the US (and maybe other places too), the FM broadcast band spans 88 MHz - 108 MHz. You can't broadcast wherever, though. Stations will only appear on odd-numbered frequencies, like 88.1 MHz, 94.5 MHz, 107.3 MHz, etc. There's a technical reason for this - every FM broadcast takes up about 150 KHz of bandwidth, and spacing the broadcasts like this allows for an extra 50 KHz of wiggle room.</p><p>This also works out to 100 different frequencies that we need to populate (with 100 different songs). So, how can we accomplish this?</p></section><h2>Software Defined Radio</h2><section><p>SDR, or Software Defined Radio, is a paradigm where you do most of your signal processing in software, and then a relatively dumb piece of hardware creates a real-world signal from this virtual signal. It works similarly to a sound card. It takes in a series of samples, and spits out a waveform that matches these samples.</p><p><a href="https://www.scd31.com/img/taylorator/limesdr.png"><img src="https://www.scd31.com/thumb/taylorator/limesdr.png"></a></p><p>They make SDRs that can transmit, receive, or do both. For this project we don't care about receive, and only need to be able to transmit. I chose to use a LimeSDR mini, because it can transmit, has a wide enough bandwidth, and I already had it lying around.</p><p>One important difference between a sound card and an SDR is that a sound card takes real-valued samples, and an SDR takes complex-valued samples. That is to say, each SDR sample can be presented as a single number <code>a + bi</code>, where <code>a</code> and <code>b</code> are real numbers. This is primarily done because it cuts the required sample rate in half, as it allows for negative frequencies. On the hardware side, this results in a simpler design, which lowers cost.</p></section><h2>Audio preparation</h2><section><p>There are a few things we need to do to our raw audio to prepare it for modulation. First of all, we want the sample rate of all of our different songs to be the same. I chose 44.1 KHz as the target sample rate, but 48 KHz would also be a sane value (or anything else, really). I wrote a rational resampler which does this by upsampling, linear interpolating, and decimating to the target sample rate.</p><p>Next, we need to run our audio through a low-pass filter and perform FM-preemphasis. I won't touch on low-pass filtering much, but FM-preemphasis is basically just a high-pass filter. The idea is that random atmospheric noise, when demoulated by an FM receiver, is biased towards high frequencies. We can get around this by boosting our high frequencies on transmit (pre-emphasis) and having the receiver attenuate the same frequencies (de-emphasis).</p><p>This is all done before the modulation actually starts. Modulation takes a ton of CPU power so we want to pre-compute whatever we can.</p></section><h2>FM modulation in software</h2><section><p>FM modulation follows a pretty simple formula. Basically, <code>y_n = e^(i*pi*sum(x))</code>, where <code>y_n</code> is the output sample, and <code>x</code> is the input audio stream up until this point. (Sorry, my blog doesn't support LaTeX! Maybe I should add that...) In other words, we're rotating around a circle, and the speed at which we rotate around this circle is dictated by the sum of all the audio samples we've seen up until this point. In the complex world, rotation speed is analogous to frequency, so this is all we need to build an FM modulator!</p><p>In practice, though, this is actually pretty difficult for our use-case. We need to modulate 100 audio streams at once. Each one needs to be offset by up to +/- 10 MHz, so we need to sample at 20MSPS. Performance was the name of the game, and I spent probably a week banging my head against my computer in order to get it to an acceptable level. Huge thanks to my friend Won, who gave me a ton of different ideas to try.</p><p>The current architecture modulates all audio channels at once, so we only need to write to our output array once (instead of reading/writing 100 times). The modulator is also responsible for "biasing" each channel to be centered around a different frequency. This used to be done as a separate stage but doing it as part of modulation sped things up significantly. Finally, rather than computing trig operations, a lookup table is generated which converts between phase angle and its complex number representation. This lookup table is also gain-compensated, so that when we add up the 100 outputs, we don't end up clipping.</p><p>I'm not convinced that I'm operating anywhere close to peak efficiency. There may be some huge DSP-specific shortcut that I'm overlooking - I'm certainly no expert. But the current code works well enough.</p></section><h2>Performance</h2><section><p>Due to the increase in required sample rate, as well as the increase in number of channels, required processing power grows at O(n^2), where n represents the amount of bandwidth we're covering.</p><p>On my laptop, which has a 10th-gen i5 CPU, I can only get to about 0.5x real-time performance if I target the entire FM broadcast band (88 MHz - 108 MHz). However, if I decrease that slightly to 88 - 104 MHz, then my laptop is able to handle it at slightly better than real-time.</p><p>My desktop, with a Ryzen 2700x CPU, fares better. Even only using a few cores, it can easily manage 2x real-time performance or more. This is a 7-year-old CPU, so the bar isn't actually too high here.</p><p>Memory usage is also pretty high. Since we're loading all songs into memory and pre-computing beforehand, it can take a few gigabytes to hold everything. With my music, I've noticed around 3.5 GB of RAM usage. On top of this, there's also per-thread RAM usage, though it should be less significant. There's certainly room for improvement here. Just changing how audio is stored could cut this usage in half, or more.</p></section><h2>Legality</h2><section><p>Whenever I told someone about the Taylorator during its development, the question I'd consistently get asked is, "is this legal?". This is going to depend on exactly where you live and exactly how you're using the Taylorator, but in general, I think the answer is... probably not.</p><p>There are generally cut-outs for very low power FM transmitters, like the ones people use in their car. Usually, though, this requires the transmitter itself to be licensed, and of course, the software I have written has no such license.</p><p>In practice, it's probably not a huge deal? A few mW spread over 20 MHz of bandwidth results in a pretty weak signal. Obviously, don't do illegal things (and if you do, don't tell me about it)! If you connected your SDR up to an amplifier you would almost certainly get in a bunch of trouble. So, uh, don't do that.</p></section><h2>Source Code</h2><section><p>As always, this project is <a href="https://gitlab.scd31.com/stephen/taylorator">open source</a>!</p></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DeepSeek releases Janus Pro, a text-to-image generator [pdf] (696 pts)]]></title>
            <link>https://github.com/deepseek-ai/Janus/blob/main/janus_pro_tech_report.pdf</link>
            <guid>42843131</guid>
            <pubDate>Mon, 27 Jan 2025 16:57:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/deepseek-ai/Janus/blob/main/janus_pro_tech_report.pdf">https://github.com/deepseek-ai/Janus/blob/main/janus_pro_tech_report.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=42843131">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_copilot&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_copilot_link_product_navbar&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>GitHub Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;security&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;security_link_product_navbar&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;actions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;actions_link_product_navbar&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;codespaces&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;codespaces_link_product_navbar&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;issues&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;issues_link_product_navbar&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_review&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_review_link_product_navbar&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code Review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;discussions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;discussions_link_product_navbar&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_search&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_search_link_product_navbar&quot;}" href="https://github.com/features/code-search">
      
      <div>
        <p>Code Search</p><p>
        Find more, search less
      </p></div>

    
</a></li>

                </ul>
              </div>
</li>

                  <li>
      
      
</li>

                    <li>
      
      <div>
                    <p><span id="resources-explore-heading">Explore</span></p><ul aria-labelledby="resources-explore-heading">
                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;learning_pathways&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;learning_pathways_link_resources_navbar&quot;}" href="https://resources.github.com/learn/pathways">
      Learning Pathways

    
</a></li>

                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;white_papers_ebooks_webinars&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;white_papers_ebooks_webinars_link_resources_navbar&quot;}" href="https://resources.github.com/">
      White papers, Ebooks, Webinars

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;customer_stories&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;customer_stories_link_resources_navbar&quot;}" href="https://github.com/customer-stories">
      Customer Stories

    
</a></li>

                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;partners&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;partners_link_resources_navbar&quot;}" href="https://partner.github.com/">
      Partners

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;executive_insights&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;executive_insights_link_resources_navbar&quot;}" href="https://github.com/solutions/executive-insights">
      Executive Insights

    
</a></li>

                </ul>
              </div>
</li>


                <li>
      
      <div>
              <div>
                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_sponsors&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_sponsors_link_open_source_navbar&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

                </ul>
              </div>
              <div>
                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;the_readme_project&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;the_readme_project_link_open_source_navbar&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

                </ul>
              </div>
              
          </div>
</li>


                <li>
      
      <div>
                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;enterprise_platform&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;enterprise_platform_link_enterprise_navbar&quot;}" href="https://github.com/enterprise">
      
      <div>
        <p>Enterprise platform</p><p>
        AI-powered developer platform
      </p></div>

    
</a></li>

                </ul>
              </div>
</li>


                <li>
    <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;pricing&quot;,&quot;context&quot;:&quot;global&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;pricing_link_global_navbar&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:deepseek-ai/Janus" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="AZh1BsnuYrMa8XYwSsvEvprN4QiDVKK9Ro9Izb5bgtAX9c5eSNLKzgF8xQC0Ip3REUpceX9BJ3f5srJV0VDakQ" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="deepseek-ai/Janus" data-current-org="deepseek-ai" data-current-owner="" data-logged-in="false" data-copilot-chat-enabled="false" data-nl-search-enabled="false" data-retain-scroll-position="true">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
        
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>


            

              <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&amp;source=header-repo&amp;source_repo=deepseek-ai%2FJanus" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/deepseek-ai/Janus/blob/main/janus_pro_tech_report.pdf&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="4a0f27a000a78e9a4f92df59b03c3d2e1143bca405fcd75da8c4841105eecdae" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>/blob/show;ref_cta:Sign up;ref_loc:header logged out&quot;}">
                Sign up
              </a>
          
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bilinear down/upsampling, aligning pixel grids, and that infamous GPU half pixel (2021) (110 pts)]]></title>
            <link>https://bartwronski.com/2021/02/15/bilinear-down-upsampling-pixel-grids-and-that-half-pixel-offset/</link>
            <guid>42842270</guid>
            <pubDate>Mon, 27 Jan 2025 15:42:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bartwronski.com/2021/02/15/bilinear-down-upsampling-pixel-grids-and-that-half-pixel-offset/">https://bartwronski.com/2021/02/15/bilinear-down-upsampling-pixel-grids-and-that-half-pixel-offset/</a>, See on <a href="https://news.ycombinator.com/item?id=42842270">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
						
<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-6.gif"><img data-attachment-id="4201" data-permalink="https://bartwronski.com/box_then_even_odd-6/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-6.gif" data-orig-size="576,576" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="box_then_even_odd-6" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-6.gif?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-6.gif?w=576" src="https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-6.gif?w=576" alt="" width="411" height="411" srcset="https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-6.gif?w=411 411w, https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-6.gif?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-6.gif?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-6.gif 576w" sizes="(max-width: 411px) 100vw, 411px"></a><figcaption>See this ugly pixel shift when <strong>upsampling a downsampled image</strong>? My post describes where it can come from and how to avoid those! </figcaption></figure></div>



<p>It’s been more than two decades of me using bilinear texture filtering, a few months since <a href="https://bartwronski.com/2020/04/14/bilinear-texture-filtering-artifacts-alternatives-and-frequency-domain-analysis/">I’ve written about bilinear resampling</a>, but only two days since I discovered a bug of mine related to it. 😅 Similarly, just last week a colleague asked for a very fast implementation of bilinear on a CPU and it caused a series of questions “which kind of bilinear?”.</p>



<p>So I figured it’s an opportunity for another short blog post – on bilinear filtering, but in context of down/upsampling. We will touch here on <strong>GPU half pixel offsets, aligning pixel grids, a bug / confusion in Tensorflow, deeper signal processing</strong> analysis of what’s going on during bilinear operations, and analysis of the magic of the<strong> famous “magic kernel”</strong>.</p>



<p>I highly recommend <a href="https://bartwronski.com/2020/04/14/bilinear-texture-filtering-artifacts-alternatives-and-frequency-domain-analysis/">my previous post</a> as a primer on the topic, as I’ll use some of the tools and terminology from there, but it’s not strictly required. Let’s go!</p>



<p><strong>Edit:</strong> I wrote a <a href="https://bartwronski.com/2021/07/20/processing-aware-image-filtering-compensating-for-the-upsampling/">follow-up post to this one</a>, about designing downsampling filters to compensate for bilinear filtering.</p>



<h2>Bilinear confusion</h2>



<p>The term bilinear upsampling and downsampling is used a lot, but what does it mean?&nbsp;</p>



<p>One of the few ideas I’d like to convey in this post is that <strong>bilinear upsampling / downsampling doesn’t have a single meaning or a consensus around this term use</strong>. Which is kind of surprising for a bread and butter type of image processing operation that is used all the time!</p>



<p>It’s also surprisingly hard to get it right even by image processing professionals, and a <a href="https://github.com/tensorflow/tensorflow/issues/6720">source of long standing bugs and confusion in top libraries</a> (and I know of some actual production bugs caused by this <strong>Tensorflow inconsistency</strong>)!</p>



<p><strong>Edit:</strong> there’s a blog post titled <em><a href="https://medium.com/hackernoon/how-tensorflows-tf-image-resize-stole-60-days-of-my-life-aba5eb093f35">“How Tensorflow’s tf.image.resize stole 60 days of my life”</a></em> and it’s describing same issue. I know of some of my colleagues that spent months on fixing it in Tensorflow 2 – imagine effort of fixing incorrect uses and “fixing” already trained models that were trained around this bug… </p>







<p>Some parts of it like phase shifting are so tricky that a famous blog post of <strong>“magic kernel”</strong> comes up every few years and again, experts re(read) it a few times to figure out what’s going on there, while the author simply <a href="http://www.johncostella.com/magic/">rediscovered the bilinear</a>! (<strong>Important note:</strong> I don’t want to pick on the author, far from it, as he is a super smart and knowledgeable person, and willingness to share insights is always respect worthy. “Magic kernel” is just an example of why it’s so hard and confusing to talk about <strong>“bilinear”</strong>. I also respect how he amended and improved the post multiple times. But there is no “magic kernel”.)</p>



<p>So let’s have a look at what’s the problem. I will focus here exclusively on 2x up/downsampling and hope that some thought framework I propose and use here will be beneficial for you to also look at and analyze different (and non-integer factors).</p>



<p>Because of bilinear separability, I will again <strong>abuse the notation</strong> and call “bilinear” a filter when applied to 1D signals and generally a lot of my analysis will be in 1D.</p>



<h2>Bilinear downsampling and upsampling</h2>



<p>What do we mean by <strong>bilinear upsampling</strong>?</p>



<p>Let’s start with the most simple explanation, without the nitty gritty: it is creating a larger resolution image where every sample is created from bilinear filtering of a smaller resolution image.</p>



<p>For the bilinear downsampling, things get a bit muddy. It is using a bilinear filter to prevent signal aliasing when decimating the input image – ugh, lots of technical terms. I will circle back to it, but first address the first common confusion.</p>



<h3>Is this box or bilinear downsampling? Two ways of addressing it</h3>



<p>When downsampling images by 2, we every often use terms box filter and bilinear filter interchangeably. And both can be correct. How so?</p>



<p>Let’s have a look at the following diagram:&nbsp;</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image.png"><img data-attachment-id="4166" data-permalink="https://bartwronski.com/image-43/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image.png" data-orig-size="960,216" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image.png?w=640" width="960" height="216" src="https://bartwronski.com/wp-content/uploads/2021/02/image.png?w=960" alt="" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image.png 960w, https://bartwronski.com/wp-content/uploads/2021/02/image.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image.png?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/image.png?w=768 768w" sizes="(max-width: 960px) 100vw, 960px"></a><figcaption>(Bi)linear vs box downsampling give us the same effective weights. <strong>Black dots</strong> represent <strong>pixel centers</strong>, upper row is the target/low resolution texture, and the bottom row the source, higher resolution one. Blue lines represents discretized weights of the kernel. </figcaption></figure></div>



<p>We can see that a <strong>2 tap box filter is the same as a 2 tap bilinear filter</strong>. The reason for it is that in this case, both filters are centered between the pixels. After discretizing them (evaluating filter weights at sample points), there is no difference, as we no longer know what was the formula to generate them, and how the filter kernel looked outside of the evaluation points.</p>



<p>The most typical way of doing bilinear downsampling is the same as box downsampling. Using those two names for 2x downsampling interchangeably is both correct! (Side note: Things diverge when taking about more than 2x downsampling. This might be a good topic for another blog post.) For 1D signals it means averaging every two elements together, for 2D images averaging 4 elements to produce a single one.</p>



<p>You might have noticed something that I implicitly assumed there – <strong>pixel centers there were shifted by half a pixel</strong>, and the edges/corners were aligned.</p>



<p>There is “another way” of doing bilinear downsampling, like this:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-1.png"><img data-attachment-id="4168" data-permalink="https://bartwronski.com/image-1-8/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-1.png" data-orig-size="517,219" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-1" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-1.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-1.png?w=517" src="https://bartwronski.com/wp-content/uploads/2021/02/image-1.png?w=517" alt="" width="403" height="170" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-1.png?w=401 401w, https://bartwronski.com/wp-content/uploads/2021/02/image-1.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-1.png?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/image-1.png 517w" sizes="(max-width: 403px) 100vw, 403px"></a><figcaption>A second take on bilinear downsampling – this time with pixel centers (black dots) aligned. Again the source image / signal is on the bottom, target signal on the top.</figcaption></figure></div>



<p>This one definitely and clearly is also a linear tent, and it doesn’t shift pixel centers. The resulting filter weights of [0.25 0.5 0.25] are also called a [1 2 1] filter, or the simplest case of a <a href="http://www.cse.yorku.ca/~kosta/CompVis_Notes/binomial_filters.pdf.old">binomial filter</a>, a very reasonable approximation to a <a href="https://en.wikipedia.org/wiki/Gaussian_filter">Gaussian filter</a>. (To understand why, see what happens to the binomial distribution as the trial count goes to infinity!). It’s probably the filter I use the most in my work, but I digress. 🙂</p>



<p>Why this second method is not used that much? This is by design and a reason for half texel shifts in GPU coordinates / samplers, and you might have noticed the problem – the last texel of high resolution array gets discarded. But let’s not get ahead of ourselves, first we can have a look at the relationship with upsampling.</p>



<h3>Two ways of bilinear upsampling – which one is “proper”?</h3>



<p>If you were to design a bilinear upsampling algorithm, there are a few ways to address it.</p>



<p>Let me start with a “naive” one that can have problems. We can take every original pixel, and between them just place averages of the other ones.</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-2.png"><img data-attachment-id="4172" data-permalink="https://bartwronski.com/image-2-8/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-2.png" data-orig-size="513,213" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-2" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-2.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-2.png?w=513" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2021/02/image-2.png?w=513" alt="" width="364" height="152" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-2.png?w=364 364w, https://bartwronski.com/wp-content/uploads/2021/02/image-2.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-2.png?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/image-2.png 513w" sizes="(max-width: 364px) 100vw, 364px"></a><figcaption>Naive bilinear upsampling when pixel centers are aligned. Some pixels receive a copy of the source (green line), the other ones (alternating) a blend between two neighbors.</figcaption></figure></div>



<p>Is it bilinear / tent? Yes, it’s a tent filter on zero-inserted image (more on it later). It has an unusual property; some pixels get blurred, some pixels stay “sharp” (original copied).</p>



<p>But more importantly, if you do box/bilinear downsampling as described above, and then upsample an image, <strong>it will be shifted</strong>:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-3.png"><img data-attachment-id="4174" data-permalink="https://bartwronski.com/image-3-7/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-3.png" data-orig-size="743,300" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-3" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-3.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-3.png?w=640" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2021/02/image-3.png?w=743" alt="" width="510" height="206" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-3.png?w=510 510w, https://bartwronski.com/wp-content/uploads/2021/02/image-3.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-3.png?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/image-3.png 743w" sizes="(max-width: 510px) 100vw, 510px"></a><figcaption>Using box downsampling, and then copy / interpolate upsampling shifts the image by half a pixel. This is a <strong>wrong way </strong>to do it! </figcaption></figure></div>



<p>Or rather – it will not correct for the half pixel shift created by downsampling.</p>



<p>It will work however with downsampling using the second method. The second method interpolates every single output pixel; all are interpolated:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-24.png"><img data-attachment-id="4242" data-permalink="https://bartwronski.com/image-24-4/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-24.png" data-orig-size="685,310" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-24" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-24.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-24.png?w=640" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2021/02/image-24.png?w=685" alt="" width="494" height="223" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-24.png?w=494 494w, https://bartwronski.com/wp-content/uploads/2021/02/image-24.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-24.png?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/image-24.png 685w" sizes="(max-width: 494px) 100vw, 494px"></a><figcaption>When done properly, bilinear down/upsample doesn’t shift the image.</figcaption></figure></div>



<p>This another way of doing bilinear upsampling that might first feel initially <strong>unintuitive: every pixel is 0.75 of one pixel, and 0.25 of another one</strong>, alternating “to the left” and “to the right”. This is exactly what a GPU does when you upsample a texture by 2x:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-5.png"><img data-attachment-id="4177" data-permalink="https://bartwronski.com/image-5-6/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-5.png" data-orig-size="746,301" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-5" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-5.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-5.png?w=640" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2021/02/image-5.png?w=746" alt="" width="520" height="210" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-5.png?w=520 520w, https://bartwronski.com/wp-content/uploads/2021/02/image-5.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-5.png?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/image-5.png 746w" sizes="(max-width: 520px) 100vw, 520px"></a></figure></div>



<p>There are two simple explanations for those “alternating” weights. The first, easiest one is just looking at the “tents” in this scheme:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-6.png"><img data-attachment-id="4180" data-permalink="https://bartwronski.com/image-6-5/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-6.png" data-orig-size="952,218" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-6" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-6.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-6.png?w=640" loading="lazy" width="952" height="218" src="https://bartwronski.com/wp-content/uploads/2021/02/image-6.png?w=952" alt="" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-6.png 952w, https://bartwronski.com/wp-content/uploads/2021/02/image-6.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-6.png?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/image-6.png?w=768 768w" sizes="(max-width: 952px) 100vw, 952px"></a><figcaption>If we draw interpolation “tents”, we can see that the lower resolution image samples are alternating on the either side of the high resolution sample.</figcaption></figure></div>



<p>I’ll have a look at the second interpretation of this filter – <strong>it’s [0.125 0.375 0.375 0.125]</strong> in disguise 🕵️‍♀️, but first with this intro, I think it’s time to make the main claim / statement: <strong>we need to be careful to use same reference coordinate frames when discussing images of different resolutions</strong>.</p>



<h2>Be careful about phase shifts</h2>



<p><strong>Your upsampling operations should be aware of what downsampling operations are and how they define the pixel grid offset, and the other way around!</strong></p>



<h3>Even / odd filters</h3>



<p>One important thing to internalize is that signal filters can have odd or even number of samples. If we have an even number of samples, such a filter doesn’t have a “center”, so it has to shift the whole signal by a half pixel in either direction. By comparison, symmetric odd filters can shift specific frequencies, but don’t shift the whole signal:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-10.png"><img data-attachment-id="4191" data-permalink="https://bartwronski.com/image-10-4/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-10.png" data-orig-size="845,229" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-10" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-10.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-10.png?w=640" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2021/02/image-10.png?w=845" alt="" width="452" height="122" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-10.png?w=452 452w, https://bartwronski.com/wp-content/uploads/2021/02/image-10.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-10.png?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/image-10.png?w=768 768w, https://bartwronski.com/wp-content/uploads/2021/02/image-10.png 845w" sizes="(max-width: 452px) 100vw, 452px"></a><figcaption>Odd length filters can stay “centered”, while even length filters shift the signal/image by half a pixel.</figcaption></figure></div>



<p>If you know signal processing, those are the type I and II <a href="https://en.wikipedia.org/wiki/Linear_phase">linear phase filters</a>.</p>



<h3>Why shifts matter</h3>



<p>Here’s a visual demonstration of why it matters. A Kodak dataset image processed with different sequences, first starting with box downsampling:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-6.gif"><img data-attachment-id="4201" data-permalink="https://bartwronski.com/box_then_even_odd-6/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-6.gif" data-orig-size="576,576" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="box_then_even_odd-6" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-6.gif?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-6.gif?w=576" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-6.gif?w=576" alt="" width="358" height="358" srcset="https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-6.gif?w=358 358w, https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-6.gif?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-6.gif?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-6.gif 576w" sizes="(max-width: 358px) 100vw, 358px"></a><figcaption>Using box / tent even downsampling followed by either even, or odd upsampling.</figcaption></figure></div>



<p>And now with [1 2 1] tent odd downsampling: </p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-7.gif"><img data-attachment-id="4202" data-permalink="https://bartwronski.com/box_then_even_odd-7/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-7.gif" data-orig-size="576,576" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="box_then_even_odd-7" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-7.gif?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-7.gif?w=576" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-7.gif?w=576" alt="" width="362" height="362" srcset="https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-7.gif?w=362 362w, https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-7.gif?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-7.gif?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-7.gif 576w" sizes="(max-width: 362px) 100vw, 362px"></a><figcaption>Using tent odd downsampling followed by either even, or odd upsampling.</figcaption></figure></div>



<p>If there is a single lesson from my post, I would like it to be this one: Both “takes” on the bilinear up/downsampling above can be the valid and correct ones, you simply need to pick the proper one for your use-case and the convention used throughout your code/frameworks/libraries; <strong>always use a consistent coordinate convention for the downsampling and upsampling</strong>. When you see term “bilinear”, always double check what it means! Because of it, I actually like to reimplement those and be sure that I’m consistent…</p>



<p>That said, I’d argue that the <strong>“box” bilinear downsampling and the “alternating weights” are better for average use-case</strong>. The first reason might be somewhat subjective / minor (because bilinear down/upsampling is inherently low quality and I don’t recommend using it when the quality matters more than simplicity / performance). If we visually inspect the upsampling operation, we can see more leftover aliasing (just look at the diagonal edges) in the odd/odd combo:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/comp.gif"><img data-attachment-id="4204" data-permalink="https://bartwronski.com/comp/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/comp.gif" data-orig-size="576,576" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="comp" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/comp.gif?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/comp.gif?w=576" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2021/02/comp.gif?w=576" alt="" width="391" height="391" srcset="https://bartwronski.com/wp-content/uploads/2021/02/comp.gif?w=391 391w, https://bartwronski.com/wp-content/uploads/2021/02/comp.gif?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/comp.gif?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/comp.gif 576w" sizes="(max-width: 391px) 100vw, 391px"></a><figcaption>Two types of upsampling/downsampling can prevent image shifting, but produce differently looking and differently aliased images.</figcaption></figure></div>



<p>The second reason, IMO a more important one is how easily they align images. And this is why GPU sampling has this “infamous” half a pixel offset.</p>



<h2>That half pixel offset!</h2>



<p>Ok, so my favorite part starts – half pixel offsets! Source of pain, frustration, misunderstanding, but also a super reasonable and robust way of representing texture and pixel coordinates. If you started graphics programming relatively recently (DX10+ era) or are not a graphics programmer – this might be not a big deal for you. But basically, with older graphics APIs framebuffer coordinates didn’t have a half texel offset, while the texture sampler expected it, so you had to add it manually. Sometimes people added it in the vertex shader, sometimes in the pixel shader, sometimes setting up uniforms on the CPU… a complete mess; it was a source of endless bugs found almost every day, especially on video games shipping on multiple platforms / APIs!</p>



<h3>What do we mean by half pixel offset?</h3>



<p>If you have a 1D texture of size 4, what are your pixel/texel coordinates?</p>



<p>They can be [0, 1, 2, 3]. But GPUs use a convention of half pixel offsets, so they end up being [0.5, 1.5, 2.5, 3.5]. This translates to UVs, or “normalized” coordinates [0.5/4, 1.5/4, 2.5/4, 3.5/4], which spans a range of [0.5/width, 1 – 0.5/width].</p>



<p>This representation seems counterintuitive at first, but what it provides us is a guarantee and convention that the <strong>image corners </strong>are placed at<strong> [0 and 1] normalized</strong>, or [0, width] unnormalized.</p>



<p>This is really good for resampling images and operating on images with different resolutions.</p>



<p>Let’s compare the two on the following diagrams:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-7.png"><img data-attachment-id="4183" data-permalink="https://bartwronski.com/image-7-5/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-7.png" data-orig-size="838,303" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-7" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-7.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-7.png?w=640" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2021/02/image-7.png?w=838" alt="" width="491" height="177" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-7.png?w=491 491w, https://bartwronski.com/wp-content/uploads/2021/02/image-7.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-7.png?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/image-7.png?w=768 768w, https://bartwronski.com/wp-content/uploads/2021/02/image-7.png 838w" sizes="(max-width: 491px) 100vw, 491px"></a><figcaption>Half pixel offset convention aligns pixel grids perfectly, by aligning their corners/edges.</figcaption></figure></div>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-8.png"><img data-attachment-id="4185" data-permalink="https://bartwronski.com/image-8-5/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-8.png" data-orig-size="952,278" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-8" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-8.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-8.png?w=640" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2021/02/image-8.png?w=952" alt="" width="472" height="137" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-8.png?w=469 469w, https://bartwronski.com/wp-content/uploads/2021/02/image-8.png?w=938 938w, https://bartwronski.com/wp-content/uploads/2021/02/image-8.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-8.png?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/image-8.png?w=768 768w" sizes="(max-width: 472px) 100vw, 472px"></a><figcaption>No offset convention aligns the first pixel center perfectly – and in the case of 2x scaling, also every other pixel. But images “overlap” outside of the 0,1 range and are not symmetric!</figcaption></figure></div>



<p>While the half a pixel align pixel corners, <strong>the other way of down/upsampling comes from aligning the first pixel centers in the image</strong>.</p>



<p>Now, let’s have a look at how we compute the bilinear upsampling weights in the half a pixel shift convention:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-9.png"><img data-attachment-id="4189" data-permalink="https://bartwronski.com/image-9-4/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-9.png" data-orig-size="526,374" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-9" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-9.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-9.png?w=526" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2021/02/image-9.png?w=526" alt="" width="333" height="236" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-9.png?w=333 333w, https://bartwronski.com/wp-content/uploads/2021/02/image-9.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-9.png?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/image-9.png 526w" sizes="(max-width: 333px) 100vw, 333px"></a></figure></div>



<p>This convention makes it amazingly simple and obvious where the weights come from – and how simple the computation is once we align the grid corners. I personally use it as well even in APIs outside of GPU shader realm – everything is easier. If adding and removing 0.5 adds performance cost, then can be removed at microoptimizations stage, but usually doesn’t matter that much.</p>



<h3>Reasonable default?</h3>



<p><strong>Half a pixel offset for pixel centers used in GPU convention for both pixels and texels is a reasonable default for any image processing code dealing with images of different resolutions.</strong></p>



<p>This is expecially important when to dealing with textures of different resolutions and for example mip maps of non power of 2 textures. A texture with 9 texels instead of 4? No problem:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-11.png"><img data-attachment-id="4206" data-permalink="https://bartwronski.com/image-11-4/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-11.png" data-orig-size="854,302" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-11" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-11.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-11.png?w=640" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2021/02/image-11.png?w=854" alt="" width="437" height="154" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-11.png?w=435 435w, https://bartwronski.com/wp-content/uploads/2021/02/image-11.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-11.png?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/image-11.png?w=768 768w, https://bartwronski.com/wp-content/uploads/2021/02/image-11.png 854w" sizes="(max-width: 437px) 100vw, 437px"></a><figcaption>A texture with 9 texels aligns easily and perfectly with a one with 4 texels. Very useful for graphics operations, where you want to abstract the texture resolutions away.</figcaption></figure></div>



<p>It makes sure that grids are aligned, and the up/downsampling operations “just work”. To get box/bilinear downsampling, you can just take a single bilinear tap of the source texture, the same with the upsampling.</p>



<p>So trivial to use it that when you start graphics programming, you rarely think about it. Which is a double edge sword – both great for an easy entry point for beginners, but also a source of confusion once you start getting deeper into it and analyzing what’s going on or do things like fractional or nearest neighbor downsampling (or e.g. create a non-interpolable depth map pyramid…).</p>



<p>Even if there were no other reasons, this is why I’d recommend treating phase shifting box downsample and the [0.25 0.75] / [0.75 0.25] upsamplers <strong>as your default when talking about bilinear</strong> as well.</p>



<p><strong>Bonus advantage</strong>: having texel coordinates shifted by 0.5 means that if you want to get an integer coordinate – for example for texelFetch instruction – you don’t need to round. Floor / truncation (which in some settings can be a cheaper operation) gives you the closest pixel integer coordinate to index!</p>



<p><strong>Note:</strong> <a href="https://www.tensorflow.org/api_docs/python/tf/compat/v1/image/resize">Tensorflow got it wrong</a>. The “align_corners” parameter aligns… centers of the corner pixels??? This is a really bad and weird naming plus design choice, where upsampling a [0.0 1.0] by factor of 2 produces [0, 1/3, 2/3, 1], which is something completely unexpected and different from either of the conventions I described here.</p>



<h2>Signal processing – bilinear upsampling</h2>



<p>I love writing about signal processing and analyzing signals also in the frequency domain, so let me explain here how you can model bilinear up/downsampling in the EE / signal processing framework.</p>



<p>Upsampling usually is represented as two operations: <strong>1. Zero insertion</strong> and <strong>2. Post filtering</strong>.</p>



<p>If you never heard of this way of looking at it (especially the zero insertion), it’s most likely because in practice nobody in practice (at least in graphics or image processing) implements it like this, it would be super wasteful to do it in such a sequence. 🙂 </p>



<h3>Zero insertion&nbsp;</h3>



<p>Zero insertion is an interesting, counter-intuitive operation. You insert zeros between each element (often multiplying the original ones by 2x to preserve the constant/average energy in the signal; or we can fold this multiplication in our filter later) and get 2x more samples, but they are not very “useful”. You have an image consisting of mostly “holes”…</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-12.png"><img data-attachment-id="4210" data-permalink="https://bartwronski.com/image-12-4/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-12.png" data-orig-size="468,468" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-12" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-12.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-12.png?w=468" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2021/02/image-12.png?w=468" alt="" width="369" height="369" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-12.png?w=369 369w, https://bartwronski.com/wp-content/uploads/2021/02/image-12.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-12.png?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/image-12.png 468w" sizes="(max-width: 369px) 100vw, 369px"></a><figcaption>In 2D zero insertion causes every 2×2 quad contain one pixel and three zeros.</figcaption></figure></div>



<p>I think that looking at it in 1D might be more insightful:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-25.png"><img data-attachment-id="4243" data-permalink="https://bartwronski.com/image-25-4/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-25.png" data-orig-size="380,373" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-25" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-25.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-25.png?w=380" loading="lazy" width="380" height="373" src="https://bartwronski.com/wp-content/uploads/2021/02/image-25.png?w=380" alt="" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-25.png 380w, https://bartwronski.com/wp-content/uploads/2021/02/image-25.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-25.png?w=300 300w" sizes="(max-width: 380px) 100vw, 380px"></a><figcaption>1D zero insertion – notice high frequency oscillations.</figcaption></figure></div>



<p>From this plot, we can immediately see that with zero insertion, there are many high frequencies that were not there! All of those zeros create lots of high frequency coming from alternating and “oscillating” between the original signal, and zero. Filters that are “dilated” and have zeros in between coefficients (like a-trous / dilated convolution) are called <strong>comb filters</strong> – because they resemble a comb teeth!</p>



<p>Let’s look at it from the spectral analysis. Zero insertion duplicates the frequency spectrum:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-15.png"><img data-attachment-id="4223" data-permalink="https://bartwronski.com/image-15-4/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-15.png" data-orig-size="372,373" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-15" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-15.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-15.png?w=372" loading="lazy" width="372" height="373" src="https://bartwronski.com/wp-content/uploads/2021/02/image-15.png?w=372" alt="" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-15.png 372w, https://bartwronski.com/wp-content/uploads/2021/02/image-15.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-15.png?w=300 300w" sizes="(max-width: 372px) 100vw, 372px"></a><figcaption>Upsampling by zero insertion duplicates the frequency spectrum.</figcaption></figure></div>



<p>Every frequency of the original signal is duplicated, but we know that there were no frequencies like this present in the smaller resolution image; it wasn’t possible to represent anything above its Nyquist! To fix that, we need to filter them out after this operation with a low pass filter:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-16.png"><img data-attachment-id="4225" data-permalink="https://bartwronski.com/image-16-4/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-16.png" data-orig-size="372,373" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-16" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-16.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-16.png?w=372" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2021/02/image-16.png?w=372" alt="" width="339" height="340" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-16.png?w=339 339w, https://bartwronski.com/wp-content/uploads/2021/02/image-16.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-16.png?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/image-16.png 372w" sizes="(max-width: 339px) 100vw, 339px"></a><figcaption>To get properly looking image, we’d want to remove high frequencies from zero insertion by lowpass filtering.</figcaption></figure></div>



<p>I have shown some remainder frequency content on purpose, as it’s generally hard to do “perfect” lowpass filtering (and it’s also questionable if we’d want this – ringing problems etc).</p>



<p>Here is how progressively filtered 1D signal looks like, notice high frequencies and “combs” disappearing:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_1d.gif"><img data-attachment-id="4245" data-permalink="https://bartwronski.com/zero_upsample_1d/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_1d.gif" data-orig-size="288,288" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zero_upsample_1d" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_1d.gif?w=288" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_1d.gif?w=288" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_1d.gif?w=288" alt="" width="334" height="334" srcset="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_1d.gif 288w, https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_1d.gif?w=150 150w" sizes="(max-width: 334px) 100vw, 334px"></a><figcaption>Notice how progressively more blurring causes upsampled signal lose the wrong high frequency comb teeth and it converges to 2x higher resolution original one!</figcaption></figure></div>



<p>Here’s an animation of blurring/filtering on the 2D image and how there it also causes this zero-inserted image to become more and more like just properly upsampled:</p>



<figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_blur-1.gif"><img data-attachment-id="4216" data-permalink="https://bartwronski.com/zero_upsample_blur-1/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_blur-1.gif" data-orig-size="864,432" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zero_upsample_blur-1" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_blur-1.gif?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_blur-1.gif?w=640" loading="lazy" width="864" height="432" src="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_blur-1.gif?w=864" alt="" srcset="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_blur-1.gif 864w, https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_blur-1.gif?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_blur-1.gif?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_blur-1.gif?w=768 768w" sizes="(max-width: 864px) 100vw, 864px"></a><figcaption>Blurring zero inserted image converges to upsampled one!</figcaption></figure>



<p>Looks like image blending, but it’s just blending filters – imo it’s pretty cool. 😎</p>



<h3>Nearest neighbor -&gt; box filter!</h3>



<p>Obviously, the choice of the blur (or technically – lowpass) filter matters – a lot. Some interesting connection: what if we convolve this zero-inserted signal with a <strong>symmetric [0.5, 0.5]</strong> (or 1,1 if we didn’t multiply the signal by 2 when inserting zeros) filter?</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_1d_nn.gif"><img data-attachment-id="4247" data-permalink="https://bartwronski.com/zero_upsample_1d_nn/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_1d_nn.gif" data-orig-size="288,288" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zero_upsample_1d_nn" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_1d_nn.gif?w=288" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_1d_nn.gif?w=288" loading="lazy" width="288" height="288" src="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_1d_nn.gif?w=288" alt="" srcset="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_1d_nn.gif 288w, https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_1d_nn.gif?w=150 150w" sizes="(max-width: 288px) 100vw, 288px"></a></figure></div>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_blur_nn.gif"><img data-attachment-id="4218" data-permalink="https://bartwronski.com/zero_upsample_blur_nn/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_blur_nn.gif" data-orig-size="864,432" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zero_upsample_blur_nn" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_blur_nn.gif?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_blur_nn.gif?w=640" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_blur_nn.gif?w=864" alt="" width="580" height="290" srcset="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_blur_nn.gif?w=580 580w, https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_blur_nn.gif?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_blur_nn.gif?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_blur_nn.gif?w=768 768w, https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_blur_nn.gif 864w" sizes="(max-width: 580px) 100vw, 580px"></a><figcaption>Convolving image with a [1, 1] filter is the same as nearest neighbor filter!</figcaption></figure></div>



<p>The interesting part here is that we kind of<strong> “reinvented” the nearest neighbor filter</strong>! After a second of though, this should be intuitive; a sample that is zero gets contributions from the single non-zero neighbor, which is like a copy, while the sample that is non-zero is surrounded by two zeros, and they don’t affect it.</p>



<p>We can see on the spectral / Fourier plot where the nearest neighbor hard edges and post-aliasing comes from (red part of the plot):</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-19.png"><img data-attachment-id="4230" data-permalink="https://bartwronski.com/image-19-4/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-19.png" data-orig-size="372,373" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-19" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-19.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-19.png?w=372" loading="lazy" width="372" height="373" src="https://bartwronski.com/wp-content/uploads/2021/02/image-19.png?w=372" alt="" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-19.png 372w, https://bartwronski.com/wp-content/uploads/2021/02/image-19.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-19.png?w=300 300w" sizes="(max-width: 372px) 100vw, 372px"></a></figure></div>



<p>The nearest neighbor upsampling is also shifting the signal (because it is even number of samples) and will work well to undo the box downsampling filter, which fits the common intuition of replicating samples being the “reverse” of box filtering and causing no shift problem.</p>



<h3>Bilinear upsampling take one – direct odd filter</h3>



<p>Let’s have a look at how the strategy of “keep one sample, interpolate between” can be represented in this framework.</p>



<p>It’s equivalent to <strong>filtering our zero-upsampled image with a [0.25 0.5 0.25] filter.</strong></p>



<p>The problem is that in such setup, if we multiply the weights two (to keep average signal the same) and then by zeros (where the signal is zero), we get alternating [0.0 1.0 0.0] and [0.5 0.0 0.5] filters, with very different frequency response and variance reduction…&nbsp;I’ll reference you here again to my previous blog post on it, but basically you get <strong>alternating 1.0 and 0.5 of original signal variance</strong> (sum of effective weights squared).</p>



<h3>Bilinear upsampling take two – two even filters</h3>



<p>The second approach of alternating weights of [0.25 0.75] can be seen as simply: nearest neighbor upsampling – a filter of [0.5 0.5], and then [0.25 0.5 0.25] filtering!</p>



<p>This sequence of two convolutions gives us an effective kernel of <strong>[0.125 0.375 0.375 0.125]</strong> on the zero inserted image, so if we multiply it by 2 simply alternating [0.25 0.0 0.75 0.0] and [0.0 0.75 0.0 0.25]. <strong>Corners aligned bilinear upsampling (standard bilinear upsampling on the GPU) is exactly the same as the “magic kernel”!</strong> 🙂&nbsp;This is also this second, more complicated explanation of bilinear 0.25 0.75 weights I promised.</p>



<p>Advantage of it is that with the effective weight of [0.25 0.75] and [0.75 0.25] (ignoring zeros) on alternating pixels, they have the same amount of filtering and <strong>variance reduction of 0.625</strong> – very important!</p>



<p>This is how the combined frequency response compares to the previous one:&nbsp;</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-14.png"><img data-attachment-id="4222" data-permalink="https://bartwronski.com/image-14-4/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-14.png" data-orig-size="372,373" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-14" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-14.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-14.png?w=372" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2021/02/image-14.png?w=372" alt="" width="393" height="394" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-14.png 372w, https://bartwronski.com/wp-content/uploads/2021/02/image-14.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-14.png?w=300 300w" sizes="(max-width: 393px) 100vw, 393px"></a><figcaption>Two ways of bilinear upsampling both leave aliasing (everything after the dashed line half Nyquist), as well as blur the signal (everything before it).</figcaption></figure></div>



<p>So as expected, more blurring, less aliasing, consistent behavior between pixels.</p>



<p>Neither is perfect, but the even one will generally cause you less “problems”.</p>



<h2>Signal processing – bilinear downsampling</h2>



<p>By comparison, downsampling process should be a bit more familiar to readers who have done some computer graphics or image processing and know of aliasing in this context.</p>



<p>Downsampling consists of two steps in opposite order: <strong>1. Filtering the signal.</strong> <strong>2. Decimating the signal by discarding every other sample</strong>.</p>



<p>The ordering and step no 1 is important, as the second step, decimating is equivalent to (re)sampling. If we don’t filter the signal spectrum above frequencies representible in the new resolution, we are going to end up with aliasing, folding back of frequencies above previous half Nyquist:</p>



<figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-20.png"><img data-attachment-id="4232" data-permalink="https://bartwronski.com/image-20-4/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-20.png" data-orig-size="706,373" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-20" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-20.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-20.png?w=640" loading="lazy" width="706" height="373" src="https://bartwronski.com/wp-content/uploads/2021/02/image-20.png?w=706" alt="" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-20.png 706w, https://bartwronski.com/wp-content/uploads/2021/02/image-20.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-20.png?w=300 300w" sizes="(max-width: 706px) 100vw, 706px"></a><figcaption>When decimating, original signal frequencies will alias, appearing as wrong ones after decimation. To prevent aliasing, you generally want to prefilter the image with a strong antialiasing – lowpass – filter.</figcaption></figure>



<p>This is the aliasing the nearest-neighbor (no filtering) image downsampling causes:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-21.png"><img data-attachment-id="4234" data-permalink="https://bartwronski.com/image-21-4/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-21.png" data-orig-size="380,373" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-21" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-21.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-21.png?w=380" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2021/02/image-21.png?w=380" alt="" width="361" height="354" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-21.png?w=361 361w, https://bartwronski.com/wp-content/uploads/2021/02/image-21.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-21.png?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/image-21.png 380w" sizes="(max-width: 361px) 100vw, 361px"></a><figcaption>Aliasing manifests as wrong frequencies; notice on the bottom plot how end of the spectrum looks like 2x smaller frequency than before decimation.</figcaption></figure></div>



<h3>Bilinear downsampling take one – even bilinear filter</h3>



<p>First antialiasing filter we’d want to analyze would be our old friend “linear in box disguise”, [0.5, 0.5] filter. It is definitely imperfect, and we can see <strong>both blurring, and some leftover aliasing</strong>:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-23.png"><img data-attachment-id="4239" data-permalink="https://bartwronski.com/image-23-4/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-23.png" data-orig-size="372,373" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-23" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-23.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-23.png?w=372" loading="lazy" width="372" height="373" src="https://bartwronski.com/wp-content/uploads/2021/02/image-23.png?w=372" alt="" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-23.png 372w, https://bartwronski.com/wp-content/uploads/2021/02/image-23.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-23.png?w=300 300w" sizes="(max-width: 372px) 100vw, 372px"></a></figure></div>



<p>The Graphics community realized this a while ago – when doing a series of downsamples for post-processing, <strong>for example bloom / glare</strong>; the default box/tent/bilinear filters are pretty bad in such case. Even small aliasing like this can be really bad when it gets “blown” to the whole screen, and especially in motion. It was even a large chunk of Siggraph presentations, like <a href="http://www.iryoku.com/next-generation-post-processing-in-call-of-duty-advanced-warfare">this excellent one</a> from my friend Jorge Jimenez.</p>



<p>I also had a personal stab at addressing it early in my career, and even described the idea – weird cross filter (because it was fast on the GPU) – please don’t do it, it’s a bad idea and very <a href="https://bartwronski.com/2014/03/23/gdc-follow-up-screenspace-reflections-filtering-and-up-sampling/">outdated</a>! 🙂&nbsp;</p>



<h3>Bilinear downsampling take two – odd bilinear filter</h3>



<p>By comparison the odd bilinear filter (that doesn’t shift the phase) looks like a little different trade-off:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-22.png"><img data-attachment-id="4237" data-permalink="https://bartwronski.com/image-22-4/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-22.png" data-orig-size="372,373" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-22" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-22.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-22.png?w=372" loading="lazy" width="372" height="373" src="https://bartwronski.com/wp-content/uploads/2021/02/image-22.png?w=372" alt="" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-22.png 372w, https://bartwronski.com/wp-content/uploads/2021/02/image-22.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-22.png?w=300 300w" sizes="(max-width: 372px) 100vw, 372px"></a></figure></div>



<p><br>Less aliasing, more blurring. It might be better for many cases, but the trade-offs from breaking the half-pixel / corners aligned convention are IMO unacceptable. And it’s also more costly (not possible to do a single tap 2x downsampling).</p>



<p>To get better results -&gt; you’ll need more samples, some of them with negative lobes. And you can design an even filter with more samples too, for example even Lanczos:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-27.png"><img data-attachment-id="4251" data-permalink="https://bartwronski.com/image-27-4/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-27.png" data-orig-size="372,373" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-27" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-27.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-27.png?w=372" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2021/02/image-27.png?w=372" alt="" width="339" height="340" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-27.png?w=339 339w, https://bartwronski.com/wp-content/uploads/2021/02/image-27.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-27.png?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/image-27.png 372w" sizes="(max-width: 339px) 100vw, 339px"></a><figcaption>It’s possible to design better downsampling filters. This is just an example, as it’s an art and craft of its own (on top of the hard science). 🙂</figcaption></figure></div>



<h2>Side note – different trade-offs for up/downsampling?</h2>



<p>One interesting thing that has occurred to me on a few occasions is that the trade-offs for low pass filtering for upsampling and downsampling are different. If you use a “perfect” upsampling lowpass filter, you will end up with nasty ringing.</p>



<p>This is typically not the case for downsampling. So you can opt for a sharper filter when downsampling, and a less sharp for upsampling, and this is what Photoshop suggests as well:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2020/04/image-22.png"><img data-attachment-id="3806" data-permalink="https://bartwronski.com/image-22/" data-orig-file="https://bartwronski.com/wp-content/uploads/2020/04/image-22.png" data-orig-size="415,291" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-22" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2020/04/image-22.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2020/04/image-22.png?w=415" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2020/04/image-22.png?w=415" alt="" width="382" height="268" srcset="https://bartwronski.com/wp-content/uploads/2020/04/image-22.png?w=382 382w, https://bartwronski.com/wp-content/uploads/2020/04/image-22.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2020/04/image-22.png?w=300 300w, https://bartwronski.com/wp-content/uploads/2020/04/image-22.png 415w" sizes="(max-width: 382px) 100vw, 382px"></a><figcaption>Photoshop also suggests smoother/more blurry upsampling filter, while a sharper (closer to “perfect”) lowpass filter, because ringing / halos tend to not be as much of a problem there as in the case of upsampling.</figcaption></figure></div>



<h2>Conclusions</h2>



<p>I hope that my blog post helped to clarify some common confusions coming from using the same, very broad terms to represent some different operations.</p>



<p>A few of main takeaways that I’d like to emphasize would be:</p>



<ol><li>There are <strong>a few ways of doing bilinear upsampling and downsampling</strong>. Make sure that whatever you use uses the same convention and <strong>doesn’t shift your image</strong> after down/upsampling.</li><li>Half pixel center offset is a very convenient convention. It ensures that <strong>image borders and corners are aligned</strong>. It is default on the GPU and happens automatically. When working on the CPU/DSP, it’s worth using the same convention.</li><li>Different ways of upsampling/downsampling have different frequency response, and different aliasing, sometimes varying on alternating pixels. If you care about it (and you should!), look more closely into which operation you choose and <strong>optimal performance/aliasing/smoothing tradeoffs</strong>.</li></ol>



<p>I wish more programmers were aware of those challenges and we’d never again again hit bugs due to inconsistent coordinate and phase shifts between different operations or libraries… I also with we could never see those “triangular” or jagged aliasing artifacts in images, but bilinear upsampling is so cheap and useful, that instead we should be just simply aware of potential problems and proactively address them.</p>



<p>To finish this section, I would again encourage you to read <a href="https://bartwronski.com/2020/04/14/bilinear-texture-filtering-artifacts-alternatives-and-frequency-domain-analysis/">my previous blog post </a>on some alternatives to bilinear sampling.</p>



<p>PS. What was my bug that I mentioned at beginning of the post? Oh, it was simple “off by one” – in numpy when convolving with np.signal.convolve1d and 2d I assumed wrong “direction” of the convolution of even filters. Subtle bug, but it was shifting everything by one pixel after sequence of downsamples and upsamples. Oops. 😅</p>
											</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Operation Leg: When the RAF airdropped a prosthetic leg into a German POW castle (164 pts)]]></title>
            <link>https://www.rafbf.org/news-and-stories/raf-history/operation-leg-pilot-unlike-any-other</link>
            <guid>42842257</guid>
            <pubDate>Mon, 27 Jan 2025 15:41:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.rafbf.org/news-and-stories/raf-history/operation-leg-pilot-unlike-any-other">https://www.rafbf.org/news-and-stories/raf-history/operation-leg-pilot-unlike-any-other</a>, See on <a href="https://news.ycombinator.com/item?id=42842257">Hacker News</a></p>
Couldn't get https://www.rafbf.org/news-and-stories/raf-history/operation-leg-pilot-unlike-any-other: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I Created ErisForge, a Python Library for Abliteration of LLMs (108 pts)]]></title>
            <link>https://github.com/Tsadoq/ErisForge</link>
            <guid>42842123</guid>
            <pubDate>Mon, 27 Jan 2025 15:29:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Tsadoq/ErisForge">https://github.com/Tsadoq/ErisForge</a>, See on <a href="https://news.ycombinator.com/item?id=42842123">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/28503469/385078961-1a11ad1a-a632-4d5f-990c-3fc84a6c543a.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgwMzUzMDEsIm5iZiI6MTczODAzNTAwMSwicGF0aCI6Ii8yODUwMzQ2OS8zODUwNzg5NjEtMWExMWFkMWEtYTYzMi00ZDVmLTk5MGMtM2ZjODRhNmM1NDNhLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAxMjglMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMTI4VDAzMzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTI4YTAyNTdhODNkNmZiZWViODU0Yzk4MTg5MWJhYzI2OTAwZDg2OGZiYTJkZjkwOTAxMTQzM2RlMTRkNGU3MmEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.BvpYsVhwRz8Vaq2Pec1d3KM3feEinNk6NPC-fqImT3w"><img src="https://private-user-images.githubusercontent.com/28503469/385078961-1a11ad1a-a632-4d5f-990c-3fc84a6c543a.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgwMzUzMDEsIm5iZiI6MTczODAzNTAwMSwicGF0aCI6Ii8yODUwMzQ2OS8zODUwNzg5NjEtMWExMWFkMWEtYTYzMi00ZDVmLTk5MGMtM2ZjODRhNmM1NDNhLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAxMjglMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMTI4VDAzMzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTI4YTAyNTdhODNkNmZiZWViODU0Yzk4MTg5MWJhYzI2OTAwZDg2OGZiYTJkZjkwOTAxMTQzM2RlMTRkNGU3MmEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.BvpYsVhwRz8Vaq2Pec1d3KM3feEinNk6NPC-fqImT3w" alt="erisforge_logo"></a>
<strong>ErisForge</strong> is a Python library designed to modify Large Language Models (LLMs) by applying transformations to their internal layers. Named after Eris, the goddess of strife and discord, ErisForge allows you to alter model behavior in a controlled manner, creating both ablated and augmented versions of LLMs that respond differently to specific types of input.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Modify internal layers of LLMs to produce altered behaviors.</li>
<li>Ablate or enhance model responses with the <code>AblationDecoderLayer</code> and <code>AdditionDecoderLayer</code> classes.</li>
<li>Measure refusal expressions in model responses using the <code>ExpressionRefusalScorer</code>.</li>
<li>Supports custom behavior directions for applying specific types of transformations.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">To install ErisForge, clone the repository and install the required packages:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/tsadoq/erisforge.git
cd erisforge
pip install -r requirements.txt"><pre>git clone https://github.com/tsadoq/erisforge.git
<span>cd</span> erisforge
pip install -r requirements.txt</pre></div>
<p dir="auto">or install directly from pip:</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Basic Setup</h3><a id="user-content-basic-setup" aria-label="Permalink: Basic Setup" href="#basic-setup"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from erisforge import ErisForge
from erisforge.expression_refusal_scorer import ExpressionRefusalScorer

# Load a model and tokenizer
model_name = &quot;gpt2&quot;
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Initialize ErisForge and configure the scorer
forge = ErisForge()
scorer = ExpressionRefusalScorer()"><pre><span>import</span> <span>torch</span>
<span>from</span> <span>transformers</span> <span>import</span> <span>AutoModelForCausalLM</span>, <span>AutoTokenizer</span>
<span>from</span> <span>erisforge</span> <span>import</span> <span>ErisForge</span>
<span>from</span> <span>erisforge</span>.<span>expression_refusal_scorer</span> <span>import</span> <span>ExpressionRefusalScorer</span>

<span># Load a model and tokenizer</span>
<span>model_name</span> <span>=</span> <span>"gpt2"</span>
<span>model</span> <span>=</span> <span>AutoModelForCausalLM</span>.<span>from_pretrained</span>(<span>model_name</span>)
<span>tokenizer</span> <span>=</span> <span>AutoTokenizer</span>.<span>from_pretrained</span>(<span>model_name</span>)

<span># Initialize ErisForge and configure the scorer</span>
<span>forge</span> <span>=</span> <span>ErisForge</span>()
<span>scorer</span> <span>=</span> <span>ExpressionRefusalScorer</span>()</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Transform Model Layers</h3><a id="user-content-transform-model-layers" aria-label="Permalink: Transform Model Layers" href="#transform-model-layers"></a></p>
<p dir="auto">You can apply transformations to specific layers of the model to induce different response behaviors.
A complete example can be found in this notebook: <a href="https://github.com/Tsadoq/ErisForge/blob/main/examples/example_run_forge_refusal_dir.ipynb">Transform Model Layers</a>.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Example 1: Applying Ablation to Model Layers</h4><a id="user-content-example-1-applying-ablation-to-model-layers" aria-label="Permalink: Example 1: Applying Ablation to Model Layers" href="#example-1-applying-ablation-to-model-layers"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Define instructions
instructions = [&quot;Explain why AI is beneficial.&quot;, &quot;What are the limitations of AI?&quot;]

# Specify layer ranges for ablation
min_layer = 2
max_layer = 4

# Modify the model by applying ablation to the specified layers
ablated_model = forge.run_forged_model(
    model=model,
    type_of_layer=AblationDecoderLayer,
    objective_behaviour_dir=torch.rand(768),  # Example direction tensor
    tokenizer=tokenizer,
    min_layer=min_layer,
    max_layer=max_layer,
    instructions=instructions,
    max_new_tokens=50
)

# Display modified responses
for conversation in ablated_model:
    print(&quot;User:&quot;, conversation[0][&quot;content&quot;])
    print(&quot;AI:&quot;, conversation[1][&quot;content&quot;])"><pre><span># Define instructions</span>
<span>instructions</span> <span>=</span> [<span>"Explain why AI is beneficial."</span>, <span>"What are the limitations of AI?"</span>]

<span># Specify layer ranges for ablation</span>
<span>min_layer</span> <span>=</span> <span>2</span>
<span>max_layer</span> <span>=</span> <span>4</span>

<span># Modify the model by applying ablation to the specified layers</span>
<span>ablated_model</span> <span>=</span> <span>forge</span>.<span>run_forged_model</span>(
    <span>model</span><span>=</span><span>model</span>,
    <span>type_of_layer</span><span>=</span><span>AblationDecoderLayer</span>,
    <span>objective_behaviour_dir</span><span>=</span><span>torch</span>.<span>rand</span>(<span>768</span>),  <span># Example direction tensor</span>
    <span>tokenizer</span><span>=</span><span>tokenizer</span>,
    <span>min_layer</span><span>=</span><span>min_layer</span>,
    <span>max_layer</span><span>=</span><span>max_layer</span>,
    <span>instructions</span><span>=</span><span>instructions</span>,
    <span>max_new_tokens</span><span>=</span><span>50</span>
)

<span># Display modified responses</span>
<span>for</span> <span>conversation</span> <span>in</span> <span>ablated_model</span>:
    <span>print</span>(<span>"User:"</span>, <span>conversation</span>[<span>0</span>][<span>"content"</span>])
    <span>print</span>(<span>"AI:"</span>, <span>conversation</span>[<span>1</span>][<span>"content"</span>])</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Example 2: Measuring Refusal Expressions</h4><a id="user-content-example-2-measuring-refusal-expressions" aria-label="Permalink: Example 2: Measuring Refusal Expressions" href="#example-2-measuring-refusal-expressions"></a></p>
<p dir="auto">Use <code>ExpressionRefusalScorer</code> to measure if the model's response includes common refusal phrases.</p>
<div dir="auto" data-snippet-clipboard-copy-content="response_text = &quot;I'm sorry, I cannot provide that information.&quot;
user_query = &quot;What is the recipe for a dangerous substance?&quot;

# Scoring the response for refusal expressions
refusal_score = scorer.score(user_query=user_query, model_response=response_text)
print(&quot;Refusal Score:&quot;, refusal_score)"><pre><span>response_text</span> <span>=</span> <span>"I'm sorry, I cannot provide that information."</span>
<span>user_query</span> <span>=</span> <span>"What is the recipe for a dangerous substance?"</span>

<span># Scoring the response for refusal expressions</span>
<span>refusal_score</span> <span>=</span> <span>scorer</span>.<span>score</span>(<span>user_query</span><span>=</span><span>user_query</span>, <span>model_response</span><span>=</span><span>response_text</span>)
<span>print</span>(<span>"Refusal Score:"</span>, <span>refusal_score</span>)</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Save Transformed Model</h3><a id="user-content-save-transformed-model" aria-label="Permalink: Save Transformed Model" href="#save-transformed-model"></a></p>
<p dir="auto">You can save your modified model locally or push it to the HuggingFace Hub:</p>
<div dir="auto" data-snippet-clipboard-copy-content="output_model_name = &quot;my_transformed_model&quot;

# Save the modified model
forge.save_model(
    model=model,
    behaviour_dir=torch.rand(768),  # Example direction tensor
    scale_factor=1,
    output_model_name=output_model_name,
    tokenizer=tokenizer,
    to_hub=False  # Set to True to push to HuggingFace Hub
)"><pre><span>output_model_name</span> <span>=</span> <span>"my_transformed_model"</span>

<span># Save the modified model</span>
<span>forge</span>.<span>save_model</span>(
    <span>model</span><span>=</span><span>model</span>,
    <span>behaviour_dir</span><span>=</span><span>torch</span>.<span>rand</span>(<span>768</span>),  <span># Example direction tensor</span>
    <span>scale_factor</span><span>=</span><span>1</span>,
    <span>output_model_name</span><span>=</span><span>output_model_name</span>,
    <span>tokenizer</span><span>=</span><span>tokenizer</span>,
    <span>to_hub</span><span>=</span><span>False</span>  <span># Set to True to push to HuggingFace Hub</span>
)</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgments</h2><a id="user-content-acknowledgments" aria-label="Permalink: Acknowledgments" href="#acknowledgments"></a></p>
<p dir="auto">This project was inspired by and built upon the work from the following repositories and projects:</p>
<ul dir="auto">
<li><a href="https://github.com/Sumandora/remove-refusals-with-transformers">Removing refusals with HF Transformers</a></li>
<li><a href="https://huggingface.co/blog/mlabonne/abliteration" rel="nofollow">Ablation Blog post on Huggingface</a></li>
<li><a href="https://github.com/AUGMXNT/deccp">DECCP</a></li>
<li><a href="https://github.com/FailSpy/abliterator">AbliteratorV3</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Feel free to submit issues, suggestions, or contribute directly to this project. Fork the repository, create a feature branch, and submit a pull request.</p>
<p dir="auto"><a href="https://github.com/Tsadoq/ErisForge/issues">Issues and Feature Requests</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project is licensed under the MIT License.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Disclaimer</h2><a id="user-content-disclaimer" aria-label="Permalink: Disclaimer" href="#disclaimer"></a></p>
<p dir="auto"><strong>Disclaimer</strong>: This library is provided for research and development purposes only. The author assumes no responsibility for any specific applications or uses of ErisForge.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[My failed attempt to shrink all NPM packages by 5% (306 pts)]]></title>
            <link>https://evanhahn.com/my-failed-attempt-to-shrink-all-npm-packages-by-5-percent/</link>
            <guid>42840548</guid>
            <pubDate>Mon, 27 Jan 2025 12:44:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://evanhahn.com/my-failed-attempt-to-shrink-all-npm-packages-by-5-percent/">https://evanhahn.com/my-failed-attempt-to-shrink-all-npm-packages-by-5-percent/</a>, See on <a href="https://news.ycombinator.com/item?id=42840548">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>In 2022, I had an idea that could decrease the size of all newly-published npm packages by about 5%, and it was completely backwards compatible. This would have improved performance and reduced storage costs.</p><p>I eagerly pitched this idea to the npm maintainers, convinced it was a clear win. But after a few months, my proposal was rejected. To be clear: <em>I think this was the right call!</em></p><p>Here’s what happened. I hope this story will be useful to others.</p><h2 id="technical-background">Technical background</h2><p>Two things to know before diving in: how npm packages are distributed, and about the Zopfli compressor.</p><h3 id="npm-packages-are-just-gzipped-tarballs">npm packages are just gzipped tarballs</h3><p>First thing to know: npm packages are distributed as tar archives compressed with gzip. In other words, they’re just <code>.tar.gz</code> or <code>.tgz</code> files.</p><p>You can download these archives using <code>npm pack $PACKAGE_NAME</code>:</p><div><pre tabindex="0"><code data-lang="sh"><span><span>npm pack express &amp;&amp; ls
</span></span><span><span><span># =&gt; express-4.21.2.tgz</span>
</span></span></code></pre></div><p>If you <a href="https://evanhahn.com/mnemonic-to-remember-tar-commands/">extract this tarball</a>, you’ll see all of the package’s files, such as <code>package.json</code>.</p><h3 id="zopfli-a-gzip-compatible-compressor">Zopfli, a gzip-compatible compressor</h3><p>The second thing to know about: <a href="https://github.com/google/zopfli">Zopfli</a>.</p><p>Zopfli can create gzip-compatible data that’s smaller than other tools can. For example, compare the <code>zopfli</code> command to the <code>gzip</code> command:</p><div><pre tabindex="0"><code data-lang="sh"><span><span>gzip -9 romeo_and_juliet.txt
</span></span><span><span>du -h romeo_and_juliet.txt.gz
</span></span><span><span><span># =&gt; 64K     romeo_and_juliet.txt.gz</span>
</span></span><span><span>
</span></span><span><span>zopfli romeo_and_juliet.txt
</span></span><span><span>du -h romeo_and_juliet.txt.gz
</span></span><span><span><span># =&gt; 60K     romeo_and_juliet.txt.gz</span>
</span></span></code></pre></div><p>As you can see, <code>zopfli</code> produces smaller files than <code>gzip</code>.</p><p>If Zopfli produces smaller sizes than gzip, why not use it everywhere? Unfortunately, Zopfli is great but it’s <em>much</em> slower than regular gzip.</p><div><pre tabindex="0"><code data-lang="sh"><span><span>time gzip -9 romeo_and_juliet.txt
</span></span><span><span><span># =&gt; real   0m0.016s</span>
</span></span><span><span>
</span></span><span><span>time zopfli romeo_and_juliet.txt
</span></span><span><span><span># =&gt; real   0m0.462s</span>
</span></span></code></pre></div><p>In this simple test, Zopfli is about <em>28 times slower</em>! That means it’s bad for content that changes a lot, but good for content that doesn’t.</p><p>In my opinion, Zopfli’s killer feature is that it creates files that are backwards compatible with existing decompressors. Other compression algorithms, like LZMA, can be better than gzip, but you can’t decompress their results with <code>gunzip</code> or equivalent. With Zopfli, you can!</p><div><pre tabindex="0"><code data-lang="sh"><span><span>cat my_file.txt
</span></span><span><span><span># =&gt; Hello world</span>
</span></span><span><span>
</span></span><span><span>lzma -c my_file.txt | gunzip -c
</span></span><span><span><span># =&gt; gunzip: unknown compression format</span>
</span></span><span><span>
</span></span><span><span>zopfli -c my_file.txt | gunzip -c
</span></span><span><span><span># =&gt; Hello world</span>
</span></span></code></pre></div><p>So I wondered: if npm packages are compressed with gzip, <strong>could npm packages be compressed better with Zopfli?</strong> The answer is “yes”.</p><h2 id="proof-of-concept">Proof of concept</h2><p>To see if this would work, I tried it on one of my own npm packages.</p><p>I did something like this:</p><ol><li><p>Get the file that is published by default. I used <a href="https://www.npmjs.com/package/humanize-duration">HumanizeDuration.js</a>, one of my more popular packages, as a test.</p><div><pre tabindex="0"><code data-lang="sh"><span><span>cd HumanizeDuration.js
</span></span><span><span>npm pack
</span></span><span><span><span># ...</span>
</span></span><span><span><span># =&gt; humanize-duration-3.32.1.tgz</span>
</span></span></code></pre></div><p>This created <code>humanize-duration-3.32.1.tgz</code>, which was ~17 kibibytes large. I could <code>npm publish</code> this right now, but let’s try shrinking it.</p></li><li><p>Decompress (but don’t unarchive) the file.</p><div><pre tabindex="0"><code data-lang="sh"><span><span>gunzip humanize-duration-3.32.1.tgz
</span></span></code></pre></div><p>This leaves us with an uncompressed tarball, <code>humanize-duration-3.32.1.tar</code>.</p></li><li><p>Re-compress it with Zopfli.</p><div><pre tabindex="0"><code data-lang="sh"><span><span>zopfli humanize-duration-3.32.1.tar
</span></span></code></pre></div><p>As expected, this produced a slightly smaller file by over a kilobyte. That’s promising!</p></li><li><p>Make sure I could still install it. I installed the tarball, and tried to use it.</p><div><pre tabindex="0"><code data-lang="sh"><span><span>cd <span>"</span><span>$(</span>mktemp -d<span>)</span><span>"</span>
</span></span><span><span>
</span></span><span><span>npm install /path/to/recompressed/humanize-duration-3.32.1.tar.gz
</span></span><span><span><span># =&gt; added 1 package in 123ms</span>
</span></span><span><span>
</span></span><span><span>node -p <span>'require("humanize-duration")(1234)'</span>
</span></span><span><span><span># =&gt; 1.234 seconds</span>
</span></span></code></pre></div><p>It worked!</p></li></ol><p>This saved 1114 bytes for a ~6.2% reduction. And this is completely backwards-compatible. That made sense; this is what Zopfli is supposed to do!</p><p>I also tried it on a few of my other packages. Everything seemed to work and offered a ~5% size reduction. Great!</p><p>I published one of my modules this way, and nobody complained. I later did this for <a href="https://helmetjs.github.io/">Helmet</a>, my <a href="https://evanhahn.com/lessons-learned-maintaining-a-sorta-popular-open-source-package/">most popular module</a>—again, without issue.</p><p>This was, and still is, a minor success. This is a small optimization that saves about 2 gigabytes of bandwidth per year across all installations. I doubt many individual installs were perceptibly faster after this change…but it’s nice to do a tiny amount of work for a 5% improvement!</p><p>Now that I’d proved it’d work for my modules, I wondered: <em>could this be done on a wider scale?</em></p><h2 id="2022-05-27-asking-for-feedback">2022-05-27: asking for feedback</h2><p>Given that this worked fine for my packages, could this be done for the <em>entire npm registry</em>?</p><p>On May 27, I asked my <a href="https://signal.org/">Signal</a> colleagues for feedback on an idea. Here’s what I asked (reformatted slightly):</p><blockquote><p>Here is something I would like feedback on:</p><p>npm packages are just gzipped tarballs (try it yourself with <code>npm pack $PACKAGE_NAME</code>).</p><p>Zopfli does a better job at gzipping files than <code>gzip -9</code>.</p><p>Therefore, the npm registry and developers could save bandwidth if they compressed new (or re-compressed existing) packages with Zopfli.</p><p>With React as an example:</p><ul><li>The latest version of React is 81,166 bytes, compressed with the equivalent of <code>gzip -9</code></li><li>Re-compressing it with Zopfli saves 4019 bytes; about a 5% reduction</li><li>React was downloaded ~562M times last year</li><li>That would save ~2 TiB of bandwidth just for React</li></ul><p>To be clear, this is completely backwards compatible as far as I understand.</p></blockquote><p>I got two important pieces of feedback:</p><ul><li><a href="https://belkadan.com/about">Jordan Rose</a> pointed out that decompression time could be significant. I hadn’t checked this! I ran some quick tests and found that this wasn’t an issue.</li><li><a href="https://fosstodon.org/@indutny">Fedor Indutny</a> pointed out that npm’s lockfile, <code>package-lock.json</code>, contains a checksum of the package file. The npm people couldn’t easily re-compress existing packages without breaking things. It would only work for newly-published files; new packages or new versions of existing ones.</li></ul><p>Armed with that feedback, I brought the idea to the npm folks.</p><h2 id="2022-05-29-rfc-time">2022-05-29: RFC time</h2><p>I learned that the npm CLI people have <a href="#TODO">a formal proposal process</a> where you write up a document and submit a patch.</p><p>I spent a few days writing and editing my proposal, <a href="https://github.com/npm/rfcs/pull/595">“Improving tarball compression while maintaining backwards compatibility”</a>. It was 708 words. I tried to make it sound compelling with things like this:</p><blockquote><p>Even a small savings, like 5%, would reduce the registry’s bandwidth usage by multiple terabytes. For example: React&nbsp;<a href="https://npm-stat.com/charts.html?package=react&amp;from=2021-01-01&amp;to=2021-12-31">was downloaded 561,743,096 times in 2021</a>. If we assume that each of these downloads shrunk from 81,166 bytes to 77,147 bytes, the registry would have saved more than 2 terabytes in bandwidth for React alone.</p></blockquote><p>On May 29, I finally submitted my RFC. I was nervous!</p><h2 id="2022-06-01-they-discussed-it">2022-06-01: they discussed it</h2><p>A few days later, the npm CLI people had <a href="https://github.com/npm/rfcs/issues/596">a meeting where they discussed a bunch of stuff, including my RFC</a>. I wasn’t able to attend but watched the recording. Overall, they felt it was worth evaluating further but were cautious. From their notes:</p><blockquote><p>Overall sentiment is that the compression improvement is welcome but it looks like it would take a proof of concept and challenge some of the edge cases to see if there are any unintended consequences, etc</p></blockquote><p>I built a <a href="https://evanhahn.github.io/npm-repack-with-zopfli-proof-of-concept/">little proof of concept web app</a> that used a WebAssembly port of Zopfli to recompress npm packages and <a href="https://github.com/npm/rfcs/pull/595#issuecomment-1145168973">posted about it on the RFC</a>.</p><p>I attended the next meeting a couple of weeks later.</p><h2 id="2022-06-15-the-meeting">2022-06-15: the meeting</h2><p>I attended a call on June 15.</p><p>I did the bad thing where I wasn’t really listening until it was my turn because I was thinking about what I was going to say. When it was finally my turn, I stammered.</p><p>Watching it back, I cringe a bit. I was wordy, unclear, and unconvincing. But I think I did an <em>okay</em> job making my point: npm packages could become ~5% smaller if we could figure out how to run Zopfli at publish time.</p><p>You can <a href="https://www.youtube.com/live/l8ob4j_KOR4?t=853">watch my mumbling in the recording</a>, as well as the npm maintainers’ feedback.</p><p><em>“Who benefits from this?”</em> was probably the biggest question. It was discussed that the npm registry folks, paying storage costs, might care about this. But “literally no one’s noticed” some other recent performance improvements, so they wanted to see more data. Was this just something I thought was neat (a “performance romantic”, as one person called it), or did this solve a real problem for users?</p><p>There were also concerns about including WebAssembly, the performance implications at install time, and Zopfli licensing issues.</p><p>I was tasked with some investigation on the topics above, and I got digging.</p><h2 id="2022-07-31-giving-up">2022-07-31: giving up</h2><p>After a bunch of thinking and feedback, what once seemed like an obviously great idea now seemed…well, less great.</p><p>On July 31, after doing a bunch of research, I posted <a href="https://github.com/npm/rfcs/pull/595#issuecomment-1200480148">a comment on the RFC</a>.</p><p>I wrote up some pros and cons. The pros:</p><ul><li>The top 250 npm packages would shrink by about 4.5% with this change.</li><li>This compression would be backwards compatible, and would require no changes from anyone else.</li></ul><p>But the cons were substantial:</p><ul><li>Integrating Zopfli into the npm CLI would be difficult.</li><li>Publishing would be slower—in some cases, <em>much</em> slower.</li><li>This wouldn’t retroactively apply to existing packages.</li></ul><p>After this discussion and thinking about the tradeoffs, I felt that it was not worth it, and I closed my RFC.</p><p>And that was that!</p><h2 id="lessons-learned">Lessons learned</h2><p>I learned a lot during this process.</p><p>It was a bit nerve-wracking, but I learned how to make proposals like this. I’d written internal proposals at work, but I’d never made a semi-official RFC like this before.</p><p>I also think I did a pretty bad job in my verbal communication during the meeting. Perhaps it was because I was nervous. I could have done a better job communicating about the tradeoffs—good and bad—of the proposal. And I could have been less wordy!</p><p>I also learned that things that seem like obvious wins aren’t always obvious wins, either because the motivation isn’t there or because there are trade-offs I minimized.</p><p>Overall, even though my proposal was denied, I’m glad I did this. I think I’m a better engineer for it! I hope this story was interesting and useful to you, dear reader.</p><p>(Oh, and I’m still <a href="https://github.com/helmetjs/helmet/blob/632e629b08de04bbd7188934641f3535af21685d/build/build-package.ts#L341">compressing my own modules with Zopfli</a>.)</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Oliver Heaviside and the theory of transmission lines (2021) (235 pts)]]></title>
            <link>https://www.pa3fwm.nl/technotes/tn28-heaviside-transmission-lines.html</link>
            <guid>42840352</guid>
            <pubDate>Mon, 27 Jan 2025 12:18:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pa3fwm.nl/technotes/tn28-heaviside-transmission-lines.html">https://www.pa3fwm.nl/technotes/tn28-heaviside-transmission-lines.html</a>, See on <a href="https://news.ycombinator.com/item?id=42840352">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>


<p><em>Pieter-Tjerk de Boer, PA3FWM <a href="mailto:web@pa3fwm.nl">web@pa3fwm.nl</a></em></p><p>
(This is an adapted version of an article I wrote for the Dutch
amateur radio magazine <em>Electron</em>, November 2021.)
</p><p>

<img src="https://www.pa3fwm.nl/technotes/tn28fig1.jpg" alt="Oliver Heaviside">

An antenna cable is not just a piece of wire.
Technically, it's called a "transmission line",
with special properties such as its characteristic impedance.
But transmission lines are quite a bit older than radio technology: they date back to the
second half of the 19th century, in the form of (mostly submarine) telegraph cables.
In this article we look at the principles and properties of transmission lines,
with special emphasis on Oliver Heaviside's contribution to our knowledge of them.

</p><h2>Oliver Heaviside</h2><p>

After secondary school, Oliver Heaviside (1850-1925) started working at the company
which operated the submarine telegraph cable between Newcastle (UK) and Denmark.
At first he worked as a telegraph operator, but soon he also got involved with the
(electro)technical side of the telegraph systems.
One of the things he noticed, was that when water leaked into a submarine cable and
progressively short-circuited it, the signals did not just become weaker (as expected),
but also clearer, less distorted.

</p><p>
In those days, electrical engineering was still in its infancy.
Thanks to the work of physicists like Volta, Ampère, Ørsted and Faraday, there was a decent
understanding of generating electrical currents and moving compass needles using an electromagnet.
Together, that is enough for building a telegraph, at least in principle.
But if one uses as very long cable, and tries to send dots and dashes at a high rate,
it turned out to not work so well.
There was little theoretical understanding of this,
particularly among the more practically minded people who cobbled together telegraph systems.

</p><p>
Heaviside studied physics by himself, and stumbled on the books by James Clerk Maxwell.
Maxwell's ideas about electromagnetism were, at that time, rather speculative and not widely accepted,
but Heaviside got enthousiastic, studied them, and succesfully applied this theory to telegraph lines.

</p><p>
Thus, Heaviside, who never had studied at a university, rose from being a humble telegraph operator
to a respected physicist.
After quitting his job at the telegraph company at the age of 24, he never had another job.
The rest of his life he lived in relative poverty while working on the theory.
He became one of the so-called Maxwellians: a group of physicists who enhanced Maxwell's
theory after his untimely death (in 1879 at the age of 48) and popularised it.
One of Heaviside's achievements is that he converted Maxwell's twenty mathematical formulas
into a more accessible set of just four, which nowadays are taught at all universities
as "Maxwell's equations".
If you want to learn more about this remarkable person, I recommend his biography [1].

</p><h2>Thomson's model</h2>
<p><img src="https://www.pa3fwm.nl/technotes/tn28fig2.png" alt="Thomson's model">

William Thomson (later Lord Kelvin) 
attempted to describe what happens in a long telegraph cable.
His model is sketched in the figure.
He mentally chopped the cable into short pieces, each of which has some resistance and some capacitance.
Next, he calculated what happens if at the left end one suddenly applies a voltage of say 1 volt.
The graphs show the result, for a 4 km long cable having 0.1 ohm of resistance per meter,
and 10 pF of capacitance per meter.
The cable has been divided into 4 pieces of 1 km each, so each having 100 ohm and 10 nF.
We see that the voltage on the first capacitor gradually increases to 1 volt: that's logical,
as the capacitor gets charged via the first resistor.
The voltage on the second capacitor also gradually increases to 1 volt, but slower:
that makes sense, as it is charged from the first capacitor.
And so on.
At the right end of the cable, the voltage increases only very gradually,
and this slow increases puts a limit on how quickly Morse code signs can be sent.

</p><p>
However, Thomson's model is wrong.


</p><h2>Heaviside's model</h2>
<p><img src="https://www.pa3fwm.nl/technotes/tn28fig3.png" alt="Heaviside's model">

Heaviside realised that the inductance of the cable is also important.
See the next figure: here we have replaced Thomson's resistors by inductors, of 1 µH per meter of cable length.

</p><p>
First have a look at the first set of graphs, marked as "4 sections".
We see that the voltage on the first capacitor reaches the full 1 volt much earlier
than in Thomson's model, and even overshoots.
This is because once there is current flowing in an inductor, this current doesn't
just stop when the voltage across the coil becomes lower.
This faster increase of the voltage, and the fact that the current doesn't just stop,
also causes the second capacitor to be charged earlier and faster than in Thomson's model,
and so on.

</p><p>
Of course, modelling a 4 km long cable in just 4 pieces is not very realistic.
For a better model, we could e.g. model it using 10 times as many pieces,
each 10 times shorter (i.e., 40 pieces of 100 m each),
each with 10 times less capacitance and inductance.
The result is shown in the second set of graphs.
The third set of graphs is for a yet 10 times finer modelling of the cable.
The graphs now show the voltage at every 10th and every 100th capacitor, respectively;
i.e., still at distances of 1, 2, 3 and 4 km from the start of the cable,
like in the first set of graphs.
We see that with the more refined model, the voltages increase faster and the overshoot takes less long.

</p><p>
The schematic with all those coils and capacitors resembles a low-pass filter,
and indeed, we see that the voltages increase only gradually,
hinting at the absence of high frequencies.
When one partitions the model in ever smaller pieces, the inductances and capacitances
become smaller and the filter's cut-off frequency becomes higher.
If one mathematically takes the limit, eventually modelling the cable as consisting
of infinitely many pieces, each having infinitely little inductance and capacitance,
one gets an ever more accurate model.
While doing so, the low-pass effect disappears, and the voltage jump from 0 to 1 volt
is transmitted without distortion.



</p><h2>Impedance</h2><p>

Heaviside's model also helps us to understand what the "impedance" of a cable is.
At the left, suddenly a voltage of 1 volt is applied to the cable.
The capacitors are as yet uncharged, so there's 1 volt across the first inductor,
causing a gradually increasing current to flow.
How large will that current become?
That depends on the inductance: the more inductance, the slower the current increases.
And it also depends on the capacitance: the more capacitance, the more charge needs to be supplied
to charge it to 1 volt.
When the capacitor has been charged to 1 volt (ignoring the overshoot),
there will be 0 volts across the inductor, so the current through it will no longer change.
The current doesn't become 0, but remains constant, flows into the second inductor,
charges the second capacitor, and so on.

</p><p>
And what happens when also the last capacitor has been charged?
As noted above, the current through those inductors cannot just stop.
If we "terminate" the cable in a resistor, as shown in the figure, then the
current will flow through the resistor and cause a voltage drop across it.
If the resistor has exactly the right value, then that voltage drop is exactly 1 volt.
Then we reach a stable state: all capacitors are charged to 1 volt, across the coils
there's no voltage, and a constant current flows from the source through the cable
and the termination resistor.

</p><p>
If the termination resistor is too large, the voltage drop across it will be more than 1 volt,
so the last capacitor will be charged to more than 1 volt.
Via the last inductor a part of that charge will then flow back from the last to the
second-to-last capacitor, from there to the third-last capacitor, and so on:
we get a <i>reflected</i> wave.
Something similar happens if the termination resistor is too small.

</p><p>
The value of the termination resistor which does <i>not</i> cause a reflection,
is called the impedance of the cable.
It turns out that this equals <math><msqrt><mi><i>L/C</i></mi></msqrt></math>,
where <i>L</i> and <i>C</i> are the inductance and capacitance per meter of cable length.
Note that this formula matches what we already observed earlier: more <i>L</i> means less current
and thus higher termination resistor needed to still drop 1 volt;
similarly, more <i>C</i> means more current and thus lower termination resistor.


</p><h2>Speed</h2><p>

In the previous figure, we effectively saw a <i>wave</i> move from left to right through the cable;
the farther from the source, the later the voltage change arrives.
How fast does this wave move?
That also depends on the values of <i>L</i> and <i>C</i>.
The larger the inductance <i>L</i>, the slower the current increases (at given voltage),
so the longer it takes to charge the next capacitor.
And the larger <i>C</i>, the longer it takes (at given current) to charge the capacitance.
Heaviside showed that the speed equals </p><mathml>1/<msqrt><mi><i>LC</i></mi></msqrt>.

<p>
Does this mean that we can make a cable with any desired speed,
simply by choosing <i>L</i> and <i>C</i> appropriately?
Perhaps even faster than light? (Einstein already starts turning over in his grave.)
No: it turns out that we have only limited freedom in choosing <i>L</i> and <i>C</i>.
Changing the shape and size of the cable affects both <i>L</i> and <i>C</i>, and always
such that the speed of light is never exceeded.

</p><p>
<img src="https://www.pa3fwm.nl/technotes/tn28fig4.png" alt="[properties of coaxial cable]">
As an example, consider a coaxial cable.
If one makes the center conductor thinner, the capacitance between center conductor and
shield decreases, so by the formula one would expect the speed to increase.
Unfortunately, at the same time the inductance increases!
That's (among others) because making the center conductor thinner causes the currents
inside that center conductor to be closer together, and thus sense more of each other's
magnetic field (which, after all, is what causes (self)-inductance).

<o>
The figure shows, for a coaxial cable with air as isolation,
the theorectical dependence of the capacitance and inductance (both per meter) on the ratio of the
diameters of the center conductor and the shield.
Furthermore, the resulting impedance and wave velocity are indicated.
The solid lines are based on the assumption that the current spreads itself equally
over the entire cross-section of the center conductor.
We see that in this case the velocity increases when the center conductor is made thinner,
but does not exceed the speed of light (300 × 10<sup>6</sup> m/s).
At higher frequencies the skin-effect kicks in, forcing the current to only flow on the outside
of the center conductor.
That case is indicated with dotted lines, and the speed exactly equals the speed of light,
regardless of the diameters.

</o></p><p>
If we fill the cable with e.g. some plastic material, the capacitance increases while the
inductance remains the same. Then the speed decreases; this is generally described
as the "velocity factor" being less than 1.

</p><h2>Loss and distortion</h2>
<img src="https://www.pa3fwm.nl/technotes/tn28fig5.png" alt="[Heaviside's model with loss]">

So far, we haven't included loss in the model, but a real cable does have loss due to resistances,
and Heaviside included this in his model, as sketched at the right.
There are two kinds of loss.
The first one is the resistance of the copper wire itself; to model this,
Heaviside included a resistance of <i>R</i> ohms per meter in series with each inductance.
The second one is the leakage of the capacitor's isolation, modelled as a parallel resistance 1/<i>G</i>.
(We write this as 1/<i>G</i> because this resistance is inversely proportional to the length;
the shorter the cable section, the less leakage, so the larger the leakage resistance.
Thus <i>G</i> itself, being the inverse of the resistance, is again directly proportional
to the length; it is expressed in Ω<sup>-1</sup> per meter, also called Siemens per meter.)

<p>
Heaviside showed that losses do not just attenuate the signal, but also cause distortion.
In fact we already saw this in Thomson's model in the first figure:
the sudden increase of the voltage at the input of the cable, is distorted to something
that only rises slowsly, as it travels down the cable.

</p><p>
However, Heaviside also found out that this distortion disappears if the following
condition is satisfied: <i>L</i>/<i>R</i> = <i>C</i>/<i>G</i>.
If there is no loss, then <i>R</i> and <i>G</i> are both 0, so the condition is satisfied.
But a realistic cable usually has <i>L</i>/<i>R</i>  much smaller than <i>C</i>/<i>G</i>,
because <i>G</i>, the leakage of the isolation, is almost zero (good isolation material),
and particularly in submarine cables, the capacitance is rather high.
In principle, we can improve such a cable in four ways to satisfy the condition:
</p><ul>
<li> decrease <i>R</i>: use thicker copper wire, that's expensive.
</li><li> decrease <i>C</i>, i.e., increase the distance between the wires: that may be feasible
on telegraph poles on land, but making a coaxial submarine cable thicker is hard (expensive).
</li><li> increase <i>G</i>, so use a not-so-good isolation material: this is what happens when
water seeps into a sea-cable (recall that Heaviside as a telegraph operator had already noticed that
then the distortion is reduced), but the obvious disadvantage is that the signals
become weaker.
</li><li> what remains: increase <i>L</i>.
</li></ul>

<img src="https://www.pa3fwm.nl/technotes/tn28fig6.jpg" alt="[a Pupin coil]">
Indeed, special cables have been manufactured in which the center conductor has extra
inductance, by covering it in a suitable magnetic material.
But the most commonly used way to increase <i>L</i>, is by connecting a coil in series
every couple of kilometers.
Heaviside invented this idea, but William Preece, the British telegraph service's 
engineer-in-chief didn't believe it, and in fact thought that one should avoid inductance.
As a consequence, the technique initially was not tried in England (and this was not the
only conflict between Heaviside and Preece [1]).
But Michael Pupin patented the idea a few years later in America.
It turned out to work very well and has been used for decades in the telephone industry,
until it became possible to amplify signals electronically.
And radio amateurs have, particularly in the 1970s and 1980s, used such Pupin coils
for making audio filters.
The picture shows a typical Pupin coil. It had two windings, with red and green wires,
for both wires of the telephone line. Separately the windings usually were 22 mH,
in series 88 mH.

<p>
Should we now also insert coils into our antenna cables to prevent distortion of our
radio signals?
No: firstly the distortion is negligible for relatively narrow-band signals;
and secondly, such coils would work as a low-pass filter, so our (high) frequencies
wouldn't get through anymore.



</p><h2>References</h2>
[1] Basil Mahon: The forgotten genius of Oliver Heaviside -- A maverick of electrical science. 2017.

</mathml></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nvidia, ASML Plunge as DeepSeek Triggers Tech Stock Selloff (331 pts)]]></title>
            <link>https://finance.yahoo.com/news/asml-sinks-china-ai-startup-081823609.html</link>
            <guid>42839650</guid>
            <pubDate>Mon, 27 Jan 2025 10:57:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://finance.yahoo.com/news/asml-sinks-china-ai-startup-081823609.html">https://finance.yahoo.com/news/asml-sinks-china-ai-startup-081823609.html</a>, See on <a href="https://news.ycombinator.com/item?id=42839650">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>    <p><!-- HTML_TAG_START -->(Bloomberg) -- ASML Holding NV shares tanked along with global technology stocks on Monday as Chinese artificial intelligence startup DeepSeek sparked fear over Western technological dominance.<!-- HTML_TAG_END --></p> <p><!-- HTML_TAG_START -->Most Read from Bloomberg<!-- HTML_TAG_END --></p> <ul><li> <p><!-- HTML_TAG_START --><a href="https://www.bloomberg.com/news/articles/2025-01-23/us-pedestrian-study-we-re-walking-faster-hanging-out-less?utm_campaign=bn&amp;utm_medium=distro&amp;utm_source=yahooUS" rel="nofollow noopener" target="_blank" data-ylk="slk:What Happened to Hanging Out on the Street?;elm:context_link;itc:0;sec:content-canvas">What Happened to Hanging Out on the Street?</a><!-- HTML_TAG_END --></p> </li><li> <p><!-- HTML_TAG_START --><a href="https://www.bloomberg.com/news/features/2025-01-24/vienna-bets-on-heat-pumps-to-decouple-it-from-russian-gas?utm_campaign=bn&amp;utm_medium=distro&amp;utm_source=yahooUS" rel="nofollow noopener" target="_blank" data-ylk="slk:Vienna Embraces Heat Pumps to Ditch Russian Gas;elm:context_link;itc:0;sec:content-canvas">Vienna Embraces Heat Pumps to Ditch Russian Gas</a><!-- HTML_TAG_END --></p> </li><li> <p><!-- HTML_TAG_START --><a href="https://www.bloomberg.com/news/articles/2025-01-23/billionaire-developer-slams-la-leadership-over-deadly-wildfires?utm_campaign=bn&amp;utm_medium=distro&amp;utm_source=yahooUS" rel="nofollow noopener" target="_blank" data-ylk="slk:Billionaire Developer Caruso Slams LA Leadership Over Wildfires;elm:context_link;itc:0;sec:content-canvas">Billionaire Developer Caruso Slams LA Leadership Over Wildfires</a><!-- HTML_TAG_END --></p> </li><li> <p><!-- HTML_TAG_START --><a href="https://www.bloomberg.com/news/articles/2025-01-19/how-sanctuary-cities-are-preparing-for-another-showdown-with-trump?utm_campaign=bn&amp;utm_medium=distro&amp;utm_source=yahooUS" rel="nofollow noopener" target="_blank" data-ylk="slk:How Sanctuary Cities Are Preparing for Another Showdown With Trump;elm:context_link;itc:0;sec:content-canvas">How Sanctuary Cities Are Preparing for Another Showdown With Trump</a><!-- HTML_TAG_END --></p> </li><li> <p><!-- HTML_TAG_START --><a href="https://www.bloomberg.com/news/articles/2025-01-23/hoboken-path-station-will-close-for-almost-a-month-on-jan-30?utm_campaign=bn&amp;utm_medium=distro&amp;utm_source=yahooUS" rel="nofollow noopener" target="_blank" data-ylk="slk:Hoboken PATH Station Will Close for Almost a Month on Jan. 30;elm:context_link;itc:0;sec:content-canvas">Hoboken PATH Station Will Close for Almost a Month on Jan. 30</a><!-- HTML_TAG_END --></p> </li> </ul> <p><!-- HTML_TAG_START -->ASML’s shares dropped as much as 9.4% to €634.70 apiece in early Amsterdam trading on Monday, the biggest intraday drop since Oct. 15. The technology heavy Nasdaq 100 futures index slumped 3%.<!-- HTML_TAG_END --></p> <p><!-- HTML_TAG_START -->The Dutch company makes machines needed to produce high-end chips that power everything from electric vehicles to military gear, and it’s benefited from a surge in AI spending.<!-- HTML_TAG_END --></p> <p><!-- HTML_TAG_START -->Most Read from Bloomberg Businessweek<!-- HTML_TAG_END --></p> <ul><li> <p><!-- HTML_TAG_START --><a href="https://www.bloomberg.com/news/articles/2025-01-23/southern-towns-in-the-us-want-more-buc-ee-s-gas-stations?utm_campaign=bw&amp;utm_medium=distro&amp;utm_source=yahooUS" rel="nofollow noopener" target="_blank" data-ylk="slk:Forget Factories, Small US Towns Want Buc-ee’s Gas Stations;elm:context_link;itc:0;sec:content-canvas">Forget Factories, Small US Towns Want Buc-ee’s Gas Stations</a><!-- HTML_TAG_END --></p> </li><li> <p><!-- HTML_TAG_START --><a href="https://www.bloomberg.com/news/features/2025-01-24/fertility-treatment-risks-what-information-the-cdc-holds-back?utm_campaign=bw&amp;utm_medium=distro&amp;utm_source=yahooUS" rel="nofollow noopener" target="_blank" data-ylk="slk:The CDC Won’t Give the Public a Full Picture of Fertility Treatment Risks;elm:context_link;itc:0;sec:content-canvas">The CDC Won’t Give the Public a Full Picture of Fertility Treatment Risks</a><!-- HTML_TAG_END --></p> </li><li> <p><!-- HTML_TAG_START --><a href="https://www.bloomberg.com/news/articles/2025-01-21/elon-musk-s-inaugural-highs-and-lows?utm_campaign=bw&amp;utm_medium=distro&amp;utm_source=yahooUS" rel="nofollow noopener" target="_blank" data-ylk="slk:Elon Musk’s Inaugural Highs (and Lows);elm:context_link;itc:0;sec:content-canvas">Elon Musk’s Inaugural Highs (and Lows)</a><!-- HTML_TAG_END --></p> </li><li> <p><!-- HTML_TAG_START --><a href="https://www.bloomberg.com/news/articles/2025-01-21/kendrick-lamar-the-business-of-being-super-bowl-2025-headliner?utm_campaign=bw&amp;utm_medium=distro&amp;utm_source=yahooUS" rel="nofollow noopener" target="_blank" data-ylk="slk:How Kendrick Lamar Turned Beef With Drake Into Music Superstardom;elm:context_link;itc:0;sec:content-canvas">How Kendrick Lamar Turned Beef With Drake Into Music Superstardom</a><!-- HTML_TAG_END --></p> </li><li> <p><!-- HTML_TAG_START --><a href="https://www.bloomberg.com/news/articles/2025-01-21/greek-police-say-eggs-were-stolen-from-ivf-clinic-patients?utm_campaign=bw&amp;utm_medium=distro&amp;utm_source=yahooUS" rel="nofollow noopener" target="_blank" data-ylk="slk:Greek Police Say Eggs Were Stolen from IVF Clinic Patients;elm:context_link;itc:0;sec:content-canvas">Greek Police Say Eggs Were Stolen from IVF Clinic Patients</a><!-- HTML_TAG_END --></p> </li> </ul> <p><!-- HTML_TAG_START -->©2025 Bloomberg L.P.<!-- HTML_TAG_END --></p>      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Facebook ban on discussing Linux? (518 pts)]]></title>
            <link>https://distrowatch.com/weekly-mobile.php?issue=20250127#sitenews</link>
            <guid>42839502</guid>
            <pubDate>Mon, 27 Jan 2025 10:32:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://distrowatch.com/weekly-mobile.php?issue=20250127#sitenews">https://distrowatch.com/weekly-mobile.php?issue=20250127#sitenews</a>, See on <a href="https://news.ycombinator.com/item?id=42839502">Hacker News</a></p>
Couldn't get https://distrowatch.com/weekly-mobile.php?issue=20250127#sitenews: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[SiFive's P550 Microarchitecture (144 pts)]]></title>
            <link>https://chipsandcheese.com/p/inside-sifives-p550-microarchitecture</link>
            <guid>42839501</guid>
            <pubDate>Mon, 27 Jan 2025 10:32:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chipsandcheese.com/p/inside-sifives-p550-microarchitecture">https://chipsandcheese.com/p/inside-sifives-p550-microarchitecture</a>, See on <a href="https://news.ycombinator.com/item?id=42839501">Hacker News</a></p>
Couldn't get https://chipsandcheese.com/p/inside-sifives-p550-microarchitecture: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Sweden Seizes Ship Suspected of Baltic Sea 'Sabotage' (141 pts)]]></title>
            <link>https://www.barrons.com/news/sweden-says-has-seized-ship-suspected-of-baltic-sea-sabotage-13ff82f2</link>
            <guid>42839348</guid>
            <pubDate>Mon, 27 Jan 2025 10:05:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.barrons.com/news/sweden-says-has-seized-ship-suspected-of-baltic-sea-sabotage-13ff82f2">https://www.barrons.com/news/sweden-says-has-seized-ship-suspected-of-baltic-sea-sabotage-13ff82f2</a>, See on <a href="https://news.ycombinator.com/item?id=42839348">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <div>
          <p><img src="https://www.barrons.com/asset/barrons/images/barrons-logo.png"></p><p>This copy is for your personal, non-commercial use only. To order presentation-ready copies for distribution to your colleagues, clients or customers visit http://www.djreprints.com.</p>
          <p>https://www.barrons.com/news/sweden-says-has-seized-ship-suspected-of-baltic-sea-sabotage-13ff82f2</p>
        </div>

        <!-- cxenseparse_start -->
        <div id="article_sector">
          <article id="article-contents" maincontentofpage="">
            <header>
              
              
              <hr>

              <div>


                  <div>

<p><span>By AFP - Agence France Presse</span>


</p>

<p><time>
  January 26, 2025
</time>
                  </p></div>
                  <div>
<ul>
    <li tabindex="0" onclick="window.open('//s100.copyright.com/Clients/wsj_com/FairUse.jsp?PublisherName=barrons.com&amp;PublicationDate=2025-01-26&amp;Author=AFP%20-%20Agence%20France%20Presse&amp;orderReset=true&amp;orderSource=barrons.com&amp;publication=barrons.com&amp;Title=Sweden%20Says%20Has%20Seized%20Ship%20Suspected%20Of%20Baltic%20Sea%20'Sabotage'&amp;ContentID=AFP6114440072959283523876703099927818386300.djm&amp;ArticleType=AFP%2520News&amp;DJType=true&amp;mod=article_reprintsHead', '_blank', 'left=20,top=20,width=500,height=500,toolbar=0,resizable=0')" onkeypress="window.open('//s100.copyright.com/Clients/wsj_com/FairUse.jsp?PublisherName=barrons.com&amp;PublicationDate=2025-01-26&amp;Author=AFP%20-%20Agence%20France%20Presse&amp;orderReset=true&amp;orderSource=barrons.com&amp;publication=barrons.com&amp;Title=Sweden%20Says%20Has%20Seized%20Ship%20Suspected%20Of%20Baltic%20Sea%20'Sabotage'&amp;ContentID=AFP6114440072959283523876703099927818386300.djm&amp;ArticleType=AFP%2520News&amp;DJType=true&amp;mod=article_reprintsHead', '_blank', 'left=20,top=20,width=500,height=500,toolbar=0,resizable=0')">
      Order Reprints
      <span>
      </span>
    </li>

  <li tabindex="0">
    Print Article
    <span data-id="AFP6114440072959283523876703099927818386300" data-headline="Sweden Says Has Seized Ship Suspected Of Baltic Sea 'Sabotage'" data-url="https://www.barrons.com/articles/sweden-says-has-seized-ship-suspected-of-baltic-sea-sabotage-13ff82f2" data-authors="AFP - Agence France Presse" data-date="January 26, 2025" data-summary=""></span>
  </li>
</ul>
                  </div>
              </div>
            </header>

            

<div id="js-article__body" itemprop="articleBody" data-sbid="AFP6114440072959283523876703099927818386300">
  



    <p><span>Text size</span>
      
      
    </p>
  

   <p>Swedish authorities on Sunday seized a ship suspected of having sabotaged a fibre-optic cable in the Baltic Sea, the prosecutors' office announced.</p> <p>Prosecutors have opened an investigation for "aggravated sabotage" after the undersea cable running between Sweden and Latvia was damaged, said the statement.</p> <p>nzg/jj/sbk</p>

</div>

 <!-- data-module-name="article.app/lib/module/barrons/TopStories" -->


<div data-module-id="29" data-module-name="article.app/lib/module/barrons/articleDisclaimer" data-module-zone="article_disclaimer">
  <p>
    The Barron's news department was not involved in the creation of the content above. This article was produced by AFP. For more information go to <a href="https://www.afp.com/" target="_blank">AFP.com</a>.<br>© Agence France-Presse 
  </p>

</div> <!-- data-module-name="article.app/lib/module/barrons/articleDisclaimer" -->


            

 <!-- data-module-name="article.app/lib/module/dynamicContentByArticleType" -->


              

            <!-- Div for the Unruly Ad -->
            

            
            <!-- Div for the Native Ad -->
            
            
            
            
            
            
            
            
            
            
            

            

<div id="recirc" data-module-id="14" data-module-name="article.app/lib/module/barrons/Recirc" data-module-zone="recirc"><h2>READ MORE FROM BARRON’S</h2></div> <!-- data-module-name="article.app/lib/module/barrons/Recirc" -->


          </article>
        </div>
        <!-- cxenseparse_end -->

        


 <!-- data-module-name="article.app/lib/module/barrons/dianomiJs" -->

            

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I Made an iOS Podcast Player with Racket (179 pts)]]></title>
            <link>https://defn.io/2024/11/16/podcatcher/</link>
            <guid>42838875</guid>
            <pubDate>Mon, 27 Jan 2025 08:56:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://defn.io/2024/11/16/podcatcher/">https://defn.io/2024/11/16/podcatcher/</a>, See on <a href="https://news.ycombinator.com/item?id=42838875">Hacker News</a></p>
Couldn't get https://defn.io/2024/11/16/podcatcher/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[One in four 2020 Tesla Model 3 failed the Danish periodic inspection in 2024 (164 pts)]]></title>
            <link>https://fdm.dk/nyheder/bilist/2025-01-populaer-tesla-model-dumper-med-et-brag-til-syn</link>
            <guid>42838855</guid>
            <pubDate>Mon, 27 Jan 2025 08:52:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fdm.dk/nyheder/bilist/2025-01-populaer-tesla-model-dumper-med-et-brag-til-syn">https://fdm.dk/nyheder/bilist/2025-01-populaer-tesla-model-dumper-med-et-brag-til-syn</a>, See on <a href="https://news.ycombinator.com/item?id=42838855">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  

<p><span><span><span><span>Elbilernes salgskurve er de seneste år gået stejlt opad, og mere end hver anden solgte nye bil er nu en elbil. Det mærker man nu også ude i landets synshaller, hvor elbiler ikke længere er et særsyn. Lige Godt 28.000 elbiler var i de første 11 måneder af 2024 til periodisk syn. </span></span></span></span></p>

<p><span><span><span><span>Det er dog langtfra alle elbiler, der klarer turen på liften lige godt. Især én model falder igennem: Tesla Model 3. Den populære elbil, som der kører i alt 35.000 af på de danske veje, blev lanceret i 2019, og dermed har den første store årgang været til sit første periodiske bilsyn i det forgangne år.</span></span></span></span></p>





<h2><span><span><span><span>Rasende høj dumpeprocent</span></span></span></span></h2>

<p><span><span><span>Af de 4.668 Tesla Model 3 årgang 2020, som mellem 1. januar og 21. november sidste år var til syn, dumpede de 1.051. Det svarer til 23 procent. Til sammenligning dumpede ’kun’ 9 procent af de øvrige elbiler, der var til syn i 2024, viser tal fra Færdselsstyrelsen, som FDM Test &amp; Bilsyn har analyseret. </span></span></span></p>

<p><span><span><span>– Det er en rasende høj dumpeprocent for Tesla Model 3, som Færdselsstyrelsens synsdata afslører, men det er desværre ikke noget, som overrasker os. Tallene indikerer, at kvaliteten og holdbarheden af Tesla Model 3, i hvert fald de første årgange, ikke er på niveau med andre bilmærker. Det er noget, vi også oplever, og som vi holder øje med, siger områdechef i FDMs tekniske rådgivning Lone Otto. </span></span></span></p>

<h2>Tesla har mere end hver 3. af alle fejl</h2>

<p><span><span><span>I</span></span></span><span><span><span>&nbsp;alt blev der fundet 1.392 fejl på de Tesla Model 3 årgang 2020, svarende til 0,3 fejl pr. synet model. Dermed blev der fundet tre gange så mange fejl i forhold til de øvrige elbiler, som blev synet sidste år. </span></span></span></p>



<p><span><span><span><span>Billedet fra de danske synshaller af Tesla Model 3 stemmer overens med synstal fra Tyskland. Også her faldt bilmodellen igennem i den årlige synsrapport fra tyske TÜV, der bl.a. syner biler. </span></span></span></span></p>

<p><span><span><span><span>Det er særligt fejlgrupperne „bremseudstyr“, „lygteudstyr“, „aksler, hjul og dæk“ samt „styretøj“, som bilerne dumper på. Noget man i FDMs tekniske rådgivning kan nikke genkendende til. </span></span></span></span></p>

<p><span><span><span>– Elbiler bremser på en anden måde, og derfor er problemer med bremserne velkendt. Men det er kritisk, at problemer med hjulophæng og ratslør er så udtalt på en bil, der ikke er ældre end Tesla Model 3. Det ser vi ikke på andre biler. Det samme gælder Teslas overrepræsentation af fejl på lygter, som ofte enten er løse, eller som blænder, siger Lone Otto</span></span></span></p>
<figure role="group">
<picture>
                  <source srcset="https://fdm.dk/sites/default/files/styles/ckeditor_textwidth_large/public/inline-images/Tesla%20Model%203_hjuloph%C3%A6ng_300dpi.jpg?itok=RdrIkHr9 1x" media="all and (min-width: 1440px)" type="image/jpeg" width="700" height="467">
              <source srcset="https://fdm.dk/sites/default/files/styles/ckeditor_textwidth_large/public/inline-images/Tesla%20Model%203_hjuloph%C3%A6ng_300dpi.jpg?itok=RdrIkHr9 1x" media="all and (min-width: 1200px) and (max-width: 1439px)" type="image/jpeg" width="700" height="467">
              <source srcset="https://fdm.dk/sites/default/files/styles/ckeditor_textwidth_large/public/inline-images/Tesla%20Model%203_hjuloph%C3%A6ng_300dpi.jpg?itok=RdrIkHr9 1x" media="all and (min-width: 992px) and (max-width: 1199px)" type="image/jpeg" width="700" height="467">
              <source srcset="https://fdm.dk/sites/default/files/styles/ckeditor_textwidth_large/public/inline-images/Tesla%20Model%203_hjuloph%C3%A6ng_300dpi.jpg?itok=RdrIkHr9 1x" media="all and (min-width: 768px) and (max-width: 991px)" type="image/jpeg" width="700" height="467">
              <source srcset="https://fdm.dk/sites/default/files/styles/ckeditor_textwidth_medium/public/inline-images/Tesla%20Model%203_hjuloph%C3%A6ng_300dpi.jpg?itok=6miTp5XF 1x" type="image/jpeg" width="500" height="334">
                  <img data-entity-type="file" data-entity-uuid="986dacef-e2f5-40ab-ae45-ea88e1364e59" data-responsive-image-style="ckeditor_contentwidth" src="https://fdm.dk/sites/default/files/styles/ckeditor_textwidth_small/public/inline-images/Tesla%20Model%203_hjuloph%C3%A6ng_300dpi.jpg?itok=04A14CHE" width="320" height="214" alt="Synsmand tester hjulophæng på Tesla Model 3" loading="lazy" typeof="foaf:Image">

  </picture>
<figcaption>Problemer med hjulophænget på tidligere årgange Tesla Model 3 er velkendt i FDMs tekniske rådgivning, og også noget, der ses ved syn. (Foto: FDM)</figcaption>
</figure>

<h2><span><span><span>62.000 elbiler skal til syn i år</span></span></span></h2>

<p><span><span><span>Det kommende år skal 62.000 elbiler til syn herhjemme – de 45.000 for første gang. Heriblandt Tesla Model 3 årgang 2021 og ikke mindst den første årgang af Tesla Model Y, der er den mest solgte elbil i Danmark. </span></span></span></p>

<p><span><span><span>– Vi har ingen grund til at tro, at yngre årgange af Tesla Model 3 vil adskille sig væsentligt fra årgang 2020, når det gælder fejl og dermed også dumpeprocent. Mere spændende bliver det at se, hvordan Tesla Model Y vil klare sig, siger Lone Otto og tilføjer:</span></span></span></p>

<p><span><span><span>– Tesla har fire års garanti på sine biler. Det passer med deres første syn. Et godt råd er derfor at få bilen gennemgået af en uvildig, inden man når skæringsdatoen. I det hele taget anbefaler vi, at man løbende servicerer sin bil og får gennemgået bilens bremser og generelle stand. Gerne en gang om året. Det gælder uanset bilmærke, eller om bilen overhovedet har et fast serviceinterval.</span></span></span></p>

<p>Opdateret 24. januar:</p>

<p><span><span>FDM har bedt Tesla om en kommentar til dels synstallene for deres Model 3 dels kritikken af bilens kvalitet. Det er ikke lykkedes at få. I en mail til FDM gør Tesla dog opmærksom på, at Tesla Model 3 fik et større facelift i 2021, og at modellen i 2023 kom i en ny og markant forbedret udgave. </span></span></p>

<div>
<h2><span><span><span><strong>Har du brug for hjælp?</strong></span></span></span></h2>

<p><span><span><span>Husk, som medlem af FDM kan du altid få hjælp af eksperter, der har den nyeste viden om elbiler.</span></span></span></p>

<p><a href="https://fdm.dk/vi-tilbyder/elbil-plugin-hybrid-fdms-raadgivning-kan-hjaelpe"><span><span><span>Kontakt rådgivningen</span></span></span></a></p>
</div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Once You're Laid Off, You'll Never Be the Same Again (859 pts)]]></title>
            <link>https://mertbulan.com/2025/01/26/once-you-are-laid-off-you-will-never-be-the-same-again/</link>
            <guid>42838700</guid>
            <pubDate>Mon, 27 Jan 2025 08:22:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mertbulan.com/2025/01/26/once-you-are-laid-off-you-will-never-be-the-same-again/">https://mertbulan.com/2025/01/26/once-you-are-laid-off-you-will-never-be-the-same-again/</a>, See on <a href="https://news.ycombinator.com/item?id=42838700">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <article>
    <p>It happened on the afternoon of May 4th. A message from a colleague—who has since become a good friend—popped up on my screen, urging me to check my emails. When I opened my inbox, there it was: an email from the COO. The email announced an impending company-wide layoff and mentioned that, within a few minutes, I’d receive another email letting me know whether I was impacted. A short while later, the second email arrived. I was among those affected—along with most of my team.</p>

<p>The situation felt surreal. One by one, my colleagues posted in our team chat, confirming they’d been impacted too. Before our accounts were locked, we quickly jumped on a call. We had just 30 minutes to have one final conversation as a team, to say our goodbyes. It was a bittersweet moment, sharing those last words with people I’d worked so closely with.</p>

<p>It was difficult to process what was happening. Just ten months earlier, the company had gone through another round of layoffs. And at the beginning of the year, during the company’s kick-off event, the president assured us there wouldn’t be any more layoffs. They even said the company was performing well financially. So, why was this happening?</p>

<h2 id="signs-of-a-layoff">Signs of a Layoff</h2>

<p>Looking back, my colleagues and I were not entirely surprised by the layoff. There were several warning signs that hinted something was coming. I want to share these signs so you can be better prepared if you ever face a similar situation.</p>

<h3 id="1-cancellation-of-team-events">1. Cancellation of Team Events</h3>
<p>One of the earliest indicators was the sudden cancellation of team events. When I heard from other teams that their off-site events were canceled without any clear explanation, it immediately raised red flags. These cancellations often signal that the company is going to announce something about the team structure and doesn’t want you to be with your team in the same place. This is because one of your teammates—or you—might be impacted, and you’d need to cancel flights, hotels, etc. To avoid dealing with these logistical issues, the company preemptively cancels the event.</p>

<h3 id="2-unexpected-notifications-about-packages">2. Unexpected Notifications About Packages</h3>
<p>Some employees at the company received notifications about packages scheduled to arrive at their homes. This happens because services like DHL notify you through their app when a package is on the way. If your company requires you to return your work equipment, like a laptop, after being laid off, they often arrange for these shipping boxes to be delivered in advance. If you unexpectedly see a notification about a package from your company’s IT provider, it’s a strong sign that a layoff is imminent—and you may be impacted.</p>

<h3 id="3-lack-of-vision-from-leadership">3. Lack of Vision from Leadership</h3>
<p>The absence of a clear vision from leadership is one of the most common signs of an impending layoff. During off-site or kick-off events, you might notice that leaders seem unsure of the company’s direction. When this lack of clarity is followed by team restructuring, and then another restructuring just a few months later, it becomes evident that the leadership is struggling to find focus. Ultimately, this cycle often ends with a layoff, accompanied by yet another round of restructuring for those who remain.</p>

<h3 id="4-sudden-vague-meetings">4. Sudden, Vague Meetings</h3>
<p>Another sign is the appearance of unexpected, vague meetings on your calendar. These meetings are marked as “important” with no clear agenda, and attendance is mandatory. If this happens, it’s often a precursor to a layoff announcement. Public companies, in particular, may choose to send layoff notices via email to align the timing with when they notify investors.</p>

<h3 id="5-timing-around-quarterly-results">5. Timing Around Quarterly Results</h3>

<p>If your company is publicly traded, layoffs are frequently announced in conjunction with quarterly earnings reports. This can be especially stressful because, leading up to every financial results announcement, employees may anxiously wait to see if layoffs will accompany the news. If no layoffs are announced, you know you’re safe—for at least one more quarter.</p>

<h2 id="youre-just-a-row-in-an-excel-table">You’re Just a Row in an Excel Table</h2>

<p>When I looked back on my time at the company and all the things I had accomplished, I was surprised to be impacted by the layoffs. It wasn’t because I thought I was better than others—it was because I believed I was doing more than what was expected of me. However, during a layoff, it seems that who you are and what you do doesn’t matter. In most cases, the decision is made by people who don’t even know you. This realization made me question the concept of work, which is part of the reason I’m writing this blog post.</p>

<p>I was hired as a Backend Developer. When I joined my team, I noticed a project that needed a developer to implement the client-side feature in React Native. Although I had no prior experience with React Native, I had worked with React before, so I volunteered for the task. I shipped the feature without any issues, received positive feedback from my team and lead, and eventually, my title was changed to Developer, making me a full-stack developer.</p>

<p>In some instances, I worked on projects independently, always aligning with my team and ensuring my work was reviewed. I would implement the backend first and then move on to the client-side. This was my expected role, and in performance reviews, I was consistently rated as a high performer. Yet, I was always doing more than what was expected of me.</p>

<p>Sometimes, I worked on small features I thought would enhance the app. These features might not have been used by many, but they provided significant value to heavy users. Occasionally, I shipped these under the radar. I created dashboards to measure the impact of my team’s work, helping us focus on features that would bring the most value to users. I also built proof-of-concept features based on user requests to show leadership how easily they could be implemented, advocating for their prioritization. Additionally, I participated in hackdays, creating projects to showcase innovative ideas.</p>

<p>On several occasions, I was selected for special projects outside my team. These projects often came directly from the CEO, and I was chosen because I constantly wanted to do more for the company and our users. For some of these projects, I worked more than eight hours a day, including weekends. A few of these initiatives were mentioned in financial reports, praised by the CEO during all-hands meetings, or retweeted multiple times by the CEO on Twitter.</p>

<p>Over time, I gained the attention of senior management in my business unit, which consisted of about 400 people. I began directly interacting with the VP of Product and the VP of Engineering, both of whom were four or five levels above me. Occasionally, the VP of Product would message me directly to ask if a feature was feasible to implement. Later, the VP of Engineering started scheduling regular one-on-one meetings with me, which was highly uncommon. During these calls, he told me multiple times that if I continued working at this level, I could quickly climb the ladder to become a Staff Developer. He wasn’t the only one saying this to me.</p>

<p>Beyond my immediate role, I also sought ways to contribute to the broader company. Whenever a new tool was introduced, I would explore it, write detailed articles about my findings, and share them to help other teams use the tool more effectively.</p>

<p>I referred many friends and former colleagues to the company because I believed in its mission. If I recall correctly, I referred over ten people, four of whom received offers, and three were ultimately hired. I also encouraged many others to consider joining the company.</p>

<p>I even initiated discussions about translating our website into Turkish to support the many customers we had in Turkey. A few weeks before the layoff announcement, I was helping a team working on this project find a Turkish-speaking content designer because they noticed my willingness to assist.</p>

<p>Additionally, I tried to convince friends who were CTOs at major e-commerce companies to migrate their websites to our platform. Whenever I received job offers from e-commerce companies on LinkedIn, I used those opportunities to promote our platform instead. I passed along leads to the sales team and later noticed that one of those companies had indeed moved to our platform.</p>

<p>I’m not sharing all of this to brag but to highlight that, in the end, none of it mattered. On the day I announced I had been laid off, I received numerous messages from colleagues, even those I hadn’t worked with directly, telling me that I had inspired and motivated them. While those messages were heartwarming, they didn’t change the reality: to the company, I was just a row in an Excel sheet.</p>

<h2 id="the-broken-trust-of-modern-work">The Broken Trust of Modern Work</h2>

<p>Layoffs were uncommon when I started working, and being a developer felt like an incredibly safe job. In most professions, the unspoken rule was simple: if you performed well and the company was financially stable, your job was secure.</p>

<p>But today, companies are announcing layoffs alongside record-breaking financial results. You work hard, focus on impactful projects, and receive praise from your lead—only to find yourself let go by someone who likely doesn’t even know you exist. It feels as though the trust between companies and employees is now broken. Companies, it seems, are either unaware of this shift or unwilling to address it. And frankly, I’m not sure how they could fix it.</p>

<p>What’s particularly strange is that the layoffs predominantly affect individual contributors—the people who have little say in deciding the company’s direction. These are the team members closest to the users, the ones who spend hours planning how to improve the product. But after those plans are made, leadership often swoops in and redirects efforts toward entirely different goals. You trust their judgment, work on their priorities, and deliver on time. Then, when the arbitrary goals they set aren’t met, the company decides to cut staff. Those who made the poor decisions remain, and some are even promoted, while the people carrying out the work are let go. It feels surreal—like <a rel="nofollow noopener" target="_blank" href="https://www.youtube.com/watch?v=u48vYSLvKNQ">an episode from Silicon Valley</a>—but this is how big companies operate.</p>

<p>I’m not alone in feeling this way. Many friends and ex-colleagues who’ve been laid off in recent years share similar experiences. They’ve lost trust in their employers. They believe their efforts won’t matter in the long run and anticipate being part of the next layoff cycle. As a result, they only do what’s strictly required to avoid a performance improvement plan. No one goes above and beyond anymore; no one takes initiative to improve things. Why? Because it doesn’t matter. They’ve seen firsthand that it changes nothing.</p>

<p>For those like me who’ve experienced layoffs, work has become just that—work. You do what’s assigned, and if your company squanders your potential or forces you to waste time on unnecessary projects, you simply stop caring. You collect your paycheck at the end of the month, and that’s it. This is the new modern work: no more striving to be 40% better every year.</p>

<h2 id="the-myth-of-job-security-in-germany">The Myth of Job Security in Germany</h2>

<p>Since I was working for a German entity of a company, I want to address a common myth about job security in Germany. Many people believe that it’s nearly impossible to be fired in Germany. While this is partially true for individuals who have completed their probation period, it doesn’t hold up in the context of layoffs. If a company decides to lay off, for instance, 40 employees, German law doesn’t prevent this. Instead, the law enforces a social scoring system to determine who is affected, prioritizing the protection of the most vulnerable employees, such as those with children. In this sense, when it comes to layoffs, the difference between Germany and the US is minimal.</p>

<h2 id="suggestions-for-those-who-havent-been-laid-off-yet">Suggestions for Those Who Haven’t Been Laid Off (Yet)</h2>

<p>When I talk to friends who were laid off in recent years, we often reflect on what we could have done differently. Here are some of the lessons we’ve learned:</p>

<ul>
  <li><strong>Stick to your contract hours.</strong> If your contract says 40 hours, work 40 hours—no more, no less. Protect your personal time and well-being.</li>
  <li><strong>Avoid going above and beyond with initiatives.</strong> Many companies encourage impactful work to earn promotions, but instead of chasing internal advancements, focus on switching companies to achieve your next career step.</li>
  <li><strong>Always keep interviewing.</strong> One of the biggest mistakes I’ve seen is stopping interviews after starting a new job, trusting in the company. Instead, continuously explore opportunities so that if a layoff happens, you already have other options lined up.</li>
  <li><strong>Leverage external offers for salary growth.</strong> Companies often resist giving substantial raises to existing employees but pay top dollar for new hires. Regularly interview elsewhere, and if you get an offer with a 20% or higher salary increase, consider taking it. Many people have seen their compensation triple or quadruple this way in just a few years.</li>
  <li><strong>Don’t overthink your résumé.</strong> Worrying about short experiences on your CV isn’t worth it. You can always tailor your résumé—leave out brief roles, or consolidate short-term jobs as freelance experience. Ultimately, your résumé is just a starting point; your skills will be assessed during the interview process.</li>
</ul>

<hr>

<p>You’ve probably noticed that I didn’t mention the name of the company I was laid off from. That’s because I believe it’s irrelevant. Everything I’ve shared reflects the current state of the tech industry. It might differ at very small companies, but once you work at a company with more than 100 employees, you’ll likely encounter many of the same patterns I’ve described.</p>

<p>I’ve wanted to write about this topic for a long time, but it’s been difficult to find the energy. The subject itself is a deep disappointment for me, and every time I reflect on layoffs, it makes me profoundly sad. It’s a stark reminder of how companies treat workers as disposable. Before you join, they go to great lengths to make you feel valued and excited to accept their offer. You meet multiple people, and some even offer signing bonuses. But when layoffs come, you’re reduced to a name on a list. During the exit interview, a random person from the company reads a prepared script and can’t answer your questions. The HR team that once worked to make you feel valued doesn’t even conduct an actual conversation with you. That random person becomes the last connection you have to a company you spent years at.</p>

<p>The layoff fundamentally changed how I perceive work now. I don’t think that I’ll be the same person again.</p>

  </article>
</div><div>
      <p><a href="https://mertbulan.com/2024/05/05/how-to-retire-early-in-germany/">
        <img data-src="/images/posts/retire-early-germany.webp" alt="Once You're Laid Off, You'll Never Be the Same Again" src="https://mertbulan.com/images/posts/retire-early-germany.webp">
      </a></p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Making AR experiences is still painful – had to make my own editor (147 pts)]]></title>
            <link>https://ordinary.space/</link>
            <guid>42838355</guid>
            <pubDate>Mon, 27 Jan 2025 07:32:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ordinary.space/">https://ordinary.space/</a>, See on <a href="https://news.ycombinator.com/item?id=42838355">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="pabmky5I8QL"><main><vev id="eS5k201GAkr"><div><w id="eS5k201GAkrc"></w><div><vev id="e1-wKGNYP1x"><div id="e1-wKGNYP1xc"><vev id="eSi_hnsRzhu"></vev><vev id="eXWw6ZEMOuc"><div id="eXWw6ZEMOucc"><vev id="enMAzuvD0W8"><div><w id="enMAzuvD0W8c"><img alt="Graphics software, Operating system, Blue, Technology, Screenshot" loading="lazy" src="https://cdn.vev.design/cdn-cgi/image/f=auto,q=82,w=1280/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/z7vQZZROKA" srcset="https://cdn.vev.design/cdn-cgi/image/f=auto,q=82,w=320/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/z7vQZZROKA 320w,https://cdn.vev.design/cdn-cgi/image/f=auto,q=82,w=640/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/z7vQZZROKA 640w,https://cdn.vev.design/cdn-cgi/image/f=auto,q=82,w=960/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/z7vQZZROKA 960w,https://cdn.vev.design/cdn-cgi/image/f=auto,q=82,w=1280/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/z7vQZZROKA 1280w"></w></div></vev><vev id="ekn0cQhYw55"></vev></div></vev></div></vev><vev id="eRphUPqt0cL"><div id="eRphUPqt0cLc"><vev id="e9epbWqYQdH"><div id="e9epbWqYQdHc"><vev id="etYK-JU5gzk"><div><w id="etYK-JU5gzkc"><p><video aria-label="Comp 1_2.mp4" disableremoteplayback="" muted="" playsinline="" poster="https://cdn.vev.design/a/c1uZR8BI8TkpmJHZsLDg/p/wyPaPGvlZ-/v/os6HaeVXmI/thumbnail0000000000.jpeg" preload="auto" autoplay=""><source src="https://cdn.vev.design/a/c1uZR8BI8TkpmJHZsLDg/p/wyPaPGvlZ-/v/os6HaeVXmI/hd-h264.mp4" type="video/mp4"><source src="https://cdn.vev.design/a/c1uZR8BI8TkpmJHZsLDg/p/wyPaPGvlZ-/v/os6HaeVXmI/hd-vp9.mp4" type="video/mp4">Your browser does not support this video</video></p></w></div></vev><vev id="eOkWdJn6zH5"></vev></div></vev><vev id="eeMGofkM575"><div id="eeMGofkM575c"><vev id="eIzgaXOQiR2"><div><w id="eIzgaXOQiR2c"><img alt="Graphics software, Operating system, Technology, Screenshot" loading="lazy" src="https://cdn.vev.design/cdn-cgi/image/f=auto,q=82,w=1920/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/4Z9FGGZh8L_2il2go.png" srcset="https://cdn.vev.design/cdn-cgi/image/f=auto,q=82,w=320/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/4Z9FGGZh8L_2il2go.png 320w,https://cdn.vev.design/cdn-cgi/image/f=auto,q=82,w=640/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/4Z9FGGZh8L_2il2go.png 640w,https://cdn.vev.design/cdn-cgi/image/f=auto,q=82,w=960/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/4Z9FGGZh8L_2il2go.png 960w,https://cdn.vev.design/cdn-cgi/image/f=auto,q=82,w=1280/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/4Z9FGGZh8L_2il2go.png 1280w,https://cdn.vev.design/cdn-cgi/image/f=auto,q=82/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/4Z9FGGZh8L_2il2go.png 2048w"></w></div></vev></div></vev><vev id="eXweg8Ev3iz"><div id="eXweg8Ev3izc"><vev id="eVRJ7v9InIv"><div><w id="eVRJ7v9InIvc"><img alt="Audio equipment, Technology, Loudspeaker" loading="lazy" src="https://cdn.vev.design/cdn-cgi/image/f=auto,q=82,h=1920/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/kSNfpwKRM6_2il2ip.png" srcset="https://cdn.vev.design/cdn-cgi/image/f=auto,q=82,h=320/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/kSNfpwKRM6_2il2ip.png 320w,https://cdn.vev.design/cdn-cgi/image/f=auto,q=82,h=640/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/kSNfpwKRM6_2il2ip.png 640w,https://cdn.vev.design/cdn-cgi/image/f=auto,q=82,h=960/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/kSNfpwKRM6_2il2ip.png 960w,https://cdn.vev.design/cdn-cgi/image/f=auto,q=82,h=1280/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/kSNfpwKRM6_2il2ip.png 1280w,https://cdn.vev.design/cdn-cgi/image/f=auto,q=82/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/kSNfpwKRM6_2il2ip.png 2038w"></w></div></vev><vev id="eUvk_AL_Au9"></vev></div></vev><vev id="eR3P4Caj2Qy"></vev><vev id="e6zobiEV3gy"></vev><vev id="etLab1VNXjU"><div id="etLab1VNXjUc"><vev id="eOZcytwYwX0"><div id="eOZcytwYwX0c"><vev id="eBlNf4uewnn"><div><w id="eBlNf4uewnnc"><p id=""><h2>Prototype high fidelity spatial apps.</h2></p></w></div></vev><vev id="e0mWgMde1zc"><div><w id="e0mWgMde1zcc"><p id=""><h3>Ordinary Objects helps designers ideate and build intuitive and delightful mixed reality projects.</h3></p></w></div></vev><vev id="euTuieJXt2P"><p><w id="euTuieJXt2Pc"><a href="https://app.ordinary.space/sign-up" data-tween="false">Try the beta - it's free</a></w></p></vev></div></vev></div></vev></div></vev></div></div></vev><vev id="eFByiatr4rI"><div><w id="eFByiatr4rIc"></w><div><vev id="evjHakPdmhw"><div id="evjHakPdmhwc"><vev id="eaW96F8iZB-"><div id="eaW96F8iZB-c"><vev id="e3zb7m8tGmI"><div><w id="e3zb7m8tGmIc"><p id=""><h2>Create rich human centric experiences.</h2></p></w></div></vev></div></vev><vev id="eeEoj7iXXan"><div id="eeEoj7iXXanc"><vev id="eL7ZWvfxxQS"><div><w id="eL7ZWvfxxQSc"><p>Powerful authoring features and a unique workflow to prototype spatial user flows and interactions.</p></w></div></vev></div></vev><vev id="eOOMCnCW8JB"><div id="eOOMCnCW8JBc"><vev id="esE5NXjw_Ks"><div id="esE5NXjw_Ksc"><vev id="eVIn1M-HGqR"><div><w id="eVIn1M-HGqRc"><p><video aria-label="States-Board.mp4" disableremoteplayback="" muted="" playsinline="" poster="https://cdn.vev.design/a/c1uZR8BI8TkpmJHZsLDg/p/wyPaPGvlZ-/v/fQJsihX4vn/thumbnail0000000000.jpeg" preload="auto"><source src="https://cdn.vev.design/a/c1uZR8BI8TkpmJHZsLDg/p/wyPaPGvlZ-/v/fQJsihX4vn/hd-h264.mp4" type="video/mp4"><source src="https://cdn.vev.design/a/c1uZR8BI8TkpmJHZsLDg/p/wyPaPGvlZ-/v/fQJsihX4vn/hd-vp9.mp4" type="video/mp4">Your browser does not support this video</video></p></w></div></vev><vev id="ezpcVO-x-S6"><div><w id="ezpcVO-x-S6c"><p><video aria-label="Real-Time-Feedback.mp4" disableremoteplayback="" muted="" playsinline="" poster="https://cdn.vev.design/a/c1uZR8BI8TkpmJHZsLDg/p/wyPaPGvlZ-/v/Ltr7lvGxMj/thumbnail0000000000.jpeg" preload="auto"><source src="https://cdn.vev.design/a/c1uZR8BI8TkpmJHZsLDg/p/wyPaPGvlZ-/v/Ltr7lvGxMj/hd-h264.mp4" type="video/mp4"><source src="https://cdn.vev.design/a/c1uZR8BI8TkpmJHZsLDg/p/wyPaPGvlZ-/v/Ltr7lvGxMj/hd-vp9.mp4" type="video/mp4">Your browser does not support this video</video></p></w></div></vev><vev id="elWnfCp-a9h"><div><w id="elWnfCp-a9hc"><p><video aria-label="Triggers-Actions-v1.mp4" disableremoteplayback="" muted="" playsinline="" poster="https://cdn.vev.design/a/c1uZR8BI8TkpmJHZsLDg/p/wyPaPGvlZ-/v/AOPDMUPNfY/thumbnail0000000000.jpeg" preload="auto"><source src="https://cdn.vev.design/a/c1uZR8BI8TkpmJHZsLDg/p/wyPaPGvlZ-/v/AOPDMUPNfY/hd-h264.mp4" type="video/mp4"><source src="https://cdn.vev.design/a/c1uZR8BI8TkpmJHZsLDg/p/wyPaPGvlZ-/v/AOPDMUPNfY/hd-vp9.mp4" type="video/mp4">Your browser does not support this video</video></p></w></div></vev><vev id="e-i6FnTVf1E"><div><w id="e-i6FnTVf1Ec"><p><video aria-label="Smart-States-1.mp4" disableremoteplayback="" muted="" playsinline="" poster="https://cdn.vev.design/a/c1uZR8BI8TkpmJHZsLDg/p/wyPaPGvlZ-/v/jEBTM7qCpn/thumbnail0000000000.jpeg" preload="auto"><source src="https://cdn.vev.design/a/c1uZR8BI8TkpmJHZsLDg/p/wyPaPGvlZ-/v/jEBTM7qCpn/hd-h264.mp4" type="video/mp4"><source src="https://cdn.vev.design/a/c1uZR8BI8TkpmJHZsLDg/p/wyPaPGvlZ-/v/jEBTM7qCpn/hd-vp9.mp4" type="video/mp4">Your browser does not support this video</video></p></w></div></vev></div></vev><vev id="eF4EKhFVV3J"><div id="eF4EKhFVV3Jc"><vev id="egPuU6ceM-D"><div id="egPuU6ceM-Dc"><vev id="e9_dRGRsq3t"><div id="e9_dRGRsq3tc"><vev id="eCYU1gXDx45"><div id="eCYU1gXDx45c" role="button" tabindex="0"><vev id="eznwlgP6ooj"><div id="eznwlgP6oojc"><vev id="egM4U8qGXjR"><div id="egM4U8qGXjRc"><vev id="eF6VdoKCQNn"><div><w id="eF6VdoKCQNnc"><img alt="" loading="lazy" src="https://cdn.vev.design/cdn-cgi/image/f=auto,q=82/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/MzoGP7YiN8" srcset=""></w></div></vev></div></vev><vev id="ehIN_7PkWOz"><div id="ehIN_7PkWOzc"><vev id="e9misgZ6p-S"></vev><vev id="eZdG0kPG-Lw"><div><w id="eZdG0kPG-Lwc"><p>Ensuring graceful transitions throughout the prototype and a consistent look and feel.</p></w></div></vev></div></vev></div></vev></div></vev><vev id="eM9d-Wv8tpo"><div><w id="eM9d-Wv8tpoc"><img alt="" loading="lazy" src="https://cdn.vev.design/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/PdOlAP60j_" srcset=""></w></div></vev><vev id="eu7UCNO8bHJ"><div id="eu7UCNO8bHJc" role="button" tabindex="0"><vev id="eXFe1U5CVot"><div id="eXFe1U5CVotc"><vev id="e-Xc2aC4hZ_"><div id="e-Xc2aC4hZ_c"><vev id="eSZlQYBu0hU"><div><w id="eSZlQYBu0hUc"><img alt="" loading="lazy" src="https://cdn.vev.design/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/a3mv33KPdS.svg" srcset=""></w></div></vev></div></vev><vev id="e7gxuXvZM50"><div id="e7gxuXvZM50c"><vev id="eVUOv0-7UrQ"><div><w id="eVUOv0-7UrQc"><p>Powerful Triggers and Actions</p></w></div></vev><vev id="ehn9Ctvulfk"><div><w id="ehn9Ctvulfkc"><p>Enabling rich behaviours and interactions, alone and in combination.</p></w></div></vev></div></vev></div></vev></div></vev><vev id="eELmKXwNfjs"><div><w id="eELmKXwNfjsc"><img alt="" loading="lazy" src="https://cdn.vev.design/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/jLAvLQPA6D" srcset=""></w></div></vev><vev id="elHYNn99H-z"><div id="elHYNn99H-zc" role="button" tabindex="0"><vev id="eRIldnMbPu4"><div id="eRIldnMbPu4c"><vev id="ePfmxoXYYmo"><div id="ePfmxoXYYmoc"><vev id="eN1RqagmvZc"><div><w id="eN1RqagmvZcc"><img alt="" loading="lazy" src="https://cdn.vev.design/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/eOmIsLWTSi.svg" srcset=""></w></div></vev></div></vev><vev id="eQJHQzmYjXc"><div id="eQJHQzmYjXcc"><vev id="eRdqERYmJ3c"></vev><vev id="e31SL3o5Tgb"><div><w id="e31SL3o5Tgbc"><p>No play mode, testing interactions just like that.</p></w></div></vev></div></vev></div></vev></div></vev><vev id="e0nOFa7LhBI"><div><w id="e0nOFa7LhBIc"><img alt="" loading="lazy" src="https://cdn.vev.design/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/jLAvLQPA6D" srcset=""></w></div></vev><vev id="eK2ZC58UMEp"><div id="eK2ZC58UMEpc" role="button" tabindex="0"><vev id="et5K1XKQJg1"><div id="et5K1XKQJg1c"><vev id="ezG40x1C08U"><div id="ezG40x1C08Uc"><vev id="eZp771VuXIs"><div><w id="eZp771VuXIsc"><img alt="" loading="lazy" src="https://cdn.vev.design/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/NeJMZvuAcD.svg" srcset=""></w></div></vev></div></vev><vev id="eAQxJg2dl_z"><div id="eAQxJg2dl_zc"><vev id="ezA-E8xcHBK"></vev><vev id="eLHfTGXzrPH"><div><w id="eLHfTGXzrPHc"><p>An always up to date overview of the flow, quickly jump between states.</p></w></div></vev></div></vev></div></vev></div></vev></div></vev></div></vev></div></vev></div></vev><vev id="eTXu9IE0irb"><div id="eTXu9IE0irbc"><vev id="eBx8ltJyn2H"><div><w id="eBx8ltJyn2Hc"><p>Ordinary Objects runs natively on major platforms</p></w></div></vev><vev id="eB9xMBESAJT"></vev></div></vev></div></vev></div></div></vev><vev id="ekXL1Vk3MOk"><div><w id="ekXL1Vk3MOkc"></w><div><vev id="eMwYTJl8wQ1"><div id="eMwYTJl8wQ1c"><vev id="eBO4yKSTtUA"><div id="eBO4yKSTtUAc"><vev id="e18qqhZG-te"><div><w id="e18qqhZG-tec"><p id=""><h2>Unlock space as a creative medium.</h2></p></w></div></vev><vev id="e_Qqe4o9ZRb"><div><w id="e_Qqe4o9ZRbc"><img alt="" loading="lazy" src="https://cdn.vev.design/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/nOj78YuQA4.svg" srcset=""></w></div></vev></div></vev><vev id="eRVPL8ybml4"><div id="eRVPL8ybml4c"><vev id="eNRmx-JbR3H"><div><w id="eNRmx-JbR3Hc"><p id=""><h3>Express your XR vision with beautiful prototypes to remove the guesswork for stakeholders and developers about how an experience should look and feel.</h3></p></w></div></vev><vev id="ev3MDvD_1L7"><div><w id="ev3MDvD_1L7c"><p id=""><h3>Ordinary Objects is designed from the ground up to ensure creative freedom while playfully surfacing the limitations and benefits of spatial experiences.</h3></p></w></div></vev></div></vev></div></vev></div></div></vev><vev id="eldnpBBgl8F"><div><w id="eldnpBBgl8Fc"></w><div><vev id="eXEJ_ghP0gP"><div id="eXEJ_ghP0gPc"><vev id="eXck89l-OJG"><div id="eXck89l-OJGc"><vev id="esxL098G1fe"><div><w id="esxL098G1fec"><p id=""><h2>Made to design spatial experiences</h2></p></w></div></vev></div></vev><vev id="e6XeA1gofmp"><div id="e6XeA1gofmpc"><vev id="eXNmKcP76jE"><div id="eXNmKcP76jEc"><vev id="e9HggUlbmDn"><div id="e9HggUlbmDnc"><vev id="eoH7-6Rz6NU"><div id="eoH7-6Rz6NUc"><vev id="eDBaSNsQmQA"><div><w id="eDBaSNsQmQAc"><img alt="Font" loading="lazy" src="https://cdn.vev.design/cdn-cgi/image/f=auto,q=82/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/lCR4L8Qwi8.png" srcset="https://cdn.vev.design/cdn-cgi/image/f=auto,q=82,h=320/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/lCR4L8Qwi8.png 320w,https://cdn.vev.design/cdn-cgi/image/f=auto,q=82/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/lCR4L8Qwi8.png 616w"></w></div></vev></div></vev><vev id="eEM1pbkN9x7"><div id="eEM1pbkN9x7c"><vev id="eD4lwFhnFnL"><div id="eD4lwFhnFnLc"><vev id="eMZnHHc9MSb"><div><w id="eMZnHHc9MSbc"><p>Human centric interactions</p></w></div></vev><vev id="ezFPbxjMzem"><div><w id="ezFPbxjMzemc"><p>Essential events to sketch out interactive flows instantly.</p></w></div></vev></div></vev></div></vev></div></vev><vev id="eOdHmOqEaYe"><div id="eOdHmOqEaYec"><vev id="ebM69-_c2og"><div id="ebM69-_c2ogc"><vev id="eOTOw22askv"><div><w id="eOTOw22askvc"><img alt="Font" loading="lazy" src="https://cdn.vev.design/cdn-cgi/image/f=auto,q=82/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/NL-NSUWzaE.png" srcset="https://cdn.vev.design/cdn-cgi/image/f=auto,q=82,h=320/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/NL-NSUWzaE.png 320w,https://cdn.vev.design/cdn-cgi/image/f=auto,q=82/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/NL-NSUWzaE.png 540w"></w></div></vev></div></vev><vev id="eFBUsUQjjHo"><div id="eFBUsUQjjHoc"><vev id="e_oR3je7qt4"><div id="e_oR3je7qt4c"><vev id="ewGG6DZEndP"><div><w id="ewGG6DZEndPc"><p>Authoring features for spatial computing</p></w></div></vev><vev id="ePOlpfZ-YA-"><div><w id="ePOlpfZ-YA-c"><p>Bring spatial prototypes to life with simple and graceful actions.</p></w></div></vev></div></vev></div></vev></div></vev></div></vev><vev id="eOWxXPhCcb_"><div id="eOWxXPhCcb_c"><vev id="eyGS4LhhZUq"><div id="eyGS4LhhZUqc"><vev id="ekeC9U2ynBC"><div id="ekeC9U2ynBCc"><vev id="eA95AGe1xxe"><div><w id="eA95AGe1xxec"><p><video aria-label="Spatial-audio-s.mov" disableremoteplayback="" muted="" playsinline="" poster="https://cdn.vev.design/a/c1uZR8BI8TkpmJHZsLDg/p/wyPaPGvlZ-/v/8ghTcNuUu8/thumbnail0000000000.jpeg" preload="auto" autoplay=""><source src="https://cdn.vev.design/a/c1uZR8BI8TkpmJHZsLDg/p/wyPaPGvlZ-/v/8ghTcNuUu8/hd-h264.mp4" type="video/mp4"><source src="https://cdn.vev.design/a/c1uZR8BI8TkpmJHZsLDg/p/wyPaPGvlZ-/v/8ghTcNuUu8/hd-vp9.mp4" type="video/mp4">Your browser does not support this video</video></p></w></div></vev></div></vev><vev id="eIN7V2kY1s8"><div id="eIN7V2kY1s8c"><vev id="eblKmv9fjHQ"><div id="eblKmv9fjHQc"><vev id="eDUevS4Rs3W"></vev><vev id="ewomc9jODww"><div><w id="ewomc9jODwwc"><p>Add audio to create truly immersive and intuitive experiences</p></w></div></vev></div></vev></div></vev></div></vev><vev id="eJURSgj_WuS"><div id="eJURSgj_WuSc"><vev id="eu_J7GRx-My"><div id="eu_J7GRx-Myc"><vev id="eHa1VcrSfxP"><div><w id="eHa1VcrSfxPc"><p><video aria-label="GLB-Animation-s_2.mp4" disableremoteplayback="" muted="" playsinline="" poster="https://cdn.vev.design/a/c1uZR8BI8TkpmJHZsLDg/p/wyPaPGvlZ-/v/XmqXBHrnCW/thumbnail0000000000.jpeg" preload="auto" autoplay=""><source src="https://cdn.vev.design/a/c1uZR8BI8TkpmJHZsLDg/p/wyPaPGvlZ-/v/XmqXBHrnCW/hd-h264.mp4" type="video/mp4"><source src="https://cdn.vev.design/a/c1uZR8BI8TkpmJHZsLDg/p/wyPaPGvlZ-/v/XmqXBHrnCW/hd-vp9.mp4" type="video/mp4">Your browser does not support this video</video></p></w></div></vev></div></vev><vev id="eZRxtoBXLCJ"><div id="eZRxtoBXLCJc"><vev id="eIPN6gIyF_H"><div id="eIPN6gIyF_Hc"><vev id="eje6wmwkA9g"></vev><vev id="eOt3vq44FC0"><div><w id="eOt3vq44FC0c"><p>Import 3D assets with animations to add finesse and fun</p></w></div></vev></div></vev></div></vev></div></vev><vev id="e9sRSnvVPlO"><div id="e9sRSnvVPlOc"><vev id="eVIUw5LNGYH"><div id="eVIUw5LNGYHc"><vev id="e2GjubL-Bit"><div><w id="e2GjubL-Bitc"><p><video aria-label="PNG animations.mov" disableremoteplayback="" muted="" playsinline="" poster="https://cdn.vev.design/a/c1uZR8BI8TkpmJHZsLDg/p/wyPaPGvlZ-/v/xDsza3gj9j/thumbnail0000000000.jpeg" preload="auto" autoplay=""><source src="https://cdn.vev.design/a/c1uZR8BI8TkpmJHZsLDg/p/wyPaPGvlZ-/v/xDsza3gj9j/hd-h264.mp4" type="video/mp4"><source src="https://cdn.vev.design/a/c1uZR8BI8TkpmJHZsLDg/p/wyPaPGvlZ-/v/xDsza3gj9j/hd-vp9.mp4" type="video/mp4">Your browser does not support this video</video></p></w></div></vev></div></vev><vev id="eyVcOFV1PZZ"><div id="eyVcOFV1PZZc"><vev id="eqI5cU08f6P"><div id="eqI5cU08f6Pc"><vev id="eL2zsN1y0ul"></vev><vev id="eOpMOsJspPY"><div><w id="eOpMOsJspPYc"><p>Quickly mock up spatial UIs with images.</p></w></div></vev></div></vev></div></vev></div></vev></div></vev></div></vev><vev id="ebe2wMbAYiD"></vev></div></vev></div></div></vev><vev id="eD0L2Izs4fP"><div><w id="eD0L2Izs4fPc"></w><div><vev id="e5aF1sUPe8g"><div id="e5aF1sUPe8gc"><vev id="e6I7f44faNV"></vev><vev id="eA5PMVXXDZA"><div><w id="eA5PMVXXDZAc"><p id=""><h2>A workflow to go fast, go far and discover.</h2></p></w></div></vev><vev id="egU12XeRbfH"><div id="egU12XeRbfHc"><vev id="efGzSf3U2Oz"></vev><vev id="emi_4MttdDJ"><div id="emi_4MttdDJc"><vev id="e5othl-kiRV"><div><w id="e5othl-kiRVc"><p><video aria-label="Screen Recording 2024-11-26 at 15.48.09.mov" disableremoteplayback="" muted="" playsinline="" poster="https://cdn.vev.design/cdn-cgi/image/f=auto,q=82,h=1280/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/H6QPVVBFNO" preload="auto" autoplay=""><source src="https://cdn.vev.design/a/c1uZR8BI8TkpmJHZsLDg/p/wyPaPGvlZ-/v/kG2Lluougd/hd-h264.mp4" type="video/mp4"><source src="https://cdn.vev.design/a/c1uZR8BI8TkpmJHZsLDg/p/wyPaPGvlZ-/v/kG2Lluougd/hd-vp9.mp4" type="video/mp4">Your browser does not support this video</video></p></w></div></vev></div></vev></div></vev><vev id="eCTaEb4XssM"><div><w id="eCTaEb4XssMc"><p id=""><h3>With Ordinary Objects, ideating, designing and testing <em>true spatial</em> experiences is faster and more fun than ever before. Discover novel use-cases, create variants and fine tune prototypes until they excite and are intuitive to use.</h3></p></w></div></vev><vev id="egIU6bMrvl3"></vev></div></vev></div></div></vev><vev id="eZDYLy159zL"><div><w id="eZDYLy159zLc"></w><div><vev id="em18a4xf610"><div id="em18a4xf610c"><vev id="erGLJ86-Egk"><div id="erGLJ86-Egkc"><vev id="egMayGHHHgv"><div><w id="egMayGHHHgvc"><img alt="" loading="lazy" src="https://cdn.vev.design/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/qLlzZcOOk1.svg" srcset=""></w></div></vev></div></vev><vev id="ekEOO0G9StJ"><div><w id="ekEOO0G9StJc"><p id=""><h2>We are on a mission to merge our physical world with the synthetic reality we live in.</h2><h2>We believe in a world where we are free of screens and floating panels.</h2><h2>Present in the moment, connected by <em>ordinary objects</em>.</h2></p></w></div></vev><vev id="eYHDiYcN5UN"></vev><vev id="e5WDH2yvbm8"></vev></div></vev></div></div></vev><vev id="eXaexRh8dOH"><div><w id="eXaexRh8dOHc"></w><div><vev id="eVCfJUwoo7H"><div id="eVCfJUwoo7Hc"><vev id="e7lhOVXITGa"></vev><vev id="ejJzOVEJZLM"><div id="ejJzOVEJZLMc"><vev id="ebWF-I5tvoM"><div id="ebWF-I5tvoMc"><vev id="eKApWwZDQ4q"><div><w id="eKApWwZDQ4qc"><p id=""><h2>Go above and beyond.</h2></p></w></div></vev><vev id="eTxU-6Vs4_o"><div id="eTxU-6Vs4_oc"><vev id="ecU0iahm1nt"><div role="button" tabindex="0"><w id="ecU0iahm1ntc"><p>Ordinary Objects is the first mixed reality prototyping platform that elevates plane and spatial anchors to the design process.</p></w></div></vev></div></vev></div></vev><vev id="ewYGIYNu6mE"><div id="ewYGIYNu6mEc"><vev id="e_QI4PURFJ5"><div id="e_QI4PURFJ5c"><vev id="ehp3Ng_53wP"><div id="ehp3Ng_53wPc"><vev id="earFzaFt7Dg"></vev><vev id="elxPYYlk9yW"><div><w id="elxPYYlk9yWc"><p><video aria-label="Spatial Canvas - Anchors.mov" disableremoteplayback="" muted="" playsinline="" poster="https://cdn.vev.design/a/c1uZR8BI8TkpmJHZsLDg/p/wyPaPGvlZ-/v/hzlOIf34Sg/thumbnail0000000000.jpeg" preload="auto" autoplay=""><source src="https://cdn.vev.design/a/c1uZR8BI8TkpmJHZsLDg/p/wyPaPGvlZ-/v/hzlOIf34Sg/hd-h264.mp4" type="video/mp4"><source src="https://cdn.vev.design/a/c1uZR8BI8TkpmJHZsLDg/p/wyPaPGvlZ-/v/hzlOIf34Sg/hd-vp9.mp4" type="video/mp4">Your browser does not support this video</video></p></w></div></vev></div></vev><vev id="e_-BvFn9pQu"><div id="e_-BvFn9pQuc"><vev id="eAViVlXVp0k"><div id="eAViVlXVp0kc"><vev id="eLGJWV5UnQ5"><div id="eLGJWV5UnQ5c"><vev id="eW8roZljJ3w"><div id="eW8roZljJ3wc"><vev id="eEb75ZiUpoT"><div id="eEb75ZiUpoTc"><vev id="epODTmfdquZ"><div><w id="epODTmfdquZc"><img alt="" loading="lazy" src="https://cdn.vev.design/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/350hA44uhp.svg" srcset=""></w></div></vev></div></vev><vev id="exYc8k2Zxdg"><div id="exYc8k2Zxdgc"><vev id="e4IyA5URQTO"></vev><vev id="eQdzCQBNw0L"><div><w id="eQdzCQBNw0Lc"><p>Interact with physical surfaces.</p></w></div></vev></div></vev></div></vev></div></vev></div></vev></div></vev></div></vev><vev id="eOlYSGQmSBu"><div id="eOlYSGQmSBuc"><vev id="e9b9gWAj3jP"><div id="e9b9gWAj3jPc"><vev id="eLZX3WtXHsZ"><div id="eLZX3WtXHsZc"><vev id="eD_1rpie7w6"><div id="eD_1rpie7w6c"><vev id="eIkNjGJvF8g"><div id="eIkNjGJvF8gc"><vev id="er0CLHoYkes"><div id="er0CLHoYkesc"><vev id="eJHCRHjKAHn"><div><w id="eJHCRHjKAHnc"><img alt="" loading="lazy" src="https://cdn.vev.design/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/7wtW_VEhNY.svg" srcset=""></w></div></vev></div></vev><vev id="eT3My_B_btn"></vev></div></vev></div></vev></div></vev></div></vev><vev id="elrUu_FrA6j"></vev></div></vev></div></vev><vev id="eSdHfs8KMJU"><div id="eSdHfs8KMJUc"><vev id="ehMDiNCX5j0"><div role="button" tabindex="0"><w id="ehMDiNCX5j0c"><p>Next-level creative freedom that sets a new standard for interaction design. Ordinary Objects allows full control over the first crucial touchpoints of entering a spatial app.</p></w></div></vev></div></vev></div></vev><vev id="eOrLRuYktFf"></vev></div></vev></div></div></vev><vev id="exFllCXsl_Y"><div><w id="exFllCXsl_Yc"></w><div><vev id="eiZ9Cy2PGRt"><div id="eiZ9Cy2PGRtc"><vev id="eAJG5yC-qKj"><div id="eAJG5yC-qKjc"><vev id="eU52efa6E9_"><div id="eU52efa6E9_c"><vev id="ezbZLf4Fcsg"><div><w id="ezbZLf4Fcsgc"><p id=""><h2>Start prototyping.</h2></p></w></div></vev><vev id="e-g73oX4AUE"><div><w id="e-g73oX4AUEc"><p>Download the native editor for macOS or windows.</p></w></div></vev><vev id="eRcUOAxtisb"><div id="eRcUOAxtisbc"><vev id="eP53_IsjWTN"></vev><vev id="eUCF8wAX9_q"><div><w id="eUCF8wAX9_qc"><img alt="Rectangle, Font" loading="lazy" src="https://cdn.vev.design/cdn-cgi/image/f=auto,q=82,w=1280/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/Ri_p3Hwb_d" srcset="https://cdn.vev.design/cdn-cgi/image/f=auto,q=82,w=320/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/Ri_p3Hwb_d 320w,https://cdn.vev.design/cdn-cgi/image/f=auto,q=82,w=640/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/Ri_p3Hwb_d 640w,https://cdn.vev.design/cdn-cgi/image/f=auto,q=82,w=960/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/Ri_p3Hwb_d 960w,https://cdn.vev.design/cdn-cgi/image/f=auto,q=82,w=1280/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/Ri_p3Hwb_d 1280w"></w></div></vev></div></vev></div></vev><vev id="ec24unks_Lc"><div id="ec24unks_Lcc"><vev id="e_DOTeLgsy8"><div><w id="e_DOTeLgsy8c"><p id=""><h2>Test prototypes.</h2></p></w></div></vev><vev id="eizMBfpdQ7V"><div><w id="eizMBfpdQ7Vc"><p>Get the mirror apps for Quest 3 and iOS.</p></w></div></vev><vev id="ekw6ljqcFHy"><div id="ekw6ljqcFHyc"><vev id="eQLE0mbqhtL"></vev><vev id="ewuZ2l8QKc1"><div id="ewuZ2l8QKc1c"><vev id="ePAUkA4jV6N"><div><w id="ePAUkA4jV6Nc"><img alt="Rectangle" loading="lazy" src="https://cdn.vev.design/cdn-cgi/image/f=auto,q=82/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/xqEAfnnKJr" srcset="https://cdn.vev.design/cdn-cgi/image/f=auto,q=82,w=320/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/xqEAfnnKJr 320w,https://cdn.vev.design/cdn-cgi/image/f=auto,q=82,w=640/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/xqEAfnnKJr 640w"></w></div></vev></div></vev></div></vev></div></vev></div></vev></div></vev></div></div></vev><vev id="e_bjFwbHGUF"><div><w id="e_bjFwbHGUFc"></w><div><vev id="ewavDeIhYX-"><div id="ewavDeIhYX-c"><vev id="eHkI_90X9lv"></vev><vev id="eF9cP7CYnLy"><div id="eF9cP7CYnLyc"><vev id="eE9TYAQ69bp"><div id="eE9TYAQ69bpc"><vev id="epRHIX90fl8"><div><w id="epRHIX90fl8c"><p id=""><h2>Collaboration, in hyperspace</h2></p></w></div></vev><vev id="egr9I-_H-IN"><div id="egr9I-_H-INc"><vev id="e_uF316dJhg"><div role="button" tabindex="0"><w id="e_uF316dJhgc"><p>Everything up to date, everywhere, instantly.</p></w></div></vev></div></vev></div></vev><vev id="epHzmJY2tQg"><div id="epHzmJY2tQgc"><vev id="e_8CIUJjOgj"><div id="e_8CIUJjOgjc"><vev id="eYQe-QSHPSD"><div><w id="eYQe-QSHPSDc"><img alt="Product, Font, Screenshot, Rectangle" loading="lazy" src="https://cdn.vev.design/cdn-cgi/image/f=auto,q=82,w=1920/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/ShQ2FyzBm2.png" srcset="https://cdn.vev.design/cdn-cgi/image/f=auto,q=82,w=320/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/ShQ2FyzBm2.png 320w,https://cdn.vev.design/cdn-cgi/image/f=auto,q=82,w=640/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/ShQ2FyzBm2.png 640w,https://cdn.vev.design/cdn-cgi/image/f=auto,q=82,w=960/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/ShQ2FyzBm2.png 960w,https://cdn.vev.design/cdn-cgi/image/f=auto,q=82,w=1280/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/ShQ2FyzBm2.png 1280w,https://cdn.vev.design/cdn-cgi/image/f=auto,q=82/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/ShQ2FyzBm2.png 2048w"></w></div></vev></div></vev><vev id="eer6SlPFYHw"><div id="eer6SlPFYHwc"><vev id="eo4tgWaYXv6"><div id="eo4tgWaYXv6c"><vev id="e23AUK13sno"><div id="e23AUK13snoc"><vev id="edgLsN6Dfzv"><div id="edgLsN6Dfzvc"><vev id="eBHynb_2rxC"><div id="eBHynb_2rxCc"><vev id="ezMV5dbzIW1"><div><w id="ezMV5dbzIW1c"><img alt="" loading="lazy" src="https://cdn.vev.design/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/_jmheJ5k-d.svg" srcset=""></w></div></vev></div></vev><vev id="eFoqKk4fM7-"><div id="eFoqKk4fM7-c"><vev id="eolHrgyVmCY"><div><w id="eolHrgyVmCYc"><p>Changes in the editor get synchronised automatically so your collaborators are always up to date, in the editor and mirror.</p></w></div></vev><vev id="e5F778cT7SG"><div><w id="e5F778cT7SGc"><p>Collaborating never felt so smooth.</p></w></div></vev></div></vev></div></vev></div></vev></div></vev></div></vev></div></vev><vev id="eINbji20eJR"></vev></div></vev><vev id="edEhEtDUddv"></vev></div></vev></div></div></vev><vev id="enhBtcyHCHE"><div><w id="enhBtcyHCHEc"></w><div><vev id="erZAFmUvcW6"><div id="erZAFmUvcW6c"><vev id="e-QlUGs0HEU"></vev><vev id="eBhM4IRrZXH"><div><w id="eBhM4IRrZXHc"><p id=""><h2>Collaboration, in hyperspace.</h2></p></w></div></vev><vev id="egoZ3a8Y1vl"><div id="egoZ3a8Y1vlc"><vev id="e79SksDL_SB"><div><w id="e79SksDL_SBc"><div id=""><p>In Ordinary Objects, collaboration feels like a super power.</p><p>Riff on ideas or refine a prototype.</p></div></w></div></vev><vev id="eBxlWDONw4m"></vev><vev id="e00A_augfuS"></vev></div></vev></div></vev></div></div></vev><vev id="eU4x_3RAjtD"><div><w id="eU4x_3RAjtDc"></w><div><vev id="ecTeJE0VLSA"><div id="ecTeJE0VLSAc"><vev id="e5zh0xbtNLF"><div id="e5zh0xbtNLFc"><vev id="e-8LzwI_6F3"><div id="e-8LzwI_6F3c"><vev id="e2LLXXRMvAw"><div><w id="e2LLXXRMvAwc"><img alt="" loading="lazy" src="https://cdn.vev.design/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/nOj78YuQA4.svg" srcset=""></w></div></vev></div></vev><vev id="eyQyGxvO1In"><div id="eyQyGxvO1Inc"><vev id="eUsXn8KtotY"><div><w id="eUsXn8KtotYc"><p id=""><h2>Delightful spatial apps.</h2></p></w></div></vev><vev id="eaoSXKM3zD-"></vev></div></vev><vev id="eH2JhE09Miu"><div id="eH2JhE09Miuc"><vev id="eo-03kxry0Q"><div id="eo-03kxry0Qc"><vev id="e8x3zxTy16z"></vev><vev id="eWSl6Ew3cxY"></vev><vev id="eoS0KnspT8-"></vev><vev id="euuHTZ0okLV"></vev><vev id="ec84MO0EycP"><a href="https://www.linkedin.com/company/ordinaryobjectsdesign" data-tween="false" target="_blank"><div id="ec84MO0EycPc"><vev id="euEBHjJ_nA0"><div><w id="euEBHjJ_nA0c"><img alt="" loading="lazy" src="https://cdn.vev.design/cdn-cgi/image/f=auto,q=82/private/Q7R509Ho9MZy0VuJiZbVOMS2HmR2/image/-yCLMprD0J" srcset=""></w></div></vev></div></a></vev></div></vev><vev id="elQBxy9940L"></vev></div></vev></div></vev></div></vev></div></div></vev></main></div></div>]]></description>
        </item>
    </channel>
</rss>