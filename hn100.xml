<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 19 Jan 2024 11:00:06 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Google will disable all but OAuth for IMAP, SMTP and POP starting Sept. 30 (196 pts)]]></title>
            <link>https://workspaceupdates.googleblog.com/2023/09/winding-down-google-sync-and-less-secure-apps-support.html</link>
            <guid>39052196</guid>
            <pubDate>Fri, 19 Jan 2024 06:30:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://workspaceupdates.googleblog.com/2023/09/winding-down-google-sync-and-less-secure-apps-support.html">https://workspaceupdates.googleblog.com/2023/09/winding-down-google-sync-and-less-secure-apps-support.html</a>, See on <a href="https://news.ycombinator.com/item?id=39052196">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<center>
			<h4>
				<a href="https://www.googlecloudcommunity.com/gc/Google-Workspace/ct-p/google-workspace" target="_blank">Join the official community for Google Workspace administrators</a>
			</h4>
			<p>
				In the Google Cloud Community, connect with Googlers and other Google Workspace admins like yourself. Participate in product discussions, check out the Community Articles, and learn tips and tricks that will make your work and life easier. Be the first to know what's happening with Google Workspace.
			</p>
<p>______________
			</p>            
            	<h4>
				<a href="https://support.google.com/a/go/whatsnew" target="_blank">Learn about more Google Workspace launches</a>
			</h4>
			<p>
				On the “What’s new in Google Workspace?” Help Center page, learn about new products and features launching in Google Workspace, including smaller changes that haven’t been announced on the Google Workspace Updates blog.
			</p>
<p>______________
			</p>            
            	</center>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Culture Change at Google (273 pts)]]></title>
            <link>https://social.clawhammer.net/blog/posts/2024-01-19-CultureChange/</link>
            <guid>39051655</guid>
            <pubDate>Fri, 19 Jan 2024 04:46:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://social.clawhammer.net/blog/posts/2024-01-19-CultureChange/">https://social.clawhammer.net/blog/posts/2024-01-19-CultureChange/</a>, See on <a href="https://news.ycombinator.com/item?id=39051655">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p><em>Disclaimer: this post is solely based on my lived experience of
working at Google for 18 years.  I don't actually know the reasoning
of the company's highest leaders, so all I can do is share my personal
hypotheses.</em></p>
<p>I've tried to write this essay three times over the past couple of
months; it's tricky.</p>
<p>It's always trendy and click-baity to attack big targets, especially
when those targets are full of hubris like Silicon Valley tech
companies.  People love "fall from grace" stories.  But my goal isn't
to throw dirt; Google is still a great place to work -- far better
than most companies -- and still doing amazing things.  My goal here
is to call out a unique, beautiful thing that happened... put it out
into the universe, with the hope that it can come back again someday,
somewhere.</p>
<p>There's no doubt that the early days of Google were "over the top".  I
deliberately <a href="https://social.clawhammer.net/blog/posts/2005-09-25-FirstWeekAtGoogle/">saved this
email</a>
for 18 years, waiting for the day I left the company, because I knew
it would be a fascinating time-capsule comparison.  But the email
mostly focuses on <em>superficial</em> differentiators, like free gourmet
food.  In truth, that's not why Googlers come to work.  I want to talk
about a deeper, more substantive aspect of the culture.</p>
<p>When Ian Hickson -- another old-timer -- left Google last fall, he
wrote a <a href="https://ln.hixie.ch/?start=1700627373">blog post</a> talking
about the shift in the types of decisions being made.  I generally
agree with him, but I won't repeat it all here -- I'm going to talk
about a different shift.</p>
<p>The most incredible and unusual thing that struck me about Google's
early culture was the <strong>tendency to value employees above all else</strong>.
I had worked in other companies, and never seen anything like this
before.  This culture lasted for at least my first decade at the
company, perhaps longer.</p>
<p>Let me explain.  In a typical company, when priorities shift, you
"downsize" (or cancel) a project, and then use the money to add people
to a different, more important project.  The common way to do this is
fire people from the first project, then rehire a bunch of new people
in the second project.  It's easy, it's simple, it's expected.</p>
<p>Google, however, had a different approach: they had an absolutely
intense hiring process to find talented people who were also
<em>generalists</em> -- that is, were able to thrive in a whole number of
roles.  The interviews were grueling for both applicants and
interviewers, often taking months.  But in the end, Google was
convinced that it was worth the effort: they believed they had hired
the best, brightest, and most flexible.</p>
<p>And so, when priorities would change, Google <strong>did not fire people</strong>,
but rather <em>moved</em> them carefully between projects.  The unstated
cultural principle was: "products come and go, but we worked <em>so hard</em>
to get our employees... so we should preserve them at all costs. They
are our most precious resource."  And so a tremendous amount of energy
was put into high-touch resettlement of each employee into new
projects.  They were generalists.  We knew they'd thrive, and that
Google would continue to make use of their talent in new ways.</p>
<p>As I moved up into leadership over the years, I became ever more
involved in this process.  In the early days as an individual
contributor, I experienced re-orgs directly and got "re-homed" into
new projects.  As a leader, I got involved in <em>finding</em> new homes for
teams during re-orgs. Eventually I wrote an internal handbook for
other directors on how to gracefully execute these re-orgs.  One of my
fondest memories is getting a peer-bonus from an engineer whose own
team re-org I had personally instigated -- he was much happier working
on the new "more important" project!</p>
<p>But things change.  In my first month at Google, I remember a
co-worker whispering to me, "the day Google revenue stops growing
without bound, is also the day all of this will change."  The change
was very gradual for a long time -- but then things accelerated during
the pandemic.  Revenue began to slow, and now, coming out of the
pandemic, we're seeing waves of layoffs.  Yes, we knew things would
change, but we didn't expect it would accelerate <em>this quickly</em>, in
the span of just a couple of years.  The academic founders are gone,
much of the C-suite is now former Wall Street execs; combine that with
revenue flattening toward a stable horizontal asymptote, and the
obvious, expected thing happens: the company suddenly moves from a
"culture of infinite abundance" to a standard "culture of limited
resources."  It's a predictable regression toward becoming a 'normal'
company.<sup><a href="#fn1" id="fnref1">[1]</a></sup></p>
<p>So what does a culture of "limited resources" mean?  It means the
execs start thinking about financial efficiency like every other
company.  You begin by trimming the more superficial perks: less fancy
food, limiting travel budgets, no more swag, smaller and fewer
internal parties and events, no more onsite dry cleaning or daycare.
But again, these things weren't the reasons Googlers came to work.  No
big deal.</p>
<p>But then you begin to cut costs further by changing the ornate hiring
and promotion processes to become "traditional".  Hiring changes from
a laborious global process (of checks and balances) to a localized one
within divisions that can tightly control their labor costs.
Meanwhile, internal promotion processes change from "competing against
yourself" to "competing against your co-workers for limited
positions."  In the early days, titles were attached to <em>people</em>, but
now they're increasingly attached to <em>roles</em>, and the number of roles
(for any given title) can be limited to save cost.</p>
<p>Finally, it comes time to do large re-orgs of projects around new
urgent priorities (like AI, for example).  But gone is the high-touch
re-homing of employees. Instead, we see waves of impersonal layoffs,
followed by (modest) rehiring in the newer projects that matter.  In
other words: doing what a normal company does.</p>
<p>Is Google evil here?  Of course not.  As I mentioned in a prior post,
<a href="https://social.clawhammer.net/blog/posts/2024-01-10-GoogleExitLetter/">Google is not a
person</a>.
And -- whether or not one agrees with it -- its leaders are trying to
be fiscally responsible and efficient, just as all public companies
are pressured to do when resources become finite.</p>
<p>But, coming back to my first decade at Google, it was incredible to
see <strong>employees</strong> valued above everything else.  Perhaps this is a
privilege only possible in a culture of infinite abundance.  Or maybe
not?  Maybe it's possible in a limited-resource culture too, but only
if the company is small.  It leaves me wondering if the sheer size of
Google (170,000+ employees) simply makes high-touch re-orgs
intractable.</p>
<p>The takeaway here is this: we should all learn from early-Google's
example.  When employees feel truly valued (which is rare!), it
creates psychological safety, high morale, productivity, and
creativity.  Early employees would often encourage each other to "fail
fast" as a means to innovation, but that's no longer easy in an
environment where failure implies a layoff.  If you're someone
building a company, challenge yourself to value employees above all
else, then watch and be amazed at the ROI.</p>
<p><img src="https://social.clawhammer.net/blog/images/bookcase.jpg" alt="bookcase knicknacks"></p>
<p><em>published January 19, 2024</em></p>
<hr>
<section>
<ol>
<li id="fn1"><p>To be clear, I believe Google is still nowhere <em>near</em> being a
normal company yet.  It has a tremendous distance to fall. <a href="#fnref1">↩︎</a></p>
</li>
</ol>
</section>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dave Mills has died (339 pts)]]></title>
            <link>https://elists.isoc.org/pipermail/internet-history/2024-January/009265.html</link>
            <guid>39051246</guid>
            <pubDate>Fri, 19 Jan 2024 03:27:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://elists.isoc.org/pipermail/internet-history/2024-January/009265.html">https://elists.isoc.org/pipermail/internet-history/2024-January/009265.html</a>, See on <a href="https://news.ycombinator.com/item?id=39051246">Hacker News</a></p>
<div id="readability-page-1" class="page">
   
    <b>vinton cerf</b> 
    <a href="mailto:internet-history%40elists.isoc.org?Subject=Re%3A%20%5Bih%5D%20Dave%20Mills%20has%20passed%20away&amp;In-Reply-To=%3CCAAFtm_UCRsBSbZYcSAzb4wGapWgpUGKPAQcgcjMqjqScmhQdtA%40mail.gmail.com%3E" title="[ih] Dave Mills has passed away">vgcerf at gmail.com
       </a><br>
    <i>Thu Jan 18 17:35:14 PST 2024</i>
    <ul>
        <li>Previous message (by thread): <a href="https://elists.isoc.org/pipermail/internet-history/2024-January/009264.html">[ih] Tell me about host names and 3com
</a></li>
        <li>Next message (by thread): <a href="https://elists.isoc.org/pipermail/internet-history/2024-January/009266.html">[ih] Dave Mills has passed away
</a></li>
         <li> <b>Messages sorted by:</b> 
              <a href="https://elists.isoc.org/pipermail/internet-history/2024-January/date.html#9265">[ date ]</a>
              <a href="https://elists.isoc.org/pipermail/internet-history/2024-January/thread.html#9265">[ thread ]</a>
              <a href="https://elists.isoc.org/pipermail/internet-history/2024-January/subject.html#9265">[ subject ]</a>
              <a href="https://elists.isoc.org/pipermail/internet-history/2024-January/author.html#9265">[ author ]</a>
         </li>
       </ul>
    <hr>  
<!--beginarticle-->
<pre>His daughter, Leigh, just sent me the news that Dave passed away peacefully
on January 17, 2024. He was such an iconic element of the early Internet.
Network Time Protocol, the Fuzzball routers of the early NSFNET, INARG
taskforce lead, COMSAT Labs and University of Delaware and so much more.

R.I.P.
vint
</pre>


<!--endarticle-->
    <hr>
    <ul>
        <!--threads-->
	<li>Previous message (by thread): <a href="https://elists.isoc.org/pipermail/internet-history/2024-January/009264.html">[ih] Tell me about host names and 3com
</a></li>
	<li>Next message (by thread): <a href="https://elists.isoc.org/pipermail/internet-history/2024-January/009266.html">[ih] Dave Mills has passed away
</a></li>
         <li> <b>Messages sorted by:</b> 
              <a href="https://elists.isoc.org/pipermail/internet-history/2024-January/date.html#9265">[ date ]</a>
              <a href="https://elists.isoc.org/pipermail/internet-history/2024-January/thread.html#9265">[ thread ]</a>
              <a href="https://elists.isoc.org/pipermail/internet-history/2024-January/subject.html#9265">[ subject ]</a>
              <a href="https://elists.isoc.org/pipermail/internet-history/2024-January/author.html#9265">[ author ]</a>
         </li>
       </ul>

<hr>
<a href="https://elists.isoc.org/mailman/listinfo/internet-history">More information about the Internet-history
mailing list</a><br>

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[A 2024 Plea for Lean Software (109 pts)]]></title>
            <link>https://berthub.eu/articles/posts/a-2024-plea-for-lean-software/</link>
            <guid>39049956</guid>
            <pubDate>Fri, 19 Jan 2024 00:21:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://berthub.eu/articles/posts/a-2024-plea-for-lean-software/">https://berthub.eu/articles/posts/a-2024-plea-for-lean-software/</a>, See on <a href="https://news.ycombinator.com/item?id=39049956">Hacker News</a></p>
<div id="readability-page-1" class="page"><article lang="en">
  

  
  

  <div>
  <blockquote>
<p>This post is dedicated to the memory of <a href="https://ethz.ch/en/news-and-events/eth-news/news/2024/01/computer-pioneer-niklaus-wirth-has-died.html">Niklaus Wirth</a>, a computing pioneer who passed away January 1st. In 1995 he wrote an influential article called “<a href="https://cr.yp.to/bib/1995/wirth.pdf">A Plea for Lean Software</a>”, and in what follows, I try to make the same case nearly 30 years later, updated for today’s computing horrors.</p>
</blockquote>
<p>The really short version: the way we build/ship software these days is mostly ridiculous, leading to <a href="https://hub.docker.com/r/grafana/grafana">350MB packages that draw graphs</a>, and simple products importing <a href="https://github.com/SashenJayathilaka/Photo-Sharing-Application">1600 dependencies of unknown provenance</a>. Software security is dire, which is a function both of the quality of the code and the sheer amount of it. Many of us know the current situation is untenable. Many programmers (and their management) sadly haven’t ever experienced anything else. And for the rest of us, we rarely get the time to do a better job.</p>
<p>In this post I briefly go over the terrible state of software security, and then spend some time on why it is so bad. I also mention some regulatory/legislative things going on that we might use to make software quality a priority again. Finally, I talk about <a href="https://berthub.eu/articles/trifecta">an actual useful piece of software I wrote</a> as a reality check of the idea that one can still make <a href="https://berthub.eu/articles/posts/trifecta-technology">minimal and simple yet modern software</a>.</p>
<p>I hope that this post provides some mental and moral support for suffering programmers and technologists who want to improve things. <strong>It is not just you, we are not merely suffering from nostalgia: software really is very weird today</strong>.</p>
<center>
<p><img src="https://berthub.eu/articles/old-man.jpeg" alt=""></p>
</center>
<h2 id="the-state-of-software">The state of software</h2>
<p>Without going all ‘<a href="https://knowyourmeme.com/memes/old-man-yells-at-cloud">old man (48) yells at cloud</a>’, let me restate some obvious things. The state of software is DIRE. If we only look at the past year, if you ran industry standard software like <a href="https://www.ncsc.gov.uk/news/exploitation-ivanti-vulnerabilities">Ivanti</a>, <a href="https://en.wikipedia.org/wiki/2023_MOVEit_data_breach">MoveIT</a>, <a href="https://www.bleepingcomputer.com/news/microsoft/russian-hackers-exploiting-outlook-bug-to-hijack-exchange-accounts/">Outlook</a>,  <a href="https://confluence.atlassian.com/security/cve-2023-22518-improper-authorization-vulnerability-in-confluence-data-center-and-server-1311473907.html">Confluence</a>, <a href="https://www.mandiant.com/resources/blog/barracuda-esg-exploited-globally">Barracuda Email Security Gateway</a>, <a href="https://www.mandiant.com/resources/blog/remediation-netscaler-adc-gateway-cve-2023-4966">Citrix NetScaler ADC and NetScaler Gateway</a>, chances are you got hacked. Even companies with near infinite resources (like Apple and Google) made <a href="https://www.schneier.com/blog/archives/2023/09/critical-vulnerability-in-libwebp-library.html">trivial “worst practice” security mistakes</a> which put <a href="https://www.bleepingcomputer.com/news/security/apple-zero-click-imessage-exploit-used-to-infect-iphones-with-spyware/">their customers in danger</a>. Yet we continue to rely on all these products.</p>
<p>Software is now (rightfully) considered so dangerous that we tell everyone not to run it themselves. Instead, you are supposed to leave that to an “as a service” provider, or perhaps to “the cloud”. Compare this to a hypothetical situation where cars are so likely to catch fire that the advice is not to drive a car yourself, but to leave that to professionals who are always accompanied by professional firefighters.</p>
<p>The assumption is then that “the cloud” is somehow able to turn insecure software into a secure service. Yet even the past year, we’ve learned that Microsoft’s <a href="https://thehackernews.com/2023/09/outlook-breach-microsoft-reveals-how.html">email platform was thoroughly hacked</a>, down to classified government email. There are also <a href="https://www.lastweekinaws.com/blog/azures-terrible-security-posture-comes-home-to-roost/">well-founded worries about the security of the Azure cloud</a>. Meanwhile, industry darling <a href="https://www.reuters.com/technology/cybersecurity/okta-says-hackers-stole-data-all-customer-support-users-cyber-breach-2023-11-29/">Okta, which provides <strong>LOG IN SOLUTIONS</strong> got comprehensively owned</a>. This was their second breach within a year. Also, there was a suspicious spate of Okta users getting hacked subsequently.</p>
<p>Clearly we need better software.</p>
<blockquote>
<p>The EU has launched three pieces of legislation to this extent (<a href="https://digital-strategy.ec.europa.eu/en/policies/nis2-directive">NIS2 for important services</a>, the Cyber Resilience Act for almost all commercial software and things with plugs, a revamped <a href="https://www.euractiv.com/section/digital/news/eu-updates-product-liability-regime-to-include-software-artificial-intelligence/">Product Liability Directive</a> that extends to software). Legislation is always hard, and it remains to be seen <a href="https://berthub.eu/articles/posts/eu-cra-what-does-it-mean-for-open-source/">if they got it right</a>. But that software security is terrible enough these days to warrant legislation seems obvious.</p>
</blockquote>
<h2 id="why-software-is-so-bad">Why software is so bad</h2>
<p>I briefly want to touch on incentives. The situation today is clearly working well for commercial operators. Making more secure software takes time and is a lot of work, and the current security incidents all don’t appear to be impacting the bottom line or stock prices. You can <a href="https://www.microsoft.com/en-us/research/publication/software-components-only-the-giants-survive/">speed up time to market by cutting corners</a>. So from an economic standpoint, what we see is what you would expect. Legislation could be very important in changing this equation.</p>
<p>The security of software depends on two factors - the <strong>density</strong> of security issues in the source code, and <strong>the sheer amount of exposed code</strong>. As the US defense community loved to point out in the 1980s, <a href="https://www.quora.com/Who-said-Quantity-has-a-quality-all-its-own">quantity has a quality all of its own</a>. The reverse applies to software - the more you have of it, the more risks you run.</p>
<p>As a case in point, Apple iPhone users got repeatedly hacked over many years because of the huge <em>attack surface</em> exposed by iMessage. It is possible to send an unsolicited iMessage to an Apple user, and the phone will then immediately process that message so it can preview it. The problem is that Apple in its wisdom decided that such unsolicited messages needed to support a vast array of image formats, accidentally <a href="https://googleprojectzero.blogspot.com/2021/12/a-deep-dive-into-nso-zero-click.html">including PDFs, including PDFs with weird embedded compressed fonts using an ancient format that effectively included a programming language</a>.</p>
<p>In this way, attackers were able to benefit from security bugs in probably millions of lines of code. You don’t need a high bug density to find an exploitable hole in millions of lines of code. And <a href="https://www.europarl.europa.eu/meetdocs/2014_2019/plmrep/COMMITTEES/PEGA/DV/2023/05-08/REPORTcompromises_EN.pdf">nation state suppliers have found lots</a>.</p>
<p>The weird thing is that Apple could have easily prevented this situation by restricting previews to a far smaller range of image formats. It is their platform, they don’t need to interoperate with anything. They could have made sending devices convert previews to a single known good image format.</p>
<p>But they didn’t. And to make matters worse, in 2023 they decided to add support for a new image format, which apparently was so important it had to be added outside of the security sandbox. <a href="https://news.ycombinator.com/item?id=37600852">This was again exploited</a>.</p>
<p>Apple could have saved themselves an enormous amount of pain simply by <a href="https://github.com/berthubert/sbox#sbox">exposing fewer lines of code</a> to attackers. Incidentally, the EU Cyber Resilience Act <a href="https://berthub.eu/articles/posts/eu-cra-secure-coding-solution/">explicitly tells vendors to minimise the attack surface</a>.</p>
<p>Please do note that Apple is (by far) not the worst offender in this field. But it is a widely respected and well resourced company that usually thinks through what they do. And even they got it wrong by needlessly shipping and exposing too much code.</p>
<h2 id="could-we-not-write-better-code">Could we not write better code?</h2>
<p>It is not just the <em>amount</em> of code that is worrying. It is also the quality, or put another way, the density of bugs. There are many interesting things happening on this front, like the use of <a href="https://www.nsa.gov/Press-Room/Press-Releases-Statements/Press-Release-View/Article/3608324/us-and-international-partners-issue-recommendations-to-secure-software-products/">memory safe languages</a> like <a href="https://www.rust-lang.org/">Rust</a>. Other languages are <a href="https://github.com/google/sanitizers/wiki/AddressSanitizer">also upping their security game</a>. <a href="https://en.wikipedia.org/wiki/Fuzzing">Fuzzers</a> are also getting ever more advanced.</p>
<p>But many security problems are not so much bad code but more bad logic. A recent example is a super duper security issue in GitLab where accounts could be <a href="https://berthub.eu/articles/pwned-gitlab.png">trivially taken over</a> through the <a href="https://about.gitlab.com/releases/2024/01/11/critical-security-release-gitlab-16-7-2-released/#account-takeover-via-password-reset-without-user-interactions">‘forgot password’ functionality</a>. Similarly, the Barracuda exploit consisted of them relying on a third party library <a href="https://www.cvedetails.com/cve/CVE-2023-7101/">that would actually execute code in scanned Excel sheets</a>. The recent Ivanti exploit is <a href="https://attackerkb.com/topics/AdUh6by52K/cve-2023-46805/rapid7-analysis">similarly logic related</a> (and extremely embarrassing).</p>
<p>Less progress is being made on improving the <a href="https://labs.watchtowr.com/the-second-wednesday-of-the-first-month-of-every-quarter-juniper-0day-revisited/">logic bugs situation</a> than on the code security front.</p>
<p>I’m all for writing more secure code, but as a first step, let’s look what code we are actually shipping. And do we even know?</p>
<h2 id="the-state-of-shipping-software">The state of shipping software</h2>
<p>I mean, wow, software has gotten HUGE. It is exceptionally painful to read <a href="https://en.wikipedia.org/wiki/Niklaus_Wirth">Niklaus Wirth</a>’s article <a href="https://cr.yp.to/bib/1995/wirth.pdf">A Plea for Lean Software</a> from 1995, which laments that 1995 era software needed whole megabytes, and then goes on to describe the <a href="https://en.wikipedia.org/wiki/Oberon_(operating_system)">Oberon Operating System</a> which he built which needed only 200KB, including an editor and a compiler. There are now likely projects that have more than 200KB of YAML alone.</p>
<p>A typical app is now built on <a href="https://www.electronjs.org/">Electron JS</a> which incorporates both Chromium (“Chrome”) and Node.JS. From what I read, I estimate this entails at least 50 million lines of code if you include dependencies. Perhaps more. The app meanwhile likely pulls in hundreds or thousands of Node modules. Many frameworks used will also, by default, snitch on your users to advertisers and other data brokers. Incidentally, dependencies pull in further dependencies and exactly what gets included in the build can change on a daily basis, and no one really knows.</p>
<p>If this app controls anything in your house, it will also connect to a software stack over at Amazon, probably also powered by Node.JS, again pulling in many dependencies. And as usual, no one is even sure what it pulls in exactly as this changes from day to day.</p>
<p>But wait, there’s more. We used to ship software as the output of a compiler, or perhaps as a bunch of files to be interpreted. Such software then had to be <em>installed</em> and <em>configured</em> to work right. Getting your code packaged to ship like this is a lot of work. But it was good work since it forced people to think about what was in their “package”. This software package would then integrate with an operating system and with local services, based on the configuration.</p>
<p>Since the software ran on a fundamentally different computer than it was developed on, people really had to know what they shipped and think it through. And sometimes it didn’t work, leading to the joke where a developer tells the operations people “Well, it works on my system”, and the retort “back up your email, we’re taking your laptop in production!”.</p>
<p>This used to be a joke, but these days we often ship software as (Docker or other) containers, and this frequently entails effectively shipping a complete computer image. Including all the stuff that happened to be included in the build. This again vastly expands the amount of code being deployed. Note that you can do good things with Docker (see below), but there are a lot of 350+MB images on the <a href="https://hub.docker.com/explore">Docker Hub</a>.</p>
<p>But, all in all, we are likely looking at 50 million+ lines of code active to open a garage door, running several operating system images on multiple servers.</p>
<p>Now, even if all the included dependencies are golden, are we sure that their security updates are making it to your garage door opener app? I wonder how many Electron apps are still shipping with the vulnerable <a href="https://www.schneier.com/blog/archives/2023/09/critical-vulnerability-in-libwebp-library.html">libwebp version</a> in there. We don’t even know.</p>
<p>But even worse, it is a known fact that all these dependencies are not golden. The Node.js ecosystem has a <a href="https://thehackernews.com/2023/02/researchers-hijack-popular-npm-package.html">comical history</a> of repositories <a href="https://snyk.io/blog/npm-security-preventing-supply-chain-attacks/">being taken over</a>, hijacked or resurrected under the same name by someone else, someone with <a href="https://www.theregister.com/2023/06/19/npm_s3_buckets_malware/">dire plans for your security</a>. <a href="https://www.theregister.com/2023/06/02/novel_pypi_attack_reversinglabs/">PyPI</a> has suffered from <a href="https://www.theregister.com/2023/01/04/pypi_pytorch_dependency_attack/">similar problems</a>. Dependencies always need scrutiny, but no one can reasonably be expected to <a href="https://medium.com/graph-commons/analyzing-the-npm-dependency-network-e2cf318c1d0d">check thousands of them frequently</a>. But we prefer not to think about this and type ’npm install’ and observe 1600 dependencies being pulled.</p>
<p>Note that one should also not overshoot and needlessly reimplement everything yourself to prevent dependencies. There are very good dependencies that <a href="https://sqlite.org/">likely are more secure</a> than what you <a href="https://en.wikipedia.org/wiki/Lightning_Memory-Mapped_Database">could type in on your own</a>.</p>
<p>Rounding off a bit, I posit that the world is 1) shipping far too much code 2) where we don’t even know what we ship and 3) we aren’t looking hard enough (or at all) at what we know we ship.</p>
<h2 id="trifecta">Trifecta</h2>
<p>Writing has been called <a href="https://fs.blog/writing-to-think/">the process by which you find out you don’t know what you are talking about</a>. Actually doing stuff meanwhile is the process by which find out you also did not know what you were writing about.</p>
<p>In a very small re-enactment of Wirth’s Oberon Project, I too wrote some code to prove a point, but also to reassure myself I still know what I am talking and writing about. Can you still make useful and modern software “the old way”?</p>
<p>Trifecta is <a href="https://berthub.eu/articles/trifecta/">actual stand-alone software</a> that you can use to paste and drag images to, for easy sharing. It has pained me for years that I had to use <a href="https://imgur.com/">imgur</a> for this purpose. Not only does imgur install lots of cookies and trackers on my browser, I also force these trackers onto the people that view the images that I share.</p>
<p>If you want to self-host a service like this, you also don’t want to get hacked. Most image sharing solutions I found that you could run yourself are based on huge frameworks that I don’t trust too much (given the dependency reasons outlined above). And perhaps that is my background, <a href="https://berthub.eu/articles/bio">I used to work with a lot of classified data, and I’ve been very exposed to what the very best state sponsored hackers can do</a>.</p>
<p>So, also to make a point, I decided to create a minimalistic but also useful image sharing solution that I could trust. And more important, that other people could trust as well, because you can check out the whole code within a few hours. It consists of <a href="https://berthub.eu/articles/posts/trifecta-technology">1600 lines of new source code</a>, plus around 5 important dependencies (line number sizes are included in the linked article).</p>
<p>And this is what you then end up with:</p>
<p><img src="https://berthub.eu/trifecta/i/qT5j9y6LOF4" alt=""></p>
<p>To contrast, <a href="https://github.com/CaramelFur/Picsur/pkgs/container/picsur">one other image sharing solution</a> ships as a 311MB Docker image, although admittedly it looks better and has some more features. But not 308MB worth of them. Another comparison is <a href="https://github.com/SashenJayathilaka/Photo-Sharing-Application">this Node based picture sharing solution</a> which clocks in at 1600 dependencies, apparently totaling 4+ million lines of JavaScript.</p>
<p>Trifecta is a self-contained solution with just a handful of dependencies that gives you a feature complete image sharing site:</p>
<ul>
<li>Full user and session management</li>
<li>Drag and drop multiple images at the same time</li>
<li>Posts can contain multiple images
<ul>
<li>Each post has an optional title, each image an optional caption</li>
<li>Posts can be public, or time limited public</li>
</ul>
</li>
<li>Passwordless accounts are possible (log in using a temporary sign-in email link)
<ul>
<li>Lost password email flow</li>
</ul>
</li>
<li>One cookie, locked tight to the site</li>
<li>Comes as source, binary, <a href="https://hub.docker.com/r/berthubert/trifecta">docker</a>, or .deb or .rpm</li>
<li><a href="https://github.com/berthubert/trifecta">Source code</a> small enough you could read all of it in a day</li>
<li>Source code <a href="https://github.com/berthubert/trifecta/blob/main/support.cc">also reusable for other web frameworks</a></li>
</ul>
<p>Note that this is not intended as a public site where random people can share images, as this does not tend to end well. It is however very suitable for company or personal use. You can read more about the project <a href="https://berthub.eu/articles/trifecta">here</a>, and there is also <a href="https://berthub.eu/articles/posts/trifecta-technology">a page about the technology used to deliver such a tiny self-contained solution</a>.</p>
<h2 id="response">Response</h2>
<p>This has been rather interesting. As noted earlier in this post, we have gone quite mad that we need 50+ million lines of code for a garage door opener. That we find this normal must come with some pathology.</p>
<blockquote>
<p>Some years ago I did a talk at a local university on cybersecurity, titled “<a href="https://berthub.eu/cyber-mad/Cyber%20and%20information%20security.pdf">Have we all gone mad</a>”. It is still worth reading today since we have gone quite mad collectively.</p>
</blockquote>
<p>The most common response to Trifecta so far has been that I should use a whole bag of AWS services to deploy it. This is an exceedingly odd response to a project with the clearly stated goal of providing standalone software that does not rely on external services. I’m not sure what is going on here.</p>
<p>Another reaction has been that I treat Docker unfairly, and that you could definitely use containers for good. And I agree wholeheartedly. But I also look at what people are actually doing (also with other forms of containers/VMs), and that’s not so great.</p>
<p>I want to end this post with some observations from <a href="https://cr.yp.to/bib/1995/wirth.pdf">Niklaus Wirth’s 1995 paper</a>.</p>
<ul>
<li>“To Some, complexity equals power. (…) Increasingly, people seem to <strong>misinterpret complexity as sophistication</strong>, which is baffling - the incomprehensible should cause suspicion rather than admiration.”</li>
</ul>
<p>I’ve similarly observed that some people prefer complicated systems. As <a href="https://en.wikipedia.org/wiki/Tony_Hoare">Tony Hoare</a> noted long ago, “There are two methods in software design. One is to make the program so simple, there are obviously no errors. The other is to make it so complicated, there are no obvious errors”. If you can’t do the first variant, the second way starts looking awfully attractive perhaps.</p>
<ul>
<li>“Time pressure is probably the foremost reason behind the emergence of bulky software. The time pressure that designers endure discourages careful planning. It also discourages improving acceptable solutions; instead, it encourages quickly conceived software additions and corrections. <strong>Time pressure gradually corrupts an engineer’s standard of quality and perfection. It has a detrimental effect on people as well as products</strong>.”</li>
</ul>
<p>Why spend weeks paring down your software when you can also ship a whole pre-installed operating system image that just works?</p>
<ul>
<li>“The plague of software explosion is not a ’law of nature’. It is avoidable, and it is the software engineer’s task to curtail it”</li>
</ul>
<p>Now, I once studied physics, and I’m not so sure if an increase in complexity is not a law of nature. However, I do know that decreasing entropy will always cost energy. And if this is indeed on the shoulders of software people, we should perhaps demand more time for it.</p>
<h2 id="summarising">Summarising</h2>
<p>The world ships too much code, most of it by third parties, sometimes unintended, most of it uninspected. Because of this there is a huge <em>attack surface</em> full of mediocre code. Efforts are ongoing to improve the quality of code itself, but many exploits are due to logic bugs, and less progress has been made scanning for those. Meanwhile, great strides could be made by paring down just how much code we expose to the world. This will increase time to market for products, but legislation is around the corner that should force vendors to take security more seriously.</p>
<p>Trifecta is, like Wirth’s Oberon Project mentioned above, meant as a verification that you can still deliver a lot of functionality based on a limited amount of code and dependencies.</p>
<p>With effort and legislation, maybe the future could again bring sub-50 million line garage door openers. Let’s try to make it happen.</p>

</div>

  



</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meta documents show 100k children sexually harassed daily on its platforms (162 pts)]]></title>
            <link>https://www.theguardian.com/technology/2024/jan/18/instagram-facebook-child-sexual-harassment</link>
            <guid>39049799</guid>
            <pubDate>Fri, 19 Jan 2024 00:02:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/technology/2024/jan/18/instagram-facebook-child-sexual-harassment">https://www.theguardian.com/technology/2024/jan/18/instagram-facebook-child-sexual-harassment</a>, See on <a href="https://news.ycombinator.com/item?id=39049799">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>Meta estimates about 100,000 children using Facebook and <a href="https://www.theguardian.com/technology/instagram" data-link-name="in body link" data-component="auto-linked-tag">Instagram</a> receive online sexual harassment each day, including “pictures of adult genitalia”, according to internal company documents made public late Wednesday.</p><p>The unsealed legal filing includes several allegations against the company based on information the New Mexico attorney general’s office received from presentations by <a href="https://www.theguardian.com/technology/meta" data-link-name="in body link" data-component="auto-linked-tag">Meta</a> employees and communications between staff. The documents describe an incident in 2020 when the 12-year-old daughter of an executive at Apple was solicited via IG Direct, Instagram’s messaging product.</p><p>“This is the kind of thing that pisses Apple off to the extent of threatening to remove us from the App Store,” a Meta employee fretted, according to the documents. A senior Meta employee described <a href="https://www.theguardian.com/technology/2023/nov/07/meta-facebook-employee-congress-testimony-instagram-child-harm-social-media" data-link-name="in body link">how his own daughter had been solicited via Instagram in testimony to the US Congress late last year</a>. His efforts to fix the problem were ignored, he said.</p><p>The filing is the <a href="https://www.theguardian.com/global-development/2023/dec/07/meta-platforms-are-marketplaces-for-child-predators-claims-lawsuit" data-link-name="in body link">latest in a lawsuit</a> initiated by the New Mexico attorney general’s office on 5 December, which alleges Meta’s social networks have become marketplaces for child predators. Raúl Torrez, the state’s attorney general, has accused Meta of enabling adults to find, message and groom children. The company has denied the suit’s claims, saying it “mischaracterizes our work using selective quotes and cherry-picked documents”.</p><p>Meta issued a statement in response to Wednesday’s filing: “We want teens to have safe, age-appropriate experiences online, and we have over 30 tools to support them and their parents. We’ve spent a decade working on these issues and hiring people who have dedicated their careers to keeping young people safe and supported online.”</p><p>A 2021 internal presentation on child safety was also referenced in the lawsuit. According to the suit, one slide stated that Meta is “underinvested in minor sexualization on IG, notable on sexualized comments on content posted by minors. Not only is this a terrible experience for creators and bystanders, it’s also a vector for bad actors to identify and connect with one another.”</p><p>The complaint also highlights Meta employees’ concerns over child safety. In a July 2020 internal Meta chat, one employee asked: “What specifically are we doing for child grooming (something I just heard about that is happening a lot on TikTok)?” According to the complaint, he received a response: “Somewhere between zero and negligible.”</p><p>Meta’s statement also says the company has taken “significant steps to prevent teens from experiencing unwanted contact, especially from adults”.</p><p>The New Mexico lawsuit follows a <a href="https://www.theguardian.com/news/2023/apr/27/how-facebook-and-instagram-became-marketplaces-for-child-sex-trafficking" data-link-name="in body link">Guardian investigation in April</a> that uncovered how Meta is failing to report or detect the use of its platforms for child trafficking. The investigation also revealed how Messenger, Facebook’s private messaging service, is used as a platform for traffickers to communicate to buy and sell children.</p><p>Meta employees discussed the use of Messenger “to coordinate trafficking activities” and facilitate “every human exploitation stage (recruitment, coordination, exploitation) is represented on our platform”, according to documents included in the suit.</p><p>Yet, an internal 2017 email describes executive opposition to scanning <a href="https://www.theguardian.com/technology/facebook" data-link-name="in body link" data-component="auto-linked-tag">Facebook</a> Messenger for “harmful content” because it would place the service “at a competitive disadvantage vs other apps who might offer more privacy”, the lawsuit states.</p><p>In December, <a href="https://www.theguardian.com/technology/2023/dec/08/facebook-messenger-encryption-child-sexual-abuse" data-link-name="in body link">Meta received widespread criticism</a> for rolling out end-to-end encryption for messages sent on Facebook and via Messenger. Encryption hides the contents of a message from anyone but the sender and the intended recipient by converting text and images into unreadable cyphers that are unscrambled on receipt. Child safety experts, policymakers and law enforcement have argued encryption obstructs efforts to rescue child sex-trafficking victims and the prosecution of predators. Privacy advocates praised the decision for shielding users from surveillance by governments and law enforcement.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AlphaFold Found Possible Psychedelics (240 pts)]]></title>
            <link>https://www.nature.com/articles/d41586-024-00130-8</link>
            <guid>39049703</guid>
            <pubDate>Thu, 18 Jan 2024 23:53:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/d41586-024-00130-8">https://www.nature.com/articles/d41586-024-00130-8</a>, See on <a href="https://news.ycombinator.com/item?id=39049703">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-024-00130-8/d41586-024-00130-8_26638900.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-024-00130-8/d41586-024-00130-8_26638900.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="An AlphaFold protein structure of the protein Vitellogenin." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-024-00130-8/d41586-024-00130-8_26638900.jpg">
  <figcaption>
   <p><span>Protein structures predicted by AlphaFold have helped to identify candidate drug compounds.</span><span>Credit: DeepMind</span></p>
  </figcaption>
 </picture>
</figure><p>Researchers have used the protein-structure-prediction tool AlphaFold to identify<sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup> hundreds of thousands of potential new psychedelic molecules — which could help to develop new kinds of antidepressant. The research shows, for the first time, that AlphaFold predictions — available at the touch of a button — can be just as useful for drug discovery as experimentally derived protein structures, which can take months, or even years, to determine.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-023-02984-w" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-024-00130-8/d41586-024-00130-8_26639072.jpg"><p>AlphaFold touted as next big thing for drug discovery — but is it?</p></a>
 </article><p>The development is a boost for AlphaFold, the artificial-intelligence (AI) tool developed by DeepMind in London that has been a game-changer in biology. The public AlphaFold database holds structure predictions for nearly every known protein. Protein structures of molecules implicated in disease are used in the pharmaceutical industry to identify and improve promising medicines. But some scientists had been starting to doubt whether AlphaFold's predictions could stand-in for gold standard experimental models in the hunt for new drugs.</p><p>“AlphaFold is an absolute revolution. If we have a good structure, we should be able to use it for drug design,” says Jens Carlsson, a computational chemist at the University of Uppsala in Sweden.</p><h2>AlphaFold scepticism</h2><p>Efforts to apply AlphaFold to finding new drugs have been met with considerable scepticism, says Brian Shoichet, a pharmaceutical chemist at the University of California, San Francisco. “There is a lot of hype. Whenever anybody says ‘such and such is going to revolutionize drug discovery’, it warrants some scepticism.”</p><p>Shoichet counts more than ten studies that have found AlphaFold’s predictions to be less useful than protein structures obtained with experimental methods, such as X-ray crystallography, when used to identify potential drugs in a modelling method called protein–ligand docking.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-022-00997-5" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-024-00130-8/d41586-024-00130-8_20323140.png"><p>What's next for AlphaFold and the AI protein-folding revolution</p></a>
 </article><p>This approach — common in the early stages of drug discovery — involves modelling how hundreds of millions or billions of chemicals interact with key regions of a target protein, in the hope of identifying compounds that alter the protein’s activity. Previous studies have tended to find that when AlphaFold-predicted structures are used, the models are poor at singling out drugs already known to bind to a particular protein.</p><p>Researchers led by Shoichet and Bryan Roth, a structural biologist at the University of North Carolina at Chapel Hill, came to a similar conclusion when they checked AlphaFold structures of two proteins implicated in neuropsychiatric conditions against known drugs. The researchers wondered whether small differences from experimental structures might cause the predicted structures to miss certain compounds that bind to proteins — but also make them able to identify different ones that were no less promising.</p><p>To test this idea, the team used experimental structures of the two proteins to virtually screen hundreds of millions of potential drugs. One protein, a receptor that senses the neurotransmitter serotonin, was previously determined using cryo-electron microscopy. The structure of the other protein, called the σ-<sub>2</sub> receptor, had been mapped using X-ray crystallography.</p><h2>Drug differences</h2><p>They ran the same screen with models of the proteins plucked from the AlphaFold database. They then synthesized hundreds of the most promising compounds identified with either the predicted and experimental structures and measured their activity in the lab.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-020-03348-4" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-024-00130-8/d41586-024-00130-8_18745984.jpg"><p>‘It will change everything’: DeepMind’s AI makes gigantic leap in solving protein structures</p></a>
 </article><p>The screens with predicted and experimental structures yielded completely different drug candidates. “There were no two molecules that were the same,” says Shoichet. “They didn’t even resemble each other.”</p><p>But to the team’s surprise, the ‘hit rates’ — the proportion of flagged compounds that actually altered protein activity in a meaningful way — were nearly identical for the two groups. And AlphaFold structures identified the drugs that activated the serotonin receptor most potently. The psychedelic drug LSD works partly through this route, and many researchers are looking for non-hallucinogenic compounds that do the same thing, as potential antidepressants. “It’s a genuinely new result,” says Shoichet.</p><h2>Prediction power</h2><p>In unpublished work, Carlsson’s team has found that AlphaFold structures are good at identifying drugs for a sought-after class of target called G-protein-coupled receptors, for which their hit rate is around 60%.</p><p>Having confidence in predicted protein structures could be game-changing for drug discovery, says Carlsson. Determining structures experimentally isn’t trivial, and many would-be targets might not yield to existing experimental tools. “It would be very convenient if we could push the button and get a structure we can use for ligand discovery,” he says.</p><figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-024-00130-8/d41586-024-00130-8_26638902.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-024-00130-8/d41586-024-00130-8_26638902.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="Photo illustration of the Isomorphic Labs logo displayed on a tablet." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-024-00130-8/d41586-024-00130-8_26638902.jpg">
  <figcaption>
   <p><span>Isomorphic Labs, a spin-off company of Google’s DeepMind in London, is ramping up its drug-discovery efforts using AlphaFold.</span><span>Credit: Igor Golovniov/SOPA Images/LightRocket via Getty</span></p>
  </figcaption>
 </picture>
</figure><p>The two proteins that Shoichet and Roth’s team picked are good candidates for relying on AlphaFold, says Sriram Subramaniam, a structural biologist at the University of British Columbia in Vancouver, Canada. Experimental models of related proteins — including detailed maps of the regions where drugs bind to them — are readily available. "If you stack the deck, AlphaFold is a paradigm shift. It changes the way we do things," he adds.</p><p>“This is not a panacea,” says Karen Akinsanya, president of research and development for therapeutics at Schrödinger, a drug-software company based in New York City that is using AlphaFold. Predicted structures are helpful for some drug targets, but not others, and it’s not always clear which applies. In about 10% of cases, predictions AlphaFold deems highly accurate are substantially different from the experimental structure, a study<sup><a href="#ref-CR3" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">3</a></sup> found.</p><p>And even when predicted structures can help to identify leads, more detailed experimental models are often needed to optimize the properties of a particular drug candidate, Akinsanya adds.</p><h2>Big bet</h2><p>Shoichet agrees that AlphaFold predictions are not universally useful. “There were a lot of models that we didn’t even try because we thought they were so bad,” he says. But he estimates that in about one-third of cases, an AlphaFold structure could jump-start a project. “Compared to actually going out and getting a new structure, you could advance the project by a couple of years and that’s huge,” he says.</p><p>That is the goal of Isomorphic Labs, DeepMind’s drug-discovery spin-off in London. On 7 January, the company announced deals worth a minimum of US$82.5 million — and up to $2.9 billion if business targets are met — to hunt for drugs on behalf of pharmaceutical giants Novartis and Eli Lilly using machine-learning tools such as AlphaFold.</p><p>The company says that the work will be aided by a new version of AlphaFold that can predict the structures of proteins when they are bound to drugs and other interacting molecules. DeepMind has not yet said when — or whether — the update will be made available to researchers, as earlier versions of AlphaFold have been. A competing tool called RoseTTAFold All-Atom<sup><a href="#ref-CR2" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">2</a></sup> will be made available soon by its developers.</p><p>Such tools won’t fully replace experiments, scientists say, but their potential to help find new drugs shouldn’t be discounted. “There’s a lot of people that want AlphaFold to do everything, and a lot of structural biologists want to find reasons to say we are still needed,” says Carlsson. “Finding the right balance is difficult.”</p>
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[YouTube and Spotify won't launch Apple Vision Pro apps, joining Netflix (122 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2024-01-18/youtube-and-spotify-join-netflix-in-not-launching-apple-vision-pro-apps</link>
            <guid>39049217</guid>
            <pubDate>Thu, 18 Jan 2024 23:07:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2024-01-18/youtube-and-spotify-join-netflix-in-not-launching-apple-vision-pro-apps">https://www.bloomberg.com/news/articles/2024-01-18/youtube-and-spotify-join-netflix-in-not-launching-apple-vision-pro-apps</a>, See on <a href="https://news.ycombinator.com/item?id=39049217">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <section>
        <h3>Why did this happen?</h3>
        <p>Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our <a href="https://www.bloomberg.com/notices/tos">Terms of
                Service</a> and <a href="https://www.bloomberg.com/notices/tos">Cookie Policy</a>.</p>
    </section>
    <section>
        <h3>Need Help?</h3>
        <p>For inquiries related to this message please <a href="https://www.bloomberg.com/feedback">contact
            our support team</a> and provide the reference ID below.</p>
        <p>Block reference ID:</p>
    </section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[My AI costs went from $100 to less than $1/day: Fine-tuning Mixtral with GPT4 (249 pts)]]></title>
            <link>https://twitter.com/wenquai/status/1748016021808595242</link>
            <guid>39048948</guid>
            <pubDate>Thu, 18 Jan 2024 22:43:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/wenquai/status/1748016021808595242">https://twitter.com/wenquai/status/1748016021808595242</a>, See on <a href="https://news.ycombinator.com/item?id=39048948">Hacker News</a></p>
Couldn't get https://twitter.com/wenquai/status/1748016021808595242: Error: Request failed with status code 400]]></description>
        </item>
        <item>
            <title><![CDATA[We migrated our PostgreSQL database with 11 seconds downtime (408 pts)]]></title>
            <link>https://gds.blog.gov.uk/2024/01/17/how-we-migrated-our-postgresql-database-with-11-seconds-downtime/</link>
            <guid>39048317</guid>
            <pubDate>Thu, 18 Jan 2024 21:51:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gds.blog.gov.uk/2024/01/17/how-we-migrated-our-postgresql-database-with-11-seconds-downtime/">https://gds.blog.gov.uk/2024/01/17/how-we-migrated-our-postgresql-database-with-11-seconds-downtime/</a>, See on <a href="https://news.ycombinator.com/item?id=39048317">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>https://gds.blog.gov.uk/2024/01/17/how-we-migrated-our-postgresql-database-with-11-seconds-downtime/</p><div>
			<p><img fetchpriority="high" decoding="async" src="https://gds.blog.gov.uk/wp-content/uploads/sites/60/2024/01/47306620-27FB-4510-BBDC-5F8F00242DDA-620x441.jpg" alt="GOV.UK Notify Team members working together" width="620" height="441" srcset="https://gds.blog.gov.uk/wp-content/uploads/sites/60/2024/01/47306620-27FB-4510-BBDC-5F8F00242DDA-620x441.jpg 620w, https://gds.blog.gov.uk/wp-content/uploads/sites/60/2024/01/47306620-27FB-4510-BBDC-5F8F00242DDA-310x220.jpg 310w, https://gds.blog.gov.uk/wp-content/uploads/sites/60/2024/01/47306620-27FB-4510-BBDC-5F8F00242DDA-768x546.jpg 768w, https://gds.blog.gov.uk/wp-content/uploads/sites/60/2024/01/47306620-27FB-4510-BBDC-5F8F00242DDA-1536x1092.jpg 1536w, https://gds.blog.gov.uk/wp-content/uploads/sites/60/2024/01/47306620-27FB-4510-BBDC-5F8F00242DDA-2048x1456.jpg 2048w" sizes="(max-width: 620px) 100vw, 620px"></p>
<p><a href="https://www.notifications.service.gov.uk/"><span>GOV.UK Notify</span></a><span> is hosted on the </span><a href="https://www.cloud.service.gov.uk/"><span>GOV.UK Platform as a Service </span></a><span>(PaaS). The PaaS is being retired, so we are migrating all of our infrastructure into our own Amazon Web Services (AWS) account. This blog post explains how we migrated our </span><a href="https://www.postgresql.org/"><span>PostgreSQL</span></a><span> database with minimal downtime.</span></p>
<p><img decoding="async" src="https://gds.blog.gov.uk/wp-content/uploads/sites/60/2024/01/govuk-notify-blog-post-image-620x413.png" alt="Graph showing a spike of errors over an 11-second period during our database migration." width="620" height="413" srcset="https://gds.blog.gov.uk/wp-content/uploads/sites/60/2024/01/govuk-notify-blog-post-image-620x413.png 620w, https://gds.blog.gov.uk/wp-content/uploads/sites/60/2024/01/govuk-notify-blog-post-image-310x207.png 310w, https://gds.blog.gov.uk/wp-content/uploads/sites/60/2024/01/govuk-notify-blog-post-image-768x512.png 768w, https://gds.blog.gov.uk/wp-content/uploads/sites/60/2024/01/govuk-notify-blog-post-image.png 999w" sizes="(max-width: 620px) 100vw, 620px"></p>
<h2><b>Migrating our database</b></h2>
<p><span>The PaaS provides a database for us and we use it to store all of our data - from data about each notification we send to the content of the hundreds of thousands of templates service teams use to send those notifications. This is an </span><a href="https://aws.amazon.com/rds/"><span>AWS RDS</span></a><span> PostgreSQL database and it lives in the PaaS’ AWS account. Our apps that run in the PaaS talk to this database. We are going to call this database our ‘source database’.</span></p>
<p><span>We needed to set up a new database in our own AWS account, and get all of our apps to talk to the new database. We are going to call this new database our ‘target database’.</span></p>
<p><span>Creating a new PostgreSQL database in our own AWS account is not too difficult. The hard part is transferring all of our data and getting our apps to use this new database, whilst incurring minimal downtime.</span></p>
<h2><b>A bit more about our source database</b></h2>
<p><span>Our source database is about 400GB in size. It has about 1.3 billion rows, 85 tables, 185 indexes and 120 foreign keys. It is PostgreSQL version 11.</span></p>
<p><span>On a usual weekday, we do somewhere in the region of 1,000 inserts or updates per second (sometimes much lower, sometimes much higher), plus a similar number of reads.&nbsp;</span></p>
<p><span>GOV.UK Notify sends millions of important and timely notifications each day, from flood alerts to updating users about their passport applications . Every notification we send requires talking to our database. Therefore it’s important that we minimise any downtime.</span></p>
<h2><b>AWS Database Migration Service</b></h2>
<p><span>The PaaS team offered us the ability to migrate databases using </span><a href="https://docs.aws.amazon.com/dms/latest/userguide/Welcome.html"><span>AWS Database Migration Service</span></a><span> (DMS).&nbsp;</span></p>
<p><span>DMS is responsible for transferring data from our source database to our target database. It can be run in either the source or target AWS account.</span></p>
<p><span>DMS works by:</span></p>
<ol>
<li><span>Copying across all of the data, table by table, up to a specific point in time. This is known as the ‘full load’ task.</span></li>
<li><span>Entering replication mode, where it ensures that all new transactions on the source database are replayed onto the target database, so that the 2 databases are in sync.</span></li>
</ol>
<p><span>We would then be responsible for getting our apps to stop talking to the source database and start talking to the target database.<br>
</span></p>
<h2><b>Database migration process</b></h2>
<p><span>The database migration process was completed in several stages.</span></p>
<h3><b>Setting up the DMS instance</b></h3>
<p><span>In our case, the DMS instance was created in the source AWS account. We chose the source account because the PaaS team had already set up instances of DMS in their account and so were able to do this quickly and easily.</span></p>
<p><span>The DMS instance also needed to be given PostgreSQL credentials to talk to both the source and target database.&nbsp;</span></p>
<p><span>The DMS instance and the target database live in different virtual private clouds (VPCs). With the help of the PaaS team, we set up </span><a href="https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html"><span>VPC peering</span></a><span> so that traffic from the DMS instance in the PaaS’s VPC could be routed directly to our VPC without the traffic going over the public internet.</span></p>
<h3><b>Setting up our target database</b></h3>
<p><span>We created our target RDS instance in our own AWS account. PostgresSQL version 11 was about to become unsupported, so we took this opportunity to upgrade our PostgreSQL version by making our new database PostgreSQL 15.</span></p>
<p><span>We then took a dump of the database schema for our source database using `pg_dump`. This gave us a file with the SQL commands to recreate our database schema.</span></p>
<p><span>From our database schema, we took the declarations for our tables and applied these to our target database.</span></p>
<p><span>We didn’t apply our foreign keys at this point because DMS’ full load process doesn’t try to copy across the data in an order that matches your foreign key constraints.</span></p>
<p><span>We didn’t create our primary keys or indexes at this point because this would massively slow down our full load task. Each individual insert would take longer; it would need to update our indexes and this would add up to a significant amount of time when inserting billions of rows. It was much quicker to first copy all of our data across and then add the indexes afterwards.</span></p>
<h3><b>Full load</b></h3>
<p><span>Once we had a target database with the tables created, we then started the DMS full load task. This copies across all the data that existed when we pressed the ‘start full load’ button. It doesn’t copy across any new data or updates that come in after this point. It took about 6 hours for the full load task to finish.</span></p>
<p><span>After the full load task completed, we applied the remainder of our source database schema file which adds our indexes and key constraints. Adding these took about 3 hours.</span></p>
<h3><b>Replication</b></h3>
<p><span>Once our full load task completed, the data in our target database matched the data from the source database at the point when we started the full load task. But many new inserts, updates and deletions had happened on our source database since then. And many more changes would keep coming in too.</span></p>
<p><span>To copy these new changes across, we then started the DMS ongoing replication (also known as change data capture) task. This reads all the transactions from our source database </span><a href="https://www.postgresql.org/docs/current/wal-intro.html"><span>transaction log</span></a><span> that were created after the full load task began and sends them to our target database. This ensures that our target database is in sync with our source database with, at most, a small amount of lag.</span></p>
<p><span>It only took a couple of hours for the replication process to catch up. At that point, we monitored the latency in the DMS replication process to make sure it could handle the number of changes happening to the source database and continued to stay in sync.</span></p>
<p><span>We ran the DMS replication process for about 10 days in the background, keeping everything in sync whilst we awaited the time for our apps to stop talking to the source database and start talking to the target database. We had announced this time to our users in advance and so had a set time already for the migration of traffic.</span></p>
<h3><b>Preparing to migrate traffic</b></h3>
<p><span>Several months ago we planned how we would stop our apps talking to our source database and get them using our target database.This was the process we used:</span><span><br>
</span></p>
<ol>
<li><span>Stop all traffic from our apps to our source database. At this point we would enter a period of downtime where Notify was unavailable.</span></li>
<li><span>Ensure our replication had caught up so that all updates to our source database had been reflected on our target database.</span></li>
<li><span>Allow our apps to start talking to our target database. This would end our downtime.</span></li>
</ol>
<p><span>It was important not to have some of our apps talking to our source database and the rest talking to our target database at the same time. If this happened any changes on our target database would not be reflected on our source database which would mean users would get inconsistent data.</span></p>
<p><span>We wrote a Python script for this process so it could be explicit, easily repeatable and much quicker than being done manually.&nbsp; The quicker it could be done, the less downtime for users of Notify. Our target was less than 5 minutes of downtime. We ended up using this script at least 40 times during our various tests and practices beforehand.</span></p>
<p><span>We picked a Saturday evening for the migration. This is because it is one of our quietest times without us having to be awake in the middle of the night when we won’t be as alert.</span></p>
<h3><b>Stopping traffic to our source database</b></h3>
<p><span>Our script would stop all traffic to our source database by calling `pg_terminate_backend` on all the connections from our apps. This took less than a second. We also changed the password for the PostgreSQL user used by our apps, meaning that if the apps attempted to reconnect to our source database they would get an authentication error.&nbsp;</span></p>
<h3><b>Checking replication had caught up</b></h3>
<p><span>DMS inserts some useful tables into our target database on the status of the replication which are updated every minute. These tables allow us to see how much lag there is between our target database and the source database. Our migration script would check these tables to make sure our target database was entirely caught up.</span></p>
<p><span>To be extra safe, after our apps had stopped talking to our source database, our migration script would write a single record to our source database and then wait to see that it safely arrived in our target database. This gave us extra certainty that all changes had been replicated.</span></p>
<h3><b>Making a smooth swap of traffic</b></h3>
<p><span>For our apps to connect to our database, they need to know the location of the database and also a username and password for a relevant PostgreSQL user. These are provided to our apps in an environment variable of the following format:&nbsp;</span></p>
<pre><code>SQLALCHEMY_DATABASE_URI = postgresql://original-username:original-password@random-identifier.eu-west-1.rds.amazonaws.com:5432</code></pre>
<p><span>If we want our apps to connect to a different database, we need to update the username, password and location in the URI and redeploy our apps so this change takes effect. Redeploying our apps takes about 5 minutes. If we redeployed our apps as part of our migration script then this would mean an extra 5 minutes of downtime. To minimise downtime we made two changes in advance of our migration so that we could use a quick Domain Name System (DNS) change instead of redeploying our apps.</span></p>
<p><span>The first change was to create a user on both our source and target database that had the same username and password. This means that we don’t need to change the username or password provided to the apps during the migration.</span></p>
<p><span>The second change was to create a DNS record in </span><a href="https://aws.amazon.com/route53/"><span>AWS Route53</span></a><span> for `database.notifications.service.gov.uk` with a 1 second TTL (time to live). It had two records with weightings:</span></p>
<ul>
<li><span>100% of DNS results were weighted to the source database location&nbsp;</span></li>
<li><span>0% of DNS results were weighted to the target database location</span></li>
</ul>
<p>We set our URI used by our apps to use our new username and password, and to use the new domain name for the location of our database.</p>
<pre><code>SQLALCHEMY_DATABASE_URI = postgresql://shared-username:shared-password@database.notifications.service.gov.uk:5432</code></pre>
<p><span>Now, when we wanted to swap the database that our apps would be pointing at, our migration script just needed to update the DNS weighting in AWS to 100% of results being sent to the target database location and wait 1 second for the TTL to expire. Then, when our apps next try to query our database they will be querying our target database.</span></p>
<h2><b>What happened on the day</b></h2>
<p><span>When we gathered on the evening of Saturday 4 November, we had set up our target database, the full load process had run and new transactions were being copied across. We checked and only had a couple of seconds lag between our target database and the source database.&nbsp;</span></p>
<p><span>We then successfully ran our migration script so that our apps would stop talking to our source database and start talking to our new target database. During the migration there was a short period of downtime, roughly 11 seconds. This was much less than our 5 minute target so we were very pleased and so were our users.</span></p>
<h2><b>What we learnt</b></h2>
<p><span>We chose to use DMS because it was well supported by the GOV.UK PaaS and we could also get support from AWS. If we were doing a PostgreSQL to PostgreSQL database migration in the future, we would invest more time in trying alternative tools such as </span><a href="https://github.com/2ndQuadrant/pglogical"><span>pglogical</span></a><span>. DMS potentially added more complexity, and an unfamiliar replication process than what we may have found with other tools. This backs up </span><a href="https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.PostgreSQL.html#CHAP_Source.PostgreSQL.Homogeneous"><span>what AWS say themselves on PostgreSQL to PostgreSQL migrations.&nbsp;</span></a></p>
<h2><b>What’s next for GOV.UK Notify’s migration to AWS</b></h2>
<p><span>Now we’ve migrated our database, our next step is to migrate our apps. Sneak peek - we are moving them to AWS Elastic Container Service (ECS). We will blog about how this goes in the coming months.</span></p>



        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[VMware Kills Off 56 Products (109 pts)]]></title>
            <link>https://www.thestack.technology/vmware-is-killing-off-56-products-including-vsphere-hypervisor-and-nsx/</link>
            <guid>39048120</guid>
            <pubDate>Thu, 18 Jan 2024 21:37:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.thestack.technology/vmware-is-killing-off-56-products-including-vsphere-hypervisor-and-nsx/">https://www.thestack.technology/vmware-is-killing-off-56-products-including-vsphere-hypervisor-and-nsx/</a>, See on <a href="https://news.ycombinator.com/item?id=39048120">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
    <div>
      <p>Broadcom’s brutal assault on VMware’s product suite continues, with the company’s new owner this week confirming that it is sunsetting a massive 56 VMware products and platforms – as investors said this week that they anticipated a “tectonic shift” in the infrastructure market as a result.&nbsp;</p><p>In a January 15 advisory VMware confirmed tersely that it was taking a sweeping range of products to “End of Availability” and that “these products are no longer available for purchase” – although most remain advertised enthusiastically, for now, on slick corporate website pages.</p><p>(VMware was bought by Broadcom for $69 billion in a deal that closed in November 2023. Its new owner is ruthlessly focused on extracting value from large corporate clients that may struggle or be disinterested in ending use of VMware widely entrenched virtualisation technology.)</p><p>Broadcom said that it is simplifying VMware’s portfolio to “a few offers focused on our best technology”. Products getting the chop include network and security virtualization platform NSX; application mobility platform HCX; “multi-cloud” platform bundle vSPhere+ and many more.</p><p>Listing the 56 VMware products and platforms being killed off, VMware said: “All licensing options including Perpetual, Support &amp; Subscription (SnS), SaaS/hosted and subscription, as well as all editions, suites and pricing metrics of each product, unless otherwise noted, are included in this announcement. These products are no longer available for purchase.”</p><p>The move comes as customers and channel partners have been left rattled by the extent of Broadcom's strategic overhaul at the company with many bewailing a complete lack of clarity on future pricing and products. </p><div data-kg-toggle-state="close">
            <div>
                <h4><span>What's no longer available?!</span></h4>
                </div>
            <div><p><span>The </span><a href="https://kb.vmware.com/s/article/96168?lang=en_US&amp;ref=thestack.technology" rel="noreferrer"><span>list</span></a><span> is as follows:</span></p><p><span>VMware vSphere Enterprise Plus</span></p><p><span>VMware vSphere+</span></p><p><span>VMware vSphere Standard (excluding subscription)</span></p><p><span>VMware vSphere ROBO</span></p><p><span>VMware vSphere Scale Out</span></p><p><span>VMware vSphere Desktop</span></p><p><span>VMware vSphere Acceleration Kits</span></p><p><span>VMware vSphere Essentials Kit</span></p><p><span>VMWare vSphere Essentials Plus Kit (excluding new subscription offering)</span></p><p><span>VMware vSphere Starter/Foundation</span></p><p><span>VMware vSphere with Operations Management</span></p><p><span>VMware vSphere Basic</span></p><p><span>VMware vSphere Advanced</span></p><p><span>VMware vSphere Storage Appliance</span></p><p><span>VMware vSphere Hypervisor</span></p><p><span>VMware Cloud Foundation (excluding new VCF subscription offering)</span></p><p><span>VMware Cloud Foundation for VDI</span></p><p><span>VMware Cloud Foundation for ROBO</span></p><p><span>VMware SDDC Manager</span></p><p><span>VMware vCenter Standard</span></p><p><span>VMware vCenter Foundation</span></p><p><span>VMware vSAN</span></p><p><span>VMware vSAN ROBO</span></p><p><span>VMware vSAN Desktop</span></p><p><span>VMware HCI Kit</span></p><p><span>VMware Site Recovery Manager</span></p><p><span>VMware Cloud Editions/Cloud Packs</span></p><p><span>VMware vCloud Suite</span></p><p><span>VMware Aria Suite (formerly vRealize Suite)</span></p><p><span>VMware Aria Universal Suite (formerly vRealize Cloud Universal)</span></p><p><span>vMware Aria Suite Term</span></p><p><span>VMware Aria Operations for Networks (formerly vRealize Network Insight)</span></p><p><span>VMWare Aria Operations for Networks Universal (formerly vRealize Network Insight Universal)</span></p><p><span>VMware vRealize Network Insight ROBO</span></p><p><span>VMWare Aria Operations for Logs (formerly vRealize Log Insight)</span></p><p><span>VMware vRealize Operations 8 Application Monitoring Add-On</span></p><p><span>VMware Aria Operations</span></p><p><span>VMware Aria Automation</span></p><p><span>VMware Aria Automation for Secure Hosts add-on (formerly SaltStack SecOps)</span></p><p><span>VMware vRealize Automation SaltStack SecOps add-on</span></p><p><span>VMware Aria Operations for Integrations (formerly vRealize True Visibility Suite)</span></p><p><span>VMware Cloud Director</span></p><p><span>Cloud Director Service</span></p><p><span>VMware NSX</span></p><p><span>VMware NSX for Desktop</span></p><p><span>VMware NSX ROBO</span></p><p><span>VMware NSX Distributed Firewall</span></p><p><span>VMware NSX Gateway Firewall</span></p><p><span>VMware NSX Threat Prevention to Distributed Firewall</span></p><p><span>VMware NSX Threat Prevention to Gateway Firewall</span></p><p><span>VMware NSX Advanced Threat Prevention to Distributed Firewall</span></p><p><span>VMware NSX Advanced Threat Prevention to Gateway Firewall</span></p><p><span>VMware NSX Advanced Load Balancer (excluding Subscription, SaaS)</span></p><p><span>VMware Container Networking Enterprise with Antrea</span></p><p><span>VMware HCX</span></p><p><span>VMware HCX+</span></p></div>
        </div><p>Broadcom said in its short note: “In the future, at the time of renewal, customers will be offered the best subscription products to fit their needs.”&nbsp;</p><p>Very clear.&nbsp;</p><p>Broadcom’s clearout of the VMware portfolio is having profound ripple effects across the market, analysts at investment bank William Blair said.</p><p>Market disruption “is turning out to be more significant than we expected” wrote analyst Jason Adar in a note to clients that suggested Nutanix would be the big winner: “The [reseller] community sees a multiyear runway for the Nutanix Cloud Platform and Nutanix AHV (Acropolis Hypervisor) to take market share from VMware (from vSAN and ESX, respectively)."</p><p>“[VMware] customers are already starting to look at alternatives and planning away from their historical dependence on VMware’s dominant ESX hypervisor, with Cisco’s Nutanix channel activity "skyrocketing" in the wake of a <a href="https://newsroom.cisco.com/c/r/newsroom/en/us/a/y2023/m08/cisco-and-nutanix-announce-strategic-partnership.html?ref=thestack.technology"><u>recent partnership</u></a><u>, he added.</u></p><p>Broadcom is meanwhile looking to <a href="https://blogs.vmware.com/euc/2023/12/an-exciting-new-era-for-end-user-computing.html?ref=thestack.technology" rel="noreferrer">offload</a> VMware's "end-user" computing unit for a reported $5 billion. That includes its Workspace ONE and Horizon offerings; respectively an access control, application management, and unified endpoint management platform; and a set of virtual desktop products, with several private equity firms <a href="https://www.bloomberg.com/news/articles/2024-01-12/eqt-kkr-are-said-among-suitors-for-5-billion-broadcom-asset?ref=thestack.technology" rel="noreferrer">reported</a> by Bloomberg to be interested. </p><p>Customers, as <a href="https://thestack.technology/?ref=thestack.technology" rel="noreferrer"><em>The Stack</em></a> has earlier noted, have other options, but also a whole lot of sunk costs into VMware expertise...</p><p><em>How does this move affect you? Want to share views. </em><a href="mailto:ed@thestack.technology" rel="noreferrer"><em>Get in touch. </em></a></p><h2 id="join-peers-following-the-stack-on-linkedin">Join peers following <a href="https://www.linkedin.com/company/stackpublishing?ref=thestack.technology" rel="noreferrer">The Stack on LinkedIn</a></h2>
    </div>


      </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Relearning math as an adult (219 pts)]]></title>
            <link>https://gmays.com/how-im-relearning-math-as-an-adult/</link>
            <guid>39047825</guid>
            <pubDate>Thu, 18 Jan 2024 21:12:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gmays.com/how-im-relearning-math-as-an-adult/">https://gmays.com/how-im-relearning-math-as-an-adult/</a>, See on <a href="https://news.ycombinator.com/item?id=39047825">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>I recently passed 100 days of practicing math every single day 💯</p>



<p>I’ve wanted to beef up my math chops for a while, but I needed a good reason that would justify the time investment. Plus, it’s always easier to learn when you have a clear goal and something meaningful to apply it to.</p>



<p>So, it never reached the top of my priority list. But then a couple things happened recently that gave me 1) sufficient motivation and 2) a clear path.</p>



<h2>The motivation</h2>



<p>I’ve worked on various AI products over the last year and like understanding the technical aspects of the products I build.</p>



<p>But as I dug in to learn more about how large language models (LLMs) and transformers worked…I was lost. It was humbling.</p>



<figure><img fetchpriority="high" decoding="async" width="932" height="1024" src="https://gmays.com/wp-content/uploads/2023/11/llm-math-932x1024.png" alt="" srcset="https://gmays.com/wp-content/uploads/2023/11/llm-math-932x1024.png 932w, https://gmays.com/wp-content/uploads/2023/11/llm-math-273x300.png 273w, https://gmays.com/wp-content/uploads/2023/11/llm-math-768x844.png 768w, https://gmays.com/wp-content/uploads/2023/11/llm-math.png 1212w" sizes="(max-width: 932px) 100vw, 932px"><figcaption>From Spencer Becker-Kahn’s <a href="https://www.lesswrong.com/posts/rFhxbWCdECxuT9xa2/notes-on-the-mathematics-of-llm-architectures" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">Notes on the Mathematics of LLM Architectures</a>.</figcaption></figure>



<p>I’d need a <em>much</em> better understanding of stats/probability, linear algebra, and calculus (<a href="https://mathacademy.com/courses/mathematics-for-machine-learning" target="_blank" rel="noreferrer noopener external" data-wpel-link="external">overview</a>).</p>



<p>Aside from corporate projections/financials, I haven’t done much with math since grad school, so I lacked a foundation. I don’t need to become a mathematician, but at a minimum, I wanted more intuition around the core concepts and connections between them.</p>



<h2>The path</h2>



<p>So, how does an adult with a job, kids, a mortgage, and <a href="https://gmays.com/recession-opportunities/" data-type="post" data-id="478" data-wpel-link="internal">other hobbies</a>, learn math in an effective, time-efficient way?</p>



<p>Fortunately, a friend I met a decade ago at MicroConf,&nbsp;<a href="https://www.codusoperandi.com/" target="_blank" rel="noreferrer noopener external" data-wpel-link="external">Jason Roberts</a>, founded a startup called&nbsp;<a href="https://mathacademy.com/" target="_blank" rel="noreferrer noopener external" data-wpel-link="external">Math Academy</a>&nbsp;with his wife, Sandy. I kept up with it since the early days (they have a&nbsp;<a href="https://mathacademy.com/about-us" target="_blank" rel="noreferrer noopener external" data-wpel-link="external">great story</a>), and now I finally had a reason to try it out.</p>



<p>It was still in beta, so I reached out, bought a license, and was in!</p>



<h3>Math Academy</h3>



<p>How would I describe Math Academy?</p>



<p>One word:&nbsp;<strong>Amazing.</strong></p>



<p>I’ve been consistently impressed since I started using it. Despite still being in beta, it feels quite polished and works well (though it’s clear an engineer designed the UI 😂).</p>



<p>Its current focus is helping kids learn math, with tools built-in for parents to help their kids be successful, but it’s quite effective for adult learners like me as well.</p>



<p>Their current courses cover math from middle school through university (graduate level) and are fully accredited. Below are the current courses.</p>



<figure><img decoding="async" width="1024" height="778" src="https://gmays.com/wp-content/uploads/2023/11/math-academy-1024x778.png" alt="" srcset="https://gmays.com/wp-content/uploads/2023/11/math-academy-1024x778.png 1024w, https://gmays.com/wp-content/uploads/2023/11/math-academy-300x228.png 300w, https://gmays.com/wp-content/uploads/2023/11/math-academy-768x583.png 768w, https://gmays.com/wp-content/uploads/2023/11/math-academy.png 1406w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Math Acedmy’s course offerings as of Nov. 2023.</figcaption></figure>



<p>The ‘<strong>Foundation Series</strong>‘ is what I’m starting with. It’s for adults to help streamline learning (it skips the stuff that kids need, but adults don’t) and work back up through college-level math relatively quickly (emphasis on relatively 😂).</p>



<hr>



<p>One of the other reasons I love Math Academy is the&nbsp;<strong>authenticity</strong> of the people in addition to the process. The <a href="https://mathacademy.com/about-us" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">founder</a> and <a href="https://www.justinmath.com/" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">others</a> are true math nerds, and they’ve spent thousands of hours teaching math.&nbsp;<strong>They know what works.</strong></p>



<figure><img decoding="async" width="1024" height="684" src="https://gmays.com/wp-content/uploads/2023/11/math-academy-jason-1024x684.jpeg" alt="" srcset="https://gmays.com/wp-content/uploads/2023/11/math-academy-jason-1024x684.jpeg 1024w, https://gmays.com/wp-content/uploads/2023/11/math-academy-jason-300x200.jpeg 300w, https://gmays.com/wp-content/uploads/2023/11/math-academy-jason-768x513.jpeg 768w, https://gmays.com/wp-content/uploads/2023/11/math-academy-jason.jpeg 1470w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Jason, founder of Math Academy, teaching kids math (including his own kid!).</figcaption></figure>



<p>This authenticity translates to authentic learning, which comes through from the very beginning. After signing up, it starts off with a diagnostic that <s>shows you how much of a dumbass you are</s> helps assess your current math proficiency so you can start off at the right level.</p>



<p>Then the self-paced, interactive lessons start. One thing I especially love as a busy adult student is that most lessons are ~10 minutes long, which makes it relatively easy to squeeze in sessions (e.g., between kids screaming or while they’re distracted watching Bluey 😂).</p>



<p>I also love how it teaches, assesses, reviews, and builds on concepts. When it comes quiz time I’m often impressed with how much I’ve learned and the ability to apply it. This thing is legit—it really works.</p>



<p>And there are no fluffy videos that just ‘feel’ like learning. The lessons have explanations between problems to make it a very concrete, hands-on process. It’s a highly effective way of learning.</p>



<figure><img loading="lazy" decoding="async" width="1024" height="521" src="https://gmays.com/wp-content/uploads/2023/11/math-academy-learning-system-1024x521.png" alt="" srcset="https://gmays.com/wp-content/uploads/2023/11/math-academy-learning-system-1024x521.png 1024w, https://gmays.com/wp-content/uploads/2023/11/math-academy-learning-system-300x153.png 300w, https://gmays.com/wp-content/uploads/2023/11/math-academy-learning-system-768x390.png 768w, https://gmays.com/wp-content/uploads/2023/11/math-academy-learning-system-1536x781.png 1536w, https://gmays.com/wp-content/uploads/2023/11/math-academy-learning-system.png 2010w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>…which gets to something important:&nbsp;<strong>Math is hard.</strong></p>



<p>Math Academy makes learning math as easy as possible, but it’s still hard work. Really hard work.</p>



<h2>Math is hard</h2>



<p>Learning math is challenging. It’s real work. This isn’t some casual app you can just flip through.</p>



<p>Unless you’re like John von Neumann, most of it requires pencil and paper to knock out. The only exception is the early lessons when you’re just refreshing and can do most of it in your head.</p>



<p>But if you’re serious about legitimately learning math, there is no better, more convenient way.</p>



<h2>Setting myself up for success</h2>



<p>To make this a habit I’d stick to, I’d have to lower the activation energy of doing it and make it as close to a new ‘default’ behavior as possible. <em>“This is just what I do.”</em></p>



<p>So, I started with convenience and consistency, which helped me form other good habits as well.</p>



<h3>Convenience</h3>



<p>Learning math is hard enough, so how can we make it somewhat convenient?</p>



<p>Math Academy’s short (~10 min) lessons help. I also use an iPad for the lessons (it’s web-based) when I’m not at a computer and a&nbsp;<a href="https://remarkable.com/store/remarkable-2" target="_blank" rel="noreferrer noopener external" data-wpel-link="external">Remarkable 2</a>&nbsp;for the work (e-ink for pencil/paper). I also found a&nbsp;<a href="https://www.amazon.com/gp/product/B0C4XG3HHN/" target="_blank" rel="noreferrer noopener external" data-wpel-link="external">cheap case</a>&nbsp;that fits both perfectly.</p>



<p>This makes it easy to knock out lessons here or there. For example, I had jury duty recently and knocked out a few lessons while waiting.</p>



<p>And just this past Wednesday, the kids were ready early, so I got to the gym about 15 minutes early and knocked out a lesson or two in the car. I’ve also started doing them during a light walk on the treadmill (I have one under my standing desk).</p>



<p>My usual routine is to do lessons in the evening after the kids go down and after I work out. It is challenging to muster the energy to work out <em>and</em> do math after a long day, but making it a habit has made it easier.</p>



<h3>Consistency</h3>



<p>I’ll say it again:&nbsp;<strong>Math is hard.</strong>&nbsp;My goal is to work up to the math needed to better understand LLMs and transformers.</p>



<p>Thankfully, Math Academy has a ‘<a href="https://mathacademy.com/courses/mathematics-for-machine-learning" target="_blank" rel="noreferrer noopener external" data-wpel-link="external">Mathematics for Machine Learning</a>‘ course I’m working towards, which gives me a clear goal. Given my limited time, it will likely take years for me to get there.</p>



<p>Why years? Two reasons:</p>



<ol>
<li>Math is <strong>hard</strong>.</li>



<li>Math is <strong>BIG</strong>.</li>
</ol>



<p>I had no idea how broad and deep math was. I probably still don’t.</p>



<p>Case in point:</p>



<figure><img loading="lazy" decoding="async" width="1024" height="486" src="https://gmays.com/wp-content/uploads/2023/11/math-academy-knowledge-graph-1024x486.png" alt="" srcset="https://gmays.com/wp-content/uploads/2023/11/math-academy-knowledge-graph-1024x486.png 1024w, https://gmays.com/wp-content/uploads/2023/11/math-academy-knowledge-graph-300x142.png 300w, https://gmays.com/wp-content/uploads/2023/11/math-academy-knowledge-graph-768x364.png 768w, https://gmays.com/wp-content/uploads/2023/11/math-academy-knowledge-graph-1536x729.png 1536w, https://gmays.com/wp-content/uploads/2023/11/math-academy-knowledge-graph-2048x971.png 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>This is the knowledge graph of the concepts in the first Mathematical Foundations course, showing the topics, how they relate, and what gaps you have.</p>



<p>Impressive, right? And this is&nbsp;<strong>just the first course</strong>&nbsp;(!) of the ~10 or so I want to take. It’s going to take years.</p>



<p>So, how do I get there? One lesson at a time. If I keep at it, I will get there.</p>



<p>I’ve been doing at least 1 lesson a day, every day since I started, giving me a 106-day streak as of today.</p>



<figure><img loading="lazy" decoding="async" width="1036" height="742" src="https://gmays.com/wp-content/uploads/2024/01/math-academy-100.png" alt="" srcset="https://gmays.com/wp-content/uploads/2024/01/math-academy-100.png 1036w, https://gmays.com/wp-content/uploads/2024/01/math-academy-100-300x215.png 300w, https://gmays.com/wp-content/uploads/2024/01/math-academy-100-1024x733.png 1024w, https://gmays.com/wp-content/uploads/2024/01/math-academy-100-768x550.png 768w" sizes="(max-width: 1036px) 100vw, 1036px"><figcaption>Screenshot of my Math Academy streak from <a href="https://habitsgarden.com/join/gabe" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">Habits Garden</a>, where I track my habits.</figcaption></figure>



<p>106 days down, ~1,000 more to go 😂</p>



<p>It’s a hard, but worthwhile and enjoyable new hobby for now.</p>



<h2>Takeway</h2>



<p>For hard subjects like math, I always assumed the only ‘real’ way to learn was in dedicated environments like college since online courses were best for softer, less technical topics. This was a bummer, especially for adult students like me.</p>



<p>But Math Academy changed my mind on this, and I’m a huge fan so far. I hope they continue to grow and expand to other complex STEM topics like computer science, physics, etc.</p>



<p>Overall, I’m just so excited there’s an effective, accessible way to learn hard things. If I were a billionaire, I’d invest to accelerate its progress and find a way to sponsor a license for every kid who was interested…</p>



<p>But for now, I’ll just talk about it to anyone who will listen 😉</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Noisy brain may underlie some of autism's sensory features (207 pts)]]></title>
            <link>https://www.thetransmitter.org/spectrum/noisy-brain-may-underlie-some-of-autisms-sensory-features/</link>
            <guid>39047541</guid>
            <pubDate>Thu, 18 Jan 2024 20:53:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.thetransmitter.org/spectrum/noisy-brain-may-underlie-some-of-autisms-sensory-features/">https://www.thetransmitter.org/spectrum/noisy-brain-may-underlie-some-of-autisms-sensory-features/</a>, See on <a href="https://news.ycombinator.com/item?id=39047541">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
        <div>
            <p>Sensory issues associated with autism may be caused by fluctuating neuronal noise — the background hum of electrical activity in the brain — according to a new mouse <a href="https://doi.org/10.1038/s41467-023-43777-z" target="_blank" rel="noopener">study</a>.</p>
<p>Up to 90 percent of autistic people report sensory problems, including heightened sensitivity to sounds or an aversion to certain smells. Yet others barely register sensory cues and may seek out sensations by making loud noises or rocking back and forth.</p>
<p>But thinking in terms of hyper- or hyposensitivity may be an oversimplification, says <a href="https://www.bordeaux-neurocampus.fr/en/staff/andreas-frick/" target="_blank" rel="noopener">Andreas Frick</a>, lead investigator and research director at INSERM. “It’s becoming clear now that things are a lot more nuanced.”</p>
<p>For instance, the brain’s response to visual patterns — measured using electroencephalography (EEG) recordings — <a href="http://www.doi.org/10.3389/fpsyg.2011.00051" target="_blank" rel="noopener">varies more between viewings</a> in autistic people than in those without the condition, one study found. And <a href="https://www.spectrumnews.org/news/noisy-brain-signals-could-underlie-autism-study-says/" target="_blank" rel="noopener">functional MRI has detected</a> similar variability among autistic people, suggesting sensory problems may arise from inconsistent brain responses.</p>
<p>In the new study, Frick and his colleagues found variability in the activity of individual neurons in a mouse model of <a href="https://www.spectrumnews.org/news/fragile-x-syndromes-link-autism-explained/" target="_blank" rel="noopener">fragile X syndrome</a>, one of the leading causes of autism. That variability in neuronal response maps to fluctuations in the levels of noise in the brain, the study found.</p>
<p>Noise within the brain isn’t necessarily a bad thing. In fact, an optimum amount is ideal: a little can give neurons the ‘push’ they might need to fire an action potential, while too much can make it difficult for the brain to distinguish between different stimuli. But in animals modeling fragile X syndrome, noise fluctuates such that they process sensory information less reliably, Frick says.</p>
<p>“It’s an exciting and ambitious study,” says <a href="https://www.sheffield.ac.uk/psychology/people/academic/elizabeth-milne" target="_blank" rel="noopener">Elizabeth Milne</a>, professor of cognitive neuroscience at the University of Sheffield, who led the EEG study but was not involved in the new work. It allows scientists to move beyond speculation that neuronal noise drives the inconsistent brain responses seen in autistic people, she says.</p>
<p>
    F</p><p>
rick’s team worked with mice lacking FMR1, the gene responsible for fragile X syndrome. Using a technique called patch-clamp recording, they monitored the activity of individual neurons in the somatosensory cortex — the brain region that processes touch — as they repeatedly tapped the rodents’ paws.</p>
<p>In wildtype mice, neurons tended to respond in a similar way each time their paw was touched. But neurons from rodents lacking FMR1 showed greater variability in the size of the electrical signal, and how long it took to initiate a response, the study found.</p>
<p>“It’s nice that they use hindpaw stimulation as it adds a translational component” says <a href="https://profiles.ucr.edu/app/home/profile/anubhutg" target="_blank" rel="noopener">Anubhuti Goel</a>, assistant professor of psychology at the University of California, Riverside, who was not involved in the study. Past research has often opted for whisker stimulation, which is clearly not relevant to people, she says.</p>
<p>The scientists then measured how much the electrical activity along the neuron’s membrane changes when the cell is unstimulated. This “baseline membrane potential” varies twice as much in fragile X mice as it does in wildtype rodents. And the more neuronal noise fluctuates, the greater the variation in the brain cell’s response to touch, the study found. That diminished signal-to-noise ratio might be responsible for the neuron’s “jittery” response to sensory signals, Frick says.</p>


<p>The neural oscillations — known as brain waves — that contribute to specific brain states also vary more in mice missing FMR1 and correlate with variability in neuronal response, the researchers found. These results hint that disorderly brain waves — which have been <a href="https://www.spectrumnews.org/news/slow-disorderly-brain-waves-may-flag-autism-in-toddlers/" target="_blank" rel="noopener">linked to autism </a>&nbsp;— may contribute to sensory issues.</p>
<p>The team then treated the animals with a compound that activates a potassium ion channel in the neuronal membrane. The ion channel is <a href="http://www.doi.org/10.1016/j.neuron.2012.12.018" target="_blank" rel="noopener">regulated by FMRP</a> — the protein product of FMR1 — and may contribute to the hyperexcitability of neurons in mice lacking the protein, <a href="http://www.doi.org/10.1038/nn.3864" target="_blank" rel="noopener">previous work</a> has shown.</p>
<p>
    A</p><p>
ctivation of the ion channel stabilizes membrane voltage, causing neuronal activity in the mice missing FMR1 to somewhat resemble that seen in wildtype mice, the study found. Attempts to rescue sensory problems by “playing with the noise” in this way have not been done before, Frick says.</p>
<p>The findings were published 30 November in <em>Nature Communications</em>.</p>
<p>Because the mice were anesthetized, the researchers couldn’t tell whether any of the neuronal differences seen in fragile X mice lead to changes in behavior. But data from <a href="http://www.doi.org/10.1038/nn.3864" target="_blank" rel="noopener">previous work</a> — which measured the rodents’ reaction to a mild sound — revealed that animals missing FMR1 displayed more variability in their behavioral response than wildtype mice. The findings suggest that inconsistent neuronal activity may lead to more diverse behavior in autism, the researchers say.</p>
<p>Some researchers speculate that variability in sensory processing may drive the social problems seen in people with autism. “Imagine being at a party, talking to a friend while music is playing in the background,” Frick says. “You have to integrate the sound of their voice with the movement of their lips.” But if there’s variability in sensory processing, socializing will be more challenging, he says.</p>
<p>Although the theory is appealing, the researchers <em>The Transmitter</em> contacted were cautious about drawing a link without the evidence to back it up. “My view is that this link isn’t particularly well defined or evidenced in current literature,” Milne says.</p>
<p>Still, the new work goes a long way to explain the variability seen in EEG recordings of autistic people, she adds.</p>
<p>But other concerns remain. Recordings from individual neurons don’t reflect the variable brain responses seen in people, says <a href="https://www.ulster.ac.uk/staff/c-odonnell2" target="_blank" rel="noopener">Cian O’Donnell</a>, lecturer in data analytics at Ulster University, who was not involved in the work. “For the activity of a single neuron to be detectable at the scalp, it would need to be synchronized among many neurons. For me, that’s a missing link between the human and the animal data.”</p>
<p>Frick and his team plan to plug that gap by recording populations of neurons using two-photon imaging. They also aim to uncover whether variability in sensory processing affects social behavior in mice, which could support or debunk the connection to core autism traits.</p>
        </div>
    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cracking the code of Linear B (112 pts)]]></title>
            <link>https://antigonejournal.com/2024/01/decipherment-linear-b/</link>
            <guid>39047102</guid>
            <pubDate>Thu, 18 Jan 2024 20:20:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://antigonejournal.com/2024/01/decipherment-linear-b/">https://antigonejournal.com/2024/01/decipherment-linear-b/</a>, See on <a href="https://news.ycombinator.com/item?id=39047102">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-24922">
	
	
	<div>
		
<p><em><strong>Theodore Nash</strong></em></p>







<p>June 1, 1952, was Whitsunday, and provided the young Michael Ventris with a convenient break from his duties as an architect. At the end of the day he would write his 20<sup>th</sup> Work Note on Minoan Language Research, with the somewhat disbelieving title, “Are the Knossos and Pylos Tablets Written in Greek?” Responsibility was disclaimed: this was only “a frivolous digression”, that would “sooner or later come to an impasse, or dissipate itself in absurdities.” It became instead one of the great intellectual achievements of the 20<sup>th</sup> century.</p>


<div>
<figure data-coblocks-animation="fadeIn"><img fetchpriority="high" decoding="async" width="682" height="1024" src="https://antigonejournal.com/wp-content/uploads/2024/01/MV2-682x1024.png" alt="" srcset="https://antigonejournal.com/wp-content/uploads/2024/01/MV2-682x1024.png 682w, https://antigonejournal.com/wp-content/uploads/2024/01/MV2-200x300.png 200w, https://antigonejournal.com/wp-content/uploads/2024/01/MV2-700x1051.png 700w, https://antigonejournal.com/wp-content/uploads/2024/01/MV2-266x400.png 266w, https://antigonejournal.com/wp-content/uploads/2024/01/MV2-560x841.png 560w, https://antigonejournal.com/wp-content/uploads/2024/01/MV2.png 705w" sizes="(max-width: 682px) 100vw, 682px"><figcaption><em>Michael Ventris.</em></figcaption></figure></div>


<p><strong>Discovery and Background</strong></p>



<p>The first Linear B tablets had been discovered at Knossos on the island of Crete. Kephala Hill, as it was then known, was one of the few true ‘tells’ in the Greek world and preserved rich layers of inhabitation dating back to the younger stone age (7,000 BC). The archaeological significance of the site had been clear already in the late 19<sup>th</sup> century, its first excavator being the fatefully named Minos Kalokairinos. He found great storage jars, <em>pithoi</em>, from which the locals began to call the site <em>Ta Pitaria</em>. These had been placed in hallways of great stone blocks, many carved with curious but distinctive signs.</p>


<div>
<figure data-coblocks-animation="fadeIn"><img decoding="async" width="765" height="1024" src="https://antigonejournal.com/wp-content/uploads/2024/01/KnosMm-765x1024.png" alt="" srcset="https://antigonejournal.com/wp-content/uploads/2024/01/KnosMm-765x1024.png 765w, https://antigonejournal.com/wp-content/uploads/2024/01/KnosMm-224x300.png 224w, https://antigonejournal.com/wp-content/uploads/2024/01/KnosMm-768x1029.png 768w, https://antigonejournal.com/wp-content/uploads/2024/01/KnosMm-150x200.png?crop=1 150w, https://antigonejournal.com/wp-content/uploads/2024/01/KnosMm-700x938.png 700w, https://antigonejournal.com/wp-content/uploads/2024/01/KnosMm-299x400.png 299w, https://antigonejournal.com/wp-content/uploads/2024/01/KnosMm-560x750.png 560w, https://antigonejournal.com/wp-content/uploads/2024/01/KnosMm.png 828w" sizes="(max-width: 765px) 100vw, 765px"><figcaption><em>Mason’s marks in the shape of double axes at Knossos.</em></figcaption></figure></div>


<p>At a time when <a href="https://antigonejournal.com/2021/03/schliemanns-excavation-of-troy/">Heinrich Schliemann</a>’s excavations were still new, and no unified sense of a Greek Bronze Age had yet been articulated, the reaction to these finds was muted. But in the early 1890s, Arthur Evans, the director of Oxford’s Ashmolean Museum, became interested in the pictographic signs found carved into semi-precious stone seals that had started to appear in Greece. These were all said to come from Crete. So, in 1894, he visited the island, and was shown the site of <em>Ta Pitaria</em>; he became convinced that the signs carved on the walls might belong to an ancient and forgotten writing system related to that which he had found on the sealstones. A year later, on returning to Crete, he was also shown “a burnt clay slip… said to have been found on the site of Kephala, presenting some incised linear signs which seemed to belong to an advanced system of writing.” This, too, was either found or disturbed by Kalokairinos’ digging, and was the first Linear B tablet known to modern investigators.</p>



<p>In 1900, having purchased the site, Evans was able to begin his own excavations. On March 31, he found “a kind of baked clay bar, rather like a stone chisel in shape though broken at one end, with a script on it and what appear to be numerals… It at once recalled a clay tablet of unknown age that I had copied at [Heraklion] also found at Knossos.” In the following years of excavation he would find over 4,000 more, albeit many of them in fragmentary condition. He would also find evidence for two other writing systems: the pictographic Cretan Hieroglyphic that he had first identified on sealstones; and another, less polished linear script. The tablets bearing this script were often disorganised, lacking ruled lines and word dividers. Evans classified it as the Linear Script of Class A, against the more regular Linear Script of Class B: our Linear A and Linear B.<span onmouseover="footnote_tooltip_show('footnote_plugin_tooltip_text_24922_1_1')" onfocus="footnote_tooltip_show('footnote_plugin_tooltip_text_24922_1_1')" onmouseout="footnote_tooltip_hide('footnote_plugin_tooltip_text_24922_1_1')" onfocusout="footnote_tooltip_hide('footnote_plugin_tooltip_text_24922_1_1')"><a role="button" tabindex="0" onclick="footnote_moveToReference_24922_1('footnote_plugin_reference_24922_1_1');"><sup id="footnote_plugin_tooltip_24922_1_1">[1]</sup></a><span id="footnote_plugin_tooltip_text_24922_1_1" onmouseout="footnote_tooltip_hide('footnote_plugin_tooltip_text_24922_1_1')">All three scripts are related, though the corpus of Cretan Hieroglyphic (CH) is so small that very little can be said with certainty. In basic terms, Linear A has many signs that look like more schematised versions of CH signs, and it probably developed from that script, although both remained in contemporary use for a period. It is not possible to say whether they both record the same language, though it is almost certain that the language of Linear A, at least, was a non-Indo-European language. Linear B is not strictly a separate script from Linear A, but rather the result of orthographic and administrative reorganisation when it became necessary or desirable to write in Greek rather than the unknown ‘Minoan’ language.</span></span></p>


<div>
<figure data-coblocks-animation="fadeIn"><img decoding="async" width="1024" height="495" src="https://antigonejournal.com/wp-content/uploads/2024/01/knosLB-1024x495.png" alt="" srcset="https://antigonejournal.com/wp-content/uploads/2024/01/knosLB-1024x495.png 1024w, https://antigonejournal.com/wp-content/uploads/2024/01/knosLB-300x145.png 300w, https://antigonejournal.com/wp-content/uploads/2024/01/knosLB-768x371.png 768w, https://antigonejournal.com/wp-content/uploads/2024/01/knosLB-700x338.png 700w, https://antigonejournal.com/wp-content/uploads/2024/01/knosLB-828x400.png 828w, https://antigonejournal.com/wp-content/uploads/2024/01/knosLB-560x271.png 560w, https://antigonejournal.com/wp-content/uploads/2024/01/knosLB.png 1180w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption><em>Linear B tablets from Knossos recording corselets, chariots, and horses.</em></figcaption></figure></div>


<p>It is one thing to excavate material, but quite another to publish it. Though Evans did produce the monumental <em><a href="https://archive.org/details/palaceofminoscom01evanuoft/page/n13/mode/2up" target="_blank" rel="noreferrer noopener">Palace of Minos</a></em> (6 vols, 1921–35), this was more a synthesis of Minoan culture as he had come to understand it than a proper archaeological publication. When Evans died in 1941 (believing, if we can credit Maurice Bowra’s report, that Knossos had just been bombed by the Germans), the vast bulk of the tablets remained unpublished. So responsibility for this material passed to Sir John Myres, the recently retired Wykeham Professor of Ancient History at Oxford. Myres had worked with Evans on Crete as far back as the 1890s, but in his retirement lacked the vigour required for this difficult task. &nbsp;</p>



<p>That Evans had published so few of the tablets in his lifetime undoubtedly delayed the possibility of decipherment. When investigating an unknown script, the greater the quantity of evidence available, the greater the possibility that recurring patterns may become visible, and from these the underlying structures deduced. Alice Kober, a professor at Brooklyn College, embraced this challenge in spite of the limited material, managing to make observations that guided the way to a successful decipherment.</p>



<p>From 1943 to 1950, Kober published a series of articles in which she demonstrated that Linear B was used to spell an inflected language – that is, a language (like Latin and Greek) which changes the endings of words to express their grammatical function. Kober would eventually collaborate with Myres to get the Knossos tablets published, but died in 1950, aged only 43, too early to see flowers blossom in the garden that she had so painstakingly tended.</p>


<div>
<figure data-coblocks-animation="fadeIn"><img loading="lazy" decoding="async" width="820" height="1024" src="https://antigonejournal.com/wp-content/uploads/2024/01/Kober-820x1024.png" alt="" srcset="https://antigonejournal.com/wp-content/uploads/2024/01/Kober-820x1024.png 820w, https://antigonejournal.com/wp-content/uploads/2024/01/Kober-240x300.png 240w, https://antigonejournal.com/wp-content/uploads/2024/01/Kober-768x959.png 768w, https://antigonejournal.com/wp-content/uploads/2024/01/Kober-700x874.png 700w, https://antigonejournal.com/wp-content/uploads/2024/01/Kober-320x400.png 320w, https://antigonejournal.com/wp-content/uploads/2024/01/Kober-560x699.png 560w, https://antigonejournal.com/wp-content/uploads/2024/01/Kober.png 834w" sizes="(max-width: 820px) 100vw, 820px"><figcaption><em>Alice Kober in 1946, reproduced in Margalit Fox’s </em>Riddle of the Labyrinth<em>.</em></figcaption></figure></div>


<p>More significant even than the publication of the Knossos tablets was the beginning of excavation atop the Englianos ridge in Western Messenia in 1939. Here Carl Blegen, returning to Greece after his great excavations at Troy, would uncover the Palace of Nestor at the Homeric “sandy Pylos”. On the first day of excavation he uncovered the palace’s archive room, and in that year alone found some 600 tablets. He entrusted study and publication of these to one of his graduate students, Emmett Bennett, who, after the interruption of war, was able to complete from photographs a study of the Linear B signs in the Pylos tablets. This in 1947: in 1951 he added a full transcription of the same tablets, which would provide a major stimulus to Ventris.</p>



<p>Especially in recent years, which have seen a new celebration of Kober’s work, it is probably Bennett’s achievement which is the most overlooked in popular accounts. But it was he who established which variations were possible within an individual signs (as I vs I) and which truly separated two signs (as G vs C). Without this, of course, no attempt at decipherment could stand on steady feet.</p>



<p>It was against this background that a young English architect took an interest in the problem. When Michael Ventris was still a pupil at Stowe School he saw a display of Greek and Minoan art at Burlington House; and by the sort of accident that changes the path of one’s life, was given an impromptu tour by Sir Arthur Evans, who happened also to be visiting. After viewing some tablets, Ventris had to confirm something that he had heard: “Did you say the tablets haven’t been deciphered, Sir?”</p>


<div>
<figure data-coblocks-animation="fadeIn"><img loading="lazy" decoding="async" width="650" height="890" src="https://antigonejournal.com/wp-content/uploads/2024/01/MV14.png" alt="" srcset="https://antigonejournal.com/wp-content/uploads/2024/01/MV14.png 650w, https://antigonejournal.com/wp-content/uploads/2024/01/MV14-219x300.png 219w, https://antigonejournal.com/wp-content/uploads/2024/01/MV14-292x400.png 292w, https://antigonejournal.com/wp-content/uploads/2024/01/MV14-560x767.png 560w" sizes="(max-width: 650px) 100vw, 650px"><figcaption><em>Michael Ventris at 14, reproduced in Andrew Robinson’s </em>The Man Who Deciphered Linear B<em>.</em></figcaption></figure></div>


<p>The challenge was seductive. Over the next few years, he entered sporadic correspondence with Evans about the tablets, sometimes offering suggestions, sometimes apologising after changing his mind (“Actually I was only 15 at the time,” he wrote at 17, “and I am afraid that my theories were nonsense.”). But by 18 he had settled on the theory to which he would cling until proven wrong by a frivolous digression. This was that the language of the tablets should be Etruscan.</p>



<p>As M.G.F. Ventris of London he submitted a manuscript entitled “Introducing the Minoan Language” to the <em>American Journal of Archaeology</em>, which was accepted and published in December 1940. An offprint sent to his former Classics master at Stowe bore a rather sheepish note: “I thought you had better see this… I did not tell them my age.” Today, as with most pre-decipherment scholarship, it is read chiefly as an historical curiosity (“the fantasy with most followers appears to be that which makes Minoan [Linear B] out as Greek”), though his insistence that Linear B and the related Cypriot syllabary should share sign values would eventually prove significant.</p>



<p>During the War, Ventris served as a bomber navigator in the RAF, and the story is told (though it has rather an apocryphal air) that, once he had set a course for his pilot, he would clear some space in the plane to pore over Linear B texts. After the War his architectural studies took priority, and though invited he did not assist Myres and Kober with the Knossos tablets. In late 1949, he circulated a questionnaire to everyone he knew to be working on the problem, now remembered as the “Mid-Century Report”, but it was not until 1951 that Ventris returned seriously to work on Linear B.</p>



<p>The first of his work notes is dated 28 January 1951, and begins: “With the forthcoming publication of the Linear B inscriptions from Pylos, it’s likely that there will be an intensification of Minoan research work and the hope of some definite progress.” Writing at a pace of just over one note per month, he himself would have the answer in less than a year and a half.</p>


<div>
<figure data-coblocks-animation="fadeIn"><img loading="lazy" decoding="async" width="724" height="590" src="https://antigonejournal.com/wp-content/uploads/2024/01/HalWWII.png" alt="" srcset="https://antigonejournal.com/wp-content/uploads/2024/01/HalWWII.png 724w, https://antigonejournal.com/wp-content/uploads/2024/01/HalWWII-300x244.png 300w, https://antigonejournal.com/wp-content/uploads/2024/01/HalWWII-700x570.png 700w, https://antigonejournal.com/wp-content/uploads/2024/01/HalWWII-491x400.png 491w, https://antigonejournal.com/wp-content/uploads/2024/01/HalWWII-560x456.png 560w" sizes="(max-width: 724px) 100vw, 724px"><figcaption><em>a A Royal Air Force Handley Page Halifax Mark II Series I flying over the English countryside, 1942: Ventris served as aircrew on one of these with No. 76 Squadron RAF.</em></figcaption></figure></div>


<p><strong>Principles of Decipherment</strong></p>



<p>Mycenaean tablets are quite austere records, and even before decipherment their basic structure could be easily grasped. Words, conveniently divided, generally appear in groups of one or two. These are often accompanied by signs of obviously pictographic nature, such as a horse’s head or chariot. Often a numeral follows. The concerns are with who has how many horses, or where they’ve taken them, but rarely with just why they have them, or what they might be doing with them. There are few complete sentences, and very little syntax: information is conveyed with a minimum of fuss. In this respect they greatly resemble modern bathroom signs. Take the following:</p>


<p>men</p>
<p>men’s</p>
<p>MEN</p>
<p>MEN’S</p>
<p>M</p>


<p>The English-speaker in search of a toilet knows that these all mean the same thing. He knows that the minuscule m is the same as M, e the same as E, and n the same as N, so these are not different words; he knows that, while strictly the possessive is correct, “men” is quite the same as “men’s”; and he knows that, when faced with a single door at the back of a restaurant, M will certainly stand for men(’s), especially if next to another door with a W on it. But he knows this because he was taught his abc’s (and ABC’s), and has been using public toilets for most of his life. 5,000 years, to the alien archaeologists and cryptographers trying to make sense of our terrestrial ruins, this will not be quite so obvious.</p>



<p>But Linear B and toilet signs offer another aid: the use of ideograms. These single signs represent not a word in any language, but rather an idea, often by form of pictorial representation. We may find on our sign:</p>


<div>
<figure data-coblocks-animation="fadeIn"><img loading="lazy" decoding="async" width="591" height="372" src="https://antigonejournal.com/wp-content/uploads/2024/01/MenT.png" alt="" srcset="https://antigonejournal.com/wp-content/uploads/2024/01/MenT.png 591w, https://antigonejournal.com/wp-content/uploads/2024/01/MenT-300x189.png 300w, https://antigonejournal.com/wp-content/uploads/2024/01/MenT-560x352.png 560w" sizes="(max-width: 591px) 100vw, 591px"></figure></div>


<p>Because in each case the word is accompanied by the same ideogram, our alien friends may develop a certain confidence that “men” is quite the same as “MEN”, and a suspicion that grammatical markers such as the possessive don’t mean all that much. Assuming that they find enough toilet signs, the aliens will be able to establish a relationship between the word “men” and the stick figure, even if they do not yet know the equivalent in their own language. They will also know that M/m, E/e, and N/n are the same letters in other contexts, and that a final -’s can be a grammatical marker. And, though no doubt quite different from their tentacled bodies, once they have dug up a grave or two, or seen any art that has survived the conflagrating years, they might even realise that this symbol represents a human body.</p>



<p>They may even be lucky enough to find this corresponding set of signs:</p>


<div>
<figure data-coblocks-animation="fadeIn"><img loading="lazy" decoding="async" width="519" height="359" src="https://antigonejournal.com/wp-content/uploads/2024/01/WomenT.png" alt="" srcset="https://antigonejournal.com/wp-content/uploads/2024/01/WomenT.png 519w, https://antigonejournal.com/wp-content/uploads/2024/01/WomenT-300x208.png 300w" sizes="(max-width: 519px) 100vw, 519px"></figure></div>


<p>As these look much the same, in format, as signs for the men’s room, our aliens might reasonably guess that they both serve the same purpose. If they have further discerned anything about how people dressed in the 20<sup>th</sup> and 21<sup>st</sup> centuries, they might even recognise that these stereotyped images appear to be wearing a dress or skirt, conventionally associated with women and girls. Even if they can’t read it, they can tentatively suggest that MEN is the word for “man” and WOMEN the word for “women”, in this system. But already we see how tricky this is: they will have no inkling that both are plural.</p>



<p>Let us return to earth for a moment. Because Linear B is a syllabary, and not an alphabet, there are other forms of internal evidence available. This process can be illustrated in English by a word we are still inclined to decline for number and gender:</p>


<p>alumnus (masculine, singular)</p>
<p>alumni (masculine, plural)</p>
<p>alumna (feminine, singular)</p>
<p>alumnae (feminine, plural)</p>


<p>If we break this word down into syllables, we get:</p>


<p>a-lum-nus</p>
<p>a-lum-ni</p>
<p>a-lum-na</p>
<p>a-lum-nae</p>


<p>If we assign arbitrary Greek letters to each syllable, for the sake of illustration, we get:</p>


<p>α λ μ</p>
<p>α λ ν</p>
<p>α λ ξ</p>
<p>α λ π</p>


<p>In each case only the final sign of the word has changed, so we might perhaps think that, rather than representing completely different words, these are forms of the same word, with the ending changing to tell us something about it. This is how nouns and adjectives work in inflected languages, like Latin and Greek, and is a feature that we generally replicate for loanwords. But even though each sign stands for a complete syllable, a consonant plus a vowel, our knowledge of inflected languages will suggest to us that what is changing is not both sounds, but only the vowel: the consonant remains consistent. Each of these signs, then, belongs to the same consonant series. Even though we do not know which consonant it is, nor which vowel in each case, we can surmise that μ, ν, ξ, and π are all related. We can visualize this relationship in a grid, as so:</p>



<figure><table><tbody><tr><td>&nbsp;</td><td>Vowel 1 (u)</td><td>Vowel 2 (i)</td><td>Vowel 3 (a)</td><td>Vowel 4 (ae)</td></tr><tr><td>Consonant 1 (n)</td><td>μ (nus)</td><td>ν (ni)</td><td>ξ (na)</td><td>π (nae)</td></tr></tbody></table></figure>



<p>This is not yet much to go by: but, like the aliens puzzling out toilet signs, we have some other tricks to hand. Mycenaean records are chiefly accounting documents, and rich in numbers. These are quite obvious, being expressed by a tally system not unlike what we use today. So we might find:</p>


<p>α λ μ | (a-lum-nus 1)</p>
<p>α λ ν || (a-lum-ni 2)</p>
<p>α λ ξ | (a-lum-na 1)</p>
<p>α λ π ||| (a-lum-nae 3)</p>


<p>Now we know that both -μ and -ξ represent the singular, and so should belong to different genders, while -ν and -π represent plurals. Here is good evidence that we do indeed have an inflected language. But which plural goes with which singular? Luckily, we find:</p>


<div>
<figure data-coblocks-animation="fadeIn"><img loading="lazy" decoding="async" width="512" height="298" src="https://antigonejournal.com/wp-content/uploads/2024/01/LBT.png" alt="" srcset="https://antigonejournal.com/wp-content/uploads/2024/01/LBT.png 512w, https://antigonejournal.com/wp-content/uploads/2024/01/LBT-300x175.png 300w" sizes="(max-width: 512px) 100vw, 512px"></figure></div>


<p>Following the aliens, if we guess that the first stick figure represents a man and the second a women, it seems that -μ and -ν belong to a masculine declension, -ξ and -π to a feminine. We can even build a basic declension table:</p>



<figure><table><tbody><tr><td>&nbsp;</td><td>Singular</td><td>Plural</td></tr><tr><td>Masculine</td><td>&nbsp;– μ</td><td>&nbsp;– ν&nbsp;</td></tr><tr><td>Feminine</td><td>&nbsp;– ξ</td><td>&nbsp;– π&nbsp;</td></tr></tbody></table></figure>



<p>All of this without any idea of what the word means, or how to read it phonetically. Let us take another Latin loanword that we still treat right:</p>


<p>emeritus (masculine, singular)</p>
<p>emeriti (masculine, plural)</p>
<p>emerita (feminine, singular)</p>
<p>emeritae (feminine, plural)</p>


<p>Syllabified, with Greek letters again used arbitrarily:</p>


<p>e-mer-i-tus&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ε μ ι τ</p>
<p>e-mer-i-ti&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ε μ ι υ</p>
<p>e-mer-i-ta&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ε μ ι φ</p>
<p>e-mer-i-tae&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ε μ ι χ</p>


<p>And, say we find:</p>


<div>
<figure data-coblocks-animation="fadeIn"><img loading="lazy" decoding="async" width="530" height="295" src="https://antigonejournal.com/wp-content/uploads/2024/01/LBT2.png" alt="" srcset="https://antigonejournal.com/wp-content/uploads/2024/01/LBT2.png 530w, https://antigonejournal.com/wp-content/uploads/2024/01/LBT2-300x167.png 300w" sizes="(max-width: 530px) 100vw, 530px"></figure></div>


<p>Now we have the same relationship between -τ and -υ (-tus and -ti), -φ and -χ (-ta and -tae) as we did between -μ and -ν (-nus and -ni), -ξ and -π (-ta and -tae). So our grid expands:</p>



<figure><table><tbody><tr><td>&nbsp;</td><td>Vowel 1 (u)</td><td>Vowel 2 (i)</td><td>Vowel 3 (a)</td><td>Vowel 4 (ae)</td></tr><tr><td>Consonant 1 (n)</td><td>μ (nus)</td><td>ν (ni)</td><td>ξ (na)</td><td>π (nae)</td></tr><tr><td>Consonant 2 (t)</td><td>τ (tus)</td><td>υ (ti)</td><td>φ (ta)</td><td>χ (tae)</td></tr></tbody></table></figure>



<p>We are, of course, playing with the benefit of hindsight. Our examples are perfectly chosen, and we know that we are right. It was rather harder the first time round. Two words, even with examples as tidy as we have here, are no basis for a true decipherment: we must be alert to coincidence, and never shape the evidence to fit our theories. Our work so far has resembled Alice Kober’s in her demonstration that declensional patterns repeated across words and tablets, and were not phantoms but proper sport for the hunt. Tedious it was, surely, but not entirely mechanical: it took a keen eye for patterns, and a mind like a steel trap.</p>



<p>Only once this path had been laid could Ventris establish his grid. It relied for its accuracy on the principle of inflexion; if this was wrong, the grid would fail. But if it was right, here was a sound method. Without ever guessing what these words should mean, or what they might sound like, relationships could be established between both words and signs. It is not glamorous, nor easy, and would fail if pushed too quickly. But once built, like Cinderella’s shoe, it would only fit the one true solution.</p>



<p>This was the drudgery of Bennett, Kober, and Ventris. Bennett established the sign list; Kober the evidence for declension; and Ventris the grid. All that remained were the sounds, and the language itself.</p>


<div>
<figure data-coblocks-animation="fadeIn"><img loading="lazy" decoding="async" width="1024" height="617" src="https://antigonejournal.com/wp-content/uploads/2024/01/NestPal-1024x617.png" alt="" srcset="https://antigonejournal.com/wp-content/uploads/2024/01/NestPal-1024x617.png 1024w, https://antigonejournal.com/wp-content/uploads/2024/01/NestPal-300x181.png 300w, https://antigonejournal.com/wp-content/uploads/2024/01/NestPal-768x463.png 768w, https://antigonejournal.com/wp-content/uploads/2024/01/NestPal-700x422.png 700w, https://antigonejournal.com/wp-content/uploads/2024/01/NestPal-664x400.png 664w, https://antigonejournal.com/wp-content/uploads/2024/01/NestPal-560x337.png 560w, https://antigonejournal.com/wp-content/uploads/2024/01/NestPal.png 1200w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption><em>Undated photograph of the “Nestor’s Palace” archaeological site at Pylos: Carl Blegen began excavating here in 1939, only to be forced to abandon operations for a dozen years</em>.</figcaption></figure></div>


<p><strong>Breakthrough</strong></p>



<p>By February 1952, Ventris’ grid had reached its third stage, with advances made by incorporating the new evidence from Pylos. He had charted 48 signs, and of these only 7 were wrong: he had his shoe, but still needed a princess.</p>



<p>Much was made, and still is, of the analytical nature of Ventris’ decipherment. It was touted as a great defence against his critics: here was the solution, not of fanciful guesswork, but stubborn and methodical cryptography. As we have seen, there was indeed a great deal of method; but the neatly-placed tinder caught fire with an imaginative spark.</p>



<p>In a CV syllabary, there is one type of sign that will almost always appear at the beginning of a word: the pure vowel. As in e-me-ri-tus and a-lum-nus, it is only at a word’s beginning that we are likely to have a vowel unaccompanied by a consonant. And, indeed, there were some signs that rarely appeared later in a word than its beginning. One especially had attracted a great deal of interest, initially for its resemblance to the Minoan double axe, now for its location and frequency. It had been suggested, and Ventris decided to play with the idea, that it might represent <em>a</em>.</p>


<div>
<figure><img loading="lazy" decoding="async" width="242" height="112" src="https://antigonejournal.com/wp-content/uploads/2024/01/LBaxe-1.png" alt=""></figure></div>


<p>He was also toying with a few identifications based on similarities between Linear B and the later Cypriot syllabary. Though it seemed that these two systems were related, there was very limited overlap in signs, and certain aspects of the syllabary’s structure disguised that both systems were used to write the same language. But Ventris got lucky:</p>


<div>
<figure data-coblocks-animation="fadeIn"><img loading="lazy" decoding="async" width="490" height="190" src="https://antigonejournal.com/wp-content/uploads/2024/01/LB11.png" alt="" srcset="https://antigonejournal.com/wp-content/uploads/2024/01/LB11.png 490w, https://antigonejournal.com/wp-content/uploads/2024/01/LB11-300x116.png 300w" sizes="(max-width: 490px) 100vw, 490px"><figcaption><em>Hypothetical equivalences between Cypriot Syllabic (left) and Linear B (right) signs.</em></figcaption></figure></div>


<p>Adding to his confidence in the identification of <em>na</em> was that <em>-n-</em> was used in Etruscan adjectives and verbs; but the grid was foolproof, and if a guess was right it did not really matter if the reasoning was wrong. This was a point that many of his critics failed to grasp.</p>



<p>Because he had now placed the consonant <em>n</em> and the vowel <em>i, </em>his grid gave him the sign likely to represent <em>ni</em>:</p>


<div>
<figure data-coblocks-animation="fadeIn"><img loading="lazy" decoding="async" width="251" height="101" src="https://antigonejournal.com/wp-content/uploads/2024/01/LB4.png" alt=""></figure></div>


<p>But how to test the theory? The cryptographer’s greatest ally is the proper noun, which tends to be spelled out phonetically in more or less the same way no matter which writing system is in use. Even if one could not read Greek, they could have a crack at the word Ἀλέξανδρος if told that it is equivalent to Alexander: λ = l, ε = e, and so on. Difficulties appear at the end, but eight of ten letters can still be established with reasonable confidence. It was indeed the identification of the name Cleopatra in Egyptian hieroglyphs that was Champollion’s essential clue.</p>



<p>Now Ventris had no idea what anyone in the tablets may have been named; but even better than personal names are place names, which exhibit the same features and are often remarkably tenacious. Just northeast of Knossos is Amnisos, where Odysseus claims the winds took him while passing Cape Malea. The <em>Odyssey</em>-Poet places a cave of Eileithyia there, and in the 1930s Spyridon Marinatos had excavated Minoan remains at the site and identified it as the port of Knossos. This name seemed more likely than most to appear in the Knossos tablets.</p>


<div>
<figure data-coblocks-animation="fadeIn"><img loading="lazy" decoding="async" width="768" height="945" src="https://antigonejournal.com/wp-content/uploads/2024/01/AmnVL.png" alt="" srcset="https://antigonejournal.com/wp-content/uploads/2024/01/AmnVL.png 768w, https://antigonejournal.com/wp-content/uploads/2024/01/AmnVL-244x300.png 244w, https://antigonejournal.com/wp-content/uploads/2024/01/AmnVL-700x861.png 700w, https://antigonejournal.com/wp-content/uploads/2024/01/AmnVL-325x400.png 325w, https://antigonejournal.com/wp-content/uploads/2024/01/AmnVL-560x689.png 560w" sizes="(max-width: 768px) 100vw, 768px"><figcaption><em>A fresco uncovered at the ‘Villa of the Lilies’ at Amnisos</em>.</figcaption></figure></div>


<p>Ventris had already suggested that certain words may be place names: they were repeated frequently, in association with various commodities, but were not personal names, and did not appear at Pylos. He had floated the suggestion to John Myres in February, and could not shake it. In a letter to Emmett Bennett dated 26 April 1952, Ventris has put his foot on the right step, but not yet committed his weight:</p>





<p><span>I expect you’ve fully sorted these out already. It would be a wonderful thing if one could sit down on the hill at Knossos and know just what the names of all the surrounding towns &amp; villages were in L(ate) M(inoan); because I’m sure some of them must occur in this series. I’m still rather intrigued by AMNISOS for </span><span>a-mi-ni-so</span><span>, which is the only B group with initial </span><span>a</span><span>– and </span><span>-ni</span><span>– as 3rd except </span><span>a-pa</span><span>–</span><span>ni</span><span>-x — &amp; this name too should occur, surely. Amnios is generally spoken of as the port of Knossos, but I gather there was a nearer harbour at the mouth of the valley? The frequent </span><span>a-mi-ni-so-do</span><span> = </span><span>AT</span><span> AMNISOS? i.e. = at a separate royal depot?? Who are the </span><span>ki-ri-re-wi-ja-i</span><span> who are mentioned after </span><span>ko-no-so</span><span> (KNOSSOS?&gt;?) and who recut at Pylos? — </span><span>ciliθeviia — </span><span>“peasants, locals, ??” (Etc cilθ, “land, country”).</span></p>
<p><span>Yours, Michael Ventris<span onmouseover="footnote_tooltip_show('footnote_plugin_tooltip_text_24922_1_2')" onfocus="footnote_tooltip_show('footnote_plugin_tooltip_text_24922_1_2')" onmouseout="footnote_tooltip_hide('footnote_plugin_tooltip_text_24922_1_2')" onfocusout="footnote_tooltip_hide('footnote_plugin_tooltip_text_24922_1_2')"><a role="button" tabindex="0" onclick="footnote_moveToReference_24922_1('footnote_plugin_reference_24922_1_2');"><sup id="footnote_plugin_tooltip_24922_1_2">[2]</sup></a><span id="footnote_plugin_tooltip_text_24922_1_2" onmouseout="footnote_tooltip_hide('footnote_plugin_tooltip_text_24922_1_2')">He has in fact already come a long way here, as Etruscan <em>ciliθeviia </em>is a nearly-enough plausible reading of the sequence <em>ki-ri-te-wi-ja-i</em>.</span></span></span></p>



<p>If <em>a </em>and <em>n-</em> were right, then the top word could be read <em>a-x-n.-x</em>. If it were to be Amnisos, our <em>n- </em>sign must be <em>ni</em>, so <em>a-x-ni-x.</em> A glance at the grid would reveal that the second sign should have the same vowel as <em>ni. </em>In a CV syllabary, consonant clusters are always a problem, but they are sometimes resolved by ‘dead vowels’, often borrowed from a neighbouring sign. So the -mn- of Amnisos might be writing –<em>mi-ni-</em>. This was quite promising, and with the final sign provisionally read as <em>so</em>, he had his name.</p>



<p>From luck to rigour: by identifying <em>mi, ni, </em>and <em>so</em>, Ventris had locked in the consonant for the entire <em>m-</em>, <em>n-</em>, and <em>s-</em> series, and all signs with the vowels <em>-i </em>and <em>-o</em>. Any further hypotheses would have to respect these values: anything that required more than a minor adjustment here or there to account for human error would necessarily be wrong.</p>



<p>Another likely place name could now be half-read:</p>


<div>
<figure data-coblocks-animation="fadeIn"><img loading="lazy" decoding="async" width="291" height="186" src="https://antigonejournal.com/wp-content/uploads/2024/01/LB5.png" alt=""></figure></div>


<p>The final sign, as in <em>a-mi-ni-so, </em>must be <em>so</em>, and according to the grid each had the vowel <em>-o</em>. What’s more, the second sign belonged to the <em>n-</em>series. So this should be read: <em>.o-no.-so</em>. Given the principle of dead vowels already suggested, it is hard not to suggest <em>ko-no-so</em>: Knossos (Κνωσός). This involved guesswork, but always within the rules that Ventris had set for himself. It was never arbitrary, and if it required certain spelling rules (dead vowels, omission of final <em>-s</em>), then these were at least consistent.</p>



<p>So far, so good: but as we are merely dealing with place names, there is nothing here that is at all very Greek. But ominous clouds had appeared on the horizon. Certain words showed a variation at the end between the <em>-o</em> vowel and the –<em>a </em>vowel, which looked suspiciously like the Greek neuter declension (<em>-on</em> singular, <em>-a</em> plural; once again we must neglect the final consonant). And Kober’s triplets, too, began to look suspiciously like Greek adjectives:</p>


<div>
<figure data-coblocks-animation="fadeIn"><img loading="lazy" decoding="async" width="601" height="333" src="https://antigonejournal.com/wp-content/uploads/2024/01/LB12.png" alt="" srcset="https://antigonejournal.com/wp-content/uploads/2024/01/LB12.png 601w, https://antigonejournal.com/wp-content/uploads/2024/01/LB12-300x166.png 300w, https://antigonejournal.com/wp-content/uploads/2024/01/LB12-560x310.png 560w" sizes="(max-width: 601px) 100vw, 601px"></figure></div>


<p>These could now be read: <em>a-mi-ni-so, a-mi-ni-si-.o, a-mi-ni-si-.a</em>; <em>ko-no-so, ko-no-si-jo, ko-no-si-.a</em>. The consonant of the final sign was not obvious, but if it were <em>j</em>– (pronounced /y/, as in German) it might stand for a sort of glide between two vowels. This would give us adjectives of the type –<em>i(j)os, i(j)a</em>, perfectly formed Greek masculine and feminines (as in the word for “saint”, <em>ayios</em> vs <em>ayia</em>, in many modern Greek place names: Ayios Nikolaos, but Ayia Triada).</p>



<p>Now to search for Greek words. One of the basic items of Mycenaean vocabulary, established early by Kober on much the same principle as our aliens working on toilet signs, were the words for “boy” and “girl”:</p>


<div>
<figure data-coblocks-animation="fadeIn"><img loading="lazy" decoding="async" width="274" height="281" src="https://antigonejournal.com/wp-content/uploads/2024/01/LB7.png" alt="" srcset="https://antigonejournal.com/wp-content/uploads/2024/01/LB7.png 274w, https://antigonejournal.com/wp-content/uploads/2024/01/LB7-50x50.png?crop=1 50w" sizes="(max-width: 274px) 100vw, 274px"></figure></div>


<p>These could be read <em>ko-.o</em> and <em>ko-.a</em>. Again, a promising variation between –<em>o(s)</em> and <em>-a</em> to distinguish masculine and feminine. As anyone who has ever visited the museums of Athens knows, a sculpture of a youthful man is called a <em>kouros</em>, of a woman a <em>kore</em>. Ventris, who had no more Greek than he had left school with and who disclaimed any philological expertise, nonetheless knew that the differences in the first syllable were the result of a disappearing consonant: the words had originally been *<em>korwos</em> and *<em>korwa</em>. The spelling was difficult, but as the final signs shared the same consonant, it was quite possible that these words should be read <em>ko-wo, ko(r)wo(s)</em>, and <em>ko-wa</em>, <em>ko(r)wa</em>.</p>


<div>
<figure data-coblocks-animation="fadeIn"><img loading="lazy" decoding="async" width="894" height="587" src="https://antigonejournal.com/wp-content/uploads/2024/01/LBKor.png" alt="" srcset="https://antigonejournal.com/wp-content/uploads/2024/01/LBKor.png 894w, https://antigonejournal.com/wp-content/uploads/2024/01/LBKor-300x197.png 300w, https://antigonejournal.com/wp-content/uploads/2024/01/LBKor-768x504.png 768w, https://antigonejournal.com/wp-content/uploads/2024/01/LBKor-700x460.png 700w, https://antigonejournal.com/wp-content/uploads/2024/01/LBKor-609x400.png 609w, https://antigonejournal.com/wp-content/uploads/2024/01/LBKor-560x368.png 560w" sizes="(max-width: 894px) 100vw, 894px"><figcaption><em>The most famous kouros and kore pair in the world, possibly brother and sister, found together, in 1972 at the cemetery of Merenda (ancient Myrrhinous) in Attica. The Kore (dated 550–540 BC) is of Phrasikleia, and is signed by Aristion of Paros; the unnamed kouros is a little later (540–530 BC) (National Archaeological Museum, Athens, Greece).</em></figcaption></figure></div>


<p>A great deal of the grid had now fallen into place, and the more Ventris looked the more Greek he found. Having identified the sign ­<em>jo</em>, a number of men’s names that ended in <em>-jo-jo</em> could now be explained as genitives (Homeric <em>-οιο</em>, –<em>oio</em>), and the <em>w-</em>series gave the key to the vowel <em>u</em> and a complicated series of archaic nouns in –<em>eus</em>.</p>



<p>This is tough stuff, and gets very technical very fast. Given the holes that remained, it is unsurprising that Ventris had his hesitations. He had entered a great catacomb with only a small torch, and it was not obvious that his light would last long enough to see him out. But, most tellingly of all, one of his great reservations reflected an error not in his method but merely his philology. This was the sign that he now had to read as <em>qe</em>:</p>


<div>
<figure data-coblocks-animation="fadeIn"><img loading="lazy" decoding="async" width="202" height="161" src="https://antigonejournal.com/wp-content/uploads/2024/01/LB8.png" alt=""></figure></div>


<p>This sign regularly appears after the second of two items, often accompanied by the number two, and in that respect almost certainly reflects a conjunction. Greek had a particle which served this function, but this was <em>te</em> (τε), not <em>qe</em>. What Ventris did not know was that Greek had once had the letter <em>q</em> (k<sup>w</sup>), but in time it had become either <em>t</em> or <em>p</em> (compare the word for “four” in Latin and Greek: <em>quattuor</em> vs <em>tettara</em>; of for “five”: <em>quinque</em> vs <em>pente</em>). So <em>qe</em> was in fact the correct earlier form.</p>



<p>Such nagging doubts may have delayed the Work Note’s postage: in a letter to Bennett of 7 June, 1952, Ventris makes no mention of his breakthrough. It is not in fact clear when he realised that Mycenaean still preserved these labiovelars, though this discovery may be the subject of a famous anecdote. On an “evening early in June”, he and his wife Lois had invited over Michael Smith, a fellow architect, and his wife Prudence, who had just completed her degree in <em>Literae Humaniores</em> at Somerville College, Oxford. Lois was left to play the host, plying the guests with sherry, while Michael remained at work rather longer than the rules of good hosting would dictate. At last he emerged from his study, too excited properly to be bashful: “I know it, I <em>know</em> it, I am certain of it.” Over dinner he shared the topic of his certainty, and afterwards took the Classicist Prudence into his study to share his notes and findings.</p>


<div>
<figure data-coblocks-animation="fadeIn"><img loading="lazy" decoding="async" width="442" height="631" src="https://antigonejournal.com/wp-content/uploads/2024/01/MVHam.png" alt="" srcset="https://antigonejournal.com/wp-content/uploads/2024/01/MVHam.png 442w, https://antigonejournal.com/wp-content/uploads/2024/01/MVHam-210x300.png 210w, https://antigonejournal.com/wp-content/uploads/2024/01/MVHam-280x400.png 280w" sizes="(max-width: 442px) 100vw, 442px"><figcaption><em>The Hampstead (North London) home that Michael Ventris’ wife Lois designed (she was also an architect), where they lived from 1952.</em></figcaption></figure></div>


<p>It is not clear what, exactly, prompted his certainty on that evening. Nor even can we be sure quite what evening it was; but so long as “early June” is right, it must have been after Work Note 20 was completed. But by 18 June, when he finally shared the decipherment with Bennett, he had given 𐀤 the value <em>pe</em>, which represents a possible reflex of the labiovelar in later Greek. A possible explanation for one of his sticking points in Work Note 20 may well have been the tipping point.</p>



<p>Whatever his breakthrough, Prudence, who knew more Greek than Ventris, was quickly convinced by what she saw, and became an important proselytizer for the decipherment. After her degree she had taken a job with the BBC, and by her fervour convinced her rather sceptical colleagues that Ventris should announce his decipherment on the air. And so he did, on the first of July:</p>


<p>During the last few weeks, I have suddenly come to the conclusion that the Knossos and Pylos tablets must, after all, be written in Greek: a difficult and archaic Greek, seeing as it is 500 years earlier than Homer, and written in a rather abbreviated form; but Greek nonetheless.</p>


<p>This is not how most would have made the announcement. Here was a great claim, sure to be controversial, presented publicly without supporting evidence. Many listeners no doubt dismissed it. But at Oxford a young philologist named John Chadwick heard digammas where he expected them, and wanted a closer look.</p>


<div>
<figure data-coblocks-animation="fadeIn"><img loading="lazy" decoding="async" width="946" height="520" src="https://antigonejournal.com/wp-content/uploads/2024/01/MV3.png" alt="" srcset="https://antigonejournal.com/wp-content/uploads/2024/01/MV3.png 946w, https://antigonejournal.com/wp-content/uploads/2024/01/MV3-300x165.png 300w, https://antigonejournal.com/wp-content/uploads/2024/01/MV3-768x422.png 768w, https://antigonejournal.com/wp-content/uploads/2024/01/MV3-700x385.png 700w, https://antigonejournal.com/wp-content/uploads/2024/01/MV3-728x400.png 728w, https://antigonejournal.com/wp-content/uploads/2024/01/MV3-560x308.png 560w" sizes="(max-width: 946px) 100vw, 946px"><figcaption><em>A cheery Ventris: screenshot from the <a href="https://www.youtube.com/watch?v=pOOGJAQ4eg4">YouTu</a>be clip of his historic radio accouncement.</em></figcaption></figure></div>


<p><strong>A helping hand arrives</strong></p>



<p>Chadwick (1920–98), the son of a civil servant, had been educated at St Paul’s School, where he was a classmate of Kenneth Dover (1920–2010), before going up to Corpus Christi, Cambridge in 1939. War and the fall of France soon intervened, and he enlisted with the Royal Navy; but in 1942 he was swept away to Alexandria and secret intelligence work, where he did not limit his efforts to the codes he was supposed to break, rather to the embarrassment of some at Bletchley Park. After the Italian surrender he joined that team, where he had to learn much Japanese technical vocabulary relating to submarines. Through this process he learned a great deal about how codes are broken, and how a decipherment might grow from a few words to an entire message.</p>


<div>
<figure data-coblocks-animation="fadeIn"><img loading="lazy" decoding="async" width="715" height="1024" src="https://antigonejournal.com/wp-content/uploads/2024/01/JCLB-715x1024.png" alt="" srcset="https://antigonejournal.com/wp-content/uploads/2024/01/JCLB-715x1024.png 715w, https://antigonejournal.com/wp-content/uploads/2024/01/JCLB-209x300.png 209w, https://antigonejournal.com/wp-content/uploads/2024/01/JCLB-768x1101.png 768w, https://antigonejournal.com/wp-content/uploads/2024/01/JCLB-700x1003.png 700w, https://antigonejournal.com/wp-content/uploads/2024/01/JCLB-279x400.png 279w, https://antigonejournal.com/wp-content/uploads/2024/01/JCLB-560x802.png 560w, https://antigonejournal.com/wp-content/uploads/2024/01/JCLB.png 836w" sizes="(max-width: 715px) 100vw, 715px"><figcaption><em>John Chadwick in Alexandria. Reproduced in Lisa Bendall’s </em>The Decipherment of Linear B and the Ventris-Chadwick Correspondence.</figcaption></figure></div>


<p>At Bletchley he also met James Wyllie, one of the editors of the <em>Oxford Latin Dictionary</em>.<span onmouseover="footnote_tooltip_show('footnote_plugin_tooltip_text_24922_1_3')" onfocus="footnote_tooltip_show('footnote_plugin_tooltip_text_24922_1_3')" onmouseout="footnote_tooltip_hide('footnote_plugin_tooltip_text_24922_1_3')" onfocusout="footnote_tooltip_hide('footnote_plugin_tooltip_text_24922_1_3')"><a role="button" tabindex="0" onclick="footnote_moveToReference_24922_1('footnote_plugin_reference_24922_1_3');"><sup id="footnote_plugin_tooltip_24922_1_3">[3]</sup></a><span id="footnote_plugin_tooltip_text_24922_1_3" onmouseout="footnote_tooltip_hide('footnote_plugin_tooltip_text_24922_1_3')">Chadwick maintained an interest in lexicography for the rest of his life, the fruits of which include the Supplement to <em>LSJ</em> and, by way of a former student’s piety, the <em>Cambridge Greek Lexicon</em>, about which you can read more <a href="https://antigonejournal.com/2021/08/cambridge-greek-lexicon/">here</a>.</span></span> After completing his degree in 1946, Wyllie – about whom there is a very different story to tell one day – would offer him the job that he needed to marry his wartime sweetheart Joan Hill in 1947. He would stay in this post until 1951, when Arthur Beattie left Cambridge for the Greek chair at Edinburgh and Chadwick was appointed to his old position.</p>



<p>And so it was that he was preparing a course of lectures on the Greek dialects when he heard Ventris’ announcement – another of history’s happy accidents. After the war ended, he had, with friends, tried his hand at Linear B, but found that too little had been published. Since moving to Oxford he had been in touch with Myres, from whom he was able to acquire a copy of Work Note 20. He didn’t expect much, but soon found that Ventris’ suggested values produced, like a half-broken wartime code, islands of sense in a sea of confusion. &nbsp;&nbsp;</p>



<p>In the meantime Myres had told Ventris of this interested new party, and Ventris soon wrote to Chadwick, suggesting rather modestly that “if you find any points of contact between your work and mine, it would be very interesting to have to the opportunity of exchanging views.” Chadwick’s response has become rather famous:</p>





<p><span>Dear Mr. Ventris,</span></p>
<p>Let me first offer you my congratulations on having solved the Minoan problem, it is a magnificent achievement and you are yet only at the beginning of your triumph…</p>



<p>The advantage of Chadwick’s philological acumen was immediately on display. Two sign groups, one a place name very common at Pylos and the other apparently a prefix, had resisted Ventris’ efforts:</p>


<div>
<figure data-coblocks-animation="fadeIn"><img loading="lazy" decoding="async" width="263" height="357" src="https://antigonejournal.com/wp-content/uploads/2024/01/LB13.png" alt="" srcset="https://antigonejournal.com/wp-content/uploads/2024/01/LB13.png 263w, https://antigonejournal.com/wp-content/uploads/2024/01/LB13-221x300.png 221w" sizes="(max-width: 263px) 100vw, 263px"><figcaption><em>Two problematic sign-groups: </em>p.-ro<em> and </em>a-p.</figcaption></figure></div>


<p>The problem was the sign 𐀢: the consonant had to be <em>p</em>-, but the vowel remained obscure. If given the value <em>pu</em>, then the first group could be read as <em>pu-ro</em>, Πύλος (Pylos), a name still missing. But then the second group, which one might rather want to read as later Greek ἀπο- (<em>apo</em>, “from”) must be the rather strange <em>a-pu</em>, ἀπυ-. Ventris’ thoughts on the matter are not recorded, but Chadwick knew that in certain dialectal inscriptions one found ἀπύ (<em>apu</em>) in place of Attic ἀπό (<em>apo</em>). This was quite remarkable confirmation: at one stroke both the name Pylos and a common preposition were both restored. It is little wonder that Chadwick was so readily and eagerly convinced.</p>



<p>His enthusiasm proved salutary. Ventris had stepped quite boldly to the edge of a cliff, and only in doing so realised just how precipitous was the drop:&nbsp;</p>




<p>Dear Mr Chadwick,<br>Thank you very much for your letter. It is very encouraging to hear from someone who has been working on the Minoan problem that they agree with the Greek approach; because frankly at the moment I feel rather in need of moral support. The whole issue is getting to the stage where a lot of people will be looking at it very skeptically, and I’m conscious that there’s a lot which so far can’t be very satisfactorily explained. There’s a kind of central area of sense, but still a great periphery which is baffling.</p>


<p><strong>New evidence provides a handle:</strong></p>



<p>Still, there was some cause for hope. Blegen’s excavations at Pylos, abandoned after 1939 with the onset of the War, had resumed that summer, and 400 new tablets were found. These, which Ventris had never seen, would provide an independent check on his work: “I feel sure that if there’s something in this vocabulary, then there should be pretty clear confirmation of it in the new material.”</p>



<p>And so Ventris sent his sign-list to Blegen, but did not remain idle. He had already reached out to the <em>Journal of Hellenic Studies</em> as a possible venue for the publication of his decipherment, and asked Chadwick to collaborate on the article. Two drafts, one meeting in Cambridge, and a great deal of cautious criticism later, the manuscript of “Evidence for Greek Dialect in the Minoan Archives” was sent to the <em>JHS</em> for typesetting in early November 1952. The result was, in the words of Rhys Carpenter, “highly condensed, almost unreadable, barely comprehensible, but thoroughly sensational.”</p>



<p>The central contention of “Evidence” was framed modestly but not without confidence: “A complete decipherment is still a long way off; but we hope to produce sufficient evidence to show that we are dealing with a true Greek dialect.” But in the time between its submission in November of 1952 and publication in the summer of 1953, Blegen and the 1952 tablets had entered the picture with dramatic results.</p>



<p>Linear B tablets were not written for posterity. Inscribed on moist clay, they were left to dry and filed away for a year, it seems, or two: never more. It is only by a historical irony that they are preserved at all, baked hard by the very fires that destroyed the palaces. Though the archive rooms at Pylos give no sign of deliberate ransacking, the collapse and conflagration of the building was hardly gentle on the tablets inside. Many were found broken, with a thick covering of hard lime, further souvenir of the fire, obscuring the signs. So it was that none of the tablets found in 1952 could be read before significant conservation work. This was accomplished over the winter of 1952–3, and no one saw them in those long months except the museum staff engaged in this task. But in March of 1953, Blegen returned to Athens, and was able to investigate the newly cleaned and mended tablets.</p>



<p>With Ventris’s proposed values in hand, a tablet that had been found in two halves that June caught his eye:</p>


<div>
<figure data-coblocks-animation="fadeIn"><img loading="lazy" decoding="async" width="927" height="485" src="https://antigonejournal.com/wp-content/uploads/2024/01/LBTri.png" alt="" srcset="https://antigonejournal.com/wp-content/uploads/2024/01/LBTri.png 927w, https://antigonejournal.com/wp-content/uploads/2024/01/LBTri-300x157.png 300w, https://antigonejournal.com/wp-content/uploads/2024/01/LBTri-768x402.png 768w, https://antigonejournal.com/wp-content/uploads/2024/01/LBTri-700x366.png 700w, https://antigonejournal.com/wp-content/uploads/2024/01/LBTri-765x400.png 765w, https://antigonejournal.com/wp-content/uploads/2024/01/LBTri-560x293.png 560w" sizes="(max-width: 927px) 100vw, 927px"><figcaption><em>PY Ta 641, the ‘Tripod Tablet’ </em>(<em>National Archaeological Museum, Athens, Greece).</em></figcaption></figure></div>


<p>He wrote to Ventris: </p>


<p>Enclosed for your information is a copy of P641, which you may find interesting. It evidently deals with pots, some with three legs, some with four handles, some with three, and others without handles. The first word seems by your system seems to be <em>ti-ri-po-de</em> and it recurs twice as <em>ti-ri-po</em> (singular?). The four-handled pot is preceded by <em>qe-to-ro-we</em>, the three-handled by <em>ti-ri-wo-we</em> or <em>ti-ri-jo-we</em>, the handleless pot by <em>a-no-we</em>. All this seems too good to be true. Is coincidence excluded?… I should like to hear what you make of this inscription.</p>


<p>Ventris was not normally an excitable man, but this did the job. He called Chadwick immediately to share the news, and though his modesty was not quite overcome, but he now held such evidence that all, in Chadwick’s later words, “who were unprejudiced could now be convinced that the system worked.”</p>





<p>The confirmation that it offered was in the form of the remarkable agreement between the proposed Greek values and the illustrations. Next to a picture of a three-footed vessel and the numeral two was the word <em>ti-ri-po-de</em>, quite transparently the Greek plural <em>tripodes</em> (τρίποδες), “tripods” – literally “three feet”. <span onmouseover="footnote_tooltip_show('footnote_plugin_tooltip_text_24922_1_4')" onfocus="footnote_tooltip_show('footnote_plugin_tooltip_text_24922_1_4')" onmouseout="footnote_tooltip_hide('footnote_plugin_tooltip_text_24922_1_4')" onfocusout="footnote_tooltip_hide('footnote_plugin_tooltip_text_24922_1_4')"><a role="button" tabindex="0" onclick="footnote_moveToReference_24922_1('footnote_plugin_reference_24922_1_4');"><sup id="footnote_plugin_tooltip_24922_1_4">[4]</sup></a><span id="footnote_plugin_tooltip_text_24922_1_4" onmouseout="footnote_tooltip_hide('footnote_plugin_tooltip_text_24922_1_4')">Alternatively, τρίποδε, the archaic dual declension that referred exclusively to two of something.</span></span> And, if we doubted the declension, next to a three-footed vessel with the numeral one, <em>ti-ri-po</em>, the singular <em>tripos</em> (τρίπος; cf. Attic τρίπους).</p>



<p>A vessel with four handles is called <em>qe-to-ro-we. </em>The first element is the numeral four (<em>k<sup>w</sup>etr-</em>; cf. Latin <em>quattuor</em>, Attic <em>tettares</em>, τέτταρες after <em>*k<sup>w</sup></em>– &gt; t-), the second a form of the word “ear”, already in Homer used of the handles of vessels. And so <em>k<sup>w</sup>etrowes</em> = “four-handled”. <span onmouseover="footnote_tooltip_show('footnote_plugin_tooltip_text_24922_1_5')" onfocus="footnote_tooltip_show('footnote_plugin_tooltip_text_24922_1_5')" onmouseout="footnote_tooltip_hide('footnote_plugin_tooltip_text_24922_1_5')" onfocusout="footnote_tooltip_hide('footnote_plugin_tooltip_text_24922_1_5')"><a role="button" tabindex="0" onclick="footnote_moveToReference_24922_1('footnote_plugin_reference_24922_1_5');"><sup id="footnote_plugin_tooltip_24922_1_5">[5]</sup></a><span id="footnote_plugin_tooltip_text_24922_1_5" onmouseout="footnote_tooltip_hide('footnote_plugin_tooltip_text_24922_1_5')">There is quite a remarkable parallel for this in the Homeric <em>Iliad </em>(11.632–4): δέπας… οὔατα δ’ αὐτοῦ τέσσαρ᾽ ἔσαν, “a <em>depas</em> of four ears/handles.” The plural -ῶες for οὔατα is defensible etymologically and attested in Theocritus’ ἀμφῶες (1.28) of a two-handled vessel (lit. “on both sides”).</span></span> Finally, our handleless vase is called <em>a-no-we</em>, where <em>an-</em> is the negative prefix (equivalent to English “un-”, Latin <em>in-</em>): “no-handles”.<span onmouseover="footnote_tooltip_show('footnote_plugin_tooltip_text_24922_1_6')" onfocus="footnote_tooltip_show('footnote_plugin_tooltip_text_24922_1_6')" onmouseout="footnote_tooltip_hide('footnote_plugin_tooltip_text_24922_1_6')" onfocusout="footnote_tooltip_hide('footnote_plugin_tooltip_text_24922_1_6')"><a role="button" tabindex="0" onclick="footnote_moveToReference_24922_1('footnote_plugin_reference_24922_1_6');"><sup id="footnote_plugin_tooltip_24922_1_6">[6]</sup></a><span id="footnote_plugin_tooltip_text_24922_1_6" onmouseout="footnote_tooltip_hide('footnote_plugin_tooltip_text_24922_1_6')">Also supported by Theocritus (<em>Ep. </em>4. 3), who has the adjective ἀνούατος, “un-handled”.</span></span></p>


<div>
<figure data-coblocks-animation="fadeIn"><img loading="lazy" decoding="async" width="757" height="686" src="https://antigonejournal.com/wp-content/uploads/2024/01/Myc4.png" alt="" srcset="https://antigonejournal.com/wp-content/uploads/2024/01/Myc4.png 757w, https://antigonejournal.com/wp-content/uploads/2024/01/Myc4-300x272.png 300w, https://antigonejournal.com/wp-content/uploads/2024/01/Myc4-700x634.png 700w, https://antigonejournal.com/wp-content/uploads/2024/01/Myc4-441x400.png 441w, https://antigonejournal.com/wp-content/uploads/2024/01/Myc4-560x507.png 560w" sizes="(max-width: 757px) 100vw, 757px"><figcaption><em>Mycenaean krater with four handles, c.1400-1190 BC (Metropolitan Museum of Art, New York, USA)</em>.</figcaption></figure></div>


<p>To these we may add the frequently recurring <em>di-pa</em>, plausibly enough δέπας, a Homeric word for a cup or goblet, and the adjectives <em>me-zo </em>(<em>mezos</em>, μέζως) and <em>me-wi-jo </em>(<em>mewios</em>, μεϝίως), “larger” and “smaller” respectively.<span onmouseover="footnote_tooltip_show('footnote_plugin_tooltip_text_24922_1_7')" onfocus="footnote_tooltip_show('footnote_plugin_tooltip_text_24922_1_7')" onmouseout="footnote_tooltip_hide('footnote_plugin_tooltip_text_24922_1_7')" onfocusout="footnote_tooltip_hide('footnote_plugin_tooltip_text_24922_1_7')"><a role="button" tabindex="0" onclick="footnote_moveToReference_24922_1('footnote_plugin_reference_24922_1_7');"><sup id="footnote_plugin_tooltip_24922_1_7">[7]</sup></a><span id="footnote_plugin_tooltip_text_24922_1_7" onmouseout="footnote_tooltip_hide('footnote_plugin_tooltip_text_24922_1_7')">Cf. historical μείζων and μείων.</span></span> Ventris had hoped for clear confirmation from Blegen’s new material; here was something to exceed any reasonable expectation.</p>



<p>From 1953 to 1956 Ventris was in high demand: the Institute of Classical Studies founded its Mycenaean Seminar; the first international colloquium on Mycenology (as the discipline came to be known) was held at Gif-sur-Yvette, south of Paris; and time had to be made for a visit to Buckingham Palace to receive an OBE from the Queen. And all the while there was writing to be done: he worked closely with Chadwick on <em>Documents in Mycenaean Greek</em>, which remains unsurpassed in its breadth and ambition to make the technical details of the tablets intelligible to the educated reader.<span onmouseover="footnote_tooltip_show('footnote_plugin_tooltip_text_24922_1_8')" onfocus="footnote_tooltip_show('footnote_plugin_tooltip_text_24922_1_8')" onmouseout="footnote_tooltip_hide('footnote_plugin_tooltip_text_24922_1_8')" onfocusout="footnote_tooltip_hide('footnote_plugin_tooltip_text_24922_1_8')"><a role="button" tabindex="0" onclick="footnote_moveToReference_24922_1('footnote_plugin_reference_24922_1_8');"><sup id="footnote_plugin_tooltip_24922_1_8">[8]</sup></a><span id="footnote_plugin_tooltip_text_24922_1_8" onmouseout="footnote_tooltip_hide('footnote_plugin_tooltip_text_24922_1_8')">Today it is generally the second edition of 1973, made necessary by the rapid development of the discipline in its first two decades, to which scholars turn. It is of course outdated: but not obsolete.</span></span> Chadwick delivered the typescript to Cambridge University Press in July 1955. Ventris was in Greece, and so had to be notified by postcard – written in Linear B. The script could not just be read again: it could be written, and after 3,000 years given the breath of new life.</p>




<p><em>i-jo-a-na mi-ka-e ka-re-e<br>sa-me-ro pu-pi-ri-jo pa-ro-do-ka<br>tu-po-ka-ra-pe-u-si<br>a-ka-ta tu-ka</em></p><p><em>ka-mo-jo ke-pu<sub>2</sub>-ra<sub>3</sub> i-jo-u-ni-jo-jo<br>me-no A-ME-RA 7</em></p>
<p>John to Michael: Greetings!<br>Today I have given the book <br>to the typesetters.<br>May it go well for us!</p>
<p>At the Bridge of Cam, on day 7<br>Of the month of June.</p>


<p>But Ventris would not see the book published. Early in the morning of 6 September, 1956, he collided with a parked lorry while driving and was killed instantly. He was 34: ὃν οἱ θεοὶ φιλοῦσιν ἀποθνῄσκει νεός.<span onmouseover="footnote_tooltip_show('footnote_plugin_tooltip_text_24922_1_9')" onfocus="footnote_tooltip_show('footnote_plugin_tooltip_text_24922_1_9')" onmouseout="footnote_tooltip_hide('footnote_plugin_tooltip_text_24922_1_9')" onfocusout="footnote_tooltip_hide('footnote_plugin_tooltip_text_24922_1_9')"><a role="button" tabindex="0" onclick="footnote_moveToReference_24922_1('footnote_plugin_reference_24922_1_9');"><sup id="footnote_plugin_tooltip_24922_1_9">[9]</sup></a><span id="footnote_plugin_tooltip_text_24922_1_9" onmouseout="footnote_tooltip_hide('footnote_plugin_tooltip_text_24922_1_9')">“He whom the gods love dies young.”</span></span></p>



<p>It is on this sad note that the story of the decipherment tends to end. And it is true that, with the publication of the Tripod Tablet in 1954, Ventris’ work won wide (though not universal) acceptance. In many ways the work that has followed since was of a different sort. The basic values of the core signs were known. The great challenge was now interpretation – and this continues today. So, too, does the decipherment: rare signs, often restricted to personal names or toponyms, still defy a fixed phonetic value. New finds are still made, though slower than many would like, and our understanding of these terse and archaic documents evolves with our understanding of the Mycenaean world more broadly.</p>


<div>
<figure data-coblocks-animation="fadeIn"><img loading="lazy" decoding="async" width="768" height="1024" src="https://antigonejournal.com/wp-content/uploads/2024/01/TN2-768x1024.png" alt="" srcset="https://antigonejournal.com/wp-content/uploads/2024/01/TN2-768x1024.png 768w, https://antigonejournal.com/wp-content/uploads/2024/01/TN2-225x300.png 225w, https://antigonejournal.com/wp-content/uploads/2024/01/TN2-1152x1536.png 1152w, https://antigonejournal.com/wp-content/uploads/2024/01/TN2-900x1200.png?crop=1 900w, https://antigonejournal.com/wp-content/uploads/2024/01/TN2-600x800.png?crop=1 600w, https://antigonejournal.com/wp-content/uploads/2024/01/TN2-450x600.png?crop=1 450w, https://antigonejournal.com/wp-content/uploads/2024/01/TN2-300x400.png?crop=1 300w, https://antigonejournal.com/wp-content/uploads/2024/01/TN2-150x200.png?crop=1 150w, https://antigonejournal.com/wp-content/uploads/2024/01/TN2-700x933.png 700w, https://antigonejournal.com/wp-content/uploads/2024/01/TN2-560x747.png 560w, https://antigonejournal.com/wp-content/uploads/2024/01/TN2.png 1170w" sizes="(max-width: 768px) 100vw, 768px"></figure></div>


<hr>



<p><em>Theodore Nash is a PhD student in Classical Archaeology at the University of Michigan</em>. <em>He has previously written for </em>Antigone<em> about <a href="https://antigonejournal.com/2022/05/murray-dodds-page/" target="_blank" rel="noreferrer noopener">Greek scholarship at Oxford b</a>etween the Wars.</em></p>



<hr>



<p><strong>Further Reading</strong></p>



<p>The debt that this essay owes to previous work will be obvious to all who read it, even presented as it is without a traditional scholarly apparatus. To render that debt explicit, and as a guide to the curious, I include this list of suggested reading.</p>



<p><em>Correspondence and Historical Documents</em></p>



<p>The Program in Aegean Scripts and Prehistory at the University of Texas has digitised a significant collection of letters between many of the principal actors in the story of decipherment, including Michael Ventris, Alice Kober, Emmett Bennett, John Chadwick. These can all be found <a href="https://hdl.handle.net/2152/11681" target="_blank" rel="noreferrer noopener">here</a>. The value of this resource cannot be expressed without resort to rather dramatic hyperbole. A few letters, including Chadwick’s first letter to Ventris, have been digitised by the <a href="https://www.classics.cam.ac.uk/research/projects/mycep/archive/correspondence" target="_blank" rel="noreferrer noopener">Faculty of Classics</a> at the University of Cambridge.</p>



<p>None of this correspondence has been published systematically, though it is frequently quoted and reproduced in biographical and historical studies (for which see below). The few publications focussed on correspondence are:</p>



<p>Lisa M. Bendall, <em>The Decipherment of Linear B and the Ventris-Chadwick Correspondence </em>(Fitzwilliam Museum Catalogue, Cambridge, 2003. (Images of many letters are reproduced, but unfortunately not in very high quality.)</p>



<p>Rupert Thompson, “The Ventris–Chadwick Correspondence and the Decipherment of Linear B: A Denier, A Dissenter and A Dubious Conclusion,” <em>Cambridge Classical Journal </em>65 (2019) 173–99.</p>



<p>Ventris’ Work Notes were compiled and published by Anna Sacconi, <em>Work Notes on Minoan Language Research and Other Unedited Papers</em> (Edizioni dell’Ateneo, Rome, 1988.They are invaluable for anyone working on the history of the decipherment, but do not make for light reading. Included are also a few letters from and to Ventris, and the text of his full radio announcement. The actual announcement can be heard <a href="https://www.youtube.com/watch?v=pOOGJAQ4eg4" target="_blank" rel="noreferrer noopener">here</a>.</p>



<p><em>Biographies</em></p>



<p>Both Ventris and Kober have been the subject of biographies in the past twenty years. These were written by journalists, not scholars, with the advantage that they are generally accessible and affordable, but they can tend towards the speculative or sensational. They are both very valuable when taken <em>cum grano salis</em>:</p>



<p>Margalit Fox, <em>The Riddle of the Labyrinth </em>(Profile Books, London, 2013). (<a href="https://doi.org/10.2972/hesperia.87.1.0001" target="_blank" rel="noreferrer noopener">Tracy 2018</a> offers a valuable rebuttal to the claims that Blegen deliberately kept Pylos material from Kober.)</p>



<p>Andrew Robinson, <em>The Man who Deciphered Linear B </em>(Thames and Hudson, London, 2005). (Best read with Lisa Bendall’s <a href="https://www.academia.edu/4501110/Review_of_The_Man_Who_Deciphered_Linear_B_The_Story_of_Michael_Ventris_by_A_Robinson_Journal_of_Hellenic_Studies_125_203_5" target="_blank" rel="noreferrer noopener"><em>JHS</em> review</a>.)</p>



<p><em>Accounts of the Decipherment</em></p>



<p>Ventris never produced a full account of his own process (which explains, <em>inter alia</em>, why we do not know how he worked out the labio-velar series). After his death, the task of explaining and defending his friend’s work was left to Chadwick, who produced the first and still best account of the decipherment. Anyone who has read this far should probably find themself a copy: John Chadwick, <em>The Decipherment of Linear B</em> (Cambridge UP. 1958, 2<sup>nd</sup> ed. 1967).</p>



<p><em>The Crunchy Stuff</em></p>



<p>All of the above was written for the curious but Greekless reader. Those with Greek may enjoy <em>Documents</em>, though its age and stages of revision make it a difficult first introduction. The best introduction to Linear B in English is the three-volume Peeter’s Companion, aimed (and, unfortunately, priced) for academics: Yves Duhoux &amp; Anna Morpurgo-Davies, <em>A Companion to Linear B: Mycenaean Greek Texts and Their World</em> (3 vols, Peeters, Louvain, 2008–14).</p>



<p>Those looking for something cheaper will find much value in Hooker’s <em>Introduction</em>. Though dated, it remains the most concise and convenient introduction to the grammar of Mycenaean Greek: J.T. Hooker, <em>Linear B: An Introduction </em>(Bristol Classical Press, 1980).</p>
	</div>
	<!-- .entry-footer -->

	</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Norwegian youth win climate court case against the Norwegian State (128 pts)]]></title>
            <link>https://www.greenpeace.org/international/press-release/64831/environmental-youth-groups-win-climate-court-case-against-norwegian-state/</link>
            <guid>39046930</guid>
            <pubDate>Thu, 18 Jan 2024 20:08:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.greenpeace.org/international/press-release/64831/environmental-youth-groups-win-climate-court-case-against-norwegian-state/">https://www.greenpeace.org/international/press-release/64831/environmental-youth-groups-win-climate-court-case-against-norwegian-state/</a>, See on <a href="https://news.ycombinator.com/item?id=39046930">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<article>
<p>Oslo, Norway –<strong> Greenpeace Nordic and Natur og Ungdom (Young Friends of the Earth Norway) secured a historic win against the Norwegian State, rendering the approvals of three oil and gas fields in the North Sea invalid.&nbsp;</strong></p>
<p>In November 2023, environmental organisation Greenpeace Nordic and youth group Natur og Ungdom took the Norwegian State to court once again. The organisations argued that recent approvals of three new oil and gas fields, Breidablikk, Yggdrasil and Tyrving, all in the North Sea, violate the Norwegian Constitution, European Economic Area law and Norway’s international human rights commitments. They also argued the Ministry of Energy failed to consider the UN Convention on the Rights of the Child during the approval of the fields, thus rendering the approvals invalid.</p>
<p>“I am extremely pleased and relieved that the Court has delivered such a positive and thorough judgement. The judgement establishes that the Breidablikk, Yggdrasil and Tyrving oil and gas fields were approved on an illegal basis and that production must be stopped immediately. We expect a halt to all further developments,” said <strong>Frode Pleym, Head of Greenpeace, Norway</strong></p>
<p>Specifically, the organisations pointed to the fact that impact assessments of the three oil and gas fields’ global climate effects, which the Ministry of Energy is required to carry out, are either highly inadequate or non-existent.</p>
<p>The judgement delivered today by the Oslo District Court, has found the approvals of all three oil and gas fields invalid and issued an injunction forbidding the state from granting any new permits necessary to construct and produce from the fields.[1]&nbsp;</p>
<p>“This is an important victory for current and future generations and the environment. With this judgement, millions of barrels of oil will remain in the ground. During the trial, the State tried to diminish the impact of the emissions&nbsp; Breidablikk, Tyrving and Yggdrasil would have globally. As confirmed by the court’s decision, emissions from the oil fields would have catastrophic effects on the global climate, on people and the planet. We are pleased the oil and gas will remain untouched in the ground, instead of further exacerbating climate disasters,” said <strong>Gytis Blaževičius, Head of Natur og Ungdom.</strong></p>
<p>The Court also emphasised procedural problems of the approval process, highlighting the lack of adequate public participation, and found that the approvals were inadequate under EU law. and that “combustion emissions [from] petroleum extraction are such a significant and particularly characteristic consequence of such projects that they must clearly be considered indirect climate impacts within the meaning of the (EU) Project Directive”.</p>
<p>The Court confirmed that the government violated legal precedent from the Norwegian Supreme Court, by not subjecting combustion emissions to an environmental impact assessment.&nbsp;</p>
<p>The Court held that the children’s right to be heard would be safeguarded through public hearings that must take place in connection with lawfully required environmental impact assessments. Anticipating <a href="https://www.greenpeace.org/international/press-release/58956/first-climate-case-heard-at-the-european-court-of-human-rights/">clarifications from the Grand Chamber</a> of the European Court of Human Rights, the District Court refrained from assessing violations of the Convention.&nbsp;</p>
<div><p>More and more people, particularly those most impacted by climate change, are using the law to protect their rights from the climate crisis. This ruling becomes another substantial reference point for all climate lawsuits pending around the world.</p><p>ENDS</p></div>
<p>Media briefing on the case<a href="https://www.greenpeace.org/norway/nyheter/klimaendringer/media-briefing-the-new-climate-lawsuit/"> here</a>Photos are available in the<a href="https://media.greenpeace.org/collection/27MZIFJFQIROK"> Greenpeace Media Library</a></p>
<p><strong>Notes:</strong></p>
<p>[1] Specifically, the court ruled that section 20 et seq. of the Petroleum Regulations, which provide the requirement on impact assessments, must be interpreted in light of Article 112 of the Norwegian Constitution. It referred to the 2020 Supreme Court decision that found that Article 112 should cover both emissions from the production and consumption of petroleum, even if it is combusted outside of Norway. The Court found that the State must assess the real impact of both forms of greenhouse gas emissions resulting from the development and operation of petroleum deposits before approving oil and gas fields and that a real test must be carried out of whether approval would be contrary to Article 112 of the Norwegian Constitution.&nbsp;</p>
<p><strong>Contacts:</strong></p>
<p>Frode Pleym, Head of Greenpeace in Norway: <a href="https://www.greenpeace.org/cdn-cgi/l/email-protection" data-cfemail="ee889c818a8bc09e828b9783ae899c8b8b809e8b8f8d8bc0819c89">[email&nbsp;protected]</a>, +47 97 30 73 78</p>
<p>Gytis Blaževičius, Head of Natur og Ungdom (Young Friends of the Earth Norway):<br><a href="https://www.greenpeace.org/cdn-cgi/l/email-protection" data-cfemail="74130d001d0716341a015a1a1b">[email&nbsp;protected]</a> , +47 452 97 542</p>
<p>Greenpeace International Press Desk: <a href="https://www.greenpeace.org/cdn-cgi/l/email-protection" data-cfemail="92e2e0f7e1e1f6f7e1f9bcfbfce6d2f5e0f7f7fce2f7f3f1f7bcfde0f5">[email&nbsp;protected]</a>, +31 (0) 20 718 2470 (available 24 hours). Follow<a href="https://twitter.com/Greenpeacepress"> @greenpeacepress</a> for our latest international press releases.</p>
</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[German developer guilty of 'hacking' for exposing hardcoded credentials in app (296 pts)]]></title>
            <link>https://infosec.exchange/@WPalant/111776937550399546</link>
            <guid>39046838</guid>
            <pubDate>Thu, 18 Jan 2024 20:01:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://infosec.exchange/@WPalant/111776937550399546">https://infosec.exchange/@WPalant/111776937550399546</a>, See on <a href="https://news.ycombinator.com/item?id=39046838">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[WebGPU is now available on Android (226 pts)]]></title>
            <link>https://developer.chrome.com/blog/new-in-webgpu-121</link>
            <guid>39045598</guid>
            <pubDate>Thu, 18 Jan 2024 18:29:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://developer.chrome.com/blog/new-in-webgpu-121">https://developer.chrome.com/blog/new-in-webgpu-121</a>, See on <a href="https://news.ycombinator.com/item?id=39045598">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

  
    






<div>
        
          <p><img alt="François Beaufort" src="https://web.dev/images/authors/beaufortfrancois.jpg" decoding="async" height="64" loading="lazy" width="64"></p>
      </div>

<h2 id="support-webgpu-on-android" data-text="Support WebGPU on Android" tabindex="-1"><a href="#support_webgpu_on_android" name="support_webgpu_on_android">Support WebGPU on Android</a></h2>

<p>The Chrome team is excited to announce that WebGPU is now enabled by default in Chrome 121 on devices running Android 12 and greater powered by Qualcomm and ARM GPUs.</p>

<p>Support will gradually expand to encompass a wider range of Android devices, including those running on Android 11 in a near future. This expansion will be dependent on further testing and optimization to ensure a seamless experience across a broader range of hardware configurations. See <a href="https://bugs.chromium.org/p/chromium/issues/detail?id=1497815">issue chromium:1497815</a>.</p>

<figure>
  <img src="https://developer.chrome.com/static/blog/new-in-webgpu-121/image/webgpu-android.png" ,="" alt="Screenshot of WebGPU sample running on Chrome for Android." width="854" height="480" srcset="https://developer.chrome.com/static/blog/new-in-webgpu-121/image/webgpu-android_36.png 36w,https://developer.chrome.com/static/blog/new-in-webgpu-121/image/webgpu-android_48.png 48w,https://developer.chrome.com/static/blog/new-in-webgpu-121/image/webgpu-android_72.png 72w,https://developer.chrome.com/static/blog/new-in-webgpu-121/image/webgpu-android_96.png 96w,https://developer.chrome.com/static/blog/new-in-webgpu-121/image/webgpu-android_480.png 480w,https://developer.chrome.com/static/blog/new-in-webgpu-121/image/webgpu-android_720.png 720w,https://developer.chrome.com/static/blog/new-in-webgpu-121/image/webgpu-android_856.png 856w,https://developer.chrome.com/static/blog/new-in-webgpu-121/image/webgpu-android_960.png 960w,https://developer.chrome.com/static/blog/new-in-webgpu-121/image/webgpu-android_1440.png 1440w,https://developer.chrome.com/static/blog/new-in-webgpu-121/image/webgpu-android_1920.png 1920w,https://developer.chrome.com/static/blog/new-in-webgpu-121/image/webgpu-android_2880.png 2880w" sizes="(max-width: 840px) 100vw, 856px">
  <figcaption>WebGPU sample running on Chrome for Android.</figcaption>
</figure>

<h2 id="use_dxc_instead_of_fxc_for_shader_compilation_on_windows" data-text="Use DXC instead of FXC for shader compilation on Windows" tabindex="-1">Use DXC instead of FXC for shader compilation on Windows</h2>

<p>Chrome now uses the power of <a href="https://github.com/microsoft/DirectXShaderCompiler/wiki">DXC</a> (DirectX Compiler) to compile shaders on Windows D3D12 machines equipped with SM6+ graphics hardware. Previously, WebGPU relied on FXC (FX Compiler) for shader compilation on Windows. While functional, FXC lacked the feature set and performance optimizations present in DXC.</p>

<p>Initial testing shows a 20% average increase in compute shader compilation speed when using DXC compared to FXC.</p>

<h2 id="timestamp_queries_in_compute_and_render_passes" data-text="Timestamp queries in compute and render passes" tabindex="-1">Timestamp queries in compute and render passes</h2>

<p><a href="https://gpuweb.github.io/gpuweb/#timestamp">Timestamp queries</a> allow WebGPU applications to measure precisely (down to the nanosecond) how much time their GPU commands take to execute compute and render passes. They are heavily used to gain insights into the performance and behavior of GPU workloads.</p>

<p>When the <code translate="no" dir="ltr">"timestamp-query"</code> feature is available in a <code translate="no" dir="ltr">GPUAdapter</code>, you can now do the following things:</p>

<ul>
<li>Request a <code translate="no" dir="ltr">GPUDevice</code> with the <code translate="no" dir="ltr">"timestamp-query"</code> feature.</li>
<li>Create a <code translate="no" dir="ltr">GPUQuerySet</code> of type <code translate="no" dir="ltr">"timestamp"</code>.</li>
<li>Use <a href="https://gpuweb.github.io/gpuweb/#dom-gpucomputepassdescriptor-timestampwrites"><code translate="no" dir="ltr">GPUComputePassDescriptor.timestampWrites</code></a> and <a href="https://gpuweb.github.io/gpuweb/#dom-gpurenderpassdescriptor-timestampwrites"><code translate="no" dir="ltr">GPURenderPassDescriptor.timestampWrites</code></a> to define where to write timestamp values in <code translate="no" dir="ltr">GPUQuerySet</code>.</li>
<li>Resolve timestamp values into a <code translate="no" dir="ltr">GPUBuffer</code> with <a href="https://gpuweb.github.io/gpuweb/#dom-gpucommandencoder-resolvequeryset"><code translate="no" dir="ltr">resolveQuerySet()</code></a>.</li>
<li>Read timestamp values back by copying the results from the <code translate="no" dir="ltr">GPUBuffer</code> to the CPU.</li>
<li>Decode timestamp values as a <code translate="no" dir="ltr">BigInt64Array</code>.</li>
</ul>

<p>See the following example and issue <a href="https://bugs.chromium.org/p/dawn/issues/detail?id=1800">dawn:1800</a>.</p>
<pre translate="no" dir="ltr"><code translate="no" dir="ltr">const adapter = await navigator.gpu.requestAdapter();
if (!adapter.features.has("timestamp-query")) {
  throw new Error("Timestamp query feature is not available");
}
// Explicitly request timestamp query feature.
const device = await adapter.requestDevice({
  requiredFeatures: ["timestamp-query"],
});
const commandEncoder = device.createCommandEncoder();

// Create a GPUQuerySet which holds 2 timestamp query results: one for the
// beginning and one for the end of compute pass execution.
const querySet = device.createQuerySet({ type: "timestamp", count: 2 });
const timestampWrites = {
  querySet,
  beginningOfPassWriteIndex: 0, // Write timestamp in index 0 when pass begins.
  endOfPassWriteIndex: 1, // Write timestamp in index 1 when pass ends.
};
const passEncoder = commandEncoder.beginComputePass({ timestampWrites });
// TODO: Set pipeline, bind group, and dispatch work to be performed.
passEncoder.end();

// Resolve timestamps in nanoseconds as a 64-bit unsigned integer into a GPUBuffer.
const size = 2 * BigInt64Array.BYTES_PER_ELEMENT;
const resolveBuffer = device.createBuffer({
  size,
  usage: GPUBufferUsage.QUERY_RESOLVE | GPUBufferUsage.COPY_SRC,
});
commandEncoder.resolveQuerySet(querySet, 0, 2, resolveBuffer, 0);

// Read GPUBuffer memory.
const resultBuffer = device.createBuffer({
  size,
  usage: GPUBufferUsage.COPY_DST | GPUBufferUsage.MAP_READ,
});
commandEncoder.copyBufferToBuffer(resolveBuffer, 0, resultBuffer, 0, size);

// Submit commands to the GPU.
device.queue.submit([commandEncoder.finish()]);

// Log compute pass duration in nanoseconds.
await resultBuffer.mapAsync(GPUMapMode.READ);
const times = new BigInt64Array(resultBuffer.getMappedRange());
console.log(`Compute pass duration: ${Number(times[1] - times[0])}ns`);
resultBuffer.unmap();
</code></pre>
<p>Due to <a href="https://gpuweb.github.io/gpuweb/#security-timing-device">timing attack</a> concerns, timestamp queries are quantized with a resolution of 100 microseconds, which provides a good compromise between precision and security. In Chrome browser, you can disable timestamp quantization by enabling the "WebGPU Developer Features" <a href="https://developer.chrome.com/docs/web-platform/chrome-flags#chromeflags">flag</a> at <code translate="no" dir="ltr">chrome://flags/#enable-webgpu-developer-features</code> during the development of your app. See <a href="https://developer.chrome.com/blog/new-in-webgpu-120#timestamp_queries_quantization">Timestamp queries quantization</a> to learn more.</p>

<p>As GPUs may reset the timestamp counter occasionally, which can result in unexpected values such as negative deltas between timestamps, I recommend you check out the <a href="https://github.com/webgpu/webgpu-samples/compare/d67ae2acb40bebfa7c7705cd28175b44fbb03b59..e59b76695212208600f5bbb5423895e6d440fd90#diff-fe64f06098aed64ad6bbd885ad77dc50f7fa856829bf22545d3aa1baaad1c0b8">git diff changes</a> that adds timestamp query support to the following <a href="https://webgpu.github.io/webgpu-samples/samples/computeBoids">Compute Boids</a> sample.</p>

<figure>
  <img src="https://developer.chrome.com/static/blog/new-in-webgpu-121/image/compute-boids.png" ,="" alt="Screenshot of Compute Boids sample featuring timestamp query." width="854" height="553" srcset="https://developer.chrome.com/static/blog/new-in-webgpu-121/image/compute-boids_36.png 36w,https://developer.chrome.com/static/blog/new-in-webgpu-121/image/compute-boids_48.png 48w,https://developer.chrome.com/static/blog/new-in-webgpu-121/image/compute-boids_72.png 72w,https://developer.chrome.com/static/blog/new-in-webgpu-121/image/compute-boids_96.png 96w,https://developer.chrome.com/static/blog/new-in-webgpu-121/image/compute-boids_480.png 480w,https://developer.chrome.com/static/blog/new-in-webgpu-121/image/compute-boids_720.png 720w,https://developer.chrome.com/static/blog/new-in-webgpu-121/image/compute-boids_856.png 856w,https://developer.chrome.com/static/blog/new-in-webgpu-121/image/compute-boids_960.png 960w,https://developer.chrome.com/static/blog/new-in-webgpu-121/image/compute-boids_1440.png 1440w,https://developer.chrome.com/static/blog/new-in-webgpu-121/image/compute-boids_1920.png 1920w,https://developer.chrome.com/static/blog/new-in-webgpu-121/image/compute-boids_2880.png 2880w" sizes="(max-width: 840px) 100vw, 856px">
  <figcaption>Compute Boids sample featuring timestamp query.</figcaption>
</figure>

<h2 id="default_entry_points_to_shader_modules" data-text="Default entry points to shader modules" tabindex="-1">Default entry points to shader modules</h2>

<p>To improve the developer experience, you can now omit the <a href="https://gpuweb.github.io/gpuweb/#dom-gpuprogrammablestage-entrypoint"><code translate="no" dir="ltr">entryPoint</code></a> of your shader module when creating a compute or render pipeline. If no unique entry point for the shader stage is found in the shader code, a <a href="https://developer.mozilla.org/docs/Web/API/GPUValidationError">GPUValidationError</a> will be triggered. See the following example and <a href="https://bugs.chromium.org/p/dawn/issues/detail?id=2254">issue dawn:2254</a>.</p>
<pre translate="no" dir="ltr"><code translate="no" dir="ltr">const code = `
    @vertex fn vertexMain(@builtin(vertex_index) i : u32) -&gt;
      @builtin(position) vec4f {
       const pos = array(vec2f(0, 1), vec2f(-1, -1), vec2f(1, -1));
       return vec4f(pos[i], 0, 1);
    }
    @fragment fn fragmentMain() -&gt; @location(0) vec4f {
      return vec4f(1, 0, 0, 1);
    }`;
const module = myDevice.createShaderModule({ code });
const format = navigator.gpu.getPreferredCanvasFormat();
const pipeline = await myDevice.createRenderPipelineAsync({
  layout: "auto",
<s>  vertex: { module, entryPoint: "vertexMain" },</s>
<s>  fragment: { module, entryPoint: "fragmentMain", targets: [{ format }] },</s>
<strong>  vertex: { module },</strong>
<strong>  fragment: { module, targets: [{ format }] },</strong>
});
</code></pre>
<h2 id="support_display-p3_as_gpuexternaltexture_color_space" data-text="Support display-p3 as GPUExternalTexture color space" tabindex="-1">Support display-p3 as GPUExternalTexture color space</h2>

<p>You can now set <code translate="no" dir="ltr">"display-p3"</code> destination color space when importing a <a href="https://developer.mozilla.org/docs/Web/API/GPUExternalTexture">GPUExternalTexture</a> from HDR videos with <a href="https://developer.mozilla.org/docs/Web/API/GPUDevice/importExternalTexture"><code translate="no" dir="ltr">importExternalTexture()</code></a>. Check out how WebGPU handles <a href="https://gpuweb.github.io/gpuweb/#color-spaces">color spaces</a>. See the following example and issue <a href="https://bugs.chromium.org/p/chromium/issues/detail?id=1330250">chromium:1330250</a>.</p>
<pre translate="no" dir="ltr"><code translate="no" dir="ltr">// Create texture from HDR video.
const video = document.querySelector("video");
const texture = myDevice.importExternalTexture({
  source: video,
  colorSpace: "display-p3",
});
</code></pre>
<h2 id="memory_heaps_info" data-text="Memory heaps info" tabindex="-1">Memory heaps info</h2>

<p>To help you anticipate memory limitations when allocating large amounts during the development of your app, <a href="https://developer.mozilla.org/docs/Web/API/GPUAdapter/requestAdapterInfo"><code translate="no" dir="ltr">requestAdapterInfo()</code></a> now exposes <code translate="no" dir="ltr">memoryHeaps</code> information such as the size and type of memory heaps available on the adapter. This experimental feature is accessible only when the "WebGPU Developer Features" <a href="https://developer.chrome.com/docs/web-platform/chrome-flags#chromeflags">flag</a> at <code translate="no" dir="ltr">chrome://flags/#enable-webgpu-developer-features</code> is enabled. See the following example and <a href="https://bugs.chromium.org/p/dawn/issues/detail?id=2249">issue dawn:2249</a>.</p>
<pre translate="no" dir="ltr"><code translate="no" dir="ltr">const adapter = await navigator.gpu.requestAdapter();
const adapterInfo = await adapter.requestAdapterInfo();

for (const { size, properties } of adapterInfo.memoryHeaps) {
  console.log(size); // memory heap size in bytes
  if (properties &amp; GPUHeapProperty.DEVICE_LOCAL)  { /* ... */ }
  if (properties &amp; GPUHeapProperty.HOST_VISIBLE)  { /* ... */ }
  if (properties &amp; GPUHeapProperty.HOST_COHERENT) { /* ... */ }
  if (properties &amp; GPUHeapProperty.HOST_UNCACHED) { /* ... */ }
  if (properties &amp; GPUHeapProperty.HOST_CACHED)   { /* ... */ }
}
</code></pre>
<figure>
  <img src="https://developer.chrome.com/static/blog/new-in-webgpu-121/image/webgpureport.png" ,="" alt="Screenshot of https://webgpureport.org featuring memory heaps in adapter info." width="854" height="560" srcset="https://developer.chrome.com/static/blog/new-in-webgpu-121/image/webgpureport_36.png 36w,https://developer.chrome.com/static/blog/new-in-webgpu-121/image/webgpureport_48.png 48w,https://developer.chrome.com/static/blog/new-in-webgpu-121/image/webgpureport_72.png 72w,https://developer.chrome.com/static/blog/new-in-webgpu-121/image/webgpureport_96.png 96w,https://developer.chrome.com/static/blog/new-in-webgpu-121/image/webgpureport_480.png 480w,https://developer.chrome.com/static/blog/new-in-webgpu-121/image/webgpureport_720.png 720w,https://developer.chrome.com/static/blog/new-in-webgpu-121/image/webgpureport_856.png 856w,https://developer.chrome.com/static/blog/new-in-webgpu-121/image/webgpureport_960.png 960w,https://developer.chrome.com/static/blog/new-in-webgpu-121/image/webgpureport_1440.png 1440w,https://developer.chrome.com/static/blog/new-in-webgpu-121/image/webgpureport_1920.png 1920w,https://developer.chrome.com/static/blog/new-in-webgpu-121/image/webgpureport_2880.png 2880w" sizes="(max-width: 840px) 100vw, 856px">
  <figcaption>Adapter info memory heaps shown on <a href="https://webgpureport.org/">https://webgpureport.org</a>.</figcaption>
</figure>

<h2 id="dawn_updates" data-text="Dawn updates" tabindex="-1">Dawn updates</h2>

<p>The <code translate="no" dir="ltr">HasWGSLLanguageFeature</code> and <code translate="no" dir="ltr">EnumerateWGSLLanguageFeatures</code> methods on <code translate="no" dir="ltr">wgpu::Instance</code> have been added to handle WGSL language features. See issue <a href="https://bugs.chromium.org/p/dawn/issues/detail?id=2260">dawn:2260</a>.</p>

<p>The non-standard <code translate="no" dir="ltr">wgpu::Feature::BufferMapExtendedUsages</code> feature lets you create a GPU buffer with <code translate="no" dir="ltr">wgpu::BufferUsage::MapRead</code> or <code translate="no" dir="ltr">wgpu::BufferUsage::MapWrite</code> and any other <code translate="no" dir="ltr">wgpu::BufferUsage</code>. See the following example and issue <a href="https://bugs.chromium.org/p/dawn/issues/detail?id=2204">dawn:2204</a>.</p>
<pre translate="no" dir="ltr"><code translate="no" dir="ltr">wgpu::BufferDescriptor descriptor = {
  .size = 128,
  .usage = wgpu::BufferUsage::MapWrite | wgpu::BufferUsage::Uniform
};
wgpu::Buffer uniformBuffer = device.CreateBuffer(&amp;descriptor);

uniformBuffer.MapAsync(wgpu::MapMode::Write, 0, 128,
   [](WGPUBufferMapAsyncStatus status, void* userdata)
   {
      wgpu::Buffer* buffer = static_cast&lt;wgpu::Buffer*&gt;(userdata);
      memcpy(buffer-&gt;GetMappedRange(), data, sizeof(data));
   },
   &amp;uniformBuffer);
</code></pre>
<p>The following features have been documented: <a href="https://dawn.googlesource.com/dawn/+/refs/heads/main/docs/dawn/features/angle_texture_sharing.md">ANGLE Texture Sharing</a>, <a href="https://dawn.googlesource.com/dawn/+/refs/heads/main/docs/dawn/features/d3d11_multithread_protected.md">D3D11 multithread protected</a>, <a href="https://dawn.googlesource.com/dawn/+/refs/heads/main/docs/dawn/features/implicit_device_synchronization.md">Implicit Device Synchronization</a>, <a href="https://dawn.googlesource.com/dawn/+/refs/heads/main/docs/dawn/features/norm16_texture_formats.md">Norm16 texture formats</a>, <a href="https://dawn.googlesource.com/dawn/+/refs/heads/main/docs/dawn/features/timestamp_query_inside_passes.md">Timestamp Query Inside Passes</a>, <a href="https://dawn.googlesource.com/dawn/+/refs/heads/main/docs/dawn/features/pixel_local_storage.md">Pixel Local Storage</a>, <a href="https://dawn.googlesource.com/dawn/+/refs/heads/main/docs/dawn/features/shader_features.md">Shader Features</a>, and <a href="https://dawn.googlesource.com/dawn/+/refs/heads/main/docs/dawn/features/multi_planar_formats.md">Multi Planar Formats</a>.</p>

<p>The Chrome team has created an <a href="https://github.com/google/dawn">official GitHub repository for Dawn</a>.</p>

<p>This covers only some of the key highlights. Check out the exhaustive <a href="https://dawn.googlesource.com/dawn/+log/chromium/6099..chromium/6167?n=1000">list of commits</a>.</p>

<h2 id="whats-new" data-text="What's New in WebGPU" tabindex="-1">What's New in WebGPU</h2>

<p>A list of everything that has been covered in the <a href="https://developer.chrome.com/docs/web-platform/webgpu/news">What's New in WebGPU</a> series.</p>

<h3 id="chrome121" data-text="Chrome 121" tabindex="-1">Chrome 121</h3>

<ul>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-121#support-webgpu-on-android">Support WebGPU on Android</a></li>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-121#use_dxc_instead_of_fxc_for_shader_compilation_on_windows">Use DXC instead of FXC for shader compilation on Windows</a></li>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-121#timestamp_queries_in_compute_and_render_passes">Timestamp queries in compute and render passes</a></li>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-121#default_entry_points_to_shader_modules">Default entry points to shader modules</a></li>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-121#support_display-p3_as_gpuexternaltexture_color_space">Support display-p3 as GPUExternalTexture color space</a></li>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-121#memory_heaps_info">Memory heaps info</a></li>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-121#dawn_updates">Dawn updates</a></li>
</ul>

<h3 id="chrome120" data-text="Chrome 120" tabindex="-1">Chrome 120</h3>

<ul>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-120#support_for_16-bit_floating-point_values_in_wgsl">Support for 16-bit floating-point values in WGSL</a></li>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-120#push_the_limits">Push the limits</a></li>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-120#changes_to_depth-stencil_state">Changes to depth-stencil state</a></li>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-120#adapter_information_updates">Adapter information updates</a></li>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-120#timestamp_queries_quantization">Timestamp queries quantization</a></li>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-120#spring-cleaning_features">Spring-cleaning features</a></li>
</ul>

<h3 id="chrome119" data-text="Chrome 119" tabindex="-1">Chrome 119</h3>

<ul>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-119#filterable_32-bit_float_textures">Filterable 32-bit float textures</a></li>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-119#unorm10-10-10-2_vertex_format">unorm10-10-10-2 vertex format</a></li>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-119#rgb10a2uint_texture_format">rgb10a2uint texture format</a></li>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-119#dawn_updates">Dawn updates</a></li>
</ul>

<h3 id="chrome118" data-text="Chrome 118" tabindex="-1">Chrome 118</h3>

<ul>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-118#htmlimageelement_and_imagedata_support_in_copyexternalimagetotexture">HTMLImageElement and ImageData support in <code translate="no" dir="ltr">copyExternalImageToTexture()</code></a></li>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-118#experimental_support_for_read-write_and_read-only_storage_texture">Experimental support for read-write and read-only storage texture</a></li>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-118#dawn_updates">Dawn updates</a></li>
</ul>

<h3 id="chrome117" data-text="Chrome 117" tabindex="-1">Chrome 117</h3>

<ul>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-117#unset_vertex_buffer">Unset vertex buffer</a></li>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-117#unset_bind_group">Unset bind group</a></li>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-117#silence_errors_from_async_pipeline_creation_when_device_is_lost">Silence errors from async pipeline creation when device is lost</a></li>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-117#spir-v_shader_module_creation_updates">SPIR-V shader module creation updates</a></li>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-117#improving_developer_experience">Improving developer experience</a></li>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-117#caching_pipelines_with_automatically_generated_layout">Caching pipelines with automatically generated layout</a></li>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-117#dawn_updates">Dawn updates</a></li>
</ul>

<h3 id="chrome116" data-text="Chrome 116" tabindex="-1">Chrome 116</h3>

<ul>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-116#webcodecs_integration">WebCodecs integration</a></li>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-116#lost_device_returned_by_gpuadapter_requestdevice">Lost device returned by GPUAdapter <code translate="no" dir="ltr">requestDevice()</code></a></li>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-116#keep_video_playback_smooth_if_importexternaltexture_is_called">Keep video playback smooth if <code translate="no" dir="ltr">importExternalTexture()</code> is called</a></li>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-116#spec_conformance">Spec conformance</a></li>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-116#improving_developer_experience">Improving developer experience</a></li>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-116#dawn_updates">Dawn updates</a></li>
</ul>

<h3 id="chrome115" data-text="Chrome 115" tabindex="-1">Chrome 115</h3>

<ul>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-115#supported_wgsl_language_extensions">Supported WGSL language extensions</a></li>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-115#experimental_support_for_direct3d_11">Experimental support for Direct3D 11</a></li>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-115#get_discrete_gpu_by_default_on_ac_power">Get discrete GPU by default on AC power</a></li>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-115#improving_developer_experience">Improving developer experience</a></li>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-115#dawn_updates">Dawn updates</a></li>
</ul>

<h3 id="chrome114" data-text="Chrome 114" tabindex="-1">Chrome 114</h3>

<ul>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-114#optimize_javascript">Optimize JavaScript</a></li>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-114#getcurrenttexture_on_unconfigured_canvas_throws_invalidstateerror">getCurrentTexture() on unconfigured canvas throws InvalidStateError</a></li>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-114#wgsl_updates">WGSL updates</a></li>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-114#dawn_updates">Dawn updates</a></li>
</ul>

<h3 id="chrome113" data-text="Chrome 113" tabindex="-1">Chrome 113</h3>

<ul>
<li><a href="https://developer.chrome.com/blog/new-in-webgpu-113#use_webcodecs_videoframe_source_in_importexternaltexture">Use WebCodecs VideoFrame source in <code translate="no" dir="ltr">importExternalTexture()</code></a></li>
</ul>

  

  
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I made a website to share rejection letters (155 pts)]]></title>
            <link>https://rejectedagain.lol</link>
            <guid>39045545</guid>
            <pubDate>Thu, 18 Jan 2024 18:26:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rejectedagain.lol">https://rejectedagain.lol</a>, See on <a href="https://news.ycombinator.com/item?id=39045545">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__next"><header><a href="https://rejectedagain.lol/"><span>RejectedAGAIN.lol</span></a><nav><a href="https://rejectedagain.lol/upload"></a></nav></header><p><span>Page <!-- -->1</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mark Zuckerberg’s new goal is creating artificial general intelligence (193 pts)]]></title>
            <link>https://www.theverge.com/2024/1/18/24042354/mark-zuckerberg-meta-agi-reorg-interview</link>
            <guid>39045153</guid>
            <pubDate>Thu, 18 Jan 2024 18:05:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2024/1/18/24042354/mark-zuckerberg-meta-agi-reorg-interview">https://www.theverge.com/2024/1/18/24042354/mark-zuckerberg-meta-agi-reorg-interview</a>, See on <a href="https://news.ycombinator.com/item?id=39045153">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content"><p>Fueling the generative AI craze is a belief that the tech industry is on a path to achieving superhuman, god-like intelligence.</p><p>OpenAI’s stated mission is to create this artificial general intelligence, or AGI. Demis Hassabis, the leader of Google’s AI efforts, <a href="https://www.theverge.com/23778745/demis-hassabis-google-deepmind-ai-alphafold-risks">has the same goal</a>.</p><p>Now, Meta CEO Mark Zuckerberg is entering the race. While he doesn’t have a timeline for when AGI will be reached, or even an exact definition for it, he wants to build it. At the same time, <a href="https://www.theverge.com/e/23807588">he’s shaking things up by moving Meta’s AI research group</a>, FAIR, to the same part of the company as the team building generative AI products across Meta’s apps. The goal is for Meta’s AI breakthroughs to more directly reach its billions of users. </p><p>“We’ve come to this view that, in order to build the products that we want to build, we need to build for general intelligence,” Zuckerberg tells me in an exclusive interview. “I think that’s important to convey because a lot of the best researchers want to work on the more ambitious problems.”</p><div><form action="#"><div><h2>Command Line</h2><p> / <span>A newsletter from Alex Heath about the tech industry’s inside conversation.</span></p></div></form></div><p>Here, Zuckerberg is saying the quiet part aloud. The battle for AI talent has never been more fierce, with every company in the space vying for an extremely small pool of researchers and engineers. Those with the needed expertise can command eye-popping compensation packages to the tune of over $1 million a year. CEOs like Zuckerberg are routinely pulled in to try to win over a key recruit or keep a researcher from defecting to a competitor.</p><p>“We’re used to there being pretty intense talent wars,” he says. “But there are different dynamics here with multiple companies going for the same profile, [and] a lot of VCs and folks throwing money at different projects, making it easy for people to start different things externally.”</p><p>After talent, the scarcest resource in the AI field is the computing power needed to train and run large models. On this topic, Zuckerberg is ready to flex. He tells me that, by the end of this year, Meta will own more than 340,000 of Nvidia’s H100 GPUs — the industry’s <a href="https://www.theverge.com/2023/8/11/23828874/inside-the-hunt-for-ai-chips-command-line">chip of choice for building generative AI</a>. </p><div><p>“We have built up the capacity to do this at a scale that may be larger than any other individual company”</p></div><p><a href="https://www.theverge.com/2023/12/4/23987953/the-gpu-haves-and-have-nots">External research has pegged</a> Meta’s H100 shipments for 2023 at 150,000, a number that is tied only with Microsoft’s shipments and at least three times larger than everyone else’s. When its Nvidia A100s and other AI chips are accounted for, Meta will have a stockpile of almost 600,000 GPUs by the end of 2024, according to Zuckerberg. </p><p>“We have built up the capacity to do this at a scale that may be larger than any other individual company,” he says. “I think a lot of people may not appreciate that.”</p><p><h3>The realization</h3></p><p>No one working on AI, including Zuckerberg, seems to have a clear definition for AGI or an idea of when it will arrive.</p><p>“I don’t have a one-sentence, pithy definition,” he tells me. “You can quibble about if general intelligence is akin to human level intelligence, or is it like human-plus, or is it some far-future super intelligence. But to me, the important part is actually the breadth of it, which is that intelligence has all these different capabilities where you have to be able to reason and have intuition.”</p><p>He sees its eventual arrival as being a gradual process, rather than a single moment. “I’m not actually that sure that some specific threshold will feel that profound.”</p><p>As Zuckerberg explains it, Meta’s new, broader focus on AGI was influenced by <a href="https://www.theverge.com/2023/7/21/23803234/the-biggest-ai-release-since-chatgpt">the release of Llama 2</a>, its latest large language model, last year. The company didn’t think that the ability for it to generate code made sense for how people would use a LLM in Meta’s apps. But it’s still an important skill to develop for building smarter AI, so Meta built it anyway.</p><p>“One hypothesis was that coding isn’t that important because it’s not like a lot of people are going to ask coding questions in WhatsApp,” he says. “It turns out that coding is actually really important structurally for having the LLMs be able to understand the rigor and hierarchical structure of knowledge, and just generally have more of an intuitive sense of logic.”</p><div><p>“Our ambition is to build things that are at the state of the art and eventually the leading models in the industry”</p></div><p>Meta is training Llama 3 now, and it will have code-generating capabilities, he says. Like <a href="https://www.theverge.com/2023/12/6/23990466/google-gemini-llm-ai-model">Google’s new Gemini model</a>, another focus is on more advanced reasoning and planning abilities. </p><p>“Llama 2 wasn’t an industry-leading model, but it was the best open-source model,” he says. “With Llama 3 and beyond, our ambition is to build things that are at the state of the art and eventually the leading models in the industry.”</p><p><h3>Open versus closed</h3></p><p>The question of who gets to eventually control AGI is a hotly debated one, as the <a href="https://www.theverge.com/23966325/openai-sam-altman-fired-turmoil-chatgpt">near implosion of OpenAI</a> recently showed the world.</p><p>Zuckerberg wields total power at Meta thanks to his voting control over the company’s stock. That puts him in a uniquely powerful position that could be dangerously amplified if AGI is ever achieved. His answer is the <a href="https://www.theverge.com/2023/7/18/23799025/meta-ai-llama-2-open-source-microsoft">playbook that Meta has followed so far for Llama</a>, which can — at least <a href="https://www.theverge.com/2023/10/30/23935587/meta-generative-ai-models-open-source">for most use cases</a> — be considered open source.</p><p>“I tend to think that one of the bigger challenges here will be that if you build something that’s really valuable, then it ends up getting very concentrated,” Zuckerberg says. “Whereas, if you make it more open, then that addresses a large class of issues that might come about from unequal access to opportunity and value. So that’s a big part of the whole open-source vision.”</p><p>Without naming names, he contrasts Meta’s approach to that of OpenAI’s, which began with the intention of open sourcing its models but has becoming increasingly less transparent. “There were all these companies that used to be open, used to publish all their work, and used to talk about how they were going to open source all their work. I think you see the dynamic of people just realizing, ‘Hey, this is going to be a really valuable thing, let’s not share it.’”</p><p>While Sam Altman and others espouse the safety benefits of a more closed approach to AI development, Zuckerberg sees a shrewd business play. Meanwhile, the models that have been deployed so far have yet to cause catastrophic damage, he argues.</p><p>“The biggest companies that started off with the biggest leads are also, in a lot of cases, the ones calling the most for saying you need to put in place all these guardrails on how everyone else builds AI,” he tells me. “I’m sure some of them are legitimately concerned about safety, but it’s a hell of a thing how much it lines up with the strategy.”</p><div><p>“I’m sure some of them are legitimately concerned about safety, but it’s a hell of a thing how much it lines up with the strategy”</p></div><p>Zuckerberg has his own motivations, of course. The end result of his open vision for AI is <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4543807">still a concentration of power</a>, just in a different shape.&nbsp;Meta already has more users than almost any company on Earth and a wildly profitable social media business. AI features can arguably make his platforms even stickier and more useful. And if Meta can effectively standardize the development of AI by releasing its models openly, its influence over the ecosystem will only grow.</p><p>There’s another wrinkle: If AGI is ever achieved at Meta, the call to open source it or not is ultimately Zuckerberg’s. He’s not ready to commit either way. </p><p>“For as long as it makes sense and is the safe and responsible thing to do, then I think we will generally want to lean towards open source,” he says. “Obviously, you don’t want to be locked into doing something because you said you would.”</p><p><h3>Don’t call it a pivot</h3></p><p>In the broader context of Meta, the timing of Zuckerberg’s new AGI push is a bit awkward. </p><p>It has been only two years since he <a href="https://www.theverge.com/22749919/mark-zuckerberg-facebook-meta-company-rebrand">changed the company name</a> to focus on the metaverse. Meta’s latest smart glasses with Ray-Ban are <a href="https://www.theverge.com/23922425/ray-ban-meta-smart-glasses-review">showing early traction</a>, but full-fledged AR glasses feel increasingly further out. Apple, meanwhile, has recently validated his bet on headsets with the <a href="https://www.theverge.com/2024/1/8/24001858/apple-vision-pro-release-date-availability-price">launch of the Vision Pro</a>, even though VR is still a niche industry.</p><p>Zuckerberg, of course, disagrees with the characterization of his focus on AI being a pivot. </p><p>“I don’t know how to more unequivocally state that we’re continuing to focus on Reality Labs and the metaverse,” he tells me, pointing to the fact that Meta is still spending north of $15 billion a year on the initiative. Its Ray-Ban smart glasses <a href="https://www.theverge.com/2023/12/12/23998780/ray-ban-smart-glasses-hey-meta-multimodal-ai-features">recently added a visual AI assistant that can identify objects</a> and translate languages. He sees generative AI playing a more critical role in Meta’s hardware efforts going forward.</p><div><p>“I don’t know how to more unequivocally state that we’re continuing to focus on Reality Labs and the metaverse”</p></div><p>He sees a future in which virtual worlds are generated by AI and filled with AI characters that accompany real people. He says a new platform is coming this year to let anyone create their own AI characters and distribute them across Meta’s social apps. Perhaps, he suggests, these AIs will even be able to post their own content to the feeds of Facebook, Instagram, and Threads.</p><p>Meta is still a metaverse company. It’s the biggest social media company in the world. It’s now trying to build AGI. Zuckerberg frames all this around the overarching mission of “building the future of connection.” </p><p>To date, that connection has been mostly humans interacting with each other. Talking to Zuckerberg, it’s clear that, going forward, it’s increasingly going to be about humans talking to AIs, too. It’s obvious that he views this future as inevitable and exciting, whether the rest of us are ready for it or not.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New cancer drug kinder than chemotherapy (236 pts)]]></title>
            <link>https://www.bbc.com/news/health-67793887</link>
            <guid>39044985</guid>
            <pubDate>Thu, 18 Jan 2024 17:54:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/health-67793887">https://www.bbc.com/news/health-67793887</a>, See on <a href="https://news.ycombinator.com/item?id=39044985">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main id="main-content" data-testid="main-content"><article><header></header><div data-component="image-block"><figure><p><span><picture><source srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/17AC4/production/_132346969_ae5236e1-4ddd-49e7-8a69-41f4470d92c2.jpg.webp 240w, https://ichef.bbci.co.uk/news/320/cpsprodpb/17AC4/production/_132346969_ae5236e1-4ddd-49e7-8a69-41f4470d92c2.jpg.webp 320w, https://ichef.bbci.co.uk/news/480/cpsprodpb/17AC4/production/_132346969_ae5236e1-4ddd-49e7-8a69-41f4470d92c2.jpg.webp 480w, https://ichef.bbci.co.uk/news/624/cpsprodpb/17AC4/production/_132346969_ae5236e1-4ddd-49e7-8a69-41f4470d92c2.jpg.webp 624w, https://ichef.bbci.co.uk/news/800/cpsprodpb/17AC4/production/_132346969_ae5236e1-4ddd-49e7-8a69-41f4470d92c2.jpg.webp 800w, https://ichef.bbci.co.uk/news/976/cpsprodpb/17AC4/production/_132346969_ae5236e1-4ddd-49e7-8a69-41f4470d92c2.jpg.webp 976w" type="image/webp"><img alt="Arthur at home playing violin" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/17AC4/production/_132346969_ae5236e1-4ddd-49e7-8a69-41f4470d92c2.jpg 240w, https://ichef.bbci.co.uk/news/320/cpsprodpb/17AC4/production/_132346969_ae5236e1-4ddd-49e7-8a69-41f4470d92c2.jpg 320w, https://ichef.bbci.co.uk/news/480/cpsprodpb/17AC4/production/_132346969_ae5236e1-4ddd-49e7-8a69-41f4470d92c2.jpg 480w, https://ichef.bbci.co.uk/news/624/cpsprodpb/17AC4/production/_132346969_ae5236e1-4ddd-49e7-8a69-41f4470d92c2.jpg 624w, https://ichef.bbci.co.uk/news/800/cpsprodpb/17AC4/production/_132346969_ae5236e1-4ddd-49e7-8a69-41f4470d92c2.jpg 800w, https://ichef.bbci.co.uk/news/976/cpsprodpb/17AC4/production/_132346969_ae5236e1-4ddd-49e7-8a69-41f4470d92c2.jpg 976w" src="https://ichef.bbci.co.uk/news/976/cpsprodpb/17AC4/production/_132346969_ae5236e1-4ddd-49e7-8a69-41f4470d92c2.jpg" width="976" height="549" loading="eager"></picture></span></p><figcaption><span>Image caption, </span><p>Arthur is back at school and pursuing hobbies again</p></figcaption></figure></div><div data-component="text-block"><p><b>Some children with cancer are receiving a new type of drug treatment far less toxic than chemotherapy. </b></p></div><div data-component="text-block"><p>Arthur, 11, is one of the first to try it, at London's Great Ormond Street Hospital, for his blood cancer.</p></div><div data-component="text-block"><p>His family call the therapy "a little bit of sunshine", since it worked without making Arthur feel much sicker. </p></div><div data-component="text-block"><p>And because it could be given on the go, rather than just in hospital, he spent more time at home with his family, enjoying more of what he loves. </p></div><div data-component="text-block"><p>He carried it with him in a rucksack - his "blina backpack". </p></div><div data-component="text-block"><p>For Arthur, blinatumomab or blina was his only real option after his chemo had failed to clear all of his cancer and had left him very weak.</p></div><div data-component="image-block"><figure><p><span><span></span></span><span role="text"><span>Image source, </span>Sandrine Heutz</span></p><figcaption><span>Image caption, </span><p>Arthur with his blina backpack</p></figcaption></figure></div><div data-component="text-block"><p>Blina is already licensed to treat adults with cancer - and experts hope to show it can safely help children too. </p></div><div data-component="text-block"><p>Some 20 centres around the UK are using it off-label for children with B-cell acute lymphoblastic leukaemia (B-ALL). </p></div><div data-component="text-block"><p>The drug is an immunotherapy that seeks out cancer cells so the body's own immune system can recognise and destroy them. </p></div><div data-component="text-block"><p>And this death hunt is precisely targeted - healthy cells are untouched, unlike with chemo. </p></div><div data-component="image-block"><figure><p><span><span></span></span><span role="text"><span>Image source, </span>Sandrine Heutz</span></p><figcaption><span>Image caption, </span><p>Arthur starting his blina treatment</p></figcaption></figure></div><div data-component="text-block"><p>Blina comes in a bag of liquid administered through a thin plastic tube that remains running into a vein in the patient's arm for many months.</p></div><div data-component="text-block"><p>A battery-operated pump controls how quickly the drug trickles into the bloodstream - a bag can last days.</p></div><div data-component="text-block"><p>All of the kit can be carried in a backpack smaller than an A4 textbook, making it fully portable.</p></div><div data-component="text-block"><p>For Arthur, that meant he could do other things - like play on the swings in his local park - while the treatment was happening.</p></div><div data-component="text-block"><p>And unlike his intensive chemotherapy, which had stopped working anyway, it did not make him too weak to enjoy his days.</p></div><p data-component="subheadline-block"><h2 tabindex="-1" id="Constant-challenge"><span role="text">'Constant challenge'</span></h2></p><div data-component="text-block"><p>Like other patients on blina, Arthur was given medication to cut the chance of serious reactions or side effects before his infusion started.</p></div><div data-component="text-block"><p>At first, he had some bouts of fever and needed to stay in hospital for checks. </p></div><div data-component="text-block"><p>But shortly after, he was able to go home. </p></div><div data-component="text-block"><p>The backpack stayed with Arthur continuously, including in bed - and even though the pump makes a noise, he was able to have a decent night's sleep.</p></div><div data-component="text-block"><p>Chemo had been rough for Arthur and moving on to blina was a relief, his mother, Sandrine, said.</p></div><div data-component="text-block"><p>"It was completely out of his control - we were living in a constant challenge as his body was getting hit by the drugs," she said.</p></div><div data-component="text-block"><p>"We were curing him by making him feel worse - it's a very difficult thing to process."</p></div><p data-component="subheadline-block"><h2 tabindex="-1" id="Big-step"><span role="text">'Big step'</span></h2></p><div data-component="text-block"><p>Arthur had to return to hospital every four days so doctors could top up the blina kit but was able to manage the treatment at home the rest of the time.  </p></div><div data-component="text-block"><p>"He enjoyed the fact that he was able to hold it and be responsible - he embraced all of it," Sandrine said.  </p></div><div data-component="text-block"><p>And at the end of April 2023, Arthur had the final operation to remove the tubing from his arm.  </p></div><div data-component="text-block"><p>"It was a big step - he was free," Sandrine said.</p></div><div data-component="text-block"><p>Doctors say blina can replace big chunks of chemo - perhaps up to 80% of it. </p></div><div data-component="text-block"><p>About 450 children a year in the UK are diagnosed with Arthur's type of cancer.  </p></div><div data-component="image-block"><figure><p><span><span></span></span><span role="text"><span>Image source, </span>Sandrine Heutz</span></p><figcaption><span>Image caption, </span><p>Arthur with his parents during his chemo and steroid treatment</p></figcaption></figure></div><div data-component="text-block"><p>Chief investigator and consultant paediatric haematologist Prof Ajay Vora said: "Chemotherapies are poisons that kill the leukaemic cells but also kill and damage normal cells - and that is what causes their side effects. </p></div><div data-component="text-block"><p>"Blinatumomab is a gentler, kinder treatment."</p></div><div data-component="text-block"><p>Another targeted immunotherapy drug, chimeric antigen receptor T-cell therapy (CAR-T), has also recently become available.</p></div><div data-component="text-block"><p>But it is more expensive than blina and the patient's own cells must be taken and then altered in the lab before being given back as the medicine, which takes time. </p></div><div data-component="text-block"><p>Thanks to all the treatment, Arthur's cancer has now gone. </p></div><div data-component="text-block"><p>Sandrine said: "New Year was when we found out that the blina had worked and there was no residual cancer - and so that was just amazing and so we had double celebrations."</p></div><div data-component="text-block"><p><i>Additional reporting by Nicki Stiastny and Neil Paton</i></p></div><section data-component="links-block"><p><h2>More on this story</h2></p></section><section data-component="related-internet-links"><p><h2>Related Internet Links</h2></p><ul role="list" spacing="responsive"><li></li></ul><p>The BBC is not responsible for the content of external sites.</p></section></article></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Haier hits Home Assistant plugin dev with takedown notice (145 pts)]]></title>
            <link>https://www.bleepingcomputer.com/news/security/haier-hits-home-assistant-plugin-dev-with-takedown-notice/</link>
            <guid>39044932</guid>
            <pubDate>Thu, 18 Jan 2024 17:50:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bleepingcomputer.com/news/security/haier-hits-home-assistant-plugin-dev-with-takedown-notice/">https://www.bleepingcomputer.com/news/security/haier-hits-home-assistant-plugin-dev-with-takedown-notice/</a>, See on <a href="https://news.ycombinator.com/item?id=39044932">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><img alt="Haier" height="900" src="https://www.bleepstatic.com/content/hl-images/2024/01/18/haier.jpg" width="1600"></p>
<p>Appliances giant Haier issued a takedown notice to a software developer for creating Home Assistant integration plugins for the company's home appliances and releasing them on GitHub.</p>
<p>Haier is a multinational home appliances and consumer electronics corporation selling a wide range of products under the brands General Electric Appliances, Hotpoint, Hoover, Fisher &amp; Paykel, and Candy.</p>
<p>Earlier this week, German software developer Andre Basche, who maintains plugins for the Home Assistant integration for Haier's hOn smart control app, claimed to receive a legal threat demanding the immediate removal of his tools from the GitHub platform.</p>
<p>Home Assistant is an open-source home automation platform enabling users to control and automate their smart home devices from a centralized interface. Apart from convenience and cost, Home Assistant offers superior security and privacy options not available on similar commercial apps.</p>
<p>The plugins offered in the GitHub repositories enable users to control Haier, Candy, and Hoover air conditioners, purifiers, dishwashers, induction hobs, ovens, fridges, washing machines, and dryers through Home Assistant.</p>
<p>According to a notice published by the repository owner, Haier claims these plugins cause the firm significant financial damage and violate copyright laws, requiring the developer to take them down to avoid further legal action.</p>
<p>"We are writing to inform you that we have discovered two Home Assistant integration plug-ins developed by you (https://github.com/Andre0512/hon and https://github.com/Andre0512/pyhOn) that are in violation of our terms of service,"&nbsp;<a href="https://github.com/Andre0512/hOn" target="_blank" rel="nofollow noopener">reads the notice</a>&nbsp;from Haier Europe Security and Governance Department.</p>
<p>"Specifically, the plug-ins are using our services in an unauthorized manner, which is causing significant economic harm to our Company."</p>
<p>"We take the protection of our intellectual property very seriously and demand that you immediately cease and desist all illegal activities related to the development and distribution of these plug-ins."</p>
<div>
<figure><img alt="Full notice" height="535" src="https://www.bleepstatic.com/images/news/u/1220909/2024/Software/01/notice.png" width="903"><figcaption><strong>Takedown notice</strong> <em>(GitHub)</em></figcaption></figure></div>
<p>The letter eventually delivers a legal threat to the developer, saying that if he fails to comply with the removal request immediately, the firm will take necessary legal action to seek compensation for the damage done to its business.</p>
<p>The plugins themselves are open-source, but it is unclear if they incorporate Haier's intellectual property, such as software code or proprietary protocols, which would give the firm a legal basis for the request.</p>
<p>On the other hand, if the plugins do not infringe on Haier's intellectual property or fall under fair use provisions, the creator could opt to defend his work and keep the plugins available to the community.</p>
<p>Nonetheless, Haier's legal threats have intimidated the developer, who announced that the project will be taken down in the next couple of days.</p>
<p>Meanwhile, the situation has sparked support for the developer and&nbsp;backlash on Haier, with users calling consumers to boycott Haier, finding the firm's approach excessively aggressive.</p>
<p><a href="https://twitter.com/bennie_pie/status/1747252779712991527" target="_blank" rel="nofollow noopener"><img alt="Tweet" height="600" src="https://www.bleepstatic.com/images/news/u/1220909/2024/Software/01/tweet.png" width="445"></a></p>
<p>Targeting open-source software developers tends to backfire for companies, as others fork or clone the code repositories to prevent the projects from disappearing.</p>
<p>At this time, the Haier home assistant plugins have been forked 228 times, many occurring since the news of the legal threats.</p>
<p>BleepingComputer has contacted Haier with questions on the case, but a comment wasn't immediately available.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What happened to the US machine tool industry? (208 pts)]]></title>
            <link>https://www.construction-physics.com/p/what-happened-to-the-us-machine-tool</link>
            <guid>39044586</guid>
            <pubDate>Thu, 18 Jan 2024 17:25:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.construction-physics.com/p/what-happened-to-the-us-machine-tool">https://www.construction-physics.com/p/what-happened-to-the-us-machine-tool</a>, See on <a href="https://news.ycombinator.com/item?id=39044586">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>Machine tools – machines that cut or form metal – are the heart of industrial civilization. Sometimes called “mother machines” (because they’re machines that make other machines), machine tools are required to make almost everything. Nearly every manufactured good is made using machine tools, or by machines which were made using machine tools:</p><blockquote><p><em>“Thus an automobile is an assembly of metal parts made by machine tools, plastic parts produced by machines made by machine tools, fabric processed on textile machines made by machine tools, rubber processed and molded by equipment made on machine tools, and glass processed by equipment produced by machine tools.” – Anderson Ashburn, </em><span>Is New Technology Enough</span><em>?</em></p></blockquote><p><span>Being able to manufacture machine tools is often considered an important capability for an industrialized country. Not only does this provide ready access to the latest manufacturing technology, but it ensures production of munitions and other military equipment won’t be bottlenecked by a lack of machine tools. This isn’t a hypothetical concern: American production of artillery shells for Ukraine has been </span><a href="https://www.defenseone.com/threats/2023/03/us-artillery-production-ukraine-limited-lack-machine-tools-army-official-says/383615/" rel="">held back</a><span> by a lack of machine tools. The military has thus historically paid close attention to the </span><a href="https://apps.dtic.mil/sti/citations/tr/ADA205642" rel="">machine tool industry</a><span> and the </span><a href="https://www.businessdefense.gov/news/2021/dod-expands-program-to-strengthen-us-machine-tool-enterprise.html" rel="">availability of machinists</a><span>.</span></p><p>For most of the 20th century, the US was unrivaled in its machine tool technology, and as late as the early 1980s it was the largest machine tool producer in the world.. But almost overnight, the industry collapsed: annual machine tool shipments declined by more than 50% in 2 years, hundreds of machine tool companies went out of business, and the US slipped from the largest producer in the world to the 4th or 5th (depending on the year), roughly where it remains today.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8acf9753-df1a-4673-9ebe-7b7f3a766cb9_654x394.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8acf9753-df1a-4673-9ebe-7b7f3a766cb9_654x394.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8acf9753-df1a-4673-9ebe-7b7f3a766cb9_654x394.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8acf9753-df1a-4673-9ebe-7b7f3a766cb9_654x394.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8acf9753-df1a-4673-9ebe-7b7f3a766cb9_654x394.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8acf9753-df1a-4673-9ebe-7b7f3a766cb9_654x394.png" width="654" height="394" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8acf9753-df1a-4673-9ebe-7b7f3a766cb9_654x394.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:394,&quot;width&quot;:654,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8acf9753-df1a-4673-9ebe-7b7f3a766cb9_654x394.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8acf9753-df1a-4673-9ebe-7b7f3a766cb9_654x394.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8acf9753-df1a-4673-9ebe-7b7f3a766cb9_654x394.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8acf9753-df1a-4673-9ebe-7b7f3a766cb9_654x394.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>What happened to the US machine tool industry? Why did it fall so far so quickly? Let’s take a look.</p><p><span>Machine tools include metal cutting machines like </span><a href="https://en.wikipedia.org/wiki/Lathe" rel="">lathes</a><span>, </span><a href="https://en.wikipedia.org/wiki/Milling_(machining)" rel="">milling machines</a><span>, drill presses, grinding machines, as well as things like wire-bending machines, stamping machines, die-casting presses, and forging machinery.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-140809402" href="https://www.construction-physics.com/p/what-happened-to-the-us-machine-tool#footnote-1-140809402" target="_self" rel="">1</a></span><span> Tools like lathes and drills have been in use for thousands of years, but modern machine tools – “</span><a href="https://www.amazon.com/New-Technology-Enough-Remaking-Industries/dp/0844736597" rel="">power-driven machines with methods of control permitting some degree of precision</a><span>” – first appeared in Britain at the beginning of the industrial revolution. Watt’s famous steam engine was made possible by the precision boring machine built by John Wilkinson to precisely bore the cylinders for it.</span></p><p><span>In the US, machine tools were a critical component of the “American System of Manufactures” – interchangeable or near-interchangeable parts produced using a series of specialized machines. This method of manufacturing was first developed for the production of guns (and was thus sometimes known as “armory practice”), and gradually spread to America’s burgeoning industries: sewing machines, </span><a href="https://eh.net/book_reviews/the-baldwin-locomotive-works-1831-1915/" rel="">locomotives</a><span>, and especially </span><a href="https://www.amazon.com/American-System-Mass-Production-1800-1932/dp/080183158X" rel="">bicycles</a><span>. By the turn of the 20th century, thanks to decades of innovation the US was producing the most advanced machine tools in the world.</span></p><p><span>US technological leadership continued with the rise of the automobile, which created an unprecedented demand for machine tools. As early as 1910, the car industry made up 30% of machine tool sales, which only increased as car sales </span><a href="https://www.construction-physics.com/p/how-the-car-came-to-la" rel="">exploded</a><span> and car manufacturers adopted mass-production methods (annual car sales went from less than 200,000 in 1910 to 3.7 million in 1925). The car industry required enormous numbers of machine tools (Ford alone eventually had more than 50,000 machine tools in its </span><a href="https://www.assemblymag.com/articles/83966-special-section-the-rouge-an-industrial-icon#:~:text=Located%20a%20few%20miles%20south,53%2C000%20machine%20tools." rel="">Rouge assembly plant</a><span>, roughly as many machine tools as the entire US made in a year), but the demands of car manufacturing pushed machine tool technology forward. Making cars required heavier, more accurate machine tools that could produce more accurate gears, better ball bearings, smoother finishes, and tighter tolerance parts. Machine tool technology was also advanced by the rise of electrification, which enabled machines driven by a series of belts and drive shafts to a central steam engine to be replaced by machines driven by their own electric motor. Between 1910 and 1929, machine tool sales nearly quadrupled, and machines continued to get more and more productive.</span></p><p>Machine tool sales were dragged down by the Depression, but bounced back as production of munitions and military hardware for WWII ramped up (the war also marked the birth of the aerospace industry as the second great consumer of machine tools after the car industry). Annual shipments of machine tools went from 50,000 in 1929, to 5,500 in 1932, to 300,000 by 1942. Over the course of the war, the US produced 800,000 machine tools, and the US emerged from WWII as the largest producer of machine tools in the world.</p><p>The machine tool industry was shaped by extreme cyclicality: orders for new machine tools might fluctuate 90% year-over-year. This was driven by what’s sometimes known as the “accelerator effect”: a small change in the demand for final goods can cause a large change in the demand for the machines that produce those goods:</p><blockquote><p><em>Consider a company that manufactures its product on lathes. Assume that it has ten lathes that can just meet production requirements. Assume also that each lathe wears out in ten years and that the company has its investment plans so well organized that one lathe is replaced every year. Now consider what will happen if there is a 10 percent increase in demand for the product. The plant will need eleven lathes to meet production requirements. It will have to buy two lathes: one as a replacement for the worn-out lathe and a second to increase capacity. Thus a 10 percent increase in the demand for the product has produced a 100 percent increase in the demand for lathes. Of course, the accelerator effect is also felt in the other direction. If the demand for the product declines 10 percent, the company will need only nine lathes. In view of the reduced sales volume, it will almost certainly not replace the lathe that has worn out. Thus a 10 percent decline in demand for the product has caused a 100 percent decrease in the demand for lathes. In the real world the relationships are not as neat as in this hypothetical case, but the accelerator effect is a very real factor. –</em><span> Is New Technology Enough?</span></p></blockquote><p><span>This accelerator effect is exacerbated by the fact that machine tools, properly maintained, can last for decades or even longer (the US is still using the </span><a href="https://en.wikipedia.org/wiki/Alcoa_50,000_ton_forging_press" rel="">50,000 ton forging press</a><span> built in 1955 as part of the </span><a href="https://en.wikipedia.org/wiki/Heavy_Press_Program" rel="">heavy press program</a><span>, for instance). An owner of a machine tool can thus put off replacing it almost indefinitely if demand for the product that requires it is uncertain.</span></p><p><span>The machine tool industry was wracked by </span><a href="https://fred.stlouisfed.org/series/M0169BUSM174NNBR#" rel="">such fluctuations</a><span> in the post-war period. It entered a four-year slump at the end of the war, as wartime demand fell off and the US government sold off huge numbers of surplus machines at rock-bottom prices. Sales then picked back up during the Korean War, fell off again when the armistice was signed, then picked back up, then declined again in the recession of 1958. But the US remained at the technological forefront during this period, and continued to lead the world in machine tool production.</span></p><p>In the 1960s, the machine tool industry underwent several major shifts.</p><p>The first was the development of numerical control. Prior to the 1950s, there were generally two ways that a machine tool could be controlled. A skilled machinist could manually move the tool and/or the part to be machined to cut it into the proper shape, drill the proper holes, and so on. Alternately, a machine tool could be mechanically automated. Ford, in pursuit of ever-greater efficiencies on the Model T assembly line, built and acquired dozens of different automated and semi-automated machine tools for tasks such as making screws, boring piston heads, and drilling engine blocks.</p><p><span>This sort of automatic machinery had been around since the 19th century, but it was somewhat fixed in the tasks it could perform. The machines couldn’t easily be changed to produce something else without substantially rebuilding the machine (these sorts of difficulties are why Ford had to shut the Rouge factory down for 6 months to retool when changing from the Model T to the Model A, and why auto producers were so interested in robotic welding).</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2-140809402" href="https://www.construction-physics.com/p/what-happened-to-the-us-machine-tool#footnote-2-140809402" target="_self" rel="">2</a></span></p><p>This changed with the development of numerical control, or NC. With NC, the movements of a machine could be encoded on a paper tape or a punched card. Changing the movements of the machine (and thus what the machine produced) just required feeding the machine a new tape with a different set of instructions.</p><p><span>Numerical control was first developed by MIT’s Servomechanism Lab in conjunction with the Air Force in the 1950s. Early on, the technology had little commercial appeal: it was expensive, complex, unreliable, and designed to solve specific complex fabrication problems in aerospace manufacturing, rather than the simpler machining needs that made up the bulk of machine tool demand. NC technology is sometimes described as “developed from the roof instead of from the foundation”. But that started to change by the early 1960s: machine tool manufacturers began to produce more reliable machines designed for broader commercial appeal (though the technology remained very expensive). In 1958, Kearney and Trecker introduced its </span><a href="https://www.youtube.com/watch?v=Y3YrbEGWE04&amp;t=211s" rel="">Milwaukee-matic</a><span>, the first of what became an entirely new category of machine tool, the machining center. A machining center contained a tool-changer with dozens of different specialized cutting tools, and could be programmed via numerical control to perform a series of machining operations, automatically changing to different cutting tools as required. This greatly increased efficiency: a typical machine tool was only operational 25-30% of the time, with much of the rest of the time spent setting up the machine and prepping the part to be worked. But Kearney and Trecker boasted that the Milwaukee-matics were operational more than 75% of the time. And because of its ability to flexibly mount different cutting tools quickly and automatically, a machining center could perform tasks which previously required many different machines.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F215668a8-df95-43b9-8dd8-4128c508df8c_800x512.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F215668a8-df95-43b9-8dd8-4128c508df8c_800x512.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F215668a8-df95-43b9-8dd8-4128c508df8c_800x512.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F215668a8-df95-43b9-8dd8-4128c508df8c_800x512.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F215668a8-df95-43b9-8dd8-4128c508df8c_800x512.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F215668a8-df95-43b9-8dd8-4128c508df8c_800x512.png" width="800" height="512" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/215668a8-df95-43b9-8dd8-4128c508df8c_800x512.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:512,&quot;width&quot;:800,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F215668a8-df95-43b9-8dd8-4128c508df8c_800x512.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F215668a8-df95-43b9-8dd8-4128c508df8c_800x512.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F215668a8-df95-43b9-8dd8-4128c508df8c_800x512.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F215668a8-df95-43b9-8dd8-4128c508df8c_800x512.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Milwaukee-matic in the late 50s or early 60s.</figcaption></figure></div><p>Early NC machines used vacuum tubes, and required large bulky control cabinets as large as the machine tool itself. But NC rode the wave of rapidly improving computer technology: vacuum tubes and paper tapes were replaced by transistor-based minicomputers, and then even smaller microprocessor-based computers, resulting in Computer Numerical Control, or CNC.</p><p>NC was adopted slowly: by 1968, just 10% of US metalworking plants had an NC machine, and they made up just 0.5% of total machine tools in the US. But it was clearly becoming a manufacturing mainstay: already by 1968 NC made up 20% of new machine tool sales in the US, and by 1983 had risen to over 40%.</p><p>The second major change to the machine tool industry was its ownership. Historically, machine tool companies were small operations run by people (often descendants of the original founders) who had spent their whole life in the machine tool industry and knew it inside and out. Because of the cyclical nature of the industry, machine tool firms tended to be managed conservatively, and they operated with little debt.</p><p>But in the 1960s, machine tool companies began to be acquired by the latest fashion in US business, the conglomerate: enormous, diversified companies that seemed to make anything and everything as long as there was profit in it. The machine tool industry was on a sales upswing that showed no signs of slowing, and machine tool firms' high profitability (and low debts) made them attractive acquisitions for the conglomerates. Additionally, the rise of NC technology made the industry seem sexy and futuristic, and many large conglomerates (like Teledyne, Litton Industries, and Bendix) had defense and aerospace divisions, to which machine tools seemed like a natural addition.</p><p><span>The third major shift in the machine tool industry was the rise of foreign producers, especially Japan. Japan’s machine tool industry had been devastated by the war and the immediate aftermath (when many of Japan’s machine tools were shipped to China and the Philippines as war reparations), and by 1955 Japanese tools were just 0.5% of world exports. But Japan was eager to become a major producer of machine tools, and its manufacturers quickly became more capable. Between 1955 and 1960 Japanese machine tool production rose by a factor of 15, and in 1960 </span><em>American Machinist</em><span> magazine noted that “Japanese machines for the first time appear to merit recognition and to be competitive with machines of the most advanced industrial nations.” By the end of the 1960s, imports remained a small fraction of the US market (around 10%), but were gaining momentum: between 1964 and 1967, Japanese machine tool exports to the US rose over 1200%.</span></p><p>From the late 1960s through the early 1980s, these trends coalesced in a way that proved disastrous for the US machine tool industry.</p><p>Japan had jumped on the NC bandwagon quickly, and began manufacturing NC tools as early as the 1950s. Their progress was aided by dozens of American machine tool firms that were all too willing to license their technology to Japanese producers. (Japanese companies were also not above simply illicitly duplicating American designs).&nbsp; By the end of the 1960s, Japanese machine tool manufacturers were exporting NC tools to the US.</p><p>With NC, US machine tool makers had primarily focused on the high end of the market, using advances in computers and other technology to create increasingly precise machines capable of tracing complex paths, and tailoring the machines to the needs of individual customers (as was common in the US machine tool industry). This focus was in part due to the guiding hand of the air force and the aerospace industry, which demanded these types of machines for production of advanced parts. Japanese builders, on the other hand, developed simpler, cheaper NC machines based on standardized designs, aimed at the large, untapped lower end of the market that previously couldn’t justify the cost of an expensive NC machine. And a focus on simpler, standardized designs, along with the traditional Japanese focus on quality, also meant that Japanese tools tended to be more reliable than US tools.</p><p>One example of Japanese standardization was controllers, the electronic components used to control the movement of the machine tools. In the US, there were many different suppliers of controllers, and many machine tool builders (like Cincinnati Milacron, Pratt and Whitney, and Warner and Swasey) offered their own, custom-built controllers to try and achieve a competitive advantage. In Japan, on the other hand, the market quickly became dominated by a single producer, FANUC. Not only were Japanese controllers cheaper, but a single standard likely meant that programming the machines and training people to use them was greatly simplified, as there was no re-learning process involved when switching to a different type of control.</p><p>Not only were the Japanese building more reliable, less expensive tools than the US builders, but they were delivering them faster. Historically, machine tool companies had dealt with the cyclicality of the industry by acquiring a large backlog of orders, then working it down during the lean times when new orders slowed. When a customer ordered a new machine tool, it might take a year or two (or more) for it to be delivered. One US manufacturer of lathes noted that it built just one size of lathe a month: if you ordered a lathe in February that was only made in January, you’d have no choice but to wait a year for them to build it again. But Japanese builders, with their focus on standardized machine designs, could deliver machine tools in weeks:</p><blockquote><p><em>“A Pennsylvania company sorely needed a cylindrical grinder, but was told that delivery of a $50,000 American machine would take at least a year. So the company decided to order from a distributor of Japanese tools. Within weeks, it had two Japanese grinders in operation for almost the same price.” – Max Holland, </em><span>When The Machine Stopped</span></p></blockquote><p>As Japanese machine tools were advancing, US firms were struggling. Though the conglomerates enjoyed owning machine tool firms in the boom times when profits were high, they quickly became disillusioned during the lean periods when new orders slowed to a trickle. The new owners’ focus on profits meant that they were reluctant to make the long-term investments in R&amp;D or new equipment needed to keep the firms competitive. Between 1968 and 1978, the book value of assets of machine tool companies declined by nearly 50% in real terms.</p><p>Conglomerates were often quick to divest their machine tool holdings when the companies began to struggle, resulting in frequent management changes and organizational whiplash as the firms tried to find their footing under new ownership. Storied machine tool manufacturer Warner and Swasey, for instance, was acquired by Bendix in 1980, which then sold it to Allied Corporation in 1983 as part of an effort to fight a hostile takeover from Martin Marietta. Just one year later in 1984, Allied then sold its machine tool group, including Warner and Swasey, to Cross and Trecker, which in turn was bought by Giddings and Lewis in 1991, which then decided to shut down Warner and Swasey the next year. This sort of management shuffling often took place at conglomerates even when ownership didn’t change, as the companies reorganized internally in the face of losses to try and right what they saw as a sinking ship.</p><p><span>Constant pressure to hit quarterly performance targets meant that machine quality often suffered. In some cases, machines would be shipped out the door unfinished so the delivery could be booked, and assembly would be completed by service technicians at the customer’s location. In his </span><a href="https://www.amazon.com/American-Industry-Personal-Perspective-EARM-OTH-240-CCS-CG/dp/5967203362" rel="">history</a><span> of the American machine tool industry, Albert Albrecht states that “the actions of these larger corporations and conglomerates, under the leadership of financial MBA’s, perhaps more than any other factor, contributed to the restructuring and decline of the US machine tool industry at the end of the 20th century.”</span></p><p><span>Amidst these struggles, US machine tool builders fell behind technologically. Not only did they lag in the design of low-cost NC machines, but they were well behind Japan in the adoption of microprocessor-based controllers. A 1983 </span><a href="https://www.amazon.com/Competitive-Status-U-S-Machine-Industry/dp/0309033942" rel="">National Research Council Report</a><span> summarized the situation:</span></p><blockquote><p><em>“While many in the industry are convinced that U.S. machine tool technology is presently equal to the most advanced level attained by any overseas competitor, the panel believes that the United States may now have slipped behind the Japanese on control technology and on the software to support such control systems. Additionally, the panel expressed concern that the Japanese are also more advanced in the general development of machining systems that allow round-the-clock operation with minimum human attention.”</em></p></blockquote><p>And Japan wasn’t the only country making progress. By 1970, Germany (which had always led the world in machine tool exports) had become the world’s largest machine tool producer (though the US would eclipse it again several years later). And between 1965 and 1980, Italy’s share of world machine tool output tripled. As late as 1973, imports of machine tools were just 10% of the US market, but by the end of the 1970s they had reached 22%.</p><p>These trends came to a head in the recession of 1982. Machine tool sales dropped precipitously, falling from $5 billion in 1981 to just $2 billion in 1983. But this time, sales didn’t bounce back. By now, tools from Japan and other countries were as good as or better than US tools, not to mention cheaper and more reliable. A strong dollar made imports even cheaper by comparison, and high interest rates reduced the demand for capital investment. US manufacturers found themselves with uncompetitive products in a highly competitive market. Whereas imports had been just 10% of the US market in 1973, by the late 1990s they were closer to 60%, and by 1986 75% of NC machines were imports. Japan alone made up 50% of the US NC market in the late 1980s.</p><p>The US industry collapsed almost overnight. From 1981 to 1983 employment in machine tools declined by a third, and continued to fall (by 1991 it was half its 1981 level). Between 1982 and 1987, half of US machine tool firms closed their doors. The US fell from the largest machine tool builder in the world to third behind Japan and Germany, and continued to decline: by 2002, it had dropped to 5th behind Italy and China, where it remains today.</p><p><span>The industry tried to get some of the same protectionist measures industries like steel had secured. Houdaille, an industrial conglomerate with a large machine tool division, </span><a href="https://www.nytimes.com/1983/04/27/business/houdaille-rebuffed-on-import-petition.html" rel="">attracted</a><span> a great deal of attention with a petition to have the investment tax credit denied to Japanese machine tools. But the machine tool industry had much less success with its pleas for protectionism than the steel industry; Houdaille’s petition was denied, and while the industry was eventually able to get some Voluntary Export Restraints (which lasted from the late 80s to the early 90s), it did little to change the overall trends in the industry.</span></p><p><span>Today, the US competes in a machine tool market that continues to be dominated by Japan, Germany, and now China. It has some bright spots, such as </span><a href="https://www.toolingandproduction.com/features/2008_February/0208_cover_story.aspx" rel="">Haas Automation</a><span> (founded in 1983, in the ashes of the industry’s collapse), but the major producers are all foreign companies. As of 2014, not a single one of the 10 largest machine tool companies in the world was a US company (Haas clocked in at number 13), a fact which as far as I know remains true today. The US is still a major </span><em>purchaser</em><span> of machine tools (2nd in the world behind China), but unlike for most of the 20th century, today its factories are full of machines made elsewhere.</span></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The case for single-stair multifamily (272 pts)]]></title>
            <link>https://www.thesisdriven.com/p/the-case-for-single-stair-multifamily</link>
            <guid>39043956</guid>
            <pubDate>Thu, 18 Jan 2024 16:41:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.thesisdriven.com/p/the-case-for-single-stair-multifamily">https://www.thesisdriven.com/p/the-case-for-single-stair-multifamily</a>, See on <a href="https://news.ycombinator.com/item?id=39043956">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55818632-6bb1-426d-ac4a-56cdaff714c9_2048x1365.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55818632-6bb1-426d-ac4a-56cdaff714c9_2048x1365.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55818632-6bb1-426d-ac4a-56cdaff714c9_2048x1365.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55818632-6bb1-426d-ac4a-56cdaff714c9_2048x1365.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55818632-6bb1-426d-ac4a-56cdaff714c9_2048x1365.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55818632-6bb1-426d-ac4a-56cdaff714c9_2048x1365.jpeg" width="1456" height="970" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/55818632-6bb1-426d-ac4a-56cdaff714c9_2048x1365.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:970,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:741635,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55818632-6bb1-426d-ac4a-56cdaff714c9_2048x1365.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55818632-6bb1-426d-ac4a-56cdaff714c9_2048x1365.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55818632-6bb1-426d-ac4a-56cdaff714c9_2048x1365.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55818632-6bb1-426d-ac4a-56cdaff714c9_2048x1365.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption><span>Barcelona, Europe's densest city, is thickly built out with single-stair apartment buildings on small lots. Picture via </span><a href="https://www.flickr.com/photos/oh-barcelona/7393006780" rel="">Oh Barcelona</a><span>.</span></figcaption></figure></div><p>Housing advocates celebrated a string of victories in 2023, including the legalization of missing middle housing and ADUs as well as permitting process reforms. But while most YIMBY attention has been focused on changes to zoning, building code reform may offer an even bigger opportunity to unlock housing construction. And bringing the US in line with its European and Asian peers by legalizing single-stair multifamily buildings is perhaps the most significant of those proposed reforms.</p><p><span>Today’s Thesis Driven is a guest letter by </span><strong>Stephen Smith</strong><span>, the Executive Director of the Center for Building in North America. The Center for Building is perhaps the most interesting under-the-radar policy group in housing today, as they do the hard work of comparing the wonkiest bits of building code across jurisdictions, looking at safety as well as cost outcomes.  The letter explores:</span></p><ul><li><p>The history and origins of current egress regulations;</p></li><li><p>Regulatory impacts on design and affordability;</p></li><li><p>How the US’s international peers approach fire safety;</p></li><li><p>Ongoing attempts to reform US code.</p></li></ul><p>Read on for more from Smith.  </p><p><span>One evening in 1860, a fire broke out in a basement bakery in a six-story tenement at 142 Elm Street, today’s Lafayette Street in Lower Manhattan. Fire and smoke spread up the building, home to at least 20 families, the fire fed in part by the building’s only exit: a wood staircase. Firefighters were able to rescue people up to the fourth floor, but their ladders could not reach any higher. “As the firemen stood on the ladders,” </span><a href="https://www.nytimes.com/1860/02/03/archives/calamitous-fire-tenement-house-on-elmstreet-destroyed-thirty.html" rel="">wrote the </a><em><a href="https://www.nytimes.com/1860/02/03/archives/calamitous-fire-tenement-house-on-elmstreet-destroyed-thirty.html" rel="">New York Times</a></em><span> the next day, “they could see many women and children lying prostrated on the floor, surrounded by the flames, which rendered all attempts to approach them ineffectual.”</span></p><p>For an American city in the 19th century, the fire was actually quite small. By then New York City had already had three great fires since 1776, and Chicago would burn 11 years later. Even into the 20th century, American cities would still not be safe from great fires, with San Francisco burning in the aftermath of the 1906 earthquake.</p><p><span>Across the Atlantic Ocean, great urban fires were largely a solved problem by the 19th century. After the Great Fire of London of 1666, Parliament did what European governments had been doing since at least Ancient Rome: they </span><a href="http://www.tara.tcd.ie/bitstream/handle/2262/86169/Final%20Dissertation%20.pdf?sequence=1&amp;isAllowed=y" rel="">limited the use of wood in buildings</a><span>. Without fuel from the structure of buildings themselves, fires still broke out in Europe, but their damage was limited, and great urban fires by the 19th century were mostly confined to smaller or poorer cities on the continent’s edge that hadn’t yet made the transition to brick, stone, metal, and concrete.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F33e610f0-2643-4ebe-a3f8-5580d2bd944e_2500x1381.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F33e610f0-2643-4ebe-a3f8-5580d2bd944e_2500x1381.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F33e610f0-2643-4ebe-a3f8-5580d2bd944e_2500x1381.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F33e610f0-2643-4ebe-a3f8-5580d2bd944e_2500x1381.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F33e610f0-2643-4ebe-a3f8-5580d2bd944e_2500x1381.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F33e610f0-2643-4ebe-a3f8-5580d2bd944e_2500x1381.jpeg" width="1456" height="804" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/33e610f0-2643-4ebe-a3f8-5580d2bd944e_2500x1381.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:804,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1210864,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F33e610f0-2643-4ebe-a3f8-5580d2bd944e_2500x1381.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F33e610f0-2643-4ebe-a3f8-5580d2bd944e_2500x1381.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F33e610f0-2643-4ebe-a3f8-5580d2bd944e_2500x1381.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F33e610f0-2643-4ebe-a3f8-5580d2bd944e_2500x1381.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>19th century Manhattan tenements, with a primitive second exit installed in the form of a fire escape. Photograph by Sean Litchfield.</figcaption></figure></div><p><span>Not so in North America. The continent’s thick woods, fast growth, and rural (and later suburban) character meant that the United States and Canada has to this day never fully moved away from wood-frame construction. Instead, just days after 142 Elm Street burned, the jury that convicted the building owner </span><a href="https://www.nytimes.com/1860/02/07/archives/the-late-catastrophes-investigations-before-coroners-juries-verdict.html" rel="">issued a call</a><span> for “the enactment of a law making it incumbent upon owners of tenement buildings to place iron stairways, or some other approved means of egress, on the outside of these structures.” Soon after the state legislature followed through on their request, and the requirement to provide a second means of egress in apartment buildings was born.</span></p><p><span>Today, the flimsy fire escapes that satisfied officials in the 1860s have morphed into something much more. Two stairs are required for new apartment buildings above three stories by America’s model International Building Code (whose claim to internationality comes largely from a handful of tiny island nations). They must sit on opposite ends of a building, and they must be fully enclosed. The standards are not unheard of in Europe and Asia, but they are typically </span><a href="https://secondegress.ca/Jurisdictions" rel="">only applied to true high-rises</a><span> –&nbsp;above around 20 stories in Singapore, or 25 stories in Italy (and even then, the required distance between the two stairs is not as large as in the US).</span></p><p><span>As a legacy code requirement, there has never been much modern analysis of America’s two-stair requirement and its impact on fire safety outcomes, but the United States in general does quite poorly on these measures. Almost every country in Western Europe—where single-stair apartment buildings can rise many times the IBC’s three-story height limit—has </span><a href="https://www.ctif.org/sites/default/files/2022-08/CTIF_Report27_ESG_0.pdf" rel="">fewer fire deaths per capita than the US</a><span>. New York City, which allows single-stair buildings up to six stories (and has many apartment buildings where the second exit is a flimsy fire escape that does not meet modern code requirements), has slightly fewer deaths per capita than the rest of the US. Since the second exit code requirement was first introduced over 150 years ago, the field of fire safety has seen countless more sophisticated innovations, from passive measures like enclosed stairways and fire-resistance-rated materials, to more technologically advanced systems like smoke detectors and fire sprinklers.</span></p><p><span>Despite the questionable effect on fire safety outcomes, North America’s unusual second stair requirements have an outsized impact on the design of multifamily buildings. The rules make it almost impossible to efficiently recreate the traditional design of apartment buildings, </span><a href="https://larchlab.com/city-of-vancouver-report-on-point-access-blocks/" rel="">termed “point access blocks” by Seattle architect Michael Eliason</a><span>—a few units around a single stair, with maybe an elevator. The common hallway that must connect the two stairs in a modern American code-compliant building cuts the structure in two, cutting off the possibility of floor-through apartments found in traditional American multifamily architecture like the New York City tenement or the Los Angeles dingbat.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15163e4d-fbca-4140-8b55-2a74dea461fa_1600x1200.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15163e4d-fbca-4140-8b55-2a74dea461fa_1600x1200.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15163e4d-fbca-4140-8b55-2a74dea461fa_1600x1200.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15163e4d-fbca-4140-8b55-2a74dea461fa_1600x1200.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15163e4d-fbca-4140-8b55-2a74dea461fa_1600x1200.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15163e4d-fbca-4140-8b55-2a74dea461fa_1600x1200.png" width="1456" height="1092" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/15163e4d-fbca-4140-8b55-2a74dea461fa_1600x1200.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1092,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15163e4d-fbca-4140-8b55-2a74dea461fa_1600x1200.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15163e4d-fbca-4140-8b55-2a74dea461fa_1600x1200.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15163e4d-fbca-4140-8b55-2a74dea461fa_1600x1200.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15163e4d-fbca-4140-8b55-2a74dea461fa_1600x1200.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>The typical American 5-over-1 fulfills the building code requirement for two exits, but requires a large lot and does not typically provide many units larger than a one-bedroom. Photo from </span><a href="https://archplanbaltimore.blogspot.com/2015/03/how-one-plus-five-is-shaping-american_27.html" rel="">ArchPlan Inc</a><span>.</span></figcaption></figure></div><p>Small plots of land, like the 25- to 75-foot wide lots that most American cities and inner suburbs have been carved up into, have become less feasible to develop. On a 25-foot-wide lot in Jersey City, NJ—just across the Hudson River from Manhattan—the second stair consumes 7 percent of the total floor area, with a few additional percentage points shifted from rentable square footage to un-rentable common area. It ends up all but eliminating windows in two bedrooms, giving them access to just a minuscule air shaft. A developer might be able to stomach it in Jersey City, but in more ordinary markets, without the housing shortage of the New York area, such lots would be rendered unbuildable above three stories.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0244318-0d7b-4207-82ee-fef4cfbb48bc_1370x522.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0244318-0d7b-4207-82ee-fef4cfbb48bc_1370x522.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0244318-0d7b-4207-82ee-fef4cfbb48bc_1370x522.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0244318-0d7b-4207-82ee-fef4cfbb48bc_1370x522.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0244318-0d7b-4207-82ee-fef4cfbb48bc_1370x522.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0244318-0d7b-4207-82ee-fef4cfbb48bc_1370x522.png" width="1370" height="522" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f0244318-0d7b-4207-82ee-fef4cfbb48bc_1370x522.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:522,&quot;width&quot;:1370,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0244318-0d7b-4207-82ee-fef4cfbb48bc_1370x522.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0244318-0d7b-4207-82ee-fef4cfbb48bc_1370x522.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0244318-0d7b-4207-82ee-fef4cfbb48bc_1370x522.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0244318-0d7b-4207-82ee-fef4cfbb48bc_1370x522.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Floor plan of a new building in Jersey City, NJ. Drawing by Alfred Twu.</figcaption></figure></div><p>The double-loaded corridor that meets modern American building code requirements can provide studio and one-bedroom apartments just fine, since these have a low ratio of living and bedroom space (which must, at least traditionally, have access to natural light and fresh air) to kitchen and bathroom space (which can be placed up against the dark common corridors). But when architects try to design apartments with two or more bedrooms, the apartments balloon in size, since every 10-foot-wide bedroom ends up coming with 30 or more feet in unit depth. Try to add a single 120-square foot bedroom and you’ll end up having to add another 180 square feet on top that you need to fill some other way—probably with walk-in closets and en-suite bathrooms, adding the expense of more plumbing, fixtures, and tiles. </p><p>Each square foot of space costs just as much to build whether it has access to light or not, so the rent that developers must achieve for these two-, three-, or—god forbid—four-bedroom apartments balloons far beyond the ability or willingness of families to pay. And if renters or condo buyers don’t like it, they can just drive till they qualify for a single-family house in the suburbs.</p><p><span>Without the requirement for a second stair, buildings can be laid out in a fundamentally more efficient way. With less vertical circulation, the circulation core can simply be repeated a few different times, with apartments of different sizes arrayed off of each core, potentially stretching from the front of the building to the back. If planners redraw zoning envelopes to accommodate thinner buildings, more bedrooms can be packed in less square footage, offering more affordable and competitive family-sized designs. With units that appeal more to families (in New York City, condo buildings average 1,189 gross sq. ft., compared to 838 sq. ft. for rentals) and reform of defect liability laws that drive insurance costs up for condo developers, America might even be able to bring its condo construction numbers up from </span><a href="https://www.urban.org/urban-wire/housing-market-needs-more-condos-why-are-so-few-being-built" rel="">historic lows</a><span>.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F195b4454-ec46-4af1-b4db-f4aec0be0295_1600x899.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F195b4454-ec46-4af1-b4db-f4aec0be0295_1600x899.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F195b4454-ec46-4af1-b4db-f4aec0be0295_1600x899.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F195b4454-ec46-4af1-b4db-f4aec0be0295_1600x899.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F195b4454-ec46-4af1-b4db-f4aec0be0295_1600x899.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F195b4454-ec46-4af1-b4db-f4aec0be0295_1600x899.jpeg" width="1456" height="818" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/195b4454-ec46-4af1-b4db-f4aec0be0295_1600x899.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:818,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F195b4454-ec46-4af1-b4db-f4aec0be0295_1600x899.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F195b4454-ec46-4af1-b4db-f4aec0be0295_1600x899.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F195b4454-ec46-4af1-b4db-f4aec0be0295_1600x899.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F195b4454-ec46-4af1-b4db-f4aec0be0295_1600x899.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Typical European floor plan. Drawing by Michael Eliason, Larch Lab.</figcaption></figure></div><p><span>The YIMBY movement and other housing reformers have made progress over the last decade or more in loosening zoning codes, but building codes in America are still largely a one-way ratchet, getting tighter and tighter with every fire, with no relief due to new technologies, provisions, and understanding of fire engineering. The model code writers are responsive to materials manufacturers and the fire service, but multifamily developers and housing affordability advocates do not exert much authority. Sometimes the manufacturers who donate to the International Code Council and can afford to pursue code changes make positive contributions (for example, the American Wood Council’s </span><a href="https://awc.org/wp-content/uploads/2022/01/tmt_toolkit.pdf" rel="">advocacy for mass timber</a><span>), but more often their interests have a questionable relationship to the public good. </span></p><p>States and localities have the power to adopt, reject, and amend model codes like the IBC, but lack the capacity to do much informed analysis and research. The better-resourced federal government used to play a larger role in code development, but has mostly retreated in recent decades.</p><p><span>Recognizing that building codes are the next frontier in housing reform, I founded the Center for Building in North America around a year ago, to do research on and advocate for changes to building codes that could bring down the cost of production of multifamily housing, and improve efficiency and quality. The opportunities for reform are as deep as a modern building is complex –&nbsp;from </span><a href="https://insideclimatenews.org/news/22052022/climate-refrigerants-air-conditioning-heat-pumps/" rel="">more modern refrigerants</a><span> that would allow Americans to buy heat pumps on the global market to more efficient </span><a href="https://www.aspe.org/pipeline/an-optimized-sanitary-stack-configuration-for-mid-rise-multifamily-building-construction/" rel="">single-stack venting</a><span> in plumbing systems that could cut plumbing costs dramatically –&nbsp;but we decided to focus on stairs as our first issue, given the importance to building design and the existing movement among architects for reform. </span></p><p><span>We’ve worked with housing advocates in </span><a href="https://www.centerforbuilding.org/singlestair-tracker" rel="">states across the US</a><span> to advance legislation and administrative changes to building codes to allow single-stair buildings up to six stories, above the current three-story limit in most of America, drawing on language already adopted in Seattle, Honolulu, and New York City, and </span><a href="https://secondegress.ca/Jurisdictions" rel="">extensive experience abroad</a><span>.</span></p><p>While the current single-stair limits were arrived at through trial-and-error and fairly rudimentary logic, status quo bias means that proposed reforms will be held to a much higher standard. To change such a fundamental tenet of American building codes, though, will require more advanced analysis. Fire protection engineering and data collection has advanced greatly in the century and a half since the second means of egress emerged in 1860, and today there are more engineered and big data approaches to proving safety.</p><p>Since the benefits are diffuse and multifamily development interests in America are poorly organized on a national level, there is not much of a natural constituency for this sort of work. One of my goals when founding the Center for Building in North America in 2022 was to produce research and organize urbanists, architects, real estate developers, and others to advocate for more rational building codes in the United States and Canada. </p><p><strong>—Stephen Smith</strong></p><p><span>For anyone who is interested in diving deeper into these topics, I’d encourage you to reach out to Stephen at </span><a href="mailto:stephen@centerforbuilding.org" rel="">stephen@centerforbuilding.org</a><span>. He’s one of the most knowledgable people I know when it comes to global building codes and policy reform, and he has been a great resource to Thesis Driven in the past.</span></p><p><span>Beyond that, his work at the Center for Building requires funding, and he’s raising $150,000 to produce data to prove the safety of taller single-stair buildings and push this code change past the finish line. If any Thesis Driven subscribers are interested in supporting this work, you can donate small sums on </span><a href="https://donorbox.org/center-for-building" rel="">their Donorbox site</a><span> or email Stephen at the email above to discuss a larger donation.</span></p><p><strong>—Brad Hargreaves</strong></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The quiet death of Ello's big dreams (393 pts)]]></title>
            <link>https://waxy.org/2024/01/the-quiet-death-of-ellos-big-dreams/</link>
            <guid>39043871</guid>
            <pubDate>Thu, 18 Jan 2024 16:35:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://waxy.org/2024/01/the-quiet-death-of-ellos-big-dreams/">https://waxy.org/2024/01/the-quiet-death-of-ellos-big-dreams/</a>, See on <a href="https://news.ycombinator.com/item?id=39043871">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-43879">

<div>
<figure><img fetchpriority="high" decoding="async" width="1024" height="640" src="https://waxy.org/wp-content/uploads/2024/01/image-10-1024x640.png" alt="" srcset="https://waxy.org/wp-content/uploads/2024/01/image-10-1024x640.png 1024w, https://waxy.org/wp-content/uploads/2024/01/image-10-300x188.png 300w, https://waxy.org/wp-content/uploads/2024/01/image-10-768x480.png 768w, https://waxy.org/wp-content/uploads/2024/01/image-10-800x500.png 800w, https://waxy.org/wp-content/uploads/2024/01/image-10.png 1280w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>
<p><a href="https://en.wikipedia.org/wiki/Ello_(social_network)">Ello</a> launched on August 7, 2014 with big dreams and big promises, a new social network defined by what it <em>wouldn’t</em> do.</p>
<p>They laid it all out in a <a href="https://web.archive.org/web/20140625082554/https://ello.co/manifesto">manifesto</a>, right on their homepage:</p>
<blockquote>
<p>Your social network is owned by advertisers.</p>
<p>Every post you share, every friend you make and every link you follow is tracked, recorded and converted into data. Advertisers buy your data so they can show you more ads. You are the product that’s bought and sold.</p>
<p>We believe there is a better way. We believe in audacity. We believe in beauty, simplicity and transparency. We believe that the people who make things and the people who use them should be in partnership.</p>
<p>We believe a social network can be a tool for empowerment. Not a tool to deceive, coerce and manipulate — but a place to connect, create and celebrate life.</p>
<p>You are not a product.</p>
</blockquote>
<figure><img decoding="async" width="908" height="498" src="https://waxy.org/wp-content/uploads/2024/01/image-11.png" alt="Screenshot of Ello's invite-only homepage with the manifesto, and two buttons: I Agree and I Disagree." srcset="https://waxy.org/wp-content/uploads/2024/01/image-11.png 908w, https://waxy.org/wp-content/uploads/2024/01/image-11-300x165.png 300w, https://waxy.org/wp-content/uploads/2024/01/image-11-768x421.png 768w, https://waxy.org/wp-content/uploads/2024/01/image-11-800x439.png 800w" sizes="(max-width: 908px) 100vw, 908px"></figure>
<p>From its launch, Ello defined itself as an alternative to ad-driven social networks like Twitter and Facebook. “You are not a product.” (The “I Disagree” button linked to Facebook’s privacy page.)</p>
<p>I’d link to that manifesto on Ello’s site, but I can’t, because Ello is dead.</p>
<p>In June 2023, the servers just started returning errors, making nine years of member contributions inaccessible, apparently forever — every post, artwork, song, portfolio, and the community built there was gone in an instant.</p>
<p>How did this happen? What happened between the idealistic manifesto above and the sudden shutdown?</p>
<p>It’s a story so old and familiar, I predicted it shortly after Ello launched.</p>
<h2>Ello’s Funding and Launch</h2>
<p>Ello, for those who don’t remember, <a href="https://web.archive.org/web/20140921072943/https://ello.co/wtf/post/about-ello">described itself</a> as a “simple, beautiful, and ad-free social network created by a small group of artists and designers.” It launched with a distinctly minimalist monochrome interface and an even more <a href="https://www.youtube.com/watch?v=QfPNdSY7vUg">minimalist</a> set of features.</p>
<p>Like Diaspora and App.net before it, Ello partly defined itself by its opposition to the exploitive business models and content moderation practices of major social networks, so quickly found itself deluged by people fleeing Facebook and dubbed by media outlets as an “<a href="https://www.engadget.com/2014-09-26-ello.html">anti-Facebook</a>” or “<a href="https://www.wired.com/2014/10/ello-pbc/">Facebook killer</a>,” something the Ello team never intended it to be. It was an uncomfortable balancing act, but they <a href="https://www.pcmag.com/news/ello-embraces-facebook-users-irked-by-real-name-demands">leaned</a> into the publicity, at least for a while.</p>
<figure><img decoding="async" width="1024" height="659" src="https://waxy.org/wp-content/uploads/2024/01/image-12-1024x659.png" alt="" srcset="https://waxy.org/wp-content/uploads/2024/01/image-12-1024x659.png 1024w, https://waxy.org/wp-content/uploads/2024/01/image-12-300x193.png 300w, https://waxy.org/wp-content/uploads/2024/01/image-12-768x495.png 768w, https://waxy.org/wp-content/uploads/2024/01/image-12-800x515.png 800w, https://waxy.org/wp-content/uploads/2024/01/image-12.png 1267w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Ello’s invite-only beta</figcaption></figure>
<p>In September 2014, one month after it opened its invite-only beta, I wrote <a href="https://web.archive.org/web/20140926023248/https://ello.co/waxpancake/post/oy73kFfDdhOPh8Jv9z9pFA">a post about Ello on Ello</a>. </p>
<p>Digging through <a href="https://www.sec.gov/Archives/edgar/data/1598000/000159800014000001/xslFormDX01/primary_doc.xml">SEC filings</a>, I discovered that the newly-launched indie social network had taken nearly half a million in seed funding from a venture capital firm, which seemed counter to its indie manifesto. Since nobody else mentioned the funding, including Ello themselves, I wrote about it.</p>
<p>Here’s what I wrote:</p>
<blockquote>
<p>Building something like Ello costs money. They have a team of at least seven people, and have worked on it for months. That doesn’t come cheap.</p>
<p>The <a href="https://web.archive.org/web/20140921072943/https://ello.co/wtf/post/about">About</a> section makes it seem like Ello was built independently, a group of artists making something for themselves, presumably funded by volunteer effort and maybe a seed investment from Ello president and CEO <a href="https://web.archive.org/web/20140921074259/https://paulbudnitz.com/about" target="_blank" rel="noreferrer noopener">Paul Budnitz</a>, who also founded Kidrobot and Budnitz Bicycles.</p>
<p>But a little digging shows a much more predictable source: they took a <a href="https://www.sec.gov/Archives/edgar/data/1598000/000159800014000001/xslFormDX01/primary_doc.xml" target="_blank" rel="noreferrer noopener">$435,000 round</a> of seed funding in January from FreshTracks Capital, a Vermont-based VC firm that <a href="https://s3.amazonaws.com/usm-studio/freshtracks/documents/the_freshtracks_flurry_march_2014.pdf" target="_blank" rel="noreferrer noopener">announced the deal</a> in March.</p>
<p>Why is this a problem?</p>
<p>The Ello founders are positioning it as an alternative to other social networks — they won’t sell your data or show you ads. “You are not the product.”</p>
<p>If they were independently-funded and run as some sort of co-op, bootstrapped until profitable, maybe that’s plausible. Hard, but possible.</p>
<p>But VCs don’t give money out of goodwill, and taking VC funding — even seed funding — creates outside pressures that shape the inevitable direction of a company.</p>
<p>Before they opened their doors, Ello became hooked on an unsustainable funding model — taking cash from VCs — and will almost certainly take a much larger Series A round once that $435,000 dries up. (Which, at their current burn rate, should be in a couple months.)</p>
<p>And they’ll have no trouble getting it. There’s a lot of money out there right now, and it will be extremely tempting to take it, especially if refusing it would mean closure or layoffs.</p>
<p>The problem, of course, is that VCs aren’t like Kickstarter backers, or even like angel investors. Kickstarter or Patreon backers just want the thing being made. Angel investors may have other reasons to invest beyond equity: fame, insider access, or maybe just the joy of helping something exist.</p>
<p>VCs may invest in things they think are interesting or want to exist, but they primarily invest money in startups to get a return on their investment, on behalf of their limited partners. That return usually takes the form of an exit: an acquisition or an IPO.</p>
<p>Unless they have a <strong>very</strong> unique relationship with their investors, Ello will inevitably be pushed towards profitability and an exit, even if it compromises their current values. Sometimes, this push comes subtly in the form of advice and questions in emails, phone calls, and chats over coffee. Sometimes, as more direct pressure from the board. (FreshTracks’ Managing Director <a href="https://web.archive.org/web/20140929184723/https://www.freshtrackscap.com/lee-e-bouyea/" target="_blank" rel="noreferrer noopener">sits</a> on their board.) Or, if things go bad, by replacing the founders.</p>
<p>The Ello team knows that how a startup is funded shapes how it behaves. They spend a good chunk of their <a href="https://web.archive.org/web/20140921072943/https://ello.co/wtf/post/about">About</a> pages talking about how they’re not going to make money (not ads or selling your data), and a little bit about how they hope to (paid premium features). I hope they’re right — it’d be great to have more startups that aren’t reliant on ads.</p>
<p>But they completely fail to disclose how Ello is being funded <strong>now</strong>, which matters just as much, if not more, as any future revenue plans.</p>
<p>I love seeing people build new stuff. More people trying to build crazy experimental communities on the Internet is a very good thing. And nothing’s more audacious than trying to build a new social network.</p>
<p>Social networks become the glue that connect people together — the foundation for friendships, relationships, and new works of creative expression.</p>
<p>Building a social network is like opening the doors to a huge party and inviting everyone in. Without a way to get your stuff out, shutting down a social network is like locking the door and burning the place down.</p>
<p>At the moment, Ello is a free, closed-source social network, with no export tools or an API, fueled by venture capital and a loose plan for paid premium features. I think it’s fair to be skeptical.</p>
<p>Like everyone else here, I hope Ello can stick to their principles, resist outside pressure, fight market forces, and find a unique and sustainable niche.</p>
<p>Let’s hope their investors feel the same way.</p>
</blockquote>
<h2>The Founders Respond</h2>
<p>That post quickly blew up on Ello, and then went far beyond it, with coverage in articles from <a href="https://www.theverge.com/2014/9/30/6874727/ello-is-the-doomed-utopia-we-cant-stop-building">The Verge</a>, <a href="https://www.theguardian.com/media/2014/sep/26/ello-anti-facebook-privacy-lgbt-social-neetwork">The Guardian</a>, <a href="https://www.vice.com/en/article/vbn8m4/what-the-hell-is-ello">VICE</a>, <a href="https://www.theatlantic.com/technology/archive/2014/09/ello-says-youre-not-a-product-but-you-are/380809/">The Atlantic</a>, and <a href="https://www.businessinsider.com/ello-needs-to-stick-to-its-principles-2014-9">Business Insider</a>, among others.</p>
<p>Ello’s CEO, co-founders, and investors dismissed the concerns I raised, starting with co-founder and CEO Paul Budnitz, who <a href="https://web.archive.org/web/20140927073322/http://betabeat.com/2014/09/ello-founder-says-vc-funding-is-no-big-secret-thats-silly/">told Betabeat</a> it was “silly.”</p>
<figure><img loading="lazy" decoding="async" width="680" height="555" src="https://waxy.org/wp-content/uploads/2024/01/image-6.png" alt="Screenshot of Betabeat article: &quot;Ello Founder Says VC Funding Is No Big Secret: ‘That’s Silly’&quot; &quot;There's no pressure for us to do anything we don't want to do,&quot; Ello founder Paul Budnitz said." srcset="https://waxy.org/wp-content/uploads/2024/01/image-6.png 680w, https://waxy.org/wp-content/uploads/2024/01/image-6-300x245.png 300w" sizes="(max-width: 680px) 100vw, 680px"></figure>
<p>“In fact, Ello is controlled executively by its 7 founders, who own a majority share in the company,” <a href="https://web.archive.org/web/20140927073322/http://betabeat.com/2014/09/ello-founder-says-vc-funding-is-no-big-secret-thats-silly/">wrote</a> Betabeat’s Jack Smith IV. “They say that the cynical claim that they’ll sell out eventually, or that anyone can tell them what to do, is ridiculous.”</p>
<p>Co-founder Todd Berger <a href="https://web.archive.org/web/20141006081010/https://gigaom.com/2014/09/25/ello-investor-co-founder-funding-or-not-we-hate-ads-and-we-want-to-shift-values/">laughed at a GigaOm writer</a> who asked him about my post. “There’s seven founders and we own 82 to 84 percent of the company, so we can do whatever the hell we want,” Berger said. </p>
<p>“We’re not going to sell out our soul to grow our company,” continued Berger. “Maybe it’s hard to believe.”</p>
<p>I <a href="https://web.archive.org/web/20140930025259/https://ello.co/waxpancake/post/Jp4o1TBtLrYytHpcEni0Kg">followed up</a> with a second Ello post:</p>
<blockquote>
<p>I’ve received a dozen emails in the last day and a half from journalists looking for quotes about Ello. I didn’t reply to any of them. I have no interest in being the anti-Ello poster boy, for one main reason:</p>
<p><strong>I think Ello’s pretty neat, and I want them to succeed.</strong></p>
<p>Like I said in <a href="https://web.archive.org/web/20140930025259/https://ello.co/waxpancake/post/oy73kFfDdhOPh8Jv9z9pFA" target="_blank" rel="noreferrer noopener">my post</a>, more experimentation with online communities is a very good thing. We’ll only break away from the dominant players by trying new crazy shit, and I think it should be applauded. (And, yes, I even like the design.)</p>
<p>But I think taking VC was a bad idea that works against their ethos, and will inevitably lead to a much larger Series A by year’s end.</p>
<p>I think the intentions of the team are pure, and they genuinely believe in what they’re building. But I’m not sure intentions matter unless they can wean themselves off outside funding.</p>
<p>I really, really hope their revenue plan works out, and quickly.</p>
</blockquote>
<h2>Series A and the PBC</h2>
<p>One month later, Ello <a href="https://www.prnewswire.com/news-releases/ello-becomes-a-public-benefit-corporation-and-legally-vows-to-forever-remain-ad-free-founders-and-investors-sign-charter-376792333.html">announced</a> they’d raised significantly more money: a <strong>$5.5 million</strong> Series A round co-led by TechStars and <a href="https://foundry.vc/blog/2014/10/our-investment-in-ello/">Foundry Group</a>, who took a board seat, with participation from FreshTracks Capital, who already sat on the board.</p>
<p>Coinciding with this funding, and perhaps anticipating the backlash, Ello also announced they had converted the company to a Public Benefit Corporation.</p>
<p>In a public letter signed by their founders and investors, they wrote:</p>
<blockquote>
<p>There has been some speculation in the press since our launch that Ello will someday be forced to allow paid ads on our social network.</p>
<p>With virtually everybody else relying on ads to make money, some members of the tech elite are finding it hard to imagine there is a better way.</p>
<p><strong>But 2014 is not 2004, and the world has changed.</strong></p>
</blockquote>
<p>Effectively, Ello would be a for-profit corporation required to pursue social good as part of its charter, instead of solely maximizing shareholder value. Unlike a B Corp certification, this would enshrine their values in their legal structure, which is a pretty big deal. They were the most notable technology company to form as a PBC until that point, preceding <a href="https://www.kickstarter.com/blog/kickstarter-is-now-a-benefit-corporation">Kickstarter’s conversion</a> by nearly a year.</p>
<p>A <a href="https://web.archive.org/web/20160319071816/https://ello.co/wtf/about/pbc/">dedicated page</a> on their site explained the significance of the PBC, and the charter they were now bound by:</p>
<blockquote>
<p>To assure that Ello always remains ad-free, Ello converted to a Public Benefit Corporation (PBC). A Benefit Corporation is a new kind of for-profit company in the USA that exists to produce a benefit for society as a whole — not just to make money for its investors.</p>
<p>The Ello PBC charter states in the strongest legal terms possible that:</p>
<ol>
<li>Ello shall never make money from selling ads;</li>
<li>Ello shall never make money from selling user data; and</li>
<li>In the event that Ello is ever sold, the new owners will have to comply by these terms.</li>
</ol>
<p>Ello exists for the benefit of the creative community, and we will never serve ads or sell personal data.</p>
</blockquote>
<figure><a href="https://web.archive.org/web/20160304012443/https://ello.co/downloads/ello-pbc.pdf"><img loading="lazy" decoding="async" width="1024" height="698" src="https://waxy.org/wp-content/uploads/2024/01/image-1024x698.png" alt="&quot;We, as founders and investors in Ello, vow to support Ello’s mission and to abide by the terms of the Ello Charter as stated above.
Paul Budnitz, Todd Berger, Lucian Föhr, Gabe Varela, Matt Kitt, Jay Zeschin,
Justin Gitlin, Brad Feld, Jason Mendelson, Seth Levine, Ryan McIntyre, David Cohen, Ari Newman, Mark Solon, Lee Bouyea, Cairn Cross, Tim Davis &amp; Damon Way&quot;" srcset="https://waxy.org/wp-content/uploads/2024/01/image-1024x698.png 1024w, https://waxy.org/wp-content/uploads/2024/01/image-300x204.png 300w, https://waxy.org/wp-content/uploads/2024/01/image-768x523.png 768w, https://waxy.org/wp-content/uploads/2024/01/image-800x545.png 800w, https://waxy.org/wp-content/uploads/2024/01/image.png 1230w" sizes="(max-width: 1024px) 100vw, 1024px"></a><figcaption>The signatures from the <a href="https://web.archive.org/web/20160304012443/https://ello.co/downloads/ello-pbc.pdf">PDF</a> of their PBC letter</figcaption></figure>
<p>This was a commendable change, though somewhere along the way, all the public debate about raising professional money and profit maximization became solely about switching to a paid advertising model and selling user data. This was a straw man argument that was easier to knock down.</p>
<p>But there are many, many ways for a social network to become worse for their users than running ads.</p>
<p>My concern wasn’t that Ello would start running paid ads. I don’t even mind ads, as long as they’re done thoughtfully and with privacy in mind. (I ran ads from <a href="https://web.archive.org/web/20090228233009/https://decknetwork.net/">The Deck</a> here for years.)</p>
<p>I was worried that, by taking outside funding, Ello’s values were no longer fully-aligned with the community: they were aligned with their investors. In time, given more money and more pressure, they would be inclined to do something the community, or even the original founders, didn’t want to do.</p>
<h2>Series B and CEO Changes</h2>
<p>In April 2015, six months after their Series A, Ello took another $5 million in a Series B round from <a href="https://www.builtincolorado.com/2015/04/03/ello-raises-another-5m-launches-new-beta-version">their previous investors</a>, giving a board seat to TechStars, and bringing their total raised to $11M.</p>
<p>Later that year, in December, Budnitz wrote a <a href="https://web.archive.org/web/20170622034939/https://ello.co/budnitz/post/qcg9cemgu69qfdawwwuubw">new post on Ello</a> looking back on their first year and looking ahead to 2016:</p>
<blockquote>
<p>This past week I gave a few interviews to online news organizations.</p>
<p>One of the journalists scoffed when I told him that Ello is built on principles we believe in, and that in 2015 we did everything we could to grow slowly. Rather than sell out and make another giant network the world doesn’t need, we decided to take our time to build the beautiful and inspiring place we have today.</p>
<p>I felt sad for the guy. It’s awful going through life never believing in anything.</p>
<p>So in the spirit of the New Year, and because it was clear that this journalist wasn’t going to believe anything I told him anyway, I figured I’d publish a short list of things Ello will <em>never</em> do:</p>
<ol>
<li>Diverge from our mission to empower and support creators to inspire one another, and move the world forward.</li>
<li>Tolerate hate. Ello has many tools, some visible and others not, that help keep this network positive.</li>
<li>Sell ads or user data to third parties.</li>
<li>Sell out.</li>
<li>Suck.</li>
</ol>
</blockquote>
<p>Three months later, in March 2016, Paul Budnitz <a href="https://web.archive.org/web/20160328082226/ello.co/budnitz">stepped down</a> as CEO, citing the distance between his home in Vermont and the rest of the team in Boulder. He was replaced by Todd Berger, one of Ello’s co-founders and lead designers.</p>
<p>Under Berger, Ello <a href="https://www.wired.com/2016/05/ello-artists/">refocused</a> its efforts on artists and creators. From a May 2016 <a href="https://www.prnewswire.com/news-releases/the-creators-network-tomorrows-trends-in-leading-edge-creativity-today-300273173.html">press release</a>:</p>
<blockquote>
<p>In recent months Ello has doubled down on its mission to support creators everywhere, becoming the premiere community for the world’s leading edge and contemporary artists, photographers, designers, illustrators, architects and GIF makers to share their work and ideas, connect with others, and build organic reach.</p>
</blockquote>
<p>In a September 2016 <a href="https://www.wired.com/2016/09/the-bizarre-second-life-of-the-utopian-facebook-killer/">interview with Wired</a>, he said that was what Ello was always meant to be:</p>
<blockquote>
<p>Berger had originally intended Ello to cater to artists, but the founding team was split on the idea. “To Paul [Budnitz] it sounded limiting. To our investors it sounded very limiting,” says Berger.</p>
</blockquote>
<p>As its new CEO, Berger continued fundraising, but the <a href="https://www.sec.gov/Archives/edgar/data/1598000/000159800017000002/xslFormDX01/primary_doc.xml">SEC filing</a> from March 2017 indicates a struggle, raising only $2.5M of the available $4M. In an <a href="https://techcrunch.com/2017/11/19/ello-again/">interview with TechCrunch</a> in November 2017, Berger said he was looking to raise more cash. “We have a lot of investment opportunities coming in from actually some fairly heavy-hitting firms that I hope to close.”</p>
<figure><img loading="lazy" decoding="async" width="1024" height="770" src="https://waxy.org/wp-content/uploads/2024/01/image-7-1024x770.png" alt="Screenshot of Ello homepage. &quot;The Creators Network: Built by artists, for artists. Ello is a global community of artists dedicated to creative excellence.&quot;" srcset="https://waxy.org/wp-content/uploads/2024/01/image-7-1024x770.png 1024w, https://waxy.org/wp-content/uploads/2024/01/image-7-300x225.png 300w, https://waxy.org/wp-content/uploads/2024/01/image-7-768x577.png 768w, https://waxy.org/wp-content/uploads/2024/01/image-7-800x601.png 800w, https://waxy.org/wp-content/uploads/2024/01/image-7.png 1191w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Ello in 2017</figcaption></figure>
<p>In the same interview, Berger said Ello now had 400,000 monthly active users, with 625,000 artists on the site. </p>
<p>It seemed like Ello finally found its niche as “The Creators Network,” a community of artists and designers using its visual-heavy design to show off their portfolios and promote their work. Their original freemium model never worked out, but sponsored content was handled thoughtfully, with like-minded brands offering giveaways. It was paid advertising, but it didn’t violate privacy or sell user data.</p>
<p>“A lot of people thought we died and went away and the whole time we’ve been cultivating a really niche and creative community that’s gotten more focused as I’ve been able to enact my vision,” Berger said.</p>
<p>The future of Ello seemed bright.</p>
<h2>The Acquisition</h2>
<p>Five months later, in March 2018, Ello was quietly sold to <a href="https://web.archive.org/web/20180927195200/https://www.talenthouse.com/about">Talenthouse</a>, a Los Angeles-based company whose primary business was running <a href="https://web.archive.org/web/20161117224953/https://www.talenthouse.com/collaborate/all">design contests</a> for brands, in which independent artists competed against each other for a cash prize.</p>
<p>I only know the sale date because it was mentioned in the <a href="https://vcet.co/wp-content/uploads/2019/01/VSCF-FY18-Annual-Public-Report-w_cover.pdf">annual report</a> of a venture capital firm who invested in their Series A. The acquisition was never announced publicly, as far as I can tell, mentioned only in <a href="https://www.thedrum.com/news/2018/10/17/creativity-re-wired-good-fast-or-cheap">this October 2018 interview</a> with Talenthouse co-founder Maya Bogle, where she said that “earlier this year we acquired Ello.co.”</p>
<p>As an Ello user, I was never notified about the ownership change, even though they sold all my data to an entirely new company. As far as I can find, none of the original founders mentioned the sale publicly when it happened.</p>
<p>There were telltale signs, though: Ello’s social media started regularly <a href="https://twitter.com/search?q=from%3Aellohype%20talenthouse">promoting design contests</a> from “our friends at Talenthouse,” while never disclosing the sale. The Ello homepage prominently featured Talenthouse “artist invites” to compete in their design contests from brands like Absolut Vodka, Amazon Prime, Pabst Blue Ribbon, Adidas, and Miller Lite.</p>
<figure><img loading="lazy" decoding="async" width="587" height="458" src="https://waxy.org/wp-content/uploads/2024/01/image-9.png" alt="Tweet screenshot from Ello's account on September 6, 2018: &quot;Check out a dope opportunity from our friends at @talenthouse. Design city-inspired bottle bags for a Limited Edition of Absolut Vodka.&quot;" srcset="https://waxy.org/wp-content/uploads/2024/01/image-9.png 587w, https://waxy.org/wp-content/uploads/2024/01/image-9-300x234.png 300w" sizes="(max-width: 587px) 100vw, 587px"><figcaption>This isn’t an ad, it’s an opportunity! And they’re just friends, really.</figcaption></figure>
<h2>The Founders Leave</h2>
<p>The following month, in September 2018, the remaining two of Ello’s original remaining co-founders announced they had left the company.</p>
<p>In a <a href="https://www.instagram.com/p/Bnox1cAhOSa/">cryptic post</a> on Ello and Instagram, Ello’s then-CEO Todd Berger and fellow co-founder Lucian Föhr wrote:</p>
<blockquote>
<p>Over the course of the past years we did our best to steer Ello per our vision, always with the intent of putting artists first. At times we succeeded, often we failed. Which brings us to today. We’re no longer at Ello, we can’t elaborate as to why, but it’s time for us to move on and return to studio life.…</p>
<p>If we let you down along the way, we’re sorry. If we didn’t, all the better.</p>
</blockquote>
<p>On a <a href="https://www.printdesignacademy.com/podcasts/the-quickie-interviews-for-graphic-designers/episodes/2147546418">July 2019 podcast</a>, Berger and Föhr spoke candidly about their time at Ello, when Berger took over CEO duties and Föhr became the Chief Product Officer.</p>
<blockquote>
<p><strong>Todd Berger:</strong> The beginning of it was super exciting, super pure, 100% authentic. We built a lot of digital products, worked with lots of startups. We felt like we knew how to do this. We got a lot of momentum. People were stoked. </p>
<p>And then investors got interested and there was pressure to do all these other things. The CEO at the time [Paul Budnitz] got maybe a little overzealous about making lots and lots of money and turning it into a crazier thing than we ever imagined. And it kind of got out of control real fast. </p>
<p>And then we were just kind of holding on, trying to steer it as best possible back to its original kind of the real place we wanted it to live in. And it was tricky, that fast-paced real startup ecosystem— once there’s VC money in there and there’s a lot of press and there’s a lot of attention and you’re not necessarily meeting expectations per the media, per your investors, etc etc.</p>
</blockquote>
<p>As for the acquisition, it doesn’t sound like it was a big payday for the original founders. (As preferred shareholders, VCs are typically paid from an acquisition before founders, employees, or other common stockholders.)</p>
<blockquote>
<p><strong>Interviewer:</strong> Do you feel that the sale of the company justified the time and effort and blood and sweat?</p>
<p><strong>Berger:</strong> No, no. Frankly, it wasn’t a lucrative exit. It was more of a, let’s carry this thing on, someone wants it. And it wasn’t about that for us. </p>
<p>Like, had it been something different, who knows what we’d be doing now? But the experience, by and large, was justified. It just, by the nature of taking money in and building a company and a lot of pressure and responsibility, it went on longer and turned into a bumpier grindier thing I think than we wish it would have.</p>
</blockquote>
<p>On August 22, 2018, then-CTO Colin Gray <a href="https://github.com/ello/wtf/commit/a8be236ba25d9c07feeafaa0e0590b39b5bd9ba4#diff-0eb547304658805aad788d320f10bf1f292797b5e6d745a3bf617584da017051">deleted the Manifesto</a>, the foundational statement that was part of Ello since before it launched, along with <a href="https://github.com/ello/wtf/commit/e454e75357f8e5ec5cf02c1c25fe44459baaec77">all references</a> to the Public Benefit Corporation and its charter. </p>
<p>Ello PBC was officially dead.</p>
<figure><img loading="lazy" decoding="async" width="942" height="476" src="https://waxy.org/wp-content/uploads/2024/01/image-8.png" alt="Screenshot from GitHub showing the Manifesto webpage was deleted." srcset="https://waxy.org/wp-content/uploads/2024/01/image-8.png 942w, https://waxy.org/wp-content/uploads/2024/01/image-8-300x152.png 300w, https://waxy.org/wp-content/uploads/2024/01/image-8-768x388.png 768w, https://waxy.org/wp-content/uploads/2024/01/image-8-800x404.png 800w" sizes="(max-width: 942px) 100vw, 942px"></figure>
<h2>The End</h2>
<p>In December 2019, Ello, Talenthouse, and Zooppa <a href="https://www.businesswire.com/news/home/20191203005403/en/Ello-Talenthouse-and-Zooppa-Join-Forces-to-Form-TLNT-Holdings-%E2%80%94-the-Worlds-Most-Diverse-and-Engaged-Creative-Network">merged</a> into TLNT Holdings, a new holding company backed by UK private equity firm AEDC Capital. TLNT was then sold to Swiss investment firm New Value AG, which renamed itself, confusingly, to Talenthouse AG.</p>
<p>In December 2021, Ello <a href="https://twitter.com/ellohype/status/1470706318814195719">changed</a> their logo to finally acknowledge what everyone on the site had figured out long ago.</p>
<figure><img loading="lazy" decoding="async" width="750" height="292" src="https://waxy.org/wp-content/uploads/2024/01/image-1.png" alt="&quot;Ello by Talenthouse&quot; logo" srcset="https://waxy.org/wp-content/uploads/2024/01/image-1.png 750w, https://waxy.org/wp-content/uploads/2024/01/image-1-300x117.png 300w" sizes="(max-width: 750px) 100vw, 750px"></figure>
<p>“Our new logo represents our parent company, Talenthouse, who y’all are already familiar with as we cross promote creative briefs on the Talenthouse platform all the time,” they wrote on the official blog. “You’re creative, you understand the way businesses develop.”</p>
<p>Behind the scenes, Talenthouse was struggling financially.</p>
<p>A February 2023 <a href="https://www.theguardian.com/media/2023/feb/04/ive-given-up-getting-paid-design-agency-accused-of-exploiting-artists">report in The Observer</a> exposed that Talenthouse was withholding funds from artists who won their creative briefs. A month later, The Guardian <a href="https://www.theguardian.com/technology/2023/apr/09/creative-tech-firm-talenthouse-is-close-to-failure-as-debts-mount">reported</a> their parent company was “close to failure as debts mount,” with most of their subsidiaries closed and staff laid off.</p>
<blockquote>
<p>Talenthouse, whose clients have included Netflix, Coca-Cola, Nike and the UN, is facing legal action by creditors in the UK and is understood to have laid off most of its workforce, with top executives also departing its parent company in recent days.</p>
<p>Its parent company, Talenthouse AG, has also announced the closure of four other subsidiaries saying they cannot afford to pay outstanding bills, including staff wages.</p>
</blockquote>
<p>In May 2023, the company <a href="https://www.musicbusinessworldwide.com/having-raised-80m-to-date-creator-agency-talenthouse-alerts-investors-to-critical-financial-situation/">released a statement</a> that it was facing a “critical financial situation,” restructuring the company while finding outside investors.</p>
<p>As a result of the upheaval, Ello’s website started seeing significant downtime starting in June 2023, delivering 500 errors on every page for <a href="https://ferdifz.tumblr.com/post/720556050114379776/ello-being-ello-again?is_related_post=1">days at a time</a>. It came back online for a few days in July, and then <a href="https://web.archive.org/web/20230719114615/https://ello.co/">more errors</a>. </p>
<p>On July 18, 2023, it shut down for good and never recovered. On August 9, the web app was apparently deleted, leaving nothing but a <a href="https://web.archive.org/web/20230809081328/http://ello.co/">Heroku error</a>.</p>
<figure><img loading="lazy" decoding="async" width="1024" height="285" src="https://waxy.org/wp-content/uploads/2024/01/image-4-1024x285.png" alt="Three screenshots of errors from the Ello homepage." srcset="https://waxy.org/wp-content/uploads/2024/01/image-4-1024x285.png 1024w, https://waxy.org/wp-content/uploads/2024/01/image-4-300x84.png 300w, https://waxy.org/wp-content/uploads/2024/01/image-4-768x214.png 768w, https://waxy.org/wp-content/uploads/2024/01/image-4-1536x428.png 1536w, https://waxy.org/wp-content/uploads/2024/01/image-4-800x223.png 800w, https://waxy.org/wp-content/uploads/2024/01/image-4.png 1607w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>The Ello homepage in July, August, and September 2023</figcaption></figure>
<p>The <a href="https://business.talenthouse.com/">Talenthouse</a> corporate site is still online, but the platform is offline, and they haven’t posted anything on social media since January 2023. Ello’s social media team stopped posting in October 2022.</p>
<hr>
<p>After leaving Ello in 2016, Budnitz returned to his Kidrobot roots with the launch of <a href="https://superplastic.co/">Superplastic</a> in 2017, a vinyl figure company that expanded into <a href="https://jankyheist.com/">NFTs</a> and <a href="https://www.fastcompany.com/90687982/superplastic-disney-metaverse-nft-janky-guggimon-paul-budnitz">the metaverse</a> in 2022, raising a total of $68M in seven rounds of funding, led by Amazon. Superplastic appears to have abandoned its NFT projects last year as the market cratered, and Budnitz stepped down from his CEO role in September, replaced by the former president of blockchain gaming company Dapper Labs. They are now focused on “<a href="https://www.modernretail.co/marketing/how-amazon-backed-superplastic-is-trying-to-transform-its-toys-into-synthetic-celebrities/">synthetic celebrities</a>” and AI influencers. </p>
<p>Todd Berger and Lucian Föhr reopened their <a href="https://bergerfohr.com/">Boulder design studio</a>, which had shuttered for five years while they worked on Ello. Berger described running Ello as the “low point of my creative career,” so I hope they’re doing better.</p>
<p>As for Ello’s users, they’re out of luck. The shutdown spawned a confused exodus of sorts, former community members trying to figure out what happened on <a href="https://www.reddit.com/r/ello/">the Ello subreddit</a>, on <a href="https://twitter.com/search?q=ello.co%20OR%20ellohype%20since%3A2023-06-15&amp;src=typed_query&amp;f=top">X/Twitter</a>, and the <a href="https://danthornton.net/2023/06/is-it-goodbye-to-ello-another-niche-network-gone/#comments">comment section</a> of the only other blog post about it. </p>
<blockquote>
<p>“years of my writing down the drain”<br>“Heartbreaking. I upload my artwork in there and I love the site because it really focused on art.”<br>“It’s really messed up that there was NO warning to allow us to download our content. That was a very personal space for me and now it’s gone forever? It was my online diary ffs!”<br>“I had two groups. One had over 18k followers and the other 17k+. No warning at all. Just gone, along with 8 years of content updated a couple of times a week.”<br>“Did you find a way? It has basically my entire diary :(“<br>“18k followers and a few years worth of educational posts on fiction writing craft essentials gone.”</p>
</blockquote>
<p>Some people tried to contact Ello or Talenthouse directly, but the emails <a href="https://twitter.com/Nerdwiththehat/status/1685830952675655680">bounced</a>.</p>
<p>Some former members set up <a href="https://www.tumblr.com/ferdifz/718400540806103040/ello-on-tumblr-group">a Tumblr group</a> to try to find each other again, “an attempt to maybe preserve and/or recapture what little magic Ello still had for us.”</p>
<h2>You Were The Product</h2>
<p>From the moment it launched, I liked Ello and wanted it to succeed. Experimentation in social networks is critically important, and there’s enormous value in making new online communities for creative people. I even loved Ello’s minimalist monochrome design, which some people bounced off of.</p>
<p>But from the moment I read about their seed funding, I worried that they wouldn’t be able to build a long-term sustainable business if they were hooked on professional funding and busy chasing growth.</p>
<p>The day after I wrote my first Ello post that blew up, Rose Eveleth published an <a href="https://www.theatlantic.com/technology/archive/2014/09/ello-says-youre-not-a-product-but-you-are/380809/">article</a> in The Atlantic with the blunt headline, “Ello Says You’re Not a Product, But You Are.”</p>
<blockquote>
<p>The fact that you, the user, even exist and use their site makes you a product. Ello already has some amount of seed funding from VCs, which means it will need to return to them with something in hand if it wants more. And when it does, or when it is eventually bought by a larger company, you are part of that transaction—a key line in the sales pitch. Your existence on that site is a unit of currency, and it’s a unit that Ello is selling to whoever will give them money for it.</p>

</blockquote>
<p>Ello’s founders wrote in their manifesto that, with other social networks, “You are the product that’s bought and sold.” They believed, I’m sure sincerely, that Ello would be different. “We believe that the people who make things and the people who use them should be in partnership. You are not a product.”</p>
<p>Despite their idealist manifesto and their <a href="https://web.archive.org/web/20150701202201/https://bill-of-rights.ello.co/">Bill of Rights</a>, I don’t believe they could ever truly be in partnership with their community once they were taking large amounts of venture funding. All of their ideals and big dreams were easily undone, even the legal restrictions they defined in their Public Benefit Corporation charter:</p>
<ol>
<li>Ello made money from selling ads to third parties;</li>
<li>Ello made money selling their user data to a third party;</li>
<li>Ello was sold, and the new owners didn’t comply with those terms.</li>
</ol>
<p>In the end, Ello was sold to a third party without notifying its users or giving them the opportunity to opt out of handing over all their content and data, then resold to a new company, and finally shut down and deleted with no notice or recourse.</p>
<p>Would things have been different if they hadn’t taken funding? It’s impossible to say. In all likelihood, it never would have been built in the first place.</p>
<p>But if it had, I doubt it would have ended like this. 🍞</p>
<figure><img loading="lazy" decoding="async" width="1024" height="768" src="https://waxy.org/wp-content/uploads/2024/01/image-3.png" alt="Screenshot from the Ello homepage in July 2023" srcset="https://waxy.org/wp-content/uploads/2024/01/image-3.png 1024w, https://waxy.org/wp-content/uploads/2024/01/image-3-300x225.png 300w, https://waxy.org/wp-content/uploads/2024/01/image-3-768x576.png 768w, https://waxy.org/wp-content/uploads/2024/01/image-3-800x600.png 800w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Ello’s homepage on July 3, 2023, shortly before it closed for good. “Built by artists, for artists.”</figcaption></figure>
</div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Escaping surveillance capitalism, at scale (245 pts)]]></title>
            <link>https://ergaster.org/posts/2024/01/18-escaping-surveillance-capitalism-at-scale/</link>
            <guid>39043547</guid>
            <pubDate>Thu, 18 Jan 2024 16:14:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ergaster.org/posts/2024/01/18-escaping-surveillance-capitalism-at-scale/">https://ergaster.org/posts/2024/01/18-escaping-surveillance-capitalism-at-scale/</a>, See on <a href="https://news.ycombinator.com/item?id=39043547">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-astro-cid-2q5oecfc=""> <p>Our relationship with computers and phones has changed. We used to rely on software installed locally on our computers, and are now shifting towards a model based on services and companion apps, sometimes with free tiers and subscriptions.</p>
<p>Most services are provided by organisations, who collect and sometimes resell users’ information to third parties. This massive, indiscriminate, and corporate collection of personal data is called <a href="https://en.wikipedia.org/wiki/Surveillance_capitalism">surveillance capitalism</a>. While organisations can argue this data collection is necessary to provide their services, this comes with significant privacy implications.</p>
<p>Self-hosting and paid subscriptions are common strategies to escape surveillance capitalism. But what guarantees do they really offer? What alternatives exist for the general public who wants to escape surveillance capitalism, and at what cost?</p>
<h2 id="paid-subscriptions-are-not-enough">Paid subscriptions are not enough</h2>
<p>When services have a free and a paid tier, it can be tempting to think that the provider is going to sell your data on the free tier but they’re going to be mindful of it on the paid tier. Paying a subscription to your provider can sound like a good idea to keep your data safe, and it sometimes is. But it’s not necessarily the case.</p>
<p>As reported by The Verge, the <a href="https://www.theverge.com/2023/3/2/23622227/betterhelp-customer-data-advertising-privacy-facebook-snapchat">mental health service provider BetterHelp shared customer’s email addresses, IP addresses, and health questionnaire information with third parties</a> including Facebook and Snapchat, while promising it was private. Mental health is precisely the type of information that should remain private, making this move particularly appalling from BetterHelp.</p>
<h2 id="self-hosting-but-as-a-service">Self-hosting, but as a service</h2>
<p>One of the major enablers of surveillance capitalism is centralisation. In that sense, paid subscriptions don’t offer any guarantee that the provider will play fair game and keep your data private.</p>
<p>Self-hosting is rather efficient at preventing surveillance capitalism, mostly because the data doesn’t live in a central repository but on the self-hoster’s infrastructure. In that sense, it doesn’t directly enable surveillance capitalism. It’s important here to make a distinction between private and public information though. Posts on federated social media platforms are not centralised, but they are public and can be scraped to be exploited. The threat we’re discussing in this article is a provider growing so big it can collect and exploit data that is not public, at a large scale.</p>
<p>It should be noted that the vendor of the self-hosted solution could theoretically still gather information about the users by making its software send data to the mothership regardless of where it’s installed. To an extent, this can be acceptable as long as the user explicitly knows what data is sent, can opt-out, and that the minimum amount of anonymised data is sent for clearly defined purposes. This practice is known as <a href="https://en.wikipedia.org/wiki/Telemetry#Software">telemetry</a>. Open-source software allows any tech-savvy person to look up the code and check what is actually sent to the mothership. Proprietary software makes it much more difficult.</p>
<p>But <a href="https://ergaster.org/posts/2023/08/09-i-dont-want-to-host-services-but-i-do">as we discussed earlier on this blog</a>, self-hosting doesn’t scale well because it requires time and knowledge. There’s a workaround: using software that can be self-hosted, but buying it as a service. A real life example would be the Google Drive ethical alternative <a href="https://nextcloud.com/">Nextcloud</a>: several providers like Ionos offer hosted Nextcloud instances. This makes solutions like these accessible (and safe!) to a broader public.</p>
<p>Using open-source licences means that the software can be audited, but it also allows anyone to take the code and offer it as service. This can lead to a race to the bottom in terms of cost and quality when the service providers are not playing fair game: they benefit from development work they didn’t invest resources in, all while not contributing financially or technically to the upstream project either.</p>
<p>If the customers of such providers encounter problems, support is often minimal: in very budget-tight environments, losing a customer can be more profitable than investigating a significant problem. Such predatory methods harm the ecosystems in which they’re deployed: customers get bad experiences, and the upstream project gets little to no benefit. Similar behaviours have been observed in the Matrix ecosystem where integrators deployed open source products without contributing anything back.</p>
<p><img alt="A screenshot of the Nextcloud website's page listing the partners working with Nextcloud." src="https://ergaster.org/_astro/nextcloud-partners.DuAZqLpZ_1XMc1K.webp" width="2880" height="1580" loading="lazy" decoding="async"></p>
<p>Ultimately, support contracts are an insurance for the service provider <em>and</em> for their customers. When the service provider pays for upstream’s support and reports an issue, the engineers who developed the product investigate the case, fix the problem, and make the fix permanent for everyone. This also allows the upstream to generate a bit of revenue, contributing to the project’s health, sustainability, and to the emergence of new exciting features.</p>
<p>Nextcloud also allows users to sign up on third party providers directly from <a href="https://nextcloud.com/">nextcloud.com</a>. The sign-up feature makes it extremely easy for the user to choose a provider. Nextcloud’s <a href="https://jospoortvliet.com/">Jos Poortvliet</a> confirmed to me that it’s not a formal certification programme, but more of a group of companies Nextcloud trusts. Nextcloud doesn’t generate revenue from this programme, intentionally, since they’re not in the business of monetising private users. Certification programmes are usually very expensive to run and not necessarily profitable for vendors.</p>
<p><img alt="A screenshot of the Nextcloud website prompting the user for their email address, and offering them to get started with a provider near them." src="https://ergaster.org/_astro/nextcloud-signup.Do-jCSrM_Z1Cnz2L.webp" width="2880" height="1580" loading="lazy" decoding="async"></p>
<h2 id="not-trusting-anyone">Not trusting anyone</h2>
<p>When buying a hosted service from a provider, we enable a form of partial re-centralisation… which technically allows the provider to start selling the users’ data for profit.</p>
<p>There’s a third option: making sure the data can only be read by its intended recipients, turning the servers into rather dumb pipes. This is End-to-End Encryption (E2EE). It certainly sounds like a silver bullet! So why doesn’t every service provider implement it, to show their good faith? Because it has drawbacks.</p>
<p>When using E2EE, the files are encrypted. Nobody apart from their owner and people who have been explicitly authorised by the owner can read them. This means neither the server software nor the technical administrator of the server can read them either. This is often what users expect, but this has consequences!</p>
<h3 id="the-server-becomes-dumb">The server becomes dumb</h3>
<p>Since the server can’t read the data, there are some legitimate operations it cannot perform anymore. A typical example is “deep” search, which is functionality where the server spends computing time to read and index all the files so it’s easy for the user to query them. When files are encrypted, indexing and search can only happen on the client. Those operations are quite expensive, and the clients don’t necessarily have the computing power, connectivity or storage required to do so.</p>
<p>There are some new techniques such as <a href="https://en.wikipedia.org/wiki/Homomorphic_encryption">homomorphic encryption</a> that could eventually enable users to offload this computation to the server&nbsp;<em>without</em>&nbsp;the server learning anything about what it is actually doing but they are not yet ready for large-scale use.</p>
<p>Having a dumb server also severely limits its ability to send automated reports or alerts based on specific workflows. In a sense, the server can’t “work” for the user anymore and becomes nothing more than a backup service.</p>
<h3 id="data-can-become-irrecoverable">Data can become irrecoverable</h3>
<p>With E2EE, encryption and decryption keys are stored on the device only, which is a significant risk. Let’s assume you host your important documents exclusively on an E2EE service and your keys only exist on your phone. If your phone is broken or lost, your decryption keys are lost with it.</p>
<p>There are three workarounds to avoid losing the keys entirely:</p>
<ol>
<li>Generating a “paper key” (also called “recovery key”, “seed passphrase”, or even “paper wallet” in the context of cryptocurrencies) directly on the client, and giving the user the 12 to 15 words to write down or print somewhere.</li>
<li>Derive the encryption key from the user’s password. This is <a href="https://hacks.mozilla.org/2018/11/firefox-sync-privacy/">the approach Firefox Sync is taking</a> for example.</li>
<li>Storing the encryption keys on the server-side, in a vault encrypted by a key derived from a passphrase. The passphrase must of course be long enough to make it difficult to break by the server administrator.</li>
</ol>
<p>The major inconvenient of these workarounds is that they require the user to either print/write a generated key and store them somewhere safe where they can recover it later, or remember a passphrase to access their en/decryption keys. If the user loses the generated key or can’t remember the passphrase, their data is lost and irrecoverable. The service provider cannot do anything about it because they can’t access the data. In other words: there’s no “forgot my password” link anymore.</p>
<p>The reputational risk of not being able to help users who forgot their password is often unacceptable for service providers. Even Apple who positions itself as a company respectful of their users’ privacy doesn’t turn on E2EE by default, and understandably takes a lot of precautions before <a href="https://support.apple.com/en-gb/108756">allowing users to turn on actual E2EE</a> on their iCloud account.</p>
<h3 id="the-e-in-e2ee-doesnt-stand-for-everything">The “E” in E2EE doesn’t stand for “Everything”</h3>
<p>While E2EE is particularly good at preventing nosy folks from looking at files and messages themselves, it doesn’t mean <em>everything</em> is encrypted. In particular, metadata can be sent in clear text either because it’s necessary for the server to provide the service, <a href="https://ssd.eff.org/module/why-metadata-matters">or because the service providers can profit from it</a>.</p>
<p>Typically, WhatsApp is an E2EE messenger, but the provider still has access to metadata. WhatsApp also has a moderation feature that <a href="https://arstechnica.com/gadgets/2021/09/whatsapp-end-to-end-encrypted-messages-arent-that-private-after-all/">allows users to decrypt an E2EE message they were sent, and send it to Meta for moderation purposes</a>. While moderation is a valid use case which doesn’t break encryption itself, it shows that the client could in theory decrypt the message and send it to the service provider behind the user’s back.</p>
<p><img alt="A screenshot of the iOS App Store displaying the &quot;App Privacy&quot; section of WhatsApp. A lot of items are mentioned in the &quot;Data Linked to You&quot; section." src="https://ergaster.org/_astro/whatsapp-data-collected.D2jizaJk_wsrgM.webp" width="1179" height="2556" loading="lazy" decoding="async"></p>
<p>This highlights that E2EE alone is also not enough: even if users don’t need to trust the service provider they need to be able to trust both the protocol and the client they rely on. This means the client necessarily needs to be open source and audited regularly by an independent third party.</p>
<h2 id="beyond-tech">Beyond tech</h2>
<p>As we have seen, there are several strategies to help the general public trying to escape surveillance capitalism. Self-hosting is efficient but doesn’t scale well, paid instances of self-hostable software work generally well but are not a silver bullet, and E2EE is very useful to protect privacy but don’t provide a full guarantee either.</p>
<p>Ultimately E2EE is a very libertarian approach to a societal issue, taking a “myself against the world” stance. It can be a valid stance, especially for minorities and in hostile contexts. But surveillance capitalism is not a technological problem. It is enabled by technology, but at the very core it is a societal problem.</p>
<p>As Molly White said, “there are never purely technological solutions to societal problems”. To fight surveillance capitalism, we need E2EE, regulation, justice, and education.</p>
<p>We need E2EE to prevent the collection from happening in the first place. We need proper regulation to define what is acceptable or not, which will ultimately define what is a viable business model and what is not. This means that the fines must make it prohibitively unprofitable to sell users’ data. We need justice and executive bodies to actually enforce the regulation. And we need education for the general public to understand the risks of surveillance capitalism.</p>
<p><em>All my gratitude to <a href="https://dkasak.github.io/">Denis Kasak (dkasak)</a>, <a href="https://ergaster.org/posts/2024/01/18-escaping-surveillance-capitalism-at-scale/benj.me">Benjamin Bouvier (bnjbvr)</a>, and <a href="https://github.com/jplatte">Jonas Platte (jplatte)</a> for their valuable time, comments and suggestions on this article .</em></p> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[IBM scraps rewards program for staff inventions, wipes away cash points (128 pts)]]></title>
            <link>https://www.theregister.com/2024/01/17/ibm_inventor_reward_program/</link>
            <guid>39043421</guid>
            <pubDate>Thu, 18 Jan 2024 16:05:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2024/01/17/ibm_inventor_reward_program/">https://www.theregister.com/2024/01/17/ibm_inventor_reward_program/</a>, See on <a href="https://news.ycombinator.com/item?id=39043421">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p><span>Exclusive</span> IBM has canceled a program that rewarded inventors at Big Blue for patents or publications, leaving some angry that they are missing out on potential bonuses.</p>
<p>By cancelling the scheme, a source told <em>The Register</em>, IBM has eliminated a financial liability by voiding the accrued, unredeemed credits issued to program participants which could have been converted into potential cash awards.</p>
<p>For years, IBM has sponsored an "Invention Achievement Award Plan" to incentivize employee innovation. In exchange for filing patents, or for publishing articles that served as defense against rival patents, IBM staff were awarded points that led to recognition and potentially cash bonuses.</p>

    

<p>According to documentation seen by <em>The Register</em>, "Invention points are awarded to all inventors listed on a successful disclosure submission."</p>

        


        

<p>One point was awarded for publishing. Three points were awarded for filing a patent or four if the filing was deemed high value. For accruing 12 points, program participants would get a payout.</p>
<p>﻿﻿"Inventors reach an invention plateau for every 12 points they achieve – which must include at least one file decision," the rules state. And for each plateau achieved, IBM would pay its inventors $1,200 in recognition of their efforts.</p>

        

<p>No longer, it seems. IBM canceled the program at the end of 2023 and replaced it with a new one that uses a different, incompatible point system called BluePoints.</p>
<p>"The previous Invention Achievement Award Plan will be sunset at midnight (eastern time) on December 31st, 2023," company FAQs explain. "Since Plateau awards are one of the items being sunset, plateau levels must be obtained on or before December 31, 2023 to be eligible for the award. Any existing plateau points that have not been applied will not be converted to BluePoints."</p>
<ul>

<li><a href="https://www.theregister.com/2024/01/16/jpmorgan_quantum_banking/">JPMorgan latest to pile into quantum upstart with $5B valuation</a></li>

<li><a href="https://www.theregister.com/2023/12/03/return_to_office/">'Return to Office' declared dead</a></li>

<li><a href="https://www.theregister.com/2023/11/30/watchdog_air_force_contracts/">Watchdog claims retaliation from military after questioning cushy federal IT contracts</a></li>

<li><a href="https://www.theregister.com/2023/11/02/ibm_401k_changes/">IBM to scrap 401(k) matching, offer something else instead</a></li>
</ul>
<p>We're told that IBM's invention review process could take months, meaning that employees just didn't have time between the announcement and the program sunset to pursue the next plateau and cash out.</p>
<p>Those involved in the program evidently were none too pleased by the points grab. And they sought answers from corporate brass through an internal Slack channel.</p>
<p>Citing the revised award scheme, one question read, "Do we allow customers to unilaterally cancel the payment schedule after work has been delivered?"</p>

        

<p>Another read, "Is it reasonable to do this to employees, many of whom worked outside of their regular job to develop IP?"</p>
<p>We're told these represented the most upvoted questions submitted to the CEO's recent monthly Office Hours meeting, but the response was evasive.</p>
<p>IBM did not respond to a request for comment.</p>
<p>A former IBMer reports that a colleague still with Big Blue said, "My opinion...the invention award program was buggered a long time [ago]. It rewarded words on a page instead of true innovation. [Former CEO] Ginni [Rometty] made it worse by advocating the program to fluff up young egos." ®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Flying kites deliver container-sized power generation (174 pts)]]></title>
            <link>https://spectrum.ieee.org/micro-wind-power-kitepower</link>
            <guid>39043306</guid>
            <pubDate>Thu, 18 Jan 2024 15:56:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/micro-wind-power-kitepower">https://spectrum.ieee.org/micro-wind-power-kitepower</a>, See on <a href="https://news.ycombinator.com/item?id=39043306">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-headline="Flying Kites Deliver Container-Sized Power Generation" data-elid="2666900303" data-post-url="https://spectrum.ieee.org/micro-wind-power-kitepower" data-authors="Rahul Rao" data-page-title="Flying Kites Deliver Container-Sized Power Generation - IEEE Spectrum"><p>On average, a humble wind turbine <a href="https://ourworldindata.org/land-use-per-energy-source" rel="noopener noreferrer" target="_blank">uses</a> less land area per megawatt-hour than almost any other power source. Even so, a wind turbine and its tower can sometimes be too cumbersome. </p><p>The still-nascent field of airborne wind energy (AWE) has a solution: Swap out the turbine for a kite on a string. Not only is a kite nimbler than a turbine, it can deliver a more constant energy supply.</p><p>The steady, intense winds some 500 meters above sea level are capable of generating 1800 terawatts: enough to power the entire planet multiple times over. Even an entire flock of Hawks will only tap the lightest touch of that potential power.</p><p>One entrant trying to put AWE to market is the appropriately named <a href="https://thekitepower.com/" target="_blank">Kitepower</a>. This year, the Netherlands-based company will begin shipping its first system: the 40-kilowatt <a href="https://thekitepower.com/kitepower-introduces-self-charging-battery-system-with-new-hawk/" rel="noopener noreferrer" target="_blank">Hawk</a>. Far from replacing traditional turbines, Kitepower hopes the Hawk can power sites that might turn to polluting diesel generators: temporary microgrids, for instance, or remote locations removed from the main grid.</p><p>The Hawk’s kite isn’t the kind you ordinarily fly for leisure. Rather, it is a fiberglass skeleton undergirding an inflatable wing, up to 60 square meters surface area. It flies in “<a href="https://thekitepower.com/the-hawk/" target="_blank">pumping” cycles</a>: the kite reels out, then the winch reels the kite back in. While the Hawk unfurls, the kite weaves to and fro in a figure-eight pattern, optimal to catch crosswinds. As the winds pull at the kite, they also pull at its tether—generating electricity on the ground. </p><p>After the tether reaches its maximum length, the ground station winches the kite back in. Though the Hawk must expend energy for reel-in, it only expends a fraction of the energy, resulting in a net energy gain that varies by wind speed. An entire cycle takes about 100 seconds: 80 for reel-out and 20 for reel-in. </p><p>Additionally, the Hawk comes attached to a 400 kWh battery: crucial for energy storage, especially in the isolated areas that Kitepower is targeting.</p><p><img alt="A diagram showing the Kitepower system, a diesel generator, solar pv array, solar inverters and a community." data-rm-shortcode-id="ec951a5a498c02191f9fce67fba64cc5" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/a-diagram-showing-the-kitepower-system-a-diesel-generator-solar-pv-array-solar-inverters-and-a-community.jpg?id=51060404&amp;width=980" height="1658" id="bfbea" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/a-diagram-showing-the-kitepower-system-a-diesel-generator-solar-pv-array-solar-inverters-and-a-community.jpg?id=51060404&amp;width=980" width="2971"><small data-gramm="false" data-lt-tmp-id="lt-774449" placeholder="Add Photo Caption..." spellcheck="false">Kitepower says their airborne wind energy system neatly complements solar and even emergency conventional generators for a full-spectrum approach to off-grid electricity. </small><small data-gramm="false" data-lt-tmp-id="lt-123188" placeholder="Add Photo Credit..." spellcheck="false">Kitepower</small></p><p>“The power of airborne wind is that you have a much lower material footprint than a wind turbine or solar PV,” says <a href="https://www.tudelft.nl/staff/r.schmehl/" rel="noopener noreferrer" target="_blank">Roland Schmehl</a>, Kitepower’s co-founder and a mechanical engineer at <a href="https://www.tudelft.nl/en/" target="_blank">Technical University Delft</a> in the Netherlands. “This means a lower environmental footprint, which also means that you could go to areas which are more sensitive.”</p><p>AWE has been slow to take flight. Take <a data-linked-post="2650278034" href="https://spectrum.ieee.org/alphabets-moonshot-wind-kites-to-fly-offshore" target="_blank">Google’s Makani</a>, which sought to generate wind power with a colossal 26-meter-wide drone. Makani’s 600-kilowatt demonstrator, tethered to a floating buoy at sea, attracted significant attention from the public and <a href="https://www.ft.com/content/c39cdb50-2e91-11e9-8744-e7016697f225" target="_blank">investment from Royal Dutch Shell</a>. None of that stopped <a href="https://spectrum.ieee.org/tag/google">Google</a> from <a href="https://spectrum.ieee.org/exclusive-airborne-wind-energy-company-closes-shop-opens-patents" target="_blank">grounding Makani</a> in 2020, citing high costs.</p><p>The entire system fits into a standard shipping container, and Kitepower says assembly at a new site takes less than 24 hours.</p><p>Still, AWE proponents can find signs that the winds are shifting in their field’s favor. In December 2022, the <a href="https://skysails-power.com/" target="_blank">German company SkySails</a> Power <a href="https://skysails-power.com/revolutionary-airborne-wind-energy-system-in-operation-in-the-republic-of-mauritius/" rel="noopener noreferrer" target="_blank">launched</a> the world’s first fully autonomous commercial AWE system: a 100-kilowatt generator tethered to a parachute-shaped kite flying 400 meters over the island of Mauritius.</p><p>Now, the Hawk is almost ready to join the skies. Kitepower is putting a trial version through its final tests at a site on the north coast of Ireland. Getting the Hawk into the air was not easy: Schmehl compares building the Hawk to building an aircraft. “It was relatively straightforward to make technical demonstrators and to show, for example, how to automatically harvest wind energy,” he says. “But to do this in a failure-proof way, that nothing breaks and no software bugs occur…that took us a long time.”</p><p>AWE’s potential rewards are immense. Although traditional wind turbines <a href="https://www.energy.gov/eere/articles/wind-turbines-bigger-better" rel="noopener noreferrer" target="_blank">have steadily grown in size</a> over the course of this century, the average turbine’s tower is still only about 100 meters tall. The Hawk can fly as high as 350 meters off the ground. The higher you go in Earth’s lower atmosphere, the stronger and steadier the winds become.</p><p>As of late 2022, the world <a href="https://gwec.net/globalwindreport2023/" rel="noopener noreferrer" target="_blank">contained</a> about 900 megawatts of wind power capacity. A 2013 study <a href="https://www.nature.com/articles/nclimate1683.epdf?sharing_token=yKH-kzQKZ1GsYmKLoFgw0tRgN0jAjWel9jnR3ZoTv0P4DlGwJ_WSeU2FSQSapDPd3lSXvKX7Kx2M74_UJq3iEym9lxqtbGj8-EWCduBOgyCMqeNSGZH3xuFKDCmAdRdY7p8yxgGe-7stL3rtB32q7PXBs3Ibm3dJ-w8jauMj142k6U-RxZz4LGfJge4HH7a7DLLZ03d-zps6uKRGJSGGKuPgf0GpCg2bH4HlbzE9JSAMqnVoR7wWYfbwUChVgzb9xJnZ7C0UfIGcaAHK6it6kwQAJLjHm8eINjxhAgi_AMxVszlVC81BfoGpeK6lVh6wt7V7MX9KQVpfdyCaA0KKM4RFJmfYq6ROUgYRGVbKHx8%3D&amp;tracking_referrer=spectrum.ieee.org" target="_self">suggested</a> that the steady, intense winds some 500 meters above sea level are capable of generating 1800 terawatts: enough to power the entire planet multiple times over. Even an entire flock of Hawks will only tap the lightest touch of that potential power. </p><p>But the Hawk is not a substitute for existing wind turbines. Kitepower believes that the Hawk is ideal for temporary events or users removed from the main grid: farms, construction sites, music festivals, humanitarian efforts, island communities. The entire system fits into a standard shipping container, and Kitepower says assembly at a new site takes less than 24 hours.</p><p>Today, these locations might rely on a diesel generator—whose logistical demands make the Hawk cost-competitive, according to Schmehl. “You need to bring the fuel—that drives the electricity generation prices to very high numbers, depending on the location,” he says. </p><p>The Hawk serves as a proof of concept for Kitepower’s next goal: a 100-kilowatt AWE system called <a href="https://thekitepower.com/the-falcon/" rel="noopener noreferrer" target="_blank">the Falcon</a>. Kitepower plans to put the Falcon on the market in 2025 or 2026. </p><p>Beyond that, Schmehl envisions 500-kilowatt containerized kites, comparable to the output of a modest wind farm with as much as 90 percent less material.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hans Reiser on ReiserFS deprecation in the Linux kernel (350 pts)]]></title>
            <link>https://ftp.mfek.org/Reiser/Letters/№2%20Hans→Fred/reiser_response.html</link>
            <guid>39042626</guid>
            <pubDate>Thu, 18 Jan 2024 15:10:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ftp.mfek.org/Reiser/Letters/&#x2116;2%20Hans&#x2192;Fred/reiser_response.html">https://ftp.mfek.org/Reiser/Letters/&#x2116;2%20Hans&#x2192;Fred/reiser_response.html</a>, See on <a href="https://news.ycombinator.com/item?id=39042626">Hacker News</a></p>
Couldn't get https://ftp.mfek.org/Reiser/Letters/№2%20Hans→Fred/reiser_response.html: TypeError [ERR_UNESCAPED_CHARACTERS]: Request path contains unescaped characters]]></description>
        </item>
        <item>
            <title><![CDATA[PostgreSQL Operations Cheat Sheet: featuring security, scaling, structure, etc. (115 pts)]]></title>
            <link>https://wiki.postgresql.org/wiki/Operations_cheat_sheet</link>
            <guid>39042456</guid>
            <pubDate>Thu, 18 Jan 2024 14:59:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wiki.postgresql.org/wiki/Operations_cheat_sheet">https://wiki.postgresql.org/wiki/Operations_cheat_sheet</a>, See on <a href="https://news.ycombinator.com/item?id=39042456">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="mw-content-text" lang="en" dir="ltr">

<h2><span id="Introduction">Introduction</span></h2>
<p>This page is aimed at learning from the wisdom of the PostgreSQL community.
</p><p>People involved in the PostgreSQL community, as organizations or individuals, are posting vast amounts of useful information on blogs, wikis, and websites. However, they are scattered, and it will not be easy to find the information you are looking for.
</p><p>Therefore, this page has compiled a collection of links to articles that the editor(s) informative, from hundreds of blog sites registered on Planet PostgreSQL, plus the PostgreSQL wiki and websites. Some notable topics from these articles are picked up here, trying to organize and summarize them for introduction purposes. Please feel free to add links to the articles you find helpful to others. Adding the summaries of them is also appreciated in addition to the links.
</p><p><b>Notes</b>
</p>
<ul><li>do not expect everything to be extracted from those articles! Diving directly into the original articles is strongly recommended.</li>
<li>Some configuration parameters, functions, statistics views, and system tables/views may only be available after some major versions. Consult PostgreSQL documentation for availability.</li></ul>

<h2><span id="Architecture">Architecture</span></h2>
<h3><span id="Client-Server_architecture">Client-Server architecture</span></h3>
<ul><li>Server
<ul><li>Database server: postgres</li>
<li>Server application: ex. initdb, pg_ctl, pg_upgrade</li></ul></li>
<li>Client
<ul><li>Client interface: ex. libpq, ECPG, pgJDBC, psqlODBC, Npgsql</li>
<li>Client application: ex. psql, pgbench, pg_dump, pg_restore</li></ul></li>
<li>Frontend/Backend protocol
<ul><li>Frontend=client, Backend=server</li>
<li>message-based protocol for communication between frontends and backends over TCP/IP and Unix-domain sockets</li>
<li>Current version has been 3.0 since PostgreSQL 7.4 in 2003</li></ul></li>
<li>Compatibility between client and server
<ul><li>psql works best with servers of the same or an older major version. It also works with servers of a newer major version, although backslash commands are likely to fail.</li>
<li>pg_dump can dump from servers older than itself (servers back to 9.2. are supported). It cannot dump from servers newer major versions.</li>
<li>Driver is server-version agnostic: Always use the latest driver version</li>
<li>ex. pgJDBC supports 8.2+ server, Npgsql is tested with supported 5 server major versions.</li></ul></li></ul>
<h3><span id="Logical_database_structures">Logical database structures</span></h3>
<ul><li>Database cluster is a collection of databases, roles, and tablespaces</li>
<li>The database cluster initially contains some databases after initdb:
<ul><li>template1: a new database is cloned from this, unless another template database is specified</li>
<li>template0: a pristine copy of the original contents of template1</li>
<li>postgres: default database used by utilities and users</li></ul></li>
<li>Each database contains its own system catalogs that store the metadata of its local database objects</li>
<li>The database cluster contains some shared system catalogs that store the metadata of the cluster-wide global objects
<ul><li>Shared system catalogs can be accessed from inside each database</li>
<li>Query to get shared system catalogs: <code>SELECT relname FROM pg_class WHERE relisshared AND relkind = 'r';</code></li>
<li>ex. pg_authid, pg_database, pg_tablespace</li></ul></li>
<li>Tablespace
<ul><li>pg_global ($PGDATA/global/): store shared system catalogs</li>
<li>pg_default ($PGDATA/base/): store template0, template1, postgres. The default tablespace for other databases.</li>
<li>User tablespaces: created with <code>CREATE TABLESPACE name LOCATION 'dir_path';</code></li></ul></li></ul>
<h3><span id="Database_object_hierarchy">Database object hierarchy</span></h3>
<ul><li>Database
<ul><li>Access method</li>
<li>Cast</li>
<li>Event trigger</li>
<li>Extension</li>
<li>Foreign-data wrapper</li>
<li>Foreign server</li>
<li>Procedural language</li>
<li>Publication</li>
<li>Row-level security policy (the name must be distinct from that of any other policy for the table)</li>
<li>Rule (the name must be distinct from that of any other rule for the same table)</li>
<li>Schema
<ul><li>Aggregate function</li>
<li>Collation</li>
<li>Conversion</li>
<li>Data type</li>
<li>Domain</li>
<li>Extended statistics</li>
<li>Foreign table</li>
<li>Function</li>
<li>Index</li>
<li>Materialized view</li>
<li>Operator</li>
<li>Operator class</li>
<li>Operator family</li>
<li>Procedure</li>
<li>Sequence</li>
<li>Table</li>
<li>Text search configuration</li>
<li>Text search dictionary</li>
<li>Text search parser</li>
<li>Text search template</li>
<li>Trigger (inherits the schema of its table)</li>
<li>View</li></ul></li>
<li>Subscription</li>
<li>Transform</li>
<li>User mapping</li></ul></li>
<li>Role</li>
<li>Tablespace</li></ul>
<h3><span id="Object_identifier_(OID)"></span><span id="Object_identifier_.28OID.29">Object identifier (OID)</span></h3>
<ul><li>OIDs are used internally by PostgreSQL as primary keys for various system tables.
<ul><li>ex. <code>SELECT oid, relname FROM pg_class WHERE relname = 'mytable';</code></li></ul></li>
<li>Type oid represents an OID.</li>
<li>oid is an unsigned four-byte integer.</li>
<li>An OID is allocated from a single cluster-wide counter, so it is not large enough to provide database-wide uniqueness.
<ul><li>A specific object is identified by two OIDs (classid and objid) in pg_depend and pg_shdepend.</li></ul></li></ul>
<h3><span id="Physical_database_structures">Physical database structures</span></h3>
<p>Directories
</p>
<ul><li>Data directory ($PGDATA)
<ul><li>base/: Subdirectory containing per-database subdirectories</li>
<li>global/: Subdirectory containing cluster-wide tables, such as pg_database</li>
<li>pg_multixact/: Subdirectory containing multitransaction status data (used for shared row locks)</li>
<li>pg_subtrans/: Subdirectory containing subtransaction status data</li>
<li>pg_tblspc/: Subdirectory containing symbolic links to tablespaces</li>
<li>pg_wal/: Subdirectory containing WAL (Write-Ahead Log) files</li>
<li>pg_xact/: Subdirectory containing transaction commit status data</li></ul></li>
<li>Configuration file directories (optional)</li>
<li>Tablespace directories (optional)</li>
<li>WAL directory (optional)</li></ul>
<p>Files in data directory
</p>
<ul><li>Configuration files (postgresql.conf, pg_hba.conf, pg_ident.conf): Can be stored in other directories</li>
<li>Control file (global/pg_control): Stores control info such as the cluster state, checkpoint log location, next OID, next XID</li>
<li>Regular relation data file
<ul><li>A relation is a set of tuples: table, index, sequence, materialized view, etc.</li>
<li>Each relation has its own set of files.</li>
<li>Each file consists of 8 KB blocks.</li>
<li>Lazy allocation: A new heap table file contains 0 blocks, while a new B-tree index file contains 1 block.</li>
<li>There are some types of data files (forks): main, FSM, VM, initialization</li>
<li>Main fork (<code>base/&lt;database_OID&gt;/&lt;relation_filenode_no&gt;</code>)
<ul><li>ex. <code>"SELECT pg_relation_filepath('mytable');"<code> returns </code>base/17354/32185<code>, where 17354 is the database's OID and 32185 is the mytable's filenode number</code></code></li><code><code>
<li>Stores tuple data.</li></code></code></ul></li><code><code>
<li>FSM (free space map) fork (<code>base/&lt;database_OID&gt;/&lt;relation_filenode_no&gt;_fsm</code>)
<ul><li>Keeps track of free space in the relation.</li>
<li>Entries are organized as a tree, where each leaf node entry stores free space in one relation block.</li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/pgfreespacemap.html">pg_freespacemap</a> and <a rel="nofollow" href="https://www.postgresql.org/docs/current/pageinspect.html">pageinspect</a> can be used to examine its contents.</li></ul></li>
<li>VM (visibility map) fork (<code>base/&lt;database_OID&gt;/&lt;relation_filenode_no&gt;_vm</code>)
<ul><li>Keeps track of:
<ul><li>which pages contain only tuples that are known to be visible to all active transactions</li>
<li>which pages contain only frozen tuples</li></ul></li>
<li>Each heap relation has a Visibility Map; an index does not have one.</li>
<li>Stores two bits per heap page:
<ul><li>All-visible bit: if set, the page does not contain any tuples that need to be vacuumed. Also used by index-only scans  to answer queries using only the index tuple.</li>
<li>All-frozen bit: if set, all tuples on the page have been frozen, therefore vacuum can skip the page.</li></ul></li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/pgvisibility.html">pg_visibility</a> can be used to examine its contents.</li></ul></li>
<li>Initialization fork (base/&lt;database_OID&gt;/&lt;relation_filenode_no&gt;_init)
<ul><li>Each unlogged table and index has an initialization fork.</li>
<li>The content is empty: table is 0 block, index is 1 block.</li>
<li>Unlogged relations are reset during recovery: the initialization fork is copied over the main fork, and other forks are erased.</li></ul></li></code></code></ul></li><code><code>
<li>Temporary relation data file
<ul><li><code>base/&lt;database_OID&gt;/tBBB_FFF</code>
<ul><li>BBB is the backend ID of the backend which created the file, and FFF is the filenode number</li>
<li>ex. <code>base/5/t3_16450</code></li></ul></li>
<li>Has main, FSM, and VM forks, but not the initialization fork.</li></ul></li>
<li>A large relation is divided into 1 GB segment files.
<ul><li>e.g., <code>12345, 12345.1, 12345.2, ...</code></li></ul></li></code></code></ul><p><code><code><p>Page (= block)
</p></code></code><code><code><ul><li>Each page is 8 KB. Configurable when building PostgreSQL.</li>
<li>Relations have the same format.</li>
<li>The content is the same in memory and on storage.</li>
<li>Each page stores multiple data values called items. In a table, an item is a row; in an index, an item is an index entry.</li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/pageinspect.html">pageinspect</a> can be used to examine the content.</li>
<li>Layout of a page:
<ol><li>Page header: 24 bytes</li>
<li>An array of item identifiers pointing to the actual items: Each entry is an (offset,length) pair. 4 bytes per item.</li>
<li>Free space</li>
<li>Items</li>
<li>Special space: 0 byte for tables, different bytes for index types (btree, GIN, GiST, etc.)</li></ol></li></ul></code></code><code><code><p>Table row
</p></code></code><code><code><ul><li><a rel="nofollow" href="https://www.postgresql.org/docs/current/pageinspect.html">[1]</a> pageinspect] can be used to examine the content.</li>
<li>Layout of a row
<ol><li>Header: 23 bytes</li>
<li>Null bitmap (optional): 1 bit for each column</li>
<li>User data: columns of the row</li></ol></li></ul></code></code><code><code><h3><span id="Instance">Instance</span></h3></code></code><code><code><p>The instance is a group of server-side processes, their local memory, and the shared memory.
</p><p>Processes
</p></code></code><code><code><ul><li>Single-threaded: postmaster launches a single backend process for each client connection. Thus, each SQL execution only uses a single CPU core. Parallel query, index build, VACUUM etc. can utilize multiple CPU cores by running multiple server processes.</li>
<li>postmaster: The parent of all server processes. Controls the server startup and shutdown. Create shared memory and semaphores. Launches other server processes and reaps dead ones. Opens and listens on TCP ports and/or Unix domain sockets, accepts connection requests, and spawns client backends to pass the connection requests to.</li>
<li>(Client) backend: Acts on behalf of a client session and handles its requests, i.e., executes SQL commands.</li>
<li>Background processes
<ul><li>logger: Catches all stderr output from other processes through pipes, and writes them to log files.</li>
<li>checkpointer: Handles all checkpoints.</li>
<li>background writer: Periodically wakes up and writes out dirty shared buffers so that other processes don't have to write them when they need to free a shared buffer to read in another page.</li>
<li>startup: Performs crash and point-in-time recovery. Ends as soon as the recovery is complete.</li>
<li>stats collector: Receives messages from other processes through UDP sockets, accumulates statistics about server activity, and writes them to files. The statistics can be viewed with pg_stat... views. This process is gone as of PostgreSQL 15.</li>
<li>walwriter: Periodically wakes up and writes out WAL buffers to reduce the amount of WAL that other processes have to write. Also ensures the writes of commit WAL records from asynchronously committed transactions.</li>
<li>archiver: Archives WAL files.</li>
<li>autovacuum launcher: Always running when autovacuum is enabled. Schedules autovacuum workers to run.</li>
<li>autovacuum worker: Connect to a database as determined in the launcher, examine system catalogs to select the tables, and vacuum them.</li>
<li>parallel worker: Executes part of a parallel query plan.</li>
<li>walreceiver: Runs on the standby server. Receives WAL from the walsender, stores it on disk, and tells the startup process to continue recovery based on it.</li>
<li>walsender: Runs on the primary server. Sends WAL to a single walreceiver.</li>
<li>logical replication launcher: Run on the subscriber. Coordinates  logical replication workers to run.</li>
<li>logical replication worker: Runs on the subscriber. An apply worker per subscription receives logical changes from walsender on the publisher and applies them. One or more tablesync workers perform initial table copy for each table.</li></ul></li>
<li>Background worker: Runs system-supplied or user-supplied code. e.g., used for parallel query and logical replication.</li></ul></code></code><code><code><p>Memory
</p></code></code><code><code><ul><li>Shared memory
<ul><li>Shared buffers: Stores the cached copy of data files (main, FSM, and VM forks).</li>
<li>WAL buffers: Transactions put WAL records here before writing them out to disk.</li>
<li>Other various areas: One large shared memory segment is divided into areas for specific uses.</li>
<li>The allocations of areas can be examined with <a rel="nofollow" href="https://www.postgresql.org/docs/current/view-pg-shmem-allocations.html">pg_shmem_allocations</a>.</li></ul></li>
<li>Local memory
<ul><li>Work memory: Allocated for a query operation such as sort and hash. Configured with work_mem and hash_mem_multiplier parameters.</li>
<li>Maintenance work memory: Allocated for maintenance operations, such as VACUUM, CREATE INDEX, and ALTER TABLE. Configured with maintenance_work_mem parameter.</li>
<li>Temporary buffers: Allocated for caching temporary table blocks. Configured with temp_buffers parameter.</li>
<li>Other various areas: A memory context is allocated for a specific usage (e.g., a message from client, transaction, query plan, execution state). Hundreds of memory contexts per session are possible.</li>
<li>The allocation and usage of memory contexts can be examined with <a rel="nofollow" href="https://www.postgresql.org/docs/current/view-pg-backend-memory-contexts.html">pg_backend_memory_contexts</a> view for the current session, and with the function <code>pg_log_backend_memory_contexts(backend_pid)</code> for other sessions.</li></ul></li></ul></code></code><code><code><h3><span id="Reading_and_writing_database_data">Reading and writing database data</span></h3></code></code><code><code><ul><li>Read:
<ul><li>First, search the shared buffers for a buffer containing the target block. If found, it's returned to the requester.</li>
<li>Otherwise, allocate a buffer from a free buffer list, and read the target block from the data file into the buffer.</li>
<li>If there's no free buffer, evict and use a used buffer. If it's dirty, writes out the buffer to disk.</li></ul></li>
<li>Write
<ul><li>Find the target shared buffer, modify its contents, and write the changes to the WAL buffers.</li>
<li>The modifying transaction writes out its WAL records from the WAL buffers to disk, including the commit WAL record.</li>
<li>The modified dirty shared buffers are flushed to disk by background writer, checkpointer, or any other process. This is asynchronous with the transaction completion.</li></ul></li>
<li>Any backend can read and write shared buffers, WAL buffers, data and WAL files. Unlike some other DBMSs, writes are not exclusively performed by a particular background process.</li>
<li>The database data file is read and written one block at a time. There's no multiblock I/O.</li>
<li>Some operations bypass shared buffers: the write of an index during index creation, CREATE DATABASE, ALTER TABLE ... SET TABLESPACE</li></ul></code></code><code><code><h3><span id="Query_processing">Query processing</span></h3></code></code><code><code><ol><li>A client connects to a database, sends a query (SQL command) to the server, and receives the result.</li>
<li>The parser first checks the query for correct syntax. Then, it interprets the semantics of the query to understand which tables, views, functions, data types, etc. are referenced.</li>
<li>The rewrite system (rewriter) transforms the query based on the rules stored in the system catalog pg_rewrite. One example is the view: a query that accesses a view is rewritten to use the base table.</li>
<li>The planner/optimizer creates a query plan.</li>
<li>The executor executes the query plan and returns the result set to the client.</li></ol></code></code><code><code><p>Notes
</p></code></code><code><code><ul><li>Each session runs on a connection to a single database. Therefore, it cannot access tables on a different database. However, one session can connect to another database and create another session via a foreign data wrapper like postgres_fdw, and access tables there. For example, an SQL command can join one table on the local database and another table on a remote database.</li>
<li>Each SQL command basically uses only one CPU core. A parallel query and some utility commands such as CREATE INDEX and VACUUM can use multiple CPU cores by running background workers.</li></ul></code></code><code><code><h3><span id="References">References</span></h3></code></code><code><code><p>PostgreSQL Documentation
</p></code></code><code><code><ul><li><a rel="nofollow" href="https://www.postgresql.org/docs/current/overview.html">Overview of PostgreSQL Internals</a></li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/storage.html">Database Physical Storage</a></li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/runtime.html">Server Setup and Operation</a></li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/managing-databases.html">Managing Databases</a></li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/protocol.html">Frontend/Backend Protocol</a></li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/datatype-oid.html">Object Identifier Types</a></li></ul></code></code><code><code><p>Other resources
</p></code></code><code><code><ul><li><a rel="nofollow" href="https://www.interdb.jp/pg/pgsql01.html">Database Cluster, Databases, and Tables</a></li>
<li><a rel="nofollow" href="https://www.interdb.jp/pg/pgsql02.html">Query Processing</a></li>
<li><a rel="nofollow" href="https://www.interdb.jp/pg/pgsql03.html">Process and Memory Architecture</a></li>
<li><a rel="nofollow" href="https://en.wikibooks.org/wiki/PostgreSQL/Architecture">PostgreSQL/Architecture</a></li>
<li><a rel="nofollow" href="https://severalnines.com/blog/understanding-postgresql-architecture/">Understanding the PostgreSQL Architecture</a></li>
<li><a rel="nofollow" href="https://www.crunchydata.com/blog/postgres-databases-and-schemas">Postgres Databases and Schemas</a></li>
<li><a rel="nofollow" href="https://www.crunchydata.com/blog/postgres-data-flow">Postgres Data Flow</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/postgresql-writer-and-wal-writer-processes-explained/">POSTGRESQL: WRITER AND WAL WRITER PROCESSES EXPLAINED</a></li>
<li><a rel="nofollow" href="https://www.interdb.jp/pg/pgsql08.html">Buffer Manager</a></li></ul></code></code><code><code></code></code><code><code><h2><span id="Reliability_and_availability">Reliability and availability</span></h2></code></code><code><code><h3><span id="Connection">Connection</span></h3></code></code><code><code><p>What to check when troubleshooting connectivity
</p></code></code><code><code><ul><li>Is the database server reachable?
<ul><li><code>telnet &lt;host&gt; &lt;port&gt;/<code>, or</code></code></li><code><code>
<li><code>nc -zv &lt;host&gt; &lt;port&gt;</code></li>
<li>Use <code>traceroute</code> (Unix/Linux) or <code>tracert/<code> (Windows), specifying the protocol allowed by the host and intermediary routers.</code></code></li></code></code></ul></li><code><code><code>
<li>Are the host and port correct?</li>
<li>Is the server running?
<ul><li><code>pg_ctl status</code></li></ul></li>
<li>Does the server-side firewall allow communication through the port?</li>
<li>Does the client-side firewall allow communication to the server port?</li>
<li>Does pg_hba.conf have any entry that allow the combination of SSL/non-SSL, client host, database and user?</li>
<li>Is the listen_addresses parameter configured to allow connection through the desired IP addresses, including IPv4 and/or IPv6?</li>
<li>Are the database, user name, and password correct?</li>
<li>Does the user have permission to connect to the database?
<ul><li>Check privileges with psql's <code>\l</code> or pg_database.datacl</li></ul></li>
<li>Does the database server have enough CPU and memory resources?</li>
<li>Isn't the maximum connection limit reached?
<ul><li>max_connections and superuser_reserved_connections parameters (at instance level)</li>
<li>CREATE/ALTER DATABASE CONNECTION LIMIT (at database level)</li>
<li>CREATE/ALTER ROLE CONNECTION LIMIT (at user level)</li></ul></li></code></code></code></ul></code></code><code><code><code><code><p>Connection termination and query cancellation
</p></code></code></code></code><code><code><code><code><ul><li>When the connection is closed, any incomplete transaction is rolled back.</li>
<li>Terminating a connection (<code>pg_terminate_backend()</code>) and canceling a query (<code>pg_cancel_backend()</code>) does not always work. For example, they don't work while the backend process is running in an uninterruptible section, such as waiting to acquire a lightweight lock, a read/write system call against a network storage device, and a loop without a cancellation point.</li>
<li>Set statement_timeout at appropriate levels (statement, user, database, instance). A short timeout is not recommended at a wide level because it cancels intentional long-running queries.</li>
<li>Set client-side timeouts appropriately.</li>
<li>Set server-side timeouts appropriately.
<ul><li>tcp_keepalives_idle, tcp_keepalives_interval, tcp_keepalives_count
<ul><li>TCP keep-alive works while the TCP connection is idle. It does not work when the socket connection is being established, or some data has been sent and waiting for its ACK.</li>
<li>The effective timeout is tcp_keepalives_idle + tcp_keepalives_interval * tcp_keepalives_count.</li></ul></li>
<li>tcp_user_timeout
<ul><li>Sets the TCP retransmission timeout. Relatively newly available on Linux.</li>
<li>Comes to the rescue when TCP keep-alive doesn't help. i.e., when the socket connection is being established, or some data has been sent and waiting for its ACK.</li>
<li>Confusing when used together with TCP keep-alive because this changes when TCP keep-alive times out. It would be safe to set tcp_user_timeout to tcp_keepalives_idle + tcp_keepalives_interval * tcp_keepalives_count.</li></ul></li>
<li>authentication_timeout</li>
<li>idle_in_transaction_session_timeout</li>
<li>idle_session_timeout</li>
<li>client_connection_check_interval</li></ul></li></ul></code></code></code></code><code><code><code><code><p>Connection failover
</p></code></code></code></code><code><code><code><code><ul><li>Client drivers allow multiple hosts to be specified in the connection string.
<ul><li>Connection timeout is applied to each host in the connection string. Therefore, making a connection may take unexpectedly long if there are many failed hosts before the running one in the list.</li></ul></li>
<li>Restore session state after failover: session variables, prepared statements, temporary tables, holdable cursors (created with DECLARE CURSOR WITH HOLD), advisory locks, session user (set with SET SESSION AUTHORIZATION), current user (set with SET ROLE).</li>
<li>Be careful about transaction retry: It's unknown whether the transaction was committed or rolled back when a database server failover happens during the transaction commit.
<ul><li><code>pg_xact_status( xid8 )</code> can be used to determine the transaction outcome.</li></ul></li></ul></code></code></code></code><code><code><code><code><h3><span id="WAL">WAL</span></h3></code></code></code></code><code><code><code><code><p>What WAL is for
</p></code></code></code></code><code><code><code><code><ul><li>Crash recovery, archive recovery (Point-In-Time-Recovery: PITR), and replication</li>
<li>Updates are redone regardless of whether the transaction was committed.</li>
<li>There is no undo log (before image) or operation, unlike other popular DBMSs.
<ul><li>Therefore, transaction rollback and crash recovery is fast.</li>
<li>Changes by an aborted transaction are left in memory and on disk, but they are invisible to other transactions thanks to MVCC.</li></ul></li></ul></code></code></code></code><code><code><code><code><p>WAL structure
</p></code></code></code></code><code><code><code><code><ul><li>A sequence of 8 KB blocks.</li>
<li>Each block can contain multiple WAL records. Also, each WAL record can span multiple blocks.</li>
<li>The content is the same both in memory and on storage.</li>
<li>The WAL buffer is a contiguous array of blocks in memory. It's used in a circular fashion.</li>
<li>The WAL on storage is divided into WAL segment files. Each WAL segment file is 16 MB by default, which is configurable with <code>initdb</code>'s <code>--wal-segsize=size</code>.</li></ul></code></code></code></code><code><code><code><code><p>Writing WAL
</p></code></code></code></code><code><code><code><code><ul><li>In memory, modify the data pages in shared buffers, and then write the change into the WAL buffer.</li>
<li>WAL buffer is always written to WAL files sequentially (no random write).</li>
<li>Before writing a dirty data page in a shared buffer out to disk, first all WAL records up to the latest one that affected the data page. This rule is the WAL (Write Ahead Log).
<ul><li>Each data page has, in its page header, the location (LSN) of the WAL record that represents the latest update to it. This is the page LSN.</li>
<li>LSN (Log Sequence Number): an unsigned 8-byte integer. It represents the WAL segment, block, and an offset in that block.</li></ul></li>
<li>If writing to the WAL file fails, the instance will crash with a PANIC message.</li>
<li>WAL volume can grow beyond max_wal_size due to the reasons such as:
<ul><li>Heavy writes like loading data with COPY</li>
<li>failure to archive WAL files</li>
<li>the large value of wal_keep_size</li>
<li>an unused replication slot</li></ul></li>
<li>SELECT can modify data pages and write WAL when:
<ul><li>acquiring row locks, e.g., SELECT FOR UPDATE. They set xmax in the tuple header, and could possibly update MultiXact data structures.</li>
<li>pruning line pointers and defragmenting the page.</li>
<li>setting hint bits to tuple headers. WAL is emitted when page checksums are enabled.</li></ul></li></ul></code></code></code></code><code><code><code><code></code></code></code></code><code><code><code><code><h3><span id="Transaction">Transaction</span></h3></code></code></code></code><code><code><code><code><p>ACID: what they are attributed to
</p></code></code></code></code><code><code><code><code><ul><li>Atomicity: transaction rollback and database recovery</li>
<li>Consistency: integrity constraints and triggers, such as non-NULL, check, primary key/unique/foreign key constraints</li>
<li>Isolation: MVCC and locks</li>
<li>Durability: WAL</li></ul></code></code></code></code><code><code><code><code><p>Transaction ID (XID)
</p></code></code></code></code><code><code><code><code><ul><li>A transaction is assigned an XID when it first modifies data, such as in INSERT, UPDATE, DELETE, and SELET FOR SHARE/UPDATE.
<ul><li>XID assignments are serialized with XidGen LWLock.</li>
<li>XID assignment is usually very fast, but it might sometimes experience hiccups. It allocates and zeros a new commit log (clog) page through SLRU cache every 32K transactions. That clog page allocation could possibly flush a dirty page for page replacement.</li>
<li>This could cause an unpredictable spike of response time.</li></ul></li>
<li>Read-only transactions do not assign an XID. They are free from the LWLock contention for assigning a new XID.</li>
<li>XIDs are stored in tuple headers and visible as xmin and xmax system columns (<code>SELECT xmin, xmax, * FROM mytable</code>).
<ul><li>xmin is the XID of a transaction that created the tuple (INSERT, UPDATE, COPY).</li>
<li>xmax is the XID of a transaction that either:
<ul><li>deleted the tuple (DELETE, UPDATE).</li>
<li>locked the tuple (e.g., SELECT FOR SHARE/UPDATE)</li></ul></li></ul></li>
<li>Special XID values
<ul><li>0: invalid XID</li>
<li>1: bootstrap XID. Used by bootstrap processing during initdb.</li>
<li>2: Frozen XID: Recent versions of PostgreSQL only use this for sequence tuples, not for tables.</li></ul></li></ul></code></code></code></code><code><code><code><code><p>MVCC: Multi-Version Concurrency Control
</p></code></code></code></code><code><code><code><code><ul><li>The major advantage of MVCC is "reading never blocks writing and writing never blocks reading." i.e., UPDATE/DELETE and SELECT on the same row do not block each other.
<ul><li>Writes to the same row block each other.</li>
<li>In the traditional lock-based concurrency control, read and write on the same row conflict.</li></ul></li>
<li>How it works overall:
<ul><li>Insert and update to a row create a new version of the row. Update leaves the old row version for other running transactions. (Multi-version)</li>
<li>XID of the creating transaction is set to xmin field in the new row version.</li>
<li>The new row version is only visible to its creating transaction until it commits.</li>
<li>Once the creating transaction commits, all new subsequent transactions will be able to see the new row version. Other existing transactions continue to see the old row version. The old row version is a "dead tuple" now.</li>
<li>Delete to a row does not remove the row version. It sets the XID of the deleting transaction to xmax field in the row version.</li>
<li>The deleted row version is only invisible to its deleting transaction until it commits.</li>
<li>Once the deleting transaction commits, all new subsequent transactions won't be able to see the row version. Other existing transactions continue to see the row version. The row version is a "dead tuple" now.</li>
<li>Finally, when there are no transactions remaining that can see the dead tuple, vacuum removes it.</li></ul></li>
<li>How tuple visibility works:
<ul><li>Each transaction uses its own snapshot, commit log (clog), and the xmin and/or xmax in the target tuple header, to determine whether it can see a given row version.</li>
<li>What snapshot is:
<ul><li>A picture of what transactions are running at a certain point of time.</li>
<li>You can run "<code>SELECT pg_current_snapshot();</code>" to see the snapshot of the current transaction.</li>
<li>The snapshot's textual representation is xmin:xmax:xip_list. e.g., 10:20:10,14,15.</li>
<li>xmin: Lowest transaction ID that was still active. All transaction IDs less than xmin are either committed and visible, or rolled back and dead.</li>
<li>xmax: One past the highest completed transaction ID. All transaction IDs greater than or equal to xmax had not yet completed as of the time of the snapshot, and thus are invisible.</li>
<li>xip_list: Transactions in progress at the time of the snapshot. A transaction ID that is xmin &lt;= X &lt; xmax and not in this list was already completed at the time of the snapshot, and thus is either visible or dead according to its commit status.</li>
<li>In a READ COMMITTED transaction, a snapshot is taken at the beginning of every SQL statement.</li>
<li>In a REPEATABLE READ or SERIALIZABLE transaction, a snapshot is obtained at the start of the first SQL statement and used throughout the transaction.</li></ul></li>
<li>What commit log (clog) is:
<ul><li>An array of bits representing the transaction status.</li>
<li>Two bits are used to indicate a transaction's outcome: IN PROGRESS, COMMITTED, ABORTED, SUBCOMMITTED.</li>
<li>Stored in a set of files in $PGDATA/pg_xact/.</li>
<li>Cached in memory buffers of 128 8 KB pages.</li></ul></li>
<li>Clog is consulted when the snapshot shows that the target transaction has been completed.</li>
<li>Based on the snapshot and clog, the change by a committed transaction is visible, and that by an aborted or running transaction is invisible.</li>
<li>The actual tuple visibility is much more complex...</li></ul></li></ul></code></code></code></code><code><code><code><code><p>Hint bit
</p></code></code></code></code><code><code><code><code><ul><li>Hint bits are the bits in the tuple header's infomask field that help determine tuple visibility.</li>
<li>They are for performance optimization. Not essential to data correctness.</li>
<li>They represent whether the transaction indicated by xmin or xmax was committed or aborted. There are four flag bits:
<ol><li><code>HEAP_XMIN_COMMITTED</code>: xmin transaction was committed</li>
<li><code>HEAP_XMIN_INVALID</code>: xmin transaction was aborted</li>
<li><code>HEAP_XMAX_COMMITTED</code>: xmax transaction was committed</li>
<li><code>HEAP_XMAX_INVALID</code>: xmax transaction was aborted</li></ol></li>
<li>How hint bits are used:
<ol><li>A transaction checks the hint bits to see if the xmin and/or xmax transaction was committed or aborted.</li>
<li>If the hint bits are set, done.</li>
<li>Otherwise, examine the commit log ($PGDATA/pg_xact/), and possibly the subtransaction hierarchy ($PGDATA/pg_subtrans/) to determine the transaction outcome. This is an expensive operation.</li>
<li>Set the hint bits. They will be persisted to disk later.</li></ol></li>
<li>Setting hint bits writes a data page, and can also write WAL if page checksums are enabled.</li></ul></code></code></code></code><code><code><code><code><h3><span id="Lock">Lock</span></h3></code></code></code></code><code><code><code><code><p>A lock request can wait, even when the requested mode is compatible with held locks.
</p></code></code></code></code><code><code><code><code><ul><li>Q: Do you think Transaction 3 goes on to run the query?
<ol><li>Transaction 1: A long-running <code>SELECT</code> is still running against mytable.</li>
<li>Transaction 2: Run "<code>ALTER TABLE mytable ADD COLUMN new_col int;</code>". Get blocked because ALTER TABLE's Access Exclusive lock request conflicts with the Access Share lock held by Transaction 1's <code>SELECT</code>.</li>
<li>Transaction 3: Run a short <code>SELECT</code> query against mytable.</li></ol></li>
<li>A: Transaction 3 waits until Transaction 2 completes, because Transaction 2 came earlier and is waiting.</li>
<li>Later requestors respect earlier waiters in the wait queue and do not overtake them. Otherwise, earlier requestors might wait for an unduly long time.</li>
<li>Therefore, execute even the DDL that is expected to run fast:
<ul><li>during off-peak hours, and/or</li>
<li>with a lock timeout. e.g., run "<code>SET lock_timeout = '5s';</code>" before the DDL. Retry the DDL if it times out.</li></ul></li>
<li>This is not true for lightweight locks. In extreme cases, an Exclusive mode request on a LWLock could wait for dozens of seconds due to later Share mode requestors coming one after another.</li></ul></code></code></code></code><code><code><code><code><p>Prepared transactions lurk holding locks
</p></code></code></code></code><code><code><code><code><ul><li>A prepared transaction continues to hold locks, but it does not appear in <a rel="nofollow" href="https://www.postgresql.org/docs/current/monitoring-stats.html#MONITORING-PG-STAT-ACTIVITY-VIEW">pg_stat_activity</a> because it has no associated session.</li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/view-pg-locks.html">pg_locks</a> shows the prepared transaction as an entry having NULL pid. Check pg_prepared_xacts.</li></ul></code></code></code></code><code><code><code><code></code></code></code></code><code><code><code><code><h3><span id="Data_integrity_validation">Data integrity validation</span></h3></code></code></code></code><code><code><code><code><p>Data checksums
</p></code></code></code></code><code><code><code><code><ul><li>Purpose and usage
<ul><li>The data page of every relation, including all forks, has a 16-bit checksum in its page header.</li>
<li>Designed to detect corruption by the I/O system (e.g., volume manager, file system, disk driver, storage firmware, storage device, etc.) Early detection prevents the propagation of corruption.</li>
<li>Not designed to detect memory errors.</li>
<li>Enabled at the full cluster level, either with <code>initdb</code>'s -k/--data-checksums or with <code>pg_checksums</code> while the database server is shut down. Disabled by default due to its performance overhead.</li>
<li>Run <code>"SHOW data_checksums"</code> to know if data checksums are enabled. It returns on or off.</li></ul></li>
<li>How it works
<ul><li>The checksum is calculated from the page content and set when the data page is about to be written out to disk.</li>
<li>Just after reading the page from disk, the checksum is verified by comparing the value set in the page header and the newly calculated value.</li>
<li>If the verification fails, a WARNING and ERROR messages are emitted, resulting in a query failure.</li>
<li>If the page header fails a basic sanity check before performing checksum verification, the query will fail with the same ERROR message, without the WARNING that indicates the checksum failure.</li></ul></li></ul></code></code></code></code><code><code><code><code><p>WAL CRC
</p></code></code></code></code><code><code><code><code><ul><li>WAL uses a 32-bit CRC in each WAL record header.</li>
<li>The CRC is set when the WAL record is put in the WAL buffer, and verified when the WAL record is read.</li></ul></code></code></code></code><code><code><code><code><p>Utilities to detect, bypass, or repair data corruption (some could be dangerous!)
</p></code></code></code></code><code><code><code><code><ul><li>Additional modules
<ul><li><a rel="nofollow" href="https://www.postgresql.org/docs/current/amcheck.html">amcheck</a>: Detects logical corruption of heaps (table, sequence, materialized view) and B-tree indexes.</li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/pgsurgery.html">pg_surgery</a>: <code>heap_force_kill()</code> and <code>heap_force_freeze()</code> forcibly removes and freezes heap tuples respectively.</li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/pgvisibility.html">pg_visibility_map</a>: <code>pg_check_frozen()</code> and <code>pg_check_visible()</code> detect visibility map corruption.</li></ul></li>
<li>Configuration parameters
<ul><li>ignore_checksum_failure</li>
<li>zero_damaged_pages</li>
<li>ignore_system_indexes</li></ul></li></ul></code></code></code></code><code><code><code><code></code></code></code></code><code><code><code><code><h3><span id="Backup_and_recovery">Backup and recovery</span></h3></code></code></code></code><code><code><code><code><p>Backup and recovery methods
</p></code></code></code></code><code><code><code><code><ol><li>File system level backup (binary format)</li>
<li>SQL Dump with pg_dump/pg_dumpall (text format)</li>
<li>Continuous archiving (binary format)</li></ol></code></code></code></code><code><code><code><code><p>Characteristics of backup and recovery methods
</p></code></code></code></code><code><code><code><code><ul><li>SQL dump and continuous archiving can be performed online. The file system level backup requires the database server to be shut down.</li>
<li>SQL dump can selectively back up and restore individual tables. The other methods cannot back up or restore only certain individual tables or tablespaces.</li>
<li>The SQL dump will typically be smaller, because the SQL script needs to contain just the index creation command, not the index data.</li>
<li>The SQL dump can be loaded into a database of a newer major version.</li>
<li>The SQL dump can transfer a database to a different machine architecture, such as going from a 32-bit to a 64-bit server.</li>
<li>Continuous archiving can perform PITR. The database cluster can be recovered up-to-date or to a certain point of time.</li>
<li>Dumps created by pg_dump are consistent; the dump of each database is a snapshot of the database when pg_dump started. pg_dumpall calls pg_dump for each database in turn, so the database cluster-wide consistency is not guaranteed.</li>
<li>pg_dump dumps all data in a database within a single transaction, issuing many SELECT commands. That long-running transaction could:
<ul><li>block other operations that require strong lock modes, such as ALTER TABLE, TRUNCATE, CLUSTER, REINDEX.</li>
<li>cause table and index bloat, because vacuum cannot remove dead tuples.</li></ul></li>
<li><code>pg_backup_start()</code> and <code>pg_basebackup</code> of continuous archiving performs a checkpoint at the beginning. The user can choose the checkpoint speed between "fast" and "spread".</li>
<li>Archive recovery, as well as crash recovery, empties the content of unlogged relations. SQL dump outputs the contents of unlogged tables.</li>
<li>pg_dumpall's --no-role-passwords option uses pg_roles instead of pg_authid to dump database roles. This allows the use of pg_dumpall in restricted environments like DBaaS where users are not permitted to read pg_authid to protect passwords. The restored roles will have NULL passwords.</li></ul></code></code></code></code><code><code><code><code></code></code></code></code><code><code><code><code><h3><span id="Streaming_replication">Streaming replication</span></h3></code></code></code></code><code><code><code><code><p>Architecture
</p></code></code></code></code><code><code><code><code><ul><li>Topology
<ul><li>Only the entire database cluster is replicated. Partial replication is not possible.</li>
<li>One primary server replicates to one or more standby servers.</li>
<li>Each standby replicates from one primary.</li>
<li>The standby can cascade changes to other standbys.</li>
<li>The primary is unaware of the locations of standbys. The standby connects to the primary specified by primary_conninfo parameter.
<ul><li>ex. <code>primary_conninfo = 'host=192.168.1.50 port=5432 user=foo password=foopass'</code></li></ul></li></ul></li>
<li>Primary and standby versions
<ul><li>Different major versions don't work.</li>
<li>Different minor versions will work because the disk format is the same, but no formal support is offered. It's advised to keep primary and standby servers at the same minor version.</li>
<li>It's safest to update the standby servers first during minor version upgrade.</li></ul></li>
<li>Processes and data flow
<ul><li>At server startup, the standby first reads and applies WAL from archive, next from $PGDATA/pg_wal/, and then launches one walreceiver, which connects to the primary and streams WAL from there. If the replication connection is terminated, it repeats this cycle at 5 second intervals, which can be configured by wal_retrieve_retry_interval.</li>
<li>The primary spawns the walsender when it accepts the connection request from walreceiver.</li>
<li>walsender reads and sends WAL to the walreceiver.</li>
<li>walreceiver writes and flushes the streamed WAL to $PGDATA/pg_wal/, and notifies the startup process.</li>
<li>A single startup process reads and applies the WAL.</li>
<li>walreceiver periodically notifies the walsender of replication progress -- how far it has written, flushed, and applied the WAL.</li>
<li>A cascading standby has walsenders as well as a walreceiver running.</li></ul></li>
<li>Replication user
<ul><li>The replication user needs REPLICATION role attribute.</li>
<li>REPLICATION enables the user to read all data for replication, but not for consumption by SELECT queries.</li></ul></li></ul></code></code></code></code><code><code><code><code><p>General administration
</p></code></code></code></code><code><code><code><code><ul><li>The standby is read-only. Any object, including roles, cannot be created only on the standby.</li>
<li>max_wal_senders should be slightly higher than the number of standby servers, so that the standby can accept connections after a temporary unexpected disconnection while the disconnected walsender still remains.</li>
<li>Backups can be taken on the standby.</li>
<li>archive_timeout is not required to reduce the data loss window.</li>
<li>Cascading replication reduces the load on the primary.</li>
<li>WAL on the primary
<ul><li>Without any measure, the primary does not care about the standby and remove/recycle old WAL files that the standby still need.</li>
<li>If the standby requests WAL that has already been removed, the primary emits a message like <code>"ERROR: requested WAL segment 000000020000000300000041 has already been removed"</code>.</li>
<li>To make the primary preserve WAL files, either use a replication slot (preferred) or set keep_wal_size (old method).</li>
<li>max_slot_wal_keep_size caps the WAL volume preserved by the replication slot.</li></ul></li>
<li>Synchronous replication
<ul><li>The transaction hangs during its commit if no synchronous standby is available.</li>
<li>To resume the hanging transaction, remove synchronous_standby_names setting and reload the configuration. That makes the replication asynchronous.</li></ul></li></ul></code></code></code></code><code><code><code><code><p>Causes of replication lag
</p></code></code></code></code><code><code><code><code><ul><li>Hardware configuration: Server, storage, and network</li>
<li>Heavy workload on the primary: The amount of WAL generated on the primary is so large that the solo startup process cannot keep up.
<ul><li>Set wal_compression = on to reduce the amount of WAL.</li></ul></li>
<li>Retrieving WAL from slow archive: The standby could not get WAL from the primary, so it has to fetch it from the WAL archive.</li>
<li>Recovery conflicts: The replay of some operations can be blocked by queries running on the standby.
<ul><li>This is relevant when hot standby is used.</li>
<li>Reduce max_standby_archive_delay and max_standby_streaming_delay to cancel conflicting queries and resume WAL replay early.</li></ul></li></ul></code></code></code></code><code><code><code><code><p>Hot standby
</p></code></code></code></code><code><code><code><code><ul><li>The ability to run read-only queries while the server is in archive recovery or standby mode.</li>
<li>To determine if the server is in hot standby, use <code>"SHOW in_hot_standby"</code> for PostgreSQL 14+, or <code>"SELECT pg_is_in_recovery()"</code> otherwise.</li>
<li>Recovery conflicts
<ul><li>Conflicts between the WAL replay and queries on the standby.</li>
<li>Either delay WAL replay or cancel queries.</li>
<li>Actions on the primary that cause recovery conflicts include:
<ul><li>Operations that take Access Exclusive locks: DDL, LOCK, file truncation by vacuum (including autovacuum)
<ul><li>Access Exclusive lock requests are WAL-logged and replayed by the standby.</li></ul></li>
<li>Dropping a tablespace where queries put temporary files on the standby</li>
<li>Dropping a database to which clients are connected on the standby</li>
<li>Vacuum cleanup of dead tuples that standby transaction still can see according to their snapshots</li>
<li>Vacuum cleanup of a page on which standby transactions have a buffer pin (e.g., the cursor is positioned on the page.)</li></ul></li>
<li>What happens upon recovery conflicts
<ul><li>WAL application waits for at most the period specified by max_standby_archive_delay and max_standby_streaming_delay (except for the replay of DROP DATABASE and ALTER DATABASE SET TABLESPACE.)</li>
<li>Then, conflicting sessions are terminated in the case of replaying DROP DATABASE, or conflicting queries are canceled in other cases.</li>
<li>If an idle session holds a lock, the session is also terminated.</li></ul></li>
<li>Monitoring recovery conflicts
<ul><li><a rel="nofollow" href="https://www.postgresql.org/docs/current/monitoring-stats.html#MONITORING-PG-STAT-DATABASE-CONFLICTS-VIEW">pg_stat_database_conflicts</a> on the standby shows the number of canceled queries due to each type of recovery conflict.</li>
<li>"log_recovery_conflict_waits = on" logs messages that the WAL application has waited longer than deadlock_timeout and the wait finished.
<ul><li><code>LOG:  recovery still waiting after 1.023 ms: recovery conflict on snapshot</code></li>
<li><code>DETAIL:  Conflicting processes: 1234, 1235</code></li>
<li><code>LOG:  recovery finished waiting after 3.234 ms: recovery conflict on snapshot</code></li></ul></li></ul></li>
<li>Minimizing the number of queries cancelled due to recovery conflict
<ul><li>Avoid operations that require Access Exclusive locks. e.g., ALTER TABLE, VACUUM FULL, CLUSTER, REINDEX, TRUNCATE</li>
<li>Disable file truncation by vacuum by setting vacuum_truncate storage parameter on the primary.
<ul><li>ex. <code>ALTER TABLE some_table SET (vacuum_truncate = off);</code></li></ul></li>
<li>Set hot_standby_feedback = on the standby.
<ul><li>sends the oldest XID to the primary, reflected in pg_stat_replication.backend_xmin, which is taken into account when vacuum decides to remove a dead tuple.</li>
<li>can incur table bloat because dead tuple removal is delayed.</li>
<li>cannot prevent all conflicts.</li></ul></li>
<li>Adjust max_standby_streaming_delay/max_standby_archive_delay on the standby.</li>
<li>Adjust vacuum_defer_cleanup_age on the primary.</li></ul></li>
<li>It's ideal to have separate standbys, some for high availability and others for read workloads that tolerate stale data.</li></ul></li></ul></code></code></code></code><code><code><code><code><p>Monitoring replication lag
</p></code></code></code></code><code><code><code><code><ul><li><a rel="nofollow" href="https://www.postgresql.org/docs/current/monitoring-stats.html#MONITORING-PG-STAT-REPLICATION-VIEW">pg_stat_replication</a>
<ul><li>Available not only on the primary but also on the cascading standby.</li>
<li>Large differences between pg_current_wal_lsn and the view's sent_lsn field might indicate that the primary server is under heavy load.</li>
<li>Differences between sent_lsn and pg_last_wal_receive_lsn on the standby might indicate network delay, or that the standby is under heavy load.</li></ul></li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/monitoring-stats.html#MONITORING-PG-STAT-WAL-RECEIVER-VIEW">pg_stat_wal_receiver</a>
<ul><li>A large difference between pg_last_wal_replay_lsn() and the view's flushed_lsn indicates that WAL is being received faster than it can be replayed.</li>
<li>ex. <code>SELECT pg_wal_lsn_diff(pg_last_wal_replay_lsn(), flushed_lsn) FROM pg_stat_wal_receiver;</code></li></ul></li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/monitoring-stats.html#MONITORING-PG-STAT-WAL-VIEW">pg_stat_wal</a>
<ul><li>Check the amount of WAL generated for heavy write workload.</li></ul></li>
<li>Storage write latency, IOPs, and throughput to check for heavy write activity.</li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/monitoring-stats.html#MONITORING-PG-STAT-DATABASE-CONFLICTS-VIEW">pg_stat_database_conflicts</a></li></ul></code></code></code></code><code><code><code><code></code></code></code></code><code><code><code><code><h3><span id="Logical_replication">Logical replication</span></h3></code></code></code></code><code><code><code><code><p>Architecture
</p></code></code></code></code><code><code><code><code><ul><li>Uses a publish and subscribe model
<ul><li>A publication is a collection of tables whose changes are to be replicated.</li>
<li>A subscription represents a connection to the publisher and its publications to subscribe to.</li>
<li>One publisher publishes one or more publications.</li>
<li>One subscriber has one or more subscriptions.</li>
<li>A publication can have multiple subscribers.</li>
<li>A subscription can subscribe to multiple publications.</li>
<li>Publications can choose to limit the changes they produce to any combination of INSERT, UPDATE, DELETE, and TRUNCATE.</li>
<li>Publications can restrict the rows and columns to be replicated.</li></ul></li>
<li>Processes and data flow
<ul><li>Processes involved: walsender on the publisher, subscription workers (apply worker, tablesync worker) on the subscriber.</li>
<li>walreceiver does not appear, even though some walreceiver-related parameters are used.</li></ul>
<ol><li>At the server startup on the subscriber, logical replication launcher is started unless max_logical_replication_workers is 0.</li>
<li>logical replication launcher starts an apply worker for each enabled subscription.</li>
<li>The apply worker connects to the publisher.</li>
<li>The apply worker launches tablesync workers for tables that have not completed initial synchronization. Those tablesync workers each connect to the publisher.</li>
<li>The publisher spawns a walsender for each connection request from the subscription workers.</li>
<li>The walsender for a tablesync worker sends the initial copy of a table to the tablesync worker. (Initial data synchronization/copy)</li>
<li>walsender reads WAL, decodes changes into the logical replication protocol format, and store them in the logical decoding work memory and possibly file. When a transaction commits, walsender sends its decoded changes to the subscription workers.</li>
<li>The subscription workers apply the received changes.</li></ol></li></ul></code></code></code></code><code><code><code><code><p>General administration
</p></code></code></code></code><code><code><code><code><ul><li>Major restrictions
<ul><li>Publications can only contain tables.</li>
<li>DDL are not replicated.
<ul><li>Add table columns on the subscriber first, then on the publisher. Reverse the order when dropping table columns.</li></ul></li>
<li>Sequence data is not replicated.</li></ul></li>
<li>Replication identity
<ul><li>A published table must have a replica identity to replicate UPDATE and DELETE operations.</li>
<li>Used as a key to identify rows to update or delete on the subscriber.</li>
<li>UPDATE and DELETE fail on the publisher if the published table has no replica identity. INSERT succeeds.</li>
<li>Can be either of the primary key (by default), unique index, or the full row.</li>
<li>Can be configured by <code>ALTER TABLE REPLICA IDENTITY</code>.</li>
<li>The old values of replica identity columns are WAL-logged.</li></ul></li>
<li>Tuning performance
<ul><li>max_sync_workers_per_subscription
<ul><li>Multiple tablesync workers (one for each table) will run in parallel based on the max_sync_workers_per_subscription configuration.</li>
<li>This may be effective to speed up initial table synchronization when there are many tables in a subscription.</li></ul></li>
<li>logical_decoding_work_mem
<ul><li>Check <a rel="nofollow" href="https://www.postgresql.org/docs/current/monitoring-stats.html#MONITORING-PG-STAT-REPLICATION-SLOTS-VIEW">pg_stat_replication_slots</a> for spilled transactions to disk. If spill_txns, spill_count, and spill_bytes are high, consider increasing this parameter value.</li></ul></li></ul></li></ul></code></code></code></code><code><code><code><code><p>Replication conflicts
</p></code></code></code></code><code><code><code><code><ul><li>The application of incoming changes on the subscriber may fail due to constraint violation or lack of permission. This is the conflict.</li>
<li>Resolving conflicts:
<ol><li>Disable the subscription if it's not yet by running <code>"ALTER SUBSCRIPTION name DISABLE;"</code>. The subscription can be configured to be automatically disabled when any errors are detected by the apply worker. Run <code>"ALTER SUBSCRIPTION ... WITH (disable_on_error = on);"</code></li>
<li>Look up the replication origin name and the end LSN of a conflicting transaction in the server log.</li>
<li>Do either of:
<ul><li>skip the transaction from publisher by running <code>"ALTER SUBSCRIPTION ... SKIP (end LSN of a conflicting transaction)"</code> or <code>"SELECT pg_replication_origin_advance(rep_origin_name, next LSN of the end of a conflicting transaction)"</code></li>
<li>fix the table data on the subscriber.</li></ul></li>
<li>Enable the subscription by running <code>"ALTER SUBSCRIPTION name ENABLE;"</code>.</li></ol></li></ul></code></code></code></code><code><code><code><code><p>Monitoring
</p></code></code></code></code><code><code><code><code><ul><li>Publisher
<ul><li><a rel="nofollow" href="https://www.postgresql.org/docs/current/monitoring-stats.html#MONITORING-PG-STAT-REPLICATION-SLOTS-VIEW">pg_stat_replication_slots</a>
<ul><li>One row per logical replication slot. Total number of transactions and amount of decoded data, and number of transactions and amount of decoded data that were spilled to disk.</li></ul></li>
<li>Other system and statistics views used for streaming replication.</li></ul></li>
<li>Subscriber
<ul><li><a rel="nofollow" href="https://www.postgresql.org/docs/current/monitoring-stats.html#MONITORING-PG-STAT-SUBSCRIPTION-STATS">pg_stat_subscription_stats</a>
<ul><li>One row per subscription. Numbers of errors during the initial table synchronization and while applying changes.</li></ul></li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/monitoring-stats.html#MONITORING-PG-STAT-SUBSCRIPTION">pg_stat_subscription</a>
<ul><li>One row per subscription worker (apply worker, tablesync worker). Table being copied, last WAL location sent/reported etc.</li></ul></li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/catalog-pg-subscription-rel.html">pg_subscription_rel</a>
<ul><li>One row for each subscribed table. Status of the table to know whether the initial synchronization is in progress.</li></ul></li></ul></li></ul></code></code></code></code><code><code><code><code></code></code></code></code><code><code><code><code><h3><span id="References_2">References</span></h3></code></code></code></code><code><code><code><code><p>PostgreSQL Documentation
</p></code></code></code></code><code><code><code><code><ul><li><a rel="nofollow" href="https://www.postgresql.org/docs/current/server-start.html#CLIENT-CONNECTION-PROBLEMS">Client Connection Problems</a></li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/sql-discard.html">DISCARD</a></li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/wal.html">Reliability and the Write-Ahead Log</a></li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/mvcc.html">Concurrency Control</a></li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/backup.html">Backup and Restore</a></li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/high-availability.html">High Availability, Load Balancing, and Replication</a></li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/logical-replication.html">Logical Replication</a></li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/transactions.html">Transaction Processing</a></li></ul></code></code></code></code><code><code><code><code><p>Connection
</p></code></code></code></code><code><code><code><code><ul><li><a rel="nofollow" href="https://mydbanotebook.org/post/cant-connect/">Can't connect to Postgres</a></li>
<li><a rel="nofollow" href="https://repost.aws/knowledge-center/rds-cannot-connect">How do I resolve problems when connecting to my Amazon RDS DB instance?</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/premiumsupport/knowledge-center/rds-dropped-db-connection/">Why did DB connections drop on my RDS DB instance?</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/premiumsupport/knowledge-center/rds-aurora-postgresql-connection-errors/">How do I troubleshoot the error "could not send data to client" or "could not receive data from client" when connecting to my Amazon RDS PostgreSQL or Aurora PostgreSQL-Compatible DB instance?</a></li>
<li><a rel="nofollow" href="https://mydbanotebook.org/post/active-directory/">SSO on Postgres with Active Directory&nbsp;: troubleshooting</a></li>
<li><a rel="nofollow" href="https://www.depesz.com/2021/01/12/waiting-for-postgresql-14-add-idle_session_timeout/">Waiting for PostgreSQL 14 – Add idle_session_timeout.</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/terminating-database-connections-in-postgresql/">TERMINATING DATABASE CONNECTIONS IN POSTGRESQL</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/cancel-hanging-postgresql-query/">Cancel a hanging PostgreSQL query</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/idle_in_transaction_session_timeout-terminating-idle-transactions-in-postgresql/">IDLE_IN_TRANSACTION_SESSION_TIMEOUT: TERMINATING IDLE TRANSACTIONS IN POSTGRESQL</a></li>
<li><a rel="nofollow" href="https://www.crunchydata.com/blog/control-runaway-postgres-queries-with-statement-timeout">Control Runaway Postgres Queries With Statement Timeout</a></li>
<li><a rel="nofollow" href="https://momjian.us/main/blogs/pgblog/2017.html#August_14_2017">Session State Failover</a></li>
<li><a rel="nofollow" href="https://www.citusdata.com/blog/2021/02/12/reconnecting-your-app-after-a-postgres-failover/">Reconnecting your application after a Postgres failover</a></li>
<li><a rel="nofollow" href="https://www.postgresql.org/message-id/160741519849.70#13355787096244067178%40wrigleys.postgresql.org">Wrong configuration of tcp_user_timeout can terribly affects tcp_keepalives mechanism</a></li>
<li><a rel="nofollow" href="https://www.2ndquadrant.com/en/blog/postgresql-10-transaction-traceability/">Transaction traceability in PostgreSQL 10 with txid_status(…)</a></li></ul></code></code></code></code><code><code><code><code><p>WAL
</p></code></code></code></code><code><code><code><code><ul><li><a rel="nofollow" href="https://www.interdb.jp/pg/pgsql09.html">Write Ahead Logging - WAL</a></li>
<li><a rel="nofollow" href="http://eulerto.blogspot.com/2011/11/understanding-wal-nomenclature.html">Understanding WAL nomenclature</a></li>
<li><a rel="nofollow" href="https://momjian.us/main/blogs/pgblog/2018.html#December_5_2018">The Meaning of Wal</a></li>
<li><a rel="nofollow" href="https://blog.okmeter.io/postgresql-why-and-how-wal-bloats-2252578985c7">PostgreSQL: why and how WAL bloats</a></li></ul></code></code></code></code><code><code><code><code><p>Transaction
</p></code></code></code></code><code><code><code><code><ul><li><a rel="nofollow" href="https://www.interdb.jp/pg/pgsql05.html">Concurrency Control</a></li>
<li><a rel="nofollow" href="https://www.interdb.jp/pg/pgsql06.html">Vacuum Processing</a></li>
<li><a rel="nofollow" href="https://blog.okmeter.io/postgresql-exploring-how-select-queries-can-produce-disk-writes-f36c8bee6b6f">PostgreSQL: Exploring how SELECT Queries can produce disk writes</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/whats-in-an-xmax/">xmax, its various meanings and how to tell them apart</a></li>
<li><a rel="nofollow" href="https://devcenter.heroku.com/articles/postgresql-concurrency">PostgreSQL Concurrency with MVCC</a></li>
<li><a rel="nofollow" href="https://brandur.org/postgres-atomicity">How Postgres Makes Transactions Atomic</a></li>
<li><a rel="nofollow" href="http://rhaas.blogspot.com/2017/12/mvcc-and-vacuum.html">MVCC and VACUUM</a></li>
<li><a rel="nofollow" href="https://blog.anayrat.info/en/2018/11/12/postgresql-and-heap-only-tuples-updates-part-1/">PostgreSQL and heap-only-tuples updates - part 1</a></li>
<li><a rel="nofollow" href="https://blog.anayrat.info/en/2018/11/19/postgresql-and-heap-only-tuples-updates-part-2/">PostgreSQL and heap-only-tuples updates - part 2</a></li>
<li><a rel="nofollow" href="https://paquier.xyz/postgresql-2/postgres-9-6-feature-highlight-pg-visibility/">Postgres 9.6 feature highlight - pg_visibility</a></li>
<li><a rel="nofollow" href="https://wiki.postgresql.org/wiki/Hint_Bits">Hint Bits</a></li>
<li><a rel="nofollow" href="https://write-skew.blogspot.com/2018/05/serializable-in-postgresql-11-and-beyond.html">SERIALIZABLE in PostgreSQL 11... and beyond</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/disabling-autocommit-in-postgresql-can-damage-your-health/">DISABLING AUTOCOMMIT IN POSTGRESQL CAN DAMAGE YOUR HEALTH</a></li></ul></code></code></code></code><code><code><code><code><p>Lock
</p></code></code></code></code><code><code><code><code><ul><li><a rel="nofollow" href="https://www.interdb.jp/pg/pgsql05.html">Concurrency Control</a></li>
<li><a rel="nofollow" href="https://www.interdb.jp/pg/pgsql06.html">Vacuum Processing</a></li>
<li><a rel="nofollow" href="https://blog.okmeter.io/postgresql-exploring-how-select-queries-can-produce-disk-writes-f36c8bee6b6f">PostgreSQL: Exploring how SELECT Queries can produce disk writes</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/whats-in-an-xmax/">xmax, its various meanings and how to tell them apart</a></li>
<li><a rel="nofollow" href="https://devcenter.heroku.com/articles/postgresql-concurrency">PostgreSQL Concurrency with MVCC</a></li>
<li><a rel="nofollow" href="https://brandur.org/postgres-atomicity">How Postgres Makes Transactions Atomic</a></li>
<li><a rel="nofollow" href="http://rhaas.blogspot.com/2017/12/mvcc-and-vacuum.html">MVCC and VACUUM</a></li>
<li><a rel="nofollow" href="https://blog.anayrat.info/en/2018/11/12/postgresql-and-heap-only-tuples-updates-part-1/">PostgreSQL and heap-only-tuples updates - part 1</a></li>
<li><a rel="nofollow" href="https://blog.anayrat.info/en/2018/11/19/postgresql-and-heap-only-tuples-updates-part-2/">PostgreSQL and heap-only-tuples updates - part 2</a></li>
<li><a rel="nofollow" href="https://paquier.xyz/postgresql-2/postgres-9-6-feature-highlight-pg-visibility/">Postgres 9.6 feature highlight - pg_visibility</a></li>
<li><a rel="nofollow" href="https://wiki.postgresql.org/wiki/Hint_Bits">Hint Bits</a></li>
<li><a rel="nofollow" href="https://write-skew.blogspot.com/2018/05/serializable-in-postgresql-11-and-beyond.html">SERIALIZABLE in PostgreSQL 11... and beyond</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/disabling-autocommit-in-postgresql-can-damage-your-health/">DISABLING AUTOCOMMIT IN POSTGRESQL CAN DAMAGE YOUR HEALTH</a></li></ul></code></code></code></code><code><code><code><code><p>Data integrity validation
</p></code></code></code></code><code><code><code><code><ul><li><a rel="nofollow" href="https://wiki.postgresql.org/wiki/Corruption">Corruption</a></li>
<li><a rel="nofollow" href="https://ardentperf.com/2019/11/08/postgresql-invalid-page-and-checksum-verification-failed/">PostgreSQL Invalid Page and Checksum Verification Failed</a></li></ul></code></code></code></code><code><code><code><code><p>Backup and recovery
</p></code></code></code></code><code><code><code><code><ul><li><a rel="nofollow" href="https://www.crunchydata.com/blog/introduction-to-postgres-backups">Introduction to Postgres Backups</a></li>
<li><a rel="nofollow" href="https://www.interdb.jp/pg/pgsql10.html">Base Backup &amp; Point-in-Time Recovery</a></li>
<li><a rel="nofollow" href="https://www.crunchydata.com/blog/how-to-recover-when-postgresql-is-missing-a-wal-file">How to Recover When PostgreSQL is Missing a WAL File</a></li>
<li><a rel="nofollow" href="https://blog.hagander.net/logging-transactions-that-dropped-tables-236/">Logging transactions that dropped tables</a></li>
<li><a rel="nofollow" href="https://www.thatguyfromdelhi.com/2017/03/using-pgdumpall-with-aws-rds-postgres.html">Using pg_dumpall with AWS RDS Postgres</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/jp/blogs/database/managed-database-backup-and-recovery-in-a-multi-tenant-saas-application/">Managed database backup and recovery in a multi-tenant SaaS application</a></li></ul></code></code></code></code><code><code><code><code><p>Streaming replication
</p></code></code></code></code><code><code><code><code><ul><li><a rel="nofollow" href="https://www.interdb.jp/pg/pgsql11.html">Streaming Replication</a></li>
<li><a rel="nofollow" href="https://www.enterprisedb.com/blog/how-set-streaming-replication-keep-your-postgresql-database-performant-and-date">How to Set Up Streaming Replication to Keep Your PostgreSQL Database Performant and Up-to-Date</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/monitoring-replication-pg_stat_replication/">MONITORING REPLICATION: PG_STAT_REPLICATION</a></li>
<li><a rel="nofollow" href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PostgreSQL.Replication.ReadReplicas.html">Working with read replicas for Amazon RDS for PostgreSQL</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/jp/blogs/database/best-practices-for-amazon-rds-postgresql-replication/">Best practices for Amazon RDS PostgreSQL replication</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/number-one-thing-to-watch-out-for-when-doing-postgres-streaming-replication/">NUMBER ONE THING TO WATCH OUT FOR WHEN DOING POSTGRES STREAMING REPLICATION</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/streaming-replication-conflicts-in-postgresql/">DEALING WITH STREAMING REPLICATION CONFLICTS IN POSTGRESQL</a></li>
<li><a rel="nofollow" href="https://richyen.com/replication/postgres/hot_standby_feedback/2019/03/05/i_fought_the_wal.html">I Fought the WAL, and the WAL Won: Why hot_standby_feedback can be Misleading</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/jp/blogs/database/manage-long-running-read-queries-on-amazon-aurora-postgresql-compatible-edition/">Manage long-running read queries on Amazon Aurora PostgreSQL-Compatible Edition</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/premiumsupport/knowledge-center/rds-postgresql-error-conflict-recovery/">How do I troubleshoot the error "canceling statement due to conflict with recovery" when querying the read replica for my RDS for PostgreSQL DB instance?</a></li>
<li><a rel="nofollow" href="https://richyen.com/replication/postgres/2019/01/08/zombie_transactions.html">Zombies!! Dealing with a Case of Stuck TransactionIDs</a></li>
<li><a rel="nofollow" href="https://hevodata.com/learn/postgresql-replication-slots/#1">Working with PostgreSQL Replication Slots: A Comprehensive Analysis</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/jp/blogs/database/best-practices-for-amazon-rds-for-postgresql-cross-region-read-replicas/">Best practices for Amazon RDS for PostgreSQL cross-Region read replicas</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/monitoring-postgresql-replication/">MONITORING POSTGRESQL REPLICATION</a></li></ul></code></code></code></code><code><code><code><code><p>Logical replication
</p></code></code></code></code><code><code><code><code><ul><li><a rel="nofollow" href="https://www.postgresql.fastware.com/blog/inside-logical-replication-in-postgresql">Inside logical replication in PostgreSQL: How it works</a></li>
<li><a rel="nofollow" href="https://www.crunchydata.com/blog/data-to-go-postgres-logical-replication">Data To Go: Postgres Logical Replication</a></li>
<li><a rel="nofollow" href="https://blog.anayrat.info/en/2018/03/10/logical-replication-internals/">Logical replication internals</a></li>
<li><a rel="nofollow" href="https://www.postgresql.fastware.com/blog/logical-replication-tablesync-workers">Logical Replication Tablesync Workers</a></li>
<li><a rel="nofollow" href="https://www.postgresql.fastware.com/blog/how-to-gain-insight-into-the-pg-stat-replication-slots-view-by-examining-logical-replication">How to gain insight into the pg_stat_replication_slots view by examining logical replication</a></li>
<li><a rel="nofollow" href="https://www.postgresql.fastware.com/blog/how-postgresql-15-improved-communication-in-logical-replication">How PostgreSQL 15 improved communication in logical replication</a></li>
<li><a rel="nofollow" href="https://www.postgresql.fastware.com/blog/introducing-publication-row-filters">Introducing publication row filters</a></li>
<li><a rel="nofollow" href="https://www.postgresql.fastware.com/blog/column-lists-in-logical-replication-publications">Column lists in logical replication publications - an overview of this useful PostgreSQL feature</a></li>
<li><a rel="nofollow" href="https://www.postgresql.fastware.com/blog/performance-impact-of-row-filters-and-column-lists-in-logical-replication">Performance impact of row filters and column lists in logical replication</a></li>
<li><a rel="nofollow" href="https://www.percona.com/blog/postgresql-logical-replication-using-an-rds-snapshot/">The 1-2-3 for PostgreSQL Logical Replication Using an RDS Snapshot</a></li>
<li><a rel="nofollow" href="https://www.percona.com/blog/2020/05/21/failover-of-logical-replication-slots-in-postgresql/">Failover of Logical Replication Slots in PostgreSQL</a></li>
<li><a rel="nofollow" href="https://mydbanotebook.org/post/replication-key/">Replica identity for logical replication</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/blogs/database/understand-replication-capabilities-in-amazon-aurora-postgresql/">Understand replication capabilities in Amazon Aurora PostgreSQL</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/blogs/database/using-logical-replication-to-replicate-managed-amazon-rds-for-postgresql-and-amazon-aurora-to-self-managed-postgresql/">Using logical replication to replicate managed Amazon RDS for PostgreSQL and Amazon Aurora to self-managed PostgreSQL</a></li></ul></code></code></code></code><code><code><code><code></code></code></code></code><code><code><code><code><h2><span id="Security">Security</span></h2></code></code></code></code><code><code><code><code><h3><span id="Authentication">Authentication</span></h3></code></code></code></code><code><code><code><code><p>Encrypt password when changing it
</p></code></code></code></code><code><code><code><code><ul><li><code>CREATE/ROLE ... PASSWORD 'some_password'</code> sends and logs the specified password as is. Thus, specifying an unencrypted password is dangerous.</li>
<li>Those statements accept an encrypted password (hashed with MD5 or SCRAM).</li>
<li>psql's \password is convenient
<ul><li>psql runs <code>"SHOW password_encryption"</code> to determine the password hash scheme (MD5 or SCRAM), hashes the supplied password, and then issues an ALTER command.</li>
<li>The hashed password still can appear in the server log. Temporarily setting log_min_error_statement to 'PANIC' prevents that.</li></ul></li></ul></code></code></code></code><code><code><code><code><p>In-database authentication profile is very limited
</p></code></code></code></code><code><code><code><code><ul><li>PostgreSQL offers only password expiration by <code>CREATE/ROLE VALID UNTIL 'some_timestamp'</code>.</li>
<li>Does not provide functionality such as:
<ul><li>Enforcing password complexity</li>
<li>Locking out a user account when the number of failed login attempts exceeds a threshold within a certain period of time</li>
<li>Restricting reuse of the same password before a certain number of days pass</li></ul></li></ul></code></code></code></code><code><code><code><code><p>Implementing password complexity: use either of:
</p></code></code></code></code><code><code><code><code><ul><li><a rel="nofollow" href="https://github.com/aws/pg_tle">Trusted Language Extensions for PostgreSQL (pg_tle)</a>
<ul><li>The user creates an extension that checks passwords in languages like SQL, PL/pgSQL, JavaScript.</li>
<li>This can be used in a managed service.</li></ul></li>
<li>An external identity service such as LDAP or Kerberos</li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/auth-cert.html">Cert authentication</a>
<ul><li>Uses SSL client certificates to perform authentication. Does not require password.</li></ul></li></ul></code></code></code></code><code><code><code><code><p>Tracking failed login attempts: Do either of:
</p></code></code></code></code><code><code><code><code><ul><li>Search the server log for messages that include "password authentication failed" or the SQLSTATE 28P01 (invalid_password)
<ul><li>Using SQLSTATE is better than the message text, because the message can vary depending on the server version and lc_message setting. (Add %e to log_line_prefix to emit the SQLSTATE.)</li></ul></li>
<li><a rel="nofollow" href="https://github.com/aws/pg_tle">Trusted Language Extensions for PostgreSQL (pg_tle)</a>
<ul><li>The user uses the client authentication hook and creates an extension that records and checks login failures.</li></ul></li></ul></code></code></code></code><code><code><code><code></code></code></code></code><code><code><code><code><h3></h3></code></code></code></code><code><code><code><code><p>Role privileges are inherited by default
</p></code></code></code></code><code><code><code><code><ul><li>In the SQL standard and other DBMSs, <code>SET ROLE</code> needs to be used to gain privileges of another role.</li>
<li>In PostgreSQL, a role automatically inherits the privileges of other roles that it is a member of. This might be surprising.</li>
<li>To approximate the SQL standard, use NOINHERIT for users and NOINHERIT for roles.</li></ul></code></code></code></code><code><code><code><code><p>Predefined roles
</p></code></code></code></code><code><code><code><code><ul><li>Some roles are provided to give part of administrative privileges to non-superusers.</li>
<li>They can be given by GRANT.</li>
<li>The representative roles are:
<ul><li>pg_monitor: can read various useful configuration settings, statistics and other system information.</li>
<li>pg_signal_backend: can send signals to other backends to cancel a query or terminate a session.</li>
<li>pg_read_server_files, pg_write_server_files and pg_execute_server_program: access files and run programs on the database server as the user the database runs as. e.g., these enable COPY data to/from files on the server or another program like gzip and curl.</li></ul></li></ul></code></code></code></code><code><code><code><code><p>Default privileges
</p></code></code></code></code><code><code><code><code><ul><li><code>ALTER DEFAULT PRIVILEGES</code> can set the default privileges that will be automatically given to database objects created in the future.
<ul><li>ex. <code>ALTER DEFAULT PRIVILEGES IN SCHEMA app_schema GRANT INSERT, UPDATE, DELETE, SELECT ON TABLES TO app_user;</code></li></ul></li>
<li>The target database objects are schema, table, view, sequence, function, and type.</li>
<li>Does not change the privileges of existing database objects.</li></ul></code></code></code></code><code><code><code><code></code></code></code></code><code><code><code><code><h3><span id="References_3">References</span></h3></code></code></code></code><code><code><code><code><p>PostgreSQL Documentation
</p></code></code></code></code><code><code><code><code><ul><li>[ The PostgreSQL User Accounthttps://www.postgresql.org/docs/current/postgres-user.html]</li>
<li>[ Database Roleshttps://www.postgresql.org/docs/current/user-manag.html]</li>
<li>[ Client Authenticationhttps://www.postgresql.org/docs/current/client-authentication.html]</li>
<li>[ Privilegeshttps://www.postgresql.org/docs/current/ddl-priv.html]</li>
<li>[ Row Security Policieshttps://www.postgresql.org/docs/current/ddl-rowsecurity.html]</li>
<li>[ Encryption Optionshttps://www.postgresql.org/docs/current/encryption-options.html]</li>
<li>[ Secure TCP/IP Connections with SSLhttps://www.postgresql.org/docs/current/ssl-tcp.html]</li>
<li>[ Secure TCP/IP Connections with GSSAPI Encryptionhttps://www.postgresql.org/docs/current/gssapi-enc.html]</li></ul></code></code></code></code><code><code><code><code><p>Authentication
</p></code></code></code></code><code><code><code><code><ul><li><a rel="nofollow" href="https://wiki.postgresql.org/wiki/List_of_drivers">List of drivers</a></li>
<li><a rel="nofollow" href="https://www.citusdata.com/blog/2020/07/28/securely-authenticate-with-scram-in-postgres-13/">How to securely authenticate with SCRAM in Postgres 13</a></li>
<li><a rel="nofollow" href="https://www.enterprisedb.com/blog/how-to-secure-postgresql-security-hardening-best-practices-checklist-tips-encryption-authentication-vulnerabilities">How to Secure PostgreSQL: Security Hardening Best Practices &amp; Tips</a></li>
<li><a rel="nofollow" href="https://www.highgo.ca/2023/03/10/tls-setup-on-postgres-15-common-practice/">TLS setup on Postgres 15 - Common Practice</a></li>
<li><a rel="nofollow" href="https://elephas.io/connecting-postgres-to-active-directory-for-authentication/">Connecting Postgres to Active Directory for Authentication</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/blogs/database/using-external-kerberos-authentication-with-amazon-rds-for-postgresql/">Using external Kerberos authentication with Amazon RDS for PostgreSQL</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/premiumsupport/knowledge-center/track-failed-login-rds-postgresql/">How can I track failed attempts to log in to my Amazon RDS DB instance that's running PostgreSQL?</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/premiumsupport/knowledge-center/rds-postgresql-drop-user-role/">Why can't I drop a user or role in my RDS for PostgreSQL DB instance?</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/jp/blogs/database/overview-of-security-best-practices-for-amazon-rds-for-postgresql-and-amazon-aurora-postgresql-compatible-edition/">Overview of security best practices for Amazon RDS for PostgreSQL and Amazon Aurora PostgreSQL-Compatible Edition</a></li></ul></code></code></code></code><code><code><code><code><p>Authorization
</p></code></code></code></code><code><code><code><code><ul><li><a rel="nofollow" href="https://www.crunchydata.com/blog/safer-application-users-in-postgres">Safer Application Users in Postgres</a></li>
<li><a rel="nofollow" href="https://www.depesz.com/2009/09/06/create-role-privilege-cannot-be-inherited/">CREATE ROLE privilege cannot be inherited?!</a></li>
<li><a rel="nofollow" href="https://www.crunchydata.com/blog/secure-postgresql-14-with-the-cis-benchmark">Secure PostgreSQL 14 with CIS Benchmark</a></li>
<li><a rel="nofollow" href="https://www.crunchydata.com/blog/pgaudit-auditing-database-operations-part-1">pgAudit: Auditing Database Operations Part 1</a></li>
<li><a rel="nofollow" href="https://www.crunchydata.com/blog/pgaudit-auditing-database-operations-part-2">pgAudit: Auditing Database Operations Part 2</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/premiumsupport/knowledge-center/rds-postgresql-pgaudit/">How do I use the pgaudit extension to audit my Amazon RDS DB instance that is running PostgreSQL?</a></li>
<li><a rel="nofollow" href="https://www.depesz.com/2022/07/04/how-to-log-selects-from-specific-table/">How to log selects from specific table?</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/row-change-auditing-options-for-postgresql/">ROW CHANGE AUDITING OPTIONS FOR POSTGRESQL</a></li>
<li><a rel="nofollow" href="https://richyen.com/postgres/2021/08/16/permissions-inheritance.html">How to See Inherited Permissions for a User</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/blogs/database/managing-postgresql-users-and-roles/">Managing PostgreSQL users and roles</a></li>
<li><a rel="nofollow" href="https://www.depesz.com/2023/03/14/system-roles-what-why-how/">System roles - what, why, how?</a></li>
<li><a rel="nofollow" href="https://www.crunchydata.com/blog/announcing-the-crunchy-data-postgresql-stig">Announcing the Crunchy Data PostgreSQL Security Technical Implementation Guide</a></li></ul></code></code></code></code><code><code><code><code><p>Hiding data
</p></code></code></code></code><code><code><code><code><ul><li><a rel="nofollow" href="https://momjian.us/main/blogs/pgblog/2020.html#July_22_2020">Passwords in Log files</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/premiumsupport/knowledge-center/rds-postgresql-cleartext-logging/?nc1=h_ls">How can I stop Amazon RDS for PostgreSQL from logging my passwords in clear-text in the log files?</a></li>
<li><a rel="nofollow" href="https://momjian.us/main/blogs/pgblog/2020.html#July_20_2020">Force Password Changes</a></li>
<li><a rel="nofollow" href="http://rhaas.blogspot.com/2012/03/security-barrier-views.html">Security Barrier Views</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/blogs/database/multi-tenant-data-isolation-with-postgresql-row-level-security/">Multi-tenant data isolation with PostgreSQL Row Level Security</a></li></ul></code></code></code></code><code><code><code><code></code></code></code></code><code><code><code><code><h2><span id="Manageability">Manageability</span></h2></code></code></code></code><code><code><code><code><h3><span id="Memory">Memory</span></h3></code></code></code></code><code><code><code><code><p>A large result set causes out of memory on the client
</p></code></code></code></code><code><code><code><code><ul><li>When running SELECT, psql retrieves the entire result set and stores all rows in the client memory.</li>
<li>With the FETCH_COUNT variable like <code>"psql -v FETCH_COUNT=100 ..."</code>, psql uses a cursor and issues DECLARE, FETCH, and CLOSE to retrieve the result set piecemeal.</li>
<li>Client drivers have similar facility, such as psqlODBC's UseDeclareFetch and PgJDBC's defaultRowFetchSize connection parameter.</li></ul></code></code></code></code><code><code><code><code><p>Common causes of server-side out of memory (OOM) issues
</p></code></code></code></code><code><code><code><code><ul><li>A high number of connections
<ul><li>Even idle connections could continue to hold much memory. PostgreSQL keeps database object metadata in memory during the session. This is for performance. You can notice this by bloated CacheMemoryContext.</li>
<li>If connection pooling is used, many connections may consume large amounts of memory over time. This is because connections are picked up randomly from the pool, used to access some relations, and released back to the pool, which results in many sessions accumulating the meta data of many relations.</li></ul></li>
<li>A high value of work_mem
<ul><li>It's advised not to set a high value to work_mem at an instance level (postgresql.conf) or a database level (ALTER DATABASE). Many sessions could allocate that amount of memory simultaneously. What's worse, each SQL statement could run such sort and/or hash operations in parallel, each of which can allocate as much memory as work_mem.</li>
<li>For a hash-based operation, work_mem * hash_mem_multiplier bytes of work memory will be allocated at maximum.</li></ul></li>
<li>Low max_locks_per_transaction
<ul><li>Each lockable object (e.g., table, index, sequence, XID, but not row) is allocated an entry in the lock table when it's locked. The entry represents the lockable object, grantees, waiters, and granted/requested lock modes.</li>
<li>The lock table is allocated in shared memory. Its size is fixed at server startup.</li>
<li>The default value of max_locks_per_transaction is 64. This means that each transaction is expected to lock 64 or fewer objects.</li>
<li>The number of entries in the lock table is (max_connections + max_prepared_transactions + alpha) * max_locks_per_transaction.</li>
<li>One transaction can use more than max_locks_per_transaction entries, if those are available.</li>
<li>If each of many concurrent transactions may access more objects, say, touch hundreds or thousands of partitions, increase max_locks_per_transaction.</li></ul></li></ul></code></code></code></code><code><code><code><code><p>Cannot retrieve a large bytea value
</p></code></code></code></code><code><code><code><code><ul><li>For example, after successfully inserting 550 MB of bytea column value, fetching it fails with  an error message like <code>"invalid memory aloc request size 1277232195"</code>.</li>
<li>Why?
<ul><li>When the PostgreSQL server sends query results to the client, it either converts the data to text format or returns it in binary format.</li>
<li>psql and client drivers instruct the server to use text format.</li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/datatype-binary.html">PostgreSQL uses either hex or escape format</a> when it converts bytea data to text format. The default format is hex. hex and escape formats use 2 and 4 bytes respectively to represent each original byte in text format.
<ul><li>ex. <code>SELECT 'abc'::bytea;</code> returns <code>\x616263</code></li></ul></li>
<li>The PostgreSQL server allocates one contiguous memory area to convert each column value to text format. This allocation size is limited to 1 GB - 1. That restriction has something to do with the handling of variable-length data types in TOAST.</li>
<li>Due to this limitation, PostgreSQL cannot return bytea data over 500 MB in text format.</li></ul></li></ul></code></code></code></code><code><code><code><code></code></code></code></code><code><code><code><code><h3><span id="Storage">Storage</span></h3></code></code></code></code><code><code><code><code><p>Common causes of full storage
</p></code></code></code></code><code><code><code><code><ul><li>Table bloat because vacuum cannot remove dead tuples: the reasons dead tuples remain are described separately.</li>
<li>WAL accumulation: the reasons WAL volume continues to grow are described separately.</li>
<li>Server log files
<ul><li>Excessive logging because of pgAudit, auto_explain, and other logging parameters such as log_statement and log_min_duration_statement</li>
<li>Log rotation and purge are not configured properly: log_rotation_age, log_rotation_size, log_truncate_on_rotation</li></ul></li>
<li>Temporary files are created
<ul><li>The work_mem is small and/or the query plan is bad.</li>
<li>A holdable cursor is kept open.
<ul><li>e.g. <code>DECLARE CURSOR cur WITH HOLD FOR SELECT * FROM mytable; COMMIT;</code></li>
<li>During the commit, the result set of the holdable cursor is stored in a work memory area of size work_mem, and the content beyond work_mem is spilled to a temporary file.</li></ul></li>
<li>Check temporary file usage with:
<ul><li>temp_files and temp_bytes of pg_stat_database</li>
<li>log_temp_files = on, which logs the file path and size when the file is deleted</li>
<li>query plans obtained by EXPLAIN ANALYZE or auto_explain</li></ul></li></ul></li></ul></code></code></code></code><code><code><code><code><p>Storage quota
</p></code></code></code></code><code><code><code><code><ul><li>PostgreSQL cannot constrain storage usage except for temporary files.</li>
<li>temp_file_limit can limit the total size of temporary files used by each session at any instant. A transaction exceeding this limit will be aborted.</li>
<li>If you want to limit the size of a database, table, or WAL ($PGDATA/pg_wal/), put it in a tablespace on a file system with limited size.
<ul><li>The tablespace of a database/table can be specified explicitly by <code>CREATE/ALTER DATABASE/TABLE ... TABLESPACE, or implicitly by the default_tablespace parameter.</code></li><code>
<li>temp_tablespaces can be used to specify where temporary files are created for temporary tables/indexes and sort/hash operations.</li>
<li>WAL directory can be specified by initdb's --waldir option. Also, after the database cluster has been created, it can be moved outside the data directory and linked with a symbolic link.</li></code></ul></li></ul></code></code></code></code><code><code><code><code><code></code></code></code></code></code><code><code><code><code><code><h3><span id="Logging_and_debugging">Logging and debugging</span></h3></code></code></code></code></code><code><code><code><code><code><p><code>FATAL:  database is starting up</code>
</p></code></code></code></code></code><code><code><code><code><code><ul><li>In old major versions, this message can be output at 1 second intervals during the server startup.</li>
<li>This may look startling, but it's not an actual problem.</li>
<li>"Why? pg_ctl start" launches postmaster in the background, and tries to connect to the database at 1 second intervals. If the connection is successful, pg_ctl returns success. Otherwise, if the server is still performing recovery and unable to accept connections, the above message is reported.</li>
<li>In newer major versions, you won't see the message any more. pg_ctl does not attempt connection. Instead, postmaster writes "ready" in postmaster.pid when it can accept connections, and pg_ctl checks it.</li></ul></code></code></code></code></code><code><code><code><code><code><p>Avoid excessive logging by restricting targets
</p></code></code></code></code></code><code><code><code><code><code><ul><li>Not only logging but many parameters can be configured for each user, database, or the combination of them. For example:
<ul><li><code>ALTER USER oltp_user SET log_min_duration_statement = '3s';</code></li>
<li><code>ALTER DATABASE analytics_db SET log_min_duration_statement = '60s';</code></li>
<li><code>ALTER USER batch_user IN DATABASE oltp_db SET log_min_duration_statement = '30s';</code></li></ul></li></ul></code></code></code></code></code><code><code><code><code><code><p>Debug-logging can be enabled for a session without cluttering the server log
</p></code></code></code></code></code><code><code><code><code><code><ul><li>It may not be acceptable to globally set log_min_messages to DEBUG1 - DEBUG5, because that would output voluminous logs.</li>
<li>You can obtain debug messages for a particular operation only on the client like this:
<ul><li><code>export PGOPTIONS="-c client_min_messages=DEBUG5"</code></li></ul></li></ul></code></code></code></code></code><code><code><code><code><code><p><code>psql -d postgres -c "select 1"</code>
</p><p>Find out what psql's backslash commands do
</p></code></code></code></code></code><code><code><code><code><code><ul><li>Use psql's -E/--echo-hidden option. It reveals the query issued in the background.</li></ul></code></code></code></code></code><code><code><code><code><code><p>Deleting duplicate rows
</p></code></code></code></code></code><code><code><code><code><code><ul><li>The following query deletes duplicate rows, leaving the one with the minimum ctid and displays the deleted row content.</li>
<li>ctid is a system column that represents the physical location of the row version within its table: (block number, item ID). ctid can change due to UPDATE and VACUUM FULL, so it'd be probably safe to lock the table in Share or stronger mode during this operation.</li></ul></code></code></code></code></code><code><code><code><code><code><code><pre>WITH x AS (SELECT some_table dup, min(ctid)
    FROM        some_table
    GROUP BY 1
    HAVING count(*) &gt; 1
)
DELETE FROM    some_table
USING     x
WHERE     (some_table) = (dup)
    AND some_table.ctid &lt;&gt; x.min
RETURNING some_table.*;
</pre></code></code></code></code></code></code><code><code><code><code><code></code></code></code></code></code><code><code><code><code><code><h3><span id="Vacuum">Vacuum</span></h3></code></code></code></code></code><code><code><code><code><code><p>Purposes of vacuum
</p></code></code></code></code></code><code><code><code><code><code><ul><li>To recover or reuse disk space occupied by updated or deleted rows.</li>
<li>To update data statistics used by the PostgreSQL query planner.</li>
<li>To update the visibility map, which speeds up index-only scans.</li>
<li>To protect against loss of very old data due to transaction ID wraparound or multixact ID wraparound.</li></ul></code></code></code></code></code><code><code><code><code><code><p>Vacuum types
</p></code></code></code></code></code><code><code><code><code><code><ul><li>Concurrent (lazy, or regular) vacuum
<ul><li>Acquires a Share Update Exclusive lock on the target relation. Does not prevent SELECT and DML commands.</li>
<li>Keeps the original data files and modifies them. TIDs do not change.</li>
<li>The data file shrinks only when there are more than certain number of contiguous empty blocks at the end. Unused space in the middle of the file is left for reuse.</li>
<li>Reports its progress in the pg_stat_progress_vacuum view.</li></ul></li>
<li>FULL vacuum
<ul><li>Acquires an Access Exclusive lock on the target relation. Prevents SELECT and DML commands.</li>
<li>Copies live tuples from the old data files to new data files, and remove old data files. Rebuilds indexes. TIDs change.</li>
<li>The data files will be packed fully and minimal.</li>
<li>Could uses twice the disk space: one for existing relations, and another for new ones.</li>
<li>Always aggressively freezes tuples.</li>
<li>The actual processing is the same as CLUSTER.</li>
<li>Reports its progress in the pg_stat_progress_cluster view.</li></ul></li>
<li>Autovacuum never runs FULL vacuum.</li></ul></code></code></code></code></code><code><code><code><code><code><p>Main steps of vacuum
</p></code></code></code></code></code><code><code><code><code><code><ol><li>Starts a transaction.
<ul><li>When there are multiple target relations, vacuum starts and commits a transaction for each relation to release locks as soon as possible.</li></ul></li>
<li>Gets an Share Update Exclusive lock for a heap and opens it. Non-wrap-around-prevention vacuum gives up vacuuming the relation if the relation cannot get the lock, emitting the following message.
<ul><li><code>LOG:  skipping of vacuum "rel_name" --- lock not available</code></li></ul></li>
<li>Gets Row Exclusive locks for the indexes and opens them.</li>
<li>Allocates the work memory to accumulate the TIDs of dead tuples.</li>
<li>Repeats the following steps until the entire heap has been processed:
<ul><li>Scans the heap: Accumulates dead tuple TIDs in the work memory until it gets full or the end of the heap is reached. The item IDs for the dead tuples are retained. Also, prunes and defragments each page if required, and possibly freezes live tuples.</li>
<li>Vacuums the indexes: Delete index entries that contain dead tuple TIDs.</li>
<li>Vacuums the heap: Reclaims the item IDs for the dead tuples. This is done here, not while scanning the heap, because the item ID cannot be freed until the index entries pointing to it have been deleted.</li>
<li>Updates the FSM and VM during the above processing.</li></ul></li>
<li>Cleans up the indexes.
<ul><li>Updates every index's stats in pg_class's relpages and reltuples.</li>
<li>Closes the indexes but retains their locks until transaction end.</li></ul></li>
<li>Truncate the heap so as to return empty pages at the end of the relation to the operating system.
<ul><li>The data file is truncated if the heap has at least the lesser of 1,000 blocks and (relation_size / 16) contiguous empty blocks at its end.</li>
<li>Takes an Access Exclusive lock on the heap. If another transaction holds a conflicting lock, wait for at most 5 seconds. If the lock cannot be obtained, gives up truncating.</li>
<li>Scans backwards the heap to verify that the end pages are still empty. Periodically checks if another transaction is waiting for a conflicting lock. If someone else is waiting, releases the Access Exclusive lock and gives up truncating.</li></ul></li>
<li>Updates relation stats.
<ul><li>Updates pg_class's relpages, reltuples, relallvisible, relhasindex, relhasrules, relhastriggers, relfrozenxid, and relminmxid.</li></ul></li>
<li>Close the relation.</li>
<li>Commits a transaction.</li>
<li>Vacuums the relation's TOAST table.</li>
<li>Repeats the above processing for each relation.</li>
<li>Updates database stats.
<ul><li>Updates pg_database.datfrozenxid to be the minimum of pg_class.relfrozenxid values, and truncates commit log in pg_xact/.</li>
<li>Updates pg_database.datminmxid to be the minimum of pg_class.relminmxid values, and truncates MultiXact data in pg_multixact/.</li></ul></li></ol></code></code></code></code></code><code><code><code><code><code><p>Autovacuum is designed to be non-intrusive
</p></code></code></code></code></code><code><code><code><code><code><ul><li>Autovacuum takes a rest (sleep) every time it has done a certain amount of work. Therefore, it does not continuously consume resources.
<ul><li>"A certain amount of work" and the sleep time can be configured by autovacuum_vacuum_cost_limit and autovacuum_vacuum_cost_delay respectively. autovacuum_vacuum_cost_delay is 2 ms by default.</li></ul></li>
<li>Autovacuum skips the relation if it cannot get a lock on it due to some conflicting lock. Wrap-around-prevention autovacuum does not do this.</li>
<li>A concurrent transaction cancels non-aggressive autovacuum, if it has waited on a conflicting relation lock for deadlock_timeout seconds and found out that the lock is held by autovacuum. These messages can be seen:
<ul><li><code>ERROR:  canceling autovacuum task</code></li>
<li><code>DETAIL:  automatic vacuum of table "mytable"</code></li></ul></li>
<li>Vacuum skips reading data pages if VM shows that they have only tuples visible to all transactions (all-visible bit is set in VM). Aggressive vacuum reads even such pages to freeze tuples.</li>
<li>Vacuum skips reading data pages if VM shows that they have only frozen tuples (all-frozen bit is set in VM).</li>
<li>Autovacuum performs reduced work on a data page when it cannot get an exclusive LWLock on it. Autovacuum for wrap-around does not do this.</li>
<li>Vacuum gives up truncating the relation if another transaction holds or is waiting for a lock on the target relation.</li></ul></code></code></code></code></code><code><code><code><code><code><p>Autovacuum is not run against a relation
</p></code></code></code></code></code><code><code><code><code><code><ul><li>Check the following to see if that is the case.
<ul><li>last_autovacuum and autovacuum_count columns of pg_stat_all_tables</li>
<li>Server logs after setting log_autovacuum_min_duration to 0</li></ul></li>
<li>Common reasons
<ul><li>The relation has to be eligible for autovacuum.
<ul><li>UPDATE/DELETE-mostly relations: updated/deleted tuples &gt;= autovacuum_vacuum_threshold + autovacuum_vacuum_scale_factor * pg_class.reltuples</li>
<li>INSERT-mostly relations: inserted tuples &gt;= autovacuum_vacuum_insert_threshold + autovacuum_vacuum_insert_scale_factor * pg_class.reltuples</li></ul></li>
<li>Autovacuum workers are busy with other many and/or large relations.</li>
<li>Some transactions continuously request or hold a conflicting relation lock for long. Non-wrap-around-prevention vacuum gives up such a relation.</li>
<li>Autovacuum cannot vacuum temporary tables. Manual vacuum needs to be run. This might lead to XID wrap-around and database shutdown.</li>
<li>Statistics stored in pg_stat/ were lost due to crash or archive recovery, including failover. Those statistics are always reset during recovery. Autovacuum depends on the statistics, which can be seen via pg_stat_all_tables, to determine if vacuuming is needed.</li></ul></li></ul></code></code></code></code></code><code><code><code><code><code><p>Why vacuum does not remove dead tuples
</p></code></code></code></code></code><code><code><code><code><code><ul><li>Slow autovacuum</li>
<li>Long-running transactions</li>
<li>Physical standbys with hot_standby_feedback = on</li>
<li>Unused replication slots</li>
<li>Orphaned prepared transactions</li></ul></code></code></code></code></code><code><code><code><code><code><p>Reducing the risk of XID wrap-around
</p></code></code></code></code></code><code><code><code><code><code><ul><li>Reduce XID consumption.
<ul><li>Each subtransaction allocates its own XID. A subtransaction is started by SAVEPOINT and PL/pgSQL's exception block (BEGIN ... EXCEPTION).</li>
<li>Some client drivers offer statement-level rollback. It encloses each SQL statement with SAVEPOINT and RELEASE SAVEPOINT.</li></ul></li>
<li>Make autovacuum run smoothly (see above).</li>
<li>Lower autovacuum_vacuum_insert_scale_factor (PostgreSQL 13+) or autovacuum_freeze_max_age so that autovacuum processes the table more frequently.</li>
<li>Schedule regular VACUUM FREEZE runs.</li></ul></code></code></code></code></code><code><code><code><code><code><p>Configuration to speed up autovacuum
</p></code></code></code></code></code><code><code><code><code><code><ul><li>Lower autovacuum_vacuum_threshold, autovacuum_vacuum_scale_factor, autovacuum_vacuum_insert_threshold, autovacuum_vacuum_insert_scale_factor for large tables.</li>
<li>Decrease autovacuum_naptime
<ul><li>Even 1s is practical if the write workload is heavy and the host has many CPU cores.</li></ul></li>
<li>Increase autovacuum_max_workers
<ul><li>Effective when there are many relations. Each relation is handled by only one autovacuum worker.</li>
<li>Increase autovacuum_vacuum_cost_limit as well. Otherwise, each autovacuum worker would sleep more frequently, because cost limits are shared among all active autovacuum workers.</li></ul></li>
<li>Increase maintenance_work_mem/autovacuum_work_mem
<ul><li>The work memory stores an array of dead tuple TIDs. A TID is (block no, item no), which is 6 bytes.</li>
<li>Setting a large value reduces the number of index scans.</li>
<li>The maximum allocated size is 1 GB - 1 no matter how large the parameter values are.</li>
<li>Does not always allocate the specified size. The actual size is large enough to accommodate all possible TIDs, so it will be small for small tables.</li>
<li>For a table with no index, only less than 2 KB is allocated. Vacuum only accumulates TIDs for one table block because it does not need to scan indexes.</li></ul></li>
<li>Increase vacuum_buffer_usage_limit
<ul><li>Vacuum uses 256 KB of ring buffers by default to cache data pages, so that it does not evict pages that are likely to be used by applications.</li>
<li>Vacuum also benefits from caching pages: heap pages are read twice, and index pages may possibly be read more than once.</li>
<li>Setting this to 0 allows vacuum to use shared buffers without limit.</li></ul></li>
<li>Decrease autovacuum_vacuum_cost_delay, increase autovacuum_vacuum_cost_limit
<ul><li>Setting autovacuum_vacuum_cost_delay to 0, which keeps autovacuum running like the manual vacuum.</li></ul></li>
<li>Partition a large table so that multiple autovacuum workers can work on its partitions concurrently.</li>
<li>Delete unnecessary indexes.
<ul><li>Autovacuum processes indexes one at a time. (Manual vacuum can process them in parallel with its PARALLEL option.)</li></ul></li></ul></code></code></code></code></code><code><code><code><code><code></code></code></code></code></code><code><code><code><code><code><h3><span id="Upgrade">Upgrade</span></h3></code></code></code></code></code><code><code><code><code><code><p>Characteristics of versions
</p></code></code></code></code></code><code><code><code><code><code><ul><li>Major version
<ul><li>Contains new features and incompatibilities.</li>
<li>Released once a year.</li>
<li>Sensitive bug fixes are only incorporated into the latest major version. "Sensitive" includes the fixes that could lead to incompatibility, adverse effects such as unstability and security, or require lots of code changes not worth the benefit.</li>
<li>The upgrade can skip intervening major versions. e.g., version 11 can be upgraded to 16 without going through 12 to 15.</li>
<li>Always requires careful planning and testing to deal with incompatible changes.</li></ul></li>
<li>Minor version
<ul><li>Contains only frequently-encountered bugs, security issues, and data corruption problems to reduce the risk associated with upgrading.</li>
<li>Running the latest minor version is always recommended. The community considers not upgrading to be riskier than upgrading.</li>
<li>Released at least once every three months, the second Thursday of February, May, August, November. Additional minor versions may be released to address urgent issues.</li>
<li>The upgrade can skip intervening minor versions.</li>
<li>Does not normally require a dump and restore; you can stop the database server, install the updated binaries, and restart the server.</li>
<li>Additional manual steps may be required for some minor versions to remedy the bad effects of fixed bugs, such as rebuilding affected indexes. See the section "Migration to Version &lt;major&gt;.&lt;minor&gt;" in the release note.</li></ul></li></ul></code></code></code></code></code><code><code><code><code><code><p>Major upgrade methods
</p></code></code></code></code></code><code><code><code><code><code><ol><li>pg_dumpall/pg_dump and psql/pg_restore: easy, long downtime</li>
<li>pg_upgrade: relatively easy, shorter downtime</li>
<li>Logical replication: complex setup and operation, minimal downtime</li></ol></code></code></code></code></code><code><code><code><code><code><p>Overview of pg_upgrade
</p></code></code></code></code></code><code><code><code><code><code><ul><li>Upgrades a database cluster to a later major version without dump/restore of user data.</li>
<li>Not an in-place upgrade: migrates data from an old database cluster to a new database cluster freshly created with initdb.</li>
<li>The basic idea is that because the relation data storage format rarely changes and only the layout of system catalogs change, pg_upgrade just dumps and restores the database schema and uses relation data files as-is.</li>
<li>Upgrade from 9.2 and later is supported.</li>
<li>Downgrade is not possible.</li>
<li>Does not migrate database statistics in pg_statistic. The user needs to run ANALYZE in every database after pg_upgrade completes.</li></ul></code></code></code></code></code><code><code><code><code><code><p>Main steps of pg_upgrade
</p></code></code></code></code></code><code><code><code><code><code><ol><li>Creates output directory for log and intermediate files.</li>
<li>Checks that the major version of the target cluster is newer, and that the old and new clusters are binary-compatible by comparing information in pg_control.</li>
<li>Gets the list of databases and relations (table, index, TOAST table, matview) of the old cluster.</li>
<li>Gets the list of library names that contain C-language functions.</li>
<li>Performs various checks to find blockers of upgrade, such as the inability to connect to databases and the presence of prepared transactions.</li>
<li>Creates a dump of global objects by running <code>pg_dumpall --globals-only</code>.</li>
<li>Creates dumps of each database by running <code>pg_dump --schema-only</code>. This is parallelized by spawning one process or thread for each database when --jobs is specified.</li>
<li>Checks the previously extracted loadable libraries with C-language functions exist in the new cluster by running <code>LOAD</code>.</li>
<li>Copies commit log files in pg_xact/ and MultiXact files in pg_multixact/ from the old cluster to the new cluster.</li>
<li>Sets next XID and MultiXact ID for the new cluster to take over the old cluster.</li>
<li>Restores global objects in the new cluster by running <code>psql</code>.</li>
<li>Restores database schemas in the new cluster by running <code>pg_restore</code>. This is parallelized by spawning one process or thread for each database when --jobs is specified.</li>
<li>Gets the list of databases and relations (table, index, TOAST table, matview) of the new cluster.</li>
<li>Links or copies user relation files from the old cluster to the new cluster. This is parallelized by spawning one process or thread for each tablespace when --jobs is specified.</li>
<li>Sets next OID for the new cluster.</li>
<li>Creates a script to delete the old cluster (delete_old_cluster.sh). This script removes the data directory and tablespace version directories.</li>
<li>Reports extensions that should be updated and creates update_extensions.sql. This script contains a list of ALTER EXTENSION ... UPDATE commands.</li></ol></code></code></code></code></code><code><code><code><code><code><p>Log files for troubleshooting pg_upgrade
</p></code></code></code></code></code><code><code><code><code><code><ul><li>Stored in $NEWPGDATA/pg_upgrade_output.d/&lt;timestamp&gt;/</li>
<li>Removed when pg_upgrade completes successfully.</li>
<li>Files:
<ul><li>pg_upgrade_server.log: The postgres server logs. Specified as pg_ctl's -l.</li>
<li>pg_upgrade_dump_&lt;DB-OID&gt;.log: Logs of pg_dump and pg_restore.</li>
<li>pg_upgrade_utility.log: Logs of miscellaneous commands run by pg_upgrade, such as psql, pg_resetwal. This includes pg_dumpall/psql to dump and restore global objects.</li>
<li>pg_upgrade_internal.log: Other pg_upgrade logs.</li>
<li>loadable_libraries.txt: List of C-language function libraries that exist in the old cluster but are not found in the new cluster.</li></ul></li></ul></code></code></code></code></code><code><code><code><code><code></code></code></code></code></code><code><code><code><code><code><h3><span id="References_4">References</span></h3></code></code></code></code></code><code><code><code><code><code><p>PostgreSQL Documentation
</p></code></code></code></code></code><code><code><code><code><code><ul><li><a rel="nofollow" href="https://www.postgresql.org/docs/current/kernel-resources.html#LINUX-MEMORY-OVERCOMMIT">Linux Memory Overcommit</a></li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/manage-ag-tablespaces.html">Tablespaces</a></li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/diskusage.html">Monitoring Disk Usage</a></li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/charset.html">Localization</a></li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/routine-vacuuming.html">Routine Vacuuming</a></li>
<li><a rel="nofollow" href="https://www.postgresql.org/support/versioning/">Versioning Policy</a></li>
<li><a rel="nofollow" href="https://www.postgresql.org/developer/roadmap/">Roadmap</a></li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/release/">Release Notes</a></li></ul></code></code></code></code></code><code><code><code><code><code><p>Memory
</p></code></code></code></code></code><code><code><code><code><code><ul><li><a rel="nofollow" href="https://momjian.us/main/blogs/pgblog/2018.html#December_7_2018">The Memory Resource Triad</a></li>
<li><a rel="nofollow" href="https://www.enterprisedb.com/blog/harnessing-shared-buffers-and-reaping-performance-benefits-part-1">Harnessing Shared Buffers (and Reaping the Performance Benefits) - Part 1</a></li>
<li><a rel="nofollow" href="https://www.enterprisedb.com/blog/harnessing-shared-buffers-part-2">Harnessing Shared Buffers - Part 2</a></li>
<li><a rel="nofollow" href="https://momjian.us/main/blogs/pgblog/2021.html#April_12_2021">Shared Memory Sizing</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/postgresql-you-might-need-to-increase-max_locks_per_transaction/">POSTGRESQL: YOU MIGHT NEED TO INCREASE MAX_LOCKS_PER_TRANSACTION</a></li>
<li><a rel="nofollow" href="https://www.depesz.com/2012/06/09/how-much-ram-is-postgresql-using/">How much RAM is PostgreSQL using?</a></li>
<li><a rel="nofollow" href="https://www.depesz.com/2011/07/03/understanding-postgresql-conf-work_mem/">Understanding postgresql.conf&nbsp;: work_mem</a></li>
<li><a rel="nofollow" href="https://www.percona.com/blog/why-linux-hugepages-are-super-important-for-database-servers-a-case-with-postgresql/">Why Linux HugePages are Super Important for Database Servers: A Case with PostgreSQL</a></li>
<li><a rel="nofollow" href="https://momjian.us/main/blogs/pgblog/2012.html#July_25_2012">I Don't Need Swap Space</a></li>
<li><a rel="nofollow" href="https://momjian.us/main/blogs/pgblog/2012.html#January_30_2012">Revisiting Memory Reporting</a></li>
<li><a rel="nofollow" href="https://www.citusdata.com/blog/2018/06/12/configuring-work-mem-on-postgres/">Configuring memory for Postgres</a></li>
<li><a rel="nofollow" href="https://paquier.xyz/postgresql-2/postgres-14-memory-dumps/">Postgres 14 highlight - Memory dumps</a></li>
<li><a rel="nofollow" href="http://rhaas.blogspot.com/2019/01/how-much-maintenanceworkmem-do-i-need.html">How Much maintenance_work_mem Do I Need?</a></li>
<li><a rel="nofollow" href="https://momjian.us/main/blogs/pgblog/2020.html#May_1_2020">Background Writes</a></li>
<li><a rel="nofollow" href="https://momjian.us/main/blogs/pgblog/2017.html#June_5_2017">Double Buffering Blues</a></li></ul></code></code></code></code></code><code><code><code><code><code><p>Storage
</p></code></code></code></code></code><code><code><code><code><code><ul><li><a rel="nofollow" href="https://www.enterprisedb.com/blog/steady-storage-stampede">Steady Storage Stampede</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/when-to-use-tablespaces-in-postgresql/">WHEN TO USE TABLESPACES IN POSTGRESQL</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/tips-and-tricks-to-kick-start-the-postgres-year-2021/">TIPS AND TRICKS TO KICK-START THE POSTGRES YEAR 2021</a></li>
<li><a rel="nofollow" href="https://www.depesz.com/2022/02/13/how-much-disk-space-you-can-save-by-using-int4-int-instead-of-int8-bigint/">How much disk space you can save by using INT4/INT instead of INT8/BIGINT?</a></li>
<li><a rel="nofollow" href="https://www.crunchydata.com/blog/postgres-is-out-of-disk-and-how-to-recover-the-dos-and-donts">Postgres is Out of Disk and How to Recover: The Dos and Don'ts</a></li>
<li><a rel="nofollow" href="https://www.highgo.ca/2021/03/20/how-to-check-and-resolve-bloat-in-postgresql/">How to check and resolve Bloat in PostgreSQL</a></li>
<li><a rel="nofollow" href="https://www.citusdata.com/blog/2017/10/20/monitoring-your-bloat-in-postgres/">Monitoring your bloat in Postgres</a></li>
<li><a rel="nofollow" href="https://wiki.postgresql.org/wiki/Show_database_bloat">Show database bloat</a></li>
<li><a rel="nofollow" href="https://www.postgresql.fastware.com/blog/how-postgresql-maps-your-tables-into-physical-files">How PostgreSQL maps your tables into physical files</a></li>
<li><a rel="nofollow" href="https://andreas.scherbaum.la/blog/archives/1117-ctid-and-other-PostgreSQL-table-internals.html">ctid and other PostgreSQL table internals</a></li>
<li><a rel="nofollow" href="https://momjian.us/main/blogs/pgblog/2017.html#March_15_2017">Column Storage Internals</a></li>
<li><a rel="nofollow" href="https://www.keithf4.com/checking-for-postgresql-bloat/">Checking for PostgreSQL Bloat</a></li>
<li><a rel="nofollow" href="https://www.keithf4.com/cleaning-up-postgresql-bloat/">Cleaning Up PostgreSQL Bloat</a></li>
<li><a rel="nofollow" href="https://rafiasabih.blogspot.com/2019/11/which-size-fits-me.html">Which size fits me?</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/blogs/database/remove-bloat-from-amazon-aurora-and-rds-for-postgresql-with-pg_repack/">Remove bloat from Amazon Aurora and RDS for PostgreSQL with pg_repack</a></li>
<li><a rel="nofollow" href="https://repost.aws/knowledge-center/diskfull-error-rds-postgresql">Why did I receive a "No space left on device” or "DiskFull" error on Amazon RDS for PostgreSQL?</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/premiumsupport/knowledge-center/postgresql-aurora-storage-issue/">How can I troubleshoot local storage issues in Aurora PostgreSQL-Compatible instances?</a></li></ul></code></code></code></code></code><code><code><code><code><code><p>Internationalization and localization
</p></code></code></code></code></code><code><code><code><code><code><ul><li><a rel="nofollow" href="https://wiki.postgresql.org/wiki/Locale_data_changes">Locale data changes</a></li>
<li><a rel="nofollow" href="http://rhaas.blogspot.com/2021/11/collation-stability.html">Collation Stability</a></li>
<li><a rel="nofollow" href="http://peter.eisentraut.org/blog/2023/03/14/how-collation-works">How collation works</a></li>
<li><a rel="nofollow" href="https://www.crunchydata.com/blog/glibc-collations-and-data-corruption">How To Correct and Identify Indexes Affected by the GNU C 2.28 Update</a></li>
<li><a rel="nofollow" href="https://simply.name/pg-lc-collate.html">One more time about collation in PostgreSQL</a></li></ul></code></code></code></code></code><code><code><code><code><code><p>Logging
</p></code></code></code></code></code><code><code><code><code><code><ul><li><a rel="nofollow" href="https://www.depesz.com/2011/05/06/understanding-postgresql-conf-log/">Understanding postgresql.conf&nbsp;: log*</a></li>
<li><a rel="nofollow" href="https://www.postgresql.fastware.com/blog/improved-logging-by-libpq-in-postgresql-14">Improved logging by libpq in PostgreSQL 14</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/jp/blogs/database/working-with-rds-and-aurora-postgresql-logs-part-1/">Working with RDS and Aurora PostgreSQL logs: Part 1</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/jp/blogs/database/working-with-rds-and-aurora-postgresql-logs-part-2/">Working with RDS and Aurora PostgreSQL logs: Part 2</a></li>
<li><a rel="nofollow" href="https://www.highgo.ca/2020/12/15/how-to-dump-out-a-backtrace-during-runtime/">How to dump out a backtrace during runtime</a></li>
<li><a rel="nofollow" href="https://amitdkhan-pg.blogspot.com/2020/07/backtraces-in-postgresql.html">Backtraces in PostgreSQL</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/removing-duplicate-rows-in-postgresql/">REMOVING DUPLICATE ROWS IN POSTGRESQL</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/removing-duplicates-in-postgresql/">REMOVING DUPLICATES IN POSTGRESQL</a></li></ul></code></code></code></code></code><code><code><code><code><code><p>Vacuum
</p></code></code></code></code></code><code><code><code><code><code><ul><li><a rel="nofollow" href="https://www.crunchydata.com/blog/checking-for-postgresql-bloat">Checking for PostgreSQL Bloat</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/detecting-table-bloat/">DETECTING TABLE BLOAT</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/table-bloat-revisited-making-tables-shrink/">TABLE BLOAT REVISITED: MAKING TABLES SHRINK</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/estimating-table-bloat/">ESTIMATING TABLE BLOAT IN POSTGRESQL</a></li>
<li><a rel="nofollow" href="https://www.citusdata.com/blog/2022/07/28/debugging-postgres-autovacuum-problems-13-tips/">Debugging Postgres autovacuum problems: 13 tips</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/reasons-why-vacuum-wont-remove-dead-rows/">VACUUM won't remove dead rows: 4 reasons why</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/idle_in_transaction_session_timeout-terminating-idle-transactions-in-postgresql/">IDLE_IN_TRANSACTION_SESSION_TIMEOUT: TERMINATING IDLE TRANSACTIONS IN POSTGRESQL</a></li>
<li><a rel="nofollow" href="http://rhaas.blogspot.com/2018/01/the-state-of-vacuum.html">The State of VACUUM</a></li>
<li><a rel="nofollow" href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.PostgreSQL.CommonDBATasks.Autovacuum.html">Working with the PostgreSQL autovacuum on Amazon RDS for PostgreSQL</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/blogs/database/understanding-autovacuum-in-amazon-rds-for-postgresql-environments/">Understanding autovacuum in Amazon RDS for PostgreSQL environments</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/blogs/database/implement-an-early-warning-system-for-transaction-id-wraparound-in-amazon-rds-for-postgresql/">Implement an Early Warning System for Transaction ID Wraparound in Amazon RDS for PostgreSQL</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/blogs/database/a-case-study-of-tuning-autovacuum-in-amazon-rds-for-postgresql/">A Case Study of Tuning Autovacuum in Amazon RDS for PostgreSQL</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/prepared-transactions/">BE PREPARED FOR PREPARED TRANSACTIONS</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/what-is-autovacuum-doing-to-my-temporary-tables/">WHAT IS AUTOVACUUM DOING TO MY TEMPORARY TABLES?</a></li>
<li><a rel="nofollow" href="https://www.crunchydata.com/blog/managing-transaction-id-wraparound-in-postgresql">Managing Transaction ID Exhaustion (Wraparound) in PostgreSQL</a></li>
<li><a rel="nofollow" href="https://www.depesz.com/2022/03/13/is-my-autovacuum-configured-properly/">Is my autovacuum configured properly?</a></li>
<li><a rel="nofollow" href="https://www.percona.com/blog/illustration-of-postgresql-bloat/">An Illustration of PostgreSQL Bloat</a></li>
<li><a rel="nofollow" href="https://www.percona.com/blog/importance-of-postgresql-vacuum-tuning-and-custom-scheduled-vacuum-job/">Importance of PostgreSQL Vacuum Tuning and Custom Scheduled Vacuum Job</a></li>
<li><a rel="nofollow" href="https://www.percona.com/blog/overcoming-vacuum-wraparound/">Overcoming VACUUM WRAPAROUND</a></li>
<li><a rel="nofollow" href="https://www.percona.com/blog/2021/06/24/understanding-pg_repack-what-can-go-wrong-and-how-to-avoid-it/">Understanding pg_repack: What Can Go Wrong – and How to Avoid It</a></li>
<li><a rel="nofollow" href="https://www.citusdata.com/blog/2016/11/04/autovacuum-not-the-enemy/">Postgres Autovacuum is Not the Enemy</a></li>
<li><a rel="nofollow" href="https://elephas.io/685-2/">Working with Amazon Aurora PostgreSQL: what happened to the stats?</a></li>
<li><a rel="nofollow" href="https://www.keithf4.com/managing-transaction-id-exhaustion-wraparound-in-postgresql/">Managing Transaction ID Exhaustion (Wraparound) in PostgreSQL</a></li>
<li><a rel="nofollow" href="https://richyen.com/postgres/2020/05/25/dont-leave-me-hanging.html">Don't Leave Me Hanging: Another Type of Transaction to Monitor</a></li>
<li><a rel="nofollow" href="https://richyen.com/postgres/2017/08/24/anatomy_of_vacuum.html">Anatomy of a VACUUM</a></li>
<li><a rel="nofollow" href="http://rhaas.blogspot.com/2019/02/tuning-autovacuumnaptime.html">Tuning autovacuum_naptime</a></li>
<li><a rel="nofollow" href="http://rhaas.blogspot.com/2020/02/useless-vacuuming.html">Useless Vacuuming</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/jp/blogs/database/parallel-vacuuming-in-amazon-rds-for-postgresql-and-amazon-aurora-postgresql/">Parallel vacuuming in Amazon RDS for PostgreSQL and Amazon Aurora PostgreSQL</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/tuning-autovacuum-postgresql/">TUNING POSTGRESQL AUTOVACUUM</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/stale-statistics-cause-table-bloat/">HOW A BAD NETWORK CONFIGURATION CAN CAUSE TABLE BLOAT</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/postgresql-autovacuum-insert-only-tables/">POSTGRESQL V13 NEW FEATURE: TUNING AUTOVACUUM ON INSERT-ONLY TABLES</a></li>
<li><a rel="nofollow" href="https://www.crunchydata.com/blog/insert-only-tables-and-autovacuum-issues-prior-to-postgresql-13">Insert-Only Tables and Autovacuum Issues Prior to PostgreSQL 13</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/what-hot_standby_feedback-in-postgresql-really-does/">WHAT HOT_STANDBY_FEEDBACK IN POSTGRESQL REALLY DOES</a></li></ul></code></code></code></code></code><code><code><code><code><code><p>Partitioning
</p></code></code></code></code></code><code><code><code><code><code><ul><li><a rel="nofollow" href="https://blog.anayrat.info/en/2021/09/01/partitioning-use-cases-with-postgresql/">Partitioning use cases with PostgreSQL</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/partition-management-do-you-really-need-a-tool-for-that/">PARTITION MANAGEMENT - DO YOU REALLY NEED A TOOL FOR THAT?</a></li>
<li><a rel="nofollow" href="https://blog.hagander.net/repartitioning-with-logical-replication-in-postgresql-13-246/">Repartitioning with logical replication in PostgreSQL 13</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/jp/blogs/database/partition-existing-tables-using-native-commands-in-amazon-rds-for-postgresql-and-amazon-aurora-postgresql/">Partition existing tables using native commands in Amazon RDS for PostgreSQL and Amazon Aurora PostgreSQL</a></li>
<li><a rel="nofollow" href="https://www.depesz.com/2021/01/17/are-there-limits-to-partition-counts/">Are there limits to partition counts?</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/jp/blogs/database/reduce-read-i-o-cost-of-your-amazon-aurora-postgresql-database-with-range-partitioning/">Reduce read I/O cost of your Amazon Aurora PostgreSQL database with range partitioning</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/jp/blogs/database/improve-performance-and-manageability-of-large-postgresql-tables-by-migrating-to-partitioned-tables-on-amazon-aurora-and-amazon-rds/">Improve performance and manageability of large PostgreSQL tables by migrating to partitioned tables on Amazon Aurora and Amazon RDS</a></li></ul></code></code></code></code></code><code><code><code><code><code><p>Upgrade
</p></code></code></code></code></code><code><code><code><code><code><ul><li><a rel="nofollow" href="https://why-upgrade.depesz.com/">Why upgrade PostgreSQL?</a></li>
<li><a rel="nofollow" href="https://momjian.us/main/blogs/pgblog/2021.html#April_5_2021">Many Upgrade Methods</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/a-primer-on-postgresql-upgrade-methods/">A PRIMER ON POSTGRESQL UPGRADE METHODS</a></li>
<li><a rel="nofollow" href="https://www.crunchydata.com/blog/how-to-perform-a-major-version-upgrade-using-pg_upgrade-in-postgresql">How to Perform a Major Version Upgrade Using pg_upgrade in PostgreSQL</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/upgrading-postgres-major-versions-using-logical-replication/">UPGRADING POSTGRES MAJOR VERSIONS USING LOGICAL REPLICATION</a></li>
<li><a rel="nofollow" href="https://www.crunchydata.com/blog/online-upgrade-in-postgres">Online Upgrades in Postgres</a></li>
<li><a rel="nofollow" href="https://andreas.scherbaum.la/blog/archives/1116-PostgreSQL-Upgrades-are-hard!.html">PostgreSQL Upgrades are hard!</a></li>
<li><a rel="nofollow" href="https://hunleyd.github.io/posts/upgrading-postgresql-5x-faster/">Upgrading PostgreSQL 5x faster</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/jp/blogs/database/part-1-upgrade-your-amazon-rds-for-postgresql-database-comparing-upgrade-approaches/">Upgrade your Amazon RDS for PostgreSQL or Amazon Aurora PostgreSQL database, Part 1: Comparing upgrade approaches</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/jp/blogs/database/upgrade-amazon-aurora-postgresql-and-amazon-rds-for-postgresql-version-10/">Upgrade Amazon Aurora PostgreSQL and Amazon RDS for PostgreSQL version 10</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/premiumsupport/knowledge-center/aurora-postgresql-major-upgrade-cpu/">Why does my Aurora PostgreSQL-Compatible instance have high CPU utilization after a major version upgrade?</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/premiumsupport/knowledge-center/rds-postgresql-version-upgrade-issues/">How do I troubleshoot issues with major version upgrades in Amazon RDS for PostgreSQL and Aurora for PostgreSQL?</a></li></ul></code></code></code></code></code><code><code><code><code><code><p>Miscellaneous tips
</p></code></code></code></code></code><code><code><code><code><code><ul><li><a rel="nofollow" href="https://www.depesz.com/2021/03/01/starting-with-pg-where-how-can-i-set-configuration-parameters/">Starting with Pg – where/how can I set configuration parameters?</a></li>
<li><a rel="nofollow" href="https://wiki.postgresql.org/wiki/Don't_Do_This">Don't Do This</a></li>
<li><a rel="nofollow" href="https://www.depesz.com/2020/01/28/dont-do-these-things-in-postgresql/">Don’t do these things in PostgreSQL</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/4955/">ALTER TABLE: HIGH-AVAILABILITY TAKEN CARE OF</a></li></ul></code></code></code></code></code><code><code><code><code><code></code></code></code></code></code><code><code><code><code><code><h2><span id="Application_development">Application development</span></h2></code></code></code></code></code><code><code><code><code><code><h3><span id="Data_type">Data type</span></h3></code></code></code></code></code><code><code><code><code><code><p>Numeric
</p></code></code></code></code></code><code><code><code><code><code><ul><li>For exact calculation and/or numbers with many digits, choose numeric.</li>
<li>For small storage space and faster calculation, choose integer types (smallint, int, bigint) and floating-point types (real, double precision, float).</li>
<li>decimal type is an alias for numeric. psql's \d and pg_dump output decimal columns as numeric instead of decimal.</li></ul></code></code></code></code></code><code><code><code><code><code><p>Timestamp
</p></code></code></code></code></code><code><code><code><code><code><ul><li>timestamp without time zone ignores the TimeZone parameter. The value is stored and returned as-is.</li>
<li>timestamp with time zone honors the explicit time zone in the input value or otherwise the TimeZone parameter. The input value is converted to UTC, and the output value is converted from the stored value, according to the time zone in effect.</li></ul></code></code></code></code></code><code><code><code><code><code><p>Binary
</p></code></code></code></code></code><code><code><code><code><code><ul><li>Available methods to store binary data
<ul><li>bytea data type</li>
<li>Large object: Use filesystem-like open/close/read/write interface, data is stored in pg_largeobject, the user table column contains an OID value that points to a row in pg_largeobject.</li>
<li>External file: The application manages data in filesystems, or object storage and stores the file path in a table character column.</li></ul></li>
<li>How to choose:
<ul><li>Need transactional (ACID) properties? -&gt; bytea, large object</li>
<li>Handle 1 GB or larger column value? -&gt; large object, external file</li>
<li>Need random and/or piecemeal access? -&gt; large object, external file</li>
<li>Want best performance with 100 MB or larger column values? -&gt; external file</li></ul></li></ul></code></code></code></code></code><code><code><code><code><code><p>Tips for using large objects
</p></code></code></code></code></code><code><code><code><code><code><ul><li><b>Do not use large objects.</b> They can be problematic. Use bytea columns or external file storage such as an OS file system and object storage.</li>
<li>Removing lots of LOBs
<ul><li>Trying to remove many large objects within a single transaction, e.g., <code>"SELECT lo_unlink(lo_oid) FROM mytable;"</code> can fail with the following message:
<ul><li><code>ERROR:  out of shared memory</code></li>
<li><code>HINT:  You might need to increase max_locks_per_transaction.</code></li></ul></li>
<li>Cause: When a large object is deleted, it is locked with Access Exclusive mode. Therefore, as many entries as the deleted LOBs are required in the lock table.</li>
<li>Solutions: Do either or both of:
<ul><li>Increase max_locks_per_transaction. The database server has to be restarted.</li>
<li>Delete LOBs in chunks, e.g., 100 LOBs per transaction.</li></ul></li></ul></li>
<li>Dealing with orphaned LOBs
<ul><li>An orphaned LOB is a large object whose OID does not appear in any oid or lo data column of the database.</li>
<li>Such an orphaned LOB would result if the application fails to delete it by calling lo_unlink() when it deletes the associated table row.
<ul><li>Solutions: Do both of:</li>
<li>Use <a rel="nofollow" href="https://www.postgresql.org/docs/current/vacuumlo.html">vacuumlo</a> to delete orphaned LOBs.</li>
<li>Use <a rel="nofollow" href="https://www.postgresql.org/docs/current/lo.htmllo">extension</a> and set a trigger on the LOB column. It automatically calls lo_unlink() when the table row containing the LOB OID is updated or deleted.</li></ul></li></ul></li></ul></code></code></code></code></code><code><code><code><code><code></code></code></code></code></code><code><code><code><code><code><h3><span id="Sequence">Sequence</span></h3></code></code></code></code></code><code><code><code><code><code><p>There is no gapless sequence
</p></code></code></code></code></code><code><code><code><code><code><ul><li>A sequence produces gaps when:
<ul><li>The transaction rolls back: Because nextval() and setval() calls are never rolled back, allocated sequence values are not reclaimed.</li>
<li>Cached values are unused: If the caching is enabled for a sequence, nextval() preallocates the specified number of values and caches them in the session's local memory. Subsequent nextval() calls fetch values from the cache until the cache is empty, and then preallocate some values again. So, if the session ends without using all the cached values, those will be gaps.</li>
<li>The server crashes: Even with a NO CACHE sequence, you can see a gap in these steps: nextval() -&gt; crash recovery -&gt; nextval(). For performance, PostgreSQL does not WAL-log every fetching of a value from a sequence. nextval() WAL-logs a value 32 numbers ahead of the current value, and the next 32 calls to nextval() don’t WAL-log anything. As a result, some numbers appear to be skipped.</li></ul></li></ul></code></code></code></code></code><code><code><code><code><code></code></code></code></code></code><code><code><code><code><code><h3><span id="References_5">References</span></h3></code></code></code></code></code><code><code><code><code><code><p>PostgreSQL Documentation
</p></code></code></code></code></code><code><code><code><code><code><ul><li><a rel="nofollow" href="https://www.postgresql.org/docs/current/datatype-numeric.html">Numeric Types</a></li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/datatype-datetime.html">Date/Time Types</a></li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/datatype-binary.html">Binary Data Types</a></li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/largeobjects.html">Large Objects</a></li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/sql-declare.html">DECLARE</a></li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/sql-createsequence.html">CREATE SEQUENCE</a></li></ul></code></code></code></code></code><code><code><code><code><code><p>Data type
</p></code></code></code></code></code><code><code><code><code><code><ul><li><a rel="nofollow" href="https://www.crunchydata.com/blog/choosing-a-postgresql-number-format">Choosing a PostgreSQL Number Format</a></li>
<li><a rel="nofollow" href="https://www.depesz.com/2014/12/06/how-much-slower-are-numerics/">How much slower are numerics?</a></li>
<li><a rel="nofollow" href="https://momjian.us/main/blogs/pgblog/2017.html#June_19_2017">Use With Time Zone</a></li>
<li><a rel="nofollow" href="https://www.depesz.com/2014/04/04/how-to-deal-with-timestamps/">How to deal with timestamps?</a></li>
<li><a rel="nofollow" href="https://wiki.postgresql.org/wiki/BinaryFilesInDB">BinaryFilesInDB</a></li>
<li><a rel="nofollow" href="https://www.crunchydata.com/blog/quick-and-easy-postgres-data-compare">Quick and Easy Postgres Data Compare</a></li></ul></code></code></code></code></code><code><code><code><code><code><p>Large object
</p></code></code></code></code></code><code><code><code><code><code><ul><li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/cleaning-up-large-number-blobs/">CLEANING UP A LARGE NUMBER OF BLOBS IN POSTGRESQL</a></li></ul></code></code></code></code></code><code><code><code><code><code><p>Pagination
</p></code></code></code></code></code><code><code><code><code><code><ul><li><a rel="nofollow" href="https://www.depesz.com/2011/05/20/pagination-with-fixed-order/">Pagination with fixed order</a></li>
<li><a rel="nofollow" href="https://www.citusdata.com/blog/2016/03/30/five-ways-to-paginate/">Five ways to paginate in Postgres, from the basic to the exotic</a></li></ul></code></code></code></code></code><code><code><code><code><code><p>Sequence
</p></code></code></code></code></code><code><code><code><code><code><ul><li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/gaps-in-sequences-postgresql/">GAPS IN SEQUENCES IN POSTGRESQL</a></li></ul></code></code></code></code></code><code><code><code><code><code></code></code></code></code></code><code><code><code><code><code><h2><span id="Scalability_and_performance">Scalability and performance</span></h2></code></code></code></code></code><code><code><code><code><code><h3><span id="Many_connections">Many connections</span></h3></code></code></code></code></code><code><code><code><code><code><p>Want to handle many concurrent clients? Then do:
</p></code></code></code></code></code><code><code><code><code><code><ul><li>Set up connection pooling on each application server as well as on a central server.</li>
<li>Limit max_connections and the actual number of connections to a few times the number of CPU cores, or at most a few hundreds.</li>
<li>Performance tends to drop above this limit, mainly because of:
<ul><li>high memory usage by client backends, possibly leading to swapping.</li>
<li>CPU context switches</li>
<li>CPU cache line contention</li>
<li>locks, particularly spinlocks: if one process holds a spinlock and other processes comes to the same protected section, those latecomers will wait spinning on the lock and continues to consume CPU.</li>
<li>processing PostgreSQL internal data structures: some data structures and its processing depends on the number of connections; creating a snapshot stands out here</li></ul></li>
<li>Even idle connections are not innocent. They contribute to high resource usage.</li></ul></code></code></code></code></code><code><code><code><code><code></code></code></code></code></code><code><code><code><code><code><h3><span id="Detecting_problems">Detecting problems</span></h3></code></code></code></code></code><code><code><code><code><code><p>Increase track_activity_query_size for ORMs (Object Relational Mappers)
</p></code></code></code></code></code><code><code><code><code><code><ul><li>Some views such as pg_stat_activity and pg_stat_statements show query strings.</li>
<li>Because those query strings are stored in fixed-size shared memory, the length of each such query string is fixed, which is track_activity_query_size. Longer queries are cut off at this limit.</li>
<li>Hibernate or some other ORMs produce very long queries. It may be useful to set track_activity_query_size to 32 KB or so.</li></ul></code></code></code></code></code><code><code><code><code><code><p>Utilize <a rel="nofollow" href="https://github.com/bigsql/plprofiler">plprofiler</a> to diagnose the bottlenecks of PL/pgSQL functions and procedures
</p></code></code></code></code></code><code><code><code><code><code><ul><li>This is an extension that creates an HTML report showing the runtimes of each step of the function and procedure, total runtimes of routines called from there, as well as the execution time of each routine.</li></ul></code></code></code></code></code><code><code><code><code><code></code></code></code></code></code><code><code><code><code><code><h3><span id="Logging">Logging</span></h3></code></code></code></code></code><code><code><code><code><code><p>Server logging could block but not show waits
</p></code></code></code></code></code><code><code><code><code><code><ul><li>The logging collector (logger) is the sole process to write logs to server log files.</li>
<li>Every backend process writes logs to its standard error, which is connected through a Unix pipe to the read endpoint of the logger. The logger reads messages from the pipe and writes them to files.</li>
<li>If the pipe gets full, the backend could block when writing to it. This happens when the logger is behind due to overwhelming amount of logging, for example, when some combination of pgAudit, auto_explain, and log_min_duration_statement is used and many concurrent sessions are running short queries.</li>
<li>The block on the pipe is not treated as a wait event (possibly by mistake), so the backend appears to be consuming CPU.</li></ul></code></code></code></code></code><code><code><code><code><code></code></code></code></code></code><code><code><code><code><code><h3><span id="Import_and_export">Import and export</span></h3></code></code></code></code></code><code><code><code><code><code><p><code>ALTER TABLE SET UNLOGGED/LOGGED</code> is heavy
</p></code></code></code></code></code><code><code><code><code><code><ul><li>This rewrites the entire table into new data files and WAL-logs those writes.</li>
<li>Hence, you cannot use it for efficient data loading - switching the table to UNLOGGED, load data into the table, and setting it back to LOGGED.</li></ul></code></code></code></code></code><code><code><code><code><code><p>Queries or the first vacuum are slow after loading data with COPY
</p></code></code></code></code></code><code><code><code><code><code><ul><li>This is because those commands have to set hint bits for rows they want to see. Setting hint bits modifies shared buffers and WAL-logs the changes if data checksums are enabled. These could generate massive writes.</li>
<li><code>COPY (FREEZE)</code> comes to the rescue. FREEZE option freezes the loaded rows and set their hint bits.</li>
<li>The table must have been created or truncated in the current subtransaction. This is to prevent other transactions from seeing the frozen rows before the COPY transaction commits.</li></ul></code></code></code></code></code><code><code><code><code><code></code></code></code></code></code><code><code><code><code><code><h3><span id="Foreign_Data_Wrapper_(FDW)"></span><span id="Foreign_Data_Wrapper_.28FDW.29">Foreign Data Wrapper (FDW)</span></h3></code></code></code></code></code><code><code><code><code><code><p>Speeding up queries via <a rel="nofollow" href="https://www.postgresql.org/docs/current/postgres-fdw.html">postgres_fdw</a>
</p></code></code></code></code></code><code><code><code><code><code><ul><li>Run ANALYZE manually on foreign tables
<ul><li>autovacuum does not execute ANALYZE on foreign tables. Hence, the local statistics may get stale and lead to poor query plans.</li></ul></li>
<li>Enable use_remote_estimate for long-running queries
<ul><li>i.e., <code>ALTER FOREIGN SERVER/TABLE ... OPTIONS (use_remote_estimate 'true');</code></li>
<li>This makes postgres_fdw issue EXPLAIN to perform the cost estimate on the remote server.</li>
<li>The query planning time will get longer due to the round-trip for EXPLAIN. So, this may not be worth the cost for short queries. You can use different foreign servers/tables with different settings for OLTP, batch, and analytics workloads.</li></ul></li>
<li>Increase fetch_size
<ul><li>i.e., <code>ALTER FOREIGN SERVER/TABLE ... OPTIONS (fetch_size '1000');</code></li>
<li>postgres_fdw uses a cursor to fetch rows from a foreign table. fetch_size determines the number of rows to fetch at a time. The default is 100.</li>
<li>If the network latency is high, reducing the round-trips by increasing this setting may help. Be aware that higher values require more memory to store fetched rows.</li></ul></li>
<li>Increase batch_size
<ul><li>i.e., <code>ALTER FOREIGN SERVER/TABLE ... OPTIONS (batch_size '1000');</code></li>
<li>By default, postgres_fdw inserts one row at a time into a foreign table during multi-row inserts (<code>INSERT ... SELECT, INSERT ... VALUES (row1), (row2),..., COPY FROM</code>).</li>
<li>Raising this setting will dramatically increase the throughput, particularly where the network latency is high.</li></ul></li>
<li>List the extension names in extensions parameter that have compatible behavior on both the local and remote servers
<ul><li>i.e., <code>ALTER FOREIGN SERVER ... OPTIONS (extensions 'extension1,extension2');</code></li>
<li>The immutable functions and operators in those extensions are considered to bring the same result on the local and remote servers. As a result, execution of them will be shipped to the remote server.</li>
<li>This is particularly beneficial when those functions and operators are used in the WHERE clause. Those filters will be executed on the remote server and thus fewer rows are transfered.</li></ul></li></ul></code></code></code></code></code><code><code><code><code><code></code></code></code></code></code><code><code><code><code><code><h3><span id="Full_text_search">Full text search</span></h3></code></code></code></code></code><code><code><code><code><code><p>Full-text search queries got much slower after inserting many new documents
</p></code></code></code></code></code><code><code><code><code><code><ul><li>When inserting data into an GIN index that has fastupdate enabled, the new index entries are not put into the index main structure. Instead, they are placed in the index's pending-list whose size is set by gin_pending_list_limit. Later, when the pending-list area becomes full, those pending-list entries are moved to the main index structure.</li>
<li>This is for good performance, because inserting one document involves many insertions into the main index, depending on the number of words in the document.</li>
<li>Full-text search queries scan the pending-list before the main index structure. Therefore, they are slow if the pending-list contains many pending entries.</li>
<li>Vacuum, including autovacuum, also moves the pending-list entries into the main index. So, the full-text search query will be faster after vacuum.</li>
<li>It is advised to tune autovacuum so that it runs reasonably frequently after inserting or updating documents.</li>
<li>The number of pending-list pages and tuples can be seen with this query (the pgstatginindex is in pgstattuple extension):
<ul><li><code>SELECT * FROM pgstatginindex('some_gin_index');</code></li></ul></li></ul></code></code></code></code></code><code><code><code><code><code></code></code></code></code></code><code><code><code><code><code><h3><span id="Utility">Utility</span></h3></code></code></code></code></code><code><code><code><code><code><p>Fast random sampling of table rows
</p></code></code></code></code></code><code><code><code><code><code><ul><li>The traditional method is slow, because it scans and sorts the entire table.
<ul><li><code>SELECT * FROM mytable ORDER BY random() LIMIT 1;</code></li></ul></li>
<li>Using TABLESAMPLE clause returns rows very quickly almost independently of the table size.
<ul><li>TABLESAMPLE fetches a sample portion of a table. Some built-in sampling methods are provided.</li>
<li>Also, The sampling method can be customized by adding an extension. For example, <a rel="nofollow" href="https://www.postgresql.org/docs/current/tsm-system-rows.html">tsm_system_rows</a> retrieves a specified number of random rows:
<ul><li><code>CREATE EXTENSION tsm_system_rows;</code></li>
<li><code>SELECT * FROM mytable TABLESAMPLE SYSTEM_ROWS(1);</code></li></ul></li>
<li>SYSTEM_ROWS picks up a random block in the table's data file, and then fetchs rows sequentially in it. If more rows are necessary, additional blocks will be chosen.</li></ul></li></ul></code></code></code></code></code><code><code><code><code><code></code></code></code></code></code><code><code><code><code><code><h3><span id="Memory_2">Memory</span></h3></code></code></code></code></code><code><code><code><code><code><p>Use huge pages
</p></code></code></code></code></code><code><code><code><code><code><ul><li>Set huge_pages = on.
<ul><li>This will reduce memory usage dramatically because the <a rel="nofollow" href="https://en.wikipedia.org/wiki/Page_table">page table</a> gets smaller.</li>
<li>Also, improved performance can be expected thanks to the reduction of CPU's TLB cache misses.</li></ul></li>
<li>"on" should be preferred for huge_pages to "try", considering the reduced memory usage and improved performance as a part of stable operation.
<ul><li>When huge_pages is set to "on", and the OS cannot allocate enough huge pages, PostgreSQL refuses to start emitting the following messages:
<ul><li><code>FATAL:  could not map anonymous shared memory: Cannot allocate memory</code></li>
<li><code>HINT:  This error usually means that PostgreSQL's request for a shared memory segment exceeded available memory, swap space, or huge pages. To reduce the request size (currently 1234567890 bytes), reduce PostgreSQL's shared memory usage, perhaps by reducing shared_buffers or max_connections.</code></li></ul></li>
<li>In this case, reboot the OS or perform failover.</li></ul></li></ul></code></code></code></code></code><code><code><code><code><code><p>Tips for shared buffers
</p></code></code></code></code></code><code><code><code><code><code><ul><li>Avoid disk writes by client backends.
<ul><li>If there is no free shared buffer when the server process wants a new page, it has to evict a used buffer. If the evicted page is dirty, the server process needs to write the page to disk. This adds to the response time.</li>
<li>This undesirable situation can be detected by checking that the buffers_backend in <a rel="nofollow" href="https://www.postgresql.org/docs/current/monitoring-stats.html#MONITORING-PG-STAT-BGWRITER-VIEW">pg_stat_bgwriter</a> is high. If buffers_backend_fsync is also high, the situation is worse.</li>
<li>To alleviate this:
<ul><li>Make more free buffers: increase shared_buffers.</li>
<li>Make more clean buffers: increase bgwriter_lru_multiplier so that the background writer writes dirty buffers more aggressively. If the maxwritten_clean of pg_stat_bgwriter rises frequently, try increasing bgwriter_lru_maxpages.</li></ul></li></ul></li>
<li>Large shared buffers may be counterproductive.
<ul><li>The benefits of shared buffers is diminished on a host with high-performance storage.</li>
<li>That's because PostgreSQL uses the OS's filesystem cache: the data is cached in the filesystem cache as well as in shared buffers (double buffering).</li>
<li>Therefore, start with 25% of RAM for shared buffers, and then increase it up to around 40% as long as you can see some improvement.</li>
<li>However, some benchmark demonstrated that 64 GB or more can do harm.</li></ul></li>
<li>Leverage <a rel="nofollow" href="https://www.postgresql.org/docs/current/pgprewarm.html">pg_prewarm</a> to quickly regain performance after failover.
<ul><li>After the database server restart or failover, the contents of shared buffers is empty or can be quite different from what was before the failover. Thus, application response times get worse until the shared buffers are warmed up.</li>
<li>Add pg_prewarm in shared_preload_libraries, and set pg_prewarm.autoprewarm to on.</li>
<li>This launches the autoprewarm worker, which periodically saves in a file the list of relation and block numbers cached in shared buffers. At server startup, pg_prewarm worker reads the file to refill shared buffers.</li></ul></li>
<li><code>"SELECT * FROM some_table;"</code> does not necessarily cache the entire table.
<ul><li>You may want to do this for performance test or application warmup, but it doesn't work. Also, it does not cache indexes at all.</li>
<li>If the size of a relation is larger than a quarter of shared buffers, its sequential scan only uses 256 KB of shared buffers.</li>
<li>The idea behind this is that a page that has been touched only by such a scan is unlikely to be needed again soon, so PostgreSQL tries to prevent such large sequential scans from evicting many useful pages out of the shared buffers.</li>
<li>Likewise, bulk writes, e.g., COPY FROM and CREATE TABLE AS SELECT, use only 16 MB of shared buffers.</li>
<li>To cache the entire relation, run <code>SELECT pg_prewarm('relation_name')</code>. This works for indexes as well.</li></ul></li></ul></code></code></code></code></code><code><code><code><code><code><p>Tips for local memory
</p></code></code></code></code></code><code><code><code><code><code><ul><li>Setting enough work_mem requires try and error.
<ul><li>Unfortunately, there is no easy way to estimate a work_mem setting to avoid disk spilling.</li>
<li>The temporary file that log_temp_files shows is not sufficient. Additional overhead for buffering temporary data in memory must be included.</li>
<li>One way to estimate work_mem is to multiply the width and number of plan rows that are sorted or hashed, found in the query plan. Add some extra for overhead, say, further multiply it by 1.1 or so.</li>
<li>If parallel query is used, divide the result by (number of parallel workers used + 1). "+1" is for the parallel leader process.</li>
<li>Run EXPLAIN ANALYZE to see if external file is used. Try increasing work_mem until the use of external file disappears.</li></ul></li>
<li>effective_cache_size does not allocate any memory.
<ul><li>This is only used to estimate the costs of index scans. The planner assumes this amount of memory is available for caching query data.</li>
<li>A higher value makes it more likely index scans will be used, a lower value makes it more likely sequential scans will be used.</li></ul></li></ul></code></code></code></code></code><code><code><code><code><code></code></code></code></code></code><code><code><code><code><code><h3><span id="Network">Network</span></h3></code></code></code></code></code><code><code><code><code><code><p>Watch out for network latency when running lots of short SQL commands
</p></code></code></code></code></code><code><code><code><code><code><ul><li>Did your batch application, which issues a lot of small SQL statements in succession, get many times slower when you migrated it to a different environment?</li>
<li>That may be because the network latency is higher. Check to see if the network communication is slow.
<ul><li>Measure the round-trip time of a simple SQL, e.g.,
<ul><li><code>\timing on</code></li>
<li><code>SELECT 1;</code></li></ul></li>
<li>Check if the wait events ClientRead and ClientWrite are increasing.</li></ul></li></ul></code></code></code></code></code><code><code><code><code><code></code></code></code></code></code><code><code><code><code><code><h3><span id="Cursor">Cursor</span></h3></code></code></code></code></code><code><code><code><code><code><ul><li>DECLARE CURSOR is fast. It creates a query plan, but does not calculate the result set. FETCH starts the calculation.</li>
<li>A cursor query is planned differently from a non-cursor query. You can see different query plans for the same SELECT statement.
<ul><li>Non-cursor queries are optimized for total runtime. The optimizer assumes that the client will consume the entire result set.
<ul><li>A sequential scan and sort is more likely to be chosen because the index scan is considered expensive.</li></ul></li>
<li>Cursor queries are optimized for the runtime of startup and initial data retrieval. The optimizer assumes that the client will fetch only a fraction of the result set.
<ul><li>The optimizer goes for an index scan to speed up the creation of the first 10% of the data.</li>
<li>The "10%" can be configured with cursor_tuple_fraction parameter.</li></ul></li></ul></li></ul></code></code></code></code></code><code><code><code><code><code></code></code></code></code></code><code><code><code><code><code><h3><span id="Lock_2">Lock</span></h3></code></code></code></code></code><code><code><code><code><code><p>Utilize fast-path locks for high performance
</p></code></code></code></code></code><code><code><code><code><code><ul><li>If lots of concurrent short transactions each touch many relations, the lwlocks to protect the lock table can become a contention bottleneck. That contention is visible as the LWLock:LockManager wait event.</li>
<li>Although the lock table is divided into 16 partitions and they are covered by different lwlocks, hundreds of concurrent transactions can lead to waits on those lwlocks.</li>
<li>Fast-path locks come to the rescue:
<ul><li>Weak locks (Access Share, Row Share, and Row Exclusive modes) are taken using the fast-path lock mechanism. It doesn't use the lock table. Instead, those locks are recorded in the per-backend area in shared memory.</li>
<li>SELECT and DMLs take those weak locks, so they don't suffer from the lock manager lwlock contention.</li></ul></li>
<li>However, fast-path locks cannot be used if either of the following is true:
<ul><li>The transaction already has 16 fast-path relation locks. The per-backend recording area is limited to 16 entries. Queries that access tables with many partitions and indexes, or join many tables will lose.</li>
<li>Some transaction tries to acquire a strong lock (Share, ShareRowExclusive, Exclusive, and AccessExclusiveLock modes).
<ul><li>Existing fast-path locks on the same relation are transferred to the lock table.</li>
<li>If someone has or requesting a strong lock, subsequent transactions which acquire a lock on a different relation may not be able to use the fast-path lock. This is because the presence of strong locks are managed using an array of 1024 integer counters, which are in effect a 1024-way partitioning of the lock space. If the requested weak lock is to be managed in the same partition as an existing strong lock, it cannot be fast-path.</li></ul></li></ul></li>
<li>The fast-path lock shows up in <a rel="nofollow" href="https://www.postgresql.org/docs/current/view-pg-locks.html">pg_locks</a> as the fastpath column being true.</li></ul></code></code></code></code></code><code><code><code><code><code></code></code></code></code></code><code><code><code><code><code><h3><span id="HOT">HOT</span></h3></code></code></code></code></code><code><code><code><code><code><p>Take advantage of HOT (Heap-Only Tuple)
</p></code></code></code></code></code><code><code><code><code><code><ul><li>HOT speeds up UPDATEs.</li>
<li>What's wrong if HOT is not used?
<ul><li>Indexes will be bigger, because each row version has an index entry in every index. Index scans using those indexes will be slower, too.</li>
<li>WAL volume will be larger and update will be slower, because the update of any column inserts new entries into all indexes.</li></ul></li>
<li>For HOT to work, both of the following conditions must be met:
<ul><li>The block containing the updated row has enough free space to accommodate the new row version.</li>
<li>The update does not modify any indexed column.</li></ul></li>
<li>Then, what should I do?
<ul><li>Set fillfactor on the table to make room for new row versions.
<ul><li>e.g., <code>CREATE TABLE mytable ... WITH (fillfactor = 90);</code>, <code>ALTER TABLE mytable SET (fillfactor = 90);</code></li>
<li>Lower fillfactor makes the table bigger, which results in shared buffer misses and longer sequential scans.</li>
<li>Maybe you should start with fillfactor = 90, and lower the setting if HOT is not working well.</li></ul></li>
<li>Drop unnecessary indexes.</li></ul></li>
<li>How do I know if HOT is working?
<ul><li>Check <a rel="nofollow" href="https://www.postgresql.org/docs/current/monitoring-stats.html#MONITORING-PG-STAT-ALL-TABLES-VIEW">pg_stat_all_tables</a> to make sure n_tup_hot_upd is high compared to n_tup_upd.</li></ul></li></ul></code></code></code></code></code><code><code><code><code><code></code></code></code></code></code><code><code><code><code><code><h3><span id="Table_layout">Table layout</span></h3></code></code></code></code></code><code><code><code><code><code><p>For best storage efficiency and performance, declare table columns from largest fixed length types (e.g., bigint, timestamp) to smallest fixed length types (e.g., smallint, bool), then variable length types (e.g., numeric, text, bytea)
</p></code></code></code></code></code><code><code><code><code><code><ul><li>Storage efficiency comes from the data alignment requirements.
<ul><li>For example, bigint is aligned on 8 byte boundary, while bool is aligned on 1 byte boundary.</li>
<li>In the following example, the former of the following returns 48, and the latter returns 39.
<ul><li><code>SELECT pg_column_size(ROW('true'::bool, '1'::bigint, '1'::smallint, '1'::int));</code></li>
<li><code>SELECT pg_column_size(ROW('1'::bigint, '1'::int, '1'::smallint, 'true'::bool));</code></li></ul></li>
<li>The alignment requirements can be seen with: <code>SELECT typalign, typname FROM pg_type ORDER BY 1, 2;</code></li></ul></li>
<li>Better performance comes from the aforementioned smaller data size, and direct column access.
<ul><li>If fixed-length columns are placed in front of the row, PostgreSQL can calculate and cache the positions of fixed-length columns in the row. So, a requested fixed-length column data of any row can be accessed directly using its offset.</li>
<li>Once a variable-length column appears, the positions of subsequent columns need to be calculated for each row, by adding up its columns' actual lengths. As a result, access to columns at the end of the row will be slow.</li></ul></li></ul></code></code></code></code></code><code><code><code><code><code><p>Use functions returning a composite type in FROM clause instead of SELECT column list
</p></code></code></code></code></code><code><code><code><code><code><ul><li>Suppose sample_func()'s return type is a composite type <code>(a int, b int, c int)</code>.</li>
<li>Bad:  <code>SELECT (sample_func()).*;</code></li></ul></code></code></code></code></code><code><code><code><code><code><p><code> Good: SELECT * FROM sample_func();</code>
</p></code></code></code></code></code><code><code><code><code><code><ul><li>In the bad case, <code>"(sample_func()).*"</code> is expanded to <code>"(sample_func()).a"</code>, <code>"(sample_func()).b"</code>, <code>"(sample_func()).c"</code>. Thus, the function is called three times.</li></ul></code></code></code></code></code><code><code><code><code><code><p>TOAST (The Oversized-Attribute Storage Technique)
</p></code></code></code></code></code><code><code><code><code><code><ul><li>This is a mechanism to store large values of up to 1 GB - 1.</li>
<li>A tuple cannot span multiple pages. Then, how is a column value stored that is larger than the page size (commonly 8 KB)?</li>
<li>Large column values of TOAST-able data types are compressed and/or broken up into chunks. Each chunk is stored as a separate row in the table's associated TOAST table. The chunk size is chosen so that four chunk rows will fit on a page. That is about 2,000 bytes for 8 KB page size.</li>
<li>A TOAST-able data type is the one which has a variable-length (varlena) representation. That is, a 1 or 4 byte varlena header followed by the column value. char(n) seems like fixed-length, but it has a varlena format.</li>
<li>TOAST table
<ul><li>Each table has 0 or 1 TOAST table and TOAST index.</li>
<li>The TOAST table and its index are created in CREATE/ALTER TABLE if needed.</li>
<li>The TOAST table is pg_toast.pg_toast_&lt;main_table_OID&gt;.</li>
<li>The TOAST index is pg_toast.pg_toast_&lt;main_table_OID&gt;_index.</li>
<li>The TOAST table's OID is stored in the table's pg_class.reltoastrelid.</li>
<li>The TOAST index's OID is stored in the TOAST table's pg_class.reltoastidxid.</li>
<li>Every TOAST table has these columns:
<ul><li>chunk_id OID: an OID identifying the particular TOASTed value</li>
<li>chunk_seq int: a sequence number for the chunk within its value</li>
<li>chunk_data bytea: the actual data of the chunk</li>
<li>Primary key (chunk_id, chunk_seq)</li></ul></li></ul></li>
<li>How TOAST works
<ul><li>It's triggered only when a row value to be stored in a table is wider than 2 KB (when the page size is 8 KB).</li>
<li>Compresses and/or moves column values to the TOAST table, until the row value is shorter than 2 KB (when the page size is 8 KB) or no more gains can be had. This 2 KB threshold can be adjusted for each table using the storage parameter toast_tuple_target in CREATE/ALTER TABLE.</li>
<li>Stores the TOASTed value's chunk_id in the main table's column. This is called a TOAST pointer.</li></ul></li>
<li>The value storage strategy - whether it should be compressed or moved to the TOAST table - can be chosen from four options using <code>ALTER TABLE ALTER COLUMN column_name SET STORAGE { PLAIN | EXTERNAL | EXTENDED | MAIN }</code></li>
<li>The compression method can be chosen between pglz and lz4. It can be set for each column by using the COMPRESSION column option in CREATE/ALTER TABLE or otherwise the default_toast_compression parameter.</li>
<li>Insertion of a TOASTed value could become unsurprisingly slow.
<ul><li>This tends to be seen when the target table already has millions of TOASTed values, particularly after inserting lots of rows in succession.</li>
<li>Why?
<ul><li>Each TOASTed value is identified by an OID.</li>
<li>OID is an unsigned 4-byte value, which is generated from a cluster-wide counter that wraps around every 4 billion values. Therefore, a single table cannot have more than 2^32 (4 billion) TOASTed values.</li>
<li>When inserting a TOASTed value, PostgreSQL generates a new OID for it, checks if an existing TOASTed value in the target table already uses the same OID. If it's used, PostgreSQL generates the next OID and perform the check again. This is repeated until a free OID is found.</li>
<li>If successive OIDs are used in the target table, this retry takes a long time.</li></ul></li></ul></li>
<li>The remedy is to partition the table. Each partition has its own TOAST table. Thus, the likelihood of duplicate OID in each partition is reduced.</li></ul></code></code></code></code></code><code><code><code><code><code></code></code></code></code></code><code><code><code><code><code><h3><span id="Transaction_2">Transaction</span></h3></code></code></code></code></code><code><code><code><code><code><p>Killed (dead) index tuples can give mysterious query speedup
</p></code></code></code></code></code><code><code><code><code><code><ul><li>If you encounter varying execution times for the same execution plan of the same query, that may be thanks to killed index tuples.</li>
<li>Whenever an index scan fetches a heap tuple only to find that it is dead, it marks the index tuple as killed (dead). Then future index scans will ignore it. This avoids its index key comparison as well as its heap tuple fetch.</li></ul></code></code></code></code></code><code><code><code><code><code><p>Subtransactions can be harmful
</p></code></code></code></code></code><code><code><code><code><code><ul><li>A subtransaction is a part of a transaction that can be rolled back without rolling back the main (top-level) transaction.</li>
<li>A subtransaction is started explicitly by a SAVEPOINT command, or implicitly when you enter a block with an EXCEPTION clause in PL/pgSQL.</li>
<li>Some client drivers provide an option to start and end a subtransaction for every SQL statement, such as PgJDBC's connection parameter "autosave". Watch out for their default values.</li>
<li>Each subtransaction allocates its own XID when it performs an operation that needs an XID, such as modifying data or locking a row.</li>
<li>The tuple header's xmin and xmax fields record the XID of the subtransaction that updated it. For checking tuple visibility, a transaction that sees the xmin/xmax needs to know whether the main transaction, not the subtransaction, has ended.</li>
<li>How to know the main transaction of a subtransaction:
<ul><li>When a subtransaction assigns its XID, it records its direct parent's XID in $PGDATA/pg_subtrans/.</li>
<li>The structure of pg_subtrans is an array of XIDS. For example, the parent XID of XID 100 is stored in the 101st element of the array. The array is divided into 8 KB pages.</li>
<li>The pg_subtrans data is cached in a memory area of 32 pages. The area is managed by SLRU (simple least-recently used) buffers. So, The cache can contain 32 pages * 8 KB / 4 = 65,536 transactions.</li>
<li>Therefore, to get the main transaction's XID, as many entries as the subtransaction nesting depth need to be traversed.</li></ul></li>
<li>Is pg_subtrans always examined for tuple visibility?
<ul><li>No. A snapshot stores not only main transactions' XIDs but also subtransactions' XIDs. If the checker's snapshot contains all subtransactions, it can get the job done without consulting pg_subtrans.</li>
<li>However, that's not always the case. Each backend can have at most 64 subtransaction XIDs in its ProcArray entry in shared memory. If the main transaction has more than 64 subtransactions, its ProcArray entry is marked overflowed.</li>
<li>When creating a snapshot, the ProcArray entries of all running transactions are scanned to collect the XIDs of main and sub transactions. If any entry is marked overflowed, the snapshot is marked suboverflowed.</li>
<li>A suboverflowed snapshot does not contain all data required to determine visibility, so the tuple's xmin/xmax must be traced back to their top-level transaction XID using pg_subtrans.</li></ul></li>
<li>So, what's the problem?
<ul><li>The readers of pg_subtrans contend for lwlocks to protect the SLRU buffers with the writers, who register their parents' XID. The reader and writer takes Share and Exclusive mode locks respectively.</li>
<li>The pg_subtrans cache is not so big. Under many concurrent subtransactions, disk I/O arise.</li></ul></li>
<li>How can I know the possibility of this happening?
<ul><li>The wait events LWLock:SubtransBuffer, LWLock:SubtransSLRU, IO:SLRURead, and IO:SLRUWrite keep growing.</li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/monitoring-stats.html#MONITORING-PG-STAT-SLRU-VIEW">pg_stat_slru</a> shows increasing blks_read and blks_hit in its row for Subtrans. (PostgreSQL 13+)</li>
<li>pg_stat_get_backend_subxact(backend_id) returns subxact_count and subxact_overflow. (PostgreSQL 16+)</li></ul></li></ul></code></code></code></code></code><code><code><code><code><code><p>MultiXact can harm performance under the hood
</p></code></code></code></code></code><code><code><code><code><code><ul><li>What is MultiXact?
<ul><li>A mechanism to record the XIDs of multiple lockers on a tuple. (Multi-transaction)</li>
<li>The xmax field in the tuple header records the XID that locks the tuple.</li>
<li>Then, what happens when multiple transactions acquire locks on the same tuple?
<ul><li>For example, the first transaction with XID 100 runs <code>SELECT ... FOR SHARE</code>. The xmax becomes 100.</li>
<li>Next, the second transaction with XID 101 runs the same <code>SELECT ... FOR SHARE</code> on the same tuple. Then, a new MultiXact ID, say 1, is allocated and set to the xmax field.</li>
<li>The mapping from MultiXact ID 1 to the actual lockers' XIDS (100, 101) is added in $PGDATA/pg_multixact/.</li></ul></li></ul></li>
<li>The foreign key constraint is implemented as a constraint trigger that executes <code>"SELECT ... FOR KEY SHARE"</code>. Therefore, MultiXact may be used without your knowledge.</li>
<li>What could be the problem?
<ul><li>Like pg_subtrans, pg_multixact is cached through the SLRU. So, it can suffer from the lwlock contention and disk I/O.</li>
<li>When an XID is added as a new member of an existing MultiXact ID, a new MultiXact ID is allocated and existing member XIDs are copied to a new location. In the above example, when XID 102 joins MultiXact ID 1 with members (100, 101), MultiXact 2 is newly allocated, (100, 101) are copied there, and 102 is added. If many transactions lock the same row concurrently, this copy gets heavier.</li></ul></li>
<li>How can I know the possibility of this happening?
<ul><li>The wait events LWLock:MultiXact*, IO:SLRURead, and IO:SLRUWrite keep growing.</li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/monitoring-stats.html#MONITORING-PG-STAT-SLRU-VIEW">pg_stat_slru</a> shows increasing blks_read and blks_hit in its rows for MultiXactOffset and MultiXactMember. (PostgreSQL 13+)</li>
<li>pg_get_multixact_members('&lt;MultiXact ID&gt;') returns a set of member XIDs and their lock modes.</li></ul></li></ul></code></code></code></code></code><code><code><code><code><code></code></code></code></code></code><code><code><code><code><code><h3><span id="WAL_and_checkpoint">WAL and checkpoint</span></h3></code></code></code></code></code><code><code><code><code><code><p>Overview of checkpoint
</p></code></code></code></code></code><code><code><code><code><code><ul><li>A processing to synchronize data both in memory and on storage by flushing unwritten (=dirty) cached data.</li>
<li>When is it performed?
<ul><li>The time specified by checkpoint_timeout has passed since the last checkpoint.</li>
<li>A certain amount of WAL has accumulateed, which is based on max_wal_size.</li>
<li>At the start of a base backup (pg_basebackup, pg_backup_start()).</li>
<li>Shutting down the database instance.</li>
<li>Completing any form of recovery.</li>
<li>Other miscellaneous required timings such as CREATE DATABASE, so that data files can be copied/moved without going through shared buffers.</li></ul></li>
<li>The checkpoint caused by checkpoint_timeout is called a scheduled checkpoint, while others are called a requested checkpoint.</li>
<li>When finishing a checkpoint, old WAL segment files are removed or recycled as new WAL segment files for future reuse, based on min_wal_size.
<ul><li>Here, "old" means "no longer necessary for crash recovery because all the changes in those WAL segments have been persisted to data files."</li>
<li>However, old WAL segment files are kept until they are archived and no longer needed by wal_keep_size or any replication slots.</li></ul></li>
<li>Checkpoint is intrusive because:
<ul><li>Storage I/O contention, for both data and WAL.</li>
<li>Buffer content lock lwlock contention: While the checkpointer is flushing a shared buffer with its buffer content lock held in Share mode, a transaction that modifies the same buffer, which requires an Exclusive lock, needs to wait for the lwlock to be released.</li>
<li>WAL volume increase due to full page writes.</li></ul></li>
<li>What is full page writes?
<ul><li>During the first modification to each data page after a checkpoint, the entire page content is WAL-logged instead of just the change. This is necessary to recover a torn page during recovery.</li>
<li>A torn page can result if the host crashes while PostgreSQL is writing a page. Because the atomic unit of I/O is usually smaller (say, 512 byte disk sector) than the PostgreSQL page size (commonly 8 KB), it could be possible that part of a page is new and the other is old.</li></ul></li></ul></code></code></code></code></code><code><code><code><code><code><p>Reducing the impact of checkpoints
</p></code></code></code></code></code><code><code><code><code><code><ul><li>Monitor the frequency of checkpoints
<ul><li>The number of scheduled and requested checkpoints can be seen by checkpoints_timed and checkpoints_req respectively in <a rel="nofollow" href="https://www.postgresql.org/docs/current/monitoring-stats.html#MONITORING-PG-STAT-BGWRITER-VIEW">pg_stat_bgwriter</a>.
<ul><li>The vast majority of checkpoints should be scheduled rather than requested. Scheduled checkpoints allow the load to be evenly spread throughout the normal operation of the system. Frequent requested checkpoints are likely to cause variations in performance.</li></ul></li>
<li>The server log shows the following messages, if the elapsed time between two successive checkpoints is shorter than checkpoint_warning and the newer one is requested by WAL accumulation.
<ul><li><code>LOG:  checkpoints are occurring too frequently (8 seconds apart)</code></li>
<li><code>HINT:  Consider increasing the configuration parameter "max_wal_size".</code></li></ul></li></ul></li>
<li>Lower the frequency of checkpoints by increasing max_wal_size and/or checkpoint_timeout.
<ul><li>Note that this can increase the amount of time needed for crash recovery.</li></ul></li>
<li>Set wal_compression to on. This reduces the WAL for full page writes.</li>
<li>Increase min_wal_size. This reduces the need for transactions to create new WAL segment files.</li></ul></code></code></code></code></code><code><code><code><code><code></code></code></code></code></code><code><code><code><code><code><h3><span id="Index">Index</span></h3></code></code></code></code></code><code><code><code><code><code><p>Disadvantages of indexes
</p></code></code></code></code></code><code><code><code><code><code><ul><li>Indexes consume disk space.</li>
<li>Larger disk space increases the size and duration of physical backups.</li>
<li>Indexes slow down INSERT/DELETE/COPY statements because they always have to modify all indexes.</li>
<li>Indexes prevent HOT updates. HOT works only for modifications to non-indexed columns.</li></ul></code></code></code></code></code><code><code><code><code><code><p>Benefits of indexes you might not notice
</p></code></code></code></code></code><code><code><code><code><code><ul><li>B-tree indexes can speed up the max() and min() aggregates. They can just read the index entries at the end of the index.</li>
<li>Indexes on expressions also gather statistics on the calculated values of the expression.
<ul><li>ex. <code>CREATE INDEX myindex1 ON mytable ((col1 + col2 * 3));</code></li>
<li>You can see the statistics of indexed expressions. For example, in the above case, the statistics appear in pg_stats as tablename=myindex1 and attname=expr.</li>
<li>The statistics target can be set for indexed expressions. e.g., <code>ALTER INDEX index_name ALTER COLUMN expr SET STATISTICS 1000;</code></li></ul></li>
<li>Indexes on foreign keys speed up constraint processing.
<ul><li>ex. <code>CREATE TABLE orders (..., product_id int REFERENCES products ON CASCADE DELETE);</code></li>
<li>You can see the time taken for the constraint cascade processing with EXPLAIN ANALYZE and auto_explain. Foreign key constraints are implemented using triggers internally.
<ul><li>ex. <code>EXPLAIN ANALYZE DELETE FROM products WHERE product_id = 2;</code></li>
<li>... <code>Trigger for constraint orders_product_id_fkey: time=0.322 calls=1</code></li></ul></li></ul></li></ul></code></code></code></code></code><code><code><code><code><code><p>Making an index-only scan work
</p></code></code></code></code></code><code><code><code><code><code><ul><li>Use EXPLAIN ANALYZE to see how many times the index-only scan had to read the heap. For example, it shows something like "Heap Fetches: 0". 0 is the best.</li>
<li>Make autovacuum more aggressive or run VACUUM to update the visibility map. That would reduce the heap fetches.</li></ul></code></code></code></code></code><code><code><code><code><code></code></code></code></code></code><code><code><code><code><code><h3><span id="Query_planning">Query planning</span></h3></code></code></code></code></code><code><code><code><code><code><p>Pitfalls of ANALYZE
</p></code></code></code></code></code><code><code><code><code><code><ul><li>Autovacuum does not run ANALYZE on temporary tables or foreign tables. Manually ANALYZE them.</li>
<li>The query plan can change after ANALYZE even when the table content hasn't changed.
<ul><li>ANALYZE takes a random sample of the table contents (300 x default_statistics_target rows). Hence, the collected statistics can vary depending on which rows are read.</li>
<li>To avoid or reduce this query plan variance, do either of:
<ul><li>Fix the query plan using third-party software like pg_hint_plan.</li>
<li>Raise the amount of statistics collected by ANALYZE, i.e., <code>ALTER TABLE ... ALTER COLUMN ... SET STATISTICS</code>. The more rows are used, the less the statistics fluctuation would be. However, this will make the ANALYZE and query planning slower because more statistics are written or read.</li>
<li>Set the table's storage parameters autovacuum_analyze_threshold and autovacuum_analyze_scale_factor to large values, so that autovacuum won't practically ANALYZE it. Then, do manual ANALYZE if needed.</li></ul></li></ul></li></ul></code></code></code></code></code><code><code><code><code><code><p>Use of a set-returning function could lead to a poor query plan
</p></code></code></code></code></code><code><code><code><code><code><ul><li>This is likely to be observed when the function is used to filter rows in WHERE clause or join.</li>
<li>That's because the planner does not have reasonably accurate information about selectivity. Thus, its cost estimate would be inaccurate.</li>
<li>CREATE/ALTER FUNCTION can set a fixed cost and the number of rows it returns. The planner support function given by the SUPPORT clause, which needs to be written in C, can change the cost and rows dynamically.
<ul><li><code>CREATE FUNCTION ... RETURNS {SETOF ... | TABLE(...)} COST execution_cost ROWS result_rows SUPPORT support_function</code></li></ul></li></ul></code></code></code></code></code><code><code><code><code><code><p>Custom plan and generic plan
</p></code></code></code></code></code><code><code><code><code><code><ul><li>PREPARE performs parse, analysis, and rewrite to generate a prepared statement.
<ul><li>ex. <code>PREPARE stmt(int) AS SELECT * FROM mytable WHERE col = $1;</code></li></ul></li>
<li>EXECUTE makes a query plan and execute it.
<ul><li>ex. <code>EXECUTE stmt(123);</code></li></ul></li>
<li>A query plan that takes specific parameter values into account is the best. Such plans are called a custom plan. On the other hand, a query plan that doesn't consider parameter values is called a generic plan.</li>
<li>You can tell a custom plan from a generic plan by the presence of a placeholder. For instance,
<ul><li>Custom  plan: <code>Filter: (col = 123)</code></li>
<li>Generic plan: <code>Filter: (col = $1)</code></li></ul></li>
<li>But planning is costly. If the generic plan is good enough, PostgreSQL uses it to avoid making custom plans.
<ul><li>PostgreSQL uses a custom plan for the first five executions of a prepared statement.</li>
<li>On the sixth execution, a generic plan is generated, and its cost is compared with the average cost of the past five executions.</li>
<li>If the cost of the generic plan is cheaper, it continues to adopt it. Custom plans won't be considered.</li>
<li>Otherwise, a new custom plan is created and used. On subsequent executions, the cost of the generic plan is compared with the average cost of all past executions of custom plans, and whichever is cheaper is chosen.</li></ul></li>
<li>You can force a generic or custom plan by setting plan_cache_mode to force_generic_plan or force_custom_plan respectively. This might be necessary to force custom plans, if the cost estimate of the generic plan is underestimated.</li></ul></code></code></code></code></code><code><code><code><code><code></code></code></code></code></code><code><code><code><code><code><h3><span id="References_6">References</span></h3></code></code></code></code></code><code><code><code><code><code><p>PostgreSQL Documentation
</p></code></code></code></code></code><code><code><code><code><code><ul><li><a rel="nofollow" href="https://www.postgresql.org/docs/current/storage-toast.html">TOAST </a></li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/indexes.html">Indexes</a></li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/performance-tips.html">Performance Tips</a></li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/parallel-query.html">Parallel Query</a></li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/monitoring-stats.html">The Cumulative Statistics System</a></li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/monitoring-locks.html">Viewing Locks</a></li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/transactions.html">Transaction Processing</a></li>
<li><a rel="nofollow" href="https://www.postgresql.org/docs/current/sql-prepare.html">PREPARE</a></li></ul></code></code></code></code></code><code><code><code><code><code><p>Many connections
</p></code></code></code></code></code><code><code><code><code><code><ul><li><a rel="nofollow" href="https://wiki.postgresql.org/wiki/Number_Of_Database_Connections">Number Of Database Connections</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/max_connections-performance-impacts/">MAX_CONNECTIONS - PERFORMANCE IMPACTS</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/blogs/database/resources-consumed-by-idle-postgresql-connections/">Resources consumed by idle PostgreSQL connections</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/blogs/database/performance-impact-of-idle-postgresql-connections/">Performance impact of idle PostgreSQL connections</a></li>
<li><a rel="nofollow" href="https://elephas.io/connection-scaling/">Connection Scaling</a></li>
<li><a rel="nofollow" href="https://brandur.org/postgres-connections">How to Manage Connections Efficiently in Postgres, or Any Database</a></li>
<li><a rel="nofollow" href="https://richyen.com/postgres/2019/06/25/pools_arent_just_for_cars.html">The Challenges of Setting max_connections and Why You Should Use a Connection Pooler</a></li>
<li><a rel="nofollow" href="https://richyen.com/postgres/2021/09/03/less-is-more-max-connections.html">When Less is More: Database Connection Scaling</a></li>
<li><a rel="nofollow" href="https://www.citusdata.com/blog/2020/10/08/analyzing-connection-scalability/">Analyzing the Limits of Connection Scalability in Postgres</a></li>
<li><a rel="nofollow" href="https://www.citusdata.com/blog/2020/10/25/improving-postgres-connection-scalability-snapshots/">Improving Postgres Connection Scalability: Snapshots</a></li>
<li><a rel="nofollow" href="https://richyen.com/postgres/2016/01/14/numa_spinlocks_issue.html">Solving Cache Line Contention in Large NUMA Systems</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/pgbouncer-types-of-postgresql-connection-pooling/">PGBOUNCER: TYPES OF POSTGRESQL CONNECTION POOLING</a></li>
<li><a rel="nofollow" href="https://www.depesz.com/2012/12/02/what-is-the-point-of-bouncing/">What is the point of bouncing?</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/estimating-connection-pool-size-with-postgresql-database-statistics/">ESTIMATING CONNECTION POOL SIZE WITH POSTGRESQL DATABASE STATISTICS</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/max_connections-performance-impacts/">MAX_CONNECTIONS - PERFORMANCE IMPACTS</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/blogs/database/set-up-highly-available-pgbouncer-and-haproxy-with-amazon-aurora-postgresql-readers/">Set up highly available PgBouncer and HAProxy with Amazon Aurora PostgreSQL readers</a></li></ul></code></code></code></code></code><code><code><code><code><code><p>Detecting problems
</p></code></code></code></code></code><code><code><code><code><code><ul><li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/3-ways-to-detect-slow-queries-in-postgresql/">3 WAYS TO DETECT SLOW QUERIES IN POSTGRESQL</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/pg_stat_statements-the-way-i-like-it/">PG_STAT_STATEMENTS: THE WAY I LIKE IT</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/postgresql-detecting-slow-queries-quickly/">POSTGRESQL: DETECTING SLOW QUERIES QUICKLY</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/fixing-track_activity_query_size-in-postgresql-conf/">FIXING TRACK_ACTIVITY_QUERY_SIZE IN POSTGRESQL.CONF</a></li>
<li><a rel="nofollow" href="https://www.crunchydata.com/blog/postgresql-monitoring-for-application-developers-dba-stats">PostgreSQL Monitoring for Application Developers: The DBA Fundamentals</a></li>
<li><a rel="nofollow" href="https://www.depesz.com/2022/07/05/understanding-pg_stat_activity/">Understanding pg_stat_activity</a></li>
<li><a rel="nofollow" href="https://repost.aws/knowledge-center/rds-aurora-postgresql-performance-issues">How do I identify and troubleshoot performance issues and slow-running queries in my RDS for PostgreSQL or Aurora PostgreSQL instance?</a></li>
<li><a rel="nofollow" href="https://repost.aws/knowledge-center/rds-postgresql-tune-query-performance">How can I log execution plans of queries for Amazon RDS PostgreSQL or Aurora PostgreSQL to tune query performance?</a></li>
<li><a rel="nofollow" href="https://repost.aws/knowledge-center/rds-postgresql-running-queries">How do I check running queries and diagnose resource consumption issues for my Amazon RDS or Aurora PostgreSQL DB instance?</a></li>
<li><a rel="nofollow" href="https://repost.aws/knowledge-center/rds-aurora-postgresql-high-cpu">How can I troubleshoot high CPU utilization for Amazon RDS or Amazon Aurora PostgreSQL?</a></li>
<li><a rel="nofollow" href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/PostgreSQL.Tuning.html">Tuning with wait events for RDS for PostgreSQL</a></li>
<li><a rel="nofollow" href="https://www.percona.com/blog/postgresql-14-database-monitoring-and-logging-enhancements/">PostgreSQL 14 Database Monitoring and Logging Enhancements</a></li>
<li><a rel="nofollow" href="https://momjian.us/main/blogs/pgblog/2017.html#February_15_2017">Going Deep on Stats</a></li>
<li><a rel="nofollow" href="https://momjian.us/main/blogs/pgblog/2012.html#June_6_2012">Timing a Query</a></li>
<li><a rel="nofollow" href="https://hunleyd.github.io/posts/explaining-intermittent-perf-problems/">EXPLAINing intermittent perf problems</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/jp/blogs/database/optimizing-and-tuning-queries-in-amazon-rds-postgresql-based-on-native-and-external-tools/">Optimizing and tuning queries in Amazon RDS PostgreSQL based on native and external tools</a></li>
<li><a rel="nofollow" href="https://github.com/bigsql/plprofiler">plprofiler</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/jp/blogs/database/profile-amazon-rds-for-postgresql-or-amazon-aurora-postgresql-pl-pgsql-code-using-plprofiler/">Profile Amazon RDS for PostgreSQL or Amazon Aurora PostgreSQL PL/pgSQL code using plprofiler</a></li></ul></code></code></code></code></code><code><code><code><code><code><p>Memory
</p></code></code></code></code></code><code><code><code><code><code><ul><li><a rel="nofollow" href="https://www.percona.com/blog/why-linux-hugepages-are-super-important-for-database-servers-a-case-with-postgresql/">Why Linux HugePages are Super Important for Database Servers: A Case with PostgreSQL</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/shared_buffers-looking-into-the-postgresql-i-o-cache/">SHARED_BUFFERS: LOOKING INTO THE POSTGRESQL I/O CACHE</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/effective_cache_size-better-set-it-right/">EFFECTIVE_CACHE_SIZE: BETTER SET IT RIGHT</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/effective_cache_size-what-it-means-in-postgresql/">EFFECTIVE_CACHE_SIZE: WHAT IT MEANS IN POSTGRESQL</a></li>
<li><a rel="nofollow" href="https://momjian.us/main/blogs/pgblog/2020.html#May_13_2020">Internals Avoiding Cache Wipe, Synchronized Scans</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/prewarming-postgresql-i-o-caches/">PREWARMING POSTGRESQL I/O CACHES</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/drop-table-killing-shared_buffers/">DROP TABLE: KILLING SHARED_BUFFERS</a></li>
<li><a rel="nofollow" href="https://www.keithf4.com/a-small-database-does-not-mean-small-shared_buffers/">A Small Database Does Not Mean Small shared_buffers</a></li>
<li><a rel="nofollow" href="https://www.keithf4.com/a-large-database-does-not-mean-large-shared_buffers/">A Large Database Does Not Mean Large shared_buffers</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/declare-cursor-in-postgresql-or-how-to-reduce-memory-consumption/">DECLARE CURSOR IN POSTGRESQL OR HOW TO REDUCE MEMORY CONSUMPTION</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/adjusting-maintenance_work_mem/">ADJUSTING MAINTENANCE_WORK_MEM</a></li>
<li><a rel="nofollow" href="https://www.crunchydata.com/blog/optimize-postgresql-server-performance">Optimize PostgreSQL Server Performance Through Configuration</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/blogs/database/tune-sorting-operations-in-postgresql-with-work_mem/">Tune sorting operations in PostgreSQL with work_mem</a></li>
<li><a rel="nofollow" href="https://thebuild.com/blog/2023/03/13/everything-you-know-about-setting-work_mem-is-wrong/">Everything you know about setting `work_mem` is wrong.</a></li>
<li><a rel="nofollow" href="https://brandur.org/sortsupport">SortSupport: Sorting in Postgres at Speed</a></li></ul></code></code></code></code></code><code><code><code><code><code><p>Storage
</p></code></code></code></code></code><code><code><code><code><code><ul><li><a rel="nofollow" href="https://www.percona.com/blog/dont-forget-to-tune-stats-collector-for-postgresql-14-and-older/">Don’t Forget to Tune Stats Collector for PostgreSQL 14 and Older</a></li>
<li><a rel="nofollow" href="https://ardentperf.com/2022/07/31/researching-the-performance-puzzle/">Researching the Performance Puzzle</a></li></ul></code></code></code></code></code><code><code><code><code><code><p>Network
</p></code></code></code></code></code><code><code><code><code><code><ul><li><a rel="nofollow" href="https://www.percona.com/blog/impact-of-network-and-cursor-on-query-performance-of-postgresql/">Impact of Network and Cursor on Query Performance of PostgreSQL</a></li>
<li><a rel="nofollow" href="https://www.percona.com/blog/how-to-measure-the-network-impact-on-postgresql-performance/">How To Measure the Network Impact on PostgreSQL Performance</a></li></ul></code></code></code></code></code><code><code><code><code><code><p>Table layout
</p></code></code></code></code></code><code><code><code><code><code><ul><li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/uuid-serial-or-identity-columns-for-postgresql-auto-generated-primary-keys/">UUID, SERIAL OR IDENTITY COLUMNS FOR POSTGRESQL AUTO-GENERATED PRIMARY KEYS?</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/column-order-in-postgresql-does-matter/">COLUMN ORDER IN POSTGRESQL DOES MATTER</a></li>
<li><a rel="nofollow" href="https://www.crunchydata.com/blog/choice-of-table-column-types-and-order-when-migrating-to-postgresql">Choice of Table Column Types and Order When Migrating to PostgreSQL</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/shrinking-the-storage-footprint-of-data/">SHRINKING THE STORAGE FOOTPRINT OF DATA</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/cluster-improving-postgresql-performance/">CLUSTER: IMPROVING POSTGRESQL PERFORMANCE</a></li>
<li><a rel="nofollow" href="https://momjian.us/main/blogs/pgblog/2021.html#May_10_2021">Clustering a Table</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/composite-type-performance-issues-in-postgresql/">COMPOSITE TYPE PERFORMANCE ISSUES IN POSTGRESQL</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/binary-data-performance-in-postgresql/">BINARY DATA PERFORMANCE IN POSTGRESQL</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/column-correlation-explained/">CORRELATION OF POSTGRESQL COLUMNS EXPLAINED</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/detecting-unstable-runtimes-postgresql/">DETECTING UNSTABLE RUNTIMES IN POSTGRESQL</a></li>
<li><a rel="nofollow" href="https://wiki.postgresql.org/wiki/TOAST">TOAST</a></li>
<li><a rel="nofollow" href="https://blog.anayrat.info/en/2022/02/14/postgresql-toast-compression-and-toast_tuple_target/">PostgreSQL: TOAST compression and toast_tuple_target</a></li>
<li><a rel="nofollow" href="https://momjian.us/main/blogs/pgblog/2012.html#January_17_2012">Toast-y Goodness</a></li>
<li><a rel="nofollow" href="https://momjian.us/main/blogs/pgblog/2012.html#January_19_2012">Toast Queries</a></li>
<li><a rel="nofollow" href="https://www.postgresql.fastware.com/blog/what-is-the-new-lz4-toast-compression-in-postgresql-14">What is the new LZ4 TOAST compression in PostgreSQL 14, and how fast is it?</a></li>
<li><a rel="nofollow" href="https://paquier.xyz/postgresql-2/postgres-14-table-compression/">Postgres 14 highlight - CREATE TABLE COMPRESSION</a></li></ul></code></code></code></code></code><code><code><code><code><code><p>SQL tricks
</p></code></code></code></code></code><code><code><code><code><code><ul><li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/func-hidden-performance-issues/">COMPOSITE TYPES IN POSTGRESQL: (FUNC()).* - HIDDEN PERFORMANCE ISSUES</a></li>
<li><a rel="nofollow" href="https://www.depesz.com/2010/10/06/performance-gains-from-using-foreign-keys/">Performance gains from using foreign keys</a></li>
<li><a rel="nofollow" href="https://www.ongres.com/blog/boost-your-user-defined-functions-in-postgresql/">Boost your User-Defined Functions in PostgreSQL</a></li></ul></code></code></code></code></code><code><code><code><code><code><p>Transaction
</p></code></code></code></code></code><code><code><code><code><code><ul><li><a rel="nofollow" href="https://aws.amazon.com/jp/blogs/database/hidden-dangers-of-duplicate-key-violations-in-postgresql-and-how-to-avoid-them/">Hidden dangers of duplicate key violations in PostgreSQL and how to avoid them</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/speeding-up-things-with-hint-bits/">SPEEDING UP THINGS WITH HINT BITS</a></li>
<li><a rel="nofollow" href="https://postgres.ai/blog/20210831-postgresql-subtransactions-considered-harmful">PostgreSQL Subtransactions Considered Harmful</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/subtransactions-and-performance-in-postgresql/">SUBTRANSACTIONS AND PERFORMANCE IN POSTGRESQL</a></li>
<li><a rel="nofollow" href="https://about.gitlab.com/blog/2021/09/29/why-we-spent-the-last-month-eliminating-postgresql-subtransactions/">Why we spent the last month eliminating PostgreSQL subtransactions</a></li>
<li><a rel="nofollow" href="https://buttondown.email/nelhage/archive/notes-on-some-postgresql-implementation-details/">Notes on some PostgreSQL implementation details</a></li>
<li><a rel="nofollow" href="https://pganalyze.com/blog/5mins-postgres-multiXact-ids-foreign-keys-performance">Avoid Postgres performance cliffs with MultiXact IDs and foreign keys</a></li>
<li><a rel="nofollow" href="https://pgconf.in/files/presentations/2023/Dilip_Kumar_RareExtremelyChallengingPostgresPerformanceProblems.pdf">Rare but extremely challenging PostgreSQL Performance</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/killed-index-tuples/">KILLED INDEX TUPLES</a></li></ul></code></code></code></code></code><code><code><code><code><code><p>HOT
</p></code></code></code></code></code><code><code><code><code><code><ul><li><a rel="nofollow" href="https://www.interdb.jp/pg/pgsql07.html">Heap Only Tuple and Index-Only Scans</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/what-is-fillfactor-and-how-does-it-affect-postgresql-performance/">WHAT IS FILLFACTOR AND HOW DOES IT AFFECT POSTGRESQL PERFORMANCE?</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/why-are-my-postgresql-updates-getting-slower/">WHY ARE MY POSTGRESQL UPDATES GETTING SLOWER?</a></li>
<li><a rel="nofollow" href="http://blog.coelho.net/database/2014/08/23/postgresql-fillfactor-and-update.html">PostgreSQL FILLFACTOR for UPDATE</a></li></ul></code></code></code></code></code><code><code><code><code><code><p>WAL and checkpoint
</p></code></code></code></code></code><code><code><code><code><code><ul><li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/reduce-wal-by-increasing-checkpoint-distance/">REDUCE WAL BY INCREASING CHECKPOINT DISTANCE</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/checkpoint-distance-and-amount-of-wal/">CHECKPOINT DISTANCE AND AMOUNT OF WAL</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/postgresql-underused-features-wal-compression/">POSTGRESQL UNDERUSED FEATURES - WAL COMPRESSION</a></li>
<li><a rel="nofollow" href="https://www.crunchydata.com/blog/tuning-your-postgres-database-for-high-write-loads">Tuning Your Postgres Database for High Write Loads</a></li>
<li><a rel="nofollow" href="https://www.enterprisedb.com/blog/tuning-maxwalsize-postgresql">Tuning max_wal_size in PostgreSQL</a></li>
<li><a rel="nofollow" href="https://www.depesz.com/2010/11/03/checkpoint_completion_target/">Understanding postgresql.conf&nbsp;: checkpoint_completion_target</a></li>
<li><a rel="nofollow" href="https://www.percona.com/blog/2020/07/28/evaluating-checkpointing-in-postgresql/">Evaluating Checkpointing in PostgreSQL</a></li>
<li><a rel="nofollow" href="https://www.enterprisedb.com/blog/tuning-maxwalsize-postgresql">Tuning max_wal_size in PostgreSQL</a></li></ul></code></code></code></code></code><code><code><code><code><code><p>Lock
</p></code></code></code></code></code><code><code><code><code><code><ul><li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/reducing-the-impact-of-locking/">REDUCING THE IMPACT OF LOCKING</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/jp/blogs/database/avoid-postgresql-lwlockbuffer_content-locks-in-amazon-aurora-tips-and-best-practices/">Avoid PostgreSQL LWLock:buffer_content locks in Amazon Aurora: Tips and best practices</a></li></ul></code></code></code></code></code><code><code><code><code><code><p>Index
</p></code></code></code></code></code><code><code><code><code><code><ul><li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/postgresql-indexing-index-scan-vs-bitmap-scan-vs-sequential-scan-basics/">POSTGRESQL INDEXING: INDEX SCAN VS. BITMAP SCAN VS. SEQUENTIAL SCAN (BASICS)</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/postgresql-parallel-create-index-for-better-performance/">POSTGRESQL: PARALLEL CREATE INDEX FOR BETTER PERFORMANCE</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/get-rid-of-your-unused-indexes/">GET RID OF YOUR UNUSED INDEXES!</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/tips-and-tricks-to-kick-start-the-postgres-year-2021/">TIPS AND TRICKS TO KICK-START THE POSTGRES YEAR 2021</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/trying-out-postgres-bloom-indexes/">TRYING OUT POSTGRES BLOOM INDEXES</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/combined-indexes-vs-separate-indexes-in-postgresql/">COMBINED INDEXES VS. SEPARATE INDEXES IN POSTGRESQL</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/index-decreases-select-performance/">ADDING AN INDEX CAN DECREASE SELECT PERFORMANCE</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/postgresql-indexes-and-foreign-keys/">INDEXES AND FOREIGN KEYS</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/index-your-foreign-key/">ARE YOUR FOREIGN KEYS INDEXED?</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/killing-proper-indexing-a-neat-idea/">KILLING PROPER INDEXING: A NEAT IDEA</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/postgresql-finding-christmas-presents/">FINDING CHRISTMAS PRESENTS</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/speeding-up-min-and-max/">SPEEDING UP “MIN” AND “MAX”</a></li>
<li><a rel="nofollow" href="https://www.crunchydata.com/blog/postgres-indexing-when-does-brin-win">Postgres Indexing: When Does BRIN Win?</a></li>
<li><a rel="nofollow" href="https://www.crunchydata.com/blog/indexes-selectivity-and-statistics">Postgres Indexes, Selectivity, and Statistics</a></li>
<li><a rel="nofollow" href="https://www.crunchydata.com/blog/postgres-indexes-for-newbies">Postgres Indexes for Newbies</a></li>
<li><a rel="nofollow" href="https://www.crunchydata.com/blog/three-easy-things-to-remember-about-postgres-indexes">Three Easy Things To Remember About Postgres Indexes</a></li>
<li><a rel="nofollow" href="https://use-the-index-luke.com/">SQL Indexing and Tuning e-Book for developers: Use The Index, Luke covers Oracle, MySQL, PostgreSQL, SQL Server, ...</a></li>
<li><a rel="nofollow" href="https://www.depesz.com/2010/09/09/why-is-my-index-not-being-used/">Why is my index not being used?</a></li>
<li><a rel="nofollow" href="https://www.crunchydata.com/blog/why-covering-indexes-are-incredibly-helpful">Why Covering Indexes in Postgres Are Incredibly Helpful</a></li>
<li><a rel="nofollow" href="https://www.crunchydata.com/blog/postgresql-brin-indexes-big-data-performance-with-minimal-storage">PostgreSQL BRIN Indexes: Big Data Performance With Minimal Storage</a></li>
<li><a rel="nofollow" href="https://www.crunchydata.com/blog/avoiding-the-pitfalls-of-brin-indexes-in-postgres">Avoiding the Pitfalls of BRIN Indexes in Postgres</a></li>
<li><a rel="nofollow" href="https://wiki.postgresql.org/wiki/Index_Maintenance">Index Maintenance</a></li>
<li><a rel="nofollow" href="https://www.percona.com/blog/2020/03/31/useful-queries-for-postgresql-index-maintenance/">Useful Queries For PostgreSQL Index Maintenance</a></li>
<li><a rel="nofollow" href="https://www.pgmustard.com/blog/why-isnt-postgres-using-my-index">Why isn't Postgres using my index?</a></li>
<li><a rel="nofollow" href="https://www.pgmustard.com/blog/2019/05/20/multi-column-indexes">Multi-column indexes</a></li>
<li><a rel="nofollow" href="https://www.pgmustard.com/blog/2019/03/04/index-only-scans-in-postgres">Index-only scans in Postgres</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/btree-vs-brin-2-options-for-indexing-in-postgresql-data-warehouses/">BTREE VS. BRIN: 2 OPTIONS FOR INDEXING IN POSTGRESQL DATA WAREHOUSES</a></li>
<li><a rel="nofollow" href="http://amitkapila16.blogspot.com/2017/03/hash-indexes-are-faster-than-btree.html">Hash indexes are faster than Btree indexes?</a></li>
<li><a rel="nofollow" href="http://rhaas.blogspot.com/2017/09/postgresqls-hash-indexes-are-now-cool.html">PostgreSQL's Hash Indexes Are Now Cool</a></li></ul></code></code></code></code></code><code><code><code><code><code><p>Query planning
</p></code></code></code></code></code><code><code><code><code><code><ul><li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/detecting-wrong-planner-estimates/">DETECTING WRONG PLANNER ESTIMATES</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/the-power-of-response-times/">THE POWER OF RESPONSE TIMES</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/changing-histogram-sizes/">CHANGING HISTOGRAM SIZES</a></li>
<li><a rel="nofollow" href="https://www.crunchydata.com/blog/postgres-query-optimization-left-join-vs-union-all">Postgres Query Optimization: LEFT JOIN vs UNION ALL</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/how-to-interpret-postgresql-explain-analyze-output/">HOW TO INTERPRET POSTGRESQL EXPLAIN ANALYZE OUTPUT</a></li>
<li><a rel="nofollow" href="https://www.depesz.com/2013/04/16/explaining-the-unexplainable/">Explaining the unexplainable</a></li>
<li><a rel="nofollow" href="https://www.depesz.com/2013/04/27/explaining-the-unexplainable-part-2/">Explaining the unexplainable 窶・part 2</a></li>
<li><a rel="nofollow" href="https://www.depesz.com/2013/05/09/explaining-the-unexplainable-part-3/">Explaining the unexplainable 窶・part 3</a></li>
<li><a rel="nofollow" href="https://www.depesz.com/2013/05/19/explaining-the-unexplainable-part-4/">Explaining the unexplainable 窶・part 4</a></li>
<li><a rel="nofollow" href="https://www.depesz.com/2013/05/30/explaining-the-unexplainable-part-5/">Explaining the unexplainable 窶・part 5</a></li>
<li><a rel="nofollow" href="https://www.depesz.com/2021/06/20/explaining-the-unexplainable-part-6-buffers/">Explaining the unexplainable 窶・part 6: buffers</a></li>
<li><a rel="nofollow" href="https://www.depesz.com/2008/05/10/prepared-statements-gotcha/">Prepared statements gotcha</a></li>
<li><a rel="nofollow" href="https://akorotkov.github.io/blog/2017/05/31/alter-index-weird/">ALTER INDEX ... SET STATISTICS ...???</a></li>
<li><a rel="nofollow" href="http://blog.rhodiumtoad.org.uk/2017/01/22/performance-issues-with-ored-conditions/">Performance issues with ORed conditions</a></li>
<li><a rel="nofollow" href="https://www.ongres.com/blog/explain_analyze_may_be_lying_to_you/">EXPLAIN ANALYZE may be lying to you</a></li>
<li><a rel="nofollow" href="https://www.pgmustard.com/blog/using-postgres-buffers-for-query-optimization">Using BUFFERS for query optimization</a></li>
<li><a rel="nofollow" href="https://www.pgmustard.com/blog/calculating-per-operation-times-in-postgres-explain-analyze">Calculating per-operation times in PostgreSQL EXPLAIN ANALYZE</a></li>
<li><a rel="nofollow" href="https://www.pgmustard.com/blog/auto-explain-overhead-with-timing">Can auto_explain (with timing) have low overhead?</a></li>
<li><a rel="nofollow" href="https://www.pgmustard.com/blog/2019/9/17/postgres-execution-plans-field-glossary">Postgres Execution Plans窶岩凪皆ield Glossary</a></li>
<li><a rel="nofollow" href="https://www.pgmustard.com/blog/2018/12/14/row-count-estimates-in-postgres">Row count estimates in Postgres</a></li>
<li><a rel="nofollow" href="https://elephas.io/analyzing-effect-on-default_statistics_target-on-analyze/">Analyzing effect on default_statistics_target on ANALYZE</a></li>
<li><a rel="nofollow" href="https://ardentperf.com/2022/02/10/a-hairy-postgresql-incident/">A Hairy PostgreSQL Incident</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/rewrite-or-to-union-in-postgresql-queries/">REWRITE OR TO UNION IN POSTGRESQL QUERIES</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/explain-that-parameterized-statement/">EXPLAIN THAT PARAMETERIZED STATEMENT IN POSTGRESQL!</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/union-all-data-types-performance/">UNION ALL, DATA TYPES AND PERFORMANCE</a></li>
<li><a rel="nofollow" href="http://rajeevrastogi.blogspot.com/2018/02/bitmap-scan-in-postgresql.html">Bitmap Scan In PostgreSQL</a></li>
<li><a rel="nofollow" href="https://richyen.com/postgres/2019/11/06/auto_explain.html">Making Mystery-Solving Easier with auto_explain</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/jp/blogs/database/understanding-statistics-in-postgresql/">Understanding statistics in PostgreSQL</a></li></ul></code></code></code></code></code><code><code><code><code><code><p>Join
</p></code></code></code></code></code><code><code><code><code><code><ul><li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/join-strategies-and-performance-in-postgresql/">JOIN STRATEGIES AND PERFORMANCE IN POSTGRESQL</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/exploding-runtime-how-nested-loops-can-destroy-speed/">EXPLODING RUNTIME: HOW NESTED LOOPS CAN DESTROY SPEED</a></li></ul></code></code></code></code></code><code><code><code><code><code><p>Logging
</p></code></code></code></code></code><code><code><code><code><code><ul><li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/logging-the-hidden-speedbrakes/">LOGGING - THE HIDDEN SPEEDBRAKES</a></li></ul></code></code></code></code></code><code><code><code><code><code><p>Parallel query
</p></code></code></code></code></code><code><code><code><code><code><ul><li><a rel="nofollow" href="https://www.percona.com/blog/2019/07/30/parallelism-in-postgresql/">Parallelism in PostgreSQL</a></li></ul></code></code></code></code></code><code><code><code><code><code><p>Import and export
</p></code></code></code></code></code><code><code><code><code><code><ul><li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/postgresql-bulk-loading-huge-amounts-of-data/">BULK LOADING HUGE AMOUNTS OF DATA</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/loading-data-in-the-most-efficient-way/">LOADING DATA IN THE MOST EFFICIENT WAY</a></li>
<li><a rel="nofollow" href="https://www.depesz.com/2007/07/05/how-to-insert-data-to-database-as-fast-as-possible/">how to insert data to database – as fast as possible</a></li>
<li><a rel="nofollow" href="https://pgsqlpgpool.blogspot.com/2021/03/speeding-up-pgbench-using-copy-freeze.html">Speeding up pgbench using COPY FREEZE</a></li>
<li><a rel="nofollow" href="https://aws.amazon.com/jp/blogs/database/speed-up-time-series-data-ingestion-by-partitioning-tables-on-amazon-rds-for-postgresql/">Speed up time series data ingestion by partitioning tables on Amazon RDS for PostgreSQL</a></li></ul></code></code></code></code></code><code><code><code><code><code><p>Foreign Data Wrapper (FDW)
</p></code></code></code></code></code><code><code><code><code><code><ul><li><a rel="nofollow" href="https://www.interdb.jp/pg/pgsql04.html">Foreign Data Wrappers and Parallel Query</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/foreign-data-wrapper-for-postgresql-performance-tuning/">FOREIGN DATA WRAPPER FOR POSTGRESQL: PERFORMANCE TUNING</a></li>
<li><a rel="nofollow" href="https://www.crunchydata.com/blog/understanding-postgres_fdw">Understanding Foreign Data Wrappers in Postgres and postgres_fdw</a></li>
<li><a rel="nofollow" href="https://www.ongres.com/blog/boost-query-performance-using-fdw-with-minimal-changes/">Boost query performance using Foreign Data Wrapper with minimal changes</a></li></ul></code></code></code></code></code><code><code><code><code><code><p>Trigger
</p></code></code></code></code></code><code><code><code><code><code><ul><li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/are-triggers-really-that-slow-in-postgres/">ARE TRIGGERS REALLY THAT SLOW IN POSTGRES?</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/more-on-postgres-trigger-performance/">MORE ON POSTGRES TRIGGER PERFORMANCE</a></li>
<li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/rules-or-triggers-to-log-bulk-updates/">RULES OR TRIGGERS TO LOG BULK UPDATES?</a></li></ul></code></code></code></code></code><code><code><code><code><code><p>Full text search
</p></code></code></code></code></code><code><code><code><code><code><ul><li><a rel="nofollow" href="https://www.cybertec-postgresql.com/en/what-postgresql-full-text-search-has-to-do-with-vacuum/">WHAT POSTGRESQL FULL-TEXT-SEARCH HAS TO DO WITH VACUUM</a></li></ul></code></code></code></code></code><code><code><code><code><code><p>Utility
</p></code></code></code></code></code><code><code><code><code><code><ul><li><a rel="nofollow" href="http://pgconfigurator.cybertec.at/">Cybertec Postgres Configurator</a></li>
<li><a rel="nofollow" href="https://explain.depesz.com/">Depesz’ EXPLAIN ANALYZE visualizer</a></li>
<li><a rel="nofollow" href="https://explain.dalibo.com/">Dalibo’s EXPLAIN ANALYZE visualizer</a></li>
<li><a rel="nofollow" href="https://blog.hagander.net/getting-random-rows-faster-very-much-faster-249/">Getting random rows faster. Very much faster.</a></li></ul></code></code></code></code></code></p><!-- 
NewPP limit report
Cached time: 20240118023657
Cache expiry: 86400
Dynamic content: false
Complications: []
CPU time usage: 0.196 seconds
Real time usage: 0.199 seconds
Preprocessor visited node count: 169/1000000
Post‐expand include size: 0/2097152 bytes
Template argument size: 0/2097152 bytes
Highest expansion depth: 2/40
Expensive parser function count: 0/100
Unstrip recursion depth: 0/20
Unstrip post‐expand size: 259/5000000 bytes
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%    0.000      1 -total
-->

<!-- Saved in parser cache with key wikidb-mediawiki-:pcache:idhash:3876-0!canonical and timestamp 20240118023657 and revision id 38563
 -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Examples of AI rip-offs making their way into Google News (132 pts)]]></title>
            <link>https://www.404media.co/google-news-is-boosting-garbage-ai-generated-articles/</link>
            <guid>39042260</guid>
            <pubDate>Thu, 18 Jan 2024 14:46:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.404media.co/google-news-is-boosting-garbage-ai-generated-articles/">https://www.404media.co/google-news-is-boosting-garbage-ai-generated-articles/</a>, See on <a href="https://news.ycombinator.com/item?id=39042260">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  
<!--kg-card-begin: html-->

<!--kg-card-end: html-->
<p>Google News is boosting sites that rip-off other outlets by using AI to rapidly churn out content, 404 Media has found. Google told 404 Media that although it tries to address spam on Google News, the company ultimately does not focus on whether a news article was written by an AI or a human, opening the way for more AI-generated content making its way onto Google News.</p><p>The presence of AI-generated content on Google News signals two things: first, the black box nature of Google News, with entry into Google News’ rankings in the first place an opaque, but apparently gameable, system. Second, is how Google may not be ready for moderating its News service in the age of consumer-access AI, where essentially anyone is able to churn out a mass of content with little to no regard for its quality or originality.</p><p>“I want to read the original stories written by journalists who actually researched them and spoke to primary sources. Any news junkie would,” Brian Penny, a ghostwriter who first flagged some of the seemingly AI-generated articles to 404 Media, said.</p><div><p>💡</p><p><b><strong>Do you know about any other AI-generated content farms? I would love to hear from you. Using a non-work device, you can message me securely on Signal at +44 20 8133 5190. Otherwise, send me an email at joseph@404media.co.</strong></b></p></div><p>One example was a news site called Worldtimetodays.com, which is littered with full page and other ads. On Wednesday it <a href="https://worldtimetodays.com/the-star-wars-theory-youtuber-drama-explained/?ref=404media.co"><u>published an article about Star Wars</u></a> fandom. The article was very similar to one <a href="https://www.distractify.com/p/star-wars-theory-drama-explained?ref=404media.co#:~:text=Star%20Wars%20Theory%20has%20kicked%20up%20all%20sorts%20of%20internet%20drama.&amp;text=According%20to%20fans%20like%20%40behartwithnoe,rumors%20simply%20to%20make%20videos."><u>published a day earlier on the website Distractify</u></a>, with even the same author photo. One major difference, though, was that Worldtimetodays.com wrote “Let’s be honest, war of stars fans,” rather than Star Wars fans. <a href="https://worldtimetodays.com/hibachi-restaurant-suspect-named-by-prosecutor/?ref=404media.co#google_vignette"><u>Another article</u></a> is a clear rip-off of <a href="https://heavy.com/news/jamal-bazile/?ref=404media.co"><u>a piece from Heavy.com</u></a>, with Worldtimetodays.com not even bothering to replace the Heavy.com watermarked artwork. Gary Graves, the listed author on Worldtimetodays.com, has published more than 40 articles in a 24 hour period.&nbsp;</p>
</div><div>
  <div>
    <h2>This post is for paid members only</h2>
    <p>Become a paid member for unlimited ad-free access to articles, bonus podcast content, and more.</p>
  </div>
  <div>
    <h2>Sign up for free access to this post</h2>
    <p>Free members get access to posts like this one along with an email round-up of our week's stories.</p>
  </div>
  <p><a href="https://www.404media.co/signup/" data-portal="signup">Subscribe</a></p><p>Already have an account? <a href="https://www.404media.co/signin/" data-portal="signin">Sign in</a></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Zig Software Foundation 2024 Financial Report and Fundraiser (154 pts)]]></title>
            <link>https://ziglang.org/news/2024-financials/</link>
            <guid>39042139</guid>
            <pubDate>Thu, 18 Jan 2024 14:37:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ziglang.org/news/2024-financials/">https://ziglang.org/news/2024-financials/</a>, See on <a href="https://news.ycombinator.com/item?id=39042139">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Zig Software Foundation is a 501(c)(3) non-profit organization which I am proud
to say makes <strong>extremely efficient use of monetary resources</strong>. Unlike many of
our peers, our primary expense is direct payments to contributors for their
enhancements to the Zig project.</p><p>Don’t take my word for it - let’s look at some numbers.</p><h2 id="2023-expenditures">2023 Expenditures</h2><table><tbody><tr><th>Expense Name</th><th>2023 Cost</th><th>Description</th></tr><tr><td>Contractors</td><td>$308,102.61</td><td>Direct compensation to contributors working on Zig at a rate of $60/hour.</td></tr><tr><td>Employees</td><td>$102,000.00</td><td>ZSF has one employee which is yours truly, Andrew Kelley, serving the role of
Lead Software Engineer. My duties as President of Zig Software Foundation are
strictly volunteer work;
I am not compensated for serving on the board of directors. The other two
members of the ZSF board of directors
<a href="https://docs.google.com/document/d/1wPQtJxIgCo7SReJev-H7OZGgYt0k8LqECc4G4QYT0aE">chose my salary</a> to be $159,790 per year, matching
the median salary for a Lead Software Engineer in New York City at the time.<p>I have never once received my full salary. At the time of writing, I am accepting
$108,000/year (before taxes) and donating the rest to ZSF. In the past I have donated
an even larger portion. In the future I think it would be nice to be fully
compensated.</p></td></tr><tr><td>CI &amp; Website</td><td>$14,661.35</td><td>Zig has great cross-compiling abilities in part
due to investing in testing infrastructure for different systems. Some of these costs were
one-time costs to purchase machines that sit in our homes and offices while others
are market-rate Hetzner bare metal machines that we run GitHub Actions on.<p>Some of this cost is for hosting ziglang.org. Since our free AWS credits have expired
we have plans to switch to Fastly which should save about $500/month.</p></td></tr><tr><td>Travel</td><td>$10,847.6</td><td>In a 2022 meeting, <a href="https://docs.google.com/document/d/1EqyZcd4AKu7Y9Zb_xdE_7q8i-NnqIz1BW8IiJxMX6Xc">the board decided</a>
that the previous year's travel budget successfully helped grow Zig adoption,
and raised the budget from $10,000 to $15,000. In 2023, ZSF spent $10,847
of those allocated funds, increasing Zig's presence in North America (e.g. Seattle, Vancouver) and Europe (e.g. Stockholm, Berlin, Amsterdam) as well as keeping up relationships with likeminded communities like
<a href="https://handmade-seattle.com/">Handmade Seattle</a>.</td></tr><tr><td>Legal System</td><td>$9,700.00</td><td>This is mostly paying our accountant,
<a href="https://www.stradafg.com/">Strada Financial Group</a>, to keep the American
legal system happy and keep our organization tax-exempt, but also includes a payment
to a legal firm to help us
<a href="https://ziglang.org/news/statement-regarding-zen-programming-language/">regain the Zig trademark in Japan from a trademark troll</a>.</td></tr><tr><td>Taxes</td><td>$8,894.66</td><td>Although ZSF is a tax-exempt organization, employees are still required
to pay income tax.</td></tr><tr><td>Sponsorships</td><td>$6,068.43</td><td>The Zig project is mostly comprised of in-house code, however,
it also relies on third party projects. Today, every Zig
installation includes some source files or ported code from
<a href="http://musl.libc.org/">musl libc</a>,
<a href="https://www.mingw-w64.org/">mingw-w64</a>, and others.
ZSF donates money to these projects as a way to say thanks, give back to the ecosystem,
and increase the sustainability of Zig's dependencies.</td></tr><tr><td>Bank Fees</td><td>$1,060.50</td><td>This is a tiny slice of the pie, but every time ZSF
wires money, there is a transaction fee. Our contractors graciously bill
infrequently when possible to help reduce this cost.</td><td></td></tr><tr><td>Total Expenses</td><td>$461,335.15</td></tr></tbody></table><p><strong>We spent 92% of our money in 2023 on paying contributors for their time.</strong></p><p>So far so good. You can see we’ve been hard at work spending our esteemed
donors' money on advancing the <a href="https://ziglang.org/zsf/#mission-statement">mission statement</a>.</p><p>However, if we look at the trend of donations over time for the year 2023, we
see overall a slow decline. This is likely due to the fact that we have slacked
off on <em>asking for money</em>.</p><h2 id="2023-donations-per-month">2023 Donations Per Month</h2><p>Meanwhile, <strong>user activity has skyrocketed</strong>. A rapidly increasing user base
is adding Zig to their software stacks, filing issues, sending pull requests,
asking for help, and shipping software that depends on Zig.</p><h2 id="new-github-issues-per-month">New GitHub Issues Per Month</h2><table><tbody><tr><th colspan="2">Average time to close issues</th></tr><tr><td>All Time</td><td>5 months</td></tr><tr><td>Past Year</td><td>7 months</td></tr><tr><td>Past Month</td><td>4 months</td></tr></tbody></table><table><tbody><tr><th colspan="2">Average time to close pull requests</th></tr><tr><td>All Time</td><td>12 days</td></tr><tr><td>Past Year</td><td>18 days</td></tr><tr><td>Past Month</td><td>about 1 month</td></tr></tbody></table><p>Source: <a href="https://www.repotrends.com/ziglang/zig" target="_blank" rel="noopener">Repo Trends</a></p><h2 id="total-github-stars">Total GitHub Stars</h2><p>In response to this rising demand, we
<a href="https://ziglang.org/news/welcome-jacob-young/">added incredible new members to the Zig core team</a>.
Thanks to the income that was available to us in 2023, we were able to offer
new contracts.</p><h2 id="2023-income">2023 Income</h2><table><tbody><tr><th>Income Name</th><th>2023 Amount</th><th>Description</th></tr><tr><td>Uber Support Contract</td><td>$184,800.00</td><td>Uber has wisely agreed to a support contract since
<a href="https://www.youtube.com/watch?v=SCj2J3HcEfc">they use the Zig toolchain</a>
and want a guaranteed Service-Level Agreement if they run into any bugs while using it.
Other companies are invited to follow in their footsteps and obtain a ZSF support contract in
order to guarantee speedy response when encountering a bug while using Zig.</td></tr><tr><td>GitHub Sponsors</td><td>$145,462.88</td><td><a href="https://github.com/sponsors/ziglang">Zig on GitHub Sponsors</a>. This category contains a numerous amount of both individuals and companies. It's pretty convenient
for both ZSF and donors, as long as Microsoft keeps being cool about it. Hopefully they don't
alter the deal anytime soon. We lost a lot of donors when they dropped PayPal support.</td></tr><tr><td>Bun</td><td>$58,666.67</td><td><a href="https://bun.sh/">Bun</a> is a
fast JavaScript all-in-one toolkit built using the Zig programming language. The company
behind Bun sponsors ZSF in order to ensure that the tech stack they depend on
continues to flourish, improve, and become more widely adopted.</td></tr><tr><td>TigerBeetle</td><td>$22,000.00</td><td><a href="https://tigerbeetle.com/">TigerBeetle</a> is a database company
whose product is built on the Zig programming language and likewise sponsors ZSF. It's
good business practice to keep your software supply chain healthy.</td></tr><tr><td>Benevity</td><td>$19,851.42</td><td>Benevity helps us collect company-matched donations from employees.
This category contains a number of individuals.</td></tr><tr><td>Pex</td><td>$15,000.00</td><td><a href="https://pex.com/">Pex</a> is a company whose product helps
enable the fair and transparent use of copyrighted content.</td></tr><tr><td>Individuals</td><td>$8,710.76</td><td>This category contains people who donate via paper checks,
<a href="https://www.every.org/zig-software-foundation-inc/">via every.org</a>, or via
UK Online Giving Foundation.</td></tr><tr><td>Total Income</td><td>$454,491.73</td></tr></tbody></table><p>However, with our current level of recurring income, we will not be able to renew
everyone’s contracts, nor offer new contracts to Zig core team members.</p><h2 id="a-plea-for-donations">A Plea for Donations</h2><p>We have extremely talented Zig core team members who want to renew their
contracts, and others who are interested to start getting paid for their
valuable work for the first time.</p><p>In order to do this, <strong>we need more recurring donations</strong>. I for one do not enjoy
asking for money, but in the interest of our users and contributors, it would
be irresponsible not to.</p><p><strong>Please sign up for a monthly donation</strong> if you can. Our preferred donation method
is <a href="https://www.every.org/zig-software-foundation-inc/f/help-zig-stay-indepe" target="_blank" rel="noopener">via Every.org</a>.
A fellow 501(c)(3) non-profit, they seamlessly manage gift receipts, and are not
pivoting to AI <a href="https://github.blog/2023-11-08-universe-2023-copilot-transforms-github-into-the-ai-powered-developer-platform/" target="_blank" rel="noopener">like GitHub is currently doing</a>, which frankly scares the shit
out of us.</p><table><tbody><tr><th>Companies</th><td>Contact us to get your logo on ziglang.org in exchange for a monthly donation.</td></tr><tr><th>Employees</th><td>Check if your company matches donations to charities such as Zig Software Foundation. That 2x multiplier makes a huge difference. We're already in the system.</td></tr><tr><th>Venture Capitalists</th><td>We are aware of a few startups betting on Zig as their language and toolchain of choice to build tomorrow's critical infrastructure. Helping the Zig Software Foundation reach v1.0 faster is one of the most efficient uses of capital you can make to boost your portfolio.</td></tr><tr><th>Individuals</th><td>Can you spare $5-10 per month? This is our favorite
kind of donation because it helps diversify ZSF's income, keeping us free from undue influence
from any single party.</td></tr></tbody></table><p>Huge thanks to all who graciously donate funds to our cause. Together we serve the users!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Identifying Rust's collect:<Vec<_>>() memory leak footgun (132 pts)]]></title>
            <link>https://blog.polybdenum.com/2024/01/17/identifying-the-collect-vec-memory-leak-footgun.html</link>
            <guid>39041520</guid>
            <pubDate>Thu, 18 Jan 2024 13:38:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.polybdenum.com/2024/01/17/identifying-the-collect-vec-memory-leak-footgun.html">https://blog.polybdenum.com/2024/01/17/identifying-the-collect-vec-memory-leak-footgun.html</a>, See on <a href="https://news.ycombinator.com/item?id=39041520">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>Over the weekend, I was working on a personal Rust project when I ran into an excessive memory usage problem. After an evening of trial and error, I found a workaround to fix the memory usage, but I still didn’t understand how the issue was even possible, so I then spent another evening digging through the source code of the Rust standard library to understand the root cause.</p>

<p>This is the story of how I identified the bug. (TLDR: <code>collect::&lt;Vec&lt;_&gt;&gt;()</code> will sometimes reuse allocations, resulting in <code>Vec</code>s with large excess capacity, even when the length is exactly known in advance, so you need to call <code>shrink_to_fit</code> if you want to free the extra memory.)</p>

<h2 id="background">Background</h2>

<p>The project I’m working on involves building a sparse weighted graph and then doing various calculations on the graph. I finally finished the initial version of the code Monday afternoon and tried running it, only for it to exhaust my computer’s 32gb of RAM before getting anywhere. Specifically, the final graph was supposed to have around three million nodes, but during graph construction, the process memory already shot up to over 18gb by the time it reached 300k nodes, and it would use up the entire RAM soon afterwards, slowing everything to a crawl and forcing me to kill the process.</p>

<p>At first, I assumed that the computations I was trying to do were just too big and my project was impossible, but I didn’t give up just yet, and spent the rest of the evening trying to figure out which parts were using so much memory and whether there was any way to optimize them.</p>

<h2 id="narrowing-down-the-memory-leak">Narrowing down the memory leak</h2>

<p>There were several places in the code where I used <a href="https://en.wikipedia.org/wiki/Memoization">memoization</a>, so I initially suspected those of being the cause of the exploding memory usage. I modified the graph construction function to stop after 300k nodes (when memory usage reached 18gb as seen via <code>top</code>, large but not quite enough to kill the computer), and added <code>println!</code>s to log the estimated total sizes of the memoized results. However, they were only in the 100mb range, nowhere near enough to explain the observed memory usage.</p>

<p>Next I tried running graph construction <em>twice</em> (but with it set to stop early at 300k nodes each time) to try to narrow down the cause. My main function looked like this (with <code>denom</code> and <code>optimistic</code> being parameters for the algorithm).</p>

<div><pre><code><span>let</span> <span>denom</span> <span>=</span> <span>257</span><span>;</span>
<span>let</span> <span>optimistic</span> <span>=</span> <span>false</span><span>;</span>
<span>let</span> <span>graph</span> <span>=</span> <span>build_graph</span><span>(</span><span>denom</span><span>,</span> <span>optimistic</span><span>);</span>
<span>let</span> <span>graph</span> <span>=</span> <span>build_graph</span><span>(</span><span>denom</span><span>,</span> <span>optimistic</span><span>);</span>
</code></pre></div>

<p>I carefully watched the process memory usage while this ran, to see if it would stop at 18gb or not. I thought that perhaps the memoization was using a lot more memory than expected due to memory fragmentation exploding the heap.</p>

<p>Since the code is deterministic, calling <code>build_graph</code> twice won’t result in any new results being memoized, and thus memory usage should stop at 18gb, even when it is run twice. Instead, memory usage continued to rapidly increase during the second <code>build_graph</code> call, which made no sense.</p>

<p>Fortunately, I then tried dropping <code>graph</code> in between:</p>

<div><pre><code><span>let</span> <span>graph</span> <span>=</span> <span>build_graph</span><span>(</span><span>denom</span><span>,</span> <span>optimistic</span><span>);</span>
<span>std</span><span>::</span><span>mem</span><span>::</span><span>drop</span><span>(</span><span>graph</span><span>);</span>
<span>let</span> <span>graph</span> <span>=</span> <span>build_graph</span><span>(</span><span>denom</span><span>,</span> <span>optimistic</span><span>);</span>
</code></pre></div>

<p>Suddenly, this time process memory usage dropped from 18gb to ~500mb after the first <code>build_graph</code> call, before going back up to 18gb. Clearly, the memory was somehow being wasted by <code>graph</code> itself. As before, I tried printing out the size of the graph, but it was nowhere near enough to explain the memory usage.</p>

<p>However, my <code>graph</code> data structure consists of <code>Vec</code>s, and Vec can use more memory than the size would indicate since they preallocate a buffer with up to 2x capacity in order to make insertion amortized O(1) time. I changed it to log the total <em>capacity</em> rather than <em>length</em> and suddenly the estimated size of the graph was 18gb. Bingo! I added <code>shrink_to_fit()</code> calls (which remove the excess capacity) into the graph construction code and suddenly the memory problems disappeared.</p>

<h2 id="wait-but-why">Wait, but why?</h2>

<p>I’d finally identified the <code>shrink_to_fit</code> workaround late Monday night and planned to continue work on the project the following weekend. I mentioned my experience to my coworkers at work the following morning, as a PSA about the importance of <code>shrink_to_fit</code>, but they didn’t believe me, and when I thought about it more, I couldn’t believe it myself either.</p>

<p>First off, <code>Vec</code> only grows the buffer by 2x at a time, which means that (ignoring the initial allocation for small vecs), the capacity will always be at most double the length (assuming you don’t remove elements), so the memory waste from excess capacity should always be at most 2x, but I was seeing over 200x.</p>

<p>Second, the memory leak was coming from the edge lists of the graph, which I was storing as <code>Vec&lt;Vec&lt;(u32, u32)&gt;&gt;</code> (a table with one <code>Vec</code> storing the edge list for each node). When I called <code>shrink_to_fit</code> on the edge lists before inserting them, the memory problem disappeared. However, the final step for producing the edge lists was a <code>vec.into_iter().map(..).collect()</code> call to convert them to the appropriate format. Since <code>collect</code> allocates a fresh vector and the length is exactly known, it only allocates exactly enough space for the elements and there should never be <em>any</em> excess capacity at all, let alone 200x excess capacity.</p>

<h2 id="the-vec-source-code">The Vec source code</h2>

<p>Clearly, my assumptions about the implementation of <code>Vec</code> were incorrect somehow, and so out of curiosity, after work Tuesday evening, I dug into the <code>Vec</code> source code to try to figure out what was going on. There are several layers of indirection and specialization, but it didn’t take long to trace through all the relevant code (or so I thought).</p>

<p>When you call <code>collect()</code>, it calls <code>FromIterator::from_iter</code>. <a href="https://github.com/rust-lang/rust/blob/c58a5da7d48ff3887afe4c618dc04defdee3dab5/library/alloc/src/vec/mod.rs#L2791"><code>Vec</code>’s <code>FromIterator</code> impl</a> in turn calls <code>SpecFromIter</code>, an internal trait.</p>

<p><a href="https://github.com/rust-lang/rust/blob/c58a5da7d48ff3887afe4c618dc04defdee3dab5/library/alloc/src/vec/spec_from_iter.rs"><code>SpecFromIter</code></a> has a specialization for <code>IntoIter</code> (i.e. if you call <code>vec.into_iter().collect()</code> with no intervening iterator adapters, it will just return the original vec instead of creating a new one). Otherwise, it calls a second internal trait, <code>SpecFromIterNested</code>.</p>

<p><a href="https://github.com/rust-lang/rust/blob/c58a5da7d48ff3887afe4c618dc04defdee3dab5/library/alloc/src/vec/spec_from_iter_nested.rs"><code>SpecFromIterNested</code></a> in turn has two implementations, one for general iterators and one for <code>TrustedLen</code> iterators (the relevant case here). However, the implementations are pretty much what you would expect. It just reserves a capacity equal to the iterator’s <code>size_hint</code>, with no sign of anything that could result in excess capacity.</p>

<p>I also checked the Vec allocation code and confirmed that it only grows the buffer by 2x each time.</p>

<h2 id="logging-before-and-after">Logging before and after</h2>

<p>Having reached a dead end in the source code, I tried adding logging to my code (and commenting out the <code>shrink_to_fit</code> again) to see how bad the problem was. I logged the length and capacity of the edge lists before and after the final <code>into_iter().map().collect()</code> step, as well as a cumulative total of the length and capacity. (<code>get_cull_all</code> here is the function that calculates the edge lists, while <code>main_edges</code> is the final list of edge lists.)</p>

<div><pre><code><span>let</span> <span>res</span> <span>=</span> <span>get_cull_all</span><span>(</span><span>ds</span><span>,</span> <span>max_denom</span> <span>as</span> <span>u128</span><span>,</span> <span>optimistic</span><span>);</span>
<span>println!</span><span>(</span><span>"precol {} {}"</span><span>,</span> <span>res</span><span>.len</span><span>(),</span> <span>res</span><span>.capacity</span><span>());</span>

<span>let</span> <span>mut</span> <span>res</span><span>:</span> <span>Vec</span><span>&lt;</span><span>_</span><span>&gt;</span> <span>=</span> <span>res</span>
    <span>.into_iter</span><span>()</span>
    <span>.map</span><span>(|(</span><span>ds2</span><span>,</span> <span>p</span><span>)|</span> <span>(</span><span>cull_map</span><span>.get</span><span>(</span><span>ds2</span><span>),</span> <span>p</span> <span>as</span> <span>u32</span><span>))</span>
    <span>.collect</span><span>();</span>

<span>main_len_tot</span> <span>+=</span> <span>res</span><span>.len</span><span>();</span>
<span>main_cap_tot</span> <span>+=</span> <span>res</span><span>.capacity</span><span>();</span>
<span>println!</span><span>(</span><span>"{} {} tot {} {}"</span><span>,</span> <span>res</span><span>.len</span><span>(),</span> <span>res</span><span>.capacity</span><span>(),</span> <span>main_len_tot</span><span>,</span> <span>main_cap_tot</span><span>);</span>
<span>// res.shrink_to_fit();</span>
<span>assert</span><span>!</span><span>(</span><span>main_edges</span><span>.len</span><span>()</span> <span>==</span> <span>id</span><span>);</span>
<span>main_edges</span><span>.push</span><span>(</span><span>res</span><span>);</span>
</code></pre></div>

<p>This produced output like</p>

<div><pre><code>precol 46 11400
46 34200 tot 10410429 2330729253
precol 54 11340
54 34020 tot 10410483 2330763273
precol 2 5520
2 16560 tot 10410485 2330779833
</code></pre></div>

<p>The cumulative wasted space up to 300k nodes was 2330779833/10410485 or over <strong>223x</strong> wasted space. This also confirmed that the edge lists had significant excess capacity following the <code>collect()</code> calls. In fact, the capacity of the new vec produced by <code>collect()</code> was always exactly three times the capacity of the original vec, regardless of length.</p>

<h2 id="playground">Playground</h2>

<p>Having confirmed that <code>collect()</code> was somehow producing vecs with excess capacity despite this seemingly being impossible per the source code, I next tried to reproduce the issue on the Rust playground. I wasn’t able to reproduce the 3x increases that I experienced in my code, but I did notice something interesting.</p>

<div><pre><code><span>let</span> <span>mut</span> <span>v</span> <span>=</span> <span>vec!</span><span>[</span><span>1</span><span>,</span><span>2</span><span>,</span><span>3</span><span>];</span>
<span>v</span><span>.push</span><span>(</span><span>4</span><span>);</span>
<span>println!</span><span>(</span><span>"len {} cap {}"</span><span>,</span> <span>v</span><span>.len</span><span>(),</span> <span>v</span><span>.capacity</span><span>());</span>
<span>let</span> <span>v</span> <span>=</span> <span>v</span><span>.into_iter</span><span>()</span><span>.map</span><span>(|</span><span>x</span><span>|</span> <span>x</span><span>+</span><span>1</span><span>)</span><span>.collect</span><span>::</span><span>&lt;</span><span>Vec</span><span>&lt;</span><span>_</span><span>&gt;&gt;</span><span>();</span>
<span>println!</span><span>(</span><span>"len {} cap {}"</span><span>,</span> <span>v</span><span>.len</span><span>(),</span> <span>v</span><span>.capacity</span><span>());</span>
<span>let</span> <span>v</span> <span>=</span> <span>v</span><span>.iter</span><span>()</span><span>.copied</span><span>()</span><span>.map</span><span>(|</span><span>x</span><span>|</span> <span>x</span><span>+</span><span>1</span><span>)</span><span>.collect</span><span>::</span><span>&lt;</span><span>Vec</span><span>&lt;</span><span>_</span><span>&gt;&gt;</span><span>();</span>
<span>println!</span><span>(</span><span>"len {} cap {}"</span><span>,</span> <span>v</span><span>.len</span><span>(),</span> <span>v</span><span>.capacity</span><span>());</span>    
</code></pre></div>

<p><a href="https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2021&amp;gist=547a67d6857b726120f73e03efd703ed">Running this</a> prints out</p>

<div><pre><code>len 4 cap 6
len 4 cap 6
len 4 cap 4
</code></pre></div>

<p>So the vecs do have excess capacity here, even on the playground, although only a little bit. Furthermore, the excess capacity goes away if you use <code>iter().copied()</code> instead of <code>into_iter()</code>. Unfortunately, that part sidetracked me for a while, because I thought the <code>IntoIter</code> specialization of <code>SpecFromIter</code> might be related. It <em>shouldn’t</em> be, because the iterator here is <code>Map&lt;IntoIter&gt;</code> rather than <code>IntoIter</code> itself, but I couldn’t see anything else related and spent a long time staring at the source code, willing an answer to materialize.</p>

<p>Since the original issue didn’t show up on the playground, I thought it might be version-specific. In my own code, I was using the nightly compiler so I could use <code>Vec::is_sorted</code>, but fortunately, I was just using that for a couple asserts, so after commenting those out, I was able to switch to the beta compiler (1.76), where the exact same behavior occurred. As for stable, I was unable to try stable (1.74) because of compiler errors due to an unrelated issue with <code>impl Trait</code>. (As it turned out, the issue is only present in beta and not stable, so this was an unfortunate miss.) I also confirmed that the issue reproduced in non-release builds, so it wasn’t optimization related.</p>

<h2 id="vec-type-changes">Vec type changes</h2>

<p>I continued attempting to narrow down and reproduce my issue on the playground. After some more trial and error, I discovered that mapping a vec to a <em>smaller</em> type resulted in excess capacity, even on the playground, but <em>only</em> on beta and nightly, not stable.</p>

<div><pre><code><span>let</span> <span>mut</span> <span>v</span> <span>=</span> <span>vec!</span><span>[</span><span>1</span><span>,</span><span>2</span><span>,</span><span>3</span><span>,</span><span>4</span><span>];</span>
<span>v</span><span>.push</span><span>(</span><span>1u128</span><span>);</span>
<span>println!</span><span>(</span><span>"len {} cap {} v={:?}"</span><span>,</span> <span>v</span><span>.len</span><span>(),</span> <span>v</span><span>.capacity</span><span>(),</span> <span>v</span><span>);</span>
<span>let</span> <span>v</span> <span>=</span> <span>v</span><span>.into_iter</span><span>()</span><span>.map</span><span>(|</span><span>x</span><span>|</span> <span>x</span> <span>as</span> <span>u64</span><span>)</span><span>.collect</span><span>::</span><span>&lt;</span><span>Vec</span><span>&lt;</span><span>_</span><span>&gt;&gt;</span><span>();</span>
<span>println!</span><span>(</span><span>"len {} cap {} v={:?}"</span><span>,</span> <span>v</span><span>.len</span><span>(),</span> <span>v</span><span>.capacity</span><span>(),</span> <span>v</span><span>);</span>
<span>let</span> <span>v</span> <span>=</span> <span>v</span><span>.into_iter</span><span>()</span><span>.map</span><span>(|</span><span>x</span><span>|</span> <span>x</span> <span>as</span> <span>u32</span><span>)</span><span>.collect</span><span>::</span><span>&lt;</span><span>Vec</span><span>&lt;</span><span>_</span><span>&gt;&gt;</span><span>();</span>
<span>println!</span><span>(</span><span>"len {} cap {} v={:?}"</span><span>,</span> <span>v</span><span>.len</span><span>(),</span> <span>v</span><span>.capacity</span><span>(),</span> <span>v</span><span>);</span>  
<span>let</span> <span>v</span> <span>=</span> <span>v</span><span>.into_iter</span><span>()</span><span>.map</span><span>(|</span><span>x</span><span>|</span> <span>x</span> <span>as</span> <span>u8</span><span>)</span><span>.collect</span><span>::</span><span>&lt;</span><span>Vec</span><span>&lt;</span><span>_</span><span>&gt;&gt;</span><span>();</span>
<span>println!</span><span>(</span><span>"len {} cap {} v={:?}"</span><span>,</span> <span>v</span><span>.len</span><span>(),</span> <span>v</span><span>.capacity</span><span>(),</span> <span>v</span><span>);</span>  
<span>let</span> <span>v</span> <span>=</span> <span>v</span><span>.into_iter</span><span>()</span><span>.map</span><span>(|</span><span>x</span><span>|</span> <span>x</span> <span>as</span> <span>u16</span><span>)</span><span>.collect</span><span>::</span><span>&lt;</span><span>Vec</span><span>&lt;</span><span>_</span><span>&gt;&gt;</span><span>();</span>
<span>println!</span><span>(</span><span>"len {} cap {} v={:?}"</span><span>,</span> <span>v</span><span>.len</span><span>(),</span> <span>v</span><span>.capacity</span><span>(),</span> <span>v</span><span>);</span>     
</code></pre></div>

<div><pre><code>len 5 cap 8 v=[1, 2, 3, 4, 1]
len 5 cap 16 v=[1, 2, 3, 4, 1]
len 5 cap 32 v=[1, 2, 3, 4, 1]
len 5 cap 128 v=[1, 2, 3, 4, 1]
len 5 cap 5 v=[1, 2, 3, 4, 1]
</code></pre></div>

<p>Furthermore, the capacity increased proportionately to the type size reduction. In this example, going from <code>u128</code> to <code>u64</code> to <code>u32</code>, etc. doubles the capacity each time. On the other hand, <em>increasing</em> the type size as in the last example (<code>u8</code> -&gt; <code>u16</code>) immediately removes the excess capacity.</p>

<h2 id="allocation-reuse">Allocation reuse</h2>

<p>Since the capacity was increasing in proportion to the decrease in element size, that implied that the size of the backing array for the new vec was suspiciously the same as that of the old vec. I hypothesized that it was somehow reusing the memory from the old vec as an optimization. This also explains why it only happens with <code>into_iter()</code> and why using <code>iter().copied()</code> eliminates the excess capacity. When using <code>iter()</code>, the old vec isn’t dropped, and so its memory can’t be reused. In my code, the last step prior to insertion maps the edge lists from <code>(u64, u128)</code> to <code>(u32, u32)</code>, which is a third the size, hence why the capacity always increased by 3x after the conversion.</p>

<p>The other thing this implied is that while <code>collect()</code> was tripling the capacity of the vec, the actual memory use was unchanged, which meant that <code>collect()</code> alone wasn’t the cause of the memory leak. Rather, it was merely <em>failing to decrease</em> the existing memory usage. In order to understand how this could have exploded the process memory usage, a bit of explanation of my code is in order.</p>

<p>My edge list calculation logic internally uses <code>u128</code> for increased precision before truncating the weights to <code>u32</code> at the end. Furthermore, in some cases, it generates a list of potential edges and then filters out most of them prior to returning, which leads to a large amount of excess capacity. As an extreme example, the last iteration in the log output shown above had a length of 2 but a capacity of 5520.</p>

<p>Ordinarily, that wouldn’t have been a problem, since the <code>into_iter().map().collect()</code> line used to pack them into <code>(u32, u32)</code>s would allocate a new vector with only the exact amount of space required. However, thanks to the allocation reuse optimization added in Rust 1.76*, the new vec shared the backing store of the input vec, and hence had a capacity of 16560, meaning it was using 132480 bytes of memory to store only 16 bytes of data.</p>

<p>Since the edge list calculation logic is run for one node at a time in sequence, the fact that it temporarily uses 132kb of memory would normally not be a problem at all. However, the new <code>collect()</code> optimization meant that that internal storage was being preserved in the final vec which got inserted into the list of edge lists, thus keeping around that 132kb of obsolete memory forever. Multiply that by 300k nodes and suddenly you’ve leaked the computer’s entire RAM.</p>

<p>* <em>Technically, Rust will sometimes reuse the allocation even on stable, as shown by the initial len=4,cap=6 example on the playground. Rust 1.76 merely made the optimization smarter so it triggers in more cases.</em></p>

<h2 id="the-source-code">The source code</h2>

<p>Now that I knew I was looking specifically for a commit added in Rust 1.76, it wasn’t hard to look through the recent changes to the <code>Vec</code> source code on Github and find <a href="https://github.com/rust-lang/rust/commit/13a843ebcbe536257a8442bf5c26b227d1c2f7c9">the likely culprit</a>. It turns out that there’s <a href="https://github.com/rust-lang/rust/blob/c58a5da7d48ff3887afe4c618dc04defdee3dab5/library/alloc/src/vec/in_place_collect.rs">an entire 412 line file dedicated to implementing this optimization</a> that I overlooked previously.</p>

<p>How was this possible? One of the more annoying features of Rust is that Trait impls can be added <em>anywhere in the same crate</em>, not just in the files where you would logically expect them to be, and there’s no way to find hidden impls other than doing a full code search.</p>

<p>In this case, the <code>SpecFromIter</code> trait was defined in <a href="https://github.com/rust-lang/rust/blob/c58a5da7d48ff3887afe4c618dc04defdee3dab5/library/alloc/src/vec/spec_from_iter.rs">spec_from_iter.rs</a>, along with two implementations of that trait, so I naturally assumed that that was all the implementations there were. I had no way to guess that there was actually a <em>third</em> implementation that was hidden in a completely different file (in_place_collect.rs).</p>

<p>Technically speaking, there was one slight hint. <code>spec_from_iter.rs</code> contains the following comment:</p>

<div><pre><code><span>/// Specialization trait used for Vec::from_iter</span>
<span>///</span>
<span>/// ## The delegation graph:</span>
<span>///</span>
<span>/// ```text</span>
<span>/// +-------------+</span>
<span>/// |FromIterator |</span>
<span>/// +-+-----------+</span>
<span>///   |</span>
<span>///   v</span>
<span>/// +-+-------------------------------+  +---------------------+</span>
<span>/// |SpecFromIter                  +----&gt;+SpecFromIterNested   |</span>
<span>/// |where I:                      |  |  |where I:             |</span>
<span>/// |  Iterator (default)----------+  |  |  Iterator (default) |</span>
<span>/// |  vec::IntoIter               |  |  |  TrustedLen         |</span>
<span>/// |  SourceIterMarker---fallback-+  |  +---------------------+</span>
<span>/// +---------------------------------+</span>
<span>/// ```</span>
</code></pre></div>

<p>This comment lists the two impls found in the file as well as a seemingly missing third impl for “SourceIterMarker”. I actually did try googling “SourceIterMarker” at one point, but the only thing that came up was <em>that same comment in spec_from_iter.rs</em>, and since there was no sign of a third impl in the file, I assumed that it was just a mistake or outdated comment. In retrospect, I should have done a full code search for “SourceIterMarker” on Github, just in case it had sneakily been hidden in a separate file.</p>

<h2 id="boxt">Box&lt;[T]&gt;</h2>

<p>As mentioned previously, adding a call to <code>shrink_to_fit()</code> prior to storing the edge lists in the final graph eliminated the memory leak issue. This is fine as a quick hack, but it is not the proper solution. The <em>real</em> fix is to call <code>into_boxed_slice()</code> instead.</p>

<p>The basic problem is that <code>Vec</code>s are optimized for the case of a <em>mutable</em> list where you’re planning to <em>add and remove</em> elements in the future. However, people also commonly use them for <em>fixed-length</em> list data, even though this is not what Vecs are designed for, simply because Vec is the easiest and most <em>intuitive</em> way to store list data in Rust. The <em>proper</em> data structure for fixed-length lists is <code>Box&lt;[T]&gt;</code> rather than <code>Vec&lt;T&gt;</code>, but it takes a lot of Rust experience to even <em>know</em> about that as an option, and the syntax looks weirder.</p>

<p>In Java, immutable strings are called <code>String</code> while mutable strings (the equivalent of Rust’s <code>String</code>) are called <code>StringBuilder</code> instead. This nudges people towards the appropriate behavior because you would look really foolish if you stored something named “StringBuilder” in long-term immutable data structures. Meanwhile in Rust, almost nobody uses <code>Box&lt;str&gt;</code> over <code>String</code>.</p>

<p>Of course, I’m not saying that Rust should have called <code>Vec</code> “ListBuilder” or anything. Mutable array lists are an <strong>extremely</strong> commonly needed component of algorithms and data structures, so it’s important to keep the name short and memorable. It’s just a little unfortunate that a side effect of Rust’s design nudges people towards <em>also</em> using <code>Vec</code> even for fixed-length data.</p>

<p>As it turns out, I actually was already using <code>Box&lt;[T]&gt;</code> for the saved results in one of the functions where I used memoization. My main motivation was to avoid wasting an extra word to store capacity on the <em>value</em> side, but as a side effect, this conveniently also made it immune to any potential excess capacity bugs. (The other place where I used memoization did just use <code>Vec</code>s out of laziness, but fortunately it didn’t end up mattering much there.)</p>

<h2 id="packed-arrays">Packed arrays</h2>

<p>Another possible optimization which I plan to try out later is to avoid storing separate lists in the first place. Instead, have a single “arena” Vec and store the data for <em>all</em> the lists <em>continguously</em> in that Vec and just pass around an (index, length) pair into that vec instead of real <code>Vec</code>/<code>Box&lt;[]&gt;</code>, etc.</p>

<p>This design is only sometimes usable (you can’t free or pass around individual lists independently of the arena vec this way) and is much more intrusive in terms of code design (you have to somehow pass a reference to the arena vec through all your code). However, in addition to making you automatically immune to excess capacity bugs, it also avoids the potential memory fragmentation issue that having lots of little individually allocated lists could cause. Additionally, it lets you represent your lists as only <code>(u32, u32)</code> assuming the total length is less than 2³², a 2x saving on the value side as well, albeit at the cost of increased bounds checks.</p>

<p>This trick has limited applicability, but it is perfect for cases like memoization (where the results live forever and are stored in a single place) or the edge list of a graph data structure (where you can store the backing vec as part of the graph) assuming you don’t need to add or remove edges after creation.</p>

<h2 id="conclusion">Conclusion</h2>

<p>This was a surprising and annoying bug to investigate, but at least I learned a lot about Rust in the process. I had no idea that the standard library was doing this kind of optimization under the hood (and since this is code-based, it happens even in debug builds). It’s also an illustration of how an optimization in one place can lead to bugs downstream by violating programmers’ expectations.</p>

<p>Anyway, I hope that writing about my experience can help other people avoid this footgun in the future, or at least teach people something interesting about Rust.</p>


  </div>
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Queues don't fix overload (2014) (167 pts)]]></title>
            <link>https://ferd.ca/queues-don-t-fix-overload.html</link>
            <guid>39041477</guid>
            <pubDate>Thu, 18 Jan 2024 13:33:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ferd.ca/queues-don-t-fix-overload.html">https://ferd.ca/queues-don-t-fix-overload.html</a>, See on <a href="https://news.ycombinator.com/item?id=39041477">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
        
            <span>2014/11/19</span>
        
        
        

<p>OK, queues.</p>

<p>People misuse queues all the time. The most egregious case being to fix issues with slow apps, and consequently, with overload. But to say why, I'll need to take bits of talks and texts I have around the place, and content that I have written in more details about in <a href="http://www.erlang-in-anger.com/">Erlang in Anger</a>.</p>

<p>To oversimplify things, most of the projects I end up working on can be visualized as a very large bathroom sink. User and data input are flowing from the faucet, down 'till the output of the system:</p>
<img src="https://ferd.ca/static/img/sink1.png" width="534" height="309">
<p>So under normal operations, your system can handle all the data that comes in, and carry it out fine:</p>
<img src="https://ferd.ca/static/img/sink2.png" width="479" height="307">
<p>Water goes in, water goes out, everyone's happy. However, from time to time, you'll see temporary overload on your system. If you do messaging, this is going to be around sporting events or events like New Year's Eve. If you're a news site, it's gonna be when a big thing happens (Elections in the US, Royal baby in the UK, someone says they dislike French as a language in Quebec).</p>

<p>During that time, you may experience that temporary overload:</p>
<img src="https://ferd.ca/static/img/sink3.png" width="452" height="315">
<p>The data that comes out of the system is still limited, and input comes in faster and faster. Web people will use stuff like caches at that point to make it so the input and output required gets to be reduced. Other systems will use a huge buffer (a queue, or in this case, a sink) to hold the temporary data.</p>

<p>The problem comes when you inevitably encounter prolonged overload. It's when you look at your system load and go "oh crap", and it's not coming down ever. Turns out Obama doesn't want to turn in his birth certificate, the royal baby doesn't look like the father, and someone says Quebec should be better off with Parisian French, and the rumor mill is going for days and weeks at a time:</p>
<img src="https://ferd.ca/static/img/sink4.png" width="449" height="303">
<p>All of a sudden, the buffers, queues, whatever, can't deal with it anymore. You're in a critical state where you can see smoke rising from your servers, or if in the cloud, things are as bad as usual, but more!</p>

<p>The system inevitably crashes:</p>
<img src="https://ferd.ca/static/img/sink5.png" width="453" height="314">
<p>Woops, everyone is dead, you're in the office at 3am (who knew so many people in the US, disgusted with their "Kenyan" president, now want news on the royal baby, while Quebec people look up 'royale with cheese baby' for some reason) trying to keep things up.</p>

<p>You look at your stack traces, at your queue, at your DB slow queries, at the APIs you call. You spend weeks at a time optimizing every component, making sure it's always going to be good and solid. Things keeps crashing, but you hit the point where every time, it takes 2-3 days more.</p>

<p>At the end of it, you see a crapload of problems still happening, but they're a week apart between each failure, which slows down your optimizing in immense ways because it's incredibly hard to measure things when they take weeks to go bad.</p>

<p>You go "okay I'm all out of ideas, let's buy a bigger server." The system in the end looks like this, and it's still failing:</p>
<img src="https://ferd.ca/static/img/sink6.png" width="476" height="315">
<p>Except now it's an unmaintainable piece of garbage full of dirty hacks to make it work that cost 5 times what it used to, and you've been paid for months optimizing it for no god damn reason because it still dies when overloaded.</p>

<p>The problem? That red arrow there. You're hitting some hard limit that even through all of your profiling, you didn't consider properly. This can be a database, an API to an external service, disk speed, bandwidth or general I/O limits, paging speed, CPU limits, whatever.</p>

<p>You've spent months optimizing your super service only to find out at some point in time, you went past its optimal speed without larger changes, and the day your system got to have an operational speed greater than this hard limit, you've doomed yourself to an everlasting series of system failures.</p>

<p>The disheartening part about it is that you discover that once your system is popular, has people using it and its APIs, and changing it to be better is very expensive and hard. Especially since you'll probably have to revisit assumptions you've made in its core design. Woops.</p>

<p>So what do you need? You'll need to pick <em>what has to give</em> whenever stuff goes bad. You'll have to pick between blocking on input (back-pressure), or dropping data on the floor (load-shedding). And that happens all the time in the real world, we just don't want to do it as developers, as if it were an admission of failure.</p>

<p>Bouncers in front of a club, water spillways to go around dams, the pressure mechanism that keeps you from putting more gas in a full tank, and so on. They're all there to impose a system-wide flow control to keep operations safe.</p>

<p>In [non-critical] software? Who cares! We never shed load because that makes stakeholders angry, and we never think about back-pressure. Usually the back-pressure in the system is implicit: 'tis slow.</p>

<p>A function/method call to something ends up taking longer? It's slow. Not enough people think of it as back-pressure making its way through your system. In fact, slow distributed systems are often the canary in the overload coal mine. The problem is that everyone just stands around and goes "durr why is everything so slow??" and devs go "I don't know! It just is! It's hard, okay!"</p>

<p>That's usually because somewhere in the system (possibly the network, or something that is nearly impossible to observe without proper tooling, such as <a href="http://www.snookles.com/slf-blog/2012/01/05/tcp-incast-what-is-it/">TCP incast</a>), something is clogged and everything else is pushing it back to the edge of your system, to the user.</p>

<p>And that back-pressure making the system slower? It slows down the rate at which users can input data. It's what is likely keeping your whole stack alive. And you know when people start using queues? Right there. When operations take too long and block stuff up, people introduce a freaking queue in the system.</p>

<p>And the effects are instant. The application that was sluggish is now fast again. Of course you need to redesign the whole interface and interactions and reporting mechanisms to become asynchronous, but man is it fast!</p>

<p>Except at some point the queue spills over, and you lose all of the data. There's a serious meeting that then takes place where everyone discusses how this could possibly have happened. Dev #3 suggests more workers are added, Dev #6 recommends the queue gets persistency so that when it crashes, no requests are lost.</p>

<p>"Cool," says everyone. Off to work. Except at some point, the system dies again. And the queue comes back up, but it's already full and uuugh. Dev #5 goes in and thinks "oh yeah, we could add more queues" (I swear I've seen this unfold back when I didn't know better). People say "oh yeah, that increases capacity" and off they go.</p>

<p>And then it dies again. And nobody ever thought of that sneaky red arrow there:</p>
<img src="https://ferd.ca/static/img/sink6.png" width="476" height="315">
<p>Maybe they do it without knowing, and decide to go with MongoDB because it's "faster than Postgres" (heh). Who knows.</p>

<p>The real problem is that everyone involved used queues as an optimization mechanism. With them, new problems are now part of the system, which is a nightmare to maintain. Usually, these problems will come in the form of ruining the <a href="https://en.wikipedia.org/wiki/End-to-end_principle">end-to-end principle</a> by using a persistent queue as a fire-and-forget mechanisms or assuming tasks can't be replayed or lost. You have more places that can time out, require new ways to detect failures and communicate them back to users, and so on.</p>

<p>Those can be worked around, don't get me wrong. The issue is that they're being introduced as part of a solution that's not appropriate for the problem it's built to solve. All of this was just premature optimization. Even when everyone involved took measures, reacted to real failures in real pain points, etc. The issue is that nobody considered what the true, central business end of things is, and what its limits are. People considered these limits locally in each sub-component, more or less, and not always.</p>

<p>But someone should have picked what had to give: do you stop people from inputting stuff in the system, or do you shed load. Those are inescapable choices, where inaction leads to system failure.</p>

<p>And you know what's cool? If you identify these bottlenecks you have for real in your system, and you put them behind proper back-pressure mechanisms, your system won't even have the right to become slow.</p>

<p>Step 1. Identify the bottleneck. Step 2: ask the bottleneck for permission to pile more data in:</p>
<img src="https://ferd.ca/static/img/sink7.png" width="387" height="259"> <img src="https://ferd.ca/static/img/sink8.png" width="383" height="259">
<p>Depending on where you put your probe, you can optimize for different levels of latency and throughput, but what you're going to do is define proper operational limits of your system.</p>

<p>When people blindly apply a queue as a buffer, all they're doing is creating a bigger buffer to accumulate data that is in-flight, only to lose it sooner or later. You're making failures more rare, but you're making their magnitude worse.</p>

<p>When you shed load and define proper operational limits to your system, you don't have these. What you may have is customers that are as unhappy (because in either case, they can't do what your system promises right), but with proper back-pressure or load-shedding, you gain:</p>

<ul>
<li>Proper metrics of your quality of service</li>
<li>An API that will be designed with either in mind (back-pressure lets you know when you're in an overload situation, and when to retry or whatever, and load-shedding lets the user know that some data was lost so they can work around that)</li>
<li>Fewer night pages</li>
<li>Fewer critical rushes to get everything fixed because it's dying all the time</li>
<li>A way to monetize your services through varying account limits and priority lanes</li>
<li>You act as a more reliable endpoint for everyone who depends on you</li>
</ul>
<p>To make stuff usable, a proper idempotent API with end-to-end principles in mind will make it so these instances of back-pressure and load shedding should rarely be a problem for your callers, because they can safely retry requests and know if they worked.</p>

<p>So when I rant about/against queues, it's because queues will often be (but not always) applied in ways that totally mess up end-to-end principles for no good reason. It's because of bad system engineering, where people are trying to make an 18-wheeler go through a straw and wondering why the hell things go bad. In the end the queue just makes things worse. And when it goes bad, it goes really bad, because everyone tried to close their eyes shut and ignore the fact they built a dam to solve flooding problems upstream of the dam.</p>

<p>And then of course, there's the use case where you use the queue as a messaging mechanism between front-end threads/processes (think PHP, Ruby, CGI apps in general, and so on) because your language doesn't support inter-process communications. It's marginally better than using a MySQL table (which I've seen done a few times and even took part in), but infinitely worse than picking a tool that supports the messaging mechanisms you need to implement your solution right.</p>

    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Corporations are not to be loved (105 pts)]]></title>
            <link>https://inessential.com/2024/01/17/corporations_are_not_to_be_loved</link>
            <guid>39041432</guid>
            <pubDate>Thu, 18 Jan 2024 13:30:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://inessential.com/2024/01/17/corporations_are_not_to_be_loved">https://inessential.com/2024/01/17/corporations_are_not_to_be_loved</a>, See on <a href="https://news.ycombinator.com/item?id=39041432">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Researcher uncovers one of the biggest password dumps in recent history (130 pts)]]></title>
            <link>https://arstechnica.com/security/2024/01/71-million-passwords-for-facebook-coinbase-and-others-found-for-sale/</link>
            <guid>39041106</guid>
            <pubDate>Thu, 18 Jan 2024 12:54:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/security/2024/01/71-million-passwords-for-facebook-coinbase-and-others-found-for-sale/">https://arstechnica.com/security/2024/01/71-million-passwords-for-facebook-coinbase-and-others-found-for-sale/</a>, See on <a href="https://news.ycombinator.com/item?id=39041106">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <header>
            <h4>
      MASSIVE DATA TRANCHE    —
</h4>
            
            <h2 itemprop="description">Roughly 25 million of the passwords have never been seen before by widely used service.</h2>
                    </header>
        <div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2022/12/password-change-800x534.jpg" alt="Calendar with words Time to change password. Password management.">
      <figcaption><p>Getty Images</p></figcaption>  </figure>

  




<!-- cache hit 24:single/related:e238b01aa520f6d68616ef64d17b8656 --><!-- empty -->
<p>Nearly 71 million unique credentials stolen for logging into websites such as Facebook, Roblox, eBay, and Yahoo have been circulating on the Internet for at least four months, a researcher said Wednesday.</p>
<p>Troy Hunt, operator of the <a href="https://haveibeenpwned.com/">Have I Been Pwned?</a> breach notification service, <a href="https://www.troyhunt.com/inside-the-massive-naz-api-credential-stuffing-list/">said</a> the massive amount of data was posted to a well-known underground market that brokers sales of compromised credentials. Hunt said he often pays little attention to dumps like these because they simply compile and repackage previously published passwords taken in earlier campaigns.</p>
<figure><a href="https://cdn.arstechnica.net/wp-content/uploads/2024/01/naz-apo-data-post.png" data-height="1184" data-width="2150" alt="Post appearing on breach site advertising the availability of naz.api password data."><img alt="Post appearing on breach site advertising the availability of naz.api password data." src="https://cdn.arstechnica.net/wp-content/uploads/2024/01/naz-apo-data-post-640x352.png" width="640" height="352" srcset="https://cdn.arstechnica.net/wp-content/uploads/2024/01/naz-apo-data-post-1280x705.png 2x"></a><figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2024/01/naz-apo-data-post.png" data-height="1184" data-width="2150">Enlarge</a> <span>/</span> Post appearing on breach site advertising the availability of naz.api password data.</p></figcaption></figure>
<h2>Not your typical password dump</h2>
<p>Some glaring things prevented Hunt from dismissing this one, specifically the contents indicating that nearly 25 million of the passwords had never been leaked before:</p>
<ol>
<li aria-level="1">319 files totaling 104GB</li>
<li aria-level="1">70,840,771 unique email addresses</li>
<li aria-level="1">427,308 individual HIBP subscribers impacted</li>
<li aria-level="1">65.03 percent of addresses already in HIBP (based on a 1,000 random sample set)</li>
</ol>
<p>“That last number was the real kicker,” Hunt wrote. “When a third of the email addresses have never been seen before, that's statistically significant. This isn't just the usual collection of repurposed lists wrapped up with a brand-new bow on it and passed off as the next big thing; it's a significant volume of new data. When you look at the above forum post the data accompanied, the reason why becomes clear: it's from ‘stealer logs’ or in other words, malware that has grabbed credentials from compromised machines.”</p>
<p>A redacted image that Hunt posted showing a small sample of the exposed credentials indicated that account credentials for a variety of sites were swept up. Sites included Facebook, Roblox, Coinbase, Yammer, and Yahoo. In keeping with the claim that the credentials were collected by a “stealer”—malware that runs on a victim’s device and uploads all user names and passwords entered into a login page—the passwords appear in plaintext. Account credentials taken in website breaches are almost always cryptographically hashed. (A sad aside: Most of the exposed credentials are weak and would easily fall to a simple <a href="https://arstechnica.com/information-technology/2013/08/thereisnofatebutwhatwemake-turbo-charged-cracking-comes-to-long-passwords/">password dictionary attack</a>.)
</p><figure><a href="https://cdn.arstechnica.net/wp-content/uploads/2024/01/nas-api-credential-sample.png" data-height="1258" data-width="2262" alt="Screenshot showing a sample of 20 credential pairs, with usernames redacted."><img alt="Screenshot showing a sample of 20 credential pairs, with usernames redacted." src="https://cdn.arstechnica.net/wp-content/uploads/2024/01/nas-api-credential-sample-640x356.png" width="640" height="356" srcset="https://cdn.arstechnica.net/wp-content/uploads/2024/01/nas-api-credential-sample-1280x712.png 2x"></a><figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2024/01/nas-api-credential-sample.png" data-height="1258" data-width="2262">Enlarge</a> <span>/</span> Screenshot showing a sample of 20 credential pairs, with usernames redacted.</p><p>Have I Been Pwned?</p></figcaption></figure>
<p>Data collected by Have I Been Pwned indicates this password weakness runs rampant. Of the 100 million unique passwords amassed, they have appeared 1.3 billion times.</p>                                            
                                                        
<p>“To be fair, there are instances of duplicated rows, but there's also a massive prevalence of people using the same password across multiple different services and completely different people using the same password (there are a finite set of dog names and years of birth out there...),” Hunt wrote. “And now more than ever, the impact of this service is absolutely huge!”</p>
<p>Hunt confirmed the authenticity of the dataset by contacting people at some of the listed emails. They confirmed that the credentials listed there were—or at least once were—accurate. For added assurance, Hunt also checked a sample of the credentials to see if the email addresses were associated with accounts on the affected websites. All of them did. Some of Hunt’s users reported that the passwords appeared to be valid as of 2020 or 2021. Whatever the date of the passwords, it stands to reason that unless they’ve been updated, they remain valid. The underground market <a href="https://breachforums.is/Thread-FREE-Full-naz-api-Dataset-Leaked-Download">post</a> advertising the dataset said it came from a breach dubbed naz.api that had been donated to a different site earlier.</p>
<p>Hunt said that a large percentage of the credentials came not from stealer malware as claimed, but from credential stuffing, a form of account-hijacking attack that collects large numbers of stolen account credentials from previous breaches. Hunt said credential stuffing sources explained how a password he used "pre-2011" landed in the dump.</p>
<p>"Some of this data does not come from malware and has been around for a significant period of time," he wrote. "My own email address, for example, accompanied a password not used for well over a decade and did not accompany a website indicating it was sourced from malware."</p>
<h2>Making passwords safe</h2>
<p>There are dozens of useful primers online explaining how to properly secure accounts. The two main ingredients to account security are: (1) choosing strong passwords and (2) keeping them out of the sight of prying eyes. This means:
</p><ul>
<li>Creating a long, randomly generated password or passphrase. These passcodes should be at least 11 characters for passwords and for passphrases at least four words randomly chosen from a dictionary of no fewer than 50,000 entries. <a href="https://bitwarden.com/go/password-management-business-sales/?msclkid=a591fe1c4a9915342db69bb41a862deb">Bitwarden</a>, a free, open source password manager, is a good choice and a great way for less experienced people to get started. Once a password is created, it should be stored in the password-manager vault.</li>
<li>Preventing strong passwords from being compromised. This entails not entering passwords into phishing sites and keeping devices free of malware.</li>
<li>Use two-factor authentication, preferably with a security key or authenticator app, whenever possible. This doubly applies to protecting the password manager with 2FA.</li>
<li>Better yet, <a href="https://arstechnica.com/information-technology/2023/05/passkeys-may-not-be-for-you-but-they-are-safe-and-easy-heres-why/">use passkeys</a>, a new, industry-wide authentication standard that's immune to theft through stealer apps and credential phishing.</li>
</ul>                                            
                                                        
<p>It’s also a good idea to either create an account with Have I Been Pwned? or periodically enter email addresses into the site search box to check if they appear in any breaches. To prevent abuse of the search, the site doesn’t log entered email addresses and no corresponding passwords are loaded with password data stored on the site. Have I Been Pwned also accepts a single email address at a time, except in certain cases. You can find more on the service and the security of using it <a href="https://haveibeenpwned.com/FAQs">here</a>.</p>
<p>Have I Been Pwned also allows users to search its database for <a href="https://haveibeenpwned.com/API/v3?ref=troyhunt.com#PwnedPasswords">specific passwords</a>. More about k-anonymity and other measures Hunt uses to prevent password exposure and abuse of his service is <a href="https://haveibeenpwned.com/API/v3?ref=troyhunt.com#PwnedPasswords">here</a>.</p>
<p><em>This post has been updated to correct inferences about how Hunt's password ended up in the dataset.</em></p>

                                                </div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Study: Field Drug Tests Generate Nearly 30k Bogus Arrests a Year (158 pts)]]></title>
            <link>https://www.techdirt.com/2024/01/17/study-field-drug-tests-generate-nearly-30000-bogus-arrests-a-year/</link>
            <guid>39040839</guid>
            <pubDate>Thu, 18 Jan 2024 12:21:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.techdirt.com/2024/01/17/study-field-drug-tests-generate-nearly-30000-bogus-arrests-a-year/">https://www.techdirt.com/2024/01/17/study-field-drug-tests-generate-nearly-30000-bogus-arrests-a-year/</a>, See on <a href="https://news.ycombinator.com/item?id=39040839">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="storywrap-430253">


<h3>from the <i>too-big-to-fail,-too-cheap-to-care</i> dept</h3>

<p><a href="https://www.techdirt.com/articles/20160712/15543134951/field-drug-tests-2-tool-that-can-destroy-lives.shtml" data-type="link" data-id="https://www.techdirt.com/articles/20160712/15543134951/field-drug-tests-2-tool-that-can-destroy-lives.shtml">Field drug tests</a> often seem to be more a triumph of imagination than a triumph of science. They’re cheap. Some popular tests run <a href="https://www.techdirt.com/2016/11/04/las-vegas-pd-continues-to-use-faulty-2-drug-field-tests-because-convictions-matter-more-than-justice/" data-type="link" data-id="https://www.techdirt.com/2016/11/04/las-vegas-pd-continues-to-use-faulty-2-drug-field-tests-because-convictions-matter-more-than-justice/">less than $3/per</a>. That’s the literal selling point. When in doubt, a cop can get probable cause by grabbing a substance, dumping it into a field test, and deciding whatever results are generated are evidence of guilt.</p>
<p>It rarely is. Sure, if you run enough tests, you’re bound to have some of these field tests confirmed by lab tests that are far more precise and less likely to be interpreted subjectively by the person performing the test. (Theoretically. There’s plenty of evidence out there showing lab drug tests can be just as faulty as field drug tests, although <a href="https://www.techdirt.com/2018/04/10/more-drug-lab-misconduct-results-massachusetts-court-tossing-nearly-12000-convictions/" data-type="link" data-id="https://www.techdirt.com/2018/04/10/more-drug-lab-misconduct-results-massachusetts-court-tossing-nearly-12000-convictions/">in these cases</a>, the problem is <a href="https://www.techdirt.com/2017/04/26/prosecutors-overturn-more-than-21000-drug-convictions-wake-massive-drug-lab-misconduct/" data-type="link" data-id="https://www.techdirt.com/2017/04/26/prosecutors-overturn-more-than-21000-drug-convictions-wake-massive-drug-lab-misconduct/">usually the person performing</a> the testing [or not!] than the test itself.)</p>
<p>Cheap, fast, and easy. And wrong. So very very often wrong. Field drug tests have labeled everything from <a href="https://www.techdirt.com/2019/08/16/prosecutor-tosses-charges-against-driver-after-field-drug-test-claims-bird-poop-cars-hood-is-cocaine/" data-type="link" data-id="https://www.techdirt.com/2019/08/16/prosecutor-tosses-charges-against-driver-after-field-drug-test-claims-bird-poop-cars-hood-is-cocaine/">bird poop</a> (on a car’s hood!) to <a href="https://www.techdirt.com/2017/10/31/man-gets-37500-payout-after-field-drug-test-says-donut-crumbs-are-methamphetamines/" data-type="link" data-id="https://www.techdirt.com/2017/10/31/man-gets-37500-payout-after-field-drug-test-says-donut-crumbs-are-methamphetamines/">donut crumbs</a> to <a href="https://www.techdirt.com/2019/08/29/man-spends-three-months-jail-because-drug-dog-field-test-said-his-honey-was-methamphetamines/" data-type="link" data-id="https://www.techdirt.com/2019/08/29/man-spends-three-months-jail-because-drug-dog-field-test-said-his-honey-was-methamphetamines/">honey</a> to the <a href="https://www.techdirt.com/2021/06/03/man-sues-after-field-drug-test-says-his-daughters-ashes-are-meth-ecstasy/" data-type="link" data-id="https://www.techdirt.com/2021/06/03/man-sues-after-field-drug-test-says-his-daughters-ashes-are-meth-ecstasy/">ashes of a deceased loved one</a> as contraband, resulting in the immediate arrest of people not actually in possession of anything illegal. </p>
<p>This is just what’s been observed by those challenging these results in court during criminal trials or filing civil rights lawsuits following wrongful arrests. It’s happened often enough that even a <a href="https://www.techdirt.com/2023/04/28/cheap-field-drug-tests-are-finally-getting-called-out-by-courts-as-the-bullshit-they-are/" data-type="link" data-id="https://www.techdirt.com/2023/04/28/cheap-field-drug-tests-are-finally-getting-called-out-by-courts-as-the-bullshit-they-are/">few courts are taking notice</a>, in some cases refusing to accept plea deals predicated on nothing more than field drug tests results.</p>
<p>Data on field drug tests is difficult to obtain. Cops are in no hurry to turn this information over and the patchwork of public records laws across the nation often allows law enforcement to refuse disclosure simply by stating the results are relevant to a criminal investigation (even if the investigation has long since been closed). </p>
<p>What data can be obtained has been collected and parsed by the Quattrone Center for Fair Administration of Justice at the University of Pennsylvania. What’s long been assumed based on mainly anecdotal evidence now has at least some scientific backing: field drug tests aren’t worth what we’re paying for them, even if it is only $3/per. According to the Center’s <a href="https://s3.documentcloud.org/documents/24360980/fdt-study.pdf" data-type="link" data-id="https://s3.documentcloud.org/documents/24360980/fdt-study.pdf" target="_blank" rel="noreferrer noopener">report</a> [PDF], nearly one third of the money spent on field drug tests is misspent. (h/t <a href="https://reason.com/2024/01/09/study-estimates-roadside-drug-tests-result-in-30000-wrongful-arrests-every-year/" data-type="link" data-id="https://reason.com/2024/01/09/study-estimates-roadside-drug-tests-result-in-30000-wrongful-arrests-every-year/" target="_blank" rel="noreferrer noopener">C.J. Ciaramella at Reason</a>)</p>
<blockquote>
<p><em>Utilizing a nationwide survey of agencies, the report offers national estimates on the frequency of test usage, finding that each year approximately 773,000 drug-related arrests involve the use of presumptive tests. Using the survey data and national estimates of drug arrests, this report examines the impact of the tests on wrongful arrests, racial disparities in their use, and their subsequent impact on drug possession prosecutions and dispositions.</em></p>
<p><em>Although the true error rate of these tests remains unknown, estimates based on the imperfect data that are available suggest that <strong><span>around 30,000 arrests</span> each year involve people who do not possess illegal substances but who are nonetheless falsely implicated by color-based presumptive tests</strong></em>.</p>
</blockquote>
<p>Not great. Not even good. Not even close to good. I don’t know where it’s acceptable to rack up a false arrest rate of nearly 4%, but America shouldn’t be one of those places. I realize it’s only probable cause at the point of arrest rather than the courtroom standard of “beyond a reasonable doubt.” But 30,000 bogus arrests a year from a single cause — field drug tests — is unacceptable. </p>
<p>Here’s how that works out in the greater scheme of things, in terms of criminal justice:</p>
<blockquote>
<p><em><strong>The use of presumptive field tests in drug arrests is one of the largest, if not the<br>largest, known contributing factor to wrongful arrests and convictions in the United States.</strong></em></p>
</blockquote>
<p>It’s not just arrests. It’s also convictions. And those convictions aren’t happening in bench trials or jury trials. They’re plea agreements where people weigh their options and decide that the <em>best</em> option is to “admit” to a crime they haven’t committed rather than subject themselves to indefinite detention and the <a href="https://interrogatingjustice.org/uncategorized/fairness-in-sentencing/the-trial-tax-in-america/" data-type="link" data-id="https://interrogatingjustice.org/uncategorized/fairness-in-sentencing/the-trial-tax-in-america/">full weight of prosecutorial forces that love to punish people</a> for insisting on their innocence and/or seek to have their rights respected.</p>
<p>The whole system is stacked against defendants, beginning with the tests that cops treat as actual probable cause when they realistically should be considered nothing more than a hunch. This is the depressing reality of criminal justice in the United States when it comes to field drug tests and people falsely accused by faulty tests.</p>
<blockquote>
<p><em>In our survey, <strong>89% of prosecutors reported that guilty pleas are permitted without confirmatory testing</strong> (i.e., follow-up testing by a lab to verify that a field test “positive” result accurately detected an illegal drug).</em></p>
<p><em><strong>67% of drug labs in the U.S. report that they are not asked to review samples when there are plea agreements, and 24% do not receive samples for confirmatory testing</strong> when there are field test results available.</em></p>
<p><em>Even when labs receive samples, <strong>46% report that they will not conduct a confirmatory test if there has been a guilty plea, and 8% report that they will not retest</strong> if there has been a presumptive identification</em></p>
</blockquote>
<p>This isn’t justice. This isn’t even a polite hat tip in the direction of justice. This is railroading, aided and abetted by drug labs beholden to cops and almost as disinterested in what happens to people facing criminal charges possibly predicated on incorrect test results. On the law enforcement side, the fact is no one cares and no one cares that no one cares. Courts are the last hope, and even most of those are more than willing to clear dockets quickly and easily by signing off on any plea deal placed in front of them.</p>
<p>As the report notes, it’s not as though cops and prosecutors aren’t aware of the limitations of field drug tests. It’s that they don’t care. All tests come packaged with plenty of verbiage stating how any field test should be verified with a lab test, how dozens of legal substances can trigger false positives, and how tests can be rendered useless by exposure to UV light or elevated temperatures.</p>
<p>But none of that is legally binding. It’s not like improper use results in a voided warranty. It’s not like field drug test makers are going to stop selling to cops just because cops ignore every single instruction printed on the package. Again, this is up to the courts. And low level courts that handle a large percentage of possession cases aren’t going to increase their own workload by forcing prosecutors to verify tests or refuse plea deals based on nothing more than what a cop claimed to have observed while misusing a cheap test prone to false positives.</p>
<p>It’s not that better field drug test tech doesn’t exist. It does. Portable Raman spectrometers, which are capable of producing results comparable to lab equipment (at least according to the Scientific Working Group for the Analysis of Seized Drugs), very few law enforcement agencies are going to trade in cheap drug tests they can buy in bulk with actually accurate testing equipment than can cost up to $20,000 per device.</p>
<p>As the report states, the 30,000 wrongful arrests is an extreme undercount. Its survey managed to only find 93 law enforcement agencies willing to discuss field drug test use and only 82 of those actually provided enough information to warrant being included in this report. </p>
<p>Of the few that did respond, there’s even more bad news. This is quite the pair of sentences:</p>
<blockquote>
<p><em>Twelve agencies recently stopped using the tests due to concerns about fentanyl exposure (i.e., a belief that physical contact with fentanyl is, by itself, dangerous for the officer), and two agencies reported that the tests were an unnecessary expense as <strong>suspects were arrested and charged regardless of the test outcome</strong>.</em></p>
</blockquote>
<p>Did you get all of that? Twelve agencies have gone <a href="https://www.techdirt.com/2022/07/20/omnipresent-fentanyl-copaganda-is-turning-normal-citizens-into-fainting-goats/" data-type="link" data-id="https://www.techdirt.com/2022/07/20/omnipresent-fentanyl-copaganda-is-turning-normal-citizens-into-fainting-goats/">full fainting goat</a> and two agencies flat out admitted they arrested people whether or not the tests indicated illegal substances. And if arrest is the inevitable outcome, why blow money on field drug tests? </p>
<p>There’s nothing positive in this report, other than the recommendations it suggests, like refusing plea deals backed by nothing but field tests and ensuring all field tests are verified by lab testing. But if <a href="https://www.techdirt.com/articles/20161206/15130336213/more-prosecutors-refuse-to-accept-guilty-pleas-based-faulty-2-field-drug-tests.shtml" data-type="link" data-id="https://www.techdirt.com/articles/20161206/15130336213/more-prosecutors-refuse-to-accept-guilty-pleas-based-faulty-2-field-drug-tests.shtml">very little of this</a> is happening yet, it’s hard to believe there will be widespread adoption in the future. So, we’ll just get what we’ve been getting for years: tens of thousands of bogus arrests every year — arrests that will be touted by those performing them as indicative of their tireless service to the War on Drugs. The reality, however, will be tens of thousands of destroyed lives and violated rights by tests so questionable they should never have been considered evidence of anything.</p>

<p>
Filed Under: <a href="https://www.techdirt.com/tag/drug-tests/" rel="tag">drug tests</a>, <a href="https://www.techdirt.com/tag/evidence/" rel="tag">evidence</a>, <a href="https://www.techdirt.com/tag/false-arrest/" rel="tag">false arrest</a>, <a href="https://www.techdirt.com/tag/field-drug-tests/" rel="tag">field drug tests</a>
<br>
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA['No AI Fraud Act' Could Outlaw Parodies, Political Cartoons, and More (108 pts)]]></title>
            <link>https://reason.com/2024/01/17/ai-fraud-act-could-outlaw-parodies-political-cartoons-and-more/</link>
            <guid>39040720</guid>
            <pubDate>Thu, 18 Jan 2024 12:01:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://reason.com/2024/01/17/ai-fraud-act-could-outlaw-parodies-political-cartoons-and-more/">https://reason.com/2024/01/17/ai-fraud-act-could-outlaw-parodies-political-cartoons-and-more/</a>, See on <a href="https://news.ycombinator.com/item?id=39040720">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
							<p>Mixing new technology and new laws is always a fraught business, especially if the tech in question relates to communication. Lawmakers routinely propose bills that would sweep up all sorts of First Amendment-protected speech. We've seen a lot of this with social media, and we're starting to see it with artificial intelligence. Case in point: the No Artificial Intelligence Fake Replicas And Unauthorized Duplications (No AI FRAUD) Act. Under the auspices of protecting "<a href="https://salazar.house.gov/media/press-releases/salazar-introduces-no-ai-fraud-act">Americans' individual right to their likeness and voice</a>," the bill would restrict a range of content wide enough to ensnare parody videos, comedic impressions, political cartoons, and much more.</p> <p>The bill's sponsors, Reps. María Elvira Salazar (R-Fla.) and Madeleine Dean (D-Pa.), say they're concerned about "AI-generated fakes and forgeries," per a <a href="https://salazar.house.gov/media/press-releases/salazar-introduces-no-ai-fraud-act">press release</a>. They aim to protect people from unauthorized use of their own images and voices by defining these things as the intellectual property of each individual.</p> <p>The No AI Fraud Act cites several instances of AI being used to make it appear that celebrities created ads or art that they did not actually create. For instance, "AI technology was used to create the song titled 'Heart on My Sleeve,' emulating the voices of recording artists Drake and The Weeknd," states the bill's text. AI technology was also used "to create a false endorsement featuring Tom Hanks' face in an advertisement for a dental plan."</p> <p>But while the examples in the bill are directly related to AI, the bill's actual reach is much more expansive, targeting a wide swath of "digital depictions" or "digital voice replicas."</p> <p>Salazar and Dean say the bill balances people's "right to control the use of their identifying characteristics" with "First Amendment protections to safeguard speech and innovation." But while the measure does nod to free speech rights, it also expands the types of speech deemed legally acceptable to restrict. It could mean way more legal hassles for creators and platforms interested in exercising their First Amendment rights, and result in a chilling effect on certain sorts of comedy, commentary, and artistic expression.</p> <h2><b>An Insanely Broad Bill&nbsp;</b></h2> <p>At its core, <a href="https://reason.com/wp-content/uploads/2024/01/090C34FC92DED2E83456EB85C8E64E44.no-ai-fraud-act.pdf">the No AI Fraud Act</a> is about creating a right to sue someone who uses your likeness or voice without your permission. It states that "every individual has a property right in their own likeness and voice," and people can only use someone's "digital depiction or digital voice replica" in a "manner affecting interstate or foreign commerce" if the individual agrees (in writing) to said use. This agreement must involve a lawyer, and its terms must be governed by a collective bargaining agreement. If any of these three elements are missing, the person whose voice or likeness was used can sue for damages.</p> <p>The bit about <i>interstate or foreign commerce</i> might appear to significantly limit this bill's provisions. But basically, anything involving the internet can be deemed a matter of interstate or foreign commerce.</p> <p>So just how broad is this bill? For starters, it applies to the voices and depictions of all human beings "living or dead." And it defines <i>digital depiction</i> as any "replica, imitation, or approximation of the likeness of an individual that is created or altered in whole or part using digital technology." <em>Likeness</em> means any "actual or simulated image… regardless of the means of creation, that is readily identifiable as the individual." <i>Digital voice replica</i> is defined as any "audio rendering that is created or altered in whole or part using digital technology and is fixed in a sound recording or audiovisual work which includes replications, imitations, or approximations of an individual that the individual did not actually perform." This includes "the actual voice or a simulation of the voice of an individual, whether recorded or generated by computer, artificial intelligence, algorithm, or other digital means, technology, service, or device."</p> <p>These definitions go way beyond using AI to create a fraudulent ad endorsement or musical recording.</p> <p>They're broad enough to include reenactments in a true-crime show, a parody TikTok account, or depictions of a historical figure in a movie.</p> <p>They're broad enough to include sketch-comedy skits, political cartoons, or those Dark Brandon memes.</p> <p>They're broad enough to encompass you using your phone to record an impression of President Joe Biden and posting this online, or a cartoon like<em> South Park</em> or <em>Family Guy</em> including a depiction of a celebrity.</p> <p>And it doesn't matter if the intent is not to trick anyone. The bill says that it's no defense to inform audiences that a depiction "was unauthorized or that the individual rights owner did not participate in the creation, development, distribution, or dissemination of the unauthorized digital depiction, digital voice replica, or personalized closing service."</p> <p>What's more, it's not just the creators of off-limits content that could be sued. Potentially liable parties include anyone who "distributes, transmits, or otherwise makes available to the public a personalized closing service"; anyone who "publishes, performs, distributes, transmits, or otherwise makes available to the public a digital voice replica or digital depiction"; and anyone who "materially contributes to, directs, or otherwise facilitates any of the above" with knowledge that the individual depicted had not consented.&nbsp;This is broad enough to ensnare social media platforms, video platforms, newsletter services, web hosting services, and any entity that enables the sharing of art, entertainment, and commentary. It also applies to the makers of tools that merely allow others to create audio replicas or visual depictions, including tools like ChatGPT that allow for the creation of AI-generated images.</p> <h2><b>But…the First Amendment</b></h2> <p>Apparently aware of obvious First Amendment issues with this proposal, the lawmakers inserted a section saying that "First Amendment protections shall constitute a defense to an alleged violation." But this isn't terribly reassuring, considering that lawmakers are simultaneously trying to expand the categories of speech unprotected by the First Amendment.</p> <p>At present, intellectual property—<a href="https://www.suffolk.edu/law/academics-clinics/what-can-i-study/intellectual-property/intellectual-property-law-basics-certificate/explore-the-four-areas-of-ip-law#:~:text=Intellectual%20Property%20Law%20includes%20patents,ways%20they%20are%20very%20different.">such as copyrighted works and trade secrets</a>—falls under free speech exceptions, meaning restrictions are permitted. By defining one's voice and likeness as intellectual property, the lawmakers are trying to shoehorn depictions of someone else's voice or likeness into the category of unprotected speech.</p> <p>Even as intellectual property, voice replicas and digital depictions of others wouldn't <em>always</em> be prohibited. Just as the doctrine of <a href="https://reason.com/tag/fair-use/">fair use</a> provides some leeway with copyright protections, this bill defines circumstances under which replicas and depictions would be OK, such as when "the public interest in access to the use" outweighs "the intellectual property interest in the voice or likeness."</p> <p>But even if people being sued ultimately prevail on First Amendment grounds, it still means they have to go to court, with all the time and expense that entails. Even for those with the resources to do this, it would be a big headache. And many people do not have the resources to do this, which means that even when the First Amendment is on their side, they're still likely to lose, to cave (by taking down whatever content is being challenged), or to avoid making said content in the first place.</p> <p>You can see how this might seriously chill speech that is protected. People may be afraid to even create art, comedy, or commentary that could get challenged.&nbsp;And tech companies could be afraid to allow such content on their platforms.</p> <p>If this measure becomes law, I would expect to see a lot more takedowns of anything that might come close to being a violation, be it a clip of a <i>Saturday Night Live</i> skit lampooning Trump, a comedic impression of Taylor Swift, or a weird ChatGPT-generated image of Ayn Rand. I would also expect to see more platforms institute blanket bans on parody accounts and the like.</p> <p>The bill stipulates that imitators aren't liable "if the harm caused by such conduct is negligible." But emotional distress counts as harm, which makes this a pretty subjective designation.</p> <p>And some categories of content—such as "sexually explicit" content and "intimate images"—are declared per se harmful (which means there could be no arguing that they did not actually harm the party being depicted and therefore were not actually violations). Supporters will likely argue that this targets things like deepfake porn (where AI is used to make it appear someone appeared in a porn video when they did not). But the language of the law is broad enough to potentially ensnare a wide range of content, including erotic art, commentary that conjures two political figures intimately involved (like those—yes, often silly and sophomoric—images of Trump and Putin in bed together), and comedic/parodic depictions of sexual encounters.</p> <p>There's no doubt that AI is opening up new parameters for creative expression <em>and</em> for deception, raising new questions and presenting new issues that society will have to deal with. But we shouldn't let lawmakers use these hiccups to justify a broad new incursion on free speech rights.</p>						</div></div>]]></description>
        </item>
    </channel>
</rss>