<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 13 Jul 2024 09:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Use a Work Journal to Recover Focus Faster and Clarify Your Thoughts (415 pts)]]></title>
            <link>https://fev.al/posts/work-journal/</link>
            <guid>40950584</guid>
            <pubDate>Sat, 13 Jul 2024 00:05:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fev.al/posts/work-journal/">https://fev.al/posts/work-journal/</a>, See on <a href="https://news.ycombinator.com/item?id=40950584">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
    
    <p>You’re working on the most complex problem in computer science: fixing permissions on a deployment pipeline. It’s been 4 days you started on that simple task already. Your manager explained to you in no uncertain terms that your performance on the subject is well below the expectations she has from a midterm intern. Your colleagues stay as far away as possible from you to avoid getting tainted by your shameful failure. 4 days of sleepless afternoons, seeing that freaking status turning to “build failed” everytime, bringing you to tears. The weather is shit, rain taping on the window of your overpriced basement suite, reflecting the state of your soul. You never felt so alone. Even your partner left you, you loser!</p>

<p>But this time you got it. This time you have a strategy beyond clicking on “retry failed steps” again and again. You finally read the logs, and you have an idea. You think you finely grasped what might be wrong. It’s a long shot: you’re gonna clear the credential cache, get an elevation you’re missing, force a permission sync, reset the service connection to the cluster, then downgrade the stupid auth library you’re using to a version that was hacked 6y ago but still works, setup everything again, then rollback. Your brain is making the connections, you got it, you’re better than that. You’re sorting through 23 different documentation tabs, waiting for the elevation to succeed, summoning all precious focus you got.</p>

<p>A red bubble shows up on IM. Conditioned by years of desperately reading your texts as soon as they arrived to create yourself what passes for a social life, your hand doesn’t even consults the sentient part of your cortex, and just moves to the little icon. <em>click</em>. It’s Mitch, your PM. He’s asking the url for a doc he wrote, and complains that it’s so complex to find doc in this organization.</p>

<p>This is a trap meant to get your focus out. Not this time. <strong>You’re better than that</strong>. You ignore his message, look at the elevation command, trigger it. Copy the id of the request that you need to preciously keep to finish the elevation to your clipboard.</p>

<p>4 minutes later you get a call request from your manager. You answer.</p>

<blockquote>
  <p>Hey, Mitch is saying he needs a doc urgently and you’re not answering his IM. Can you get to it?</p>
</blockquote>

<p><em>poof</em></p>

<p>Where the freak was I?</p>

<p><img src="https://fev.al/img/2024/focus.png" alt="poof"> 
<em>Credit: <a href="https://monkeyuser.com/">monkeyuser.com</a>. notice how much better it conveys my story.</em></p>

<p>Like everyone, I sometimes struggle to maintain focus. This was particularly true when I was a manager switching context all day long, this is also true as a  dev working on three antagonistic projects, including one that’s very consultory in nature, and working with processes that sometimes take hours to complete.</p>

<p>The typical situation is that I start something, switch to something else, get into a meeting, forget the very essence of what I was doing. Turn on autopilot, read all my emails, all my IMs. Then it’s 5PM, I’m exhausted, and I realize I’m at the same stage I was at 8AM, tell myself I should really achieve something that day, and open HN.</p>

<p>It was so bad at the beginning of this year that I seriously started wondering if I had ADHD<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup>. I started working on something badly documented. As in: there’s no documentation, the people who built that thing are halfway across the world and they don’t even really work for the company anymore. It wasn’t even just interruption, but also procrastination. I felt so frustrated by the situation that I started writing that in my daily notes on Obsidian.</p>

<blockquote>
  <p>I asked Berna how to turn on super-compression 5000, and she’s not answering again! I tried it with the <code>--yo-compress-shit really-well-like-5000-or-something</code> and it didn’t work. It still spits me that <code>yo is not proper English, be civilized</code> error. What the hell can I do about that.</p>
</blockquote>

<p>Mitch called me to ask for that doc again, which I gave him, not without mentioning that little “star” icon in the url bar. And then got back at it.</p>

<p>And boy oh boy did that help! I just re-read what I was doing, and boom! I was back in.</p>

<p>I started listing all the commands I was running, and their results. Writing down my train of thoughts, the things I was doing and what I wanted to do next. And I have been doing that for the past 3-4 months. I feel like I invented something new. It helps me think more clearly, and restore the context so, so much faster when I switch between things. I’m almost looking forward to an interruption to get a chance to marvel again at my genius!</p>

<p>Except that it’s nothing new, right? “Writing helps you organize your thoughts more clearly”: everyone and their grandmother know that! Writing a plan, writing a diary? People keep listing how transformative that’s been for them. I’m not proposing a new framework. I’m just saying - every movie has a scientist recording themselves on one of these shitty little cassette recorders. They might be onto something. Write notes of what you’re doing and what you’re thinking. When you drop the pen and get back at it, read the last bit. That’s it.</p>

<p>I’ve just been too lazy to ever do it. Or not necessarily lazy, but more: I didn’t trust the tool enough to think it was a good use of my time, and instead just mash on the keyboard till it works. After all, I’m writing pages of text, of which I will never read more than a fraction. But that’s not the point. The point is structure, and the point is caching.</p>

<p>I guess that’s kind of it: if you’re having trouble switching between things or getting focused, try writing what you’re doing, and read the last couple sentences when you resume. Maybe it will help you. Maybe it won’t. Or maybe I’m an idiot who needs crutches. But hey, who knows!</p>

<h2 id="notes">notes</h2>



  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Crafting Interpreters (209 pts)]]></title>
            <link>https://craftinginterpreters.com/</link>
            <guid>40950235</guid>
            <pubDate>Fri, 12 Jul 2024 23:00:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://craftinginterpreters.com/">https://craftinginterpreters.com/</a>, See on <a href="https://news.ycombinator.com/item?id=40950235">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<p><em>Crafting Interpreters</em> contains everything you need to implement a
full-featured, efficient scripting language. You’ll learn both high-level
concepts around parsing and semantics and gritty details like bytecode
representation and garbage collection. Your brain will light up with new ideas,
and your hands will get dirty and calloused. It’s a blast.</p>

<p>Starting from <code>main()</code>, you build a language that features rich
syntax, dynamic typing, garbage collection, lexical scope, first-class
functions, closures, classes, and inheritance. All packed into a few thousand
lines of clean, fast code that you thoroughly understand because you write each
one yourself.</p>

<p>The book is available in four delectable formats:</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA["GitHub" Is Starting to Feel Like Legacy Software (108 pts)]]></title>
            <link>https://www.mistys-internet.website/blog/blog/2024/07/12/github-is-starting-to-feel-like-legacy-software/</link>
            <guid>40949034</guid>
            <pubDate>Fri, 12 Jul 2024 20:19:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mistys-internet.website/blog/blog/2024/07/12/github-is-starting-to-feel-like-legacy-software/">https://www.mistys-internet.website/blog/blog/2024/07/12/github-is-starting-to-feel-like-legacy-software/</a>, See on <a href="https://news.ycombinator.com/item?id=40949034">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I’ve used a lot of tools over the years, which means I’ve seen a lot of tools hit a plateau. That’s not always a problem; sometimes something is just “done” and won’t need any changes. Often, though, it’s a sign of what’s coming. Every now and then, something will pull back out of it and start improving again, but it’s often an early sign of long-term decline. I can’t always tell if something’s just coasting along or if it’s actually started to get worse; it’s easy to be the boiling frog. That changes for me when something that <em>really</em> matters to me breaks.</p>

<p>To me, one of GitHub’s killer power user features is its <code>blame</code> view. <code>git blame</code> on the commandline is useful but hard to read; it’s not the interface I reach for every day. GitHub’s web UI is not only convenient, but the ease by which I can click through to older versions of the blame view on a line by line basis is uniquely powerful. It’s one of those features that anchors me to a product: I stopped using offline graphical git clients because it was just that much nicer.</p>

<p>The other day though, I tried to use the blame view on a large file and ran into an issue I don’t remember seeing before: I just <em>couldn’t find</em> the line of code I was searching for. I threw various keywords from that line into the browser’s command+F search box, and nothing came up. I was stumped until a moment later, while I was idly scrolling the page while doing the search again, and it finally found the line I was looking for. I realized what must have happened.</p>

<p>I’d heard rumblings that GitHub’s in the middle of shipping a frontend rewrite in React, and I realized this must be it. The problem wasn’t that the line I wanted wasn’t on the page—it’s that the whole document wasn’t being rendered at once, so my browser’s builtin search bar just <em>couldn’t find it</em>. On a hunch, I tried disabling JavaScript entirely in the browser, and suddenly it started working again. GitHub is <em>able</em> to send a fully server-side rendered version of the page, which actually works like it should, but doesn’t do so unless JavaScript is completely unavailable.</p>

<p>I’m hardly anti-JavaScript, and I’m not anti-React either. Any tool’s perfectly fine when used in the right place. The problem: this <em>isn’t the right place</em>, and what is to me personally a key feature suddenly doesn’t work right all the time anymore. This isn’t the only GitHub feature that’s felt subtly worse in the past few years—the once-industry-leading status page no longer reports minor availability issues in an even vaguely timely manner; Actions runs randomly drop network connections to GitHub’s own APIs; hitting the merge button sometimes scrolls the page to the wrong position—but this is the first moment where it really hit me that GitHub’s probably not going to get better again from here.</p>

<p>The corporate branding, the new “AI-powered developer platform” slogan, makes it clear that what I think of as “GitHub”—the traditional website, what are to me the core features—simply isn’t Microsoft’s priority at this point in time. I know many talented people at GitHub who care, but the company’s priorities just don’t seem to value what I value about the service. This isn’t an anti-AI statement so much as a recognition that the tool I still need to use every day is past its prime. Copilot isn’t navigating the website for me, replacing my need to the website as it exists today. I’ve had tools hit this phase of decline and turn it around, but I’m not optimistic. It’s still plenty usable now, and probably will be for some years to come, but I’ll want to know what other options I have <em>now</em> rather than when things get worse than this.</p>

<p>And in the meantime, well… I still need to use GitHub everyday, but maybe it’s time to start exploring new platforms—and find a good local <code>blame</code> tool that works as well as the GitHub web interface used to. (Got a fave? Send it to me at <a href="https://digipres.club/@misty">misty@digipres.club</a> / <a href="https://bsky.app/profile/cdrom.ca">@cdrom.ca</a>. Please!)</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Responsive bar charts in HTML and CSS (103 pts)]]></title>
            <link>https://9elements.com/blog/responsive-bar-charts-in-html-and-css/</link>
            <guid>40949021</guid>
            <pubDate>Fri, 12 Jul 2024 20:17:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://9elements.com/blog/responsive-bar-charts-in-html-and-css/">https://9elements.com/blog/responsive-bar-charts-in-html-and-css/</a>, See on <a href="https://news.ycombinator.com/item?id=40949021">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><strong>Building flexible data visualizations for international sites</strong></p><p>For our international clients, we have created dynamic charts and data visualizations for the web. Charts typically render shapes like lines and paths, rectangles and circles. They contain text for titles, axis labels, numerical values and legends.</p><p>SVG is the good fit for this purpose. It embeds directly into HTML and pairs well with CSS. However, for dynamic data visualizations on the web, SVG poses a challenge.</p><h2 id="Responsive-charts-and-the-problems-of-SVG">Responsive charts and the problems of SVG</h2><p>The websites we build feature responsive layouts and fluid typography. We employ CSS Flexbox and Grid together with media and container queries to fit in the content. There is not one single fixed presentation, but many possible presentations depending on the content and the reading environment.</p><p>In contrast, SVG does not have layout techniques like Flexbox, Grid or even <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_flow_layout">Normal Flow</a>. In SVG, all shapes are absolutely positioned. Text does not wrap automatically. The shapes and text need to be laid out manually by the code that generates the SVG.</p><p>SVG does scale continuously, as the name says – but for charts on the web, we usually do not want that. A small chart should not look like a downscaled big chart. Text would become unreadable, shapes would become tiny pixel mush – even with techniques that <a href="https://developer.mozilla.org/en-US/docs/Web/SVG/Attribute/vector-effect#non-scaling-stroke">prevent the scaling of some graphical features</a>.</p><p>For charts on the web, we want quantitative and qualitative responsive scaling. A small and a large chart should be designed and laid out differently. A small chart should focus on clear, distinguishable marks that represent the data. A large chart should take advantage of the screen estate to show more items and details as well as provide context.</p><p>For example, a line chart with multiple lines may switch to small multiples on smaller viewports or containers.</p><figure><picture><source type="image/avif" srcset="https://9elements.com/images/ctfl/6GQrENkrcOjoCMIY8TZIQO-352w-embedded.avif 352w, https://9elements.com/images/ctfl/6GQrENkrcOjoCMIY8TZIQO-704w-embedded.avif 704w, https://9elements.com/images/ctfl/6GQrENkrcOjoCMIY8TZIQO-1408w-embedded.avif 1408w" sizes="(min-width: 176em) 176rem, 100vw"><source type="image/jpeg" srcset="https://9elements.com/images/ctfl/6GQrENkrcOjoCMIY8TZIQO-352w-embedded.jpeg 352w, https://9elements.com/images/ctfl/6GQrENkrcOjoCMIY8TZIQO-704w-embedded.jpeg 704w, https://9elements.com/images/ctfl/6GQrENkrcOjoCMIY8TZIQO-1408w-embedded.jpeg 1408w" sizes="(min-width: 176em) 176rem, 100vw"><img alt="Line chart with six lines representing six world regions (Western Pacific, Europe, Americas, South-East Asia, Eastern Mediterranean, Africa). Lines are colored differently and sometimes overlap." loading="lazy" decoding="async" src="https://9elements.com/images/ctfl/6GQrENkrcOjoCMIY8TZIQO-352w-embedded.jpeg" width="1408" height="1051"></picture></figure><figure><picture><source type="image/avif" srcset="https://9elements.com/images/ctfl/1IqU5V32yN34rKj2RNREFi-352w-embedded.avif 352w, https://9elements.com/images/ctfl/1IqU5V32yN34rKj2RNREFi-704w-embedded.avif 704w" sizes="(min-width: 176em) 176rem, 100vw"><source type="image/jpeg" srcset="https://9elements.com/images/ctfl/1IqU5V32yN34rKj2RNREFi-352w-embedded.jpeg 352w, https://9elements.com/images/ctfl/1IqU5V32yN34rKj2RNREFi-704w-embedded.jpeg 704w" sizes="(min-width: 176em) 176rem, 100vw"><img alt="Two-column grid of six small line charts, one line chart for each world region. All lines are colored blue. The y axes are aligned so the lines are comparable." loading="lazy" decoding="async" src="https://9elements.com/images/ctfl/1IqU5V32yN34rKj2RNREFi-352w-embedded.jpeg" width="704" height="960"></picture></figure><p>We have typically implemented this responsiveness with client-side JavaScript logic. JavaScript is able to read the container size and measure text in order to compute all shape coordinates and sizes. This often involves decollision with <a href="https://d3js.org/d3-force">force simulations</a>.</p><p>This approach has severe disadvantages. The cycle of forcing the browser to compute the style, reading sizes and setting positions leads to <a href="https://web.dev/articles/avoid-large-complex-layouts-and-layout-thrashing">layout thrashing</a> and slows down the chart rendering.</p><p>When the container size changes, for example due to a browser resize or orientation change, the JavaScript needs to compute all SVG positions and sizes from scratch. Assuming this takes 50-100ms per chart, a page with 20 charts freezes the browser for 1-2 seconds.</p><h2 id="HTML-CSS-and-SVG-hybrid">HTML, CSS and SVG hybrid</h2><p>Horizontal bar charts are simple yet effective, intuitive and accessible visualizations. They are versatile regarding the bar design, labeling, value placement and axes. And they can be highly flexible regarding the container size.</p><p>We have a pretty solid implementation of a responsive bar chart. In narrow containers, the row label is shown on top of the bar. In wide containers, it is shown next to the bar.</p><figure><picture><source type="image/avif" srcset="https://9elements.com/images/ctfl/5Sulbp7D4VfW4YXX4s1hKD-352w-embedded.avif 352w, https://9elements.com/images/ctfl/5Sulbp7D4VfW4YXX4s1hKD-704w-embedded.avif 704w" sizes="(min-width: 176em) 176rem, 100vw"><source type="image/jpeg" srcset="https://9elements.com/images/ctfl/5Sulbp7D4VfW4YXX4s1hKD-352w-embedded.jpeg 352w, https://9elements.com/images/ctfl/5Sulbp7D4VfW4YXX4s1hKD-704w-embedded.jpeg 704w" sizes="(min-width: 176em) 176rem, 100vw"><img alt="Horizontal bar chart for four countries. The country names are placed on top of the bars. The vertical x axis lines span the whole height. Next to each bar, there is a label with the value for the country." loading="lazy" decoding="async" src="https://9elements.com/images/ctfl/5Sulbp7D4VfW4YXX4s1hKD-352w-embedded.jpeg" width="704" height="357"></picture></figure><figure><picture><source type="image/avif" srcset="https://9elements.com/images/ctfl/22MPkZRNittq3duJ5JAx2X-352w-embedded.avif 352w, https://9elements.com/images/ctfl/22MPkZRNittq3duJ5JAx2X-704w-embedded.avif 704w, https://9elements.com/images/ctfl/22MPkZRNittq3duJ5JAx2X-1408w-embedded.avif 1408w" sizes="(min-width: 176em) 176rem, 100vw"><source type="image/jpeg" srcset="https://9elements.com/images/ctfl/22MPkZRNittq3duJ5JAx2X-352w-embedded.jpeg 352w, https://9elements.com/images/ctfl/22MPkZRNittq3duJ5JAx2X-704w-embedded.jpeg 704w, https://9elements.com/images/ctfl/22MPkZRNittq3duJ5JAx2X-1408w-embedded.jpeg 1408w" sizes="(min-width: 176em) 176rem, 100vw"><img alt="Horizontal bar chart for four countries. The country names are placed in a column on the left side. The bars as well as the x axis ticks and labels are placed in a column on the right side. Next to each bar, there is a label with the value for the country." loading="lazy" decoding="async" src="https://9elements.com/images/ctfl/22MPkZRNittq3duJ5JAx2X-352w-embedded.jpeg" width="1408" height="359"></picture></figure><p>This chart is a hybrid of HTML, CSS and SVG. We wanted to use essential CSS layout methods like Flexbox instead of re-implementing layout algorithms in JavaScript. However, the synchronization with the SVG parts is still slow, complex client-side JavaScript code.</p><h2 id="lessstronggreaterBar-chart-in-plain-HTML-andamp-CSSlessstronggreater"><strong>Bar chart in plain HTML &amp; CSS</strong></h2><p>We were wondering: Can we achieve this with HTML and CSS alone, preferably without SVG and with less JavaScript logic? We fiddled around, but never finished this idea.</p><p>Then we saw the <a href="https://2023.stateofjs.com/en-US/features/#syntax_features">beautiful responsive bar charts of State of JS</a>, made with HTML &amp; CSS only. On narrow viewports, they use a two-column grid:</p><figure><picture><source type="image/avif" srcset="https://9elements.com/images/ctfl/7xQa0or3SZ1lXzbFdIfGs8-352w-embedded.avif 352w, https://9elements.com/images/ctfl/7xQa0or3SZ1lXzbFdIfGs8-704w-embedded.avif 704w" sizes="(min-width: 176em) 176rem, 100vw"><source type="image/jpeg" srcset="https://9elements.com/images/ctfl/7xQa0or3SZ1lXzbFdIfGs8-352w-embedded.jpeg 352w, https://9elements.com/images/ctfl/7xQa0or3SZ1lXzbFdIfGs8-704w-embedded.jpeg 704w" sizes="(min-width: 176em) 176rem, 100vw"><img alt="Bar chart from &quot;State of JS&quot; showing how many people have used a certain JavaScript features. X axis values on the top, lines spanning the whole chart. Three rows, one for each JavaScript feature. Feature label and number of users on top of the bar." loading="lazy" decoding="async" src="https://9elements.com/images/ctfl/7xQa0or3SZ1lXzbFdIfGs8-352w-embedded.jpeg" width="704" height="506"></picture></figure><p>On wide viewports, this is a three-column grid with subgrids that inherit the column setup:</p><figure><picture><source type="image/avif" srcset="https://9elements.com/images/ctfl/3Psc17pbAY5Zyd8WZZ61dx-352w-embedded.avif 352w, https://9elements.com/images/ctfl/3Psc17pbAY5Zyd8WZZ61dx-704w-embedded.avif 704w, https://9elements.com/images/ctfl/3Psc17pbAY5Zyd8WZZ61dx-1408w-embedded.avif 1408w" sizes="(min-width: 176em) 176rem, 100vw"><source type="image/jpeg" srcset="https://9elements.com/images/ctfl/3Psc17pbAY5Zyd8WZZ61dx-352w-embedded.jpeg 352w, https://9elements.com/images/ctfl/3Psc17pbAY5Zyd8WZZ61dx-704w-embedded.jpeg 704w, https://9elements.com/images/ctfl/3Psc17pbAY5Zyd8WZZ61dx-1408w-embedded.jpeg 1408w" sizes="(min-width: 176em) 176rem, 100vw"><img alt="Bar chart from &quot;State of JS&quot; showing usage percentage of JavaScript features. X axis values on the top and bottom, lines spanning the whole height. Left column contains feature name, middle column the bar, right column the absolute user number." loading="lazy" decoding="async" src="https://9elements.com/images/ctfl/3Psc17pbAY5Zyd8WZZ61dx-352w-embedded.jpeg" width="1408" height="588"></picture></figure><p>These well-made charts encouraged us to try to migrate our bar charts to HTML &amp; CSS.</p><h2 id="lessstronggreaterGrid-setuplessstronggreater"><strong>Grid setup</strong></h2><p>For a start, we <a href="https://codepen.io/molily/pen/gOJVQgB?editors=1100">rebuild the basic structure</a>:</p><figure><p data-height="300" data-default-tab="html,result" data-slug-hash="gOJVQgB" data-pen-title="Responsive bar chart with ticks" data-preview="true" data-editable="true" data-user="molily"><span>See the Pen <a href="https://codepen.io/molily/pen/gOJVQgB">Responsive bar chart with ticks</a> by molily (<a href="https://codepen.io/molily">@molily</a>) on <a href="https://codepen.io/">CodePen</a>.</span></p></figure><p>In the narrow version, the each row (<code>li</code> element) is a two-column grid:</p><pre><span>css</span><code><span>display</span><span>:</span> grid<span>;</span>
<span>grid-template-columns</span><span>:</span> <span>minmax</span><span>(</span>0<span>,</span> 1fr<span>)</span> min-content<span>;</span>
<span>grid-template-areas</span><span>:</span>
  <span>"dimension value"</span>
  <span>"bar bar"</span><span>;</span>
<span>position</span><span>:</span> relative<span>;</span></code></pre><p>In the wide version, the wrapper (<code>ol</code> element) is a three-column grid:</p><pre><span>css</span><code><span>display</span><span>:</span> grid<span>;</span>
<span>grid-template-areas</span><span>:</span> <span>"dimension bar value"</span><span>;</span>
<span>grid-template-columns</span><span>:</span> <span>fit-content</span><span>(</span>10rem<span>)</span> 1fr min-content<span>;</span></code></pre><p>The row (<code>li</code>) is a subgrid that spans all columns:</p><pre><span>css</span><code><span>display</span><span>:</span> grid<span>;</span>
<span>grid-template-columns</span><span>:</span> subgrid<span>;</span>
<span>grid-template-areas</span><span>:</span> none<span>;</span>
<span>grid-column</span><span>:</span> 1 / -1<span>;</span></code></pre><h2 id="lessstronggreaterReal-world-requirementslessstronggreater"><strong>Real-world requirements</strong></h2><p>Our real bar chart, however, is much more complex and has the following requirements:</p><ul><li><strong>Internationalization with bidirectional text</strong>: We're building charts for sites in six languages and two text directions: Left-to-right (LTR, like English and Russian) and right-to-left (RTL, like Arabic and Hebrew).</li><li><strong>Positive and negative values</strong>. Bars grow to both sides.</li><li><strong>Row labels</strong> may have an arbitrary length and should wrap and align nicely.</li><li><strong>Value labels</strong> should be positioned at the end of the bars, not inside them or in a separate column.</li><li><strong>Do not repeat the axis tick lines</strong> for each row if it's avoidable.</li></ul><p>This is the solution we came up with:</p><p><strong></strong><a href="https://codepen.io/molily/pen/JjqgxVR?editors=1100"><strong>Responsive bar chart in HTML &amp; CSS</strong></a></p><p><i>This is version 2 which implements essential feedback from </i><a href="https://vesa.piittinen.name/"><i>Vesa Piitiinen</i></a><i>.</i></p><figure><p data-height="300" data-default-tab="html,result" data-slug-hash="JjqgxVR" data-pen-title="Bar chart version 2" data-preview="true" data-editable="true" data-user="molily"><span>See the Pen <a href="https://codepen.io/molily/pen/JjqgxVR">Bar chart</a> by molily (<a href="https://codepen.io/molily">@molily</a>) on <a href="https://codepen.io/">CodePen</a>.</span></p></figure><p>Let's dive into the implementation.</p><h3><strong>Responsive grid setup</strong></h3><p>The HTML structure looks like this:</p><pre><span>html</span><code><span><span><span>&lt;</span>div</span> <span>class</span><span><span>=</span><span>"</span>bar-chart<span>"</span></span><span>&gt;</span></span>
  <span><span><span>&lt;</span>div</span> <span>class</span><span><span>=</span><span>"</span>ticks<span>"</span></span> <span>aria-hidden</span><span><span>=</span><span>"</span>true<span>"</span></span><span>&gt;</span></span>
    <span><span><span>&lt;</span>div</span> <span>class</span><span><span>=</span><span>"</span>tick<span>"</span></span> <span><span>style</span><span><span>=</span><span>"</span><span><span>inset-inline-start:</span> <span>{</span>percent<span>}</span>%</span><span>"</span></span></span><span>&gt;</span></span>{tick value}<span><span><span>&lt;/</span>div</span><span>&gt;</span></span>
    <span>&lt;!-- … more ticks … --&gt;</span>
  <span><span><span>&lt;/</span>div</span><span>&gt;</span></span>
  <span><span><span>&lt;</span>ol</span><span>&gt;</span></span>
    <span><span><span>&lt;</span>li</span><span>&gt;</span></span>
      <span><span><span>&lt;</span>div</span> <span>class</span><span><span>=</span><span>"</span>dimension<span>"</span></span><span>&gt;</span></span>
        <span><span><span>&lt;</span>span</span> <span>class</span><span><span>=</span><span>"</span>dimension-label<span>"</span></span><span>&gt;</span></span>{dimension label}<span><span><span>&lt;/</span>span</span><span>&gt;</span></span>
      <span><span><span>&lt;/</span>div</span><span>&gt;</span></span>
      <span><span><span>&lt;</span>div</span> <span>class</span><span><span>=</span><span>"</span>bar<span>"</span></span> <span><span>style</span><span><span>=</span><span>"</span><span><span>margin-inline-start:</span> <span>{</span>bar start<span>}</span>%<span>;</span> <span>width:</span> <span>{</span>bar width<span>}</span>%</span><span>"</span></span></span><span>&gt;</span></span>
        <span><span><span>&lt;</span>span</span> <span>class</span><span><span>=</span><span>"</span>bar-label<span>"</span></span><span>&gt;</span></span>{ bar label }<span><span><span>&lt;/</span>span</span><span>&gt;</span></span>
      <span><span><span>&lt;/</span>div</span><span>&gt;</span></span>
      <span><span><span>&lt;</span>div</span> <span>class</span><span><span>=</span><span>"</span>value<span>"</span></span> <span>aria-hidden</span><span><span>=</span><span>"</span>true<span>"</span></span><span>&gt;</span></span>{ bar label again }<span><span><span>&lt;/</span>div</span><span>&gt;</span></span>
    <span><span><span>&lt;/</span>li</span><span>&gt;</span></span>
    <span>&lt;!-- Bars with negative values require a class is-negative: --&gt;</span>
    <span><span><span>&lt;</span>li</span> <span>class</span><span><span>=</span><span>"</span>is-negative<span>"</span></span><span>&gt;</span></span>
      <span><span><span>&lt;</span>div</span> <span>class</span><span><span>=</span><span>"</span>dimension<span>"</span></span><span>&gt;</span></span>
        <span><span><span>&lt;</span>span</span> <span>class</span><span><span>=</span><span>"</span>dimension-label<span>"</span></span><span>&gt;</span></span>{dimension label}<span><span><span>&lt;/</span>span</span><span>&gt;</span></span>
      <span><span><span>&lt;/</span>div</span><span>&gt;</span></span>
      <span>&lt;!-- And the value need to placed before the bar: --&gt;</span>
      <span><span><span>&lt;</span>div</span> <span>class</span><span><span>=</span><span>"</span>value<span>"</span></span> <span>aria-hidden</span><span><span>=</span><span>"</span>true<span>"</span></span><span>&gt;</span></span>{bar label again}<span><span><span>&lt;/</span>div</span><span>&gt;</span></span>
      <span><span><span>&lt;</span>div</span> <span>class</span><span><span>=</span><span>"</span>bar<span>"</span></span> <span><span>style</span><span><span>=</span><span>"</span><span><span>margin-inline-start:</span> <span>{</span>bar start<span>}</span>%<span>;</span> <span>width:</span> <span>{</span>bar width<span>}</span>%</span><span>"</span></span></span><span>&gt;</span></span>
        <span><span><span>&lt;</span>span</span> <span>class</span><span><span>=</span><span>"</span>bar-label<span>"</span></span><span>&gt;</span></span>{bar label}<span><span><span>&lt;/</span>span</span><span>&gt;</span></span>
      <span><span><span>&lt;/</span>div</span><span>&gt;</span></span>
    <span><span><span>&lt;/</span>li</span><span>&gt;</span></span>
    <span>&lt;!-- … more li elements … --&gt;</span>
  <span><span><span>&lt;/</span>ol</span><span>&gt;</span></span>
<span><span><span>&lt;/</span>div</span><span>&gt;</span></span></code></pre><p>In the narrow version, the <code>.bar-chart</code> wrapper is a three-column grid:</p><pre><span>css</span><code><span>display</span><span>:</span> grid<span>;</span>
<span>grid-template-columns</span><span>:</span> min-content 1fr min-content<span>;</span>
<span>grid-template-areas</span><span>:</span>
  <span>"dimension dimension dimension"</span>
  <span>"valuePaddingStart bar valuePaddingEnd"</span><span>;</span></code></pre><p>The <code>ol</code> element and <code>li</code> elements are subgrids:</p><pre><span>css</span><code><span>display</span><span>:</span> grid<span>;</span>
<span>grid-column</span><span>:</span> 1 / -1<span>;</span>
<span>grid-template-columns</span><span>:</span> subgrid<span>;</span></code></pre><p>In the wide version, the wrapper becomes a four-column grid:</p><pre><span>css</span><code><span>grid-template-areas</span><span>:</span> <span>"dimension valuePaddingStart bar valuePaddingEnd"</span><span>;</span>
<span>grid-template-columns</span><span>:</span> <span>fit-content</span><span>(</span>10rem<span>)</span> min-content 1fr min-content<span>;</span>
</code></pre><p>Each row remains a subgrid.</p><h2 id="lessstronggreaterBidirectional-textlessstronggreater"><strong>Bidirectional text</strong></h2><p>Internationalization is where HTML and CSS shine compared to SVG.</p><p>Our JavaScript code that generates SVG charts is full or <code>if (isLTR) {…} else {…}</code> conditionals. In SVG, the origin of the coordinate system is always top left. X coordinates need to be calculated using those LTR/RTL switches.</p><p>In HTML and CSS, we can simply use <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_logical_properties_and_values">logical properties</a> like <code>inset-inline-start/-end</code>, <code>margin-inline-start/-end</code> as well as <code>padding-inline-start/-end</code> to solve most left-to-right vs. right-to-left differences. When laying out the boxes in CSS, we can work with the text direction.</p><p>For example, each bar is a Flexbox container with the value label nested inside. Then the label is positioned next to the bar: For positive values, we add a box with <code>::before</code> plus <code>content: ''</code> with a <code>padding-inline-start</code> of 100%. For negative values, we add a box with <code>::after</code> plus <code>content: ''</code> with a <code>padding-inline-end: 100%</code>. These boxes push the label out of the bar so it sits right next to it.</p><p>We still need to handle positive and negative values differently, but by using Flexbox, logical properties and the current text direction, we don't need to handle left-to-right and right-to-left differently.</p><p>The bar labels are <i>also</i> rendered into the columns named <code>valuePaddingStart</code> and <code>valuePaddingEnd</code>. These invisible placeholders ensure the columns have the correct width to accommodate the absolutely positioned value labels. So the labels appear twice in the DOM. The placeholders have <code>aria-hidden="true"</code> and <code>visibility: hidden</code> though.</p><h2 id="lessstronggreaterTick-lines-spanning-the-full-heightlessstronggreater"><strong>Tick lines spanning the full height</strong></h2><p>Our goal to put the axis tick lines in the DOM only once instead of repeating them for each row complicates the grid. The challenge is to constrain the tick lines in the bar column horizontally, but let them span the whole grid vertically.</p><p>This is possible with <code>grid-row: 1 / -1</code> given the grid has <strong>explicit rows</strong>. It does not work with an arbitrary number of implicitly-created rows.</p><p>So we defined an outer grid that <strong>has two fixed rows</strong>. The ticks are then positioned in the first row, spanning two rows.</p><pre><span>css</span><code><span>.ticks</span> <span>{</span>
  <span>grid-column</span><span>:</span> bar<span>;</span>
  <span>grid-row</span><span>:</span> 1 / span 2<span>;</span>
<span>}</span></code></pre><p>The list of bars is then positioned in the second row and spans all columns of the parent grid. It creates a subgrid that inherits the grid configuration from the parent grid.</p><pre><span>css</span><code><span>ol</span> <span>{</span>
  <span>display</span><span>:</span> grid<span>;</span>
  <span>grid-row</span><span>:</span> 2<span>;</span>
  <span>grid-column</span><span>:</span> 1 / -1<span>;</span>
  <span>grid-template-columns</span><span>:</span> subgrid<span>;</span>
<span>}</span></code></pre><p>The subgrid may then create an arbitrary number of implicit rows. It remains nested in the second row of the outer grid.</p><p><a href="https://codepen.io/molily/pen/wvbVNbY?editors=1100">Minimal example on CodePen</a>:</p><figure><p data-height="300" data-default-tab="html,result" data-slug-hash="wvbVNbY" data-pen-title="Grid: Span whole grid" data-preview="true" data-editable="true" data-user="molily"><span>See the Pen <a href="https://codepen.io/molily/pen/wvbVNbY">Grid: Span whole grid</a> by molily (<a href="https://codepen.io/molily">@molily</a>) on <a href="https://codepen.io/">CodePen</a>.</span></p></figure><h2 id="lessstronggreaterAccessibility-considerationslessstronggreater"><strong>Accessibility considerations</strong></h2><p>Accessibility of data visualizations is a top priority for us and our clients. In our SVG charts and HTML / SVG hybrids, we have assigned ARIA roles and accessible labels so graphical shapes have proper semantics and textual representation. In the accessibility tree, these charts appear either as lists (like <code>ul</code> or <code>ol</code> elements) or tables (like the <code>table</code> element) so users can read and navigate the chart in a familiar way.</p><p>While we have made SVG charts accessible, it is simpler and more robust to use semantic HTML directly. The shown HTML &amp; CSS bar chart uses plain <code>ol</code> and <code>li</code> elements with built-in ARIA roles. Screen readers and other assistive tools read out the labels and values.</p><p>Edge with JAWS on Windows:</p><p><video controls=""><source src="https://videos.ctfassets.net/bo3k2i3mbg0o/2vLU4rEGRrRvviFPMB0GqU/b9a0d2171b17072ca3995c4c95532e11/jaws-edge.mp4" type="video/mp4">Reading the bar chart with JAWS and Edge. Navigating through the labels and values by keyboard. <a href="https://videos.ctfassets.net/bo3k2i3mbg0o/2vLU4rEGRrRvviFPMB0GqU/b9a0d2171b17072ca3995c4c95532e11/jaws-edge.mp4">jaws-edge.mp4</a></video></p><p>Chrome with VoiceOver on MacOS:</p><p><video controls=""><source src="https://videos.ctfassets.net/bo3k2i3mbg0o/1TaiDq3atkiGPaZqVFWSFc/6aafd1a35f5931061420cbd171daa94b/voiceover-chrome.mp4" type="video/mp4">Reading the bar chart with VoiceOver and Chrome. Navigating through the labels and values by keyboard. <a href="https://videos.ctfassets.net/bo3k2i3mbg0o/1TaiDq3atkiGPaZqVFWSFc/6aafd1a35f5931061420cbd171daa94b/voiceover-chrome.mp4">voiceover-chrome.mp4</a></video></p><h2 id="lessstronggreaterRecaplessstronggreater"><strong>Recap</strong></h2><p>Today's websites feature responsive layout and fluid typography. Data visualizations should adapt these design techniques.</p><p>While responsive and accessible SVGs are possible, they require manual client-side JavaScript logic. HTML and CSS allow us to create charts using declarative layouts and bidirectional positioning without computing positions and preventing overlap manually.</p><p>We've demonstrated this for a bar chart. We've also created HTML, CSS and SVG hybrids where each technology does what it is good at.</p><h2 id="Building-your-next-data-visualizations">Building your next data visualizations</h2><p>At 9elements, we have been visualizing data for our clients for more than 10 years. In 2013, we developed <a href="https://9elements.com/blog/ged-viz-an-html5-data-visualization-tool/">GED VIZ</a> for the Bertelsmann Foundation, visualizing global economic relations. From 2014 on, we developed the front-end and the chart rendering of the <a href="https://9elements.com/blog/new-project-oecd-data-portal/">OECD Data Portal</a>. In 2015, we contributed to the <a href="https://9elements.com/blog/project-launched-wef-inclusive-growth-report-2015/">World Economic Forum Inclusive Growth Report</a>. The bar charts described in this article are part of a long-term work for an international organization in the public health sector.</p><p>Let us discuss how we can help you to explore, present and visualize the data of your organization or business! <a href="https://9elements.com/contact/">Contact us</a>.</p><h2 id="Acknowledgements">Acknowledgements</h2><p>Thanks to my colleagues <a href="https://9elements.com/blog/author/nils-binder/">Nils Binder</a>, <a href="https://9elements.com/blog/author/julian-laubstein/">Julian Laubstein</a> and Matthias von Schmettow for this collaboration.</p><p>Thanks to <a href="https://vesa.piittinen.name/">Vesa Piittinen</a> for substantial feedback and many valuable ideas on how to improve and simplify the HTML and CSS. Please have a look at <a href="https://codepen.io/Merri/pen/RwzwbdV">Vesa's version of the bar chart</a> which demonstrates more clever optimizations.</p><p>Thanks to the data visualization designers <a href="https://www.alicethudt.de/">Alice Thudt</a>, <a href="https://christianlaesser.com/">Christian Laesser</a> and <a href="https://truth-and-beauty.net/">Moritz Stefaner</a> for their stellar work on the <a href="https://truth-and-beauty.net/projects/who">Data Design Language</a>.</p><p>Thanks to the <a href="https://www.devographics.com/">Devographics</a> team behind the “State of HTML/CSS/JS“ surveys for the inspiration.</p><p>Thanks to our client for the opportunity to work on ambitious data visualizations.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Goldman Sachs: AI Is overhyped, expensive, and unreliable (119 pts)]]></title>
            <link>https://www.404media.co/goldman-sachs-ai-is-overhyped-wildly-expensive-and-unreliable/</link>
            <guid>40948971</guid>
            <pubDate>Fri, 12 Jul 2024 20:12:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.404media.co/goldman-sachs-ai-is-overhyped-wildly-expensive-and-unreliable/">https://www.404media.co/goldman-sachs-ai-is-overhyped-wildly-expensive-and-unreliable/</a>, See on <a href="https://news.ycombinator.com/item?id=40948971">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <article>
          <div>
              
<!--kg-card-begin: html-->

<!--kg-card-end: html-->
<p>Investment giant Goldman Sachs published a research paper about the economic viability of generative AI which notes that there is “little to show for” the huge amount of spending on generative AI infrastructure and questions “whether this large spend will ever pay off in terms of AI benefits and returns.”&nbsp;</p><p>The paper, called “<a href="https://www.goldmansachs.com/intelligence/pages/gs-research/gen-ai-too-much-spend-too-little-benefit/report.pdf?ref=404media.co"><u>Gen AI: too much spend, too little benefit?</u></a>” is based on a series of interviews with Goldman Sachs economists and researchers, MIT professor Daron Acemoglu, and infrastructure experts. The paper ultimately questions whether generative AI will ever become the transformative technology that Silicon Valley and large portions of the stock market are currently betting on, but says investors may continue to get rich anyway. “Despite these concerns and constraints, we still see room for the AI theme to run, either because AI starts to deliver on its promise, or because bubbles take a long time to burst,” the paper notes.&nbsp;</p><p>Goldman Sachs researchers also say that AI optimism is driving large growth in stocks like Nvidia and other S&amp;P 500 companies (the largest companies in the stock market), but say that the stock price gains we’ve seen are based on the assumption that generative AI is going to lead to higher productivity (which necessarily means automation, layoffs, lower labor costs, and higher efficiency). These stock gains are already baked in, Goldman Sachs argues in the paper: “Although the productivity pick-up that AI promises could benefit equities via higher profit growth, we find that stocks often anticipate higher productivity growth before it materializes, raising the risk of overpaying. And using our new long-term return forecasting framework, we find that a very favorable AI scenario may be required for the S&amp;P 500 to deliver above-average returns in the coming decade.”&nbsp;(Ed Zitron also has a <a href="https://www.wheresyoured.at/pop-culture/?ref=404media.co" rel="noreferrer">thorough writeup of the Goldman Sachs report</a> over at Where's Your Ed At.)</p><p>It adds that “outside of the most bullish AI scenario that includes a material improvement to the structural growth/inflation mix and peak US corporate profitability, we forecast that S&amp;P 500 returns would be below their post-1950 average. AI’s impact on corporate profitability will matter critically.”</p><blockquote>"Despite its expensive price tag, the technology is nowhere near where it needs to be in order to be useful for even such basic tasks"</blockquote><p>What this means in plain English is that one of the largest financial institutions in the world is seeing what people who are paying attention are seeing with their eyes: Companies are acting like generative AI is going to change the world and are acting as such, while the reality is that this is a technology that is currently deeply unreliable and may not change much of anything at all. Meanwhile, their stock prices are skyrocketing based on all of this hype and investment, which may not ultimately change much of anything at all.</p><p>Acemoglu, the MIT professor, told Goldman that the industry is banking on the idea that largely scaling the amount of AI training data—which may not actually be possible given the massive amount of training data already ingested—is going to solve some of generative AI’s growing pains and problems. But there is no evidence that this will actually be the case: “What does a doubling of data really mean, and what can it achieve? Including twice as much data from Reddit into the next version of GPT may improve its ability to predict the next word when engaging in an informal conversation, but it won't necessarily improve a customer service representative’s ability to help a customer troubleshoot problems with their video service,” he said. “The quality of the data also matters, and it’s not clear where more high-quality data will come from and whether it will be easily and cheaply available to AI models.” He also posits that large language models themselves “may have limitations” and that the current architecture of today’s AI products may not get measurably better.&nbsp;</p><p>Jim Covello, who is Goldman Sachs’ head of global equity research, meanwhile, said that he is skeptical about both the cost of generative AI and its “ultimate transformative potential.”&nbsp;</p><p>“AI technology is exceptionally expensive, and to justify those costs, the technology must be able to solve complex problems, which it isn’t designed to do,” he said. “People generally substantially overestimate what the technology is capable of today. In our experience, even basic summarization tasks often yield illegible and nonsensical results. This is not a matter of just some tweaks being required here and there; despite its expensive price tag, the technology is nowhere near where it needs to be in order to be useful for even such basic tasks.” He added that Goldman Sachs has tested AI to “update historical data in our company models more quickly than doing so manually, but at six times the cost.”&nbsp;</p><p>Covello then likens the “AI arms race” to “virtual reality, the metaverse, and blockchain,” which are “examples of technologies that saw substantial spend but have few—if any—real world applications today.”&nbsp;</p><p>The Goldman Sachs report comes on the heels of a piece by David Cahn, partner at the venture capital firm Sequoia Capital, which is one of the largest investors in generative AI startups, titled “<a href="https://www.sequoiacap.com/article/ais-600b-question/?ref=404media.co"><u>AI’s $600 Billion Question</u></a>,” which attempts to analyze how much revenue the AI industry as a whole needs to make in order to simply pay for the processing power and infrastructure costs being spent on AI right now.&nbsp;</p><p>To break even on what they’re spending on AI compute infrastructure, companies need to vastly scale their revenue, which Sequoia argues is not currently happening anywhere near the scale these companies need to break even. OpenAI’s annualized revenue has doubled from $1.6 billion in late 2023 to $3.4 billion, but Sequoia’s Cahn asks in his piece: “Outside of ChatGPT, how many AI products are consumers really using today? Consider how much value you get from Netflix for $15.49/month or Spotify for $11.99. Long term, AI companies will need to deliver significant value for consumers to continue opening their wallets.”</p><p>This is all to say that journalists, artists, workers, and even people who <em>use</em> generative AI are not the only ones who are skeptical about the transformative potential of it. The very financial institutions that have funded and invested in the AI frenzy, and are responsible for billions of dollars in investment decisions are starting to wonder what this is all for. </p>
<!--kg-card-begin: html-->

<!--kg-card-end: html-->

                    <div>
    <div>
      <p>About the author</p>
      <p>Jason is a cofounder of 404 Media. He was previously the editor-in-chief of Motherboard. He loves the Freedom of Information Act and surfing.</p>
      
    </div>
      <p><img data-src="/content/images/2023/08/404-jason-01-copy.jpeg" alt="Jason Koebler" src="https://www.404media.co/content/images/2023/08/404-jason-01-copy.jpeg">  
      </p>
  </div>
          </div>
        </article>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Free-threaded CPython is ready to experiment with (363 pts)]]></title>
            <link>https://labs.quansight.org/blog/free-threaded-python-rollout</link>
            <guid>40948806</guid>
            <pubDate>Fri, 12 Jul 2024 19:52:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://labs.quansight.org/blog/free-threaded-python-rollout">https://labs.quansight.org/blog/free-threaded-python-rollout</a>, See on <a href="https://news.ycombinator.com/item?id=40948806">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>First, a few announcements:</p>
<p>Yesterday, <a href="https://py-free-threading.github.io/">py-free-threading.github.io</a> launched!
It's both a resource with documentation around adding support for free-threaded
Python, and a status tracker for the rollout across open source projects in the
Python ecosystem. We hope and expect both of these to be very useful, with the
status tracker providing a one-stop-shop to check the support status of the
dependencies of your project (e.g., "what was the first release of a package on
PyPI to support free-threaded Python?" or "are there nightly wheels and where
can I find them?") and get an overview of ecosystem-wide progress:</p>
<p><img alt="Tracking website for package compatibility with free-threaded CPython." src="https://labs.quansight.org/posts/free-threaded-python-rollout/py_free_threading_tracker.png" width="60%"></p>
<p>Later today, the Birds-of-a-Feather session
<a href="https://cfp.scipy.org/2024/talk/HDR7WZ/">"Supporting free-threaded Python"</a>
will be held at the SciPy 2024 conference (co-organized by one of our team
members, Nathan Goldbaum, together with Madicken Munck), focusing on knowledge
and experience sharing.</p>
<h2 id="free-threaded-cpython---what-why-how">Free-threaded CPython - what, why, how?</h2>
<p>You may be wondering by now what "free threading" or "free-threaded CPython"
is, and why you should care. In summary: it is a major change to CPython that
allows running multiple threads in parallel within the same interpreter. It is
becoming available as an experimental feature in CPython 3.13. A free-threaded
interpreter can run with the global interpreter lock (GIL) disabled - a
capability that is finally arriving as a result of the efforts that went into
<a href="https://peps.python.org/pep-0703/">PEP 703 - Making the Global Interpreter Lock Optional in CPython</a>.</p>
<p>Why? Performance. Multi-threaded performance. It makes it significantly easier
to write code that efficiently runs in parallel and will utilize multiple CPU
cores effectively. The core counts in modern CPUs continue to grow, while clock
speeds do not grow, so multi-threaded performance will continue to grow in
importance.</p>
<p>How? It's now <a href="https://py-free-threading.github.io/installing_cpython/">easy to get started by installing a free-threaded interpreter</a>:
macOS/Linux/Windows &amp; python.org/pyenv/apt/yum/conda - your preferred option is
probably available now.</p>
<h2 id="sounds-awesome---whats-the-catch">Sounds awesome - what's the catch?</h2>
<p>Implementing free-threading in CPython itself is a massive effort already, and
worthy of its own (series of) blog post(s). For the wider ecosystem, there's
also a ton of work involved, mainly due to two problems:</p>
<ol>
<li>Thread-safety. While pure Python code should work unchanged, code written in
other languages or using the CPython C API may not. The GIL was implicitly
protecting a lot of thread-unsafe C, C++, Cython, Fortran, etc. code - and
now it no longer does. Which may lead to all sorts of fun outcomes (crashes,
intermittent incorrect behavior, etc.).</li>
<li>ABI incompatibility between the default and free-threaded CPython builds.
The result of a free-threaded interpreter having a different ABI is that
each package that has extension modules must now build extra wheels.</li>
</ol>
<p>Out of these two, the thread-safety one is the more hairy problem. Having to
implement and maintain extra wheel build jobs is not ideal, but the work itself
is well-understood - it just needs doing for each project with extension
modules. Thread-safety on the other hand is harder to understand, improve,
and even test reliably. Because multithreaded code is usually sensitive to the
timing of how multiple threads run and access shared state, bugs may manifest
rarely. And a crash or failure that is hard to reproduce locally is
harder to fix then one that is always reproducible.</p>
<p>Here are a couple of examples of such intermittent failures:</p>
<p><a href="https://github.com/numpy/numpy/issues/26690">numpy#26690</a> shows an example
where a simple call to the <code>.sum()</code> method of a numpy array fails with a
fairly mysterious</p>
<div data-ch-theme="solarized-dark"><p><code><br><div><p><span>RuntimeError: Identity cache already includes the item.</span></p></div><br></code></p></div>
<p>when used with the Python <code>threading</code> and <code>queue</code> modules. This was noticed
in a scikit-learn CI job - it never failed in NumPy's own CI (scikit-learn has
more tests involving parallelism). After the bug report with a reproducer was
submitted, the fix to a numpy-internal cache wasn't that hard.</p>
<p><a href="https://github.com/PyWavelets/pywt/issues/758">pywavelets#758</a> was a report
of another fairly obscure failure in a test using <code>concurrent.futures</code>:</p>
<div data-ch-theme="solarized-dark"><p><code><br><div><p><span>TypeError: descriptor '__enter__' for '_thread.RLock' objects doesn't apply to a '_thread.lock' object</span></p></div><br></code></p></div>
<p>That looked a lot like a problem in CPython, and after some investigating it
was found there as well <a href="https://github.com/python/cpython/issues/121368">cpython#121368</a>
and fixed fairly quickly (the fix required some deep expertise in both CPython
internals and multithreaded programming in C though).</p>
<p>There are a fair amount of examples like that, e.g. <a href="https://github.com/PyWavelets/pywt/pull/753#issuecomment-2190335170">undefined behavior in
Cython code</a> that
no longer worked due to changes in CPython 3.13, a <a href="https://github.com/scipy/scipy/issues/21142">crash from C code in
<code>scipy.signal</code></a> that hadn't been
touched for 24 years (it was always buggy, but the GIL offered enough
protection), and a <a href="https://github.com/hugovk/Pillow/pull/123">crash in Pillow</a>
due to <a href="https://github.com/python/cpython/issues/121403">Python C API usage that wasn't
supported</a>.</p>
<p>It's encouraging though that issues like the ones above do get understood and
resolved fairly quickly. With a good test strategy, and over time also test
suites of libraries that cover Python-level threading better (such tests are
largely non-existent now in most packages), detecting or guarding against
thread-safety issues does seem doable. That test strategy will have to be
multi-pronged: from writing new tests and running tests in loops with <code>pytest-repeat</code> &amp;
co., to getting <a href="https://clang.llvm.org/docs/ThreadSanitizer.html">ThreadSanitizer</a>
to work in CI and doing integration-level and real-world testing with users.</p>
<h2 id="the-road-ahead--what-our-team-will-be-working-on">The road ahead &amp; what our team will be working on</h2>
<p>Free-threaded CPython becoming the default, and eventually the only, build of
CPython is several years away. What we're hoping to see, and help accomplish,
is that for Python 3.13 many projects will work on compatibility and start
releasing <code>cp313t</code> wheels on PyPI (and possibly nightly builds too, for projects
with a lot of dependencies), so users and packages further downstream can start
experimenting as well. After a full year of maturing support in the ecosystem
and further improvements in performance in CPython itself, we should have a
good picture of both the benefits and the remaining challenges with robustness.</p>
<p>Our team (currently <a href="https://github.com/ngoldbaum">Nathan</a>,
<a href="https://github.com/Fidget-Spinner">Ken Jin</a>,
<a href="https://github.com/lysnikolaou/">Lysandros</a>,
<a href="https://github.com/andfoy/">Edgar</a>, and
<a href="https://github.com/rgommers/">myself</a>) has now been working on this topic for
a few months, starting at the bottom of the PyData stack (most effort so far
has gone to NumPy, Cython, and CPython), and slowly working our way up from
there.</p>
<p>For each package, the approach has been similar so far - and a lot of that can
be used as a template by others we think. The steps are roughly:</p>
<ol>
<li>Add a first CI job, usually Linux x86-64 with the latest Python 3.13
pre-release candidate, and ensure the test suite passes,</li>
<li>Based on knowledge from maintainers, fix known issues with thread-safety and
shared/global state in native code,</li>
<li>Add free-threaded support to the wheel build CI jobs, and start uploading
nightly wheels (if appropriate for the project),</li>
<li>Do some stress testing locally and monitor CI jobs, and fix failures that
are observed (take the opportunity to add regression tests using <code>threading</code>
or <code>concurrent.futures.ThreadPoolExecutor</code>)</li>
<li>Mark extension modules as supporting running without the GIL</li>
<li>Move on to a next package (e.g., a key dependency) and using its test suite
to exercise the first package more, circling back to fix issues or address
follow-up actions as needed.</li>
</ol>
<p>Our main takeaway so far: it's challenging, but tractable! And fun as well:)</p>
<p>We've only just scratched the surface, there'll be a lot to do - from key
complex packages like PyO3 (important for projects using Rust) and
PyTorch, to the sheer volume of smaller packages with extension modules.
The lessons we are learning, as far as they are reusable, are going into the
documentation at
<a href="https://py-free-threading.github.io/">py-free-threading.github.io</a>.
The <a href="https://github.com/Quansight-Labs/free-threaded-compatibility/">repository</a>
that contains the sources for that website also has an issue tracker that is used
to link to the relevant project-specific tracking issues for free-threaded support,
as well as for ecosystem-wide issues and tasks (contributions and ideas are
very welcome here!).</p>
<p>Furthermore, we'd like to spend time on whatever may be impactful in helping
the ecosystem adopt free-threaded CPython, from answering questions
to helping with debugging - please don't hesitate to reach out or ping one of
us directly on GitHub!</p>
<h2 id="conclusion--acknowledgements">Conclusion &amp; acknowledgements</h2>
<p>We're really excited about what is becoming possible with free-threaded CPython!
While our team is busy with implementing CI jobs and fixing thread-safety issues,
we are as curious as anyone to see what performance improvements and
interesting experiments are going to show up with real-world code soon.</p>
<p>It's hard to acknowledge and thank everyone involved in moving free-threaded
CPython forward, because so much activity is happening. First of all we have
to thank Meta for funding the efforts of our team to help the ecosystem adopt
free-threaded CPython at the pace that will be needed to make this whole
endeavour a success, and Sam Gross and the whole Python Runtime team at Meta
for the close collaboration. Then the list is long - from the Python Steering
Council, for its thoughtful approach to (and acceptance of) PEP 703, to the
many library maintainers and community members who are proactively adding
support to their own projects or guide and review our contributions whenever we
work on projects we are not ourselves maintainers of.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Beating the Compiler (137 pts)]]></title>
            <link>https://www.mattkeeter.com/blog/2024-07-12-interpreter/</link>
            <guid>40948353</guid>
            <pubDate>Fri, 12 Jul 2024 18:54:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mattkeeter.com/blog/2024-07-12-interpreter/">https://www.mattkeeter.com/blog/2024-07-12-interpreter/</a>, See on <a href="https://news.ycombinator.com/item?id=40948353">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
<!-- End header -->






<p>In modern times, everyone knows that writing assembly is a fool's errand:
compilers are the result of literal engineer-centuries of work, and they know
the processor much better than you do.</p>
<p>And yet – one hears <em>rumors</em>.</p>
<p>Written in <a href="https://jilp.org/vol5/v5paper12.pdf">ancient tomes</a>,
muttered in <a href="http://lua-users.org/lists/lua-l/2011-02/msg00742.html">quiet watering holes</a>,
scrawled on the walls of
<a href="https://www.reddit.com/r/programming/comments/badl2/luajit_2_beta_3_is_out_support_both_x32_x64/c0lrus0/">bygone temples</a>,
hinted at by
<a href="https://llvm.org/docs/LangRef.html#calling-conventions">mysterious texts</a>;
the rumors paint a specific picture:</p>
<blockquote>
<p>Compilers are bad at generating code for interpreters, and it's possible to
outperform them by writing your interpreter in assembly.</p>
</blockquote>
<p>I recently <a href="https://www.mattkeeter.com/projects/raven">wrote a fast interpreter</a> for the
<a href="https://wiki.xxiivv.com/site/uxn.html">Uxn CPU</a>,
a stack-based architecture with 256 opcodes.  The interpreter is a simple loop
which reads a byte from RAM then selects the appropriate instruction:</p>
<pre><code>impl Uxn {
    /// Runs the VM starting at the given address until it terminates
    #[inline]
    pub fn run&lt;D: Device&gt;(&amp;mut self, dev: &amp;mut D, mut pc: u16) {
        loop {
            let op = self.ram[usize::from(pc)];
            pc = pc.wrapping_add(1);
            let Some(next) = self.op(op, dev, pc) else {
                break;
            };
            pc = next;
        }
    }

    /// Executes a single operation
    #[inline]
    fn op&lt;D: Device&gt;(&amp;mut self, op: u8, dev: &amp;mut D, pc: u16) -&gt; Option&lt;u16&gt; {
        match op {
            0x00 =&gt; op::brk(self, dev, pc),
            0x01 =&gt; op::inc::&lt;0b000&gt;(self, dev, pc),
            0x02 =&gt; op::pop::&lt;0b000&gt;(self, dev, pc),
            0x03 =&gt; op::nip::&lt;0b000&gt;(self, dev, pc),
            0x04 =&gt; op::swp::&lt;0b000&gt;(self, dev, pc),
            0x05 =&gt; op::rot::&lt;0b000&gt;(self, dev, pc),
            0x06 =&gt; op::dup::&lt;0b000&gt;(self, dev, pc),
            0x07 =&gt; op::ovr::&lt;0b000&gt;(self, dev, pc),
            0x08 =&gt; op::equ::&lt;0b000&gt;(self, dev, pc),
            0x09 =&gt; op::neq::&lt;0b000&gt;(self, dev, pc),
            0x0a =&gt; op::gth::&lt;0b000&gt;(self, dev, pc),
            0x0b =&gt; op::lth::&lt;0b000&gt;(self, dev, pc),
            0x0c =&gt; op::jmp::&lt;0b000&gt;(self, dev, pc),
            0x0d =&gt; op::jcn::&lt;0b000&gt;(self, dev, pc),
            0x0e =&gt; op::jsr::&lt;0b000&gt;(self, dev, pc),
            // ... etc
        }
    }
}
</code></pre>
<p>All of the opcode implementations end up monomorphized and inlined into the body
of <code>Uxn::run(..)</code>, and the compiler is smart enough to keep key values in
registers.  This makes it relatively fast; I see 10-20% speedup over the
<a href="https://git.sr.ht/%7Erabbits/uxn/">reference implementation</a>.</p>
<p>Let's look at the assembly and see what the compiler is doing – and whether we
can do any better.  For context, the Uxn CPU has four different memories:</p>
<ul>
<li>The data stack, which is a <code>[u8; 256]</code> along with a <code>u8</code> index</li>
<li>The return stack, which has the same format</li>
<li>RAM, which is a <code>[u8; 65535]</code></li>
<li>Device memory, which we'll ignore for the moment (along with the <code>D: Device</code>
argument)</li>
</ul>
<p>During evaluation, we also track the program counter <code>pc</code>, which is a <code>u16</code> used
to index into the RAM.  In each cycle, we load a byte from RAM, then call the
appropriate opcode.  Some opcodes can also read and write to RAM, so
self-modifying code is possible!</p>
<p>By examining the <a href="https://www.mattkeeter.com/projects/raven/disassembly.html">assembly</a>, we can
reverse-engineer which values are stored where.  Consider the <code>INC</code> operation,
which loads a value from the top of the data stack and increments it:</p>
<pre><code>; INC
0x100002d4c: ldrb w8, [x25]     ; read the current data stack index
0x100002d50: ldrb w9, [x24, x8] ; read a byte from the data stack
0x100002d54: add w9, w9, #1     ; increment that byte
0x100002d58: strb w9, [x24, x8] ; write that byte back to the stack
0x100002d5c: b 0x100002d1c      ; jump back to the dispatch loop
</code></pre>
<p>From this assembly, we learn the following:</p>
<ul>
<li><code>x25</code> is the <em>address</em> of the data stack index (not its value!)</li>
<li><code>x24</code> is the address of the data stack array</li>
<li><code>w9</code> is used as a temporary register</li>
</ul>
<p>Similarly, <code>INCr</code> – increment the top value in the <strong>return</strong> stack – teaches us
that <code>x22</code> and <code>x23</code> are the return stack's data and index addresses.</p>
<p><code>JMP</code> shows that our program counter is stored in <code>w27</code>:</p>
<pre><code>; JMP
0x100002eac: ldrb w8, [x25]      ; read the current data stack index
0x100002eb0: ldrsb w9, [x24, x8] ; read a signed jump offset from the data stack
0x100002eb4: sub w8, w8, #1      ; decrement the data stack index
0x100002eb8: strb w8, [x25]      ; write back the data stack index
0x100002ebc: add w27, w27, w9    ; apply the jump to our program counter
0x100002ec0: b 0x100002d1c       ; jump back to the dispatch loop
</code></pre>
<p>Finally, the dispatch loop itself is worth examining:</p>
<pre><code>0x100002d1c: and x10, x27, #0xffff          ; mask pc to a u16
0x100002d20: ldr x8, [x20, #256]            ; load RAM base from *mut Uxn
0x100002d24: ldrb w10, [x8, x10]            ; load opcode byte from RAM
0x100002d28: add w27, w27, #1               ; increment pc
0x100002d2c: adr x11, #-96                  ; load base for jump
0x100002d30: ldrh w12, [x27, x10, lsl  #1]  ; load per-opcode jump amount
0x100002d34: add x11, x11, x12, lsl #2      ; compute jump location
0x100002d38: br x11                         ; jump into opcode implementation
</code></pre>
<p>The compiler has generated a jump table of 256 offsets (each a 2-byte value,
indicated by <code>lsl #1</code>).  It reads an opcode-specific value from this table to
compute a jump target, then performs an indirect branch to jump into the
opcode's implementation.</p>
<p>We can run this in a debugger and dump the actual jump table:</p>
<pre><code>(lldb) disas -p -c3
raven-cli`raven_uxn::Uxn::run::had9dba0d7d1b5105:
-&gt;  0x100002d30 &lt;+236&gt;: ldrh   w12, [x27, x10, lsl  #1]
    0x100002d34 &lt;+240&gt;: add    x11, x11, x12, lsl #2
    0x100002d38 &lt;+244&gt;: br     x11
(lldb) reg read x27
     x27 = 0x0000000100170b10
(lldb) memory read -s2 -fu -c256 0x0000000100170b10
0x100170b10: 2923
0x100170b12: 31
0x100170b14: 36
0x100170b16: 40
0x100170b18: 44
0x100170b1a: 52
0x100170b1c: 28
0x100170b1e: 64
0x100170b20: 70
0x100170b22: 78
0x100170b24: 86
0x100170b26: 94
0x100170b28: 119
0x100170b2a: 102
0x100170b2c: 110
0x100170b2e: 125
; etc...
</code></pre>
<p>(indeed, this is how I generated the <a href="https://www.mattkeeter.com/projects/raven/disassembly.html">per-opcode instruction listing</a>)</p>
<hr>
<p>Having looked at the assembly, there are two things that stick out as possible
inefficiencies:</p>
<ul>
<li>Some critical values (stack indices, the base address of RAM) are kept in
memory instead of registers; for example, <code>INC</code> has an extra load operation to
get the current data stack index.</li>
<li>The dispatch loop takes a single indirect branch to the opcode-specific
implementation.  This means that the branch will be nigh unpredictable!</li>
</ul>
<p>Profiling the code, the hottest instructions are all in the dispatch loop; the
<code>ldrh</code> takes over 1/3 of the total runtime!</p>
<p><img src="https://www.mattkeeter.com/blog/2024-07-12-interpreter/profile.png" alt="screenshot of profiling info"></p>
<p>(I'm not confident that the profiler is attributing the time to the correct
specific instruction here, but the vibes definitely indicate that dispatch is
expensive)</p>
<hr>
<p><a href="http://luajit.org/">LuaJIT</a> is the fast interpreter <em>par excellence</em>, and it's
written in assembly.  Mike Pall
<a href="https://www.reddit.com/r/programming/comments/badl2/luajit_2_beta_3_is_out_support_both_x32_x64/c0lrus0/">specifically calls out</a>
keeping state in registers and indirect threading as two contributors to its
speed, which can only be
<a href="http://lua-users.org/lists/lua-l/2011-02/msg00742.html">accomplished reliably</a>
in assembly.</p>
<p>Since persuading our compiler to generate extremely specific patterns is hard,
let's get started writing some assembly of our own.  My home machine is an M1
Macbook, so all of the assembly will be AArch64-flavored. The implementation
uses general-purpose registers; be aware that <code>w*</code> and <code>x*</code> refer to 32-bit and
64-bit views of the same register.</p>
<hr>
<h3>Register assignment</h3>
<p>Our first optimization is to store <strong>all</strong> important data in registers, to avoid
superfluous loads and stores.  My implementation ends up using 9 registers
(<code>x0-x8</code>), along with a handful of scratch registers:</p>
<pre><code>; x0 - stack pointer (&amp;mut [u8; 256])
; x1 - stack index (u8)
; x2 - return stack pointer (&amp;mut [u8; 256])
; x3 - return stack index (u8)
; x4 - RAM pointer (&amp;mut [u8; 65536])
; x5 - program counter (u16), offset of the next value in RAM
; x6 - VM pointer (&amp;mut Uxn)
; x7 - Device handle pointer (&amp;DeviceHandle)
; x8 - Jump table pointer
; x9-15 - scratch registers
</code></pre>
<p>The <a href="https://en.wikipedia.org/wiki/Calling_convention#ARM_(A64)">AArch64 calling convention</a>
only gives you 8 input arguments, so we can't call a function directly with all
of these values in registers;
we'll need a C ABI-flavored entry point (<a href="#shims">discussed below</a>).</p>
<h3>Indirect threading</h3>
<p>Our second optimization is using
<a href="https://en.wikipedia.org/wiki/Threaded_code">threaded code</a>
to eliminate the dispatch loop.
Each opcode's implementation will end with a jump to the next opcode's
implementation.</p>
<p>Opcodes are stored as single bytes in VM RAM, with a base address of <code>x4</code>.  I'll
build a separate jump table of function pointers, then pass its address in
register <code>x8</code>.  On the Rust side, here's what that table looks like:</p>
<pre><code>extern "C" {
    fn BRK();
    fn INC();
    fn POP();
    fn NIP();
    fn SWP();
    fn ROT();
    fn DUP();
    fn OVR();
    fn EQU();
    // ...etc
}

const JUMP_TABLE: [unsafe extern "C" fn(); 256] = [
    (BRK as unsafe extern "C" fn()),
    (INC as unsafe extern "C" fn()),
    (POP as unsafe extern "C" fn()),
    (NIP as unsafe extern "C" fn()),
    (SWP as unsafe extern "C" fn()),
    (ROT as unsafe extern "C" fn()),
    (DUP as unsafe extern "C" fn()),
    (OVR as unsafe extern "C" fn()),
    (EQU as unsafe extern "C" fn()),
    (NEQ as unsafe extern "C" fn()),
    // ... etc
];
</code></pre>
<p>In assembly, we want to read the current byte from VM RAM (<code>x4</code>), use it to pick
an address in the jump table (<code>x8</code>), then jump to that address.  I defined a
macro to do this dispatch:</p>
<pre><code>.macro next
    ldrb w9, [x4, x5]          ; load the byte from RAM
    add x5, x5, #1             ; increment the program counter
    and x5, x5, #0xffff        ; wrap the program counter
    ldr x10, [x8, x9, lsl #3]  ; load the opcode implementation address
    br x10                     ; jump to the opcode's implementation
.endm
</code></pre>
<p>Notice that this is a <strong>macro</strong>, not a function; we'll add <code>next</code> to the end of
each opcode, which will expand into this text.</p>
<p>For example, here's <code>INC</code>:</p>
<pre><code>.global _INC
_INC:
    ldrb w9, [x0, x1]   ; read the byte from the top of the stack
    add w9, w9, #1      ; increment it
    strb w9, [x0, x1]   ; write it back
    next                ; jump to the next opcode
</code></pre>
<p>Unlike LuaJIT, there's no <strong>decoding</strong> step for instructions; there are no
register arguments, and the single-byte opcode uniquely defines program
behavior.</p>
<h3>Implementation</h3>
<p>Implementing the other 255 opcodes is mostly just turning the crank;
there's nothing particularly exotic here, just good honest assembly.</p>
<p>In many cases, I'll use helper macros to generate code for a group of
instructions:</p>
<pre><code>.macro binary_op op
    ldrb w10, [x0, x1]  ; read the top value from the data stack
    pop                 ; decrement the data stack index (this is a macro!)
    ldrb w11, [x0, x1]  ; read the next value from the data stack
    \op w10, w11, w10   ; do the actual math operation
    strb w10, [x0, x1]  ; write the result into the data stack
    next
.endm

.global _ADD
_ADD:
    binary_op add

.global _SUB
_SUB:
    binary_op sub

.global _MUL
_MUL:
    binary_op mul

.global _DIV
_DIV:
    binary_op udiv
</code></pre>
<p>The whole implementation ends up being about
<a href="https://github.com/mkeeter/raven/blob/main/raven-uxn/src/native/aarch64.s">2400 lines</a>.
It sounds like a lot, but only about half of that is unique:
most opcodes come in two flavors (with and without the <code>RET</code> flag),
which only differ in which stack is used.</p>
<h3>C Shims</h3>
<p>Of course, my whole program isn't hand-written in assembly.  We need a way to
call our assembly function from the rest of our (Rust) implementation.  This
looks like a (Rust) <code>entry</code> function, which calls into an (assembly)
<code>aarch64_entry</code> point (which is compatible with the C ABI):</p>
<p><img src="https://www.mattkeeter.com/blog/2024-07-12-interpreter/entry.png" alt="diagram showing entry points"></p>
<p>What do we actually pass into <code>aarch64_entry</code>?  We have too much state to pass
in function argument registers (<code>x0-x7</code>), so I defined a helper object which
contains everything we need:</p>
<pre><code>#[repr(C)]
pub(crate) struct EntryHandle {
    stack_data: *mut u8,
    stack_index: *mut u8,
    ret_data: *mut u8,
    ret_index: *mut u8,
    ram: *mut u8,
    vm: *mut core::ffi::c_void,  // *Uxn
    dev: *mut core::ffi::c_void, // *DeviceHandle
}

struct DeviceHandle&lt;'a&gt;(&amp;'a mut dyn Device);
</code></pre>
<p>The <code>DeviceHandle</code> is needed because <code>&amp;mut dyn Device</code> is a fat pointer, and is
therefore not safe to pass into a C function.  Like all computer problems, we
solve this with an extra level of indirection: put the <code>&amp;mut dyn Device</code> into a
<code>DeviceHandle</code>, then pass <em>its</em> address instead.</p>
<p>Calling into assembly is a simple matter of populating an <code>EntryHandle</code> object,
then branching into the danger zone:</p>
<pre><code>// Declaration of our entry point, written in assembly
extern "C" {
    pub fn aarch64_entry(
        h: *const EntryHandle,
        pc: u16,
        table: *const unsafe extern "C" fn(),
    ) -&gt; u16;
}

pub fn entry(vm: &amp;mut Uxn, dev: &amp;mut dyn Device, pc: u16) -&gt; u16 {
    let mut h = DeviceHandle(dev);
    let mut e = EntryHandle {
        stack_data: vm.stack.data.as_mut_ptr(),
        stack_index: &amp;mut vm.stack.index as *mut _,
        ret_data: vm.ret.data.as_mut_ptr(),
        ret_index: &amp;mut vm.ret.index as *mut _,
        ram: (*vm.ram).as_mut_ptr(),
        vm: vm as *mut _ as *mut _,
        dev: &amp;mut h as *mut _ as *mut _,
    };

    // SAFETY: do you trust me?
    unsafe {
        aarch64::aarch64_entry(&amp;mut e as *mut _, pc, JUMP_TABLE.as_ptr())
    }
}
</code></pre>
<p><code>aarch64_entry</code> is a hand-written entry point in the assembly code.  It shuffles
around registers to put everything in the right place for our opcodes, then
begins execution with the usual <code>next</code> macro:</p>
<pre><code>.global _aarch64_entry
_aarch64_entry:
    sub sp, sp, #0x200  ; make room in the stack
    stp   x29, x30, [sp, 0x0]   ; store stack and frame pointer
    mov   x29, sp

    // Unpack from EntryHandle into registers
    mov x5, x1 ; move PC (before overwriting x1)
    mov x8, x2 ; jump table (before overwriting x2)
    ldr x1, [x0, 0x8]  ; stack index pointer
    ldr x2, [x0, 0x10] ; ret data pointer
    ldr x3, [x0, 0x18] ; ret index pointer
    ldr x4, [x0, 0x20] ; RAM pointer
    ldr x6, [x0, 0x28] ; *mut Uxn
    ldr x7, [x0, 0x30] ; *mut DeviceHandle
    ldr x0, [x0, 0x00] ; stack data pointer (overwriting *EntryHandle)

    ; Convert from index pointers to index values in w1 / w3
    stp x1, x3, [sp, 0x10]      ; save stack index pointers
    ldrb w1, [x1]               ; load stack index
    ldrb w3, [x3]               ; load ret index

    ; Jump into the instruction list
    next
</code></pre>
<p>Finally, when exiting (via the <code>BRK</code> opcode), we need to update the data and
return stack indices, moving values from registers into the appropriate memory
addresses:</p>
<pre><code>.global _BRK
_BRK:
    ; Write index values back through index pointers
    ldp x9, x10, [sp, 0x10]     ; restore stack index pointers
    strb w1, [x9]               ; save data stack index
    strb w3, [x10]              ; save return stack index

    ldp   x29, x30, [sp, 0x0]   ; restore stack and frame pointer
    add sp, sp, #0x200          ; undo our stack offset

    mov x0, x5 ; return PC from function
    ret
</code></pre>
<h3>Device IO</h3>
<p>The <code>DEI</code> and <code>DEO</code> opcodes perform "device I/O", which lets you attach
arbitrary peripherals to the system.  The most common set of peripherals is the
<a href="https://wiki.xxiivv.com/site/varvara.html">Varvara system</a>,
which adds everything you need to make the CPU into an actual computer: a
screen, keyboard and mouse input, audio, etc.</p>
<p>To keep the Uxn implementation generic, I defined a trait for a device:</p>
<pre><code>/// Trait for a Uxn-compatible device
pub trait Device {
    /// Performs the `DEI` operation for the given target
    ///
    /// This function must write its output byte to `vm.dev[target]`; the CPU
    /// evaluation loop will then copy this value to the stack.
    fn dei(&amp;mut self, vm: &amp;mut Uxn, target: u8);

    /// Performs the `DEO` operation on the given target
    ///
    /// The input byte will be written to `vm.dev[target]` before this function
    /// is called, and can be read by the function.
    ///
    /// Returns `true` if the CPU should keep running, `false` if it should
    /// exit.
    #[must_use]
    fn deo(&amp;mut self, vm: &amp;mut Uxn, target: u8) -&gt; bool;
}
</code></pre>
<p>The opcode implementation takes a <code>&amp;mut dyn Device</code>, i.e. something implementing
this trait, and calls trait methods on it:</p>
<pre><code>pub fn deo&lt;const FLAGS: u8&gt;(
    vm: &amp;mut Uxn,
    dev: &amp;mut dyn Device,
    pc: u16,
) -&gt; Option&lt;u16&gt; {
    let mut s = vm.stack_view::&lt;FLAGS&gt;();
    let i = s.pop_byte();
    let mut run = true;
    match s.pop() {
        Value::Short(v) =&gt; {
            let [lo, hi] = v.to_le_bytes();
            let j = i.wrapping_add(1);
            vm.dev[usize::from(i)] = hi;
            run &amp;= dev.deo(vm, i);
            vm.dev[usize::from(j)] = lo;
            run &amp;= dev.deo(vm, j);
        }
        Value::Byte(v) =&gt; {
            vm.dev[usize::from(i)] = v;
            run &amp;= dev.deo(vm, i);
        }
    }
    if run {
        Some(pc)
    } else {
        None
    }
}
</code></pre>
<p>However, this function is not compatible with the C ABI – it's both generic
<em>and</em> takes a trait object – so it can't be called directly from the <code>DEO</code>
opcode in assembly.</p>
<p>To let my opcodes call <code>DEO</code> and <code>DEI</code> functions, I again wrote a bunch of
shims:</p>
<pre><code>#[no_mangle]
extern "C" fn deo_entry(vm: &amp;mut Uxn, dev: &amp;mut DeviceHandle) -&gt; bool {
    vm.deo::&lt;0b000&gt;(dev.0, 0).is_some()
}

#[no_mangle]
extern "C" fn deo_2_entry(vm: &amp;mut Uxn, dev: &amp;mut DeviceHandle) -&gt; bool {
    vm.deo::&lt;0b001&gt;(dev.0, 0).is_some()
}

#[no_mangle]
extern "C" fn deo_r_entry(vm: &amp;mut Uxn, dev: &amp;mut DeviceHandle) -&gt; bool {
    vm.deo::&lt;0b010&gt;(dev.0, 0).is_some()
}

// etc, 16 functions in total for all DEI / DEO variants
</code></pre>
<p>The full path of the function looks something like this:</p>
<p><img src="https://www.mattkeeter.com/blog/2024-07-12-interpreter/deo.png" alt="diagram showing deo calls"></p>
<p>On the assembly side, there's one subtlety: during our normal opcode processing,
we keep data and return stack index values in <code>x1</code> and <code>x3</code> (leaving the
original values in the <code>&amp;mut Uxn</code> unchanged).  We have to write those registers
back into the appropriate memory locations in the <code>&amp;mut Uxn</code> <em>before</em> calling a
function that expects those values to be correct.</p>
<p>Here's the assembly code to call into our shim functions:</p>
<pre><code>.global _DEI
_DEI:
    ; We have to write our stack index pointers back into the &amp;mut Uxn
    ldp x11, x12, [sp, 0x10] ; restore stack index pointers
    strb w1, [x11]   ; modify stack index pointer
    strb w3, [x12]   ; modify return stack index pointer

    ; We're using caller-saved registers, so we have to back them up
    stp x0, x1, [sp, #0x20] ; store register state
    stp x2, x3, [sp, #0x30]
    stp x5, x4, [sp, #0x40]
    stp x6, x7, [sp, #0x50]
    str x8,     [sp, #0x60]

    ; set up our arguments, then call the shim function:
    mov x0, x6 ; x0 = Uxn pointer
    mov x1, x7 ; x1 = DeviceHandle pointer
    bl _dei_entry

    ldp x0, x1, [sp, #0x20] ; restore register state
    ldp x2, x3, [sp, #0x30]
    ldp x5, x4, [sp, #0x40]
    ldp x6, x7, [sp, #0x50]
    ldr x8,     [sp, #0x60]

    ; The DEO operation may have changed stack pointers, so reload them here 
    ldp x11, x12, [sp, 0x10]
    ldrb w1, [x11]  ; update stack index pointer
    ldrb w3, [x12]  ; update return stack index pointer
    next
</code></pre>
<h3>Performance</h3>
<p>I used two CPU-heavy workloads to test interpreter performance:</p>
<ul>
<li><a href="https://git.sr.ht/%7Erabbits/uxn/tree/main/item/projects/examples/exercises/fib.tal"><code>fib.tal</code></a>,
modified to print the first <strong>35</strong> numbers of the Fibonacci sequence</li>
<li><a href="https://git.sr.ht/%7Erabbits/uxn/tree/main/item/projects/examples/demos/mandelbrot.tal"><code>mandelbrot.tal</code></a>,
with <code>%SCALE</code> set to <code>#0020</code> (rendering a 672 × 512 image)</li>
</ul>
<p>Both of these programs do all of their computation at startup, so I added
instrumentation to print time spent in the entry vector (at <code>0x100</code>).</p>
<p>There are four different implementations being tested here:</p>
<ul>
<li>The <a href="https://git.sr.ht/%7Erabbits/uxn/"><code>uxnemu</code></a> reference implementation,
running natively on my laptop</li>
<li>The baseline <code>raven-uxn</code> interpreter, running natively on my laptop</li>
<li>The optimized <code>raven-uxn</code> interpreter (hand-written in assembly), running
natively on my laptop</li>
<li>The baseline <code>raven-uxn</code> interpreter, running in my browser (compiled to
WebAsembly)</li>
</ul>
<p>Here are the performance numbers that you've been waiting for:</p>
<table>
    <tbody><tr><th>Interpreter</th><th>Target</th><th>Fibonacci</th><th>Mandelbrot
    </th></tr><tr><td><code>uxnemu</code> (reference)</td><td><code>AArch64</code></td><td>1.57 s</td><td>2.03 s
    </td></tr><tr><td><code>raven-uxn</code> (baseline)</td><td><code>AArch64</code></td><td>1.38 s</td><td>1.56 s
    </td></tr><tr><td><code>raven-uxn</code> (assembly)</td><td><code>AArch64</code></td><td>1.00 s</td><td>1.10 s
    </td></tr><tr><td><code>raven-uxn</code> (baseline)</td><td><code>wasm32</code></td><td>2.54 s</td><td>2.82 s
</td></tr></tbody></table>
<p>There are three clear trends:</p>
<ul>
<li><code>raven-uxn</code>'s baseline interpreter (written in safe Rust) is faster than the
reference implementation; we already knew that from previous work</li>
<li>The assembly implementation is about <strong>30% faster</strong> than the baseline!</li>
<li>WebAssembly encurs a roughly 1.8× slowdown compared to the baseline</li>
</ul>
<h3>Ablation testing</h3>
<p>It's not obvious whether the speedup is due to keeping values in registers, or
adding dispatch to the end of each opcode (instead of a central branch).</p>
<p>We can easily test for the latter by changing our <code>next</code> macro:</p>
<pre><code>.macro next
    b next_dispatch
.endm
next_dispatch:
    ldrb w9, [x4, x5]
    add x5, x5, #1
    and x5, x5, #0xffff
    ldr x10, [x8, x9, lsl #3]
    br x10
</code></pre>
<p>Adding these new results to the chart( as "assembly*"), here's what I see:</p>
<table>
    <tbody><tr><th>Interpreter</th><th>Target</th><th>Fibonacci</th><th>Mandelbrot
    </th></tr><tr><td><code>raven-uxn</code> (baseline)</td><td><code>AArch64</code></td><td>1.38 s</td><td>1.56 s
    </td></tr><tr><td><code>raven-uxn</code> (assembly)</td><td><code>AArch64</code></td><td>1.00 s</td><td>1.10 s
    </td></tr><tr><td><code>raven-uxn</code> (assembly*)</td><td><code>AArch64</code></td><td>1.34 s</td><td>1.41 s
</td></tr></tbody></table>
<p>Centralized dispatch is a significant slowdown, and is nearly as slow as the
baseline interpreter!
It just goes to show:
<a href="https://www.mattkeeter.com/blog/2023-01-25-branch/">do not taunt happy fun branch predictor</a>.</p>
<h3>Things that didn't work</h3>
<p>I did a bunch of other experiments, which didn't make things faster:</p>
<ul>
<li>Expanding RAM to store both user bytes and the jump targets (i.e. making RAM a
<code>[u64; 65536]</code>).  The user byte is stored in bits 48-54 of the pointer, since
those are unused, and I added masking + shifting depending on whether we were
using the data or pointer component.  This was noticeably slower, probably
because it's less cache-friendly (512 KiB, rather than 64 KiB + 1 KiB of jump
table)</li>
<li>Making all of the opcode implementations the same size (padding to the size of
the largest opcode implementation with <code>.balign 256</code>), then removing the jump
table entirely.  This was also slower, also probably because of cache
friendliness: the opcode implementations go from 16.6 KiB total to 64 KiB.</li>
</ul>
<h3>Conclusion</h3>
<p>I've proven to my satisfaction that writing an interpreter in assembly is both
fun and performant!</p>
<p>There are strategies to get similar performance in high-level languages:
<a href="https://eli.thegreenplace.net/2012/07/12/computed-goto-for-efficient-dispatch-tables">using computed goto</a>
and the <a href="https://github.com/wasm3/wasm3/blob/main/docs/Interpreter.md#m3-massey-meta-machine">Massey Meta Machine</a>
are both relevant prior art.</p>
<p>However, neither of these are feasible in Rust; to quote
<a href="https://pliniker.github.io/post/dispatchers/">this excellent writeup</a>.</p>
<blockquote>
<p>At this time there is no portable way to produce computed gotos or tail call
optimization in compiled machine code from Rust.</p>
</blockquote>
<p>On a brighter note, it should be relatively easy to port all of the assembly
code to x86-64, but I'll leave that as a challenge for someone else!</p>
<p>All of the relevant code is <a href="https://github.com/mkeeter/raven">on Github</a>, gated
by the <code>native</code> feature.  The <code>uxn-cli</code> and <code>uxn-gui</code> executables both accept a
<code>--native</code> flag to select the assembly interpreter backend.</p>
<p>Have fun!</p>

<!-- Begin footer -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What could explain the gallium anomaly? (154 pts)]]></title>
            <link>https://www.quantamagazine.org/what-could-explain-the-gallium-anomaly-20240712/</link>
            <guid>40948202</guid>
            <pubDate>Fri, 12 Jul 2024 18:35:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/what-could-explain-the-gallium-anomaly-20240712/">https://www.quantamagazine.org/what-could-explain-the-gallium-anomaly-20240712/</a>, See on <a href="https://news.ycombinator.com/item?id=40948202">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="postBody"><div><p>Physicists have ruled out a mundane explanation for the strange findings of an old Soviet experiment, leaving open the possibility that the results point to a new fundamental particle.</p></div><figure><div><p><img alt="" src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2024/07/GalliumAnomaly-crNicoRoper-Lede-scaled.webp"></p></div><figcaption><div><p>Gallium occasionally converts into germanium, but not as often as expected.</p><p>Nico Roper/<em>Quanta Magazine</em></p></div></figcaption></figure><div><h2>Introduction</h2><div><p>Deep in the Caucasus Mountains, on the border between Russia and Georgia, an unusual experiment is taking place. In an underground lab shielded by a mountain of rock, highly radioactive material sits inside a vat of liquid gallium, blasting out particles called neutrinos that break the gallium down into atoms of germanium.</p>
<p>The goal is to resolve a little-known mystery of physics: the gallium anomaly. “I think it’s one of the most compelling anomalies in neutrino physics that we have today,” said <a href="https://www.uta.edu/academics/faculty/profile?username=jonesb">Ben Jones</a>, a neutrino physicist at the University of Texas, Arlington. Some three decades ago, in a previous version of the current experiment, scientists first detected a dearth of the expected germanium atoms that still can’t be explained.</p>
<p>Since then, physicists have worked to rule out possible mismeasurements or inaccuracies that could explain the anomaly. Now they’ve eliminated another one. <a href="https://nuc.berkeley.edu/people/eric-norman/">Eric Norman</a>, a nuclear physicist at the University of California, Berkeley, and colleagues <a href="https://journals.aps.org/prc/abstract/10.1103/PhysRevC.109.055501#fulltext">have announced</a> that one possible solution, an incorrect calculation of the half-life of germanium, can’t be the cause.</p>
<p>“The half-life is correct,” Norman said. “This is not the explanation for the gallium anomaly.”</p>
<p>That leaves few possibilities. One is that some still-unknown experimental defect caused the anomaly. Perhaps a different mismeasurement is throwing things off, or a misunderstanding of nuclear physics. Or maybe, just maybe, the anomaly points to a monumental discovery, the existence of a new type of elementary particle called a sterile neutrino. Sterile neutrinos were initially proposed to explain why the masses of the three known neutrinos are so tiny, but they could also account for at least some of the invisible “dark matter” that fills the cosmos.</p>
<p>“We cannot find some huge uncertainty in our experimental procedures,” said Vladislav Barinov, a particle physicist at the Institute for Nuclear Research of the Russian Academy of Sciences who works on the experiment in the Caucasus. “Is it a new type of neutrino? We don’t know.”</p>
<h2><strong>Neutrino Village</strong></h2>
<p>At the height of the Cold War, before the fall of the Berlin Wall in 1989 and the subsequent dissolution of the Soviet Union, an unlikely partnership arose in the form of an experiment called SAGE, the Soviet-American Gallium Experiment. “The Soviet Union had a phenomenal group of theoretical scientists,” said Steven Elliott, a nuclear physicist at Los Alamos National Laboratory who worked on the project. But they lacked money and access to certain technologies that would make SAGE possible, he said. “Los Alamos was able to provide those types of resources.”</p>
</div></div><figure><div><p><img alt="" src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2024/07/BaksanNeutrinoObservatory-crMaximBabenko_TheNewYorkTimes_Diptych-scaled.webp"></p></div><figcaption><div><p>For the last half-century, physicists have studied neutrinos in experiments deep underground at the Baksan Neutrino Observatory in Russia’s Caucasus Mountains. In the lab that houses the BEST experiment, fish serve as an early warning system about any leaking radiation.</p><p>Maxim Babenko/The New York Times</p></div></figcaption></figure><div><h2>Introduction</h2><div><p>SAGE was constructed at the Baksan Neutrino Observatory, a <a href="https://cerncourier.com/a/baksan-scales-new-neutrino-heights/">neutrino physics facility</a> built in the 1960s and 1970s inside a mountain in Russia’s Baksan Valley, about 3 miles from the Georgian border. The 13,000-foot-tall Mount Andyrchi shielded the facility from cosmic rays and other sources of noise, allowing precise neutrino experiments to take place.</p>
<p>A nearby residential area called Neutrino Village housed the families of the scientists who worked at the facility, as well as visiting international scientists like Elliott. “I did go out for a number of trips,” he said. “I found it an adventure.”</p>
<p>SAGE began in 1989 and continued for more than 20 years despite attempts by the Russian government to <a href="https://www.science.org/content/article/new-twist-gallium-struggle">sell its gallium</a>, a precious metal that’s liquid at room temperature. The project was designed to investigate the solar neutrino problem, a measured deficit of neutrinos streaming from the sun. Specifically, scientists were finding a shortage of electron neutrinos, one of three known types, or “flavors.” That problem was ultimately resolved in the 2000s with the <a href="https://www.symmetrymagazine.org/article/nobel-prize-awarded-for-discovery-of-neutrino-oscillations">Nobel Prize-winning discovery</a> that neutrinos oscillate between flavors as they travel. By the time many of the electron neutrinos from the sun reach Earth, they have become something else.</p>
<p>SAGE used a tank of 57 metric tons of gallium. Incoming electron neutrinos would occasionally combine with a neutron inside a gallium atom and convert it into a proton, turning the gallium into germanium. The scientists counted the germanium atoms in a monthlong extraction process. They chose gallium for the experiment because it has a “low threshold for this reaction,” Elliott said. A similar experiment began in Italy in 1991, called Gallex.</p>
</div></div><figure><div><p><img alt="" src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2024/07/GALLAXExperimentAtGranSassoLab-crTommasoGuicciardini_INFN_ScienceSource.webp"></p></div><figcaption><div><p>A researcher with the Gallex experiment, which ran in the 1990s at Gran Sasso National Laboratory in Italy, is shown holding a device called a proportional counter that was used for detecting germanium atoms.</p><p>Tommaso Guicciardini/INFN/ScienceSource</p></div></figcaption></figure><div><h2>Introduction</h2><div><p>In the mid-1990s, researchers tweaked both experiments to use neutrinos from radioactive elements. They hoped to avoid unknown errors related to the solar neutrino problem. But both experiments generated roughly 20% less germanium than expected — surprise results that couldn’t have been caused by the solar neutrino problem. “They exactly knew the source activity and how many neutrinos are produced,” said Inwook Kim, a nuclear physicist at Los Alamos. Soon, the puzzling discrepancy had a name: the gallium anomaly. “It was really surprising,” Barinov said.</p>
<p>A follow-up experiment that began at Baksan in 2014, called the Baksan Experiment on Sterile Transitions (BEST), uses two gallium chambers instead of one, to determine whether the anomaly could be explained by the distance from the source of the neutrinos. “BEST was constructed to resolve this tension,” said Barinov, who has worked on the experiment since 2015. But both chambers have continued to show a shortfall relative to what models predict. “It’s a really unusual result,” he said.</p>
<h2><strong>Half-Life Theory</strong></h2>
<p>Repeated results from BEST continue to show the anomaly <a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.128.232501">as recently as 2022</a>. One chamber contained only 79% of the expected amount of germanium, the other only 77%. “Everybody was hoping that anomaly would go away,” said Wick Haxton, a theoretical physicist at Berkeley. “There is still not any clean understanding of what’s going on.”</p>
<p>A possible explanation was floated: that the half-life of germanium-71 (the specific isotope produced in the experiment), <a href="https://journals.aps.org/prc/abstract/10.1103/PhysRevC.31.666">measured</a> in 1985 to be 11.43 days, was actually longer. The same constant controls germanium-71’s decay rate and the rate at which gallium captures neutrinos to produce that germanium. That means a longer germanium-71 half-life would imply a lower rate of neutrino capture and hence germanium production, which could explain the lack of germanium seen by SAGE, Gallex and BEST.</p>
</div></div><figure><div><p><img alt="A young man wearing a red scarf stands in front of a mountain." src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2024/07/VladislavBarinov-crSviatoslavBorisov.webp"></p></div><figcaption><div><p>Vladislav Barinov, a particle physicist at the Institute for Nuclear Research of the Russian Academy of Sciences, is part of the team that reported the anomalous results of the BEST experiment.</p><p>Sviatoslav Borisov</p></div></figcaption></figure><div><h2>Introduction</h2><div><p>Norman and colleagues published a reinvestigation of this half-life in <a href="https://journals.aps.org/prc/abstract/10.1103/PhysRevC.109.055501"><em>Physical Review C</em></a> in late May. Using a nuclear reactor at the McClellan Nuclear Research Center at the University of California, Davis, they irradiated “very pure germanium material,” Norman said, producing germanium-71. They then analyzed the samples over 80 days to see how long it took the atoms to decay.</p>
<p>They arrived at a half-life of 11.468 days, extremely close to the 1985 measurement, ruling the half-life out as the explanation for the gallium anomaly. While no one ever quite believed the original half-life measurement to be wildly incorrect, researchers still considered it worth checking. “It was a measurement that needed to be done,” Jones said.</p>
<p>Another proposed explanation was that physicists had miscalculated the probability of neutrinos from the source interacting with the gallium. But in September 2023, Haxton and his colleagues <a href="https://journals.aps.org/prc/abstract/10.1103/PhysRevC.108.035502">also ruled out this possibility</a>. “You can’t get rid of the anomaly,” he said.</p>
<p>That leaves physicists in an uncomfortable position. Either there is still some error that no one has thought of, or, as Haxton put it, “something unusual is going on with neutrinos.” For instance, the experiments might point to a controversial additional type of neutrino, undetected by most other experiments, that might also help to explain dark matter.</p>
<h2><strong>Sterile Neutrinos</strong></h2>
<p>The three known flavors of neutrinos, which are all millions of times lighter than electrons, interact with other elementary particles via the weak force, which makes them detectable. Sterile neutrinos, on the other hand, would interact only via gravity. If they’re much heavier than the known neutrinos, their existence could explain why the known neutrinos are so light, through an inverse relationship hypothesized around 1980 called the seesaw mechanism.</p>
</div></div><figure><div><p><img alt="A yellow-lit laboratory crammed with equipment, including a set of 10 chemical reactors with red caps." src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2024/07/Gallium%E2%80%93Germanium_Neutrino_Telescope_main_room_-_2010-07-19_-_DSC_0764.webp"></p></div><figcaption><div><p>The experimental apparatus used in the SAGE and BEST experiments at the Baksan Neutrino Observatory.</p><p>Konstantin Malanchev</p></div></figcaption></figure><div><h2>Introduction</h2><div><p>The gallium anomaly, however, would point toward a lighter-weight sterile neutrino, with the electron neutrinos emitted by the radioactive source sometimes oscillating into a sterile neutrino that wouldn’t interact with the gallium.</p>
<p>In some models, lightweight sterile neutrinos could comprise a fraction of the universe’s dark matter, though not all of it because they would be too light to gravitationally shape the universe in the way dark matter does. “They could be a small subset of it,” said <a href="https://physics.mit.edu/faculty/lindley-winslow/">Lindley Winslow</a>, an experimental nuclear and particle physicist at the Massachusetts Institute of Technology.</p>
<p>Other attempts to find sterile neutrinos by studying neutrino oscillation patterns, however, have been largely unsuccessful. The number of researchers who support the light sterile neutrino “is sort of shrinking,” Winslow said. <a href="https://sites.uci.edu/abazajian/">Kevork Abazajian</a>, an astrophysicist at the University of California, Irvine, said they are the “underdogs of the particle physics community.”</p>
<p>If they do exist, light sterile neutrinos will “wreak havoc” on our current understanding of cosmology, Abazajian said, including ideas of how atoms formed in the minutes following the Big Bang and the theory of the cosmic microwave background, the remnant heat from the initial expansion of the universe. “You would expect to see the presence of this extra neutrino,” Abazajian said. However, he added that <a href="https://arxiv.org/abs/2205.09777">recent work has shown</a> that alternative models of the sequence of events in those first minutes “can accommodate light sterile neutrinos.”</p>

<p>In lieu of other explanations for the gallium anomaly, light sterile neutrinos remain a possibility that we just can’t eradicate. “I’ve been a bit skeptical of the sterile neutrino hypothesis, but I can’t tell you why it’s not right,” Elliott said. “There’s never been a convincing explanation of why the experiment might be wrong.”</p>
<p>While Russia’s invasion of Ukraine “has complicated things,” Elliott said, the collaboration between the U.S. and Russia on BEST is still ongoing, for now. Barinov says the team at Baksan is considering using a new source of neutrinos, such as zinc, to further test the result. They may even construct a third chamber of gallium around the source. For now, the anomaly remains unsolved, with no sign of a resolution on the horizon. “It has us all puzzled,” Haxton said.</p>
</div></div></div><div><h2>Next article</h2><p>How America’s Fastest Swimmers Use Math to Win Gold</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CISA broke into a US federal agency, and no one noticed for a full 5 months (166 pts)]]></title>
            <link>https://www.theregister.com/2024/07/12/cisa_broke_into_fed_agency/</link>
            <guid>40948064</guid>
            <pubDate>Fri, 12 Jul 2024 18:19:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2024/07/12/cisa_broke_into_fed_agency/">https://www.theregister.com/2024/07/12/cisa_broke_into_fed_agency/</a>, See on <a href="https://news.ycombinator.com/item?id=40948064">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>The US Cybersecurity and Infrastructure Security Agency (CISA) says a red team exercise at a certain unnamed federal agency in 2023 revealed a string of security failings that exposed its most critical assets.</p>
<p>CISA calls these SILENTSHIELD assessments. The agency's dedicated red team picks a federal civilian executive branch (FCEB) agency to probe and does so without prior notice – all the while trying to simulate the maneuvers of a long term hostile nation-state threat group.</p>
<p>According to the agency's account of the exercise, the red team was able to gain initial access by exploiting an unpatched vulnerability (<a target="_blank" href="https://www.theregister.com/2024/02/27/manufacturing_sector_malware/">CVE-2022-21587</a> - 9.8) in the target agency's Oracle Solaris enclave, leading to what it said was a full compromise.</p>

    

<p>It's worth noting that CVE-2022-21587, an unauthenticated remote code execution (RCE) bug carrying a near-maximum 9.8 CVSS rating, was added to CISA's known exploited vulnerability (KEV) catalog in February 2023. The initial intrusion by CISA's red team was made on January 25, 2023.</p>

        


        

<p>"After gaining access, the team promptly informed the organization's trusted agents of the unpatched device, but the organization took over two weeks to apply the available patch," CISA's report reads. "Additionally, the organization did not perform a thorough investigation of the affected servers, which would have turned up IOCs and should have led to a full incident response.&nbsp;</p>
<p>"About two weeks after the team obtained access, exploit code was released publicly into a popular open source exploitation framework. CISA identified that the vulnerability was exploited by an unknown third party. CISA added this CVE to its Known Exploited Vulnerabilities Catalog on February 2, 2023."</p>

        

<p>Vulnerabilities added to the KEV catalog mean a few things. First, they are serious, known to be exploited by cybercriminals, and can lead to serious consequences. Second, when bugs are added to the catalog, they also come with deadlines by which FCEB agencies have to patch them.</p>
<p>Since introducing the KEV catalog, CISA has always been cagey about the degree to which federal agencies meet these deadlines, but this case shows they aren't always being met.</p>
<p><em>The Register</em> fielded a question about deadline compliance to CISA's director Jen Easterly at the <a href="https://www.theregister.com/2024/07/01/cisa_big_tech_security/">Oxford Cyber Forum</a> last month who said, without referring to specific figures she didn't have access to at the time, that "compliance is very high." Plus, a recent survey showed the catalog is <a href="https://www.theregister.com/2024/05/07/cisas_vulnerability_deadlines/">helping the private sector too</a>.</p>

        

<p>After gaining access to the Solaris enclave, the red team discovered they couldn't pivot into the Windows part of the network because missing credentials blocked their path, despite enjoying months of access to sensitive web apps and databases.</p>
<p>Undeterred, CISA managed to make its way into the Windows network after carrying out <a href="https://www.theregister.com/2024/05/23/google_phishing_tests/">phishing attacks</a> on unidentified members of the target agency, one of which was successful.</p>
<p>It said real adversaries may have instead used prolonged password-praying attacks rather than phishing at this stage, given that several service accounts were identified as having <a href="https://www.theregister.com/2024/04/29/uk_lays_password_legislation/">weak passwords</a>.</p>
<p>After gaining that access, the red team injected a persistent RAT and later discovered unsecured admin credentials, which essentially meant it was game over for the agency being assessed.&nbsp;</p>
<p>"None of the accessed servers had any noticeable additional protections or network access restrictions despite their sensitivity and critical functions in the network," CISA said.</p>
<p>CISA described this as a "full domain compromise" that gave the attackers access to tier zero assets – the most highly privileged systems.</p>
<p>"The team found a password file left from a previous employee on an open, administrative IT share, which contained plaintext usernames and passwords for several privileged service accounts," the report reads. "With the harvested Lightweight Directory Access Protocol (LDAP) information, the team identified one of the accounts had system center operations manager (SCOM) administrator privileges and domain administrator privileges for the parent domain.&nbsp;</p>
<p>"They identified another account that also had administrative permissions for most servers in the domain. The passwords for both accounts had not been updated in over eight years and were not enrolled in the organization's identity management (IDM)."</p>
<p>From here, the red team realized the victim organization had trust relationships with multiple external FCEB organizations, which CISA's team then pivoted into using the access they already had.</p>
<p>The team "kerberoasted" one partner organization. Kerberoasting is an attack on the Kerberos authentication protocol typically used in Windows networks to authenticate users and devices. However, it wasn't able to move laterally with the account due to low privileges, so it instead used those credentials to exploit a second trusted partner organization. Kerberoasting yielded a more privileged account at the second external org, the password for which was crackable.</p>
<p>CISA said that due to network ownership, legal agreements, and/or vendor opacity, these kinds of cross-organizational attacks are rarely tested during assessments.</p>
<p>However, SILENTSHIELD assessments are able to be carried out following new-ish powers afforded to CISA by the FY21 National Defense Authorization Act (NDAA), the same powers that also allow CISA's Federal Attack Surface Testing (FAST) pentesting program to operate.</p>
<p>It's crucial that these avenues are able to be explored in such exercises because they're routes into systems adversaries will have no reservations about exploring in a real-world scenario.</p>
<h3>Disclosure time</h3>
<p>For the first five months of the assessment, the target FCEB agency failed to detect or remediate any of the SILENTSHIELD activity, raising concerns over its ability to spot genuine malicious activity.</p>
<p>Detection issues were suspected earlier in the proceedings. The RAT, which was injected in the Solaris phase of the exercise, caused 8GB of network traffic to flow through its C2 seemingly without alerting anyone at the agency, for example.</p>
<p>After CISA eventually put the agency out of its misery, weekly meetings were held with its security team and sysadmins which led to "measurable improvements in response times for known techniques and behavior-based detections that uncovered previously unknown tradecraft."</p>
<p>One of the main issues discussed in the post mortem was the agency's log collection, which was deemed to be "ineffective and inefficient." Various issues impeded the agency's ability to collect logs, which you can read about in the <a href="https://www.cisa.gov/news-events/cybersecurity-advisories/aa24-193a" rel="nofollow">full writeup</a>, but CISA's compromise of Solaris and Windows hosts had a big impact as packet capturing happened here, and so CISA was able to disrupt the process.</p>
<ul>

<li><a href="https://www.theregister.com/2024/07/02/supreme_court_chevron/">Brace for new complications in big tech takedowns after Supreme Court upended regulatory rules</a></li>

<li><a href="https://www.theregister.com/2024/06/28/cisa_open_source/">CISA looked at C/C++ projects and found a lot of C/C++ code. Wanna redo any of it in Rust?</a></li>

<li><a href="https://www.theregister.com/2024/06/27/congress_china_drones/">US lawmakers wave red flags over Chinese drone dominance</a></li>

<li><a href="https://www.theregister.com/2024/07/11/fcc_warns_yet_again_of/">FCC: US telcos a long way off, several billions short of removing Chinese kit</a></li>
</ul>
<p>The assessed agency also placed too great a reliance on known indicators of compromise (IoCs) for detecting intrusions, plus various system misconfigurations and procedural issues hindered the analysis of network activity.&nbsp;</p>
<p>CISA said the exercise demonstrated the need for FCEB agencies to apply defense-in-depth principles – multiple layers of detection and analysis measures for maximum effectiveness. Network segmentation was recommended and the red team wanted to stress the danger of over-relying on known IOCS.</p>
<p>It also wouldn't be a CISA communiqué without a plug for its <a href="https://www.theregister.com/2024/05/09/68_tech_firms_sign_cisas/">secure-by-design</a> push. It said that insecure software contributes to the issues faced by the target agency and re-upped its call to stamp out default passwords, provide free logging to customers, and for vendors to work with SIEM and SOAR providers to make better use of those logs. ®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hackers Steal Phone, SMS Records for Nearly All AT&T Customers (306 pts)]]></title>
            <link>https://krebsonsecurity.com/2024/07/hackers-steal-phone-sms-records-for-nearly-all-att-customers/</link>
            <guid>40948035</guid>
            <pubDate>Fri, 12 Jul 2024 18:15:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://krebsonsecurity.com/2024/07/hackers-steal-phone-sms-records-for-nearly-all-att-customers/">https://krebsonsecurity.com/2024/07/hackers-steal-phone-sms-records-for-nearly-all-att-customers/</a>, See on <a href="https://news.ycombinator.com/item?id=40948035">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
												<p><strong>AT&amp;T Corp.</strong> disclosed today that a new data breach has exposed phone call and text message records for roughly 110 million people — nearly all of its customers. AT&amp;T said it delayed disclosing the incident in response to “national security and public safety concerns,” noting that some of the records included data that could be used to determine where a call was made or text message sent. AT&amp;T also acknowledged the customer records were exposed in a cloud database that was protected only by a username and password (no multi-factor authentication needed).</p>
<p><img decoding="async" src="https://krebsonsecurity.com/wp-content/uploads/2023/03/attbldg.png" alt="" width="749" height="497"></p>
<p>In <a href="https://www.sec.gov/Archives/edgar/data/732717/000073271724000046/t-20240506.htm" target="_blank" rel="noopener">a regulatory filing</a> with the <strong>U.S. Securities and Exchange Commission</strong> today, AT&amp;T said cyber intruders accessed an AT&amp;T workspace on a third-party cloud platform in April, downloading files containing customer call and text interactions between May 1 and October 31, 2022, as well as on January 2, 2023.</p>
<p>The company said the stolen data includes records of calls and texts for mobile providers that resell AT&amp;T’s service, but that it does not include the content of calls or texts, Social Security numbers, dates of birth, or any other personally identifiable information.</p>
<p>However, the company said a subset of stolen records included information about the location of cellular communications towers closest to the subscriber, data that could be used to determine the approximate location of the customer device initiating or receiving those text messages or phone calls.</p>
<p>“While the data does not include customer names, there are often ways, using publicly available online tools, to find the name associated with a specific telephone number,” AT&amp;T allowed.</p>
<p>AT&amp;T’s said it learned of the breach on April 19, but delayed disclosing it at the request of federal investigators. The company’s SEC disclosure says at least one individual has been detained by the authorities in connection with the breach.</p>
<p>In a written statement shared with KrebsOnSecurity, the FBI confirmed that it asked AT&amp;T to delay notifying affected customers.</p>
<p>“Shortly after identifying a potential breach to customer data and before making its materiality decision, AT&amp;T contacted the FBI to report the incident,” the FBI statement reads. “In assessing the nature of the breach, all parties discussed a potential delay to public reporting under Item 1.05(c) of the SEC Rule, due to potential risks to national security and/or public safety. AT&amp;T, FBI, and DOJ worked collaboratively through the first and second delay process, all while sharing key threat intelligence to bolster FBI investigative equities and to assist AT&amp;T’s incident response work.”</p>
<p><a href="https://techcrunch.com/2024/07/12/att-phone-records-stolen-data-breach/?guccounter=1" target="_blank" rel="noopener">Techcrunch</a> quoted an AT&amp;T spokesperson saying the customer data was stolen as a result of a still-unfolding data breach involving more than 160 customers of the cloud data provider <strong>Snowflake</strong>.</p>
<p>Earlier this year, malicious hackers figured out that many major companies have uploaded massive amounts of valuable and sensitive customer data to Snowflake servers, all the while protecting those Snowflake accounts with little more than a username and password.<span id="more-68041"></span></p>
<p><a href="https://www.wired.com/story/epam-snowflake-ticketmaster-breach-shinyhunters/" target="_blank" rel="noopener">Wired reported</a> last month how the hackers behind the Snowflake data thefts purchased stolen Snowflake credentials from dark web services that sell access to usernames, passwords and authentication tokens that are siphoned by information-stealing malware. For its part, Snowflake says it now requires all new customers to use multi-factor authentication.</p>
<p>Other companies with millions of customer records stolen from Snowflake servers include <strong>Advance Auto Parts</strong>, <strong>Allstate</strong>, <strong>Anheuser-Busch</strong>, <strong>Los Angeles Unified</strong>, <strong>Mitsubishi</strong>, <strong>Neiman Marcus</strong>, <strong>Progressive</strong>, <strong>Pure Storage</strong>, <strong>Santander Bank</strong>, <strong>State Farm</strong>, and <strong>Ticketmaster</strong>.</p>
<p>Earlier this year, AT&amp;T <a href="https://techcrunch.com/2024/03/30/att-reset-account-passcodes-customer-data/" target="_blank" rel="noopener">reset passwords for millions of customers</a> after the company <a href="https://krebsonsecurity.com/2022/08/it-might-be-our-data-but-its-not-our-breach/" target="_blank" rel="noopener">finally acknowledged a data breach from 2018</a> involving approximately 7.6 million current AT&amp;T account holders and roughly 65.4 million former account holders.</p>
<p><strong>Mark Burnett</strong> is an application security architect, consultant and author. Burnett said the only real use for the data stolen in the most recent AT&amp;T breach is to know who is contacting whom and how many times.</p>
<p>“The most concerning thing to me about this AT&amp;T breach of ALL customer call and text records is that this isn’t one of their main databases; it is metadata on who is contacting who,” Burnett <a href="https://infosec.exchange/@zcutlip@hachyderm.io/112774443764622821" target="_blank" rel="noopener">wrote</a> on Mastodon. “Which makes me wonder what would call logs without timestamps or names have been used for.”</p>
<p>It remains unclear why so many major corporations persist in the belief that it is somehow acceptable to store so much sensitive customer data with so few security protections. For example, Advance Auto Parts said the data exposed included full names, Social Security numbers, drivers licenses and government issued ID numbers on <a href="https://www.bleepingcomputer.com/news/security/advance-auto-parts-data-breach-impacts-23-million-people/" target="_blank" rel="noopener">2.3 million people</a> who were former employees or job applicants.</p>
<p>That may be because, apart from the class-action lawsuits that invariably ensue after these breaches, there is little holding companies accountable for sloppy security practices. AT&amp;T told the SEC it does not believe this incident is likely to materially impact AT&amp;T’s financial condition or results of operations. AT&amp;T reported revenues of more than $30 billion in its most recent quarter.</p>
											</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ex-Meta scientists debut gigantic AI protein design model (113 pts)]]></title>
            <link>https://www.nature.com/articles/d41586-024-02214-x</link>
            <guid>40947540</guid>
            <pubDate>Fri, 12 Jul 2024 17:22:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/d41586-024-02214-x">https://www.nature.com/articles/d41586-024-02214-x</a>, See on <a href="https://news.ycombinator.com/item?id=40947540">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-024-02214-x/d41586-024-02214-x_27319756.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-024-02214-x/d41586-024-02214-x_27319756.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="Molecular model of the bright green fluorescent protein StayGold from Cytaeis uchidae." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-024-02214-x/d41586-024-02214-x_27319756.jpg">
  <figcaption>
   <p><span>A structural model of green fluorescent protein, a workhorse of biotechnology.</span><span>Credit: Laguna Design/Science Photo Library</span></p>
  </figcaption>
 </picture>
</figure><p>An <a href="https://www.nature.com/subjects/machine-learning" data-track="click" data-label="https://www.nature.com/subjects/machine-learning" data-track-category="body text link">artificial intelligence</a> (AI) model that speaks the <a href="https://www.nature.com/subjects/protein-design" data-track="click" data-label="https://www.nature.com/subjects/protein-design" data-track-category="body text link">language of proteins</a> — one of the largest yet developed for biology — has been used to create new fluorescent molecules.</p><p>The proof-of-principle demonstration was announced this month by EvolutionaryScale in New York City, alongside US$142 million in new funding to apply its model to <a href="https://www.nature.com/articles/d41586-023-02227-y" data-track="click" data-label="https://www.nature.com/articles/d41586-023-02227-y" data-track-category="body text link">drug development</a>, sustainability and other pursuits. The company, launched by scientists who previously worked at tech giant Meta, is the latest entrant in an increasingly crowded field that is applying cutting-edge machine-learning models trained on language and images to biological data.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-023-02227-y" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-024-02214-x/d41586-024-02214-x_26069432.jpg"><p>AI tools are designing entirely new proteins that could transform medicine</p></a>
 </article><p>“We want to build tools that can make biology programmable,” says Alex Rives, the company’s chief scientist, who was part of Meta’s efforts to apply AI to biological data.</p><p>EvolutionaryScale’s AI tool, called ESM3, is what’s known as a protein language model. It was trained on more than 2.7 billion protein sequences and structures, as well as information about these proteins’ functions. The model can be used to <a href="https://www.nature.com/articles/d41586-023-02227-y" data-track="click" data-label="https://www.nature.com/articles/d41586-023-02227-y" data-track-category="body text link">create proteins</a> to specifications provided by users, akin to the text spit out by chatbots such as ChatGPT.</p><p>“It’s going to be one of the AI models in biology that everybody’s paying attention to,” says Anthony Gitter, a computational biologist at the University of Wisconsin–Madison.</p><h2>Glowing up</h2><p>Rives and his colleagues worked on earlier iterations of the ESM model at Meta, but struck out on their own last year, after Meta ended its work in this area. They had previously used the model ESM-2 to create a <a href="https://www.nature.com/articles/d41586-022-03539-1" data-track="click" data-label="https://www.nature.com/articles/d41586-022-03539-1" data-track-category="body text link">freely available database of 600 million predicted protein structures</a><sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup>. Other teams have since used versions of ESM-1 to <a href="https://www.nature.com/articles/d41586-023-01516-w" data-track="click" data-label="https://www.nature.com/articles/d41586-023-01516-w" data-track-category="body text link">design antibodies with improved activity against pathogens</a> including SARS-CoV-2<sup><a href="#ref-CR2" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">2</a></sup> and to re-engineer ‘anti-CRISPR’ proteins to improve the efficiency of gene-editing tools<sup><a href="#ref-CR3" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">3</a></sup>.</p><p>This year, another biology AI company, Profluent in Berkeley, California, used its own protein language model to <a href="https://www.nature.com/articles/d41586-024-01243-w" data-track="click" data-label="https://www.nature.com/articles/d41586-024-01243-w" data-track-category="body text link">create new CRISPR-inspired gene-editing proteins</a>, and made one such molecule freely available for use.</p><p>To demonstrate its latest model, Rives’ team set out to overhaul another biotechnology workhorse: the green fluorescent protein (GFP), which absorbs blue light and glows green. Researchers isolated GFP in the 1960s, from the bioluminescent jellyfish <i>Aequorea victoria</i>. Later work — which, with the discovery, was recognized with a Nobel prize — showed how GFP could label other proteins viewed under a microscope, explained the molecular basis for its fluorescence and developed synthetic versions of the protein that glowed much more brightly and in different colours.</p><p>Researchers have since identified other similarly shaped fluorescent proteins, all sharing a light-absorbing and -emitting ‘chromophore’ core surrounded by a barrel-shaped scaffold. Rives’ team asked ESM3 to create examples of GFP-like proteins that contained a set of key amino acids found in GFP’s chromophore.</p><p>The researchers synthesized 88 of the most promising designs and measured their ability to fluoresce. Most were duds, but one design, dissimilar to known fluorescent proteins, glowed faintly — about 50 times weaker than natural forms of GFP. Using this molecule’s sequence as a starting point, the researchers tasked ESM3 with improving on its work. When the researchers made around 100 of the resulting designs, several were as bright as natural GFPs, which are still vastly dimmer than lab-engineered variants.</p><p>One of the brightest ESM3-designed proteins, dubbed esmGFP, is predicted to have a structure resembling those of natural fluorescent proteins. However, its amino-acid sequence is vastly different, matching less than 60% of the sequence of the most closely related fluorescent protein in its training data set. In a preprint posted on the server bioRxiv<sup><a href="#ref-CR4" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">4</a></sup>, Rives and his colleagues say that on the basis of natural mutation rates, this level of sequence difference equates to “over 500 million years of evolution”.</p><p>But Gitter worries that this comparison is an unhelpful, and potentially misleading, way of describing the product of a cutting-edge AI model. “It sounds scary when you think about AI and accelerating evolution,” he says. “I feel like overhyping what a model does can hurt the field and it can be dangerous for the public.”</p><p>Rives sees ESM3’s generation of new proteins by iterating through various sequences as analogous to evolution. “We think the perspective of what it would take for nature to generate something like this is an interesting one,” he adds.</p><h2>Risk threshold</h2><p>ESM3 is among the first biological AI models to use enough computing power during its training to require developers to notify the US government and report <a href="https://www.nature.com/articles/d41586-024-00699-0" data-track="click" data-label="https://www.nature.com/articles/d41586-024-00699-0" data-track-category="body text link">risk-mitigation measures</a>, under a 2023 presidential executive order. EvolutionaryScale says it has already been in touch with the US Office of Science and Technology Policy.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-024-00699-0" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-024-02214-x/d41586-024-02214-x_26828914.jpg"><p>Could AI-designed proteins be weaponized? Scientists lay out safety guidelines</p></a>
 </article><p>The version of ESM3 that eclipsed that threshold — comprising nearly 100 billion parameters, or variables the model uses to represent relationships between sequences — is not publicly available. For a smaller open-source version, certain sequences, such as those from viruses and a US government list of worrying pathogens and toxins, were excluded from training. Neither can ESM3-open — which scientists anywhere can download and run independently — be prompted to generate such proteins.</p><p>Martin Pacesa, a structural biologist at the Swiss Federal Institute of Technology in Lausanne, is excited to begin working with ESM3. It is one of the first biological models to allow researchers to specify designs using natural-language descriptions of its properties and functions, he notes, and he is eager to see how this and other features perform experimentally.</p><p>Pacesa is impressed that EvolutionaryScale released an open-source version of ESM3, and a clear description of how the largest version was trained. But the largest model would take immense computing resources to develop independently, he says. “No academic lab will be able to replicate it.”</p><p>Rives is eager to apply ESM3 to other designs. Pacesa, who was part of the team that used a different protein language model to make new CRISPR proteins, says it will be interesting to see how ESM3 does at this. Rives envisions applications in sustainability — a video on the company’s website shows the design of plastic-eating enzymes — and in the development of antibodies and other protein-based drugs. “It’s really a model at the frontier,” he says.</p>
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Dropbase AI – A Prompt-Based Python Web App Builder (112 pts)]]></title>
            <link>https://github.com/DropbaseHQ/dropbase</link>
            <guid>40947415</guid>
            <pubDate>Fri, 12 Jul 2024 17:08:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/DropbaseHQ/dropbase">https://github.com/DropbaseHQ/dropbase</a>, See on <a href="https://news.ycombinator.com/item?id=40947415">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
  <a href="https://www.dropbase.io/" rel="nofollow">
    <img src="https://camo.githubusercontent.com/aeb3a756b72f99981f6265cba4f5f607a4c6ffbd265266bc65756b70756fe1a3/68747470733a2f2f6173736574732d676c6f62616c2e776562736974652d66696c65732e636f6d2f3566326338373234366231376663663636323238323539342f3631323561316661313136303539326664333733643333625f44726f70626173652532306c6f676f253230776562736974652e737667" width="200px" alt="Dropbase logo" data-canonical-src="https://assets-global.website-files.com/5f2c87246b17fcf662282594/6125a1fa1160592fd373d33b_Dropbase%20logo%20website.svg">
  </a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Prompt-Based Python Web App Builder</h2><a id="user-content-prompt-based-python-web-app-builder" aria-label="Permalink: Prompt-Based Python Web App Builder" href="#prompt-based-python-web-app-builder"></a></p>
<p dir="auto">
<a href="https://dropbase.io/" rel="nofollow">Website</a> · <a href="https://docs.dropbase.io/" rel="nofollow">Docs</a> · <a href="https://discord.gg/K4Hys7Czzp" rel="nofollow">Discord</a></p>
<p dir="auto">
  <a href="https://dropbase.io/" rel="nofollow">
      <img src="https://camo.githubusercontent.com/ca9c447690ada75cbecab748e5f13c660ae7c404545812f0f39a719c326b3c5b/68747470733a2f2f63646e2e70726f642e776562736974652d66696c65732e636f6d2f3566326338373234366231376663663636323238323539342f3636316630626131336162306262383961313864653032395f61646d696e70616e656c2d6865726f2e77656270" alt="Dropbase hero" data-canonical-src="https://cdn.prod.website-files.com/5f2c87246b17fcf662282594/661f0ba13ab0bb89a18de029_adminpanel-hero.webp">
  </a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overview</h2><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<p dir="auto">Dropbase helps you build and prototype web apps faster with AI. Developers can quickly build anything from admin panels, back-office tools, billing dashboards, and internal engineering tools that can fetch data and trigger action across any internal or external service.</p>
<p dir="auto">Existing low-code/no code tools lack flexibility, confine devs to building app logic by filling up UI forms, and have big learning curves. Dropbase uses AI to generate app code that you can verify and/or edit. We combine the convenience of a drag-and-drop app builder with the flexibility of code, making it easy to build and customize, while learning to use the product as you see how the AI generates code using the Dropbase web framework.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why Dropbase?</h2><a id="user-content-why-dropbase" aria-label="Permalink: Why Dropbase?" href="#why-dropbase"></a></p>
<ol dir="auto">
<li>Write any custom business logic with code.</li>
<li>Built-in web framework with pre-built UI components - no need to hassle with frontend libraries/code.</li>
<li>Local-first, self-hosted. No creds are shared with us.</li>
<li>Dropbase lives in your codebase, making it easy to import or resuse custom scripts/libraries.</li>
<li>It's built on Python and you can import any PyPI package.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Demo</h2><a id="user-content-demo" aria-label="Permalink: Demo" href="#demo"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Build a simple app to search for customer orders and send slack messages</h3><a id="user-content-build-a-simple-app-to-search-for-customer-orders-and-send-slack-messages" aria-label="Permalink: Build a simple app to search for customer orders and send slack messages" href="#build-a-simple-app-to-search-for-customer-orders-and-send-slack-messages"></a></p>
<a href="https://youtu.be/RaxHOjhy3hY" rel="nofollow">
  <img src="https://camo.githubusercontent.com/4f3ea18038e1001a81b5f33c04cb2763b2849448b08e4354bff1578840e41d02/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f526178484f6a68793368592f6d617872657364656661756c742e6a7067" data-canonical-src="https://img.youtube.com/vi/RaxHOjhy3hY/maxresdefault.jpg">
</a>
<p dir="auto"><h3 tabindex="-1" dir="auto">Build an orders app that uses charts</h3><a id="user-content-build-an-orders-app-that-uses-charts" aria-label="Permalink: Build an orders app that uses charts" href="#build-an-orders-app-that-uses-charts"></a></p>
<a href="https://youtu.be/YWtdD7THTxE" rel="nofollow">
  <img src="https://camo.githubusercontent.com/1ff9cd713661b57143185f14d4f126f21718f48d613b47ef8cc530538055ec90/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f59577464443754485478452f6d617872657364656661756c742e6a7067" data-canonical-src="https://img.youtube.com/vi/YWtdD7THTxE/maxresdefault.jpg">
</a>
<p dir="auto"><h2 tabindex="-1" dir="auto">Get Started</h2><a id="user-content-get-started" aria-label="Permalink: Get Started" href="#get-started"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">0. Pre-requisites</h3><a id="user-content-0-pre-requisites" aria-label="Permalink: 0. Pre-requisites" href="#0-pre-requisites"></a></p>
<ul dir="auto">
<li>Install Docker. We strongly recommend using <a href="https://www.docker.com/products/docker-desktop/" rel="nofollow">Docker Desktop</a>, especially if you're on Apple M chips. Alternatively, you can install <code>docker</code> and <code>docker-compose</code>.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">1. Clone the <code>dropbase</code> repo</h3><a id="user-content-1-clone-the-dropbase-repo" aria-label="Permalink: 1. Clone the dropbase repo" href="#1-clone-the-dropbase-repo"></a></p>
<p dir="auto">Clone the Dropbase repository</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/DropbaseHQ/dropbase.git"><pre><span>git</span> <span>clone</span> <span>https</span>:<span>//</span><span>github</span>.<span>com</span><span>/</span><span>DropbaseHQ</span><span>/</span><span>dropbase</span>.<span>git</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">2. Start the server</h3><a id="user-content-2-start-the-server" aria-label="Permalink: 2. Start the server" href="#2-start-the-server"></a></p>
<p dir="auto">Start the server by running start.sh</p>
<p dir="auto"><strong>NOTE:</strong> When starting the server for the first time, make <code>start.sh</code> executable.</p>

<p dir="auto">You can start the server by running</p>

<p dir="auto"><h3 tabindex="-1" dir="auto">3. Create your first Dropbase app</h3><a id="user-content-3-create-your-first-dropbase-app" aria-label="Permalink: 3. Create your first Dropbase app" href="#3-create-your-first-dropbase-app"></a></p>
<p dir="auto">Go to the Dropbase App <code>http://localhost:3030/apps</code> from your browser and click on the <code>Create app</code> button to create your first Dropbase app.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Enabling AI features</h2><a id="user-content-enabling-ai-features" aria-label="Permalink: Enabling AI features" href="#enabling-ai-features"></a></p>
<p dir="auto">Dropbase uses LLM (gpt, sonnet) to provide AI Developer feature. To enable it, add your OpenAI or Anthropic api key into <code>server.toml</code>. Example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="[llm.openai]
api_key = &quot;YOUR_API_KEY&quot;
model = &quot;gpt-4o&quot;"><pre>[llm.openai]
api_key = <span><span>"</span>YOUR_API_KEY<span>"</span></span>
model = <span><span>"</span>gpt-4o<span>"</span></span></pre></div>
<p dir="auto"><strong>IMPORTANT:</strong> If you add additional environmental variables, make sure to add them before LLM configurations (at the top-level table), since LLM configurations are defined as <a href="https://toml.io/en/v1.0.0#table" rel="nofollow">table</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuring Worker</h2><a id="user-content-configuring-worker" aria-label="Permalink: Configuring Worker" href="#configuring-worker"></a></p>
<p dir="auto"><code>worker.toml</code> contains environmental variables for the worker. This includes database sources, API keys, or access token to third party services.</p>
<p dir="auto">To include API keys or tokens, add a name for the token and enter your string token. Though not required, adding a descriptive name helps Dropbase AI infer the key to use.</p>
<div dir="auto" data-snippet-clipboard-copy-content="stripe_key=&quot;rk_test_123&quot;
mailgun_api_key=&quot;abc123&quot;"><pre>stripe_key=<span><span>"</span>rk_test_123<span>"</span></span>
mailgun_api_key=<span><span>"</span>abc123<span>"</span></span></pre></div>
<p dir="auto">To include database sources, use the following format: <code>database</code>.<code>database_type</code>.<code>database_nickname</code></p>
<p dir="auto">For example, if you want to add a <code>postgres</code> database to a list of sources and use <code>my_source</code> as its nickname, add the following:</p>
<div dir="auto" data-snippet-clipboard-copy-content="[database.postgres.my_source]
host = &quot;localhost&quot;
database = &quot;postgres&quot;
username = &quot;username&quot;
password = &quot;password&quot;
port = 5432"><pre>[database.postgres.my_source]
host = <span><span>"</span>localhost<span>"</span></span>
database = <span><span>"</span>postgres<span>"</span></span>
username = <span><span>"</span>username<span>"</span></span>
password = <span><span>"</span>password<span>"</span></span>
port = 5432</pre></div>
<p dir="auto"><strong>NOTE:</strong> The built-in demo requires <code>database.sqlite.demo</code> to be present in <code>worker.toml</code>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gemini Pro refuses to acknowledge yt-dlp (122 pts)]]></title>
            <link>https://twitter.com/adocomplete/status/1811802857022324904</link>
            <guid>40947378</guid>
            <pubDate>Fri, 12 Jul 2024 17:03:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/adocomplete/status/1811802857022324904">https://twitter.com/adocomplete/status/1811802857022324904</a>, See on <a href="https://news.ycombinator.com/item?id=40947378">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Intel is selling defective 13-14th Gen CPUs (131 pts)]]></title>
            <link>https://alderongames.com/intel-crashes</link>
            <guid>40946644</guid>
            <pubDate>Fri, 12 Jul 2024 15:46:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://alderongames.com/intel-crashes">https://alderongames.com/intel-crashes</a>, See on <a href="https://news.ycombinator.com/item?id=40946644">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>My team at Alderon Games, working on the multiplayer dinosaur survival game <a href="https://pathoftitans.com/">Path of Titans</a>, has been encountering significant problems with Intel CPU stability. These issues, including crashes, instability, and memory corruption, are confined to the 13th and 14th generation processors. Despite all released microcode, BIOS, and firmware updates, the problem remains unresolved.</p>
<p>We have identified failures in five main areas:</p>
<ul>
<li>
<strong>End Customers:</strong> Thousands of crashes on Intel CPUs on 13th and 14th Gen CPUs in our crash reporting tools.</li>
<li>
<strong>Official Dedicated Game Servers:</strong> Experiencing constant crashes, taking entire servers down.</li>
<li>
<strong>Development Team:</strong> Developers using these CPUs face frequent instability while building and working on the game. It can also cause SSD and memory corruption.</li>
<li>
<strong>Game Server Providers:</strong> Hosting community servers with persistent crashing issues.</li>
<li>
<strong>Benchmarking Tools:</strong> Decompression and memory tests unrelated to Path of Titans also fail.</li>
</ul>
<p>Over the last 3–4 months, we have observed that CPUs initially working well deteriorate over time, eventually failing. The failure rate we have observed from our own testing is nearly 100%, indicating it's only a matter of time before affected CPUs fail. This issue is gaining attention from news outlets and has been noted by Fortnite and RAD Game Tools, which powers decompression behind Unreal Engine.</p>
<p>Users are also receiving misleading error messages about running out of video driver memory, despite having sufficient memory.</p>
<h2>Actions We Are Taking</h2>
<p>To prevent further harm to our game, we are implementing the following measures:</p>
<ul>
<li>
<strong>Server Migration:</strong> We are swapping all our servers to AMD, which experience 100 times fewer crashes compared to Intel CPUs that were found to be defective.</li>
<li>
<strong>Hosting Recommendations:</strong> We advise anyone hosting Path of Titans servers or selling game servers to avoid purchasing or using 13th and 14th gen Intel CPUs.</li>
<li>
<strong>In-Game Notifications:</strong> We are adding a popup message in-game to inform users with these processors about the issue. Many users are currently unaware of why their game is crashing and what they can do about it.</li>
</ul>
<h2>Resources</h2>
<ul>
<li>
<a href="https://www.epicgames.com/help/en-US/c-Category_Fortnite/c-Fortnite_TechnicalSupport/frequent-crashes-in-fortnite-on-i9-13900k-kf-ks-or-i9-14900k-kf-ks-cpus-a000086852?sessionInvalidated=true">Frequent Crashes in Fortnite on i9-13900K/KF/KS or i9-14900K/KF/KS CPUs</a>
</li>
<li>
<a href="https://www.radgametools.com/oodleintel.htm">RAD Game Tools Intel CPU Issues</a>
</li>
<li>
<a href="https://hardwaretimes.com/pc-gamers-amd-ryzen-intel-13900k-14900k-crash-fail/">PC Gamers are Switching to AMD Ryzen as Intel 13900K/14900K Chips Continue to Crash &amp; Fail</a>
</li>
<li>
<a href="https://www.techspot.com/review/2836-intel-cpu-crash-baseline-spec/">Intel CPUs Are Crashing and It's Intel's Fault: Intel Baseline Profile Benchmark</a>
</li>
</ul>
<p>We look forward to more information becoming available about these problems.</p>
<p>For Intel's sake, we hope they recall these CPUs and refund consumers. This post isn't a endorsement of AMD CPUs or any other PC company. Keep in mind any product can have defects and issues, we just want to let you know where these crashes are coming from and what is going on.</p>
<p>By Matthew Cassells</p>
<p>Founder of Alderon Games</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tau: Open-source PaaS – A self-hosted Vercel / Netlify / Cloudflare alternative (390 pts)]]></title>
            <link>https://github.com/taubyte/tau</link>
            <guid>40946033</guid>
            <pubDate>Fri, 12 Jul 2024 14:41:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/taubyte/tau">https://github.com/taubyte/tau</a>, See on <a href="https://news.ycombinator.com/item?id=40946033">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p><a href="https://discord.gg/KbN3KN7kpQ" rel="nofollow">
    <img src="https://github.com/taubyte/tau/raw/main/images/discord-btn.png" alt="Join our Discord" height="30">
  </a>
  <a href="https://tau.how/" rel="nofollow">
    <img src="https://github.com/taubyte/tau/raw/main/images/docs-btn.png" alt="Read the Docs" height="30">
  </a>
  <a href="https://console.taubyte.com/" rel="nofollow">
    <img src="https://github.com/taubyte/tau/raw/main/images/sandbox-btn.png" alt="Try our Sandbox" height="30">
  </a>
</p>
<br>
<div dir="auto">
  <a href="https://taubyte.com/" rel="nofollow">
    <themed-picture data-catalyst-inline="true"><picture>
      <source media="(prefers-color-scheme: dark)" srcset="https://github.com/taubyte/tau/raw/main/images/logo-with-text-tau-white.png">
      <img width="160" src="https://github.com/taubyte/tau/raw/main/images/logo-with-text-tau-black.png" alt="Tau Logo">
    </picture></themed-picture>
  </a>
  
  <p dir="auto"><a href="https://github.com/taubyte/tau/releases"><img src="https://camo.githubusercontent.com/ebb1a5c3e4b55473746c5de17b99da02cab50ea1679c9d7d76e56d4d03904a1f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f746175627974652f7461752e737667" alt="Release" data-canonical-src="https://img.shields.io/github/release/taubyte/tau.svg"></a>
<a href="https://github.com/taubyte/tau/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/9a7bccf3d134b27ddedef3543cff4930fdd5744a7249f17e6550aa9f111ea310/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f746175627974652f746175" alt="License" data-canonical-src="https://img.shields.io/github/license/taubyte/tau"></a></p>
  <p><strong>
  <p dir="auto"><h2 tabindex="-1" dir="auto">Open Source Git-Native CDN PaaS</h2><a id="user-content-open-source-git-native-cdn-paas" aria-label="Permalink: Open Source Git-Native CDN PaaS" href="#open-source-git-native-cdn-paas"></a></p>
  </strong>
</p></div>

<hr>

<p dir="auto">Tau is a framework for building low maintenance &amp; highly scalable cloud computing platforms that software developers will love!</p>
<p dir="auto"><code>tau</code> is a single binary with no external dependencies except standard system libraries. On top of that, it requires minimal configuration. These are the main steps:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Install Tau</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="curl https://get.tau.link/tau | sh"><pre>curl https://get.tau.link/tau <span>|</span> sh</pre></div>
</li>
<li>
<p dir="auto"><strong>Configure</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="tau config generate -n yourdomain.com -s compute --services all --ip your_public_ip"><pre>tau config generate -n yourdomain.com -s compute --services all --ip your_public_ip</pre></div>
</li>
<li>
<p dir="auto"><strong>Launch</strong></p>

</li>
</ol>
<p dir="auto">For a complete step-by-step guide, refer to <a href="https://tau.how/01-getting-started/04-deploy-a-cloud/" rel="nofollow">Deploy tau</a>.</p>
<p dir="auto">Building <code>tau</code> youself is a straightforward <code>go build</code> given you have Go installed.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Background</h2><a id="user-content-background" aria-label="Permalink: Background" href="#background"></a></p>
<p dir="auto">The cost and time required to build software, take it from the development environment to production, and then scale it effectively to meet end-user demand are extremely high.</p>
<p dir="auto">Developer-friendly platforms, like the major cloud computing providers, are expensive, lock users in, and overlook local development and E2E testing.</p>
<p dir="auto">This is really a two-sided problem. Do you save on infrastructure cost, or do you lower development time?</p>
<p dir="auto">If you invest in your own platform, it's a rocky road that impedes the speed of development and generally ends up costing more. We all know the Kubernetes fairy tale does not end well!</p>
<p dir="auto">If you invest in development speed, you're limited by your provider's features and cost.</p>
<p dir="auto">To us, solving this problem means:</p>
<ul dir="auto">
<li>Giving you, or your very small team, the ability to build and maintain a cloud computing platform that will go head-to-head with the ones backed by thousands of engineers.</li>
<li>Setting software developers free from infrastructure and operational constraints. We refer to this as "Local Coding Equals Global Production."</li>
</ul>
<p dir="auto"><code>tau</code> solves for building and maintaining a cloud computing platform, and also provides the foundations for an amazing developer experience.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Minimal Configuration</h2><a id="user-content-minimal-configuration" aria-label="Permalink: Minimal Configuration" href="#minimal-configuration"></a></p>
<p dir="auto">One of the reasons <code>tau</code> requires minimal configuration is because it has built-in auto-discovery.
Just like a self-driving car gathering information through sensors, <code>tau</code> will gather information and try to find the best ways to be reachable, available, etc.</p>
<p dir="auto">That said, some configuration like bootstrap peers is necessary. Unless you're running a single-node cloud, each node will need to know at least one other peer.</p>
<p dir="auto">A Cloud built with <code>tau</code> is very dynamic; at a low level, nodes communicate assets, routes, and services, and they also exchange information about other peers. Enriched by distributed services like <code>seer</code> and <code>gateway</code>, the cloud can load-balance incoming requests to ensure optimal performance and reliability.</p>
<p dir="auto">This behavior is built into cloud resources as well. For example, a protocol we call <code>hoarder</code> ensures object storages and databases are replicated; all you need to do is enable it on a few nodes.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Local Coding Equals Global Production</h2><a id="user-content-local-coding-equals-global-production" aria-label="Permalink: Local Coding Equals Global Production" href="#local-coding-equals-global-production"></a></p>
<p dir="auto">In your traditional setup, the platform is a complex set of templates, pipelines, and integrations that ultimately help turn configuration into API calls and code into assets. Because of that complexity, and also the fact that many components need to run inside a very complex environment of their own, it's impossible to satisfy the 'local == production' equation.</p>
<p dir="auto">Granted, there are some solutions that either mock or reroute to dev/prod resources, enabling developers to build or debug locally. However, it's still a 3rd party service you need to integrate and manage.</p>
<p dir="auto">In order to satisfy the equation, we decided to build <code>tau</code> so it simplifies, ports, and/or sandboxes every aspect of the cloud.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Git-Native</h2><a id="user-content-git-native" aria-label="Permalink: Git-Native" href="#git-native"></a></p>
<p dir="auto">Traditionally, you interface with infrastructure through API calls. This is the case for every cloud computing provider alongside orchestration solutions like Kubernetes.</p>
<p dir="auto">A few years back, the concept of GitOps started to make waves, and that was around the time we started building, so we decided to cut the unnecessary garbage between the definition of a cloud resource, which should be stored in Git, and its instantiation.</p>
<p dir="auto">As a result, <code>tau</code> has no API calls to create a serverless function, for example. Instead, it adopts Git as the only way to alter infrastructure.</p>
<p dir="auto">Also, git being core to <code>tau</code> means that nodes in the cloud do tune to a specific branch, by default main or master. Among what it enables is an easy way to set up development environments, for example.</p>
<p dir="auto">A specific use case is local development in which case <a href="https://github.com/taubyte/tau/tree/main/tools/dream">dream-cli</a> nodes can also be tuned to the current branch.</p>
<p dir="auto">In addition to the nodes being on a branch, the application registry, managed by the 'tns' protocol, uses commit ids to version entries, allowing nodes serving the assets to detect new versions, or a roll-back for that matter.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Networking</h2><a id="user-content-networking" aria-label="Permalink: Networking" href="#networking"></a></p>
<p dir="auto">Internally, <code>tau</code>, using <a href="https://github.com/libp2p/go-libp2p">libp2p</a>, builds an overlay peer-to-peer network between the nodes, enabling some pretty cool features like:</p>
<ul dir="auto">
<li>Automatic node and protocol discovery &amp; routing. If, for example, a node is down, changes its IP address/port, or the services it supports, other nodes will update the info automatically.</li>
<li>Transport independent. Nodes can use any combination of TCP/IP, WebSocket, QUIC, and more.</li>
<li>NAT Traversal &amp; Circuit Relay, which allow nodes that are not public to be part of the cloud.</li>
</ul>
<p dir="auto">Unless absolutely required, which is extremely rare, no well-designed software should rely on IP addresses and ports. This is why every <code>tau</code> cloud is identified with an FQDN (i.e., enterprise.starships.ws) so no absolute network reference is used in an application. Under the hood, the Cloud will transparently take care of DNS resolution and HTTP load balancing, eliminating the need to set these up.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Storage</h2><a id="user-content-storage" aria-label="Permalink: Storage" href="#storage"></a></p>
<p dir="auto">In every other cloud computing implementation, storage means a location and generally a path. For example, <code>https://tau.how/assets/logo-w.svg</code> has two main components <code>tau.how</code>, which translates to an IP address and a location, and <code>/assets/logo-w.svg</code>, which is a path relative to the location. This way of addressing, called "location-based addressing," is simply not portable. Why? you might ask. Well, for starters, nothing guarantees the data returned is an SVG logo in this case. The other issue is the <code>tau.how</code> host we connected to might not have it.</p>
<p dir="auto">To solve this issue, <code>tau</code> uses content-addressing, a concept introduced by torrent networks and popularized by <a href="https://github.com/taubyte/tau/blob/main">IPFS</a>.</p>
<p dir="auto">So when you request <code>https://tau.how/assets/logo-w.svg</code>, which is actually hosted by a <code>tau</code> Cloud, the host that handles the request will resolve (<code>host=tau.how, path=/assets/logo-w.svg</code>) to a content address, or CID, then retrieve the content reader and then forward it through an HTTP writer to you.</p>
<p dir="auto">A few cool facts about this approach:</p>
<ul dir="auto">
<li>Content is chunked and then stored in a DAG, which means it's deduplicated.</li>
<li>Content can be downloaded from multiple peers in parallel.</li>
<li>Content can be verified as the CID is its hash.</li>
<li>When content is in demand, the cloud automatically dedicates more peers to its distribution.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Computing</h2><a id="user-content-computing" aria-label="Permalink: Computing" href="#computing"></a></p>
<p dir="auto">As of today, <code>tau</code> supports <a href="https://webassembly.org/" rel="nofollow">WebAssembly</a> for computing. The reason we started with it is that it's highly portable and sandboxed. We support containers for CI/CD but not for computing yet. We're working on a way to implement containers and virtual machines while abiding by our principles of portability and sandboxing.</p>
<p dir="auto">Code, binary, images, along with any attached assets, are stored and retrieved using the same principles described in <a href="#storage">Storage</a>, which considerably reduces provisioning time and brings computing close to data (data gravity) and/or user (edge computing).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">E2E Testing</h2><a id="user-content-e2e-testing" aria-label="Permalink: E2E Testing" href="#e2e-testing"></a></p>
<p dir="auto">If you're looking to create E2E tests for projects hosted on <code>tau</code>, you can use <code>dream</code>, a sub-package within <code>tau</code>. We don't have documentation for it yet, but you can quickly learn from tests like <a href="https://github.com/taubyte/tau/blob/main/services/seer/tests/dns_test.go#L35">services/seer/tests/dns_test.go</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Running a Local Cloud</h2><a id="user-content-running-a-local-cloud" aria-label="Permalink: Running a Local Cloud" href="#running-a-local-cloud"></a></p>
<p dir="auto">While you can't practically run <code>tau</code> on your local machine, you can do so using <a href="https://github.com/taubyte/tau/tree/main/tools/dream">dream-cli</a>, which is a CLI wrapper around <code>dream</code>. It creates local cloud environments mirroring production settings. Unlike <code>tau</code>, it offers an API for real-time configuration and testing.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Extending Tau</h2><a id="user-content-extending-tau" aria-label="Permalink: Extending Tau" href="#extending-tau"></a></p>
<p dir="auto"><code>tau</code> can be extended using a plugin system we call <a href="https://github.com/taubyte/tau/tree/main/pkg/vm-orbit">orbit</a>. An open-source example is <a href="https://github.com/ollama-cloud">ollama-cloud</a>, which demonstrates how to add LLM capabilities to your cloud.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Documentation</h2><a id="user-content-documentation" aria-label="Permalink: Documentation" href="#documentation"></a></p>
<p dir="auto">To learn more, check:</p>
<ul dir="auto">
<li><a href="https://taubyte.com/blog/introduction-to-taubyte/" rel="nofollow">Introduction to Taubyte</a></li>
<li><a href="https://taubyte.com/blog/be-competitive-in-few-minutes/" rel="nofollow">Be Competitive in a Few Minutes: Deployment Guide</a></li>
</ul>
<p dir="auto">For comprehensive documentation, visit our <a href="https://tau.how/" rel="nofollow">documentation</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Support</h2><a id="user-content-support" aria-label="Permalink: Support" href="#support"></a></p>
<p dir="auto">Questions or need assistance? Ping us on <a href="https://discord.com/invite/KbN3KN7kpQ" rel="nofollow">Discord</a>!</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Born into slavery, he rose to the top of France's art world (106 pts)]]></title>
            <link>https://www.washingtonpost.com/entertainment/art/2024/07/12/guillaume-lethiere-exhibition-life/</link>
            <guid>40945355</guid>
            <pubDate>Fri, 12 Jul 2024 13:24:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.washingtonpost.com/entertainment/art/2024/07/12/guillaume-lethiere-exhibition-life/">https://www.washingtonpost.com/entertainment/art/2024/07/12/guillaume-lethiere-exhibition-life/</a>, See on <a href="https://news.ycombinator.com/item?id=40945355">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="WXAXOKXTBVHJVOK4D2ZWD2SLQY" data-el="text" dir="null">WILLIAMSTOWN, Mass. — During the most tumultuous period in France’s modern history, Guillaume Lethière was one of its most venerated artists. His story is epic. Charles Dickens or Alexandre Dumas (who delivered a eulogy at Lethière’s funeral) would have struggled to make it sound credible. Pity me, your poor reviewer.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="O5HNZZMHM5EKRHCH3IAITG3OSY" data-el="text" dir="null">He was the third child (“Le Thière” is French for “the third”) of an enslaved, mixed-race woman and a White plantation owner. Today, his paintings — some of them cinematic in scale — can be found in museums in the United States and Europe, including the Louvre, and also in Port-au-Prince, Haiti. Among his smaller works is one of the most tender and beautiful portraits I know.</p></div><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="TVHSXV7FVJCGXKREE2UKUFIKOQ" data-el="text" dir="null">Don’t feel bad if you’ve never heard of him. But be aware that in Guadeloupe, where he was born in 1760, Lethière has long been celebrated. According to Esther Bell, the curator of an extraordinary new <a href="https://www.clarkart.edu/exhibition/detail/guillaume-lethiere" target="_blank">exhibition </a>about Lethière, there is an auto-body repair shop in the coastal town of Sainte-Anne bearing the name “Guillaume Lethière.” Nearby, in the center of a busy rotary in the French neighborhood — previously the site of the plantation where<b> </b>Lethière grew up — is a huge steel sculpture in the shape of an artist’s palette alongside two enormous paintbrushes. Shapes cut out of the steel reveal the face of Lethière as he looked in an 1815 drawing by his pupil, the great neoclassical artist Jean-Auguste-Dominique Ingres.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="F5OKJVBTVJAGJDCZX2GK6R2BSY" data-el="text" dir="null">This summer, you might see Lethière’s loveliest portrait (scholars think it probably depicts his stepdaughter, Eugénie Servières, herself an accomplished artist) blown up on highway billboards advertising “Guillaume Lethière” at the Clark Art Institute in Williamstown, Mass., through Oct. 14. The exhibition will travel to the Louvre in November.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="V45IDFDUWRBTVCCH4P5HPYIEMY" data-el="text" dir="null">Researched and developed over many years by Bell, the Clark’s deputy director and chief curator, with Olivier Meslay, the museum’s director, and accompanied by a 432-page catalogue, the exhibition tells the story of Lethière’s improbable life.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="7VSJFCA5FFEY7KHC4KTY43EUTU" data-el="text" dir="null">To understand his significance, it’s not enough just to look at his paintings and drawings — although these are very good and earned him accolades aplenty during his lifetime. You need to consider his own complicated proximity to the world-historical events through which he lived.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="63RS7XEHOBHBDD6CS5AWB3MSVA" data-el="text" dir="null">Born into slavery (or so it’s assumed, given his parentage and the telling absence of baptismal records), Lethière was brought to France by his father, the French king’s public prosecutor in Guadeloupe, in 1774, when he was 14. He began training as an artist in Rouen. Thanks to his father’s influence, he was already close to serious power by his late teens.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="EOUR5NB6CFDLVI72ILOQOM7W2U" data-el="text" dir="null">But of course, staying close to power is not easy when the personnel keeps changing. Like others of his generation, Lethière had to steer a course through the last days of the Ancien Régime, the French Revolution, the Terror, the rise of Napoleon Bonaparte, European conquest, imperial collapse, a brief Bonapartist revival, a restored monarchy, and finally, just before Lethière’s death in 1832, a constitutional monarchy.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="5P5AP3VPJBGT3GBJ3BV5YG2FUA" data-el="text" dir="null">What makes him uniquely interesting is that he managed all this while also navigating the shifting implications of his illegitimate, mixed-race origins in Guadeloupe.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="M4E6DNINXRHXRJ6SXDZ2YKLJW4" data-el="text" dir="null">Lethière was neither smarmy nor sycophantic, but he knew how to ingratiate himself to others. He “won the esteem and friendship of everyone by his honesty, his politeness, and a frank and loyal character that never wavered,” wrote Francois-Guillaume Ménageot, the director of the French Academy.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="JU5KHZAIUJDCBMNSJNVH3TKYSM" data-el="text" dir="null">Lethière and his mother, Marie-Françoise Pepeye, were both emancipated by his father, Pierre Guillon. But it was many years before changes to the law allowed Guillon to recognize Lethière as his son. Lethière and his sister were named as Guillon’s heirs around the time Napoleon seized power in 1799.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="JSORK6JGTZBKVGKOB4RRFGQFHA" data-el="text" dir="null">Even so, years later, Lethière had to defend himself against an embarrassing challenge by a distant cousin, who claimed he was the rightful heir. This was in 1819, when the artist was at the height of his renown. The courts eventually found in Lethière’s favor — but not before humiliating references in the press to the esteemed painter’s “naive and modest genealogy.”</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="BQXUR5ZH6VCQRBEL5U3OFRCZQA" data-el="text" dir="null">Moral and political complexities choked almost every aspect of Lethière’s life. There’s no doubt, for instance, that he was an abolitionist. And yet he benefited financially from his father’s plantation, which depended on enslaved labor.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="QYYS6J4HAJEKZB3NBOL4YXRZY4" data-el="text" dir="null">Although Lethière never returned to the Caribbean, he cared deeply about the fate of its people. He supported the revolution in Haiti, which began in 1791, just before the French monarchy was abolished, and welcomed the French government’s decision, in 1794, to end slavery in all its territories.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="R3NQKS3M6FFRTGK2K6GN3S3Q2Q" data-el="text" dir="null">When, eight years later, Napoleon  reinstated slavery in the colonies, brutally suppressing an attempt at resistance in Guadeloupe, Lethière was surely disappointed. But by now he was in with the Bonapartes. He painted portraits of, among others, Napoleon’s Caribbean-born wife, the Empress Joséphine, and hitched his fortunes to Lucien Bonaparte, Napoleon’s brother.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="F6J3KOWYYND7XNXRXPZ7CNWUUA" data-el="text" dir="null">In 1807, Lethière’s friendship with Lucien Bonaparte led directly to his appointment as director of the French Academy in Rome — an immensely prestigious post. There he reinvigorated the academy and<b> </b>oversaw the training of dozens of France’s best artists — among them Ingres, who made a series of stunning <a href="https://www.themorgan.org/drawings/item/109863" target="_blank">drawings </a>of Lethière’s family (included in the show), and a female pupil, <a href="https://nmwa.org/art/artists/antoine-cecile-hortense-haudebourt-lescot/" target="_blank">Antoinette Cécile Hortense Lescot</a>, who went on to exhibit more than 100 paintings in the Paris Salon.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="BVJX3NBA4RDGPDBFBBP3OPCANA" data-el="text" dir="null">Ancient Rome was of intense interest not only to France’s revolutionaries, who looked to republican Rome as a model, but also to Napoleon, who of course saw more upside for himself in Rome’s imperial period. Art played a huge role in establishing these lines of pedigree.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="6TZINEYDOREDZDE4RQ635KIUCM" data-el="text" dir="null">The French Revolution had broken out while Lethière was a student at the same academy in Rome. At the time, inspired by his environs, he worked on a major canvas, “Brutus Condemning His Sons to Death.” In a carefully structured, frieze-like composition, he depicted the founder of the Roman republic, Lucius Junius Brutus, looking on stoically as his sons, who had plotted to restore a monarchy, are decapitated.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="A7XF45IVFZH5NC6U6AYLLBSWDE" data-el="text" dir="null">Lethière returned repeatedly to this subject and to another episode from ancient Rome,  “The Death of Virginia.” We can perhaps imagine the painting’s special significance for him when we understand that its subject — a father killing his daughter, at her own request — hinges on the dishonor of being enslaved.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="S5ZPM5V6O5FDDHAPPEHZID2XQA" data-el="text" dir="null">Versions of both paintings enjoyed great success when they were exhibited in Rome and London. But in Paris, tastes were changing, and by the 19th century’s second decade, romanticism was on the rise. Lethière’s neoclassical style began to fall out of favor.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="QHKZUOINCZBA3DUPQYZY4ENUEY" data-el="text" dir="null">Winning the 1819 inheritance case seems to have inspired Lethière to turn his attention back to the Caribbean, and in 1822 he painted one of his most audacious canvases — an enormous (approximately 11 by 7 feet) painting owned by the Musée du Panthéon National Haitien in Port-au-Prince. It shows two generals, one mixed-race and the other Black, swearing an oath to fight together for the freedom and independence of the people of Saint-Domingue (now Haiti).</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="FX7K3HNEVNDRVCS2VVZCWGVS4I" data-el="text" dir="null">After a risky and clandestine sea voyage, Lethière’s son personally delivered the painting to Haiti’s President Jean-Pierre Boyer in Port-au-Prince. Two years later, France’s Charles X grudgingly recognized Haiti — but only in return for an indemnity payment that would cripple the young nation for decades.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="RQUTT5FJTNERVOJBR5ISA3XBNQ" data-el="text" dir="null">Unfortunately, the <a href="https://www.washingtonpost.com/world/2024/03/12/haiti-gang-violence-crisis-explained/?itid=lk_inline_manual_42" target="_blank">recent civil strife in Haiti</a> has prevented the painting from traveling to the United States. Lethière himself intended the painting for a Haitian audience and, according to Bell, who has tastefully installed a reproduction of it in the exhibition, it “encapsulates Lethière’s fidelity to his place of origin.”</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="6UVTDO3T2RDGBPQCXIBCYDOBN4" data-el="text" dir="null">The Clark show immerses us in several decades of political tumult that continue to reverberate today. It has much to say about other French artists and writers with ties to the Caribbean. So it is much more than just a monographic exhibition. For all the stately arrangement of the Clark’s galleries and the superficial stiffness of Lethière’s neoclassical style, the exhibit is like a pinwheeling firecracker, blazing out light, knowledge and cultural energy, and deepening our understanding of a remarkable inheritance.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="BWVGRUM255C6TLOA3SGJJY4C3M" data-el="text" dir="null"><i><b>Guillaume Lethière</b></i><i> Through Oct. 14 at the Clark Art Institute in Williamstown, Mass., and then at the Louvre in Paris from Nov. 13 through Feb. 17. </i><a href="https://www.clarkart.edu/museum/overview" target="_blank"><i>clarkart.edu</i></a><i>.</i></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Windows NT for Power Macintosh (282 pts)]]></title>
            <link>https://github.com/Wack0/maciNTosh</link>
            <guid>40945076</guid>
            <pubDate>Fri, 12 Jul 2024 12:51:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Wack0/maciNTosh">https://github.com/Wack0/maciNTosh</a>, See on <a href="https://news.ycombinator.com/item?id=40945076">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Windows NT for Power Macintosh</h2><a id="user-content-windows-nt-for-power-macintosh" aria-label="Permalink: Windows NT for Power Macintosh" href="#windows-nt-for-power-macintosh"></a></p>
<p dir="auto">This repository currently contains the source code for the ARC firmware and its loader, targeting New World Power Macintosh systems using the <em>Gossamer</em> architecture (that is, MPC106 "Grackle" memory controller and PCI host, and "Heathrow" or "Paddington" super-I/O chip on the PCI bus). That is, the following systems:</p>
<ul dir="auto">
<li>iMac G3 (tray-loading)</li>
<li>Power Macintosh G3 (Blue &amp; White) <em>"Yosemite"</em></li>
<li>Macintosh PowerBook G3 Bronze Keyboard <em>"Lombard"</em></li>
<li>Power Macintosh G4 PCI <em>"Yikes!"</em></li>
</ul>
<p dir="auto">The ARC firmware itself runs at a low enough level that it should be compatible with Old World systems using the same chipset too, but there is currently no loader for these systems; these are the following:</p>
<ul dir="auto">
<li>Power Macintosh G3 (beige)</li>
<li>Macintosh PowerBook G3 Series <em>"Wallstreet"</em>, <em>"PDQ"</em></li>
</ul>
<p dir="auto">There may be issues on your hardware; with real hardware, this has only been tested on a Lombard.</p>
<p dir="auto">NT HAL and drivers have no source present for now.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Drivers present in ARC firmware</h2><a id="user-content-drivers-present-in-arc-firmware" aria-label="Permalink: Drivers present in ARC firmware" href="#drivers-present-in-arc-firmware"></a></p>
<ul dir="auto">
<li>Cuda and PMU (albeit Cuda is untested on real hardware)
<ul dir="auto">
<li>ADB keyboard</li>
</ul>
</li>
<li>Flat 32bpp video framebuffer, set up by the loader. Currently the loader only supports ATI hardware (there may be issues with any ATI hardware with fcode version prior to 1.69, only the ATI Rage Pro LT (as present in Lombard) has been tested)</li>
<li>Mac I/O internal IDE controllers, forked from OpenBIOS (<strong>there are no drivers for PCI IDE controllers!</strong>)</li>
<li>USB OHCI forked from OpenBIOS (<strong>broken, nonworking, and initialisation code commented out</strong>)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Drivers currently done for NT</h2><a id="user-content-drivers-currently-done-for-nt" aria-label="Permalink: Drivers currently done for NT" href="#drivers-currently-done-for-nt"></a></p>
<ul dir="auto">
<li>HAL, including: NT boot time framebuffer, super I/O interrupt controller, Grackle PCI bus support, Cuda and PMU (including low level ADB), serial port for kernel debugging only
<ul dir="auto">
<li>(please note Cuda support is currently untested on real hardware)</li>
</ul>
</li>
<li>Mac I/O internal IDE controller, forked from <code>atapi.sys</code> from NT4 DDK</li>
<li>General HID/storage driver, intended to also contain a USB stack in future but currently only implements ADB keyboard/mouse and ramdisk as floppy drive for installing drivers at text setup time</li>
<li>Flat 32bpp video framebuffer miniport driver</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Software compatibility</h2><a id="user-content-software-compatibility" aria-label="Permalink: Software compatibility" href="#software-compatibility"></a></p>
<p dir="auto">NT4 only, currently. NT 3.51 may become compatible if HAL and drivers get ported to it. NT 3.5 will never be compatible, as it only supports PowerPC 601.
(The additional suspend/hibernation features in NT 3.51 PMZ could be made compatible in theory but in practise would require all of the additional drivers for that to be reimplemented.)</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installing</h2><a id="user-content-installing" aria-label="Permalink: Installing" href="#installing"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Preliminary</h3><a id="user-content-preliminary" aria-label="Permalink: Preliminary" href="#preliminary"></a></p>
<ul dir="auto">
<li>Grab binaries from the releases page. Burn the image to optical media.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Partitioning Disk</h3><a id="user-content-partitioning-disk" aria-label="Permalink: Partitioning Disk" href="#partitioning-disk"></a></p>
<ul dir="auto">
<li>Boot your PowerMac from the burned optical media. When you get to ARC firmware menu, go to <code>Run firmware setup</code>, then <code>Repartition disk for NT installation</code>.</li>
<li>The disk partitioner will first let you enter partition size of the NT partition (up to the 16383x16x63 CHS limit, minus 32 MB ARC system partition + 1 MB for partition tables / MBR backup / OS 9 drivers / ARC environment variable storage, giving a maximum possible size of 8030 MB), then will drop to a menu allowing the creation of additional Mac partitions.
<ul dir="auto">
<li>After adding a partition to the list, the only way to remove from the list is by cancelling the operation and starting the partitioner again.</li>
</ul>
</li>
<li>After you have created all Mac partitions you want, choose <code>Finish partitioning and install</code>, and confirm the operation.</li>
<li>When finished, the partitioner will ask to <code>Press any key to restart</code>. Do so, and boot your PowerMac from the CD again.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Installing NT</h3><a id="user-content-installing-nt" aria-label="Permalink: Installing NT" href="#installing-nt"></a></p>
<ul dir="auto">
<li>Eject CD and insert your NT4 CD.</li>
<li>Go to <code>Run a program</code> and enter the path <code>cd:\ppc\setupldr</code> - this may be <code>cd01:</code> or <code>cd02:</code> (...) if you have multiple optical drives present on your system.
<ul dir="auto">
<li>This may error with <code>The file or device does not exist</code>, just go back to <code>Run a program</code> and try again if so.</li>
</ul>
</li>
<li>NT4 setupldr will start.
<ul dir="auto">
<li>You will receive the message <code>Setup could not determine the type of computer you have</code>.</li>
<li>Choose <code>Other</code> (default selected option), just press <code>Enter</code> when asked for hardware support disk.</li>
<li>Pick your system from the list - all are equivalent and will load the Gossamer chipset HAL <code>halgoss</code>.</li>
</ul>
</li>
<li>Next you will receive the message <code>Setup could not determine the type of one or more mass storage drivers installed in your system</code>. Two drivers need to be loaded at this point:
<ul dir="auto">
<li>press <code>S</code> to pick a driver, choose <code>Other</code> from the list, press <code>Enter</code> when asked for hardware support disk</li>
<li>Choose the first driver <code>Mac I/O IDE Controller</code></li>
<li>follow the previous steps again, but this time choose the second driver <code>PowerMac General HID &amp; Storage</code></li>
<li>finally, press Enter to continue</li>
</ul>
</li>
<li>You will receive the message <code>Setup could not determine the type of video adapter installed in the system</code>. Choose <code>Other</code> from the list, press <code>Enter</code> when asked for hardware support disk, and choose the only option <code>Open Firmware Frame Buffer</code>.</li>
<li>NT will boot and text setup will start. Go through the text setup.</li>
<li>Under <code>Setup has determined that your computer contains the following hardware and software components</code>, change <code>Keyboard</code> from <code>Unknown</code> to <code>XT, AT or Enhanced Keyboard (83-104 keys)</code> and <code>Pointing Device</code> from <code>Unknown</code> to <code>No Mouse or Other Pointing Device</code>.</li>
<li>Choose the <code>C:</code> drive from the partition list. If you chose to create an NT partition of size 2GB or less, it must be formatted.</li>
<li>If you chose to create an NT partition of over 2GB in size, <code>chkdsk</code> will find errors and require a reboot. Boot your PowerMac from the ARC firmware CD again and follow the steps to boot the NT4 text setup again.</li>
<li>Proceed through the rest of NT text and graphical setup as normal.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Known issues</h2><a id="user-content-known-issues" aria-label="Permalink: Known issues" href="#known-issues"></a></p>
<ul dir="auto">
<li>If you are looking for a stable operating system, this is not it. Expect bugchecks, expect graphical setup to fail and restart because of bugchecks, etc.
<ul dir="auto">
<li>On a laptop system you may wish to remove the battery. At least on Lombard, the only way to power off the system when it bugchecks is via PMU reset or via total power removal.</li>
</ul>
</li>
<li>Currently the implemented drivers are the bare minimum to run and use NT.</li>
<li>I have observed PMU hard shutdowns on NT boot, fixed only by a PMU reset. No idea what caused this.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Dualboot quirks</h2><a id="user-content-dualboot-quirks" aria-label="Permalink: Dualboot quirks" href="#dualboot-quirks"></a></p>
<p dir="auto">If you create additional Mac partitions, please make note of the following:</p>
<ul dir="auto">
<li>The Mac partitions are listed in the partition table as HFS partitions but are not formatted. Use Disk Utility from OS X 10.1 or above to format the partitions. (Erase the <strong>volumes</strong>, not the <strong>drive</strong>!)</li>
<li>The OS X installer, and just booting OS 8/OS 9, will error if a valid MBR is present on the disk at all, which is required for NT. In ARC firmware, go to <code>Run firmware setup</code> then <code>Reboot to OSX install or OS8/OS9</code> if you wish to boot to those listed operating systems.
<ul dir="auto">
<li>Booting back to the ARC firmware will fix the MBR, so be sure to always use this option when unsure.</li>
<li>In particular, formatting the created HFS partitions in OS X 10.2 and 10.3 will not work when a valid MBR is present!</li>
</ul>
</li>
<li>To allow OS 9 to mount the hard disk, boot from an OS 9 CD, run Drive Setup, select the drive and use the <code>Update Driver</code> option from the <code>Functions</code> menu.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Building ARC firmware</h2><a id="user-content-building-arc-firmware" aria-label="Permalink: Building ARC firmware" href="#building-arc-firmware"></a></p>
<p dir="auto">You need devkitPPC. Additionally, a <code>libgcc.a</code> compiled for <code>powerpcle</code> must be present in <code>arcgrackle/gccle</code>. If you need to find one, it should be present on any Void Linux mirror, the current filename to search for as of 2024-07-12 is <code>cross-powerpcle-linux-gnu-0.34_1.x86_64.xbps</code> - decompress it by <code>zstdcat cross-powerpcle-linux-gnu-0.34_1.x86_64.xbps -o cross-powerpcle-linux-gnu-0.34_1.x86_64.tar</code>, then pull the file out of the tarball: <code>usr/lib/gcc/powerpcle-linux-gnu/10.2/libgcc.a</code>.</p>
<ul dir="auto">
<li>Ensure <code>DEVKITPPC</code> environment variable is set to your devkitPPC directory, usually <code>/opt/devkitpro/devkitPPC</code></li>
<li>Build the big endian libc: <code>cd baselibc ; make ; cd ..</code></li>
<li>Build the ARC firmware loader: <code>cd arcloader_grackle ; make ; cd ..</code></li>
<li>Build the little endian libc: <code>cd arcgrackle/baselibc ; make ; cd ../..</code></li>
<li>Build the ARC firmware itself: <code>cd arcgrackle ; make ; cd ..</code></li>
</ul>
<p dir="auto">Replace <code>stage1.elf</code> and <code>stage2.elf</code> inside the release image. For recreating the image from a folder dump, use your preferred tool to create a hybrid HFS+ISO image, make sure <code>System</code> folder is blessed and <code>BootX</code> file is of type <code>tbxi</code>.</p>
<p dir="auto">Please note that <code>stage1.elf</code> must not be larger than 16KB and <code>stage2.elf</code> must not be larger than 224KB.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgements</h2><a id="user-content-acknowledgements" aria-label="Permalink: Acknowledgements" href="#acknowledgements"></a></p>
<ul dir="auto">
<li>libc used is <a href="https://github.com/PetteriAimonen/Baselibc">baselibc</a></li>
<li>ELF loader and makefiles adapted from <a href="https://github.com/fail0verflow/hbc">The Homebrew Channel</a></li>
<li>Some lowlevel powerpc stuff, and ARC firmware framebuffer console implementation and font, adapted from <a href="https://github.com/devkitPro/libogc">libogc</a></li>
<li>Some ARC firmware drivers (IDE, USB) adapted from <a href="https://github.com/openbios/openbios">OpenBIOS</a>
<ul dir="auto">
<li>USB drivers in OpenBIOS were themselves adapted from <a href="https://github.com/coreboot/coreboot">coreboot</a></li>
</ul>
</li>
<li>ISO9660 FS implementation inside ARC firmware is <a href="https://github.com/erincandescent/lib9660">lib9660</a> with some modifications.</li>
<li>FAT FS implementation inside ARC firmware is <a href="http://elm-chan.org/fsw/ff/00index_p.html" rel="nofollow">Petit FatFs</a> with some modifications.</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AT&T says criminals stole phone records of 'nearly all' customers in data breach (783 pts)]]></title>
            <link>https://techcrunch.com/2024/07/12/att-phone-records-stolen-data-breach/</link>
            <guid>40944505</guid>
            <pubDate>Fri, 12 Jul 2024 11:17:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2024/07/12/att-phone-records-stolen-data-breach/">https://techcrunch.com/2024/07/12/att-phone-records-stolen-data-breach/</a>, See on <a href="https://news.ycombinator.com/item?id=40944505">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p id="speakable-summary">U.S. phone giant AT&amp;T confirmed Friday it will begin notifying millions of consumers about a fresh data breach that allowed cybercriminals to steal the phone records of “nearly all” of its customers, a company spokesperson told TechCrunch.</p>

<p>In a statement, AT&amp;T said that the stolen data contains phone numbers of both cellular and landline customers, as well as AT&amp;T records of calls and text messages — such as who contacted who by phone or text — during a six-month period between May 1, 2022 and October 31, 2022.&nbsp;</p>

	
	


<p>AT&amp;T said some of the stolen data includes more recent records from January 2, 2023 for a smaller but unspecified number of customers.</p>

	
	


<p>The stolen data also includes call records of customers with phone service from other cell carriers that rely on AT&amp;T’s network, the company said.&nbsp;</p>

<p>AT&amp;T said the stolen data “does not contain the content of calls or texts,” but does include calling and texting records that an AT&amp;T phone number interacted with during the six-month period, as well as the total count of a customer’s calls and texts, and call durations — information that is often referred to as metadata. The stolen data does not include the time or date of calls or texts, AT&amp;T said.</p>

<p>Some of the stolen records include cell site identification numbers associated with phone calls and text messages, information that can be used to determine the approximate location of where a call was made or text message sent.</p>

<p>In all, the phone giant said it will notify around 110 million AT&amp;T customers of the data breach, company spokesperson Andrea Huguely told TechCrunch.&nbsp;</p>

	
	


	
	


<p>AT&amp;T published <a href="https://www.att.com/DataIncident" target="_blank" rel="noreferrer noopener nofollow">a website with information for customers</a> about the data incident.<strong> </strong>AT&amp;T also disclosed the data breach in <a rel="nofollow" href="https://www.sec.gov/ix?doc=/Archives/edgar/data/0000732717/000073271724000046/t-20240506.htm">a filing with regulators</a> before the market opened on Friday.</p>

<h2 id="h-breach-linked-to-snowflake">Breach linked to Snowflake</h2>

<p>AT&amp;T said it learned of the data breach on April 19, and that it was <a href="https://techcrunch.com/2024/03/30/att-reset-account-passcodes-customer-data/" target="_blank" rel="noreferrer noopener">unrelated to its earlier security incident</a> in March.&nbsp;</p>

<p>AT&amp;T’s Huguely told TechCrunch that the most recent compromise of customer records were stolen from the cloud data giant Snowflake <a href="https://techcrunch.com/2024/06/10/mandiant-hackers-snowflake-stole-significant-volume-data-customers/" target="_blank" rel="noreferrer noopener">during a recent spate of data thefts</a> targeting Snowflake’s customers.</p>

	
	


<p>Snowflake allows its corporate customers, like tech companies and telcos, to analyze huge amounts of customer data in the cloud. It’s not clear for what reason AT&amp;T was storing customer data in Snowflake, and the spokesperson would not say.</p>

<p>AT&amp;T is the latest company in recent weeks to confirm it had data stolen from Snowflake, <a href="https://techcrunch.com/2024/05/31/live-nation-confirms-ticketmaster-was-hacked-says-personal-information-stolen-in-data-breach/" target="_blank" rel="noreferrer noopener">following Ticketmaster</a> and <a href="https://techcrunch.com/2024/06/07/snowflake-ticketmaster-lendingtree-customer-data-breach/" target="_blank" rel="noreferrer noopener">LendingTree subsidiary QuoteWizard</a>, and others.</p>

	
	


<p>Snowflake blamed the data thefts on its customers for not using multi-factor authentication to secure their Snowflake accounts, a security feature that the cloud data giant did not enforce or require its customers to use.&nbsp;</p>

<p>Cybersecurity incident response firm Mandiant, which Snowflake called in to help with notifying customers, later said <a href="https://techcrunch.com/2024/06/10/mandiant-hackers-snowflake-stole-significant-volume-data-customers/" target="_blank" rel="noreferrer noopener">about 165 Snowflake customers had a “significant volume of data” stolen from their customer accounts</a>.&nbsp;</p>

	
	


<p>Mandiant attributed the breach to an as-yet-uncategorized cybercriminal group tracked only as UNC5537. Mandiant’s researchers say the hackers are financially motivated and have members in North America and at least one member in Turkey.&nbsp;</p>

<p>Some of the other corporate victims of the Snowflake account thefts had data subsequently published on known cybercrime forums. For AT&amp;T’s part, the company said that it does not believe that the data is publicly available at this time.</p>

<p>AT&amp;T’s statement said it was working with law enforcement to arrest the cybercriminals involved in the breach. AT&amp;T said that “at least one person has been apprehended.” AT&amp;T’s spokesperson said that the arrested individual was not an AT&amp;T employee, but deferred questions about the alleged criminals to the FBI. </p>

	
	


<p>An FBI spokesperson confirmed to TechCrunch on Friday that that after the phone giant contacted the agency to report the breach, AT&amp;T, the FBI and the Department of Justice agreed to delay notifying the public and customers on two occasions, citing “potential risks to national security and/or public safety.” </p>

	
	


<p>“AT&amp;T, FBI, and DOJ worked collaboratively through the first and second delay process, all while sharing key threat intelligence to bolster FBI investigative equities and to assist AT&amp;T’s incident response work,” the FBI spokesperson said.</p>

<p>The FBI did not comment on the arrest of one of the alleged cybercriminals.</p>

<p>This is <a href="https://techcrunch.com/2024/06/29/2024-in-data-breaches-1-billion-stolen-records-and-rising/" target="_blank" rel="noreferrer noopener">the second security incident AT&amp;T has disclosed this year</a>. AT&amp;T was forced to reset the account passcodes of millions of its customers after a cache of customer account information — including encrypted passcodes for accessing AT&amp;T customer accounts — was published on a cybercrime forum. A security researcher told TechCrunch at the time that the encrypted passcodes could be easily decrypted, prompting AT&amp;T to <a href="https://techcrunch.com/2024/04/10/att-notifies-regulators-after-customer-data-breach/" target="_blank" rel="noreferrer noopener">take precautionary action to protect customer accounts</a>.</p>

<p><strong>Read more on TechCrunch:</strong></p>

	
	


<ul>
<li><a href="https://techcrunch.com/2024/07/11/mspy-spyware-millions-customers-data-breach/" target="_blank" rel="noreferrer noopener">Data breach exposes millions of mSpy spyware customers</a></li>



<li><a href="https://techcrunch.com/2024/07/10/apple-alerts-iphone-users-in-98-countries-to-mercenary-spyware-attacks/" target="_blank" rel="noreferrer noopener">Apple warns iPhone users in 98 countries of spyware attacks</a></li>



<li><a href="https://techcrunch.com/2024/07/09/evolve-bank-says-ransomware-gang-stole-personal-data-on-millions-of-customers/" target="_blank" rel="noreferrer noopener">Evolve Bank says ransomware gang stole personal data on millions of customers</a></li>



<li><a href="https://techcrunch.com/2024/07/05/openai-breach-is-a-reminder-that-ai-companies-are-treasure-troves-for-hackers/" target="_blank" rel="noreferrer noopener">OpenAI breach is a reminder that AI companies are treasure troves for hackers</a></li>
</ul>

<p><em>Updated with comment from the FBI.</em></p>

<figure></figure>
</div></div>]]></description>
        </item>
    </channel>
</rss>