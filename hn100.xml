<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 12 Feb 2025 01:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[BYD to offer Tesla-like self-driving tech in all models for free (132 pts)]]></title>
            <link>https://www.asiafinancial.com/byd-to-offer-tesla-like-self-driving-tech-in-all-models-for-free</link>
            <guid>43018989</guid>
            <pubDate>Tue, 11 Feb 2025 22:08:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.asiafinancial.com/byd-to-offer-tesla-like-self-driving-tech-in-all-models-for-free">https://www.asiafinancial.com/byd-to-offer-tesla-like-self-driving-tech-in-all-models-for-free</a>, See on <a href="https://news.ycombinator.com/item?id=43018989">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
									<p><span>February 11, 2025</span></p>
									
																	<div>
										<h4> <p>The Chinese EV giant’s move to introduce self-driving to its cheapest EV models — priced as low as just $9,555 — far undercuts rivals such as Tesla</p>
</h4>
										</div>
                     

								
																<br>
							 	 
									
									
								</div><div>
								
																
									
<p>China’s biggest carmaker BYD has shaken up the electric vehicle industry yet again this week after announcing it will offer cutting-edge self-driving systems on all its vehicles — including its cheapest Seagull model — for free.</p>
<p>The carmaker put 21 models equipped with its “God’s Eye” advanced driver-assistance system (ADAS) on sale on Monday.</p>
<p>“[We are] starting an era where autonomous driving is for everyone,” BYD’s founder Wang Chuanfu said at a livestreamed launch event.</p>

<h3>Also on AF: <a href="https://www.asiafinancial.com/more-concern-on-deepseeks-data-policies-after-bytedance-link" target="_blank" rel="noopener">More Concern on DeepSeek’s Data Policies After ByteDance Link</a></h3>

<p>The technology is “no longer an unattainable luxury, but an essential tool . . . like safety belts and airbags”, Wang was quoted as saying <strong><a href="https://www.ft.com/content/ef36aff7-6be4-4cb5-9928-33beabf6c443" target="_blank" rel="noopener">by The Financial Times</a></strong>.</p>
<p>BYD has deployed three different versions of the “God’s Eye” ADAS across its line-up. All three offer automated parking and lane-keeping features, with the basic version – available on models costing up to 219,800 yuan ($30,078.69) – enabling autonomous driving on highways. The driver must keep their hands on the wheel and take control when necessary.</p>
<p>Two higher-end versions will be installed on more expensive BYD-branded cars and its premium Denza and Yangwang brands. These will enable a car to drive autonomously — though also under human supervision — in more complex urban traffic.</p>
<p>BYD’s move to introduce self-driving to its cheapest EV models — priced as low as just $9,555 — far undercuts its rivals.</p>
<p>Analysts say it could start a new price war in an already hyper-competitive market, comparing it to how China’s DeepSeek recently <strong><a href="https://www.asiafinancial.com/tech-selloff-extends-to-japan-as-deepseek-puts-focus-on-ai-costs" target="_blank" rel="noopener">roiled the global AI sector</a></strong> with its low prices.</p>
<p>The “era of smart driving popularisation has come,” Nomura analysts wrote in a note on BYD’s announcement.</p>
<p>Shares in Chinese automakers Xpeng and Geely Auto tumbled on Tuesday on concern they will struggle to compete against BYD’s move.</p>

<h2>Fresh struggles for Musk’s Tesla</h2>
<p>BYD’s market-breaking move is also likely to deliver a deep blow to its closest rival Tesla at a time when the Elon Musk-led carmaker is already struggling to shore up sales.</p>
<p>Tesla has these ADAS features available in China in its EVs priced from $32,000. It&nbsp;charges $8,000&nbsp;for its Full Self-Driving (FSD) driver assistant software in the United States, or $99 a month on a subscription basis. FSD is not yet available in China.</p>
<p>Tesla share prices tumbled 3.8% on Tuesday [1621 GMT], against a smaller 0.1% fall on the broader Nasdaq Composite index.</p>
<p>The carmaker is already struggling with a sharp slump in sales as consumers <strong><a href="https://www.reuters.com/business/autos-transportation/musk-effect-tesla-sales-slump-five-european-markets-january-2025-02-04/" target="_blank" rel="noopener">grow increasingly wary of Musk’s divisive politics</a></strong>. Tesla stock is down more than 11% in the past five trading sessions.</p>
<p>In contrast, BYD’s shares are up nearly 17% over the past five trading sessions.</p>

<h2>The European challenge</h2>
<p>BYD’s offerings could also weigh on Tesla’s sales in Europe, where <strong><a href="https://www.proactiveinvestors.co.uk/companies/news/1064131/tesla-shame-in-europe-over-musk-s-pay-package-and-meddling-in-politics-1064131.html" target="_blank" rel="noopener">a phenomenon dubbed as “Tesla shame”</a></strong> has led potential buyers and existing owners to steer clear of the once pioneer EV-maker.</p>
<p>BYD already enjoys steep price margins in European markets, compared to its local rivals, despite facing an additional 17% tariffs on its shipments to the bloc.</p>
<p>It remains to be seen, however, how much of BYD’s ADAS technology will be available in Europe, where autonomous driving regulations are stricter than in the US. Tesla’s FSD tech is only partially available in the region so far.</p>
<p>For now, BYD has listed its Dolphin compact electric car — which it also sells in Europe — on the list of models that will be equipped with ‘God’s Eye.’</p>

<ul>
<li>Vishakha Saxena, with Reuters</li>
</ul>

<h3>Also read:</h3>
<h3><a href="https://www.asiafinancial.com/chinas-byd-beat-tesla-again-in-ev-sales-in-q4-last-year-scmp" target="_blank" rel="noopener">China’s BYD Beat Tesla Again in EV Sales in Q4 Last Year – SCMP</a></h3>
<h3><a href="https://www.asiafinancial.com/byd-promises-driving-range-of-over-2000km-with-new-hybrid-tech" target="_blank" rel="noopener">BYD Promises Driving Range of Over 2000km With New Hybrid Tech</a></h3>
<h3><a href="https://www.asiafinancial.com/brazil-to-fine-byd-for-each-worker-found-doing-slave-labour" target="_blank" rel="noopener">Brazil to Fine BYD For Each Worker Found Doing ‘Slave Labour’</a></h3>
<h3><a href="https://www.asiafinancial.com/tesla-sees-record-year-in-china-despite-first-global-sales-slump" target="_blank" rel="noopener">Tesla Sees Record Year in China Despite First Global Sales Slump</a></h3>
<h3><a href="https://www.asiafinancial.com/chinas-byd-gaining-greater-sales-may-outsell-ford-honda" target="_blank" rel="noopener">China’s BYD Gaining Greater Sales, May Outsell Ford, Honda</a></h3>
<h3><a href="https://www.asiafinancial.com/byd-sales-boom-in-southeast-asia-while-tesla-growth-slows" target="_blank" rel="noopener">BYD Sales Boom in Southeast Asia While Tesla Growth Slows</a></h3>
<h3><a href="https://www.asiafinancial.com/in-u-turn-elon-musk-says-us-tariffs-on-chinese-evs-not-good" target="_blank" rel="noopener">In U-Turn, Elon Musk Says US Tariffs on Chinese EVs ‘Not Good’</a></h3>
<h3><a href="https://www.asiafinancial.com/trump-planning-to-clamp-down-on-chinese-ev-supply-chains" target="_blank" rel="noopener">Trump Planning to Clamp Down on Chinese EV Supply Chains</a></h3>
<h3><a href="https://www.asiafinancial.com/chinese-electric-vehicles-selling-well-in-non-tariff-norway" target="_blank" rel="noopener">Chinese Electric Vehicles Selling Well in Non-Tariff Norway</a></h3>
<h3><a href="https://www.asiafinancial.com/brazil-now-top-destination-for-chinese-evs-as-exports-explode" target="_blank" rel="noopener">Brazil Now Top Destination For Chinese EVs As Exports Explode</a></h3>
<h3><strong><a href="https://www.asiafinancial.com/musk-may-build-china-data-centre-for-tesla-self-driving-system" target="_blank" rel="noopener">Musk May Build China Data Centre for Tesla Self Driving System</a></strong></h3>


								



			
			<div>
				<p><img width="96" height="96" src="https://www.asiafinancial.com/wp-content/uploads/2024/01/Untitled-design-4-96x96.jpg" alt="" loading="lazy" srcset="https://www.asiafinancial.com/wp-content/uploads/2024/01/Untitled-design-4-96x96.jpg 96w, https://www.asiafinancial.com/wp-content/uploads/2024/01/Untitled-design-4-300x300.jpg 300w, https://www.asiafinancial.com/wp-content/uploads/2024/01/Untitled-design-4-150x150.jpg 150w, https://www.asiafinancial.com/wp-content/uploads/2024/01/Untitled-design-4-768x768.jpg 768w, https://www.asiafinancial.com/wp-content/uploads/2024/01/Untitled-design-4-24x24.jpg 24w, https://www.asiafinancial.com/wp-content/uploads/2024/01/Untitled-design-4-48x48.jpg 48w, https://www.asiafinancial.com/wp-content/uploads/2024/01/Untitled-design-4.jpg 1000w" sizes="100vw"></p>				
				 <div>
									<h3><a href="https://www.asiafinancial.com/author/vishakha-saxenaasiafinancial-com">Vishakha Saxena </a></h3>
					<p> Vishakha Saxena is the Multimedia and Social Media Editor at Asia Financial. She has worked as a digital journalist since 2013, and is an experienced writer and multimedia producer. As a trader and investor, she is keenly interested in new economy, emerging markets and the intersections of finance and society. You can write to her at <a href="https://www.asiafinancial.com/cdn-cgi/l/email-protection" data-cfemail="3046594358515b58511e435148555e51705143595156595e515e5359515c1e535f5d">[email&nbsp;protected]</a> </p>
				 
								</div>
			</div>
			
			
		
<div>
	<p>
									<h3>You Might also Like</h3>
								</p>
								




</div>


		<!--<div class="comment-sec">
		<div class="comment-head">
			<div class="tt-title-block custom-color-properties-1001 style2">
			<h3 class="tt-title-text">Leave a Comment</h3>
			</div>
		</div>
		<p>Your email address will not be published. Required ﬁelds are marked *</p>
		<form class="comment-form">
			<textarea placeholder="Your Comment"></textarea>
			<input type="text" name="" placeholder="Name*">
			<input type="text" name="" placeholder="Email*">
			<input type="text" name="" placeholder="Website">
			<label><input type="checkbox" name="">Save my name, email, and website in this browser for the next time I comment.</label>
			<button class="comment-button">POST COMMENT</button>
		</form>

		</div> -->
								</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Thomson Reuters wins first major AI copyright case in the US (214 pts)]]></title>
            <link>https://www.wired.com/story/thomson-reuters-ai-copyright-lawsuit/</link>
            <guid>43018251</guid>
            <pubDate>Tue, 11 Feb 2025 20:56:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/thomson-reuters-ai-copyright-lawsuit/">https://www.wired.com/story/thomson-reuters-ai-copyright-lawsuit/</a>, See on <a href="https://news.ycombinator.com/item?id=43018251">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-journey-hook="client-content" data-testid="ArticlePageChunks"><p>Thomson Reuters has <a data-offer-url="https://storage.courtlistener.com/recap/gov.uscourts.ded.72109/gov.uscourts.ded.72109.770.0.pdf" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://storage.courtlistener.com/recap/gov.uscourts.ded.72109/gov.uscourts.ded.72109.770.0.pdf&quot;}" href="https://storage.courtlistener.com/recap/gov.uscourts.ded.72109/gov.uscourts.ded.72109.770.0.pdf" rel="nofollow noopener" target="_blank">won</a> the first major AI copyright case in the United States.</p><p>In 2020, the media and technology conglomerate filed an unprecedented <a href="https://www.wired.com/story/ai-copyright-case-tracker/">AI copyright lawsuit</a> against the legal AI startup Ross Intelligence. In the complaint, Thomson Reuters claimed the AI firm reproduced materials from its legal research firm Westlaw. Today, a judge ruled in Thomson Reuters’ favor, finding that the company’s copyright was indeed infringed by Ross Intelligence’s actions.</p><p>“None of Ross’s possible defenses holds water. I reject them all,” wrote US District Court of Delaware judge Stephanos Bibas, in a summary judgement.</p><p>Thomson Reuters and Ross Intelligence did not immediately respond to requests for comment.</p><p>The generative AI boom has led to a spate of additional <a href="https://www.wired.com/story/matthew-butterick-ai-copyright-lawsuits-openai-meta/">legal fights</a> about how AI companies can use copyrighted material, as many major AI tools were developed by training on copyrighted works including books, films, visual artwork, and websites. Right now, there are several dozen lawsuits currently winding through the US court system, as well as international challenges in China, Canada, the UK, and other countries.</p><p>Notably, Judge Bibas ruled in Thomson Reuters’ favor on the question of fair use. The <a href="https://www.wired.com/story/andy-warhol-fair-use-prince-generative-ai/">fair use doctrine</a> is a <a href="https://www.wired.com/story/battle-over-books3/">key component</a> of how AI companies are seeking to defend themselves against claims that they used copyrighted materials illegally. The idea underpinning fair use is that sometimes it’s legally permissible to use copyrighted works without permission—for example, to create parody works, or in noncommercial research or news production. When determining whether fair use applies, courts use a four-factor test, looking at the reason behind the work, the nature of the work (whether it’s poetry, nonfiction, private letters, et cetera), the amount of copyrighted work used, and how the use impacts the market value of the original. Thomson Reuters prevailed on two of the four factors, but Bibas described the fourth as the most important, and ruled that Ross “meant to compete with Westlaw by developing a market substitute.”</p><p>Thomson Reuters spokesperson Jeffrey McCoy applauded the ruling in a statement emailed to WIRED. “We are pleased that the court granted summary judgment in our favor and concluded that Westlaw’s editorial content created and maintained by our attorney editors, is protected by copyright and cannot be used without our consent,” he wrote. “The copying of our content was not ‘fair use.’”</p><p>Even before this ruling, Ross Intelligence had already felt the impact of the court battle: The startup <a data-offer-url="https://www.lawnext.com/2020/12/legal-research-company-ross-to-shut-down-under-pressure-of-thomson-reuters-lawsuit.html" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.lawnext.com/2020/12/legal-research-company-ross-to-shut-down-under-pressure-of-thomson-reuters-lawsuit.html&quot;}" href="https://www.lawnext.com/2020/12/legal-research-company-ross-to-shut-down-under-pressure-of-thomson-reuters-lawsuit.html" rel="nofollow noopener" target="_blank">shut down</a> in 2021, citing the cost of litigation. In contrast, many of the AI companies still duking it out in court, like OpenAI and Google, are financially equipped to weather prolonged legal fights.</p><p>Still, this ruling is a blow to AI companies, according to Cornell University professor of digital and internet law James Grimmelmann: “If this decision is followed elsewhere, it's really bad for the generative AI companies.” Grimmelmann believes that Bibas’ judgement suggests that much of the case law that generative AI companies are citing to argue fair use is “irrelevant.”</p><p>Chris Mammen, a partner at Womble Bond Dickinson who focuses on intellectual property law, concurs that this will complicate AI companies’ fair use arguments, although it could vary from plaintiff to plaintiff. “It puts a finger on the scale towards holding that fair use doesn’t apply,” he says.</p><p><em>Update 2/11/25 5:09 ET: This story has been updated to include additional comment from Thomson Reuters.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DeepScaleR: Surpassing O1-Preview with a 1.5B Model by Scaling RL (258 pts)]]></title>
            <link>https://pretty-radio-b75.notion.site/DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2</link>
            <guid>43017599</guid>
            <pubDate>Tue, 11 Feb 2025 19:59:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pretty-radio-b75.notion.site/DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2">https://pretty-radio-b75.notion.site/DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2</a>, See on <a href="https://news.ycombinator.com/item?id=43017599">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[The subtle art of designing physical controls for cars (174 pts)]]></title>
            <link>https://www.theturnsignalblog.com/the-subtle-art-of-designing-physical-control-for-cars/</link>
            <guid>43017010</guid>
            <pubDate>Tue, 11 Feb 2025 19:15:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theturnsignalblog.com/the-subtle-art-of-designing-physical-control-for-cars/">https://www.theturnsignalblog.com/the-subtle-art-of-designing-physical-control-for-cars/</a>, See on <a href="https://news.ycombinator.com/item?id=43017010">Hacker News</a></p>
Couldn't get https://www.theturnsignalblog.com/the-subtle-art-of-designing-physical-control-for-cars/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla Cybertruck Drives Itself into a Pole, Owner Says 'Thank You Tesla' (194 pts)]]></title>
            <link>https://www.thedrive.com/news/tesla-cybertruck-drove-itself-into-a-pole-owner-says-thank-you-tesla</link>
            <guid>43016931</guid>
            <pubDate>Tue, 11 Feb 2025 19:09:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.thedrive.com/news/tesla-cybertruck-drove-itself-into-a-pole-owner-says-thank-you-tesla">https://www.thedrive.com/news/tesla-cybertruck-drove-itself-into-a-pole-owner-says-thank-you-tesla</a>, See on <a href="https://news.ycombinator.com/item?id=43016931">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-6497463">

		
												<div>
						<figure>
							<img src="https://www.thedrive.com/wp-content/uploads/2025/02/GjV25F6bQAAiTKO.jpg?quality=85&amp;w=1700" srcset="https://www.thedrive.com/wp-content/uploads/2025/02/GjV25F6bQAAiTKO.jpg 1700w, https://www.thedrive.com/wp-content/uploads/2025/02/GjV25F6bQAAiTKO.jpg?w=768&amp;h=432 768w, https://www.thedrive.com/wp-content/uploads/2025/02/GjV25F6bQAAiTKO.jpg?w=1536&amp;h=864 1536w, https://www.thedrive.com/wp-content/uploads/2025/02/GjV25F6bQAAiTKO.jpg?w=800&amp;h=450 800w, https://www.thedrive.com/wp-content/uploads/2025/02/GjV25F6bQAAiTKO.jpg?w=356&amp;h=200 356w, https://www.thedrive.com/wp-content/uploads/2025/02/GjV25F6bQAAiTKO.jpg?w=1174&amp;h=660 1174w, https://www.thedrive.com/wp-content/uploads/2025/02/GjV25F6bQAAiTKO.jpg?w=711&amp;h=400 711w, https://www.thedrive.com/wp-content/uploads/2025/02/GjV25F6bQAAiTKO.jpg?w=1487&amp;h=836 1487w, https://www.thedrive.com/wp-content/uploads/2025/02/GjV25F6bQAAiTKO.jpg?w=1227&amp;h=690 1227w, https://www.thedrive.com/wp-content/uploads/2025/02/GjV25F6bQAAiTKO.jpg?w=384&amp;h=216 384w, https://www.thedrive.com/wp-content/uploads/2025/02/GjV25F6bQAAiTKO.jpg?w=718&amp;h=404 718w, https://www.thedrive.com/wp-content/uploads/2025/02/GjV25F6bQAAiTKO.jpg?w=1590&amp;h=894 1590w, https://www.thedrive.com/wp-content/uploads/2025/02/GjV25F6bQAAiTKO.jpg?w=1337&amp;h=752 1337w, https://www.thedrive.com/wp-content/uploads/2025/02/GjV25F6bQAAiTKO.jpg?w=1234&amp;h=694 1234w, https://www.thedrive.com/wp-content/uploads/2025/02/GjV25F6bQAAiTKO.jpg?w=280&amp;h=157 280w, https://www.thedrive.com/wp-content/uploads/2025/02/GjV25F6bQAAiTKO.jpg?w=1440&amp;h=810 1440w, https://www.thedrive.com/wp-content/uploads/2025/02/GjV25F6bQAAiTKO.jpg?w=289&amp;h=163 289w, https://www.thedrive.com/wp-content/uploads/2025/02/GjV25F6bQAAiTKO.jpg?w=370&amp;h=208 370w, https://www.thedrive.com/wp-content/uploads/2025/02/GjV25F6bQAAiTKO.jpg?w=308&amp;h=173 308w, https://www.thedrive.com/wp-content/uploads/2025/02/GjV25F6bQAAiTKO.jpg?w=50&amp;h=28 50w" sizes="(max-width: 1700px) 100vw, 1700px" fetchpriority="high" alt="Tesla Cybertruck after crashing into utility pole.">
															<p>
									<figcaption> <em>MrChallinger via X</em></figcaption>
								</p>
													</figure>
											</div>
												<div data-toc-container="" data-module="TableOfContents">
						
<p>A <a href="https://www.thedrive.com/category/tesla-news" target="_blank">Tesla</a> Cybertruck owner is praising the automaker for the performance of his electric pickup after its Full Self-Driving system drove it into a utility pole in Reno, Nevada. According to the driver, the <a href="https://www.thedrive.com/car-reviews/2024-tesla-cybertruck-review" target="_blank">Cybertruck</a> ignored the fact that its travel lane was ending and plowed straight into a pedestrian crossing, which was fortunately empty. Then, the owner (going by <a href="https://x.com/MrChallinger" rel="noreferrer" target="_blank">@MrChallinger</a>) took to Elon Musk’s X to praise Tesla for the vehicle’s performance in the crash, which did not cause him any injuries.</p>



<blockquote><div lang="en" dir="ltr"><p>Soooooo my <a href="https://twitter.com/Tesla?ref_src=twsrc%5Etfw" rel="noreferrer" target="_blank">@Tesla
</a> <a href="https://twitter.com/cybertruck?ref_src=twsrc%5Etfw" rel="noreferrer" target="_blank">@cybertruck
</a> crashed into a curb and then a light post on v13.2.4. </p><p>Thank you <a href="https://twitter.com/Tesla?ref_src=twsrc%5Etfw" rel="noreferrer" target="_blank">@Tesla
</a> for engineering the best passive safety in the world. I walked away without a scratch. </p><p>It failed to merge out of a lane that was ending (there was no one on my left) and made… <a href="https://t.co/vpT4AGz8jZ" rel="noreferrer" target="_blank">pic.twitter.com/vpT4AGz8jZ</a></p></div>— Jonathan Challinger (@MrChallinger) <a href="https://twitter.com/MrChallinger/status/1888546351572726230?ref_src=twsrc%5Etfw" rel="noreferrer" target="_blank">February
 9, 2025</a></blockquote> 



<p>“It failed to merge out of a lane that was ending (there was no one on my left) and made no attempt to slow down or turn until it had already hit the curb. Big fail on my part, obviously,” he said.</p>



<p>It’s a far bigger failure on the part of a “Full” Self-Driving system, we’d argue. Per one of the user’s replies, the incident occurred on Stead Boulevard near the Sierra Sage Golf Course in northwest Reno. The Cybertruck was headed northbound in the far right lane, which merges into the center lane to make way for a signaled crosswalk. The Cybertruck failed to merge and simply drove up onto the curb island, striking the pole.</p>



<p>“Don’t make the same mistake I did. Pay attention. It can happen. I follow Tesla and FSD pretty closely and haven’t heard of any accident on V13 at all before this happened. It is easy to get complacent now – don’t,” he said. </p>







<p>So, to recap, this AI-based system failed to perform a basic merge while driving straight along a multi-lane road with no other traffic, and managed to hit the only two solid objects anywhere in its path—both of which were built on a separate curb island engineered to isolate pedestrians from vehicular traffic. And all of that happened without any intervention from the Cybertruck’s suite of active safety systems.</p>



<p>And rather than being upset with Tesla for selling him a smart dumpster on wheels, the driver <em>took blame</em> for the incident, saying he should have been paying attention. Maybe the average Cybertruck owner is too young to remember, but we’ve seen something like this before…</p>



<figure>
	<div data-video-id="ILqnYx7XnwQ" data-iframe-classes="wp-embed-aspect-4-3 wp-has-aspect-ratio">
		<p><img loading="lazy" src="https://i.ytimg.com/vi/ILqnYx7XnwQ/hqdefault.jpg"></p><svg viewBox="0 0 68 48" xmlns="http://www.w3.org/2000/svg">
			<path d="M66.52 7.74c-.78-2.93-2.49-5.41-5.42-6.19C55.79.13 34 0 34 0S12.21.13 6.9 1.55c-2.93.78-4.63 3.26-5.42 6.19C.06 13.05 0 24 0 24s.06 10.95 1.48 16.26c.78 2.93 2.49 5.41 5.42 6.19C12.21 47.87 34 48 34 48s21.79-.13 27.1-1.55c2.93-.78 4.64-3.26 5.42-6.19C67.94 34.95 68 24 68 24s-.06-10.95-1.48-16.26z" fill="red"></path>
			<path d="M45 24 27 14v20" fill="white"></path>
		</svg>
	</div>
	</figure>





<p>Why? Well, MrChallinger is on team Tesla, and you gotta protect the team, right? (Emphasis added)</p>



<p>“@Tesla_AI how do I make sure you have the data you need from this incident? Service center etc has been less than responsive on this. I do have the dashcam footage. I want to get it out there as a PSA that it can happen, even on v13, <strong>but I’m hesitant because I don’t want the attention and I don’t want to give the bears/haters any material</strong>.”</p>



<p>Per the owner, the Tesla had Full Self-Driving build v13.2.4 installed when the crash happened. MrChallinger praised the company for “engineering the best passive safety in the world,” but what is truly impressive here is just how efficiently Tesla has molded its owner base into an unpaid PR firm. Please, won’t <em>somebody </em>think of the investors?</p>



<p><em>Got tips? Send ’em to tips@thedrive.com</em></p>
					</div>
				

				

					</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Japan can be a science heavyweight once more if it rethinks funding (107 pts)]]></title>
            <link>https://www.nature.com/articles/d41586-025-00394-8</link>
            <guid>43016353</guid>
            <pubDate>Tue, 11 Feb 2025 18:30:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/d41586-025-00394-8">https://www.nature.com/articles/d41586-025-00394-8</a>, See on <a href="https://news.ycombinator.com/item?id=43016353">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-test="access-teaser"> <p>From CRISPR gene editing to protein-structure predictions driven by artificial intelligence, great innovations stem from interdisciplinary research. Solutions to climate change, biodiversity loss, health inequities and other global crises will also rely on insights that bridge many fields.</p><p>Yet interdisciplinary research is still sidelined in many countries. Even though these papers attract more citations<sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup> and have more impact on research<sup><a href="#ref-CR2" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">2</a></sup>, interdisciplinary research proposals tend to be less likely to receive funding than are those with a narrower scope<sup><a href="#ref-CR3" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">3</a></sup>.</p><p>Some countries have adjusted their research funding strategies accordingly. For example, between 2016 and 2018, UK research councils awarded 30% more grants to interdisciplinary investigators than they had a decade earlier, with 44% of funded projects in 2018 spanning at least two research subjects<sup><a href="#ref-CR4" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">4</a></sup>. A similar move has been made in the United States: between 2015 and 2020, university departments that submitted some interdisciplinary grant proposals received almost five times more funding — in terms of total and individual awards — from the National Institutes of Health (NIH) and National Science Foundation (NSF) than did those that submitted in only one discipline<sup><a href="#ref-CR5" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">5</a></sup>.</p><article data-label="Related"><a href="https://www.nature.com/articles/d41586-025-00403-w" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-025-00394-8/d41586-025-00394-8_50617232.jpg"><p>‘Male-dominated campuses belong to the past’: the University of Tokyo tackles the gender gap</p></a></article><p>Alas, this isn’t the case in Japan. The nation’s funding agencies still mostly support research in tight disciplinary boundaries, such as engineering or chemistry. Specialists who evaluate these grant proposals <a href="https://www.nature.com/articles/d41586-023-03290-1" data-track="click" data-label="https://www.nature.com/articles/d41586-023-03290-1" data-track-category="body text link">tend to favour work in fields that they are familiar with</a> over interdisciplinary studies they do not understand well.</p><p>This narrow approach is leading to substantial underfunding of interdisciplinary research in Japan. It also means that the nation is missing out on breakthroughs. Japan’s natural resources are limited, and its economy has long relied on science and technology. The decline in its research and innovation is unmistakable, as indicated by the country’s share of the world’s top 10% of most-cited research articles, which has dropped from 6% to 2% over the past two decades or so.</p><p>As scientists and engineers based in Japanese research institutions, we urge the government and funding agencies to do more to support interdisciplinary research — or risk eroding the country’s global standing in science and its economy. Here we highlight five directions to foster such a shift.</p><h2><b>Fund people, not projects</b></h2><p>We argue that Japanese funding agencies should pivot from funding projects to supporting talented researchers. Such a model has proven to be effective in leading institutions worldwide. For example, the Howard Hughes Medical Institute’s (HHMI) Investigator Program provides outstanding scientists with flexible, long-term funding to pursue high-impact biomedical research — some US$11 million over a seven-year term, which can be renewed. This approach has led to groundbreaking discoveries, including the mechanistic understanding of circadian rhythms, the computational prediction of protein folding and the discovery of RNA interference.</p><article data-label="Related"><a href="https://www.nature.com/articles/d41586-023-03290-1" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-025-00394-8/d41586-025-00394-8_26968274.jpg"><p>Japanese research is no longer world class — here’s why</p></a></article><p>Similarly, in Germany, the Max Planck Society prioritizes curiosity-driven research that has long-term objectives over immediate applications. By fostering interdisciplinary collaboration and providing institute directors with generous internal funding, free from the continual need to secure external grants, this strategy has enabled breakthroughs including the development of CRISPR–Cas9 as a gene-editing tool and structural insights into ribosomes.</p><p>Japan does have some programmes to support talented researchers, but these are too timid. The Japan Science and Technology Agency (JST) and the Japan Society for the Promotion of Science (JSPS), for example, dedicate some funds to basic science and some to curiosity-driven research. But the JST adopts a top-down approach, which often favours research on trending topics and can miss more-original ideas or projects . And the annual budget for the JSPS’s main grant programme, the Grants-in-Aid for Scientific Research (KAKENHI), has remained stagnant for the past decade. Adjusted for inflation and the weakening yen, the average funding per project has halved since 2013.</p><p>However, one Japanese institution has adopted researcher-focused funding with great success: the Okinawa Institute of Science and Technology Graduate University (OIST). Unlike national universities, which are funded by the Ministry of Education, Culture, Sports, Science and Technology, OIST is a private university that is funded by Japan’s Cabinet Office. Launched in 2011, it has a horizontal organizational structure and a strong commitment to diversity and interdisciplinary collaboration (see ‘Steps to success’). OIST is now ranked as <a href="https://www.nature.com/articles/d42473-021-00382-2" data-track="click" data-label="https://www.nature.com/articles/d42473-021-00382-2" data-track-category="body text link">Japan’s leading research institute</a> by the Nature Index, and around 20% of its publications involve contributions across more than one discipline. More Japanese institutions should follow its lead.</p><div><h3>Steps to success</h3><div><p>At the Okinawa Institute of Science and Technology Graduate University (OIST), faculty members receive core funding for five years — with flexibility in what to do with it. Unlike many other universities, in which laboratories must cover the costs of core facilities and technical staff, access to these resources at OIST is provided free of charge. This fosters collaboration and enables high-risk, high-reward projects. A review of each faculty member every five years ensures accountability, with continued support granted only to those who demonstrate significant achievements.</p><p>OIST also operates without conventional departments, helping scientists from different fields to collaborate. The buildings host open spaces, lounges, shared kitchens and open desk areas that encourage casual interactions. PhD students must also rotate through multiple labs during their first year to widen their horizons. These rotations facilitate the exchange of ideas and technologies, embedding interdisciplinary collaboration into the fabric of the institution.</p><p>Although further progress is needed, including in terms of gender and disability, OIST fosters a greater diversity in its community than do other Japanese institutions. This is partly because it attracts scholars from around the world. In 2024, more than 60% of faculty members and 80% of students at OIST were not Japanese citizens, bringing a dynamic mix of perspectives and expertise.</p></div></div><h2><b>Embrace high-stakes projects</b></h2><p>Interdisciplinary research is often perceived as riskier than more conventional, single-discipline work. This might be because it can be hard to learn concepts and methods, and even communicate efficiently, across disciplines. And priorities between fields can conflict.</p><p>To overcome these challenges, Japanese funding agencies should consider adopting a ‘high risk, high impact’ funding model, similar to that of the US Defense Advanced Research Projects Agency (DARPA), which anticipates a success rate of just 50%. They should recognize that even ‘unsuccessful’ high-risk projects can generate valuable knowledge, contributing to broader scientific and technological advancements. The DARPA funding model has inspired others, such as the Advanced Research Projects Agency for Health (ARPA-H), launched in the NIH to drive biomedical breakthroughs<sup><a href="#ref-CR6" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">6</a></sup>.</p><article data-label="Related"><a href="https://www.nature.com/articles/d41586-024-02449-8" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-025-00394-8/d41586-025-00394-8_27487864.jpg"><p>How to improve assessments of publication integrity</p></a></article><p>Another example is the Chan Zuckerberg Biohub in California, which supports interdisciplinary teams comprising biologists, engineers and data scientists to work on ambitious goals, such as the eradication of infectious diseases. It <a href="https://www.nature.com/articles/nature.2017.21440" data-track="click" data-label="https://www.nature.com/articles/nature.2017.21440" data-track-category="body text link">provides substantial funding over several years</a> to enable teams to pivot research directions in response to emerging challenges. Success is assessed through regular milestone-based evaluations. Projects that show limited potential are promptly terminated, ensuring that resources can be reallocated to more-promising endeavours.</p><p>In Japan, we are not advocating for existing funds to be redistributed. Rather, Japanese policymakers should allocate money to high-risk, high-reward projects from a separate pot, managed through dedicated DARPA-like programmes.</p><h2><b>Expand grant panels</b></h2><p>Initiatives such as the JST’s Diversity and Inclusiveness programme aim to lessen the homogeneity of the research workforce — for example, through awards and networking opportunities for women in science and by providing support or extensions for life events as well as for childcare and caregiving commitments — which tend to fall mostly on women (see page 295). This is laudable, but these efforts should be more widespread: they should be extended across genders and also include funding agency officers, programme managers and reviewers.</p><figure><picture><source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-025-00394-8/d41586-025-00394-8_50602814.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-025-00394-8/d41586-025-00394-8_50602814.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px"><img alt="The Japan Agency for Marine-Earth Science and Technology lifts a research submersible onto a ship." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-025-00394-8/d41586-025-00394-8_50602814.jpg"><figcaption><p><span>The crewed research submersible <i>Shinkai 6500</i> has conducted more than 1,700 dives.</span><span>Credit: Associated Press/Alamy</span></p></figcaption></picture></figure><p>Japan’s funding agencies should also ensure that their own decision makers are less homogeneous in terms of their discipline, background, gender, age, nationality and culture. This would help to favour interdisciplinary research, which by nature weaves different approaches together and can benefit from being planned, conducted and assessed by a diverse community.</p><p>The role of immigration in boosting diversity should also be recognized.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[UnitedHealth hired a defamation law firm to go after social media posts criticiz (219 pts)]]></title>
            <link>https://fortune.com/2025/02/10/unitedhealth-defamation-law-firm-social-media/</link>
            <guid>43015713</guid>
            <pubDate>Tue, 11 Feb 2025 17:44:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fortune.com/2025/02/10/unitedhealth-defamation-law-firm-social-media/">https://fortune.com/2025/02/10/unitedhealth-defamation-law-firm-social-media/</a>, See on <a href="https://news.ycombinator.com/item?id=43015713">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-cy="article-wrapper" id="content" role="article"><p><img alt="Doctors and patients in New York City protesting UnitedHealth after the company announced earnings in January." fetchpriority="high" width="768" height="512" decoding="async" data-nimg="1" sizes="100vw" srcset="https://fortune.com/img-assets/wp-content/uploads/2025/02/GettyImages-2194245156-e1739212281473.jpg?w=320&amp;q=75 320w, https://fortune.com/img-assets/wp-content/uploads/2025/02/GettyImages-2194245156-e1739212281473.jpg?w=384&amp;q=75 384w, https://fortune.com/img-assets/wp-content/uploads/2025/02/GettyImages-2194245156-e1739212281473.jpg?w=480&amp;q=75 480w, https://fortune.com/img-assets/wp-content/uploads/2025/02/GettyImages-2194245156-e1739212281473.jpg?w=576&amp;q=75 576w, https://fortune.com/img-assets/wp-content/uploads/2025/02/GettyImages-2194245156-e1739212281473.jpg?w=768&amp;q=75 768w, https://fortune.com/img-assets/wp-content/uploads/2025/02/GettyImages-2194245156-e1739212281473.jpg?w=1024&amp;q=75 1024w, https://fortune.com/img-assets/wp-content/uploads/2025/02/GettyImages-2194245156-e1739212281473.jpg?w=1280&amp;q=75 1280w, https://fortune.com/img-assets/wp-content/uploads/2025/02/GettyImages-2194245156-e1739212281473.jpg?w=1440&amp;q=75 1440w" src="https://fortune.com/img-assets/wp-content/uploads/2025/02/GettyImages-2194245156-e1739212281473.jpg?w=1440&amp;q=75"></p><p>Doctors and patients in New York City protesting UnitedHealth after the company announced earnings in January.</p><p>Eugene Gologursky/Getty Images for People's Action Institute</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[IT Unemployment Rises to 5.7% as AI Hits Tech Jobs (121 pts)]]></title>
            <link>https://www.wsj.com/articles/it-unemployment-rises-to-5-7-as-ai-hits-tech-jobs-7726bb1b</link>
            <guid>43015397</guid>
            <pubDate>Tue, 11 Feb 2025 17:18:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/articles/it-unemployment-rises-to-5-7-as-ai-hits-tech-jobs-7726bb1b">https://www.wsj.com/articles/it-unemployment-rises-to-5-7-as-ai-hits-tech-jobs-7726bb1b</a>, See on <a href="https://news.ycombinator.com/item?id=43015397">Hacker News</a></p>
Couldn't get https://www.wsj.com/articles/it-unemployment-rises-to-5-7-as-ai-hits-tech-jobs-7726bb1b: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Launch HN: A0.dev (YC W25) – React Native App Generator (156 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=43015267</link>
            <guid>43015267</guid>
            <pubDate>Tue, 11 Feb 2025 17:08:35 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=43015267">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Hi HN — we’re Seth and Ayo and we’re building a0.dev (<a href="https://a0.dev/">https://a0.dev</a>). a0.dev is a platform built to cut down React Native development time from weeks to just a few hours.</p><p>We’ve been building mobile apps together for seven years and have had several successes. One thing that’s always bothered us though is how much harder it is to build a mobile app than a website. If you’ve built an app with React Native before you’re familiar with the pains of working with both Xcode and Android Studio, writing tons of boilerplate before you can even start making the app, setting up state management, and of course going through the dreaded app review. We decided to build a platform that would make the app development process faster.</p><p>We’ve seen the success of new code-gen platforms like v0 and wanted something for React Native that goes further. We built an AI app generator that takes a user's prompt and creates a custom React Native app with an instant live preview.</p><p>Here’s a 5min demo where we recreated the Hacker News UI: <a href="https://youtu.be/f3lzBRBUous" rel="nofollow">https://youtu.be/f3lzBRBUous</a></p><p>a0.dev is great for quickly prototyping components and screens in React Native and users are able to copy the code from our generator into their preferred development environment. Our landing page has a couple of example prompts, but we encourage you to get creative when trying it out. We’ve had success generating not only functional screens like an Instagram Feed but also 2d games like Minesweeper or Flappy Bird.</p><p>Our chat has a “UI Expert” and “Advanced Logic” model users can switch between depending on the task at hand. Users can upgrade from working on a single screen to creating a full app by clicking on the “Need a Full App” button in the top right of the page. This changes the scope from a standalone chat with a single file to a full project that can include multiple chats and files. We launched an IOS app that users can download in order to preview the app on a physical device. We find that many apps look and feel better on a physical device so we recommend trying it out.</p><p>Our goal is to continue to improve the app generator while adding more features to help developers get their apps to the app store and make money from those apps. The main features on our roadmap right now are a Supabase integration and a “one click submit” button to let developers publish their app to the App Store.</p><p>There are a few limitations to note. We’re working on releasing our Android app, but Android users should be able to preview their app using the Expo Go App. The app is running React Native Web in the browser so any dependencies that don’t support web won’t work with the web preview but should work on the phone. There are also some dependencies that our system can’t handle because they require native modules that aren’t packaged into our app currently.</p><p>We hope you guys will check it out and try making an app with a0.dev. We’re available on Discord around the clock to help developers with any problems they may face and want to guide people to actually releasing their app on the App Store. Let us know what features you’d like to see and any problems you’ve faced building apps, we’d love to hear about your experience.</p><p>Here’s the link again to check it out: (<a href="https://a0.dev/">https://a0.dev</a>)</p><p>We dropped the need to sign up for the first message so you can just jump in and try it out.</p><p>Looking forward to your thoughts!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LLMs can teach themselves to better predict the future (136 pts)]]></title>
            <link>https://arxiv.org/abs/2502.05253</link>
            <guid>43014918</guid>
            <pubDate>Tue, 11 Feb 2025 16:40:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2502.05253">https://arxiv.org/abs/2502.05253</a>, See on <a href="https://news.ycombinator.com/item?id=43014918">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2502.05253">View PDF</a></p><blockquote>
            <span>Abstract:</span>We present an outcome-driven fine-tuning framework that enhances the forecasting capabilities of large language models (LLMs) without relying on human-curated reasoning samples. Our method leverages model self-play to generate pairs of diverse reasoning trajectories and probabilistic forecasts for a set of diverse questions that resolve after the models' knowledge cutoff date. We then rank pairs of these reasoning traces by their distance to the actual outcomes before fine-tuning the model via Direct Preference Optimization (DPO). On a separate test set, our approach increases prediction accuracy of Phi-4 14B and DeepSeek-R1 14B by between 7--10\% over a base model and a DPO fine-tuned control model with randomized labels, bringing them on par with forecasting capabilities of much larger frontier models like GPT-4o.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Philipp Schoenegger [<a href="https://arxiv.org/show-email/16b2504e/2502.05253" rel="nofollow">view email</a>]      <br>    <strong>[v1]</strong>
        Fri, 7 Feb 2025 17:21:16 UTC (151 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Intel's Battlemage Architecture (148 pts)]]></title>
            <link>https://chipsandcheese.com/p/intels-battlemage-architecture</link>
            <guid>43014408</guid>
            <pubDate>Tue, 11 Feb 2025 16:00:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chipsandcheese.com/p/intels-battlemage-architecture">https://chipsandcheese.com/p/intels-battlemage-architecture</a>, See on <a href="https://news.ycombinator.com/item?id=43014408">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p>Intel’s Alchemist architecture gave the company a foot in the door to the high performance graphics segment. The Arc A770 proved to be a competent first effort, able to run many games with credible performance. Now, Intel is passing the torch to a new graphics architecture, named Battlemage.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa01015c1-e904-4218-a331-f80fc0852a4a_688x458.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa01015c1-e904-4218-a331-f80fc0852a4a_688x458.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa01015c1-e904-4218-a331-f80fc0852a4a_688x458.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa01015c1-e904-4218-a331-f80fc0852a4a_688x458.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa01015c1-e904-4218-a331-f80fc0852a4a_688x458.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa01015c1-e904-4218-a331-f80fc0852a4a_688x458.jpeg" width="688" height="458" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a01015c1-e904-4218-a331-f80fc0852a4a_688x458.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:458,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa01015c1-e904-4218-a331-f80fc0852a4a_688x458.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa01015c1-e904-4218-a331-f80fc0852a4a_688x458.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa01015c1-e904-4218-a331-f80fc0852a4a_688x458.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa01015c1-e904-4218-a331-f80fc0852a4a_688x458.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>Like Alchemist, Battlemage targets the midrange segment. It doesn’t try to compete with AMD or Nvidia’s high end cards. While it’s not as flashy as Nvidia’s RTX 4090 or AMD’s RX 7900 XTX, midrange GPUs account for a much larger share of the discrete GPU market, thanks to their lower prices. Unfortunately, today’s midrange cards like the RTX 4060 and RX 7600 only come with 8 GB of VRAM, and are poor value. Intel takes advantage of this by launching the Arc B580 at $250, undercutting both competitors while offering 12 GB of VRAM.</p><p>For B580 to be successful, its new Battlemage architecture has to execute well across a variety of graphics workloads. Intel has made numerous improvements over Alchemist, aiming to achieve better performance with less compute power and less memory bandwidth. I’ll be looking at the Arc B580, with comparison data from the A770 and A750, as well as scattered data I have lying around.</p><p>Battlemage is organized much like its predecessor. Xe Cores continue to act as a basic building block. Four Xe Cores are grouped into a Render Slice, which also contains render backends, a rasterizer, and associated caches for those fixed function units. The entire GPU shares an 18 MB L2 cache.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fb437e0-8218-4919-a3ca-a5ab7c3be9ac_1520x888.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fb437e0-8218-4919-a3ca-a5ab7c3be9ac_1520x888.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fb437e0-8218-4919-a3ca-a5ab7c3be9ac_1520x888.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fb437e0-8218-4919-a3ca-a5ab7c3be9ac_1520x888.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fb437e0-8218-4919-a3ca-a5ab7c3be9ac_1520x888.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fb437e0-8218-4919-a3ca-a5ab7c3be9ac_1520x888.png" width="1456" height="851" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3fb437e0-8218-4919-a3ca-a5ab7c3be9ac_1520x888.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:851,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:389547,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fb437e0-8218-4919-a3ca-a5ab7c3be9ac_1520x888.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fb437e0-8218-4919-a3ca-a5ab7c3be9ac_1520x888.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fb437e0-8218-4919-a3ca-a5ab7c3be9ac_1520x888.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fb437e0-8218-4919-a3ca-a5ab7c3be9ac_1520x888.png 1456w" sizes="100vw"></picture></div></a><figcaption>Block diagram of Intel’s Arc B580. B570 disables two Xe Cores. Only FP32 units shown because I generated this diagram using Javascript and heavy abuse of the CSS box model</figcaption></figure></div><p>The Arc B580 overall is a smaller GPU than its outgoing Alchemist predecessors. B580 has five Render Slices to A770’s eight. In total, B580 has 2560 FP32 lanes to A770’s 4096.</p><p>Battlemage launches with a smaller memory subsystem too. The B580 has a 192-bit GDDR6 bus running at 19 GT/s, giving it 456 GB/s of theoretical bandwidth. A770 has 560 GB/s of GDDR6 bandwidth, thanks to a 256-bit bus running at 17.5 GT/s.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2d57d9c-5da9-444d-be78-669ae197e94d_1658x756.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2d57d9c-5da9-444d-be78-669ae197e94d_1658x756.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2d57d9c-5da9-444d-be78-669ae197e94d_1658x756.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2d57d9c-5da9-444d-be78-669ae197e94d_1658x756.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2d57d9c-5da9-444d-be78-669ae197e94d_1658x756.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2d57d9c-5da9-444d-be78-669ae197e94d_1658x756.webp" width="1456" height="664" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d2d57d9c-5da9-444d-be78-669ae197e94d_1658x756.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:664,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:10640,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2d57d9c-5da9-444d-be78-669ae197e94d_1658x756.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2d57d9c-5da9-444d-be78-669ae197e94d_1658x756.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2d57d9c-5da9-444d-be78-669ae197e94d_1658x756.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2d57d9c-5da9-444d-be78-669ae197e94d_1658x756.webp 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Block diagram of the A770. A750 disables four Xe Cores (a whole Render Slice)</figcaption></figure></div><p>Even the host interface has been cut down. B580 only has a PCIe 4.0 x8 link, while A770 gets a full size x16 one. Intel’s new architecture has a lot of heavy lifting to do if it wants to beat a much larger implementation of its predecessor.</p><p>Battlemage’s architectural changes start at its Xe Cores. The most substantial changes between the two generations actually debuted on Lunar Lake. Xe Cores are further split into XVEs, or Xe Vector engines. Intel merged pairs of Alchemist XVEs into ones that are twice as wide, completing a transition towards larger execution unit partitions. Xe Core throughput stays the same at 128 FP32 operations per cycle.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbfc8b23b-abec-4d67-9f51-c324a6e2f5f5_688x386.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbfc8b23b-abec-4d67-9f51-c324a6e2f5f5_688x386.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbfc8b23b-abec-4d67-9f51-c324a6e2f5f5_688x386.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbfc8b23b-abec-4d67-9f51-c324a6e2f5f5_688x386.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbfc8b23b-abec-4d67-9f51-c324a6e2f5f5_688x386.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbfc8b23b-abec-4d67-9f51-c324a6e2f5f5_688x386.jpeg" width="688" height="386" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bfc8b23b-abec-4d67-9f51-c324a6e2f5f5_688x386.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:386,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbfc8b23b-abec-4d67-9f51-c324a6e2f5f5_688x386.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbfc8b23b-abec-4d67-9f51-c324a6e2f5f5_688x386.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbfc8b23b-abec-4d67-9f51-c324a6e2f5f5_688x386.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbfc8b23b-abec-4d67-9f51-c324a6e2f5f5_688x386.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>A shared instruction cache feeds all eight XVEs in a Xe Core. Alchemist had a 96 KB instruction cache, and Battlemage almost certainly has an instruction cache at least as large. Instructions on Intel GPUs are generally 16 bytes long, with a 8 byte compacted form in some cases. A 96 KB instruction cache therefore has a nominal capacity of 6-12K instructions.</p><p>XVEs form the smallest partition in Intel GPUs. Each XVE tracks up to eight threads, switching between them to hide latency and keep its execution units fed. A 64 KB register file stores thread state, giving each thread up to 8 KB of registers while maintaining maximum occupancy. Giving a register count for Intel GPUs doesn’t really work, because Intel GPU instructions can address the register file with far more flexibility than Nvidia or AMD architectures. Each instruction can specify a vector width, and access a register as small as a single scalar element.</p><p><span>For most math instructions, Battlemage sticks with 16-wide or 32-wide vectors, dropping the SIMD8 mode that could show up with Alchemist. Vector execution reduces instruction control overhead because a single operation gets applied across all lanes in the vector. However, that results in lost throughput if some lanes take a different branch direction. On paper, Battlemage’s longer native vector lengths would make it more prone to suffering such divergence penalties. But Alchemist awkwardly shared control logic between XVE pairs, making SIMD8 act like SIMD16, and SIMD16 act a lot like SIMD64 aside from a funny corner case (see the </span><a href="https://chipsandcheese.com/p/intels-ambitious-meteor-lake-igpu" rel="">Meteor Lake article</a><span> for more on that).</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bc6bd-5a8f-459c-ae20-bbaab86418a2_1336x680.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bc6bd-5a8f-459c-ae20-bbaab86418a2_1336x680.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bc6bd-5a8f-459c-ae20-bbaab86418a2_1336x680.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bc6bd-5a8f-459c-ae20-bbaab86418a2_1336x680.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bc6bd-5a8f-459c-ae20-bbaab86418a2_1336x680.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bc6bd-5a8f-459c-ae20-bbaab86418a2_1336x680.png" width="1336" height="680" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4b6bc6bd-5a8f-459c-ae20-bbaab86418a2_1336x680.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:680,&quot;width&quot;:1336,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:119096,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bc6bd-5a8f-459c-ae20-bbaab86418a2_1336x680.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bc6bd-5a8f-459c-ae20-bbaab86418a2_1336x680.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bc6bd-5a8f-459c-ae20-bbaab86418a2_1336x680.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b6bc6bd-5a8f-459c-ae20-bbaab86418a2_1336x680.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Battlemage’s divergence behavior by comparison is intuitive and straightforward. SIMD16 achieves full utilization if groups of 16 threads go the same way. The same applies for SIMD32 and groups of 32 coherent threads. Thus Battlemage is actually more agile than its predecessor when dealing with divergent branches, while enjoying the efficiency advantage of using larger vectors.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F294af72e-1736-492f-b647-f40edfd1fcd0_1650x580.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F294af72e-1736-492f-b647-f40edfd1fcd0_1650x580.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F294af72e-1736-492f-b647-f40edfd1fcd0_1650x580.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F294af72e-1736-492f-b647-f40edfd1fcd0_1650x580.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F294af72e-1736-492f-b647-f40edfd1fcd0_1650x580.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F294af72e-1736-492f-b647-f40edfd1fcd0_1650x580.png" width="1456" height="512" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/294af72e-1736-492f-b647-f40edfd1fcd0_1650x580.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:512,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:153269,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F294af72e-1736-492f-b647-f40edfd1fcd0_1650x580.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F294af72e-1736-492f-b647-f40edfd1fcd0_1650x580.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F294af72e-1736-492f-b647-f40edfd1fcd0_1650x580.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F294af72e-1736-492f-b647-f40edfd1fcd0_1650x580.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Maybe XMX is on a separate port. Maybe not. I’m not sure</figcaption></figure></div><p>Like Alchemist, Battlemage executes most math operations down two ports (ALU0, ALU1). ALU0 handles basic FP32 and FP16 operations, while ALU1 handles integer math and less common instructions. Intel’s port layout has parallels to Nvidia’s Turing, which also splits dispatch bandwidth between 16-wide FP32 and INT32 units. A key difference is that Turing uses fixed 32-wide vectors, and keeps both units occupied by feeding them on alternate cycles. Intel can issue instructions of the same type back-to-back, and can select multiple instructions to issue per cycle to different ports.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dc7aecd-d6fe-40df-9180-8c3e4b7196b2_688x388.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dc7aecd-d6fe-40df-9180-8c3e4b7196b2_688x388.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dc7aecd-d6fe-40df-9180-8c3e4b7196b2_688x388.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dc7aecd-d6fe-40df-9180-8c3e4b7196b2_688x388.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dc7aecd-d6fe-40df-9180-8c3e4b7196b2_688x388.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dc7aecd-d6fe-40df-9180-8c3e4b7196b2_688x388.jpeg" width="688" height="388" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5dc7aecd-d6fe-40df-9180-8c3e4b7196b2_688x388.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:388,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dc7aecd-d6fe-40df-9180-8c3e4b7196b2_688x388.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dc7aecd-d6fe-40df-9180-8c3e4b7196b2_688x388.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dc7aecd-d6fe-40df-9180-8c3e4b7196b2_688x388.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dc7aecd-d6fe-40df-9180-8c3e4b7196b2_688x388.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>In another similarity to Turing, Battlemage carries forward Alchemist’s “XMX” matrix multiplication units. Intel claims 3-way co-issue, implying XMX is on a separate port. However, VTune only shows multiple pipe active metrics for ALU0+ALU1 and ALU0+XMX. I’ve drawn XMX as a separate port above, but the XMX units could be on ALU1.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba832324-3a01-430d-a741-1cf0ce541498_1639x804.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba832324-3a01-430d-a741-1cf0ce541498_1639x804.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba832324-3a01-430d-a741-1cf0ce541498_1639x804.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba832324-3a01-430d-a741-1cf0ce541498_1639x804.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba832324-3a01-430d-a741-1cf0ce541498_1639x804.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba832324-3a01-430d-a741-1cf0ce541498_1639x804.png" width="1456" height="714" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ba832324-3a01-430d-a741-1cf0ce541498_1639x804.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:714,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:228654,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba832324-3a01-430d-a741-1cf0ce541498_1639x804.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba832324-3a01-430d-a741-1cf0ce541498_1639x804.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba832324-3a01-430d-a741-1cf0ce541498_1639x804.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba832324-3a01-430d-a741-1cf0ce541498_1639x804.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Data collected from Intel’s VTune profiler, zoomed in to show what’s happening at the millisecond scale. VTune’s y-axis scaling is funny (relative to max observed utilization rather than 100%), so I’ve labeled some interesting points.</figcaption></figure></div><p>Gaming workloads tend to use more floating point operations. During compute heavy sections, ALU1 offloads other operations and keeps ALU0 free to deal with floating point math. XeSS exercises the XMX unit, with minimal co-issue alongside vector operations. A generative AI workload shows even less XMX+vector co-issue.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9cf9e38-bd8c-40f8-88f3-5901ccd38e36_1095x954.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9cf9e38-bd8c-40f8-88f3-5901ccd38e36_1095x954.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9cf9e38-bd8c-40f8-88f3-5901ccd38e36_1095x954.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9cf9e38-bd8c-40f8-88f3-5901ccd38e36_1095x954.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9cf9e38-bd8c-40f8-88f3-5901ccd38e36_1095x954.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9cf9e38-bd8c-40f8-88f3-5901ccd38e36_1095x954.png" width="1095" height="954" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c9cf9e38-bd8c-40f8-88f3-5901ccd38e36_1095x954.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:954,&quot;width&quot;:1095,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:129388,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9cf9e38-bd8c-40f8-88f3-5901ccd38e36_1095x954.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9cf9e38-bd8c-40f8-88f3-5901ccd38e36_1095x954.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9cf9e38-bd8c-40f8-88f3-5901ccd38e36_1095x954.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9cf9e38-bd8c-40f8-88f3-5901ccd38e36_1095x954.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>As expected for any specialized execution unit, XMX software support is far from guaranteed. Running AI image generation or language models using other frameworks heavily exercises B580’s regular vector units, while leaving the XMX units idle.</p><p>In microbenchmarks, Intel’s older A770 and A750 can often use their larger shader arrays to achieve higher compute throughput than B580. However, B580 behaves more consistently. Alchemist had trouble with FP32 FMA operations. Battlemage in contrast has no problem getting right up to its theoretical throughput. FP32+INT32 dual issue doesn’t happen perfectly on Battlemage, but it barely happened at all on A750.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb0c6ed0-9249-4362-824f-6c2e1e1b3376_951x681.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb0c6ed0-9249-4362-824f-6c2e1e1b3376_951x681.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb0c6ed0-9249-4362-824f-6c2e1e1b3376_951x681.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb0c6ed0-9249-4362-824f-6c2e1e1b3376_951x681.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb0c6ed0-9249-4362-824f-6c2e1e1b3376_951x681.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb0c6ed0-9249-4362-824f-6c2e1e1b3376_951x681.png" width="951" height="681" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/db0c6ed0-9249-4362-824f-6c2e1e1b3376_951x681.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:681,&quot;width&quot;:951,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:84661,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb0c6ed0-9249-4362-824f-6c2e1e1b3376_951x681.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb0c6ed0-9249-4362-824f-6c2e1e1b3376_951x681.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb0c6ed0-9249-4362-824f-6c2e1e1b3376_951x681.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb0c6ed0-9249-4362-824f-6c2e1e1b3376_951x681.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>On the integer side, Battlemage is better at dealing with lower precision INT8 operations. Using Meteor Lake’s iGPU as a proxy, Intel’s last generation architecture used </span><code>mov</code><span> and </span><code>add</code><span> instruction pairs to handle </span><code>char16</code><span> adds, while Battlemage gets it done with just an </span><code>add</code><span>.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69cc2d52-20ed-4dc8-8804-9ee913cd2522_950x572.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69cc2d52-20ed-4dc8-8804-9ee913cd2522_950x572.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69cc2d52-20ed-4dc8-8804-9ee913cd2522_950x572.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69cc2d52-20ed-4dc8-8804-9ee913cd2522_950x572.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69cc2d52-20ed-4dc8-8804-9ee913cd2522_950x572.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69cc2d52-20ed-4dc8-8804-9ee913cd2522_950x572.png" width="950" height="572" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/69cc2d52-20ed-4dc8-8804-9ee913cd2522_950x572.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:572,&quot;width&quot;:950,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:81043,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69cc2d52-20ed-4dc8-8804-9ee913cd2522_950x572.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69cc2d52-20ed-4dc8-8804-9ee913cd2522_950x572.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69cc2d52-20ed-4dc8-8804-9ee913cd2522_950x572.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69cc2d52-20ed-4dc8-8804-9ee913cd2522_950x572.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Each XVE also has a branch port for control flow instructions, and a “send” port that lets the XVE talk with the outside world. Load on these ports is typically low, because GPU programs don’t branch as often as CPU ones, and shared functions accessed through the “send” port won’t have enough throughput to handle all XVEs hitting it at the same time.</p><p>Battlemage’s memory subsystem has a lot in common with Alchemist’s, and traces its origins to Intel’s integrated graphics architectures over the past decade. XVEs access the memory hierarchy by sending a message to the appropriate shared functional unit. At one point, the entire iGPU was basically the equivalent of a Xe Core, with XVE equivalents acting as basic building blocks. XVEs would access the iGPU’s texture units, caches, and work distribution hardware over a messaging fabric. Intel has since built larger subdivisions, but the terminology remains.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa07a5a41-b1b0-4ba1-b0c8-963e6d0d324a_993x562.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa07a5a41-b1b0-4ba1-b0c8-963e6d0d324a_993x562.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa07a5a41-b1b0-4ba1-b0c8-963e6d0d324a_993x562.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa07a5a41-b1b0-4ba1-b0c8-963e6d0d324a_993x562.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa07a5a41-b1b0-4ba1-b0c8-963e6d0d324a_993x562.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa07a5a41-b1b0-4ba1-b0c8-963e6d0d324a_993x562.png" width="993" height="562" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a07a5a41-b1b0-4ba1-b0c8-963e6d0d324a_993x562.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:562,&quot;width&quot;:993,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:97381,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa07a5a41-b1b0-4ba1-b0c8-963e6d0d324a_993x562.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa07a5a41-b1b0-4ba1-b0c8-963e6d0d324a_993x562.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa07a5a41-b1b0-4ba1-b0c8-963e6d0d324a_993x562.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa07a5a41-b1b0-4ba1-b0c8-963e6d0d324a_993x562.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Each Xe Core has eight TMUs, or texture samplers in Intel terminology. The samplers have a 32 KB texture cache, and can return 128 bytes/cycle to the XVEs. Battlemage is no different from Alchemist in this respect. But the B580 has less texture bandwidth on tap than its predecessor. Its higher clock speed isn’t enough to compensate for having far fewer Xe Cores.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa15ce21c-ff3c-4eec-b4d6-24fc1252809f_736x457.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa15ce21c-ff3c-4eec-b4d6-24fc1252809f_736x457.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa15ce21c-ff3c-4eec-b4d6-24fc1252809f_736x457.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa15ce21c-ff3c-4eec-b4d6-24fc1252809f_736x457.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa15ce21c-ff3c-4eec-b4d6-24fc1252809f_736x457.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa15ce21c-ff3c-4eec-b4d6-24fc1252809f_736x457.png" width="736" height="457" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a15ce21c-ff3c-4eec-b4d6-24fc1252809f_736x457.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:457,&quot;width&quot;:736,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:50376,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa15ce21c-ff3c-4eec-b4d6-24fc1252809f_736x457.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa15ce21c-ff3c-4eec-b4d6-24fc1252809f_736x457.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa15ce21c-ff3c-4eec-b4d6-24fc1252809f_736x457.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa15ce21c-ff3c-4eec-b4d6-24fc1252809f_736x457.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>B580 runs at higher clock speeds, which brings down texture cache hit latency too. In clock cycle terms though, Battlemage has nearly identical texture cache hit latency to its predecessor. L2 latency has improved significantly, so missing the texture cache isn’t as bad on Battlemage.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1bea6719-f463-4a5d-ac27-741807b76ed0_1299x641.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1bea6719-f463-4a5d-ac27-741807b76ed0_1299x641.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1bea6719-f463-4a5d-ac27-741807b76ed0_1299x641.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1bea6719-f463-4a5d-ac27-741807b76ed0_1299x641.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1bea6719-f463-4a5d-ac27-741807b76ed0_1299x641.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1bea6719-f463-4a5d-ac27-741807b76ed0_1299x641.png" width="1299" height="641" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1bea6719-f463-4a5d-ac27-741807b76ed0_1299x641.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:641,&quot;width&quot;:1299,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:96524,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1bea6719-f463-4a5d-ac27-741807b76ed0_1299x641.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1bea6719-f463-4a5d-ac27-741807b76ed0_1299x641.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1bea6719-f463-4a5d-ac27-741807b76ed0_1299x641.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1bea6719-f463-4a5d-ac27-741807b76ed0_1299x641.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Global memory accesses are first cached in a 256 KB block, which serves double duty as Shared Local Memory (SLM). It’s larger than Alchemist and Lunar Lake’s 192 KB L1/SLM block, so Intel has found the transistor budget to keep more data closer to the execution units. Like Lunar Lake, B580 favors SLM over L1 capacity even when a compute kernel doesn’t allocate local memory.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa89ce42-cbf8-4fb4-b660-67a5ab3f47a2_1042x377.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa89ce42-cbf8-4fb4-b660-67a5ab3f47a2_1042x377.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa89ce42-cbf8-4fb4-b660-67a5ab3f47a2_1042x377.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa89ce42-cbf8-4fb4-b660-67a5ab3f47a2_1042x377.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa89ce42-cbf8-4fb4-b660-67a5ab3f47a2_1042x377.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa89ce42-cbf8-4fb4-b660-67a5ab3f47a2_1042x377.png" width="1042" height="377" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/aa89ce42-cbf8-4fb4-b660-67a5ab3f47a2_1042x377.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:377,&quot;width&quot;:1042,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:58696,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa89ce42-cbf8-4fb4-b660-67a5ab3f47a2_1042x377.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa89ce42-cbf8-4fb4-b660-67a5ab3f47a2_1042x377.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa89ce42-cbf8-4fb4-b660-67a5ab3f47a2_1042x377.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa89ce42-cbf8-4fb4-b660-67a5ab3f47a2_1042x377.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Intel may be able to split the L1/SLM block in another way, but a latency test shows exactly the same result regardless of whether I allocate local memory. Testing with Nemes’s Vulkan test suite also shows 96 KB of L1.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20da4947-57bc-47b6-bbd2-ee0555fdec06_1477x757.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20da4947-57bc-47b6-bbd2-ee0555fdec06_1477x757.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20da4947-57bc-47b6-bbd2-ee0555fdec06_1477x757.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20da4947-57bc-47b6-bbd2-ee0555fdec06_1477x757.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20da4947-57bc-47b6-bbd2-ee0555fdec06_1477x757.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20da4947-57bc-47b6-bbd2-ee0555fdec06_1477x757.png" width="1456" height="746" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/20da4947-57bc-47b6-bbd2-ee0555fdec06_1477x757.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:746,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:126435,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20da4947-57bc-47b6-bbd2-ee0555fdec06_1477x757.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20da4947-57bc-47b6-bbd2-ee0555fdec06_1477x757.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20da4947-57bc-47b6-bbd2-ee0555fdec06_1477x757.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20da4947-57bc-47b6-bbd2-ee0555fdec06_1477x757.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Global memory access on Battlemage offers lower latency than texture accesses, even though the XVEs have to handle array address generation. With texture accesses, the TMUs do all the address calculations. All the XVEs do is send them a message. L1 data cache latency is similar to Alchemist in clock cycle terms, though again higher clock speeds give B580 an actual latency advantage.</p><p>Battlemage gets a clock cycle latency reduction too with scalar memory accesses. Intel does not have separate scalar instructions like AMD. But Intel’s GPU ISA lets each instruction specify its SIMD width, and SIMD1 instructions are possible. Intel’s compiler has been carrying out scalar optimizations and opportunistically generating SIMD1 instructions well before Battlemage, but there was no performance difference as far as I could tell. Now there is.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe081ae70-648a-4b5a-939f-d81f36c126d3_1288x596.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe081ae70-648a-4b5a-939f-d81f36c126d3_1288x596.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe081ae70-648a-4b5a-939f-d81f36c126d3_1288x596.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe081ae70-648a-4b5a-939f-d81f36c126d3_1288x596.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe081ae70-648a-4b5a-939f-d81f36c126d3_1288x596.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe081ae70-648a-4b5a-939f-d81f36c126d3_1288x596.png" width="1288" height="596" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e081ae70-648a-4b5a-939f-d81f36c126d3_1288x596.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:596,&quot;width&quot;:1288,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:103635,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe081ae70-648a-4b5a-939f-d81f36c126d3_1288x596.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe081ae70-648a-4b5a-939f-d81f36c126d3_1288x596.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe081ae70-648a-4b5a-939f-d81f36c126d3_1288x596.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe081ae70-648a-4b5a-939f-d81f36c126d3_1288x596.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Forcing SIMD16 mode saves one cycle of latency over SIMD32, because address generation instructions don’t have to issue over two cycles</figcaption></figure></div><p>On B580, L1 latency for a SIMD1 (scalar) access is about 15 cycles faster than a SIMD16 access. SIMD32 accesses take one extra cycle when microbenchmarking, though that’s because the compiler generates two sets of SIMD16 instructions to calculate addresses across 32 lanes. I also got Intel’s compiler to emit scalar INT32 adds, but those didn’t see improved latency over vector ones. Therefore, the scalar latency improvements almost certainly come from an optimized memory pipeline.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb0e06c6-20db-4237-8b4b-9f07c3352917_1619x425.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb0e06c6-20db-4237-8b4b-9f07c3352917_1619x425.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb0e06c6-20db-4237-8b4b-9f07c3352917_1619x425.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb0e06c6-20db-4237-8b4b-9f07c3352917_1619x425.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb0e06c6-20db-4237-8b4b-9f07c3352917_1619x425.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb0e06c6-20db-4237-8b4b-9f07c3352917_1619x425.png" width="1456" height="382" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bb0e06c6-20db-4237-8b4b-9f07c3352917_1619x425.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:382,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:215818,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb0e06c6-20db-4237-8b4b-9f07c3352917_1619x425.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb0e06c6-20db-4237-8b4b-9f07c3352917_1619x425.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb0e06c6-20db-4237-8b4b-9f07c3352917_1619x425.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb0e06c6-20db-4237-8b4b-9f07c3352917_1619x425.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Scalar load, with simple explanations</figcaption></figure></div><p>SIMD1 instructions also help within the XVEs. Intel doesn’t use a separate scalar register file, but can more flexibly address their vector register file than AMD or Nvidia. Instructions can access individual elements (sub-registers) and read out whatever vector width they want. Intel’s compiler could pack many “scalar registers” into the equivalent of a vector register, economizing register file capacity.</p><p><span>I was able to get better efficiency out of B580’s L1 than A750’s using </span><code>float4</code><span> loads from a small array. Intel suggests Xe-HPG’s </span><a href="https://www.intel.com/content/www/us/en/docs/oneapi/optimization-guide-gpu/2024-2/intel-xe-gpu-architecture.html" rel="">L1 can deliver 512 bytes per cycle</a><span>, but I wasn’t able to get anywhere close on either Alchemist or Battlemage. Microbenchmarking puts per-Xe Core bandwidth at a bit under 256 bytes per cycle on both architectures.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc80d1a19-ac52-4b5b-b239-f4b191f1145e_889x520.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc80d1a19-ac52-4b5b-b239-f4b191f1145e_889x520.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc80d1a19-ac52-4b5b-b239-f4b191f1145e_889x520.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc80d1a19-ac52-4b5b-b239-f4b191f1145e_889x520.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc80d1a19-ac52-4b5b-b239-f4b191f1145e_889x520.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc80d1a19-ac52-4b5b-b239-f4b191f1145e_889x520.png" width="889" height="520" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c80d1a19-ac52-4b5b-b239-f4b191f1145e_889x520.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:520,&quot;width&quot;:889,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:47538,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc80d1a19-ac52-4b5b-b239-f4b191f1145e_889x520.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc80d1a19-ac52-4b5b-b239-f4b191f1145e_889x520.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc80d1a19-ac52-4b5b-b239-f4b191f1145e_889x520.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc80d1a19-ac52-4b5b-b239-f4b191f1145e_889x520.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Even if the L1 can only provide 256 bytes per cycle, that still gives Intel’s Xe Core as much L1 bandwidth as an AMD RDNA WGP, and twice as much L1 bandwidth as an Nvidia Ampere SM. 512 bytes per cycle would let each XVE complete a SIMD16 load every cycle, which is kind of overkill anyway.</p><p>Battlemage uses the same 256 KB block for L1 cache and SLM. SLM provides an address space local to a group of threads, and acts as a fast software managed scratchpad. In OpenCL, that’s exposed via the local memory type. Everyone likes to call it something different, but for this article I’ll use OpenCL and Intel’s term.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a3e4c7c-5499-4586-bbb1-5cf6c33c379a_829x333.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a3e4c7c-5499-4586-bbb1-5cf6c33c379a_829x333.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a3e4c7c-5499-4586-bbb1-5cf6c33c379a_829x333.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a3e4c7c-5499-4586-bbb1-5cf6c33c379a_829x333.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a3e4c7c-5499-4586-bbb1-5cf6c33c379a_829x333.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a3e4c7c-5499-4586-bbb1-5cf6c33c379a_829x333.png" width="829" height="333" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7a3e4c7c-5499-4586-bbb1-5cf6c33c379a_829x333.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:333,&quot;width&quot;:829,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:26686,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a3e4c7c-5499-4586-bbb1-5cf6c33c379a_829x333.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a3e4c7c-5499-4586-bbb1-5cf6c33c379a_829x333.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a3e4c7c-5499-4586-bbb1-5cf6c33c379a_829x333.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a3e4c7c-5499-4586-bbb1-5cf6c33c379a_829x333.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Even though both local memory and L1 cache hits are backed by the same physical storage, SLM accesses enjoy better latency. Unlike cache hits, SLM accesses don’t need tag checks or address translation. Accessing Battlemage’s 256 KB block of memory in SLM mode brings latency down to just over 15 ns. It’s faster than doing the same on Alchemist, and is very competitive against recent GPUs from AMD and Nvidia.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F058a3b2d-785d-4e51-9771-af3cd7ab10e5_1035x685.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F058a3b2d-785d-4e51-9771-af3cd7ab10e5_1035x685.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F058a3b2d-785d-4e51-9771-af3cd7ab10e5_1035x685.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F058a3b2d-785d-4e51-9771-af3cd7ab10e5_1035x685.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F058a3b2d-785d-4e51-9771-af3cd7ab10e5_1035x685.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F058a3b2d-785d-4e51-9771-af3cd7ab10e5_1035x685.png" width="1035" height="685" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/058a3b2d-785d-4e51-9771-af3cd7ab10e5_1035x685.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:685,&quot;width&quot;:1035,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:73939,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F058a3b2d-785d-4e51-9771-af3cd7ab10e5_1035x685.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F058a3b2d-785d-4e51-9771-af3cd7ab10e5_1035x685.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F058a3b2d-785d-4e51-9771-af3cd7ab10e5_1035x685.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F058a3b2d-785d-4e51-9771-af3cd7ab10e5_1035x685.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Local memory/SLM also lets threads within a workgroup synchronize and exchange data. From testing with </span><code>atomic_cmpxchg</code><span> on local memory, B580 can bounce values between threads a bit faster than its predecessor. Nearly all of that improvement is down to higher clock speed, but it’s enough to bring B580 in line with AMD and Nvidia’s newer GPUs.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95c108e-b36a-4f79-b482-6b3e4ef4fa25_699x396.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95c108e-b36a-4f79-b482-6b3e4ef4fa25_699x396.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95c108e-b36a-4f79-b482-6b3e4ef4fa25_699x396.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95c108e-b36a-4f79-b482-6b3e4ef4fa25_699x396.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95c108e-b36a-4f79-b482-6b3e4ef4fa25_699x396.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95c108e-b36a-4f79-b482-6b3e4ef4fa25_699x396.png" width="699" height="396" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d95c108e-b36a-4f79-b482-6b3e4ef4fa25_699x396.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:396,&quot;width&quot;:699,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:48111,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95c108e-b36a-4f79-b482-6b3e4ef4fa25_699x396.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95c108e-b36a-4f79-b482-6b3e4ef4fa25_699x396.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95c108e-b36a-4f79-b482-6b3e4ef4fa25_699x396.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd95c108e-b36a-4f79-b482-6b3e4ef4fa25_699x396.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Backing structures for local memory often contain dedicated ALUs for handling atomic operations. For example, the LDS on AMD’s RDNA architecture is split into 32 banks, with one atomic ALU per bank. Intel almost certainly has something similar, and I’m testing that with </span><code>atomic_add</code><span> operations on local memory. Each thread targets a different address across an array, aiming to avoid contention.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1702a6a8-8df1-4620-a778-cdd153c70ac8_853x487.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1702a6a8-8df1-4620-a778-cdd153c70ac8_853x487.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1702a6a8-8df1-4620-a778-cdd153c70ac8_853x487.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1702a6a8-8df1-4620-a778-cdd153c70ac8_853x487.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1702a6a8-8df1-4620-a778-cdd153c70ac8_853x487.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1702a6a8-8df1-4620-a778-cdd153c70ac8_853x487.png" width="853" height="487" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1702a6a8-8df1-4620-a778-cdd153c70ac8_853x487.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:487,&quot;width&quot;:853,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:43470,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1702a6a8-8df1-4620-a778-cdd153c70ac8_853x487.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1702a6a8-8df1-4620-a778-cdd153c70ac8_853x487.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1702a6a8-8df1-4620-a778-cdd153c70ac8_853x487.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1702a6a8-8df1-4620-a778-cdd153c70ac8_853x487.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Alchemist and Battlemage both appear to have 32 atomic ALUs attached to each Xe Core’s SLM unit, much like AMD’s RDNA and Nvidia’s Pascal. Meteor Lake’s Xe-LPG architecture may have half as many atomic ALUs per Xe Core.</p><p>Battlemage has a two level cache hierarchy like its predecessor and Nvidia’s current GPUs. B580’s 18 MB L2 is slightly larger than A770’s 16 MB L2. A770 divided its L2 into 32 banks, each capable of handling a 64 byte access every cycle. At 2.4 GHz, that’s good for nearly 5 TB/s of bandwidth.</p><p>Intel didn’t disclose B580’s L2 topology, but a reasonable assumption is that Intel increased bank size from 512 to 768 KB, keeping 4 L2 banks tied to each memory controller. If so, B580’s L2 would have 24 banks and 4.3 TB/s of theoretical bandwidth at 2.85 GHz. Microbenchmarking using Nemes’s Vulkan test gets a decent proportion of that bandwidth. Efficiency is much lower on the older A750, which gets approximately as much bandwidth as B580 despite probably having more theoretical L2 bandwidth on tap.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F315873c9-03fb-4cb8-9854-c5f37ce6f630_724x440.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F315873c9-03fb-4cb8-9854-c5f37ce6f630_724x440.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F315873c9-03fb-4cb8-9854-c5f37ce6f630_724x440.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F315873c9-03fb-4cb8-9854-c5f37ce6f630_724x440.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F315873c9-03fb-4cb8-9854-c5f37ce6f630_724x440.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F315873c9-03fb-4cb8-9854-c5f37ce6f630_724x440.png" width="724" height="440" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/315873c9-03fb-4cb8-9854-c5f37ce6f630_724x440.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:440,&quot;width&quot;:724,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:52255,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F315873c9-03fb-4cb8-9854-c5f37ce6f630_724x440.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F315873c9-03fb-4cb8-9854-c5f37ce6f630_724x440.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F315873c9-03fb-4cb8-9854-c5f37ce6f630_724x440.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F315873c9-03fb-4cb8-9854-c5f37ce6f630_724x440.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Besides insulating the execution units from slow VRAM, the L2 can act as a point of coherency across the GPU. B580 is pretty fast when bouncing data between threads using global memory, and is faster than its predecessor.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5fa1162-43e1-4e31-b19a-5037f21cdc7d_857x474.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5fa1162-43e1-4e31-b19a-5037f21cdc7d_857x474.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5fa1162-43e1-4e31-b19a-5037f21cdc7d_857x474.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5fa1162-43e1-4e31-b19a-5037f21cdc7d_857x474.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5fa1162-43e1-4e31-b19a-5037f21cdc7d_857x474.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5fa1162-43e1-4e31-b19a-5037f21cdc7d_857x474.png" width="857" height="474" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f5fa1162-43e1-4e31-b19a-5037f21cdc7d_857x474.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:474,&quot;width&quot;:857,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:42994,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5fa1162-43e1-4e31-b19a-5037f21cdc7d_857x474.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5fa1162-43e1-4e31-b19a-5037f21cdc7d_857x474.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5fa1162-43e1-4e31-b19a-5037f21cdc7d_857x474.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5fa1162-43e1-4e31-b19a-5037f21cdc7d_857x474.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>With atomic add operations on global memory, Battlemage does fine for a GPU of its size and massively outperforms its predecessor.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff1425276-fd44-4ec3-87b7-9cb8b62f0170_753x447.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff1425276-fd44-4ec3-87b7-9cb8b62f0170_753x447.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff1425276-fd44-4ec3-87b7-9cb8b62f0170_753x447.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff1425276-fd44-4ec3-87b7-9cb8b62f0170_753x447.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff1425276-fd44-4ec3-87b7-9cb8b62f0170_753x447.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff1425276-fd44-4ec3-87b7-9cb8b62f0170_753x447.png" width="753" height="447" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f1425276-fd44-4ec3-87b7-9cb8b62f0170_753x447.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:447,&quot;width&quot;:753,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:56292,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff1425276-fd44-4ec3-87b7-9cb8b62f0170_753x447.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff1425276-fd44-4ec3-87b7-9cb8b62f0170_753x447.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff1425276-fd44-4ec3-87b7-9cb8b62f0170_753x447.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff1425276-fd44-4ec3-87b7-9cb8b62f0170_753x447.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>I’m using INT32 operations, so 86.74 GOPS on the A750 would correspond to 351 GB/s of L2 bandwidth. On the B580, 220.97 GOPS would require 883.9 GB/s. VTune however reports far higher L2 bandwidth on A750. Somehow, A750 sees 1.37 TB/s of L2 bandwidth during the test, or nearly 4x more than it should need.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7687cc11-ce66-45b3-9e89-55aeb71a864d_1431x747.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7687cc11-ce66-45b3-9e89-55aeb71a864d_1431x747.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7687cc11-ce66-45b3-9e89-55aeb71a864d_1431x747.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7687cc11-ce66-45b3-9e89-55aeb71a864d_1431x747.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7687cc11-ce66-45b3-9e89-55aeb71a864d_1431x747.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7687cc11-ce66-45b3-9e89-55aeb71a864d_1431x747.webp" width="1431" height="747" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7687cc11-ce66-45b3-9e89-55aeb71a864d_1431x747.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:747,&quot;width&quot;:1431,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:24472,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7687cc11-ce66-45b3-9e89-55aeb71a864d_1431x747.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7687cc11-ce66-45b3-9e89-55aeb71a864d_1431x747.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7687cc11-ce66-45b3-9e89-55aeb71a864d_1431x747.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7687cc11-ce66-45b3-9e89-55aeb71a864d_1431x747.webp 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>VTune capture of the test running on A750</figcaption></figure></div><p>Meteor Lake’s iGPU is a close relative of Alchemist, but its ratio of global atomic add throughput to Xe Core count is similar to Battlemage’s. VTune reports Meteor Lake’s iGPU using more L2 bandwidth than required, but only by a factor of 2x. Curiously, it also shows the expected bandwidth coming off the XVEs. I wonder if something in Intel’s cross-GPU interconnect didn’t scale well with bigger GPUs.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f0a54d8-c90e-41a0-a6dc-4811dad42b59_1652x665.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f0a54d8-c90e-41a0-a6dc-4811dad42b59_1652x665.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f0a54d8-c90e-41a0-a6dc-4811dad42b59_1652x665.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f0a54d8-c90e-41a0-a6dc-4811dad42b59_1652x665.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f0a54d8-c90e-41a0-a6dc-4811dad42b59_1652x665.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f0a54d8-c90e-41a0-a6dc-4811dad42b59_1652x665.webp" width="1456" height="586" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1f0a54d8-c90e-41a0-a6dc-4811dad42b59_1652x665.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:586,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:31714,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f0a54d8-c90e-41a0-a6dc-4811dad42b59_1652x665.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f0a54d8-c90e-41a0-a6dc-4811dad42b59_1652x665.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f0a54d8-c90e-41a0-a6dc-4811dad42b59_1652x665.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f0a54d8-c90e-41a0-a6dc-4811dad42b59_1652x665.webp 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>With Battlemage, atomics are broken out into a separate category and aren’t reported as regular L2 bandwidth. VTune indicates atomics are passed through the load/store unit to L2 without any inflation. Furthermore, the L2 was only 79.6% busy, suggesting there’s a bit of headroom at that layer.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe26b6f9e-81eb-48ab-bba3-e74410a11f4f_1621x871.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe26b6f9e-81eb-48ab-bba3-e74410a11f4f_1621x871.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe26b6f9e-81eb-48ab-bba3-e74410a11f4f_1621x871.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe26b6f9e-81eb-48ab-bba3-e74410a11f4f_1621x871.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe26b6f9e-81eb-48ab-bba3-e74410a11f4f_1621x871.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe26b6f9e-81eb-48ab-bba3-e74410a11f4f_1621x871.webp" width="1456" height="782" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e26b6f9e-81eb-48ab-bba3-e74410a11f4f_1621x871.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:782,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:36124,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe26b6f9e-81eb-48ab-bba3-e74410a11f4f_1621x871.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe26b6f9e-81eb-48ab-bba3-e74410a11f4f_1621x871.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe26b6f9e-81eb-48ab-bba3-e74410a11f4f_1621x871.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe26b6f9e-81eb-48ab-bba3-e74410a11f4f_1621x871.webp 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>And the same test on B580</figcaption></figure></div><p><span>This could just be a performance monitoring improvement, but performance counters are typically closely tied to the underlying architecture. I suspect Intel made major changes to how they handle global memory atomics, letting performance scale better on larger GPUs. I’ve noticed that newer games </span><a href="https://x.com/lamchester/status/1853882650186203540" rel="">sometimes use global atomic operations</a><span>. Perhaps Intel noticed that too, and decided it was time to optimize them.</span></p><p>B580 has a 192-bit GDDR6 VRAM subsystem, likely configured as six 2×16-bit memory controllers. Latency from OpenCL is higher than it was in the previous generation.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a9adab3-e152-4a18-9f8f-ba89ad7fba6f_1468x755.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a9adab3-e152-4a18-9f8f-ba89ad7fba6f_1468x755.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a9adab3-e152-4a18-9f8f-ba89ad7fba6f_1468x755.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a9adab3-e152-4a18-9f8f-ba89ad7fba6f_1468x755.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a9adab3-e152-4a18-9f8f-ba89ad7fba6f_1468x755.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a9adab3-e152-4a18-9f8f-ba89ad7fba6f_1468x755.png" width="1456" height="749" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3a9adab3-e152-4a18-9f8f-ba89ad7fba6f_1468x755.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:749,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:109928,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a9adab3-e152-4a18-9f8f-ba89ad7fba6f_1468x755.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a9adab3-e152-4a18-9f8f-ba89ad7fba6f_1468x755.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a9adab3-e152-4a18-9f8f-ba89ad7fba6f_1468x755.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a9adab3-e152-4a18-9f8f-ba89ad7fba6f_1468x755.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>I suspect this only applies to OpenCL, because latency from Vulkan (with Nemes’s test) shows just over 300 ns of latency. Latency at large test sizes will likely run into TLB misses, and I suspect Intel is using different page sizes for different APIs.</p><p><span>Compared to its peers, the Arc B580 has more theoretical VRAM bandwidth at 456 GB/s, but also less L2 capacity. For example, </span><a href="https://www.techpowerup.com/gpu-specs/geforce-rtx-4060.c4107" rel="">Nvidia’s RTX 4060</a><span> has 272 GB/s VRAM bandwidth using a 128-bit GDDR6 bus running at 17 GT/s, with 24 MB of L2 in front of it. I profiled a few things with VTune and picked out spikes in VRAM bandwidth usage. I also checked reported L2 bandwidth over the same sampling interval.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1820e772-413c-4314-b3e1-274a32cf3490_909x532.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1820e772-413c-4314-b3e1-274a32cf3490_909x532.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1820e772-413c-4314-b3e1-274a32cf3490_909x532.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1820e772-413c-4314-b3e1-274a32cf3490_909x532.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1820e772-413c-4314-b3e1-274a32cf3490_909x532.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1820e772-413c-4314-b3e1-274a32cf3490_909x532.png" width="909" height="532" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1820e772-413c-4314-b3e1-274a32cf3490_909x532.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:532,&quot;width&quot;:909,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:58325,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1820e772-413c-4314-b3e1-274a32cf3490_909x532.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1820e772-413c-4314-b3e1-274a32cf3490_909x532.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1820e772-413c-4314-b3e1-274a32cf3490_909x532.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1820e772-413c-4314-b3e1-274a32cf3490_909x532.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Intel’s balance of cache capacity and memory bandwidth seems to work well, at least in the few examples I checked. Even when VRAM bandwidth demands are high, the 18 MB L2 is able to catch enough traffic to avoid pushing GDDR6 bandwidth limits. If Intel hypothetically used a smaller GDDR6 memory subsystem like Nvidia’s RTX 4060, B580 would need a larger cache to avoid reaching VRAM bandwidth limits.</p><p>Probably as a cost cutting measure, B580 has a narrower PCIe link than its predecessor. Still, a x8 Gen 4 link provides as much theoretical bandwidth as a x16 Gen 3 one. Testing with OpenCL doesn’t get close to theoretical bandwidth, but B580 is at a disadvantage compared to A750.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd95b9ef-0bc9-47f2-af97-89fbd2fc9bfd_571x338.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd95b9ef-0bc9-47f2-af97-89fbd2fc9bfd_571x338.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd95b9ef-0bc9-47f2-af97-89fbd2fc9bfd_571x338.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd95b9ef-0bc9-47f2-af97-89fbd2fc9bfd_571x338.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd95b9ef-0bc9-47f2-af97-89fbd2fc9bfd_571x338.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd95b9ef-0bc9-47f2-af97-89fbd2fc9bfd_571x338.png" width="571" height="338" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bd95b9ef-0bc9-47f2-af97-89fbd2fc9bfd_571x338.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:338,&quot;width&quot;:571,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:35563,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd95b9ef-0bc9-47f2-af97-89fbd2fc9bfd_571x338.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd95b9ef-0bc9-47f2-af97-89fbd2fc9bfd_571x338.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd95b9ef-0bc9-47f2-af97-89fbd2fc9bfd_571x338.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd95b9ef-0bc9-47f2-af97-89fbd2fc9bfd_571x338.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>PCIe link bandwidth </span><a href="https://www.techpowerup.com/review/nvidia-geforce-rtx-4090-pci-express-scaling/28.html" rel="">often has minimal impact on gaming performance</a><span>, as long as you have enough VRAM. B580 has a comparatively large 12 GB VRAM pool compared to its immediate competitors, which also have PCIe 4.0 x8 links. That could give B580 an advantage within the midrange market, but that doesn’t mean it’s immune to problems.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39b616bf-cab9-4f91-bb07-034202985109_1555x619.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39b616bf-cab9-4f91-bb07-034202985109_1555x619.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39b616bf-cab9-4f91-bb07-034202985109_1555x619.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39b616bf-cab9-4f91-bb07-034202985109_1555x619.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39b616bf-cab9-4f91-bb07-034202985109_1555x619.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39b616bf-cab9-4f91-bb07-034202985109_1555x619.webp" width="1456" height="580" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/39b616bf-cab9-4f91-bb07-034202985109_1555x619.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:580,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:13050,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39b616bf-cab9-4f91-bb07-034202985109_1555x619.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39b616bf-cab9-4f91-bb07-034202985109_1555x619.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39b616bf-cab9-4f91-bb07-034202985109_1555x619.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39b616bf-cab9-4f91-bb07-034202985109_1555x619.webp 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>DCS for example will uses over 12 GB of VRAM with mods. Observing different aircraft in different areas often causes stutters on the B580. VTune shows high PCIe traffic as the GPU must frequently read from host memory. </p><p>Battlemage retains Alchemist’s high level goals and foundation, but makes a laundry list of improvements. Compute is easier to utilize, cache latency improves, and weird scaling issues with global memory atomics have been resolved. Intel has made some surprising optimizations too, like reducing scalar memory access latency. The result is impressive, with Arc B580 easily outperforming the outgoing A770 despite lagging in nearly every on-paper specification.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2f29fc8-1043-4793-a905-cd34cddaf017_688x386.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2f29fc8-1043-4793-a905-cd34cddaf017_688x386.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2f29fc8-1043-4793-a905-cd34cddaf017_688x386.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2f29fc8-1043-4793-a905-cd34cddaf017_688x386.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2f29fc8-1043-4793-a905-cd34cddaf017_688x386.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2f29fc8-1043-4793-a905-cd34cddaf017_688x386.jpeg" width="688" height="386" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b2f29fc8-1043-4793-a905-cd34cddaf017_688x386.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:386,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2f29fc8-1043-4793-a905-cd34cddaf017_688x386.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2f29fc8-1043-4793-a905-cd34cddaf017_688x386.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2f29fc8-1043-4793-a905-cd34cddaf017_688x386.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2f29fc8-1043-4793-a905-cd34cddaf017_688x386.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Some of Intel’s GPU architecture changes nudge it a bit closer to AMD and Nvidia’s designs. Intel’s compiler often prefers SIMD32, a mode that AMD often chooses for compute code or vertex shaders, and one that Nvidia exclusively uses. SIMD1 optimizations create parallels to AMD’s scalar unit or Nvidia’s uniform datapath. Battlemage’s memory subsystem emphasizes caching more than its predecessor, while relying less on high VRAM bandwidth. AMD’s RDNA 2 and Nvidia’s Ada Lovelace made similar moves with their memory subsystems.</p><p>Of course Battlemage is still a very different animal from its discrete GPU competitors. Even with larger XVEs, Battlemage still uses smaller execution unit partitions than AMD or Nvidia. With SIMD16 support, Intel continues to support shorter vector widths than the competition. Generating SIMD1 instructions gives Intel some degree of scalar optimization, but stops short of having a full-out scalar/uniform datapath like AMD or post-Turing Nvidia. And 18 MB of cache is still less than the 24 or 32 MB in Nvidia and AMD’s midrange cards.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62f51acd-d131-49f8-9830-d0d60a964a00_688x458.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62f51acd-d131-49f8-9830-d0d60a964a00_688x458.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62f51acd-d131-49f8-9830-d0d60a964a00_688x458.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62f51acd-d131-49f8-9830-d0d60a964a00_688x458.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62f51acd-d131-49f8-9830-d0d60a964a00_688x458.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62f51acd-d131-49f8-9830-d0d60a964a00_688x458.jpeg" width="688" height="458" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/62f51acd-d131-49f8-9830-d0d60a964a00_688x458.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:458,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62f51acd-d131-49f8-9830-d0d60a964a00_688x458.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62f51acd-d131-49f8-9830-d0d60a964a00_688x458.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62f51acd-d131-49f8-9830-d0d60a964a00_688x458.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62f51acd-d131-49f8-9830-d0d60a964a00_688x458.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Differences from AMD and Nvidia aside, Battlemage is a worthy step on Intel’s journey to take on the midrange graphics market. A third competitor in the discrete GPU market is welcome news for any PC enthusiast. For sure, Intel still has some distance to go. </span><a href="https://chipsandcheese.com/p/digging-into-driver-overhead-on-intels" rel="">Driver overhead</a><span> and reliance on resizable BAR are examples of areas where Intel is still struggling to break from their iGPU-only background.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77170034-aec9-4efe-8a7f-4c72650dce21_688x458.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77170034-aec9-4efe-8a7f-4c72650dce21_688x458.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77170034-aec9-4efe-8a7f-4c72650dce21_688x458.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77170034-aec9-4efe-8a7f-4c72650dce21_688x458.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77170034-aec9-4efe-8a7f-4c72650dce21_688x458.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77170034-aec9-4efe-8a7f-4c72650dce21_688x458.jpeg" width="688" height="458" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/77170034-aec9-4efe-8a7f-4c72650dce21_688x458.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:458,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77170034-aec9-4efe-8a7f-4c72650dce21_688x458.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77170034-aec9-4efe-8a7f-4c72650dce21_688x458.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77170034-aec9-4efe-8a7f-4c72650dce21_688x458.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77170034-aec9-4efe-8a7f-4c72650dce21_688x458.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>But I hope Intel goes after higher-end GPU segments once they’ve found firmer footing. A third player in the high end dGPU market would be very welcome as many folks are still on Pascal or GCN due to folks feeling as if there is not a reasonable upgrade yet. Intel’s Arc B580 addresses some of that pent-up demand, at least when it’s not out-of-stock. I look forward to seeing Intel’s future GPU efforts.</p><p><span>If you like the content then consider heading over to the </span><a href="https://www.patreon.com/ChipsandCheese" rel="">Patreon</a><span> or </span><a href="https://www.paypal.com/donate/?hosted_button_id=4EMPH66SBGVSQ" rel="">PayPal</a><span> if you want to toss a few bucks to Chips and Cheese. Also consider joining the </span><a href="https://discord.gg/TwVnRhxgY2" rel="">Discord</a><span>.</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I Tasted Honda's Spicy Rodent-Repelling Tape – And I will do it again (1073 pts)]]></title>
            <link>https://haterade.substack.com/p/i-tasted-hondas-spicy-rodent-repelling</link>
            <guid>43013615</guid>
            <pubDate>Tue, 11 Feb 2025 15:08:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://haterade.substack.com/p/i-tasted-hondas-spicy-rodent-repelling">https://haterade.substack.com/p/i-tasted-hondas-spicy-rodent-repelling</a>, See on <a href="https://news.ycombinator.com/item?id=43013615">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p>A while back, after a series of car troubles too boring to enumerate, I learned about the existence of mouse tape. </p><p><span>Mouse tape exists solely to make Important Wires less delicious. Honda started selling rolls of the stuff a few years back to keep rodents from partying in their engine blocks.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-39232142" href="https://haterade.substack.com/p/i-tasted-hondas-spicy-rodent-repelling#footnote-1-39232142" target="_self" rel="">1</a></span><span> You see, the thing about rodents—be they rat or shrew or vole—is that they really like to </span><em>gnaw.</em></p><p>I don’t know much about cars, but I do know you’re not supposed to chew on them. So I nodded along while my mechanic explained that there was “rodent damage” on some mysterious bundle of wires beneath the manifold and that he would need to wrap the fresh, un-gnawed wires in a rodent repellent.</p><p><span>Then he told me the “rodent repellent” was a roll of gray vinyl tape printed with cute little mice silhouettes and coated in pure capsaicin. </span><em>Spicy mouse tape. </em></p><p>When I got home, I Googled it. I had a mad compulsion to taste it, but I didn’t want to gnaw on my brand new car wires. Plus, my head wouldn’t fit beneath the manifold. </p><p>I found the tape immediately—OEM Honda part 4019-2317. It was outrageously expensive, like a white truffle or a tin of Ossetra caviar. Fortunately, a reader had recently Venmo’ed me $50—almost the exact cost of a roll of mouse tape. I took it as a sign. </p><p>This is the Haterade promise: I will only ever use your money irresponsibly. </p><p>Almost as soon as the tape arrived, I began to have second thoughts. Each printed mouse had a little gray “X” on its head. This could mean only one of two things: either the mice were being poisoned, or someone was about to drop a piano on them. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F154de6a8-4139-4930-9ef2-4bb62f8de9a0_4032x1487.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F154de6a8-4139-4930-9ef2-4bb62f8de9a0_4032x1487.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F154de6a8-4139-4930-9ef2-4bb62f8de9a0_4032x1487.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F154de6a8-4139-4930-9ef2-4bb62f8de9a0_4032x1487.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F154de6a8-4139-4930-9ef2-4bb62f8de9a0_4032x1487.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F154de6a8-4139-4930-9ef2-4bb62f8de9a0_4032x1487.jpeg" width="1456" height="537" data-attrs="{&quot;src&quot;:&quot;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/154de6a8-4139-4930-9ef2-4bb62f8de9a0_4032x1487.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:537,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1167767,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F154de6a8-4139-4930-9ef2-4bb62f8de9a0_4032x1487.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F154de6a8-4139-4930-9ef2-4bb62f8de9a0_4032x1487.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F154de6a8-4139-4930-9ef2-4bb62f8de9a0_4032x1487.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F154de6a8-4139-4930-9ef2-4bb62f8de9a0_4032x1487.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>Was the “repellent” really just capsaicin? Fearing the worst, I reached out to Honda: </p><p>Honda never replied to my tweet. To be fair, I hadn’t really expected anyone in the company to get back to me, a clear and present danger, on whether their autoparts could be licked. That is a pervert’s question. </p><p>Still, I fired off a couple emails to Honda’s PR team just in case. My first went to Chris Martin, who was listed on the website as the media contact for “Safety, Regulatory, and Recalls.”</p><p>To my great surprise, Chris got back to me the next business day:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ff79bf0f4-fcb7-46c5-86e0-c6dcb6b336dd_659x501.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ff79bf0f4-fcb7-46c5-86e0-c6dcb6b336dd_659x501.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ff79bf0f4-fcb7-46c5-86e0-c6dcb6b336dd_659x501.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ff79bf0f4-fcb7-46c5-86e0-c6dcb6b336dd_659x501.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ff79bf0f4-fcb7-46c5-86e0-c6dcb6b336dd_659x501.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ff79bf0f4-fcb7-46c5-86e0-c6dcb6b336dd_659x501.png" width="659" height="501" data-attrs="{&quot;src&quot;:&quot;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/f79bf0f4-fcb7-46c5-86e0-c6dcb6b336dd_659x501.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:501,&quot;width&quot;:659,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:48275,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ff79bf0f4-fcb7-46c5-86e0-c6dcb6b336dd_659x501.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ff79bf0f4-fcb7-46c5-86e0-c6dcb6b336dd_659x501.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ff79bf0f4-fcb7-46c5-86e0-c6dcb6b336dd_659x501.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ff79bf0f4-fcb7-46c5-86e0-c6dcb6b336dd_659x501.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>…</span><em>wink, </em><span>am I right? </span></p><p>Look, I resigned myself a long time ago to the fact that I was going to die doing something so stupid, no one would ever mourn me—like French kissing a komodo dragon or running the Kentucky Derby as a pantomime horse.   </p><p>But for Chris, I scanned the accompanying Materials Sheet. There were no alarming skull-and-crossbones labels on “rodent-proof vinyl adhesive tape No.347”—just one “Class 1 Designated Chemical Substances”: bis(2-ethylhexyl)phthalate, better known as DEHP. </p><p><span>DEHP is a compound added to plastics (like vinyl tape) to make them more flexible. It’s used in hundreds of household products, which means lots of people who bathe in Dr. Bronner’s Useless Fluid think it will kill them. And sure—at high doses, DEHP can reduce both </span><a href="https://www.hindawi.com/journals/bmri/2018/1750368/" rel="">sperm count and sperm motility</a><span> in rats.</span></p><p>But I scaled the dose up for my own weight and determined that I would need to ingest 36,000 mg a day to approach the Rat Contraceptive dose.</p><p>I licked the tape. </p><p><span>It smelled like a Band-Aid-flavored Rockstar Energy drink. It tasted like…</span><em>heat. </em><span>The capsaicin was subtler than I expected: nothing abrasive or punishing, just a blushing, ambient warmth like a string of white Christmas lights. There was almost a numbing, mala element, in the vein of a Sichuan peppercorn. </span></p><p>Mouse Tape could have a future in modernist dining circles—you know, for the Willy Wonka, lick-the-wallpaper set. (“The schnozzberries taste like schnozzberries!” “The pepper mouse tastes like a pepper mouse!”)</p><p>That might be a better gambit for Honda. I found a live shrew in my slipper yesterday, so I’m not sure the tape is working as originally intended. (I am surrounded by rodents always, like a dumpster princess.) Plus, I really lathered my tongue with it and never felt like I was in any danger of being repelled.</p><p>As a culinary novelty, though? I can think of a few uses: </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F3d279760-0918-4914-9378-c0fcf1151173_4032x3024.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F3d279760-0918-4914-9378-c0fcf1151173_4032x3024.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F3d279760-0918-4914-9378-c0fcf1151173_4032x3024.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F3d279760-0918-4914-9378-c0fcf1151173_4032x3024.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F3d279760-0918-4914-9378-c0fcf1151173_4032x3024.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F3d279760-0918-4914-9378-c0fcf1151173_4032x3024.jpeg" width="1456" height="1092" data-attrs="{&quot;src&quot;:&quot;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/3d279760-0918-4914-9378-c0fcf1151173_4032x3024.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1092,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2625020,&quot;alt&quot;:&quot;A Bloody Mary with a strip of gray vinyl mouse tape around the lip of the glass.&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="A Bloody Mary with a strip of gray vinyl mouse tape around the lip of the glass." title="A Bloody Mary with a strip of gray vinyl mouse tape around the lip of the glass." srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F3d279760-0918-4914-9378-c0fcf1151173_4032x3024.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F3d279760-0918-4914-9378-c0fcf1151173_4032x3024.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F3d279760-0918-4914-9378-c0fcf1151173_4032x3024.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F3d279760-0918-4914-9378-c0fcf1151173_4032x3024.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>I was </span><em>always </em><span>going to make this into a Bloody Caesar rim. </span></figcaption></figure></div><p>This is not a recommendation, of course. You should not take any advice from this newsletter in general, but you should not lick Mouse Tape in particular. Chris strongly recommends against it.</p><p><span>When I am on my deathbed, wasting from Stagnant Rat Sperm Disease, my last words will be “CHRIS MARTIN DID EVERYTHING HE COULD TO TALK ME OUT OF THIS.”</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2-39232142" href="https://haterade.substack.com/p/i-tasted-hondas-spicy-rodent-repelling#footnote-2-39232142" target="_self" rel="">2</a></span></p><p>I haven’t emailed him yet to break the news. But I think he knows. I think he always knew. I think he will be unsurprised when this arrives in his inbox: </p><p><strong>This Is Just to Say</strong></p><p><span>I have eaten </span><br><span>the tape</span><br><span>that was in </span><br><span>the mice box</span></p><p><span>and which </span><br><span>you had probably </span><br><span>not intended for </span><br><span>human ingestion</span></p><p><span>Forgive me </span><br><span>it was delicious</span><br><span>so spiced </span><br><span>and so bold</span></p><p><em>If you enjoy(?) Haterade, please share, subscribe, or send to a friend. And if you’d like to donate to the Liz Cook Angel Fund for Spice-Loving Rats with Shrunken Testes, you can do so here: @lizcookkc on Venmo and $lizcookkc on CashApp. </em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Backblaze Drive Stats for 2024 (369 pts)]]></title>
            <link>https://www.backblaze.com/blog/backblaze-drive-stats-for-2024/</link>
            <guid>43013431</guid>
            <pubDate>Tue, 11 Feb 2025 14:55:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.backblaze.com/blog/backblaze-drive-stats-for-2024/">https://www.backblaze.com/blog/backblaze-drive-stats-for-2024/</a>, See on <a href="https://news.ycombinator.com/item?id=43013431">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
          <main id="main">

<div>

		<article id="post-111927">

			<!-- .entry-header -->

			<section>
				
<figure><img fetchpriority="high" decoding="async" width="1441" height="820" src="https://www.backblaze.com/blog/wp-content/uploads/2025/02/2024DS-header.png" alt="A decorative image with the title 2024 Year End Drive Stats. " srcset="https://www.backblaze.com/blog/wp-content/uploads/2025/02/2024DS-header.png 1441w, https://www.backblaze.com/blog/wp-content/uploads/2025/02/2024DS-header-300x171.png 300w, https://www.backblaze.com/blog/wp-content/uploads/2025/02/2024DS-header-1024x583.png 1024w, https://www.backblaze.com/blog/wp-content/uploads/2025/02/2024DS-header-768x437.png 768w" sizes="(max-width: 1441px) 100vw, 1441px"></figure>







<p>As of December 31, 2024, we had 305,180 drives under management. Of that number, there were 4,060 boot drives and 301,120 data drives. This report will focus on those data drives as we review the Q4 2024 annualized failure rates (AFR), the 2024 failure rates, and the lifetime failure rates for the drive models in service as of the end of 2024. Along the way, we’ll share our observations and insights on the data presented, and, as always, we look forward to you doing the same in the comments section at the end of the post.</p>



<div>
<h4>Sign up for the Drive Stats webinar</h4>
<p>Tune in to ask those questions you’ve had spinning ‘round your head <a href="https://youtu.be/cwznr0ZHuLY" rel="”noopener”" “nofollow”="" target="”_blank”">like so many drives,</a> and meet the new Drive Stats team—Stephanie Doyle and David Johnson of Backblaze Blog fame. Yes, you heard that right: It’s my last Drive Stats before I head off to retirement (but more on that later in the report). Read on, and sign up, for analysis and insights from the 2024 report.</p>
<!--HubSpot Call-to-Action Code --><p><span id="hs-cta-wrapper-d68f5876-70f8-411c-84d1-41ae3bbf9727"><span id="hs-cta-d68f5876-70f8-411c-84d1-41ae3bbf9727"><!--[if lte IE 8]><div id="hs-cta-ie-element"></div><![endif]--><a href="https://cta-redirect.hubspot.com/cta/redirect/2832298/d68f5876-70f8-411c-84d1-41ae3bbf9727" target="_blank" rel="noopener"><img decoding="async" id="hs-cta-img-d68f5876-70f8-411c-84d1-41ae3bbf9727" src="https://no-cache.hubspot.com/cta/default/2832298/d68f5876-70f8-411c-84d1-41ae3bbf9727.png" alt="Sign Up ➔&nbsp;"></a></span></span></p><!-- end HubSpot Call-to-Action Code -->

</div>



<h2>Q4 2024 hard drive failure rates</h2>



<p>As of the end of 2024, Backblaze was monitoring 301,120 hard drives used to store data. For our evaluation, we removed from consideration 487 drives, as they did not meet the criteria to be included. We’ll discuss the criteria we used in the next section of this report. Removing these drives leaves us with 300,633 hard drives to analyze. The table below shows the annualized failure rates for Q4 2024 for this collection of drives.</p>


<div>
<figure data-wp-context="{&quot;imageId&quot;:&quot;67ab872d0411a&quot;}" data-wp-interactive="core/image"><img decoding="async" width="680" height="990" data-wp-class--hide="state.isContentHidden" data-wp-class--show="state.isContentVisible" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://www.backblaze.com/blog/wp-content/uploads/2025/02/1-Quarterly-AFR-table-Q4-2024.png" alt="A table showing the quarterly failure rates for Q4 2024." srcset="https://www.backblaze.com/blog/wp-content/uploads/2025/02/1-Quarterly-AFR-table-Q4-2024.png 680w, https://www.backblaze.com/blog/wp-content/uploads/2025/02/1-Quarterly-AFR-table-Q4-2024-206x300.png 206w" sizes="(max-width: 680px) 100vw, 680px"></figure></div>






<h3>Notes and observations</h3>



<ul>
<li><strong>24TB drives are here</strong>. Seagate 24TB drives (model: ST24000NM002H) arrived in early December. The 1,200 drives filled one Backblaze Vault with no failed drives through the end of Q4. The 24TB Seagate drives join the 20TB Toshiba and 22TB WDC drive models in the 20-plus capacity club as we continue to dramatically increase storage capacity while optimizing existing storage server space.</li>



<li><strong>Zero failures for the quarter</strong>. Five drive models had zero failures for the quarter starting with the 24TB Seagate drive model noted above. The others are the 4TB HGST (model: HMS5C4040ALE640), the 8TB Seagate (model: ST8000NM000A), the 14TB Seagate (model: ST14000NM000J), and the 16TB Seagate (model: ST16000NM002J). All of the zeroes come with the caveat of having a relatively small number of drives and drive days, but zero failures in a quarter is always a good thing.</li>



<li><strong>The 4TB drives are nearly extinct</strong>. The 4TB drive count decreased by another 1,774 drives in Q4. (I discussed exactly <a href="https://www.backblaze.com/blog/how-backblaze-scales-our-storage-cloud/" target="_blank" rel="noreferrer noopener">how we migrate them</a> in more detail if you want to dig in.) The remaining ~4,000 drives should be gone by the end of Q1 2025. They will be replaced by the incoming 20TB, 22TB, and 24TB drives. It should be noted that out of the 4TB drives in operation in Q4, only one failed, so those 20-plus TB drives have a lot to live up to from a failure perspective.</li>



<li><strong>The quarterly failure rate is down.</strong> The AFR for Q4 dropped from 1.89% in Q3 to 1.35% in Q4. While all drive sizes delivered some improvement from Q3 to Q4, one of the primary drivers is the addition of over 14,000 new 20-plus TB drives. As a group, these drives delivered an AFR of 0.77% for the quarter.</li>
</ul>



<h2>Drive model criteria</h2>



<p>We noted earlier we removed 487 drives from consideration when we produced the table above covering Q4 2024. There are two primary reasons we did not consider these drive models.</p>



<ul>
<li><strong>Testing</strong>. These are drives of a given model that we monitor and collect Drive Stats data on, but are not considered production drives at this time. For example, drives undergoing certification testing to determine if they are performant enough for our environment are not included in our Drive Stats calculations.</li>



<li><strong>Insufficient data points. </strong>When we calculate the annualized failure rate for a drive model for a given period of time (quarterly, annual, or lifetime), we want to ensure we have enough data to reliably do so. Therefore we have defined criteria for a drive model to be included in the tables and charts for the specified period of time. Models that do not meet these criteria are not included in the tables and charts for the period in question.</li>
</ul>



<table id="tablepress-78">
<thead>
<tr>
	<th>Period</th><th>Drive Count</th><th>Drive Days</th>
</tr>
</thead>
<tbody>
<tr>
	<td><strong>Quarterly</strong></td><td>&gt; 100</td><td>&gt; 10,000</td>
</tr>
<tr>
	<td><strong>Annual</strong></td><td>&gt; 250</td><td>&gt; 50,000</td>
</tr>
<tr>
	<td><strong>Lifetime</strong></td><td>&gt; 500</td><td>&gt;100,000</td>
</tr>
</tbody>
</table>
<!-- #tablepress-78 from cache -->







<p>Regardless of whether or not a given drive model is included in the charts and tables, all of the data for all of the drives we use is included in our Drive Stats dataset which you can download by visiting our <a href="https://www.backblaze.com/cloud-storage/resources/hard-drive-test-data" target="_blank" rel="noreferrer noopener">Drive Stats page</a>.</p>



<p>As with the Q4 quarterly results, we will apply these criteria to the annual and lifetime charts that follow in this report.</p>



<h2>2024 annual hard drive failure rates</h2>



<p>As of the end of 2024, Backblaze was monitoring 301,120 hard drives used to store data. We removed nine drive models consisting of 2,012 drives from consideration as they did not meet the annual criteria we have defined. This leaves us with 298,954 drives divided across 27 different drive models. The table below shows the AFRs for 2024 for this collection of drives.</p>


<div>
<figure data-wp-context="{&quot;imageId&quot;:&quot;67ab872d045da&quot;}" data-wp-interactive="core/image"><img decoding="async" width="710" height="840" data-wp-class--hide="state.isContentHidden" data-wp-class--show="state.isContentVisible" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://www.backblaze.com/blog/wp-content/uploads/2025/02/2-Annual-AFR-table-for-2024.png" alt="A table showing the annualized failure rates for 2024 data. " srcset="https://www.backblaze.com/blog/wp-content/uploads/2025/02/2-Annual-AFR-table-for-2024.png 710w, https://www.backblaze.com/blog/wp-content/uploads/2025/02/2-Annual-AFR-table-for-2024-254x300.png 254w" sizes="(max-width: 710px) 100vw, 710px"></figure></div>






<h3>Notes and observations</h3>



<ul>
<li><strong>No zeros for the year</strong>. There were no qualifying drive models with zero failures in 2024. That said, the 16TB Seagate (model: ST16000NM002J) got close by recording just one drive failure back in Q3, giving the drive an AFR of 0.22% for 2024.&nbsp;</li>



<li><strong>Busy data center techs</strong>. During 2024, our data center techs installed 53,337 drives. If we assume there are 2,080 work hours a year (52 weeks times 40 hours), that math is <code>53,337/2,080</code>, and that means our intrepid DC techs installed 26 drives per hour. Busy, busy, busy!&nbsp;</li>



<li><strong>The 24TB Seagate drives? </strong>While there were 1,200 new 24TB Seagate drives added in 2024, they were installed in early December and did not accumulate enough drive days to make the cut for the annual, or lifetime, tables. Including the 24TB Seagate drive, there were three models that missed out on being included in the 2024 annual tables, these drive models are listed below.</li>
</ul>



<table id="tablepress-79">
<thead>
<tr>
	<th>MFG</th><th>Model</th><th>Drive Count</th><th>Drive Days</th><th>2024 AFR </th>
</tr>
</thead>
<tbody>
<tr>
	<td>Seagate</td><td>ST8000NM000A</td><td>247</td><td>22,684</td><td>0.84%</td>
</tr>
<tr>
	<td>Seagate</td><td>ST14000NM000J</td><td>232</td><td>19,696</td><td>1.32%</td>
</tr>
<tr>
	<td>Seagate</td><td>ST24000NM002H</td><td>1,200</td><td>18,000</td><td>0.00%</td>
</tr>
</tbody>
</table>
<!-- #tablepress-79 from cache -->







<p>As a reminder, a drive model needs to have over 250 drives by the end of Q4 and accumulate at least 50,000 drive days during 2024 to be included in the annual tables.</p>



<h2>Comparing Drive Stats for 2022, 2023, and 2024</h2>



<p>The table below compares the annual failure rates by drive model for each of the last three years. The table includes just those drive models which met the annual criteria as of the end of 2024. The data for each year is inclusive of that year only for the operational drive models present at the end of each year. The table is sorted by drive size and then AFR.</p>


<div>
<figure data-wp-context="{&quot;imageId&quot;:&quot;67ab872d0496e&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="720" height="940" data-wp-class--hide="state.isContentHidden" data-wp-class--show="state.isContentVisible" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://www.backblaze.com/blog/wp-content/uploads/2025/02/3-Annual-three-year-comparison.png" alt="A table comparing annual Drive Stats results for 2022, 2023, and 2024. " srcset="https://www.backblaze.com/blog/wp-content/uploads/2025/02/3-Annual-three-year-comparison.png 720w, https://www.backblaze.com/blog/wp-content/uploads/2025/02/3-Annual-three-year-comparison-230x300.png 230w" sizes="auto, (max-width: 720px) 100vw, 720px"></figure></div>






<h3>Notes and observations</h3>



<ul>
<li><strong>The annual AFR is down.</strong> The 2024 AFR for all drives listed was 1.57%, this is down from 1.70% in 2023.&nbsp; We expect the overall failure rates to continue to fall in 2025, but we will be watching the following for indicators.
<ul>
<li><strong>The failure rates of the 8TB and 12TB drive models. </strong>All of the models will exceed their five years of service. In general, the failure rate will noticeably increase as the drives exceed five years of service. And, while there are outliers like the current HGST 4TB drives, you can’t assume that will happen.</li>



<li><strong>The failure rates of the 14TB and 16TB drive models.</strong> These models are approaching middle age—three to five years in operation. This is where, according to <a href="https://www.backblaze.com/blog/drive-failure-over-time-the-bathtub-curve-is-leaking/" target="_blank" rel="noreferrer noopener">the bathtub curve</a>, their failure rates could gradually increase—but not as severely as when they exceed five years.&nbsp;</li>



<li><strong>The failure rates for the 20TB, 22TB, and 24TB drives models.</strong> These drives will enter the flat portion of the bathtub curve, that is where their failure rate should be the lowest.</li>
</ul>
</li>
</ul>



<h2>Annualized failure rates vs. drive size</h2>



<p>Now, we can dig into the numbers to see what else we can learn. We’ll start by looking at the quarterly annualized failure rate by drive size over the last three years.</p>


<div>
<figure data-wp-context="{&quot;imageId&quot;:&quot;67ab872d04cf1&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="860" height="780" data-wp-class--hide="state.isContentHidden" data-wp-class--show="state.isContentVisible" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://www.backblaze.com/blog/wp-content/uploads/2025/02/4-Quarterly-3-Year-AFR-by-Drive-Size.png" alt="A chart plotting annualized failure rates by drive size. " srcset="https://www.backblaze.com/blog/wp-content/uploads/2025/02/4-Quarterly-3-Year-AFR-by-Drive-Size.png 860w, https://www.backblaze.com/blog/wp-content/uploads/2025/02/4-Quarterly-3-Year-AFR-by-Drive-Size-300x272.png 300w, https://www.backblaze.com/blog/wp-content/uploads/2025/02/4-Quarterly-3-Year-AFR-by-Drive-Size-768x697.png 768w" sizes="auto, (max-width: 860px) 100vw, 860px"></figure></div>






<p>Let’s take a look at the different drive sizes and how they affect the overall annualized failure rate over time.</p>



<p><strong>Minimal impact</strong>. The 4TB (blue line) drives and 10TB (gold line) drives have had little impact over the last year on the overall failure rate as each finished the year with a relatively small number of drives. Still, the wild ride delivered by the 10TB drives keeps our DC techs on their toes.&nbsp;</p>



<p><strong>Older drives</strong>. The 8TB (gray line) drives and 12TB (purple line) drives range in age from five to eight years and as such their overall failure rates should be increasing over time. The 12TB drives are following that pattern moving up from about 1% AFR back in 2021 to just about 3% in 2024. The failure rates of the 8TB drives, while erratic from quarter-to-quarter, have a nearly flat trendline over the same period.</p>



<p><strong>Workhorse drives</strong>. The 14TB (green line) and 16TB (azure* line) drives comprise 57% of the drives in service and on average they range in age from two to four years. They are in the prime of their working lives. As such, they should have low and stable failure rates, and as you can see, they do.</p>



<p>*&nbsp; Maybe azure isn’t quite right, but robin’s egg blue seemed a bit pretentious.</p>



<p><strong>New drives on the block</strong>. The 22TB (orange line) drives are in their early days as we continue to add more drives on a regular basis. Once the drive population settles down, we’ll have a better sense of the AFR direction. Still, the early results are solid with a lifetime AFR of 1.06%.</p>



<h2>Annualized failure rates vs. manufacturer</h2>



<p>One of the more popular ways we can look at this data is by the drive manufacturer as we’ve done below.</p>


<div>
<figure data-wp-context="{&quot;imageId&quot;:&quot;67ab872d05772&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="860" height="780" data-wp-class--hide="state.isContentHidden" data-wp-class--show="state.isContentVisible" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://www.backblaze.com/blog/wp-content/uploads/2025/02/5-Quarterly-3-Year-AFR-by-MFG.png" alt="A chart tracking annualized failure rates by manufacturer. " srcset="https://www.backblaze.com/blog/wp-content/uploads/2025/02/5-Quarterly-3-Year-AFR-by-MFG.png 860w, https://www.backblaze.com/blog/wp-content/uploads/2025/02/5-Quarterly-3-Year-AFR-by-MFG-300x272.png 300w, https://www.backblaze.com/blog/wp-content/uploads/2025/02/5-Quarterly-3-Year-AFR-by-MFG-768x697.png 768w" sizes="auto, (max-width: 860px) 100vw, 860px"></figure></div>






<p>To complete the picture, the chart below uses the same data, but displays just the linear trendlines for each of the manufacturers over the same three-year period.</p>


<div>
<figure data-wp-context="{&quot;imageId&quot;:&quot;67ab872d05c7c&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="830" height="590" data-wp-class--hide="state.isContentHidden" data-wp-class--show="state.isContentVisible" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://www.backblaze.com/blog/wp-content/uploads/2025/02/6-Quarterly-3-year-Trendlines-by-MFG.png" alt="A chart tracking annualized failure rates by manufacturer. " srcset="https://www.backblaze.com/blog/wp-content/uploads/2025/02/6-Quarterly-3-year-Trendlines-by-MFG.png 830w, https://www.backblaze.com/blog/wp-content/uploads/2025/02/6-Quarterly-3-year-Trendlines-by-MFG-300x213.png 300w, https://www.backblaze.com/blog/wp-content/uploads/2025/02/6-Quarterly-3-year-Trendlines-by-MFG-768x546.png 768w" sizes="auto, (max-width: 830px) 100vw, 830px"></figure></div>






<p><strong>HGST</strong>. While the HGST trendline is not pretty, it doesn’t tell the entire story. Looking at the first chart, until Q4 2023, the HGST drives were at or below the average for all of the drives, that is all manufacturers. At that point, HGST has exceeded the average, and then some. The table below contains results for just the HGST drives for 2024. We’ve sorted them, high to low, by the 2024 AFR.</p>


<div>
<figure data-wp-context="{&quot;imageId&quot;:&quot;67ab872d07039&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="636" height="280" data-wp-class--hide="state.isContentHidden" data-wp-class--show="state.isContentVisible" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://www.backblaze.com/blog/wp-content/uploads/2025/02/7-HGST-2024-AFR.png" alt="A chart showing 2024 annualized failure rates for HGST broken down by model." srcset="https://www.backblaze.com/blog/wp-content/uploads/2025/02/7-HGST-2024-AFR.png 636w, https://www.backblaze.com/blog/wp-content/uploads/2025/02/7-HGST-2024-AFR-300x132.png 300w" sizes="auto, (max-width: 636px) 100vw, 636px"></figure></div>






<p>As you can see, there are two 12TB drive models driving the high AFR for the HGST drives. The HUH721212ALN604 model began showing signs of an increased quarterly AFR in Q1 2023 and the HUH721212ALE604 model followed suit in Q3 2024. Without these drive models, the 2024 AFR for HGST drive would be 0.55%.</p>



<p><strong>Seagate</strong>. The quarterly AFR trendline decreased for the Seagate drives from 2022 through 2024. While the decrease was slight, from 2.25% to 2.0%, Seagate was the only manufacturer to do so. The decrease appears, at least in part, to be due to the removal of the Seagate 4TB drives during that period.&nbsp;</p>



<p><strong>Toshiba</strong>. Over the 2022 to 2024 period, the quarterly AFR for the Toshiba drive models varied within a fairly narrow range between 0.80% and 1.52%, with most quarters hovering slightly around 1.2%. Most importantly, none of the individual drive models were outliers, as the highest quarterly AFR for any Toshiba drive model was 1.58%. We like consistency.&nbsp;</p>



<p><strong>WDC</strong>. While WDC drive models delivered a similar level of consistency as the Toshiba models, they did so with a lower AFR each quarter. From 2022 through 2024, the range of quarterly AFR values for the WDC models was 0.0% to 0.85%. The 0.0% AFR was in Q1 2022 when none of the 12,207 WDC drives in operation failed during that quarter.</p>



<h2>Lifetime hard drive stats</h2>



<p>As of the end of 2024, Backblaze was monitoring 301,120 hard drives used to store data. Applying our drive criteria noted above for the lifetime period, we removed 11 drive models consisting of 2,736 drives from consideration as they did not meet the lifetime criteria we defined. This leaves us with 298,230 drives divided across 25 different drive models. The table below shows the lifetime AFRs for this collection of drives.</p>


<div>
<figure data-wp-context="{&quot;imageId&quot;:&quot;67ab872d07b42&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="720" height="840" data-wp-class--hide="state.isContentHidden" data-wp-class--show="state.isContentVisible" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://www.backblaze.com/blog/wp-content/uploads/2025/02/8-Lifetime-AFR.png" alt="A chart showing lifetime hard drive failure rates. " srcset="https://www.backblaze.com/blog/wp-content/uploads/2025/02/8-Lifetime-AFR.png 720w, https://www.backblaze.com/blog/wp-content/uploads/2025/02/8-Lifetime-AFR-257x300.png 257w" sizes="auto, (max-width: 720px) 100vw, 720px"></figure></div>






<p>The current lifetime AFR for all of the drives is 1.31%. This is down from 1.46% in 2023. The drop is primarily due to the completion of the migration of the 4TB Seagate drives in 2024, which left us with only two of these drives still in operation as of the end of 2024. As a consequence, the 79 million drive days and over 5,600 drive failures racked up by the 4TB Seagate drives by the end of 2023 are not included in the data presented in the 2024 lifetime table above.&nbsp;&nbsp;</p>



<p>In the final table below, we’ve taken the lifetime table and sorted out the drive models that have a lifetime AFR of 1.50% or less by drive size.</p>


<div>
<figure data-wp-context="{&quot;imageId&quot;:&quot;67ab872d07e3f&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="720" height="750" data-wp-class--hide="state.isContentHidden" data-wp-class--show="state.isContentVisible" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://www.backblaze.com/blog/wp-content/uploads/2025/02/9-Best-Lifetime-AFR.png" alt="A chart showing the best lifetime annualize failure rates broken down by drive size. " srcset="https://www.backblaze.com/blog/wp-content/uploads/2025/02/9-Best-Lifetime-AFR.png 720w, https://www.backblaze.com/blog/wp-content/uploads/2025/02/9-Best-Lifetime-AFR-288x300.png 288w" sizes="auto, (max-width: 720px) 100vw, 720px"></figure></div>






<p>A couple of caveats as you review the table.</p>



<ul>
<li>There is enough data for each model to say the AFR values are solid. That said, everything could change tomorrow. In general, the hard drive failure rate follows the bathtub curve as the drives age—unless it doesn’t. Some drives refuse to fail as they age, like the 4TB HGST drives. Other drives are great, and then “hit the wall” and bend the failure curve upward, fast.</li>



<li>A drive model with a 1% annualized failure rate means that you can expect one drive out of 100 to fail in a year. If you’re a personal drive user, that one drive could be yours. If you have exactly one drive, your personal annualized failure rate is 100%. In other words, always have a backup, and don’t forget to test it.</li>
</ul>



<h2>Migration time</h2>



<p>I have been authoring the various Drive Stats reports for the past ten years and this will be my last one. I am retiring, or perhaps in Drive Stats vernacular, it would be “migrating.” Either way, after 10 years in the U.S. Air Force and 30+ years in Silicon Valley Tech, it is time. Drive Stats will continue with <a href="https://www.backblaze.com/blog/author/stephanie/" target="_blank" rel="noreferrer noopener">Stephanie Doyle</a> and <a href="https://www.backblaze.com/blog/author/davidjohnson/" target="_blank" rel="noreferrer noopener">David Johnson</a> as the replacement drive models beginning with the Q1 2025 report. I wish them well.</p>



<p>I want to say thank you to each of you who have taken your time to peruse and engage with the Drive Stats reports and data over the last 10 years. And, thank you as well for the comments, questions, and discussions that raced and raged across the various communities that care about something as mundane and awesome as a hard drive. It has been quite the ride—thanks again.</p>



<h2>The Hard Drive Stats data</h2>



<p>The complete data set used to create the tables and charts in this report is available on our&nbsp;<a href="https://www.backblaze.com/cloud-storage/resources/hard-drive-test-data/" target="_blank" rel="noreferrer noopener">Hard Drive Test Data</a>&nbsp;page. You can download and use this data for free for your own purpose. All we ask are three things: 1) you cite Backblaze as the source if you use the data, 2) you accept that you are solely responsible for how you use the data, and 3) you do not sell this data itself to anyone; it is free.</p>



<p>Good luck, and let us know if you find anything interesting.</p>

							</section><!-- .entry-content -->

			
		<!-- taxonomy -->
		
		<!-- .entry-footer -->

						<section>
		<img alt="" data-del="avatar" src="https://www.backblaze.com/blog/wp-content/uploads/2019/04/andy.jpg" height="100" width="100">		<div>
			
			<p> Andy Klein is the Principal Cloud Storage Storyteller at Backblaze. He has over 25 years of experience in technology marketing and during that time, he has shared his expertise in cloud storage and computer security at events, symposiums, and panels at RSA, SNIA SDC, MIT, the Federal Trade Commission, and hundreds more. He currently writes and rants about drive stats, Storage Pods, cloud storage, and more.</p><!-- .author-description -->
		</div><!-- .author-bio-content -->
	</section><!-- .author-bio -->
				

		<!-- end .related-posts -->
	</article><!-- #post-111927 -->
	




	</div><!-- end .main-content -->


			</main><!-- #main -->
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Boring tech is mature, not old (439 pts)]]></title>
            <link>https://rubenerd.com/boring-tech-is-mature-not-old/</link>
            <guid>43012862</guid>
            <pubDate>Tue, 11 Feb 2025 14:08:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rubenerd.com/boring-tech-is-mature-not-old/">https://rubenerd.com/boring-tech-is-mature-not-old/</a>, See on <a href="https://news.ycombinator.com/item?id=43012862">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
<p>I’ve talked before about how I think NetBSD is “boring”, and that it’s among the highest forms of praise I can give tech as a sysadmin and architect. But I’ve never elaborated why that is.</p>
<p>The opposite of being bored is to be surprised, and that’s not something a sysadmin desires when building, maintaining, scaling, troubleshooting, upgrading, or even replacing a complex system… especially when you’re woken up at 03:30 by a monitoring server. That’s also why the phrase <em>work doing what you love and you never work a day again in your life</em> is an empty platitude of nonsense. But I digress.</p>
<p>Boring tech behaves in predictable ways. It’s a well trodden path others have evaluated, optimised, troubleshooted, and understood. Using tech that has been subjected to all those people hours of use means you’re less likely to run into edge cases, unexpected behaviour, or attributes and features that lack documentation or community knowledge. In other words, when something goes wrong, can you turn to someone or something?</p>
<p>Likewise, tech (generally) doesn’t exist in a vacuum. It interacts with other components and systems, some of which are even conveniently under our control, sometimes. Multiply out the potential for surprises by the number of components and their relative maturity, and your head can start to spin.</p>
<p>This isn’t to say there isn’t room for innovation, or that staying put is a guaranteed recipe for success. What it does teach is that it pays to make informed decisions, and that often times the understood, reliable, boring tech will get you there over something new, shiny or propped up with marketing spin. The number of people I’ve talked with who’ve replaced complicated K8s clusters with a few VMs and seen massive improvements in reliability, cost, and uptime would make some people at the Orange Peanut Gallery more than a little perturbed, for example.</p>
<p>There have been talks about this, perhaps <a href="https://boringtechnology.club/">most famously by Dan McKinley</a>. But there’s been some pushback to this idea, which is intriguing. <a href="https://mastodon.social/@raiderrobert/113977311777631282">Robert Roskam</a>:</p>
<blockquote>
<p>I used to agree with this. Now I don’t think so any more. You should prefer “boring” tech, and boring should be read as has been around for a while and therefore is well understood.</p>
<p>Ubiquity is a bad test for well-understood technology. Age as a test for ubiquity is also bad.</p>
</blockquote>
<p>There are a few assumptions here:</p>
<ul>
<li>
<p>Boring should be read as something being around for a while. I wouldn’t necessarily agree with that. I’ve been a DBA, and I never in a million years would call Oracle “boring”. It’s fiendishly complicated and difficult to maintain, and commands above-average salaries in part for that reason. A sign saying “there be Ellison dragons” isn’t boring, it’s frankly terrifying.</p>
</li>
<li>
<p>Ubiquity is a test for “boringness”. He’s right here; while age gives something an opportunity to become ubiquitous, it’s not a guarantee. I’d look to the BSDs here; I’d consider them boring, but they’re not exactly widely deployed compared to Penguins.</p>
</li>
<li>
<p>Boring should be read as well-understood. That’s true. Time gives something more of a chance of being well-understood, until suddenly it doesn’t and nobody is around with sufficient knowledge and inclination to maintain your COBOL stack. At that stage, I’d say you have a decidedly <em>un-boring</em> issue on your hands.</p>
</li>
</ul>
<p>I’d conclude by suggesting boring tech isn’t old, but <em>mature</em>. Maturity not just in the software, but its documentation, community, and track record. <em>Age</em> is often used as an analogue for maturity, but it’s not the same thing. Otherwise I’d be more mature than my Zoomer friends, and I very much doubt that to be the case. <code>#BIRDISTHEWORD</code>.</p>
<p>This is why I don’t hesitate to call NetBSD boring, and why I say that’s a compliment.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FLAC 1.5 Delivers Multi-Threaded Encoding (190 pts)]]></title>
            <link>https://www.phoronix.com/news/FLAC-1.5-Released</link>
            <guid>43012751</guid>
            <pubDate>Tue, 11 Feb 2025 13:58:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.phoronix.com/news/FLAC-1.5-Released">https://www.phoronix.com/news/FLAC-1.5-Released</a>, See on <a href="https://news.ycombinator.com/item?id=43012751">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><img alt="MULTIMEDIA" src="https://www.phoronix.com/assets/categories/multimedia.webp" width="100" height="100"></p><p>
FLAC 1.5 is out today as the newest feature update to the software built around the Free Lossless Audio Codec.
</p><p>
Similar to <a href="https://www.phoronix.com/news/WavPack-5.8-Released">WavPack only recently adding multi-threaded encode support</a>, FLAC 1.5 also introduces multi-threaded audio encoding.</p><p><img src="https://www.phoronix.net/image.php?id=2025&amp;image=flac_15_1" alt="FLAC 1.5 build"></p>
<p>With the <em>flac</em> utility the number of threads can be set either using "-j #" or the "--threads=#" argument. It's a long overdue change having multi-threaded audio encoding for FLAC given the past decade of rising CPU core counts. I will have out new <a href="https://openbenchmarking.org/test/pts/encode-flac#results">FLAC audio encoding CPU benchmarks</a> shortly for the multi-threaded encoder.
</p><p><img src="https://www.phoronix.net/image.php?id=2025&amp;image=flac_15_2" alt="FLAC 1.5 multi-threaded encode"></p>
<p>On the decode side, FLAC 1.5 can now handle chained Ogg FLAC files. FLAC 1.5 also brings various library fixes, build system updates, and other improvements.
</p><p>
Downloads and more details on FLAC 1.5 via <a href="https://github.com/xiph/flac/releases/tag/1.5.0">GitHub</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft open sources PostgreSQL extensions to muscle in on NoSQL (138 pts)]]></title>
            <link>https://www.theregister.com/2025/02/11/microsoft_postgresql_extensions/</link>
            <guid>43012294</guid>
            <pubDate>Tue, 11 Feb 2025 13:05:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2025/02/11/microsoft_postgresql_extensions/">https://www.theregister.com/2025/02/11/microsoft_postgresql_extensions/</a>, See on <a href="https://news.ycombinator.com/item?id=43012294">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p><span>Analysis</span> When Microsoft rolled out an open source extension stack for PostgreSQL to handle document-style data, it wasn't just taking aim at MongoDB – the dominant NoSQL player – but also blurring the lines between relational and non-relational databases, according to one expert.</p>
<p>Despite the tech giant having built a multitrillion-dollar valuation largely on proprietary software, its latest bid to back open source systems and compete in the document database market promises a schema-light approach popular with developers.</p>
<div><p><img src="https://regmedia.co.uk/2025/01/27/shutterstock_os_postgresql_db.jpg?x=174&amp;amp;y=115&amp;amp;crop=1" width="174" height="115" alt="PostgreSQL open source conceptual illustration (penguins against DB)"></p><h2 title="We're not in Kansas anymore">Microsoft builds open source document database on PostgreSQL, suggests FerretDB as front end</h2>
<p><a href="https://www.theregister.com/2025/01/27/microsoft_builds_open_source_document/"><span>READ MORE</span></a></p></div>
<p>Late last month, Redmond announced two extensions to <a target="_blank" href="https://www.theregister.com/2023/12/26/michael_stonebraker_feature/">PostgreSQL</a>, the open source relational database system with roots reaching back to the 1980s.</p>
<p>It has built pg_documentdb_core, a custom PostgreSQL extension that enables support for Binary JavaScript Object Notation (BSON, a binary-encoded serialization of JSON documents), and pg_documentdb_api, a data layer providing MongoDB-compatible commands for create, read, update and delete (CRUD) operations, queries, and index management. They are set to run on the Azure Cosmos DB PostgreSQL database service.</p>
<p>Microsoft also notes that its extensions can be used with FerretDB, an open source MongoDB alternative. FerretDB claims its 2.0 version delivers up to 20x performance improvements for certain workloads over earlier versions, partly due to backend optimizations.</p>

    

<p>Andrew Pavlo, associate professor of databaseology at Carnegie Mellon University, told <em>The Register</em> the move was another sign that document databases, classified as part of the NoSQL or non-relational database category, would become a feature of relational systems rather than a category on their own.</p>

        


        

<p>"The intellectual distance between document/JSON DBMSes and relational DBMSes is shrinking. At some point, the two system categories will be indistinguishable (at least in terms of the data model). The DocumentDB/FerretDB announcement is further evidence of this," he said.</p>
<p>"All the NoSQL systems are becoming relational, except for [key-value database] Redis. They now expose a relational data model with support for nested data (for example, JSON). They also expose a SQL interface, but they can't get themselves to call it SQL for some reason. Instead, they give their query languages a different name – Cassandra has CQL and Aerospike has AQL – and claim it is 'inspired' by SQL. Even MongoDB added support for <a target="_blank" href="https://www.mongodb.com/blog/post/introducing-atlas-sql-interface-connectors-drivers">SQL in their Atlas service</a> in 2022."</p>

        

<p>MongoDB helped establish the market for document databases in the early 2010s. In 2018, it <a target="_blank" href="https://www.theregister.com/2025/02/11/microsoft_postgresql_extensions/introduced%20a%20Server%20Side%20Public%20License">introduced a Server Side Public License</a> – which requires any organization offering MongoDB as a service to release the source code of the entire service – and also offers proprietary licenses. Its claim to be a favorite among developers is not without justification as it ranks highly on the Stack Overflow survey and counts Wells Fargo bank, Sega, and L'Oréal among its customers.</p>
<p>It was predictably unimpressed by Microsoft's effort to muscle in on its market.</p>
<p>A spokesperson at MongoDB said: "The rise of MongoDB imitators proves our document model is the industry standard. But bolting an API onto a relational database isn't innovation – it's just kicking the complexity can down the road. These 'modern alternatives' come with a built-in sequel: the inevitable second migration when performance, scale, and flexibility hit a wall. Developers building modern, AI-powered applications don't have time for do-overs. MongoDB is purpose-built to get it right the first time."</p>

        

<p>FerretDB 1.0 <a target="_blank" href="https://www.theregister.com/2023/04/14/ferretdb_10_ga/">launched in 2023</a>, claiming to be "a truly open source MongoDB alternative, built on PostgreSQL, and released under the Apache 2.0 license."</p>
<ul>

<li><a href="https://www.theregister.com/2025/02/06/windows_midi_services_2/">Microsoft makes sweet, sweet music with Windows MIDI Services</a></li>

<li><a href="https://www.theregister.com/2024/05/10/sql_cocreator_nosql/">Father of SQL says yes to NoSQL</a></li>

<li><a href="https://www.theregister.com/2025/01/31/microsoft_open_ai_reasoning_copilot/">You begged Microsoft to be reasonable. Instead it made Copilot reason-able with OpenAI GPT-o1</a></li>

<li><a href="https://www.theregister.com/2024/10/11/tencent_x_store_multi_model_nosql/">Tencent builds one NoSQL database to rule all data models</a></li>
</ul>
<p>Speaking to <em>The Register</em>, CEO Peter Farkas said FerretDB wanted to work with Microsoft to achieve the long-term aim of building a "more solid foundation for users to have a MongoDB alternative."</p>
<p>"The goal with this is to work broadly among the providers of MongoDB alternatives and have more cooperation. After a while, they would have one solid foundation for MongoDB alternatives, which everybody could build on," he said.</p>
<p>The reason for building on PostgreSQL was that, like Pavlo, Farkas sees the NoSQL group of databases eventually becoming a feature of relational systems.</p>
<p>"From time to time, specialized databases appear on the market, and these would live independently for a good while, but then going by history, large databases like Oracle or PostgreSQL would start to support features, either at their core or through extensions like what we did with Microsoft," he said.</p>
<p>PostgreSQL began adding support for JSON documents in 2013 and has developed it ever since, but that is not enough to create a rival to MongoDB on its own, Farkas argued.</p>
<p>"With PostgreSQL, working with extensions is much easier than getting something into the core database. Even large enterprises like Microsoft may not be able to steer the PostgreSQL community in that way, but extensions provide a very similar experience to the user. It's not complicated to install a Postgres extension and use it alongside PostgreSQL and for now, the innovation with PostgreSQL happens through extensions," he said.</p>
<p>Meanwhile, MongoDB has introduced new data types and features that could not be supported by the current JSON functionality of PostgreSQL, Farkas said. "It would not be possible to implement a performant MongoDB-compatible experience on top of the JSON support in PostgreSQL, and it was never a goal to become MongoDB-compatible."</p>
<p>Through the partnership with Microsoft, Farkas wants to create a developer-friendly experience for a document database without having to get their hands dirty with PostgreSQL itself. But he also wants to build a common standard for other PostgreSQL database services from alternative cloud vendors such as Google's Cloud SQL for PostgreSQL. To this end, he has <a target="_blank" rel="nofollow" href="https://opendocdb.org/the-need-for-standardization-in-document-databases-moving-beyond-mongodb-compatible">called for</a> collaboration among vendors to improve compatibility between alternatives to MongoDB.</p>
<blockquote>

<p>While Microsoft is very committed to SQL Server continuing, it's also clear that they are very serious about support for Postgres</p>
</blockquote>
<p>Industry analysts are skeptical that Microsoft's partnership with FerretDB signals a larger open source shift. Gartner senior director analyst Aaron Rosenbaum said it was unusual for Redmond to form a market partnership – even one just for PR purposes – as they've done with FerretDB and announced it so publicly.</p>
<p>But that does not mean it is likely to support other open source database projects. "We see this as continuing their work with Postgres," he said. "They have been a significant contributor to PostgreSQL. This has no signs of a broadening trend as they don't have that track record of contributing to other open source DBMSes. While Microsoft is very committed to SQL Server continuing, it's also clear that they are very serious about support for Postgres. This isn't dissimilar to their support for Linux and Windows Server on Azure."</p>
<p>Henry Cook, Gartner director analyst, said that the Microsoft extensions to PostgreSQL would continue to strengthen the open source system's position in the market and as an interface for DBMS services.</p>
<p>However, the tie-up with FerretDB was not likely to seriously damage MongoDB either. "MongoDB has established itself in a firm position within the market championing the NoSQL approach. This will provide more competition but there are already other NoSQL offerings out there. MongoDB will continue to defend its position based on its merits," Cook said. ®</p>                                


                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Firing programmers for AI will destroy everything (625 pts)]]></title>
            <link>https://defragzone.substack.com/p/techs-dumbest-mistake-why-firing</link>
            <guid>43010814</guid>
            <pubDate>Tue, 11 Feb 2025 09:42:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://defragzone.substack.com/p/techs-dumbest-mistake-why-firing">https://defragzone.substack.com/p/techs-dumbest-mistake-why-firing</a>, See on <a href="https://news.ycombinator.com/item?id=43010814">Hacker News</a></p>
Couldn't get https://defragzone.substack.com/p/techs-dumbest-mistake-why-firing: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[How about trailing commas in SQL? (191 pts)]]></title>
            <link>http://peter.eisentraut.org/blog/2025/02/11/how-about-trailing-commas-in-sql</link>
            <guid>43010365</guid>
            <pubDate>Tue, 11 Feb 2025 08:26:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://peter.eisentraut.org/blog/2025/02/11/how-about-trailing-commas-in-sql">http://peter.eisentraut.org/blog/2025/02/11/how-about-trailing-commas-in-sql</a>, See on <a href="https://news.ycombinator.com/item?id=43010365">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>Anecdotally, this might be the most requested feature in SQL: Allow
some trailing commas.</p>

<p>The classic example is</p>

<div><pre><code>SELECT a,
       b,
       c,  -- here
FROM ...
</code></pre></div>

<p>Another one is</p>

<div><pre><code>CREATE TABLE tab1 (
    a int,
    b int,
    c int,  -- here
)
</code></pre></div>

<p>There might be a few other popular ones.  (Please send feedback.)</p>

<p>How could we support that?  And by “we”, I primarily mean, either in
PostgreSQL, which I work on, or in the SQL standard, which I work on.
And ultimately in other SQL implementations, which may or may not
follow either of these two sources.</p>

<p>Implementing the above two cases in PostgreSQL is easy.  Done?!?</p>

<p>But there are loads more places in SQL with comma-separated lists, for
example, array constructors, row constructors, function calls, as well
as in various commands, such as function definitions, type
definitions, <code>COPY</code>, <code>CREATE PUBLICATION</code>, many others.  What to do
about this?  Is there a line to draw somewhere?</p>

<p>I can see a few possible approaches:</p>

<ol>
  <li>
    <p>We just support a few of the most requested cases.  Over time, we
can add a few more if people request it.</p>
  </li>
  <li>
    <p>We support most cases, except the ones that are too complicated to
implement or cause grammar conflicts.</p>
  </li>
  <li>
    <p>We rigorously support trailing commas everywhere commas are used
and maintain this for future additions.</p>
  </li>
</ol>

<p>These are all problematic, in my opinion.  If we do option 1, then how
do we determine what is popular?  And if we change it over time, then
there will be a mix of versions that support different things, and it
will be very confusing.  Option 2 is weird, how do you determine the
cutoff?  Option 3 would do the job, but it would obviously be a lot of
work.  And there might be some cases where it’s impossible, and it
would have to degrade into option 2.</p>

<p>In any case, it would also be nice to get this into the SQL standard.
Then we can aim for some consistency across implementations in the
long run.</p>

<p>There, we have the same questions, but we need to be even more
rigorous.</p>

<ol>
  <li>
    <p>If we just add support for a few that we feel like are most
requested and add more over time, then we need to manage different
conformance rules and a bunch of feature codes.  I think this would
be confusing for implementers and users.  I still remember that C
supported trailing commas in structure values in C89 but in enum
declarations only in C99.  That was confusing!  And now consider
having that but many more times!</p>
  </li>
  <li>
    <p>There is likely no option 2, because adding <code>&lt;optional trailing
comma&gt;</code> (or whatever it might be called) all over the standard text
is easy.  There are no cases that are “too complicated”.</p>
  </li>
  <li>
    <p>We could add it everywhere.  That’d be straightforward.  But then
we don’t take implementation concerns into account.  Are there
cases where it’s too complicated too parse?  Would it require
additional reserved words?  How do we verify that this is actually
implementable?  I think this would result in fragmentation in the
actual implementations, which is exactly not the point.</p>
  </li>
</ol>

<p>I researched about a dozen programming languages.  Most of them
nowadays have some trailing-comma support somewhere.  But that’s where
the similarities end.  Usually, you can see trailing commas support in
something like enum definitions or array initialization.  But how
about function calls?  Or function definitions?  You can easily find
three different answers!  Also, programming languages generally have
much fewer syntactic constructs than SQL, which is generally a good
thing, but then it’s hard to make a good comparison or find good
guidance.</p>

<p>On the SQL side, DuckDB has <a href="https://duckdb.org/docs/sql/dialect/friendly_sql.html#trailing-commas">advertised
support</a>
for trailing commas, but I have not found rigorous answers to the
above questions.  Trying it out, they cover some of the popular cases,
but you can easily find places where it doesn’t work for no obvious
reason.  Now if other implementations do it their own way as well,
we’ll just get chaos.</p>

<p>What do you think?  What’s the way forward here?</p>

  </div>
  
  
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Working from home is here to stay (104 pts)]]></title>
            <link>https://wolfstreet.com/2025/02/10/there-hasnt-been-much-if-any-reduction-in-wfh-in-over-2-years-despite-all-the-hype-about-rto/</link>
            <guid>43010037</guid>
            <pubDate>Tue, 11 Feb 2025 07:35:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wolfstreet.com/2025/02/10/there-hasnt-been-much-if-any-reduction-in-wfh-in-over-2-years-despite-all-the-hype-about-rto/">https://wolfstreet.com/2025/02/10/there-hasnt-been-much-if-any-reduction-in-wfh-in-over-2-years-despite-all-the-hype-about-rto/</a>, See on <a href="https://news.ycombinator.com/item?id=43010037">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		    <h2><strong><span>Hopes that Return to Office will bail out the office sector of CRE seem premature: Data on office attendance and Working from Home.</span></strong></h2>
<h4>By&nbsp;<a href="https://wolfstreet.com/author/wolf-richter/" target="_blank" rel="noopener noreferrer">Wolf Richter</a>&nbsp;for&nbsp;<a href="https://wolfstreet.com/" target="_blank" rel="noopener noreferrer">WOLF STREET</a>.</h4>
<p>The good folks in commercial real estate, including institutional investors, have been saying for over a year that the Return-to-Office mandates will scuttle Working-from-Home and fill up office space and boost the office sector out of its <a href="https://wolfstreet.com/category/all/commercial-property/" target="_blank" rel="noopener">epic depression</a>.</p>
<p>The media have jumped all over it with breathtaking headlines every time a company mandated a full five-days-a-week RTO, and enough companies have done so to keep these stories percolating, such as Tesla in 2022 already, then Amazon, Goldman Sachs, J.P. Morgan, Morgan Stanley, X, Dell, etc. Lots of times, these in-office policies come with the explicit or implicit threat of “or else.” And the Trump White House has been in the news with its battle trying to force it to happen with whoever will be left working for the government.</p>
<p>But those media reports may be giving the wrong impression, and in fact very little has changed since the beginning of 2023 in terms of actual office attendance and the percentage of full paid days worked from home – which is what should matter to the CRE industry, rather than the breathless media reports. So we’ll look at these two data sets.</p>
<h3><strong>Office attendance has barely ticked up in two years</strong>.</h3>
<p>Kastle’s weekly back-to-work barometer – which tracks how many people enter an office building for which Kastle provides the electronic access system – has barely ticked up since the start of 2023. Its <a href="https://www.kastle.com/safety-wellness/getting-america-back-to-work/">weekly barometer</a> measures office occupancy as a percentage of what it had been before Covid. So if pre-Covid levels of office occupancy come back, the barometer would rise to 100%, meaning same occupancy as before Covid, the good old days, so to speak.</p>
<p>But far from it. The average occupancy in the top 10 office markets in the latest week was still only at 54% of where it had been before Covid, so still down by 46% from pre-Covid, and only a few percentage points higher of where it had been at the same time in 2023, and just a hair higher than at the same time in 2024.</p>
<p>In the chart, the top gridline = 100% = pre-Covid level. The 10-city weekly average (red line) has been around 50% since the beginning of 2023, sometimes over, sometimes under. During the latest week, it was 54%, with Houston being at 65% at the top end and Philadelphia being at 44% at the bottom. The epicenter of WFH, San Francisco, was at 45% (chart from Kastle, click on the chart to enlarge it):</p>
<p><a href="https://wolfstreet.com/wp-content/uploads/2025/02/US-office-occupancy-kastle-2025-02-10.jpg" target="_blank" rel="noopener"><img fetchpriority="high" decoding="async" src="https://wolfstreet.com/wp-content/uploads/2025/02/US-office-occupancy-kastle-2025-02-10.jpg" alt="" width="1000" height="689" srcset="https://wolfstreet.com/wp-content/uploads/2025/02/US-office-occupancy-kastle-2025-02-10.jpg 1000w, https://wolfstreet.com/wp-content/uploads/2025/02/US-office-occupancy-kastle-2025-02-10-560x386.jpg 560w, https://wolfstreet.com/wp-content/uploads/2025/02/US-office-occupancy-kastle-2025-02-10-768x529.jpg 768w, https://wolfstreet.com/wp-content/uploads/2025/02/US-office-occupancy-kastle-2025-02-10-260x179.jpg 260w, https://wolfstreet.com/wp-content/uploads/2025/02/US-office-occupancy-kastle-2025-02-10-160x110.jpg 160w" sizes="(max-width: 1000px) 100vw, 1000px"></a></p>
<p>The prevalence for hybrid work shows up in the huge difference between the days of the week with the lowest occupancy, which for the month of January was Friday, with an average occupancy rate of 37% of pre-Covid levels, according to Kastle. The day of the week with the highest occupancy rate was Tuesday at 63%.</p>
<p>The average occupancy on Fridays in January was 28% in San Francisco and 30% in New York City. Even at the high end, in Houston, it was only 48%.</p>
<h3><strong>No decline in the share of WFH in 2023-2025</strong>.</h3>
<p>Another data set shows the same principle from a different angle: The share of full paid days worked from home had spiked by mid-2020 to over 60%, in part because companies with office workers switched to WFH, and in part because other employment that could not be shifted to WFH, such as in accommodation and food services, collapsed. During that time, tech workers switched to full WFH and kept their jobs, while restaurants and hotel workers weren’t working at all.</p>
<p>But as the economy reopened, those service workers were called back, and the share of WFH as a percent of overall full paid days plunged. But for office workers, WFH has remained a big factor. As a result, the share of full paid days worked from home remains at far higher levels than before Covid and has not come down further in 2023, 2024, and in January 2025.</p>
<p>This according to the ongoing study and data collection by Jose Maria Barrero, Nicholas Bloom, Shelby Buckman, and Steven J. Davis, published by <a href="https://wfhresearch.com/">WFH Research</a> (based on the original paper: Barrero, Jose Maria, Nicholas Bloom, and Steven J. Davis, 2021. “Why working from home will stick,” National Bureau of Economic Research Working Paper 28731).</p>
<p>In January 2025, over 29% of all full paid days were worked from home, up a hair from the Januarys in 2023 and 2024.</p>
<p>So the share of WFH has given up about half of the Covid spike (which peaked in June 2020 at 62%) and then got stuck at a share that is roughly <em>four times higher</em> than it had been in 2019 (7%). While the author say that they may over-sample people who are “more tech and internet savvy, especially among the least educated,” the trend still works out about the same.</p>
<p><img decoding="async" src="https://wolfstreet.com/wp-content/uploads/2025/02/US-working-from-home-2026-02-10_.png" alt="" width="1640" height="1401" srcset="https://wolfstreet.com/wp-content/uploads/2025/02/US-working-from-home-2026-02-10_.png 1640w, https://wolfstreet.com/wp-content/uploads/2025/02/US-working-from-home-2026-02-10_-560x478.png 560w, https://wolfstreet.com/wp-content/uploads/2025/02/US-working-from-home-2026-02-10_-1024x875.png 1024w, https://wolfstreet.com/wp-content/uploads/2025/02/US-working-from-home-2026-02-10_-768x656.png 768w, https://wolfstreet.com/wp-content/uploads/2025/02/US-working-from-home-2026-02-10_-1536x1312.png 1536w, https://wolfstreet.com/wp-content/uploads/2025/02/US-working-from-home-2026-02-10_-260x222.png 260w, https://wolfstreet.com/wp-content/uploads/2025/02/US-working-from-home-2026-02-10_-160x137.png 160w" sizes="(max-width: 1640px) 100vw, 1640px"></p>
<p>Of all full-time employees, 13% were full WFH in late 2024, while 26% were in a hybrid-WFH situation, so about 39% were at partially or fully WFH, according to the study. The remaining 61% were working fully on site.</p>

<p>WFH and hybrid arrangements dominated in these six industries, particularly in Information, which includes a lot of tech and social media companies, where only 28% of the full-time employees worked fully on site.</p>
<p>Percentage of full-time employees working fully on site, in a hybrid situation, or full WFH:</p>
<ul>
<li>Information: 28% fully on site, 51% hybrid, 21% full WFH,</li>
<li>Finance &amp; Insurance: 33% fully on site, 40% hybrid, 27% full WFH</li>
<li>Wholesale trade: 44% fully on site, 40% hybrid, 15% full WFH</li>
<li>Professional &amp; business services: 45% fully on site, 30% hybrid, 24% full WFH</li>
<li>Utilities: 49% fully on site, 28% hybrid, 23% full WFH</li>
<li>Real estate: 49% fully on site, 40% hybrid, 11% full WFH</li>
</ul>
<h3><strong>Hopes that RTO will bail out the office sector seem premature.</strong></h3>
<p>Turns out, many companies have stuck with hybrid work, often involving hot-desking, hoteling, and other arrangements, such as Google (3 days a week in office), Microsoft (50% of the time), Apple (3 days), Starbucks office workers (3 days), Adobe (2-3 days), Meta (1-2 days), etc., though they’re enforcing those days in the office more strictly, including on an “or-else” basis. Countless smaller companies have moved into the same direction.</p>
<p>And many many other companies, including Nvidia, Airbnb, Coinbase, Drobox, etc., and lots of smaller companies, startups, etc., whose employment is growing rapidly, have no RTO mandate at all. They have finetuned their WFH policies, created meeting places, and made lots of other adjustments to make remote work productive. For the company, it saves on office expenses, and it makes hiring good talent easier since WFH remains immensely popular. And it seems to be working for them.</p>
<p>But the media don’t write breathless headlines about companies’ sticking to and refining their WFH policies. These companies dedicated to WFH just don’t get that kind of screaming attention. And these companies wouldn’t do it if it didn’t work for them. So these hopes in the CRE industry that RTO will somehow refill offices and bail out the office sector seem premature.</p>
<p><i><strong>Enjoy reading WOLF STREET and want to support it? You can donate. I appreciate it immensely. Click on the beer and iced-tea mug to find out how:</strong></i></p>
<p>
<a href="https://wolfstreet.com/how-to-donate-to-wolf-street/" target="_blank" rel="noopener"><img decoding="async" src="https://wolfstreet.com/wp-content/uploads/2019/10/BeerMug2.jpg" alt="" width="100" height="115"></a>
</p>
<p><i><strong>Would you like to be notified via email when WOLF STREET publishes a new article? <a href="https://wolfstreet.com/sign-up-here-for-wolf-street-email-updates/" target="_blank" rel="noopener noreferrer">Sign up here</a>.</strong></i></p>
<p><img loading="lazy" decoding="async" src="https://wolfstreet.com/wp-content/uploads/2020/08/placeholder2.png" alt="" width="26" height="65"></p>

	    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[TSMC 2nm Process Disclosure – How Does It Measure Up? (120 pts)]]></title>
            <link>https://semiwiki.com/semiconductor-services/techinsights/352972-iedm-2025-tsmc-2nm-process-disclosure-how-does-it-measure-up/</link>
            <guid>43009850</guid>
            <pubDate>Tue, 11 Feb 2025 07:05:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://semiwiki.com/semiconductor-services/techinsights/352972-iedm-2025-tsmc-2nm-process-disclosure-how-does-it-measure-up/">https://semiwiki.com/semiconductor-services/techinsights/352972-iedm-2025-tsmc-2nm-process-disclosure-how-does-it-measure-up/</a>, See on <a href="https://news.ycombinator.com/item?id=43009850">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<div><h2>Key Takeaways</h2><ul><li>TSMC presented its 2nm Platform Technology at IEDM 2024, focusing on energy-efficient nanosheet transistors and 3DIC co-optimization for various applications.</li><li>The 2nm process is reported to deliver a 30% power improvement and a 15% performance gain compared to TSMC's previous 3nm node.</li><li>TSMC's 2nm technology is characterized as potentially the densest and most power-efficient in the 2nm class, with promising early yield reports.</li></ul></div><p><strong>Initial thoughts</strong></p>
<p>At IEDM held in December 2024, TSMC presented: “2nm Platform Technology featuring Energy-efficient Nanosheet Transistors and Interconnects co-optimized with 3DIC for AI, HPC and Mobile SoC Applications,” the authors are:</p>
<p>Geoffrey Yeap, S.S. Lin, H.L. Shang, H.C. Lin, Y.C. Peng, M. Wang, PW Wang, CP Lin, KF Yu, WY Lee, HK Chen, DW Lin, BR Yang, CC Yeh, CT Chan, JM Kuo, C-M Liu, TH Chiu, MC Wen, T.L. Lee, CY Chang, R. Chen, P-H Huang, C.S. Hou, YK Lin, FK Yang, J. Wang, S. Fung, Ryan Chen, C.H. Lee, TL Lee, W. Chang, DY Lee, CY Ting, T. Chang, HC Huang, HJ Lin, C. Tseng, CW Chang, KB Huang, YC Lu, C-H Chen, C.O. Chui, KW Chen, MH Tsai, CC Chen, N. Wu, HT Chiang, XM Chen, SH Sun, JT Tzeng, K. Wang, YC Peng, HJ Liao, T. Chen, YK Cheng, J. Chang, K. Hsieh, A. Cheng, G. Liu, A. Chen, HT Lin, KC Chiang, CW Tsai, H. Wang, W. Sheu, J. Yeh, YM Chen, CK Lin, J. Wu, M. Cao, LS Juang, F. Lai, Y. Ku, S.M. Jang, L.C. Lu- with Jeffrey Yeap presenting the work.</p>
<p>This paper continued TSMC’s trend over the last several years of presenting marketing papers at IEDM instead of technical papers. In fact, this paper took the trend even further, there are no pitches in the paper, no SRAM cell size, and that graphs are all relative performance graphs without real units. Although the paper doesn’t present the kind of technical details that would typically be included in an IEDM paper, it does paint a picture of a process ready for 2025 production and the session was packed.</p>
<p>In this review we will take the few substantive details that are in the paper as well as our own analysis and present how the process compares to competing 2nm class processes.</p>
<p>In terms of the overreaching Power, Performance, and Area (PPA), the paper states that the process delivers a 30% power improvement or 15% performance gain and &gt;1.15x density versus the previous 3nm node. Note: the 3nm paper reference suggests this is in comparison to N3E, not N3.</p>
<h6><strong>Power </strong></h6>
<p>At the 14nm (Samsung)/16nm (TSMC) node Samsung and TSMC both produced the Apple A9 processors. Measurements by Tom’s hardware found the Samsung version had slightly better power performance compared to TSMC. We believe the A9 was designed for Samsung first so that may simply reflect a design that is more optimized for Samsung that was ported to TSMC, nevertheless, the power was very close between the two. Going forward from 14nm/16nm, to 10nm, 7nm, 5nm, 3nm, and now 2nm Samsung and TSMC have both provided relative power improvement for each node versus the previous node.</p>
<p>At 10nm TSMC provided a larger power reduction than Samsung and maintained that lead until 3nm where Samsung Gate All Around (GAA) provided a large enough improvement to mostly close the gap to TSMC’s 3nm FinFET process in power (GAA versus FinFET is expected to provide a greater power improvement).</p>
<p>TSMC 2nm announced power improvement of 30% versus 3nm is greater than Samsung’s 25% improvement and TSMC once again maintains a lead.</p>
<p>Specific power factor numbers are available in the full article available with free registration on the TechInsights platform <a href="https://bit.ly/42B3DDE">here</a>.</p>
<p>During the presentation of the paper, graphs were shown of power efficiency and performance per watt versus node. The power efficiency graph was in one version of the paper although it is not in the “final” version of the paper published in the proceeding. Thankfully we captured the power efficiency graph because it is very interesting to analyze, see figure 1.</p>
<figure id="attachment_352973" aria-describedby="caption-attachment-352973"><img decoding="async" title="Figure 1" src="https://semiwiki.com/wp-content/uploads/2025/02/Figure-1.jpg" alt="tsmc iedm" width="414" height="283" srcset="https://semiwiki.com/wp-content/uploads/2025/02/Figure-1.jpg 414w, https://semiwiki.com/wp-content/uploads/2025/02/Figure-1-300x205.jpg 300w, https://semiwiki.com/wp-content/uploads/2025/02/Figure-1-150x103.jpg 150w" sizes="(max-width: 414px) 100vw, 414px"><figcaption id="caption-attachment-352973">Figure 1. TSMC Power Efficiency.</figcaption></figure>
<p>We took the graph image, pulled it into Excel and created an Excel graph overlaying it with the 28nm bar normalized to 1 and then entering values for the other bars until they matched the graph. If we then build a set of bars starting at 28nm = 1 scaled up based on the TSMC announced node to node power improvements we get a total improvement of less than 9x. Nodes from N28 to N10 match well but from N7 on the bars on the graph show more improvement per node than TSMC has announced. Just the N3 to N2 bars on the graph show a 55% improvements versus the announced 30% improvement.</p>
<p>Figure 2 is in the full article available with free registration on the TechInsights platform <a href="https://bit.ly/42B3DDE">here</a>.</p>
<p>It isn’t clear what may be driving this difference, but it is a big disconnect. This may be why the graph was removed from the final paper.</p>
<h6><strong>Performance </strong></h6>
<p>Similar to the power analysis above, at Samsung 14nm/TSMC 16nm the Apple A9 processor had identical performance on the 2 processes. Normalizing both processes to 1 and applying the announced node to node performance improvements from both companies it is possible to compare performance per node. It has also been possible to use an Intel 10SF versus AMD processors on TSMC 7nm process, to add Intel to the analysis and forward calculate based on Intel performance by node announcements.</p>
<p>Based on this analysis it is our belief that Intel 18A has the highest performance for a 2nm class process with TSMC in second place and Samsung in third place.</p>
<p>Our performance index values are in the full article available with free registration on the TechInsights platform <a href="https://bit.ly/42B3DDE">here</a>.</p>
<h6><strong>Area </strong></h6>
<p>The third part of PPA is area. We analyze two “area” related factors, one is high density logic cell transistor density and the second is SRAM cell size. TechInsights has done detailed reverse engineering work on TSMC N3E process and we have all the pitches necessary to calculate our standard high density logic cell transistor density. Similarly, we have analyzed Samsung SF3E and SF3. Both TSMC in this paper and Samsung in public statements have provided density improvement values for 2nm. In the case of Intel we have all of the pitches for 18A under NDA and while we can’t disclose the specific pitches we can do a density comparison. For high density logic cells TSMC is well ahead of Samsung and Intel on density, Intel is second, and Samsung is third.</p>
<p>The high density logic cell transistor density is in the full article available with free registration on the TechInsights platform <a href="https://bit.ly/42B3DDE">here</a>.</p>
<p>As previously mentioned, the TSMC paper does not include SRAM cell sizes, however there is a graph of SRAM density versus node, see figure 3.</p>
<figure id="attachment_352974" aria-describedby="caption-attachment-352974"><img loading="lazy" decoding="async" title="Figure 3. SRAM Array Density Versus Node." src="https://semiwiki.com/wp-content/uploads/2025/02/Figure-3.jpg" alt="Figure 3" width="414" height="255" srcset="https://semiwiki.com/wp-content/uploads/2025/02/Figure-3.jpg 414w, https://semiwiki.com/wp-content/uploads/2025/02/Figure-3-300x185.jpg 300w, https://semiwiki.com/wp-content/uploads/2025/02/Figure-3-150x92.jpg 150w" sizes="auto, (max-width: 414px) 100vw, 414px"><figcaption id="caption-attachment-352974">Figure 3. SRAM Array Density Versus Node.</figcaption></figure>
<p>The problem with this is an SRAM array includes not only the SRAM cell but also overhead, for example 7nm has 25.0 Mb/mm<sup>2</sup>, the SRAM cell size at 7nm was 0.0270um<sup>2</sup>. If you multiply 25.0Mb by the SRAM cell size, you get 0.675mm<sup>2</sup>. The difference between 1.000 and 0.675mm<sup>2</sup> is the overhead and it isn’t constant from node to node, see table 1.</p>
<p>The SRAM cell size analysis is in the full article available with free registration on the TechInsights platform <a href="https://bit.ly/42B3DDE">here</a>.</p>
<h6><strong>Yield </strong></h6>
<p>Yield is a hot topic these days with lot of reports about Samsung struggling with yield at 3nm and losing customers due to low yield, there have also been some recent reports that Intel’s 18A yield is 10%.</p>
<p>In the paper TSMC reports that a 256Mb SRAM array has &gt;80% average yield and &gt;90% peak yield. These yields at this point in development indicate excellent defect densities. There are other yield components beside those tested in an SRAM array, but these are impressive results.</p>
<p>With respect to Intel’s 10% yield report, we have had two separate credible sources that tell us that simply isn’t true, that yields are much better than that. The other things about a report of 10% yield is how big/what is the die and at what point in development was that yield seen if it is even true. Our belief based on our sources is the 10% reported yield is either wrong or old data.</p>
<h6><strong>Wafer price </strong></h6>
<p>Another number that has been widely circulated is that TSMC is going to charge $30,000 per wafer for 2nm.</p>
<p>TechInsights produces the world’s leading cost and price models for semiconductors. Prior to 3nm entering production we were projecting &lt;$20,000 per wafer and a few customers contacted us insisting 3nm prices would be $20,000 to $25,000 per wafer. Once 3nm entered production we were able to run our proprietary forensics on TSMC’s financials and determine we were correct, and the volume price was &lt;$20,000/wafer by thousands of dollars.</p>
<p>To go from a price of &lt;$20,000/wafer for 3nm wafers to $30,000/wafer for 2nm wafers is a &gt;1.5x price increase for a 1.15x density improvement, that is a dramatic increase in transistor cost and it raises the question of who would pay that, our price estimates are &lt;$30,000/wafer. There have also been reports that Apple who is typically TSMC’s lead customer for each node may be forgoing initial 2nm use due to price although we have also heard push back on that.</p>
<p>Another element to this discussion is what volumes the pricing is for TSMC’s high volume wafer price is a lot lower than their low volume wafer price, so volume needs to be considered in any discussion. In general, we believe $30,000 is higher than the average to high volume pricing will be.</p>
<p>If TSMC prices 2nm wafers at $30,000/wafer they will create a lot of pressure for customers to switch to Intel and Samsung for 2nm class wafer supplies.</p>
<h6><strong>Backside Power Delivery</strong></h6>
<p>The TSMC paper does not address backside power delivery but competing 2nm processes will be implementing backside power delivery.</p>
<p>Intel 18A will have backside power delivery – with a 2025 ramp Intel will be the first to implement this technology. In 2026 Samsung SF2P process is due to also implement backside power delivery. Finally, TSMC is not expected to implement backside power delivery on their 2nm process variants at all and will wait until 2027 (recent reports are that this is being pulled in to 2026) to implement it on their A16 process. The A16 backside power delivery is expected to be a direct backside connection that can provide smaller track heights than Intel’s and likely Samsung’s implementation.</p>
<p>Since Intel is the most performance focused of the three companies it makes sense, they are implementing backside power delivery first.</p>
<p>Another interesting thing we are hearing about backside power delivery is that foundry HPC customers want it but mobile customers don’t due to cost.</p>
<p>For multiple nodes we may see nodes with and without backside power delivery and given the effect it has on metal 0 the design rules would likely be different. In addition to this for the highest performance we expect molybdenum to be introduced first for vias and later from critical interconnect. This could lead to nodes splitting between backside power delivery and molybdenum metallization for HPC and no backside power and copper metallization for mobile.</p>
<h6><strong>Other</strong></h6>
<p>One final interesting item in the paper is the comment about “flat passivation”. Many processes have a top aluminum metal layer and passivation follows the metal contours, if something like hybrid bonding is desired the wafer surface must be flat. Flat passivation is presumably a planarized top layer to enable bonding.</p>
<h6><strong>Conclusion</strong></h6>
<p>TSMC has disclosed a 2nm process likely to be the densest available 2nm class process. It also appears to be the most power efficient at least when compared to Samsung. In terms of performance, we believe Intel 18A is the leader. The early yield reports appear promising, but the reports of $30,000/wafer pricing do not in our opinion represent acceptable value for the process and may present an opportunity for Intel and Samsung to capture market share . TSMC 2nm should be in production in the second half of this year.</p>
<h6>Also Read:</h6>
<p><a href="https://semiwiki.com/semiconductor-services/349292-5-expectations-for-the-memory-markets-in-2025/" rel="bookmark">5 Expectations for the Memory Markets in 2025</a></p>
<p><a href="https://semiwiki.com/semiconductor-manufacturers/intel/346992-vlsi-technology-symposium-intel-describes-i3-process-how-does-it-measure-up/" rel="bookmark">VLSI Technology Symposium – Intel describes i3 process, how does it measure up?</a></p>
<p><a href="https://semiwiki.com/semiconductor-services/344572-intel-high-na-adoption/" rel="bookmark">Intel High NA Adoption</a></p>
<p><a href="https://semiwiki.com/semiconductor-manufacturers/tsmc/342934-no-tsmc-does-not-make-90-of-advanced-silicon/" rel="bookmark">No! TSMC does not Make 90% of Advanced Silicon</a></p>
<p><span>Share this post via: </span></p>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Jeep Introduces Pop-Up Ads That Appear Every Time You Stop (272 pts)]]></title>
            <link>https://tech.slashdot.org/story/25/02/11/0016258/jeep-introduces-pop-up-ads-that-appear-every-time-you-stop</link>
            <guid>43009682</guid>
            <pubDate>Tue, 11 Feb 2025 06:34:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tech.slashdot.org/story/25/02/11/0016258/jeep-introduces-pop-up-ads-that-appear-every-time-you-stop">https://tech.slashdot.org/story/25/02/11/0016258/jeep-introduces-pop-up-ads-that-appear-every-time-you-stop</a>, See on <a href="https://news.ycombinator.com/item?id=43009682">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="fhbody-176175319">
	
		
	

	
		
		<p>
			
		 	
				"<a href="https://techstory.in/stellantis-introduces-pop-up-ads-in-vehicles-sparking-outrage-among-owners/">In-dash advertising is here</a> and Stellantis, the parent company of Jeep, Dodge, Chrysler, and Ram, beat everyone to <a href="https://tech.slashdot.org/story/25/02/06/0454231/the-enshittification-hall-of-shame">further enshittification</a>," writes longtime Slashdot reader <a href="https://tech.slashdot.org/~sinij">sinij</a>. "Ads can be seen in <a href="https://www.youtube.com/watch?v=A31PkJaTZqU">this video</a>." From a report: <i> In a move that has left drivers both frustrated and bewildered, Stellantis has introduced full-screen pop-up ads on its infotainment systems. Specifically, Jeep owners have reported being bombarded with advertisements for Mopar's extended warranty service. The kicker? These ads appear every time the vehicle comes to a stop. Imagine pulling up to a red light, checking your GPS for directions, and suddenly, the entire screen is hijacked by an ad. That's the reality for some Stellantis owners. Instead of seamless functionality, drivers are now forced to manually close out of ads just to access basic vehicle functions.
<p> 
One Jeep 4xe owner recently shared their frustration on an online forum, detailing how these pop-ups disrupt the driving experience. Stellantis, responding through their "JeepCares" representative, confirmed that these ads are part of the contractual agreement with SiriusXM and suggested that users simply tap the "X" to dismiss them. While the company claims to be working on reducing the frequency of these interruptions, the damage to customer trust may already be done.</p></i><br>
		 	
		</p>

		

		

		
			
		
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meta's Hyperscale Infrastructure: Overview and Insights (276 pts)]]></title>
            <link>https://cacm.acm.org/research/metas-hyperscale-infrastructure-overview-and-insights/</link>
            <guid>43008920</guid>
            <pubDate>Tue, 11 Feb 2025 04:19:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cacm.acm.org/research/metas-hyperscale-infrastructure-overview-and-insights/">https://cacm.acm.org/research/metas-hyperscale-infrastructure-overview-and-insights/</a>, See on <a href="https://news.ycombinator.com/item?id=43008920">Hacker News</a></p>
<div id="readability-page-1" class="page"><div lang="en"><section id="sec1"><p id="p-1">Hyperscalers, such as Alibaba, Amazon, ByteDance, Google, Meta, Microsoft, and Tencent, have developed planetary-scale infrastructure to deliver cloud, Web, or mobile services to their global users. And though most practitioners may not directly build such hyperscale infrastructure, we believe it is beneficial to learn a bit about it. Historically, many widely used technologies have originated from advanced environments, including mainframes in the 1960s and hyperscale infrastructure in the past two decades. For instance, virtual memory had its origin in mainframes and is now common even in smartwatches. Similarly, Kubernetes and PyTorch originated in Google and Facebook, respectively, but have been adopted by organizations of all sizes. In addition to these specific technologies, the principles and lessons from hyperscale infrastructure may assist practitioners in building better systems in general.</p><p id="p-2">This article provides a high-level overview of Meta’s hyperscale infrastructure, focusing on key insights from its development, particularly in systems software. Where relevant, we highlight differences from public clouds, as varying constraints have led to distinct optimizations. While much of the knowledge presented here has been shared and practiced within the industry and research community, including insights from our past publications, the article’s primary contribution is to provide a holistic perspective that helps readers build a comprehensive mental model of hyperscale infrastructure end to end.</p><div><h2>Key Insights</h2><ul data-jats-list-type="bullet"><li><p id="p-3">Meta’s engineering culture emphasizes moving fast, technology openness, research in production, and shared infrastructure.</p></li><li><p id="p-4">To boost developer productivity, Meta has adopted continuous deployment universally and enabled more developers to write serverless functions rather than traditional service code.</p></li><li><p id="p-5">To reduce hardware costs, Meta utilizes hardware-software co-design at the datacenter scale and autonomously optimizes resource allocations, including workload migration, across global datacenters instead of limiting them to individual clusters.</p></li><li><p id="p-6">Meta’s AI strategy involves co-designing the entire stack, from PyTorch to AI accelerators, networks, and ML models such as Llama.</p></li></ul></div></section><section id="sec2"><h2>Engineering Culture</h2><p id="p-7">Before delving into the details of Meta’s infrastructure, we first highlight several aspects of the company’s engineering culture, because an organization’s culture heavily influences its technology.</p><section id="sec3"><p data-jats-content-type="inline-heading"><strong>Move fast.</strong>&nbsp; Since its inception, Facebook has ingrained and retained the “move-fast” culture, emphasizing agility and rapid iteration. This philosophy is evident in its strong commitment to continuous software deployment, which involves releasing the latest code into production as early as possible. Additionally, product engineers predominantly write code in stateless, serverless functions in PHP, Python, and Erlang for their benefits in simplicity, productivity, and iteration speed. Teams have the ability to quickly pivot their execution priorities without undergoing a lengthy replanning process, leaving ambiguous issues to be sorted out during iterative execution. This allows teams to quickly adapt and launch new products in response to evolving market conditions.</p></section><section id="sec4"><p data-jats-content-type="inline-heading"><strong>Technology openness.</strong>&nbsp; Meta champions technology openness, both internally and externally. Internally, we adopt the monorepo approach, storing the code for all projects in a single repository to facilitate code discovery and reuse, as well as cross-team contributions. While other organizations also use monorepos, they vary in the degree of openness. Some require designated owners for each project, with only these owners authorized to accept code changes, although others may propose changes. In contrast, with few exceptions, the vast majority of projects at Meta do not enforce such strict ownership rules. This openness encourages cross-team contributions and code reuse while discouraging the reinvention of similar technologies.</p><p id="p-10">At Meta, engineers directly commit code changes to the mainline of the monorepo, and software deployments are compiled from the mainline, that is, from the latest code, as opposed to some stable branches. For example, when a widely used library, such as the RPC library, is updated, the next release of every application dependent on this library will be automatically compiled with the latest version.</p><p id="p-11">Externally, Meta’s commitment to technology openness is demonstrated through its open-source hardware designs via the Open Compute Project<a href="#B28" data-jats-ref-type="bibr" data-jats-rid="B28"><sup>28</sup></a> and open-source software projects such as PyTorch, Llama, Presto, RocksDB, and Cassandra. Also, much of Meta’s infrastructure technology has been shared through research papers, with many examples in this article’s references.</p></section><section id="sec5"><p data-jats-content-type="inline-heading"><strong>Research in production.</strong>&nbsp; Meta’s hyperscale infrastructure requires continuous innovation, but unlike most hyperscalers, the company does not have a dedicated systems research lab. Instead, all of its systems research papers are authored by teams developing production systems. These teams advance the state of the art while tackling challenging production issues at scale, then reflect on these experiences to distill working solutions into research papers. This approach ensures that the addressed problems are real and the solutions work at scale, aligning well with key criteria for successful systems research.</p></section><section id="sec6"><p data-jats-content-type="inline-heading"><strong>Common infrastructure.</strong>&nbsp; While some organizations empower individual teams to make local decisions about their technology stack, Meta prioritizes standardization and global optimization. On the hardware side, servers supporting different products are all allocated from a shared server pool.<a href="#B34" data-jats-ref-type="bibr" data-jats-rid="B34"><sup>34</sup></a> Moreover, for non-AI compute workloads, we offer only a single server type, equipped with one CPU and the same amount of DRAM (previously 64GB, now 256GB). Unlike public clouds, which must provide various server types to accommodate diverse customer applications, Meta can optimize its applications to suit the hardware, thereby avoiding the proliferation of server types.</p><p id="p-14">Standardization also prevails on the software side. For instance, different Meta products previously used Cassandra, HBase, and ZippyDB<a href="#B24" data-jats-ref-type="bibr" data-jats-rid="B24"><sup>24</sup></a> for key-value stores, but now all have converged to ZippyDB. Further, each common capability—such as software deployment,<a href="#B19" data-jats-ref-type="bibr" data-jats-rid="B19"><sup>19</sup></a> configuration management,<a href="#B33" data-jats-ref-type="bibr" data-jats-rid="B33"><sup>33</sup></a> service mesh,<a href="#B31" data-jats-ref-type="bibr" data-jats-rid="B31"><sup>31</sup></a> pre-production performance testing,<a href="#B11" data-jats-ref-type="bibr" data-jats-rid="B11"><sup>11</sup></a> in-production performance monitoring,<a href="#B39" data-jats-ref-type="bibr" data-jats-rid="B39"><sup>39</sup></a> and in-production load testing<a href="#B35" data-jats-ref-type="bibr" data-jats-rid="B35"><sup>35</sup></a>—is supported by a universally adopted tool.</p><p id="p-15">Besides standardization, a key principle in achieving common infrastructure is our preference for reusable components over monolithic solutions. A good example of this is the component-reuse chain in our distributed file system, Tectonic.<a href="#B29" data-jats-ref-type="bibr" data-jats-rid="B29"><sup>29</sup></a> Tectonic enhances scalability by using a distributed key-value store, ZippyDB,<a href="#B24" data-jats-ref-type="bibr" data-jats-rid="B24"><sup>24</sup></a> to store its metadata. ZippyDB further employs a common sharding framework, Shard Manager, to manage its data shards; Shard Manager, in turn, depends on Meta’s mesh, ServiceRouter,<a href="#B31" data-jats-ref-type="bibr" data-jats-rid="B31"><sup>31</sup></a> for shard discovery and request routing. Finally, ServiceRouter stores the service discovery and configuration data of the entire infrastructure, which is critical for the site’s continuous operation, in the highly reliable, zero-dependency data store Delos.<a href="#B3" data-jats-ref-type="bibr" data-jats-rid="B3"><sup>3</sup></a> Therefore, the component-reuse chain is Tectonic→ZippyDB→Shard Manager→ServiceRouter→Delos. All of these reusable components also serve many other use cases. In contrast, HDFS, a popular open-source distributed file system, is a monolithic system that implements all of these components internally.</p></section><section id="sec7"><p data-jats-content-type="inline-heading"><strong>Culture case study: The Threads app.</strong>&nbsp; The development of the Threads app,<a href="#B6" data-jats-ref-type="bibr" data-jats-rid="B6"><sup>6</sup></a> often compared to Twitter/X, exemplifies the aforementioned culture. Emphasizing moving fast, a small team developed Threads with just five months of technical work in a startup-like environment. Moreover, once it was developed, the infrastructure teams were given only two day’s notice to prepare for its production launch. Most large organizations would take longer than two days just to draft a project plan involving dozens of interdependent teams, let alone execute it. At Meta, however, we quickly established war rooms across distributed sites, bringing together both infrastructure and product teams to address issues in real-time. Despite the tight timeline, the app’s launch was highly successful, reaching 100 million users within just five days, making it the fastest-growing app in history.<a href="#B6" data-jats-ref-type="bibr" data-jats-rid="B6"><sup>6</sup></a></p><p id="p-17">Common infrastructure was crucial for enabling teams to swiftly implement Threads and scale it reliably. Threads reused Instagram’s Python backend as well as Meta’s shared infrastructure components, such as the social-graph database,<a href="#B5" data-jats-ref-type="bibr" data-jats-rid="B5"><sup>5</sup></a> key-value store,<a href="#B24" data-jats-ref-type="bibr" data-jats-rid="B24"><sup>24</sup></a> serverless platform,<a href="#B30" data-jats-ref-type="bibr" data-jats-rid="B30"><sup>30</sup></a>&nbsp;machine-learning (ML) training and inference platforms,<a href="#B10" data-jats-ref-type="bibr" data-jats-rid="B10"><sup>10</sup></a> and configuration-management framework for mobile apps.<a href="#B20" data-jats-ref-type="bibr" data-jats-rid="B20"><sup>20</sup></a></p><p id="p-18">Meta’s internal technology openness, using a monorepo, allowed Threads to reuse some Instagram application code to accelerate its development. In terms of external technology openness, Threads aims to integrate with ActivityPub, the open social networking protocol, for interoperability with other apps. We have also publicly shared our experiences of rapidly developing Threads.<a href="#B6" data-jats-ref-type="bibr" data-jats-rid="B6"><sup>6</sup></a></p></section></section><section id="sec8"><h2>End-to-End User Request Flow</h2><p id="p-20">We now dive into Meta’s infrastructure technology. Meta products are supported by a shared service infrastructure. To provide a holistic view of this infrastructure, we explain how a user request is processed end-to-end, detailing all the components involved.</p><section id="sec9"><p><strong>Request routing.</strong></p><section id="sec10"><p data-jats-content-type="inline-heading"><em>Dynamic DNS mapping.</em>&nbsp; When a user initiates a request to facebook.com, Meta’s DNS server dynamically returns an IP address that is mapped to a Meta-operated small edge datacenter, known as point of presence (PoP), as depicted in Figure <a href="#F1" data-jats-ref-type="fig" data-jats-rid="F1">1</a>. This dynamic DNS mapping ensures that the chosen PoP is close to the user, while balancing load across PoPs. The user’s TCP connection is terminated at the PoP, which maintains separate, long-lived TCP connections with Meta’s datacenters. This split-TCP setup offers several advantages, including reduced TCP-establishment latency through the reuse of pre-established connections between PoPs and datacenters. A PoP typically has hundreds of servers but may have up to a few thousand. Hundreds of PoPs are positioned worldwide to ensure that most users have a PoP close to them, thereby ensuring short network latencies.</p><figure id="F1" data-jats-position="float"><p><a data-fslightbox="https://cacm.acm.org/wp-content/uploads/2024/12/3701296_fig01.jpg" data-type="image" data-caption="Figure 1. Meta’s global infrastructure." href="https://cacm.acm.org/wp-content/uploads/2024/12/3701296_fig01.jpg">
				<img decoding="async" title="Figure 1. Meta’s global infrastructure." src="https://cacm.acm.org/wp-content/uploads/2024/12/3701296_fig01.jpg" alt="Meta’s global infrastructure." data-image-id="F1" data-image-type="figure">
			</a>
		</p><figcaption><span>Figure 1.&nbsp;</span> <span>Meta’s global infrastructure.</span></figcaption></figure></section><section id="sec11"><p data-jats-content-type="inline-heading"><em>Static-content caching.&nbsp;</em> If the user request is for static content, such as images and videos, it can be directly served at the PoP if the content is already cached there. Additionally, static content may be cached by the content delivery network (CDN), as shown in Figure <a href="#F1" data-jats-ref-type="fig" data-jats-rid="F1">1</a>. When a significant volume of Meta product traffic originates from an Internet service provider’s (ISP’s) network, Meta seeks to establish a mutually beneficial partnership by providing Meta Network Appliances to be hosted in the ISP’s network to cache static content, thereby forming a CDN site. A CDN site typically has tens of servers, with some having over a hundred. Thousands of CDN sites across the globe form our CDN for distributing static content.</p><p id="p-23">Meta products use URL rewrites to redirect user requests to a nearby CDN site. When a Meta product provides a URL for a user to access static content, it rewrites the URL, for example, from <code>facebook.com/image.jpg</code> to <code>CDN109.meta.com/image.jpg.</code> If the image is not cached at CDN109 when the user requests it, CDN109 forwards the request to a nearby PoP. The PoP then forwards the request to the load balancer in a datacenter region, which retrieves the image from the storage system. On the return path, both the PoP and the CDN site cache the image for future use.</p></section><section id="sec12"><p data-jats-content-type="inline-heading"><em>Dynamic-content request routing.&nbsp;</em> If the user request is for dynamic content like a newsfeed, the PoP forwards it to a datacenter region. The selection of the target region is guided by a traffic-engineering tool<a href="#B9" data-jats-ref-type="bibr" data-jats-rid="B9"><sup>9</sup></a> that periodically computes the optimal distribution of global traffic from PoPs to datacenters, considering factors such as datacenter capacity and network latency.</p><p id="p-25">PoP-to-datacenter traffic travels through Meta’s private wide-area network (WAN),<a href="#B12" data-jats-ref-type="bibr" data-jats-rid="B12"><sup>12</sup></a> which globally interconnects Meta’s PoPs and datacenters using optical fibers spanning tens of thousands of miles. Internal network traffic among our datacenters and PoPs significantly surpasses external-facing traffic between users and PoPs by several orders of magnitude, primarily due to data replication across datacenters and interactions among our microservices. The private WAN provides high bandwidth to serve this internal traffic.</p></section></section><section id="sec13"><p data-jats-content-type="inline-heading"><strong>Infrastructure topology.</strong>&nbsp; Table &nbsp;<a href="#T1" data-jats-ref-type="table" data-jats-rid="T1">1</a> &nbsp;summarizes the aforementioned infrastructure components. Globally, there are tens of datacenter regions, hundreds of edge datacenters (PoPs), and thousands of CDN sites. Each datacenter region has multiple datacenters located within the radius of a few miles. Each datacenter uses up to a dozen main switchboards (MSBs) for power distribution, which also act as the primary sub-datacenter fault domains. An MSB failure can render 10 to 20 thousand servers unavailable.</p><figure id="T1" data-jats-position="float"><div><p><span>Table 1.&nbsp;</span></p><p>Number and size of infrastructure components.</p></div><div><table data-jats-frame="hsides" data-jats-rules="rows"><colgroup> <col> <col> <col> </colgroup><thead><tr><th>Entity type</th><th>Entity count</th><th>Servers in each entity</th></tr></thead><tbody><tr><td>Region</td><td>O(10)</td><td>Up to one million</td></tr><tr><td>PoP</td><td>O(100)</td><td>Typically O(100) but up to O(1,000)</td></tr><tr><td>CDN site</td><td>O(1,000)</td><td>Typically O(10) but up to 100+</td></tr><tr><td>Datacenter</td><td>Multiple datacenters per region</td><td>O(100,000)</td></tr><tr><td>MSB</td><td>Up to a dozen MSBs per datacenter</td><td>Typically 10K to 20K</td></tr></tbody></table></div></figure><section id="sec14"><p data-jats-content-type="inline-heading"><em>Edge network.</em>&nbsp; A PoP is connected to multiple autonomous systems on the Internet and typically has multiple paths to reach a user network. When choosing a path between a PoP and a user, Border Gateway Protocol (BGP), by default, does not consider network capacity and performance. The PoP’s network, however, takes these factors into consideration and advertises its preferred route to a network prefix.<a href="#B32" data-jats-ref-type="bibr" data-jats-rid="B32"><sup>32</sup></a></p></section><section id="sec15"><p data-jats-content-type="inline-heading"><em>Datacenter network.&nbsp;</em> Servers in a datacenter are interconnected by a datacenter fabric,<a href="#B2" data-jats-ref-type="bibr" data-jats-rid="B2"><sup>2</sup></a> where network switches form a three-level Clos topology that can be scaled incrementally by adding more switches at the top level. With a sufficient number of top-level switches, the fabric can provide a non-blocking and non-oversubscribed network, enabling communication between any two servers at their full NIC bandwidth. We are moving toward eliminating network oversubscription within a datacenter.</p></section><section id="sec16"><p data-jats-content-type="inline-heading"><em>Regional network.&nbsp;</em> A fabric aggregator<a href="#B14" data-jats-ref-type="bibr" data-jats-rid="B14"><sup>14</sup></a> connects datacenters within a region and further connects them to our private WAN. The fabric aggregator employs a topology akin to the fat-tree, enabling the incremental addition of more switches to boost bandwidth. We aim to significantly reduce network oversubscription in a region so that cross-datacenter communication within a region is not a bottleneck. This allows most services, except for ML training, to be scattered across datacenters in a region without worrying about a significant performance penalty.</p></section></section><section id="sec17"><p><strong>Request processing.</strong></p><section id="sec18"><p data-jats-content-type="inline-heading"><em>Online processing.&nbsp;</em> When a user request reaches a datacenter region, it is processed along the path depicted in Figure&nbsp;<a href="#F2" data-jats-rid="F2" data-jats-ref-type="fig">2</a>. The load balancer spreads user requests across tens of thousands of servers that execute “frontend serverless functions.” To process a user request, a frontend serverless function may invoke many backend services, some of which may further call “ML inference,” for example, to retrieve recommendations for ads or newsfeed content.</p><figure id="F2" data-jats-position="float"><p><a data-fslightbox="https://cacm.acm.org/wp-content/uploads/2024/12/3701296_fig02.jpg" data-type="image" data-caption="Figure 2. High-level architecture of software components running in a datacenter region. This is a highly simplified diagram, as Meta internally has O(10,000) backend services that exhibit a complex call graph." href="https://cacm.acm.org/wp-content/uploads/2024/12/3701296_fig02.jpg">
				<img decoding="async" title="Figure 2. High-level architecture of software components running in a datacenter region. This is a highly simplified diagram, as Meta internally has O(10,000) backend services that exhibit a complex call graph." src="https://cacm.acm.org/wp-content/uploads/2024/12/3701296_fig02.jpg" alt="High-level architecture of software components running in a datacenter region. This is a highly simplified diagram, as Meta internally has O(10,000) backend services that exhibit a complex call graph." data-image-id="F2" data-image-type="figure">
			</a>
		</p><figcaption><span>Figure 2.&nbsp;</span><span>High-level architecture of software components running in a datacenter region. This is a highly simplified diagram, as Meta internally has O(10,000) backend services that exhibit a complex call graph.</span></figcaption></figure><p id="p-32">During its execution, a frontend serverless function can enqueue events in the “event queue” for “event-driven serverless functions”<a href="#B30" data-jats-ref-type="bibr" data-jats-rid="B30"><sup>30</sup></a> to process asynchronously. One such event could be sending a confirmation email after the user performs an action on the site. While frontend serverless functions directly affect user-perceived response time and hence have a tight latency service-level objective (SLO), event-driven serverless functions work asynchronously without affecting user-perceived response time, and are optimized for throughput and hardware utilization instead of latency. The ratio of servers executing frontend serverless functions to event-driven serverless functions is approximately 5:1.</p></section><section id="sec19"><p data-jats-content-type="inline-heading"><em>Offline processing.&nbsp;</em> The components on the right side of Figure <a href="#F2" data-jats-rid="F2" data-jats-ref-type="fig">2</a> perform various offline processing to assist online processing on the left side. Decoupling online and offline processing enables independent optimization based on their respective workload characteristics. When handling user requests, frontend serverless functions and backend services log various types of data, such as ad-click-through or video-watch metrics, into the “data warehouse.” This data feeds various offline processing. For instance, “ML training”<a href="#B10" data-jats-ref-type="bibr" data-jats-rid="B10"><sup>10</sup></a> uses the data to update ML models, while “stream processing” can use the data to update the most-discussed topics on the site and store them in “databases and caches,” which are then used during online user-request processing. Additionally, “batch analytics,” powered by Spark and Presto, can periodically perform operations such as updating friend recommendations in response to new activities on the site. Finally, data updates in the data warehouse serve as a primary event source that triggers the execution of event-driven serverless functions.<a href="#B30" data-jats-ref-type="bibr" data-jats-rid="B30"><sup>30</sup></a></p></section></section></section><section id="sec20"><h2>Boosting Developer Productivity</h2><p id="p-35">A main purpose of a shared infrastructure is to boost developer productivity. While it is widely recognized that continuous software deployment and serverless functions can help boost developer productivity, we have taken these approaches to the extreme.</p><section id="sec21"><p data-jats-content-type="inline-heading"><strong>Continuous deployment.</strong>&nbsp; Aligning with the move-fast culture, we take continuous deployment of both code and configuration to extreme speeds and scales. It enables developers to quickly release new features and bug fixes, receive immediate feedback, and iterate rapidly.</p><p id="p-37">For configuration changes, our configuration-management tool<a href="#B33" data-jats-ref-type="bibr" data-jats-rid="B33"><sup>33</sup></a> deploys more than 100,000 live changes daily in production, spanning O(10,000) services and millions of servers. These changes facilitate a variety of tasks, including load balancing,<a href="#B9" data-jats-ref-type="bibr" data-jats-rid="B9"><sup>9</sup></a><sup>,</sup><a href="#B31" data-jats-ref-type="bibr" data-jats-rid="B31"><sup>31</sup></a> feature rollouts, A/B tests, and overload protection.<a href="#B25" data-jats-ref-type="bibr" data-jats-rid="B25"><sup>25</sup></a> At Meta, nearly every engineer who writes code also makes live configuration changes in production. Following the configuration-as-code paradigm, manual configuration changes undergo peer code review before being committed to a code repository. Once committed, these changes immediately enter the continuous deployment pipeline. Within seconds, the updated configuration can be pushed to potentially millions of subscribed Linux processes, triggering an upcall notification. The processes can immediately adjust their runtime behavior without restarts. In addition to manual changes, automation tools also drive configuration changes, for example, for load balancing.<a href="#B9" data-jats-ref-type="bibr" data-jats-rid="B9"><sup>9</sup></a><sup>,</sup><a href="#B31" data-jats-ref-type="bibr" data-jats-rid="B31"><sup>31</sup></a></p><p id="p-38">For code changes, our deployment tool<a href="#B19" data-jats-ref-type="bibr" data-jats-rid="B19"><sup>19</sup></a> manages more than 30,000 pipelines to deploy software upgrades. At Meta, 97% of services adopt fully automated software deployments without any manual intervention: 55% utilize continuous deployment, instantly deploying every code change to production after passing automated tests, while the remaining 42% are automatically deployed on a fixed schedule, mostly daily or weekly. Take the frontend serverless functions in Figure <a href="#F2" data-jats-rid="F2" data-jats-ref-type="fig">2</a> as an example. They run on more than half a million servers, with more than 10,000 product developers changing their code and thousands of code commits every workday. Despite this extremely dynamic environment, a new version of all serverless functions is released into production every three hours.</p><p id="p-39">Even our network software is designed like regular services and optimized for frequent updates. For example, our private WAN<a href="#B12" data-jats-ref-type="bibr" data-jats-rid="B12"><sup>12</sup></a> divides its network topology into multiple parallel planes, each responsible for a portion of the traffic and equipped with its own controller. This enables frequent updates of the controller software. Developers can experiment with new control algorithms by diverting traffic from one plane and deploying the new algorithm exclusively within that plane, without affecting other planes. Similarly, our network switch software<a href="#B8" data-jats-ref-type="bibr" data-jats-rid="B8"><sup>8</sup></a> undergoes frequent updates, just like standard services. Leveraging the switch ASIC’s “warm boot” feature, the data plane keeps forwarding traffic while the switch software undergoes an update.</p><p id="p-40">Frequent code and configuration updates enable agile software development but increase the risk of site outages. To address this risk, we invest heavily in testing, staged rollouts, and health checks during updates.<a href="#B19" data-jats-ref-type="bibr" data-jats-rid="B19"><sup>19</sup></a><sup>,</sup><a href="#B33" data-jats-ref-type="bibr" data-jats-rid="B33"><sup>33</sup></a> Previously, we launched a company-wide campaign to boost code-deployment automation, increasing the adoption of fully automated code deployment guarded by health checks from 12% to 97%. Similarly, we implemented another initiative to ensure that all configuration changes undergo automated canary tests to uphold configuration safety. Overall, we find these investments in continuous deployment worthwhile, as it significantly boosts developer productivity.</p></section><section id="sec22"><p data-jats-content-type="inline-heading"><strong>Serverless functions.</strong>&nbsp; The widespread use of serverless functions (also known as function-as-a-service or FaaS) is another key driver that boosts developer productivity. Unlike traditional backend services, which can exhibit arbitrary complexity, FaaS is stateless and implements a simple function interface.<a href="#B30" data-jats-ref-type="bibr" data-jats-rid="B30"><sup>30</sup></a> Each FaaS invocation is managed independently, with no side effects on other concurrent invocations, except through states stored in external databases. Due to its stateless nature, FaaS relies heavily on external caching systems<a href="#B5" data-jats-ref-type="bibr" data-jats-rid="B5"><sup>5</sup></a><sup>,</sup><a href="#B27" data-jats-ref-type="bibr" data-jats-rid="B27"><sup>27</sup></a> to achieve good performance when accessing databases.</p><p id="p-43">Developers write FaaS code and leave it to the infrastructure to handle everything else through automation, including code deployment and auto-scaling in response to load changes. This simplicity allows Meta’s more than 10,000 product developers to focus solely on product logic without concern for infrastructure management. Moreover, it prevents hardware waste caused by product developers over-provisioning resources.</p><p id="p-44">Meta takes the usage of FaaS to the extreme to maximize developer productivity. Among O(10,000) engineers at Meta, the number of engineers writing FaaS code is about 50% greater than those writing code for regular services that they operate by themselves. This success is attributed not only to relieving product engineers of managing infrastructure but also to the usability of the integrated development environment (IDE) for FaaS. This IDE enables easy access to the social-graph database<a href="#B5" data-jats-ref-type="bibr" data-jats-rid="B5"><sup>5</sup></a> and various backend systems through high-level language constructs. It also provides fast feedback through continuous integration tests.</p><p id="p-45">As shown in Figure <a href="#F2" data-jats-rid="F2" data-jats-ref-type="fig">2</a>, Meta operates two FaaS platforms: one for “frontend serverless functions” and another for “event-driven serverless functions.” We refer to them as <i>FrontFaaS</i> and <i>XFaaS,</i><a href="#B30" data-jats-ref-type="bibr" data-jats-rid="B30"><sup>30</sup></a> respectively. FrontFaaS functions are written in PHP (we also have FaaS platforms for Python, Erlang, and Haskell functions). To support the high load generated by billions of users, we maintain over half a million servers that keep the PHP runtime running at all times. When a user request arrives, it is routed to one of these servers for immediate processing, without experiencing cold start time. When the site’s load is low, we utilize auto-scaling to release some FrontFaaS servers for other services to use.</p><p id="p-46">XFaaS shares many similarities with FrontFaaS, the key difference being that it executes non-user-facing functions that do not require sub-second response times but exhibit a highly spiky load pattern.<a href="#B30" data-jats-ref-type="bibr" data-jats-rid="B30"><sup>30</sup></a> To avoid overprovisioning resources for peak loads, XFaaS employs a combination of optimizations to spread out function execution, including deferring the execution of delay-tolerant functions to off-peak hours, globally load-balancing function calls across regions, and implementing throttling based on quotas.</p><p id="p-47">Product developers at Meta have been using FaaS as their primary coding paradigm since the late 2000s, even before the term <i>FaaS</i> became popular. Compared with serverless platforms in the industry, a unique aspect of our serverless platforms is that they allow multiple functions to execute concurrently in the same Linux process for higher hardware efficiency,<a href="#B30" data-jats-ref-type="bibr" data-jats-rid="B30"><sup>30</sup></a> unlike public clouds that have to execute one function per virtual machine in order to ensure stronger isolation between different customers.</p></section></section><section id="sec23"><h2>Reducing Hardware Costs</h2><p id="p-49">Besides boosting developer productivity, another main purpose of a shared infrastructure is to lower the cost of hardware. In this section, we highlight several examples of how software solutions help reduce hardware costs.</p><section id="sec24"><p data-jats-content-type="inline-heading"><strong>All global datacenters as a computer.</strong>&nbsp; Most infrastructures place the burden of managing the complexities of geo-distributed datacenters on users, requiring them to manually determine the number of replicas for their services and select the regions for deployment, all while ensuring that service-level objectives are met. This complexity often leads to hardware wastage due to overprovisioning, uneven load distribution across regions, and insufficient cross-region migration to adapt to changes in workload demand and datacenter supply.</p><p id="p-51">In contrast, Meta is evolving from the practice of “the datacenter as a computer”<a href="#B4" data-jats-ref-type="bibr" data-jats-rid="B4"><sup>4</sup></a> (DaaC) to the vision of “all global datacenters as a computer” (Global-DaaC).<a href="#B40" data-jats-ref-type="bibr" data-jats-rid="B40"><sup>40</sup></a> With Global-DaaC, users simply request the global deployment of a service, leaving the infrastructure to manage all the details: determining the optimal number of service replicas, placing these replicas across datacenter regions based on service-level objectives and available hardware, selecting the best-matching hardware type, optimizing traffic routing, and continuously adapting service placement in response to workload changes. Compared with public clouds, Meta can more easily realize Global-DaaC because it owns all its applications and can move them across regions as needed; public clouds lack this flexibility with their customers’ applications.</p><p id="p-52">To implement Global-DaaC, our tools seamlessly coordinate resource allocation across all levels: global, regional, and within individual servers. First, our global capacity-management tool<a href="#B13" data-jats-ref-type="bibr" data-jats-rid="B13"><sup>13</sup></a> uses RPC tracing to identify service dependencies and construct resource-consumption models, then employs mixed-integer programming to break down a service’s global capacity needs into regional quotas. Next, our regional capacity-management tool<a href="#B26" data-jats-ref-type="bibr" data-jats-rid="B26"><sup>26</sup></a> assigns server resources to these regional quotas to form virtual clusters. Unlike physical clusters, a virtual cluster can comprise servers from different datacenters in the same region, and its size may dynamically grow or shrink. During runtime, our container-management tool<a href="#B34" data-jats-ref-type="bibr" data-jats-rid="B34"><sup>34</sup></a> allocates containers in these virtual clusters, often spreading a job’s containers across multiple datacenters in the same region for improved fault tolerance. Finally, at the server level, our kernel mechanisms<a href="#B21" data-jats-ref-type="bibr" data-jats-rid="B21"><sup>21</sup></a><sup>,</sup><a href="#B37" data-jats-ref-type="bibr" data-jats-rid="B37"><sup>37</sup></a> ensure proper sharing and isolation of memory and I/O resources allocated to individual containers.</p><p id="p-53">Stateful services, such as databases, benefit from Global-DaaC. These services are typically sharded, with each container hosting multiple data shards for efficiency. Our global service placer (GSP) uses constrained optimization to determine the optimal number of replicas for each data shard and their placement across regions. Then, our sharding framework<a href="#B23" data-jats-ref-type="bibr" data-jats-rid="B23"><sup>23</sup></a> works within the constraints set by GSP to allocate shard replicas to containers and dynamically migrate them in response to load changes.</p><p id="p-54">Similarly, ML workloads benefit from Global-DaaC. For ML inference, models are managed similarly to data shards, with the number of model replicas and their locations determined by GSP. For ML training, it requires the collocation of training data and GPUs in the same datacenter region. Each team receives a global GPU capacity quota and submits training jobs to a global job queue. Our ML training scheduler<a href="#B10" data-jats-ref-type="bibr" data-jats-rid="B10"><sup>10</sup></a> automatically selects regions for data replication and GPU allocation to ensure the collocation of data and GPUs while maximizing GPU utilization.</p></section><section id="sec25"><p data-jats-content-type="inline-heading"><strong>Hardware and software co-design.</strong>&nbsp; While hardware and software co-design within a single server is common, we have elevated it to the global scale to use software solutions to overcome the limitations of lower-cost hardware.</p><section id="sec26"><p data-jats-content-type="inline-heading"><em>Low-cost fault tolerance.</em>&nbsp; Public clouds tend to provide hardware with higher availability because their customers’ applications might not be sufficiently fault tolerant. In contrast, since all our applications are under our control, we can ensure they are implemented in a fault-tolerant manner to run on cheaper hardware with lower availability guarantees. For example, a server rack in public clouds may use dual power supplies and dual top-of-rack (ToR) switches to ensure high availability and facilitate switch maintenance without disrupting running workloads. In contrast, our racks have neither dual power supplies nor dual ToR switches. Instead, hardware redundancies occur only at the much larger scope of the power main switchboards (MSBs), each covering about 10,000 to 20,000 servers. For every six MSBs, there is only one reserve MSB as a backup. Moreover, virtual machines (VMs) in public clouds often use network-attached block devices, which enable live VM migration. In contrast, our containers use low-cost, directly attached SSDs for root disks, which hinders live-container migration during datacenter maintenance operations.</p><p id="p-58">We use software solutions to overcome the limitations of lower-cost hardware. First, our resource-allocation tools<a href="#B23" data-jats-ref-type="bibr" data-jats-rid="B23"><sup>23</sup></a><sup>,</sup><a href="#B26" data-jats-ref-type="bibr" data-jats-rid="B26"><sup>26</sup></a><sup>,</sup><a href="#B34" data-jats-ref-type="bibr" data-jats-rid="B34"><sup>34</sup></a> ensure that a service’s containers and data shards are sufficiently spread across different sub-datacenter fault domains (MSBs) for better fault tolerance. Second, through a cooperative protocol that allows a service to weigh in on the lifecycle management of its containers,<a href="#B19" data-jats-ref-type="bibr" data-jats-rid="B19"><sup>19</sup></a> we ensure that maintenance operations respect application-level constraints, such as avoiding simultaneous shut-downs of two replicas of the same data shard. Finally, Global-DaaC ensures that services are deployed to withstand the simultaneous loss of an entire datacenter region, one MSB in each region, and a certain percentage of random servers in each region. We routinely conduct tests in production to ensure that these properties hold so our services are fault tolerant.<a href="#B36" data-jats-ref-type="bibr" data-jats-rid="B36"><sup>36</sup></a></p><p id="p-59">While our infrastructure is designed to withstand the loss of an entire datacenter region without affecting users, the increasing number of regions has raised the possibility of two nearby regions being simultaneously affected by a large-scale natural disaster, such as a hurricane. Instead of over-provisioning capacity to tolerate the simultaneous loss of two regions, we employ a software-based approach<a href="#B25" data-jats-ref-type="bibr" data-jats-rid="B25"><sup>25</sup></a> that, in the event of losing multiple regions, deactivates less-critical product features and gracefully degrades service quality, such as delivering lower-quality videos, to reduce the load.</p></section><section id="sec27"><p data-jats-content-type="inline-heading"><em>Eliminating the costs of routing proxies.</em>&nbsp; Unlike traditional service meshes that predominantly use sidecar proxies to route RPC requests, Meta’s service mesh<a href="#B31" data-jats-ref-type="bibr" data-jats-rid="B31"><sup>31</sup></a> uses proxies to route only 1% of RPC requests across our fleet. The remaining 99% use a routing library linked into service executables for direct client-to-server routing, bypassing intermediate proxies. While this unconventional approach saves us O(100,000) servers needed for proxies, it introduces deployment challenges due to the library being compiled into around O(10,000) services, each with its own deployment schedule. Our software deployment and configuration-management tools<a href="#B19" data-jats-ref-type="bibr" data-jats-rid="B19"><sup>19</sup></a><sup>,</sup><a href="#B33" data-jats-ref-type="bibr" data-jats-rid="B33"><sup>33</sup></a> help make these challenges manageable.</p></section><section id="sec28"><p data-jats-content-type="inline-heading"><em>Tiered storage and local SSDs.&nbsp;</em> Based on access frequency and latency tolerance, we categorize data as hot, warm, or cold, with each category using a different storage system to optimize cost-effectiveness. Hot databases and caches, such as the social graph database,<a href="#B5" data-jats-ref-type="bibr" data-jats-rid="B5"><sup>5</sup></a> store data in memory and solid state drives (SSDs).</p><p id="p-62">Warm data, including videos, images, and data in the data warehouse (for example, user activity logs), is stored in a distributed file system<a href="#B29" data-jats-ref-type="bibr" data-jats-rid="B29"><sup>29</sup></a> that utilizes hard disk drives (HDDs) to store data. Each storage server is equipped with one CPU, 36 HDDs, and two SSDs for metadata cache.</p><p id="p-63">For rarely accessed cold data, such as a decade-old high-resolution video, we archive them with high-density HDD servers, each with one CPU and 216 HDDs, which provides a good balance between total cost of ownership and data-restoration speed. These HDDs are powered off most of the time, as they are not in active use.</p><p id="p-64">Among workloads that store data on SSDs, some can tolerate longer-tail latencies and opt for SSD-based shared remote storage for better SSD utilization. However, workloads with strict latency requirements still use directly attached local SSDs. Compared with other hyperscale infrastructures, we more frequently employ local SSDs to reduce costs, despite the management complexities involved. For instance, imbalanced load distribution can lead to underutilization and stranding of local SSDs. Additionally, failure recovery is complicated by data becoming trapped in the SSDs of failed servers. To address these challenges, we use our common sharding framework<a href="#B23" data-jats-ref-type="bibr" data-jats-rid="B23"><sup>23</sup></a> to implement stateful services with local SSDs, solving the issues once and reusing the solution across many services.</p></section></section><section id="sec29"><p data-jats-content-type="inline-heading"><strong>In-house hardware design.</strong>&nbsp; We design our own datacenters<a href="#B17" data-jats-ref-type="bibr" data-jats-rid="B17"><sup>17</sup></a> and hardware—servers, network switches, video accelerators, and AI chips<a href="#B15" data-jats-ref-type="bibr" data-jats-rid="B15"><sup>15</sup></a>—for better costs and power efficiency. In datacenters, power is the most constrained resource because it is fixed at the time of datacenter construction and hard to expand later during a datacenter’s 20-to-30-year lifespan. In contrast, the network and servers can be upgraded as needed. Power in a datacenter is often oversubscribed. To prevent over-drawing power when workloads surge, an automation tool<a href="#B38" data-jats-ref-type="bibr" data-jats-rid="B38"><sup>38</sup></a> coordinates power-capping actions across the power-delivery hierarchy.</p><p id="p-67">Our hardware designs often achieve cost and power savings through hardware/software co-design (for example, optimizing SRAM usage in our AI chip based on our workloads<a href="#B15" data-jats-ref-type="bibr" data-jats-rid="B15"><sup>15</sup></a>), and by removing components unnecessary to us (for example, eliminating compressor-cooled air conditioning<a href="#B17" data-jats-ref-type="bibr" data-jats-rid="B17"><sup>17</sup></a>). Additionally, in-house development of network switches and their companion software<a href="#B8" data-jats-ref-type="bibr" data-jats-rid="B8"><sup>8</sup></a> enables us to treat switch software like a regular service and deploy updates frequently. Most of our hardware designs are open source through the Open Compute Project.<a href="#B28" data-jats-ref-type="bibr" data-jats-rid="B28"><sup>28</sup></a></p></section></section><section id="sec30"><h2>Designing Scalable Systems</h2><p id="p-69">A recurring theme in hyperscale infrastructure is the design of scalable systems. Decentralized systems designed for the Internet environment, such as BGP, BitTorrent, and distributed hash tables (DHTs), are often lauded for their scalability. However, in a datacenter environment, which is less resource constrained and under the control of a single organization, our experiences indicate that centralized controllers not only achieve ample scalability but also are simpler and can make higher-quality decisions.</p><section id="sec31"><p data-jats-content-type="inline-heading"><strong>Deprecating decentralized controllers.</strong>&nbsp; In this section, we discuss several examples of the trade-off between centralized and decentralized controllers. For network switches in our datacenter fabric, although they still use BGP for compatibility, the fabric has a centralized controller capable of overriding routing paths during network congestion or link failures.<a href="#B1" data-jats-ref-type="bibr" data-jats-rid="B1"><sup>1</sup></a></p><p id="p-71">Except for BGP, we have migrated almost all decentralized controllers to centralized ones. For example, in our private WAN,<a href="#B12" data-jats-ref-type="bibr" data-jats-rid="B12"><sup>12</sup></a> we transitioned from decentralized RSVP-TE to a centralized controller to compute preferred traffic paths and proactively establish backup paths for common failure scenarios. This has resulted in more efficient network resource usage and faster convergence during network failures.</p><p id="p-72">For key-value stores, DHTs use multi-hop routing to determine the server responsible for a given key, while Cassandra uses consistent hashing for this purpose. Both function without a central controller. In contrast, to achieve better load balance, our sharding framework<a href="#B23" data-jats-ref-type="bibr" data-jats-rid="B23"><sup>23</sup></a> uses a central controller to dynamically reassign key-encapsulating shards to servers.</p><p id="p-73">For bulk-data distribution, we transitioned from BitTorrent to Owl,<a href="#B16" data-jats-ref-type="bibr" data-jats-rid="B16"><sup>16</sup></a> which centralizes the decision of where a peer should fetch data, resulting in significantly faster download speeds. Note that both Owl and our private WAN<a href="#B12" data-jats-ref-type="bibr" data-jats-rid="B12"><sup>12</sup></a> centralize the control plane for better decision making but still use a decentralized data plane for actual data forwarding or downloading.</p><p id="p-74">For small-metadata distribution (further explained in Figure <a href="#F4" data-jats-rid="F4" data-jats-ref-type="fig">4</a>), we initially used a three-level distribution tree implemented in Java. The tree’s intermediate nodes were dedicated proxy servers, and its leaf nodes were application subscribers that could dynamically join and leave. When this implementation could not scale further, we transitioned to a peer-to-peer distribution tree, where intermediate nodes were also application subscribers that forwarded data to other subscribers. Among millions of application subscribers, however, a subset often experienced noisy performance issues due to their non-dedicated nature. Consequently, using them as intermediate nodes to forward traffic was less reliable, leading to frequent and time-consuming debugging. Eventually, after a few years of production use, we abandoned the peer-to-peer distribution tree and reverted to the original architecture that uses dedicated proxy servers. We replaced the original Java implementation with a more performant C++ implementation, which scaled well to tens of millions of subscribers.</p></section><section id="sec32"><p data-jats-content-type="inline-heading"><strong>Case study: Scalable service mesh.</strong>&nbsp; In this section, we use Meta’s service mesh, ServiceRouter,<a href="#B31" data-jats-ref-type="bibr" data-jats-rid="B31"><sup>31</sup></a> as a case study to illustrate the design of scalable systems and demonstrate that centralized controllers combined with a decentralized data plane can scale well in a datacenter environment. ServiceRouter routes billions of RPCs per second across millions of layer-7 (L7, that is, application layer) routers.</p><p id="p-77">Figure <a href="#F3" data-jats-ref-type="fig" data-jats-rid="F3">3</a> depicts a commonly used service mesh in the industry, where each service process is accompanied by an L7 sidecar proxy that routes RPCs for the service. For example, when service A on server 1 sends requests to service B, the proxy on server 1 load balances them across servers 2, 3, and 4. While this solution is widely adopted, it is not scalable for hyperscale infrastructure because the central controller cannot scale to directly configure the routing tables of millions of sidecar proxies. The central controller has a dual function of generating global routing metadata and managing each L7 router. To scale out, we keep the former in the central controller but transfer the latter to L7 routers, making each L7 router self-configuring and self-managing.</p><figure id="F3" data-jats-position="float"><p><a data-fslightbox="https://cacm.acm.org/wp-content/uploads/2024/12/3701296_fig03.jpg" data-type="image" data-caption="Figure 3. Sidecar-proxy-based service mesh." href="https://cacm.acm.org/wp-content/uploads/2024/12/3701296_fig03.jpg">
				<img decoding="async" title="Figure 3. Sidecar-proxy-based service mesh." src="https://cacm.acm.org/wp-content/uploads/2024/12/3701296_fig03.jpg" alt="Sidecar-proxy-based service mesh." data-image-id="F3" data-image-type="figure">
			</a>
		</p><figcaption><span>Figure 3.&nbsp;</span> <span>Sidecar-proxy-based service mesh.</span></figcaption></figure><p id="p-78">Figure <a href="#F4" data-jats-rid="F4" data-jats-ref-type="fig">4</a> illustrates the scalable architecture of ServiceRouter. At the top, different controllers independently execute distinct functions such as registering services, updating measured network latencies, and computing a per-service cross-region routing table. Each controller independently updates the central routing information base (RIB) and is not concerned with configuring or managing individual L7 routers. The RIB is a Paxos-based database<a href="#B3" data-jats-ref-type="bibr" data-jats-rid="B3"><sup>3</sup></a> and can scale out through sharding. With the help of the RIB, the controllers become stateless and can easily scale out through sharding as well. For example, multiple controller instances can concurrently compute cross-region routing tables for different services.</p><figure id="F4" data-jats-position="float"><p><a data-fslightbox="https://cacm.acm.org/wp-content/uploads/2024/12/3701296_fig04.jpg" data-type="image" data-caption="Figure 4. ServiceRouter’s scalable service-mesh architecture." href="https://cacm.acm.org/wp-content/uploads/2024/12/3701296_fig04.jpg">
				<img decoding="async" title="Figure 4. ServiceRouter’s scalable service-mesh architecture." src="https://cacm.acm.org/wp-content/uploads/2024/12/3701296_fig04.jpg" alt="ServiceRouter’s scalable service-mesh architecture." data-image-id="F4" data-image-type="figure">
			</a>
		</p><figcaption><span>Figure 4.&nbsp;</span> <span>ServiceRouter’s scalable service-mesh architecture.</span></figcaption></figure><p id="p-79">In the middle of Figure&nbsp;<a href="#F4" data-jats-rid="F4" data-jats-ref-type="fig">4</a>, the distribution layer leverages thousands of RIB replicas to handle read traffic from millions of L7 routers. At the bottom, guided by the RIB, each L7 router self-configures without the direct involvement of the control plane. Heterogeneous L7 routers are supported, which can be load balancers, services with embedded routing libraries, or sidecar proxies.</p><p id="p-80">As ServiceRouter shows, we can achieve good scalability with centralized controllers through techniques like stateless controllers, controller sharding, and removing non-essential functions, such as managing individual L7 routers, from central controllers.</p></section></section><section id="sec33"><h2>Future Directions</h2><p id="p-81">Despite the complexity of Meta’s hyperscale infrastructure, here we provided a concise, high-level overview, emphasizing key insights from its development. To conclude, we share our thoughts on potential future trends for hyperscale infrastructure.</p><section id="sec34"><p data-jats-content-type="inline-heading"><strong>AI.</strong>&nbsp; AI workloads have become the single largest category of workload in datacenters. We anticipate that, before the end of this decade, more than half of the power in datacenters will be dedicated to AI workloads. Due to its distinct characteristics, such as being more resource-intensive and requiring higher-bandwidth networks, AI is expected to profoundly reshape every aspect of infrastructure. In the past two decades, hyperscale infrastructures have succeeded mostly by taking the <i>scaling-out</i> approach to utilize a large number of low-cost commodity servers. Future AI clusters, however, will more likely take the <i>scale-up</i> approach used by past supercomputers, such as using remote direct memory access (RDMA) over Ethernet to provide the high-bandwidth, low-latency network required for large-scale ML training.<a href="#B18" data-jats-ref-type="bibr" data-jats-rid="B18"><sup>18</sup></a> Meta’s approach to AI is distinguished by co-designing the full stack, from PyTorch to ML models, AI chips, networks, datacenters, servers, storage, power, and cooling.</p></section><section id="sec35"><p data-jats-content-type="inline-heading"><strong>Domain-specific hardware.</strong>&nbsp; Reversing the trend of diminishing hardware diversity in the 2000s, we anticipate a proliferation of custom and specialized hardware for various purposes, such as AI training and inference, virtualization, video encoding, encryption, compression, tiered memory, as well as in-network and in-storage processing. This is because economies of scale allow hyperscalers to design and deploy specialized hardware in large quantities to reduce costs. Consequently, this will pose challenges for the software stack in utilizing and managing a highly heterogeneous fleet.</p></section><section id="sec36"><p data-jats-content-type="inline-heading"><strong>Edge datacenters.</strong>&nbsp; We expect a substantial increase in metaverse and Internet of Things (IoT) applications. Cloud gaming, for instance, shifts graphics rendering from user devices to GPU servers in edge datacenters, necessitating less than 25ms network latency. The demand for real-time responsiveness will likely drive considerable growth in both the quantity and size of edge datacenters. As a result, the infrastructure control plane needs to adapt to managing a more dispersed fleet, ideally by enhancing Global-DaaC to shield application developers from the complexity of a dispersed infrastructure.</p></section><section id="sec37"><p data-jats-content-type="inline-heading"><strong>Developer productivity.</strong>&nbsp; Over the past two decades, automation tools have significantly boosted the productivity of system administrators, resulting in a considerably higher server-to-administrator ratio. In contrast, general software development remains labor-intensive, with comparatively slower productivity growth. In this decade, we anticipate a shift in this trend, with developer productivity increasing rapidly for two reasons: AI-powered code generation and debugging, and fully integrated serverless programming paradigms in vertical domains. Meta’s FrontFaaS is an example of the latter, and we anticipate the emergence of highly productive programming paradigms for more vertical domains.</p><p id="p-86">We anticipate that the rapid innovation in hyperscale infrastructure seen over the past two decades will continue into the next decade, driven especially by advancements in AI. We encourage hyperscalers to share their insights, enabling the community to collectively accelerate progress.</p></section></section><section id="sec38"><h2>Acknowledgments</h2><p id="p-87">This article summarizes the work done by thousands of Meta infrastructure engineers over a time span of more than a decade. While the author contributed to some systems described in this article,<a href="#B7" data-jats-ref-type="bibr" data-jats-rid="B7"><sup>7</sup></a><sup>,</sup><a href="#B10" data-jats-ref-type="bibr" data-jats-rid="B10"><sup>10</sup></a><sup>,</sup><a href="#B11" data-jats-ref-type="bibr" data-jats-rid="B11"><sup>11</sup></a><sup>,</sup><a href="#B13" data-jats-ref-type="bibr" data-jats-rid="B13"><sup>13</sup></a><sup>,</sup><a href="#B19" data-jats-ref-type="bibr" data-jats-rid="B19"><sup>19</sup></a><sup>–</sup><a href="#B24" data-jats-ref-type="bibr" data-jats-rid="B24"><sup>24</sup></a><sup>,</sup><a href="#B26" data-jats-ref-type="bibr" data-jats-rid="B26"><sup>26</sup></a><sup>,</sup><a href="#B30" data-jats-ref-type="bibr" data-jats-rid="B30"><sup>30</sup></a><sup>,</sup><a href="#B31" data-jats-ref-type="bibr" data-jats-rid="B31"><sup>31</sup></a><sup>,</sup><a href="#B33" data-jats-ref-type="bibr" data-jats-rid="B33"><sup>33</sup></a><sup>,</sup><a href="#B34" data-jats-ref-type="bibr" data-jats-rid="B34"><sup>34</sup></a><sup>,</sup><a href="#B37" data-jats-ref-type="bibr" data-jats-rid="B37"><sup>37</sup></a><sup>,</sup><a href="#B39" data-jats-ref-type="bibr" data-jats-rid="B39"><sup>39</sup></a><sup>,</sup><a href="#B40" data-jats-ref-type="bibr" data-jats-rid="B40"><sup>40</sup></a> there are also many systems that the author did not work on directly.</p></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nvidia's RTX 5090 power connectors are melting (284 pts)]]></title>
            <link>https://www.theverge.com/news/609207/nvidia-rtx-5090-power-connector-melting-burning-issues</link>
            <guid>43008879</guid>
            <pubDate>Tue, 11 Feb 2025 04:13:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/news/609207/nvidia-rtx-5090-power-connector-melting-burning-issues">https://www.theverge.com/news/609207/nvidia-rtx-5090-power-connector-melting-burning-issues</a>, See on <a href="https://news.ycombinator.com/item?id=43008879">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="zephr-anchor"><p>Ah shit, here we go again. Two owners of Nvidia’s new RTX 5090 Founders Edition GPUs have reported melted power connectors and damage to their PSUs. The images look identical to reports of <a href="https://www.theverge.com/2022/10/25/23422349/nvidia-rtx-4090-power-cables-connectors-melting-burning">RTX 4090 power cables burning</a> or melting from two years ago. <a href="https://www.theverge.com/2022/11/18/23466974/nvidia-rtx-4090-power-cable-12vhpwr-melt-burn-plugged-in">Nvidia blamed</a> the issue on people not properly plugging the 12VHPWR power connection in fully and the PCI standards body <a href="https://www.theverge.com/2022/12/1/23488276/nvidia-12vhpwr-cable-16-pin-pci-sig-response">blamed Nvidia</a>.</p><p><a href="https://www.reddit.com/r/nvidia/comments/1ilhfk0/rtx_5090fe_molten_12vhpwr/">A Reddit poster</a> upgraded from an RTX 4090 to an RTX 5090 and noticed “a burning smell playing <em>Battlefield 5</em>,” before turning off their PC and finding the damage. The images show burnt plastic at both the PSU end of the power connector and the part that connects directly to the GPU. The cable is one from <a href="https://www.moddiy.com/products/ATX-3.0-PCIe-5.0-600W-12VHPWR-16-Pin-to-16-Pin-PCIE-Gen-5-Power-Cable.html">MODDIY</a>, a popular manufacturer of custom cables, and the poster claims it was “securely fastened and clicked on both sides (GPU and PSU).”</p><div><p><a href="https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/rtx-5090fe-molten-12vhpwr-v0-26w.jpg?quality=90&amp;strip=all&amp;crop=0,5.5555555555556,100,88.888888888889" data-pswp-height="720" data-pswp-width="1080" target="_blank" rel="noreferrer"><img alt="The melted power connector and damaged RTX 5090 Founders Edition." data-chromatic="ignore" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 639px) 100vw, (max-width: 1023px) 50vw, 700px" srcset="https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/rtx-5090fe-molten-12vhpwr-v0-26w.jpg?quality=90&amp;strip=all&amp;crop=0%2C5.5555555555556%2C100%2C88.888888888889&amp;w=256 256w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/rtx-5090fe-molten-12vhpwr-v0-26w.jpg?quality=90&amp;strip=all&amp;crop=0%2C5.5555555555556%2C100%2C88.888888888889&amp;w=376 376w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/rtx-5090fe-molten-12vhpwr-v0-26w.jpg?quality=90&amp;strip=all&amp;crop=0%2C5.5555555555556%2C100%2C88.888888888889&amp;w=384 384w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/rtx-5090fe-molten-12vhpwr-v0-26w.jpg?quality=90&amp;strip=all&amp;crop=0%2C5.5555555555556%2C100%2C88.888888888889&amp;w=415 415w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/rtx-5090fe-molten-12vhpwr-v0-26w.jpg?quality=90&amp;strip=all&amp;crop=0%2C5.5555555555556%2C100%2C88.888888888889&amp;w=480 480w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/rtx-5090fe-molten-12vhpwr-v0-26w.jpg?quality=90&amp;strip=all&amp;crop=0%2C5.5555555555556%2C100%2C88.888888888889&amp;w=540 540w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/rtx-5090fe-molten-12vhpwr-v0-26w.jpg?quality=90&amp;strip=all&amp;crop=0%2C5.5555555555556%2C100%2C88.888888888889&amp;w=640 640w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/rtx-5090fe-molten-12vhpwr-v0-26w.jpg?quality=90&amp;strip=all&amp;crop=0%2C5.5555555555556%2C100%2C88.888888888889&amp;w=750 750w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/rtx-5090fe-molten-12vhpwr-v0-26w.jpg?quality=90&amp;strip=all&amp;crop=0%2C5.5555555555556%2C100%2C88.888888888889&amp;w=828 828w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/rtx-5090fe-molten-12vhpwr-v0-26w.jpg?quality=90&amp;strip=all&amp;crop=0%2C5.5555555555556%2C100%2C88.888888888889&amp;w=1080 1080w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/rtx-5090fe-molten-12vhpwr-v0-26w.jpg?quality=90&amp;strip=all&amp;crop=0%2C5.5555555555556%2C100%2C88.888888888889&amp;w=1200 1200w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/rtx-5090fe-molten-12vhpwr-v0-26w.jpg?quality=90&amp;strip=all&amp;crop=0%2C5.5555555555556%2C100%2C88.888888888889&amp;w=1440 1440w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/rtx-5090fe-molten-12vhpwr-v0-26w.jpg?quality=90&amp;strip=all&amp;crop=0%2C5.5555555555556%2C100%2C88.888888888889&amp;w=1920 1920w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/rtx-5090fe-molten-12vhpwr-v0-26w.jpg?quality=90&amp;strip=all&amp;crop=0%2C5.5555555555556%2C100%2C88.888888888889&amp;w=2048 2048w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/rtx-5090fe-molten-12vhpwr-v0-26w.jpg?quality=90&amp;strip=all&amp;crop=0%2C5.5555555555556%2C100%2C88.888888888889&amp;w=2400 2400w" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/rtx-5090fe-molten-12vhpwr-v0-26w.jpg?quality=90&amp;strip=all&amp;crop=0%2C5.5555555555556%2C100%2C88.888888888889&amp;w=2400"></a></p></div><p>While it’s tempting to blame the MODDIY cable, Spanish <a href="https://www.youtube.com/watch?v=Nw7HaVRUN9k">YouTuber Toro Tocho</a> has experienced the same burnt cable (both at the GPU and PSU ends) with an RTX 5090 Founders Edition while using a cable supplied by PSU manufacturer FSP. Plastic has also melted into the PCIe 5.0 power connector on the power supply. MODDIY also <a href="https://www.reddit.com/r/GamersNexus/comments/1ilgii6/comment/mc0t80d/">responded in a Reddit thread</a>, ruling out the “possibility of a defective cable or manufacturing error” and offering to cover the cost of repair if Nvidia and Asus don’t honor their warranties.</p><p><a href="https://www.youtube.com/watch?v=Ndmoi1s0ZaY">YouTuber der8auer</a> has also examined the Reddit poster’s equipment in person, and ruled out any form of user error in the process. He’s also found that this could be related to a current distribution problem with RTX 5090 Founders Edition models instead. Either way, nobody should be blaming 12VHPWR on end users.</p><p>Nvidia originally introduced the 12VHPWR power connector on its RTX 40-series GPUs, and power supplies also debuted to support the new standard. The RTX 4090 Founders Edition was able to draw 450 watts over the 12VHPWR connector, while the new RTX 5090 draws up to 575 watts over a cable that’s rated up to 600 watts. After early issues with RTX 4090 connectors melting, PCI-SIG, the standards organization responsible for the 12VHPWR connector, has now updated it to a new 12V-2x6 connector on the GPU side and in some cases the PSU side, too.</p><p>The 12V-2x6 connector has shorter sensing pins and longer conductor terminals, to improve reliability. “This might not sound like a huge difference, but it matters in ensuring that the power cable has been properly connected to whatever device is going to be pulling power from your system’s power supply,” <a href="https://go.corsair.com/c/482924/490888/8513?u=https%3A%2F%2Fwww.corsair.com%2Fuk%2Fen%2Fexplorer%2Fdiy-builder%2Fpower-supply-units%2Fevolving-standards-12vhpwr-and-12v-2x6%2F%3F" rel="sponsored">explains Corsair</a>.</p><p>Nvidia uses the 12V-2x6 connector on its RTX 50-series GPUs, but you can still use existing 12VHPWR cables. “To be clear, this is not a new cable, it is an updated change to the pins in the socket, which is referred to as 12V-2x6,” says Corsair. PSU manufacturers like Corsair and MSI have adopted colored pins on their 12VHPWR cables so that if you can still see the yellow or grey pins it means the connector isn’t seated properly.</p><div><p><a href="https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/rtx5090burnedcable.jpg?quality=90&amp;strip=all&amp;crop=0,0,100,100" data-pswp-height="864" data-pswp-width="1600" target="_blank" rel="noreferrer"><img alt="Damage to a PSU and the 12VHPWR&nbsp;power connector." data-chromatic="ignore" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 639px) 100vw, (max-width: 1023px) 50vw, 700px" srcset="https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/rtx5090burnedcable.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=256 256w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/rtx5090burnedcable.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=376 376w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/rtx5090burnedcable.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=384 384w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/rtx5090burnedcable.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=415 415w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/rtx5090burnedcable.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=480 480w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/rtx5090burnedcable.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=540 540w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/rtx5090burnedcable.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=640 640w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/rtx5090burnedcable.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=750 750w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/rtx5090burnedcable.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=828 828w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/rtx5090burnedcable.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1080 1080w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/rtx5090burnedcable.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1200 1200w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/rtx5090burnedcable.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1440 1440w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/rtx5090burnedcable.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1920 1920w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/rtx5090burnedcable.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=2048 2048w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/rtx5090burnedcable.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=2400 2400w" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/rtx5090burnedcable.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=2400"></a></p></div><p>While Intel and AMD are both members of the PCI-SIG group that helped develop the 12VHPWR power connector, only Nvidia has adopted the standard so far for consumer GPUs. Even AMD’s upcoming <a href="https://www.theverge.com/2025/1/6/24336246/amd-radeon-rx-9070-series-rdna-4-fsr-4-ces-2025">Radeon RX 9070-series</a> are using existing 8-pin PCIe connections instead. AMD even suggested the 12VHPWR connector was a fire hazard in late 2022, when the company’s gaming marketing director Sasa Marinkovic <a href="https://x.com/SasaMarinkovic/status/1593243804538372096?">tweeted</a> “Stay safe this holiday season” alongside a picture of 8-pin connectors.</p><p>12VHPWR has been <a href="https://www.youtube.com/watch?v=Y36LMS5y34A">branded a “dumpster fire,</a>” thanks to design oversights that make it relatively easy for end users to not properly connect the cable securely. Cablemod was also <a href="https://www.theverge.com/2024/2/8/24029465/after-74500-in-damage-cablemod-is-fully-recalling-its-nvidia-12vhpwr-gpu-power-adapters">forced to recall</a> its 12VHPWR GPU power adapters last year after reports of melted adapters.</p><p>We reached out to Nvidia to comment on these latest reports of RTX 5090 power connector issues, but the company refused to comment.</p><p><em><strong>Update, February 11th</strong>: Article updated with comment from MODDIY and a new discovery from YouTuber der8auer.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple software update “bug” enables Apple Intelligence (117 pts)]]></title>
            <link>https://lapcatsoftware.com/articles/2025/2/3.html</link>
            <guid>43008422</guid>
            <pubDate>Tue, 11 Feb 2025 02:55:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lapcatsoftware.com/articles/2025/2/3.html">https://lapcatsoftware.com/articles/2025/2/3.html</a>, See on <a href="https://news.ycombinator.com/item?id=43008422">Hacker News</a></p>
Couldn't get https://lapcatsoftware.com/articles/2025/2/3.html: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Make Your Own Website: A beginner's guide (180 pts)]]></title>
            <link>https://web.pixelshannon.com/make/</link>
            <guid>43008315</guid>
            <pubDate>Tue, 11 Feb 2025 02:37:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://web.pixelshannon.com/make/">https://web.pixelshannon.com/make/</a>, See on <a href="https://news.ycombinator.com/item?id=43008315">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

    <h2>Getting Started</h2>
        <p>This is not a complete <a href="https://developer.mozilla.org/en-US/docs/Web/HTML">HTML</a>/<a href="https://developer.mozilla.org/en-US/docs/Web/CSS">CSS</a> tutorial, but will attempt to guide you through making your first page using HTML and CSS. If you have questions or comments, you can <a href="mailto:reply25@pixelshannon.com?subject=%5Breply%20to%5D%20Make%20Your%20Own%20Website%22">reply by email</a>, <a href="https://bookstodon.com/@shannonkay/113981703308638892">reply on Mastodon</a>, or <a href="https://bsky.app/profile/shannonkay.com/post/3lhu5ebhrvk2l">reply on Bluesky</a>.</p>
    <blockquote>
        <strong>Personal note:</strong> I originally wrote this guide for my daughter, then 12 years old, to learn how to make a website for a class project. For that reason, I included a mix of things I thought she needed to know and things I thought she would want to use. She created her website at school, without my direct assistance. <a href="https://web.pixelshannon.com/make/ballet/index.html">Here's a copy of the website that she made using this guide</a>.
    </blockquote>
<nav id="web">
    <a href="#software">Software</a> 
    <a href="#make-a-folder">Make a Folder</a> 
    <a href="#html-page-structure">HTML Page Structure</a> 
    <a href="#page-content-in-html">Page Content in HTML</a> 
    <a href="#sections">Sections</a> 
    <a href="#linking">Linking</a> 
    <a href="#images">Images</a> 
    <a href="#create-a-stylesheet-with-css">Create a Stylesheet With CSS</a> 
    <a href="#link-your-stylesheet">Link Your Stylesheet</a>
    <a href="#stylish-bonus">Stylish Bonus</a> 
    <a href="#more-style">More Style</a>
    <a href="#more-pages">More Pages</a>
    <a href="#lists">Lists</a>
    <a href="#ids-and-classes">IDs and Classes</a>
    <a href="#styling-boxes-and-borders">Styling Boxes and Borders</a>
    <a href="#photos">Photos</a>
    <a href="#using-emoji">Using Emoji</a> 
    <a href="#examples">Examples</a> 
    <a href="#host-and-publish-your-website-on-the-internet">Publish</a>
    <a href="#more-resources">More Resources</a>

</nav>

<section id="example">
<figure><a href="https://web.pixelshannon.com/make/lesson/index.html"><img src="https://web.pixelshannon.com/make/examplepage.png" alt="A screenshot of the example website. The title of the website is I Love Cats and it has a light pink background with hot pink text and a photo of a cat"></a>
<figcaption><a href="https://web.pixelshannon.com/make/lesson/index.html">Here's an example website</a> that you can make with this guide.</figcaption>
</figure>
</section>

        <h3 id="software">Software</h3>
        <p>You will need a plain text or code editor. I use <a href="https://vscodium.com/">VSCodium</a> or <a href="https://code.visualstudio.com/">Visual Studio Code</a>(it also has a <a href="https://vscode.dev/">web-based version</a>). Another option is <a href="https://phcode.io/">Phoenix Code</a>(also has a desktop and web version). You can also use a plain text editor like Notepad or <a href="https://support.apple.com/guide/textedit/work-with-html-documents-txted0b6cd61/mac">TextEdit</a>.</p>
        <hr>
        <h3 id="make-a-folder">Make a Folder</h3>
        <p>On your computer, create a folder that you will put all the files for this website into. I like to have a "Websites" folder with subfolders for each of my different sites. You can name the folder whatever you want, like "homepage" or "cats".</p>
        <p><strong>Open the folder in your code editor, or open your text editor. Create a new file(or save your text document) and name it <em>index.html</em></strong></p>
        
        <details>
        <summary>Why is the file named "index.html"?</summary>
        In websites, the "index" page is the default page that will be shown if someone browses to the root of a website address or folder, so whatever you want people to see first should be called "index". If you want to make a page with an address like shannonkay.com/books without having to see a "page.html" file name, you need to make an "index" page inside the "books" folder. Our file has a .html ending because we're putting html code in it. 
        </details>
        
    <hr>
    <h3 id="html-page-structure">HTML Page Structure</h3>
    <p>Most HTML tags have an opening and a closing tag. The first one we need is the <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/html"><code>&lt;html&gt;</code></a> tag. All the rest of our HTML will go between these opening and closing tags.</p>
    <pre><code>
    &lt;html&gt;
    &lt;/html&gt;</code></pre>

    <p>There's one more thing we need to put before our opening <code>&lt;html&gt;</code> tag, and it's the <a href="https://developer.mozilla.org/en-US/docs/Glossary/Doctype"><code>!DOCTYPE</code></a> declaration. It tells the browser what type of document to expect. This is the !DOCTYPE declaration for HTML 5.</p>
    <pre><code>
    &lt;!DOCTYPE html&gt;
        &lt;html&gt;
        &lt;/html&gt;</code></pre>

    <p>After the <code>&lt;html&gt;</code> tag, there are two important tags that further divide our HTML document, <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/head"><code>&lt;head&gt;</code></a> and <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/body"><code>&lt;body&gt;</code></a>. The <code>&lt;head&gt;</code> tag contains important information that doesn't show on the page. The <code>&lt;body&gt;</code> tag contains all the content of the page.</p>
    
    <pre><code>
    &lt;!DOCTYPE html&gt;
    &lt;html&gt;
    
        &lt;head&gt;&lt;/head&gt;
        &lt;body&gt;&lt;/body&gt;
    
    &lt;/html&gt;</code></pre>

    <p id="head-section"><a href="#head-section">In the head section</a>, we will put the <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/title"><code>&lt;title&gt;</code></a> tag. The title that goes in this tag will not show up on the page, but it will show up in the browser tab and be the title for bookmarks, sharing, etc. Don't add any other HTML tags in the title.</p>
    <pre><code>
    &lt;!DOCTYPE html&gt;
    &lt;html&gt;
    
        &lt;head&gt;
        &lt;title&gt;I Love Cats&lt;/title&gt;
        &lt;/head&gt;
    
    &lt;body&gt;&lt;/body&gt;
    
    &lt;/html&gt;</code></pre>

    <p id="viewport-meta">For now, we'll just add one more thing to the head section. In these modern times, people view websites on a wide variety of device sizes, including smart phones. <a href="#viewport-meta">The</a> <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Viewport_meta_tag"><code>viewport &lt;meta&gt; tag</code></a> will help it resize properly on those devices. </p>
    <pre><code>
    &lt;!DOCTYPE html&gt;
    &lt;html&gt;
    
        &lt;head&gt;
        &lt;title&gt;I Love Cats&lt;/title&gt;
        &lt;meta name="viewport" content="width=device-width, initial-scale=1.0"&gt;
        &lt;/head&gt;
    
    &lt;body&gt;&lt;/body&gt;
    
    &lt;/html&gt;</code></pre>
<hr>
    <h3 id="page-content-in-html">Page Content in HTML</h3>
    <p>Let's start adding content to the body section of the page with a header. We'll start with a <a href="https://developer.mozilla.org/en-US/docs/Glossary/Semantics#semantic_elements">semantic</a> <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/header"><code>&lt;header&gt;</code></a> tag, and our page title will go inside it. I'm going to use the same text as my title for this page, <em>I Love Cats</em>. Put the header text between <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/Heading_Elements"><code>&lt;h1&gt;</code></a> tags. The <code>&lt;h1&gt;</code> tag means that this is the top-level header, usually what's at the very top of the page. </p>
    <pre><code>
    &lt;!DOCTYPE html&gt;
    &lt;html&gt;
    
        &lt;head&gt;
        &lt;title&gt;I Love Cats&lt;/title&gt;
        &lt;meta name="viewport" content="width=device-width, initial-scale=1.0"&gt;
        &lt;/head&gt;
    
    &lt;body&gt;
        &lt;header&gt;&lt;h1&gt;I Love Cats&lt;/h1&gt;&lt;/header&gt;
    &lt;/body&gt;
    
    &lt;/html&gt;</code></pre>

    <p>Let's use another semantic tag, <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/main"><code>&lt;main&gt;</code></a>, and put whatever we want to be the main content of this page. I'm going to use <code>&lt;h2&gt;</code> for a header that's smaller than and "below" the <code>&lt;h1&gt;</code> at the top of the page, and I'll use the <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/p"><code>&lt;p&gt;</code></a>(paragraph) tag for my text.</p>
    <pre><code>
    &lt;body&gt;
        &lt;header&gt;&lt;h1&gt;I Love Cats&lt;/h1&gt;&lt;/header&gt;
    
        &lt;main&gt;
            &lt;h2&gt;Favorite Cats&lt;/h2&gt;
            &lt;p&gt;I love tabby cats, bengal cats, and siamese cats!&lt;/p&gt;
        &lt;/main&gt;
    &lt;/body&gt;</code></pre>

    <p>We can also add a <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/footer"><code>&lt;footer&gt;</code></a> for some extra information at the bottom of the page, like telling everyone who made this website.</p>
    <pre><code>
    &lt;body&gt;
        &lt;header&gt;&lt;h1&gt;I Love Cats&lt;/h1&gt;&lt;/header&gt;
    
        &lt;main&gt;
            &lt;h2&gt;Favorite Cats&lt;/h2&gt;
            &lt;p&gt;I love tabby cats, bengal cats, and siamese cats!&lt;/p&gt;
        &lt;/main&gt;
    
        &lt;footer&gt;This website was made by Shannon&lt;/footer&gt;
    &lt;/body&gt;</code></pre>
<hr>
    <h3 id="sections">Sections</h3>
    <p>Let's make some <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/section">sections</a> to divide our page's content. We'll keep our sections within the main tags, and remember that everything that will be shown on your page should stay within the body tags.</p>
    <p>I'll make the My Favorite Cats header and paragraph my first section by putting <code>&lt;section&gt;</code> tags around it.</p>
    <pre><code>
    &lt;main&gt;
        &lt;section&gt;
            &lt;h2&gt;Favorite Cats&lt;/h2&gt;
            &lt;p&gt;I love tabby cats, bengal cats, and siamese cats!&lt;/p&gt;
        &lt;/section&gt;
    &lt;/main&gt;</code></pre>

    <p>Let's add more sections. You can make as many sections as you want.</p>
    <pre><code>
    &lt;main&gt;
        &lt;section&gt;
            &lt;h2&gt;Favorite Cats&lt;/h2&gt;
            &lt;p&gt;I love tabby cats, bengal cats, and siamese cats!&lt;/p&gt;
        &lt;/section&gt;
    
        &lt;section&gt;
            &lt;h2&gt;Tabby Cats&lt;/h2&gt;
            &lt;p&gt;Tabby cats have a striped pattern and are usually brown or grey.&lt;/p&gt;
        &lt;/section&gt;
    
        &lt;section&gt;
            &lt;h2&gt;Great Names for Cats&lt;/h2&gt;
            &lt;p&gt;Some people like to name their cats with names like 
            Fluffy, Frisky, or Patches.&lt;/p&gt;
        &lt;/section&gt;
    &lt;/main&gt;</code></pre>
<hr>
    <h3 id="linking">Linking</h3>
    <p>Linking pages and websites together is what makes the internet a network. The HTML for creating links is an <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/a"><code>&lt;a&gt;</code></a>(anchor) tag with an attribute called <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/a#href"><code>href</code></a>. </p>
    <p>Writing a link in HTML looks like this.</p>
    <pre><code>&lt;a href="about.html"&gt;About&lt;/a&gt;</code></pre>
    <p>The a tag has an opening and closing, just like the other tags we've been using. The text between the tags is what will show on the page for users to click(or tap) to follow the link. In this example, I'm linking to a file called "about.html" and my link's text will say "About". The file needs to be in the same folder as the page I'm putting the link on for it to work with just the file name like this. If the file is in a subfolder, include the path like this.</p>
    <pre><code>&lt;a href="about/index.html"&gt;About&lt;/a&gt;</code></pre>
    <p>And when you're linking to different website, be sure to include the full <a href="https://developer.mozilla.org/en-US/docs/Glossary/URL">url</a> like this.</p>
    <pre><code>&lt;a href="https://web.pixelshannon.com"&gt;Make Your Own Website&lt;/a&gt;</code></pre>
<hr>
<h3 id="images">Images</h3>
    <p>To embed images in your page, use the <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/img"><code>&lt;img&gt;</code></a> tag. Put the filename of your image in the src attribute. I put my cat pixel images in a subfolder called "cats", so I included that path just like with the links. Many people like to have a subfolder for all of their images, named something like "img" or "images".</p>
    <pre><code>&lt;img src="cats/cat1.gif" /&gt;</code></pre>
    <p><img src="https://web.pixelshannon.com/make/lesson/cats/cat1.gif" alt="Pixel drawing of a black and white cat"></p>
    <p>For accessibility, you should always add alt text to your images. This helps people who use screen readers. To do this, put the alt attribute in your <code>&lt;img&gt;</code> tag. Write a short description of the image. Even a word or two will help!</p>
    <pre><code>&lt;img src="cats/cat1.gif" alt="Pixel drawing of a black and white cat" /&gt;</code></pre>
<hr>
<h3 id="create-a-stylesheet-with-css">Create a Stylesheet With CSS</h3>
    <p>Now you have a basic HTML page that you can open and view in the browser. If you open your index.html file, it will look very plain. The CSS used for a website is called the stylesheet. </p>
    <p>CSS looks like this. The selector describes what in the HTML is being selected to be styled. </p>
    <pre><code>selector {
        property: value;
    }</code></pre>
    <p>Create a new file called style.css and save it in the same folder as your index file. Unlike the index HTML file, you can name the CSS file for your stylesheet anything you want. </p>
    <p>Let's start simply by adding a background color. I'm using hexadecimal color values. There are some named colors that you can use, but unless it's white or black, I find it's easier to get the color you want with a color's hex code. I'm using a light pink for the background, <a href="https://www.color-hex.com/color/f9dee1">#f9dee1</a>. </p>
    <p>The selector is "body" because we're selecting the <code>&lt;body&gt;</code> tag in our HTML. Remember how everything visible on the page goes inside the <code>&lt;body&gt;</code> tags? That means that styles applied to "<code>body</code>" in our CSS will affect the whole page.</p>
    <pre><code>body {
    
        background-color: #f9dee1;
    
    }</code></pre>

    <hr>

    <h3 id="link-your-stylesheet">Link Your Stylesheet</h3>
    <p>Now that you've begun a stylesheet, add it to your HTML page to see the CSS applied to the page.</p>
    <p>Back at the index page, CSS goes within the <code>&lt;head&gt;</code> element, like the <code>&lt;title&gt;</code> does. We're going to <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/link">link</a> to the stylesheet like this.</p>
    <pre><code>
    &lt;head&gt;
        &lt;title&gt;I Love Cats&lt;/title&gt;
        &lt;meta name="viewport" content="width=device-width, initial-scale=1.0"&gt;
        &lt;link rel="stylesheet" href="style.css"&gt;
    &lt;/head&gt;</code></pre>
    <p>You could also put your CSS directly on the page, and it would still go within the <code>&lt;head&gt;</code> tags, and between <code>&lt;style&gt;</code> tags, like this. </p>
    <pre><code>
    &lt;head&gt;
        &lt;title&gt;I Love Cats&lt;/title&gt;
        &lt;style&gt;
        body {
    
        background-color: #f9dee1;
    
        }
        &lt;/style&gt;
    &lt;/head&gt;
</code></pre>
    <p>Using an external stylesheet, like the one we linked in our header is usually preferred. It's more flexible, and you can link to the same stylesheet on multiple pages so you don't have to make changes multiple times.</p>
<hr>
<h3 id="stylish-bonus">Stylish Bonus</h3>
    <p>In case someone views your website on a mobile device, you can set a background color for the top of the screen to match your website's background color. It's not CSS, and you would have to change it in every page if you change your background color, but it's a nice detail that I like to add.</p>
    <p>All you need to do this is a <code>&lt;meta&gt;</code> tag with the <code>theme-color</code> attribute. Put this in the <code>&lt;head&gt;</code> section of your page. Add your color in the <code>content</code> attribute. I chose the same color I used for the <code>&lt;body&gt;</code> background color.</p>
    <pre><code>
    &lt;head&gt;
        &lt;title&gt;I Love Cats&lt;/title&gt;
        &lt;meta name="viewport" content="width=device-width, initial-scale=1.0"&gt;
        &lt;meta name="theme-color" content="#f9dee1"&gt;
        &lt;link rel="stylesheet" href="style.css"&gt;
    &lt;/head&gt;
</code></pre>

<hr>

<h3 id="more-style">More Style</h3>
    <p>Let's go back to the stylesheet to add more style to our page. I've added to the style for <code>body</code> with the <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/font-family"><code>font-family</code></a> and <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/color"><code>color</code></a> properties. This will change the font and color of the text. </p>
    <p>I've styled centered the header by styling <code>h1</code> with <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/text-align"><code>text-align: center</code></a> and added <code>color</code> to <code>h1</code> and <code>h2</code>. </p>
    <p>I've also added rules to the <code>footer</code>, centering the text with <code>text-align</code>, adding a <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/text-align"><code>border</code></a> to the top of the footer with <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/border-top"><code>border-top</code></a>, and adding <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/border-top"><code>padding</code></a>.</p>
    <pre><code>
    body {
        background-color: #f9dee1;
        font-family: Verdana, Geneva, Tahoma, sans-serif;
        color: #282A36;
    }
    
    h1 {
        color: #FF1493;
        text-align: center;
    }
    
    h2 {
        color: #FF69B4;
    }
    
    footer {
        text-align: center;
        border-top: 2pt solid #fcc9ce;
        padding: 5pt;
    }</code></pre>
    <p>CSS is very extensive, and this is barely scratching the surface of what you can do. Check out a full <a href="#more-resources">CSS tutorial</a> when you're ready to do more. You can also <a href="https://web.pixelshannon.com/make/home.css">check out the stylesheet</a> for this page.</p>
 <hr>
    <h3 id="more-pages">More Pages</h3>
    <p>You might want your website to have more than one page. Linking your pages together from the index page creates a multipage website.</p>
    <p>Now that you know the basics that every web page needs, you can make additional pages. Create a new html file and use your index page as a template for any other pages you want to make. You'll want to change the title and header, and most likely put most of the new page's content within the <code>&lt;main&gt;</code> tags. Additional pages can be named whatever you want, but the files should have a <code>.html</code> ending and have no spaces in the name.</p>
    <pre><code>
    &lt;!DOCTYPE html&gt;
    &lt;html&gt;
        &lt;head&gt;
            &lt;title&gt;I Love Cats&lt;/title&gt;
            &lt;meta name="viewport" content="width=device-width, initial-scale=1.0"&gt;
            &lt;meta name="theme-color" content="#f9dee1"&gt;
            &lt;link rel="stylesheet" href="style.css"&gt;
        &lt;/head&gt;
        &lt;body&gt;  
            &lt;header&gt;&lt;h1&gt;I Love Cats&lt;/h1&gt;&lt;/header&gt;
    
                &lt;main&gt;
                &lt;/main&gt;
    
            &lt;footer&gt;This website was made by Shannon&lt;/footer&gt;
        &lt;/body&gt;
    &lt;/html&gt;</code></pre>

<hr>

<h3 id="lists">Lists</h3>
    <p>Creating a list in HTML can be very useful. use <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/ol"><code>&lt;ol&gt;</code></a> for an ordered list(usually a numbered list) or <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/ul"><code>&lt;ul&gt;</code></a> for an unordered list. Each item in your list should be wrapped in <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/li"><code>&lt;li&gt;</code></a> tags, for "list item".</p>
    <pre><code>
    &lt;section&gt;
        &lt;h2&gt;Great Names for Cats&lt;/h2&gt;
        &lt;ul&gt;
        &lt;li&gt;Fluffy&lt;/li&gt;
        &lt;li&gt;Frisky&lt;/li&gt;
        &lt;li&gt;Patches&lt;/li&gt;
        &lt;/ul&gt;
    &lt;/section&gt;
</code></pre>

<hr>

    <h3 id="ids-and-classes">IDs and Classes</h3>
    <p>Adding classes and IDs to the HTML can help us to create more specific styles, and organize the page more.<br>
    Use an <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Global_attributes/id"><code>id</code></a> attribute when there's only one on the page, and a <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Global_attributes/class"><code>class</code></a> attribute when you might use it multiple times. You can also link to an ID to create a link to a different page section. You can put the ID in pretty much any tag, but the h2/h3 tag of a header, or the section tag are the most common.</p>
    <p>Here's an example</p>
    <pre><code>
    &lt;section class="box"&gt;
        &lt;h2 id="tabbycats"&gt;Tabby Cats&lt;/h2&gt;
        &lt;p&gt;Tabby cats have a striped pattern and are usually brown or grey.&lt;/p&gt;
    &lt;/section&gt;</code></pre>

    <p>Link to an ID on the same page like this<br>
    <code>&lt;a href="#tabbycats"&gt;Tabby Cats&lt;/a&gt;</code></p>

<hr>

<h3 id="styling-boxes-and-borders">Styling Boxes and Borders</h3>
    <p>To style my sections, I can use CSS to select all <code>&lt;section&gt;</code> tags, or I can use a class or ID. Since I added the class "box" to all of my sections in the middle, I can use the <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/Class_selectors">class selector</a> to style only the sections with the "box" class. </p>
    <p>I can do things like add a <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/border">border</a>, a different background color, font color, font sizes, etc. </p>
    <pre><code>
    .box {
        border-style: solid;
        border-color: white;
        border-width: 1pt;
    }</code></pre>
    <p>This will apply the style to all elements with the class "box". I can also select only <code>&lt;section&gt;</code> elements with the class "box" using <code>element.class</code> like this.</p>
    <pre><code>
    section.box {
        border: solid white 2pt;
        padding: 5pt;
        margin-bottom: 5pt;
    }</code></pre>
    <p>There are several options to choose from with <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/border-style"><code>border-style</code></a>, including dotted, dashed, and double. </p>
    <p>You can make a rounded border with the <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/border-radius"><code>border-radius</code></a> property.</p>
    <pre><code>
    section.box { 
        border-radius: 10%;
    }</code></pre>
    <p>You can select an ID as well, using an <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/ID_selectors">ID selector</a> like <code>#id</code></p>
    <pre><code>
    #tabbycats {
        color: #c095e4;
    }</code></pre>
    <p>In this example, the <code>&lt;h2&gt;</code> with the id attribute <code>tabbycats</code> will be styled with a different text color. </p>

<hr>

<h3 id="photos">Photos</h3>
    <p>I exported my photo with a max-width of 1024px from my photo manager and put it in a subfolder named "photos". The <code>&lt;img&gt;</code> code is the same as before, with a different source and alt text, and I've added a class called <code>photo</code></p>
    <pre><code>&lt;img src="photos/kittenbaby.jpg" alt="A grey and white cat" class="photo" /&gt;</code></pre>
    <p>Now I can style my photo, and any other photos that I give the <code>photo</code> class to, in my stylesheet. I'm giving it a white border, resizing it, and giving it slightly rounded corners.</p>
    <pre><code>
    .photo {
        border: solid white 2pt;
        width: 400px;
        max-width: 100%;
        height: auto;
        border-radius: 10px;
    }</code></pre>

    <p>You might want to link to the full sized photo, using the resized embedded photo as a thumbnail. The HTML for this is just like a regular link, but instead of text for the reader to click on, the <code>&lt;img&gt;</code> code goes there.</p>
    <pre><code>
        &lt;a href="photos/kittenbaby.jpg"&gt;&lt;img src="photos/kittenbaby.jpg" 
        alt="A grey and white cat" class="photo" /&gt;&lt;/a&gt;</code></pre>
    <p>If you want to have a photo with a caption attached, you can keep them all together with the <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/figure"><code>&lt;figure&gt;</code></a> and <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/figcaption"><code>&lt;figcaption&gt;</code></a> tags. Put your <code>&lt;img&gt;</code> after the opening <code>&lt;figure&gt;</code> tag, and write a caption between the <code>&lt;figcaption&gt;</code> tags.</p>
    <pre><code>
    &lt;figure&gt;
    &lt;a href="photos/kittenbaby.jpg"&gt;&lt;img src="photos/kittenbaby.jpg" 
    alt="A grey and white cat" class="photo" /&gt;&lt;/a&gt;
    &lt;figcaption&gt;A cat named Kitten Baby.&lt;/figcaption&gt;
    &lt;/figure&gt;</code></pre>
    <p>We can style these too, if we want.</p>
    <pre><code>
    figure {
        margin: 0;
    }
    
    figcaption {
        color: #FF69B4;
    }</code></pre>

<hr>

<h3 id="using-emoji">Using Emoji</h3>
    <p>All special characters used in HTML documents need a special code to show up properly. Every special character is part of the <a href="https://developer.mozilla.org/en-US/docs/Glossary/Unicode">Unicode</a> character set, and that includes emoji. To get the code for your chosen emoji, you can use a website like <a href="https://emojiguide.org/">EmojiGuide.com</a>. I searched "<a href="https://emojiguide.org/cat">cat</a>" on <a href="https://emojiguide.org/">Emoji Guide</a>, and copied the HTML code, which is this: <code>&amp;#128008;</code> </p>
    <p>Just pasting that code into the HTML as if it were a word made the emoji appear. You can also style your emoji by wrapping it in a tag such as <code>&lt;span&gt;</code> and adding a class such as <code>emoji</code> to it.</p>
    <pre><code>&lt;span class="emoji"&gt;&amp;#128008;&lt;/span&gt;</code></pre>
    <p>Then style the class. I want my emoji to be a little bigger than the rest of my text.</p>
    <pre><code>.emoji {
        font-size: 2rem;
    }</code></pre>

    <hr>

    <h3 id="example">View Examples</h3>
    <p><a href="https://web.pixelshannon.com/make/lesson/index.html">View the example page</a> and the <a href="https://web.pixelshannon.com/make/lesson/style.css">example CSS stylesheet</a></p>
    <p><a href="https://web.pixelshannon.com/make/ballet/index.html">Ballet Website</a> - This is a copy of the website my daughter made using this guide, at age 12, for a school project.</p>
    <p>If you have questions or comments, you can <a href="mailto:reply25@pixelshannon.com?subject=%5Breply%20to%5D%20Make%20Your%20Own%20Website%22">reply by email</a>, <a href="https://bookstodon.com/@shannonkay/113981703308638892">reply on Mastodon</a>, or <a href="https://bsky.app/profile/shannonkay.com/post/3lhu5ebhrvk2l">reply on Bluesky</a>.</p>

<hr>

    <h3 id="host-and-publish-your-website-on-the-internet">Host and Publish Your Website on the Internet</h3>
    <p>To publish your website on the internet, you need to upload your files to a web host. You will need to upload your index.html file as well as any other files your website is using, including your stylesheet css file, images used, and any other html pages.</p>
    <p>If you can, I suggest registering your own domain name (I recommend <a href="https://porkbun.com/">porkbun.com</a> for domain registration). With a domain name, you don't have to change your website address if you change your hosting provider. You simply point the domain name at whichever host you want.</p>
    <h4 id="where-can-i-publish-my-website-for-free">Where can I publish my website for free?</h4>
    <p>Check out my <a href="https://web.pixelshannon.com/freehosts/index.html">Test of Free Web Hosts</a> for more details about using these and even more free hosting options.</p>
    <ul>
    <li><a href="https://yay.boo/">Yay.boo</a> - Super fast and easy to use, just drag and drop your website files into the uploader. They offer free websites up to 10mb(<em>for reference, my example site from this lesson is less than 0.5mb</em>) for free on the yay.boo subdomain, and there's a little ghost icon in the corner of the site. You can subscribe to support the site starting at $25/year to use your own domain names, upload sites up to 25mb, and remove the ghost mascot.</li>
    <li><a href="https://neocities.org/">Neocities</a> - A fun place for creative websites made by individuals. You get a subdomain web address on the free plan. Use their browser-based dashboard to upload your website. Their low cost <a href="https://neocities.org/supporter">supporter plan</a> supports custom domain names and some other extra features.</li>
    <li><a href="https://nekoweb.org/">Nekoweb</a> - Free website community similar to Neocities. Upload or edit your website in the browser. Their <a href="https://nekoweb.org/donate">donator tier</a> offers custom domain support, more space and some other perks.</li>
    <li><a href="https://codeberg.page/">Codeberg Pages</a> - Register an account at <a href="https://codeberg.org/">Codeberg</a> to use this option. They support custom domain names for free. </li>
    <li><a href="https://www.netlify.com/">Netlify</a> - Netlify has a generous free tier and support for <a href="https://docs.netlify.com/domains-https/custom-domains/configure-external-dns/#app">custom domain names</a>. You can use their <a href="https://docs.netlify.com/site-deploys/create-deploys/#drag-and-drop">drag and drop uploader</a>, or deploy with git.</li>
    <li><a href="https://glitch.com/">Glitch</a> - A nice choice if you want to write your website code all in the browser. You get a glitch.me subdomain. Support for custom domains, but you have to <a href="https://help.glitch.com/hc/en-us/articles/16287558909965-Adding-a-Custom-Domain">use fastly to set it up</a>.</li>
    </ul>
    <h4 id="paid-hosting">Paid Hosting</h4>
    <ul>
    <li><a href="https://www.dreamhost.com/hosting/shared/">DreamHost</a> - If you want more advanced web hosting, DreamHost is a reliable hosting provider that's been around for a long time. Their shared hosting plans are good for most people.</li>
    <li><a href="https://neocities.org/supporter">Neocities supporter plan</a> - Get custom domain name support, more space and bandwidth, and some other things, while supporting the Neocities community.</li>
    <li><a href="https://nekoweb.org/donate">Nekoweb donator tier</a> - Support through patreon to get more space, extra sites, and other benefits. Donators help support the whole website.</li>
    <li><a href="https://home.omg.lol/">omg.lol</a> - For $20/yr you get a yourname.omg.lol domain, profile page, webpage, blog, and a bunch of other things like a Mastodon instance. </li>
    <li><a href="https://www.nearlyfreespeech.net/">NearlyFreeSpeech</a> - Pay only for what you use web hosting.</li>
    </ul>

    <hr>
<section id="links">

    <h3 id="more-resources">More Resources</h3>
    <ul>
    <li><a href="https://developer.mozilla.org/en-US/docs/Learn/Getting_started_with_the_web">Getting Started With the Web</a></li>
    <li><a href="https://32bit.cafe/cyowebsite/">32-Bit Cafe: Creating Your Own Website</a></li>
    <li><a href="https://htmlforpeople.com/">HTML For People</a> - This fantastic web book is another take on the beginner's guide to making a first website.</li>  
    <li><a href="https://stefanbohacek.com/blog/resources-for-keeping-the-web-free-open-and-poetic/">Resources for keeping the web free, open, and poetic</a></li>
    <li><a href="https://nowebwithoutwomen.com/">No Web Without Women</a> - A collection of innovations by women in the fields of computer science and technology.</li>
    </ul>
    <h4 id="html-and-css">HTML and CSS</h4>
    <ul>
    
    <li><a href="https://html.com/#tutorial">HTML Tutorial at HTML.com</a></li>
    <li><a href="https://www.w3schools.com/html/">HTML Tutorial at W3 Schools</a></li>
    <li><a href="https://www.w3schools.com/css/css_intro.asp">CSS Tutorial at W3 Schools</a></li>
    <li><a href="https://developer.mozilla.org/en-US/docs/Learn/Getting_started_with_the_web/CSS_basics">CSS Basics at mdn</a></li>
    <li><a href="https://girlswhocode.com/programs/code-at-home">Girls Who Code Activities</a> </li>
    <li><a href="https://cssreference.io/">CSS Reference</a> - A free visual guide to CSS</li>
    <li><a href="https://flexboxfroggy.com/">Flexbox Froggy</a> - A game for learning CSS flexbox</li>
    <li><a href="http://cssgridgarden.com/">Grid Garden</a> - A game for learning CSS grid layout</li>
    <li><a href="https://developer.mozilla.org/en-US/docs/Web">mdn web docs References</a></li>
    </ul>
    <h4 id="design">Design</h4>
    <ul>
    <li><a href="https://www.color-hex.com/">Color Hex</a> - color hex codes</li>
    <li><a href="https://colorkit.co/">Color Kit</a></li>
    <li><a href="https://girlswhocode.com/assets/images/craft-prod/images/Build-your-best-wireframe-2.pdf">Create a Website Wireframe in Google Slides (Girls Who Code)</a></li>
    </ul>
    <p><a href="https://raindrop.io/shannonkay/resources-34220799/search/sort=-sort&amp;perpage=30&amp;page=0&amp;search=%22%23Make+Your+Own+Website%22">More Resource Links</a></p>
</section>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[We replaced our React front end with Go and WebAssembly (239 pts)]]></title>
            <link>https://dagger.io/blog/replaced-react-with-go</link>
            <guid>43008190</guid>
            <pubDate>Tue, 11 Feb 2025 02:13:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dagger.io/blog/replaced-react-with-go">https://dagger.io/blog/replaced-react-with-go</a>, See on <a href="https://news.ycombinator.com/item?id=43008190">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-framer-name="Post" name="Post"><div data-framer-name="Content" data-framer-component-type="RichTextContainer"><p>A few weeks ago, we <!--$--><a href="https://dagger.io/blog/dagger-cloud-v3" rel="noopener">launched Dagger Cloud v3</a><!--/$-->, a completely new user interface for <!--$--><a href="https://dagger.cloud/" rel="noopener">Dagger Cloud</a><!--/$-->. One of the main differences between v3 and its v2 predecessor is that the new UI is written in <!--$--><a href="https://en.wikipedia.org/wiki/WebAssembly" rel="noopener">WebAssembly (WASM)</a><!--/$--> using Go. At first glance, this might seem an odd choice - Go typically isn't the first language you think of when deciding to program a Web UI - but we had good reasons. In this blog post, I'll explain why we chose WebAssembly, some of our implementation challenges (and how we worked around them), and the results.</p><h4>Two Codebases = More Work, Fewer Features</h4><p>Dagger works by building up a DAG of operations and evaluating them, often in parallel. By nature, this is a difficult thing to display. To help users make sense of it, we offer <!--$--><a href="https://docs.dagger.io/features/visualization" rel="noopener">two real-time visualization interfaces</a><!--/$-->: the Dagger terminal UI (TUI), included in the Dagger CLI, and Dagger Cloud, an online Web dashboard. The Dagger TUI is implemented in Go, and Dagger Cloud (pre-v3) was written in React.</p><p>Obviously, we want both user interfaces to be as close to each other as possible. But the actual act of interpreting Dagger's event stream in real-time and producing a UI is pretty involved. Some of the more complex event streams we've seen have hundreds of thousands of OpenTelemetry spans, and managing the data structures around them gets very complicated, very quickly. The Web UI often couldn't keep up with the huge volume of data it had to process and it would become laggy and slow; to fix this performance bottleneck, we were forced into a different implementation model for the React application.</p><p>So, we ended up with two interfaces trying to accomplish the same thing, one of them in one language and ecosystem (TypeScript/React), the other in a totally different language and ecosystem (Go), and we couldn't easily share business logic between them. As a small team, we need to ship fast. Having to re-implement every feature twice was just a massive tax on our velocity.</p><p>We started thinking about a new approach to Dagger Cloud, with two main goals:</p><ul><li data-preset-tag="p"><p>Unify the codebases, to eliminate duplication and make it more efficient to ship new features</p></li><li data-preset-tag="p"><p>Deliver on the promise of a crisp, snappy Web UI, matching the speed and performance of the terminal UI</p></li></ul><h4>Choosing Go + WebAssembly</h4><p>Our starting goal was to be able to reuse one codebase for both Dagger Cloud and the TUI. We decided fairly early to make it a Go codebase. Technically, we could have gone the other way and used TypeScript for the TUI. But we're primarily a team of Go engineers, so selecting Go made it easier for others in the team to contribute, to add a feature or drop in for a few hours to help debug an issue. In addition to standardizing on a single language, it gave us flexibility and broke down silos in our team.</p><p>Once we decided to run Go code directly in the browser, WebAssembly was the logical next step. But there were still a couple of challenges:</p><ul><li data-preset-tag="p"><p>The Go + WebAssembly combination is still not as mature as React and other JavaScript frameworks. There are no ready-made component libraries to pull from, the developer tooling isn't as rich, and so on. We knew that we would need to build most of our UI components from scratch.</p></li><li data-preset-tag="p"><p>There is a hard 2 GB memory limit for WebAssembly applications in most browsers. We expected this to be a problem when viewing large traces, and we knew we would have to do a lot of optimization to minimize memory usage and keep the UI stable. This wasn't entirely bad though; the silver lining here was that any memory usage improvements made to the WebAssembly UI would also benefit TUI users, since it was now a shared codebase.</p></li></ul><h4>De-Risking the Project</h4><p>Once we'd made the decision, the next question was, "how do we build this?" We decided to build the new WebAssembly-based UI in the <!--$--><a href="https://go-app.dev/" rel="noopener">Go-app framework</a><!--/$-->. Go-app is a high-level framework specifically for <!--$--><a href="https://en.wikipedia.org/wiki/Progressive_web_app" rel="noopener">Progressive Web Apps (PWAs)</a><!--/$--> in WebAssembly. It offers key Go benefits, like fast compilation and native static typing, and it also follows a component-based UI model, like React, which made the transition easier.</p><p>Since the Go + WebAssembly combination isn't mainstream, there was some healthy skepticism within the Dagger team about its feasibility. For example, there was no real ecosystem for Go-app UI components and we knew we’d have to write our own, but we weren’t sure how easy or difficult this would be. We also had concerns over integrations with other services (Tailwind, Auth0, Intercom, PostHog), and about rendering many hundreds of live-updating components at the same time.&nbsp;</p><p>To answer these questions and de-risk the project, I spent almost a month prototyping, with the goal of re-implementing as much of the existing UI as possible in Go-app. As it turned out, there weren't many blockers: WebAssembly is already a <!--$--><a href="https://webassembly.org/specs/" rel="noopener">well-documented open standard</a><!--/$--> and most other questions were answered in <!--$--><a href="https://go-app.dev/reference" rel="noopener">Go-app’s own documentation</a><!--/$-->. The biggest challenge, as expected, was the memory usage limit, which required careful design and optimization.</p><h4>From Prototype to Production</h4><p>Once we had a working proof of concept, the team's comfort level increased significantly and we kicked off project "awesome wasm" to deliver a production implementation. Here are a few notes from the journey:</p><ul><li data-preset-tag="p"><p>Memory usage was easily the most existential threat to the project’s success. I spent a lot of time figuring out how to render 200k+ lines of log output without crashing. This led to optimizations deep in our <!--$--><a href="https://github.com/vito/midterm" rel="noopener">virtual terminal rendering library</a><!--/$-->, which dramatically reduced TUI memory usage at the same time (as mentioned already, sharing codebases means that important optimizations in one interface become "free" in the other!)</p></li><li data-preset-tag="p"><p>Go WASM is slow at parsing large amounts of JSON, which led to dramatic architecture changes and the creation of a “smart backend” for incremental data loading over WebSockets, using Go's rarely-used <!--$--><a href="https://pkg.go.dev/encoding/gob" rel="noopener">encoding/gob format</a><!--/$-->.</p></li><li data-preset-tag="p"><p>Initially, the WASM file was around 32 MB. By applying <!--$--><a href="https://github.com/google/brotli" rel="noopener">Brotli compression</a><!--/$-->, we were able to bring it down to around 4.6 MB. We tried to perform Brotli compression on-the-fly in our CDN but the file was too large, so eventually we just included the compression step into our build process.</p></li><li data-preset-tag="p"><p>Apart from the memory challenges, most of our other initial worries turned out unfounded. The UI components weren’t very hard to write, integrations with other services were straightforward, and I found good techniques for handling component updates in real-time.</p></li><li data-preset-tag="p"><p>There were a number of useful NPM packages I found, so I wondered if I could use them with Go. WebAssembly has a straightforward interface to both Go and JavaScript, so I built a <!--$--><a href="https://daggerverse.dev/mod/github.com/vito/daggerverse/browserify@d368836636284116d090e271742904fea369cf72" rel="noopener">Dagger module that uses Browserify to load an NPM package</a><!--/$-->. This module allows us to generate a JavaScript file that can be included in a Go application. This means that we can work primarily in Go and then, if needed, we have a way to load helpers that are implemented in native JavaScript.</p></li><li data-preset-tag="p"><p>Disclaimer: I'm not a React professional so with that in mind...it seemed to me that React had a very rigid way of implementing components, while Go-app was much more flexible. In Go-app, you can have any component update whenever you like, which gives you many more degrees of freedom for optimization. For example, I needed to optimize a component rendering 150,000+ lines of output. Just having the ability to try different approaches and then pick the one that worked best, made the entire exercise much easier!</p></li><li data-preset-tag="p"><p>Even though Go-app doesn't have React-like developer tools built into the browser, I was able to use Go's own tools (pprof) plus the default profiler built into the browser for profiling and debugging. This was very useful to inspect functions calls, track CPU and memory usage, and evaluate the effectiveness of different approaches for optimizing memory usage.</p></li><li data-preset-tag="p"><p>I discovered a side benefit of using Go-app: since Dagger Cloud is built as a PWA, it can be installed as a desktop or a mobile application. This makes it possible to launch Dagger Cloud like a native application and get a full-screen experience without needing to open a browser first, or just have a dedicated icon in your desktop taskbar/dock.</p></li></ul><p>We soft-launched Dagger Cloud v3 to our <!--$--><a href="https://dagger.io/commanders" rel="noopener">Dagger Commanders</a><!--/$--> a few weeks ago to collect feedback and made it available to everyone shortly thereafter.</p><h4>Benefits</h4><p>Our switch from React to WASM has resulted in a more consistent user experience across all Dagger interfaces, and better overall performance and lower memory usage, especially when rendering large and complex traces.</p><p>From an engineering perspective too, the benefits to our team are significant. Optimizations very often involve just as much, if not more, work than actually implementing features. So it's great to not have to spend time optimizing the Web UI, and then more time optimizing the TUI, and instead actually focus on delivering new features.</p><h4>Should You Do This?</h4><p>Dagger Cloud v3 has the Dagger community buzzing and one of the more common questions we've been fielding recently is: who should consider doing this and who shouldn't?</p><p>We want to be clear that we're not generally recommending making front-ends in Go. We had some very good reasons to do it: a team of strong Go engineers; a complex UI that TypeScript/React didn't scale well for; a requirement for standardization and reuse between two codebases; and a company-wide mandate to increase our velocity. That's a fairly specific set of circumstances. If you're in similar circumstances, this is certainly an option worth evaluating; if not, there are other tools and standards that you should consider first.</p><p>Dagger Cloud v3 is still in beta and we're excited for you to <!--$--><a href="https://v3.dagger.cloud/" rel="noopener">try it out</a><!--/$-->. If you'd like to know more about our implementation or simply have feedback to share on the new UI, join our Discord and <!--$--><a href="https://discord.com/invite/dagger-io" rel="noopener">let us know</a><!--/$--> what you think!</p></div><div data-framer-name="Content" data-framer-component-type="RichTextContainer"><p>A few weeks ago, we <!--$--><a href="https://dagger.io/blog/dagger-cloud-v3" rel="noopener">launched Dagger Cloud v3</a><!--/$-->, a completely new user interface for <!--$--><a href="https://dagger.cloud/" rel="noopener">Dagger Cloud</a><!--/$-->. One of the main differences between v3 and its v2 predecessor is that the new UI is written in <!--$--><a href="https://en.wikipedia.org/wiki/WebAssembly" rel="noopener">WebAssembly (WASM)</a><!--/$--> using Go. At first glance, this might seem an odd choice - Go typically isn't the first language you think of when deciding to program a Web UI - but we had good reasons. In this blog post, I'll explain why we chose WebAssembly, some of our implementation challenges (and how we worked around them), and the results.</p><h4>Two Codebases = More Work, Fewer Features</h4><p>Dagger works by building up a DAG of operations and evaluating them, often in parallel. By nature, this is a difficult thing to display. To help users make sense of it, we offer <!--$--><a href="https://docs.dagger.io/features/visualization" rel="noopener">two real-time visualization interfaces</a><!--/$-->: the Dagger terminal UI (TUI), included in the Dagger CLI, and Dagger Cloud, an online Web dashboard. The Dagger TUI is implemented in Go, and Dagger Cloud (pre-v3) was written in React.</p><p>Obviously, we want both user interfaces to be as close to each other as possible. But the actual act of interpreting Dagger's event stream in real-time and producing a UI is pretty involved. Some of the more complex event streams we've seen have hundreds of thousands of OpenTelemetry spans, and managing the data structures around them gets very complicated, very quickly. The Web UI often couldn't keep up with the huge volume of data it had to process and it would become laggy and slow; to fix this performance bottleneck, we were forced into a different implementation model for the React application.</p><p>So, we ended up with two interfaces trying to accomplish the same thing, one of them in one language and ecosystem (TypeScript/React), the other in a totally different language and ecosystem (Go), and we couldn't easily share business logic between them. As a small team, we need to ship fast. Having to re-implement every feature twice was just a massive tax on our velocity.</p><p>We started thinking about a new approach to Dagger Cloud, with two main goals:</p><ul><li data-preset-tag="p"><p>Unify the codebases, to eliminate duplication and make it more efficient to ship new features</p></li><li data-preset-tag="p"><p>Deliver on the promise of a crisp, snappy Web UI, matching the speed and performance of the terminal UI</p></li></ul><h4>Choosing Go + WebAssembly</h4><p>Our starting goal was to be able to reuse one codebase for both Dagger Cloud and the TUI. We decided fairly early to make it a Go codebase. Technically, we could have gone the other way and used TypeScript for the TUI. But we're primarily a team of Go engineers, so selecting Go made it easier for others in the team to contribute, to add a feature or drop in for a few hours to help debug an issue. In addition to standardizing on a single language, it gave us flexibility and broke down silos in our team.</p><p>Once we decided to run Go code directly in the browser, WebAssembly was the logical next step. But there were still a couple of challenges:</p><ul><li data-preset-tag="p"><p>The Go + WebAssembly combination is still not as mature as React and other JavaScript frameworks. There are no ready-made component libraries to pull from, the developer tooling isn't as rich, and so on. We knew that we would need to build most of our UI components from scratch.</p></li><li data-preset-tag="p"><p>There is a hard 2 GB memory limit for WebAssembly applications in most browsers. We expected this to be a problem when viewing large traces, and we knew we would have to do a lot of optimization to minimize memory usage and keep the UI stable. This wasn't entirely bad though; the silver lining here was that any memory usage improvements made to the WebAssembly UI would also benefit TUI users, since it was now a shared codebase.</p></li></ul><h4>De-Risking the Project</h4><p>Once we'd made the decision, the next question was, "how do we build this?" We decided to build the new WebAssembly-based UI in the <!--$--><a href="https://go-app.dev/" rel="noopener">Go-app framework</a><!--/$-->. Go-app is a high-level framework specifically for <!--$--><a href="https://en.wikipedia.org/wiki/Progressive_web_app" rel="noopener">Progressive Web Apps (PWAs)</a><!--/$--> in WebAssembly. It offers key Go benefits, like fast compilation and native static typing, and it also follows a component-based UI model, like React, which made the transition easier.</p><p>Since the Go + WebAssembly combination isn't mainstream, there was some healthy skepticism within the Dagger team about its feasibility. For example, there was no real ecosystem for Go-app UI components and we knew we’d have to write our own, but we weren’t sure how easy or difficult this would be. We also had concerns over integrations with other services (Tailwind, Auth0, Intercom, PostHog), and about rendering many hundreds of live-updating components at the same time.&nbsp;</p><p>To answer these questions and de-risk the project, I spent almost a month prototyping, with the goal of re-implementing as much of the existing UI as possible in Go-app. As it turned out, there weren't many blockers: WebAssembly is already a <!--$--><a href="https://webassembly.org/specs/" rel="noopener">well-documented open standard</a><!--/$--> and most other questions were answered in <!--$--><a href="https://go-app.dev/reference" rel="noopener">Go-app’s own documentation</a><!--/$-->. The biggest challenge, as expected, was the memory usage limit, which required careful design and optimization.</p><h4>From Prototype to Production</h4><p>Once we had a working proof of concept, the team's comfort level increased significantly and we kicked off project "awesome wasm" to deliver a production implementation. Here are a few notes from the journey:</p><ul><li data-preset-tag="p"><p>Memory usage was easily the most existential threat to the project’s success. I spent a lot of time figuring out how to render 200k+ lines of log output without crashing. This led to optimizations deep in our <!--$--><a href="https://github.com/vito/midterm" rel="noopener">virtual terminal rendering library</a><!--/$-->, which dramatically reduced TUI memory usage at the same time (as mentioned already, sharing codebases means that important optimizations in one interface become "free" in the other!)</p></li><li data-preset-tag="p"><p>Go WASM is slow at parsing large amounts of JSON, which led to dramatic architecture changes and the creation of a “smart backend” for incremental data loading over WebSockets, using Go's rarely-used <!--$--><a href="https://pkg.go.dev/encoding/gob" rel="noopener">encoding/gob format</a><!--/$-->.</p></li><li data-preset-tag="p"><p>Initially, the WASM file was around 32 MB. By applying <!--$--><a href="https://github.com/google/brotli" rel="noopener">Brotli compression</a><!--/$-->, we were able to bring it down to around 4.6 MB. We tried to perform Brotli compression on-the-fly in our CDN but the file was too large, so eventually we just included the compression step into our build process.</p></li><li data-preset-tag="p"><p>Apart from the memory challenges, most of our other initial worries turned out unfounded. The UI components weren’t very hard to write, integrations with other services were straightforward, and I found good techniques for handling component updates in real-time.</p></li><li data-preset-tag="p"><p>There were a number of useful NPM packages I found, so I wondered if I could use them with Go. WebAssembly has a straightforward interface to both Go and JavaScript, so I built a <!--$--><a href="https://daggerverse.dev/mod/github.com/vito/daggerverse/browserify@d368836636284116d090e271742904fea369cf72" rel="noopener">Dagger module that uses Browserify to load an NPM package</a><!--/$-->. This module allows us to generate a JavaScript file that can be included in a Go application. This means that we can work primarily in Go and then, if needed, we have a way to load helpers that are implemented in native JavaScript.</p></li><li data-preset-tag="p"><p>Disclaimer: I'm not a React professional so with that in mind...it seemed to me that React had a very rigid way of implementing components, while Go-app was much more flexible. In Go-app, you can have any component update whenever you like, which gives you many more degrees of freedom for optimization. For example, I needed to optimize a component rendering 150,000+ lines of output. Just having the ability to try different approaches and then pick the one that worked best, made the entire exercise much easier!</p></li><li data-preset-tag="p"><p>Even though Go-app doesn't have React-like developer tools built into the browser, I was able to use Go's own tools (pprof) plus the default profiler built into the browser for profiling and debugging. This was very useful to inspect functions calls, track CPU and memory usage, and evaluate the effectiveness of different approaches for optimizing memory usage.</p></li><li data-preset-tag="p"><p>I discovered a side benefit of using Go-app: since Dagger Cloud is built as a PWA, it can be installed as a desktop or a mobile application. This makes it possible to launch Dagger Cloud like a native application and get a full-screen experience without needing to open a browser first, or just have a dedicated icon in your desktop taskbar/dock.</p></li></ul><p>We soft-launched Dagger Cloud v3 to our <!--$--><a href="https://dagger.io/commanders" rel="noopener">Dagger Commanders</a><!--/$--> a few weeks ago to collect feedback and made it available to everyone shortly thereafter.</p><h4>Benefits</h4><p>Our switch from React to WASM has resulted in a more consistent user experience across all Dagger interfaces, and better overall performance and lower memory usage, especially when rendering large and complex traces.</p><p>From an engineering perspective too, the benefits to our team are significant. Optimizations very often involve just as much, if not more, work than actually implementing features. So it's great to not have to spend time optimizing the Web UI, and then more time optimizing the TUI, and instead actually focus on delivering new features.</p><h4>Should You Do This?</h4><p>Dagger Cloud v3 has the Dagger community buzzing and one of the more common questions we've been fielding recently is: who should consider doing this and who shouldn't?</p><p>We want to be clear that we're not generally recommending making front-ends in Go. We had some very good reasons to do it: a team of strong Go engineers; a complex UI that TypeScript/React didn't scale well for; a requirement for standardization and reuse between two codebases; and a company-wide mandate to increase our velocity. That's a fairly specific set of circumstances. If you're in similar circumstances, this is certainly an option worth evaluating; if not, there are other tools and standards that you should consider first.</p><p>Dagger Cloud v3 is still in beta and we're excited for you to <!--$--><a href="https://v3.dagger.cloud/" rel="noopener">try it out</a><!--/$-->. If you'd like to know more about our implementation or simply have feedback to share on the new UI, join our Discord and <!--$--><a href="https://discord.com/invite/dagger-io" rel="noopener">let us know</a><!--/$--> what you think!</p></div><div data-framer-name="Content" data-framer-component-type="RichTextContainer"><p>A few weeks ago, we <!--$--><a href="https://dagger.io/blog/dagger-cloud-v3" rel="noopener">launched Dagger Cloud v3</a><!--/$-->, a completely new user interface for <!--$--><a href="https://dagger.cloud/" rel="noopener">Dagger Cloud</a><!--/$-->. One of the main differences between v3 and its v2 predecessor is that the new UI is written in <!--$--><a href="https://en.wikipedia.org/wiki/WebAssembly" rel="noopener">WebAssembly (WASM)</a><!--/$--> using Go. At first glance, this might seem an odd choice - Go typically isn't the first language you think of when deciding to program a Web UI - but we had good reasons. In this blog post, I'll explain why we chose WebAssembly, some of our implementation challenges (and how we worked around them), and the results.</p><h4>Two Codebases = More Work, Fewer Features</h4><p>Dagger works by building up a DAG of operations and evaluating them, often in parallel. By nature, this is a difficult thing to display. To help users make sense of it, we offer <!--$--><a href="https://docs.dagger.io/features/visualization" rel="noopener">two real-time visualization interfaces</a><!--/$-->: the Dagger terminal UI (TUI), included in the Dagger CLI, and Dagger Cloud, an online Web dashboard. The Dagger TUI is implemented in Go, and Dagger Cloud (pre-v3) was written in React.</p><p>Obviously, we want both user interfaces to be as close to each other as possible. But the actual act of interpreting Dagger's event stream in real-time and producing a UI is pretty involved. Some of the more complex event streams we've seen have hundreds of thousands of OpenTelemetry spans, and managing the data structures around them gets very complicated, very quickly. The Web UI often couldn't keep up with the huge volume of data it had to process and it would become laggy and slow; to fix this performance bottleneck, we were forced into a different implementation model for the React application.</p><p>So, we ended up with two interfaces trying to accomplish the same thing, one of them in one language and ecosystem (TypeScript/React), the other in a totally different language and ecosystem (Go), and we couldn't easily share business logic between them. As a small team, we need to ship fast. Having to re-implement every feature twice was just a massive tax on our velocity.</p><p>We started thinking about a new approach to Dagger Cloud, with two main goals:</p><ul><li data-preset-tag="p"><p>Unify the codebases, to eliminate duplication and make it more efficient to ship new features</p></li><li data-preset-tag="p"><p>Deliver on the promise of a crisp, snappy Web UI, matching the speed and performance of the terminal UI</p></li></ul><h4>Choosing Go + WebAssembly</h4><p>Our starting goal was to be able to reuse one codebase for both Dagger Cloud and the TUI. We decided fairly early to make it a Go codebase. Technically, we could have gone the other way and used TypeScript for the TUI. But we're primarily a team of Go engineers, so selecting Go made it easier for others in the team to contribute, to add a feature or drop in for a few hours to help debug an issue. In addition to standardizing on a single language, it gave us flexibility and broke down silos in our team.</p><p>Once we decided to run Go code directly in the browser, WebAssembly was the logical next step. But there were still a couple of challenges:</p><ul><li data-preset-tag="p"><p>The Go + WebAssembly combination is still not as mature as React and other JavaScript frameworks. There are no ready-made component libraries to pull from, the developer tooling isn't as rich, and so on. We knew that we would need to build most of our UI components from scratch.</p></li><li data-preset-tag="p"><p>There is a hard 2 GB memory limit for WebAssembly applications in most browsers. We expected this to be a problem when viewing large traces, and we knew we would have to do a lot of optimization to minimize memory usage and keep the UI stable. This wasn't entirely bad though; the silver lining here was that any memory usage improvements made to the WebAssembly UI would also benefit TUI users, since it was now a shared codebase.</p></li></ul><h4>De-Risking the Project</h4><p>Once we'd made the decision, the next question was, "how do we build this?" We decided to build the new WebAssembly-based UI in the <!--$--><a href="https://go-app.dev/" rel="noopener">Go-app framework</a><!--/$-->. Go-app is a high-level framework specifically for <!--$--><a href="https://en.wikipedia.org/wiki/Progressive_web_app" rel="noopener">Progressive Web Apps (PWAs)</a><!--/$--> in WebAssembly. It offers key Go benefits, like fast compilation and native static typing, and it also follows a component-based UI model, like React, which made the transition easier.</p><p>Since the Go + WebAssembly combination isn't mainstream, there was some healthy skepticism within the Dagger team about its feasibility. For example, there was no real ecosystem for Go-app UI components and we knew we’d have to write our own, but we weren’t sure how easy or difficult this would be. We also had concerns over integrations with other services (Tailwind, Auth0, Intercom, PostHog), and about rendering many hundreds of live-updating components at the same time.&nbsp;</p><p>To answer these questions and de-risk the project, I spent almost a month prototyping, with the goal of re-implementing as much of the existing UI as possible in Go-app. As it turned out, there weren't many blockers: WebAssembly is already a <!--$--><a href="https://webassembly.org/specs/" rel="noopener">well-documented open standard</a><!--/$--> and most other questions were answered in <!--$--><a href="https://go-app.dev/reference" rel="noopener">Go-app’s own documentation</a><!--/$-->. The biggest challenge, as expected, was the memory usage limit, which required careful design and optimization.</p><h4>From Prototype to Production</h4><p>Once we had a working proof of concept, the team's comfort level increased significantly and we kicked off project "awesome wasm" to deliver a production implementation. Here are a few notes from the journey:</p><ul><li data-preset-tag="p"><p>Memory usage was easily the most existential threat to the project’s success. I spent a lot of time figuring out how to render 200k+ lines of log output without crashing. This led to optimizations deep in our <!--$--><a href="https://github.com/vito/midterm" rel="noopener">virtual terminal rendering library</a><!--/$-->, which dramatically reduced TUI memory usage at the same time (as mentioned already, sharing codebases means that important optimizations in one interface become "free" in the other!)</p></li><li data-preset-tag="p"><p>Go WASM is slow at parsing large amounts of JSON, which led to dramatic architecture changes and the creation of a “smart backend” for incremental data loading over WebSockets, using Go's rarely-used <!--$--><a href="https://pkg.go.dev/encoding/gob" rel="noopener">encoding/gob format</a><!--/$-->.</p></li><li data-preset-tag="p"><p>Initially, the WASM file was around 32 MB. By applying <!--$--><a href="https://github.com/google/brotli" rel="noopener">Brotli compression</a><!--/$-->, we were able to bring it down to around 4.6 MB. We tried to perform Brotli compression on-the-fly in our CDN but the file was too large, so eventually we just included the compression step into our build process.</p></li><li data-preset-tag="p"><p>Apart from the memory challenges, most of our other initial worries turned out unfounded. The UI components weren’t very hard to write, integrations with other services were straightforward, and I found good techniques for handling component updates in real-time.</p></li><li data-preset-tag="p"><p>There were a number of useful NPM packages I found, so I wondered if I could use them with Go. WebAssembly has a straightforward interface to both Go and JavaScript, so I built a <!--$--><a href="https://daggerverse.dev/mod/github.com/vito/daggerverse/browserify@d368836636284116d090e271742904fea369cf72" rel="noopener">Dagger module that uses Browserify to load an NPM package</a><!--/$-->. This module allows us to generate a JavaScript file that can be included in a Go application. This means that we can work primarily in Go and then, if needed, we have a way to load helpers that are implemented in native JavaScript.</p></li><li data-preset-tag="p"><p>Disclaimer: I'm not a React professional so with that in mind...it seemed to me that React had a very rigid way of implementing components, while Go-app was much more flexible. In Go-app, you can have any component update whenever you like, which gives you many more degrees of freedom for optimization. For example, I needed to optimize a component rendering 150,000+ lines of output. Just having the ability to try different approaches and then pick the one that worked best, made the entire exercise much easier!</p></li><li data-preset-tag="p"><p>Even though Go-app doesn't have React-like developer tools built into the browser, I was able to use Go's own tools (pprof) plus the default profiler built into the browser for profiling and debugging. This was very useful to inspect functions calls, track CPU and memory usage, and evaluate the effectiveness of different approaches for optimizing memory usage.</p></li><li data-preset-tag="p"><p>I discovered a side benefit of using Go-app: since Dagger Cloud is built as a PWA, it can be installed as a desktop or a mobile application. This makes it possible to launch Dagger Cloud like a native application and get a full-screen experience without needing to open a browser first, or just have a dedicated icon in your desktop taskbar/dock.</p></li></ul><p>We soft-launched Dagger Cloud v3 to our <!--$--><a href="https://dagger.io/commanders" rel="noopener">Dagger Commanders</a><!--/$--> a few weeks ago to collect feedback and made it available to everyone shortly thereafter.</p><h4>Benefits</h4><p>Our switch from React to WASM has resulted in a more consistent user experience across all Dagger interfaces, and better overall performance and lower memory usage, especially when rendering large and complex traces.</p><p>From an engineering perspective too, the benefits to our team are significant. Optimizations very often involve just as much, if not more, work than actually implementing features. So it's great to not have to spend time optimizing the Web UI, and then more time optimizing the TUI, and instead actually focus on delivering new features.</p><h4>Should You Do This?</h4><p>Dagger Cloud v3 has the Dagger community buzzing and one of the more common questions we've been fielding recently is: who should consider doing this and who shouldn't?</p><p>We want to be clear that we're not generally recommending making front-ends in Go. We had some very good reasons to do it: a team of strong Go engineers; a complex UI that TypeScript/React didn't scale well for; a requirement for standardization and reuse between two codebases; and a company-wide mandate to increase our velocity. That's a fairly specific set of circumstances. If you're in similar circumstances, this is certainly an option worth evaluating; if not, there are other tools and standards that you should consider first.</p><p>Dagger Cloud v3 is still in beta and we're excited for you to <!--$--><a href="https://v3.dagger.cloud/" rel="noopener">try it out</a><!--/$-->. If you'd like to know more about our implementation or simply have feedback to share on the new UI, join our Discord and <!--$--><a href="https://discord.com/invite/dagger-io" rel="noopener">let us know</a><!--/$--> what you think!</p></div></div></div>]]></description>
        </item>
    </channel>
</rss>