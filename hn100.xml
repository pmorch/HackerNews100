<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 18 Dec 2023 03:00:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Qatar Airways Bans YouTuber for Negative Review (185 pts)]]></title>
            <link>https://onemileatatime.com/news/qatar-airways-bans-youtuber-negative-review/</link>
            <guid>38677344</guid>
            <pubDate>Sun, 17 Dec 2023 23:08:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://onemileatatime.com/news/qatar-airways-bans-youtuber-negative-review/">https://onemileatatime.com/news/qatar-airways-bans-youtuber-negative-review/</a>, See on <a href="https://news.ycombinator.com/item?id=38677344">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<p>Qatar Airways has reportedly banned a YouTuber after he posted a negative review of a flight, and also fired the crew. The carrier’s approach here simply defies logic…</p>
<div id="ez-toc-container">

<nav><ul><li><a href="#qatar_airways_bans_josh_cahill_over_unfavorable_review" title="Qatar Airways bans Josh Cahill over unfavorable review">Qatar Airways bans Josh Cahill over unfavorable review</a></li><li><a href="#my_take_on_this_qatar_airways_situation" title="My take on this Qatar Airways situation">My take on this Qatar Airways situation</a></li><li><a href="#bottom_line" title="Bottom line">Bottom line</a></li></ul></nav></div>
<h2 id="h-qatar-airways-bans-josh-cahill-over-unfavorable-review"><span id="qatar_airways_bans_josh_cahill_over_unfavorable_review"></span>Qatar Airways bans Josh Cahill over unfavorable review<span></span></h2>
<p>Josh Cahill is a popular YouTuber who reviews flights, and on Saturday he published a roughly 20-minute video about how he was “banned and bribed” by Qatar Airways. This followed an August 2023 video review he published, entitled “the shocking decline of Qatar Airways,” about an economy class flight that he took from Colombo to Doha.</p>
<p>Let me try to summarize what happened as briefly but accurately as possible, according to Josh:</p>
<ul>
<li>Days after he published the video review, Qatar Airways reached out to ask him to hop on a call, which he agreed to, and that included the company’s VP of corporate communications</li>
<li>Once on the call, the Qatar Airways representatives immediately said that whatever is talked about is off the record, which Josh didn’t agree to; he was then asked what his motive was for such a negative review</li>
<li>They said the video reflects negatively on the airline, and they offered Josh a free flight if he’s willing to delete the video, which he rejected</li>
<li>They then asked Josh to delete the negative comments on the video from Qatar Airways employees complaining about poor working conditions at the airline, which he also refused to do</li>
<li>A week later, he received an email informing him that an upcoming booking on Qatar Airways had been canceled, and that the airline can no longer permit additional bookings from him</li>
<li>The email references <a href="https://www.qatarairways.com/en-us/legal/conditions-of-carriage.html">articles 8 &amp; 12 of the contract of carriage</a>, which require permission in order to film on an aircraft, though that was never otherwise enforced for him or any other YouTubers publishing positive reviews</li>
<li>A short time later, Josh received an email from one of the flight attendants who was working his flight, acknowledging that “I know I gave you permission to film the interaction, but I must ask you to remove me from the video,” because “I was pressured to write a statement that I didn’t give you permission to be filmed,” and “if I don’t write it, I will most likely be dismissed”</li>
<li>Since Josh wasn’t willing to voluntarily take down the video, Qatar Airways took all of these statements from crew and sent them to YouTube, demanding that the video be removed for privacy reasons </li>
<li>For those with an IP address in Qatar, Josh’s initial flight review has apparently been blocked, so that you can no longer watch it</li>
<li>While Josh can’t personally vouch for this, he claims that many sources have told him that the entire crew was terminated after the airline wasn’t able to remove the video</li>
</ul>
<p>You can watch the video for yourself below…</p>
<figure></figure>
<h2 id="h-my-take-on-this-qatar-airways-situation"><span id="my_take_on_this_qatar_airways_situation"></span>My take on this Qatar Airways situation<span></span></h2>
<p>It’s kind of unbelievable how much an airline can get in its own way. I mean, my gosh, how much more poorly could Qatar Airways have handled this? The unprofessionalism and short-sightedness here is astounding. </p>
<p>The best thing Qatar Airways could have done is to either issue an apology for the poor experience or to just ignore the video. Frankly, the video wasn’t even that negative, though Josh does tend to have rather dramatic video titles.</p>
<p>All Qatar Airways has done with its response is to draw way more attention to what happened, which seems to be the opposite of what the goal was. I mean, when he rejected the free flight and said he wasn’t willing to speak off the record, didn’t they get a sense that he would make a video about it? And did they think they would look good in it?</p>

<p>And let’s not even talk about the concept of forcing an employee to email Josh to literally admit that they were being told to lie in order to save their jobs. Some people should be fired here for incompetence, but it shouldn’t be the crew of the flight…</p>
<p>I understand in a country like Qatar, it’s easy enough to control the narrative of things, and force people to do what you’d like, if you have enough money or power. However, that approach doesn’t work when you’re trying to run a global airline. And while it’s absolutely within the carrier’s rights to ban passengers for whatever reason, it’s not a great look if you’re trying to make the airline or country have wide appeal.</p>
<p>Now, I think there’s one very important point to acknowledge, and it kind of sheds some light on how this all played out. This happened while Akbar Al Baker was still CEO of Qatar Airways, as he <a href="https://onemileatatime.com/news/qatar-airways-ceo-akbar-al-baker-resigns/" target="_blank" rel="noopener">suddenly resigned several weeks ago</a>. He was known for creating a culture of fear, and where so many decisions were driven by what “the chief” wanted, rather than actually using logic.</p>
<p>So it’s easy enough to figure out exactly how this happened. Al Baker was somehow made aware of this video, and told his minions to have the video removed. Then everyone did everything they could to get that result, rather than pointing out to him that this might not be a good idea.</p>
<p>But there’s some good news. Qatar Airways’ new CEO,&nbsp;Badr Mohammed Al Meer, has <a href="https://onemileatatime.com/news/new-qatar-airways-ceo/" target="_blank" rel="noopener">shared some refreshing new priorities</a> for the airline, including stating that creating “a culture of trust and empowerment will be the building blocks of our shared success.” </p>
<p>So far there’s actually some substance to this, as he has <a href="https://onemileatatime.com/news/qatar-airways-ends-employee-curfew/" target="_blank" rel="noopener">ended the controversial employee curfew</a> that existed for years within weeks of starting his job. Hopefully this is the first of many changes. I think it would be really smart of him to undo this and apologize for the way that Qatar Airways handled this.</p>
<figure><img fetchpriority="high" decoding="async" width="1200" height="823" src="https://cdn.onemileatatime.com/wp-content/uploads/2023/05/Doha-Hamad-Airport-Teddy-Bear.jpg" alt=""><figcaption>Qatar Airways took a short-sighted approach here</figcaption></figure>
<h2 id="h-bottom-line"><span id="bottom_line"></span>Bottom line<span></span></h2>
<p>Qatar Airways has banned a popular YouTuber after he posted a negative review of a flight back in August. The airline first tried to get him to remove the video by offering him a free flight, then had the crew email him to revoke permission to appear in the video, and then approached YouTube directly. </p>
<p>This happened under the Al Baker “regime,” so here’s to hoping under Qatar Airways’ new CEO, things change a bit… </p>
<p><strong>What do you make of this Qatar Airways and Josh Cahill situation?</strong></p>





 </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[“Yes” means “no”: The language of VCs (116 pts)]]></title>
            <link>https://jacobbartlett.substack.com/p/yes-actually-means-no-the-curious</link>
            <guid>38677251</guid>
            <pubDate>Sun, 17 Dec 2023 22:53:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jacobbartlett.substack.com/p/yes-actually-means-no-the-curious">https://jacobbartlett.substack.com/p/yes-actually-means-no-the-curious</a>, See on <a href="https://news.ycombinator.com/item?id=38677251">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>Everyone wants investment from venture capitalists.</p><p>It’s the validation that catapults you onto the cover of Forbes. Your seat at the pantheon of famed tech bros like Adam Neumann, Elizabeth Holmes, and Sam Bankman-Fried. </p><p>VCs can be a curious bunch, however. They have a special secret language — a dialect that’s borderline incomprehensible to the uninitiated.</p><p>For instance, much like the Inuit people, who have many words for snow, were you aware that venture capitalists have more than 400 words for “no”?</p><p><span>Language betrays purpose. In Orwell’s 1984, </span><em>Newspeak</em><span> was created to narrow the range of thought and, ultimately, make dissidence impossible. In the same vein, </span><em>VCspeak</em><span> has evolved to serve one key tenet:</span></p><p><em><span>“Never say no to a founder, </span><strong>just in case</strong><span>.”</span></em></p><blockquote><p><em><span>If you don’t feel like subscribing, please </span><a href="https://twitter.com/jacobs_handle" rel="">follow me on X/Twitter</a><span> if you like this post!</span></em></p></blockquote><p>VCs have no incentive to sour their relationship with you. What if you become the next Zuck and remember the deal associate who let you down gently?</p><p><span>You’ll never hear the true reason for turning you down </span><em>this time</em><span>. For a pre-seed or seed round, this is either:</span></p><ol><li><p>They don’t believe in the management team.</p></li><li><p>They don’t believe in the market opportunity.</p></li></ol><p><span>Today, I’m telling the story of my multitude of VC rejections. I’ll impart my hard-earned knowledge of </span><em>VCspeak</em><span> and explain what VCs are saying when, really, they just mean “no”. Finally, I’ll close with my top 10 survival tips for when you’re out fundraising.</span></p><p><span>My first real startup, Fixr — think Uber for car repairs — was a mess from the start. When I joined up as ‘CTO’, there were endemic power struggles between the existing founders and a chronic case of “if we build one more feature, </span><em>then</em><span> the users will flock to us”.</span></p><p>Through a Herculean effort of LinkedIn networking, we set up some calls with very junior VCs. None were brave enough to rip off the poisoned shirt we had woven for ourselves:</p><p><em>“Talk to us again when you have traction.”</em></p><p><span>In our first-time founder naïveté, we took this to heart. Our startup lived or died on whether we had </span><em>traction</em><span>.</span></p><p>Screw talking to users. Screw validation. We needed to get our product out of stealth mode and into the open. First, we’d take the market by storm. Then, we’d hoover up those promised VC dollars.</p><p><span>For our two-sided marketplace startup, this meant Android and iOS apps for customers, and </span><em>another pair of apps</em><span> for mechanics. 6 months later, we released our fully-fledged MVP to a deafening silence from customers, mechanics, and investors.</span></p><p>The junior VCs didn’t speak to us again.</p><p>My second startup fared far better.</p><p>Carbn was an app to help people develop green habits. It was a marked improvement — we were only building the iOS app and barrelled from customer validation to MVP in our first 4 months!</p><p>From here, we had a few months of bootstrapping left in us and were desperate for our first capital injection. Unfortunately, we heard another classic refrain from the venture capitalists:</p><p><em>“We think you’re too early for us, speak to us when you raise your next round.”</em></p><p>Admittedly, that was fair when we had zero revenue and low-thousands of users. We weren’t fazed — we approached various angel syndicates, accelerators and exhausted all the goodwill we could muster via our networks.</p><p><span>Through the complementary magic of a </span><a href="https://apps.apple.com/app/carbn-cut-carbon-footprint/id1533681322" rel="">strong early app store rating</a><span> and my socially hypercompetent cofounder, we landed </span><a href="https://www.crunchbase.com/organization/carbn" rel="">a cool £200k in angel funding</a><span>.</span></p><p>With a business bank account brimming with angel funding, we went on a hiring spree and maintained a consistent 10% week-on-week DAU growth — nothing to sniff at. But was it enough?</p><p>We were wrestling with a critical strategic issue: Should we remain a B2C consumer habits app or position ourselves in the B2B space as a climate-conscious employee perk? Having dipped our toes into both approaches, we’d hit our pre-seed growth targets and began to reach out to VCs for our next round.</p><p><span>We encountered the customary mix of ghosting, half-arsed lukewarm interest and genuine leads. This time, as well as the classic </span><em>“come to us with more traction”</em><span> and </span><em>“talk to us in your next round,”</em><span> we had some bigger fish biting for a second meeting.</span></p><p><span>We’d been speaking with a partner at a top-tier Silicon Valley VC. He was enthusiastic about Carbn, and asked us to present at their Monday Partner Meeting. Our presentation and subsequent conversation lasted over 2 hours. They told us they saw us becoming </span><em>the</em><span> climate B2C play. We felt it went well.</span></p><p>February 2022.</p><p>I was suffering the worst norovirus of my life. I’d been playing Elden Ring for 10 hours straight. I couldn’t tell whether I was dying more in-game or in real life.</p><p>We got the call from the VC.</p><p><em>“We like you, we want to invest, you should start hiring now.”</em></p><p>I was in too much pain to feel any satisfaction. Surely this was the VC funding I’d been waiting for my whole life? Through my fever delirium, I saw myself on the cover of Forbes, high-fiving the Fyre Festival guy.</p><p><span>My cofounder and I switched gears </span><em>hard</em><span> to focus on landing top-tier talent for our nascent engineering, product, and growth departments. In the meantime, we sent draft terms to our VC and… crickets.</span></p><p><span>The markets were spooked by some </span><a href="https://www.bbc.co.uk/news/live/world-europe-60454795" rel="">geopolitical shenanigans</a><span>, and VCs were suddenly hesitant to dole out cash at 2021 valuations. As our other leads ran cold, we held onto the hope that our deal was still in the bag.</span></p><p>Cue months of umming and ahhing from our friend in Silicon Valley. After their white-hot conviction about the B2C opportunity for a climate action app, our VC developed a newfound passion for the comfortable cashflow prospects of a B2B proposition.</p><p>Which we would have been fine with had the markets not been deteriorating further, and our borderline deal was consequently losing its lustre.</p><p>It eventually became clear that our seed round wasn’t happening. My cofounder and I agreed to call it a day.</p><p><span>Let’s delve deeper into </span><em>VCspeak</em><span>. We’ll analyze the VC rejections I’ve encountered and try to find the true meaning behind the words.</span></p><blockquote><p><strong>When they say:</strong><em><strong> </strong><span>“Talk to us again when you have traction.”</span></em></p><p><strong>They really mean: </strong><em>“If you prove there is a market opportunity and that you can execute as a management team, then we might consider you. But because I don’t believe either of those things will happen, I will not be taking a risk on you”.</em></p></blockquote><p>Passing on Fixr was the correct choice. In retrospect, there was no chance our management team was capable of successfully creating a two-sided marketplace, which is widely regarded as the hardest kind of startup to build. Don’t get me started on the abysmal decision to build 4 native apps on 2 platforms before we had a single  transaction.</p><blockquote><p><strong>When they say: </strong><em>“We think you’re too early for us, speak to us when you raise your next round.”</em></p><p><strong>They really mean: </strong><em>“We’re an early-stage fund, but generally speaking, we only invest pre-seed in second-time founders and people with executive-level industry experience in this sector. We like the opportunity but don’t believe your management team is the right one in which to invest.”</em></p></blockquote><p>Again, this is a pretty reasonable stance. While VC is inherently risky, these risks can be mitigated by backing management teams with a proven track record. Naturally, the riskiest pre-seed dollars are allocated accordingly to increase the chances of achieving a 3x ‘venture rate of return’ for a given VC fund.</p><blockquote><p><strong>When they say: </strong><em>“We like you, we want to invest, you should start hiring now.”</em></p><p><strong>They really mean: </strong><em>“We do really like your vibe as a founding team. We see your potential. We do genuinely want to invest, and want to set you up to move quickly once the deal closes. Please note however that this is neither a term sheet or a wire transfer. Perhaps we want more time to evaluate other options in the space. We lack conviction.”</em></p></blockquote><p>Venture capitalists are a famously fickle bunch. One day, they can be keen as a bean on your lean, mean, consumer business. The next day, Putin invades Ukraine and you’re a borderline B2B prospect that needs to show unit economics.</p><p>My story is typical of my fellow failed founders — some gaffes in our early ventures as we learn the ropes, a few promising steps as we gain entrepreneurial experience, and a veritable funding brick wall in 2022.</p><p>Here are some quick and dirty thoughts to keep your head screwed on right as you look for startup funding:</p><ol><li><p>If a VC gives a reason for turning you down, don’t necessarily take it at face value.</p></li><li><p>Be self-critical about your management and the market. Your team or the opportunity might just not be right.</p></li><li><p><span>For your first round, it’s </span><em>much</em><span> easier to convince a single angel investor than a roomful of VC partners.</span></p></li><li><p>When you land a lead angel investor, the rest become easier to find.</p></li><li><p>If you aren’t a big dog in your industry or haven’t had a startup exit before, the bar for pre-seed VC investment is pretty high.</p></li><li><p>In a VC firm, you need an influential internal champion who will fight hard for your startup.</p></li><li><p>Don’t play coy with VCs. Don’t stop talking to them, and don’t let a term sheet get away before the market turns.</p></li><li><p>“Yes” doesn’t always mean “yes”. Your raise isn’t over until a wire transfer hits your account.</p></li><li><p>VC funding is cool and all, but essentially, it is a waste of time if you haven’t validated your market: bootstrapping to get paying customers quickly is the most effective tactic available.</p></li><li><p><span>In all honesty, you don’t </span><em>want </em><span>to take external funding unless you have intensely strong user growth; and being unable to hire becomes your bottleneck.</span></p></li></ol><p><span>Have you encountered any rejections we could add to our </span><em>VCspeak</em><span> phrasebook? Do you have any hard-earned wisdom to append to the survival guide? Let us know in the comments!</span></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[0% of the phrases of the original Wikipedia "Ship of Theseus" article remain (218 pts)]]></title>
            <link>https://twitter.com/depthsofwiki/status/1735800801455419697</link>
            <guid>38677124</guid>
            <pubDate>Sun, 17 Dec 2023 22:32:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/depthsofwiki/status/1735800801455419697">https://twitter.com/depthsofwiki/status/1735800801455419697</a>, See on <a href="https://news.ycombinator.com/item?id=38677124">Hacker News</a></p>
Couldn't get https://twitter.com/depthsofwiki/status/1735800801455419697: Error: Request failed with status code 400]]></description>
        </item>
        <item>
            <title><![CDATA[How Africans are changing French, one joke, rap and book at a time (115 pts)]]></title>
            <link>https://www.nytimes.com/2023/12/12/world/africa/africa-french-language.html</link>
            <guid>38675836</guid>
            <pubDate>Sun, 17 Dec 2023 19:58:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2023/12/12/world/africa/africa-french-language.html">https://www.nytimes.com/2023/12/12/world/africa/africa-french-language.html</a>, See on <a href="https://news.ycombinator.com/item?id=38675836">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2023/12/12/world/africa/africa-french-language.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[3M knew its chemicals were harmful decades ago, but didn't tell the public (391 pts)]]></title>
            <link>https://minnesotareformer.com/2022/12/15/toxic-3m-knew-its-chemicals-were-harmful-decades-ago-but-didnt-tell-the-public-government/</link>
            <guid>38675616</guid>
            <pubDate>Sun, 17 Dec 2023 19:30:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://minnesotareformer.com/2022/12/15/toxic-3m-knew-its-chemicals-were-harmful-decades-ago-but-didnt-tell-the-public-government/">https://minnesotareformer.com/2022/12/15/toxic-3m-knew-its-chemicals-were-harmful-decades-ago-but-didnt-tell-the-public-government/</a>, See on <a href="https://news.ycombinator.com/item?id=38675616">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="dataContent">
                                    <p><em>This is part 2 of 2. <a target="_blank" href="https://minnesotareformer.com/2022/12/14/there-must-be-something-in-the-water/">Read part 1</a>, about East Metro residents who wonder if 3M chemicals made them sick.&nbsp;</em></p>
<p><span>3M toxicologist Richard Purdy did a</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1523.pdf"> <span>study</span></a><span> in 1998 to see whether any of the company’s perfluorochemicals showed up in the blood of eagles and albatrosses.</span></p>
<p><span>That seemed unlikely, given the birds’ diet consists mostly of fish. So Purdy was surprised and disturbed when he found levels in their blood similar to those found in human blood. It even showed up in bald eagle nestlings whose only food was fish their parents fed them from remote lakes.</span></p>
<p><span>That indicated what Purdy</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1001.pdf"> <span>later called</span></a><span> “widespread environmental contamination” — the likelihood the manmade, toxic chemicals were moving through the food chain and accumulating in animals.</span></p>
<p><span>Purdy warned 3M that if wild birds’ blood contained the chemicals, then fish-eating mammals — like otters, mink, porpoise and seals —&nbsp;could have it, too. A study of rats found they had significant levels of a 3M chemical in their livers, likely from eating fishmeal.&nbsp;</span></p>
<p><span>He told company officials in an</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1533.pdf"> <span>email</span></a><span> there was a significant risk of ecological harm, which should be reported to the EPA.</span></p>
<p><span>In response, 3M managers dispersed the team collecting the data, Purdy alleged.</span></p>
<p><span>Purdy resigned in 1999 and sent his </span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1001.pdf"><span>resignation letter</span></a><span> to the EPA, informing them that while 3M had disclosed to the EPA that a chemical called PFOS “had been found in the blood of animals,” it didn’t mention that it was found in the blood of eaglets.</span></p>
<p><span>The EPA began investigating the chemicals that year. But by then, 3M had reaped billions of dollars in profits from chemicals that the company had been warned were harming the environment and risking human health.&nbsp;</span></p>
<p><span>The per-and polyfluoroalkyl substances (PFAS) had spread — through groundwater and products like Scotchgard stain repellent,</span> <span>Teflon cookware, food wrapping and fire retardant — and were showing up in the blood of people and animals in every corner of the world. They were in nearly every living thing, from house dust to human blood, in wildlife in the Arctic circle and drinking water, rivers, streams and breast milk.&nbsp;</span></p>
<p><span>Purdy’s warnings were clear, as revealed by former Attorney General Attorney General Lori Swanson, who sued 3M in 2010, alleging the company failed for decades to report that its chemicals could be toxic to humans, animals and the environment, keeping information from regulators and scientists to protect its lucrative revenue stream.&nbsp;</span></p>
<p><span>The morning the case was set to go to trial in 2018, after 22 hours of negotiation, 3M and the state settled. 3M agreed to pay $850 million to help provide Minnesotans clean drinking water.&nbsp;</span></p>
<p><span>The settlement with Minnesota is the third largest natural resource damage settlement in U.S. history, behind the Deepwater Horizon and Exxon Valdez oil spills.&nbsp;</span></p>
<p><span>But it amounted to just 2.6% of 3M’s nearly $33 billion in revenue in 2018.&nbsp;</span></p>
<p><span>The company admitted nothing, and maintains to this day that its chemicals have no adverse health or environmental consequences.&nbsp;</span></p>
<p><span>3M spokesman Grant Thompson said in an email that 3M’s position reflects the weight of&nbsp; </span><span>scientific evidence from decades of research showing exposure to PFOA and PFOS at current and historical levels found in people and the environment has not been shown to cause adverse health effects.</span></p>
<blockquote data-secret="Vvx1EzNtcF"><p><a target="_blank" href="https://minnesotareformer.com/2022/12/14/there-must-be-something-in-the-water/">There must be something in the water</a></p></blockquote>

<p><span>Still, 3M’s settlement with the state of Minnesota is likely the beginning — not the end — of the company’s legal, regulatory and political challenges stemming from both the invention and dumping of the chemicals. 3M and other companies that made the chemicals may have to pay out billions for the damage they caused the environment and people.&nbsp;</span></p>
<p><span>During a 2019 congressional hearing, U.S. Rep. Harley Rouda of California</span><a target="_blank" href="https://www.govinfo.gov/content/pkg/CHRG-116hhrg37952/html/CHRG-116hhrg37952.htm"> <span>called</span></a><span> the contamination of Americans’ drinking water, groundwater, air and food supplies a national emergency.</span></p>
<p><span>“These companies got away with poisoning people for more than a half century,” Rouda said.</span></p>
<p><span>In August, the EPA proposed designating two perfluorochemicals as hazardous substances under the Superfund law, which would spark federal cleanup standards and could put chemical companies on the hook for billions in cleanup costs.&nbsp;</span></p>
<p><span>The EPA also published new drinking water health advisory levels for several perfluorochemical compounds and plans to propose a national drinking water perfluorochemical regulation soon.</span></p>
<p><span>A federal judge in Charleston, S.C., also dealt the company a blow in September, </span><a target="_blank" href="https://news.bloomberglaw.com/environment-and-energy/3m-loses-government-contractor-defense-in-pfas-litigation"><span>denying</span></a><span> 3M’s request for government contractor immunity in a mass tort case alleging 3M and other companies’ firefighting foam are linked to health problems.</span></p>
<p><span>Judge Richard ​​Gergel said 3M conducted over 1,000 studies of perfluorochemicals’ effect on human health and the environment, the results of which should have been disclosed to the EPA.&nbsp;</span></p>
<p><span>He wrote that 3M and other chemical manufacturers “had significantly greater knowledge than the government about the properties and risks associated with their products and knowingly withheld highly material information from the government.”</span></p>
<p><span>Closer to 3M’s Minnesota headquarters, some sickened residents in the East Metro — where groundwater was contaminated with 3M chemicals — say they’re working with attorneys on a lawsuit.&nbsp;</span></p>
<p><span>David Sunding, a University of California Berkeley professor, published a 2017 report saying Washington County residents who lived in areas where groundwater was contaminated with 3M chemicals had elevated rates of bladder, breast, kidney and prostate cancers, as well as leukemia and non-Hodgkin’s lymphoma.</span></p>
<p><span>3M disputes that, pointing to a 2018 Minnesota health department </span><a target="_blank" href="https://www.health.state.mn.us/data/mcrs/docs/rpteastmetro.pdf"><span>report</span></a><span> showing that the overall cancer rate in Washington County was “virtually identical” to the statewide average, despite chemical contamination.&nbsp;&nbsp;</span></p>
<p><span>Given the stakes of the litigation, the future of the company — which employs 7,000 people at its massive Maplewood campus and about 13,500 statewide — will hinge in part on how it confronts its own history with these toxic chemicals.&nbsp;</span></p>
<p><span>A recent </span><i><span>Bloomberg</span></i><span> analysis </span><a target="_blank" href="https://news.bloomberglaw.com/litigation/3m-combat-earplug-fight-at-crossroads-as-court-strategies-falter"><span>estimated</span></a><span> 3M liabilities for the mass torts case and another over defective earplugs&nbsp;could reach $30 billion, or nearly half of market cap.&nbsp;</span></p>
    <h4>
What they knew, when they knew it
</h4>

	
<p><span>A key problem in any 3M defense: Despite the flurry of recent legal, regulatory and political activity, the chemicals’ dangers have been known —&nbsp;and known to 3M —&nbsp;for decades.&nbsp;</span></p>
<p><span>As early as the 1950s, 3M and DuPont scientists began discovering that the chemicals were accumulating in the bodies of humans and animals.&nbsp;</span></p>
<p><span>After compiling 27 million pages of documents and deposing about 200 witnesses in seven years, Minnesota’s former attorney general, Swanson, didn’t just walk away after settling with 3M. She released </span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/StatesExhibits.asp"><span>thousands of internal 3M documents</span></a><span>.&nbsp;</span></p>
<p><span>The </span><i><span>Reformer</span></i><span> reviewed the documents, which</span><span> show that company officials were repeatedly warned that the chemicals were accumulating in the environment and detected in the blood of humans and animals, while showing worrisome signs of toxicity.&nbsp;</span></p>
<p><span>Time and again, the company found reasons to delay a full accounting to government regulators, Minnesota communities, and even its own workers. Like tobacco companies’ tardy admission about its cancer-causing drug and the NFL’s approach to concussions, 3M </span><span>ignored, delayed, minimized and obscured research that raised red flags about the chemicals.&nbsp;</span></p>
<p><span>Internal 3M documents show:&nbsp;</span></p>
<ul>
<li><span> In the 1950s, 3M animal studies consistently found its PFAS chemicals were toxic.</span></li>
<li><span> By the early 1960s, 3M knew the chemicals didn’t degrade in the environment.</span></li>
<li><span> 3M knew by the 1970s its chemicals were widely present in the blood of the general U.S. population.</span></li>
<li><span> A 1970</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1083.pdf"> <span>study</span></a><span> of fish had to be abandoned “to avoid severe stream pollution” and because all the fish died. After being exposed to a chemical, the fish couldn’t stay upright and kept crashing into the fish tank and dying.</span></li>
<li><span> By 1976, 3M knew the chemicals were in its plant workers’ blood at higher levels than normal.</span></li>
<li><span> A study of a chemical’s effect on 20 rhesus monkeys in 1978</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1193.pdf"> <span>had to be aborted</span></a><span> after 20 days because all the exposed monkeys&nbsp; died.</span></li>
<li><span> In 1979, a 3M scientist warned that perfluorochemicals posed a cancer risk because they are “known to persist for a long time in the body and thereby give long-term chronic exposure.”</span></li>
<li><span> In 1979, 3M lawyers</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX2534.pdf"> <span>advised</span></a><span> the company to conceal a 3M chemical compound found in human blood.</span></li>
<li><span> In 1983, 3M scientists</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1282.pdf"> <span>concluded</span></a><span> that concerns about its chemicals “give rise to legitimate questions about the persistence, accumulation potential, and ecotoxicity of fluorochemicals in the environment.”</span></li>
<li><span>Purdy wrote in his </span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1001.pdf"><span>resignation letter</span></a><span> that in the 1990s, 3M told researchers not to write down their thoughts or have email discussions because of how their “speculations” might be viewed in legal discovery.</span></li>
<li><span> 3M told employees to mark documents as “attorney-client privileged” regardless of whether attorneys were involved, the state alleged, and minutes of meetings were edited to omit references to health hazards.</span></li>
<li><span> In 1997, 3M gave DuPont a “material safety data sheet” — which lays out potential hazards — for a chemical. It read, “Warning: contains a chemical which can cause cancer,” citing 1983 and 1993 studies by 3M and DuPont. But 3M removed the label that same year and continued to sell the products for decades without warning.</span></li>
</ul>
<p><span>Thompson, the 3M spokesman, said the documents released by Swanson portray an “incomplete and misleading story that distorts the full record regarding 3M’s PFAS stewardship and who we are as a company.”&nbsp;</span></p>
<p><span>He said 3M disclosed many studies to the EPA over the course of decades, including on the chemicals’ toxicity and “the materials produced and discussed with EPA addressed relevant information and issues.”&nbsp;</span></p>
	
    

	
    <h4>‘The wildest hellcat’</h4>

	
<p><span>3M’s man-made, toxic chemicals can be traced back to World War II, and the U.S. race to develop atomic weapons in the top-secret Manhattan Project.</span></p>
<p><span>Scientists used fluorine gas to separate uranium, and discovered that when fluorine weds with carbon, the bonds are almost impossible to break.</span></p>
<p><span>After the war, some of the Manhattan Project scientists were hired by the Minnesota Mining and Manufacturing Company (3M), which bought the patent to develop perfluorochemicals, according to a 3M</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1365.pdf"> <span>book</span></a><span> celebrating the company’s history of chemical engineering, called “A Chemical History of 3M.”&nbsp;</span></p>
<p><span>Figuring out how to handle fluorine was a major hurdle for the scientists.</span></p>
<p><span>“In its pure, uncontrolled state — fortunately never found in nature — it is one of the most active, most dangerous elements known to man,” the book says. “The greenish-yellow gas will burn steel, water and even asbestos, which earned it a nickname — the wildest hellcat. Strangely, its wildness contributes to fluorine’s unique stability when it is combined with certain compounds.”</span></p>
<p><span>When combined with carbon, the resulting fluorochemical can repel water and oil and withstand fire, which had obvious commercial potential.</span></p>
<p><span>3M began manufacturing chemicals in Minnesota in the 1950s, and for the next 50 years they were used to make stain repellents,</span> <span>Teflon and other waterproof and fireproof products.</span></p>
<figure id="attachment_19468"><a href="http://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.05.16-PM.png" target="_blank" data-slb-active="1" data-slb-asset="355035654" data-slb-internal="0" data-slb-group="19467"><img decoding="async" src="http://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.05.16-PM.png" alt="" width="818" height="570" srcset="https://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.05.16-PM.png 818w, https://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.05.16-PM-300x209.png 300w, https://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.05.16-PM-768x535.png 768w" sizes="(max-width: 818px) 100vw, 818px"></a><figcaption><i></i>  This 1961 3M Scotchgard ad that ran in LIFE magazine was going to be an exhibit in the state’s lawsuit against 3M. Courtesy state of Minnesota</figcaption></figure>
<p><span>By the 1990s, the chemicals</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1386.pdf"> <span>were in many consumer products</span></a><span>, such as window cleaners, floor waxes and polishes, fabric and leather protective coatings and carpet and upholstery treatments.</span></p>
<p><span>The products were a huge success, and the company was making almost a half a billion dollars per year off them by 2000, when it began —&nbsp;at the EPA’s urging —&nbsp;to phase out production of the chemical used to make Scotchgard. Production of other chemicals continued.&nbsp;</span></p>
<p><span>But the chemicals wouldn’t go away easily: They don’t break down in the environment, and they accumulate in the human body.</span></p>
    <h4>
3M employee: We pled ignorance
</h4>

	
<p><span>In 1975, a Florida professor called 3M after he and two colleagues discovered a fluorine chemical in human blood samples from Texas and New York.</span></p>
<p><span>The scientists suspected the source might be 3M chemicals used in household items such as Teflon cookware and Scotchgard.</span></p>
<p><span>Donald Taves, a researcher at the University of Rochester, first reported in the scientific journal </span><i><span>Nature</span></i><span> in 1968 that the general population had been exposed to the compounds. Then Taves discovered his own blood contained it, according to a 3M </span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1118.pdf"><span>document</span></a><span> marked “confidential,” obtained in the Minnesota attorney general’s lawsuit.</span></p>
<p><span>Taves was working with Warren Guy and Wallace Brey at the University of Florida on a research paper.&nbsp;</span></p>
<p><span>3M chemist G.H. Crawford took the phone call from Taves, and admitted nothing. He</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1118.pdf"> <span>wrote</span></a><span> in a confidential interoffice memo: “We (pleaded) ignorance but advised him that Scotchgard was a polymeric material not a F.C. acid.”</span></p>
<p><span>(In fact, by this point, the company knew its chemicals accumulated in the human body and were toxic, Swanson </span><a target="_blank" href="https://docs.house.gov/meetings/GO/GO28/20190910/109902/HHRG-116-GO28-Wstate-SwansonL-20190910.pdf"><span>told</span></a><span> a congressional committee. Moreover, Swanson added, 3M refused to identify the chemicals in its products, which for a generation thwarted the scientific community’s understanding of their health impacts.)&nbsp;</span></p>
<figure id="attachment_19466"><a href="http://minnesotareformer.com/wp-content/uploads/2022/12/3m-nocrop.jpg" target="_blank" data-slb-active="1" data-slb-asset="1522226148" data-slb-internal="0" data-slb-group="19467"><img decoding="async" loading="lazy" src="http://minnesotareformer.com/wp-content/uploads/2022/12/3m-nocrop-1024x575.jpg" alt="" width="1024" height="575" srcset="https://minnesotareformer.com/wp-content/uploads/2022/12/3m-nocrop-1024x575.jpg 1024w, https://minnesotareformer.com/wp-content/uploads/2022/12/3m-nocrop-300x168.jpg 300w, https://minnesotareformer.com/wp-content/uploads/2022/12/3m-nocrop-768x431.jpg 768w, https://minnesotareformer.com/wp-content/uploads/2022/12/3m-nocrop-1536x863.jpg 1536w, https://minnesotareformer.com/wp-content/uploads/2022/12/3m-nocrop.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></a><figcaption><i></i>  3M still manufactures perfluorochemicals in Cottage Grove, as well as Cordova, Ill., Decatur, Ala., Zwijndrecht, Belgium, and Gendorf, Germany. Photo by Chad Davis</figcaption></figure>
<p><span>Crawford, the 3M scientist, suggested Guy get blood samples from “uncivilized areas” such as New Guinea “where they don’t use too much Teflon cookware or Scotchgard.”</span></p>
<p><span>He told his colleagues that the chemical 3M sold to DuPont to make Teflon cookware was the “least unlikely” explanation, but he didn’t tell Guy that. Crawford wrote that he “adopted a position of scientific curiosity and desire to assist in any way possible” and told Guy that 3M’s people might be able to “clarify” his study findings.&nbsp;</span></p>
<p><span>Another</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX2771.pdf"> <span>internal document</span></a><span> shows Guy, the university researcher, also talked to a 3M employee identified as J.D. LaZerte about his quest to track down the source of chemicals in human blood.</span></p>
<p><span>LaZerte</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX2771.pdf"> <span>wrote</span></a><span> in an internal document that he told Guy not to speculate.</span></p>
<p><span>Taves, Guy and Brey later</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1121.pdf"> <span>discovered</span></a><span> plasma from blood banks in five cities suggested “widespread contamination of human tissues with trace amounts of organic fluorocompounds derived from commercial products” such as floor waxes, wax paper, leather and fabric conditioning agents.</span></p>
<p><span>After getting the phone calls from researchers, 3M began analyzing its fluorine compounds. Within weeks,</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1123.pdf"><span> they found a compound that was </span><span>a likely match</span></a><span>.&nbsp;</span></p>
<p><span>By late 1975, 3M sent employees to see Guy and Taves at the University of Rochester, where they agreed to try to isolate and identify fluorochemicals in blood.</span></p>
<p><span>In 1976, the company began sampling employees’ blood.&nbsp;</span></p>
<p><span>Tests </span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1144.pdf"><span>show</span></a><span>ed</span><span> workers at 3M’s Cottage Grove plant called Chemolite had up to 1,000 times the normal amount of fluorochemicals in their blood.</span></p>
<p><span>In plant after plant, elevated levels were found, from Decatur, Alabama, to Antwerp, Belgium.</span></p>
<p><span>Gergel, the federal judge in South Carolina, wrote in his recent ruling that although 3M helped Guy and Taves identify the compound found in blood, the company</span><span> told no one else outside 3M for nearly a <a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1691.pdf">quarter century</a>, </span><span>despite the company’s legal duty to alert the EPA about potential harm to human health and the environment.</span></p>
<p><span>The judge cited a potential culprit: 3M lawyers, who urged 3M’s lab not to release the true identity of the compound (PFOS), according to an </span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX2534.pdf"><span>internal 3M documen</span></a><span>t.</span></p>
<p><span>Gergel said it would be reasonable to infer that the company knowingly withheld information that PFOS was in the blood of the general population and sought to discredit independent scientific work that would have disclosed this.</span></p>
<p><span>“3M did more than simply stay silent despite the company’s knowledge that the mystery compound was PFOS,” Gergel wrote.&nbsp;</span></p>
<p><span>The company went even further in its effort to obfuscate, the judge charged. In 1981, an author of an 1976 internal 3M report that confirmed that the unidentified chemical was in fact PFOS published an article in the same scientific journal as Guy and Taves stating that the mystery compound was not man-made but was a naturally occurring substance.&nbsp;</span></p>
    <h4>
DuPont asks 3M for ‘defensive information’
</h4>

	
<p><span>One of 3M’s biggest customers was DuPont, for which it produced chemicals to make Teflon products.</span></p>
<p><span>But by late 1975, DuPont was concerned about the possible toxic effects of Teflon and asked 3M for “defensive information” after a rat study found “sub-acute toxicity,” according to a</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1124.pdf"> <span>3M document</span></a><span>.&nbsp;</span></p>
<p><span>After a 1979 meeting between 3M and DuPont, a 3M committee decided its data on the chemicals in workers’ blood samples wasn’t important enough to notify the EPA. Minutes from the meeting said DuPont asked if 3M had done any “chronic studies” on fluorochemicals or planned any in the future. The answer was no, they wouldn’t do such studies unless forced to by regulators.</span></p>
<p><span>3M told DuPont that because they’d seen no adverse human health effects and no widespread potential for the chemicals to accumulate, they did not need to notify the EPA, according to a report by Philippe Grandjean, a Dutch scientist who provided expert testimony for the state of Minnesota in its case against 3M.</span></p>
<p><span>“3M either closed its eyes to the evidence, or chose purposefully not to find it, or being generous to 3M, it seems possible that 3M may have mistakenly relied on the absence of evidence, despite the old dictum that ‘the absence of evidence is not evidence of absence,’ which later became famous in U.S. politics,” Grandjean wrote.</span></p>
<figure id="attachment_19469"><a href="http://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.00.50-PM.png" target="_blank" data-slb-active="1" data-slb-asset="31343240" data-slb-internal="0" data-slb-group="19467"><img decoding="async" loading="lazy" src="http://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.00.50-PM.png" alt="" width="462" height="596" srcset="https://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.00.50-PM.png 462w, https://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.00.50-PM-233x300.png 233w" sizes="(max-width: 462px) 100vw, 462px"></a><figcaption><i></i>  This 1961 Scotchgard ad in LIFE magazine was going to be an exhibit in the state’s lawsuit against 3M. Courtesy state of Minnesota</figcaption></figure>
    <h4>
Employees notified of chemicals in blood
&nbsp;
</h4>

	
<p><span>In 1978, 3M began notifying chemical workers that trace amounts of chemicals were found in the blood of employees at the Cottage Grove, Decatur and Cordova</span> <span>plants.</span></p>
<p><span>“There did not appear to be any significant grouping of abnormalities,” according to</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1168.pdf"><span> confidential </span><span>meeting minutes </span></a><span>of 3M’s Fluorochemicals Technical Review Committee.</span></p>
<p><span>The committee discussed the potential carcinogenicity of the chemicals, and whether to notify workers and “the appropriate government agency,” given studies showing a PFAS compound was toxic in animals and a 1979</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1199.pdf"> <span>report on toxicity studies</span></a><span> on monkeys and rats found PFOS was “certainly more toxic than anticipated.”&nbsp;&nbsp;</span></p>
<p><span>But because there was “no evidence of ill effects,” the committee decided it didn’t constitute a substantial risk based on EPA guidelines pertaining to the Toxic Substances Control Act, which regulates chemicals.</span></p>
<p><span>The committee decided to keep exposure to all fluorochemicals to a minimum in all factory operations, and look into monitoring employee urine.</span></p>
<p><span>But it was becoming increasingly clear that several of the chemicals were toxic. Soon after,</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1179.pdf"> <span>a 3M study</span></a><span> of two chemicals found they were “likely to persist in the environment for extended periods.”</span></p>
<p><span>“Because of the apparent persistence of these fluorochemicals in the body, the most important question remains possible long-term effects,” the report said.</span></p>
    <h4>
Prominent toxicologist warns ‘we could have a serious problem’
</h4>

	
<p><span>In the spring of 1979, 3M officials met at the Hilton Hotel in San Francisco to talk about their fluorochemical studies and the future.</span></p>
<p><span>They also heard from toxicologist Harold Hodge, a professor from the University of California, which</span><a target="_blank" href="https://oac.cdlib.org/view?docId=hb5f59n9gs&amp;doc.view=frames&amp;chunk.id=div00015&amp;toc.depth=1&amp;toc.id="> <span>dubbed</span></a><span> him “the dean of American toxicology.”</span></p>
<p><span>An epidemiology study was being done on 3,500 people, but so far there were no “unusual” causes of death.</span></p>
<p><span>Hodge recommended the company study the carcinogenicity of its chemicals.</span></p>
<p><span>A week later, Hodge asked that 3M add to the</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1204.pdf"> <span>meeting</span></a><span> minutes that it was of “utmost importance” that the company study whether a certain chemical was present in humans, at what level, and the degree of its persistence.</span></p>
<p><span>“If the levels are high and widespread and the half-life is long, we could have a serious problem,” Hodge warned.</span></p>
<p><span>Months later, 3M scientist M.T. Case</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1212.pdf"> <span>expressed similar concerns</span></a><span> — as “responsible 3M scientists” — about the lack of chronic toxicity data more than one year after the rat studies were done.</span></p>
<p><span>“I believe it is paramount to begin now an assessment of the potential (if any) of long term (carcinogenic) effects for these compounds which are known to persist for a long time in the body and thereby give long term chronic exposure,” Case wrote in a memo.</span></p>
    <h4>‘3M will likely be embarrassed’</h4>

	
<p><span>Other 3M employees were trying to persuade the company to come clean.</span></p>
<p><span>After a California company bought firefighting foam from 3M, it later learned that 3M chemist Eric Reiner told the client that the foam wasn’t biodegradable, contrary to 3M’s advertising claims.&nbsp;</span></p>
<p><span>Furious, the client </span><a target="_blank" href="https://docs.house.gov/meetings/GO/GO28/20190910/109902/HHRG-116-GO28-Wstate-SwansonL-20190910.pdf"><span>wrote</span></a><span> to 3M in 1988, demanding an explanation.</span></p>
<p><span>Reiner </span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1351.pdf"><span>implored</span></a><span> company officials to do tests on the biodegradability of the chemicals, calling out those responsible in an internal memo.</span></p>
<p><span>“I don’t think it is in 3M’s long-term interest to perpetuate the myth that these fluorochemical surfactants are biodegradable,” he wrote. “It is probable that this misconception will eventually be discovered, and when that happens, 3M will likely be embarrassed, and we and our customers may be fined and forced to immediately withdraw products from the market.”</span></p>
<p><span>Three years later, company officials were still debating whether to study the environmental effects of fluorochemicals. A</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1372.pdf"> <span>draft proposal</span></a><span> for a study of long-term effects noted the problem with previous studies was there’s rarely a single fluorochemical in the product, making generalizations difficult.</span></p>
<p><span>“Perhaps the most important conclusion from previous studies is the stability of fluorochemicals although stability is one of the most desirable properties fluorochemicals possess,” it said. “For many applications, from an environmental perspective, stability connotes persistence which can be the cause of concern especially when coupled with other properties… taken together, stability, the tendency to bioaccumulate, and biological activity are a potentially troublesome combination.”</span></p>
    <h4>
3M vice president delays reporting to EPA
</h4>

	
<p><span>By the mid-1990s, that “potentially troublesome combination” was becoming a threat to 3M.</span></p>
<p><span>The company’s Toxic Substances Control Act committee recommended in 1998 that 3M notify the EPA and FDA that the chemicals were widely found in human blood.</span></p>
<p><span>A “</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1488.pdf"><span>communications plan</span></a><span>” included steps for an “orderly exit” from the market.</span></p>
<p><span>But one month later, 3M Group Vice President Charles Reich</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1496.pdf"><span> told</span></a><span> the committee he decided instead to do a review with a “wider spectrum” of internal and external experts.&nbsp;</span></p>
<p><span>“I have concluded that 3M is not presently in possession of information that would be new to EPA and that reasonably supports a conclusion that suggests a substantial risk of injury to human health or the environment,” he wrote.</span></p>
<p><span>This, despite decades of research suggesting otherwise.</span></p>
<p><span>3M finally </span><a target="_blank" href="https://static.ewg.org/reports/2020/pfas-epa-timeline/1998_3M-Alerts-EPA.pdf"><span>notified</span></a><span> the EPA in May 1998 that a fluorochemical (PFOS) was found in the general population’s blood at “very low” levels. The company said its studies of 3M workers found “no adverse effects,” saying, “3M does not believe that any reasonable basis exists to conclude that PFOS presents a substantial risk of injury to health or the environment.”</span></p>
<p><span>Judge Gergel recently noted that despite those assurances, 3M’s manager of corporate </span><span>toxicology, John Butenhoff, urged 3M in 1998 to replace “PFOS-based chemistry as these compounds [are] VERY persistent and thus insidiously toxic.”&nbsp;</span></p>
<p><span>Butenhoff calculated a “safe” level of PFOS in human blood at a little more than 1 part per billion. But 3M’s </span><i><span>own studies</span></i><span> from roughly the same period found that PFOS concentrations in the blood of the general public were in the range of 30 parts per billion.&nbsp;</span></p>
<p><span>Gergel said Butenhoff’s findings were never reported to the EPA and were revealed only during discovery in the firefighting foam litigation.&nbsp;</span></p>
    <h4>
‘This chemical is more stable than many rocks’
</h4>

	
<p><span>By 1998, 3M toxicologist</span> <span>Richard Purdy, the one studying chemicals in eagles and albatrosses, was growing increasingly concerned about those studies of wild birds.&nbsp;</span></p>
<p><span>On Dec. 3, 1998, Purdy said in an</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1533.pdf"> <span>email</span></a><span> there was a significant risk of ecological harm, which should be reported to the EPA, warning, “The levels we are seeing in eagles and other biota is likely to climb each year.”</span></p>
<p><span>He wasn’t alone.</span></p>
<p><span>In March 1999, a 3M worker </span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1003.pdf"><span>emailed</span></a><span> several colleagues and 3M’s general counsel, Thomas J. DiPasquale, questioning why three months had passed since a committee had reviewed Purdy’s hypothesis on food chain contamination.</span></p>
<p><span>DiPasquale wasn’t in a hurry, though.</span></p>
<p><span>“I’m not sure there is a need to support or refute the hypothesis within any particular time frame,” he replied.</span></p>
<p><span>Purdy, who was on the email chain,</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1003.pdf"> <span>retorted</span></a><span>: “Plan! That is the same stalling technique you have been using for the last year.”</span></p>
<p><span>“There is a high probability that PFOS is killing marine mammals and you want another plan when we could have had data to support the risk assessment long ago,” Purdy wrote. “You were given a plan in 1983. Again in the early 90s. And you authorized no testing.”</span></p>
<figure id="attachment_19470"><a href="http://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.03.07-PM.png" target="_blank" data-slb-active="1" data-slb-asset="1682286043" data-slb-internal="0" data-slb-group="19467"><img decoding="async" loading="lazy" src="http://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.03.07-PM.png" alt="" width="634" height="510" srcset="https://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.03.07-PM.png 634w, https://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.03.07-PM-300x241.png 300w" sizes="(max-width: 634px) 100vw, 634px"></a><figcaption><i></i>  This undated photograph showing open burning of drums in a landfill was an exhibit in the state lawsuit against 3M. Courtesy state of Minnesota.</figcaption></figure>
<p><span>Meanwhile, his preliminary research indicated adult eagles had 50 times as much PFOS in their plasma as the eaglets.</span></p>
<p><span>“For 20 years the division has been stalling the collection of data needed for evaluating the environmental impact of fluorochemicals,” Purdy wrote. “PFOS is the most onerous pollutant since PCB and you want to avoid collecting data that indicates that it is probably worse. I am outraged.”</span></p>
<p><span>Two days later, Purdy resigned, and forwarded his</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1001.pdf"> <span>resignation letter</span></a><span> to the EPA.</span></p>
<p><span>“I have continually met roadblocks, delays, and indecision. For weeks on end I have received assurances that my samples would be analyzed soon — never to see results. There are always excuses and little is accomplished,” he wrote.</span></p>
<p><span>3M continued to make the chemicals after Purdy warned they were spreading through the food chain and harming sea mammals.</span></p>
<p><span>“This chemical is more stable than many rocks,” he wrote. “And the chemicals the company is considering for replacement are just as stable and biologically available. The risk assessment I performed was simple, and not worst case.”</span></p>
<p><span>3M told the people working on the fluorochemical project not to write down their thoughts or have email discussions because of how their speculation could be viewed in potential litigation, Purdy alleged.</span></p>
<p><span>“For me it is unethical to be concerned with markets, legal defensibility and image over environmental safety,” he wrote.</span></p>
<p><span>Purdy did not respond to a request for comment, but his view of 3M’s behavior seemed to soften over time. In an interview with MPR from his Wisconsin farm in 2005, he spoke “with pride” about the company’s investment in science and chemicals.&nbsp;</span></p>
<p><span>“3M is like somebody who ran the stop sign, got through the stop sign, ‘Oh my God,’ and stopped,” he was quoted saying.</span></p>
    <h4>
3M begins working to ‘command the science’
</h4>

	
<p><span>With the EPA on notice, the agency pressured 3M to stop manufacturing the compound used in Scotchgard (PFOS) in the U.S. in 2000. Six years later, the EPA fined the company for not turning over hundreds of reports on the chemicals’ toxicity.</span></p>
<p><span>The EPA said 3M’s own data indicated its chemicals didn’t break down and could pose a long-term threat to human health and the environment.</span></p>
<p><span>Still, the Minnesota Pollution Control Agency didn’t begin investigating the chemicals for two years, according to</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX2004.pdf"> <span>MPR</span></a><span>, which reported that all the agency had on file for 3M’s Cottage Grove plant in 2001 was a press clipping headlined “Scotchgard sticks in the environment.”</span></p>
<p><span>Once 3M had finally alerted regulators, the company worked on a</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1587.pdf"> <span>communications plan</span></a><span>.&nbsp;</span></p>
<p><span>The first goal: “Protect and enhance 3M’s reputation.”</span></p>
<p><span>Indeed, its primary concern seemed to be controlling the narrative around the science. The plan included a list of “high-priority” candidates to be spokespersons for the company, including Michigan State University professor John Giesy, a 3M advisor on environmental studies. 3M employee Dale Bacon said he would gauge Giesy’s interest.</span></p>
<p><span>3M wanted to get scientific papers on their chemicals published before others, according to</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1740.pdf"> <span>internal emails</span></a><span>.</span></p>
<p><span>A 2003</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX2604.pdf"> <span>internal memo</span></a><span> showed 3M looking to fund outside research using 3M “grant” money, particularly with people who would be influential in risk assessment and “other science policy matters.”</span></p>
<p><span>Among their action items: Develop a list of 3M and “industry-preferred” nominees for science advisory panels.</span></p>
<p><span>Giesy was the ideal candidate. He was editor of more than half the academic journals about PFAS and considered an independent expert.&nbsp;</span></p>
<p><span>3M went on to pay Giesy to review and share studies with 3M before they were published, Minnesota alleged in its lawsuit against 3M.</span></p>
<p><span>It began when Giesy</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1740.pdf"> <span>emailed</span></a><span> 3M officials in August 2000 informing the company he had a draft manuscript ready and wanted to submit it to </span><i><span>Science</span></i><span> before others beat him to it.</span></p>
<p><span>“I think it is important to publish our work before theirs,” Giesy wrote. “Otherwise, it looks like we (ie 3M) was pressured into the investigations they have done and subsequent release of the data.”</span></p>
<p><span>A 3M official warned his colleagues that publishing the paper “could set off a chain reaction of speculation that could reopen the issue with the media and move it back to a health story; something up-to-now we have avoided.”</span></p>
<p><span>Instead, the 3M official wrote that the company should keep “our” scientific publications “in the right order as we had already agreed,” noting he presumed Giesy’s work was done under contract with 3M and was only publishable “if and when we agree.”</span></p>
<p><span>The official added, however: “We also can’t dilly dally around either. It will take a great deal of sensitivity and people skills to bring Dr. Giesy around to our thinking on this and to be sure he doesn’t misinterpret our position as trying to hide the winnie. We just want the winnie in the bun, complete with mustard and ketchup.”</span></p>
<p><span>3M went on to develop a campaign to “command the science” and create “defensive barriers to litigation,” the state alleged in its lawsuit, by selectively funding outside research and editing scientific papers before they were published.</span></p>
<p><span>“The company, unfortunately, engaged in a campaign to hide its own studies and to, in fact, shape the science through the funding of these other studies,” Swanson told Congress.</span></p>
<p><span>Giesy explained how it worked in a</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX2204.pdf"> <span>March 2008 email</span></a><span> to 3M Laboratory Manager William Reagen: He edited a lot of PFAS papers for scientific journals, but in his 3M billings, he listed the work as “literature searches” on timesheets “so that there was no paper trail to 3M.”</span></p>
<p><span>“Some journals will allow this, but others, for conflict of interest issues, will not allow an industry to review a paper about one of their products,” he wrote. “That is where I came in for Dale (Bacon, the 3M employee).”</span></p>
<p><span>Giesy</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX2209.pdf"> <span>said</span></a><span> in a later email “Dale (Bacon) had me doing things to keep a finger on the pulse of things going on around the world, especially to try to keep bad papers out of the literature.”</span></p>
<p><span>The state lawsuit alleged 3M paid Giesy at least $2 million, and that he had a net worth of about $20 million despite working at public universities most of his career.</span></p>
<p><span>3M</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX2773.pdf"> <span>records</span></a><span> show he was first paid by the company in 1993. Beginning in 1998, Entrix, Inc. — Giesy’s environmental consulting company — was paid nearly $1.7 million for his work through 2009, at a rate of $275 an hour, according to one</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX2102.pdf"> <span>billing</span></a><span>.</span></p>
<p><span>By 2008, the arrangement appeared to be ending. In an email, Giesy offered some closing words:&nbsp;</span></p>
<p><span>“My personal advise (sic) is that you want to keep ‘bad’ papers out of the literature, otherwise in litigation situations they can be a large obstacle to refute,” he wrote. “Judges seem to be of the opinion that if information is in the peer-reviewed, open literature, it is accurate.”</span></p>
<p><span>Giesy — who now works at the University of Saskatchewan — did not respond to multiple requests for comment, but in the past he has</span><a target="_blank" href="https://www.cbc.ca/news/canada/saskatoon/u-of-s-professor-denies-suppressing-toxic-pollution-research-for-3m-1.4554634"> <span>denied</span></a><span> any wrongdoing. He said he was only trying to keep mistakes out of the literature — and accused Swanson of trying to smear his reputation because he refused to be an expert for the state.</span></p>
<p><span>“The documents speak for themselves,” Swanson said in an interview.</span></p>
    <h4>
Goal: ‘Sell PFCs as long and as broadly as we can’
</h4>

	
<p><span>For more than a quarter century, 3M has known its fluorochemicals could have devastating consequences for the company’s long-term financial health.&nbsp;</span></p>
<p><span>A 1995 internal</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1445.pdf"> <span>strategic planning document</span></a><span> said “obstacle No. 1” to 3M’s major vision in its chemical business was “the persistence of fluorochemicals,” and “environmental, health, safety and regulatory issues and trends that threaten to limit our business.”</span></p>
<p><span>Among the “key actions” listed: “Continue to maintain regulatory approval to sell PFCs as long and as broadly as we can.”</span></p>
<p><span>It’s easy to understand why they were so committed to the chemicals, despite the massive risks: $500 million per year in revenue, year after year after year.&nbsp;</span></p>
<p><span>“Unfortunately, it succeeded for more than 50 years,” Swanson told Congress. “And now states and local governments around the nation are grappling with the consequences.”</span></p>
<p><span>To this day, 3M still manufactures perfluorochemicals in Cottage Grove, as well as Cordova, Ill., Decatur, Ala., Zwijndrecht, Belgium, and Gendorf, Germany.</span></p>
                                </div><div>
                                        
                                        <p>Our stories may be republished online or in print under Creative Commons license CC BY-NC-ND 4.0. We ask that you edit only for style or to shorten, provide proper attribution and link to our web site. Please see our republishing guidelines for use of photos and graphics.</p>
                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AMD's CDNA 3 Compute Architecture (164 pts)]]></title>
            <link>https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/</link>
            <guid>38675258</guid>
            <pubDate>Sun, 17 Dec 2023 18:51:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/">https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/</a>, See on <a href="https://news.ycombinator.com/item?id=38675258">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>AMD has a long history of vying for GPU compute market share. Ever since Nvidia got first dibs with their Tesla architecture, AMD has been playing catch up. Terascale 3 moved from VLIW5 to VLIW4 to improve execution unit utilization in compute workloads. GCN replaced Terascale and emphasized consistent performance for both GPGPU and graphics applications. Then, AMD diverged their GPU architecture development into separate CDNA and RDNA lines specialized for compute and graphics respectively. </p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24552"><img decoding="async" width="664" height="411" data-attachment-id="24552" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/amd_gpu_lines/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/amd_gpu_lines.jpg?fit=664%2C411&amp;ssl=1" data-orig-size="664,411" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="amd_gpu_lines" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/amd_gpu_lines.jpg?fit=664%2C411&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/amd_gpu_lines.jpg?fit=664%2C411&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/amd_gpu_lines.jpg?resize=664%2C411&amp;ssl=1" alt="" data-recalc-dims="1"></a></figure></div>
<p>CDNA 2 finally brought AMD notable success. MI250X and MI210 GPUs won several supercomputer contracts including ORNL’s Frontier, which holds first place on November 2023’s TOP500 list. But while CDNA2 delivered solid and cost efficient FP64 compute, H100 had better AI performance and offered a larger unified GPU.</p>
<p>CDNA 3 looks to close those gaps by bringing forward everything AMD has to offer. The company’s experience in advanced packaging technology is on full show, with MI300X getting a sophisticated chiplet setup. Together with Infinity Fabric components, advanced packaging lets MI300X scale to compete with Nvidia’s largest GPUs. On the memory side, Infinity Cache from the RDNA line gets pulled into the CDNA world to mitigate bandwidth issues. But that doesn’t mean MI300X is light on memory bandwidth. It still gets a massive HBM setup, giving it the best of both worlds. Finally, CDNA 3’s compute architecture gets significant generational improvements to boost throughput and utilization.</p>
<h2>GPU Layout</h2>
<p>AMD has a tradition of using chiplets to cheaply scale core counts in their Ryzen and Epyc CPUs. MI300X uses a similar strategy at a high level, with compute split off onto Accelerator Complex Dies, or XCDs. XCDs are analogous to CDNA 2 or RDNA 3’s Graphics Compute Dies (GCDs) or Ryzen’s Core Complex Dies (CCDs). AMD likely changed the naming because CDNA products lack the dedicated graphics hardware present in the RDNA line.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24592"><img decoding="async" width="688" height="384" data-attachment-id="24592" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/mi300_xcd_slide/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300_xcd_slide.jpg?fit=1275%2C711&amp;ssl=1" data-orig-size="1275,711" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="mi300_xcd_slide" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300_xcd_slide.jpg?fit=1275%2C711&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300_xcd_slide.jpg?fit=688%2C384&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300_xcd_slide.jpg?resize=688%2C384&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300_xcd_slide.jpg?w=1275&amp;ssl=1 1275w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300_xcd_slide.jpg?resize=768%2C428&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300_xcd_slide.jpg?resize=1200%2C669&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Each XCD contains a set of cores and a shared cache. Specifically, every XCD physically has 40 CDNA 3 Compute Units, with 38 of these being enabled per XCD on the MI300X. A 4 MB L2 cache sits on the XCD as well, and serves all of the die’s CUs. MI300X has eight XCDs, giving it 304 total Compute Units. </p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24645"><img decoding="async" width="635" height="499" data-attachment-id="24645" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/mi300x_drawio/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_drawio.jpg?fit=635%2C499&amp;ssl=1" data-orig-size="635,499" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="mi300x_drawio" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_drawio.jpg?fit=635%2C499&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_drawio.jpg?fit=635%2C499&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_drawio.jpg?resize=635%2C499&amp;ssl=1" alt="" data-recalc-dims="1"></a></figure></div>
<p>That’s a large increase over the MI250X’s 220 CUs. Even better, MI300X can expose all of those CUs as a single GPU. On MI250X, a programmer would have to manually split up work across the two GPUs because each has a separate pool of memory.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24647"><img loading="lazy" decoding="async" width="553" height="418" data-attachment-id="24647" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/mi250x_drawio/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi250x_drawio.jpg?fit=553%2C418&amp;ssl=1" data-orig-size="553,418" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="mi250x_drawio" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi250x_drawio.jpg?fit=553%2C418&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi250x_drawio.jpg?fit=553%2C418&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi250x_drawio.jpg?resize=553%2C418&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi250x_drawio.jpg?w=553&amp;ssl=1 553w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi250x_drawio.jpg?resize=200%2C150&amp;ssl=1 200w" sizes="(max-width: 553px) 100vw, 553px" data-recalc-dims="1"></a></figure></div>
<p>Nvidia’s H100 consists of 132 Streaming Multiprocessors (SMs) and also presents them to programmers as a big unified GPU. H100 takes a conventional approach by implementing all of that compute on a large monolithic die. Even with everything on the same die, H100 is too large to give all of its SMs equal access to cache. So, H100 splits the L2 into two instances. A single SM can use all 50 MB of L2, but access to more than 25 MB will incur a performance penalty.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24646"><img loading="lazy" decoding="async" width="688" height="337" data-attachment-id="24646" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/h100_drawio/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/h100_drawio.jpg?fit=787%2C386&amp;ssl=1" data-orig-size="787,386" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="h100_drawio" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/h100_drawio.jpg?fit=787%2C386&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/h100_drawio.jpg?fit=688%2C337&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/h100_drawio.jpg?resize=688%2C337&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/h100_drawio.jpg?w=787&amp;ssl=1 787w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/h100_drawio.jpg?resize=768%2C377&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Still, Nvidia’s strategy makes more efficient use of cache capacity than MI300X’s. A MI300X XCD doesn’t use L2 capacity on other XCDs for caching, just as CCDs on Epyc/Ryzen don’t allocate into each other’s L3 caches.</p>
<p>Intel’s Ponte Vecchio (PVC) compute GPUs make for a very interesting comparison. PVC places its basic compute building blocks in dies called Compute Tiles, which are roughly analogous to CDNA 3’s XCDs. Similarly, PVC’s Base Tile serves a similar function to CDNA 3’s IO dies. Both contain a large last level cache and HBM memory controllers. Like MI300X, a Ponte Vecchio card can be exposed as a single GPU with a unified memory pool.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24652"><img loading="lazy" decoding="async" width="451" height="339" data-attachment-id="24652" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/pvc_drawio/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/pvc_drawio.jpg?fit=451%2C339&amp;ssl=1" data-orig-size="451,339" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="pvc_drawio" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/pvc_drawio.jpg?fit=451%2C339&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/pvc_drawio.jpg?fit=451%2C339&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/pvc_drawio.jpg?resize=451%2C339&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/pvc_drawio.jpg?w=451&amp;ssl=1 451w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/pvc_drawio.jpg?resize=400%2C300&amp;ssl=1 400w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/pvc_drawio.jpg?resize=200%2C150&amp;ssl=1 200w" sizes="(max-width: 451px) 100vw, 451px" data-recalc-dims="1"></a></figure></div>
<p>However, there are important differences. Ponte Vecchio’s Compute Tiles are smaller with only eight Xe Cores, compared to 38 Compute Units on a CDNA 3 XCD. Instead of using a Compute Tile wide cache, Intel uses larger L1 caches to reduce cross-die traffic demands. Using a two-stack Ponte Vecchio part as a unified GPU presents challenges too. The EMIB bridge between the two stacks <a href="https://www.intel.com/content/www/us/en/developer/articles/technical/intel-data-center-gpu-max-series-overview.html#gs.1p3uqo">only offers 230 GB/s of bandwidth</a>, which isn’t enough to fully utilize HBM bandwidth if accesses are striped across all memory controllers. To address this, Intel has APIs that can let programs work with the GPU in a NUMA configuration. </p>
<p>In terms of physical construction, PVC and CDNA 3’s designs have different challenges. CDNA 3’s ability to present a unified memory pool with HBM requires high bandwidth between the IO dies. PVC gets by with a relatively low bandwidth EMIB link. But PVC’s design gets complicated because it uses four die types with different process nodes and foundries. AMD only uses two die types in MI300X, and both nodes (6 nm and 5 nm) are from TSMC.</p>
<h2>Tackling the Bandwidth Problem</h2>
<p>Compute has been outpacing memory for decades. Like CPUs, GPUs have countered this with increasingly sophisticated caching strategies. CDNA 2 used a conventional two-level cache hierarchy with a 8 MB L2, relying on HBM2e to keep the execution units fed. But even with HBM2e, MI250X was more bandwidth starved than Nvidia’s H100. If AMD simply added more compute, bandwidth starvation could be come a serious issue. So, AMD took a leaf out of RDNA(2)’s book and added an “Infinity Cache”.</p>
<p>Much like the consumer RDNA GPUs, MI300’s Infinity Cache is what the technical documentation calls Memory Attached Last Level (MALL), which is a fancy way to say that the last level cache level is a memory side cache. Compared to L1 and L2 caches that are closer to the Compute Units, the Infinity Cache is attached to the memory controllers. All memory traffic passes through the Infinity Cache regardless of what block it’s coming from. That includes IO traffic, so communications between peer GPUs can benefit from Infinity Cache bandwidth. Because the Infinity Cache always has the most up to date view of DRAM contents, It doesn’t have to handle snoops or other cache maintenance operations.</p>
<div>
<figure><a href="https://chipsandcheese.com/polaris_l2_clients/"><img loading="lazy" decoding="async" width="688" height="385" data-attachment-id="24743" data-permalink="https://chipsandcheese.com/polaris_l2_clients/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/polaris_l2_clients.png?fit=1275%2C713&amp;ssl=1" data-orig-size="1275,713" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="polaris_l2_clients" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/polaris_l2_clients.png?fit=1275%2C713&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/polaris_l2_clients.png?fit=688%2C385&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/polaris_l2_clients.png?resize=688%2C385&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/polaris_l2_clients.png?w=1275&amp;ssl=1 1275w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/polaris_l2_clients.png?resize=768%2C429&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/polaris_l2_clients.png?resize=1200%2C671&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>From AMD’s presentation on their RDNA architecture. L2 slices may be associated with memory controllers, but the L2 is not a memory side cache because many agents can write to DRAM without going through L2</figcaption></figure></div>
<p>But because a memory side cache is farther away from compute, it generally suffers from higher latency. Therefore, AMD has multi-megabyte L2 caches on both CDNA 3 and RDNA 2 to insulate compute from the lower performance of a memory side cache.</p>
<p>Like RDNA 2, CDNA 3’s Infinity Cache is 16-way set associative. However, CDNA 3’s implementation is more optimized for bandwidth than capacity. It’s composed of 128 slices, each with 2 MB of capacity and 64 bytes per cycle of read bandwidth. All of the slices together can deliver 8192 bytes per cycle, which is good for 17.2 TB/s at 2.1 GHz.</p>
<div>
<figure><a href="https://chipsandcheese.com/cdna3_cache_hierarchy_compared/"><img loading="lazy" decoding="async" width="688" height="554" data-attachment-id="24577" data-permalink="https://chipsandcheese.com/cdna3_cache_hierarchy_compared/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cache_hierarchy_compared.jpg?fit=920%2C741&amp;ssl=1" data-orig-size="920,741" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cdna3_cache_hierarchy_compared" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cache_hierarchy_compared.jpg?fit=920%2C741&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cache_hierarchy_compared.jpg?fit=688%2C554&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cache_hierarchy_compared.jpg?resize=688%2C554&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cache_hierarchy_compared.jpg?w=920&amp;ssl=1 920w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cache_hierarchy_compared.jpg?resize=768%2C619&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>For comparison, RDNA 2’s 128 MB Infinity Cache can provide 1024 bytes per cycle across all slices, giving it 2.5 TB/s of theoretical bandwidth at 2.5 GHz.<a href="https://www.flickr.com/photos/130561288@N04/51703830661/"> Die shots</a> suggest each Infinity Cache slice has 4 MB of capacity and provides 32B/cycle. RDNA 2 therefore uses bigger slices, fewer of them and has less bandwidth from each slice.</p>
<p>MI300X’s focus on bandwidth means workloads with lower compute density can still enjoy decent performance if they can get enough Infinity Cache hits. That should make CDNA 3’s execution units easier to feed even though the main memory bandwidth to compute ratio hasn’t changed much and remains behind Nvidia’s.</p>
<div>
<figure><a href="https://chipsandcheese.com/cdna3_ic_roofline/"><img loading="lazy" decoding="async" width="688" height="382" data-attachment-id="24579" data-permalink="https://chipsandcheese.com/cdna3_ic_roofline/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_ic_roofline.png?fit=796%2C442&amp;ssl=1" data-orig-size="796,442" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cdna3_ic_roofline" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_ic_roofline.png?fit=796%2C442&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_ic_roofline.png?fit=688%2C382&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_ic_roofline.png?resize=688%2C382&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_ic_roofline.png?w=796&amp;ssl=1 796w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_ic_roofline.png?resize=768%2C426&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>MI250X figures are for a single GCD</figcaption></figure></div>
<p>If we construct a roofline model for MI300X using Infinity Cache’s theoretical bandwidth, we can achieve full FP64 throughput with 4.75 FLOPs per byte loaded. It’s a massive improvement over DRAM, which would require 14.6 to 15 FLOPs per byte loaded.</p>
<h4>Possible Challenges with Cross-Die Bandwidth</h4>
<p>MI300X’s Infinity Fabric spans four IO dies, each of which connects to two HBM stacks and associated cache partitions. However, the bandwidth of the die to die connections may limit achieving full Infinity Cache bandwidth when MI300X operates as a single logical GPU with a unified memory pool. If memory accesses are striped evenly across the memory controllers (and thus cache partitions), as is typical for most GPU designs, the available die-to-die bandwidth may prevent applications from reaching theoretical Infinity Cache bandwidth.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24650"><img loading="lazy" decoding="async" width="688" height="389" data-attachment-id="24650" data-permalink="https://chipsandcheese.com/?attachment_id=24650" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?fit=1976%2C1117&amp;ssl=1" data-orig-size="1976,1117" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-1-1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?fit=1976%2C1117&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?fit=688%2C389&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?resize=688%2C389&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?w=1976&amp;ssl=1 1976w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?resize=768%2C434&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?resize=1536%2C868&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?resize=1200%2C678&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?resize=1600%2C904&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?resize=1320%2C746&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>First, let’s focus on a single IO die partition. It has 2.7 TB/s of ingress bandwidth along two edges adjacent to other IO dies. Its two XCDs can get 4.2 TB/s of Infinity cache bandwidth. If L2 miss requests are evenly striped across the dies, 3/4 of that bandwidth, or 3.15 TB/s, must come from peer dies. Since 3.15 TB/s is greater than 2.7 TB/s, cross-die bandwidth will limit achievable cache bandwidth.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24840"><img loading="lazy" decoding="async" width="688" height="404" data-attachment-id="24840" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/mi300x_ic_bw_singlepart/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_ic_bw_singlepart.png?fit=912%2C536&amp;ssl=1" data-orig-size="912,536" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="mi300x_ic_bw_singlepart" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_ic_bw_singlepart.png?fit=912%2C536&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_ic_bw_singlepart.png?fit=688%2C404&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_ic_bw_singlepart.png?resize=688%2C404&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_ic_bw_singlepart.png?w=912&amp;ssl=1 912w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_ic_bw_singlepart.png?resize=768%2C451&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>We can add the die in the opposite corner without any differences because all of its required die-to-die bandwidth goes in the opposite direction. MI300X has bidirectional die-to-die links.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24841"><img loading="lazy" decoding="async" width="688" height="398" data-attachment-id="24841" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/mi300x_ic_bw_opposite_corners/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_ic_bw_opposite_corners.png?fit=912%2C527&amp;ssl=1" data-orig-size="912,527" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="mi300x_ic_bw_opposite_corners" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_ic_bw_opposite_corners.png?fit=912%2C527&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_ic_bw_opposite_corners.png?fit=688%2C398&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_ic_bw_opposite_corners.png?resize=688%2C398&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_ic_bw_opposite_corners.png?w=912&amp;ssl=1 912w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_ic_bw_opposite_corners.png?resize=768%2C444&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>If all dies demand maximum Infinity Cache bandwidth in a unified configuration, things get more complex. Extra cross-die bandwidth is consumed because transfers between dies in opposite corners require two hops, and that’ll cut into ingress bandwidth available for each die.</p>
<p>While MI300X was engineered to act like one big GPU, splitting MI300X into multiple NUMA domains could give higher combined Infinity Cache bandwidth. It’s possible that AMD will have an API that will transparently split up programs among the different IO dies. Additionally, the likelihood of bandwidth issues would be minimized by high L2 hit rates, which would help avoid those bottlenecks. And in cases where the Infinity Cache hit rate are low, the MI300X’s die-to-die links are sufficiently robust and offer ample bandwidth to smoothly handle HBM traffic.</p>
<h3> Cross-XCD Coherency</h3>
<p>Even though the Infinity Cache doesn’t have to worry about coherency, the L2 caches do. Ordinary GPU memory accesses follow a relaxed coherency model, but programmers can use atomics to enforce ordering between threads. Memory accesses on AMD GPUs can also be marked with a GLC bit (Global Level Coherent). Those mechanisms still have to work if AMD wants to expose MI300X as a single big GPU, rather than a multi-GPU configuration as MI250X had done.</p>
<div>
<figure><a href="https://chipsandcheese.com/fah_atomics/"><img loading="lazy" decoding="async" width="688" height="256" data-attachment-id="24617" data-permalink="https://chipsandcheese.com/fah_atomics/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/fah_atomics.png?fit=1007%2C375&amp;ssl=1" data-orig-size="1007,375" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="fah_atomics" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/fah_atomics.png?fit=1007%2C375&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/fah_atomics.png?fit=688%2C256&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/fah_atomics.png?resize=688%2C256&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/fah_atomics.png?w=1007&amp;ssl=1 1007w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/fah_atomics.png?resize=768%2C286&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>Snippet of RDNA 2 code from Folding at Home, showing use of global memory atomics</figcaption></figure></div>
<p>On prior AMD GPUs, atomics and coherent accesses were handled at L2. Loads with the GLC bit set would bypass L1 caches, and thus get the most up-to-date copy of data from L2. That doesn’t work with MI300X because the most up-to-date copy of a cacheline could be on another XCD’s L2 cache. AMD could make coherent accesses bypass L2, but that would lower performance. That may have worked for a gaming GPU where coherent accesses aren’t too important. But AMD wants MI300X to perform well with compute workloads, and needs MI300A (the APU variant) to efficiently share data between the CPU and GPU. That’s where Infinity Fabric comes in.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24583"><img loading="lazy" decoding="async" width="688" height="385" data-attachment-id="24583" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/cdna3_cs/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cs.jpg?fit=1533%2C857&amp;ssl=1" data-orig-size="1533,857" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cdna3_cs" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cs.jpg?fit=1533%2C857&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cs.jpg?fit=688%2C385&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cs.jpg?resize=688%2C385&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cs.jpg?w=1533&amp;ssl=1 1533w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cs.jpg?resize=768%2C429&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cs.jpg?resize=1200%2C671&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cs.jpg?resize=1320%2C738&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cs.jpg?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>CM = Coherent Master. CS = Coherent Slave</figcaption></figure></div>
<p>Like Infinity Fabric on Ryzen, CDNA 3 has Coherent Masters (CMs) where the XCDs connect to the IO dies. Coherent Slaves (CS) sit at each memory controller alongside Infinity Cache (IC) slices. We can infer how these work via Ryzen documentation, which shows Coherent Slaves have a probe filter and hardware for handling atomic transactions. MI300X likely has a similar CS implementation.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24585"><img loading="lazy" decoding="async" width="688" height="501" data-attachment-id="24585" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/zen_ppr_cs/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/zen_ppr_cs.png?fit=736%2C536&amp;ssl=1" data-orig-size="736,536" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zen_ppr_cs" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/zen_ppr_cs.png?fit=736%2C536&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/zen_ppr_cs.png?fit=688%2C501&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/zen_ppr_cs.png?resize=688%2C501&amp;ssl=1" alt="" data-recalc-dims="1"></a><figcaption>From AMD’s Zen PPR, showing error reporting available at the Coherent Slave (CS).</figcaption></figure></div>
<p>If a coherent write shows up at the CS, it has to ensure any thread doing a coherent read will observe that write regardless of where that thread is running on the GPU. That means any XCD with the line cached will have to reload it from Infinity Cache to get the most up to date data. Naively, the CS would have to probe L2 caches across all XCDs because any of them could have the corresponding data cached. The probe filter helps avoid this by tracking which XCDs actually have the line cached, thus avoiding unnecessary probe traffic. CDNA 3’s whitepaper says the snoop filter (another name for a probe filter) is large enough to cover multiple XCD L2 caches. I certainly believe them because MI300X has 32 MB of L2 across all eight XCDs. Even consumer Ryzen parts can have more CCD-private cache for the probe filter to cover.</p>
<p>Thanks to CPU-like Infinity Fabric components like CS and CM, a XCD can have a private write-back L2 cache capable of handling intra-die coherent accesses without going across the IO die fabric. AMD could have gone for a naive solution where coherent operations and atomics go straight to the Infinity Cache, bypassing L2. Such a solution would save engineering effort and create a simpler design at the cost of lower performance for coherent operations. Evidently, AMD thought optimizing atomics and coherent accesses was important enough to go the extra mile.</p>
<blockquote>
<p>To ensure coherence of local memory writes of CUs in different agents a <code>buffer_wbl2 sc1</code> is required. It will writeback dirty L2 cache lines.</p>
<p>To ensure coherence of local memory reads of CUs in different agents a <code>buffer_inv sc0 sc1</code> is required. It will invalidate non-local L2 cache<br>lines if configured to have multiple L2 caches.</p>
<cite><a href="https://github.com/llvm/llvm-project/blob/main/llvm/docs/AMDGPUUsage.rst">LLVM Documentation</a> for the GFX942 Target</cite></blockquote>
<p>However, CDNA 3 within the XCD still works a lot like prior GPUs. Evidently normal memory writes will not automatically invalidate written lines from peer caches as in CPUs. Instead, code must explicitly tell the L2 to write back dirty lines and have peer L2 caches invalidate non-local L2 lines.</p>
<h3>L2 Cache</h3>
<p>Closer to the Compute Units, each MI300X XCD packs a 4 MB L2 cache. The L2 is a more traditional GPU cache, and is built from 16 slices. Each 256 KB slice can provide 128 bytes per cycle of bandwidth. At 2.1 GHz, that’s good for 4.3 TB/s. As the last level of cache on the same die as the Compute Units, the L2 plays an important role in acting as a backstop for L1 misses.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24664"><img loading="lazy" decoding="async" width="651" height="396" data-attachment-id="24664" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/mi300x_l2_roofline-1/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_l2_roofline-1.png?fit=651%2C396&amp;ssl=1" data-orig-size="651,396" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="mi300x_l2_roofline-1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_l2_roofline-1.png?fit=651%2C396&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_l2_roofline-1.png?fit=651%2C396&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_l2_roofline-1.png?resize=651%2C396&amp;ssl=1" alt="" data-recalc-dims="1"></a></figure></div>
<p>Compared to H100 and MI250X, MI300X has a higher L2 bandwidth to compute ratio. Because each XCD comes with a L2, L2 bandwidth naturally scales as a CDNA 3 product comes with more XCDs. In other words, MI300X’s L2 arrangement avoids the problem of getting a single cache hooked up to a lot of Compute Units and maintain a ton of bandwidth.</p>
<p>PVC’s L2 is a clear contrast. As Intel adds more Compute Tiles, the Base Tile’s shared L2 gets increasing bandwidth demands. From a cache design standpoint, PVC’s configuration is simpler because the L2 acts as a single point of coherency and a backstop for L1 misses. But it can’t offer as much bandwidth as MI300X’s L2. MI300X also likely enjoys better L2 latency, making it easier for applications to utilize cache bandwidth.</p>
<h3>L1 Cache</h3>
<p>CDNA 3’s focus on high cache bandwidth continues to the L1. In a move that matches RDNA, CDNA 3 sees its L1 throughput increased from 64 to 128 bytes per cycle. CDNA 2 increased per-CU vector throughput to 4096 bits per cycle compared to 2048 in GCN, so CDNA 3’s doubled L1 throughput helps maintain the same compute to L1 bandwidth ratio as GCN.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24667"><img loading="lazy" decoding="async" width="651" height="395" data-attachment-id="24667" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/mi300x_l1_roofline/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_l1_roofline.png?fit=651%2C395&amp;ssl=1" data-orig-size="651,395" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="mi300x_l1_roofline" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_l1_roofline.png?fit=651%2C395&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_l1_roofline.png?fit=651%2C395&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_l1_roofline.png?resize=651%2C395&amp;ssl=1" alt="" data-recalc-dims="1"></a></figure></div>
<p>Besides higher bandwidth, CDNA 3 increases L1 capacity from 16 to 32 KB. It’s a move that again mirrors developments in the RDNA line, where RDNA 3 received a similar size boost for its first level cache. Higher hitrates from the larger cache would lower average memory access latency, improving execution unit utilization. Transferring data from L2 and beyond costs power, so higher hitrate can help power efficiency too.</p>
<p>While CDNA 3 improves first level caching, Ponte Vecchio is still the champion in that category. Each Xe Core in PVC can deliver 512 bytes per cycle, giving Intel a very high L1 bandwidth to compute ratio. The L1 is large as well at 512 KB. Memory bound kernels that fit in L1 will do very well on Intel’s architecture. However, Ponte Vecchio lacks a mid-level cache at the Compute Tile level, and could face a harsh performance cliff as data spills out of L1.</p>
<h2>Scheduling and Execution Units</h2>
<p>A complex chiplet setup and modified cache hierarchy let AMD present MI300X as a single GPU, thus addressing one of MI250X’s biggest weaknesses. But AMD didn’t settle with that. They also made iterative improvements to the core Compute Unit architecture, addressing CDNA 2’s difficulties with utilizing its FP32 units.</p>
<div>
<figure><a href="https://chipsandcheese.com/cdna3_cu/"><img loading="lazy" decoding="async" width="688" height="175" data-attachment-id="24812" data-permalink="https://chipsandcheese.com/cdna3_cu/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cu.png?fit=692%2C176&amp;ssl=1" data-orig-size="692,176" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cdna3_cu" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cu.png?fit=692%2C176&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cu.png?fit=688%2C175&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cu.png?resize=688%2C175&amp;ssl=1" alt="" data-recalc-dims="1"></a><figcaption>From the CDNA 3 whitepaper</figcaption></figure></div>
<p>When CDNA 2 shifted to handling FP64 natively, AMD provided double rate FP32 via packed execution. The compiler would have to pack two FP32 values into adjacent registers and perform the same instruction on both. Often, the compiler struggled to pull this off unless programmers explicitly used vectors.</p>
<p>CDNA 3 gets around this with a more flexible dual issue mechanism. Most likely, this is an extension of GCN’s multi-issue capability rather than RDNA 3’s VOPD/wave64 method. Each cycle, the CU scheduler selects one of the four SIMDs and checks whether any of its threads are ready to execute. If multiple threads are ready, GCN could select up to five of them to send to execution units. Of course a GCN SIMD only has a single 16-wide vector ALU, so GCN would have to select threads with different instruction types ready to multi-issue. For example, a scalar ALU instruction can issue alongside a vector ALU one.</p>
<div>
<figure><img loading="lazy" decoding="async" width="688" height="280" data-attachment-id="24753" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/mi300x_multi_issue_occupancy/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue_occupancy.jpg?fit=1400%2C570&amp;ssl=1" data-orig-size="1400,570" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="mi300x_multi_issue_occupancy" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue_occupancy.jpg?fit=1400%2C570&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue_occupancy.jpg?fit=688%2C280&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue_occupancy.jpg?resize=688%2C280&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue_occupancy.jpg?w=1400&amp;ssl=1 1400w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue_occupancy.jpg?resize=768%2C313&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue_occupancy.jpg?resize=1200%2C489&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue_occupancy.jpg?resize=1320%2C537&amp;ssl=1 1320w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></figure></div>
<p>An alternative approach would be to take advantage of wave64’s wider width and let a thread complete two vector instructions over four cycles. However, doing so would break GCN’s model of handling VALU instructions in multiples of 4 clock cycles. CDNA 3 is still more closely related to GCN than RDNA is, and reusing GCN’s multi-issue strategy is a sensible move. AMD also could have used RDNA 3’s VOPD mechanism, where a special instruction format can contain two operations. While that method could increase per-thread performance, relying on the compiler to find dual issue pairs could be hit or miss.</p>
<div>
<figure><a href="https://chipsandcheese.com/gcn_cu_scheduler/"><img loading="lazy" decoding="async" width="688" height="381" data-attachment-id="24655" data-permalink="https://chipsandcheese.com/gcn_cu_scheduler/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/gcn_cu_scheduler.png?fit=1032%2C571&amp;ssl=1" data-orig-size="1032,571" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gcn_cu_scheduler" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/gcn_cu_scheduler.png?fit=1032%2C571&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/gcn_cu_scheduler.png?fit=688%2C381&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/gcn_cu_scheduler.png?resize=688%2C381&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/gcn_cu_scheduler.png?w=1032&amp;ssl=1 1032w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/gcn_cu_scheduler.png?resize=768%2C425&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>From an old AMD presentation</figcaption></figure></div>
<p>Instead of relying on the compiler, CDNA 3’s dual issue approach likely pushes responsibility to the programmer to expose more thread level parallelism via larger dispatch sizes. If a SIMD has more threads in flight, it’ll have a better chance of finding two threads with FP32 instructions ready to execute. At minimum, a SIMD will need two threads active to achieve full FP32 throughput. In practice CDNA 3 will need much higher occupancy to achieve good FP32 utilization. GPUs use in-order execution so individual threads will often be blocked by memory or execution latency. Keeping one set of execution units fed can be difficult even at full occupancy.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24752"><img loading="lazy" decoding="async" width="688" height="480" data-attachment-id="24752" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/mi300x_multi_issue/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue.jpg?fit=997%2C695&amp;ssl=1" data-orig-size="997,695" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="mi300x_multi_issue" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue.jpg?fit=997%2C695&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue.jpg?fit=688%2C480&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue.jpg?resize=688%2C480&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue.jpg?w=997&amp;ssl=1 997w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue.jpg?resize=768%2C535&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Therefore, AMD has dramatically increased the number of threads each CDNA 3 SIMD can track <a href="https://gpuopen.com/learn/amd-lab-notes/amd-lab-notes-register-pressure-readme/">from 8</a> to 24. If a programmer can take advantage of this, CDNA 3 will be better positioned to multi-issue. But this can be difficult. AMD did not mention an increase in vector register file capacity, which often limits how many threads a SIMD can have in flight. The vector register file can hold state for more threads if each thread uses fewer registers, so CDNA 3’s multi-issue capability may work best for simple kernels with few live variables.</p>
<p>Register file bandwidth presents another challenge for dual issue. CDNA 2’s packed FP32 execution didn’t require extra reads from the vector register file because it took advantage of wider register file ports needed to deliver 64-bit values. But separate instructions can reference different registers and require more reads from the register file. Adding more register file ports would be expensive, so CDNA 3 “generationally improves the source caching to provide better re-use and bandwidth amplification so that each vector register read can support more downstream vector or matrix operations”<sup>1</sup>. Most likely, AMD is using a larger register cache to mitigate port conflicts and keep the execution units fed.</p>
<h2>Matrix Operations</h2>
<p>Matrix multiplication has become increasingly important as machine learning picks up. Nvidia invested heavily in this area, adding matrix multiplication units (tensor cores) to their Volta and Turing architectures years ago. AMD’s CDNA architecture added matrix multiply support, but contemporary Nvidia architectures invested more heavily in matrix multiplication throughput. This especially applies to lower precision data types like FP16, which are often used in AI.</p>
<figure><table><tbody><tr><td></td><td>Matrix FP16 FMAs/Clk</td><td>Rate Relative to Packed FP16</td></tr><tr><td>AMD MI100 (CDNA) Compute Unit</td><td>512</td><td>4x</td></tr><tr><td>AMD MI250X (CDNA 2) Compute Unit</td><td>512</td><td>4x</td></tr><tr><td>AMD MI300X (CDNA 3) Compute Unit</td><td>1024</td><td>8x</td></tr><tr><td>Nvidia V100 Streaming Multiprocessor</td><td>512</td><td>4x<sup>4</sup></td></tr><tr><td>Nvidia A100 Streaming Multiprocessor</td><td>1024</td><td>4x</td></tr><tr><td>Nvidia H100 Streaming Multiprocessor</td><td>2048</td><td>8x</td></tr></tbody></table></figure>
<p>MI300X plays catch up by doubling per-CU matrix throughput compared to prior CDNA generations. On top of that, MI300X’s chiplet design allows a massive number of CUs. But Nvidia’s higher per-SM matrix performance still makes it a force to be reckoned with. Therefore, CDNA 3 continues AMD’s trend of hitting Nvidia hard from the vector FP64 performance side while maintaining strong AI performance in isolation.</p>
<h2>Instruction Cache</h2>
<p>Besides handling memory accesses requested by instructions, a Compute Unit has to fetch the instructions themselves from memory. GPUs traditionally had an easier time with instruction delivery because GPU code tends to be simple and not occupy a lot of memory. In the DirectX 9 era, Shader Model 3.0 <a href="https://learn.microsoft.com/en-us/windows/win32/direct3dhlsl/dx9-graphics-reference-asm-ps-3-0">even imposed limits on code size</a>. As GPUs evolved to take on compute, AMD rolled out their GCN architecture with 32 KB instruction caches. Today, CDNA 2 and RDNA GPUs continue to use 32 KB instruction caches.</p>
<p>CDNA 3 increases instruction cache capacity to 64 KB. Associativity doubles too, from 4-way to 8-way. That means higher instruction cache hitrates for CDNA 3 with bigger, more complex kernels. I suspect AMD is targeting CPU code naively ported to GPUs. Complex CPU code can be <a href="https://streamhpc.com/blog/2018-03-14/selecting-applications-suitable-for-porting-to-the-gpu/">punishing on GPUs</a>, since they can’t hide instruction cache miss latency with long distance instruction prefetching and accurate branch prediction. Higher instruction cache capacity helps contain larger kernels, while increased associativity helps avoid conflict misses.</p>
<p>Like CDNA 2, each CDNA 3 instruction cache instance services <a href="https://elixir.bootlin.com/linux/latest/source/drivers/gpu/drm/amd/amdkfd/kfd_crat.c#L328">two Compute Units</a>. GPU kernels are usually launched with large enough work sizes to fill many Compute Units, so sharing the instruction cache is a good way to efficiently use SRAM storage. I suspect AMD didn’t share the cache across even more Compute Units because a single cache instance may struggle to satisfy instruction bandwidth demands.</p>
<h2>Final Words</h2>
<p>CDNA 3’s whitepaper says that “the greatest generational changes in the AMD CDNA 3 architecture lie in the memory hierarchy” and I would have to agree. While AMD improved the Compute Unit’s low precision math capabilities compared to CDNA 2, the real improvement was the addition of the Infinity Cache.</p>
<p>MI250X’s primary issue was that it wasn’t really one GPU. It was two GPUs sharing the same package which only has 200 Gigabyte per second per direction between the GCDs. In AMD’s assessment that 200 Gigabyte per second per direction was not enough to have the MI250X show up as one GPU which is why AMD significantly increased the die to die bandwidth.</p>
<div>
<figure><img loading="lazy" decoding="async" width="688" height="389" data-attachment-id="24650" data-permalink="https://chipsandcheese.com/?attachment_id=24650" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?fit=1976%2C1117&amp;ssl=1" data-orig-size="1976,1117" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-1-1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?fit=1976%2C1117&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?fit=688%2C389&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?resize=688%2C389&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?w=1976&amp;ssl=1 1976w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?resize=768%2C434&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?resize=1536%2C868&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?resize=1200%2C678&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?resize=1600%2C904&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?resize=1320%2C746&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"><figcaption>For this image, I am considering North-South as the vertical axis and East-West as the horizontal axis</figcaption></figure></div>
<p>AMD increased the total East-West bandwidth to 2.4TB/sec per direction which is a 12 fold increase from MI250X. And the total North-South bandwidth is an even higher 3.0TB/sec per direction. With these massive bandwidth increases, AMD was able to make the MI300 appear as one large, unified accelerator instead of as 2 separate accelerators like MI250X. </p>
<p>4.0 TB/s of total ingress bandwidth for one die may not seem like enough if both XCD needs all available memory bandwidth. However, both XCDs combined can only access up to 4.2TB/s of bandwidth from the IO die so realistically the 4.0TB/s of ingress bandwidth is a non-issue. What the maximum of 4.0TB/s of ingress bandwidth does mean is that a single IO die can’t take advantage of all 5.3TB/s of memory bandwidth. </p>
<p>This is similar to desktop Ryzen 7000 parts where one CCD can’t take full advantage of DDR5 bandwidth due to Infinity Fabric limits. However this is likely to be a non-issue on MI300X because the bandwidth demands will be highest with all dies in play. In that case, each die will consume about 1.3 TB/s of bandwidth and getting 3/4 of that over cross-die links won’t be a problem. </p>
<div>
<figure><img loading="lazy" decoding="async" width="688" height="387" data-attachment-id="24792" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/image-1-2-2/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-2.jpg?fit=1495%2C840&amp;ssl=1" data-orig-size="1495,840" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-1-2" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-2.jpg?fit=1495%2C840&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-2.jpg?fit=688%2C387&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-2.jpg?resize=688%2C387&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-2.jpg?w=1495&amp;ssl=1 1495w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-2.jpg?resize=1280%2C720&amp;ssl=1 1280w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-2.jpg?resize=768%2C432&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-2.jpg?resize=1200%2C674&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-2.jpg?resize=1320%2C742&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-2.jpg?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></figure></div>
<p>But MI300 isn’t just a GPGPU part, it also has an APU part as well, which is in my opinion is the more interesting of the two MI300 products. AMD’s first ever APU, Llano, was released in 2011 which was based on AMD’s K10.5 CPU paired with a Terascale 3 GPU. Fast forward to 2023 and for their first “big iron” APU, the MI300A, AMD paired 6 of their CDNA3 XCDs with 24 Zen 4 cores all while reusing the same base die. This allows for the CPU and the GPU to shared the same memory address space which removes the need to copy data over an external bus to keep the CPU and GPU coherent with each other. </p>
<p>We look forward to what AMD could do with future “big iron” APUs as well as their future GPGPU line up. Maybe they’ll have specialized CCDs with wider vector units or maybe they’ll have networking on their base die that can directly connect to the xGMI switches that Broadcom have said to be making. Regardless of what future Instinct products look like, we are excited to both be looking forward to those products as well as testing the MI300 series.</p>
<p>We would like to thank AMD for inviting Chips and Cheese to the MI300 launch event. We were able to ask a lot of questions and gain some extra information without which this article would have been much shorter.</p>
<p>If you like our articles and journalism, and you want to support us in our endeavors, then consider heading over to our&nbsp;<a href="https://www.patreon.com/ChipsandCheese">Patreon</a>&nbsp;or our&nbsp;<a href="https://www.paypal.com/donate/?hosted_button_id=4EMPH66SBGVSQ">PayPal</a>&nbsp;if you want to toss a few bucks our way. If you would like to talk with the Chips and Cheese staff and the people behind the scenes, then consider joining our&nbsp;<a href="https://discord.gg/TwVnRhxgY2">Discord</a>.</p>
<h2>References</h2>
<ol>
<li><a href="https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/white-papers/amd-cdna-3-white-paper.pdf">CDNA 3 Whitepaper</a></li>
<li><a href="https://www.amd.com/content/dam/amd/en/documents/instinct-business-docs/white-papers/amd-cdna2-white-paper.pdf">CDNA 2 Whitepaper</a></li>
<li><a href="https://www.amd.com/content/dam/amd/en/documents/instinct-business-docs/white-papers/amd-cdna-white-paper.pdf">CDNA Whitepaper</a></li>
<li><a href="https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf">Volta Whitepaper</a></li>
<li><a href="https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf">Nvidia A100 Whitepaper</a></li>
<li><a href="https://resources.nvidia.com/en-us-tensor-core/gtc22-whitepaper-hopper">Nvidia H100 Whitepaper</a></li>
<li><a href="https://www.intel.com/content/www/us/en/developer/articles/technical/intel-data-center-gpu-max-series-overview.html#gs.1p3uqo">Intel Data Center GPU Max Series Technical Overview</a></li>
</ol>

<div data-post_id="10949" data-instance_id="1" data-additional_class="pp-multiple-authors-layout-boxed.multiple-authors-target-the-content" data-original_class="pp-multiple-authors-boxes-wrapper pp-multiple-authors-wrapper box-post-id-10949 box-instance-id-1">

<ul>
<li>
<p><img alt="clamchowder" src="https://secure.gravatar.com/avatar/7c39d2e6d35e77c8fd15c4b2d9ce4e64?s=80&amp;d=identicon&amp;r=g" srcset="https://secure.gravatar.com/avatar/7c39d2e6d35e77c8fd15c4b2d9ce4e64?s=160&amp;d=identicon&amp;r=g 2x" height="80" width="80"> </p>

</li>
<li>
<p><img alt="Cheese" src="https://secure.gravatar.com/avatar/eb262496276a5c8c0a375be578f81db9?s=80&amp;d=identicon&amp;r=g" srcset="https://secure.gravatar.com/avatar/eb262496276a5c8c0a375be578f81db9?s=160&amp;d=identicon&amp;r=g 2x" height="80" width="80"> </p>

</li>
</ul>
</div>





</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Misra C++:2023 (106 pts)]]></title>
            <link>https://forum.misra.org.uk/thread-1668.html</link>
            <guid>38674158</guid>
            <pubDate>Sun, 17 Dec 2023 16:52:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://forum.misra.org.uk/thread-1668.html">https://forum.misra.org.uk/thread-1668.html</a>, See on <a href="https://news.ycombinator.com/item?id=38674158">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post_3679">
<div>
	<!-- start: postbit_avatar -->
<p><a href="https://forum.misra.org.uk/user-4.html"><img src="https://forum.misra.org.uk/images/default_avatar.png" alt="" width="55" height="55"></a></p>
<!-- end: postbit_avatar -->
	
	<div>
		<!-- start: postbit_author_user --><p>

	Posts: 103<br>
	Threads: 76<br>
	Joined: May 2004
	</p><!-- start: postbit_reputation -->
<p>Reputation: </p><!-- start: postbit_reputation_formatted_link -->
<p><a href="https://forum.misra.org.uk/reputation.php?uid=4"><strong>3</strong></a></p><!-- end: postbit_reputation_formatted_link -->
<!-- end: postbit_reputation -->
<!-- end: postbit_author_user -->
	</div>
</div>
<div>
	<div>
		<!-- start: postbit_posturl -->

<!-- end: postbit_posturl -->
		
		<p><span>29-11-2023, 07:42 PM <span id="edited_by_3679"><!-- start: postbit_editedby -->
<span>(This post was last modified: 30-11-2023, 04:03 PM by <a href="https://forum.misra.org.uk/user-3.html">forum admin</a>.)</span>
<!-- end: postbit_editedby --></span></span>
		
	</p></div>
	<div id="pid_3679"><p>
		MISRA is very pleased to announce the release of the new version of MISRA C++; MISRA C++:2023 <span>Guidelines for the use C++:17 in critical systems</span></p><p>

Published in October 2023, this is the latest and current edition of MISRA C++. It is specifically targetting the 2017 language version (C++:17) as defined by ISO/IEC 14882:2017.</p><p>

The document is available in PDF form from our webstore, and you can also purchase hardcopies using a “print on demand” service.</p><p>

We will create an FAQ section shortly with answers to questions on the new document as well as a new area in this forum for discussion on its guidelines. Please wait until we have created the new forum topics before posting questions.</p><p>

Webstore purchases are for single-user individual licenses. Other uses including but not limited to corporate (shared) use, use within a tool by tool vendors and training courses require a license; details are available on request. Please use the "contact us" form on the MISRA website to get in touch.
	</p></div>
	
	<!-- start: postbit_signature -->
<p>
Dr David Ward<br>
MISRA Operations Director
</p>
<!-- end: postbit_signature -->
	
	
</div>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[microsoft/promptbase: All things prompt engineering (164 pts)]]></title>
            <link>https://github.com/microsoft/promptbase</link>
            <guid>38673954</guid>
            <pubDate>Sun, 17 Dec 2023 16:36:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/microsoft/promptbase">https://github.com/microsoft/promptbase</a>, See on <a href="https://news.ycombinator.com/item?id=38673954">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">promptbase</h2>
<p dir="auto"><code>promptbase</code> is an evolving collection of resources, best practices, and example scripts for eliciting the best performance from foundation models like <code>GPT-4</code>. We currently host scripts demonstrating the <a href="https://arxiv.org/abs/2311.16452" rel="nofollow"><code>Medprompt</code> methodology</a>, including examples of how we further extended this collection of prompting techniques ("<code>Medprompt+</code>") into non-medical domains:</p>
<table>
<thead>
<tr>
<th>Benchmark</th>
<th>GPT-4 Prompt</th>
<th>GPT-4 Results</th>
<th>Gemini Ultra Results</th>
</tr>
</thead>
<tbody>
<tr>
<td>MMLU</td>
<td>Medprompt+</td>
<td>90.10%</td>
<td>90.04%</td>
</tr>
<tr>
<td>GSM8K</td>
<td>Zero-shot</td>
<td>95.3%</td>
<td>94.4%</td>
</tr>
<tr>
<td>MATH</td>
<td>Zero-shot</td>
<td>68.4%</td>
<td>53.2%</td>
</tr>
<tr>
<td>HumanEval</td>
<td>Zero-shot</td>
<td>87.8%</td>
<td>74.4%</td>
</tr>
<tr>
<td>BIG-Bench-Hard</td>
<td>Few-shot + CoT</td>
<td>89.0%</td>
<td>83.6%</td>
</tr>
<tr>
<td>DROP</td>
<td>Zero-shot + CoT</td>
<td>83.7%</td>
<td>82.4%</td>
</tr>
<tr>
<td>HellaSwag</td>
<td>10-shot</td>
<td>95.3%</td>
<td>87.8%</td>
</tr>
</tbody>
</table>
<p dir="auto">In the near future, <code>promptbase</code> will also offer further case studies and structured interviews around the scientific process we take behind prompt engineering. We'll also offer specialized deep dives into specialized tooling that accentuates the prompt engineering process. Stay tuned!</p>
<h2 tabindex="-1" dir="auto"><code>Medprompt</code> and The Power of Prompting</h2>
<details>
<summary>
    <em>"Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine" (H. Nori, Y. T. Lee, S. Zhang, D. Carignan, R. Edgar, N. Fusi, N. King, J. Larson, Y. Li, W. Liu, R. Luo, S. M. McKinney, R. O. Ness, H. Poon, T. Qin, N. Usuyama, C. White, E. Horvitz 2023)</em>
</summary>
<br>
<pre><p dir="auto">@article{nori2023can,
title={Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine},
author={Nori, Harsha and Lee, Yin Tat and Zhang, Sheng and Carignan, Dean and Edgar, Richard and Fusi, Nicolo and King, Nicholas and Larson, Jonathan and Li, Yuanzhi and Liu, Weishung and others},
journal={arXiv preprint arXiv:2311.16452},
year={2023}
}
</p></pre>
<a href="https://arxiv.org/pdf/1909.09223.pdf" rel="nofollow">Paper link</a>
</details>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/promptbase/blob/main/images/medprompt_radar.png"><img src="https://github.com/microsoft/promptbase/raw/main/images/medprompt_radar.png" alt=""></a></p>
<p dir="auto">In a recent <a href="https://arxiv.org/abs/2311.16452" rel="nofollow">study</a>, we showed how the composition of several prompting strategies into a method that we refer to as <code>Medprompt</code> can efficiently steer generalist models like GPT-4 to achieve top performance, even when compared to models specifically finetuned for medicine. <code>Medprompt</code> composes three distinct strategies together -- including dynamic few-shot selection, self-generated chain of thought, and choice-shuffle ensembling -- to elicit specialist level performance from GPT-4. We briefly describe these strategies here:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/promptbase/blob/main/images/medprompt_sa_graphic.png"><img src="https://github.com/microsoft/promptbase/raw/main/images/medprompt_sa_graphic.png" alt=""></a></p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Dynamic Few Shots</strong>: Few-shot learning -- providing several examples of the task and response to a foundation model -- enables models quickly adapt to a specific domain and
learn to follow the task format. For simplicity and efficiency, the few-shot examples applied in prompting for a particular task are typically fixed; they are unchanged across test examples. This necessitates that the few-shot examples selected are broadly representative and relevant to a wide distribution of text examples. One approach to meeting these requirements is to have domain experts carefully hand-craft exemplars. Even so, this approach cannot guarantee that the curated, fixed few-shot examples will be appropriately representative of every test example. However, with enough available data, we can select <em>different</em> few-shot examples for different task inputs. We refer to this approach as employing dynamic few-shot examples. The method makes use of a mechanism to identify examples based on their similarity to the case at hand. For Medprompt, we did the following to identify representative few shot examples: Given a test example, we choose k training examples that are semantically similar using a k-NN clustering in the embedding space. Specifically, we first use OpenAI's <code>text-embedding-ada-002</code> model to embed candidate exemplars for few-shot learning. Then, for each test question x, we retrieve its nearest k neighbors x1, x2, ..., xk from the training set (according to distance in the embedding space of text-embedding-ada-002). These examples -- the ones most similar in embedding space to the test question -- are ultimately registered in the prompt.</p>
</li>
<li>
<p dir="auto"><strong>Self-Generated Chain of Thought (CoT)</strong>: Chain-of-thought (CoT) uses natural language statements, such as “Let’s think step by step,” to explicitly encourage the model to generate a series of intermediate reasoning steps. The approach has been found to significantly improve the ability of foundation models to perform complex reasoning. Most approaches to chain-of-thought center on the use of experts to manually compose few-shot examples with chains of thought for prompting. Rather than rely on human experts, we pursued
a mechanism to automate the creation of chain-of-thought examples. We found that we could simply ask GPT-4 to generate chain-of-thought for the training examples, with appropriate guardrails for reducing risk of hallucination via incorrect reasoning chains.</p>
</li>
<li>
<p dir="auto"><strong>Majority Vote Ensembling</strong>: <a href="https://en.wikipedia.org/wiki/Ensemble_learning" rel="nofollow">Ensembling</a> refers to combining the output of several algorithms together to yield better predictive performance than any individual algorithm. Frontier models like <code>GPT-4</code> benefit from ensembling of their own outputs. A simple technique is to have a variety of prompts, or a single prompt with varied <code>temperature</code>, and report the most frequent answer amongst the ensemble constituents. For multiple choice questions, we employ a further trick that increases the diversity of the ensemble called <code>choice-shuffling</code>, where we shuffle the relative order of the answer choices before generating each reasoning
path. We then select the most consistent answer, i.e., the one that is least sensitive to choice shuffling, which increases the robustness of the answer.</p>
</li>
</ul>
<p dir="auto">The combination of these three techniques led to breakthrough performance in Medprompt for medical challenge questions. Implementation details of these techniques can be found here: <a href="https://github.com/microsoft/promptbase/tree/main/src/promptbase/mmlu">https://github.com/microsoft/promptbase/tree/main/src/promptbase/mmlu</a></p>
<h2 tabindex="-1" dir="auto"><code>Medprompt+</code> | Extending the power of prompting</h2>
<p dir="auto">Here we provide some intuitive details on how we extended the <code>medprompt</code> prompting framework to elicit even stronger out-of-domain performance on the MMLU (Measuring Massive Multitask Language Understanding) benchmark.  MMLU was established as a test of general knowledge and reasoning powers of large language models.  The complete MMLU benchmark contains tens of thousands of challenge problems of different forms across 57 areas from basic mathematics to United States history, law, computer science, engineering, medicine, and more.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/promptbase/blob/main/images/mmlu_accuracy_ablation.png"><img src="https://github.com/microsoft/promptbase/raw/main/images/mmlu_accuracy_ablation.png" alt=""></a></p>
<p dir="auto">We found that applying Medprompt without modification to the whole MMLU achieved a score of 89.1%. Not bad for a single policy working across a great diversity of problems!  But could we push Medprompt to do better?  Simply scaling-up MedPrompt can yield further benefits. As a first step, we increased the number of ensembled calls from five to 20.  This boosted performance to 89.56%.</p>
<p dir="auto">On working to push further with refinement of Medprompt, we noticed that performance was relatively poor for specific topics of the MMLU. MMLU contains a great diversity of types of questions, depending on the discipline and specific benchmark at hand. How might we push GPT-4 to perform even better on MMLU given the diversity of problems?</p>
<p dir="auto">We focused on extension to a portfolio approach based on the observation that some topical areas tend to ask questions that would require multiple steps of reasoning and perhaps a scratch pad to keep track of multiple parts of a solution. Other areas seek factual answers that follow more directly from questions. Medprompt employs “chain-of-thought” (CoT) reasoning, resonating with multi-step solving.  We wondered if the sophisticated Medprompt-classic approach might do less well on very simple questions and if the system might do better if a simpler method were used for the factual queries.</p>
<p dir="auto">Following this argument, we found that we could boost the performance on MMLU by extending MedPrompt with a simple two-method prompt portfolio. We add to the classic Medprompt a set of 10 simple, direct few-shot prompts soliciting an answer directly without Chain of Thought. We then ask GPT-4 for help with deciding on the best strategy for each topic area and question. As a screening call, for each question we first ask GPT-4:</p>
<div data-snippet-clipboard-copy-content="# Question
{{ question }}
 
# Task
Does answering the question above require a scratch-pad?
A. Yes
B. No"><pre><code># Question
{{ question }}
 
# Task
Does answering the question above require a scratch-pad?
A. Yes
B. No
</code></pre></div>
<p dir="auto">If GPT-4 thinks the question does require a scratch-pad, then the contribution of the Chain-of-Thought component of the ensemble is doubled. If it doesn't, we halve that contribution (and let the ensemble instead depend more on the direct few-shot prompts). Dynamically leveraging the appropriate prompting technique in the ensemble led to a further +0.5% performance improvement across the MMLU.</p>
<p dir="auto">We note that Medprompt+ relies on accessing confidence scores (logprobs) from GPT-4. These are not publicly available via the current API but will be enabled for all in the near future.</p>
<h2 tabindex="-1" dir="auto">Running Scripts</h2>
<blockquote>
<p dir="auto">Note: Some scripts hosted here are published for reference on methodology, but may not be immediately executable against public APIs. We're working hard on making the pipelines easier to run "out of the box" over the next few days, and appreciate your patience in the interim!</p>
</blockquote>
<p dir="auto">First, clone the repo and install the promptbase package:</p>

<p dir="auto">Next, decide which tests you'd like to run. You can choose from:</p>
<ul dir="auto">
<li>bigbench</li>
<li>drop</li>
<li>gsm8k</li>
<li>humaneval</li>
<li>math</li>
<li>mmlu</li>
</ul>
<p dir="auto">Before running the tests, you will need to download the datasets from the original sources (see below) and place them in the <code>src/promptbase/datasets</code> directory.</p>
<p dir="auto">After downloading datasets and installing the promptbase package, you can run a test with:</p>
<p dir="auto"><code>python -m promptbase dataset_name</code></p>
<p dir="auto">For example:</p>
<p dir="auto"><code>python -m promptbase gsm8k</code></p>
<h2 tabindex="-1" dir="auto">Dataset Links</h2>
<p dir="auto">To run evaluations, download these datasets and add them to /src/promptbase/datasets/</p>
<ul dir="auto">
<li>MMLU: <a href="https://github.com/hendrycks/test">https://github.com/hendrycks/test</a></li>
<li>HumanEval: <a href="https://huggingface.co/datasets/openai_humaneval" rel="nofollow">https://huggingface.co/datasets/openai_humaneval</a></li>
<li>DROP: <a href="https://allenai.org/data/drop" rel="nofollow">https://allenai.org/data/drop</a></li>
<li>GSM8K: <a href="https://github.com/openai/grade-school-math">https://github.com/openai/grade-school-math</a></li>
<li>MATH: <a href="https://huggingface.co/datasets/hendrycks/competition_math" rel="nofollow">https://huggingface.co/datasets/hendrycks/competition_math</a></li>
<li>Big-Bench-Hard: <a href="https://github.com/suzgunmirac/BIG-Bench-Hard">https://github.com/suzgunmirac/BIG-Bench-Hard</a>
The contents of this repo need to be put into a directory called <code>BigBench</code> in the <code>datasets</code> directory</li>
</ul>
<h2 tabindex="-1" dir="auto">Other Resources:</h2>
<p dir="auto">Medprompt Blog: <a href="https://www.microsoft.com/en-us/research/blog/the-power-of-prompting/" rel="nofollow">https://www.microsoft.com/en-us/research/blog/the-power-of-prompting/</a></p>
<p dir="auto">Medprompt Research Paper: <a href="https://arxiv.org/abs/2311.16452" rel="nofollow">https://arxiv.org/abs/2311.16452</a></p>
<p dir="auto">Medprompt+: <a href="https://www.microsoft.com/en-us/research/blog/steering-at-the-frontier-extending-the-power-of-prompting/" rel="nofollow">https://www.microsoft.com/en-us/research/blog/steering-at-the-frontier-extending-the-power-of-prompting/</a></p>
<p dir="auto">Microsoft Introduction to Prompt Engineering: <a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/prompt-engineering" rel="nofollow">https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/prompt-engineering</a></p>
<p dir="auto">Microsoft Advanced Prompt Engineering Guide: <a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions" rel="nofollow">https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions</a></p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[BrainGPT turns thoughts into text (251 pts)]]></title>
            <link>https://www.iflscience.com/new-mind-reading-braingpt-turns-thoughts-into-text-on-screen-72054</link>
            <guid>38673854</guid>
            <pubDate>Sun, 17 Dec 2023 16:22:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.iflscience.com/new-mind-reading-braingpt-turns-thoughts-into-text-on-screen-72054">https://www.iflscience.com/new-mind-reading-braingpt-turns-thoughts-into-text-on-screen-72054</a>, See on <a href="https://news.ycombinator.com/item?id=38673854">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><section><header><div><p><img alt="clock" title="clock" width="16" height="16" src="https://assets.iflscience.com/svg/_clock.svg"><span>PUBLISHED</span><time data-published-date="2023-12-17T09:56:04Z" date-time="2023-12-17T09:56:04Z"></time></p></div><h2>New Mind-Reading "BrainGPT" Turns Thoughts Into Text On Screen</h2><h2>It offers new hope to people unable to communicate in other ways.</h2></header><figure><picture><source media="(min-width: 1000px)" srcset="https://assets.iflscience.com/assets/articleNo/72054/aImg/72811/braingpt-l.webp" type="image/webp"><source media="(min-width: 1000px)" srcset="https://assets.iflscience.com/assets/articleNo/72054/aImg/72811/braingpt-l.jpg" type="image/jpeg"><source media="(max-width: 567px)" srcset="https://assets.iflscience.com/assets/articleNo/72054/aImg/72811/braingpt-m.webp" type="image/webp"><source media="(max-width: 567px)" srcset="https://assets.iflscience.com/assets/articleNo/72054/aImg/72811/braingpt-m.jpg" type="image/jpeg"><img src="https://assets.iflscience.com/assets/articleNo/72054/aImg/72811/braingpt-m.jpg" alt="Thought signals in the brain." title="&quot;BrainGPT&quot;" type="image/jpeg" loading="lazy"></picture><figcaption><p>The researchers can decode information in the brain without invasive technology.&nbsp;</p><p>Image Credit: Chaikom/Shutterstock.com</p></figcaption></figure><article><div><p id="isPasted">“Mind reading” may be about to become a reality – and in the most literal sense possible, as a new breakthrough from researchers at the University of Technology Sydney’s GrapheneX-UTS Human-centric Artificial Intelligence Centre sees thoughts transformed into words on a screen.</p><p>“This research represents a pioneering effort in translating raw EEG waves directly into language, marking a significant breakthrough in the field,” <a href="https://www.uts.edu.au/news/tech-design/portable-non-invasive-mind-reading-ai-turns-thoughts-text" target="_blank">said</a> <a href="https://profiles.uts.edu.au/Chin-Teng.Lin" target="_blank" rel="noopener noreferrer">Ching-Ten Lin</a>, Distinguished Professor at the UTS School of Computer Science and Director of the GrapheneX-UTS HAI Centre.</p><p>“It is the first to incorporate discrete encoding techniques in the brain-to-text translation process, introducing an innovative approach to neural decoding,” Lin, who led the research, explained. “The integration with large language models is also opening new frontiers in neuroscience and AI.”</p><p>In a study that has been selected as a spotlight paper at the NeurIPS conference, an annual meeting of researchers in artificial intelligence and machine learning, participants silently read passages of text while an AI model called DeWave – using only their brainwaves as input – projected those words onto a screen.&nbsp;</p><p><span contenteditable="false" draggable="true"><iframe width="560" height="315" src="https://www.youtube.com/embed/crJst7Yfzj4?si=pyhjs0xS8pSsRE65" title="YouTube video player" frameborder="0" allowfullscreen=""></iframe></span><br></p><p>While it’s <a href="https://www.iflscience.com/ai-brain-activity-decoder-can-translate-thoughts-into-written-words-68686" target="_blank">not the first</a> technology to be able to <a href="https://www.iflscience.com/write-book-your-mind-29055" target="_blank">translate brain signals into language</a>, it’s the only one so far to require neither <a href="https://www.iflscience.com/scientists-peek-inside-a-persons-minds-eye-by-reading-their-brain-waves-63037" target="_blank">brain implants</a> nor access to <a href="https://www.iflscience.com/new-brain-scanning-algorithm-can-read-your-thoughts-65945" target="_blank">a full-on MRI machine</a>. It also has an edge on predecessors that require additional input such as eye-tracking software, the researchers say, as the new technology can be used with or without such extras.</p><p>Instead, users need only to wear a cap that records their brain activity via electroencephalogram (EEG) – much more practical and convenient than an eye-tracker (not to mention an MRI machine). That meant the signal was a bit noisier than information gained from implants, the researchers admitted – though even then, the tech performed pretty well in trials. Accuracy measurements using the BLEU algorithm – a way to evaluate the similarity of an original text to a machine-translated output by giving it a score between 0 and 1 – put the new tech at about 0.4.&nbsp;</p><p>That, admittedly, isn’t as good as some of the other options that depend on these more invasive methods. “The model is more adept at matching verbs than nouns,” explained Yiqun Duan, first author on the paper accompanying the research – and “when it comes to nouns, we saw a tendency towards synonymous pairs rather than precise translations, such as ‘the man’ instead of ‘the author’.”&nbsp;</p><p>“We think [these errors are] because when the brain processes these words, semantically similar words might produce similar brain wave patterns,” Duan said.&nbsp;</p><p>But the researchers believe they can improve this accuracy up to 0.9 – a level comparable with traditional language translation programs. They already have an advantage, they suspect, due to carrying out their tests on 29 participants – it may not sound like a lot, but it’s an order of magnitude higher than many other decoding tech trials.</p><p>“Despite the challenges, our model yields meaningful results,” Duan said, “aligning keywords and forming similar sentence structures.”</p><p>The results were shown at the <a href="https://neurips.cc/virtual/2023/index.html" target="_blank">NeurIPS conference</a> and a preprint can be found on <a href="https://arxiv.org/abs/2309.14030v2" target="_blank">ArXiV</a>. It has yet to be peer reviewed.</p></div></article><div><hr><div><h3>ARTICLE POSTED IN</h3></div></div></section><br><div><header><img alt="technology" title="technology" width="16" height="16" src="https://assets.iflscience.com/svg/label/_technology.svg"><h2>More Technology Stories</h2></header></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US nuclear-fusion lab enters new era: achieving 'ignition' over and over (161 pts)]]></title>
            <link>https://www.nature.com/articles/d41586-023-04045-8</link>
            <guid>38673654</guid>
            <pubDate>Sun, 17 Dec 2023 15:55:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/d41586-023-04045-8">https://www.nature.com/articles/d41586-023-04045-8</a>, See on <a href="https://news.ycombinator.com/item?id=38673654">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <p>In December 2022, after <a href="https://www.nature.com/articles/d41586-022-02022-1" data-track="click" data-label="https://www.nature.com/articles/d41586-022-02022-1" data-track-category="body text link">more than a decade of effort and frustration</a>, scientists at the US National Ignition Facility (NIF) <a href="https://www.nature.com/articles/d41586-022-04440-7" data-track="click" data-label="https://www.nature.com/articles/d41586-022-04440-7" data-track-category="body text link">announced that they had set a world record</a> by producing a fusion reaction that released more energy than it consumed — a phenomenon known as ignition. They have now proved that the feat was no accident by replicating it again and again, and the administration of US President Joe Biden is looking to build on this success by establishing a trio of US research centres to help advance the science.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-023-03923-5" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-023-04045-8/d41586-023-04045-8_26526928.jpg"><p>Nuclear-fusion breakthrough: this physicist helped to achieve the first-ever energy gain</p></a>
 </article><p>The stadium-sized laser facility, housed at the Lawrence Livermore National Laboratory (LLNL) in California, has unequivocally achieved its goal of ignition in four out of its last six attempts, creating a reaction that generates pressures and temperatures greater than those that occur inside the Sun.</p><p>“I’m feeling pretty good,” says Richard Town, a physicist who heads the lab’s inertial-confinement fusion science programme at the LLNL. “I think we should all be proud of the achievement.”</p><p>The NIF was designed not as a power plant, but as a facility to recreate and study the reactions that occur during thermonuclear detonations after the United States halted underground weapons testing in 1992. The higher fusion yields are already being used to advance nuclear-weapons research, and have also fuelled enthusiasm about fusion as a limitless source of clean energy. US secretary of state John Kerry called for new international partnerships to advance fusion energy at the COP28 climate summit in Dubai last week, and the US Department of Energy (DOE), which oversees the NIF, followed up by announcing the new research hubs, to be led by Lawrence Livermore, the University of Rochester in New York and Colorado State University in Fort Collins.</p><p>Building the NIF was “a leap of faith” for many, and its success has had a real impact on the fusion community, as well as on public perception, says Saskia Mordijck, a physicist at the College of William and Mary in Willamsburg, Virginia. “In that sense, what is important is that scientists said they could do something, and then they actually did do something.”</p><h2>Hot shots</h2><p>The NIF works by firing 192 laser beams at a frozen pellet of the hydrogen isotopes deuterium and tritium that is housed in a diamond capsule suspended inside a gold cylinder. The resulting implosion causes the isotopes to fuse, creating helium and copious quantities of energy. On 5 December 2022, those fusion reactions for the first time generated more energy — roughly 54% more — than the laser beams delivered to the target.</p><p>The facility set a new record on 30 July when its beams delivered the same amount of energy to the target — 2.05 megajoules — but, this time, the implosion generated 3.88 megajoules of fusion energy, an 89% increase over the input energy. Scientists at the laboratory achieved ignition during two further attempts in October (see ‘A year of progress’). And the laboratory’s calculations suggest that two others in June and September generated slightly more energy than the lasers provided, but not enough to confirm ignition.</p><figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-023-04045-8/d41586-023-04045-8_26532550.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-023-04045-8/d41586-023-04045-8_26532550.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="A year of progress: Timeline of 'ignition' experiments conducted by the US National Ignition Facility since December 2022." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-023-04045-8/d41586-023-04045-8_26532550.jpg">
  <figcaption>
   <p><span>Source: Lawrence Livermore National Laboratory</span></p>
  </figcaption>
 </picture>
</figure><p>For many scientists, the results confirm that the laboratory is now operating in a new regime: researchers can repeatedly hit a goal they’ve been chasing for more than a decade. Tiny variations in the laser pulses or minor defects in the diamond capsule can still allow energy to escape, making for an imperfect implosion, but the scientists now better understand the main variables at play and how to manipulate them.</p><p>“Even when we have these issues, we can still get more than a megajoule of fusion energy, which is good,” says <a href="https://www.nature.com/articles/d41586-023-03923-5" data-track="click" data-label="https://www.nature.com/articles/d41586-023-03923-5" data-track-category="body text link">Annie Kritcher</a>, the NIF’s lead designer on this series of experiments.</p><h2>New hubs</h2><p>It’s a long way from there to providing fusion energy to the power grid, however, and the NIF, although currently home to the world’s largest laser, is not well-suited for that task. The facility’s laser system is enormously inefficient, and more than 99% of the energy that goes into a single ignition attempt is lost before it can reach the target.</p><p>Developing more efficient laser systems is one goal of the DOE’s new inertial-fusion-energy research programme. This month, the agency announced US$42 million over four years to establish three new research centres — each involving a mix of national laboratories, university researchers and industry partners — that will work towards this and other advances.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-022-04440-7" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-023-04045-8/d41586-023-04045-8_25275218.jpg"><p>Nuclear-fusion lab achieves ‘ignition’: what does it mean?</p></a>
 </article><p>This investment is the first coordinated effort to develop not just the technologies, but also the workforce for a future laser-fusion industry, says Carmen Menoni, a physicist who is heading up the hub at Colorado State University.</p><p>So far, most government investments in fusion-energy research have gone towards devices known as tokamaks, which use magnetic fields inside a doughnut-shaped ‘torus’ to confine fusion reactions. This is the approach under development at ITER, an international partnership to build the world’s largest fusion facility near Saint-Paul-lez-Durance, France. Tokamaks have also been the focus of many fusion investments in the private sector, but dozens of companies are pursuing other approaches, such as laser fusion.</p><p>The timing for a dedicated laser-fusion programme is right, says Menoni, and the decision to pursue it wouldn’t have happened without the NIF’s recent success. “We now know it will work,” she says. “What will take time is to develop the technology to a level where we can build a power plant.”</p><p>Back at the NIF, Kritcher’s latest series of experiments features a 7% boost in laser energy, which should, in theory, lead to even larger yields. The first experiment in this series was one of the successful ignitions, on 30 October. Although it didn’t break the record, an input of 2.2 megajoules of laser energy yielded an output of 3.4 megajoules of fusion energy.</p><p>Kritcher chalks up the fact that it didn’t break the record for energy yield to growing pains with the new laser configuration, which is designed to squeeze more energy into the same gold cylinder. Before moving to a larger cylinder, Kritcher says her team is going to focus on changes to the laser pulse that could produce a more symmetrical implosion. “We’ve got four experiments next year,” she says. “Let’s see.”</p>
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Final Speech from The Great Dictator (403 pts)]]></title>
            <link>https://www.charliechaplin.com/en/articles/29-the-final-speech-from-the-great-dictator-</link>
            <guid>38673292</guid>
            <pubDate>Sun, 17 Dec 2023 15:03:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.charliechaplin.com/en/articles/29-the-final-speech-from-the-great-dictator-">https://www.charliechaplin.com/en/articles/29-the-final-speech-from-the-great-dictator-</a>, See on <a href="https://news.ycombinator.com/item?id=38673292">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<article>




<p>
  <iframe width="640" height="480" src="https://www.youtube.com/embed/J7GY1Xg6X20" frameborder="0" allowfullscreen=""></iframe>
</p>


<p>I’m sorry, but I don’t want to be an emperor. That’s not my business. I don’t want to rule or conquer anyone. I should like to help everyone - if possible - Jew, Gentile - black man - white. We all want to help one another. Human beings are like that. We want to live by each other’s happiness - not by each other’s misery. We don’t want to hate and despise one another. In this world there is room for everyone. And the good earth is rich and can provide for everyone. The way of life can be free and beautiful, but we have lost the way.</p>

<p>Greed has poisoned men’s souls, has barricaded the world with hate, has goose-stepped us into misery and bloodshed. We have developed speed, but we have shut ourselves in. Machinery that gives abundance has left us in want. Our knowledge has made us cynical. Our cleverness, hard and unkind. We think too much and feel too little. More than machinery we need humanity. More than cleverness we need kindness and gentleness. Without these qualities, life will be violent and all will be lost…</p>

<p>The aeroplane and the radio have brought us closer together. The very nature of these inventions cries out for the goodness in men - cries out for universal brotherhood - for the unity of us all. Even now my voice is reaching millions throughout the world - millions of despairing men, women, and little children - victims of a system that makes men torture and imprison innocent people.</p>

<figure><img src="https://photo.charliechaplin.com/images/photos/0000/0225/gd_p_140_big.jpg" alt=""><figcaption></figcaption></figure>

<p>To those who can hear me, I say - do not despair. The misery that is now upon us is but the passing of greed - the bitterness of men who fear the way of human progress. The hate of men will pass, and dictators die, and the power they took from the people will return to the people. And so long as men die, liberty will never perish…</p>

<p>Soldiers! don’t give yourselves to brutes - men who despise you - enslave you - who regiment your lives - tell you what to do - what to think and what to feel! Who drill you - diet you - treat you like cattle, use you as cannon fodder. Don’t give yourselves to these unnatural men - machine men with machine minds and machine hearts! You are not machines! You are not cattle! You are men! You have the love of humanity in your hearts! You don’t hate! Only the unloved hate - the unloved and the unnatural! Soldiers! Don’t fight for slavery! Fight for liberty!</p>

<p>In the 17th Chapter of St Luke it is written: “the Kingdom of God is within man” - not one man nor a group of men, but in all men! In you! You, the people have the power - the power to create machines. The power to create happiness! You, the people, have the power to make this life free and beautiful, to make this life a wonderful adventure.</p>

<p>Then - in the name of democracy - let us use that power - let us all unite. Let us fight for a new world - a decent world that will give men a chance to work - that will give youth a future and old age a security. By the promise of these things, brutes have risen to power. But they lie! They do not fulfil that promise. They never will!</p>

<p>Dictators free themselves but they enslave the people! Now let us fight to fulfil that promise! Let us fight to free the world - to do away with national barriers - to do away with greed, with hate and intolerance. Let us fight for a world of reason, a world where science and progress will lead to all men’s happiness. Soldiers! in the name of democracy, let us all unite!</p>

<p><em>Final speech from The Great Dictator Copyright © Roy Export S.A.S. All rights reserved</em></p>

<hr>

<p><a href="https://www.charliechaplin.com/en/films/7-The-Great-Dictator">The Great Dictator</a> was Chaplin’s first film with dialogue.    Chaplin plays both a little Jewish barber, living in the ghetto, and Hynkel, the dictator ruler of Tomainia. In his autobiography Chaplin quotes himself as having said: “One doesn’t have to be a Jew to be anti Nazi.  All one has to be is a normal decent human being.”</p>

<p>Chaplin and Hitler were born within a week of one another.  “There was something uncanny in the resemblance between the Little Tramp and Adolf Hitler, representing opposite poles of humanity, ” writes Chaplin biographer David Robinson, reproducing an unsigned article from The Spectator dated 21st April 1939:<br>
“Providence was in an ironical mood when, fifty years ago this week, it was ordained that Charles Chaplin and Adolf Hitler should make their entry into the world within four days of each other….Each in his own way has expressed the ideas, sentiments, aspirations of the millions of struggling citizens ground between the upper and the lower millstone of society. (…) Each has mirrored the same reality – the predicament of the “little man” in modern society.  Each is a distorting mirror, the one for good, the other for untold evil.”</p>

<p>Chaplin spent many months drafting and re-writing the speech for the end of the film, a call for peace from the barber who has been mistaken for Hynkel. Many people criticized the speech, and thought it was superfluous to the film.  Others found it uplifting. Regrettably Chaplin’s words are as relevant today as they were in 1940.</p>

<h4 id="transcript-of-charlie-chaplins-final-speech-in-the-great-dictatorenfilms7-the-great-dictator">Transcript of Charlie Chaplin’s Final Speech in <a href="https://www.charliechaplin.com/en/films/7-The-Great-Dictator">The Great Dictator</a>
</h4>



</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Evaluating New Software Forges (137 pts)]]></title>
            <link>https://notgull.net/finding-a-forge/</link>
            <guid>38672386</guid>
            <pubDate>Sun, 17 Dec 2023 12:26:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://notgull.net/finding-a-forge/">https://notgull.net/finding-a-forge/</a>, See on <a href="https://news.ycombinator.com/item?id=38672386">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p>What options <em>are</em> there other than <a href="https://github.com/">GitHub</a>?</p>

<p>Oh boy, I sure do love contributing to open source software on the largest software forge in the world! I hope they haven’t started down the slow and painful process of <a href="https://en.wikipedia.org/wiki/Enshittification">enshittification</a> by following vague, ill-defined industry trends!</p>

<p><img src="https://notgull.net/images/github-home.png" alt="[GitHub]'s home page"></p>

<p>Wait, what’s that? Computer! Enhance!</p>

<p><img src="https://notgull.net/images/github-ai.png" alt="The same image but zoomed in on AI"></p>

<p>Well, I guess I’m ready to find a new forge!</p>

<h2 id="software-host-hellscape">Software Host Hellscape</h2>

<p>In all seriousness, I’ve been looking to move off of <a href="https://github.com/">GitHub</a> for a while now. Let me be clear, <a href="https://github.com/">GitHub</a> is still far and away the best website for open source discovery. Not to mention, its CI offerings are very nice, especially for something free. Yes, there are better paid CI offerings, but for something that costs zero dollars I’ve found it incredibly useful.</p>

<p>However, one thing has made me skeptical of <a href="https://github.com/">GitHub</a> is its “<a href="https://github.com/features/copilot">Copilot</a>” offering. I’ll admit, I was in the beta program for Copilot, and found it really neat. Being able to write large amounts of code from small comments was very nice, even if it was really bad practice.</p>

<p>Then I found out it was <a href="https://www.zdnet.com/article/is-github-copilots-code-legal-ethically-right/">training on GPL-licensed data</a>, which left a pretty bad taste in my mouth. In addition to the fact that I’m increasingly uncomfortable with hosting my free software on a closed source forge, run by Microsoft.</p>

<p>Let’s take a look at everybody else.</p>

<h2 id="gitlab-gauss">GitLab Gauss</h2>

<p><a href="https://about.gitlab.com/">GitLab</a> is the original <a href="https://github.com/">GitHub</a> competitor. The Linux to Microsoft’s Windows, or the MariaDB to Oracle’s MySQL. This has made it the most popular <a href="https://github.com/">GitHub</a> competitor by far, by virtue of people vocally quitting <a href="https://github.com/">GitHub</a> in favor of [GitLab].</p>

<p>Unfortunately, I didn’t really consider GitLab when I was finding a place to move. First of all, <a href="https://about.gitlab.com/blog/2016/07/20/gitlab-is-open-core-github-is-closed-source/">they aren’t actually open source</a>. They’re “open core”, which, I admit, is better than closed source. However, like I said, I’m uncomfortable building free software on infrastructure that isn’t.</p>

<p>I know I can download GitLab and set it up on my own server. However, I’m a software developer, not a sysadmin. I want to spend my time developing software, not putting out fires and paying AWS bills for the rest of time.</p>

<p>Also, GitLab has adopted the unfortunate strategy of “following along with whatever <a href="https://github.com/">GitHub</a> does”. They’ve tried to jump onto the bandwagon so frequently, they’ve gotten splinters. For instance, what happens when we check their homepage?</p>

<p><img src="https://notgull.net/images/gitlab-home.png" alt="GitLab's home page"></p>

<p>Computer! Do the thing again!</p>

<p><img src="https://notgull.net/images/gitlab-ai.png" alt="Same image but with &quot;AI&quot; zoomed in"></p>

<p>Good golly, it’s even the same wording! Yeah, I’ll pass.</p>

<h2 id="sourcehut-scramble"><a href="https://sr.ht/">SourceHut</a> Scramble</h2>

<p><a href="https://sr.ht/"><code>sr.ht</code></a> takes the opposite approach as GitLab. Instead of trying to follow along with <a href="https://github.com/">GitHub</a>’s trends, it’s elected to do go in the other direction. Whenever <a href="https://github.com/">GitHub</a> does something, <a href="https://sr.ht/">SourceHut</a> does the exact opposite.</p>

<p>Pull requests? Too centralized, let’s construct a suitable code contribution system around email. Discussion? Why not IRC, it’s been around since the Bronze Age. Get rid of Mercurial support? Not interested.</p>

<p>I really like <a href="https://sr.ht/">SourceHut</a>. When you go to their homepage, they’re not showing off their fancy CSS effects or telling you about their AI offerings. They give you a simple user interface and some of the projects they host.</p>

<p><img src="https://notgull.net/images/sourcehut-home.png" alt="sr.ht's homepage"></p>

<p>There was also much to impress me. Their CI offerings are better than <a href="https://github.com/">GitHub</a>, which alone justified me paying the humble $2/month price tag. Rather than needing a complicated YAML file to run a CI system, it’s just cloning Git repos and running commands. It’s delightfully simple yet powerful. Having native BSD and Plan9 runners doesn’t quite make up for its inability to run Windows, but I’m sure I can work around that.</p>

<p>Not to mention, <a href="https://sr.ht/">SourceHut</a> has the <em>second</em> best repository discovery system. When I go to <code>sr.ht</code>’s “explore” tab, I’m immediately greeted by a slew of interesting projects. Whether it’s a <a href="https://git.sr.ht/~crc_/retroforth">powerful Forth dialect</a> that brings a lot of genuinely exciting ideas to the table, or a <a href="https://sr.ht/~mcf/cproc/">tiny C11 compiler</a> written in simple ANSI C, I’m always amazed whenever I open up that tab.</p>

<p>I liked it so much that <a href="https://hachyderm.io/@notgull/111207497257139313">I announced that I was moving my personal projects to SourceHut</a>. However, after moving my <a href="https://crates.io/crates/theo"><code>theo</code></a> project to <a href="https://sr.ht/">SourceHut</a>, I found myself dissatisfied with a few things.</p>

<p>For one, the email-based workflow was a lot clunkier than I expected. In theory, building code contribution on top of a standard protocol that’s been around since the 60’s sounds like a good idea. In practice, it’s <a href="https://social.treehouse.systems/@marcan/109863991681394714">a lot clunkier</a> than you’d expect, especially since most modern email clients are simply not built to read and write code.</p>

<p>After trying it for myself I can see that it might turn off a lot of people from contributing. I’m already losing a lot of potential contributors by moving off of <a href="https://github.com/">GitHub</a>. I don’t need those remaining contributors to also be turned off by a workflow completely different than what they’re probably used to.</p>

<p>Still, I can imagine this workflow working for many people, especially ones who already have a decent setup for email-based projects like Linux.</p>

<h2 id="codeberg">Codeberg</h2>

<p><a href="https://codeberg.org/">Codeberg</a> is a public instance of <a href="https://forgejo.org/">Forgejo</a>, which is in turn a fork of <a href="https://about.gitea.com/">Gitea</a>. It’s got a pretty nice interface similar enough to <a href="https://github.com/">GitHub</a>’s. It’s got the familiar pull-request-based contribution interface. The CI is <em>good enough</em>, I suppose. Docker containers aren’t the best CI environment, but I can certainly think of worse.</p>

<p><img src="https://notgull.net/images/codeberg-home.png" alt="Codeberg's home page"></p>

<p>So what’s not to like?</p>

<p>My main problem is that <a href="https://codeberg.org/">Codeberg</a> has a very limited CI capacity, and I have a lot of projects that require significant testing. <a href="https://crates.io/crates/theo"><code>theo</code></a>, for instance, requires these things to be tested:</p>

<ul>
  <li>Make sure it compiles on Windows, Mac, Linux, WASM, Redox, and whatever oblique platform people run Rust on nowadays.</li>
  <li>Check the various different backends that <a href="https://crates.io/crates/theo"><code>theo</code></a> supports: pure software rendering, <a href="https://crates.io/crates/wgpu"><code>wgpu</code></a> and OpenGL. Not to mention all of the different interfaces to OpenGL, so <code>wgl</code>, GLX, EGL…</li>
  <li>Check formatting and linting.</li>
</ul>

<p>…which doesn’t even cover testing. For rendering frameworks like <a href="https://crates.io/crates/theo"><code>theo</code></a>, you want to have some pre-defined rendering programs that render your code to images. That way, you can regenerate these images and compare against an existing set of images in order to check for regressions.</p>

<p>This isn’t a practical concern, although it really should be. It’s a moral concern. You have an organization like <a href="https://codeberg.org/">Codeberg</a>, donating a significant amount of time and resources to try to make a positive difference in the world of software. Now, here I am, sucking up all of those compute resources for my insignificant little projects.</p>

<p>Of course, while pondering this moral concern, I realized that I’ve locked myself into a <a href="https://en.wikipedia.org/wiki/Catch-22_(logic)">Catch-22</a>. I can’t use any independent project’s CI because of my concerns that I would drain too many of their resources. On the other hand, I can’t use any large company’s CI because I don’t want to host my project with a large company. I can’t self host, because that would be a pain.</p>

<p>Would it?</p>

<h2 id="self-hosting-gitea">Self-Hosting Gitea</h2>

<p>I said I didn’t want to self-host. I worked in IT for two years, so I’ve already gotten my fill of fighting with both servers and people.</p>

<p>However, in a Discord I’m in, an acquaintance of mine talked about how they set up <a href="https://about.gitea.com/">Gitea</a> and <a href="https://www.drone.io/">Drone</a> CI on a school Kubernetes cluster they had access to. I mentioned my predicaments in finding a forge service, and they said that it was only two configuration files.</p>

<p>That tempted me. Not enough to deal with the absolute nightmare that is Kubernetes, but enough for me to rent out a couple of <a href="https://en.wikipedia.org/wiki/Timeline_of_Amazon_Web_Services">Lightsail</a> servers to experiment.</p>

<p>I’d like to say that I set up the entire thing in a weekend, but it wasn’t that simple. Sure, it was easy enough to install <a href="https://about.gitea.com/">Gitea</a> and <a href="https://www.drone.io/">Drone</a> in Docker containers. Sure, it wasn’t too hard after that to set up my DNS records to forward <a href="https://src.notgull.net/">src.notgull.net</a> to that new serer. Yeah, it’s probably hacky to set up my CI system on a public cloud, but as long as I keep people from abusing it that aren’t me it shouldn’t be <em>too</em> bad, right?</p>

<p>Of course, I forget my crucial weakness: my perfectionism. Sure, my site looks pretty good… but it looks a bit ugly. Let’s play around with themes for three days until I find one I like. Oh, the logo doesn’t look good with my new theme. Let’s convert the MS paint drawing I call my avatar into SVG and then set it up to be this site’s logo. Oh, Gitea has a weird templating system; let’s compile it from scratch!</p>

<p><img src="https://notgull.net/images/srcnotgull-home.png" alt="src.notgull.net home"></p>

<p>At the end of this week(!)long process, I had a somewhat functional <a href="https://src.notgull.net/">Git forge</a>, with the following features:</p>

<ul>
  <li>A decent Linux <a href="https://www.drone.io/">Drone</a> CI setup.</li>
  <li>A <a href="https://docs.renovatebot.com/">Dependabot clone</a> for automatic updates.</li>
  <li>A system that allows everyone to create an account and open issues, but doesn’t allow people to create repos or mess with the CI.
    <ul>
      <li>If you want to create a repo for PR’s sake, email me at dev at notgull dot net and I can set you up.</li>
    </ul>
  </li>
  <li>A mirror to <a href="https://github.com/">GitHub</a>, so I can still take advantage of their code discovery.
    <ul>
      <li>Yeah yeah, I know, but keeping an arm’s length from <a href="https://github.com/">GitHub</a> is a win in my book.</li>
    </ul>
  </li>
</ul>

<p>I’ve uploaded a lot of my code to there, and it seems to be working well so far. I have to admit, it is somewhat liberating to have full control of how your code is forged.</p>

<h2 id="parting-shots">Parting Shots</h2>

<p>I still consider this to be in the “experimental” stage. If it ends up being too inconvenient or too expensive, I’ll probably move it somewhere else. Still, having my own space for code that I can do whatever I want with is very nice. Let’s hope it keeps working well and this blog post doesn’t age like milk.</p>

<blockquote>
  <p>All photos are from public websites and fall under free use, as this is a review.</p>
</blockquote>


  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The origin of the law of torture: A cautionary tale (117 pts)]]></title>
            <link>https://daviddfriedman.substack.com/p/torture</link>
            <guid>38670866</guid>
            <pubDate>Sun, 17 Dec 2023 06:40:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://daviddfriedman.substack.com/p/torture">https://daviddfriedman.substack.com/p/torture</a>, See on <a href="https://news.ycombinator.com/item?id=38670866">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>People in the past worried about convicting the innocent too. In the early Middle Ages they had a solution: Let God judge. A defendant could be subjected to an ordeal such as plunging his hand into boiling water, carrying a red hot iron, being dumped bound into water. Various passages in the Bible were interpreted to imply that God would reveal guilt (hand injured or body sank) or innocence (not injured, floated). Since God was omniscient it was an approach that guaranteed a correct verdict.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-139789134" href="https://daviddfriedman.substack.com/p/torture#footnote-1-139789134" target="_self" rel="">1</a></span></p><p>The use of ordeals was eventually abandoned on theological grounds. A more careful examination of the biblical passages found little support for it and it could be viewed as an attempt by humans to compel God to serve them, religiously dubious. In 1215 the fourth Lateran council rejected the religious legitimacy of judicial ordeals and banned priests from participating in them. Over the next few decades most European countries abandoned their use.</p><p>That left medieval judicial systems with the problem of finding another way of being certain a defendant was guilty. The solution was to impose a very high standard of proof,&nbsp; evidence “clear as the noonday sun.” Conviction required either two unimpeachable eyewitnesses to the crime or a voluntary confession. Circumstantial evidence, however strong, was insufficient.</p><blockquote><p><span>In the history of Western culture no legal system has ever made a more valiant effort to perfect its safeguards and thereby to exclude completely the possibility of mistaken conviction. But the Europeans learned in due course the inevitable lesson. They had set the level of safeguard too high. They had constructed a system of proof that could as a practical matter be effective only in cases involving overt crime or repentant criminals. Because society cannot long tolerate a legal system that lacks the capacity to convict unrepentant persons who commit clandestine crimes, something had to be done … .(</span><a href="http://digitalcommons.law.yale.edu/fss_papers/543" rel="">Langbein 1978</a><span>)</span></p></blockquote><p>The solution was the law of torture. Once the court had half-proof, one eyewitness or the equivalent in circumstantial evidence, the defendant could be tortured into confessing. A confession under torture was not voluntary so did not count, but that problem could be dealt with. Stop the torture and the next day ask the defendant if he is still willing to confess. Since he is now not being tortured the confession is voluntary. If he doesn’t confess, torture him again.</p><p>John Langbein, my source for this account, offers a parallel story in modern law. Two hundred years ago, jury trials were short:</p><blockquote><p>In the Old Bailey in the 1730s we know that the court routinely processed between twelve and twenty jury trials for felony in a single day. A single jury would be impaneled and would hear evidence in numerous unrelated cases before retiring to formulate verdicts in all. Lawyers were not employed in the conduct of ordinary criminal trials, either for the prosecution or the defense. The trial judge called the witnesses (whom the local justice of the peace had bound over to appear), and the proceeding transpired as a relatively unstructured “altercation” between the witnesses and the accused. In the 1790s, when the Americans were constitutionalizing English jury trial, it was still rapid and efficient. “The trial of Hardy for high treason in 1794 was the first that ever lasted more than one day, and the court seriously considered whether it had any power to adjourn… .”</p></blockquote><p><span>Over the years since trials have become longer and much more complicated, at least in part to reduce the risk of convicting the wrong person. Patricia Hearst’s trial for bank robbery lasted forty days. That was unusually long, but the average felony jury trial in Los Angeles in 1968 took 7.2 days, more than a hundred times the length of a felony trial in the Old Bailey in the 1730’s. If every felony conviction in the U.S. took that long, felony trials alone would require the full time efforts of more than the total number of judges in the state and federal systems</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2-139789134" href="https://daviddfriedman.substack.com/p/torture#footnote-2-139789134" target="_self" rel="">2</a></span><span> and close to a million jurors, court attendants, and the like. Not impossible but very expensive.</span></p><p>The American legal system found a less expensive alternative. Like its medieval predecessor, it substituted confession for trial. The medieval confession was motivated by the threat of torture. The modern version, a plea bargain, is motivated by the threat of a much more severe sentence if the defendant insists on a trial and is convicted. Like the medieval version, it preserves the form — every felony defendant has the right to a jury trial, a lawyer, and all the paraphernalia of the modern law of criminal defense — but not the substance. Conviction after a lengthy and careful jury trial is arguably evidence of guilt beyond a reasonable doubt. The willingness to accept a sentence of a year, possibly a year already served while awaiting trial, instead of the risk of ten years if convicted is not. Currently in the U.S. about ninety-seven percent of felony convictions are the result of plea bargains, three percent of jury trials.</p><p>Under both Athenian and Roman law, slave testimony could only be taken under torture. Presumably the theory was that slaves were interrogated to get evidence against their owners, the owner had ways of putting pressure on the slave, so torture was needed to get the slave to tell the truth. In Imperial Chinese law, not only the defendant but also witnesses could be tortured.</p><p>Our main source of information on Athenian law consists of orations written by professional orators to be memorized and delivered by a party to a law suit. There is a surviving oration which claims that slave testimony under torture is perfectly reliable, that there has never been a case where it turned out to be false. There is another oration making the obvious argument on the other side, that such testimony is worthless since the slave will say whatever the torturer wants him to say.</p><p>They were both written by the same orator.</p><p>People in other legal systems that used torture were also aware of the problem. There is a collection of Chinese cases compiled in the 13th century for the use of magistrates. Many of them are cases where a clever judge realizes that an innocent person has been forced to confess under torture and figures out who is really guilty.</p><p>That raises an obvious question: If they saw the problem with torture, why did they continue to employ it? One answer is that extracting information might only have been an excuse, that the real purpose was to punish someone without having to first convict him. That is a possible explanation in some contexts. But it does not explain contexts where the person being tortured was not the suspect but a potential witness.</p><p>A second possible explanation is the belief that a competent interrogator could distinguish a real confession from a fake one. That strikes me as the most likely explanation in the Roman and Athenian cases, where it was the defendant's slave, not the defendant, who was being interrogated.</p><p>A third explanation is that torture might produce information that could be checked. That is the situation in the hypothetical cases sometimes offered in defense of the use of torture: The suspect is being forced to say where the kidnap victim or the time bomb is concealed. More plausibly, where the loot is hidden.</p><p>An example of this approach occurs in the law of the Visigoths, the earliest of the surviving Germanic law codes. Before a suspect could be tortured the accuser had to provide the judge with details of the crime that an innocent defendant would not have known. The defendant's confession was only accepted if it matched the details; if the accuser had made the details public, the defendant could not be tortured. How satisfactory the system was for the defendant would depend on how severe the torture was and how much permanent damage it might do to him but it at least was a way of distinguishing a true confession from a false one. The same approach is used in modern law enforcement, where a confession is validated by the fact that it contains information only a guilty defendant would have.</p><p>Both the Visigothic and the modern versions depend on the honesty of the people conducting the interrogation. A policeman who extracts a confession by either physical pressure or the threat of additional charges can make it more convincing by leaking the relevant information to the defendant in the course of the interrogation. That is an argument for recording all interrogations and making the recordings available to the defense, an option not available to the Visigoths. I do not know if they employed the period equivalent of neutral witnesses to the interrogation.</p><p>The problem with arguments for the use of torture, ancient and modern, is that although one can imagine situations where it would be justified, once legal it is not likely to be limited to such situations. </p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Apollo Syndrome (123 pts)]]></title>
            <link>https://www.teamtechnology.co.uk/tt/t-articl/apollo.htm</link>
            <guid>38670760</guid>
            <pubDate>Sun, 17 Dec 2023 06:16:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.teamtechnology.co.uk/tt/t-articl/apollo.htm">https://www.teamtechnology.co.uk/tt/t-articl/apollo.htm</a>, See on <a href="https://news.ycombinator.com/item?id=38670760">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
<p>
<a href="https://www.metarasa.com/team-dynamics-assessment/questionnaire/">
  <video controls="" autoplay="" muted="">
    <source src="https://www.teamtechnology.co.uk/tdasm.mp4" type="video/mp4">
    Your browser does not support HTML5 video.
  </video>
</a>
</p>






<p>
This page describes 'The Apollo Syndrome', a phenomenon discovered by Dr Meredith
Belbin where teams of highly capable individuals can, collectively, perform
badly. </p>
<p>Dr Meredith Belbin is one of the original 'gurus' of Team Building.
In his first book on Management Teams (Belbin, 1981) he reported some unexpectedly
poor results with teams formed of people who had sharp, analytical minds
and high mental ability - he called this the Apollo Syndrome. </p>
<p>His criteria for selecting these teams have elements in common with
criteria for selecting IT, academic or scientific staff - using ability and aptitude tests to select
those with high analytical skills. The initial perception of Belbin's Apollo
teams was that they were bound to win in the team competitions. However,
the results were quite the reverse, and the Apollo teams often finished
near the bottom of eight teams. </p>
<p>This failure seemed to be due to certain flaws in the way the team operated:
</p>
<ul>
<li>They spent excessive time in abortive or destructive debate, trying
to persuade other team members to adopt their own view, and demonstrating
a flair for spotting weaknesses in others' arguments. This led to the discussion
equivalent of '<a href="#The Deadly Embrace">the deadly embrace</a>'. </li>
<li>They had difficulties in their decision making, with little coherence
in the decisions reached (several pressing and necessary jobs were often
omitted). </li>
<li>Team members tended to act along their own favourite lines without
taking account of what fellow members were doing, and the team proved difficult
to manage. </li>
<li>In some instances, teams recognised what was happening but over compensated
- they avoided confrontation, which equally led to problems in decision
making. </li>
</ul>
<h3><b>How Apollo teams succeed</b></h3>
<p>There were successful Apollo teams, however, that were characterised
by </p>
<ul>
<li>the absence of highly dominant individuals, and </li>
<li>a particular style of leadership. </li>
</ul>
<p>Successful leaders were suspicious and sceptical people who sought to
impose some shape or pattern on group discussion, and on the outcome of
group activities. They focused attention on the setting of objectives and
priorities, and shaping the way team effort was applied. Rather than 'drawing
out' team members, the successful leaders were tough, discriminating people
who could both hold their ground in any company, yet not dominate the group.
</p>
<p>A key lesson from Belbin's work is that putting together a team of the
cleverest individuals does not necessarily produce the best results, and
the team needs to be designed ensuring that there is a blend of team roles.
</p>
<h3><b>Apollo Syndrome (Version 2)</b></h3>
<p>The term 'Apollo Syndrome' has also been used to describe the condition
where someone has an overly important view of their role within a team.
It is based on the (supposed) claim of someone to have played a vital role
in the success of NASA's Apollo missions to the Moon, where scientists
had to work all through the night on many occasions, battling against fatigue.
One person claimed a vital role to the whole programme - by making the
coffee that kept them awake! </p>
<p>Perhaps a 'Double Apollo' is where a team is composed of highly capable
people, that achieves little, but claims great success! </p>
<a name="The Deadly Embrace"><h4><b>Definition of The Deadly Embrace</b></h4>
This is a term used in computing some years ago to signify a problem
between two computer programs - where each prevents the other from making
progress.
<blockquote><i>What happens is that Program A takes exclusive control of record
1, and program B takes record 2. Program A then tries to get exclusive
access to record 2, but as this is under exclusive control of the other
program, it can't. The program then waits until record 2 is released. Meanwhile,
program B tries to get exclusive control of record 1, but can't, as it
is under the exclusive control of program A. Program B waits until record
1 is released. Therefore, neither program can make any progress because
it is waiting for the other program to give way. A similar situation can
occur in discussions if each person is trying to get the other to concede
the flaws in his/her argument, without conceding the flaws in his own.
The way out of this situation is to look for the points of agreement, rather
than trying to spot flaws. </i>
</blockquote>
<h4><b>References</b></h4>
<p>Management Teams - Why They Succeed or Fail, (Belbin, 1981), ISBN: 0-7506-0253-8
</p>

<br>

</a><p><a name="The Deadly Embrace">©<span id="copydate">2013</span> Team Technology. </a><a href="https://www.teamtechnology.co.uk/cookies.php">Privacy policy and cookies</a>.</p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI bots are now outperforming humans in solving CAPTCHAs (345 pts)]]></title>
            <link>https://arxiv.org/abs/2307.12108</link>
            <guid>38670465</guid>
            <pubDate>Sun, 17 Dec 2023 05:10:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2307.12108">https://arxiv.org/abs/2307.12108</a>, See on <a href="https://news.ycombinator.com/item?id=38670465">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2307.12108.pdf">Download PDF</a></p><blockquote>
            <span>Abstract:</span>For nearly two decades, CAPTCHAs have been widely used as a means of protection against bots. Throughout the years, as their use grew, techniques to defeat or bypass CAPTCHAs have continued to improve. Meanwhile, CAPTCHAs have also evolved in terms of sophistication and diversity, becoming increasingly difficult to solve for both bots (machines) and humans. Given this long-standing and still-ongoing arms race, it is critical to investigate how long it takes legitimate users to solve modern CAPTCHAs, and how they are perceived by those users.
<br>In this work, we explore CAPTCHAs in the wild by evaluating users' solving performance and perceptions of unmodified currently-deployed CAPTCHAs. We obtain this data through manual inspection of popular websites and user studies in which 1,400 participants collectively solved 14,000 CAPTCHAs. Results show significant differences between the most popular types of CAPTCHAs: surprisingly, solving time and user perception are not always correlated. We performed a comparative study to investigate the effect of experimental context -- specifically the difference between solving CAPTCHAs directly versus solving them as part of a more natural task, such as account creation. Whilst there were several potential confounding factors, our results show that experimental context could have an impact on this task, and must be taken into account in future CAPTCHA studies. Finally, we investigate CAPTCHA-induced user task abandonment by analyzing participants who start and do not complete the task.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Yoshimichi Nakatsuka [<a href="https://arxiv.org/show-email/59287c22/2307.12108">view email</a>]      <br>    <strong>[v1]</strong>
        Sat, 22 Jul 2023 15:36:13 UTC (1,815 KB)<br>
</p></div></div>]]></description>
        </item>
    </channel>
</rss>