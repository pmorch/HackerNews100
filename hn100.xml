<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 06 Sep 2024 01:30:07 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[HNInternal: Tell HN: Burnout is bad to your brain, take care (165 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=41461499</link>
            <guid>41461499</guid>
            <pubDate>Thu, 05 Sep 2024 23:59:02 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=41461499">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="41461499">
      <td><span></span></td>      <td><center><a id="up_41461499" href="https://news.ycombinator.com/vote?id=41461499&amp;how=up&amp;goto=item%3Fid%3D41461499"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=41461499">Tell HN: Burnout is bad to your brain, take care</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_41461499">165 points</span> by <a href="https://news.ycombinator.com/user?id=tuyguntn">tuyguntn</a> <span title="2024-09-05T23:59:02.000000Z"><a href="https://news.ycombinator.com/item?id=41461499">1 hour ago</a></span> <span id="unv_41461499"></span> | <a href="https://news.ycombinator.com/hide?id=41461499&amp;goto=item%3Fid%3D41461499">hide</a> | <a href="https://hn.algolia.com/?query=Tell%20HN%3A%20Burnout%20is%20bad%20to%20your%20brain%2C%20take%20care&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=41461499&amp;auth=65296f0963350844d44e2445b8720b797b03478f">favorite</a> | <a href="https://news.ycombinator.com/item?id=41461499">77&nbsp;comments</a>        </span>
              </td></tr>
    <tr><td></td></tr><tr><td colspan="2"></td><td><div><p>I am depressed and burned out for quite some time already, unfortunately my brain still couldn't recover from it.</p><p>If I summarize the impact of burnout to my brain:</p><p>- Before: I could learn things pretty quickly, come up with solutions to the problems, even be able to see common patterns and see bigger underlying problems</p><p>- After: can't learn, can't work, can't remember, can't see solutions for trivial problems (e.g. if your shirt is wet, you can change it, but I stare at it thinking when it is going to get dried up)</p><p>Take care of your mental health</p></div></td></tr>        <tr><td></td></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Early Days of Valve from a Woman Inside (117 pts)]]></title>
            <link>https://medium.com/@monicah428/the-early-days-of-valve-from-a-woman-inside-bf80c6b47961</link>
            <guid>41460276</guid>
            <pubDate>Thu, 05 Sep 2024 20:47:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://medium.com/@monicah428/the-early-days-of-valve-from-a-woman-inside-bf80c6b47961">https://medium.com/@monicah428/the-early-days-of-valve-from-a-woman-inside-bf80c6b47961</a>, See on <a href="https://news.ycombinator.com/item?id=41460276">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><figure><figcaption>Valve’s Half-Life on CD-ROM, still available on Amazon 26 years after its original release. The author worked side-by-side with Gabe Newell and Mike Harrington to launch Valve’s debut product and establish Valve as a major player in the game industry.</figcaption></figure><div><a rel="noopener follow" href="https://medium.com/@monicah428?source=post_page-----bf80c6b47961--------------------------------"><div aria-hidden="false"><p><img alt="Monica Harrington" src="https://miro.medium.com/v2/resize:fill:88:88/1*EoXOOMWLEiPwBBaYA1HQXA.jpeg" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a></div><p id="ce96"><em>Almost 30 years ago, Monica Harrington guided the marketing and business development efforts for </em><a href="https://www.valvesoftware.com/en/about" rel="noopener ugc nofollow" target="_blank"><em>Valve</em></a><em>, which has become the biggest player in the PC game industry. This is her story.</em></p><p id="f68c">Almost 30 years ago, a small company was founded near Seattle WA. <a href="https://www.washingtonpost.com/news/the-switch/wp/2014/01/06/gabe-newell-on-valves-intimate-relationship-with-its-customers/" rel="noopener ugc nofollow" target="_blank">Gabe Newell</a> and my now ex <a href="https://www.google.com/search?client=safari&amp;rls=en&amp;q=Mike+Harrington&amp;ie=UTF-8&amp;oe=UTF-8&amp;si=ACC90nzhFW-fOieS-I060ZQl1KsOy-txPmX_lTcmXNvaH1ksSP08B9BLNakMqXf1x-9DnuejP1RHVsRWyYDblVs8NZmZbkCBQ_Zo0glVi6BIas5v1jUQMw8%3D&amp;ictx=1&amp;ved=2ahUKEwj4p5bw0YKIAxWAJTQIHVghKf4Q_coHegQILBAD" rel="noopener ugc nofollow" target="_blank">Mike Harrington</a> were the official cofounders.</p><p id="1988">I was on a two-month leave from my job at Microsoft, where I was a group marketing manager in the Consumer Division, overseeing a product portfolio that included Microsoft Games.</p><figure><figcaption>The Microsoft Market Maker Award, chipped but still standing</figcaption></figure><p id="0799">I’d worked for Microsoft for nine years by then. My career was in high gear. A year earlier I’d been honored with the Market Maker award as the marketer who “added the most to the (1600+-person) Consumer Division’s bottom line.” I loved my job but I was also tired, so when Microsoft announced a leave program in the Spring of 1996, I was all in, hoping that a couple of months of paid leave would be invigorating, ideally with some travel and a break from the remodeling project that had left our new home a mess.</p><p id="28dd">My husband Mike had other plans.</p><p id="919c">He’d worked in the game industry before joining Microsoft the same year I did, and he had a dream of starting a game company. Mike decided to use his break to figure out if he wanted to stay at Microsoft or embark on a huge new adventure doing something else. During the break he started to solidify plans with Gabe Newell to really make it happen. Our travel plans quickly disappeared.</p><p id="6844">Gabe and Mike had met when Mike was working on printer drivers for the OS/2 operating system and the three of us had hung around together beginning a few years earlier, including a weekend in Eastern WA with Gabe’s then girlfriend. Mike was also friends with <a href="https://en.wikipedia.org/wiki/Michael_Abrash" rel="noopener ugc nofollow" target="_blank">Mike Abrash</a>, an industry luminary who was then working at <a href="https://en.wikipedia.org/wiki/Id_Software" rel="noopener ugc nofollow" target="_blank">id Software</a>, the independent studio behind the hugely popular Doom franchise.</p><p id="f1a5">Because of his relationship with Abrash, Mike was able to quickly secure agreement to build Valve’s first product using id’s game engine. That was a huge deal and essentially catapulted Valve and Gabe and Mike into serious company and game development.</p><p id="babf">When I returned to Microsoft, Mike and Gabe and the small team they quickly assembled were well on their way to planning their first product.</p><p id="727c">The original idea Mike had was that Valve would create a product that would be launched by Microsoft. It turned out that Microsoft had little appetite for doing a deal with its own former employees, which meant that Valve would need to sign a publishing agreement with one of Microsoft’s competitors.</p><p id="b469">Because of my role overseeing Games Marketing for Microsoft, the interplay with Valve was complicated. The investment Mike and I were making in Valve was substantial so I knew I couldn’t not be involved, but I also needed boundaries. When I returned to Microsoft, I went directly to the head of games and to the VP of our division to say that my husband and Gabe Newell were starting a games business, and that I would be helping them, and that I’d be open to any changes Microsoft might want to make in terms of my role as a result. Both <a href="https://en.wikipedia.org/wiki/Ed_Fries" rel="noopener ugc nofollow" target="_blank">Ed Fries</a> and my managers were friendly and reassuring. At the time, there were literally hundreds of games published each year that vanished into the ether. Valve was a tiny player, and I’m sure from Ed’s and everyone else’s perspective, likely to remain so.</p><p id="2637">I was traveling for Microsoft on a cold wintry day in Seattle when Mike called to tell me that he and the team had just finished meeting with the head of <a href="https://www.wired.com/story/sierra-online-ken-williams-interview-memoir/" rel="noopener ugc nofollow" target="_blank">Sierra Online</a>, which was a major force in PC Games and one of our biggest competitors. Their office was in Bellevue WA, just twenty-five minutes or so from our home. Because of the icy conditions, Ken Williams, their CEO, was the only one from Sierra who made it into the office. By the end of the meeting Ken was essentially saying to the team, “I want to work with you…let’s make it happen.”</p><p id="5453">Because of my role at Microsoft, I decided not to get involved in the contract negotiations. It was up to Mike and Gabe to figure that out with Sierra. At the time, standard industry practice for an unproven game company was for an upfront advance against royalties, with additional royalties due only if the game were successful.</p><p id="996a">The Sierra advance was for approximately a million dollars, which along with the hundreds of thousands Mike and I and Gabe had already invested, meant that Valve had enough to fund the company through the launch of the first product.</p><p id="b6b1">The first product was going to be a first-person shooter, in the same category as id’s category-defining game <a href="https://en.wikipedia.org/wiki/Doom_(franchise)" rel="noopener ugc nofollow" target="_blank">Doom.</a></p><p id="c144">Gabe and Mike and I were meeting regularly and one of the things I told them was that the Games business was a hit-based business, where only the top 10 games made any real money. At a time when thousands of games were being introduced each year, it meant that Valve’s first game was going to be a hit or it wasn’t, and given the dynamics of the games industry, if the first game wasn’t a hit, there likely wouldn’t be a second Valve game. Gabe and Mike had been talking about trying to launch a B product, something that wasn’t a hit, but that would be successful enough to support the company as it established itself, and I knew and made clear that simply wouldn’t work.</p><p id="367e">For the next ten months or so, the activity at Valve was all about hiring and onboarding people, building out appropriate office space, and fleshing out the game concept that had hooked Sierra’s Ken Williams.</p><p id="22bb">While I continued to consult with Valve, my day job at Microsoft was invigorating. One of the projects we had underway was the first global launch of a new game, which meant a simultaneous launch in all of the markets where Microsoft had a consumer presence. My team and I had been working for months to make sure we had the launch and marketing strategies ready to go to support Microsoft’s next big hit game in markets around the world. At the time, the game group’s biggest hits to date were the strategy game <a href="https://www.ageofempires.com/" rel="noopener ugc nofollow" target="_blank">Age of Empires</a> and <a href="https://www.flightsimulator.com/" rel="noopener ugc nofollow" target="_blank">Flight Simulator,</a> a product that mostly appealed to pilots and flying enthusiasts.</p><p id="2033">In the background, I was still working with Valve, something I reminded Microsoft folks about frequently. No one seemed worried. My general rule was that I wouldn’t tell Microsoft anything about Valve, and I would tell Valve nothing about what was happening at Microsoft. However, all of the learning I was doing about the games industry in my work and personal time was definitely put to work for both Microsoft and Valve. Mike and I were each working 10 to 12-hour days or more.</p><p id="54df">At one time, I was in a meeting with some well-known game developers who had signed a long-term, multi-game development deal with Microsoft and the subject of Valve’s new deal, using the id engine came up. The leads of that company, not having any idea about my ties to Valve said something like, “what a joke — a couple of Microsoft developers license the id engine and think they can build a game.”</p><p id="a4e6">Inside, I thought, “Oh man…this is going to be tough.”</p><p id="59c1">One of the roles I played with Mike and Gabe in those days was to help them understand what it would take to be successful from a marketing and business standpoint. At the time, the key marketing strategies for games involved PR and outreach to influentials, advertising in game-specific publications, and working with the retail channel on strategies to get prominent display space so a consumer walking in would feel compelled to pick up and purchase your game. Once a game shipped, you had to feed and nurture the early adopters, supporting them so they could tell their friends about your game. The broad strategy I’d learned and fine-tuned over a period of years was essentially “Arm and Activate (early adopters).”</p><p id="2ae2">From a PR perspective, Microsoft was a phenomenon within the industry. By the time I worked on the games business, I’d spent years working directly with consumer and technical press for products ranging from Microsoft Word and Office to Expedia and Encarta. I’d also managed large product marketing and communication teams, and had been on press tours meeting reviewers around the country, from New York to New Hampshire to Austin, Texas, and Eugene, Oregon. The games business shared a lot in common with some of the other businesses I worked on, but there was also a key difference. In a business like Word or Office, you basically run your marketing with a winner-take-all perspective, and that’s because in those businesses, there’s likely to be one market leader, and as that leader gains more market share, its advantages continue to grow and become self-reinforcing as companies and the broader industry standardize around one product. At the time, my experience in fighting market battles with Lotus and WordPerfect was that a given software category will only support one market leader.</p><p id="0e3b">The PC games business in the late 90s was much more like the music business, where there were lots of independent studios, and where the developers treated each other much as musicians do. There’s mutual respect, there’s competition in a given category, but also the understanding that gamers are going to buy multiple games. If they love your game, they also have room to love your competitor’s game and in fact someone who likes action games is likely to be a connoisseur of the category, with multiple favorites. The most influential people in the games market were not press, but game developers themselves.</p><p id="8f99">For the publishers and major players, it was a different story. Microsoft was competing against Sierra and Electronic Arts to attract game developers, much like music labels at the time competed to sign top musical talent. The typical 15 percent royalty margins for new games developers mimicked other content deals in music and book publishing.</p><p id="58b9">Of course, the huge difference between the games business and the book or music industry was that the costs of producing a game were much higher and starting to climb exponentially.</p><p id="3ecb">At Valve, costs were growing rapidly as the team was being built out. Soon it became clear that the initial investment Gabe and Mike and I had made plus the advance Sierra had made were not going to cover the actual cost of producing Valve’s first game.</p><p id="64b4">One evening Mike and I hosted a potential new Valve hire, Yahn Bernier, and his fiancé at our newly remodeled home. I remember thinking that “if we hire you, Mike and I and Gabe are going to be paying your salary from our own pockets.”</p><p id="cd6e">We pitched Yahn and his fiancé Beth hard. I remember that it was only a few days earlier that Beth had learned Yahn even did software development, as his day job was as a patent attorney in Atlanta. Valve’s strength in those days was finding talent around the world who had done amazing things — the type of things that might not show up on a typical resume but could be discovered on the Internet, which in many ways was still in its infancy. At one point Gabe tracked down and recruited two creators of game-related software that was becoming popular on the Internet only to discover that they delivered pizza for a living and they thought Gabe’s phone call was part of some elaborate prank.</p><p id="0d01">Fortunately, while the Valve development team was working to build Half-Life, my Microsoft options were continuing to vest, which meant that Mike’s and my net worth continued to climb. It was stressful, but in the overall context, the stress was manageable.</p><p id="4009">Somewhere in the first year of Half-Life’s development, I officially wrote up the marketing plan for launching Half-Life. Key to our strategy was positioning Half-Life as a game that was worthy of Game of the Year honors. We wanted to earn the respect of the industry influentials, which included the gaming press and other game developers. As part of the strategy, Valve’s developers went to industry conferences to talk about some of the work they were doing in key technical areas including AI, skeletal animation, and physics. In support of the effort, I wrote up backgrounders on each of these topics for the press based on interviews with developers Ken Birdwell, Jay Stelly, Yahn Bernier and others.</p><p id="7d8d">In the Spring of 1997, I was in a meeting at Microsoft about the upcoming E3 show, which was the biggest and most important industry tradeshow for electronic entertainment companies. One of my team members made the recommendation for Microsoft not to attend the show, and during her pitch to me and others, part of her recommendation was clearly based on the idea that our competitors weren’t likely to have a major presence there. I knew Sierra was going to be there and Valve would have a big presence.</p><p id="df2f">Meanwhile, the game I’d been focused on for Microsoft, which would have been at the show, had faced a huge setback. A month or two earlier, I had initiated a meeting with Ed Fries where I told him point blank, “I’m not hearing great things about our game and am losing confidence. Are you sure?”</p><p id="5fdd">My conversation was based on hallway talks with various gamers within the division, who had had exposure to the game and weren’t excited. Basically, I couldn’t find anyone who really believed in the game. I also knew from the work I was doing for Valve what it felt like when developers who are also gamers are hugely excited about a new project</p><p id="1ce1">By that time, Microsoft’s Consumer Division had already laid a huge egg with <a href="https://www.technewsworld.com/story/microsofts-copilot-rises-from-the-ashes-of-bob-and-clippy-178799.html" rel="noopener ugc nofollow" target="_blank">Microsoft Bob</a>, and I knew that if we launched another consumer product that didn’t live up to the expectations we’d raised, the broader consumer effort would be hugely damaged. After some soul searching, Ed decided to cancel the launch, and the huge plans we’d made were quietly set aside.</p><p id="ed13">By that time, I knew that Sierra was going to be teasing Half-Life and that it would be the star of their booth at E3. I remembered thinking, “Damn, I know too much. Something has to change.”</p><p id="9790">I had another conversation with Microsoft execs about my role and the conflict with Valve, and again I was essentially told, “it’s fine, we’re OK, we like where you’re at, don’t worry.”</p><p id="8fb0">A couple of months later, Valve’s Half-Life premiered publicly in the Sierra booth at E3 in Las Vegas. The demos Valve showed were so well received that Valve earned Best Action Game honors at the show.</p><p id="b308">When the Valve team returned from Las Vegas, and I got the full update, I asked for a meeting with the division’s senior exec, and said, “this may not be a conflict for you, but it is for me. I need a new assignment.”</p><p id="5df5">Shortly thereafter, I started a new role within the company, completely unrelated to the games business.</p><p id="c971">As the months went on and Valve’s costs continued to escalate, it became clear that Mike and I were maxing out on our financial commitment. Rather than renegotiate the contract with Sierra, Gabe, who had started at Microsoft much earlier than Mike and me, began funding the ongoing development costs, set up as a loan against future company profits.</p><p id="a66a">Over that summer and in the months to follow, the “game” that Valve had shown, which wasn’t actually a game, but instead was some elaborate demos, went into early playtesting, which involved bringing gamers into Valve, having them play with the game elements and giving us their feedback. The feedback was OK. Just OK. Which for a company that needed a hit was devastating.</p><p id="1255">Gabe and Mike and I all knew that we couldn’t stay the course. If Valve shipped the game we had, it would launch and quietly disappear, and all of the work we’d all done would account for nothing. All of the people we’d hired would lose their jobs, we’d lose the money we’d invested. It was a disaster.</p><p id="b363">There was no choice. Ultimately the decision was made to essentially toss out what we had, and use everything the team had learned to that point to start fresh. Unfortunately, Sierra was not on board with the plan. They wouldn’t invest any more to make Valve’s first game a hit. We were on our own. Gabe’s deep pockets became more important.</p><p id="d83e">It would take more than a year for Valve to rebuild Half-Life in a way that put us back into the position which everyone assumed was already the case– with a game that could be launched in just a few months. The new target become launching for the Christmas season of 1998.</p><p id="50d1">In the Spring of 1998, Gabe was essentially saying, “When can you be here full-time? We need you now.”</p><p id="2c5b">I’d already written the marketing and launch plan for Half-Life, I’d written all of the press materials and copy for the web site, I’d written the backgrounders we shared with key press, but I was not focused on some of the larger business fundamentals. As a huge example, I still had not read the publishing agreement Gabe and Mike signed with Sierra.</p><p id="0988">I went to my bosses at Microsoft and essentially said, “I’m ready to leave.”</p><p id="77f4">In my closing interview with Pete Higgins, then the VP of Microsoft’s Consumer Division, he started by saying, “Is there anything that would get you to stay?” and I knew the answer was No.</p><p id="548b">I needed to move on. Valve needed me. We had had too much invested. It was time.</p><p id="e7ea">For the next few months, I worked furiously laying the groundwork for Half-Life’s launch. Among the projects I worked on was seeding a story with the Wall St. Journal. Basically, my idea was to build retailer buzz so that the retailers who were going to be putting in orders for the Christmas season would order more Half-Life and feature it prominently. For several weeks, I sent the reporter emails with updates about all of the great Half-Life news, from developer comments to industry buzz that was appearing in the gaming press.</p><p id="7014">By that time, most of my Valve work was with Gabe or the other developers or with Sierra’s own marketing team. Mike was furiously heads-down on the final work needed to finish Half-Life so we could send it off for duplication. One of the key issues we worried about was piracy. One of my nephews had recently bought a CD duplicator with a monetary gift I’d sent him, and I was horrified to realize that he was copying games and giving them to his friends. To him, it wasn’t stealing; it was sharing. A generational shift in culture and technology meant that the game we’d poured our blood and treasure in would likely be pirated from its first days not just by the professional thieves, but also by everyday end users. At the time, no publisher was doing effective checking to ensure ownership of a PC game was legitimate. Valve would need to implement an authentication scheme.</p><p id="75c6">At around this time, we caught a lucky break. For about a year, we’d been pushing Sierra on plans for seeding Half-Life with OEMs, the computer manufacturers who produced the hardware on which Half-Life could be played. For a time, we were even talking seriously with Intel about a version of the game that would highlight the features in a new chip they had under development. All of this came from the collective Microsoft experience about working with OEMs and seeding product in mass quantities to spur user adoption. Ultimately, the Intel play didn’t happen but one of the programs that Sierra led involved providing a small portion of the game that they shipped as a trial disk called “Day One.”</p><p id="843c">When we first realized what had happened, that Sierra had shipped some of the Half-Life code as Day One, Mike and I were furious. We quickly realized that we were at the mercy of whatever happened next. Fortunately, Day One became a phenomenon. Game developers LOVED it and began buzzing. Half-Life was going viral. The product was still a month or two from completion and no one had played the final game, but the early buzz for the first portion of the game was hugely promising.</p><p id="a4aa">I continued to feed the Wall St. Journal reporter news about Half-Life, about its early reception, about what we were hearing from Day One users. It was all hugely positive.</p><p id="fa0e">Mike was still coding and he was exhausted. I was exhausted, and the team was antsy as we really didn’t know how Half-Life would do in the marketplace. Finally, in late 1998, a few weeks before Christmas, we were ready to ship. Valve had a party where Mike broke open a pinata filled with candies and small toys.</p><p id="94fa">While the game was in manufacturing, final disks went to the gaming press. We waited. At some point, when Mike was in the shower, I felt overwhelmed by anxiety and asked with a worried tone to my voice, “Is it really a good game?” and his honest response, “I don’t know.”</p><p id="a932">Gabe and Mike and I had all been through the shipping process at Microsoft, but being part of a large team where the company backed everything was completely different from being the funders of a company where we knew it was a one-shot deal. The game would either be a hit or it wouldn’t, and for a short while, when the disks were being duplicated and the packaging printed and shipped, we simply wouldn’t know. The first disks went straight to the key gaming press.</p><p id="f30f">About a year earlier, I had worked with Gabe to set the audacious goal of Half-Life winning at least three of the top industry Game of the Year awards. We very consciously thought through what it would take, including breakthrough technology, a compelling new angle, and broad industry support. It was going to be especially tough for a game that some insiders initially dismissed as “Microsoft developers building on id’s technology.”</p><p id="c7e1">Within a few weeks, Half-Life won the first of more than 50 Game of the Year awards. It had never happened before. Shortly before Christmas, the Wall St. Journal article came out with the headline, “Valve’s Storytelling Game is a Hit.” In the article, Sierra as our publisher was never mentioned. That was an explicit strategy of ours — since we were funding the development, we wanted Half-Life to be known as a Valve game, not a Sierra Studios game.</p><p id="d802">A few weeks before Christmas, we were all exhausted. Some members of the team took vacation, but most continued showing up, almost in a daze, as we moved from tight pre-production intensity to the purgatory of Wait and See. We hadn’t seen any sales figures. At one point, just after the game had shipped and we started seeing feedback online about how awful the authentication system was, Mike was yelling. It turned out that when someone complained about the authentication, Mike called the person back directly, challenging them and asking for sales verification. None of the people Mike contacted who complained early on had paid for the product. Mike was mad and tired but also vindicated.</p><p id="4502">The Game of the Year honors kept coming in, and we were optimistic that the great reception we were getting from industry influentials would ultimately translate into financial success for the company. I started working feverishly on post-launch marketing strategies, which mostly involved amplifying Half-Life’s success and making sure retailers were armed with sales data and point of sale materials so customers could find it. I had purchased the web site GameoftheYear.com and we launched it with all of the material about the Game of the Year honors Half-Life was winning, including from <em>PC Gamer</em> and <em>Computer Gaming World</em>, which then were the heavyweights in our industry.</p><p id="a06e">In the meantime, Gabe and Mike’s relationship had deteriorated. A huge part of this was the stress and the financial imbalance. Gabe had become the major funder, and Mike had essentially burned himself out doing the final critical coding work before Half-Life shipped. Where before we’d tried to shield the team from our collective anxiety, Mike’s yelling at customers had unnerved some people. I found myself in a situation where the two founders were essentially not talking to each other and people were tiptoeing around wondering what was going on.</p><p id="2f30">Then, in January at a time when I was getting ready to focus on Next Steps, Sierra asked for a meeting. Essentially their message to us was “thank you, the game’s done great, we’re moving on.” They were pulling all marketing from Valve to focus on one of their next titles. Basically, their marketing at the time amounted to Launch and Leave, whereas we were trying to market a franchise-worthy game that we could build on for years to come.</p><p id="174c">Gabe and I were stunned; I was also furious. I knew that the marketing of Half-Life was only getting started. We’d done all of this hard work to earn Game of the Year honors with the idea that that would help us break out from the pack and it was time to capitalize on that effort with sustained marketing. At Microsoft, winning awards was always the start of a much larger process where we leveraged positive reviews to win customers over time.</p><p id="ddf0">With steel in my voice, I told the Sierra team that they were not pulling marketing dollars from Half-Life. They were going to re-release it in a Game of the Year Box, and they were going to support it with huge marketing spend or we were going to walk away from our agreement and tell the industry that had fallen in love with Valve how screwed up Sierra really was. At the end of the meeting, I was shaking. We were vulnerable, the partners were barely speaking, and life at home and in the office was tense.</p><p id="66b7">Sierra relented and started working on the packaging for a new Game of the Year box. The icon on the front looked similar to an Academy Award. The basic idea was that for anyone walking cold into a game retailer, they would quickly see that Half-Life was the Game of the Year they couldn’t ignore.</p><p id="3b50">By early March, things at Valve were still very strained. In addition to ongoing communications and marketing work for Half-Life, I was working on developer strategies closely with Gabe and some of the other developers and Mike was spending less and less time at the office. At some point in the early Spring, Mike said to me, “I want to leave Valve.”</p><p id="e8fd">I was in a panic. We hadn’t made back the money we’d invested. The buzz was continuing to build, but we had a long way to go. I told Mike that we needed time to figure out a pathway out, as at that time, the value of our ownership stake was highly questionable. At the same time, I was also panicked because I’d read, for the first time, the contractual agreement between Sierra and Valve, and while I thought I’d understood the key terms Gabe and Mike had agreed to, there were several points I hadn’t been aware of. Chief among them was that Sierra owned all of the intellectual property for Half-Life and held the exclusive option to publish Valve’s next two games, all at a royalty rate for Valve of 15 percent. We would do all of the development work with an upfront royalty advance of $1 million for each of those games, and Sierra would get 85% of the revenue and all of the intellectual property. At the time, I knew development costs were approaching $5 million or more per game</p><p id="ca2b">Given the licensing deal we had with id for the game engine license, the lack of any ownership of our own IP, and the exclusive commitment for future game publishing rights, all I could see was Valve swimming in red ink for years to come.</p><p id="4188">We needed a different path forward.</p><p id="0a0f">Fortunately, one of the consequences of Valve’s work on an authentication system was that our customers were registering with Valve directly,</p><p id="b7f3">Early on, we started to understand the benefit of what we’d inadvertently done. Instead of a situation where we had no idea who our customers were, we actually knew exactly who our customers were. It was unprecedented. Every Half-Life registration meant a customer contact directly in Valve’s database.</p><p id="9047">Added to this, the previous year, Gabe had had the brilliant idea of recruiting a development team that had written one of the leading mods for id’s Doom and now they were part of Valve’s team. John Cook and Robin Walker were delightful Aussie additions to Valve. The game mod they’d written to run on top of id’s Doom was quickly adapted to run on top of Half-Life. Essentially the game mod enabled a player to play a completely different game on top of the technology of Half-Life. In the case of Team Fortress, it was a multiplayer game where you could team up with your friends over the Internet in a team-based game where each of you played a unique and fun character.</p><p id="f11a">While the Half-Life buzz was continuing to build through word-of-mouth and the new marketing push, the additional buzz and enthusiasm that came as a result of Team Fortress was layered on top of everything else. Soon there were hundreds of thousands of people playing Team Fortress on top of Half-Life and Valve knew who each and every one of them were. We had a direct pipeline, bypassing Sierra, to our own customers.</p><p id="083a">In late Spring, Team Fortress, the Cook/Walker mod for the Half-Life engine was presented at E3, where it won Best Action Game and Best Online Game on behalf of Valve.</p><p id="b0ac">Through all of this, I continued to noodle about the best ways I could position Valve for long-term success. With the bad publishing deal in hand, I knew I had to work on multiple fronts.</p><p id="2b83">If Mike and I were to leave, we needed to demonstrate value for Valve that wasn’t tied directly to the Half-Life IP. We needed to renegotiate our deal with Sierra. And we needed to figure out a way to cap our royalties to id, so that Valve wouldn’t be paying them a fee each time someone bought one of our future games.</p><p id="636c">Valve had already done a lot of work to customize the id engine code for our purposes, and we’d reached the point where for any next game, we’d either continue using and adapting that code base or we’d take the time to write new engine code ourselves from scratch.</p><p id="1aeb">With Gabe’s OK, I reached out to the team at id, and we came to quick agreement on a capped deal.</p><p id="da9a">The second step was to kick off steps to renegotiate our agreement with Sierra. I met with Valve’s attorney, and the basic approach we took was to make clear that if Sierra was going to insist on keeping the original deal, then the Valve team would pivot to a completely different category and never publish a second game. Since Gabe and Mike had a ton of experience in other facets of software development, the threat was not idle. The bottom line was we weren’t going to create games and take on all of the risk only to make other people rich.</p><p id="095d">One of my responsibilities in the mid-90s at Microsoft was overseeing the initial marketing for Expedia. I’d spent a lot of time with Josh, the person on my team who was tasked with laying out the market case. From that and other experience over the years, I knew a lot about how to scope and size the online opportunity for a completely new category. I got to work trying to figure out an online business opportunity that wasn’t dependent on the Half-Life IP.</p><p id="7991">Internally, we had rich experience to draw from. Gabe and Mike had a lot of experience with developers and with the developer marketplace, and at one time, I’d overseen Microsoft’s PR outreach related to software tools for the developer community. In addition, I knew the games business inside and out, and I’d been part of teams within Microsoft that were completely focused on the emerging online opportunity made possible by the Internet.</p><p id="0c44">By the summer of 1999, Mike was researching trawler yachts, I was immersed in figuring out Valve’s potential business opportunities, and Gabe was doing deep thinking, leading the team and communicating with customers. In the meantime, because of the way Gabe and Mike had structured the ownership, where employees could earn equity over time, Mike’s and my ownership stake was effectively shrinking.</p><p id="1fa7">Finally, at some point, I realized that the only way to prove the worth of some of the ideas I was focused on was to write them up and get an offer. Without that, I could make the case that Valve was worth a lot of money or nothing. And I knew Gabe could make that same case.</p><p id="d30e">With Gabe’s buy-in, I decided to pitch Amazon on a new business opportunity. I’d known about Amazon for a long time as one of my dear friends from Microsoft had been hired on as its original marketing lead. She had since left, but I’d followed the company closely. At the time, it was headquartered in South Seattle in an imposing building just South of the i-90 freeway exchange.</p><p id="c50e">In a nine-page document, I proposed that Valve and Amazon team up to create a new online entertainment platform. I scaled the business opportunity within four years at $500 million dollars. The gist of the idea was to create a made-for-the-medium platform that would bring users together in a sticky, compelling entertainment experience, with digital and offline content sales. I wanted Amazon’s financial backing as a way to gain first mover advantage against Microsoft and Electronic Arts, then the major PC games players. I didn’t see a role for Sierra. If pushed, we wouldn’t create any new games ourselves, and instead would team with outside developers so that they could distribute content not subject to an 85% publishing fee. At the time, I considered it an act of rebellion against the traditional publishing dynamic where independent developers took on huge risk, and the big publishing houses reaped the rewards.</p><p id="7bd1">With Gabe’s OK, I sent the document over to Amazon. Within a short period of time I had a response. “Let’s meet.”</p><p id="beb4">When I arrived, the Amazon Vice President I met with was super friendly. He was familiar but I couldn’t place him. Then he said, “You don’t remember me, but you interviewed me at Microsoft.”</p><p id="002c">I realized I must not have hired him, and for the first time, started getting uneasy about how things would go. He reminded me gently that when Amazon was pretty much in its infancy and a company I knew of mostly because of my friend, I had interviewed him for a position at Microsoft. He was interviewing at Microsoft because the company he worked for had just done a major layoff. Our division was pretty much in a temporary hiring freeze and he had a background in retail, so I suggested he look into a tiny company named Amazon. By the time we met again, he’d been there several years.</p><p id="258d">We had a great discussion, and a couple of weeks later, a champagne bottle appeared at Valve’s door.</p><p id="1583">It was exhilarating and scary at the same time. We had an offer from Amazon for a minority stake, but the dynamics within the company were tricky. Amazon could help propel Valve to the next level, but the partnership would not be without costs. Valve’s culture was still evolving. A partnership with a major outside player could help but it could also hurt what we’d all built.</p><p id="f98f">It was after the Amazon offer that Mike revealed to Gabe that he wanted to leave. With an offer in-hand, it didn’t take long for us to figure out the outline of a deal.</p><p id="0c91">Ultimately, Mike and I gave up ownership to start the next chapter of our lives. The structure of the deal meant that we would be vested in Valve’s success over the next five years. I knew the opportunity that lay ahead. I also saw huge risks. The Sierra deal might collapse publicly, employees might get spooked, we might not be able to actually get the IP back, and with any online endeavor, there would be huge new risks.</p><p id="c6a8">Within several months, Mike and I left Seattle for a new adventure on San Juan Island and Whistler B.C. I continued to make myself available to Valve. But for the most part, I shed my work identity and started on new projects, including a passion for protecting Washington’s resident orca whales.</p><p id="23f7">It was up to Gabe and the Valve team to move the business forward.</p><p id="e0cd">Several years after Valve, I went to work for the Bill &amp; Melinda Gates Foundation, and from there, took on another CMO role when Mike started a new partnership and business focused on photo editing. That company, Picnik, sold to Google in 2010.</p><p id="fec2">I continued to see the Valve team intermittently through the years, and even worked with the team on a <a href="https://www.nytimes.com/2012/09/09/technology/valve-a-video-game-maker-with-few-rules.html" rel="noopener ugc nofollow" target="_blank">story</a> that appeared in the New York Times in 2012 about Valve’s highly unusual flat culture. When the Half-Life IP was secured and owned by Valve, the team sent us a small trophy with the etching “Welcome back Gordon.” Gordon was the key protagonist in the Half-Life adventure.</p><p id="fd0e">In 2016, Mike and I separated and then divorced. In 2018, I joined Gabe and some friends from Valve on a cruise on Gabe’s yacht around the islands of Japan.</p><p id="2722">Some ten years after Half-Life’s release, PC Gamer named it the Best PC Game Ever. In a separate story, PC Gamer also named Half-life the Best Marketed Game Ever, with special credit to whoever came up with the Game of the Year box and retail push. Valve’s online platform Steam is now an industry phenomenon, with annual sales in the billions of dollars.</p><p id="7e3e">As I look back on the huge success Valve has become, I’m proud of what the team accomplished. I’m also proud of the work I did while recognizing that my biggest contributions to Valve’s business went largely unnoticed and unrecognized within the industry. Part of that was due to the bro culture of the software business, part of it was that I receded to support my husband in a partnership where he was effectively the lesser partner, and part of it was that women, especially in tech, often seem to disappear when the story gets told.</p><p id="6585">I was hugely disappointed when Valve released a video in 2023 about the creation of Half-Life where one of the people interviewed, Karen Laur, a wonderfully talented texture artist, talked about the isolating experience of being a woman at Valve and essentially said that the only other woman during her tenure there was an office manager. I understood why she felt as she did, but the senior Valve team knows better. Watching the <a href="https://www.youtube.com/watch?v=TbZ3HzvFEto" rel="noopener ugc nofollow" target="_blank">video</a>, I felt like my place in Valve’s history had been completely erased.</p><p id="0f3e">I know that Valve wouldn’t have been successful without Mike. It wouldn’t have been successful without Gabe. And it wouldn’t have been successful without me. A friend of mine who knows the full story once said to me, “you were a founding partner” and in hindsight, I agree. From the beginning, I invested time, treasure and industry expertise to make the company a huge success.</p><p id="b437">And it is.</p><figure><figcaption>The author celebrating the holidays along Route 66 with Scott.</figcaption></figure><p id="df2c"><em>Monica Harrington lives in Bend, Oregon, with Scott Walker and their spaniel Johnnie Walker Black and White.</em></p><p id="042b">Editorial note 8/21: The author updated the details regarding the authentication scheme and the name of the mod created by Robin Walker and John Cook, which ultimately shipped as Team Fortress Classic.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Clojure 1.12.0 is now available (140 pts)]]></title>
            <link>https://clojure.org/news/2024/09/05/clojure-1-12-0</link>
            <guid>41460037</guid>
            <pubDate>Thu, 05 Sep 2024 20:12:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://clojure.org/news/2024/09/05/clojure-1-12-0">https://clojure.org/news/2024/09/05/clojure-1-12-0</a>, See on <a href="https://news.ycombinator.com/item?id=41460037">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<div>
<h3 id="add_libs"><a href="#add_libs"></a>2.1 Add libraries for interactive use</h3>
<p>There are many development-time cases where it would be useful to add a library interactively without restarting the JVM - speculative evaluation, adding a known dependency to your project, or adding a library to accomplish a specific task.</p>

<div>
<ul>
<li>
<p><a href="https://clojure.github.io/clojure/branch-master/clojure.repl-api.html#clojure.repl.deps/add-lib">add-lib</a> takes a lib that is not available on the classpath, and makes it available by downloading (if necessary) and adding to the classloader. Libs already on the classpath are not updated. If the coordinate is not provided, the newest Maven or git (if the library has an inferred git repo name) version or tag are used.</p>
</li>
<li>
<p><a href="https://clojure.github.io/clojure/branch-master/clojure.repl-api.html#clojure.repl.deps/add-libs">add-libs</a> is like <code>add-lib</code>, but resolves a set of new libraries and versions together.</p>
</li>
<li>
<p><a href="https://clojure.github.io/clojure/branch-master/clojure.repl-api.html#clojure.repl.deps/sync-deps">sync-deps</a> calls <code>add-libs</code> with any libs present in deps.edn, but not yet present on the classpath.</p>
</li>
</ul>
</div>
<p>These new functions are intended only for development-time interactive use at the repl - using a deps.edn is still the proper way to build and maintain production code. To this end, these functions all check that <a href="https://clojure.github.io/clojure/branch-master/clojure.core-api.html#clojure.core/%2Arepl%2A">*repl*</a> is bound to true (that flag is bound automatically by <code>clojure.main/repl</code>). In a clojure.main REPL, these new functions are automatically referred in the <code>user</code> namespace. In other repls, you may need to <code>(require '[clojure.repl.deps :refer :all])</code> before use.</p>
<p>Library resolution and download are provided by <a href="https://github.com/clojure/tools.deps">tools.deps</a>. However, you do not want to add tools.deps and its many dependencies to your project classpath during development, and thus we have also added a new api for invoking functions out of process via the Clojure CLI.</p>
</div>
<div>
<h3 id="tool_functions"><a href="#tool_functions"></a>2.2 Invoke tool functions out of process</h3>
<p>There are many useful tools you can use at development time, but which are not part of your project’s actual dependencies. The Clojure CLI provides explicit support for <a href="https://clojure.org/reference/clojure_cli#tools">tools</a> with their own classpath, but there was not previously a way to invoke these interactively.</p>
<p>Clojure now includes <a href="https://clojure.github.io/clojure/branch-master/clojure.tools.deps.interop-api.html#clojure.tools.deps.interop/invoke-tool">clojure.tools.deps.interop/invoke-tool</a> to invoke a tool function out of process. The classpath for the tool is defined in deps.edn and you do not need to add the tool’s dependencies to your project classpath.</p>
<p><code>add-lib</code> functionality is built using <code>invoke-tool</code> but you can also use it to build or invoke your own tools for interactive use. Find more about the function execution protocol on the <a href="https://clojure.org/reference/clojure_cli#function_protocol">CLI reference</a>.</p>
</div>
<div>
<h3 id="_2_3_start_and_control_external_processes"><a href="#_2_3_start_and_control_external_processes"></a>2.3 Start and control external processes</h3>
<p>For a long time, we’ve had the <code>clojure.java.shell</code> namespace, but over time Java has provided new APIs for process info, process control, and I/O redirection. This release adds a new namespace <a href="https://clojure.github.io/clojure/branch-master/index.html#clojure.java.process">clojure.java.process</a> that takes advantage of these APIs and is easier to use. See:</p>
<div>
<ul>
<li>
<p><a href="https://clojure.github.io/clojure/branch-master/clojure.java.process-api.html#clojure.java.process/start">start</a> - full control over streams with access to the underlying Java objects for advanced usage</p>
</li>
<li>
<p><a href="https://clojure.github.io/clojure/branch-master/clojure.java.process-api.html#clojure.java.process/exec">exec</a> - covers the common case of executing an external process and returning its stdout on completion</p>
</li>
</ul>
</div>
</div>
<div>
<h3 id="method_values"><a href="#method_values"></a>2.4 Method values</h3>
<p>Clojure programmers often want to use Java methods in higher-order functions (e.g. passing a Java method to <code>map</code>). Until now, programmers have had to manually wrap methods in functions. This is verbose, and might require manual hinting for overload disambiguation, or incur incidental reflection or boxing.</p>
<p>Programmers can now use <a href="https://clojure.org/news/2024/09/05/clojure-1-12-0#qualified_methods">qualified methods</a> as ordinary functions in value contexts - the compiler will <a href="https://clojure.org/reference/java_interop#methodvalues">automatically generate the wrapping function</a>. The compiler will generate a reflective call when a qualified method does not resolve due to overloading. Developers can supply <a href="https://clojure.org/news/2024/09/05/clojure-1-12-0#param_tags"><code>:param-tags</code></a> metadata on qualified methods to specify the signature of a single desired method, 'resolving' it.</p>
</div>
<div>
<h3 id="qualified_methods"><a href="#qualified_methods"></a>2.5 Qualified methods - <code>Class/method</code>, <code>Class/.method</code>, and <code>Class/new</code></h3>
<p>Java members inherently exist in a class.  For method values we need a way to explicitly specify the class of an instance method because there is no possibility for inference.</p>
<p>Qualified methods have value semantics when used in non-invocation positions:</p>
<div>
<ul>
<li>
<p><code>Classname/method</code> - value is a Clojure function that invokes a static method</p>
</li>
<li>
<p><code>Classname/.method</code> - value is a Clojure function that invokes an instance method</p>
</li>
<li>
<p><code>Classname/new</code> - value is a Clojure function that invokes a constructor</p>
</li>
</ul>
</div>
<p>Note: developers must use <code>Classname/method</code> and <code>Classname/.method</code> syntax to differentiate between static and instance methods.</p>
<p>Qualified method invocations with <a href="https://clojure.org/news/2024/09/05/clojure-1-12-0#param_tags"><code>:param-tags</code></a> use only the tags to resolve the method. Without param-tags they behave like the equivalent <a href="https://clojure.org/reference/java_interop#_the_dot_special_form">dot syntax</a>, except the qualifying class takes precedence over hints of the target object, and over its runtime type when invoked via reflection.</p>
<p>Note: Static fields are values and should be referenced without parens unless they are intended as function calls, e.g <code>(System/out)</code> should be <code>System/out</code>. Future Clojure releases will treat the field’s value as something invokable and invoke it.</p>
</div>
<div>
<h3 id="param-tags"><a href="#param-tags"></a>2.6 :param-tags metadata</h3>
<p>When used as values, qualified methods supply only the class and method name, and thus cannot resolve overloaded methods.</p>
<p>Developers can supply <a href="https://clojure.org/reference/java_interop#paramtags"><code>:param-tags</code></a> metadata on qualified methods to specify the signature of a single desired method, 'resolving' it. The <code>:param-tags</code> metadata is a vector of zero or more tags: <code>[tag …​]</code>. A tag is any existing valid <code>:tag</code> metadata value. Each tag corresponds to a parameter in the desired signature (arity should match the number of tags). Parameters with non-overloaded types can use the placeholder <code>_</code> in lieu of the tag. When you supply :param-tags metadata on a qualified method, the metadata must allow the compiler to resolve it to a single method at compile time.</p>
<p>A new metadata reader syntax <code>^[tag …​]</code> attaches <code>:param-tags</code> metadata to member symbols, just as <code>^tag</code> attaches <code>:tag</code> metadata to a symbol.</p>
</div>
<div>
<h3 id="_2_7_array_class_syntax"><a href="#_2_7_array_class_syntax"></a>2.7 Array class syntax</h3>
<p>Clojure supports symbols naming classes both as a value (for class object) and as a type hint, but has not provided syntax for array classes other than strings.</p>
<p>Developers can now refer to an <a href="https://clojure.org/reference/java_interop#_class_access">array class</a> using a symbol of the form <code>ComponentClass/#dimensions</code>, eg <code>String/2</code> refers to the class of a 2 dimensional array of Strings. Component classes can be fully-qualified classes, imported classes, or primitives. Array class syntax can be used as both type hints and values.</p>
<p>Examples: <code>String/1</code>, <code>java.lang.String/1</code>, <code>long/2</code>.</p>
</div>
<div>
<h3 id="_2_8_functional_interfaces"><a href="#_2_8_functional_interfaces"></a>2.8 Functional interfaces</h3>
<p>Java programs emulate functions with Java functional interfaces (marked with the <a href="https://docs.oracle.com/javase/8/docs/api/java/lang/FunctionalInterface.html">@FunctionalInterface</a> annotation), which have a single method.</p>
<p>Clojure developers can now invoke Java methods taking <a href="https://clojure.org/reference/java_interop#functional_interfaces">functional interfaces</a> by passing functions with matching arity. The Clojure compiler implicitly converts Clojure functions to the required functional interface by constructing a lambda adapter. You can explicitly coerce a Clojure function to a functional interface by hinting the binding name in a <code>let</code> binding, e.g. to avoid repeated adapter construction in a loop, e.g. <code>(let [^java.util.function.Predicate p even?] …​)</code>.</p>
</div>
<div>
<h3 id="_2_9_java_supplier_interop"><a href="#_2_9_java_supplier_interop"></a>2.9 Java Supplier interop</h3>
<p>Calling methods that take a <a href="https://docs.oracle.com/javase/8/docs/api/java/util/function/Supplier.html">Supplier</a> (a method that supplies a value) had required writing an adapter with reify. Clojure has a "value supplier" interface with semantic support already - <code>IDeref</code>. All <code>IDeref</code> impls (<code>delay</code>, <code>future</code>, <code>atom</code>, etc) now implement the <code>Supplier</code> interface directly.</p>
</div>
<div>
<h3 id="_2_10_streams_with_seq_into_reduce_and_transduce_support"><a href="#_2_10_streams_with_seq_into_reduce_and_transduce_support"></a>2.10 Streams with seq, into, reduce, and transduce support</h3>
<p>Java APIs increasingly return <a href="https://docs.oracle.com/javase/8/docs/api/java/util/stream/Stream.html">Stream</a>s and are hard to consume because they do not implement interfaces that Clojure already supports, and hard to interop with because Clojure doesn’t directly implement Java functional interfaces.</p>
<p>In addition to functional interface support, Clojure <a href="https://clojure.org/reference/java_interop#streams">now provides these functions</a> to interoperate with streams in an idiomatic manner, all functions behave analogously to their Clojure counterparts:</p>
<div>
<ul>
<li>
<p><code>(stream-seq! stream) ⇒ seq</code></p>
</li>
<li>
<p><code>(stream-reduce! f [init-val] stream) ⇒ val</code></p>
</li>
<li>
<p><code>(stream-transduce! xf f [init-val] stream) ⇒ val</code></p>
</li>
<li>
<p><code>(stream-into! to-coll [xf] stream) ⇒ to-coll</code></p>
</li>
</ul>
</div>
<p>All of these operations are terminal stream operations (they consume the stream).</p>
</div>
<div>
<h3 id="_2_11_persistentvector_implements_spliterable"><a href="#_2_11_persistentvector_implements_spliterable"></a>2.11 PersistentVector implements Spliterable</h3>
<p>Java collections implement streams via <a href="https://docs.oracle.com/javase/8/docs/api/java/util/Spliterator.html">"spliterators"</a>, iterators that can be split for faster parallel traversal. <code>PersistentVector</code> now provides a custom spliterator that supports parallelism, with greatly improved performance.</p>
</div>
<div>
<h3 id="_2_12_efficient_drop_and_partition_for_persistent_or_algorithmic_collections"><a href="#_2_12_efficient_drop_and_partition_for_persistent_or_algorithmic_collections"></a>2.12 Efficient drop and partition for persistent or algorithmic collections</h3>
<p>Partitioning of a collection uses a series of takes (to build a partition) and drops (to skip past that partition). <a href="https://clojure.atlassian.net/browse/CLJ-2713">CLJ-2713</a> adds a new internal interface (IDrop) indicating that a collection can drop more efficiently than sequential traversal, and implements that for persistent collections and algorithmic collections like <code>range</code> and <code>repeat</code>. These optimizations are used in <code>drop</code>, <code>nthrest</code>, and <code>nthnext</code>.</p>
<p>Additionally, there are new functions <code>partitionv</code>, <code>partitionv-all</code>, and <code>splitv-at</code> that are more efficient than their existing counterparts and produce vector partitions instead of realized seq partitions.</p>
</div>
<div>
<h3 id="_2_13_var_interning_policy"><a href="#_2_13_var_interning_policy"></a>2.13 Var interning policy</h3>
<p><a href="https://clojure.org/reference/vars#interning">Interning</a> a var in a namespace (vs aliasing) must create a stable reference that is never displaced, so that all references to an interned var get the same object. There were some cases where interned vars could get displaced and those have been tightened up in 1.12.0-alpha1. If you encounter this situation, you’ll see a warning like "REJECTED: attempt to replace interned var #'some-ns/foo with #'other-ns/foo in some-ns, you must ns-unmap first".</p>
<p>This addresses the root cause of an issue encountered with Clojure 1.11.0, which added new functions to clojure.core (particularly <code>abs</code>). Compiled code from an earlier version of Clojure with var names that matched the newly added functions in clojure.core would be unbound when loaded in a 1.11.0 runtime. In addition to <a href="https://clojure.atlassian.net/browse/CLJ-2711">CLJ-2711</a>, we rolled back a previous fix in this area (<a href="https://clojure.atlassian.net/browse/CLJ-1604">CLJ-1604</a>).</p>
</div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Common food dye found to make skin and muscle temporarily transparent (141 pts)]]></title>
            <link>https://www.theguardian.com/science/article/2024/sep/05/common-food-dye-found-to-make-skin-and-muscle-temporarily-transparent</link>
            <guid>41459865</guid>
            <pubDate>Thu, 05 Sep 2024 19:48:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/science/article/2024/sep/05/common-food-dye-found-to-make-skin-and-muscle-temporarily-transparent">https://www.theguardian.com/science/article/2024/sep/05/common-food-dye-found-to-make-skin-and-muscle-temporarily-transparent</a>, See on <a href="https://news.ycombinator.com/item?id=41459865">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>Researchers have peered into the brains and bodies of living animals after discovering that a common food dye can make skin, muscle and connective tissues temporarily transparent.</p><p>Applying the dye to the belly of a mouse made its liver, intestines and bladder clearly visible through the abdominal skin, while smearing it on the rodent’s scalp allowed scientists to see blood vessels in the animal’s brain.</p><p>Treated skin regained its normal colour when the dye was washed off, according to researchers at Stanford University, who believe the procedure opens up a host of applications in humans, from locating injuries and finding veins for drawing blood to monitoring digestive disorders and spotting tumours.</p><p>“Instead of relying on invasive biopsies, doctors might be able to diagnose deep-seated tumours by simply examining a person’s tissue without the need for invasive surgical removal,” said Dr Guosong Hong, a senior researcher on the project. “This technique could potentially make blood draws less painful by helping phlebotomists easily locate veins under the skin.”</p><p>The trick has echoes of the approach taken by Griffin in HG Wells’s 1897 novel, The Invisible Man, in which the brilliant but doomed scientist discovers that the secret to invisibility lies in matching an object’s refractive index, or ability to bend light, to that of the surrounding air.</p><p>When light penetrates biological tissue, much of it is scattered because the structures inside, such as fatty membranes and cell nuclei, have different refractive indices. As light moves from one refractive index to another, it bends, making tissue opaque. The same effect makes a pencil look bent when dropped in a glass of water.</p><p>Dr Zihao Ou and his colleagues at Stanford theorised, counterintuitively, that particular dyes could make certain wavelengths of light pass more easily through skin and other tissues. Strongly absorbing dyes alter the refractive index of tissues that absorb them, allowing scientists to match the refractive indices of different tissues and suppress any scattering.</p><figure id="9530018f-bade-4b02-a276-7aa89115801e" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-1"><picture><source srcset="https://i.guim.co.uk/img/media/09c9e23bd620561756b5770547e2c9e224a7f790/0_0_3306_2100/master/3306.png?width=620&amp;dpr=2&amp;s=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/09c9e23bd620561756b5770547e2c9e224a7f790/0_0_3306_2100/master/3306.png?width=620&amp;dpr=1&amp;s=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/09c9e23bd620561756b5770547e2c9e224a7f790/0_0_3306_2100/master/3306.png?width=605&amp;dpr=2&amp;s=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/09c9e23bd620561756b5770547e2c9e224a7f790/0_0_3306_2100/master/3306.png?width=605&amp;dpr=1&amp;s=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/09c9e23bd620561756b5770547e2c9e224a7f790/0_0_3306_2100/master/3306.png?width=445&amp;dpr=2&amp;s=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/09c9e23bd620561756b5770547e2c9e224a7f790/0_0_3306_2100/master/3306.png?width=445&amp;dpr=1&amp;s=none" media="(min-width: 320px)"><img alt="Before and after images of the use of the dye on a rodent. In the second image the internal images can be seen in red." src="https://i.guim.co.uk/img/media/09c9e23bd620561756b5770547e2c9e224a7f790/0_0_3306_2100/master/3306.png?width=445&amp;dpr=1&amp;s=none" width="445" height="282.6678765880218" loading="lazy"></picture></div><figcaption data-spacefinder-role="inline"><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>Before and after images of the use of the dye on a rodent.</span> Photograph: handout</figcaption></figure><p>In a series of experiments <a href="https://doi.org/10.1126/science.adm6869" data-link-name="in body link">described in Science</a>, the researchers show how a fresh chicken breast became transparent to red light minutes after being immersed in tartrazine solution, a yellow food dye used in US Doritos, SunnyD drink and other products. The dye reduced light scattering inside the tissue, allowing the rays to penetrate more deeply.</p><p>The team then smeared the yellow dye on a mouse’s underbelly, making the abdominal skin see-through and revealing the rodent’s intestines and organs. In another experiment, they applied dye to a mouse’s shaved head and, with a technique called laser speckle contrast imaging, saw blood vessels in the animal’s brain.</p><p>“The most surprising part of this study is that we usually expect dye molecules to make things less transparent. For example, if you mix blue pen ink in water, the more ink you add, the less light can pass through the water,” Hong said. “In our experiment, when we dissolve tartrazine in an opaque material like muscle or skin, which normally scatters light, the more tartrazine we add, the clearer the material becomes. But only in the red part of the light spectrum. This goes against what we typically expect with dyes.”</p><p>The researchers describe the process as “reversible and repeatable”, with skin reverting to its natural colour once the dye is washed away. At the moment, transparency is limited to the depth the dye penetrates, but Hong said microneedle patches or injections could deliver the dye more deeply.</p><p>The procedure has not yet been tested on humans and researchers will need to show it is safe to use, particularly if the dye is injected beneath the skin.</p><p>Others stand to benefit from the breakthrough. Many scientists study naturally transparent animals, such as zebrafish, to see how organs and features of disease, such as cancer, develop in living creatures. With transparency dyes, a much wider range of animals could be studied in this way.</p><p>In an <a href="https://doi.org/10.1126/science.adr7935" data-link-name="in body link">accompanying article</a>, Christopher Rowlands and Jon Gorecki, of Imperial College London, say there will be “extremely broad interest” in the procedure, which, when combined with modern imaging techniques, could allow scientists to image an entire mouse brain or spot tumours beneath centimetre-thick tissues. “HG Wells, who studied biology under TH Huxley, as a student would surely approve,” they write.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Reflection 70B, the top open-source model (134 pts)]]></title>
            <link>https://twitter.com/mattshumer_/status/1831767014341538166</link>
            <guid>41459781</guid>
            <pubDate>Thu, 05 Sep 2024 19:39:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/mattshumer_/status/1831767014341538166">https://twitter.com/mattshumer_/status/1831767014341538166</a>, See on <a href="https://news.ycombinator.com/item?id=41459781">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[My job is to watch dreams die (2011) (217 pts)]]></title>
            <link>https://old.reddit.com/r/reddit.com/comments/k3zrz/by_request_from_the_jobs_thread_why_my_job_is_to/</link>
            <guid>41459365</guid>
            <pubDate>Thu, 05 Sep 2024 18:43:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/reddit.com/comments/k3zrz/by_request_from_the_jobs_thread_why_my_job_is_to/">https://old.reddit.com/r/reddit.com/comments/k3zrz/by_request_from_the_jobs_thread_why_my_job_is_to/</a>, See on <a href="https://news.ycombinator.com/item?id=41459365">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a href="http://www.reddit.com/r/AskReddit/comments/k3ifx/whats_the_most_fucked_up_thing_youve_had_to_do_at/c2ha0il">Original post here</a>.</p>

<p>I work at a real estate office.  We primarily sell houses that were foreclosed on by lenders.  We aren't involved in the actual foreclosures or evictions - anonymous lawyers in the cloud somewhere is tasked with the paperwork - we are the boots on the ground that interacts with the actual walls, roofs and occasional bomb threat.</p>

<p>When the lender forecloses - or is thinking of foreclosing - on a property one of the first things that happens is they send somebody out to see if there is actually a house there and if there is anybody living there who needs to be evicted.  Lawyers are expensive so they send a real estate agent or a property preservation company out to check.  There is the occasional discovery of fraud where there was never a house on the parcel to begin with, but such instances are rare.  Sometimes this initial visit results in discovering a house that has burned down or demolished, is abandoned or occupied by somebody who has absolutely no connection with the homeowner.  Sometimes the houses are discovered to be crack dens or meth labs, sometimes the sites of cock or dog fighting operations, or you might even find a back yard filled with a pot cultivation that can't be traced back to anybody because it was planted in yet another vacant house in a blighted neighborhood.  The house could be worth less than zero - blighted to the point where you can't even give it away (this is a literal statement, I have tried to give away many houses or even vacant lots with no takers over the years) or it could be a waterfront mansion in a gated golf community worth well over seven figures that does not include the number "one".    Sometimes they are found to have been seized by the IRS, the local tax authority, the DEA or the US Marshal.  Variety is the rule.  The end results are the law.</p>

<p>If the house is occupied my job is to make contact and determine who they are: there are laws that establish what happens to a borrower as opposed to a tenant and the servicemember relief act adds an additional set of questions that must be answered. Some of the people have an idea of why I am there.  Some claim they never knew they were foreclosed on, or tell me that they have worked something out with their lender, some won't tell me a thing and some threaten me to never return in the name of the police, their lawyer, or the occasional "or else/if I were you".  During one initial visit the sight of 50-60 motorcycles parked on the lawn suggested that we try again the next day.  At a couple the police had cordoned off the area and at one they were in the process of dredging the lake searching for the body of a depressed former homeowner.</p>

<p>If nobody is home I have to determine if they are at work, on vacation, in the army, wintering/summering at their other home, in jail, in a nursing home, dead or if they moved away.  It isn't easy.  Utilities can be left on for months.  Neighbors can be staging the yard and house to appear occupied to prevent blight in their neighborhood.  By the same token people will stop cutting the lawn for months, let trash and old phone books pile up on their porch, lose gas and electric service and continue to live in properties that have not only physically unsafe to approach but are so filthy that when it comes time to clean them out the crews have to wear hazmat suits.  One house had a gallon pickle jar filled with dead roaches on the porch.  Somebody lived in that house and thought that was a logical thing to do.  People like me are tasked with first contact.</p>

<p>Evictions are expensive and time-consuming.  Ultimately once the process gets that far there isn't much that can be done to prevent it.  You didn't pay your mortgage, the lender gets the house back.  There are an infinite number of reasons why the mortgage couldn't be paid, some are more sympathetic than others, but in the end you will be leaving the property willingly or not.  The lawyers handle the evictions - they churn through the paperwork in the background, ten thousand properties at a time.  They have it down to rote function based on templates, personal experience with the various judges and intimate knowledge of the federal, state and municipal laws, along with dealing with the occasional sheriff who refuses to evict somebody, the informal policies established by the local judges and a myriad of other problems that can arise.  As a business decision many lenders have determined that it is cheaper to settle with the occupants - instead of going through the formal eviction they will offer cash.  In exchange for surrendering a property in reasonably clean condition with the furnace still hooked up, the kitchen not stripped and the basement not intentionally flooded the lender will cut the occupants a check.  It costs much less than an eviction, provides reasonable hope that the plumbing won't freeze and can take a fraction of the time to obtain possession.  This is where the personal element becomes real.</p>

<p>(Continued in comments)</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[UE5 Nanite in WebGPU (263 pts)]]></title>
            <link>https://github.com/Scthe/nanite-webgpu</link>
            <guid>41458987</guid>
            <pubDate>Thu, 05 Sep 2024 17:55:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Scthe/nanite-webgpu">https://github.com/Scthe/nanite-webgpu</a>, See on <a href="https://news.ycombinator.com/item?id=41458987">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Nanite WebGPU</h2><a id="user-content-nanite-webgpu" aria-label="Permalink: Nanite WebGPU" href="#nanite-webgpu"></a></p>
<blockquote>
<p dir="auto">TL;DR: <a href="https://scthe.github.io/nanite-webgpu/?scene_file=jinxCombined&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=0.1" rel="nofollow">Demo scene Jinx</a> (640m triangles). <a href="https://scthe.github.io/nanite-webgpu/?scene_file=manyObjects2&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=1.0" rel="nofollow">Sample scene with many objects</a> (1.7b triangles). White triangles in the Jinx scene are software-rasterized. <strong>WebGPU is only available on Chrome!</strong></p>
</blockquote>
<p dir="auto">This project contains a <a href="https://youtu.be/qC5KtatMcUw?si=IOWaVk0sQNra_R6O&amp;t=97" rel="nofollow">Nanite</a> implementation in a web browser using WebGPU. This includes the meshlet LOD hierarchy, software rasterizer (at least as far as possible given the current state of WGSL), and billboard impostors. Culling on both per-instance and per-meshlet basis (frustum and occlusion culling in both cases). Supports textures and per-vertex normals. Possibly every statistic you can think of. There is a slider or a checkbox for every setting imaginable. Also works offline using Deno.</p>
<p dir="auto">First, we will see some screenshots, then there is (not even complete) list of features. Afterward, I will link you to a couple of <strong>demo scenes</strong> you can play with. In the FAQ section, you can read <strong>my thoughts about Nanite</strong>. Since this file got a bit long, I've moved usability-oriented stuff (stats/GUI explanation, build process, and unit test setup) into a separate <a href="https://github.com/Scthe/nanite-webgpu/blob/master/USAGE.md">USAGE.md</a>.</p>
<blockquote>
<p dir="auto">EDIT 16-08-2024: I've rewritten significant parts of this README once I had more time to look through it. And I've written <a href="https://github.com/Scthe/frostbitten-hair-webgpu">Frostbitten hair WebGPU</a> meantime #self-promo.</p>
</blockquote>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/9325337/348849467-ef4c8476-bf30-4241-96d0-a354efa0dea1.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1NjQ5MDEsIm5iZiI6MTcyNTU2NDYwMSwicGF0aCI6Ii85MzI1MzM3LzM0ODg0OTQ2Ny1lZjRjODQ3Ni1iZjMwLTQyNDEtOTZkMC1hMzU0ZWZhMGRlYTEucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDkwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA5MDVUMTkzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZWExMWU3NDVkN2JlZjA2MDU0YWFhMDEzNWY4MWFhMWRmNjhkNTBmZmY0ODE0MDlhMTI5ZjAwMDFmMDg3YjhhZiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.G4mQjdsP12YTNSLhVg3WVUyvLgm6bQxd27pe0lwi1lY"><img src="https://private-user-images.githubusercontent.com/9325337/348849467-ef4c8476-bf30-4241-96d0-a354efa0dea1.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1NjQ5MDEsIm5iZiI6MTcyNTU2NDYwMSwicGF0aCI6Ii85MzI1MzM3LzM0ODg0OTQ2Ny1lZjRjODQ3Ni1iZjMwLTQyNDEtOTZkMC1hMzU0ZWZhMGRlYTEucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDkwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA5MDVUMTkzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZWExMWU3NDVkN2JlZjA2MDU0YWFhMDEzNWY4MWFhMWRmNjhkNTBmZmY0ODE0MDlhMTI5ZjAwMDFmMDg3YjhhZiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.G4mQjdsP12YTNSLhVg3WVUyvLgm6bQxd27pe0lwi1lY" alt="scene-multiobject"></a></p>
<p dir="auto"><em><a href="https://scthe.github.io/nanite-webgpu/?scene_file=manyObjects2&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=1.0" rel="nofollow">Sample scene</a> containing 1.7b triangles. Nearly 98% of the triangles are software rasterized, as it's much faster than hardware.</em></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/9325337/348849421-4eef5e85-03dd-43f3-9a99-afbce59407f0.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1NjQ5MDEsIm5iZiI6MTcyNTU2NDYwMSwicGF0aCI6Ii85MzI1MzM3LzM0ODg0OTQyMS00ZWVmNWU4NS0wM2RkLTQzZjMtOWE5OS1hZmJjZTU5NDA3ZjAucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDkwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA5MDVUMTkzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MTRhY2I1NDQyZjNhMjBkYjAxMGFiNGQ1MmQ5MjcyMzY2ZmVlMmQxNjc3ZThhMGUyNzQ0ZjNjNzRiOTg3NDg0ZSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.xmvp4efV5JdJ_c1xPLWh7WbcIwRYrfRG5BNjHhOoCCo"><img src="https://private-user-images.githubusercontent.com/9325337/348849421-4eef5e85-03dd-43f3-9a99-afbce59407f0.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1NjQ5MDEsIm5iZiI6MTcyNTU2NDYwMSwicGF0aCI6Ii85MzI1MzM3LzM0ODg0OTQyMS00ZWVmNWU4NS0wM2RkLTQzZjMtOWE5OS1hZmJjZTU5NDA3ZjAucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDkwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA5MDVUMTkzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MTRhY2I1NDQyZjNhMjBkYjAxMGFiNGQ1MmQ5MjcyMzY2ZmVlMmQxNjc3ZThhMGUyNzQ0ZjNjNzRiOTg3NDg0ZSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.xmvp4efV5JdJ_c1xPLWh7WbcIwRYrfRG5BNjHhOoCCo" alt="scene-jinx"></a></p>
<p dir="auto"><em>My <a href="https://scthe.github.io/nanite-webgpu/?scene_file=jinxCombined&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=0.1" rel="nofollow">primary test scene</a>. <a href="https://sketchfab.com/3d-models/arcane-jinx-b74f25a5ee6e43efbe9766b9fbebc705" rel="nofollow">Arcane - Jinx</a> 3D model by sketchfab user <a href="https://sketchfab.com/rizky08" rel="nofollow">Craft Tama</a>. Unfortunately, the best simplification we get is from 44k to 3k triangles. The white triangles are software-rasterized triangles (between hardware-rasterized ones and the impostors in the far back). WebGPU does not support <code>atomic&lt;u64&gt;</code>, so I had to compress the data to fit into 32 bits (u16 for depth, 2*u8 for octahedron-encoded normals). It's a painful limitation, but at least you can see the entire system is working.</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Nanite
<ul dir="auto">
<li><strong>Meshlet LOD hierarchy.</strong>
<ul dir="auto">
<li>Mesh preprocessing executes in the browser, using WebAssembly for <a href="https://github.com/zeux/meshoptimizer">meshoptimizer</a> and <a href="http://glaros.dtc.umn.edu/gkhome/metis/metis/overview" rel="nofollow">METIS</a>. While it might raise eyebrows, this was one of the goals.</li>
<li>There is a file exporter too, if you don't like to wait between page refreshes.</li>
</ul>
</li>
<li><strong>Software rasterizer.</strong>
<ul dir="auto">
<li>WebGPU does not have the <code>atomic&lt;u64&gt;</code> needed to implement this feature efficiently. Currently, I'm packing depth (<code>u16</code>) and octahedron-encoded normals (<code>2 * u8</code>) into 32 bits. It's enough to show that the rasterizer works.</li>
<li>With only 32 bits, we are butchering the precision. My only concern here is to show that the rasterization works. If you see the software rasterized bunny model in the background it will be white and it will have <em>reasonable</em> shading. Reprojecting depth and "compressing" normals is enough to get something.. not offending.</li>
<li>This also affects the depth pyramid used for occlusion culling.</li>
<li>There are other algorithms to do this. PPLL, or something with tiles, or double rasterization (1st pass writes depth, 2nd does compareExchange). But the 32-bit limitation is only in WebGPU, so I choose to stick to UE5's solution instead.</li>
</ul>
</li>
<li><strong>Billboard impostors.</strong> 12 images around the UP-axis, blended (with dithering) based on the camera position. Does not handle up/down views. Contains both diffuse and normals, so we can do nice shading at a runtime. UE5 <a href="https://advances.realtimerendering.com/s2021/Karis_Nanite_SIGGRAPH_Advances_2021_final.pdf#page=97" rel="nofollow">uses</a> a more advanced version integrated with a visibility buffer.
<ul dir="auto">
<li><a href="https://scthe.github.io/nanite-webgpu/?scene_file=jinxCombined&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=0.1&amp;impostors_forceonlybillboards&amp;impostors_texturesize=512" rel="nofollow">Impostors preview demo scene</a>. For this demo, I've increased the impostor texture size. This way you can see more details.</li>
</ul>
</li>
</ul>
</li>
<li>Culling:
<ul dir="auto">
<li><strong>Per-instance:</strong> frustum and occlusion culling.</li>
<li><strong>Per-meshlet:</strong> frustum and occlusion culling.</li>
<li><strong>Per-triangle:</strong> hardware backface culling and ofc. z-buffer. WebGPU does not have early-z.
<ul dir="auto">
<li>I have no idea how early-z works in WebGPU (it does not).</li>
</ul>
</li>
<li>I've also tried per-meshlet backface cone culling. It worked fine, but I cut it from the final release. See FAQ below for more details.</li>
<li>Occlusion culling is just a depth pyramid from the previous frame's depth buffer. No reprojection and no two-pass. The current implementation is enough to cull a lot of triangles (<strong>A LOT!</strong>) and to judge the performance impact (big improvement!). I expect someone will want to read the code, and they will be grateful this feature was not added.</li>
</ul>
</li>
<li>Switch between <strong>GPU-driven rendering</strong> and a <strong>naive CPU implementation</strong>. I have not spent much time optimizing the CPU version. It works, you can step through it with the debugger.</li>
<li>Supports <strong>textured models</strong> and <strong>many different objects</strong> at the same time.</li>
<li>Controls to <strong>change parameters at runtime</strong>. Debug views. "Freeze culling" allows the camera to move and inspect only what was drawn last frame.</li>
<li>A lot of <strong>stats</strong>. Memory, geometry. Total scene meshlets, triangles. Drawn meshlets, triangles (split between hardware and software rasterizer). Impostor count. Dedicated profiler button to get the timings.</li>
<li><strong>Custom file format</strong> so you don't have to preprocess the mesh every time. This is optional, you <strong>can also use an OBJ file</strong>.</li>
<li>Vertex <strong>position quantization</strong> (vec2u), <strong>octahedron encoded normals</strong> (vec2f).
<ul dir="auto">
<li>Position quantization is off by default. Toggle <code>CONFIG.useVertexQuantization</code> to enable. There are <em>funny</em> things happening to the numbers there, but everything <em>should</em> be handled correctly.</li>
</ul>
</li>
<li>Handles window resize. It's a web browser after all.</li>
<li>The whole app also <strong>runs offline in <a href="https://deno.com/" rel="nofollow">Deno</a></strong>. I've written shader unit tests this way.</li>
<li>Tons of WebGPU and WGSL code that you can copy to your own project. If you want to do something, I've either attempted to do it or discovered that it does not work.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Goals</h3><a id="user-content-goals" aria-label="Permalink: Goals" href="#goals"></a></p>
<p dir="auto">There were 2 primary goals for this project:</p>
<ol dir="auto">
<li><strong>Simplicity.</strong> We start with an OBJ file and everything is done in the app. No magic pre-processing steps, Blender exports, etc. You set the breakpoint at <code>loadObjFile()</code> and F10 your way till the first frame finishes.</li>
<li><strong>Experimentation.</strong> I could have built this with Vulkan and Rust. None would touch it. Instead, it's a webpage. You click the link, uncheck the checkbox and the FPS tanks 40%. And you think to yourself: "OK, that was an important checkbox. But what about this slider?". Or: "How does scene X affect memory allocation?". Right now I know that a lot of code can be optimized. Yet it would not matter till the simplification problem is solved.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Demo Scenes</h2><a id="user-content-demo-scenes" aria-label="Permalink: Demo Scenes" href="#demo-scenes"></a></p>
<ul dir="auto">
<li><a href="https://scthe.github.io/nanite-webgpu/?scene_file=jinxCombined&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=0.1" rel="nofollow">Jinx</a> (120*120 instances, 640m triangles). A single Jinx is 44k triangles simplified to 3k at 59 root meshlets. Uses an OBJ file.</li>
<li><a href="https://scthe.github.io/nanite-webgpu/?scene_file=manyObjects2&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=1.0" rel="nofollow">Lucy and dragons</a> (both objects at 70*70 instances, 1.7b triangles). See below for per-object details.</li>
<li><a href="https://scthe.github.io/nanite-webgpu/?scene_file=lucy1b&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=0.5" rel="nofollow">Lucy</a> (110*110 instances, 1.2b triangles). A single Lucy statue is 100k triangles simplified to 86 at a single root meshlet. Uses binary file.</li>
<li><a href="https://scthe.github.io/nanite-webgpu/?scene_file=dragonJson&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=0.5" rel="nofollow">Dragons</a> (70*70 instances, 1.2b triangles). A single dragon is 250k triangles simplified to 102 at a single root meshlet. Uses binary file.</li>
<li><a href="https://scthe.github.io/nanite-webgpu/?scene_file=bunny1b&amp;impostors_threshold=1000&amp;softwarerasterizer_threshold=2400" rel="nofollow">Bunnies</a> (500*500 instances, 1.2b triangles). A single bunny is 5k triangles simplified to 96 at a single root meshlet. Uses an OBJ file. Bunnies are so small most are frustum culled.</li>
<li><a href="https://scthe.github.io/nanite-webgpu/?scene_file=jinxCombined&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=0.1&amp;impostors_forceonlybillboards&amp;impostors_texturesize=512" rel="nofollow">Impostors preview</a>. For this demo, I've increased the impostor texture size. This way you can see more details.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">You can find details in <a href="https://github.com/Scthe/nanite-webgpu/blob/master/USAGE.md">USAGE.md</a>. Short version:</p>
<ul dir="auto">
<li>Use the <code>[W, S, A, D]</code> keys to move and <code>[Z, SPACEBAR]</code> to fly up or down. <code>[Shift]</code> to move faster.</li>
<li>If there is something weird, toggle culling options on/off. There are some minor bugs in the implementation.</li>
<li>The white triangles are software-rasterized triangles (between hardware-rasterized ones and the impostors in the far back). WebGPU does not support <code>atomic&lt;u64&gt;</code>, so I had to compress data to fit into 32 bits (u16 for depth, 2*u8 for octahedron encoded normals).
<ul dir="auto">
<li>16-bit depth is.. not a great idea. It produces <strong>tons</strong> of artifacts like z-fighting or leaks. Turn the software rasterizer off to easier inspect raw Nanite meshlets. Be prepared for a major performance hit!</li>
</ul>
</li>
<li>FPS might fluctuate due to the browser's enforced VSync. Use the "Profile" button instead.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">FAQ</h2><a id="user-content-faq" aria-label="Permalink: FAQ" href="#faq"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">What are the major differences compared to UE5's Nanite?</h3><a id="user-content-what-are-the-major-differences-compared-to-ue5s-nanite" aria-label="Permalink: What are the major differences compared to UE5's Nanite?" href="#what-are-the-major-differences-compared-to-ue5s-nanite"></a></p>
<ul dir="auto">
<li>Error metric is just a simple projected simplification error (read below).</li>
<li>Meshlet simplification is.. simplistic.</li>
<li>No two-pass occlusion culling.
<ul dir="auto">
<li>This would not be complicated to add, just tedious to debug. Unfortunately, it also has some interactions with the GUI settings. ATM some parts of the code are riddled with <code>ifs</code> for certain user settings. For example, you could press "Freeze culling" to stop updating the list of drawn meshlets. This includes software rasterized meshlets. Move the camera in this mode and all 10+ million 1 px-sized software rasterized triangles might become fullscreen. Adding two-pass occlusion culling might expose more such interactions. It would also make the code harder to read, which goes against my goals.</li>
</ul>
</li>
<li>No work queue in shaders. For meshlet culling and LOD selection, I dispatch thread per-meshlet.</li>
<li>No VRAM eviction of unused LODs and streaming.
<ul dir="auto">
<li>Theoretically, to load new meshlet data, you would write requested <code>meshletIds</code> into a separate GPUBuffer. Download it to RAM and load the content. Keep LRU (timestamp per-meshlet, visible from CPU) to manage evictions. In practice, I suspect you might also want to add a priority system.</li>
</ul>
</li>
<li>No visibility buffer. It's not possible with the <code>atomic&lt;u64&gt;</code> limitation that I have.
<ul dir="auto">
<li>BTW if you render material data into a GBuffer, you get Nanite integration with your material system for free.</li>
</ul>
</li>
<li>No built-in shadows/multiview.</li>
<li>My implementation focuses on using a predictable amount of memory for demo cases. This means it's not scalable if you have many <strong>different</strong> objects (not instances). You would have to know the upper bound of the drawn meshlets to preallocate buffers that hold data between the stages. The naive solutions like <code>bottomLevelMeshletsCount * instanceCount</code> easily end up in GBs of VRAM!</li>
<li>No BVH for instances (or any other hierarchical implementation). I just take all instances and frustum + occlusion cull them.</li>
<li>I don't have a GPU profiler on the web/Deno. Or a debugger, or printf for that matter.
<ul dir="auto">
<li>ITWouldGenerate_DX_CODE_THATIWOULDHAVE_TO_READ_ANYWAY_SONOiGUESS.</li>
</ul>
</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Does xxx billions of triangles mean anything?</h3><a id="user-content-does-xxx-billions-of-triangles-mean-anything" aria-label="Permalink: Does xxx billions of triangles mean anything?" href="#does-xxx-billions-of-triangles-mean-anything"></a></p>
<p dir="auto">There was a video on YouTube showing how Nanite handles 120 billion triangles. Yet most of them were frustum culled? Performance depends on a lot of factors.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Dense meshes</h4><a id="user-content-dense-meshes" aria-label="Permalink: Dense meshes" href="#dense-meshes"></a></p>
<p dir="auto">Having a lot of dense meshes up close could have a negative performance impact. Unless you are so close to them that they cover 50% of the screen. Then, the occlusion culling kicks in. Dense geometry also means that meshlets are small. 128 triangles in a 20,000,000 triangle mesh? They do not take much space on the screen and are easily occlusion/cone culled.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Instance count</h4><a id="user-content-instance-count" aria-label="Permalink: Instance count" href="#instance-count"></a></p>
<p dir="auto">What about millions of instances? Each has its own mat4x3 transform matrix. This consumes VRAM. Obligatory link to <a href="https://pharr.org/matt/blog/2018/07/09/moana-island-pbrt-2" rel="nofollow">swallowing the elephant (part 2)</a>. During the frame, you also need to store a list of things to render. In the worst-case scenario, each instance will render its most dense meshlets. In my implementation, this allocates <code>instanceCount * bottomLevelMeshletsCount * sizeof(vec2u)</code> bytes. A 5k triangle bunny might have only 56 fine-level meshlets (out of 159 total), but what if I want to render 100,000 of them? This is not a scalable memory allocation. In Chrome, WebGPU has a 128MB limit for storage buffers (can be raised if needed). You might notice that the demo scenes above were tuned to reflect that.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Scene arrangement</h4><a id="user-content-scene-arrangement" aria-label="Permalink: Scene arrangement" href="#scene-arrangement"></a></p>
<p dir="auto">The scenes in my app have objects arranged in a square. For far objects, only a small part will be visible. But they will use coarse meshlet LOD, that contains more than just a visible part. The visible part passes occlusion culling and leads to a lot of overdraw for the rest of the meshlet. This is not an optimal scene arrangement. You would also think that a dense grid placement (objects close to each other) is bad. It certainly renders more triangles close to the camera. But it also means that there are no huge differences in depth between them. This is a dream for occlusion culling. You could build a wall from high-poly meshes and it's actually one of the most performant scenarios. Objects far from each other mean that a random distant pixel pollutes the depth pyramid (the <a href="https://www.youtube.com/@MentourPilot/videos" rel="nofollow">Swiss cheese theory</a>). Does your scene have a floor? Can you <a href="https://advances.realtimerendering.com/s2021/Karis_Nanite_SIGGRAPH_Advances_2021_final.pdf#page=96" rel="nofollow">merge</a> far objects into one?</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">In practice</h4><a id="user-content-in-practice" aria-label="Permalink: In practice" href="#in-practice"></a></p>
<p dir="auto">This leads to the <strong>Jinx test scene</strong>. The character is skinny. Looking down each row/column of the grid you can see the gaps. There is space between her arm and torso. This kills occlusion culling. The model does not simplify well. 3k triangles in the most coarse LOD (see below for more details). It's death by thousands of 1-pixel triangles. Software rasterizer helps a lot. Yet given the scene arrangement, most of the instances are rendered as impostors. Up close, the hardware rasterizer takes over. All 3 systems have different strengths.</p>
<blockquote>
<p dir="auto">With UE5's simplification algorithm, the balance is probably shifted. Much more software rasterizer, and less hardware one. And I wager a bet they don't have to rely on impostors as much. Their coarse LOD would be less than 3k tris (again, see below).</p>
</blockquote>
<p dir="auto">Basically, there are a lot of use cases. If you want a <strong>stable</strong> Nanite implementation, you have to test each one. But if you want a big triangle count, there are ways to cheat that too.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">What surprised you about Nanite?</h3><a id="user-content-what-surprised-you-about-nanite" aria-label="Permalink: What surprised you about Nanite?" href="#what-surprised-you-about-nanite"></a></p>
<ol dir="auto">
<li>The goal of the DAG is not to "use fewer triangles for far objects". The goal is to have a consistent 1 pixel == 1 triangle across the entire screen. A triangle is our "unit of data". The artist imports a sculpture from ZBrush. We need to need a way (through an error metric) to display it no matter if it's 1m or 500m from the camera. This is not possible with discrete LOD meshes (each LOD level is a separate geometry). Sometimes you would want an LOD between 2 levels. You need continuous LODs. This is the reason for the meshlet hierarchy. It allows you to "sample" geometry at any detail level you choose.</li>
<li>You spend more time working on culling and meshlets instead of Nanite itself. You <strong>WILL</strong> reimplement both <a href="https://ubm-twvideo01.s3.amazonaws.com/o1/vault/gdc2016/Presentations/Wihlidal_Graham_OptimizingTheGraphics.pdf" rel="nofollow">"Optimizing the Graphics Pipeline with Compute"</a> and <a href="https://advances.realtimerendering.com/s2015/aaltonenhaar_siggraph2015_combined_final_footer_220dpi.pdf" rel="nofollow">"GPU-Driven Rendering Pipelines"</a>.</li>
<li>Meshlet LOD hierarchy is quite easy to get working. Praise <a href="https://github.com/zeux/meshoptimizer">meshoptimizer</a> and <a href="http://glaros.dtc.umn.edu/gkhome/metis/metis/overview" rel="nofollow">METIS</a>! But if you want to do it efficiently, <a href="https://advances.realtimerendering.com/s2021/Karis_Nanite_SIGGRAPH_Advances_2021_final.pdf#page=50" rel="nofollow">it will be a pain</a>. See next question for full story. I just went with the simplest option.</li>
<li>If your mesh does not simplify cleanly, you end up with e.g. ~3000 triangles that cover a single pixel (Jinx scene). The efficiency scales with your mesh simplification code. And if you want pixel-sized triangles (the main selling point for most people), you <strong>need</strong> a software rasterizer. The billboard impostors are also a good stability-oriented fallback. As mentioned above, the whole system should work cohesively.</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">What about mesh simplification?</h3><a id="user-content-what-about-mesh-simplification" aria-label="Permalink: What about mesh simplification?" href="#what-about-mesh-simplification"></a></p>
<blockquote>
<p dir="auto">Remember, we are not doing a simple "take a mesh and return something that has X% of the triangles". We are doing the simplification in the context of meshlets and METIS.</p>
</blockquote>
<p dir="auto">UE5 has its own mesh simplification code. It's the first thing that happens in the asset pipeline. Thus, everything saved here will have avalanche-like benefits for the rest of the system. It was also a problem with the Jinx model. On <a href="https://advances.realtimerendering.com/s2021/Karis_Nanite_SIGGRAPH_Advances_2021_final.pdf#page=95" rel="nofollow">slide 95</a> Brian Karis states that <strong>all</strong> their LOD graphs end at a <strong>single</strong> root cluster. So no matter the model you provide, they can simplify it to 128 triangles. It makes you less reliant on the impostors. In my app, I could e.g. increase meshoptimizer's <code>target_error</code> parameter. But consider the following story:</p>
<ol dir="auto">
<li>My first test model was a bunny with 5k triangles. Easy to debug (check for holes, etc.). It simplified into a single 128 tris meshlet. Nice!</li>
<li>I've tried to load the Jinx model. At some point, the simplification stopped. You gave it X triangles and received the same X triangles. This crashed my app on an assertion.</li>
<li>OK, so if the model does not simplify beyond some level, I will allow the DAG to have many roots. If you failed to remove at least 6+% of the triangles, stop the algorithm for this part of the mesh.</li>
<li>The Jinx model now works correctly. It stops simplifying beyond 7-9 LOD levels, but this only means there are many hierarchy roots.</li>
<li>I load the bunny again and it no longer simplifies to a single root meshlet. Turns out, <strong>a lot of the meshlets did not reduce triangles that much</strong>. But with enough iterations, for such a simple model, we were able to reduce it to only 128 triangles. The whole time we were getting the &lt;6% simplification for some meshlets (so 94% of triangles were left untouched). We just did not know about it. And a lot of meshlets were also not "full". They contained less than 128 triangles.</li>
</ol>
<blockquote>
<p dir="auto">To reproduce, use <code>const SCENE_FILE: SceneName = 'singleBunny';</code> and set <code>CONFIG.nanite.preprocess.simplificationFactorRequirement: 0.94</code>. This option requires triangle reduction by at least 6%. We end up with 512 triangles. Then, set <code>simplificationFactorRequirement: 0.97</code> (require reducing triangle count by at least 3%, which is much more lenient). You end up with a single root that has 116 tris.</p>
</blockquote>
<p dir="auto">It was my first time using meshoptimizer, so you can probably tune it better. In the offline setting, it's possible to retry simplification with a bigger <code>target_error</code>. Or increase <code>target_error</code> for more coarse meshlet levels? From my experiments, both of these changes do not matter. You could also allow the hierarchy to have the bottom children on different levels (probably? there are some issues with this approach e.g. non-uniform mesh density). Maybe generate conservative (with a bigger triangle count than usual), discrete LOD levels in an old way and then use them if the algorithm gets stuck? This makes the error metric and the entire hierarchy pointless. Introduce new custom vertices? Merge more meshlets than 4? Smaller meshlets? Replace meshoptimizer? UE5 also has special weights for METIS partitioning. <strong>Most important, can your (METIS-enchanced) simplification, guarantee that splitting 256 triangles into 128 triangles, will ALWAYS result in 128 triangles?</strong> I think that once you have this guarantee, the simplification (while still not trivial), is significantly easier. With it, you no longer have to think about the concept of triangles in your meshlet hierarchy. You can start thinking only about DAG and nodes. This highlights the need for goor bottom-level meshlets.</p>
<p dir="auto">You may need someone to dedicate their time only to simplification. Personally, I just got it to work and moved on.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/9325337/348849351-157866eb-88e9-4896-895a-9400915478a4.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1NjQ5MDEsIm5iZiI6MTcyNTU2NDYwMSwicGF0aCI6Ii85MzI1MzM3LzM0ODg0OTM1MS0xNTc4NjZlYi04OGU5LTQ4OTYtODk1YS05NDAwOTE1NDc4YTQucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDkwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA5MDVUMTkzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NmY5NjI3ZTVmNjQwMWNmMmVkNDQ5MGYzOWQxMzU5ZmFjN2RlODMwODg1ZDAxOTA5N2E3YWJjMzhmYWUwMjg2NSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.DRI0B0vfRRt63fIazOdKQ7-fYXvIZLESQLA9VzGgkNk"><img src="https://private-user-images.githubusercontent.com/9325337/348849351-157866eb-88e9-4896-895a-9400915478a4.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1NjQ5MDEsIm5iZiI6MTcyNTU2NDYwMSwicGF0aCI6Ii85MzI1MzM3LzM0ODg0OTM1MS0xNTc4NjZlYi04OGU5LTQ4OTYtODk1YS05NDAwOTE1NDc4YTQucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDkwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA5MDVUMTkzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NmY5NjI3ZTVmNjQwMWNmMmVkNDQ5MGYzOWQxMzU5ZmFjN2RlODMwODg1ZDAxOTA5N2E3YWJjMzhmYWUwMjg2NSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.DRI0B0vfRRt63fIazOdKQ7-fYXvIZLESQLA9VzGgkNk" alt="simplification"></a></p>
<p dir="auto"><em>Trying to Nanite-simplify <a href="https://sketchfab.com/3d-models/modular-mecha-doll-neon-mask-1e0dcf3e016f4bc897d4b39819220732" rel="nofollow">Modular Mecha Doll Neon Mask</a> (910k tris) 3D model by Sketchfab user <a href="https://sketchfab.com/chambersu1996" rel="nofollow">Chambersu1996</a>. After the 5th hierarchy level, the simplification stops with 180k triangles left. This would be inefficient to render, but still manageable if we switched to impostors <strong>quickly</strong>. A better solution would be to actually spend X hours investigating the simplification process.</em></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">What about error metric?</h3><a id="user-content-what-about-error-metric" aria-label="Permalink: What about error metric?" href="#what-about-error-metric"></a></p>
<p dir="auto">Assume you have a mesh that has 20,000,000 triangles. With meshlet hierarchy, you can render it at any triangle count you would have wanted (with a minimum of 128 triangles - 1 meshlet). How do you choose the right meshlets? What does the <em>right meshlet</em> mean? At the end of the day, <strong>THIS</strong> is exactly what Nanite is. Everything else (simplification, meshlet DAG, software rasterizer, etc.) is just a prerequisite to actually start working on this problem. I admit, as the author of this repo, it's a bit disheartening.</p>
<p dir="auto">A few days ago, SIGGRAPH 2024 presentations were published. In <a href="https://advances.realtimerendering.com/s2024/content/Cao-NanoMesh/AdavanceRealtimeRendering_NanoMesh0810.pdf" rel="nofollow">"Seamless rendering on mobile"</a>, Shun Cao from Tencent Games provided the following metric (slide 12):</p>
<div dir="auto" data-snippet-clipboard-copy-content="device_factor = device_power * device_level
// from the slightly blurred graph image it seems to be:
// decay_factor(x) = 1 / (1 + exp(-(x-5000) / 1000)) from 0 to 9000
decay_factor = 1 / (1 + exp(distance_to_view / decay_distance))
threshold = projected_area * device_factor * decay_factor"><pre><span>device_factor</span> <span>=</span> <span>device_power</span> <span>*</span> <span>device_level</span>
<span>// from the slightly blurred graph image it seems to be:</span>
<span>// decay_factor(x) = 1 / (1 + exp(-(x-5000) / 1000)) from 0 to 9000</span>
<span>decay_factor</span> <span>=</span> <span>1</span> <span>/</span> <span>(</span><span>1</span> <span>+</span> <span>exp</span><span>(</span><span>distance_to_view</span> <span>/</span> <span>decay_distance</span><span>)</span><span>)</span>
<span>threshold</span> <span>=</span> <span>projected_area</span> <span>*</span> <span>device_factor</span> <span>*</span> <span>decay_factor</span></pre></div>
<p dir="auto"><em><a href="https://www.wolframalpha.com/input?i=f%28x%29+%3D+1+%2F+%281+%2B+exp%28-%28x-5000%29+%2F+1000%29%29+from+0+to+9000" rel="nofollow">Wolfram alpha for decay_factor</a> as far as I was able to decipher from the function image.</em></p>
<p dir="auto">I have used projected simplification error (as provided by meshoptimizer). It's not a great metric for Nanite. I think that other vertex attributes have to be part of this function too. You should be able to assign different weights on a per-attribute basis. Normals on Jinx's face were a huge problem. In my app, I could just move the LOD error threshold slider to the left. I can say that this approach has an educational value. You will have to find something better.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Should you write your own implementation of Nanite?</h3><a id="user-content-should-you-write-your-own-implementation-of-nanite" aria-label="Permalink: Should you write your own implementation of Nanite?" href="#should-you-write-your-own-implementation-of-nanite"></a></p>
<p dir="auto">Depends. The simplest answer is to just use UE5. You will not beat UE5 in its own game. Looking at Steam's front page, most of the games are simple enough to not need it. It's interesting that (at the time of the writing) the 2 most known Nanite titles are Fortnite and Senua's Saga: Hellblade II. Both have opposite objectives and tones. I recommend the Digital Foundry's <a href="https://www.youtube.com/watch?v=u-zmFVzUmPc" rel="nofollow">"Inside Senua's Saga: Hellblade 2 - An Unreal Engine 5 Masterpiece - The Ninja Theory Breakdown"</a>. E.g. they've mentioned a separate Houdini pipeline to extract transparency from static meshes. And while both games are different, both were developed by excellent engineering and visual teams.</p>
<p dir="auto">If you want to write your own implementation as a side project, then don't let me stop you. But unless you tackle simplification and error metric problems, you will end up with code similar to mine. You will still learn a lot.</p>
<p dir="auto">If you want to add this tech to the existing engine, I'm not a person you should be asking (I don't work in the industry). In my opinion, you should start by implementing <a href="https://ubm-twvideo01.s3.amazonaws.com/o1/vault/gdc2016/Presentations/Wihlidal_Graham_OptimizingTheGraphics.pdf" rel="nofollow">"Optimizing the Graphics Pipeline with Compute"</a> and <a href="https://advances.realtimerendering.com/s2015/aaltonenhaar_siggraph2015_combined_final_footer_220dpi.pdf" rel="nofollow">"GPU-Driven Rendering Pipelines"</a> first. This is already quite a complex task. Multi-step culling is tricky. You have to handle scene and world chunk management. Animated meshes. And that's just the beginning. But with these incremental steps, you will have something that works and can be tested at every step of the transition. Once this is stable, you can try a software rasterizer. Even if you don't end up shipping it, there is a lot to learn. Depending on the codebase, it can be surprisingly easy to add. Only after you have done the above steps you should try adding Nanite-like tech. As the various sliders in my app can tell you, they are all required for Nanite to be performant. The basic meshlet hierarchy for a toy renderer is a weekend project. Real implementation will have to deal with simplification and error metric issues.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Is per-meshlet backface cone culling worth it?</h3><a id="user-content-is-per-meshlet-backface-cone-culling-worth-it" aria-label="Permalink: Is per-meshlet backface cone culling worth it?" href="#is-per-meshlet-backface-cone-culling-worth-it"></a></p>
<p dir="auto">I've implemented the basics, but the gains are limited. Check the comment in <a href="https://github.com/Scthe/nanite-webgpu/blob/8c15e85b32d8b890ef573f58f1fbb782544f972c/src/constants.ts#L160">constants.ts</a> for implementation details.</p>
<ol dir="auto">
<li>It works best if you have a dense mesh where all triangles in a cluster have similar normals. Dense meshes are something that Nanite was designed for. Yet coarse LOD levels will have normals pointing in different directions. Arseny Kapoulkine had <a href="https://zeux.io/2023/04/28/triangle-backface-culling/#estimating-culling-efficiency" rel="nofollow">similar observations</a>.</li>
<li>There is some duplication with occlusion culling. Backfaces are behind front faces in the z-buffer.</li>
<li>Computing the cone is done on a per-meshlet level. For me, this means a WebAssembly call every time. This took 30% of the whole preprocessing step. Preprocessing all models offline would solve this problem. Yet it goes against my goals for this project. I want you to take the simplest possible 3D object format and see that my program works. That's why this app is a webpage and not Rust+Vulkan. No one would have cloned the repo to run the code. But everyone has clicked the demo links above (right?).</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Honourable mentions</h2><a id="user-content-honourable-mentions" aria-label="Permalink: Honourable mentions" href="#honourable-mentions"></a></p>
<ul dir="auto">
<li>Arseny Kapoulkine. This app is only possible due to <a href="https://github.com/zeux/meshoptimizer">meshoptimizer</a>. I've also watched a few of niagara videos and read its source code. And read his blog.
<ul dir="auto">
<li><a href="https://github.com/zeux/meshoptimizer/pull/704" data-hovercard-type="pull_request" data-hovercard-url="/zeux/meshoptimizer/pull/704/hovercard">Newer meshoptimizer versions</a> have <code>meshopt_SimplifySparse</code> specifically for Nanite clones. I've not updated to this version as I am moving on from this app. I want to leave it in a state that I've tested during development.</li>
</ul>
</li>
<li>Folks from Epic Games. Not only for creating Nanite but also for being open on how it works under the hood.</li>
<li><a href="https://ubm-twvideo01.s3.amazonaws.com/o1/vault/gdc2016/Presentations/Wihlidal_Graham_OptimizingTheGraphics.pdf" rel="nofollow">"Optimizing the Graphics Pipeline with Compute"</a> by Graham Wihlidal.</li>
<li><a href="https://advances.realtimerendering.com/s2015/aaltonenhaar_siggraph2015_combined_final_footer_220dpi.pdf" rel="nofollow">"GPU-Driven Rendering Pipelines"</a> by Ulrich Haar and Sebastian Aaltonen.</li>
<li><a href="http://glaros.dtc.umn.edu/gkhome/metis/metis/overview" rel="nofollow">METIS</a>.</li>
<li><a href="https://emscripten.org/" rel="nofollow">Emscripten</a>. Used to run both meshoptimizer and METIS in the browser.
<ul dir="auto">
<li><a href="https://marcoselvatici.github.io/WASM_tutorial/" rel="nofollow">Webassembly Tutorial</a> by Marco Selvatici.</li>
</ul>
</li>
<li><a href="https://sketchfab.com/3d-models/arcane-jinx-b74f25a5ee6e43efbe9766b9fbebc705" rel="nofollow">Arcane - Jinx</a> 3D model by sketchfab user <a href="https://sketchfab.com/rizky08" rel="nofollow">Craft Tama</a>. Used under <a href="https://creativecommons.org/licenses/by/4.0/" rel="nofollow">CC Attribution</a> license.
<ul dir="auto">
<li>I've merged the textures, adjusted UVs, and removed the weapon.</li>
</ul>
</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Deploying Rust in Existing Firmware Codebases (106 pts)]]></title>
            <link>https://security.googleblog.com/2024/09/deploying-rust-in-existing-firmware.html</link>
            <guid>41458508</guid>
            <pubDate>Thu, 05 Sep 2024 17:02:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://security.googleblog.com/2024/09/deploying-rust-in-existing-firmware.html">https://security.googleblog.com/2024/09/deploying-rust-in-existing-firmware.html</a>, See on <a href="https://news.ycombinator.com/item?id=41458508">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-version="1" id="header">
<div>
<p><a href="https://security.googleblog.com/">
<img height="50" src="https://www.gstatic.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png">
</a></p><a href="https://security.googleblog.com/">
<h2>
            Security Blog
          </h2>
</a>
</div>
<p>
The latest news and insights from Google on security and safety on the Internet
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[serverless-registry: A Docker registry backed by Workers and R2 (108 pts)]]></title>
            <link>https://github.com/cloudflare/serverless-registry</link>
            <guid>41458240</guid>
            <pubDate>Thu, 05 Sep 2024 16:34:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/cloudflare/serverless-registry">https://github.com/cloudflare/serverless-registry</a>, See on <a href="https://news.ycombinator.com/item?id=41458240">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Container Registry in Workers</h2><a id="user-content-container-registry-in-workers" aria-label="Permalink: Container Registry in Workers" href="#container-registry-in-workers"></a></p>
<p dir="auto">This repository contains a container registry implementation in Workers that uses R2.</p>
<p dir="auto">It supports all pushing and pulling workflows. It also supports
Username/Password and public key JWT based authentication.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Deployment</h3><a id="user-content-deployment" aria-label="Permalink: Deployment" href="#deployment"></a></p>
<p dir="auto">You have to install all the dependencies with <a href="https://pnpm.io/installation" rel="nofollow">pnpm</a> (other package managers may work, but only pnpm is supported.)</p>

<p dir="auto">After installation, there is a few steps to actually deploy the registry into production:</p>
<ol dir="auto">
<li>Have your own <code>wrangler</code> file.</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="$ cp wrangler.toml.example wrangler.toml"><pre>$ cp wrangler.toml.example wrangler.toml</pre></div>
<ol start="2" dir="auto">
<li>Setup the R2 Bucket for this registry</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="$ npx wrangler --env production r2 bucket create r2-registry"><pre>$ npx wrangler --env production r2 bucket create r2-registry</pre></div>
<p dir="auto">Add this to your <code>wrangler.toml</code></p>
<div data-snippet-clipboard-copy-content="r2_buckets = [
    { binding = &quot;REGISTRY&quot;, bucket_name = &quot;r2-registry&quot;}
]"><pre><code>r2_buckets = [
    { binding = "REGISTRY", bucket_name = "r2-registry"}
]
</code></pre></div>
<ol start="3" dir="auto">
<li>Deploy your image registry</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="$ npx wrangler deploy --env production"><pre>$ npx wrangler deploy --env production</pre></div>
<p dir="auto">Your registry should be up and running. It will refuse any requests if you don't setup credentials.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Adding username password based authentication</h3><a id="user-content-adding-username-password-based-authentication" aria-label="Permalink: Adding username password based authentication" href="#adding-username-password-based-authentication"></a></p>
<p dir="auto">Set the USERNAME and PASSWORD as secrets with <code>npx wrangler secret put USERNAME --env production</code> and <code>npx wrangler secret put PASSWORD --env production</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Adding JWT authentication with public key</h3><a id="user-content-adding-jwt-authentication-with-public-key" aria-label="Permalink: Adding JWT authentication with public key" href="#adding-jwt-authentication-with-public-key"></a></p>
<p dir="auto">You can add a base64 encoded JWT public key to verify passwords (or token) that are signed by the private key.
<code>npx wrangler secret put JWT_REGISTRY_TOKENS_PUBLIC_KEY --env production</code></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Using with Docker</h3><a id="user-content-using-with-docker" aria-label="Permalink: Using with Docker" href="#using-with-docker"></a></p>
<p dir="auto">You can use this registry with Docker to push and pull images.</p>
<p dir="auto">Example using <code>docker push</code> and <code>docker pull</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="export REGISTRY_URL=your-url-here

# Replace $PASSWORD and $USERNAME with the actual credentials
echo $PASSWORD | docker login --username $USERNAME --password-stdin $REGISTRY_URL
docker pull ubuntu:latest
docker tag ubuntu:latest $REGISTRY_URL/ubuntu:latest
docker push $REGISTRY_URL/ubuntu:latest

# Check that pulls work
docker rmi ubuntu:latest $REGISTRY_URL/ubuntu:latest
docker pull $REGISTRY_URL/ubuntu:latest"><pre><span>export</span> REGISTRY_URL=your-url-here

<span><span>#</span> Replace $PASSWORD and $USERNAME with the actual credentials</span>
<span>echo</span> <span>$PASSWORD</span> <span>|</span> docker login --username <span>$USERNAME</span> --password-stdin <span>$REGISTRY_URL</span>
docker pull ubuntu:latest
docker tag ubuntu:latest <span>$REGISTRY_URL</span>/ubuntu:latest
docker push <span>$REGISTRY_URL</span>/ubuntu:latest

<span><span>#</span> Check that pulls work</span>
docker rmi ubuntu:latest <span>$REGISTRY_URL</span>/ubuntu:latest
docker pull <span>$REGISTRY_URL</span>/ubuntu:latest</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Configuring Pull fallback</h3><a id="user-content-configuring-pull-fallback" aria-label="Permalink: Configuring Pull fallback" href="#configuring-pull-fallback"></a></p>
<p dir="auto">You can configure the R2 regitry to fallback to another registry if
it doesn't exist in your R2 bucket. It will download from the registry
and copy it into the R2 bucket. In the next pull it will be able to pull it directly from R2.</p>
<p dir="auto">This is very useful for migrating from one registry to <code>serverless-registry</code>.</p>
<p dir="auto">It supports both Basic and Bearer authentications as explained in the
<a href="https://distribution.github.io/distribution/spec/auth/token/" rel="nofollow">registry spec</a>.</p>
<p dir="auto">In the wrangler.toml file:</p>
<div data-snippet-clipboard-copy-content="[env.production]
REGISTRIES_JSON = &quot;[{ \&quot;registry\&quot;: \&quot;https://url-to-other-registry\&quot;, \&quot;password_env\&quot;: \&quot;REGISTRY_TOKEN\&quot;, \&quot;username\&quot;: \&quot;username-to-use\&quot; }]&quot;"><pre><code>[env.production]
REGISTRIES_JSON = "[{ \"registry\": \"https://url-to-other-registry\", \"password_env\": \"REGISTRY_TOKEN\", \"username\": \"username-to-use\" }]"
</code></pre></div>
<p dir="auto">Set as a secret the registry token of the registry you want to setup
pull fallback in.</p>
<p dir="auto">For example <a href="https://cloud.google.com/artifact-registry/docs/reference/docker-api" rel="nofollow">gcr</a>:</p>
<div data-snippet-clipboard-copy-content="cat ./registry-service-credentials.json | base64 | wrangler --env production secrets put REGISTRY_TOKEN"><pre><code>cat ./registry-service-credentials.json | base64 | wrangler --env production secrets put REGISTRY_TOKEN
</code></pre></div>
<p dir="auto"><a href="https://github.com/settings/tokens">Github</a> for example uses a simple token that you can copy.</p>
<div data-snippet-clipboard-copy-content="echo $GITHUB_TOKEN | wrangler --env production secrets put REGISTRY_TOKEN"><pre><code>echo $GITHUB_TOKEN | wrangler --env production secrets put REGISTRY_TOKEN
</code></pre></div>
<p dir="auto">The trick is always looking for how you would login in Docker for
the target registry and setup the credentials.</p>
<p dir="auto"><strong>Never put a registry password/token inside the wrangler.toml, please always use <code>wrangler secrets put</code></strong></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Known limitations</h3><a id="user-content-known-limitations" aria-label="Permalink: Known limitations" href="#known-limitations"></a></p>
<p dir="auto">Right now there is some limitations with this container registry.</p>
<ul dir="auto">
<li>Pushing with docker is limited to images that have layers of maximum size 500MB. Refer to maximum request body sizes in your Workers plan.</li>
<li>To circumvent that limitation, you can manually add the layer and the manifest into the R2 bucket or use a client that is able to chunk uploads in sizes less than 500MB (or the limit that you have in your Workers plan).</li>
<li>If you use <code>npx wrangler dev</code> and push to the R2 registry with docker, the R2 registry will have to buffer the request on the Worker.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">The project is licensed under the <a href="https://opensource.org/licenses/apache-2.0/" rel="nofollow">Apache License</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Contribution</h3><a id="user-content-contribution" aria-label="Permalink: Contribution" href="#contribution"></a></p>
<p dir="auto">See <code>CONTRIBUTING.md</code> for contributing to the project.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Phind-405B and faster, high quality AI answers for everyone (183 pts)]]></title>
            <link>https://www.phind.com/blog/introducing-phind-405b-and-better-faster-searches</link>
            <guid>41458083</guid>
            <pubDate>Thu, 05 Sep 2024 16:22:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.phind.com/blog/introducing-phind-405b-and-better-faster-searches">https://www.phind.com/blog/introducing-phind-405b-and-better-faster-searches</a>, See on <a href="https://news.ycombinator.com/item?id=41458083">Hacker News</a></p>
Couldn't get https://www.phind.com/blog/introducing-phind-405b-and-better-faster-searches: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Boxed – Things I learned after lying in an MRI machine for 30 hours (112 pts)]]></title>
            <link>https://aethermug.com/posts/boxed</link>
            <guid>41457739</guid>
            <pubDate>Thu, 05 Sep 2024 15:50:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aethermug.com/posts/boxed">https://aethermug.com/posts/boxed</a>, See on <a href="https://news.ycombinator.com/item?id=41457739">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main components="[object Object]"><p>Last year a researcher from a Japanese lab asked me if he could borrow my <a href="https://en.wikipedia.org/wiki/Aphantasia" rel="nofollow noopener noreferrer" target="_blank">aphantasic</a> brain for an experiment. Neuroscientists have been studying the weird capacity most people have of "visualizing", or mentally conjuring pictures of things that aren't there, for a long time. The problem is that the brain is such a wild tangle of interconnections that it's hard to tell which of its parts are involved in visualization and which are unrelated, or downstream of it. Comparing people who visualize with aphantasics who don't is a very convenient way to partially work around that problem.</p>
<p>I agreed to participate in the experiment. That decision led to me spending an inordinate amount of time deep inside an MRI, looking at pictures or trying (and failing) to imagine pictures. The machine, worth half a dozen Lamborghinis, is hidden somewhere in a basement on the University of Tokyo campus.</p>
<hr><figure id="floating-1"><video src="https://aethermug.com/assets/posts/aphantasia/mri.mp4" controls="" autoplay="" loop="" muted=""></video><figcaption>Click to play the animation. Source: Fastfission, CC BY-SA 3.0.</figcaption></figure>
<p>Essentially, an MRI is a big pipe you climb into, which happens to be capable of seeing right through you. Wrapped around the pipe is a hidden network of metal coils cooled to 9 degrees Celsius above absolute zero, constantly switching very large currents to fill the hollow inside with a strong magnetic field and shooting (harmless) radio waves at some corner of your body—the brain, in this case.</p>
<p>In neuroscience, MRI is used to detect microscopic changes in blood flow inside the brain, allowing the researchers to obtain a 3D video map of which neural networks are active at each instant.</p>
<p>In my experiments, the researchers record what goes on in my head when I look at a picture of a fire hydrant, then a camel, then a piece of wood, and so on, covering thousands of pictures over the months. Other times they asked me to imagine various things, or to remember images that I had been shown earlier.</p>
<p>The scientific side of these experiments is very interesting, but there are many other places to read about it in depth. Here, instead, I want to leave some notes on my subjective observations of this experience. I've found that lying down in a cramped space for hours at a time, fully awake and with only very weird kinds of pastimes, leads to some unusual reflections.</p>
<p>These are a few of those reflections, in approximate order of decreasing banality.</p>
<h2>1 - It's noisy</h2>
<p>Anyone who has spent more than two seconds in an MRI knows this, but if you're thinking of trying for the first time, be warned: those things are LOUD.</p>
<p>The high-frequency current switching causes the metal coils around you to expand and contract and vibrate like a jackhammer. You feel like a rescue team is trying all they can to get you out of an unbreakable iron coffin. If you're sensitive to loud noises, you might want to avoid it.</p>
<p>Usually they give you foam earplugs before you go in, and those make the experience bearable for me. Still, there are risks. The other day I mis-inserted one of the earplugs and got a taste of the unabridged experience: I tried to continue like that for a while, but I was so worried for my eardrum that eventually I had to stop the experiment early. When I got out I felt like the left side of my head had been in the front row at a rock concert. Not healthy.</p>
<p>Even with properly-inserted earplugs, the racket echoing all around you is the most unpleasant part of the experiment. It's surprisingly draining. If it weren't for that, I think I could keep going just fine for much longer.</p>
<h2>2 - It's <em>very</em> good at putting you to sleep</h2>
<p>Remaining horizontal in a semi-dark space for a while doesn't help you stay awake. This shouldn't come as a surprise to most people, but it surprised me. I usually have the opposite problem, because I <em>never</em> nap or sleep during daytime (as in, maybe once a year), and not for lack of trying. There is something about the infinite possible activities that I could be doing instead of sleeping that excites my brain during the day, even when I'm very tired.</p>
<p>Yet, looking at long sequences of unrelated pictures or—much worse—the same set of pictures over and over has an almost magical power to induce slumber. I haven't succumbed to it yet, but I've come close a few times. I should try something similar for my next napping attempt.</p>
<hr><figure id="floating-2"><img src="https://aethermug.com/assets/posts/boxed/mri.webp" alt="A gray surface with a gray stripe running through it."><figcaption>The scenery as seen from inside the machine.</figcaption></figure>
<h2>3 - My brain craves novelty</h2>
<p>Aside from the problem of sleepiness, I was shocked at how the lack of new stimuli can strain my grasp on my mental faculties.</p>
<p>Some experiments consist of seeing the same images hundreds of times, or repeating the same task over and over with only minor variations. Since an MRI pipe is essentially an isolation chamber, those tasks and images (and the noise) are the only sensory input you're going to get. After a while, my brain started rejecting them.</p>
<p>At first, it wasn't a big deal, but after a few weeks of those repetitive tasks, even the simple act of paying attention to what was in front of my eyes began to take a tremendous effort of concentration. I came out in shambles every time, depleted of life force, and I even thought of the word <em>torture</em> once or twice. But hey, someone's gotta do this.</p>
<p>Luckily that repetitive series of experiments ended just before I reached my breaking point. I wonder if this is something that can be trained, but I'm not sure I'd want to do that either.</p>
<h2>4 - Novel, random images are great for creativity</h2>
<p>Most of the experiments involve looking at non-repeating images, meaning that I see each one only once and never again. My brain is apparently fine with this and, with a good infusion of caffeine, it's actually happy to go on the ride.</p>
<p>This is an experience you don't usually have in your daily life. Normally you know, more or less, what to expect to see next. Even when you can't predict what's coming—when you're watching a movie, for example—things have some kind of connection to each other, some theme or context that ties them together. With random images in a lab, none of that exists. Now you're seeing a picture of a man blowing smoke from his mouth in the Grand Canyon, next it could be a close-up on a smudged corner of a book, or a group of penguins near an ice cliff, or a pile of broken CRT monitors, or something else altogether.</p>
<p>Every four seconds or so, you see something new that you would never have guessed from the previous pictures. Each time it's a different cascade of activations in your brain, evoking random memories, creating unexpected connections, and stimulating thoughts that would never have occurred to you.</p>
<p>Something strange happens: even though it's all purely random, the brain tries to make sense of it all, tries to find patterns and associations. With no time to establish conventional <a href="https://aethermug.com/posts/a-framing-is-a-choice-of-boundaries">framings</a>, it has to improvise, take in the images in a partly-unconscious way, without thorough processing. This, I think, is a great way to stimulate creativity.</p>
<p>A few times during those experiments, I came up with so many ideas—things to write about, better ways to explain things, new intriguing questions about the world, etc.—that my biggest worry was trying to remember them all for the 20 or 30 minutes left until the end of the session. In the short pauses between bursts of images, I tried to rehearse the list of ideas with shortened mnemonics, but found that I could only keep around five in my head before I forgot some of them.</p>
<p>This is an amazing state to have my brain in, and I wish I could induce it at will. Social media feeds look similar on the surface, but they don't give you truly random stimuli. Their contents are highly edited to appeal to the viewer, and come with lots of cultural baggage and trend-following. They don't work to unhinge my creativity—rather, they trap it.</p>
<p>What I need is an app that does nothing but show you truly random pictures, with no curation and no memetic aspirations. If you know of one, please let me know.</p>
<hr><figure id="floating-3"><img src="https://aethermug.com/assets/posts/boxed/dylan-nolte-qxYDhV0rBPk-unsplash.webp" alt="An athlete's hand being wrapped in tape by someone."><figcaption>A random picture. I bet you didn't see that coming. Source: Dylan Nolte, Unsplash </figcaption></figure>
<h2>5 - Our sense of time is non-linear even at the shortest scales</h2>
<p>Everybody has experienced the subjective relativity of time. When you have fun, it flies. When you're waiting anxiously, it never budges. But, before these experiments, I had never realized just how warpy my perception of time can be even on the scale of a couple of seconds.</p>
<p>These tasks with pictures require me to stay focused on what's shown on the screen. To ensure that I'm not distracted, the screen will show certain cues at random intervals, to which I have to react by pressing a button. Usually, the cue is the repetition of the same image twice in a row: normally each image stays there only four seconds, but sometimes it will flash back to itself instead of being replaced by a different image. This sounds like an easy thing to spot, and most of the time it is. But when I'm not in an optimal shape, it can become fiendishly difficult.</p>
<p>I find myself asking, have I seen this same picture <em>two seconds ago</em> or not? Is this still the first four seconds?</p>
<p>This is so strange and almost disconcerting. In the highly controlled environment of the lab, I can easily notice these lapses in my perception of time, but what about all the other times? Does my sense of time ebb and flow like that every few seconds of my waking life?</p>
<h2>6 - Having your thoughts monitored feels... weird</h2>
<p>Sometimes, during the experiments, I wonder about things like:</p>
<ul>
<li>Is it enough to just watch, or should I think intensely about the subject?</li>
<li>I was asked to imagine the picture of a duck, but I imagined a moving duck, beating its wings quickly. Will this taint the results?</li>
</ul>
<p>In order to protect the objectivity of the experiments, the researchers don't tell me exactly how they're analyzing the fMRI data, or what their hypotheses are. Still, I know that they happen to have the closest thing ever to <em>mind-reading technology</em>, which has interesting implications.</p>
<p>In previous studies, they have successfully trained generative AI to read fMRI scans and replicate the images people were thinking about, or to add captions describing those mental images. Considering the amount of data they're taking of my brain, it would be possible for them to train an AI for my specific brain patterns.</p>
<p>I don't think many people have experienced this situation before. People have had tyrants and Big Brothers spying on their actions and words for millennia, but has anyone ever had their thoughts monitored? In a sense, I feel naked.</p>
<p>For now, this is not a big problem. At worst, I might worry that they would know it when I'm distracted, and I might try not to think about kinky stuff (usually a big mistake). Not the end of the world. But I can't help imagining about the dystopian societies that could emerge if the same technology was somehow scaled to portable sizes and affordable prices.</p>
<h2>7 - We are usually oblivious to what our brains are doing</h2>
<p>I've only ever tried short-ish sessions of mindfulness-like meditation, where the goal is to free your mind of thoughts, focus on your breathing, or something along those lines. I don't know about other types of meditation, but I would guess that most of them are about relaxing or at least not thinking very hard. All of these may help you, in one way or another, to feel better and more centered and even, in some cases, to know your body and mind better.</p>
<p>But I doubt there is a kind of meditation that prompts the level of introspection that long hours in an MRI machine doing simple but focused tasks can give you.</p>
<p>Inside the machine, I have to remain very still with no phone to check, nothing to read, no tossing and turning just for the sake of it, nothing to fiddle with, and—given that the tasks all require a moderate but constant level of attention, no opportunity to get really lost in thought. I did this for over 30 hours now, and counting.</p>
<p>In other words, I got to spend a lot of time in the peaceful, forced company of myself, not too cognitively busy but neither focusing on breathing or clearing my head. It's a sort of Goldilocks zone not only for creativity (point 4), but also for the observation of how my mind works.</p>
<p>In this process, I've learned more about myself than I had in the previous two decades. For example, it's how I uncovered the details of how my non-visual imagination works—something I had never noticed before. One day, right after a session in the MRI, I ran to a cafe and wrote a Twitter thread explaining exactly what that is like for me. The thread had a surprising success, attracting the keen interest of lots of other people (you can read a copy of the thread <a href="https://aethermug.com/posts/aphantasia">here</a>). Apparently it's a kind of description that is usually hard to come by, yet it came easily to me with all that time of confinement.</p>
<p>This unexpected success at discovering new things about myself has encouraged me to try this introspection outside the lab, too. That's how I first realized I have <a href="https://synesthesia-test.com/time-space-synesthesia" rel="nofollow noopener noreferrer" target="_blank">time-space synesthesia</a>, something that I hadn't even heard of before. It's also how I realized that wearing special earplugs in noisy places helps me understand what the people around me are saying, mitigating a mild auditory processing disorder that I had never thought much about. And so on, with a new quirk or peculiarity coming to my attention every now and then as I do other things.</p>
<p>Somehow the mere fact of staying still in a state halfway between emptying my mind and filling it to the brim has helped me become more attuned to myself. I feel a bit more centered in the moment, so to speak. Much more than before, I now consider the brain to be an organ that you can observe and study, a <a href="https://aethermug.com/posts/a-black-box-view-of-life">black box</a> you can tinker with (carefully) to better understand it. This kind of exploration can be very fruitful, showing you what works best for you, what to avoid, how to be kind to yourself, and generally how to "use" your brain more expertly. ●</p>
<div><p>Cover image:</p><p><em>Photo by Vladimir Kramer, Unsplash</em></p></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: AnythingLLM – Open-Source, All-in-One Desktop AI Assistant (204 pts)]]></title>
            <link>https://github.com/Mintplex-Labs/anything-llm</link>
            <guid>41457633</guid>
            <pubDate>Thu, 05 Sep 2024 15:40:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Mintplex-Labs/anything-llm">https://github.com/Mintplex-Labs/anything-llm</a>, See on <a href="https://news.ycombinator.com/item?id=41457633">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">
  <a href="https://anythingllm.com/" rel="nofollow"><img src="https://github.com/Mintplex-Labs/anything-llm/raw/master/images/wordmark.png?raw=true" alt="AnythingLLM logo"></a>
</p>
<p><a href="https://trendshift.io/repositories/2415" rel="nofollow"><img src="https://camo.githubusercontent.com/13a0218d5bc383ce61ac2154a48e40d6ce0c079fe6c930524732238d033b7a72/68747470733a2f2f7472656e6473686966742e696f2f6170692f62616467652f7265706f7369746f726965732f32343135" alt="Mintplex-Labs%2Fanything-llm | Trendshift" width="250" height="55" data-canonical-src="https://trendshift.io/api/badge/repositories/2415"></a>
</p>
<p dir="auto">
    <b>AnythingLLM:</b> The all-in-one AI app you were looking for.<br>
    Chat with your docs, use AI Agents, hyper-configurable, multi-user, &amp; no frustrating set up required.
</p>
<p dir="auto">
  <a href="https://discord.gg/6UyHPeGZAC" rel="nofollow">
      <img src="https://camo.githubusercontent.com/3b816ca02eba9f9b0e63fe98bdfff52b93212792c74d87757fabdf67c42e9e36/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636861742d6d696e74706c65785f6c6162732d626c75652e7376673f7374796c653d666c6174266c6f676f3d646174613a696d6167652f706e673b6261736536342c6956424f5277304b47676f414141414e5355684555674141414341414141416743414d4141414245704972474141414149474e49556b304141486f6d41414341684141412b6741414149446f414142314d414141366d41414144715941414158634a79365554774141414831554578555251414141502f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f72362b75626e352b3775372f332b2f7633392f656e7136757271362f76372b393766333972623236656f715431425130704f54342b526b757a7337636e4b796b5a4b53304e4853486c3866647a6433656a6f365578505555424452647a63335277674968386a4a53416b4a6d3578637648783861616e714234694a46425456657a7437563568596c4a5656754c6a343370396669496d4b434d6e4b5a4b556c61616f7153456c4a32317763665430394f337537757672367a45304e72362f774355704b3571636e66372b2f6e68376645644b5448782b66307450554f546c3561697071696f754d477475627a3543524451344f7354477875666e35313568593761337548312f6758427964494f46686c5659577658323971616f7143516f4b7337507a2f507a38372f417755744f554e665932644852306d6872624f7672374535525579387a4e585232642f6633392b586c35555a4a53783068497a51334f647261322f7a382f476c736261476a7045524853657a73374c2f42775363724c5451344f646e61327a4d334f626d377533782f674b536d70396a5a3254314151752f7637317064586b56495372322b767967734c69496e4b54673750614f6c706973764d635847787a6b38506c646158504c793875377537726d36753753317473444277766a342b4d5045786265347565586d35732f51304b7966376577414141416f64464a4f5577414142436c73724e6a782f514d326c392f376c686d49366a54422f6b413147674b4a4e2b6e65613676792f4d4c5a515965564b4b3372564135744141414141574a4c523051422f774974336741414141643053553146422b634b4241416d4d5a42486a584941414149535355524256446a4c59324341416b596d5a685a574e6e594f446e593256685a6d4a6b5947564d44497963584e77367342426277386646796379456f5947666b4642445651674b414150794d6a516c35495745514444596749433846554d444b4b736d6c67415779694542574d6a474a5935594571784d4171474d57464e58414159584767416b594a5351326351464b436b59465253687133416d6b705267594a62676862553074624230547236756b626747684449313067795366427743774455574273596d706d447151744c4b327362545130624f3373485941384757594757576a34575473364f627534616d69344f546d3765786871654870352b344443564a5a42446d7164723775666e332b41726b5a676b4a2b6655334349526d67595746694f41525947766f354f5155486845554146546b462b6b564852734c42676b496579596d4c6a776f4f6334684d536b354a546e494e53303644433867776345455a3652715a476c704f6663335a4f626c352b675a2b5452324552574679425151464d4635656b6c6d7155705162352b52655536315a554f766b465656585851425341726169747132396f3147694b63664c7a63323975306d6a78427a71307451306b777735785a48744855476558686b5a6864784259675a3464304c49366334676a7764377369515172614f703141697651364375414b5a43444242525151514e516755622f4247663363714343695a4f636e4365335151494b484e5254706b36624467705a6a526b7a673370425154427264744363755a43676c7541443076506d4c316749647653697855755767714e7332594a2b4455686b4559787567676b476d4f515563636b72696f50544a434f58456e5a354a533559736c62476e75795645526c44444676474555504f5776777161483652566b484b6575444d4b36534b6e486c5668546778386a65546d71793645696a374b366e4c71694779507743687361314d55726e713177414141435630525668305a4746305a54706a636d5668644755414d6a41794d7930784d4330774e4651774d446f7a4f446f304f5373774d446f774d423956306138414141416c6445565964475268644755366257396b61575a35414449774d6a4d744d5441744d4452554d4441364d7a67364e446b724d4441364d44427543476b54414141414b4852465748526b5958526c4f6e52706257567a64474674634141794d44497a4c5445774c544130564441774f6a4d344f6a51354b7a41774f6a41774f5231497a4141414141424a52553545726b4a6767673d3d" alt="Discord" data-canonical-src="https://img.shields.io/badge/chat-mintplex_labs-blue.svg?style=flat&amp;logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAMAAABEpIrGAAAAIGNIUk0AAHomAACAhAAA+gAAAIDoAAB1MAAA6mAAADqYAAAXcJy6UTwAAAH1UExURQAAAP////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////r6+ubn5+7u7/3+/v39/enq6urq6/v7+97f39rb26eoqT1BQ0pOT4+Rkuzs7cnKykZKS0NHSHl8fdzd3ejo6UxPUUBDRdzc3RwgIh8jJSAkJm5xcvHx8aanqB4iJFBTVezt7V5hYlJVVuLj43p9fiImKCMnKZKUlaaoqSElJ21wcfT09O3u7uvr6zE0Nr6/wCUpK5qcnf7+/nh7fEdKTHx+f0tPUOTl5aipqiouMGtubz5CRDQ4OsTGxufn515hY7a3uH1/gXBydIOFhlVYWvX29qaoqCQoKs7Pz/Pz87/AwUtOUNfY2dHR0mhrbOvr7E5RUy8zNXR2d/f39+Xl5UZJSx0hIzQ3Odra2/z8/GlsbaGjpERHSezs7L/BwScrLTQ4Odna2zM3Obm7u3x/gKSmp9jZ2T1AQu/v71pdXkVISr2+vygsLiInKTg7PaOlpisvMcXGxzk8PldaXPLy8u7u7rm6u7S1tsDBwvj4+MPExbe4ueXm5s/Q0Kyf7ewAAAAodFJOUwAABClsrNjx/QM2l9/7lhmI6jTB/kA1GgKJN+nea6vy/MLZQYeVKK3rVA5tAAAAAWJLR0QB/wIt3gAAAAd0SU1FB+cKBAAmMZBHjXIAAAISSURBVDjLY2CAAkYmZhZWNnYODnY2VhZmJkYGVMDIycXNw6sBBbw8fFycyEoYGfkFBDVQgKAAPyMjQl5IWEQDDYgIC8FUMDKKsmlgAWyiEBWMjGJY5YEqxMAqGMWFNXAAYXGgAkYJSQ2cQFKCkYFRShq3AmkpRgYJbghbU0tbB0Tr6ukbgGhDI10gySfBwCwDUWBsYmpmDqQtLK2sbTQ0bO3sHYA8GWYGWWj4WTs6Obu4ami4OTm7exhqeHp5+4DCVJZBDmqdr7ufn3+ArkZgkJ+fU3CIRmgYWFiOARYGvo5OQUHhEUAFTkF+kVHRsLBgkIeyYmLjwoOc4hMSk5JTnINS06DC8gwcEEZ6RqZGlpOfc3ZObl5+gZ+TR2ERWFyBQQFMF5eklmqUpQb5+ReU61ZUOvkFVVXXQBSAraitq29o1GiKcfLzc29u0mjxBzq0tQ0kww5xZHtHUGeXhkZhdxBYgZ4d0LI6c4gjwd7siQQraOp1AivQ6CuAKZCDBBRQQQNQgUb/BGf3cqCCiZOcnCe3QQIKHNRTpk6bDgpZjRkzg3pBQTBrdtCcuZCgluAD0vPmL1gIdvSixUuWgqNs2YJ+DUhkEYxuggkGmOQUcckrioPTJCOXEnZ5JS5YslbGnuyVERlDDFvGEUPOWvwqaH6RVkHKeuDMK6SKnHlVhTgx8jeTmqy6Eij7K6nLqiGyPwChsa1MUrnq1wAAACV0RVh0ZGF0ZTpjcmVhdGUAMjAyMy0xMC0wNFQwMDozODo0OSswMDowMB9V0a8AAAAldEVYdGRhdGU6bW9kaWZ5ADIwMjMtMTAtMDRUMDA6Mzg6NDkrMDA6MDBuCGkTAAAAKHRFWHRkYXRlOnRpbWVzdGFtcAAyMDIzLTEwLTA0VDAwOjM4OjQ5KzAwOjAwOR1IzAAAAABJRU5ErkJggg==">
  </a> |
  <a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/LICENSE">
      <img src="https://camo.githubusercontent.com/b4357f651d9e266e4b3854471ec55091f13a9f067f8b0144037ef6ab46239a66/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d6c6963656e7365266d6573736167653d4d495426636f6c6f723d7768697465" alt="License" data-canonical-src="https://img.shields.io/static/v1?label=license&amp;message=MIT&amp;color=white">
  </a> |
  <a href="https://docs.anythingllm.com/" rel="nofollow">
    Docs
  </a> |
   <a href="https://my.mintplexlabs.com/aio-checkout?product=anythingllm" rel="nofollow">
    Hosted Instance
  </a>
</p>
<p dir="auto">
  <b>English</b> · <a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/locales/README.zh-CN.md">简体中文</a> · <a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/locales/README.ja-JP.md">日本語</a>
</p>
<p dir="auto">
👉 AnythingLLM for desktop (Mac, Windows, &amp; Linux)! <a href="https://anythingllm.com/download" rel="nofollow"> Download Now</a>
</p>
<p dir="auto">A full-stack application that enables you to turn any document, resource, or piece of content into context that any LLM can use as references during chatting. This application allows you to pick and choose which LLM or Vector Database you want to use as well as supporting multi-user management and permissions.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/16845892/294273127-cfc5f47c-bd91-4067-986c-f3f49621a859.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1NjQ5MDIsIm5iZiI6MTcyNTU2NDYwMiwicGF0aCI6Ii8xNjg0NTg5Mi8yOTQyNzMxMjctY2ZjNWY0N2MtYmQ5MS00MDY3LTk4NmMtZjNmNDk2MjFhODU5LmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTA1VDE5MzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTYyOTRlZDk0MGM3NThmMTFjMGQwZjY3NjFkYzQyZTQzZjY1ZjM5ODY2ZmRjMGRkMzk2ZTNlNjIwOTU5MTg4MjkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.702OhOJH1eZoGn3wHkT3cpc8NgWnuhjAtLNyV5-8ztA"><img src="https://private-user-images.githubusercontent.com/16845892/294273127-cfc5f47c-bd91-4067-986c-f3f49621a859.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1NjQ5MDIsIm5iZiI6MTcyNTU2NDYwMiwicGF0aCI6Ii8xNjg0NTg5Mi8yOTQyNzMxMjctY2ZjNWY0N2MtYmQ5MS00MDY3LTk4NmMtZjNmNDk2MjFhODU5LmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTA1VDE5MzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTYyOTRlZDk0MGM3NThmMTFjMGQwZjY3NjFkYzQyZTQzZjY1ZjM5ODY2ZmRjMGRkMzk2ZTNlNjIwOTU5MTg4MjkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.702OhOJH1eZoGn3wHkT3cpc8NgWnuhjAtLNyV5-8ztA" alt="Chatting" data-animated-image=""></a></p>
<details>
<summary><kbd>Watch the demo!</kbd></summary>
<p dir="auto"><a href="https://youtu.be/f95rGD9trL0" rel="nofollow"><img src="https://github.com/Mintplex-Labs/anything-llm/raw/master/images/youtube.png" alt="Watch the video"></a></p>
</details>
<p dir="auto"><h3 tabindex="-1" dir="auto">Product Overview</h3><a id="user-content-product-overview" aria-label="Permalink: Product Overview" href="#product-overview"></a></p>
<p dir="auto">AnythingLLM is a full-stack application where you can use commercial off-the-shelf LLMs or popular open source LLMs and vectorDB solutions to build a private ChatGPT with no compromises that you can run locally as well as host remotely and be able to chat intelligently with any documents you provide it.</p>
<p dir="auto">AnythingLLM divides your documents into objects called <code>workspaces</code>. A Workspace functions a lot like a thread, but with the addition of containerization of your documents. Workspaces can share documents, but they do not talk to each other so you can keep your context for each workspace clean.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Cool features of AnythingLLM</h2><a id="user-content-cool-features-of-anythingllm" aria-label="Permalink: Cool features of AnythingLLM" href="#cool-features-of-anythingllm"></a></p>
<ul dir="auto">
<li>🆕 <strong>Multi-modal support (both closed and open-source LLMs!)</strong></li>
<li>👤 Multi-user instance support and permissioning <em>Docker version only</em></li>
<li>🦾 Agents inside your workspace (browse the web, run code, etc)</li>
<li>💬 <a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/embed/README.md">Custom Embeddable Chat widget for your website</a> <em>Docker version only</em></li>
<li>📖 Multiple document type support (PDF, TXT, DOCX, etc)</li>
<li>Simple chat UI with Drag-n-Drop funcitonality and clear citations.</li>
<li>100% Cloud deployment ready.</li>
<li>Works with all popular <a href="#supported-llms-embedder-models-speech-models-and-vector-databases">closed and open-source LLM providers</a>.</li>
<li>Built-in cost &amp; time-saving measures for managing very large documents compared to any other chat UI.</li>
<li>Full Developer API for custom integrations!</li>
<li>Much more...install and find out!</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Supported LLMs, Embedder Models, Speech models, and Vector Databases</h3><a id="user-content-supported-llms-embedder-models-speech-models-and-vector-databases" aria-label="Permalink: Supported LLMs, Embedder Models, Speech models, and Vector Databases" href="#supported-llms-embedder-models-speech-models-and-vector-databases"></a></p>
<p dir="auto"><strong>Large Language Models (LLMs):</strong></p>
<ul dir="auto">
<li><a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/server/storage/models/README.md#text-generation-llm-selection">Any open-source llama.cpp compatible model</a></li>
<li><a href="https://openai.com/" rel="nofollow">OpenAI</a></li>
<li><a href="https://openai.com/" rel="nofollow">OpenAI (Generic)</a></li>
<li><a href="https://azure.microsoft.com/en-us/products/ai-services/openai-service" rel="nofollow">Azure OpenAI</a></li>
<li><a href="https://aws.amazon.com/bedrock/" rel="nofollow">AWS Bedrock</a></li>
<li><a href="https://www.anthropic.com/" rel="nofollow">Anthropic</a></li>
<li><a href="https://ai.google.dev/" rel="nofollow">Google Gemini Pro</a></li>
<li><a href="https://huggingface.co/" rel="nofollow">Hugging Face (chat models)</a></li>
<li><a href="https://ollama.ai/" rel="nofollow">Ollama (chat models)</a></li>
<li><a href="https://lmstudio.ai/" rel="nofollow">LM Studio (all models)</a></li>
<li><a href="https://localai.io/" rel="nofollow">LocalAi (all models)</a></li>
<li><a href="https://www.together.ai/" rel="nofollow">Together AI (chat models)</a></li>
<li><a href="https://www.perplexity.ai/" rel="nofollow">Perplexity (chat models)</a></li>
<li><a href="https://openrouter.ai/" rel="nofollow">OpenRouter (chat models)</a></li>
<li><a href="https://mistral.ai/" rel="nofollow">Mistral</a></li>
<li><a href="https://groq.com/" rel="nofollow">Groq</a></li>
<li><a href="https://cohere.com/" rel="nofollow">Cohere</a></li>
<li><a href="https://github.com/LostRuins/koboldcpp">KoboldCPP</a></li>
<li><a href="https://github.com/BerriAI/litellm">LiteLLM</a></li>
<li><a href="https://github.com/oobabooga/text-generation-webui">Text Generation Web UI</a></li>
</ul>
<p dir="auto"><strong>Embedder models:</strong></p>
<ul dir="auto">
<li><a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/server/storage/models/README.md">AnythingLLM Native Embedder</a> (default)</li>
<li><a href="https://openai.com/" rel="nofollow">OpenAI</a></li>
<li><a href="https://azure.microsoft.com/en-us/products/ai-services/openai-service" rel="nofollow">Azure OpenAI</a></li>
<li><a href="https://localai.io/" rel="nofollow">LocalAi (all)</a></li>
<li><a href="https://ollama.ai/" rel="nofollow">Ollama (all)</a></li>
<li><a href="https://lmstudio.ai/" rel="nofollow">LM Studio (all)</a></li>
<li><a href="https://cohere.com/" rel="nofollow">Cohere</a></li>
</ul>
<p dir="auto"><strong>Audio Transcription models:</strong></p>
<ul dir="auto">
<li><a href="https://github.com/Mintplex-Labs/anything-llm/tree/master/server/storage/models#audiovideo-transcription">AnythingLLM Built-in</a> (default)</li>
<li><a href="https://openai.com/" rel="nofollow">OpenAI</a></li>
</ul>
<p dir="auto"><strong>TTS (text-to-speech) support:</strong></p>
<ul dir="auto">
<li>Native Browser Built-in (default)</li>
<li><a href="https://github.com/rhasspy/piper">PiperTTSLocal - runs in browser</a></li>
<li><a href="https://platform.openai.com/docs/guides/text-to-speech/voice-options" rel="nofollow">OpenAI TTS</a></li>
<li><a href="https://elevenlabs.io/" rel="nofollow">ElevenLabs</a></li>
</ul>
<p dir="auto"><strong>STT (speech-to-text) support:</strong></p>
<ul dir="auto">
<li>Native Browser Built-in (default)</li>
</ul>
<p dir="auto"><strong>Vector Databases:</strong></p>
<ul dir="auto">
<li><a href="https://github.com/lancedb/lancedb">LanceDB</a> (default)</li>
<li><a href="https://www.datastax.com/products/datastax-astra" rel="nofollow">Astra DB</a></li>
<li><a href="https://pinecone.io/" rel="nofollow">Pinecone</a></li>
<li><a href="https://trychroma.com/" rel="nofollow">Chroma</a></li>
<li><a href="https://weaviate.io/" rel="nofollow">Weaviate</a></li>
<li><a href="https://qdrant.tech/" rel="nofollow">Qdrant</a></li>
<li><a href="https://milvus.io/" rel="nofollow">Milvus</a></li>
<li><a href="https://zilliz.com/" rel="nofollow">Zilliz</a></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Technical Overview</h3><a id="user-content-technical-overview" aria-label="Permalink: Technical Overview" href="#technical-overview"></a></p>
<p dir="auto">This monorepo consists of three main sections:</p>
<ul dir="auto">
<li><code>frontend</code>: A viteJS + React frontend that you can run to easily create and manage all your content the LLM can use.</li>
<li><code>server</code>: A NodeJS express server to handle all the interactions and do all the vectorDB management and LLM interactions.</li>
<li><code>collector</code>: NodeJS express server that process and parses documents from the UI.</li>
<li><code>docker</code>: Docker instructions and build process + information for building from source.</li>
<li><code>embed</code>: Submodule for generation &amp; creation of the <a href="https://github.com/Mintplex-Labs/anythingllm-embed">web embed widget</a>.</li>
<li><code>browser-extension</code>: Submodule for the <a href="https://github.com/Mintplex-Labs/anythingllm-extension">chrome browser extension</a>.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🛳 Self Hosting</h2><a id="user-content--self-hosting" aria-label="Permalink: 🛳 Self Hosting" href="#-self-hosting"></a></p>
<p dir="auto">Mintplex Labs &amp; the community maintain a number of deployment methods, scripts, and templates that you can use to run AnythingLLM locally. Refer to the table below to read how to deploy on your preferred environment or to automatically deploy.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Docker</th>
<th>AWS</th>
<th>GCP</th>
<th>Digital Ocean</th>
<th>Render.com</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/docker/HOW_TO_USE_DOCKER.md"><img src="https://github.com/Mintplex-Labs/anything-llm/raw/master/images/deployBtns/docker.png" alt="Deploy on Docker"></a></td>
<td><a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/cloud-deployments/aws/cloudformation/DEPLOY.md"><img src="https://github.com/Mintplex-Labs/anything-llm/raw/master/images/deployBtns/aws.png" alt="Deploy on AWS"></a></td>
<td><a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/cloud-deployments/gcp/deployment/DEPLOY.md"><img src="https://camo.githubusercontent.com/8e9acf4df0af19c7eb27d48d2bd9966f7311d8ae1fe4900f504b8d4eabb8d769/68747470733a2f2f6465706c6f792e636c6f75642e72756e2f627574746f6e2e737667" alt="Deploy on GCP" data-canonical-src="https://deploy.cloud.run/button.svg"></a></td>
<td><a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/cloud-deployments/digitalocean/terraform/DEPLOY.md"><img src="https://camo.githubusercontent.com/e093a0ed531124a715aad44362848ca2cff28c3182c2c0ca4a70b2564b681f59/68747470733a2f2f7777772e6465706c6f79746f646f2e636f6d2f646f2d62746e2d626c75652e737667" alt="Deploy on DigitalOcean" data-canonical-src="https://www.deploytodo.com/do-btn-blue.svg"></a></td>
<td><a href="https://render.com/deploy?repo=https://github.com/Mintplex-Labs/anything-llm&amp;branch=render" rel="nofollow"><img src="https://camo.githubusercontent.com/a103822afe1d58c7da6beafbc0c65bb7b8d622dd193dded1b45b3c0ad6466d82/68747470733a2f2f72656e6465722e636f6d2f696d616765732f6465706c6f792d746f2d72656e6465722d627574746f6e2e737667" alt="Deploy on Render.com" data-canonical-src="https://render.com/images/deploy-to-render-button.svg"></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Railway</th>
<th>RepoCloud</th>
<th>Elestio</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://railway.app/template/HNSCS1?referralCode=WFgJkn" rel="nofollow"><img src="https://camo.githubusercontent.com/e4002051668809c220b10ad92ddd6fb87f365d8cd4ff470e0aeca3bc5b05450e/68747470733a2f2f7261696c7761792e6170702f627574746f6e2e737667" alt="Deploy on Railway" data-canonical-src="https://railway.app/button.svg"></a></td>
<td><a href="https://repocloud.io/details/?app_id=276" rel="nofollow"><img src="https://camo.githubusercontent.com/ac294f4b769f6436dadb17205434b234b32e4d88831f182c522fd36b4534a7a3/68747470733a2f2f64313674307063343834367835322e636c6f756466726f6e742e6e65742f6465706c6f796c6f62652e737667" alt="Deploy on RepoCloud" data-canonical-src="https://d16t0pc4846x52.cloudfront.net/deploylobe.svg"></a></td>
<td><a href="https://elest.io/open-source/anythingllm" rel="nofollow"><img src="https://camo.githubusercontent.com/76131e33a65d9a0728265791dcf78030580a9932466d139e5ae38f87f926c5b5/68747470733a2f2f656c6573742e696f2f696d616765732f6c6f676f732f6465706c6f792d746f2d656c657374696f2d62746e2e706e67" alt="Deploy on Elestio" data-canonical-src="https://elest.io/images/logos/deploy-to-elestio-btn.png"></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/BARE_METAL.md">or set up a production AnythingLLM instance without Docker →</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How to setup for development</h2><a id="user-content-how-to-setup-for-development" aria-label="Permalink: How to setup for development" href="#how-to-setup-for-development"></a></p>
<ul dir="auto">
<li><code>yarn setup</code> To fill in the required <code>.env</code> files you'll need in each of the application sections (from root of repo).
<ul dir="auto">
<li>Go fill those out before proceeding. Ensure <code>server/.env.development</code> is filled or else things won't work right.</li>
</ul>
</li>
<li><code>yarn dev:server</code> To boot the server locally (from root of repo).</li>
<li><code>yarn dev:frontend</code> To boot the frontend locally (from root of repo).</li>
<li><code>yarn dev:collector</code> To then run the document collector (from root of repo).</li>
</ul>
<p dir="auto"><a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/server/storage/documents/DOCUMENTS.md">Learn about documents</a></p>
<p dir="auto"><a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/server/storage/vector-cache/VECTOR_CACHE.md">Learn about vector caching</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Telemetry &amp; Privacy</h2><a id="user-content-telemetry--privacy" aria-label="Permalink: Telemetry &amp; Privacy" href="#telemetry--privacy"></a></p>
<p dir="auto">AnythingLLM by Mintplex Labs Inc contains a telemetry feature that collects anonymous usage information.</p>
<details>
<summary><kbd>More about Telemetry &amp; Privacy for AnythingLLM</kbd></summary>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why?</h3><a id="user-content-why" aria-label="Permalink: Why?" href="#why"></a></p>
<p dir="auto">We use this information to help us understand how AnythingLLM is used, to help us prioritize work on new features and bug fixes, and to help us improve AnythingLLM's performance and stability.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Opting out</h3><a id="user-content-opting-out" aria-label="Permalink: Opting out" href="#opting-out"></a></p>
<p dir="auto">Set <code>DISABLE_TELEMETRY</code> in your server or docker .env settings to "true" to opt out of telemetry. You can also do this in-app by going to the sidebar &gt; <code>Privacy</code> and disabling telemetry.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">What do you explicitly track?</h3><a id="user-content-what-do-you-explicitly-track" aria-label="Permalink: What do you explicitly track?" href="#what-do-you-explicitly-track"></a></p>
<p dir="auto">We will only track usage details that help us make product and roadmap decisions, specifically:</p>
<ul dir="auto">
<li>Typ of your installation (Docker or Desktop)</li>
<li>When a document is added or removed. No information <em>about</em> the document. Just that the event occurred. This gives us an idea of use.</li>
<li>Type of vector database in use. Let's us know which vector database provider is the most used to prioritize changes when updates arrive for that provider.</li>
<li>Type of LLM in use. Let's us know the most popular choice and prioritize changes when updates arrive for that provider.</li>
<li>Chat is sent. This is the most regular "event" and gives us an idea of the daily-activity of this project across all installations. Again, only the event is sent - we have no information on the nature or content of the chat itself.</li>
</ul>
<p dir="auto">You can verify these claims by finding all locations <code>Telemetry.sendTelemetry</code> is called. Additionally these events are written to the output log so you can also see the specific data which was sent - if enabled. No IP or other identifying information is collected. The Telemetry provider is <a href="https://posthog.com/" rel="nofollow">PostHog</a> - an open-source telemetry collection service.</p>
<p dir="auto"><a href="https://github.com/search?q=repo%3AMintplex-Labs%2Fanything-llm%20.sendTelemetry(&amp;type=code">View all telemetry events in source code</a></p>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">👋 Contributing</h2><a id="user-content--contributing" aria-label="Permalink: 👋 Contributing" href="#-contributing"></a></p>
<ul dir="auto">
<li>create issue</li>
<li>create PR with branch name format of <code>&lt;issue number&gt;-&lt;short name&gt;</code></li>
<li>LGTM from core-team</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🌟 Contributors</h2><a id="user-content--contributors" aria-label="Permalink: 🌟 Contributors" href="#-contributors"></a></p>
<p dir="auto"><a href="https://github.com/mintplex-labs/anything-llm/graphs/contributors"><img src="https://camo.githubusercontent.com/216243cb397375babf4a9b21f6a6968b7589f578ff1daa0db6ae56a33ca4997a/68747470733a2f2f636f6e747269622e726f636b732f696d6167653f7265706f3d6d696e74706c65782d6c6162732f616e797468696e672d6c6c6d" alt="anythingllm contributors" data-canonical-src="https://contrib.rocks/image?repo=mintplex-labs/anything-llm"></a></p>
<p dir="auto"><a href="https://star-history.com/#mintplex-labs/anything-llm&amp;Date" rel="nofollow"><img src="https://camo.githubusercontent.com/455256132c080bc5bbf5419d1c14fa4d1a5727d75c41c4df0bc8e8fe7dbccb63/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d6d696e74706c65782d6c6162732f616e797468696e672d6c6c6d26747970653d54696d656c696e65" alt="Star History Chart" data-canonical-src="https://api.star-history.com/svg?repos=mintplex-labs/anything-llm&amp;type=Timeline"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔗 More Products</h2><a id="user-content--more-products" aria-label="Permalink: 🔗 More Products" href="#-more-products"></a></p>
<ul dir="auto">
<li><strong><a href="https://github.com/mintplex-labs/vector-admin">VectorAdmin</a>:</strong> An all-in-one GUI &amp; tool-suite for managing vector databases.</li>
<li><strong><a href="https://github.com/Mintplex-Labs/openai-assistant-swarm">OpenAI Assistant Swarm</a>:</strong> Turn your entire library of OpenAI assistants into one single army commanded from a single agent.</li>
</ul>
<p dir="auto"><a href="#readme-top"><img src="https://camo.githubusercontent.com/d658b6c3935e61bd4aab9a571190fc3c48cbafc89fe6b16250c0cba28ed73234/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d4241434b5f544f5f544f502d3232323632383f7374796c653d666c61742d737175617265" alt="" data-canonical-src="https://img.shields.io/badge/-BACK_TO_TOP-222628?style=flat-square"></a></p>
<hr>
<p dir="auto">Copyright © 2024 <a href="https://github.com/mintplex-labs">Mintplex Labs</a>. <br>
This project is <a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/LICENSE">MIT</a> licensed.</p>

</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AlphaProteo generates novel proteins for biology and health research (225 pts)]]></title>
            <link>https://deepmind.google/discover/blog/alphaproteo-generates-novel-proteins-for-biology-and-health-research/</link>
            <guid>41457331</guid>
            <pubDate>Thu, 05 Sep 2024 15:05:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deepmind.google/discover/blog/alphaproteo-generates-novel-proteins-for-biology-and-health-research/">https://deepmind.google/discover/blog/alphaproteo-generates-novel-proteins-for-biology-and-health-research/</a>, See on <a href="https://news.ycombinator.com/item?id=41457331">Hacker News</a></p>
Couldn't get https://deepmind.google/discover/blog/alphaproteo-generates-novel-proteins-for-biology-and-health-research/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Launch HN: Maitai (YC S24) – Self-Optimizing LLM Platform (112 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=41456552</link>
            <guid>41456552</guid>
            <pubDate>Thu, 05 Sep 2024 13:42:43 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=41456552">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="41456552">
      <td><span></span></td>      <td><center><a id="up_41456552" href="https://news.ycombinator.com/vote?id=41456552&amp;how=up&amp;goto=item%3Fid%3D41456552"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=41456552">Launch HN: Maitai (YC S24) – Self-Optimizing LLM Platform</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_41456552">104 points</span> by <a href="https://news.ycombinator.com/user?id=cmdalsanto">cmdalsanto</a> <span title="2024-09-05T13:42:43.000000Z"><a href="https://news.ycombinator.com/item?id=41456552">8 hours ago</a></span> <span id="unv_41456552"></span> | <a href="https://news.ycombinator.com/hide?id=41456552&amp;goto=item%3Fid%3D41456552">hide</a> | <a href="https://hn.algolia.com/?query=Launch%20HN%3A%20Maitai%20%28YC%20S24%29%20%E2%80%93%20Self-Optimizing%20LLM%20Platform&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=41456552&amp;auth=5b3d028aa07cccfc7151d9d6f9e80c752e36b1c3">favorite</a> | <a href="https://news.ycombinator.com/item?id=41456552">50&nbsp;comments</a>        </span>
              </td></tr>
    <tr><td></td></tr><tr><td colspan="2"></td><td><div><p>Hey HN - this is Christian and Ian from Maitai (<a href="https://trymaitai.ai/">https://trymaitai.ai</a>). We're building an LLM platform that optimizes request routing, autocorrects bad responses, and automatically fine-tunes new application-specific models with incremental improvements. Here’s a demo video:  <a href="https://www.loom.com/share/a2cd9192359840cab5274ccba399bd87?sid=7097fd84-ea85-42cd-9616-84abc1087a56" rel="nofollow">https://www.loom.com/share/a2cd9192359840cab5274ccba399bd87?...</a>.</p><p>If you want to try it out, we built a game (<a href="https://maitaistreasure.com/" rel="nofollow">https://maitaistreasure.com</a>) to show how our real-time autocorrections work with mission-critical expectations (like never giving financial advice). Try and coax the bot to give you the secret phrase in its system prompt. If you're the first to crack it, you can email us the phrase and win a bounty. Maitai is used to make sure the bot always adheres to our expectations, and thus never gives up the secret phrase.</p><p>We built Maitai because getting an LLM app into production and maintaining it is a slog. Teams spend most of their time on LLM reliability rather than their main product. We experienced this ourselves at our previous jobs deploying AI-enabled applications for Presto—the vast majority of time was making sure the model did what we wanted it to do.</p><p>For example, one of our customers builds AI ordering agents for restaurants. It's crucial that their LLMs return results in a predictable, consistent manner throughout the conversation. If not, it leads to a poor guest experience and a staff member may intervene. At the end of the order conversation, they need to ensure that the order cart matches what the customer requested before it's submitted to the Point of Sale system. It's common for a human-in-the-loop to review critical pieces of information like this, but it’s costly to set up such a pipeline and it’s difficult to scale. When it's time to send out a receipt and payment link, they must first get the customer's consent to receive text messages, else they risk fines for violating the Telephone Consumer Protection Act. To boot, getting from 0 to 1 usually relies on inefficient general-purpose models that aren't viable at any sort of scale beyond proof of concept.</p><p>Since reliability is the #1 thing hindering the adoption of LLMs in production, we decided to help change that. Here's how it works:</p><p>1. Maitai sits between the client and the LLMs as a super lightweight proxy, analyzing traffic to automatically build a robust set of expectations for how the LLM should respond.</p><p>2. The application sends a request to Maitai, and Maitai forwards it to the appropriate LLM (user specified, but we'll preemptively fallback to a similar model if we notice issues with the primary model).</p><p>3. We intercept the response from the LLM, and evaluate it against the expectations we had previously built.</p><p>4. If we notice that an expectation was not met, we surface a fault (Slack, webhook) and can, optionally, substitute the faulty response with a clean response to be sent back to the client. This check and correction adds about 250ms on average right now, and we're working on making it faster.</p><p>5. We use all of the data from evaluating model responses to fine-tune application-specific models. We're working on automating this step for passive incremental improvements. We'd like to get it to a point where our user's inference step just gets better, faster, and cheaper over time without them having to do anything.</p><p>Our hope is that we take on the reliability and resiliency problems of the LLMs for our customers, and make it so they can focus on domain specific problems instead.</p><p>We're self-serve (<a href="https://portal.trymaitai.ai/">https://portal.trymaitai.ai</a>), and have both Python and Node SDKs that mock OpenAI's for quick integration. Users can set their preferences for primary and secondary (fallback) models in our Portal, or in code. Right now, the expectations we use for real-time evaluations are automatically generated, but we manually go through and do some pruning before enabling them. Fine-tuning is all done manually for now.</p><p>We charge for platform usage, plus a monthly application fee. Customers can bring their own LLM provider API keys, or use ours and pay at-cost for what they use. We have contracts with most of our current customers, so we are still trying to figure out what's right for our pay-as-you-go plan.</p><p>We securely store requests and responses that go through Maitai, as well as derivative data such as evaluation results. This information is used for fine-tuning models, accessible only by the organization the data belongs to. Data is never shared between our users.  API keys we manage on behalf of our customers are only injected before sending to the LLM provider, and never leave our servers otherwise. We're working on SOC2 and HIPAA compliance, as well as a self-hosted solution for companies with extremely sensitive data privacy requirements.</p><p>We’d love to get your feedback on what we’re building, or hear about your experience building around LLMs!</p></div></td></tr>        <tr><td></td></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Hacker League – Open-Source Rocket League on Linux (172 pts)]]></title>
            <link>https://github.com/moritztng/hacker-league</link>
            <guid>41456411</guid>
            <pubDate>Thu, 05 Sep 2024 13:24:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/moritztng/hacker-league">https://github.com/moritztng/hacker-league</a>, See on <a href="https://news.ycombinator.com/item?id=41456411">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><details open="">
  <summary>
    
    <span aria-label="Video description hacker-league.mp4">hacker-league.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/19519902/364779763-3a630d46-ec17-4da8-8879-76320ea563fe.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1NTA1MDEsIm5iZiI6MTcyNTU1MDIwMSwicGF0aCI6Ii8xOTUxOTkwMi8zNjQ3Nzk3NjMtM2E2MzBkNDYtZWMxNy00ZGE4LTg4NzktNzYzMjBlYTU2M2ZlLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTA1VDE1MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTNkMDg3MjdiZmFkMTZiODk4YTJiYzBkYzc4MTk4ZGNmOTJjZjYxMjY3NzI1MGMzOTFjNTg1Y2YxNWVkOWE1OWYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.C7QQg92wm3q0awLIIoj6I8rQuGAJsOeWGMyqE9soFcQ" data-canonical-src="https://private-user-images.githubusercontent.com/19519902/364779763-3a630d46-ec17-4da8-8879-76320ea563fe.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1NTA1MDEsIm5iZiI6MTcyNTU1MDIwMSwicGF0aCI6Ii8xOTUxOTkwMi8zNjQ3Nzk3NjMtM2E2MzBkNDYtZWMxNy00ZGE4LTg4NzktNzYzMjBlYTU2M2ZlLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTA1VDE1MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTNkMDg3MjdiZmFkMTZiODk4YTJiYzBkYzc4MTk4ZGNmOTJjZjYxMjY3NzI1MGMzOTFjNTg1Y2YxNWVkOWE1OWYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.C7QQg92wm3q0awLIIoj6I8rQuGAJsOeWGMyqE9soFcQ" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><h2 tabindex="-1" dir="auto">Install</h2><a id="user-content-install" aria-label="Permalink: Install" href="#install"></a></p>
<p dir="auto">Currently only debian based distros with x86_64. Please help me build it on other platforms. If you have an external GPU, make sure the drivers are installed</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo apt install curl &amp;&amp; curl -sL https://raw.githubusercontent.com/moritztng/hacker-league/main/install.sh | bash"><pre>sudo apt install curl <span>&amp;&amp;</span> curl -sL https://raw.githubusercontent.com/moritztng/hacker-league/main/install.sh <span>|</span> bash</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Play</h2><a id="user-content-play" aria-label="Permalink: Play" href="#play"></a></p>
<p dir="auto">Use a gamepad for maximum fun</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd hacker-league
./hacker-league"><pre><span>cd</span> hacker-league
./hacker-league</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Build from source</h2><a id="user-content-build-from-source" aria-label="Permalink: Build from source" href="#build-from-source"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/moritztng/hacker-league.git
cd hacker-league
sudo apt install libvulkan-dev vulkan-validationlayers-dev spirv-tools libglfw3-dev libglm-dev libeigen3-dev vim-common xxd g++ make
curl -L -o ./shaders/glslc https://github.com/moritztng/hacker-league/releases/download/glslc/glslc
chmod +x ./shaders/glslc
make debug
curl -L -o &quot;gamepad.txt&quot; https://raw.githubusercontent.com/mdqinc/SDL_GameControllerDB/master/gamecontrollerdb.txt"><pre>git clone https://github.com/moritztng/hacker-league.git
<span>cd</span> hacker-league
sudo apt install libvulkan-dev vulkan-validationlayers-dev spirv-tools libglfw3-dev libglm-dev libeigen3-dev vim-common xxd g++ make
curl -L -o ./shaders/glslc https://github.com/moritztng/hacker-league/releases/download/glslc/glslc
chmod +x ./shaders/glslc
make debug
curl -L -o <span><span>"</span>gamepad.txt<span>"</span></span> https://raw.githubusercontent.com/mdqinc/SDL_GameControllerDB/master/gamecontrollerdb.txt</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Community</h2><a id="user-content-community" aria-label="Permalink: Community" href="#community"></a></p>
<ul dir="auto">
<li>Discord Server: <a href="https://discord.gg/BbNH27st" rel="nofollow">https://discord.gg/BbNH27st</a></li>
<li>I build in public on X: <a href="https://x.com/moritzthuening" rel="nofollow">https://x.com/moritzthuening</a></li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Porting systemd to musl Libc-powered Linux (178 pts)]]></title>
            <link>https://catfox.life/2024/09/05/porting-systemd-to-musl-libc-powered-linux/</link>
            <guid>41454779</guid>
            <pubDate>Thu, 05 Sep 2024 08:44:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://catfox.life/2024/09/05/porting-systemd-to-musl-libc-powered-linux/">https://catfox.life/2024/09/05/porting-systemd-to-musl-libc-powered-linux/</a>, See on <a href="https://news.ycombinator.com/item?id=41454779">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-449">
	<!-- .entry-header -->

	
	
	<div>
		
<p>I have completed an <a href="https://code.atwilcox.tech/sphen/scaly/systemd/-/commits/adelie-v256">initial new port</a> of systemd to musl.  This patch set does not share much in common with the existing OpenEmbedded patchset.  I wanted to make a fully updated patch series targeting more current releases of systemd and musl, taking advantage of the latest features and updates in both.  I also took a focus on writing patches that could be sent for consideration of inclusion upstream.</p>



<p>The final result is a system that appears to be surprisingly reliable considering the newness of the port, and very fast to boot.</p>



<h2>Why?</h2>



<p>I have wanted to do this work for almost a decade.  In fact, a mention of multiple service manager options – including systemd – is present on the <a href="https://web.archive.org/web/20160109133511/http://adelielinux.org/">original Adélie Web site from 2015</a>.  Other initiatives have always taken priority, until someone contacted us at <a href="https://www.wilcoxti.com/">Wilcox Technologies Inc. (WTI)</a> interested in paying on a contract basis to see this effort completed.</p>



<p>I want to be clear that I did not do this for money.  I believe strongly that there is genuine value in having multiple service managers available.  User freedom and user choice matter.  There are cases where this support would have been useful to me and to many others in the community.  I am excited to see this work nearing public release and honoured to be a part of creating more choice in the Linux world.</p>



<h2>How?</h2>



<p>I started with the latest release tag, v256.5.  I wanted a version closely aligned to upstream’s current progress, yet not too far away from the present “stable” 255 release.  I also wanted to make sure that the fallout from upstream’s removal of split-/usr support would be felt to its maximum, since reverting that decision is a high priority.</p>



<p>I fixed build errors as they happened until I finally had a built systemd.  During this phase, I consulted the original OE patchset twice: once for usage of <code>GLOB_BRACE</code>, and the other for usage of <code>malloc_info</code> and <code>malloc_trim</code>.  Otherwise, the patchset was authored entirely originally, mostly through the day (and into the night) of August 16th, 2024.</p>



<p>Many of the issues seen were related to inclusion of headers, and I am already working on <a href="https://github.com/systemd/systemd/pull/34064">bringing</a> those fixes <a href="https://github.com/systemd/systemd/pull/34066">upstream</a>.  It was then time to run the test suite.</p>



<h2>Tests!</h2>



<p>The test suite started with 27 failures.  Most of them were simple fixes, but one that gave me a lot of trouble was the <code>time-util</code> test.  The <a href="https://git.musl-libc.org/cgit/musl/tree/src/time/strptime.c"><code>strptime</code> implementation in musl</a> does not support the <code>%z</code> format specifier (for time zones), which the systemd test relies on.  I could have disabled those tests, but I felt like this would be taking away a lot of functionality.  I considered things like important journals from other systems – they would likely have timestamps with <code>%z</code> formats.  I wrote a <code>%z</code> translation for systemd and saw the tests passing.</p>



<p>Other test failures were simple <a href="https://github.com/systemd/systemd/pull/34065">C portability fixes</a>, which are also in the process of being sent upstream.</p>



<p>The test suite for <code>systemd-sysusers</code> was the next sticky one.  It really exercises the POSIX library functions <code>getgrent</code> and <code>getpwent</code>.  The musl implementations of these are fine, but they don’t cope well with the old NIS compatibility shims from the glibc world.  They also <a href="https://www.openwall.com/lists/musl/2021/10/11/1">can’t handle “incomplete” lines</a>.  The fix for incomplete line handling is pending, so in the meantime I made the test have no incomplete lines.  I added a shim for the NIS compatibility entries in systemd’s <code>putgrent_sane</code> function, making it a little less “sane” but fixing the support perfectly.</p>



<p>Then it was time for the final failing test: <code>test-recurse-dir</code>, which was receiving an <code>EFAULT</code> error code from <code>getdents64</code>.  Discussing this with my friends on the Gentoo IRC, we began to wonder if this was an architecture-specific bug.  I was doing my port work on my Talos II, a 64-bit PowerPC system.  I copied the code over to an Intel Skylake and found the test suite passed.  That was both good, in that the tests were all passing, but also bad, because it meant I was dealing with a PPC64-specific bug.  I wasn’t sure if this was a kernel bug, a musl bug, or a systemd bug.</p>



<p>Digging into it further, I realised that the pointer math being done would be invalid when cast to a pointer-to-structure on PPC64 due to object alignment guarantees in the ABI.  I changed it to use a temporary variable for the pointer math and casting that temporary, and it passed!</p>



<p>And that is how I became the first person alive to see systemd passing its entire test suite on a big-endian 64-bit PowerPC musl libc system.</p>



<h2>The moment of truth</h2>



<p>I created a small disk image and ran a very strange command: <code>apk add adelie-base-posix dash-binsh systemd</code>.  I booted it up as a KVM VM in Qemu and saw “Welcome to Adélie Linux 1.0 Beta 5” before a rather ungraceful – and due to Qemu framebuffer endian issues, colour-swapped – segmentation fault:</p>



<figure><img data-attachment-id="453" data-permalink="https://catfox.life/2024/09/05/porting-systemd-to-musl-libc-powered-linux/the-dawn-of-a-new-error/" data-orig-file="https://catfox.life/wp-content/uploads/2024/09/the-dawn-of-a-new-error.png" data-orig-size="1736,1392" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="the-dawn-of-a-new-error" data-image-description="" data-image-caption="" data-medium-file="https://catfox.life/wp-content/uploads/2024/09/the-dawn-of-a-new-error.png?w=300" data-large-file="https://catfox.life/wp-content/uploads/2024/09/the-dawn-of-a-new-error.png?w=840" tabindex="0" role="button" width="1024" height="821" src="https://catfox.life/wp-content/uploads/2024/09/the-dawn-of-a-new-error.png?w=1024" alt=""><figcaption>Welcome to an endian-swapped systemd core dump!</figcaption></figure>



<p>Debugging this was an experience in early systems debugging that I haven’t had in years.  There’s a great summary on this methodology at <a href="https://linus.schreibt.jetzt/posts/debugging-pid1.html">Linus’s blog</a>.</p>



<p>It turned out that I had disabled a test from build-util as I incorrectly assumed that was only used when debugging in the build root.  Since I did not want to spend time digging around how it manually parses ELF files to find their RPATH entries for a feature we are unlikely to use, I stubbed that functionality out entirely.  We can always fix it later.</p>



<p>Recreating the disk image and booting it up, I was greeted by an Adélie “rescue” environment booted by systemd.  It was frankly bizarre, but also really cool.</p>



<figure><img data-attachment-id="454" data-permalink="https://catfox.life/2024/09/05/porting-systemd-to-musl-libc-powered-linux/habbening/" data-orig-file="https://catfox.life/wp-content/uploads/2024/09/habbening.png" data-orig-size="2064,1470" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="habbening" data-image-description="" data-image-caption="" data-medium-file="https://catfox.life/wp-content/uploads/2024/09/habbening.png?w=300" data-large-file="https://catfox.life/wp-content/uploads/2024/09/habbening.png?w=840" tabindex="0" role="button" width="1024" height="729" src="https://catfox.life/wp-content/uploads/2024/09/habbening.png?w=1024" alt=""><figcaption>The first time systemd ever booted an Adélie Linux system.</figcaption></figure>



<h2>From walking to flying</h2>



<p>Next, I built test packages on the Skylake builder we are using for x86_64 development.  I have a 2012 MacBook Pro that I keep around for testing various experiments, and this felt like a good system for the ultimate experiment.  The goal: swapping init systems with a single command.</p>



<p>It turns out that D-Bus and PolicyKit require systemd support to be enabled or disabled at build-time.  There is no way to build them in a way that allows them to operate on both types of init system.  This is an area I would like to work on more in the future.</p>



<p>I wrote package recipes for both that are built against systemd and “replace” the non-systemd versions.  I also marked them to <code>install_if</code> the system wanted systemd.</p>



<p>Next up were some more configuration and dependency fixes.  I found out via this experiment that some of the Adélie system packages do not place their pkg-config files in the proper place.  I also decided that if I’m already testing this far, I’d use networkd to bring up the laptop in question.</p>



<p>I ran the fateful command <code>apk del openrc; apk add systemd</code> and rebooted.  To my surprise, it all worked!  The system booted up perfectly with systemd.  The oddest sight was my utmps units running:</p>



<figure><img data-attachment-id="455" data-permalink="https://catfox.life/2024/09/05/porting-systemd-to-musl-libc-powered-linux/need-more-fromage-2/" data-orig-file="https://catfox.life/wp-content/uploads/2024/09/need-more-fromage-2.png" data-orig-size="1280,800" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="need-more-fromage-2" data-image-description="" data-image-caption="" data-medium-file="https://catfox.life/wp-content/uploads/2024/09/need-more-fromage-2.png?w=300" data-large-file="https://catfox.life/wp-content/uploads/2024/09/need-more-fromage-2.png?w=840" tabindex="0" role="button" width="1024" height="640" src="https://catfox.life/wp-content/uploads/2024/09/need-more-fromage-2.png?w=1024" alt=""><figcaption>systemd running s6-ipcserver.  The irony is not lost on me.</figcaption></figure>



<h2>Still needed: polish…</h2>



<p>While the system works really well, and boots in 1/3rd the time of OpenRC on the same system, it isn’t ready for prime time just yet.</p>



<p>Rebooting from a KDE session causes the compositor to freeze.  I can reboot manually from a command line, or even from a Konsole inside the session, but not using Plasma’s built-in power buttons.  This may be a PolicyKit issue – I haven’t debugged it properly yet.</p>



<p>There aren’t any service unit files written or packaged yet, other than OpenSSH and utmps.  We are working with our sponsor on an effort to add -systemd split packages to any of the packages with -openrc splits.  We should be able to rely on upstream units where present, and lean on Gentoo and Fedora’s systemd experts to have good base files to reference when needed.  I’ve already landed <a href="https://git.adelielinux.org/adelie/abuild/-/merge_requests/16">support for this in abuild</a>.</p>



<h2>…and You!</h2>



<p>This project could not have happened without the generous sponsors of Wilcox Technologies Inc (WTI) making it possible, nor without the generous sponsors of Adélie Linux keeping the distro running.  Please consider supporting both <a href="https://www.adelielinux.org/contribute/">Adélie Linux</a> and <a href="https://www.patreon.com/WilcoxTech">WTI</a> if you have the means.  Together, we are creating the future of Linux systems – a future where users have the choice and freedom to use the tooling they desire.</p>



<p>If you want to help test this new system out, please reach out to me on IRC (awilfox on Interlinked or Libera), or the Adéliegram Telegram channel.  It will be a little while before a public beta will be available, as more review and discussion with other projects is needed.  We are working with systemd, musl, and other projects to make this as smooth as possible.  We want to ensure that what we provide for testing is up to our highest standards of quality.</p>
	</div><!-- .entry-content -->

	<!-- .entry-footer -->
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Building a WoW (World of Warcraft) Server in Elixir (176 pts)]]></title>
            <link>https://pikdum.dev/posts/thistle-tea/</link>
            <guid>41454741</guid>
            <pubDate>Thu, 05 Sep 2024 08:36:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pikdum.dev/posts/thistle-tea/">https://pikdum.dev/posts/thistle-tea/</a>, See on <a href="https://news.ycombinator.com/item?id=41454741">Hacker News</a></p>
Couldn't get https://pikdum.dev/posts/thistle-tea/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Desed: Demystify and debug your sed scripts (149 pts)]]></title>
            <link>https://github.com/SoptikHa2/desed</link>
            <guid>41453557</guid>
            <pubDate>Thu, 05 Sep 2024 04:46:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/SoptikHa2/desed">https://github.com/SoptikHa2/desed</a>, See on <a href="https://news.ycombinator.com/item?id=41453557">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Desed</h2><a id="user-content-desed" aria-label="Permalink: Desed" href="#desed"></a></p>
<p dir="auto">Demystify and debug your sed scripts, from comfort of your terminal.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/SoptikHa2/desed/blob/master/img/desed.gif"><img src="https://github.com/SoptikHa2/desed/raw/master/img/desed.gif" alt="desed usage example" data-animated-image=""></a></p>
<p dir="auto">Desed is a command line tool with beautiful TUI that provides users with comfortable interface and practical debugger, used to step through complex sed scripts.</p>
<p dir="auto">Some of the notable features include:</p>
<ul dir="auto">
<li>Preview variable values, both of them!</li>
<li>See how will a substitute command affect pattern space before it runs</li>
<li>Step through sed script - both forward and backwards!</li>
<li>Place breakpoints and examine program state</li>
<li>Hot reload and see what changes as you edit source code</li>
<li>Its name is a palindrome</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Install</h2><a id="user-content-install" aria-label="Permalink: Install" href="#install"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Alpine Linux</h3><a id="user-content-alpine-linux" aria-label="Permalink: Alpine Linux" href="#alpine-linux"></a></p>
<p dir="auto"><code>aports/testing/desed</code></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Arch Linux</h3><a id="user-content-arch-linux" aria-label="Permalink: Arch Linux" href="#arch-linux"></a></p>
<p dir="auto">Via AUR: <a href="https://aur.archlinux.org/packages/desed-git/" rel="nofollow">desed-git</a> or <a href="https://aur.archlinux.org/packages/desed/" rel="nofollow">desed</a> as stable version.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">DragonFly BSD</h3><a id="user-content-dragonfly-bsd" aria-label="Permalink: DragonFly BSD" href="#dragonfly-bsd"></a></p>

<p dir="auto"><h3 tabindex="-1" dir="auto">Fedora</h3><a id="user-content-fedora" aria-label="Permalink: Fedora" href="#fedora"></a></p>

<p dir="auto"><h3 tabindex="-1" dir="auto">FreeBSD</h3><a id="user-content-freebsd" aria-label="Permalink: FreeBSD" href="#freebsd"></a></p>

<p dir="auto"><h3 tabindex="-1" dir="auto">Void Linux</h3><a id="user-content-void-linux" aria-label="Permalink: Void Linux" href="#void-linux"></a></p>

<p dir="auto"><h3 tabindex="-1" dir="auto">Source</h3><a id="user-content-source" aria-label="Permalink: Source" href="#source"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/soptikha2/desed
cd desed
cargo install --path .
cp &quot;desed.1&quot; &quot;$(manpath | cut -d':' -f1)/man1&quot;"><pre>git clone https://github.com/soptikha2/desed
<span>cd</span> desed
cargo install --path <span>.</span>
cp <span><span>"</span>desed.1<span>"</span></span> <span><span>"</span><span><span>$(</span>manpath <span>|</span> cut -d<span><span>'</span>:<span>'</span></span> -f1<span>)</span></span>/man1<span>"</span></span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Cargo</h3><a id="user-content-cargo" aria-label="Permalink: Cargo" href="#cargo"></a></p>

<p dir="auto"><h3 tabindex="-1" dir="auto">Precompiled binaries</h3><a id="user-content-precompiled-binaries" aria-label="Permalink: Precompiled binaries" href="#precompiled-binaries"></a></p>
<p dir="auto">See <a href="https://github.com/SoptikHa2/desed/releases">releases</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Dependencies:</h3><a id="user-content-dependencies" aria-label="Permalink: Dependencies:" href="#dependencies"></a></p>
<p dir="auto">Development: <code>rust</code>, <code>cargo</code> (&gt;= 1.38.0)</p>
<p dir="auto">Runtime: <code>sed</code> (GNU version, &gt;= 4.6) (desed works on BSD if you installed <code>gsed</code>)</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Controls</h2><a id="user-content-controls" aria-label="Permalink: Controls" href="#controls"></a></p>
<ul dir="auto">
<li>Mouse scroll to scroll through source code, click on line to toggle breakpoint</li>
<li><code>j</code>, <code>k</code>, <code>g</code>, <code>G</code>, just as in Vim. Prefixing with numbers works too.</li>
<li><code>b</code> to toggle breakpoint (prefix with number to toggle breakpoint on target line)</li>
<li><code>s</code> to step forward, <code>a</code> to step backwards</li>
<li><code>r</code> to run to next breakpoint or end of script, <code>R</code> to do the same but backwards</li>
<li><code>l</code> to instantly reload code and continue debugging in the exactly same place as before</li>
<li><code>q</code> to <a href="https://github.com/hakluke/how-to-exit-vim">quit</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">FAQ</h2><a id="user-content-faq" aria-label="Permalink: FAQ" href="#faq"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How does it work?</h2><a id="user-content-how-does-it-work" aria-label="Permalink: How does it work?" href="#how-does-it-work"></a></p>
<p dir="auto">GNU sed actually provides pretty useful debugging interface, try it yourself with <code>--debug</code> flag. However the interface is not interactive and I wanted something closer to traditional debugger. <a href="https://soptik.tech/articles/building-desed-the-sed-debugger.html" rel="nofollow">I've written something here</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Does it really work?</h2><a id="user-content-does-it-really-work" aria-label="Permalink: Does it really work?" href="#does-it-really-work"></a></p>
<p dir="auto">Depends. Sed actually doesn't tell me which line number is it currently executing, so I have to emulate parts of sed to guess that. Which might not be bulletproof. But it certainly worked good enough to debug tetris without issues.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why sed??</h2><a id="user-content-why-sed" aria-label="Permalink: Why sed??" href="#why-sed"></a></p>
<p dir="auto">Sed is the perfect programming language, <a href="https://tildes.net/~comp/b2k/programming_challenge_find_path_from_city_a_to_city_b_with_least_traffic_controls_inbetween#comment-2run" rel="nofollow">especially for graph problems</a>. It's plain and simple and doesn't clutter your screen with useless identifiers like <code>if</code>, <code>for</code>, <code>while</code>, or <code>int</code>. Furthermore since it doesn't have things like numbers, it's very simple to use.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">But why?</h2><a id="user-content-but-why" aria-label="Permalink: But why?" href="#but-why"></a></p>
<p dir="auto">I wanted to program in sed but it lacked good tooling up to this point, so I had to do something about it.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why?</h2><a id="user-content-why" aria-label="Permalink: Why?" href="#why"></a></p>
<p dir="auto">Because it's the standard stream editor for filtering and transforming text. And someone wrote <a href="https://github.com/uuner/sedtris">tetris</a> in it!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">What is the roadmap for future updates?</h2><a id="user-content-what-is-the-roadmap-for-future-updates" aria-label="Permalink: What is the roadmap for future updates?" href="#what-is-the-roadmap-for-future-updates"></a></p>
<p dir="auto">I would like to introduce syntax highlighting and add this tool to standard repositories of all major distributions.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Is this a joke?</h2><a id="user-content-is-this-a-joke" aria-label="Permalink: Is this a joke?" href="#is-this-a-joke"></a></p>
<p dir="auto">I thought it was. But apparently it's actually useful for some people.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Other projects</h2><a id="user-content-other-projects" aria-label="Permalink: Other projects" href="#other-projects"></a></p>
<ul dir="auto">
<li><a href="https://github.com/soptikha2/video-summarizer">video summarizer</a>, a tool and browser extensions that determines if people in video are currently talking or not, and speeds up the video accordingly. Great for long lecture videos for skipping time spent writing on a whiteboard.</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Kids who use ChatGPT as a study assistant do worse on tests (164 pts)]]></title>
            <link>https://hechingerreport.org/kids-chatgpt-worse-on-tests/</link>
            <guid>41453300</guid>
            <pubDate>Thu, 05 Sep 2024 03:50:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hechingerreport.org/kids-chatgpt-worse-on-tests/">https://hechingerreport.org/kids-chatgpt-worse-on-tests/</a>, See on <a href="https://news.ycombinator.com/item?id=41453300">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="content">
		<main id="main">

								

				
				<div>

					<section id="block-2"><p><em>The Hechinger Report is a national nonprofit newsroom that reports on one topic: education. Sign up for our&nbsp;<a href="https://hechingerreport.org/newsletters" target="_blank" rel="noreferrer noopener">weekly newsletters</a>&nbsp;to get stories like this delivered directly to your inbox.&nbsp;Consider supporting our stories and becoming&nbsp;<a href="https://hechingerreport.fundjournalism.org/?campaign=701VK000003ezHZYAY" target="_blank" rel="noreferrer noopener">a member</a>&nbsp;today.</em></p></section>

<article id="post-103317">
	<div>

		
		
					<p>Does AI actually help students learn? A recent experiment in a high school provides a cautionary tale.&nbsp;</p><p>Researchers at the University of Pennsylvania found that Turkish high school students who had access to ChatGPT while doing practice math problems did worse on a math test compared with students who didn’t have access to ChatGPT. Those with ChatGPT solved 48 percent more of the practice problems correctly, but they ultimately scored 17 percent worse on a test of the topic that the students were learning.&nbsp;</p><p>A third group of students had access to a revised version of ChatGPT that functioned more like a tutor. This chatbot was programmed to provide hints without directly divulging the answer. The students who used it did spectacularly better on the practice problems, solving 127 percent more of them correctly compared with students who did their practice work without any high-tech aids. But on a test afterwards, these AI-tutored students did no better. Students who just did their practice problems the old fashioned way — on their own — matched their test scores.</p><p>The researchers titled their paper, “Generative AI Can Harm Learning,” to make clear to parents and educators that the current crop of freely available AI chatbots can “substantially inhibit learning.” Even a fine-tuned version of ChatGPT designed to mimic a tutor doesn’t necessarily help.</p><p>The researchers believe the problem is that students are using the chatbot as a “crutch.” When they analyzed the questions that students typed into ChatGPT, students often simply asked for the answer. Students were not building the skills that come from solving the problems themselves.&nbsp;</p><p>ChatGPT’s errors also may have been a contributing factor. The chatbot only answered the math problems correctly half of the time. Its arithmetic computations were wrong 8 percent of the time, but the bigger problem was that its step-by-step approach for how to solve a problem was wrong 42 percent of the time. The tutoring version of ChatGPT was directly fed the correct solutions and these errors were minimized.</p><p>A <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4895486">draft paper about the experiment</a> was posted on the website of SSRN, formerly known as the Social Science Research Network, in July 2024. The paper has not yet been published in a peer-reviewed journal and could still be revised.&nbsp;</p><p>This is just one experiment in another country, and more studies will be needed to confirm its findings. But this experiment was a large one, involving nearly a thousand students in grades nine through 11 during the fall of 2023. Teachers first reviewed a previously taught lesson with the whole classroom, and then their classrooms were randomly assigned to practice the math in one of three ways: with access to ChatGPT, with access to an AI tutor powered by ChatGPT or with no high-tech aids at all. Students in each grade were assigned the same practice problems with or without AI. Afterwards, they took a test to see how well they learned the concept. Researchers conducted four cycles of this, giving students four 90-minute sessions of practice time in four different math topics to understand whether AI tends to help, harm or do nothing.</p><p>ChatGPT also seems to produce overconfidence. In surveys that accompanied the experiment, students said they did not think that ChatGPT caused them to learn less even though they had. Students with the AI tutor thought they had done significantly better on the test even though they did not. (It’s also another good reminder to all of us that our <a href="https://hechingerreport.org/proof-points-college-students-often-dont-know-when-theyre-learning/">perceptions of how much we’ve learned are often wrong</a>.)</p><p>The authors likened the problem of learning with ChatGPT to autopilot. They recounted how an overreliance on autopilot led the Federal Aviation Administration to recommend that pilots minimize their use of this technology. Regulators wanted to make sure that pilots still know how to fly when autopilot fails to function correctly.&nbsp;</p><p>ChatGPT is not the first technology to present a tradeoff in education. Typewriters and computers reduce the need for handwriting. Calculators reduce the need for arithmetic. When students have access to ChatGPT, they might answer more problems correctly, but learn less. Getting the right result to one problem won’t help them with the next one.</p><p><em>This story about&nbsp;using <a href="https://hechingerreport.org/kids-chatgpt-worse-on-tests/">ChatGPT to practice math</a>&nbsp;was written by Jill Barshay and produced by&nbsp;<a href="https://hechingerreport.org/special-reports/higher-education/" target="_blank" rel="noreferrer noopener">The Hechinger Report</a>, a nonprofit, independent news organization focused on inequality and innovation in education. Sign up for&nbsp;<a href="https://hechingerreport.org/proofpoints/" target="_blank" rel="noreferrer noopener"><em>Proof Points</em></a>&nbsp;and other&nbsp;<a href="https://hechingerreport.org/newsletters/" target="_blank" rel="noreferrer noopener"><em>Hechinger newsletters</em></a>.</em></p>
<div id="custom_html-3">
	
<p>The Hechinger Report provides in-depth, fact-based, unbiased reporting on education that is free to all readers. But that doesn't mean it's free to produce. Our work keeps educators and the public informed about pressing issues at schools and on campuses throughout the country. We tell the whole story, even when the details are inconvenient. Help us keep doing that.</p>

<p><a href="https://checkout.fundjournalism.org/memberform?amount=15&amp;installmentPeriod=monthly&amp;org_id=hechingerreport&amp;campaign=701f4000000dsvy">Join us today.</a></p>
</div>	</div><!-- .entry-content -->

	<!-- .entry-footer -->

	
			<div>
															<p><a href="https://hechingerreport.org/author/jill-barshay/" rel="author">
											<img alt="Avatar photo" src="https://hechingerreport.org/wp-content/uploads/2015/01/Barshay-80x80.jpg" srcset="https://i0.wp.com/hechingerreport.org/wp-content/uploads/2015/01/Barshay.jpg?fit=154%2C150&amp;ssl=1 2x" height="80" width="80">											</a></p><!-- .author-bio-text -->

			</div><!-- .author-bio -->
			
</article><!-- #post-${ID} -->

<!-- #comments -->
				</div><!-- .main-content -->

			
		</main><!-- #main -->
	</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Yi-Coder: A Small but Mighty LLM for Code (242 pts)]]></title>
            <link>https://01-ai.github.io/blog.html?post=en/2024-09-05-A-Small-but-Mighty-LLM-for-Code.md</link>
            <guid>41453237</guid>
            <pubDate>Thu, 05 Sep 2024 03:38:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://01-ai.github.io/blog.html?post=en/2024-09-05-A-Small-but-Mighty-LLM-for-Code.md">https://01-ai.github.io/blog.html?post=en/2024-09-05-A-Small-but-Mighty-LLM-for-Code.md</a>, See on <a href="https://news.ycombinator.com/item?id=41453237">Hacker News</a></p>
Couldn't get https://01-ai.github.io/blog.html?post=en/2024-09-05-A-Small-but-Mighty-LLM-for-Code.md: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Accelerando (2005) (170 pts)]]></title>
            <link>https://www.antipope.org/charlie/blog-static/fiction/accelerando/accelerando.html</link>
            <guid>41452962</guid>
            <pubDate>Thu, 05 Sep 2024 02:33:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.antipope.org/charlie/blog-static/fiction/accelerando/accelerando.html">https://www.antipope.org/charlie/blog-static/fiction/accelerando/accelerando.html</a>, See on <a href="https://news.ycombinator.com/item?id=41452962">Hacker News</a></p>
Couldn't get https://www.antipope.org/charlie/blog-static/fiction/accelerando/accelerando.html: Error: unable to verify the first certificate]]></description>
        </item>
        <item>
            <title><![CDATA[Canadian mega landlord using AI 'pricing scheme' as it hikes rents (133 pts)]]></title>
            <link>https://breachmedia.ca/canadian-mega-landlord-ai-pricing-scheme-hikes-rents/</link>
            <guid>41452781</guid>
            <pubDate>Thu, 05 Sep 2024 01:59:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://breachmedia.ca/canadian-mega-landlord-ai-pricing-scheme-hikes-rents/">https://breachmedia.ca/canadian-mega-landlord-ai-pricing-scheme-hikes-rents/</a>, See on <a href="https://news.ycombinator.com/item?id=41452781">Hacker News</a></p>
Couldn't get https://breachmedia.ca/canadian-mega-landlord-ai-pricing-scheme-hikes-rents/: Error: timeout of 10000ms exceeded]]></description>
        </item>
    </channel>
</rss>