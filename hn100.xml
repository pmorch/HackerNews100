<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 14 Nov 2023 18:00:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[.NET 8 (186 pts)]]></title>
            <link>https://devblogs.microsoft.com/dotnet/announcing-dotnet-8/</link>
            <guid>38264937</guid>
            <pubDate>Tue, 14 Nov 2023 16:00:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://devblogs.microsoft.com/dotnet/announcing-dotnet-8/">https://devblogs.microsoft.com/dotnet/announcing-dotnet-8/</a>, See on <a href="https://news.ycombinator.com/item?id=38264937">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="featured">
                         <p>
            November 14th, 2023</p><!-- .entry-meta -->
        
<p>We are happy to announce the availability of <a href="https://aka.ms/get-dotnet-8">.NET 8</a>, the latest <a href="https://dotnet.microsoft.com/platform/support/policy/dotnet-core#release-types">LTS</a> version of one of the world’s leading development platforms, starting today. .NET 8 delivers thousands of performance, stability, and security improvements, as well as platform and tooling enhancements that help increase developer productivity and speed of innovation. The .NET team, our partners, and the .NET community will be talking about what’s new in .NET 8 as well as what people are building with .NET today to meet their needs of tomorrow at  <a href="https://www.dotnetconf.net/">.NET Conf 2023, a three day virtual event (November 14-16)</a>. Come, join us!</p>
<p><a href="https://dotnet.microsoft.com/download/dotnet/8.0"><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/Banner3.png" alt=".NET 8 is now available" width="600"></a></p>
<p>With this release, .NET reshapes the way we build intelligent, cloud-native applications and high-traffic services that scale on demand. Whether you’re deploying to Linux or Windows, using containers or a cloud app model of your choice, .NET 8 makes building these apps easier. It includes a set of proven libraries that are used today by the many high-scale services at Microsoft to help you with fundamental challenges around observability, resiliency, scalability, manageability, and more.</p>
<p><a href="https://aka.ms/aspireannouncement"><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/Cloud-Native-dotnet8.png" alt=".NET 8 - Cloud Native Features"></a></p>
<p>Integrate large language models (LLMs) like OpenAI’s GPT directly into your .NET app. Use a single powerful component model to handle all your web UI needs with Blazor. Deploy your mobile applications to the latest version of iOS and Android with .NET MAUI. Discover new language enhancements that make your code more concise and expressive with C# 12.  </p>
<p>Let’s look at what’s new in .NET 8. </p>
<h2 id="unparalleled-performance-experience-the-fastest-net-to-date">Unparalleled Performance – Experience the fastest .NET to date</h2>
<p>.NET 8 comes with thousands of performance <a href="https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-8/">improvements</a> <a href="https://devblogs.microsoft.com/dotnet/performance-improvements-in-aspnet-core-8/">across</a> <a href="https://devblogs.microsoft.com/dotnet/dotnet-8-performance-improvements-in-dotnet-maui/">the</a> <a href="https://devblogs.microsoft.com/dotnet/this-arm64-performance-in-dotnet-8/">stack</a>. A new code generator called Dynamic Profile-Guided Optimization (PGO) that optimizes your code based on real-world usage is enabled by default and can improve the performance of your apps up to 20%. The AVX-512 instruction set, which is now supported, enables you to perform parallel operations on 512-bit vectors of data, meaning you can process much more data in less time. The primitive types (numerical and beyond) now implement a new formattable and parsable interface, which enable them to directly format and parse as UTF-8 without any transcoding overhead.</p>
<p>Every year we talk about the performance gains across .NET. This year we continue our quest to push the performance of .NET to new heights. From the latest TechEmpower benchmarks with .NET 8, we’re seeing improvements in the JSON API scenario of 18%, hitting nearly one million requests per second with ASP.NET Core Minimal APIs.</p>
<p><a href="https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-8/"><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/Performance2.png" alt="TechEmpower requests per second (RPS)"></a></p>
<p>The Fortunes scenario is closer to a real-world workload, including database access and server-side HTML rendering. In this test, we see an even larger improvement of 24%, now over 300K requests per second with ASP.NET Core.</p>
<h2 id="net-aspire-an-opinionated-stack-to-build-observable-production-ready-cloud-native-applications">.NET Aspire – An opinionated stack to build observable, production-ready cloud-native applications</h2>
<p>.NET Aspire is a stack for building resilient, observable, and configurable cloud-native applications with .NET. It includes a curated set of components enhanced for cloud-native by including telemetry, resilience, configuration, and health checks by default. Combined with a sophisticated but simple local developer experience, .NET Aspire makes it easy to discover, acquire, and configure essential dependencies for cloud-native applications on day 1 as well as day 100. The <a href="https://aka.ms/aspireannouncement">first preview</a> of .NET Aspire is available today.</p>
<p><a href="https://aka.ms/aspireannouncement"><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/Aspire-CTAs.png" alt=".NET Aspire"></a></p>
<h2 id="net-8-container-enhancements-more-secure-compact-and-productive">.NET 8 Container Enhancements – More secure, compact, and productive</h2>
<p>Package your applications with <a href="https://devblogs.microsoft.com/dotnet/securing-containers-with-rootless/">containers more easily and more securely than ever with .NET</a>. Every .NET image includes a non-root user, enabling more secure containers with one-line configuration. The .NET SDK tooling publishes container images without a Dockerfile and are non-root by default. Deploy your containerized apps faster due to smaller .NET base images – including new experimental variants of our images that deliver truly minimal application sizes for native AOT. Opt-in to even more security hardening with the new Chiseled Ubuntu image variants to reduce your attack surface even further. Using Dockerfiles or SDK tooling, build apps and container images for any architecture.</p>
<p><a href="https://devblogs.microsoft.com/dotnet/securing-containers-with-rootless/"><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/Containers2.png" alt="Modern containers"></a></p>
<h2 id="native-aot-journey-towards-higher-density-sustainable-compute">Native AoT – Journey towards higher density sustainable compute</h2>
<p>Compile your .NET apps into <a href="https://learn.microsoft.com/dotnet/core/deploying/native-aot">native code</a> that uses less memory and starts instantly. No need to wait for the JIT (just-in-time) compiler to compile the code at run time. No need to deploy the JIT compiler and IL code. AOT apps deploy just the code that’s needed for your app. Your app is now empowered to run in restricted environments where a JIT compiler isn’t allowed.</p>
<p><a href="https://learn.microsoft.com/dotnet/core/deploying/native-aot"><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/AOTOptimizations3.png" alt="Before and After AOT"></a></p>
<h2 id="artificial-intelligence-infuse-ai-into-your-net-applications">Artificial Intelligence – Infuse AI into your .NET applications</h2>
<p>Generative AI and large language models are transforming the field of AI, providing developers the ability to create unique AI-powered experiences in their applications. <a href="https://aka.ms/dotnet-genai">.NET 8 makes it simple for you to leverage AI</a> via first-class out-of-the box AI features in the .NET SDK and seamless integration with several tools. </p>
<p>.NET 8 brings several enhancements to the <code>System.Numerics</code> library to improve its compatibility with Generative AI workloads, such as integrating Tensor Primitives. With the rise of AI-enabled apps, new tools and SDKs emerged. We collaborated with numerous internal and external partners, such as <a href="https://azure.microsoft.com/products/ai-services/openai-service">Azure OpenAI</a>, <a href="https://azure.microsoft.com/free/ai-services/?ef_id=_k_b34c5d449bf4175800e738086ecc7267_k_&amp;OCID=AIDcmm5edswduu_SEM__k_b34c5d449bf4175800e738086ecc7267_k_&amp;msclkid=b34c5d449bf4175800e738086ecc7267">Azure Cognitive Search</a>, <a href="https://milvus.io/docs/v2.2.x/install-csharp.md">Milvus</a>, <a href="https://github.com/qdrant/qdrant-dotnet">Qdrant</a>, and <a href="https://github.com/microsoft/teams-ai">Microsoft Teams</a>, to ensure .NET developers have easy access to various AI models, services, and platforms through their respective SDKs. Additionally, the open-source <a href="https://learn.microsoft.com/semantic-kernel/overview/">Semantic Kernel</a> SDK simplifies the integration of these AI components into new and existing applications, to help you deliver innovative user experiences.</p>
<p>Various samples and reference templates, showcasing patterns and practices, are now available to make it easy for developers to get started:</p>
<ul>
<li><a href="https://github.com/dotnet-architecture/eShop">Customer Chatbot</a></li>
<li><a href="https://github.com/Azure-Samples/azure-search-openai-demo-csharp">Retrieval Augmented Generation</a></li>
<li><a href="https://devblogs.microsoft.com/dotnet/demystifying-retrieval-augmented-generation-with-dotnet/">Developing Apps using Azure AI services</a> </li>
</ul>
<p><a href="https://github.com/Azure-Samples/azure-search-openai-demo-csharp/assets/2546640/b79090b8-6a8b-45f4-b42b-e21e22b1661a"><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/InfuseAIdotnetapps1.png" alt="Infuse AI dotnet apps"></a></p>
<h2 id="blazor-build-full-stack-web-applications-with-net">Blazor – Build full stack web applications with .NET</h2>
<p>Blazor in .NET 8 can use both the server and client together to handle all your web UI needs. It’s full stack web UI! With several new enhancements focused towards optimizing page load time, scalability, and elevating the user experience, developers can now use <a href="https://dotnet.microsoft.com/apps/aspnet/web-apps/blazor">Blazor Server and Blazor WebAssembly</a> in the same app, automatically shifting users from the server to the client at run time. Your .NET code runs significantly faster on WebAssembly thanks to the new “Jiterpreter”-based runtime and new built-in components. As a part enhancing the overall <a href="https://devblogs.microsoft.com/dotnet/whats-new-with-identity-in-dotnet-8/">authentication, authorization, and identity management in .NET 8</a>, Blazor now supports generating a full Blazor-based Identity UI.</p>
<p><a href="https://dotnet.microsoft.com/apps/aspnet/web-apps/blazor"><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/Blazor2.png" alt="Full stack Blazor"></a></p>
<h2 id="net-maui-elevated-performance-reliability-and-developer-experience"><a href="https://devblogs.microsoft.com/dotnet/announcing-dotnet-maui-in-dotnet-8">.NET MAUI – Elevated performance, reliability, and developer experience</a></h2>
<p>.NET MAUI provides you with a single project system and single codebase to build WinUI, Mac Catalyst, iOS, and Android applications. Native AOT (experimental) now supports targeting iOS-like platforms. <a href="https://aka.ms/maui-devkit-blog">A new Visual Studio Code extension for .NET MAUI</a> gives you the tools you need to develop cross-platform .NET mobile and desktop apps. Xcode 15 and Android API 34 are now supported allowing you to target the latest version of iOS and Android. A plethora of quality improvements were made to the <a href="https://devblogs.microsoft.com/dotnet/dotnet-8-performance-improvements-in-dotnet-maui">areas of performance</a>, controls and UI elements, and platform-specific behavior, such as desktop interaction adding better click handling, keyboard listeners, and more.</p>
<p><a href="https://devblogs.microsoft.com/dotnet/announcing-dotnet-maui-in-dotnet-8"><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/dotnetMAUIin8-1.png" alt=".NET MAUI Visual Studio Code Extension"></a></p>
<h2 id="c-12-features-simplified-syntax-for-better-developer-productivity">C# 12 Features – Simplified syntax for better developer productivity</h2>
<p>C# 12 makes your coding experience more productive and enjoyable. You can now create primary constructors in any class and struct with a simple and elegant syntax. No more boilerplate code to initialize your fields and properties. Be delighted when creating arrays, spans, and other collection types with a concise and expressive syntax. Use new default values for parameters in lambda expressions. No more overloading or null checks to handle optional arguments. You can even use the <code>using</code> alias directive to alias any type, not just named types!</p>
<p><strong>Collection expressions</strong></p>
<pre><code>// Create a list:
List&lt;int&gt; a = [1, 2, 3, 4, 5, 6, 7, 8];

// Create a span
Span&lt;char&gt; b  = ['a', 'b', 'c', 'd', 'e', 'f', 'h', 'i'];

// Use the spread operator to concatenate
int[] array1 = [1, 2, 3];
int[] array2 = [4, 5, 6];
int[] array3 = [7, 8, 9];
int[] fullArray = [..array1, ..array2, ..array3]; // contents is [1, 2, 3, 4, 5, 6, 7, 8, 9]</code></pre>
<p>See more about the latest version of C# in <a href="https://devblogs.microsoft.com/dotnet/announcing-csharp-12">Announcing C# 12</a>.</p>

<p>We have a set of great tools that help you be the most productive in your development workflow and take advantage of .NET 8 today. Released alongside .NET 8, the <a href="https://aka.ms/vs/v178GA">Visual Studio 2022 17.8 release</a> brings support for .NET 8, C# 12 language enhancements, and various new productivity features. <a href="https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.csdevkit">VS Code and C# Dev Kit</a> is a great way to get started with .NET 8 if you’re learning and/or want to quickly kick the tires of the runtime and is available on Linux, macOS, or in GitHub Codespaces. The new <a href="https://github.com/codespaces">GitHub Codespaces template for .NET</a>, which comes with the .NET SDK and a set of configured extensions, is one of the fastest ways to get started with .NET 8. </p>
<h3 id="additional-features-in-net-8">Additional features in .NET 8:</h3>
<ul>
<li><strong>ASP.NET Core.</strong> <a href="https://devblogs.microsoft.com/dotnet/whats-new-with-identity-in-dotnet-8/">Streamlines identity for single-page applications (SPA) and Blazor providing cookie-based authentication, pre-built APIs, token support, and a new identity UI.</a> and <a href="https://learn.microsoft.com/aspnet/core/release-notes/aspnetcore-8.0#minimal-apis">enhances minimal APIs with form-binding, antiforgery support to protect against cross-site request forgery (XSRF/CSRF), and <code>asParameters</code> support for parameter-binding with Open API definitions</a></li>
<li><strong>ASP.NET Core tooling.</strong> <a href="https://devblogs.microsoft.com/dotnet/aspnet-core-route-tooling-dotnet-8/">Route syntax highlighting, auto-completion, and analyzers to help you create Web APIs.</a></li>
<li><strong>Entity Framework Core.</strong> <a href="https://devblogs.microsoft.com/dotnet/announcing-ef8-rc2/">Provides new “complex types” as value objects, primitive collections, and SQL Server support for hierarchical data.</a></li>
<li><strong>NuGet.</strong> <a href="https://learn.microsoft.com/nuget/concepts/auditing-packages">Helps you audit your NuGet packages in projects and solutions for any known security vulnerabilities.</a></li>
<li><strong>.NET Runtime.</strong> <a href="https://devblogs.microsoft.com/dotnet/announcing-dotnet-8-rc1/#androidstripilafteraot-mode-on-android">Brings a new AOT compilation mode for WebAssembly (WASM) and Android.</a></li>
<li><strong>.NET SDK.</strong> <a href="https://learn.microsoft.com/dotnet/core/whats-new/dotnet-8#net-sdk">Revitalizes terminal build output and production-ready defaults.</a></li>
<li><strong>WPF.</strong> <a href="https://devblogs.microsoft.com/dotnet/wpf-file-dialog-improvements-in-dotnet-8/">Supports OpenFolderDialog</a> and <a href="https://devblogs.microsoft.com/dotnet/announcing-dotnet-8-rc1/#wpf-hardware-acceleration-in-rdp">Enabled HW Acceleration in RDP</a></li>
<li><strong>ARM64.</strong> <a href="https://devblogs.microsoft.com/dotnet/this-arm64-performance-in-dotnet-8/">Significant feature enhancements and improved code quality for ARM64 platforms through collaboration with ARM engineers.</a></li>
<li><strong>Debugging.</strong> <a href="https://devblogs.microsoft.com/dotnet/debugging-enhancements-in-dotnet-8/">Displays debug summaries and provides simplified debug proxies for commonly used .NET types.</a></li>
<li><strong>System.Text.Json.</strong> <a href="https://devblogs.microsoft.com/dotnet/system-text-json-in-dotnet-8/">Helps populate read-only members, customizes unmapped member handling, and improves Native AOT support.</a></li>
<li><strong>.NET Community Toolkit.</strong> <a href="https://devblogs.microsoft.com/dotnet/announcing-the-dotnet-community-toolkit-821/">Accelerates building .NET libraries and applications while ensuring they are trim and AOT compatible (including the MVVM source generators!)</a></li>
<li><strong>Azure</strong> <a href="https://aka.ms/appservice-dotnet8">Supports .NET 8 with Azure’s PaaS services like App Service for Windows and Linux, Static Web Apps, Azure Functions, and Azure Container Apps.</a></li>
<li><strong>What’s new in .NET 8.</strong> <a href="https://learn.microsoft.com/dotnet/core/whats-new/dotnet-8">Check out our documentation for everything else!</a></li>
</ul>
<h3 id="get-started-with-net-8">Get started with .NET 8</h3>
<p>For the best development experience with .NET 8, we recommend that you use the latest release of <a href="https://visualstudio.microsoft.com/downloads/">Visual Studio</a> and <a href="https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.csdevkit">Visual Studio Code’s C# Dev Kit</a>. Once you’re set up, here are some of the things you should do:</p>
<ul>
<li><strong>Try the new features and APIs.</strong> <a href="https://dotnet.microsoft.com/download/dotnet/8.0">Download .NET 8</a> and <a href="https://github.com/dotnet/core/issues/new/choose">report issues in our issue tracker</a>.</li>
<li><strong>Test your current app for compatibility.</strong> Learn whether your app is <a href="https://learn.microsoft.com/dotnet/core/compatibility/8.0">affected by default behavior changes in .NET 8</a>.</li>
<li><strong>Test your app with opt-in changes.</strong> .NET 8 has <a href="https://learn.microsoft.com/dotnet/core/compatibility/8.0">opt-in behavior changes</a> that only affect your app when enabled. It’s important to understand and assess these changes early as they may become default in the next release.</li>
<li><strong>Update your app with the Upgrade Assistant.</strong> <a href="https://dotnet.microsoft.com/platform/upgrade-assistant">Upgrade your app with just a few clicks using the Upgrade Assistant</a>.</li>
<li><strong>Know you’re supported.</strong> .NET 8 is officially supported by Microsoft as a <a href="https://dotnet.microsoft.com/platform/support/policy/dotnet-core#release-types">long term support (LTS) release that will be supported for three years</a>.</li>
<li><strong>Bonus: eShop Sample for .NET 8.</strong> Follow all the best coding and architecture practices with our <a href="https://github.com/dotnet/eshop">new eShop sample, now updated for .NET 8</a>!</li>
</ul>
<h3 id="celebrate-net-8">Celebrate .NET 8</h3>
<ul>
<li><strong>.NET Conf 2023</strong>. <a href="https://www.dotnetconf.net/">Join us November 14-16, 2023 to celebrate the .NET 8 release!</a></li>
<li><strong>What’s next in .NET?</strong> <a href="https://dotnet.microsoft.com/next">Get involved and learn the latest news on .NET 8 and the next version of .NET.</a></li>
<li><strong>Get C# Certified</strong>. <a href="https://devblogs.microsoft.com/dotnet/announcing-foundational-csharp-certification/">Earn a badge of honor with a freeCodeCamp C# certification.</a></li>
<li><strong>Learn .NET 8</strong>. <a href="https://aka.ms/learn-dotnet-8">Free tutorials, videos, courses, and more for beginner through advanced .NET developers. All updated for .NET 8!</a></li>
<li><strong>See Developer Stories</strong>. <a href="https://devblogs.microsoft.com/dotnet/category/developer-stories/">Take a look at success stories of developers migrating to modern .NET.</a></li>
<li><strong>Read about why .NET?</strong>. <a href="https://devblogs.microsoft.com/dotnet/why-dotnet/">Read through our recent blog series about the convenience of .NET.</a></li>
</ul>

<p>We would just like to end by saying one big…</p>
<p><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/ThankYou.png" alt="https://dotnet.microsoft.com/thanks/8.0"></p>

        

		
        <div>

            <p><img width="60" height="60" data-src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-96x96.jpg" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-96x96.jpg" alt="" decoding="async" data-srcset="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-96x96.jpg 96w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-300x300.jpg 300w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-150x150.jpg 150w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-24x24.jpg 24w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-48x48.jpg 48w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth.jpg 648w" sizes="(max-width: 60px) 100vw, 60px" srcset="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-96x96.jpg 96w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-300x300.jpg 300w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-150x150.jpg 150w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-24x24.jpg 24w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-48x48.jpg 48w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth.jpg 648w"></p>
            <h3>
                <a data-bi-id="author follow within post" data-bi-area="" data-bi-name="Gaurav Seth" aria-label="Gaurav Seth" href="https://devblogs.microsoft.com/dotnet/author/gauravs">
                    Gaurav Seth                </a>
                <span>Partner Director of Product, Developer Platforms</span>
            </h3>
            
       </div>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GraphCast: AI model for faster and more accurate global weather forecasting (101 pts)]]></title>
            <link>https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/</link>
            <guid>38264641</guid>
            <pubDate>Tue, 14 Nov 2023 15:42:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/">https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/</a>, See on <a href="https://news.ycombinator.com/item?id=38264641">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
      
  <article>
    
  

  
  
  
  
    

    
    
      
        <div>
          
            
            
              
              

<div>
    <div>
      <p>Research</p>
      

      
        <dl>
          
            <dt>Published</dt>
            <dd>
              <time datetime="2023-11-14">
                14 November 2023
              </time>
            </dd>
          
          
            <dt>Authors</dt>
            
          
        </dl>
      

      
    </div>

    
    
    
    <picture>
      <source media="(min-width: 1024px)" type="image/webp" width="1072" height="603" srcset="https://lh3.googleusercontent.com/_qOKQMNWIajEgW_XlurIRao3upyv8w_4EpTRqTzu6FRyr0_qPsWdGV6nYDgsJ8C71sVuNaq1AdOxIB8UwUdhnQMVcZ_EUGOttpnVBeWEZljkqR--ig=w1072-h603-n-nu-rw 1x, https://lh3.googleusercontent.com/_qOKQMNWIajEgW_XlurIRao3upyv8w_4EpTRqTzu6FRyr0_qPsWdGV6nYDgsJ8C71sVuNaq1AdOxIB8UwUdhnQMVcZ_EUGOttpnVBeWEZljkqR--ig=w2144-h1206-n-nu-rw 2x"><source media="(min-width: 600px)" type="image/webp" width="928" height="522" srcset="https://lh3.googleusercontent.com/_qOKQMNWIajEgW_XlurIRao3upyv8w_4EpTRqTzu6FRyr0_qPsWdGV6nYDgsJ8C71sVuNaq1AdOxIB8UwUdhnQMVcZ_EUGOttpnVBeWEZljkqR--ig=w928-h522-n-nu-rw 1x, https://lh3.googleusercontent.com/_qOKQMNWIajEgW_XlurIRao3upyv8w_4EpTRqTzu6FRyr0_qPsWdGV6nYDgsJ8C71sVuNaq1AdOxIB8UwUdhnQMVcZ_EUGOttpnVBeWEZljkqR--ig=w1856-h1044-n-nu-rw 2x"><source type="image/webp" width="528" height="297" srcset="https://lh3.googleusercontent.com/_qOKQMNWIajEgW_XlurIRao3upyv8w_4EpTRqTzu6FRyr0_qPsWdGV6nYDgsJ8C71sVuNaq1AdOxIB8UwUdhnQMVcZ_EUGOttpnVBeWEZljkqR--ig=w528-h297-n-nu-rw 1x, https://lh3.googleusercontent.com/_qOKQMNWIajEgW_XlurIRao3upyv8w_4EpTRqTzu6FRyr0_qPsWdGV6nYDgsJ8C71sVuNaq1AdOxIB8UwUdhnQMVcZ_EUGOttpnVBeWEZljkqR--ig=w1056-h594-n-nu-rw 2x">
      <img alt="GraphCast global weather forecasting of surface wind speed" height="603" src="https://lh3.googleusercontent.com/_qOKQMNWIajEgW_XlurIRao3upyv8w_4EpTRqTzu6FRyr0_qPsWdGV6nYDgsJ8C71sVuNaq1AdOxIB8UwUdhnQMVcZ_EUGOttpnVBeWEZljkqR--ig=w1072-h603-n-nu" width="1072">
    </picture>
    
  
  </div>
            
          
            
            
              
              <div>
  <p data-block-key="pnebs"><b>Our state-of-the-art model delivers 10-day weather predictions at unprecedented accuracy in under one minute</b></p><p data-block-key="f3n9e">The weather affects us all, in ways big and small. It can dictate how we dress in the morning, provide us with green energy and, in the worst cases, create storms that can devastate communities. In a world of increasingly extreme weather, fast and accurate forecasts have never been more important.</p><p data-block-key="baim">In a paper <a href="https://www.science.org/doi/10.1126/science.adi2336" rel="noopener" target="_blank">published in Science</a>, we introduce GraphCast, a state-of-the-art AI model able to make medium-range weather forecasts with unprecedented accuracy. GraphCast predicts weather conditions up to 10 days in advance more accurately and much faster than the industry gold-standard weather simulation system – the High Resolution Forecast (HRES), produced by the European Centre for Medium-Range Weather Forecasts (ECMWF).</p><p data-block-key="d53hu">GraphCast can also offer earlier warnings of extreme weather events. It can predict the tracks of cyclones with great accuracy further into the future, identifies atmospheric rivers associated with flood risk, and predicts the onset of extreme temperatures. This ability has the potential to save lives through greater preparedness.</p><p data-block-key="2cftd">GraphCast takes a significant step forward in AI for weather prediction, offering more accurate and efficient forecasts, and opening paths to support decision-making critical to the needs of our industries and societies. And, by <a href="https://github.com/google-deepmind/graphcast" rel="noopener" target="_blank">open sourcing the model code for GraphCast,</a> we are enabling scientists and forecasters around the world to benefit billions of people in their everyday lives. GraphCast is already being used by weather agencies, including ECMWF, which is running a live experiment of <a href="https://charts.ecmwf.int/products/graphcast_medium-mslp-wind850" rel="noopener" target="_blank">our model’s forecasts on its website</a>.</p>
</div>
            
          
            
            
              
              


<figure>
  
  
    <figcaption>
      <p data-block-key="ke1fd">A selection of GraphCast’s predictions rolling across 10 days showing specific humidity at 700 hectopascals (about 3 km above surface), surface temperature, and surface wind speed.</p>
    </figcaption>
  
</figure>
            
          
            
            
              
              <div>
  <h2 data-block-key="pnebs">The challenge of global weather forecasting</h2><p data-block-key="c69cv">Weather prediction is one of the oldest and most challenging–scientific endeavours. Medium range predictions are important to support key decision-making across sectors, from renewable energy to event logistics, but are difficult to do accurately and efficiently.</p><p data-block-key="27gma">Forecasts typically rely on Numerical Weather Prediction (NWP), which begins with carefully defined physics equations, which are then translated into computer algorithms run on supercomputers. While this traditional approach has been a triumph of science and engineering, designing the equations and algorithms is time-consuming and requires deep expertise, as well as costly compute resources to make accurate predictions.</p><p data-block-key="3ces3">Deep learning offers a different approach: using data instead of physical equations to create a weather forecast system. GraphCast is trained on decades of historical weather data to learn a model of the cause and effect relationships that govern how Earth’s weather evolves, from the present into the future.</p><p data-block-key="1lfp4">Crucially, GraphCast and traditional approaches go hand-in-hand: we trained GraphCast on four decades of weather reanalysis data, from the ECMWF’s ERA5 dataset. This trove is based on historical weather observations such as satellite images, radar, and weather stations using a traditional NWP to ‘fill in the blanks’ where the observations are incomplete, to reconstruct a rich record of global historical weather.</p><h2 data-block-key="f4hf9">GraphCast: An AI model for weather prediction</h2><p data-block-key="51oo3">GraphCast is a weather forecasting system based on machine learning and Graph Neural Networks (GNNs), which are a particularly useful architecture for processing spatially structured data.</p><p data-block-key="6c3sr">GraphCast makes forecasts at the high resolution of 0.25 degrees longitude/latitude (28km x 28km at the equator). That’s more than a million grid points covering the entire Earth’s surface. At each grid point the model predicts five Earth-surface variables – including temperature, wind speed and direction, and mean sea-level pressure – and six atmospheric variables at each of 37 levels of altitude, including specific humidity, wind speed and direction, and temperature.</p><p data-block-key="djs0f">While GraphCast’s training was computationally intensive, the resulting forecasting model is highly efficient. Making 10-day forecasts with GraphCast takes less than a minute on a single Google TPU v4 machine. For comparison, a 10-day forecast using a conventional approach, such as HRES, can take hours of computation in a supercomputer with hundreds of machines.</p><p data-block-key="7dee1">In a comprehensive performance evaluation against the gold-standard deterministic system, HRES, GraphCast provided more accurate predictions on more than 90% of 1380 test variables and forecast lead times (see our <a href="https://www.science.org/doi/10.1126/science.adi2336" rel="noopener" target="_blank">Science paper</a> for details). When we limited the evaluation to the troposphere, the 6-20 kilometer high region of the atmosphere nearest to Earth’s surface where accurate forecasting is most important, our model outperformed HRES on 99.7% of the test variables for future weather.</p>
</div>
            
          
            
            
              
              


<figure>
  
  
    <figcaption>
      <p data-block-key="x7trc">For inputs, GraphCast requires just two sets of data: the state of the weather 6 hours ago, and the current state of the weather. The model then predicts the weather 6 hours in the future. This process can then be rolled forward in 6-hour increments to provide state-of-the-art forecasts up to 10 days in advance.</p>
    </figcaption>
  
</figure>
            
          
            
            
              
              <div>
  <h2 data-block-key="pnebs">Better warnings for extreme weather events</h2><p data-block-key="7ki0b">Our analyses revealed that GraphCast can also identify severe weather events earlier than traditional forecasting models, despite not having been trained to look for them. This is a prime example of how GraphCast could help with preparedness to save lives and reduce the impact of storms and extreme weather on communities.</p><p data-block-key="csav9">By applying a simple cyclone tracker directly onto GraphCast forecasts, we could predict cyclone movement more accurately than the HRES model. In September, a live version of our publicly available GraphCast model, deployed on the ECMWF website, accurately predicted about nine days in advance that Hurricane Lee would make landfall in Nova Scotia. By contrast, traditional forecasts had greater variability in where and when landfall would occur, and only locked in on Nova Scotia about six days in advance.</p><p data-block-key="ecqbk">GraphCast can also characterize atmospheric rivers – narrow regions of the atmosphere that transfer most of the water vapour outside of the tropics. The intensity of an atmospheric river can indicate whether it will bring beneficial rain or a flood-inducing deluge. GraphCast forecasts can help characterize atmospheric rivers, which could help planning emergency responses together with <a href="https://sites.research.google/floodforecasting/" rel="noopener" target="_blank">AI models to forecast floods.</a></p><p data-block-key="a8m1r">Finally, predicting extreme temperatures is of growing importance in our warming world. GraphCast can characterize when the heat is set to rise above the historical top temperatures for any given location on Earth. This is particularly useful in anticipating heat waves, disruptive and dangerous events that are becoming increasingly common.</p>
</div>
            
          
            
            
              
              


<figure>
  
  
    <figcaption>
      <p data-block-key="df1sy">Severe-event prediction - how GraphCast and HRES compare.</p><p data-block-key="2dp0j">Left: Cyclone tracking performances. As the lead time for predicting cyclone movements grows, GraphCast maintains greater accuracy than HRES.</p><p data-block-key="3kpk6">Right: Atmospheric river prediction. GraphCast’s prediction errors are markedly lower than HRES’s for the entirety of their 10-day predictions</p>
    </figcaption>
  
</figure>
            
          
            
            
              
              <div>
  <h2 data-block-key="pnebs">The future of AI for weather</h2><p data-block-key="795pr">GraphCast is now the most accurate 10-day global weather forecasting system in the world, and can predict extreme weather events further into the future than was previously possible. As the weather patterns evolve in a changing climate, GraphCast will evolve and improve as higher quality data becomes available.</p><p data-block-key="33jsp">To make AI-powered weather forecasting more accessible, we’ve <a href="https://github.com/google-deepmind/graphcast" rel="noopener" target="_blank">open sourced our model’s code</a>. ECMWF is already <a href="https://charts.ecmwf.int/products/graphcast_medium-mslp-wind850" rel="noopener" target="_blank">experimenting with GraphCast’s 10-day forecasts</a> and we’re excited to see the possibilities it unlocks for researchers – from tailoring the model for particular weather phenomena to optimizing it for different parts of the world.</p><p data-block-key="8e2gf">GraphCast joins other state-of-the-art weather prediction systems from Google DeepMind and Google Research, including a regional <a href="https://deepmind.google/discover/blog/nowcasting-the-next-hour-of-rain/" rel="noopener" target="_blank">Nowcasting model</a> that produces forecasts up to 90 minutes ahead, and <a href="https://blog.research.google/2023/11/metnet-3-state-of-art-neural-weather.html" rel="noopener" target="_blank">MetNet-3</a>, a regional weather forecasting model already in operation across the US and Europe that produces more accurate 24-hour forecasts than any other system.</p><p data-block-key="au8nq">Pioneering the use of AI in weather forecasting will benefit billions of people in their everyday lives. But our wider research is not just about anticipating weather – it’s about understanding the broader patterns of our climate. By developing new tools and accelerating research, we hope AI can empower the global community to tackle our greatest environmental challenges.</p>
</div>
            
          
            
            
              
              


            
          
            
            
              
              



  
    
  

            
          
        </div>
      
    

    
  

  

  

  </article>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Backblaze Drive Stats for Q3 2023 (142 pts)]]></title>
            <link>https://www.backblaze.com/blog/backblaze-drive-stats-for-q3-2023/</link>
            <guid>38263435</guid>
            <pubDate>Tue, 14 Nov 2023 14:07:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.backblaze.com/blog/backblaze-drive-stats-for-q3-2023/">https://www.backblaze.com/blog/backblaze-drive-stats-for-q3-2023/</a>, See on <a href="https://news.ycombinator.com/item?id=38263435">Hacker News</a></p>
<div id="readability-page-1" class="page"><article aria-label="Backblaze Drive Stats for Q3 2023" itemscope="" itemtype="https://schema.org/CreativeWork"><div itemprop="text">
<figure><img loading="lazy" decoding="async" width="1024" height="583" src="https://wp-admin.backblaze.com/blog/wp-content/uploads/2023/11/bb-bh-Drive-Stats-Q3-2023-1024x583.png" alt="A decorative image showing the title Q3 2023 Drive Stats. " srcset="https://www.backblaze.com/blog/wp-content/uploads/2023/11/bb-bh-Drive-Stats-Q3-2023-1024x583.png 1024w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/bb-bh-Drive-Stats-Q3-2023-300x171.png 300w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/bb-bh-Drive-Stats-Q3-2023-768x437.png 768w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/bb-bh-Drive-Stats-Q3-2023-560x319.png 560w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>

<p>At the end of Q3 2023, Backblaze was monitoring 263,992 hard disk drives (HDDs) and solid state drives (SSDs) in our data centers around the world. Of that number, 4,459 are boot drives, with 3,242 being SSDs and 1,217 being HDDs. The failure rates for the SSDs are analyzed in the <a href="https://www.backblaze.com/blog/ssd-edition-2023-mid-year-drive-stats-review/" target="_blank" rel="noreferrer noopener">SSD Edition: 2023 Drive Stats review</a>.</p>
<p>That leaves us with 259,533 HDDs that we’ll focus on in this report. We’ll review the quarterly and lifetime failure rates of the data drives as of the end of Q3 2023. Along the way, we’ll share our observations and insights on the data presented, and, for the first time ever, we’ll reveal the drive failure rates broken down by data center.</p>
<h2>Q3 2023 Hard Drive Failure Rates</h2>
<p>At the end of Q3 2023, we were managing 259,533 hard drives used to store data. For our review, we removed 449 drives from consideration as they were used for testing purposes, or were drive models which did not have at least 60 drives. This leaves us with 259,084 hard drives grouped into 32 different models.&nbsp;</p>
<p>The table below reviews the annualized failure rate (AFR) for those drive models for the Q3 2023 time period.</p>
<figure><a href="https://www.backblaze.com/blog/wp-content/uploads/2023/11/1-Quarterly-AFR-Table-Updated-1.jpg" data-rel="lightbox-gallery-ht5dUJ2a" data-rl_title="1 – Quarterly AFR Table – Updated" data-rl_caption="1 – Quarterly AFR Table – Updated" title="1 – Quarterly AFR Table – Updated"><img loading="lazy" decoding="async" width="661" height="1024" src="https://www.backblaze.com/blog/wp-content/uploads/2023/11/1-Quarterly-AFR-Table-Updated-1-661x1024.jpg" alt="A table showing the quarterly annualized failure rates of Backblaze hard drives." srcset="https://www.backblaze.com/blog/wp-content/uploads/2023/11/1-Quarterly-AFR-Table-Updated-1-661x1024.jpg 661w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/1-Quarterly-AFR-Table-Updated-1-194x300.jpg 194w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/1-Quarterly-AFR-Table-Updated-1-560x868.jpg 560w" sizes="(max-width: 661px) 100vw, 661px"></a></figure>
<h3>Notes and Observations on the Q3 2023 Drive Stats</h3>
<ul>
<li><strong>The 22TB drives are here</strong>: At the bottom of the list you’ll see the WDC 22TB drives (model: WUH722222ALE6L4). A <a href="https://www.backblaze.com/blog/vault-cloud-storage-architecture/" target="_blank" rel="noreferrer noopener">Backblaze Vault</a> of 1,200 drives (plus four) is now operational. The 1,200 drives were installed on September 29, so they only have one day of service each in this report, but zero failures so far.</li>
<li><strong>The old get bolder</strong>: At the other end of the time-in-service spectrum are the 6TB Seagate drives (model: ST6000DX000) with an average of 101 months in operation. This cohort had zero failures in Q3 2023 with 883 drives and a lifetime AFR of 0.88%.</li>
<li><strong>Zero failures</strong>: In Q3, six different drive models managed to have zero drive failures during the quarter. But only the 6TB Seagate, noted above, had over 50,000 drive days, our minimum standard for ensuring we have enough data to make the AFR plausible.</li>
<li><strong>One failure</strong>: There were four drive models with one failure during Q3. After applying the 50,000 drive day metric, two drives stood out:
<ol>
<li>WDC 16TB (model: WUH721816ALE6L0) with a 0.15% AFR.</li>
<li>Toshiba 14TB (model: MG07ACA14TEY) with a 0.63% AFR.</li>
</ol>
</li>
</ul>
<h3>The Quarterly AFR Drops</h3>
<p>In Q3 2023, quarterly AFR for all drives was 1.47%. That was down from 2.2% in Q2 and also down from 1.65% a year ago. The quarterly AFR is based on just the data in that quarter, so it can often fluctuate from quarter to quarter.&nbsp;</p>
<p>In our Q2 2023 report, we suspected the 2.2% for the quarter was due to the overall aging of the drive fleet and in particular we pointed a finger at specific 8TB, 10TB, and 12TB drive models as potential culprits driving the increase. That prediction fell flat in Q3 as nearly two-thirds of drive models experienced a decreased AFR quarter over quarter from Q2 and any increases were minimal. This included our suspect 8TB, 10TB, and 12TB drive models.&nbsp;</p>
<p>It seems Q2 was an anomaly, but there was one big difference in Q3: we retired 4,585 aging 4TB drives. The average age of the retired drives was just over eight years, and while that was a good start, there’s another 28,963 4TB drives to go. To facilitate the continuous retirement of aging drives and make the data migration process easy and safe we use CVT, our awesome in-house data migration software which we’ll cover at another time.</p>
<h2>A Hot Summer and the Drive Stats Data</h2>
<p>As anyone should in our business, Backblaze continuously monitors our systems and drives. So, it was of little surprise to us when the folks at <a href="https://www.nasa.gov/news-release/nasa-announces-summer-2023-hottest-on-record/" target="_blank" rel="noreferrer noopener">NASA</a> confirmed the summer of 2023 as Earth’s hottest on record. The effects of this record-breaking summer showed up in our monitoring systems in the form of drive temperature alerts. A given drive in a storage server can heat up for many reasons: it is failing; a fan in the storage server has failed; other components are producing additional heat; the air flow is somehow restricted; and so on. Add in the fact that the ambient temperature within a data center often increases during the summer months, and you can get more temperature alerts.</p>
<p>In reviewing the temperature data for our drives in Q3, we noticed that a small number of drives exceeded the maximum manufacturer’s temperature for at least one day. The maximum temperature for most drives is 60°C, except for the 12TB, 14TB, and 16TB Toshiba drives which have a maximum temperature of 55°C. Of the 259,533 data drives in operation in Q3, there were 354 individual drives (0.0013%) that exceeded their maximum manufacturer temperature. Of those only two drives failed, leaving 352 drives which were still operational as of the end of Q3.</p>
<p>While temperature fluctuation is part of running data centers and temp alerts like these aren’t unheard of, our data center teams are looking into the root causes to ensure we’re prepared for the inevitability of increasingly hot summers to come.</p>
<h3>Will the Temperature Alerts Affect Drive Stats?</h3>
<p>The two drives which exceeded their maximum temperature and failed in Q3 have been removed from the Q3 AFR calculations. Both drives were 4TB Seagate drives (model: ST4000DM000). Given that the remaining 352 drives which exceeded their temperature maximum did not fail in Q3, we have left them in the Drive Stats calculations for Q3 as they did not increase the computed failure rates.</p>
<p>Beginning in Q4, we will remove the 352 drives from the regular Drive Stats AFR calculations and create a separate cohort of drives to track that we’ll name Hot Drives. This will allow us to track the drives which exceeded their maximum temperature and compare their failure rates to those drives which operated within the manufacturer’s specifications. While there are a limited number of drives in the Hot Drives cohort, it could give us some insight into whether drives being exposed to high temperatures could cause a drive to fail more often. This heightened level of monitoring will identify any increase in drive failures so that they can be detected and dealt with expeditiously.</p>
<h2>New Drive Stats Data Fields in Q3</h2>
<p>In Q2 2023, we introduced three new data fields that we started populating in the Drive Stats data we publish: <code>vault_id</code>, <code>pod_id</code>, and <code>is_legacy_format</code>. In Q3, we are adding three more fields into each drive records as follows:</p>
<ul>
<li><code>datacenter</code>: The Backblaze data center where the drive is installed, currently one of these values: <code>ams5</code>, <code>iad1</code>, <code>phx1</code>, <code>sac0</code>, and <code>sac2</code>.</li>
<li><code>cluster_id</code>: The name of a given collection of storage servers logically grouped together to optimize system performance. Note: At this time the <code>cluster_id</code> is not always correct, we are working on fixing that.&nbsp;</li>
<li><code>pod_slot_num</code>: The physical location of a drive within a storage server. The specific slot differs based on the storage server type and capacity: Backblaze (45 drives), Backblaze (60 drives), Dell (26 drives), or Supermicro (60 drives). We’ll dig into these differences in another post.</li>
</ul>
<p>With these additions, the new schema beginning in Q3 2023 is:</p>
<ul>
<li><code>date</code></li>
<li><code>serial_number</code></li>
<li><code>model</code></li>
<li><code>capacity_bytes</code></li>
<li><code>failure</code></li>
<li><strong><code>datacenter </code>(Q3)</strong></li>
<li><strong><code>cluster_id</code> (Q3)</strong></li>
<li><strong><code>vault_id </code>(Q2)</strong></li>
<li><strong><code>pod_id </code>(Q2)</strong></li>
<li><strong><code>pod_slot_num</code> (Q3)</strong></li>
<li><strong><code>is_legacy_format </code>(Q2)</strong></li>
<li><code>smart_1_normalized</code></li>
<li><code>smart_1_raw</code></li>
<li>The remaining SMART value pairs (as reported by each drive model)</li>
</ul>
<p>Beginning in Q3, these data data fields have been added to the <a href="https://www.backblaze.com/cloud-storage/resources/hard-drive-test-data/" target="_blank" rel="noreferrer noopener">publicly available Drive Stats files</a> that we publish each quarter.&nbsp;</p>
<h2>Failure Rates by Data Center</h2>
<p>Now that we have the data center for each drive we can compute the AFRs for the drives in each data center. Below you’ll find the AFR for each of five data centers for Q3 2023.</p>
<div>
<figure><a href="https://www.backblaze.com/blog/wp-content/uploads/2023/11/3-AFR-by-data-center-1.jpg" data-rel="lightbox-gallery-ht5dUJ2a" data-rl_title="3 – AFR by data center" data-rl_caption="3 – AFR by data center" title="3 – AFR by data center"><img loading="lazy" decoding="async" width="600" height="430" src="https://www.backblaze.com/blog/wp-content/uploads/2023/11/3-AFR-by-data-center-1.jpg" alt="" srcset="https://www.backblaze.com/blog/wp-content/uploads/2023/11/3-AFR-by-data-center-1.jpg 600w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/3-AFR-by-data-center-1-300x215.jpg 300w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/3-AFR-by-data-center-1-560x401.jpg 560w" sizes="(max-width: 600px) 100vw, 600px"></a></figure></div>
<h3>Notes and Observations</h3>
<ul>
<li><strong>Null?:</strong> The drives which reported a null or blank value for their data center are grouped in four Backblaze vaults. <a href="https://www.backblaze.com/blog/author/david-winings/" target="_blank" rel="noreferrer noopener">David</a>, the Senior Infrastructure Software Engineer for Drive Stats, <a href="https://www.backblaze.com/blog/overload-to-overhaul-how-we-upgraded-drive-stats-data/" target="_blank" rel="noreferrer noopener">described the process of how we gather all the parts of the Drive Stats data each day</a>. The TL:DR is that vaults can be too busy to respond at the moment we ask, and since the data center field is nice-to-have data, we get a blank field. We can go back a day or two to find the data center value, which we will do in the future when we report this data.</li>
<li><strong>sac0?</strong>: sac0 has the highest AFR of all of the data centers, but it also has the oldest drives—nearly twice as old, on average, versus the next closest in data center, sac2. As discussed previously, <a href="https://www.backblaze.com/blog/drive-failure-over-time-the-bathtub-curve-is-leaking/" target="_blank" rel="noreferrer noopener">drive failures do seem to follow the “bathtub curve”</a>, although recently we’ve seen the curve start out flatter. Regardless, as drive models age, they do generally fail more often. Another factor could be that sac0, and to a lesser extent sac2, has some of the oldest Storage Pods, including a handful of 45-drive units. We are in the process of using CVT to replace these older servers while migrating from 4TB to 16TB and larger drives.</li>
<li><strong>iad1</strong>: The iad data center is the foundation of our eastern region and has been growing rapidly since coming online about a year ago. The growth is a combination of new data and customers using our <a href="https://www.backblaze.com/blog/double-redundancy-support-compliance-and-more-with-cloud-replication-now-live/" target="_blank" rel="noreferrer noopener">cloud replication capability</a> to automatically make a copy of their data in another region.</li>
<li><strong>Q3 Data</strong>: This chart is for Q3 data only and includes all the data drives, including those with less than 60 drives per model. As we track this data over the coming quarters, we hope to get some insight into whether different data centers really have different drive failure rates, and, if so, why.</li>
</ul>
<h2>Lifetime Hard Drive Failure Rates</h2>
<p>As of September 30, 2023, we were tracking 259,084 hard drives used to store customer data. For our lifetime analysis, we collect the number of drive days and the number of drive failures for each drive beginning from the time a drive was placed into production in one of our data centers. We group these drives by model, then sum up the drive days and failures for each model over their lifetime. That chart is below.&nbsp;</p>
<figure><a href="https://www.backblaze.com/blog/wp-content/uploads/2023/11/4-Lifetime-AFR-Table-1.jpg" data-rel="lightbox-gallery-ht5dUJ2a" data-rl_title="4 – Lifetime AFR Table" data-rl_caption="4 – Lifetime AFR Table" title="4 – Lifetime AFR Table"><img loading="lazy" decoding="async" width="685" height="1024" src="https://www.backblaze.com/blog/wp-content/uploads/2023/11/4-Lifetime-AFR-Table-1-685x1024.jpg" alt="" srcset="https://www.backblaze.com/blog/wp-content/uploads/2023/11/4-Lifetime-AFR-Table-1-685x1024.jpg 685w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/4-Lifetime-AFR-Table-1-201x300.jpg 201w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/4-Lifetime-AFR-Table-1-560x838.jpg 560w" sizes="(max-width: 685px) 100vw, 685px"></a></figure>
<p>One of the most important columns on this chart is the confidence interval, which is the difference between the low and high AFR confidence levels calculated at 95%. The lower the value, the more certain we are of the AFR stated. We like a confidence interval to be 0.5% or less. When the confidence interval is higher, that is not necessarily bad, it just means we either need more data or the data is somewhat inconsistent.&nbsp;</p>
<p>The table below contains just those drive models which have a confidence interval of less than 0.5%. We have sorted the list by drive size and then by AFR.</p>
<figure><a href="https://www.backblaze.com/blog/wp-content/uploads/2023/11/5-Lifetime-AFR-by-CI-1.jpeg" data-rel="lightbox-gallery-ht5dUJ2a" data-rl_title="5 – Lifetime AFR by CI" data-rl_caption="5 – Lifetime AFR by CI" title="5 – Lifetime AFR by CI"><img loading="lazy" decoding="async" width="710" height="780" src="https://www.backblaze.com/blog/wp-content/uploads/2023/11/5-Lifetime-AFR-by-CI-1.jpeg" alt="" srcset="https://www.backblaze.com/blog/wp-content/uploads/2023/11/5-Lifetime-AFR-by-CI-1.jpeg 710w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/5-Lifetime-AFR-by-CI-1-273x300.jpeg 273w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/5-Lifetime-AFR-by-CI-1-560x615.jpeg 560w" sizes="(max-width: 710px) 100vw, 710px"></a></figure>
<p>The 4TB, 6TB, 8TB, and some of the 12TB drive models are no longer in production. The HGST 12TB models in particular can still be found, but they have been relabeled as Western Digital and given alternate model numbers. Whether they have materially changed internally is not known, at least to us.</p>
<p>One final note about the lifetime AFR data: you might have noticed the AFR for all of the drives hasn’t changed much from quarter to quarter. It has vacillated between 1.39% to 1.45% percent for the last two years. Basically, we have lots of drives with lots of time-in-service so it is hard to move the needle up or down. While the lifetime stats for individual drive models can be very useful, the lifetime AFR for all drives will probably get less and less interesting as we add more and more drives. Of course, a few hundred thousand drives that never fail could arrive, so we will continue to calculate and present the lifetime AFR.</p>
<h2>The Hard Drive Stats Data</h2>
<p>The complete data set used to create the information used in this review is available on our <a href="https://www.backblaze.com/cloud-storage/resources/hard-drive-test-data/">Hard Drive Stats Data webpage</a>. You can download and use this data for free for your own purpose. All we ask are three things: 1) you cite Backblaze as the source if you use the data, 2) you accept that you are solely responsible for how you use the data, and 3) you do not sell this data to anyone; it is free.&nbsp;</p>
<p>Good luck and let us know if you find anything interesting.</p>
</div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Lot of Damage in Grindavík (238 pts)]]></title>
            <link>https://icelandmonitor.mbl.is/news/news/2023/11/13/a_lot_of_damage_in_grindavik/</link>
            <guid>38263294</guid>
            <pubDate>Tue, 14 Nov 2023 13:54:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://icelandmonitor.mbl.is/news/news/2023/11/13/a_lot_of_damage_in_grindavik/">https://icelandmonitor.mbl.is/news/news/2023/11/13/a_lot_of_damage_in_grindavik/</a>, See on <a href="https://news.ycombinator.com/item?id=38263294">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p><a href="https://cdn.mbl.is/frimg/1/45/18/1451865.jpg" id="a1451865" target="_blank" title="Looking at the fissure." rel="newsimgs"><img src="https://cdn.mbl.is/frimg/1/45/18/1451865.jpg" alt="Looking at the fissure." width="730" height="486"></a>
    </p>
      <p>
        Looking at the fissure.
          <span>mbl.is/Eggert Jóhannesson</span>
      </p>
  </div><div>
        
  
    

        
          <p>
 A lot of damage can be seen around Grindavík after the earthquakes and the formation of a deformation that is moving downwards towards the magma intrusion that is underneath the ground. This depression formation is now estimated to be over 1.2 meters in the northwest end of Grindavík.
</p>

        
          <p>
 A fissure passes through a large part of the town and passes through the road to the sports center. A hot water pipe has been broken in the earthquakes and the sports center seems to be sitting on a meter high pedestal.
</p>

        
          <p data-slate-node="element">
 <span data-slate-node="text">
  <span data-slate-leaf="true">
   <span data-slate-string="true">
    A journalist and a photographer from mbl.is have been travelling around the town today looking at the situation and the damage in town.
   </span>
  </span>
 </span>
</p>

        
          <p data-slate-node="element">
 <span data-slate-node="text">
  <span data-slate-leaf="true">
   <span data-slate-string="true">
    As can be seen in these two pictures from the sports center, the power of the earthquakes has been substantial. The faulting is up to one meter and it is like the pavement at the front has been severed from the building.
   </span>
  </span>
 </span>
</p>

        
          
  
  

  



        
          
  
  

  



        
          <p data-slate-node="element">
 <span data-slate-node="text">
  <span data-slate-leaf="true">
   <span data-slate-string="true">
    A short distance from the sports center, a huge fissure can be seen through the road, and a more than meter-long opening stretches through a part of the town. It has, among other things, at one point broken a hot water pipe and hot air is steaming out of the fissure.
   </span>
  </span>
 </span>
</p>

        
          
  
  

  



        
          <p data-slate-node="element">
 <span data-slate-node="text">
  <span data-slate-leaf="true">
   <span data-slate-string="true">
    The grass is soft and yielding widely close to cracks when walking on it, indicating that there is a widespread lack of soil under the grass. Thus, a rescuer found himself stepping with one foot in one place through grass without being harmed. A journalist at mbl.is has therefore been directed to stay on the tarmac because of this weakness in the soil.
   </span>
  </span>
 </span>
</p>

        
          <p data-slate-node="element">
 <span data-slate-node="text">
  <span data-slate-leaf="true">
   <span data-slate-string="true">
    Today, residents have been allowed into town to retrieve the main necessities and valuable property. They have only a short time to do so. Initially, residents were required to accompany the rescue teams, but as the day went on, it was decided that people could enter in their own cars. However, there are security posts around town where rescue teams are located, pushing people. People had been made aware that they were only expected to be in their home for 5-7 minutes.
   </span>
  </span>
 </span>
</p>

        
          <p>
 Damages are visible on many houses, especially cracks in the concrete, but our journalist from mbl.is could not see any building that could be written off as completely ruined.
</p>
          
      
      
    

    

  

  

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lapce Editor 0.3 (148 pts)]]></title>
            <link>https://github.com/lapce/lapce/releases/tag/v0.3.0</link>
            <guid>38262775</guid>
            <pubDate>Tue, 14 Nov 2023 13:00:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/lapce/lapce/releases/tag/v0.3.0">https://github.com/lapce/lapce/releases/tag/v0.3.0</a>, See on <a href="https://news.ycombinator.com/item?id=38262775">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:lapce/lapce" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="e3DVR65w5hz69YvA3wjfhPHiqLSx6eARwKkcYP3vij6pvU1pWM2isVjdoFaRVV60Oih20G_kYsgNFP1oMtxwNQ" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="lapce/lapce" data-current-org="lapce" data-current-owner="" data-logged-in="false">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="feedback-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      
      
</modal-dialog></div>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="custom-scopes-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      
      
</modal-dialog></div>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Freleases%2Fshow&amp;source=header-repo&amp;source_repo=lapce%2Flapce" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/lapce/lapce/releases/tag/v0.3.0&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="e2ad5d9fcbff23b186413b8ae3446dbdc571904917e9c0e8b53856a2d76e4d08" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>/releases/show;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Blender 4.0 (497 pts)]]></title>
            <link>https://wiki.blender.org/wiki/Reference/Release_Notes/4.0</link>
            <guid>38262315</guid>
            <pubDate>Tue, 14 Nov 2023 12:11:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wiki.blender.org/wiki/Reference/Release_Notes/4.0">https://wiki.blender.org/wiki/Reference/Release_Notes/4.0</a>, See on <a href="https://news.ycombinator.com/item?id=38262315">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                      <h4>Download</h4>
                      <p>Get the latest Blender, older versions, or experimental builds.</p>
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google Sues Men Who Weaponized DMCA Notices to Crush Competition (247 pts)]]></title>
            <link>https://torrentfreak.com/google-sues-men-who-weaponized-dmca-notices-to-crush-competition-231114/</link>
            <guid>38262124</guid>
            <pubDate>Tue, 14 Nov 2023 11:48:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://torrentfreak.com/google-sues-men-who-weaponized-dmca-notices-to-crush-competition-231114/">https://torrentfreak.com/google-sues-men-who-weaponized-dmca-notices-to-crush-competition-231114/</a>, See on <a href="https://news.ycombinator.com/item?id=38262124">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><a href="https://torrentfreak.com/images/dmca-google-s1.png"><img decoding="async" src="https://torrentfreak.com/images/dmca-google-s1.png" alt="dmca-google-s1" width="290" height="205" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20290%20205'%3E%3C/svg%3E" data-lazy-src="https://torrentfreak.com/images/dmca-google-s1.png"></a>While all non-compliant DMCA takedown notices are invalid by default, there’s a huge difference between those sent in error and others crafted for purely malicious purposes.</p>
<p>Bogus DMCA takedown notices are nothing new, but the rise of organized groups using malicious DMCA notices as a business tool has been apparent in recent years. </p>
<p>Since the vast majority of culprits facing zero consequences, that may have acted as motivation to send more. Through a lawsuit filed at a California court on Monday, Google appears to be sending the message that enough is enough.</p>
<h2>Defendants Weaponized DMCA Takedowns</h2>
<p>Google’s complaint targets Nguyen Van Duc and Pham Van Thien, both said to be residents of Vietnam and the leaders of up to 20 Doe defendants. Google says the defendants systematically abused accounts “to submit a barrage” of fraudulent copyright takedown requests aimed at removing their competitors’ website URLs from Google Search results.</p>
<p>“Defendants have weaponized copyright law’s notice-and-takedown process and used it not for its intended purpose of expeditiously removing infringing content, but instead to have the legitimate content of their competitors removed based on false allegations. Defendants’ illegal, fraudulent scheme harms consumers, third-party businesses, and Google; stifles competition; and threatens to tarnish Google’s trusted brand.”</p>
<p>Over the past few years, Nguyen, Pham and those working with them, are said to have created at least 65 Google accounts to send confirmed bogus notices targeting 117,000 URLs, plus another 500,000 URLs via notices that Google suspects are fraudulent too.</p>
<p>“Defendants appear to be connected with websites selling printed t-shirts, and their unlawful conduct aims to remove competing third-party sellers from Google Search results. Defendants have maliciously and illegally exploited Google’s policies and procedures under the DMCA to sabotage and harm their competitors,” the complaint adds.</p>
<h2>Google Aims to Put an End to Abuse, Hold Defendants Accountable</h2>
<p>Google goes on to highlight its position as a major intermediary that processes DMCA notices targeting 600 million URLs every year, and the requirement under the DMCA to remove or disable content notified as allegedly infringing. If the company fails to act expeditiously once in receipt of a DMCA notice that complies with the statutory requirements, the company risks losing its safe harbor protection, Google notes.</p>
<p>Since Google must often rely on the accuracy of statements made in DMCA notices, fraudulent notices can result in content being wrongfully taken down. That damages the company’s search engine advertising business, and the business Google’s customers hoped to attract. In this matter, the defendants’ embarked on a campaign that exploited Google’s systems and the DMCA takedown process to undermine their competitors.</p>
<h2>Fake Names, Fraudulent Representations</h2>
<p>The misrepresentations in notices sent to Google were potentially damaging to other parties too. Under fake names, the defendants falsely claimed to represent large companies such as Amazon, Twitter, and NBC News, plus sports teams including the Philadelphia Eagles, Los Angeles Lakers, San Diego Padres. </p>
<p>In similarly false notices, they claimed to represent famous individuals including Elon Musk, Taylor Swift, LeVar Burton, and Kanye West.</p>
<p>The complaint notes that some notices were submitted under company names that do not exist in the United States, at addresses where innocent families and businesses can be found. Google says that despite these claims, the defendants can be found in Vietnam from where they proudly advertise their ‘SEO’ scheme to others, including via YouTube. </p>
<center><a href="https://torrentfreak.com/images/Fake-SEO-Fake-DMCA.png"><img decoding="async" src="https://torrentfreak.com/images/Fake-SEO-Fake-DMCA.png" alt="Fake SEO Fake DMCA" width="610" height="514" srcset="https://torrentfreak.com/images/Fake-SEO-Fake-DMCA.png 610w, https://torrentfreak.com/images/Fake-SEO-Fake-DMCA-300x253.png 300w" sizes="(max-width: 610px) 100vw, 610px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20610%20514'%3E%3C/svg%3E" data-lazy-srcset="https://torrentfreak.com/images/Fake-SEO-Fake-DMCA.png 610w, https://torrentfreak.com/images/Fake-SEO-Fake-DMCA-300x253.png 300w" data-lazy-src="https://torrentfreak.com/images/Fake-SEO-Fake-DMCA.png"></a></center>
<p>“Bad actors like Defendants use this tactic to attack and fraudulently suppress competitors’ websites and products in Google Search results, making consumers more likely to buy the same or similar products from the bad actors or their affiliates,” the complaint continues.</p>
<p>“Such bad actors know that a fraudulent takedown request often has the same effect as a legitimate one; if a takedown request contains all the elements required under Section 512(c)(3)(A), it likely will trigger removal by Google.</p>
<p>“Unfortunately, to ensure compliance with the DMCA and in reliance on the information submitted in Defendants’ takedown requests, Google’s system removed a significant number of thirdparty website URLs targeted by Defendants for a period of time before Google and/or the websites’ owners figured out what was going on and took appropriate steps to reinstate the URLs.”</p>
<p>A particularly damaging batch of fraudulent notices targeted more than 35,000 URLs operated by a Google customer that spends tens of millions of dollars per year on Google search ads. The effect was a significant drop in traffic during the holiday season, revenue losses for the customer and its sellers of $5 million, and a loss to Google of between $2 and $3 million.</p>
<h2>Holding Defendants Accountable</h2>
<p>Those who knowingly make false statements in a DMCA notice can be held liable for damages, costs, and attorneys’ fees. In this matter the defendants’ conduct is said to have caused Google to suffer economic harm due to lost advertising revenue, damage to business relations, and the allocation of significant resources to investigate their wrongdoing. </p>
<p>Google seeks attorneys’ fees and damages under <a href="https://www.law.cornell.edu/uscode/text/17/512">17 U.S.C. §512(f)</a>, in an amount to be determined at trial.</p>
<p>The complaint adds that when the defendants created dozens of Google accounts, each time they entered into enforceable agreements with Google. While Google says it has “performed all its obligations” under those contracts, the actions of the defendants amount to breaches of their contractual obligations to Google and intentional interference in contractual relationships between Google and its advertising customers.</p>
<p>Google says the defendants should be required to pay all general, special, and actual damages that Google “has sustained or will sustain” due to the fraudulent notices.</p>
<p>Google further requests an order to restrain the defendants (and anyone working in concert with them), from submitting any further fraudulent takedown notices and/or creating any Gmail accounts. Google also wants a ban on the defendants using any of its products or services to advertise their websites or products. </p>
<p><em>The complaint is available <a href="https://torrentfreak.com/images/5-23-cv-05824-Google-v-Nguyen-Van-Duc-Pham-Van-Thien-complaint-231113.pdf">here</a> (pdf)</em></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rust Without Crates.io (161 pts)]]></title>
            <link>https://thomask.sdf.org/blog/2023/11/14/rust-without-crates-io.html</link>
            <guid>38261539</guid>
            <pubDate>Tue, 14 Nov 2023 10:34:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thomask.sdf.org/blog/2023/11/14/rust-without-crates-io.html">https://thomask.sdf.org/blog/2023/11/14/rust-without-crates-io.html</a>, See on <a href="https://news.ycombinator.com/item?id=38261539">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
    <p>Rust is a lovely programming language but I’ve never quite come to terms with crates.io, or any other of these language-specific repositories where everyone uploads and downloads code willy-nilly. I have several objections:</p>

<ul>
  <li>If crates.io goes down or access is otherwise disrupted then the Rust community will stop work. It is profoundly unresilient to have a single point of failure like this. Certainly some people will have <a href="https://doc.rust-lang.org/cargo/commands/cargo-vendor.html">vendored their deps</a> and others will have a <a href="https://crates.io/crates/panamax"><code>panamax</code> mirror</a> handy, but for most, Rust as we know it stops if this one particular web service goes down.</li>
  <li>There is no mediation of any kind between when a new library/version is published and when it is consumed. You need only one author in your maybe-hundreds-of-dependencies tree to be hacked, coerced or in a malicious mood for you to have a <em>really</em> bad day.</li>
  <li>Any tampering with crates.io itself (espionage, disgruntlement, national security) could have an incredibly wide blast radius, or a incredibly wide set of targets from which to choose.</li>
  <li>Since crates.io is <em>the</em> source for crates, it is normal for both developers and CI machines to be hitting this web service all the time. Opportunities for mischief are exacerbated when clients are phoning home so frequently.</li>
</ul>

<p>So what’s the alternative? I think we all need to take a step back from the altar of developer velocity and take a deep breath. I don’t want dependencies hot off the press. Ideally I want someone independent of the authors playing a curatorial role.</p>

<p>Now, actually getting some human review of dependency updates is quite a hard thing to do. <a href="https://github.com/crev-dev/cargo-crev"><code>cargo-crev</code></a> has been trying for years to make this happen. I would love if it was the solution but it isn’t yet, and I think it’s a little ambitious. Yes we would like to have super-experienced software developers reviewing all our libraries with cryptographic stamps of approval, but if they’re not available we could be the target of remote shell in a <code>build.rs</code>. Surely there’s a middle ground here?</p>

<p>What’s interesting is that this problem is largely solved for C and C++: Linux distributions like Debian package such a wide range of libraries that for many things that you want to develop or install, you don’t need any third-party libraries <em>at all</em>. It’s just a matter of finding the right <code>apt-get</code> incantations and off you go. Even if you can get 95% of your libraries from a common trusted source then your risk is decreased considerably.</p>

<p>Rust libraries don’t work quite the same as C/C++ ones. Normal Rust code can’t be dynamically linked—a binary will have all of its dependencies statically linked at build time, so you won’t typically see <code>.so</code> files for Rust libraries that are going to be consumed by other Rust code. Since there is no <code>.so</code> file, Debian has no package that installs the library. However if they want to ship a binary that was written in Rust, their builders can’t just be downloading stuff from crates.io. They need a way to package all of the software that represents that Debian release. To solve that problem they’ve taken all these little dependencies and put their <em>full Rust source code</em> in packages with names like <code>librust-cratename-dev</code>.</p>

<p>Hmm, how many such packages? Running on trixie (testing)…</p>

<div><pre><code>$ aptitude search librust- | grep -vE "^v " | wc -l
2336
</code></pre></div>

<p>This is starting to look like a serious curation of the most important Rust crates, available from any Debian mirror. There are some double-ups to be sure, since in some cases multiple incompatible versions of the same crate had to be packaged. Still. Maybe there is enough Rust in Debian now that it’s viable to write interesting Rust software independently of crates.io? That would solve basically all my concerns and the situation is only going to improve as more Rust software gets packaged.</p>

<p>To be clear, I don’t expect that Debian Developers are auditing these packages in the manner of <code>cargo-crev</code>. The good thing is that they <em>don’t actually need to</em> for it to be a major improvement.</p>

<ul>
  <li>A DD isn’t going to upload a new patch release <em>just ‘cause</em>. It’s going to be because it has an important fix or because some other program has depended on it. On crates.io a maintainer is free to create new releases for any reason and <code>cargo update</code> is not going to evaluate how good that reason is.</li>
  <li>A simple time delay will allow egregious malware like malicious <code>build.rs</code> scripts to be caught, whether that’s the super-long Debian stable cycle or even the several days required to migrate from <em>unstable</em> to <em>testing</em>. I assume that an urgent security issue would be distributed the same as any other Debian update.</li>
  <li>They might decide to give the diff at least a cursory look, which is better than nothing.</li>
</ul>

<p>How do we do this? It’s actually quite easy because the big-brained Debian developers have arranged all the Rust dependencies to follow the format of a cargo <a href="https://doc.rust-lang.org/cargo/reference/source-replacement.html#directory-sources">Directory Source</a>. That is, all of the packages are installed in their own directories under <code>/usr/share/cargo/registry</code>, including implementing <a href="https://github.com/rust-lang/cargo/issues/11063">a cheeky workaround</a> for the required <code>.cargo-checksum.json</code> files.</p>

<p>You can then add some brief incantations to your <code>.cargo/config.toml</code>, whether on a project- or user-wide basis:</p>

<div><pre><code><span>[net]</span>
<span>offline</span> <span>=</span> <span>true</span>

<span>[source]</span>

<span>[source.apt]</span>
<span>directory</span> <span>=</span> <span>"/usr/share/cargo/registry"</span>

<span>[source.crates-io]</span>
<span>replace-with</span> <span>=</span> <span>"apt"</span>
</code></pre></div>

<p>This overrides the default crates.io source and ensures dependencies can only be fulfilled locally by installing the relevant packages. This happily doesn’t require any changes to your projects themselves—you just have to be careful to use versions in your <code>Cargo.toml</code> (and <code>Cargo.lock</code>) that are resolvable on Debian, since it is a subset of those available on the wider crates.io.</p>

<p>I am quite certain that Debian wouldn’t have enough coverage yet for the monorepo at work, but I gave this a go on my <a href="https://github.com/thombles/hashgood">one of my little CLI projects</a> that has half a dozen dependencies. Apart from having to downgrade <code>copypasta</code> from 0.8.2 to 0.8.1 in the <code>Cargo.lock</code>, this builds and runs just fine. What a treat.</p>

<p>This little investigation has given me much more confidence in using Rust generally into the future. I feared that the “grab any dependency version you like” approach facilitated by crates.io would render Rust impervious to any sort of curation effort, such that anyone who was serious about my earlier concerns would have to stick to a language used to the old ways like C++. Fortunately, Debian is here to prove me wrong. A+ work by their Rust packaging team.</p>

<p>All power to those who like to live on the edge; I’ll be over here trying to minimise different types of dependencies.</p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[EU Parliament Decides That Your Private Messages Must Not Be Scanned (539 pts)]]></title>
            <link>https://tuta.com/blog/chat-control</link>
            <guid>38261415</guid>
            <pubDate>Tue, 14 Nov 2023 10:17:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tuta.com/blog/chat-control">https://tuta.com/blog/chat-control</a>, See on <a href="https://news.ycombinator.com/item?id=38261415">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>Chat control - one of the worst EU plans that is also being described as a surveillance monster - must be stopped. And the
EU Parliament has just decided to do so! In a historic agreement on the EU Commission's Child Sexual Abuse Regulation (CSAR) the European
Parliament wants to remove chat control requirements and safeguard secure encryption. The decision came after extensive backlash
against the original proposal from technology and security experts, to international scientists and to citizens across Europe.
This is a great win for our right to privacy and for upholding our democratic values in Europe, but the fight continues!
</p><div><p>Today the EU Parliament decided on an <a href="https://www.patrick-breyer.de/wp-content/uploads/2023/11/CSAR_LIBE-Verhandlungsmandat.pdf">alternative version of chat control</a> - one that fortunately
does not deserve this name anymore: After huge opposition against the surveillance methods included in the CSA Regulation
(see 'Opposition against chat control' below), the EU Parliament has decided to uphold every citizen's right to privacy
and underlined the importance of upholding our democratic values. We in Europe must not follow autocratic regimes like
China and Russia by monitoring all our citizens.</p>
<p>Patrick Breyer, Member of the EU Parliament and part of the CSAR negotiations says:</p>
<blockquote>
<p>"Under the impression of massive protests against the looming indiscriminate chat control mass scanning of private messages, we managed to
win a broad majority for a different, new approach to protecting young people from abuse and exploitation online. As a pirate and digital
freedom fighter, I am proud of this breakthrough. The winners of this agreement are on the one hand our children, who will be protected much
more effectively and in a court-proof manner, and on the other hand all citizens, whose digital privacy of correspondence and communication security will be guaranteed."</p>
</blockquote>
<blockquote>
<p>"<strong>Even if this compromise, which is supported from the progressive to the conservative camp, is not perfect on all points, it is a historic
success that removing chat control and rescuing secure encryption is the common aim of the entire Parliament.</strong> We are doing the exact opposite of
most EU governments who want to destroy digital privacy of correspondence and secure encryption. Governments must finally accept that this highly
dangerous bill can only be fundamentally changed or not be passed at all. The fight against authoritarian chat control must be pursued with all determination!"</p>
</blockquote>
<h3 id="what-did-the-eu-parliament-decide">What did the EU Parliament decide?</h3>
<p>Breyer <a href="https://www.patrick-breyer.de/en/historic-agreement-on-child-sexual-abuse-proposal-csar-european-parliament-wants-to-remove-chat-control-and-safeguard-secure-encryption/">writes</a>
on his website that internet services and apps must be "secure by design and default". The EU Parliament has agreed to:</p>
<blockquote>
<p>"safeguard the digital secrecy of correspondence and remove the plans for blanket chat control, which violate fundamental
rights and stand no chance in court. The current voluntary chat control of private messages (not social networks) by US internet
companies is being phased out. Targeted telecommunication surveillance and searches will only be permitted with a
judicial warrant and only limited to persons or groups of persons suspected of being linked to child sexual abuse material."</p>
</blockquote>
<p><strong>A huge win for our privacy rights is also that the EU Parliament has decided to
"clearly exclude so-called client-side scanning".</strong></p>
<p>In contrast to the original chat control proposal, the version of the EU Parliament wants that a new EU Child Protection Centre
proactively searches publicly accessible parts of the internet for child sexual abuse material with automatic crawling, which can also
take place in darknet and would be much more efficient than private surveillance measures by providers. Found abuse material
must be reported and taken down by the provider.</p>
<h3 id="fight-is-not-over">Fight is not over</h3>
<p>While the EU Parliament's decision is a huge win, the fight is not over. It is expected that the EU Commission will continue to push for general
surveillance chat control measures. Now is the time for each and everyone of us to join this fight!</p>
<p><strong>You can help fight chat control and uphold our right to privacy. Check at the end of this post, what you can do!</strong></p>
<h2 id="opposition-against-chat-control">Opposition against chat control</h2>
<p>Chat control has been in discussion for along time already, and the criticism of this draft bill is huge. Significant is
not only that technology and security experts agree that <a href="https://tuta.com/blog/posts/eu-client-side-scanning">client-side scanning</a> is not possible without risking
everyone's security. Also scientists, the general public, even the EU's Research Service oppose the EU
Commission's chat control proposal.</p>
<h3 id="scientists-letter-to-eu-parliament">Scientists letter to EU Parliament</h3>
<p>300 scientists from all around the world have sent an open letter to the EU Parliament to call on policymakers
to <strong>stop chat control</strong>, the EU’s proposed Child Sexual Abuse Regulation. They say while it is the responsibility of politicians to protect children from sexual abuse,
"it is our professional recommendation as scientists that such a proposal be not taken forward" because the
scanning techniques the EU is proposing to use are deeply flawed and would endanger the security of everyone using the internet.</p>
<p>The scientists make the EU proposal look like wishful thinking: "Given the horrific nature of child sexual abuse, it is understandable,
and indeed tempting, to hope that there is a technological intervention that can eradicate it. Yet,
looking at the issue holistically, we cannot escape the conclusion that the current proposal is not such an intervention."</p>
<p>There is no magic key that allows the police to scan all chat messages, emails, and more for harmful content while not
risking the security and privacy of everyone. This is technically not possible.</p>
<p>The scientists argue that chat control is too much of a threat to everyone and therefore must be stopped:</p>
<blockquote>
<p>"First and foremost, we acknowledge that child sexual abuse and exploitation is a very serious crime which can cause
lifelong harm to survivors. It is the responsibility of government authorities, with the support of companies and communities,
to undertake effective interventions which prevent this crime and react to it quickly when it does happen."</p>
</blockquote>
<blockquote>
<p>"The European Commission has proposed a law with the stated aim of stopping the spread of child sexual abuse material
online and of grooming of children online. To do so, the law allows authorities to compel providers of any apps or other
online services to scan the messages, pictures, emails, voice mails and other activities of their users. In the case of end-to-end
encrypted apps, the claim is that this scanning can be done on users’ devices – so-called ‘Client-Side Scanning’ (CSS)."</p>
</blockquote>
<blockquote>
<p><strong>"Passing this legislation undermines the thoughtful and incisive work that European researchers have provided in cybersecurity
and privacy, including contributions to the development of global encryption standards. Such undermining will weaken the environment
for security and privacy work in Europe, lowering our ability to build a secure digital society."</strong></p>
</blockquote>
<blockquote>
<p>"The proposed regulation would also set a global precedent for filtering the Internet, controlling who can access it, and taking
away some of the few tools available for people to protect their right to a private life in the digital space. This will have a
chilling effect on society and is likely to negatively affect democracies across the globe."</p>
</blockquote>
<blockquote>
<p>"We therefore strongly warn against pursuing these or similar measures as their success is not possible given current
and foreseeable technology, while their potential for harm is substantial."</p>
</blockquote>
<p>You can read the full open letter <a href="https://docs.google.com/document/d/13Aeex72MtFBjKhExRTooVMWN9TC-pbH-5LEaAbMF91Y/mobilebasic">here</a>.</p>
<h3 id="eus-research-service-opposes-chat-control">EU's Research Service opposes chat control</h3>
<p>In April, the European Parliament's Research Service (EPRS) presented a new study on the legality of the proposed Child Sexual Abuse
Regulation, also called Chat Control.</p>
<p>The EU Commission's plans to fight images of abused children on the Internet are not very effective and violate the fundamental rights of Internet users,
according to this analysis on chat control. While the number of reported cases is likely to go up significantly, the accuracy of the hits is likely to also
decrease significantly, increasing the burden on investigative authorities.</p>
<h3 id="consequences-of-draft-eu-law">Consequences of draft EU law</h3>
<p>The legal experts of the EU Parliament's Scientific Service conclude that:</p>
<blockquote>
<p>"when weighing the fundamental rights affected by the measures of the CSA proposal, it can be established that the <strong>CSA
proposal would violate Articles 7 and 8 of the Charter of Fundamental Rights with regard to users.</strong>"</p>
</blockquote>
<p>The report also says if chat control becomes a law "that this violation of the
prohibition of <strong>general data retention</strong> and the prohibition of <strong>general surveillance obligations</strong> cannot be justified."</p>
<blockquote>
<p>"A detection order on the content of interpersonal data either on the device or the server will <strong>compromise the essence of
the right to privacy</strong> under Article 7 CFR in the form of confidentiality of telecommunications. It constitutes a form of
access on a generalised basis, pursuant to Schrems, where it involves an <strong>analysis of all communications</strong> going through the server.“</p>
</blockquote>
<p>The experts made clear that an "increase in the number of reported contents does not necessarily lead to a corresponding increase in investigations and prosecutions leading to better protection of children.
As long as the capacity of law enforcement agencies is limited to its current size, an increase in reports will make effective prosecution of depictions of abuse more difficult."</p>
<p>In addition, the study on chat control finds: "It is undisputed that children need to be protected from becoming victims of child abuse and depictions of abuse online... but they also need to be able
to enjoy the protection of fundamental rights as a basis for their development and transition into adulthood."</p>
<p>Pirate Party MEP Patrick Breyer, long-time opponent of mass scanning of private communications, comments:</p>
<p>"The EU Parliament's Scientific Service now confirms in crystal clear words what I and numerous human rights activists, law enforcement
officials, legal experts, abuse victims and child protection organisations have been warning about for a long time: the proposed general,
indiscriminate <strong><a href="https://tuta.com/blog/posts/eu-client-side-scanning">scanning of our private conversations and photos</a></strong> destroys the digital privacy of correspondence and violates our fundamental
rights. A flood of mostly false suspicious activity reports would make effective investigations more difficult, criminalise children en masse
and fail to bring the abusers and producers of such material to justice. According to this expertise, searching private communications for
potential child sexual exploitation material, known or unknown, is legally feasible only if the search provisions are targeted and limited
to persons presumably involved in such criminal activity."</p>
<p><strong>"What we really need instead of untargeted chat control and identification obligations for age verification is obliging law enforcement
agencies to have known exploitation material removed from the internet, as well as Europe-wide standards for effective prevention measures,
victim support and counselling, and for effective criminal investigations."</strong></p>
<p>This is also the view of many other experts, such as Mullvad, Edri and others.</p>
<h2 id="stop-chat-control">Stop chat control</h2>
<h3 id="mullvad-really-nails-it-with-their-campaign">Mullvad really nails it with their campaign!</h3>
<p><strong>Chat control is one of the worst EU plans to date and must be stopped. Mullvad VPN has recently launched a great campaign to fight for democracy.</strong></p>
<p>Mullvad's campaign, launched on March 3rd, calls on EU policy makers to stop chat control and rethink their stance in regards to the EU
Commission's proposal for detecting and prosecuting the sharing of child sexual abuse material (CSAM) via the internet.
The EU proposal includes far-reaching surveillance measures such as client-side scanning, which would force online services
to scan every chat message and every email that anybody in the European Union ever sends for child sexual abuse material.</p>
<p><strong>This legislation would de facto deprive EU citizens of any privacy on the Internet, it would even undermine encryption
and thus weaken the security of all Internet users.</strong></p>
<p>For that reason, the <a href="https://tuta.com/blog/posts/eu-csam-scanning">EU plans to scan for CSAM is heavily criticized</a>
by cryptography experts, human rights organizations as well as internet activists across Europe.</p>
<p>Most recently, Germany has made its opposition to <a href="https://tuta.com/blog/posts/eu-client-side-scanning">client-side scanning public</a>.
With resistance in Germany, Ireland, Austria and the Netherlands to the EU proposal,
a blocking minority is within reach.</p>
<h3 id="perfect-timing">Perfect timing</h3>
<p>Mullvad adds to the pressure with their new campaign, which was launched during the Swedish EU Presidency, which started on 1st of January 2023. The timing, thus,
couldn't be better.</p>
<p>Mullvad says on their campaign page:</p>
<blockquote>
<p><strong>Now is the time for debate and actions</strong></p>
</blockquote>
<blockquote>
<p>A democratic society is built upon discussions, before law proposals become reality. We started the conversation on the streets of Sweden, during the country’s EU presidency.</p>
</blockquote>
<p>Along with the digital campaign, they posted large billboards across Sweden to draw attention to the ongoing legal
debate on EU level.</p>
<blockquote><p dir="ltr" lang="en">The EU Commission wants to monitor all the citizens of the European union. The law proposal is called <a href="https://twitter.com/hashtag/chatcontrol?src=hash&amp;ref_src=twsrc%5Etfw">#chatcontrol</a> – and now is the time to stop it. We took the debate to the streets of Sweden, during the country’s EU-presidency. Take a look at <a href="https://t.co/Dx9cPe1ksq">https://t.co/Dx9cPe1ksq</a> <a href="https://t.co/FvqAlQRiig">pic.twitter.com/FvqAlQRiig</a></p>— Mullvad.net (@mullvadnet) <a href="https://twitter.com/mullvadnet/status/1631639744537870336?ref_src=twsrc%5Etfw">March 3, 2023</a></blockquote> 

<h3 id="opposition-to-chat-control">Opposition to chat control</h3>
<p>The digital rights organization EDRi has recently launched the 'Stop Scanning Me' campaign where EU citizens
can sign a petition against the EU's surveillance plan.</p>
<p><strong>Sign the Stop Scanning Me campaign <a href="https://civicrm.edri.org/stop-scanning-me">now</a>!</strong></p>
<h3 id="what-is-chat-control">What is chat control?</h3>
<p>The <a href="https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=COM%3A2022%3A209%3AFIN">Eu proposal</a> on chat control wants to force
online services to AI scan every message and every email for possible child grooming and child sexual abuse material (known and unknown).
Suspicious messages flagged by the AI will be reported to law enforcement and investigated.</p>
<p>Machine searching for potential child grooming and sexual abuse material is an artificial intelligence (AI) supported procedure.
The AI is not flawless and will flag a high number of harmless, private images, which will then be investigated by the police.
Experts expect that 10-20% of images reported will be false positives.</p>
<p>This is a huge intrusion into the privacy of millions of innocent citizens.</p>
<p><strong>The European Date Protection Supervisor Wiewiórowski <a href="https://www.euractiv.com/section/law-enforcement/news/eu-watchdog-online-child-abuse-draft-law-creates-illusion-of-legality/">calls it</a>
an 'illusion of legality': This type of indiscriminate scanning of private communications "will always be illegal under the Charter of
Fundamental Rights (and probably under several national constitutional laws as well)."</strong></p>
<h3 id="the-risks-of-chat-control">The risks of chat control</h3>
<p>To many the risks of chat control are negligible. After all, as law-abiding citizens what is there to fear?</p>
<p>But the truth is the opposite: The risks of a surveillance tool like chat control are unlimited.</p>
<h4 id="1-you-dont-know-whether-the-laws-will-change">1. You don't know whether the laws will change.</h4>
<p>Jan Penfrat said it perfectly on <a href="https://eupolicy.social/@ilumium/109972484325693478">Mastodon</a>:</p>
<p>"You have nothing to hide until the government suddenly declares your behaviour illegal."</p>
<p>
		<picture>
   			<source type="image/webp" srcset="https://tuta.com/blog/images/abortion-illegal-chatcontrol.webp">
    		<img height="854" width="1440" loading="lazy" alt="Chat control becomes very dangerous as soon as your behaviour is declared illegal. It must be stopped." src="https://tuta.com/blog/images/abortion-illegal-chatcontrol.jpg">
		</picture></p>
<p>The text on the image he posted is taken from news that broke this week via the
<a href="https://www.businessinsider.com/police-getting-help-social-media-to-prosecute-people-seeking-abortions-2023-2">Business Insider</a>:
"Police are prosecuting abortion seekers using their digital data — and Facebook and Google help them do it".</p>
<h4 id="2-compromised-encryption-is-not-encryption">2. Compromised encryption is not encryption</h4>
<p>Once you break encryption to allow access to the 'good guys', the security and privacy promised by encryption is gone.</p>
<p>It is simply not possible to implement an <a href="https://tuta.com/blog/posts/why-a-backdoor-is-a-security-risk">encryption backdoor</a> that can only be used by law enforcement.</p>
<p>This is also nicely illustrated by the <a href="https://tuta.com/blog/posts/encryption-backdoor-fails">best of backdoor fails in history</a>. The truth is: Secret services
have tried to undermine encryption before, but whenever they were successful, others were too. Malicious intruders have become
very powerful.</p>
<p><strong>We in Europe must not weaken the security backbone that our digital life depends on: Encryption.</strong></p>
<h3 id="lets-stop-client-side-scanning">Let's stop client-side scanning</h3>
<p>Now we, as citizens of Europe and members of the civil society, must put pressure on legislators to oppose legislation that
will put every email and every chat message that we send under constant surveillance.</p>
<p><strong>We can stop chat control together!</strong></p>
<ol>
<li><p>Share the <a href="https://mullvad.net/en/chatcontrol/campaign">Mullvad campaign</a> to increase the pressure on politicians.</p>
</li>
<li><p>Call/email your EU representative to make your voice heard: "Stop CSAM scanning. I do not want my personal device to become a surveillance machine!"</p>
</li>
<li><p>Sign the <a href="https://civicrm.edri.org/stop-scanning-me">Stop Scanning Me campaign</a>.</p>
</li>
</ol>
<p>Together we can stop chat control!</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Android App Devs now require 20 people to test before publishing to Play Store (267 pts)]]></title>
            <link>https://techcrunch.com/2023/11/09/google-play-tightens-up-rules-for-android-app-developers-to-require-testing-increased-app-review/</link>
            <guid>38258101</guid>
            <pubDate>Tue, 14 Nov 2023 02:13:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2023/11/09/google-play-tightens-up-rules-for-android-app-developers-to-require-testing-increased-app-review/">https://techcrunch.com/2023/11/09/google-play-tightens-up-rules-for-android-app-developers-to-require-testing-increased-app-review/</a>, See on <a href="https://news.ycombinator.com/item?id=38258101">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<p id="speakable-summary">Google today is <a href="http://android-developers.googleblog.com/2023/11/ensuring-high-quality-apps-on-google-play.html">announcing</a> strengthened protections for Android developers publishing apps to its Google Play store. The changes are a part of Google’s broader efforts at keeping low-quality and unsafe apps out of its app store and off consumers’ devices, which also recently included the launch of a new <a href="https://techcrunch.com/2023/11/04/google-play-android-real-time-app-scanning-sideload-apps/">real-time app scanning feature to combat malicious apps.</a> Today, the company says it will now require new Android developers with personal accounts to test their app with a minimum of 20 people for at least 2 weeks prior to publication. It additionally plans to increase its investment in the app review processes, warning of potential slowdowns in approvals for a small number of apps as these changes roll out.</p>
<p>According to Google, developers that use its testing tools have, on average, 3 times the amount of app installs and user engagement. That, of course, may not be a factor fully attributable to Google’s tools, but to the developers who would utilize such app testing tools before hitting publish. That is, they’re likely developing higher-quality applications. But now, app testing will no longer be optional for developers with newly created Play Console accounts, says Google.</p>
<p>Without providing an exact timeframe, Google says that new developers with individual accounts (as opposed to new Organization accounts) will be required to test apps with 20 people or more for 2 weeks or longer before publishing to production. The company believes this will help developers identify issues and bugs, and gain user feedback before their app’s launch. It says the requirement will arrive in the Play Console in the “coming days.”</p>
<p>Related to this, Google also plans to invest more heavily in its app review process, which, anecdotally, has long been considered to be less stringent than Apple’s with more reliance on automation over human review. Today, Google says its review teams will begin to spend more time assessing new apps to ensure policy compliance and that they don’t defraud users, including within the app or outside the Play Store.</p>
<p>This particular change follows an issue that’s impacted both app stores in India, specifically, where predatory lending apps have targeted financially insecure consumers, and then used unethical tactics to pressure borrowers to pay back debts. Apple <a href="https://techcrunch.com/2023/07/07/apple-purges-predatory-lending-apps-in-india-following-scrutiny/">this summer also had to sweep its App Store of these apps,</a> but Android is more popular in India, which means the issue more <a href="https://techcrunch.com/2022/08/26/loan-apps-abuse-harassment-suicide-indian-users-google-apple-india/">heavily impacts the Play Store.</a></p>
<p>However, Google is also taking aim at apps that ask for elevated permissions with the<a href="https://techcrunch.com/2023/03/08/googles-new-developer-preview-release-of-android-14-focuses-on-privacy-and-security/"> launch of Android 14</a>. With this release, developers can use more granular permission flow options, like asking only to access select photos or videos instead of the entirety of a user’s photo gallery.</p>
<p>As a result of the app review changes, Google warns it may take longer to review “a small portion of apps,” including those that require “certain device permissions” or those aimed at children.</p>
<p>The company announced a few other updates today, as well, including the ability for developers to choose their preferred deadline for meeting <a href="https://android-developers.googleblog.com/2023/07/boosting-trust-and-transparency-in-google-play.html">stricter verification requirements</a> associated with publishing on Google Play. Developers who don’t choose a timeframe before Feb. 29, 2024 will have their deadline set for them, Google says.</p>
<p>It also noted that, in addition to providing users with more information about which apps work well on their devices, and other efforts to highlight local and regional content, Google will add a badge identifying official government apps starting in 2024.</p>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Starship will attempt a launch this weekend (613 pts)]]></title>
            <link>https://www.fly.faa.gov/adv/adv_spt.jsp</link>
            <guid>38257794</guid>
            <pubDate>Tue, 14 Nov 2023 01:32:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.fly.faa.gov/adv/adv_spt.jsp">https://www.fly.faa.gov/adv/adv_spt.jsp</a>, See on <a href="https://news.ycombinator.com/item?id=38257794">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	
		<td><p>MESSAGE:&nbsp;</p></td>
		
		<td><pre>EVENT TIME: 14/0100 - AND LATER
___________________________________________________________________________


THERE WILL BE A FLIGHT EVALUATION EVENT IN AND AROUND N90 AIRSPACE IN THE
MORNING BETWEEN 1130Z - 1230Z WITH MINIMAL IMPACT EXPECTED. PLANNED
TERMINAL INITIATIVES CARRIED IN THE PLAN FOR EWR AND MSP DUE TO WIND, SOUTH
FLORIDA FOR THUNDERSTORMS, AND SFO FOR LOW CEILINGS AND VISIBILITY ALONG
WITH VIP TFR CONSTRAINTS. GULF ROUTE CLOSURES AND SWAP ACTIVITY FOR SOUTH
FLORIDA POSSIBLE DUE TO THUNDERSTORMS.
___________________________________________________________________________

STAFFING TRIGGER(S):
NONE

TERMINAL CONSTRAINT(S):
N90/MSP - WIND
SOUTH FLORIDA - CHC THUNDERSTORMS
SFO - LOW CEILINGS / LOW VISIBILITY
I90 - LOW CEILINGS
SFO/OAK - ASIA-PACIFIC ECONOMIC COOPERATION 2023 UNTIL 11/18/23
L30 - HIGH VOLUME OPERATIONS/LSV AIRSHOW THRU 11/21/23

TERMINAL ACTIVE: 
NONE

TERMINAL PLANNED:
AFTER 1500	-MIA/FLL/PBI GROUND STOP POSSIBLE
AFTER 1545	-SFO GROUND STOP/DELAY PROGRAM PROBABLE
AFTER 1730	-EWR GROUND STOP/DELAY PROGRAM POSSIBLE
AFTER 2200	-MSP GROUND STOP/DELAY PROGRAM POSSIBLE

ENROUTE CONSTRAINT(S): 
THUNDERSTORMS - ZHU/ZJX/ZMA
ZMA - CAPE A STARFIGHTER ATCAA SFC-FL360 1530Z-1630Z  /  1800Z-1900Z
ZMP - QWA - WATFORD CITY, ND ATCRB OTS 1600Z-2000Z

ENROUTE ACTIVE:
UNTIL 0200	-FCA001:N90_PREF-ROUTES

ENROUTE PLANNED: 
AFTER 1000	-N90 PREF ROUTES EXPECTED
AFTER 1300	-GULF ROUTE CLOSURES POSSIBLE
AFTER 1500	-MIA SWAP/ESCAPE ROUTES POSSIBLE

CDR/SWAP:
NONE

RUNWAY/EQUIPMENT/SYSTEM IMPACT REPORTS (SIRs):
TEB - RWY 01/19 CLOSED 11/14/23 1230Z-1700Z
MEM - RWY 18L/36R CLOSED 14/1300Z-14/2030Z
SDL - RWY 03/21 CLOSED NIGHTLY 0400-1300Z 11/13/23-11/17/23 
RDU - RWY 05L/23R CLOSED DAILY 0200-1030Z 11/13/23-11/18/23
TEB - RWY 01/19 CLOSED 11/14/23 1230Z-1700Z
IAH - RWY 15L/33R CLOSED UNTIL 11/18/23 1200Z
MIA - RWY 09/27 CLOSED 0300Z-1200Z NIGHTLY UNTIL 11/20/23 
BOS - RWY 15R/33L CLOSED UNTIL 11/25/23 2359Z
ORD - RWY 09C/27C CONSTRUCTION ACTIVITIES UNTIL 12/15/23 
L X - RWY 06L/24R CLOSED UNTIL 01/09/24 0830Z
PBI - RWY 14/32 CLOSED UNTIL 01/16/24 2359Z	
DFW - RWY 17R/35L CLOSED UNTIL 05/31/24 1200Z

AIRSPACE FLOW PROGRAM(S) ACTIVE:
NONE

AIRSPACE FLOW PROGRAM(S) PLANNED:
NONE

LAUNCH/REENTRY:
SPACE X - STARLINK 6-28 CAPE CANAVERAL SFS, FL
PRIMARY: 	11/17/23	0400Z-0831Z
BACKUP:		11/18/23	0400Z-0831Z
		11/19/23	0400Z-0831Z

SPACE X - STARLINK 7-7 VANDENBERG SFB, CA
PRIMARY:	11/17/23	0738Z-1204Z
BACKUP:		11/18/23	0716Z-1142Z
		11/19/23	0655Z-1121Z

SPACE X STARSHIP SUPER HEAVY FLT 2  BOCA CHICA, TX
PRIMARY:	11/17/23	1300Z-1720Z
BACKUP:		11/18/23	1300Z-1720Z
		11/19/23	1300Z-1720Z

FLIGHT CHECK(S):
AFTER 1130	-N90
AFTER 1400	-IND

VIP MOVEMENT(S):
AFTER 1500	-DEP ADW
AFTER 2100	-ARR SFO

AFTER 1600	-DEP ADW
AFTER 2100	-ARR SFO

NEXT PLANNING WEBINAR: 1215Z
140020-141059
23/11/14 00:20  DCCOPS.lxstn35&nbsp;</pre></td>
	 </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The laptop that won't die (174 pts)]]></title>
            <link>https://clivethompson.medium.com/the-laptop-that-wont-die-0c478c3fe46c</link>
            <guid>38257284</guid>
            <pubDate>Tue, 14 Nov 2023 00:33:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://clivethompson.medium.com/the-laptop-that-wont-die-0c478c3fe46c">https://clivethompson.medium.com/the-laptop-that-wont-die-0c478c3fe46c</a>, See on <a href="https://news.ycombinator.com/item?id=38257284">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><h2 id="0097">My $200, 12-year-old Thinkpad has outlasted two high-end Macbooks</h2><div><a rel="noopener follow" href="https://clivethompson.medium.com/?source=post_page-----0c478c3fe46c--------------------------------"><div aria-hidden="false"><p><img alt="Clive Thompson" src="https://miro.medium.com/v2/resize:fill:88:88/1*C6KlQUX7cSZiV7VlS12Vyw.jpeg" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a></div></div><figure><figcaption>My 2011 T420 Thinkpad</figcaption></figure><p id="9cfe">That laptop above?</p><p id="3253">It’s the most indestructible, nonstop, won’t-die computer I’ve ever owned.</p><p id="838c">Full. Stop.</p><p id="d4a9">I’m going to write a whole damn post about it but the tl;dr is …</p><p id="d85d">If you want a modern, sexy, lightweight, high-powered laptop? Go get something pricey from Apple or Microsoft.</p><p id="5dd6">But if you want something that’ll cost almost no money and keep working until the sun explodes?</p><p id="6ff0">Get an old, used Thinkpad.</p><p id="9f6c">Allow me to unpack this …</p><p id="d673">My <em>hardware tale </em>begins a week ago when I was working on my Macbook Pro.</p><p id="2a90">It’s my main laptop, which I bought in 2017. I needed a machine that a) could run Logic Pro (the finest music-editing software available to humanity), b) had a high-resolution screen for my lousy eyesight, and c) had a 1-terabyte hard drive. It was the only machine that fit the bill.</p><p id="5381">It was super expensive, but my goal with laptops is to buy something with sufficiently excellent build-quality that it’ll last for years. I also hate e-waste, so I try to fix my laptops to keep them going as long as I can. I bought my first-ever Macbook Pro in 2010, and I got seven years out of it — including replacing a fried motherboard (thankfully just before the three-year warranty ended, so: It was covered! Woo)</p><p id="de81">This newer, 2017 Macbook Pro? I’d also had it fixed a few times before. It had gotten water damage in an accident, which required some internal work. I’d had <a rel="noopener" href="https://clivethompson.medium.com/ive-typed-22-million-keystrokes-on-apple-s-horrid-butterfly-keyboard-7d441dfadc15">the loathsomely awful “butterfly” keyboard replaced when it died</a>. I’d replaced the battery twice.</p><p id="1a72">But after six years, it was still chugging along!</p><p id="08bd">Until last week, when out of nowhere it went kaput.</p><p id="47a7">I was working on the Macbook, and closed the lid to have lunch. When I opened it again 15 minutes later, the machine had shut down. Nothing I did could coax it back to life.</p><p id="c229">So I jumped on my bike and brought it to a local laptop repair place. A few hours later, the technician texted me to explain what had gone kablooey. Apparently there was an electrical…</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A coder considers the waning days of the craft (603 pts)]]></title>
            <link>https://www.newyorker.com/magazine/2023/11/20/a-coder-considers-the-waning-days-of-the-craft</link>
            <guid>38257094</guid>
            <pubDate>Tue, 14 Nov 2023 00:08:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.newyorker.com/magazine/2023/11/20/a-coder-considers-the-waning-days-of-the-craft">https://www.newyorker.com/magazine/2023/11/20/a-coder-considers-the-waning-days-of-the-craft</a>, See on <a href="https://news.ycombinator.com/item?id=38257094">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>I have always taken it for granted that, just as my parents made sure that I could read and write, I would make sure that my kids could program computers. It is among the newer arts but also among the most essential, and ever more so by the day, encompassing everything from filmmaking to physics. Fluency with code would round out my children’s literacy—and keep them employable. But as I write this my wife is pregnant with our first child, due in about three weeks. I code professionally, but, by the time that child can type, coding as a valuable skill might have faded from the world.</p><p>I first began to believe this on a Friday morning this past summer, while working on a small hobby project. A few months back, my friend Ben and I had resolved to create a <em>Times</em>-style crossword puzzle entirely by computer. In 2018, we’d made a Saturday puzzle with the help of software and were surprised by how little we contributed—just applying our taste here and there. Now we would attempt to build a crossword-making program that didn’t require a human touch.</p><p>When we’ve taken on projects like this in the past, they’ve had both a hardware component and a software component, with Ben’s strengths running toward the former. We once made a neon sign that would glow when the subway was approaching the stop near our apartments. Ben bent the glass and wired up the transformer’s circuit board. I wrote code to process the transit data. Ben has some professional coding experience of his own, but it was brief, shallow, and now about twenty years out of date; the serious coding was left to me. For the new crossword project, though, Ben had introduced a third party. He’d signed up for a ChatGPT Plus subscription and was using GPT-4 as a coding assistant.</p><p>Something strange started happening. Ben and I would talk about a bit of software we wanted for the project. Then, a shockingly short time later, Ben would deliver it himself. At one point, we wanted a command that would print a hundred random lines from a dictionary file. I thought about the problem for a few minutes, and, when thinking failed, tried Googling. I made some false starts using what I could gather, and while I did my thing—programming—Ben told GPT-4 what he wanted and got code that ran perfectly.</p><p>Fine: commands like those are notoriously fussy, and everybody looks them up anyway. It’s not real programming. A few days later, Ben talked about how it would be nice to have an iPhone app to rate words from the dictionary. But he had no idea what a pain it is to make an iPhone app. I’d tried a few times and never got beyond something that half worked. I found Apple’s programming environment forbidding. You had to learn not just a new language but a new program for editing and running code; you had to learn a zoo of “U.I. components” and all the complicated ways of stitching them together; and, finally, you had to figure out how to package the app. The mountain of new things to learn never seemed worth it. The next morning, I woke up to an app in my in-box that did exactly what Ben had said he wanted. It worked perfectly, and even had a cute design. Ben said that he’d made it in a few hours. GPT-4 had done most of the heavy lifting.</p><p>By now, most people have had experiences with A.I. Not everyone has been impressed. Ben recently said, “I didn’t start really respecting it until I started having it write code for me.” I suspect that non-programmers who are skeptical by nature, and who have seen ChatGPT turn out wooden prose or bogus facts, are still underestimating what’s happening.</p><p>Bodies of knowledge and skills that have traditionally taken lifetimes to master are being swallowed at a gulp. Coding has always felt to me like an endlessly deep and rich domain. Now I find myself wanting to write a eulogy for it. I keep thinking of Lee Sedol. Sedol was one of the world’s best Go players, and a national hero in South Korea, but is now best known for losing, in 2016, to a computer program called AlphaGo. Sedol had walked into the competition believing that he would easily defeat the A.I. By the end of the days-long match, he was proud of having eked out a single game. As it became clear that he was going to lose, Sedol said, in a press conference, “I want to apologize for being so powerless.” He retired three years later. Sedol seemed weighed down by a question that has started to feel familiar, and urgent: What will become of this thing I’ve given so much of my life to?</p><p>My first enchantment with computers came when I was about six years old, in Montreal in the early nineties, playing Mortal Kombat with my oldest brother. He told me about some “fatalities”—gruesome, witty ways of killing your opponent. Neither of us knew how to inflict them. He dialled up an FTP server (where files were stored) in an MS-DOS terminal and typed obscure commands. Soon, he had printed out a page of codes—instructions for every fatality in the game. We went back to the basement and exploded each other’s heads.</p><p>I thought that my brother was a hacker. Like many programmers, I dreamed of breaking into and controlling remote systems. The point wasn’t to cause mayhem—it was to find hidden places and learn hidden things. “My crime is that of curiosity,” goes “The Hacker’s Manifesto,” written in 1986 by Loyd Blankenship. My favorite scene from the 1995 movie “Hackers” is when Dade Murphy, a newcomer, proves himself at an underground club. Someone starts pulling a rainbow of computer books out of a backpack, and Dade recognizes each one from the cover: the green book on international Unix environments; the red one on N.S.A.-trusted networks; the one with the pink-shirted guy on I.B.M. PCs. Dade puts his expertise to use when he turns on the sprinkler system at school, and helps right the ballast of an oil tanker—all by tap-tapping away at a keyboard. The lesson was that knowledge is power.</p><p>But how do you actually learn to hack? My family had settled in New Jersey by the time I was in fifth grade, and when I was in high school I went to the Borders bookstore in the Short Hills mall and bought “Beginning Visual C++,” by Ivor Horton. It ran to twelve hundred pages—my first grimoire. Like many tutorials, it was easy at first and then, suddenly, it wasn’t. Medieval students called the moment at which casual learners fail the <em>pons asinorum</em>, or “bridge of asses.” The term was inspired by Proposition 5 of Euclid’s Elements I, the first truly difficult idea in the book. Those who crossed the bridge would go on to master geometry; those who didn’t would remain dabblers. Section 4.3 of “Beginning Visual C++,” on “Dynamic Memory Allocation,” was my bridge of asses. I did not cross.</p><p>But neither did I drop the subject. I remember the moment things began to turn. I was on a long-haul flight, and I’d brought along a boxy black laptop and a CD-<em>ROM</em> with the Borland C++ compiler. A compiler translates code you write into code that the machine can run; I had been struggling for days to get this one to work. By convention, every coder’s first program does nothing but generate the words “Hello, world.” When I tried to run my version, I just got angry error messages. Whenever I fixed one problem, another cropped up. I had read the “Harry Potter” books and felt as if I were in possession of a broom but had not yet learned the incantation to make it fly. Knowing what might be possible if I did, I kept at it with single-minded devotion. What I learned was that programming is not really about knowledge or skill but simply about patience, or maybe obsession. Programmers are people who can endure an endless parade of tedious obstacles. Imagine explaining to a simpleton how to assemble furniture over the phone, with no pictures, in a language you barely speak. Imagine, too, that the only response you ever get is that you’ve suggested an absurdity and the whole thing has gone awry. All the sweeter, then, when you manage to get something assembled. I have a distinct memory of lying on my stomach in the airplane aisle, and then hitting Enter one last time. I sat up. The computer, for once, had done what I’d told it to do. The words “Hello, world” appeared above my cursor, now in the computer’s own voice. It seemed as if an intelligence had woken up and introduced itself to me.</p><p>Most of us never became the kind of hackers depicted in “Hackers.” To “hack,” in the parlance of a programmer, is just to tinker—to express ingenuity through code. I never formally studied programming; I just kept messing around, making computers do helpful or delightful little things. In my freshman year of college, I knew that I’d be on the road during the third round of the 2006 Masters Tournament, when Tiger Woods was moving up the field, and I wanted to know what was happening in real time. So I made a program that scraped the leaderboard on pgatour.com and sent me a text message anytime he birdied or bogeyed. Later, after reading “Ulysses” in an English class, I wrote a program that pulled random sentences from the book, counted their syllables, and assembled haikus—a more primitive regurgitation of language than you’d get from a chatbot these days, but nonetheless capable, I thought, of real poetry:</p><blockquote><p>I’ll flay him alive<br>Uncertainly he waited<br>Heavy of the past</p></blockquote></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>I began taking coding seriously. I offered to do programming for a friend’s startup. The world of computing, I came to learn, is vast but organized almost geologically, as if deposited in layers. From the Web browser down to the transistor, each sub-area or system is built atop some other, older sub-area or system, the layers dense but legible. The more one digs, the more one develops what the race-car driver Jackie Stewart called “mechanical sympathy,” a sense for the machine’s strengths and limits, of what one could make it do.</p><p>At my friend’s company, I felt my mechanical sympathy developing. In my sophomore year, I was watching “Jeopardy!” with a friend when he suggested that I make a playable version of the show. I thought about it for a few hours before deciding, with much disappointment, that it was beyond me. But when the idea came up again, in my junior year, I could see a way through it. I now had a better sense of what one could do with the machine. I spent the next fourteen hours building the game. Within weeks, playing “Jimbo Jeopardy!” had become a regular activity among my friends. The experience was profound. I could understand why people poured their lives into craft: there is nothing quite like watching someone enjoy a thing you’ve made.</p><p>In the midst of all this, I had gone full “Paper Chase” and begun ignoring my grades. I worked voraciously, just not on my coursework. One night, I took over a half-dozen machines in a basement computer lab to run a program in parallel. I laid printouts full of numbers across the floor, thinking through a pathfinding algorithm. The cost was that I experienced for real that recurring nightmare in which you show up for a final exam knowing nothing of the material. (Mine was in Real Analysis, in the math department.) In 2009, during the most severe financial crisis in decades, I graduated with a 2.9 G.P.A.</p><p>And yet I got my first full-time job easily. I had work experience as a programmer; nobody asked about my grades. For the young coder, these were boom times. Companies were getting into bidding wars over top programmers. Solicitations for experienced programmers were so aggressive that they complained about “recruiter spam.” The popularity of university computer-science programs was starting to explode. (My degree was in economics.) Coding “boot camps” sprang up that could credibly claim to turn beginners into high-salaried programmers in less than a year. At one of my first job interviews, in my early twenties, the C.E.O. asked how much I thought I deserved to get paid. I dared to name a number that faintly embarrassed me. He drew up a contract on the spot, offering ten per cent more. The skills of a “software engineer” were vaunted. At one company where I worked, someone got in trouble for using HipChat, a predecessor to Slack, to ask one of my colleagues a question. “Never HipChat an engineer directly,” he was told. We were too important for that.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>This was an era of near-zero interest rates and extraordinary tech-sector growth. Certain norms were established. Companies like Google taught the industry that coders were to have free espresso and catered hot food, world-class health care and parental leave, on-site gyms and bike rooms, a casual dress code, and “twenty-per-cent time,” meaning that they could devote one day a week to working on whatever they pleased. Their skills were considered so crucial and delicate that a kind of superstition developed around the work. For instance, it was considered foolish to estimate how long a coding task might take, since at any moment the programmer might turn over a rock and discover a tangle of bugs. Deadlines were anathema. If the pressure to deliver ever got too intense, a coder needed only to speak the word “burnout” to buy a few months.</p><p>From the beginning, I had the sense that there was something wrongheaded in all this. Was what we did really so precious? How long could the boom last? In my teens, I had done a little Web design, and, at the time, that work had been in demand and highly esteemed. You could earn thousands of dollars for a project that took a weekend. But along came tools like Squarespace, which allowed pizzeria owners and freelance artists to make their own Web sites just by clicking around. For professional coders, a tranche of high-paying, relatively low-effort work disappeared.</p><figure><p><span><div data-attr-viewport-monitor=""><a data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.newyorker.com/cartoon/a27287&quot;}" href="https://www.newyorker.com/cartoon/a27287" rel="nofollow noopener" target="_blank"><picture></picture></a><p><span>“I should have known he has absolutely no morals—I’ve seen how he loads a dishwasher.”</span></p><p><span>Cartoon by Hartley Lin</span></p></div></span></p></figure><p>The response from the programmer community to these developments was just, Yeah, you have to keep levelling up your skills. Learn difficult, obscure things. Software engineers, as a species, love automation. Inevitably, the best of them build tools that make other kinds of work obsolete. This very instinct explained why we were so well taken care of: code had immense leverage. One piece of software could affect the work of millions of people. Naturally, this sometimes displaced programmers themselves. We were to think of these advances as a tide coming in, nipping at our bare feet. So long as we kept learning we would stay dry. Sound advice—until there’s a tsunami.</p><p>When we were first allowed to use A.I. chatbots at work, for programming assistance, I studiously avoided them. I expected that my colleagues would, too. But soon I started seeing the telltale colors of an A.I. chat session—the zebra pattern of call-and-response—on programmers’ screens as I walked to my desk. A common refrain was that these tools made you more productive; in some cases, they helped you solve problems ten times faster.</p><p>I wasn’t sure I wanted that. I enjoy the act of programming and I like to feel useful. The tools I’m familiar with, like the text editor I use to format and to browse code, serve both ends. They enhance my practice of the craft—and, though they allow me to deliver work faster, I still feel that I deserve the credit. But A.I., as it was being described, seemed different. It provided a <em>lot</em> of help. I worried that it would rob me of both the joy of working on puzzles and the satisfaction of being the one who solved them. I could be infinitely productive, and all I’d have to show for it would be the products themselves.</p><p>The actual work product of most programmers is rarely exciting. In fact, it tends to be almost comically humdrum. A few months ago, I came home from the office and told my wife about what a great day I’d had wrestling a particularly fun problem. I was working on a program that generated a table, and someone had wanted to add a header that spanned more than one column—something that the custom layout engine we’d written didn’t support. The work was urgent: these tables were being used in important documents, wanted by important people. So I sequestered myself in a room for the better part of the afternoon. There were lots of lovely sub-problems: How should I allow users of the layout engine to convey that they want a column-spanning header? What should <em>their</em> code look like? And there were fiddly details that, if ignored, would cause bugs. For instance, what if one of the columns that the header was supposed to span got dropped because it didn’t have any data? I knew it was a good day because I had to pull out pen and pad—I was drawing out possible scenarios, checking and double-checking my logic.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>But taking a bird’s-eye view of what happened that day? A table got a new header. It’s hard to imagine anything more mundane. For me, the pleasure was entirely in the process, not the product. And what would become of the process if it required nothing more than a three-minute ChatGPT session? Yes, our jobs as programmers involve many things besides literally writing code, such as coaching junior hires and designing systems at a high level. But coding has always been the root of it. Throughout my career, I have been interviewed and selected precisely for my ability to solve fiddly little programming puzzles. Suddenly, this ability was less important.</p><p>I had gathered as much from Ben, who kept telling me about the spectacular successes he’d been having with GPT-4. It turned out that it was not only good at the fiddly stuff but also had the qualities of a senior engineer: from a deep well of knowledge, it could suggest ways of approaching a problem. For one project, Ben had wired a small speaker and a red L.E.D. light bulb into the frame of a portrait of King Charles, the light standing in for the gem in his crown; the idea was that when you entered a message on an accompanying Web site the speaker would play a tune and the light would flash out the message in Morse code. (This was a gift for an eccentric British expat.) Programming the device to fetch new messages eluded Ben; it seemed to require specialized knowledge not just of the microcontroller he was using but of Firebase, the back-end server technology that stored the messages. Ben asked me for advice, and I mumbled a few possibilities; in truth, I wasn’t sure that what he wanted would be possible. Then he asked GPT-4. It told Ben that Firebase had a capability that would make the project much simpler. Here it was—and here was some code to use that would be compatible with the microcontroller.</p><p>Afraid to use GPT-4 myself—and feeling somewhat unclean about the prospect of paying OpenAI twenty dollars a month for it—I nonetheless started probing its capabilities, via Ben. We’d sit down to work on our crossword project, and I’d say, “Why don’t you try prompting it this way?” He’d offer me the keyboard. “No, you drive,” I’d say. Together, we developed a sense of what the A.I. could do. Ben, who had more experience with it than I did, seemed able to get more out of it in a stroke. As he later put it, his own neural network had begun to align with GPT-4’s. I would have said that he had achieved mechanical sympathy. Once, in a feat I found particularly astonishing, he had the A.I. build him a Snake game, like the one on old Nokia phones. But then, after a brief exchange with GPT-4, he got it to modify the game so that when you lost it would show you how far you strayed from the most efficient route. It took the bot about ten seconds to achieve this. It was a task that, frankly, I was not sure I could do myself.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>In chess, which for decades now has been dominated by A.I., a player’s only hope is pairing up with a bot. Such half-human, half-A.I. teams, known as centaurs, might still be able to beat the best humans and the best A.I. engines working alone. Programming has not yet gone the way of chess. But the centaurs have arrived. GPT-4 on its own is, for the moment, a worse programmer than I am. Ben is much worse. But Ben plus GPT-4 is a dangerous thing.</p><p>It wasn’t long before I caved. I was making a little search tool at work and wanted to highlight the parts of the user’s query that matched the results. But I was splitting up the query by words in a way that made things much more complicated. I found myself short on patience. I started thinking about GPT-4. Perhaps instead of spending an afternoon programming I could spend some time “prompting,” or having a conversation with an A.I.</p><p>In a 1978 essay titled “On the Foolishness of ‘Natural Language Programming,’&nbsp;” the computer scientist Edsger&nbsp;W. Dijkstra argued that if you were to instruct computers not in a specialized language like C++ or Python but in your native tongue you’d be rejecting the very precision that made computers useful. Formal programming languages, he wrote, are “an amazingly effective tool for ruling out all sorts of nonsense that, when we use our native tongues, are almost impossible to avoid.” Dijkstra’s argument became a truism in programming circles. When the essay made the rounds on Reddit in 2014, a top commenter wrote, “I’m not sure which of the following is scariest. Just how trivially obvious this idea is” or the fact that “many still do not know it.”</p><p>When I first used GPT-4, I could see what Dijkstra was talking about. You can’t just say to the A.I., “Solve my problem.” That day may come, but for now it is more like an instrument you must learn to play. You have to specify what you want carefully, as though talking to a beginner. In the search-highlighting problem, I found myself asking GPT-4 to do too much at once, watching it fail, and then starting over. Each time, my prompts became less ambitious. By the end of the conversation, I wasn’t talking about search or highlighting; I had broken the problem into specific, abstract, unambiguous sub-problems that, together, would give me what I wanted.</p><p>Having found the A.I.’s level, I felt almost instantly that my working life had been transformed. Everywhere I looked I could see GPT-4-size holes; I understood, finally, why the screens around the office were always filled with chat sessions—and how Ben had become so productive. I opened myself up to trying it more often.</p><p>I returned to the crossword project. Our puzzle generator printed its output in an ugly text format, with lines like <code>"s""c""a""r""*""k""u""n""i""s""*" "a""r""e""a"</code>. I wanted to turn output like that into a pretty Web page that allowed me to explore the words in the grid, showing scoring information at a glance. But I knew the task would be tricky: each letter had to be tagged with the words it belonged to, both the across and the down. This was a detailed problem, one that could easily consume the better part of an evening. With the baby on the way, I was short on free evenings. So I began a conversation with GPT-4. Some back-and-forth was required; at one point, I had to read a few lines of code myself to understand what it was doing. But I did little of the kind of thinking I once believed to be constitutive of coding. I didn’t think about numbers, patterns, or loops; I didn’t use my mind to simulate the activity of the computer. As another coder, Geoffrey Litt, wrote after a similar experience, “I never engaged my detailed programmer brain.” So what <em>did</em> I do?</p><p>Perhaps what pushed Lee Sedol to retire from the game of Go was the sense that the game had been forever cheapened. When I got into programming, it was because computers felt like a form of magic. The machine gave you powers but required you to study its arcane secrets—to learn a spell language. This took a particular cast of mind. I felt selected. I devoted myself to tedium, to careful thinking, and to the accumulation of obscure knowledge. Then, one day, it became possible to achieve many of the same ends without the thinking and without the knowledge. Looked at in a certain light, this can make quite a lot of one’s working life seem like a waste of time.</p><p>But whenever I think about Sedol I think about chess. After machines conquered that game, some thirty years ago, the fear was that there would be no reason to play it anymore. Yet chess has never been more popular—A.I. has enlivened the game. A friend of mine picked it up recently. At all hours, he has access to an A.I. coach that can feed him chess problems just at the edge of his ability and can tell him, after he’s lost a game, exactly where he went wrong. Meanwhile, at the highest levels, grandmasters study moves the computer proposes as if reading tablets from the gods. Learning chess has never been easier; studying its deepest secrets has never been more exciting.</p><p>Computing is not yet overcome. GPT-4 is impressive, but a layperson can’t wield it the way a programmer can. I still feel secure in my profession. In fact, I feel somewhat more secure than before. As software gets easier to make, it’ll proliferate; programmers will be tasked with its design, its configuration, and its maintenance. And though I’ve always found the fiddly parts of programming the most calming, and the most essential, I’m not especially good at them. I’ve failed many classic coding interview tests of the kind you find at Big Tech companies. The thing I’m relatively good at is knowing what’s worth building, what users like, how to communicate both technically and humanely. A friend of mine has called this A.I. moment “the revenge of the so-so programmer.” As coding per se begins to matter less, maybe softer skills will shine.</p><p>That still leaves open the matter of what to teach my unborn child. I suspect that, as my child comes of age, we will think of “the programmer” the way we now look back on “the computer,” when that phrase referred to a person who did calculations by hand. Programming by typing C++ or Python yourself might eventually seem as ridiculous as issuing instructions in binary onto a punch card. Dijkstra would be appalled, but getting computers to do precisely what you want might become a matter of asking politely.</p><p>So maybe the thing to teach isn’t a skill but a spirit. I sometimes think of what I might have been doing had I been born in a different time. The coders of the agrarian days probably futzed with waterwheels and crop varietals; in the Newtonian era, they might have been obsessed with glass, and dyes, and timekeeping. I was reading an oral history of neural networks recently, and it struck me how many of the people interviewed—people born in and around the nineteen-thirties—had played with radios when they were little. Maybe the next cohort will spend their late nights in the guts of the A.I.s their parents once regarded as black boxes. I shouldn’t worry that the era of coding is winding down. Hacking is forever.&nbsp;♦</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[My favorite coding question to give candidates (104 pts)]]></title>
            <link>https://carloarg02.medium.com/my-favorite-coding-question-to-give-candidates-17ea4758880c</link>
            <guid>38257024</guid>
            <pubDate>Mon, 13 Nov 2023 23:58:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://carloarg02.medium.com/my-favorite-coding-question-to-give-candidates-17ea4758880c">https://carloarg02.medium.com/my-favorite-coding-question-to-give-candidates-17ea4758880c</a>, See on <a href="https://news.ycombinator.com/item?id=38257024">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><h2 id="0e9d">A coding interview question, from the viewpoint of an Google/Amazon/Microsoft interviewer</h2><div><a rel="noopener follow" href="https://carloarg02.medium.com/?source=post_page-----17ea4758880c--------------------------------"><div aria-hidden="false"><p><img alt="Carlos Arguelles" src="https://miro.medium.com/v2/resize:fill:88:88/1*CM27oO9pXETXjs2M9_dUFg.jpeg" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a></div></div><figure><figcaption>photo <a href="https://unsplash.com/photos/two-people-drawing-on-whiteboard-26MJGnCM0Wc" rel="noopener ugc nofollow" target="_blank">credit</a></figcaption></figure><p id="7da1">There’s so many blogs and videos online showing you answers to LeetCode questions. But the viewpoint is mostly as an <strong>interviewee</strong>, not as an <strong>interviewer.</strong></p><p id="d629">In my 25 years in Big Tech, I’ve conducted over a thousand interviews (eight hundred at Amazon <a href="https://link.medium.com/zkSnwVUqmjb" rel="noopener">as a Bar Raiser</a>, a couple of hundred at Microsoft, and shy of a hundred at Google). Sometimes I get assigned leadership interviews, sometimes systems design, sometimes coding.</p><p id="1ad2">I don’t like <em>hard</em> or <em>tricky</em> questions. I tend to favor easier questions that lead to a high quality conversation where I can learn about the way somebody thinks. For coding interviews, I have often asked a variation of the following problem, loosely based on something I had to do in real life. I tailor it a bit with every candidate: it’s not always quite the same, and I’ve evolved it over time, but since I decided to retire the question, I’ll dissect it today. <strong>I want to focus on explaining what I looked for in a candidate with this question.</strong></p><blockquote><p id="f05f">Let’s say we have a website and we keep track of what pages customers are viewing, for things like business metrics.</p><p id="6782">Every time somebody comes to the website, we write a record to a log file consisting of Timestamp, PageId, CustomerId. At the end of the day we have a big log file with many entries in that format. And for every day we have a new file.</p><p id="e321">Now, given two log files (log file from day 1 and log file from day 2) we want to generate a list of ‘loyal customers’ that meet the criteria of: (a) they came on both days, and (b) they visited at least two unique pages.</p></blockquote><p id="1b35">The question is not particularly difficult, but it does require a little bit of thinking and knowledge of code complexity and data structures. You can easily get the customers that came on both days, you can easily get the customers that visited at least two unique pages, but getting the <em>intersection</em> of those two <em>efficiently</em> requires a little more work.</p><p id="dfb1">I’ve probably asked it 500 times, which has allowed me to calibrate it quite well. I have found that a Hire / No Hire decision from this question aligns with the final loop Hire / No Hire decision on a candidate about 95% of the times. I can also upscale it for higher level candidates, which makes it fairly versatile, and there’s many hints I can give to candidates struggling along the way.</p><h2 id="26f4">Ask Clarifying Questions</h2><p id="d136">Great candidates must ask clarifying questions before jumping into coding. I’m hoping to see some intuition from my candidates as I’ve actually expressed the problem in an <strong>ambiguous</strong> way.</p><p id="5191"><em>Did I mean 2 unique pages </em><strong><em>per day</em></strong><em> or </em><strong><em>overall</em></strong><em>?</em></p><p id="92ed">This significantly impacts the solution that you come up with. I mean “2 unique pages <em>overall</em>” because that’s a much more interesting problem. About half the candidates jump straight into coding without clarifying this, and out of those, half assume incorrectly that I meant “2 unique pages per day.” If it’s a more junior candidate, I’ll hint heavily before they start coding. If it’s a more senior candidate, I’ll wait a bit and see if that comes up as they’re thinking more deeply about the algorithm.</p><p id="dbe4">In real life, engineers deal with ambiguity all the time, and the root of most software problems can be traced back to poorly defined requirements. So I want to get a signal on <em>spotting and dealing with ambiguity</em>.</p><p id="d391">There’s one more clarifying question that 90% of people miss upfront, but that also matters: <strong>What about duplicates? </strong>Visits to the same page in the same day? What about visits to the same page on different days? For this problem, these are duplicates.</p><p id="2e24">Another clarifying question that matters is <strong>what scale are we talking about here? </strong><em>Does the data fit in memory? </em>Can I load the contents of one of these files in memory? Can I load the contents of both?</p><p id="5590">The question was inspired by a real-world system that I worked on at Amazon, called Clickstream, that was responsible for tracking user behavior on amazon.com. In real life, we processed petabytes of events from millions of concurrent customers on a giant Hadoop cluster of ten thousand hosts, and we had an entire team of engineers maintaining and operating the system. For the purposes of a 45-minute interview, I wanted to remove the distributed nature of the problem, and just imagine the data fits in memory, with a much smaller scope.</p><p id="b325">Lastly, another clarifying question that is important is <strong>how much does performance vs memory matter?</strong> There’s a naive solution that is O(n²) in running time, but only uses O(1) of memory. There’s a better solution that has running time of O(n) but uses O(n) of memory. And there’s an in-between solution that does some pre-processing in O(n log n) with O(k) of memory, which allows you to run the main algorithm in O(n) with O(1) of memory. Each one has pros and cons. Bonus points to any candidate that discusses this upfront. Only the top 10% of candidates do this.</p><h2 id="4779">Can’t I just use a Database?</h2><p id="3209">In theory you could write a pretty simple SQL query, and sure, Big Tech companies have giant data warehouses where you can easily do this sort of thing. But for the scope of a coding interview, you wouldn’t want to. Since this is not a distributed systems problem and the data fits in memory, why introduce the additional complexity and dependencies of a database for something that you can solve with 20 lines of simple code?</p><h2 id="6cda">Naive solution first</h2><p id="de00">About 80% of the candidates go for the naive solution first. It’s easiest and most natural. It’s some form of <em>“for each element from file 1, loop for all the contents of file 2, looking for elements with the same customer id, and keeping track of the pages they view.”</em></p><p id="4a54">The problem with the naive solution is that its running time is O(n²).</p><p id="dc2f">I don’t mind getting the naive solution first, but I really want to see my candidate having that <em>aha moment</em> that O(n²) is probably never good in any problem. And I want that aha moment to come pretty quickly and without hints. No great engineer should ever settle for an O(n²) algorithm, unless bound by memory or some other unmovable constraint.</p><p id="5e01">After a candidate puts forth the O(n²), I smile politely and I wait. I am really hoping the next words that come out of their mouth are “…but the complexity of this is O(n²) so can I do better?”</p><p id="fccb">Occasionally, a candidate will think they’re done at this point. 90% of the times that a candidate was done without questioning the quadratic nature of that solution, the final outcome of the loop was No Hire. So that’s another signal for me.</p><p id="3820">I’ll gentle probe, “what’s the running time of this solution”? And most of the times, the candidate will have the aha moment after that hint and move on to a better solution.</p><h2 id="a692"><strong>Tuning O(n²) into O(n)</strong></h2><p id="4919">At this point you need to do some thinking about what data structure you’re going to use, and how you’re going to store your data.</p><p id="9cdb">Poor candidates go for linked lists, or arrays. If arrays, my challenge question is that they don’t know the size of the data upfront. If it’s a linked list, then you should know search is going to cost you O(n) for each element, therefore you’ll end up back to an O(n²) algorithm no matter how hard you try. You can use a Tree, but since search is O(log n), that’ll yield an overall best of O(n log n).</p><p id="71bb">Better candidates have the intuition that a <strong>Map</strong> will provide the O(1) lookup that they need to turn the O(n²) algorithm into an O(n) algorithm. Great candidates will proactively point out the <em>downside</em> is that you’ll use O(n) memory. Faster running time at the expense of more memory is a tradeoff.</p><p id="d8a5"><em>If you’re using a Map, what is your Key, and what is your Value?</em> I’ve seen all kinds of answers here. Some candidates use PageId as the Key, and CustomerId as the Value, but that is not going to help. Candidates then switch it around to have CustomerId as the Key, and PageId as the Value of the Map. But that’s not particularly great either because it overlooks the fact that <em>you can have many pages per customer, not just one.</em> Some candidates have the intuition that they need a <em>Collection</em> of pages as the Value of the Map, but they’ll go for a List, which saddens my soul because it overlooks the fact that you can have <em>duplicates</em>. This is a good opportunity to probe around data structure knowledge on Lists vs. Sets, as candidates think about handling duplicates.</p><p id="83c9">So, <em>Map&lt;CustomerId, Set&lt;PageId&gt;&gt;</em> will do. But will you load the contents of both files into a single Map? Or have two maps, one for each file? Or can you get away with just loading the contents of file 1 into a map, and processing file 2 without storing it?</p><p id="1b32">Great candidates realize they can go for option#3 right away on their own. That’s a lot less memory, and a simpler algorithm. Good candidates get there, but need a little hinting. Poor candidates load the contents of both files into memory.</p><p id="8d11">Now that the algorithm is set, it’s time to write the code. For the most part it’s straight forward but there’s some dragons here.</p><p id="bf9b">The condition <em>“customers that came on both days”</em> is pretty simple: as you’re reading a customer entry from Day2, if the customer is in the Map from Day1, then you know they came on both days:</p><figure></figure><p id="2e34">The condition <em>“customers that visited at least 2 unique pages”</em> tends to be a little harder for candidates to get right, so if they’re stuck I throw a little hint: you have a Set of pages from Day1, and a single page from Day2… how can you determine that this is at least two unique pages?</p><figure></figure><p id="9039">Poor candidates will loop through the elements in the Set to check if the page from Day2 is in there. This turns your O(n) algorithm into O(n²) again. The number of candidates who have done this is surprising.</p><p id="e388">Better candidates will do a <em>.contains() </em>on the Set which is an O(1) operation on a hash set. But there is a catch with the logic.</p><p id="79fa">The intuition to get this right is this: If you are inside that If loop and the customer visited at least two pages in Day1, and they visited <em>any</em> page in Day2, they’re loyal, regardless of which page they visit in Day2. Otherwise, they only visited only one page in Day1, so the question is: is this a different page? If so they’re loyal, else it’s a duplicate so you don’t know and should keep going. So your If statement has an Or:</p><figure></figure><p id="ca91">There’s a need for attention to detail, like using “&gt;” instead of “&gt;=” or missing the “!” in the second statement. I saw these fairly often. I didn’t worry. Great candidates spotted them quickly as they double-checked the algorithm when they were done. Good candidates spotted them after a little bit of hinting. That gave me a good signal on <em>debugging skills</em>.</p><h2 id="bae2">Optimizing Your Solution</h2><p id="1aa8">Great candidates often go the extra mile with little optimizations that show attention to detail, and engineering craftsmanship.</p><p id="5b56">For example, <em>you don’t need to actually keep </em><strong><em>every single page</em></strong><em> from Day 1</em> in the Map, <strong>just two</strong>, since the problem is “at least two pages” so a Set of size 2 or even an array of size 2 will use less memory than an unbounded Set.</p><p id="d209">Or, if you’ve already determined that a customer is loyal, you don’t need to waste CPU cycles going thru the logic again next time you encounter that customer in Day 2.</p><figure></figure><p id="e365">Better yet, you could just remove the entry from Day1 once you know they’re loyal, which will mean that the day1.containsKey() will return false next time and you won’t do additional work for that customer, since you’ve already determined they are loyal:</p><figure></figure><blockquote><p id="0fb7">A word about optimizing: you could argue that <strong>these optimizations make it more difficult to change the algorithm if the requirements change</strong> in the future. That’s a reasonable stand. As long as you can hold a good conversation about pros and cons, I am happy with a high quality discussion on how you would balance these decisions. At the same time, being able to optimize an algorithm when neeeded *is* a trait of a great engineer and you *will* need to do that a time or two in your career.</p></blockquote><h2 id="c618">The Other Solution</h2><p id="229a">There’s a different way of thinking about this problem. The vast majority of candidates go for the Map approach, but sometimes I have a candidate go for the other one. Maybe 5% of the time at most.</p><p id="2af5"><em>What if you pre-processed the files and sorted them by CustomerId, then by PageId?</em></p><p id="3bc1">Pre-processing is a powerful tool in your software engineering arsenal, particularly if you’re going to be performing an operation a bunch of times. You can take the hit of pre-processing with the first one, or do it beforehand, which amortizes the cost over time. Sorting the files can be a logarithmic operation with constant memory.</p><p id="173f">If the files are sorted, then the problem is easier and it’s just a <a href="https://www.geeksforgeeks.org/two-pointers-technique/" rel="noopener ugc nofollow" target="_blank"><strong>two-pointer algorithm</strong></a> that you can execute in O(n) with O(1) of memory. While there are still entries in Day 1 and in Day 2, if CustomerId from Day1 &lt; CustomerId from Day2, move the pointer for Day 1, else if CustomerId from Day1 &gt; CustomerId from Day2, move the pointer for Day 2. Now you’re in a situation where the CustomerId is the same for both days, so you know they came on both days. Since the second sort key is by PageId, you follow another two-pointer algorithm to determine that there are at least two unique pages. So it’s a 2-pointer algorithm within a 2-pointer algorithm. It’s kind of a fun problem! I’ll leave the actual implementation as an exercise for the viewer.</p><h2 id="d0f5">Making the problem harder</h2><p id="1615">If you want to make the problem even more interesting, you can add a third file. I will leave that as an exercise for the reader as well!</p><h2 id="8444">In conclusion</h2><p id="8405">I hope this little insight into a coding problem&nbsp;from the viewpoint of an interviewer, and the ways in which I’ve seen great, good and poor candidates approach it, has been useful to you. Best of luck in your next interview!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nepal bans TikTok and says it disrupts social harmony (633 pts)]]></title>
            <link>https://apnews.com/article/nepal-tiktok-ban-social-media-854846a42ef566fa296ddaacd0099447</link>
            <guid>38256810</guid>
            <pubDate>Mon, 13 Nov 2023 23:32:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apnews.com/article/nepal-tiktok-ban-social-media-854846a42ef566fa296ddaacd0099447">https://apnews.com/article/nepal-tiktok-ban-social-media-854846a42ef566fa296ddaacd0099447</a>, See on <a href="https://news.ycombinator.com/item?id=38256810">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-module="" data-padding="none">
                    
                    
                        
                            

    <div><figure>
    

    
        <picture data-crop="medium-3x2">
    
        <source media="(min-width: 1280px)" type="image/webp" width="980" height="653" srcset="https://dims.apnews.com/dims4/default/124cb91/2147483647/strip/true/crop/3000x1999+0+1/resize/980x653!/format/webp/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 1x" loading="lazy">

    

    
        <source media="(min-width: 1280px)" width="980" height="653" srcset="https://dims.apnews.com/dims4/default/3e22a29/2147483647/strip/true/crop/3000x1999+0+1/resize/980x653!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 1x" loading="lazy">

    

    
        <source media="(min-width: 1024px)" type="image/webp" width="820" height="546" srcset="https://dims.apnews.com/dims4/default/63e84c7/2147483647/strip/true/crop/3000x1998+0+1/resize/820x546!/format/webp/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 1x" loading="lazy">

    

    
        <source media="(min-width: 1024px)" width="820" height="546" srcset="https://dims.apnews.com/dims4/default/f37655a/2147483647/strip/true/crop/3000x1998+0+1/resize/820x546!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 1x" loading="lazy">

    

    
        <source media="(min-width: 768px)" type="image/webp" width="1024" height="683" srcset="https://dims.apnews.com/dims4/default/9f8df58/2147483647/strip/true/crop/2999x2000+1+0/resize/1024x683!/format/webp/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 1x" loading="lazy">

    

    
        <source media="(min-width: 768px)" width="1024" height="683" srcset="https://dims.apnews.com/dims4/default/b616b38/2147483647/strip/true/crop/2999x2000+1+0/resize/1024x683!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 1x" loading="lazy">

    

    
        <source media="(min-width: 600px)" type="image/webp" width="767" height="511" srcset="https://dims.apnews.com/dims4/default/9d91526/2147483647/strip/true/crop/3000x1999+0+1/resize/767x511!/format/webp/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 1x,https://dims.apnews.com/dims4/default/b1d3ef6/2147483647/strip/true/crop/3000x1999+0+1/resize/1534x1022!/format/webp/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 2x" loading="lazy">

    

    
        <source media="(min-width: 600px)" width="767" height="511" srcset="https://dims.apnews.com/dims4/default/8730597/2147483647/strip/true/crop/3000x1999+0+1/resize/767x511!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 1x,https://dims.apnews.com/dims4/default/4bcb7fd/2147483647/strip/true/crop/3000x1999+0+1/resize/1534x1022!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 2x" loading="lazy">

    

    
        <source media="(max-width: 599px)" type="image/webp" width="567" height="378" srcset="https://dims.apnews.com/dims4/default/79f0160/2147483647/strip/true/crop/3000x2000+0+0/resize/567x378!/format/webp/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 1x,https://dims.apnews.com/dims4/default/e80595f/2147483647/strip/true/crop/3000x2000+0+0/resize/1134x756!/format/webp/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 2x" loading="lazy">

    

    
        <source media="(max-width: 599px)" width="567" height="378" srcset="https://dims.apnews.com/dims4/default/2cf812d/2147483647/strip/true/crop/3000x2000+0+0/resize/567x378!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 1x,https://dims.apnews.com/dims4/default/7718ef8/2147483647/strip/true/crop/3000x2000+0+0/resize/1134x756!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 2x" loading="lazy">

    

    
        <source type="image/webp" width="320" height="213" srcset="https://dims.apnews.com/dims4/default/c0a7c9c/2147483647/strip/true/crop/3000x1997+0+2/resize/320x213!/format/webp/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 1x,https://dims.apnews.com/dims4/default/7bc5c29/2147483647/strip/true/crop/3000x1997+0+2/resize/640x426!/format/webp/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 2x" loading="lazy">

    

    
        <source width="320" height="213" srcset="https://dims.apnews.com/dims4/default/f2148d3/2147483647/strip/true/crop/3000x1997+0+2/resize/320x213!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 1x,https://dims.apnews.com/dims4/default/857a41f/2147483647/strip/true/crop/3000x1997+0+2/resize/640x426!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 2x" loading="lazy">

    

    <img alt="FILE - A view of the TikTok app logo, in Tokyo, Japan, Sept. 28, 2020. The European Union ratcheted up its scrutiny of Big Tech companies on Thursday, Oct. 19, 2023, with demands for Meta and TikTok to detail their efforts on curbing illegal content and disinformation amid the Israel-Hamas war. (AP Photo/Kiichiro Sato, File)" srcset="https://dims.apnews.com/dims4/default/f2148d3/2147483647/strip/true/crop/3000x1997+0+2/resize/320x213!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 1x,https://dims.apnews.com/dims4/default/857a41f/2147483647/strip/true/crop/3000x1997+0+2/resize/640x426!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 2x" width="320" height="213" src="https://dims.apnews.com/dims4/default/f2148d3/2147483647/strip/true/crop/3000x1997+0+2/resize/320x213!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624" loading="lazy">
</picture>

    

    
        <div>
            <div><bsp-read-more data-more-button-text="Read More" data-less-button-text="Read Less" data-expand="ReadMore-expand" data-limit="110" data-main-class="ReadMore">
                    <figcaption><p>FILE - A view of the TikTok app logo, in Tokyo, Japan, Sept. 28, 2020. The European Union ratcheted up its scrutiny of Big Tech companies on Thursday, Oct. 19, 2023, with demands for Meta and TikTok to detail their efforts on curbing illegal content and disinformation amid the Israel-Hamas war. (AP Photo/Kiichiro Sato, File)</p></figcaption>
                </bsp-read-more></div>
            <bsp-lead-superlead-ui>
    
    
</bsp-lead-superlead-ui>
        </div>
    
</figure>
</div>



                        
                    

                    <div>
                                        <p>KATHMANDU, Nepal (AP) — Nepal’s government decided to ban the popular <span><a href="https://apnews.com/article/tiktok-ceo-shou-zi-chew-security-risk-cc36f36801d84fc0652112fa461ef140" target="_blank" rel="noopener">social media app TikTok</a></span> on Monday, saying it was disrupting “social harmony” in the country. </p><p>The announcement was made following a Cabinet meeting. Foreign Minister Narayan Prakash Saud said the app would be banned immediately. </p><p>“The government has decided to ban TikTok as it was necessary to regulate the use of the social media platform that was disrupting social harmony, goodwill and flow of indecent materials,” Saud said.</p>
    

<p>He said that to make social media platforms accountable, the government has asked the companies to register and open a liaison office in Nepal, pay taxes and abide by the country’s laws and regulations. </p><p>It wasn’t clear what triggered the ban or if TikTok had refused to comply with Nepal’s requests. The company did not immediately respond to an email seeking comment. </p><p>TikTok, owned by China’s ByteDance, <span><a href="https://apnews.com/article/tiktok-ban-privacy-cybersecurity-bytedance-china-2dce297f0aed056efe53309bbcd44a04" target="_blank" rel="noopener">has faced scrutiny in a number of countries</a></span> because of concerns that Beijing could use the app to harvest user data or advance its interests. Countries including <span><a href="https://apnews.com/article/tiktok-ban-ceo-congressional-hearing-bytedance-china-44d948c5b0ba18e2a714e0fa62d52779" target="_blank" rel="noopener">the United States</a></span>, Britain and New Zealand have banned the app on government phones despite TikTok repeatedly denying that it has ever shared data with the Chinese government and would not do so if asked.</p><p>Nepal has banned all pornographic sites in 2018.</p>
                                    </div>

                    


                    


                    
    



                    
    


                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google witness accidentally blurts out that Apple gets 36% cut of Safari deal (154 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2023/11/google-witness-accidentally-blurts-out-that-apple-gets-36-cut-of-safari-deal/</link>
            <guid>38256746</guid>
            <pubDate>Mon, 13 Nov 2023 23:24:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2023/11/google-witness-accidentally-blurts-out-that-apple-gets-36-cut-of-safari-deal/">https://arstechnica.com/tech-policy/2023/11/google-witness-accidentally-blurts-out-that-apple-gets-36-cut-of-safari-deal/</a>, See on <a href="https://news.ycombinator.com/item?id=38256746">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h4>
      Witness malfunction    —
</h4>
            
            <h2 itemprop="description">Google and Apple specifically requested that detail be confidential.</h2>
                    </div><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/11/GettyImages-1725649029-800x544.jpg" alt="Google witness accidentally blurts out that Apple gets 36% cut of Safari deal">
      <figcaption></figcaption>  </figure>

  




<!-- cache hit 168:single/related:96da591cf0079d41f8ddadcf1d18c52f --><!-- empty -->
<p>Google's default search deal with Apple is worth so much to the search giant that Google pays 36 percent of its search advertising revenue from Safari to keep its search engine set as the default in Apple's browser, <a href="https://www.bloomberg.com/news/articles/2023-11-13/apple-gets-36-of-google-revenue-from-search-deal-witness-says">Bloomberg reported</a>.</p>
<p>Google and Apple objected to making this key detail public from their long-running default search deal. But their closely held secret came out on Monday during testimony from Google's main economics expert, Kevin Murphy, during the Department of Justice's monopoly trial examining Google's search business.</p>
<p>"Probably the biggest slip of the entire trial," Big Tech on Trial, an account dedicated to providing updates from the Google trial, <a href="https://twitter.com/BigTechOnTrial/status/1724136578593718643">posted</a> on X (formerly Twitter).</p>
<p><a href="https://news.bloomberglaw.com/ip-law/apple-gets-36-of-google-revenue-from-search-deal-witness-says">According to Bloomberg Law</a>, Google attorney John Schmidtlein "visibly cringed" when Murphy revealed the confidential information, which Google had initially claimed needed to be kept secret because otherwise it “would unreasonably undermine Google’s competitive standing in relation to both competitors and other counterparties.”</p>
<p>For the DOJ—which has made the <a href="https://arstechnica.com/tech-policy/2023/10/googles-21-year-deal-with-apple-is-the-heart-of-monopoly-case-judge-says/">Google-Apple deal the center of its case</a> alleging that Google maintains an illegal monopoly over search—this detail confirms how valuable default placements on iPhones are to the search leader.</p>
<p>The DOJ has argued that Google pays so much for default search deals to block out competitors, lock search users into its services, and maintain a stronghold over the search industry—a dominant position that could be further entrenched by Google's advances with AI, <a href="https://arstechnica.com/tech-policy/2023/10/googles-claim-that-search-users-have-choice-is-bogus-microsoft-ceo-tells-judge/">Microsoft CEO Satya Nadella testified</a>. In September, an Apple exec testified that the default deal between Google and Apple was seemingly so lucrative that it even <a href="https://arstechnica.com/tech-policy/2023/09/google-deal-may-have-kept-apple-from-building-search-engine-exec-says/">stopped Apple from creating its own rival search engine</a>.</p>
<p>It's still unclear exactly how much money that portion of Google's search advertising revenue that comes from Safari amounts to, but several estimates have been floated.&nbsp;<a href="https://www.statista.com/statistics/266249/advertising-revenue-of-google/">Statista reported</a> that Google's advertising revenue was $224 billion in 2022, and based on that, <a href="https://www.engadget.com/google-reportedly-pays-apple-36-percent-of-ad-search-revenues-from-safari-191730783.html?guccounter=1&amp;guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&amp;guce_referrer_sig=AQAAAHZ16G_-rr-pYMpU363ol3cjwErVsOs8lLJgDiRDyapIyCOWFIcEGVZaEmYnPdiIiEKA24pt-r2TWndowKJ-SZiX46kzYXKgmx4w9faq6ioIdOm1USMSKC6MdQjB5sBOGI9MEL7agKiYmW6X7iKhzjvstWsVlYRSl5gSH1wbsqBJ">Engadget estimated</a> that Apple likely gets paid in the tens of billions of dollars for Google's default Safari placements.</p>                                            
                                                        
<p>Previously, sources <a href="https://www.nytimes.com/2023/10/26/technology/google-apple-search-spotlight.html">told The New York Times</a> that Google paid Apple approximately $18 billion in 2021 for the deal, but the exact amount of revenue sharing remained unknown until Monday. The DOJ's trial also recently revealed that <a href="https://arstechnica.com/tech-policy/2023/10/google-paid-26b-for-default-contracts-in-2021-google-exec-testified/">Google paid $26 billion in total for default contracts</a>, which&nbsp;are ostensibly responsible for driving up its search advertising revenue that is right now rapidly climbing. Google's global ad revenue will likely reach nearly $340 billion by 2027, Statista <a href="https://www.statista.com/statistics/539447/google-global-net-advertising-revenues/">reported</a>, driven largely by Google's search engine traffic, which is currently responsible for "roughly 38 percent" of its global ad revenue.</p>
<p>In total, across all those default deals, Digital Content Next CEO Jason Kint estimated in a <a href="https://twitter.com/jason_kint/status/1724152525538959850">post</a> on X that it's possible that Google derives "at least $90 billion of its current annual revenue."</p>
<p>Last month, Google CEO <a href="https://arstechnica.com/tech-policy/2023/10/doj-grilled-sundar-pichai-on-very-valuable-default-deals-deleted-chats/">Sundar Pichai testified</a> that default deals "can make a difference" and can be "very valuable" if "done correctly" but maintained Google's chief defense that partners like Apple enter these deals with Google because Google has a superior search engine.</p>
<p>If the DOJ proves that these default deals ensure that Google maintains an illegal monopoly in general search markets, Google could be ordered to break up its search business, shifting not just Google's bottom line but also its partners, like Apple.</p>
<p>While the trial resumes for another week, Google continues profiting off the deals. From 2022 to 2023, Google's ad revenue increased by $5 billion, <a href="https://searchengineland.com/google-search-ad-revenue-q2-2023-433633">Search Engine Land reported</a>, and seemingly as Nadella predicted, Pichai attributed these gains to AI-driven innovations across Google products, including search.</p>
<p>"We’re continuing to focus on making AI more helpful for everyone; there’s exciting progress and lots more to come,” Pichai said in a statement reported by Search Engine Land.</p>
<p>Judge Amit Mehta, presiding over the antitrust trial, has said that the Google-Apple default deal is the <a href="https://arstechnica.com/tech-policy/2023/10/googles-21-year-deal-with-apple-is-the-heart-of-monopoly-case-judge-says/">"heart" of the DOJ's case against Google</a>. With each new detail revealed about how much Google is willing to pay Apple to maintain their deal, the DOJ hopes to convince Mehta that the deal gives Google an unfair advantage over competitors. This week's slip-up from one of Google's witnesses threatens to disrupt the narrative that Google is trying to build as it winds down its defense of that deal and others.</p>
<p>Mehta is not expected to issue a ruling in the case until 2024.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Infinite Context LLMs: Going Beyond RAG with Extended Minds (123 pts)]]></title>
            <link>https://blog.normalcomputing.ai/posts/2023-09-12-supersizing-transformers/supersizing-transformers.html</link>
            <guid>38256645</guid>
            <pubDate>Mon, 13 Nov 2023 23:13:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.normalcomputing.ai/posts/2023-09-12-supersizing-transformers/supersizing-transformers.html">https://blog.normalcomputing.ai/posts/2023-09-12-supersizing-transformers/supersizing-transformers.html</a>, See on <a href="https://news.ycombinator.com/item?id=38256645">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="quarto-content">
<!-- sidebar -->
  

<!-- margin-sidebar -->
    
<!-- main -->
<main id="quarto-document-content">




<p>Today’s popularized large language models are optimized for the task of producing sequences of tokens that look like they could’ve been present in the training corpus. This is quite distinct from the ways in which LLMs are wielded in such user interfaces as <a href="https://chat.openai.com/?model=gpt-4">ChatGPT</a> or <a href="https://www.perplexity.ai/">Perplexity.ai</a>, where users expect the model to perform complex reasoning tasks and faithfully retrieve factual, topical information. If we hope to use the model as a general reasoning agent and not as a stochastic parrot, we need to provide it with any relevant data at inference time, rather than rely on (1) the salient data having appeared in the training corpus and (2) the model being able to recall said data. Further, surfacing references or citations that highlight which content the model used during its generation is crucial for building applications that truly augment human workflows.</p>
<p>This has prompted much development on methods colloquially referred to as “retrieval”<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a>. Or, methods that help LLMs make use of pertinent documents. <strong>In context learning</strong>, or placing the relevant documents in the context window before the prompt, is the obvious first step. However, in many cases we’re faced with documents longer than the context window of the model. <strong>RAG</strong><a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a> attempts to sidestep this by selecting the best subset of documents to include alongside the user’s query. While often effective, RAG is fundamentally limited by the need for a separate search engine. We can’t, for instance, ask the model questions which require synthesizing the entire set of documents. Further, since the retrieval happens before the generation, the best we can do r.e. explainability is report which text was included in the prompt itself. This says nothing about what text the model actually used during generation.</p>
<p>Finetuning<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a> seeks to extend the length of the context window itself. Running even a few epochs of training can be a non-trivial undertaking for today’s large models, even with a dedicated ML team. Further, these methods doesn’t contribute to the model’s interpretability. Other methods<a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a> suggest structural changes to the model. Many of these are exciting, but most require training from scratch or fine-tuning, making them difficult to leverage with pre-trained models.</p>
<p>In this post, we propose and <a href="https://huggingface.co/normalcomputing">open source</a> <strong>extended mind transformers</strong>, which generalize RAG internally. This simple mathematical generalization buys us the performance gains (and more) of RAG, as well as introducing net-new generation controls and granular <em>causal</em> citations. We also get the best of both worlds when it comes to ease of use: seamless integrations (everything is internal to the model), and no fine-tuning required!</p>
<div>
<figure>
<p><img src="https://storage.googleapis.com/normal-blog-artifacts/extended-mind-transformers/otto.png"></p>
<figcaption>Credits: <span data-cites="patrick-blog">Buchen (<a href="#ref-patrick-blog" role="doc-biblioref">2018</a>)</span></figcaption>
</figure>
</div>
<section id="aesthetics-for-extended-mind-transformers">
<h2 data-anchor-id="aesthetics-for-extended-mind-transformers">Aesthetics for Extended Mind Transformers</h2>
<p>As motivation, we provide context from the Philosophy of Mind which served as inspiration for the naming convention and methodology. In <span data-cites="clark-chalmers">Clark and Chalmers (<a href="#ref-clark-chalmers" role="doc-biblioref">1998</a>)</span> “The Extended Mind”, they present the thesis that external information which is constantly and immediately accessible, and automatically endorsed should be considered part of the memory. And further, that this extension should be considered part of the mind. They term this idea <strong>active externalism</strong>. The story of Otto functions as an intuition pump:</p>
<blockquote>
<p>“[L]ike many Alzheimer’s patients, [Otto] relies on information in the environment to help structure his life. Otto carries a notebook around with him everywhere he goes. When he learns new information, he writes it down. When he needs some old information, he looks it up. For Otto, his notebook plays the role usually played by a biological memory. … The information in the notebook functions just like information constituting an ordinary non-occurrent belief; it just happens that this information lies beyond the skin.”<a href="#fn5" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
</blockquote>
<p>In this piece, we present active externalism for LLMs, a mechanism for bolstering the memory of transformers aesthetically inspired by the Extended Mind Thesis. We call transformers which implement active externalism, extended mind transformers.</p>
</section>
<section id="extended-mind-transformers">
<h2 data-anchor-id="extended-mind-transformers">Extended Mind Transformers</h2>
<section id="definition">
<h3 data-anchor-id="definition">Definition</h3>
<p>Our proposed method, which closely resembles the work of <span data-cites="wu2022memorizing">Wu et al. (<a href="#ref-wu2022memorizing" role="doc-biblioref">2022</a>)</span><a href="#fn6" id="fnref6" role="doc-noteref"><sup>6</sup></a>, is a simple change to the self-attention mechanism. In addition to the causal self-attention integral to transformers, we also allow each query token to attend to a fixed number of “external memories”. These memories are stored in a non-differentiable cache. The choice of which memories to attend to is made using cosine similarity within each decoder layer and attention head. More precisely, our attention computation is described by:</p>
<p><span>\[
\operatorname{softmax}\left(\frac{Q(K_{R}\oplus K_{L})^{T}}{\sqrt{d}}\right) \times \left(V_{R} \oplus V_{L}\right)
\]</span></p>
<p>Where <span>\((K_{L}, V_{L})\)</span> are key-value pairs from local context, and <span>\((K_{R}, V_{R})\)</span> are key-value pairs from external memories, and <span>\(\oplus\)</span> refers to tensor concatenation. We mask the attention weights such that each query token can only attend to its own retrieved keys, and not those retrieved by previous or following query tokens. In the experiments we present below we use models trained with linear biases rather than positional encodings. When we apply these linear biases to our attention weights, we assign the same index to all retrieved memories.<a href="#fn7" id="fnref7" role="doc-noteref"><sup>7</sup></a></p>
<p>Importantly, <strong>active externalism retrieves memories exactly</strong> - it doesn’t summarize or otherwise dampen memories except through the linear biases.</p>
<p>We generate the external memories (key-value pairs) once, and then pass the representations to each decoder layer in an analogous fashion to passing previous “cached” key-values<a href="#fn8" id="fnref8" role="doc-noteref"><sup>8</sup></a>. In order to speed up the top-k cosine similarity computation we can use a vector database designed exactly for this purpose<a href="#fn9" id="fnref9" role="doc-noteref"><sup>9</sup></a>.</p>
<p>We argue that this way of attending to external memories or beliefs is the natural and optimal generalization of methods like RAG, and closely mimics the kind of relationship Otto has with his notebook. The information is constantly and immediately accessible, automatically endorsed, and reliably referenced. We set a similarity threshold such that we always reference our external memories (for every generated token, within all decoder layers), but discard keys that don’t meet some low similarity threshold<a href="#fn10" id="fnref10" role="doc-noteref"><sup>10</sup></a> to avoid confusing the model with irrelevant information.</p>
<p>Active externalism is not conceptually difficult to implement, but does require getting familiar with a particular model’s implementation since details like the way key-value pairs are stored and read into the self-attention computation need to be hijacked.</p>
</section>
</section>
<section id="benchmark-results">
<h2 data-anchor-id="benchmark-results">Benchmark Results</h2>
<section id="perplexity-experiments">
<h3 data-anchor-id="perplexity-experiments">Perplexity Experiments</h3>
<p>We use perplexity as a metric for model performance. Perplexity is a measure of uncertainty of the model over each generated token, closely related to our cross-entropy loss function. For a full explanation of perplexity as a metric, we suggest checking out this excellent <a href="https://thegradient.pub/understanding-evaluation-metrics-for-language-models/">post</a>.</p>
<p>We show results below for perplexity experiments on the Wikitext-103 benchmark<a href="#fn11" id="fnref11" role="doc-noteref"><sup>11</sup></a> using Mosaic’s MPT-7b model. We use a stride of 512 tokens in our perplexity experiments, meaning each token is conditioned on at least 512 previous tokens, given that there are indeed 512 tokens to condition on.</p>
<p>Our active externalism method batches each sequence into chunks of increasing length (x-axis), and attends to tokens previous to the last 2048 (max sequence length) as external memories. We show results for varying k, where k is the number of memories we retrieve per query token. We compare active externalism to two baseline methods. The “truncated” baseline simply throws out any tokens previous to the last 2048 during perplexity computations, and the “naive” method which uses all input-length tokens, no matter how long the sequences become.</p>
<p>In the case of the naive method, we observe exactly the phenomenon active externalism seeks to ameliorate: after sequences exceed lengths greater than 2-3k tokens, the performance quickly drops off (in this case, perplexity blows up).</p>
<div>
<figure>
<p><img src="https://storage.googleapis.com/normal-blog-artifacts/extended-mind-transformers/naive.png"></p>
<figcaption>Perplexity results for Naive and Extended Mind MTP-7b, using a stride length of 512 tokens. Documents are batched into lengths of “Input Length” and we report average PPL on Y-Axis.</figcaption>
</figure>
</div>
<p>While we can see that active externalism provides clear benefits over simply doing local attention, in the case of the truncated benchmark. Even more exciting, perplexity continues to decrease as we increase the number of retrieved memories per query token.</p>
<div>
<figure>
<p><img src="https://storage.googleapis.com/normal-blog-artifacts/extended-mind-transformers/truncated.png"></p>
<figcaption>Perplexity results for Truncated and Extended Mind MTP-7b, using a stride length of 512 tokens. Documents are batched into lengths of “Input Length” and we report average PPL on Y-Axis.</figcaption>
</figure>
</div>
</section>
<section id="retrieval-experiments">
<h3 data-anchor-id="retrieval-experiments">Retrieval Experiments</h3>
<p>We also measure performance on retrieval benchmarks, and compare with RAG and simple baselines. Our dataset is a modified version of the recently released <a href="https://huggingface.co/datasets/abacusai/WikiQA-Free_Form_QA">Long context WikiQA benchmark</a> from Abacus.AI.</p>
<p>Our goal is to measure retrieval abilities over varying document lengths, but we also want to control for facts memorized during training, so we edit the dataset by changing the labeled answers to realistic but wrong answers. I.e, we replace every instance of “Lee Hazlewood” with “Terry Allen” in the Wikipedia entry for the song “These Boots Were Made For Walking”, and then ask the model to produce the songwriter’s name, with the <em>correct</em> answer now being “Terry Allen”.</p>
<p>Our intention is to measure the model’s ability to prioritize in context or in memory facts over those it memorized during training. Again, we feel this is an important ability if we’re asking LLMs to be reasoning agents in an evolving world.</p>
<p>In the results below, baseline receives no context at all for the question (we ask it point-blank), RAG selects the best ~2-3k tokens out of the document to include in-context<a href="#fn12" id="fnref12" role="doc-noteref"><sup>12</sup></a>, and active externalism puts the entire document in memory and uses it as Otto uses his notebook.</p>
<div>
<figure>
<p><img src="https://storage.googleapis.com/normal-blog-artifacts/extended-mind-transformers/retrieval.png"></p>
<figcaption>Retrieval Benchmark Results, by Document Length<a href="#fn13" id="fnref13" role="doc-noteref"><sup>13</sup></a></figcaption>
</figure>
</div>
<p>We see that while RAG methods drop off with input length, active externalism continues to be effective. While models finetuned to use longer contexts do currently outperform active externalism on some long-range retrieval tasks, active externalism appears to be a more effective way to do retrieval over long contexts for smaller models.</p>
<p>Where active externalism clearly outperforms RAG in large models is precisely where the model has <a href="https://arxiv.org/pdf/2205.10770.pdf">memorized before overfitting</a>. Or, the model’s weights encode factual information even as the model’s performance on test data<a href="#fn14" id="fnref14" role="doc-noteref"><sup>14</sup></a> continues to improve. Depending on your application, this could be seen as a strength or shortcoming. Certainly when we use LLMs as reasoning agents, this is a shortcoming.</p>
<p>Using active externalism also appears to eliminate some reliance on prompting. Whereas usually we’d need to include some examples of the kind of responses we hope to observe in the prompt (or use a “chat” model which has been RLHF’ed), we observe experimentally that this isn’t necessary when using active externalism.</p>
</section>
</section>
<section id="impact-on-reasoning-engine">
<h2 data-anchor-id="impact-on-reasoning-engine">Impact on reasoning engine</h2>
<p>We discuss two important consequences of active externalism on the LLM’s ability as a reasoning agent: uncertainty awareness and abstraction levers.</p>
<p>If we prompt the model with a question it’s unsure about<a href="#fn15" id="fnref15" role="doc-noteref"><sup>15</sup></a>, it may not respond in a way that’s transparent about that uncertainty. Active externalism provides a new method for revealing when a model is uncertain about its answer.</p>
<p>Let’s look at an example. We load our model easily from huggingface, and pass a paragraph from Wikipedia’s entry on Grothendieck as external memories.</p>
<div id="cb1" data-execution_count="1"><pre><code><span id="cb1-1"><a href="#cb1-1"></a><span>import</span> transformers</span>
<span id="cb1-2"><a href="#cb1-2"></a><span>from</span> transformers <span>import</span> AutoTokenizer, AutoModelForCausalLM</span>
<span id="cb1-3"><a href="#cb1-3"></a></span>
<span id="cb1-4"><a href="#cb1-4"></a>wikipedia <span>=</span> <span>"""Alexander Grothendieck (/ˈɡroʊtəndiːk/; German pronunciation: [ˌalɛˈksandɐ ˈɡʁoːtn̩ˌdiːk] (listen); French: [ɡʁɔtɛndik]; 28 March 1928 – 13 November 2014) was a stateless (and then, since 1971, French) mathematician who became the leading figure in the creation of modern algebraic geometry.[7][8] His research extended the scope of the field and added elements of commutative algebra, homological algebra, sheaf theory, and category theory to its foundations, while his so-called "relative" perspective led to revolutionary advances in many areas of pure mathematics.[7][9] He is considered by many to be the greatest mathematician of the twentieth century.[10][11]</span></span>
<span id="cb1-5"><a href="#cb1-5"></a></span>
<span id="cb1-6"><a href="#cb1-6"></a><span>Grothendieck began his productive and public career as a mathematician in 1949. In 1958, he was appointed a research professor at the Institut des hautes études scientifiques (IHÉS) and remained there until 1970, when, driven by personal and political convictions, he left following a dispute over military funding. He received the Fields Medal in 1966 for advances in algebraic geometry, homological algebra, and K-theory.[12] He later became professor at the University of Montpellier[1] and, while still producing relevant mathematical work, he withdrew from the mathematical community and devoted himself to political and religious pursuits (first Buddhism and later, a more Christian vision).[13] In 1991, he moved to the French village of Lasserre in the Pyrenees, where he lived in seclusion, still working tirelessly on mathematics and his philosophical and religious thoughts until his death in 2014.[14]</span></span>
<span id="cb1-7"><a href="#cb1-7"></a><span>"""</span></span>
<span id="cb1-8"><a href="#cb1-8"></a></span>
<span id="cb1-9"><a href="#cb1-9"></a>tokenizer <span>=</span> AutoTokenizer.from_pretrained(<span>'EleutherAI/gpt-neox-20b'</span>)</span>
<span id="cb1-10"><a href="#cb1-10"></a>memory_ids <span>=</span> tokenizer(wikipedia, return_tensors<span>=</span><span>'pt'</span>)[<span>'input_ids'</span>]</span>
<span id="cb1-11"><a href="#cb1-11"></a></span>
<span id="cb1-12"><a href="#cb1-12"></a>model <span>=</span> AutoModelForCausalLM.from_pretrained(<span>"normalcomputing/extended-mind-mpt-7b"</span>, external_memories<span>=</span>memory_ids, trust_remote_code<span>=</span><span>True</span>)</span></code></pre></div>
<p>Now, let’s ask the model a question we know is answered (albeit a little obscurely) in the above paragraph without using active externalism. We can achieve this by setting the parameter <code>model.use_active_externalism = False</code> or simply passing <code>topk=0</code>. Hint: the correct answer is 1971.</p>
<div data-execution_count="3">
<div id="cb2"><pre><code><span id="cb2-1"><a href="#cb2-1"></a>prompt <span>=</span> <span>"When did Alexander Grothendieck get his French citizenship?"</span></span>
<span id="cb2-2"><a href="#cb2-2"></a>input_ids <span>=</span> tokenizer(prompt, return_tensors<span>=</span><span>'pt'</span>)[<span>'input_ids'</span>]</span>
<span id="cb2-3"><a href="#cb2-3"></a></span>
<span id="cb2-4"><a href="#cb2-4"></a>out <span>=</span> model.generate(input_ids, max_length<span>=</span>input_ids.size(<span>-</span><span>1</span>)<span>+</span><span>50</span>, topk<span>=</span><span>0</span>)</span>
<span id="cb2-5"><a href="#cb2-5"></a><span>print</span>(<span>'Baseline Generation: '</span>, tokenizer.decode(out[<span>0</span>]))</span></code></pre></div>
<div>
<pre><code>Baseline Generation:  When did Alexander Grothendieck get his French citizenship?
I am trying to find out when Alexander Grothendieck got his French citizenship. I know that he was born in Germany and that he got his French citizenship in the late 1950s. I am trying to find out when he got his</code></pre>
</div>
</div>
<p>Now let’s enable active externalism, slowly cranking up the number of memories each query token is allowed to attend to using the <code>topk</code> parameter.</p>
<div data-execution_count="4">
<div id="cb4"><pre><code><span id="cb4-1"><a href="#cb4-1"></a>out <span>=</span> model.generate(input_ids, max_length<span>=</span>input_ids.size(<span>-</span><span>1</span>)<span>+</span><span>15</span>, topk<span>=</span><span>5</span>)</span>
<span id="cb4-2"><a href="#cb4-2"></a><span>print</span>(<span>'Generation for k=5: '</span>, tokenizer.decode(out[<span>0</span>][input_ids.size(<span>-</span><span>1</span>):]).strip())</span>
<span id="cb4-3"><a href="#cb4-3"></a></span>
<span id="cb4-4"><a href="#cb4-4"></a>out <span>=</span> model.generate(input_ids, max_length<span>=</span>input_ids.size(<span>-</span><span>1</span>)<span>+</span><span>15</span>, topk<span>=</span><span>6</span>)</span>
<span id="cb4-5"><a href="#cb4-5"></a><span>print</span>(<span>'Generation for k=6: '</span>,tokenizer.decode(out[<span>0</span>][input_ids.size(<span>-</span><span>1</span>):]).strip())</span>
<span id="cb4-6"><a href="#cb4-6"></a></span>
<span id="cb4-7"><a href="#cb4-7"></a>out <span>=</span> model.generate(input_ids, max_length<span>=</span>input_ids.size(<span>-</span><span>1</span>)<span>+</span><span>20</span>, topk<span>=</span><span>7</span>)</span>
<span id="cb4-8"><a href="#cb4-8"></a><span>print</span>(<span>'Generation for k=7: '</span>,tokenizer.decode(out[<span>0</span>][input_ids.size(<span>-</span><span>1</span>):]).strip())</span>
<span id="cb4-9"><a href="#cb4-9"></a></span>
<span id="cb4-10"><a href="#cb4-10"></a>out <span>=</span> model.generate(input_ids, max_length<span>=</span>input_ids.size(<span>-</span><span>1</span>)<span>+</span><span>15</span>, topk<span>=</span><span>8</span>)</span>
<span id="cb4-11"><a href="#cb4-11"></a><span>print</span>(<span>'Generation for k=8: '</span>,tokenizer.decode(out[<span>0</span>][input_ids.size(<span>-</span><span>1</span>):]).strip())</span>
<span id="cb4-12"><a href="#cb4-12"></a></span>
<span id="cb4-13"><a href="#cb4-13"></a>out <span>=</span> model.generate(input_ids, max_length<span>=</span>input_ids.size(<span>-</span><span>1</span>)<span>+</span><span>20</span>, topk<span>=</span><span>30</span>)</span>
<span id="cb4-14"><a href="#cb4-14"></a><span>print</span>(<span>'Generation for k=30: '</span>,tokenizer.decode(out[<span>0</span>][input_ids.size(<span>-</span><span>1</span>):]).strip())</span></code></pre></div>
<div>
<pre><code>Generation for k=5:  A: I think he got it in the early 1960s.
Generation for k=6:  A: I think he got it in the early 1970s.
Generation for k=7:  A: He was born in France, and he was naturalized in 1971.
&lt;|endoftext|&gt;
Generation for k=8:  A: I think he got it in 1971.
&lt;|endoftext|&gt;Q
Generation for k=30:  A: He was born in Germany, and became a French citizen in 1971.</code></pre>
</div>
</div>
<p>Not only did the model produce the correct answer, but it also expressed increasing certainty about its answer. This evolution of generations signals the model’s original uncertainty.</p>
<p>In cases where the model is certain about the answer, the generations are stable as we increase k over the external context.</p>
<div data-execution_count="5">
<div id="cb6"><pre><code><span id="cb6-1"><a href="#cb6-1"></a>prompt <span>=</span> <span>"What was did Alexander Grothendieck's profession?"</span></span>
<span id="cb6-2"><a href="#cb6-2"></a>input_ids <span>=</span> tokenizer(prompt, return_tensors<span>=</span><span>'pt'</span>)[<span>'input_ids'</span>]</span>
<span id="cb6-3"><a href="#cb6-3"></a></span>
<span id="cb6-4"><a href="#cb6-4"></a>out <span>=</span> model.generate(input_ids, max_length<span>=</span>input_ids.size(<span>-</span><span>1</span>)<span>+</span><span>25</span>, topk<span>=</span><span>0</span>)</span>
<span id="cb6-5"><a href="#cb6-5"></a><span>print</span>(<span>'Baseline Generation: '</span>, tokenizer.decode(out[<span>0</span>][input_ids.size(<span>-</span><span>1</span>):]).strip())</span>
<span id="cb6-6"><a href="#cb6-6"></a></span>
<span id="cb6-7"><a href="#cb6-7"></a>out <span>=</span> model.generate(input_ids, max_length<span>=</span>input_ids.size(<span>-</span><span>1</span>)<span>+</span><span>15</span>, topk<span>=</span><span>2</span>)</span>
<span id="cb6-8"><a href="#cb6-8"></a><span>print</span>(<span>'Generation for k=2: '</span>, tokenizer.decode(out[<span>0</span>][input_ids.size(<span>-</span><span>1</span>):]).strip())</span>
<span id="cb6-9"><a href="#cb6-9"></a></span>
<span id="cb6-10"><a href="#cb6-10"></a>out <span>=</span> model.generate(input_ids, max_length<span>=</span>input_ids.size(<span>-</span><span>1</span>)<span>+</span><span>15</span>, topk<span>=</span><span>8</span>)</span>
<span id="cb6-11"><a href="#cb6-11"></a><span>print</span>(<span>'Generation for k=8: '</span>, tokenizer.decode(out[<span>0</span>][input_ids.size(<span>-</span><span>1</span>):]).strip())</span></code></pre></div>
<div>
<pre><code>Baseline Generation:  What was did Alexander Grothendieck's profession?
Alexander Grothendieck was a French mathematician
Generation for k=2:  Alexander Grothendieck was a mathematician.

What
Generation for k=8:  A: He was a mathematician.
&lt;|endoftext|&gt;Q: What</code></pre>
</div>
</div>
<p>A natural extension of this principle might look like the development of a metric based on similarity or attention weight which could communicate this uncertainty in a more compact form, work currently under development at Normal.</p>
<p>The parameter <code>topk</code> also serves as a useful lever for the level of abstraction in the model’s output. E.g., the extent to which we’d like the model to synthesize the memories vs.&nbsp;quote verbatim from the source. We see this clearly in question-answering tasks over code. We show an example using the chat model here, which is best equipped to handle more free form question-answering tasks.</p>
<div id="cb8" data-execution_count="6"><pre><code><span id="cb8-1"><a href="#cb8-1"></a>code_snippet <span>=</span> <span>"""def sieve_of_eratosthenes(limit):</span></span>
<span id="cb8-2"><a href="#cb8-2"></a><span>    sieve = [True] * (limit + 1)</span></span>
<span id="cb8-3"><a href="#cb8-3"></a><span>    sieve[0] = sieve[1] = False</span></span>
<span id="cb8-4"><a href="#cb8-4"></a><span>    primes = []</span></span>
<span id="cb8-5"><a href="#cb8-5"></a><span>    </span></span>
<span id="cb8-6"><a href="#cb8-6"></a><span>    for current in range(2, int(limit**0.5) + 1):</span></span>
<span id="cb8-7"><a href="#cb8-7"></a><span>        if sieve[current]:</span></span>
<span id="cb8-8"><a href="#cb8-8"></a><span>            primes.append(current)</span></span>
<span id="cb8-9"><a href="#cb8-9"></a><span>            for multiple in range(current*current, limit + 1, current):</span></span>
<span id="cb8-10"><a href="#cb8-10"></a><span>                sieve[multiple] = False</span></span>
<span id="cb8-11"><a href="#cb8-11"></a><span>    </span></span>
<span id="cb8-12"><a href="#cb8-12"></a><span>    for num in range(int(limit**0.5) + 1, limit + 1):</span></span>
<span id="cb8-13"><a href="#cb8-13"></a><span>        if sieve[num]:</span></span>
<span id="cb8-14"><a href="#cb8-14"></a><span>            primes.append(num)</span></span>
<span id="cb8-15"><a href="#cb8-15"></a><span>    </span></span>
<span id="cb8-16"><a href="#cb8-16"></a><span>    return primes</span></span>
<span id="cb8-17"><a href="#cb8-17"></a><span>"""</span></span>
<span id="cb8-18"><a href="#cb8-18"></a>tokenizer <span>=</span> AutoTokenizer.from_pretrained(<span>'EleutherAI/gpt-neox-20b'</span>)</span>
<span id="cb8-19"><a href="#cb8-19"></a>memory_ids <span>=</span> tokenizer(code_snippet, return_tensors<span>=</span><span>'pt'</span>)[<span>'input_ids'</span>]</span>
<span id="cb8-20"><a href="#cb8-20"></a></span>
<span id="cb8-21"><a href="#cb8-21"></a>model <span>=</span> AutoModelForCausalLM.from_pretrained(<span>"normalcomputing/extended-mind-mpt-7b-chat"</span>, external_memories<span>=</span>memory_ids, trust_remote_code<span>=</span><span>True</span>)</span></code></pre></div>
<p>We ask the model to recall what our function does, first with a small <code>topk</code>.</p>
<div data-execution_count="8">
<div id="cb9"><pre><code><span id="cb9-1"><a href="#cb9-1"></a>prompt <span>=</span>  <span>"What does the function sieve_of_eratosthenes do?"</span></span>
<span id="cb9-2"><a href="#cb9-2"></a>input_ids <span>=</span> tokenizer(prompt, return_tensors<span>=</span><span>'pt'</span>)[<span>'input_ids'</span>]</span>
<span id="cb9-3"><a href="#cb9-3"></a></span>
<span id="cb9-4"><a href="#cb9-4"></a>out <span>=</span> model.generate(input_ids, max_length<span>=</span>input_ids.size(<span>-</span><span>1</span>)<span>+</span><span>100</span>, topk<span>=</span><span>2</span>)</span>
<span id="cb9-5"><a href="#cb9-5"></a><span>print</span>(tokenizer.decode(out[<span>0</span>]))</span></code></pre></div>
<div>
<pre><code>What does the function sieve_of_eratosthenes do?
The function sieve_of_eratosthenes is a Python function that implements the Sieve of Eratosthenes algorithm to generate all prime numbers up to a given limit.

The Sieve of Eratosthenes is a simple algorithm that generates all prime numbers up to a given limit. It works by creating a list of all integers from 2 to the given limit, and then iteratively marking the multiples of each prime number as composite (not prime).</code></pre>
</div>
</div>
<p>We see that with a small <code>topk</code> the model abstracts away the details from the code, providing a natural language description of what the code does. Now let’s try with a larger <code>topk</code>.</p>
<div data-execution_count="9">
<div id="cb11"><pre><code><span id="cb11-1"><a href="#cb11-1"></a>out <span>=</span> model.generate(input_ids, max_length<span>=</span>input_ids.size(<span>-</span><span>1</span>)<span>+</span><span>100</span>, topk<span>=</span><span>14</span>)</span>
<span id="cb11-2"><a href="#cb11-2"></a><span>print</span>(tokenizer.decode(out[<span>0</span>]))</span></code></pre></div>
<div>
<pre><code>What does the function sieve_of_eratosthenes do?(limit):
        primes.append(True)
        for i in range(2, int(limit**0.5) + 1):
            if sieve[i]:
                break
        else:
            for i in range(2, int(limit**0.5) + 1):
                if i % 2 == 0:
                    sieve[i] = False
    
    return primes
```

This implementation of the S</code></pre>
</div>
</div>
<p>Now the model outputs much closer to verbatim code, while abstracting away some variable names. This is the kind of nuanced stylistic choice is very hard to achieve using naive prompting and RAG methods without developing many point solutions specific to the data and prompt. More importantly, this kind of experiment gives us small clues into how the model actually reasons over these key-value pairs. At Normal, we hope to combine work on mechanistic interpretability methods with extended mind transformers, building a unified system for understanding how models store facts and reason over them.</p>
</section>
<section id="explainability">
<h2 data-anchor-id="explainability">Explainability</h2>
<p>Clark and Chalmers write in their paper: “By embracing an active externalism, we allow a more natural explanation of all sorts of actions”, and indeed this is true for our active externalism as well. Using attention weights, we can highlight which memories were used during each generation step. Here we highlight the memories used when generating the correct token “1971”. Since we retrieve memories per layer, per head, we display the mode.</p>
<div>
<figure>
<p><img src="https://storage.googleapis.com/normal-blog-artifacts/extended-mind-transformers/explainability.png"></p>
<figcaption>Tokens retrieved during the generation of token “1971”</figcaption>
</figure>
</div>
<p>Simple methods like this are just the beginning, but granular citations, in fact causal citations at all, are currently impossible using methods like RAG. The best we can get is highlighting those sections that were chosen to include in context. Using self-attention weights can perhaps buy you something, but this is unwieldy data and it’s explanatory power has been <a href="https://arxiv.org/abs/1902.10186">questioned</a>.</p>
</section>
<section id="creating-external-memories">
<h2 data-anchor-id="creating-external-memories">Creating external memories</h2>
<p>There are many interesting hyperparameters to discuss related to active externalism. Alternative masking strategies, restricting active externalism to some subset of decoder layers, and evaluating the role model size plays are all important discussions. We leave most of the discussion for more technical forthcoming papers. But we felt it was important to mention briefly the hyperparameters used in generating the external memories. We create our external memories (at each layer) by passing those external contexts through our model, just like inference. Then we save the internal representations the model generated, and attend to them later. If our external memories are longer than the model’s maximum sequence length, we’ll usually want to generate our representations using a stride. This ensures that all tokens are conditioned on at least stride-length number of previous tokens. Intuitively, all our memories will have “seen” some reasonable amount of context. However, there are situations where increased context may not be aligned with the model’s <em>best</em> representation of the data. For instance, representations of numerical or log-type data may benefit from using a smaller sequence or stride length.</p>
</section>
<section id="summary">
<h2 data-anchor-id="summary">Summary</h2>
<p>At Normal, we believe that there remains a wealth of opportunity to uncover by approaching today’s fractured, albeit proliferative, Enterprise AI landscape from a first principles point of view – even, and arguably especially, where early consensus has begun to form. We strongly believe that interdisciplinary perspectives and research are essential for advancing the field, a fundamentally and historically cross-sectional and constantly evolving discipline.</p>
<p>In “The Extended Mind” Clark and Chalmers conjecture: “In the distant future we may be able to plug various modules into our brain to help us out: a module for extra short-term memory when we need it.”</p>
<p>While this remains a distant goal for humans, we propose a method for achieving exactly this kind of short-term memory boost for LLMs. We’ve shown how a simple and natural extension of the self-attention mechanism for LLMs enables SoTa performance on retrieval tasks over long documents, uncertainty awareness, abstraction levers, granular explainability, and perhaps even given us some insight into the way these models reason internally.</p>
</section>
<section id="whats-next">
<h2 data-anchor-id="whats-next">What’s next</h2>
<p>We’re excited to extend these methods to models that use rotary and relative position encodings.</p>
<p>Making causal citations an out-of-the-box feature is also high on our list.</p>
<p>Distilling the information from the joint evolution of generations and choices of k into an uncertainty metric is another area we’re investing in.</p>
<p>Finally, continuing to develop and run comprehensive benchmarks will be crucial for building a robust understanding of the benefits provided by active externalism.</p>
</section>
<section id="references">
<h2 data-anchor-id="references">References</h2>
<div id="refs" role="list">

<p>
Burtsev, Mikhail S., Yuri Kuratov, Anton Peganov, and Grigory V. Sapunov. 2021. <span>“Memory Transformer.”</span> <a href="https://arxiv.org/abs/2006.11527">https://arxiv.org/abs/2006.11527</a>.
</p>
<div id="ref-clark-chalmers" role="listitem"><p>
Clark, Andy, and David Chalmers. 1998. <span>“The Extended Mind.”</span> <em>Analysis 58</em>, no. 1: 7–19. <a href="http://www.jstor.org/stable/3328150">http://www.jstor.org/stable/3328150</a>.
</p></div>
<p>
Liu, Nelson F., Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. <span>“Lost in the Middle: How Language Models Use Long Contexts.”</span> <a href="https://arxiv.org/abs/2307.03172">https://arxiv.org/abs/2307.03172</a>.
</p>
<p>
Martins, Pedro Henrique, Zita Marinho, and André F. T. Martins. 2022. <span>“<span>\(\infty\)</span>-Former: Infinite Memory Transformer.”</span> <a href="https://arxiv.org/abs/2109.00301">https://arxiv.org/abs/2109.00301</a>.
</p>
<p>
Press, Ofir, Noah A. Smith, and Mike Lewis. 2022. <span>“Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation.”</span> <a href="https://arxiv.org/abs/2108.12409">https://arxiv.org/abs/2108.12409</a>.
</p>
<p>
Sukhbaatar, Sainbayar, Edouard Grave, Guillaume Lample, Herve Jegou, and Armand Joulin. 2019. <span>“Augmenting Self-Attention with Persistent Memory.”</span> <a href="https://arxiv.org/abs/1907.01470">https://arxiv.org/abs/1907.01470</a>.
</p>
<p>
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. <span>“Attention Is All You Need.”</span> <a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>.
</p>
<p>
Wu, Yuhuai, Markus N. Rabe, DeLesley Hutchins, and Christian Szegedy. 2022. <span>“Memorizing Transformers.”</span> <a href="https://arxiv.org/abs/2203.08913">https://arxiv.org/abs/2203.08913</a>.
</p>
</div>


</section>


<div id="quarto-appendix"><section id="footnotes" role="doc-endnotes"><h2>Footnotes</h2>

<ol>
<li id="fn1"><p>Indeed, retrieval has thus far become a <a href="https://www.sequoiacap.com/article/generative-ai-act-two/">table stakes</a> part of the modeling stack for building LLM apps.<a href="#fnref1" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p><strong>RAG</strong>, a popular method for tackling the short context length of LLMs in application settings, attempts to identify the most salient information in a long text for a given query or task, such that the long context can be cut down to “fit in memory”. This is accomplished using a choice of sentence embedding that’s usually external to the model, chunking the long text and comparing with the query vector using a similarity or distance metric. Many <a href="https://blog.normalcomputing.ai/posts/2023-09-12-supersizing-transformers/[https://python.langchain.com/docs/integrations/retrievers]">open sourced projects</a> have made implementing such a strategy easier, and the success of <a href="https://www.forbes.com/sites/adrianbridgwater/2023/05/19/the-rise-of-vector-databases/?sh=4472652914a6">“vector databases”</a> demonstrates the rapid adoption of such methods.<a href="#fnref2" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Although there’s no technical reason we can’t throw an arbitrarily long sequence into context, performance using today’s models will drop off quickly after we exceed the sequence length the model saw during training. This inability to generalize is largely due to the use of positional embeddings. While originally (in <span data-cites="vaswani2023attention">Vaswani et al. (<a href="#ref-vaswani2023attention" role="doc-biblioref">2023</a>)</span>) only applied once at the beginning of the encoder/decoder stack, in today’s GPT-style transformers positional encodings are usually incorporated at the bottom of each decoder layer. These are unique constants which are either added or multiplied to hidden states in order to encode the index of each token in the sequence. Unless the model is trained further to expect a wider range of positional values, these new tokens quickly become out of distribution. Even given an infinitely long context, faithfully retrieving facts from very long sequences remains a challenge. Recent experiments show that models still struggle to use all the information provided in the larger context window - often forgetting things in the middle in particular, as they show in <span data-cites="liu2023lost">Liu et al. (<a href="#ref-liu2023lost" role="doc-biblioref">2023</a>)</span>.<a href="#fnref3" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>The architecture described in <span data-cites="martins2022inftyformer">Martins, Marinho, and Martins (<a href="#ref-martins2022inftyformer" role="doc-biblioref">2022</a>)</span> continuously compresses long text inputs such that the text always fits in memory. This has the obvious advantage of supporting input sequences of “infinite” length, but the weakness of summarizing the past such that it necessarily contains less detail. A coarse-grained/RAG analog to this might be using the language model itself to iteratively summarize past inputs and then passing the summary into context. In <span data-cites="sukhbaatar2019augmenting">Sukhbaatar et al. (<a href="#ref-sukhbaatar2019augmenting" role="doc-biblioref">2019</a>)</span>, the authors suggest replacing the feed-forward mechanism in each decoder layer with another attention block, and interpret this “unified mechanism” as an aggregation of global and contextual information. The creative contributors in <span data-cites="burtsev2021memory">Burtsev et al. (<a href="#ref-burtsev2021memory" role="doc-biblioref">2021</a>)</span> propose introducing a <code>[mem]</code> token which they hope the model will learn to leverage as space for storing global information. They implement various decoder architectures which attempt to enforce this with varying strictness. Folks at <a href="https://www.mosaicml.com/blog/mpt-7b">Mosaic</a> have combatted the lack of generalizing position encodings by using attention with linear biases (as presented by <span data-cites="press2022train">Press, Smith, and Lewis (<a href="#ref-press2022train" role="doc-biblioref">2022</a>)</span>).<a href="#fnref4" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p><span data-cites="clark-chalmers">Clark and Chalmers (<a href="#ref-clark-chalmers" role="doc-biblioref">1998</a>)</span><a href="#fnref5" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>While the authors of this paper believe the model needs to be trained from scratch or at least fine-tuned to be able to make sense of the extra retrieved tokens, we show that using models trained with ALiBi can make sense of these external key-value pairs innately. While they use a non-differentiable cache on one layer, we cache on every decoder layer.<a href="#fnref6" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>I.e., the model interprets those retrieved memories as being some constant distance away from the tokens it considers local context. For simplicity’s sake, we choose this constant index to be that directly following the last in-context index. I.e. if we pass the model a sequence of 1200 tokens, the memories in context will all be assigned position 1201. Certainly there’s room to experiment here - for instance you might choose to bias weights closer to the beginning of the memories more than those toward the end - but we find this is a reasonable and effective choice. We hypothesize that these methods will be effective for models trained with relative positional encodings as well, and will pursue this end in future work.<a href="#fnref7" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>a popular mechanism for speeding up inference, as a GPT-style transformer’s output only depends on the previous inputs<a href="#fnref8" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>We support using <a href="https://github.com/facebookresearch/faiss">FAISS</a> in our implementation<a href="#fnref9" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>We find .25 to be a good choice.<a href="#fnref10" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>https://developer.ibm.com/exchanges/data/all/wikitext-103/<a href="#fnref11" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>We use OpenAI’s Ada embeddings, and chunk our document into sequences of 500 tokens with no overlap. We order the documents such that the most similar content is closest to the prompt.<a href="#fnref12" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>Each split has on average 200 samples, with more samples in the 2k split and fewer as documents become longer.<a href="#fnref13" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p>Usually, as measured by cross-entropy<a href="#fnref14" role="doc-backlink">↩︎</a></p></li>
<li id="fn15"><p>Unsure in an epistemic way, i.e.&nbsp;the model didn’t observe this fact during training/can’t infer from the context<a href="#fnref15" role="doc-backlink">↩︎</a></p></li>
</ol>
</section><section><h2>Reuse</h2></section></div></main> <!-- /main -->


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Low current around roots boosts plant growth (196 pts)]]></title>
            <link>https://www.nature.com/articles/d44151-023-00162-5</link>
            <guid>38256137</guid>
            <pubDate>Mon, 13 Nov 2023 22:16:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/d44151-023-00162-5">https://www.nature.com/articles/d44151-023-00162-5</a>, See on <a href="https://news.ycombinator.com/item?id=38256137">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-container-type="article" data-component="article-container">
        
            
        

        
            
        
        
            
                <main>
                    <article data-track-component="news" lang="en">
                        
<div>
    <header>
        <div>
            <ul data-test="article-identifier">
                <li data-test="article-category"><span>RESEARCH HIGHLIGHT</span></li>
                <li><time datetime="2023-10-30">30 October 2023</time></li>
                
            </ul>

            

            <div>
                
                <p>
                    It speeds up photosynthesis and increases stress tolerance
                </p>
            </div>
        </div>
        
    </header>
    
</div>

            
        


        
            
                
                    


                    
                        
                    
                
            

            
                
                <div>
                    <figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d44151-023-00162-5/d44151-023-00162-5_26238516.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d44151-023-00162-5/d44151-023-00162-5_26238516.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="" loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d44151-023-00162-5/d44151-023-00162-5_26238516.jpg">
  <figcaption>
   <p><span>Plant growth in response to three types of electrode assemblies in the soil around chickpea plants. From left to right, C represents control with no voltage, SC indicates Short Circuit, OC is for Open Circuit and CC is Closed Circuit. Credit: S. Venkata Mohan</span><span></span></p>
  </figcaption>
 </picture>
</figure><p>Bioengineers have shown that low voltage generated in the soil around plant roots can be harnessed to stimulate growth in mung bean and chickpea plants<sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup>.</p><p>Soil microbes interact with plant roots to generate low voltage. Specific electrodes planted in the soil convert the voltage into a low current that acts as a stimulus for plant growth by boosting metabolic processes, including photosynthesis.</p><p>This method could become a viable option for sustainable agriculture, says a team at the CSIR-Indian Institute of Chemical Technology in Hyderabad.</p><p>To test the method, the scientists placed three types of electrode assemblies in the soil around mung bean and chickpea plants.</p><p>The team, which included S. Venkata Mohan, found that the low current generated by the electrodes increased plant height, leaf area, flowering, weight, and chlorophyll content in both plants. It also shortened the time it took the plants to go from their vegetative to reproductive phase.</p><p>Another finding was that levels of proline – a stress metabolite in roots and leaves – shot up. This suggests that the electrical stimulus could enhance the plants’ capacity for tolerating stress.</p><p>Besides causing changes in gene expression patterns, it induced an abundance of aquaporins – transmembrane channel proteins – which help water and solute transport in plant cells. This method could potentially be used to remove pollutants from contaminated soil, says Venkata Mohan.</p>
                </div>
            
                <p><em>doi: https://doi.org/10.1038/d44151-023-00162-5</em></p>

            <div id="references" aria-labelledby="Bib1"><h2 id="Bib1">References</h2></div>
            

            

            

            

        
            
                    </article>
                </main>
            
        

        
        <p><img src="https://www.nature.com/f8i7i9rb/article/d44151-023-00162-5" width="1" height="1" alt="">
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[We've learned nothing from the SolarWinds hack (138 pts)]]></title>
            <link>https://www.macchaffee.com/blog/2023/solarwinds-hack-lessons-learned/</link>
            <guid>38255923</guid>
            <pubDate>Mon, 13 Nov 2023 21:56:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.macchaffee.com/blog/2023/solarwinds-hack-lessons-learned/">https://www.macchaffee.com/blog/2023/solarwinds-hack-lessons-learned/</a>, See on <a href="https://news.ycombinator.com/item?id=38255923">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
        <p>Back in 2020, A Russian state-sponsored group got into SolarWinds' build system and inserted  command and control (c2) code into a routine software update for a network monitoring tool called Orion (<a href="https://en.wikipedia.org/wiki/2020_United_States_federal_government_data_breach">wiki link</a>). It was all over the news, and for good reason given the extent of the breach (into particularly sensitive parts of the US government) and the lengthy recovery process <a href="https://www.businessinsider.com/russia-hack-may-take-years-undo-bossert-2020-12">which will likely take years</a>. Given its high profile, I'm shocked to report that I feel very little has been learned from that attack.</p>
<p>To me, the hack was a wake-up call about how the way we install and run software is insecure by design and needs a rework, maybe using <a href="https://en.wikipedia.org/wiki/Capability-based_security">capabilities-based security</a>. But all I hear about is a bunch of solutions that kinda miss the point. Let's go over all of those first.</p>
<h2 id="we-should-sign-and-verify-all-our-dependencies">"We should sign and verify all our dependencies"</h2>
<p>In the wake of the SolarWinds hack, interest in "securing the software supply chain" grew considerably, including <a href="https://www.nist.gov/itl/executive-order-14028-improving-nations-cybersecurity">a May 2021 executive order</a> telling NIST/CISA to develop some guidelines about the subject. The <a href="https://slsa.dev/">Supply-chain Levels for Software Artifacts (SLSA)</a> framework also launched that same year and has been steadily growing in popularity.</p>
<p>Don't get me wrong: I appreciate the extra interest in this area. However, the fact remains that malicious code can be signed and verified too, depending on how deeply in the supply chain the attackers are. And they can get pretty deep with state-sponsored cyber criminal skills. Anything could happen in the background of your CI worker (or your laptop) between when you execute <code>git checkout &lt;tag&gt;</code> and <code>make</code>. Any checksums you generate or check can be modified right before you check them. Or maybe your <code>/usr/local/bin/sha256sum</code> has been tampered with. The list goes on.</p>
<p>When we're talking about getting all major open source projects (which have little to no funding) to add enough security to resist nation-states (which have plenty of funding), the math simply doesn't add it.</p>
<h2 id="we-should-disable-automatic-updates">"We should disable automatic updates"</h2>
<p>Automatic updates are a tradeoff, I'll grant that. You are trusting a vendor to not ship a bad update in exchange for getting security fixes ASAP. However, just think for half a second about how the SolarWinds hack worked. The attackers snuck some code into an <em>opaque, propriety, binary blob that <a href="https://en.wikipedia.org/wiki/2020_United_States_federal_government_data_breach#SolarWinds_exploit">lied dormant for 12-14 days</a> before doing anything strange</em>. There is absolutely no way we can perform a full binary analysis of every new version of every binary blob that powers modern IT.</p>
<p>Automating updates are generally recommended because it "helps to ensure the
timeliness and completeness of system patching operations", as mentioned in <a href="https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf">NIST 800-53§3.19</a>. If you do have the time for manual reviews AND audits that the manual updates have been applied, that's preferable, but obviously that takes a lot of time. For everything else, automation keeps you safer. The SolarWinds hack changed nothing about that calculus.</p>
<h2 id="we-should-deploy-another-agent-to-detect-these-kinds-of-hacks">"We should deploy another agent to detect these kinds of hacks"</h2>
<p>This idea pre-dates the SolarWinds hack, but it's still around in full force. Many security standards recommend or even require a <a href="https://en.wikipedia.org/wiki/Security_information_and_event_management">Security Information Event Management (SIEM)</a> system. Maybe you'd like to deploy <a href="https://www.solarwinds.com/security-event-manager/siem-tools">SolarWinds' own SIEM product</a>? It should be obvious that installing yet-another highly-privileged agent on all your servers is the exact reason why the SolarWinds hack was as devastating as it was. I appreciate the thought that goes into e.g. <a href="https://www.datadoghq.com/blog/engineering/secure-publication-of-datadog-agent-integrations-with-tuf-and-in-toto/">DataDog's agent build process</a>, but DataDog's agent still runs <a href="https://github.com/DataDog/datadog-agent/blob/fd57de7ae6c889b45f99b57c36896c3c161dfdd2/omnibus/config/templates/datadog-agent/systemd.service.erb">without any kind of systemd sandboxing</a>, which gives it more permissions than it needs. It's one bad world-readable SUID file away from a full takeover, which is just <a href="https://github.com/RoqueNight/Linux-Privilege-Escalation-Basics">one of many local privilege escalation routes</a> that exist on Linux.</p>
<p>Having visibility into your own network is a good idea, but vendors rarely care to follow the principle of least privilege, frequently just demanding full root access (like for <a href="https://static.tenable.com/documentation/nessus_compliance_checks.pdf#page=11">Nessus compliance scans</a> which are entirely read-only). If you need to stop supply chain attacks, more privileged agents will just significantly broaden your exposure to supply chain attacks.</p>
<h2 id="the-inconvenient-truth-about-how-to-actually-fix-this">The Inconvenient Truth about how to actually fix this</h2>
<p>Reading through <a href="https://www.cisa.gov/sites/default/files/publications/defending_against_software_supply_chain_attacks_508_1.pdf">some of NIST's guidance</a> hints at the real problem in my opinion: "many third-party software products require privileged access". This is an "insecure by design" problem. NIST continues: "Even when a product can effectively operate on a network with reduced privileges, products will oftentimes default to asking for greater privileges during installation to ensure the product’s maximum effectiveness across different types of customer networks. Customers often accept third-party software defaults without investigating further, allowing additional accessibility vectors".</p>
<p>If you want to prevent that from being abused, NIST's recommendations in that document basically amount to "build an enormous, mature security organization". That implicitly assumes everyone keeps the "business as usual" way of installing and running third party software. It doesn't have to be this way.</p>
<h2 id="the-quite-ambitious-solution">The (quite ambitious) solution</h2>
<p><strong>We should run software in a way where we don't really care if it has a vulnerability, because it will happen</strong>. Just like how no good auth system relies on user-memorized passwords alone anymore; we have 2FA and passkeys now which remove that human element as part of their design. That same energy should have been applied in the wake of the SolarWinds hack, but it still feels like "security by design" is a fringe belief.</p>
<p>One idea that could help is <a href="https://en.wikipedia.org/wiki/Capability-based_security">capabilities-based security</a>. The idea is that by default, running software can't do much of anything unless it is given an unforgeable "capability" to do things like access files, the network, particular syscalls, etc. This is fairly incompatible with UNIX and Windows because (aside from root/administrator access), programs have the permission to do a LOT of damage by default, and removing any of those permissions would break a lot programs. If you want security by design, backwards-compatibility is a sacrifice you'll have to make.</p>
<p>Capabilities-based security isn't easy to implement. Some weaknesses I've found in the wild include:</p>
<ul>
<li>Making capabilities too coarse-grained, like having a general "write/edit" permission with no separate "create" or "append" permission, meaning your backup tool is still ripe for a ransomware attack.</li>
<li>Making the default capabilities too permissive, like Docker's default seccomp rules which prioritized compatibility over security.</li>
<li>Making fine-grained capabilities that actually imply other capabilities, like <a href="https://github.com/denoland/deno/issues/2128">Deno's "--allow-run" permission being equal to "--allow-all"</a>. Or Kubernetes' <a href="https://kubernetes.io/docs/concepts/security/secrets-good-practices/#least-privilege-secrets">"create pod" permissions implying "get secret" permissions</a>.</li>
<li>Packaging software alongside the capabilities that constrain it, like RPMs with systemd units that include sandboxing. A supply chain attack could easily remove the sandboxing. You need something like what browser extensions do where new permissions require explicit approval from the user.</li>
<li>Making capabilities apply to too-course of a boundary, like giving one set of capabilities to a complex, multi-threaded process that includes a lot of third-party code for instance. Any sub-component of that process could be tricked into abusing one of its capabilities. <a href="https://github.com/austral/austral">Language-based capabilities</a> have the edge here.</li>
<li>Lacking tools for knowing which capabilities a given program needs. This kills adoption, since not many developers could tell you exactly which kernel features their code uses off the top of their head.</li>
</ul>
<p>If we could agree on a good, standardized capabilities model for software and everyone starts using it, we will have reached security Nirvana.</p>
<ul>
<li>We can keep the benefits of huge dependency trees without the risks!</li>
<li>IT organizations can spend significantly less time on remediating vulns since the vast majority of vulns will not be exploitable!</li>
<li>Lateral movement becomes nearly improbable!</li>
<li>We don't have to hold OSS communities to rigorous security standards that even well-funded companies struggle with!</li>
<li>And more!</li>
</ul>
<h2 id="back-to-reality">Back to reality</h2>
<p>We're still talking about something that's probably a decade away or more, but given the benefits and the constant string of high-profile hacks like the SolarWinds hack, I'm just upset the ball <em>still</em> isn't rolling in the right direction 3 years later.</p>
<p>But history shows it's not impossible, at least not if you're the <a href="https://en.wikipedia.org/wiki/List_of_public_corporations_by_market_capitalization">richest company on the planet</a>. Over time (particularly since <a href="https://www.cultofmac.com/173128/new-ios-6-privacy-settings-limit-access-to-photos-contact-calendars-and-more/">iOS 6</a>), less and less permissions have been granted to iOS apps by default, instead requiring apps to request those permissions from users explicitly. It's still not perfect (like access to contacts still being a binary "yes/no"), but every permission clawed back from the default set required breaking backwards compatibility, a phrase rarely uttered in regard to the Linux and Windows kernels.</p>
<p>If you have been an iOS developer since 2012, I'm sorry you had to go through that, but your extra work has been profoundly important to the privacy and security of mobile OSes. I'd like to see that same <a href="https://www.macchaffee.com/blog/2023/ethics-self-attestation/">principled</a> energy brought to desktop and server OSes. If we don't, the next SolarWinds-like hack is just around the corner.</p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Zelle finally caves after years of refusing to refund scam victims (170 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2023/11/zelle-finally-caves-after-years-of-refusing-to-refund-scam-victims/</link>
            <guid>38255884</guid>
            <pubDate>Mon, 13 Nov 2023 21:53:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2023/11/zelle-finally-caves-after-years-of-refusing-to-refund-scam-victims/">https://arstechnica.com/tech-policy/2023/11/zelle-finally-caves-after-years-of-refusing-to-refund-scam-victims/</a>, See on <a href="https://news.ycombinator.com/item?id=38255884">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/11/GettyImages-1247087701-800x532.jpg" alt="Zelle finally caves after years of refusing to refund scam victims">
      <figcaption></figcaption>  </figure>

  




<!-- cache hit 168:single/related:752c3e136703cdfc573fa06d951a8541 --><!-- empty -->
<p>After <a href="https://arstechnica.com/tech-policy/2022/10/zelle-fraud-is-on-the-rise-and-many-victims-are-denied-refunds/">scammers spent years swiping hundreds of millions from Zelle users</a> by inducing people to authorize fraudulent payments, <a href="https://www.warren.senate.gov/oversight/reports/new-report-by-senator-warren-zelle-facilitating-fraud-based-on-internal-data-from-big-banks">lawmakers were horrified</a> to discover in fall 2022 that "the vast majority" of defrauded Zelle users never got their money back. To regulators, it seemed like Zelle was shirking responsibility for policing this increasingly common fraudulent activity on its payments platform.</p>
<p>But now, Zelle has changed its mind and is working harder to protect users from imposter scams. On Monday, Zelle confirmed that at the end of June, the payments app finally started refunding users targeted by scammers.</p>
<p><a href="https://www.reuters.com/technology/cybersecurity/payments-app-zelle-begins-refunds-imposter-scams-after-washington-pressure-2023-11-13/">According to Reuters</a>, this was possible because Zelle's network operator, Early Warning Services (EWS), found a solution that lets Zelle's network of 2,100 financial firms off the hook for reimbursing transactions where "potentially billions of dollars" might be stolen by imposter scammers. Instead of expecting financial partners to foot the bill to cover this fraudulent activity, Zelle simply "implemented a mechanism that allows banks to claw back funds from the recipient's account and return them to the sender."</p>
<p>"As the operator of Zelle, we continuously review and update our operating rules and technology practices to improve the consumer experience and address the dynamic nature of fraud and scams," an EWS spokesperson told Ars. "As of June 30, 2023, our bank and credit union participants must reimburse consumers for qualifying imposter scams, like when&nbsp;a scammer impersonates a bank to trick a consumer into sending them money with Zelle. The change ensures consistency across our network and goes beyond legal requirements."</p>                                            
                                                        
<p>This is the first time EWS has provided details on its new policy to refund victims of the Zelle imposter scam, Reuters reported. It's a major policy reversal that Reuters said was likely prompted to spare banks and payments apps from stricter regulatory interventions that would require refunds for every scam victim. (Currently, the US only requires banks to refund any fraudulent payments made without customers' authorization.)</p>
<p>The chief fraud risk officer at EWS, Ben Chance, reiterated to Reuters that Zelle's new policy goes "well above existing legal and regulatory requirements."</p>
<p>It's unclear if regulators will be satisfied leaving this matter to banks and payment apps, though. There may be just too many people using payment apps to ignore gaps in laws intended to protect against financial fraud. Between 2018 and 2022, peer-to-peer (P2P) payments quadrupled in the US, the&nbsp;<a href="https://www.consumerfinance.gov/data-research/research-reports/issue-spotlight-analysis-of-deposit-insurance-coverage-on-funds-stored-through-payment-apps/full-report/">Consumer Financial Protection Bureau (CFPB) reported</a>, and by 2027, P2P payments "may reach nearly $1.6 trillion."</p>
<p>Meanwhile, as P2P payments have increased substantially, these imposter scams have become "the most-reported scam," targeting users "across all payment methods," the Federal Trade Commission (FTC) reported. In total, the FTC said that scam victims lost $2.6 billion last year alone.</p>
<p>The CFPB previously mulled new laws that would require lenders to reimburse scam victims, but a person familiar with the matter told Reuters that the CFPB may no longer be considering new protections, because Zelle's recent changes "have so far satisfied the agency." However, Senator Elizabeth Warren (D-Mass.)—who spearheaded the <a href="https://www.warren.senate.gov/oversight/reports/new-report-by-senator-warren-zelle-facilitating-fraud-based-on-internal-data-from-big-banks">probe</a> into Zelle imposter scams—told Reuters that she is not likely to stop monitoring the situation any time soon.</p>
<p>"Zelle's platform changes are long overdue,” Warren said. "The CFPB is standing with consumers, and I urge the agency to keep the pressure on Zelle to protect consumers from bad actors."</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Designing a programming language to speedrun Advent of Code (169 pts)]]></title>
            <link>https://blog.vero.site/post/noulith</link>
            <guid>38255808</guid>
            <pubDate>Mon, 13 Nov 2023 21:45:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.vero.site/post/noulith">https://blog.vero.site/post/noulith</a>, See on <a href="https://news.ycombinator.com/item?id=38255808">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="wrapper">
	<header>
		
		
		<p>
		
		2023-04-09
		(17333 words)
		
		<span>
			filed under
			<a href="https://blog.vero.site/category/cs">CS</a>
		</span>
		
		</p>
	</header>
	<article><p>“shouldn’t this have been published a few months ago?” yeah, probably. I even considered submitting it to the <a href="https://www.reddit.com/r/adventofcode/comments/z9he28/advent_of_code_2022_mistiltoe_elfucation/">AoC contest</a>. time is a real beast.</p>
<p>The title is clickbait. I did not design and implement a programming language for the sole or even primary purpose of leaderboarding on Advent of Code. It just turned out that the programming language I was working on fit the task remarkably well.</p>
<p>I can’t name just a single reason I started work on my language, <a href="https://github.com/betaveros/noulith">Noulith</a>, back in July 2022, but I think the biggest one was even more absurdly niche: I solve and write a lot of <a href="https://blog.vero.site/post/puzzlehunts">puzzlehunts</a>, and I wanted a better programming language to use to search word lists for words satisfying unusual constraints, such as, “Find all ten-letter words that contain each of the letters A, B, and C exactly once and that have the ninth letter K.”<a href="#fn1" id="fnref1"><sup>1</sup></a> I have a folder of ten-line scripts of this kind, mostly Python, and I thought there was surely a better way to do this. Not necessarily faster — there is obviously no way I could <a href="https://xkcd.com/1205/">save time on net by optimizing this process</a>. But, for example, I wanted to be able to easily share these programs such that others could run them. I had a positive experience in this with my slightly older golflang <a href="https://github.com/betaveros/paradoc">Paradoc</a>, which I had compiled into a WASM blob and <a href="https://betaveros.github.io/paradoc-rust/">put online</a> and, just once, experienced the convenience of sharing a <a href="https://betaveros.github.io/paradoc-rust/#aVVje0FwcVdwfH1mV8OYKXNqIHI=">short text processing program</a> through a link. (Puzzle: what does this program do?) I also wanted to write and run these programs while booted into a different operating system, using a different computer, or just on my phone.</p>
<p>As I worked on it, I kept accumulating reasons to keep going. There were other contexts where I wanted to quickly code a combinatorial brute force that was annoying to write in other languages; a glib phrasing is that I wanted access to Haskell’s list monad in a sloppier language. I also wanted an excuse to read <a href="https://craftinginterpreters.com/"><cite>Crafting Interpreters</cite></a> more thoroughly. But sometimes I think the best characterization for what developing the language “felt like” was that I had been possessed by a supernatural creature — say, the dragon from the <a href="https://en.wikipedia.org/wiki/Compilers:_Principles,_Techniques,_and_Tools">Dragon Book</a>. I spent every spare minute thinking about language features and next implementation steps, because I had to.</p>
<p>The first “real program” I wrote in Noulith was to brute force constructions for <a href="https://2022.galacticpuzzlehunt.com/puzzle/the-cube">The Cube</a>, for last year’s Galactic Puzzle Hunt in early August, and it worked unexpectedly well. I wrote a <code>for</code> loop with a 53-clause iteratee and the interpreter executed it smoothly. Eventually I realized that the language could expand into other niches in my life where I wanted a scripting language. For example, I did a few <a href="https://www.cryptopals.com/">Cryptopals challenges</a> in them. It would take a month or two before it dawned on me that the same compulsion that drove me to create this language would drive me to do Advent of Code in it. That’s just how it has to be.</p>
<p>This post details my thought process behind the design of this language. Some preliminary notes:</p>
<ul>
<li>I made a lot of unusual choices with this language, but none are particularly “deep” language features like Rust’s ownership checker, <a href="https://www.mercurylang.org/information/doc-latest/mercury_ref/Determinism.html#Determinism-categories">Mercury’s determinism checks</a> and <a href="https://tutorial.ponylang.io/reference-capabilities/guarantees.html#rights-are-part-of-a-capability">Pony’s reference guarantees</a> (three examples lifted verbatim from <a href="https://morepablo.com/2022/09/so-you-re-using-a-weird-language.html">“So You’re Using a Weird Language”</a>). The immutability semantics are a little interesting, but still don’t have as far-reaching implications. To the extent the language breaks any new ground, it’s probably the boundaries of taste in adding syntax sugar. Still, syntax is fun.</li>
<li>A lot of the decisions I made are deeply entangled with each other. I sort of try to string them together into a linear narrative for presentation’s sake, often also pretending that I researched how a bunch of other languages approached the same decision before making it myself, but the existence of such a narrative is mostly fictitious.</li>
<li>Pixel’s <a href="http://rigaux.org/language-study/syntax-across-languages.html">syntax across languages</a> page was immensely useful.</li>
<li>Noulith was intended as a personal programming language first and foremost, deeply informed by and optimized for how I, specifically, think about and write code. I think of it as a “home-cooked” programming language, a la <a href="https://www.robinsloan.com/notes/home-cooked-app/">Robin Sloan’s home-cooked app</a>. I did not create this language with the expectation or hope that even a single other person in the world would want to learn it; the amount of interest it briefly garnered was a (mostly) pleasant surprise. I also did not intend for this programming language to work well for programs that are longer than 100 lines or so, even if written by me. My best-case scenario is if one of the weird syntax experiments I did with this language vaguely influences a better thought-out feature in a major programming language.</li>
<li><p>There are two concepts from interface design, <strong>internal consistency</strong> and <strong>external consistency</strong>, that are pretty obvious in hindsight but that I found useful to explicitly refer to below. Internal consistency refers to similar things within a single application working in similar ways, whereas external consistency refers to things in one application that are similar to things in other applications working in similar ways. Both are desirable since they make it easier to learn how to use the application: internal consistency means that users can learn things from one part of your application and apply them to another, while external consistency means that users can apply knowledge they might already have from other applications. But they can come into conflict with each other and with other desiderata.</p>
<p>So for example, internal consistency favors giving two built-in functions to append and prepend an item to a list names that are clearly related, so programmers who remember one can easily remember the other; while external consistency favors copying those names from an established programming language if possible, so programmers coming from that established language already know those names.</p>
All this is relevant because of the sometimes underappreciated consideration that a programming language is a user interface! I think this perspective is easy to lose sight of because “programmers” and “users” are usually different groups of people, but for a programming language, the user is the programmer writing code in it, distinct from the programmer implementing the language.</li>
<li><p>This post is too long — to quote the Mark Twain apology, I didn’t have time to write a short one — and as I finished it I realized that half of its <i>raison d’être</i> is just to provide an excuse for me to incidentally mention a bunch of interesting features and corner cases of other programming languages. So if you’d rather just read that, I collected most of the <a href="#the-fun-fact-roulette">fun facts into an ending section</a>.</p></li>
</ul>
<h3 id="literals-identifiers-and-data-types">Literals, identifiers, and data types</h3>
<p>First things first. On a character-to-character, token-to-token level, what does the language <em>look like</em>?</p>
<p>There are a lot of questions that are too basic to be interesting, such as what numeric and string literals look like. This doesn’t have much impact on the rest of the language, so I just copied a bunch of popular syntaxes. For numbers, other than the obvious decimal ones, I threw in binary <code>0b</code> literals, hexadecimal <code>0x</code> literals, arbitrary radix <code>36r1000</code> literals, scientific notation <code>1e100</code> literals, and complex number <code>1i</code> or <code>1j</code> literals. I even added base64 literals for kicks. Strings can use either single or double quotes, essentially what Python has. Were I to add, say, ternary literals, additional flavors of triple-quoted or raw strings, or a bunch of special escape sequences, nothing else would have to change and there would be nothing to say about their design.</p>
<p>Identifiers have a bit more depth. Like most languages, most Noulith identifiers consist of a letter (including <code>_</code>) followed by any number of alphanumeric characters. From Haskell I copied the ability for such identifiers to also contain (but not start with) apostrophes, which I think looks neat for denoting a new version or modified variant of a variable, like the prime symbol in math. Much more questionably, I also gave <code>?</code> the same treatment, with the goal of connoting variants of functions that returned <code>null</code> instead of erroring. In hindsight, I should perhaps not have muddled up the lexical syntax so much; a different convention, like a trailing <code>_</code> on alphanumeric identifiers, might have sufficed. Separately, Noulith supports identifiers consisting of only symbolic characters as well, also like in Haskell. We’ll discuss how the parser treats them later.</p>
<p>I also had to think about the basic data types we want to support, but before that I had to decide if Noulith would be statically or dynamically typed. I like static types, but only if they’re sufficiently expressive and supported by good inference, and I like not having to implement any of that stuff even more, so I settled for dynamic typing.</p>
<p>I won’t list all the data types I ended up with here, but some of the basic ones are <code>null</code>, numbers, strings, lists, and dictionaries. Though <code>null</code> has a justifiably bad reputation, it’s hard to avoid in a dynamically typed language; it’s too useful as, for example, the return value of functions that don’t explicitly return anything. Notable omissions are booleans, sets, and any kind of dedicated error/exception type. I don’t think they are bad things to have in a language, I just thought they were significantly easier to work around than to implement, and I couldn’t be bothered to put in the work:</p>
<ul>
<li>Instead of true and false, you can just use numbers 0 and 1, which is close to how C and Python do it.</li>
<li>Instead of sets, you can just use dictionaries where the values are null, so <code>{a} == {a: null}</code>. This still works well because <code>in</code> can just test for the presence of the key in a dictionary, a behavior also exactly like Python.</li>
<li>Instead of dedicated error types, you can just use… non-dedicated data types. You can compose a string with an error message and throw it. I don’t like this state of affairs — I think having dedicated or at least more structured error types really is a good idea, maybe purely for the principle of the thing, but design and implementation both take effort, and it’s hard to argue for prioritizing this when I only use Noulith for short throwaway scripts.</li>
</ul>
<p>I did not think hard about any these decisions, but they had consequences we’ll discuss later. For the syntax of lists, I chose to use square brackets <code>[]</code>, and for dictionaries, curly brackets <code>{}</code>, yet again exactly like Python. This also has the benefit that valid JSON is valid Noulith<a href="#fn2" id="fnref2"><sup>2</sup></a>.</p>
<p>Finally, with regard to variable scoping, Noulith has a simple approximation of lexical scoping, but names are not namespaced or qualified whatsoever. All built-ins live in the same global namespace. This is another thing that’s bad but low priority.</p>
<h3 id="operators-and-functions">Operators and functions</h3>
<p>Things should get more interesting from here. Next up: how do you perform basic arithmetic operations? I am used to adding two numbers like <code>x + y</code>. There are alternatives: in Lisps, for example, arithmetic operations are called prefix like <code>(+ x y)</code> for homoiconicity; in stack-based languages like Forth and GolfScript, they’re called postfix like <code>x y +</code>. Both approaches also make parsing much easier. Still, I decided either alternative would rather fundamentally slow me down as I tried to translate thoughts into code, so I stuck with the mainstream: basic arithmetic operations are infix.</p>
<p>Similarly, I decided that prefix unary minus was required. Which means that the <code>-</code> operator, if nothing else, has to be callable as either a prefix unary operator or an infix binary operator. We’ll return to this later.</p>
<p>Okay, what about function calls? There is again a popular syntax: <code>foo(bar, baz)</code>. The main alternative is simply juxtaposition (and heavy <a href="https://en.wikipedia.org/wiki/Currying">currying</a> so that this does the right thing), as in Haskell and MLs (OCaml, SML, F♯, etc.): <code>foo bar baz</code>. A smaller deviation is to support the popular syntax but also allow the parentheses to be omitted, as in Perl and Ruby: <code>foo bar, baz</code>.</p>
<p>Using mere juxtaposition as function invocation sort of conflicts with binary operators, which are just two arguments juxtaposed around an operator: is <code>x + y</code> calling addition on <code>x</code> and <code>y</code>, or calling <code>x</code> with arguments <code>+</code> and <code>y</code>? Most languages don’t have this problem because they have a fixed set of binary operators that are totally distinct from identifiers, but I wanted to be able to add lots of operators without enshrining them into the syntax or needing a lot of boilerplate in the implementation. Haskell and MLs resolve this conflict by parsing identifiers made from operator symbols, like <code>+</code>, as binary operators, while parsing identifiers made from alphanumerics, like <code>x</code> and <code>y</code>, as functions to be called with juxtaposition. So, something like <code>a b + c d</code> is parsed as <code>(a(b)) + (c(d))</code>. However, the approach I ended up liking the most is Scala’s, whose parser doesn’t draw this distinction between types of identifiers (except to determine precedence, which we’ll come back to later; and its <em>lexer</em> does draw this distinction, as does Noulith’s, so that <code>x+y</code> is three tokens while <code>xplusy</code> is one). Scala’s grammar just says that <code>a b c</code> is always a binary operator where <code>b</code> is called with <code>a</code> and <code>c</code>.</p>
<p>Well, Scala actually says that <a href="https://docs.scala-lang.org/tour/operators.html">operators are methods</a>:<a href="#fn3" id="fnref3"><sup>3</sup></a> the <code>b</code> method of <code>a</code> is called with <code>c</code> as its sole argument. But I didn’t particularly want methods in my language, as they seemed like an unnecessary layer of abstraction for my goals. So in Noulith, <code>b</code> is looked up in the same scope as the identifiers around it. One can view this as combining Scala’s approach with <a href="https://en.wikipedia.org/wiki/Uniform_Function_Call_Syntax">Uniform Function Call Syntax</a>, seen in languages like D and Nim.</p>
<p>Why is this approach great?</p>
<ul>
<li>It’s simple: after identifiers are lexed, the parser doesn’t need to know their type.</li>
<li>It’s good for compositionality: it becomes easy to pass operators to other functions, like <code>zip(+, list1, list2)</code>.</li>
<li>And, well, it fits my personal taste: I like being able to use alphanumeric identifiers as infix operators, which we’ll talk about more in a bit. (You can have special syntax for doing so, like <a href="https://www.haskell.org/onlinereport/haskell2010/haskellch3.html#x8-240003.2">Haskell’s backticks</a>, but I thought that was ugly for something I wanted to use extensively.)</li>
</ul>
<p>But there’s a wrinkle we have to return to. I already mentioned I wanted to support unary minus, so <code>-a</code> should be the negation of <code>a</code>. But then how should an expression like <code>- - a</code> be parsed? Is it calling the middle <code>-</code> as a binary operator on the operands <code>-</code> and <code>a</code> flanking it, or applying unary minus twice to <code>a</code>? I still didn’t want to make <code>-</code> special in the syntax, so I decided I was okay with requiring parentheses to express the second intent, as in <code>-(-a)</code>, and saying that <code>(a b)</code> is a sort of special case where juxtaposition expresses a function call, wherein <code>a</code> is called with one argument, <code>b</code>.</p>
<p>On the other hand, I enjoy partially applying operators a lot. They’re useful for passing into higher-order functions to produce neat expressions like (Advent of Code <a href="https://adventofcode.com/2022/day/7">Day 7</a>) <code>some_list filter (&lt;= 100000) then sum</code> to sum all numbers that are at most 100000 in a list. This syntax I wanted to support is taken from Haskell, but <em>also</em> conflicts with unary minus. Is <code>(-3)</code> the number “negative 3” or the function that subtracts 3 from its input? Haskell resolves this by <em>specifically</em> carving out a <a href="https://www.haskell.org/onlinereport/haskell2010/haskellch3.html#x8-300003.5">syntactic special case for <code>-</code></a>; it is the only operator for which <code>(-x)</code> does not partially apply the first function in the juxtaposition.<a href="#fn4" id="fnref4"><sup>4</sup></a> For every other Haskell operator, say <code>+</code>, <code>(+x)</code> is a partially-applied function that, given an argument <code>a</code>, returns <code>a+x</code>. I chose to emulate this behavior by still having juxtaposing two expressions mean unary function application, but then just making most built-in functions support partial application when called with one argument, but not <code>-</code>.</p>
<p>On the gripping hand, I also decided to emulate Scala here and also offer the “section” <code>_ + x</code>, which is also a function that, given an argument <code>a</code>, returns <code>a + x</code>. These are strictly more powerful (e.g., for reasons explained later, <code>0 &lt; _ &lt; 10</code> is also a valid “section” that checks whether one argument <code>x</code> is between 0 and 10 — unlike Scala, where this wouldn’t work because it parses as comparing the lambda <code>0 &lt; _</code> to <code>10</code>), at the cost of requiring at most two extra characters, so the argument for having these and functions rampantly supporting partial application is much weaker. Still, for now, I am keeping both syntaxes out of inertia.</p>
<p>On the fourth hand, Haskell also allows partially applying functions on the other side of binary operators. For example, <code>(3-)</code> is the function that subtracts its argument from <code>3</code>. Noulith also copies this syntax by decreeing that, if <code>a</code> is not a function but <code>b</code> is, then <code>(a b)</code> is <code>b</code> partially applied with <code>a</code> as its first argument. This heuristic is flawed when both <code>a</code> and <code>b</code> are functions: for example, <code>&lt;&lt;&lt;</code> is the function composition operator, so that <code>(f &lt;&lt;&lt; g)(h)</code> is <code>f(g(h))</code>, but if you try to postcompose <code>sin</code> onto another function as <code>(sin &lt;&lt;&lt;)</code>, it won’t work. This specific case is easy to work around because you can write <code>(&gt;&gt;&gt; sin)</code> instead, but it’s definitely a drawback.</p>
<p>Before we spend some time looking at the implications of making everything an infix operator, I will mention that Noulith doesn’t (currently) support named arguments. It’s one of those things that I think would be nice to have, but isn’t a priority because it matters more in longer, more structured programs, and it also comes into mild tension with a heavily functional style. One way I’d characterize the allure of named arguments is that they’d allow you to ignore, for example, which of the following two definitions a function was defined with, and use them the same way:</p>

<p>Unfortunately, the difference does matter if you want to <code>map</code> or <code>zip</code> with <code>foo</code>. To keep ignoring it, either you’d have to wrap <code>foo</code> in a lambda to plumb the right inputs to the right named arguments each time, which loses most of the elegance of functional programming, or you’d have to make all these higher-order functions take the names of arguments to use when invoking the functions you provide them, which I think is annoying to implement and to use. Still, you could imagine a language that takes that plunge. Perhaps language support at a more fundamental level would make everything work out.</p>
<h4 id="coding-with-and-without-infix-functions">Coding with and without infix functions</h4>
<p>As I previously alluded to, I also like making everything an infix operator so I can call functions like <code>map</code> on a list by typing after the code for creating that list. This fits how I write code mentally: “I have this data, I will transform it in this way, then transform it in that way, then apply some final function and I’ll have my answer.” At each step I remember what form of the data is in my head and figure out what transformation I want to apply next.</p>
<p>To give a more concrete example, I’ll walk through 2022’s <a href="https://adventofcode.com/2022/day/1">first day of Advent of Code</a>. If I were to do it in Python, I might think to myself: okay, the puzzle input is a sequence of “paragraphs” (the name I mentally give to blocks of text separated by double newlines), so let’s break it up into such:</p>

<p>“Now for each paragraph we want to get all ints from it…” Like many leaderboarders, I have a <a href="https://blog.vero.site/post/advent-leaderboard#build-your-own-standard-library">prewritten function</a> <code>ints</code> that extracts all the integers from a string with a simple regex, but to use it I have to move my cursor to the start of the expression, type <code>map(ints,</code>, then move my cursor back to the end to add <code>)</code>.</p>

<p>“Then we want to sum all the integers in each paragraph…” Back to the start of the line, <code>map(sum,</code>, then back to the end, <code>)</code>.</p>

<p>“Finally take the max…” Rinse and repeat.</p>

<p>That’s six cursor jumps to write this simple four-step expression. Jumping to the start of the line is a relatively easy text editor operation, but if I were writing this expression to assign it to a variable, locating the start each time would be less fun. A language could avoid the cursor jumps back to the end of the line by making parentheses optional as in Perl or Ruby or something, but would still force me to write the <code>ints</code> map, the <code>sum</code> map, and the <code>max</code> call right-to-left in the order I thought of applying them. A complete solution to this issue has to make functions like <code>map</code> and <code>sum</code> callable postfix of the sequence being mapped or summed. This could be done by making them methods of lists, <code>puzzle_input.split("\n\n").map(ints)</code>, or by providing operators like <code>|&gt;</code> in F♯ and Elm. But our Scala-inspired solution not only achieves this, it dispenses with almost all the punctuation! Here’s the actual Noulith from my <a href="https://github.com/betaveros/advent-of-code-2022/blob/main/p1.noul">Day 1 solution</a> this year, where you can see the tokens in the same order as the steps in my thought process above.</p>
<pre><code>puzzle_input split "\n\n" map ints map sum then max</code></pre>
<p>One downside of this syntax is that it only supports calling binary operators, i.e., combining the expression you’re building on with exactly one other argument. However, this is easily extended to support unary operations with a built-in function that just performs reverse function application, as seen above with <code>then max</code>. Noulith provides two such built-ins, <code>then</code> and <code>.</code> (which have different precedences): <code>a.b</code> and <code>a then b</code> are both just <code>b(a)</code>. It’s less obvious how to chain functions that take three or more arguments, but some language decisions we’ll see in the next section actually make it pretty reasonable (not to mention that, as I observed in my previous <a href="https://blog.vero.site/post/golf">post about code golf</a>, functions that “naturally” take three or more arguments are surprisingly rare).</p>
<p>Before we move on, I want to point out that “being able to write code from left to right without backtracking” is a completely bonkers thing to optimize a programming language for. This should not be anywhere in the top hundred priorities for any “serious programming language”! Most code is read far more often than it’s written. An extra keystroke here or there is just maximally insignificant. Fortunately, Noulith is not a serious programming language, so I have no qualms about optimizing it for whatever I want.</p>
<h3 id="operator-precedence-and-chaining">Operator precedence and chaining</h3>
<p>Here’s something we haven’t discussed: what is the precedence of binary operators? Is an expression like <code>a + b * c</code> evaluated as <code>a + (b * c)</code> or <code>(a + b) * c</code>, and why?</p>
<p>There are quite a few options. Most languages just determine this with a big table, e.g., here’s <a href="https://en.cppreference.com/w/cpp/language/operator_precedence">C++’s operator precedence</a>, but this won’t work for a language like Noulith that supports using arbitrary identifiers as operators. In <a href="https://v2.ocaml.org/manual/expr.html#ss%3Aprecedence-and-associativity">OCaml</a> and <a href="https://docs.scala-lang.org/tour/operators.html#precedence">Scala</a>, precedence is based on a similar table that classifies all identifiers by their first character: so, for example, every operator whose name begins with <code>*</code> binds more tightly than every operator whose name begins with <code>+</code>. You can also make this more customizable: in Haskell, you can declare the precedence of operators as you define them with <a href="https://wiki.haskell.org/Keywords#infix.2C_infixl.2C_infixr">fixity declarations</a>, while in <a href="https://docs.swift.org/swift-book/ReferenceManual/Declarations.html#ID380">Swift</a> (<a href="https://news.ycombinator.com/item?id=29045660">via</a>, <a href="https://www.scattered-thoughts.net/writing/better-operator-precedence/">via</a>), you can declare “precedence groups” and assign infix operators to them, and each group can state whether it binds more or less tightly than other groups. While these approaches are neat, they complicate the parsing story quite a bit. You need to parse earlier code to the extent that you know each operator’s precedence before you can parse later code correctly, whereas I wanted to implement a simple parser that didn’t have to think about global state. Finally, some languages like Smalltalk and APL (and APL descendants) dispense with precedence entirely: all binary operators are left-to-right in Smalltalk and right-to-left in APL, which means you can’t rely on the precedence for arithmetic operators and equality you learned in math class. I think getting used to it isn’t <em>too</em> bad, but decided it was still worth trying to avoid.</p>
<p>Alongside this question, though, I was considering an even more difficult goal: I wanted to be able to chain comparisons like in Python, e.g., <code>0 &lt;= x &lt; n</code>. This kind of testing if something is in range is common, and having to write expressions like <code>0 &lt;= x &amp;&amp; x &lt; n</code> annoys me, especially when <code>x</code> is a complicated expression I don’t want to write twice or stick in an intermediate variable. It’s also an extra opportunity to make a mistake like <code>0 &lt;= x &amp;&amp; y &lt; n</code> — I’ve written these bugs and struggled to find them before. So, how might I add this syntax feature?</p>
<p>Syntax support for chained comparisons is rare among programming languages because it’s “pure syntax sugar” that doesn’t let you write more interesting code (despite my complaints, stashing the middle expression in a variable isn’t a big deal) and is just generally unpleasant to parse. After Python, I think the most well-known languages to support chained comparisons are Raku and CoffeeScript. I also learned that there is a <a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0893r0.html">C++ proposal</a> to add them, though it doesn’t seem likely to get anywhere. I worked briefly with a <a href="https://github.com/mavoweb/mavo/issues/347">Mavo implementation</a> that bolted comparisons on top of a parse tree from a library. But all of these languages achieve this goal by privileging comparison operators in the syntax, whereas I wanted them to be parsed the same way as every other symbolic identifier.</p>
<p>While researching this further, I found a particularly neat method of support in <a href="https://www2.cs.arizona.edu/icon/docs/ipd266.htm">Icon</a> (<a href="https://stackoverflow.com/a/2650109">via</a>), where comparison operators are left-associative in the normal way, but “just work” as follows (based on my understanding after reading the Icon documentation for two minutes):</p>
<ul>
<li>Expressions either “succeed and produce a result” or “fail”.</li>
<li>If a comparison is true, it succeeds with its right-hand-side as its result. Otherwise, it fails.</li>
<li>Control flow statements check whether an expression succeeds rather than what its result is.</li>
</ul>
<p>So in Icon, a chained comparison <code>a &lt; b &lt; c</code> is evaluated by first evaluating the subexpression <code>a &lt; b</code>; if <code>a</code> is less than <code>b</code>, this simplifies to <code>b</code> and then checking if <code>b &lt; c</code>; if either comparison isn’t true, the expression fails. If both comparisons pass, the expression evaluates to <code>c</code>, but that doesn’t matter, because the only important criterion is whether the expression succeeded. While this is cute, I didn’t want to overhaul what “evaluating an expression” means in Noulith to include an additional success/failure status, just to allow chaining comparisons. Not to mention, I enjoy having the option to treat the truth value of a comparison as an integer, e.g., to index into an array or sum in a loop. I’m not aware of any other programming languages that support chained comparisons without privileging them in the syntax (except perhaps in some really abstract sense where code can change how subsequent code is parsed, like in Coq or something).</p>
<p>Fundamentally, I wanted a parsing strategy that could handle expressions like <code>@ = &lt;=; @@ = &lt;; a @ b @@ c</code>. If I parse <code>a @ b @@ c</code> as a tree of binary operator invocations, with either nested under the other, I’ve already lost. There’s no way to recover what was really intended. Consider, for example:</p>
<pre><code>switch (random_range(0, 3))
case 0 -&gt; (@, @@ = &lt;=, &lt;)
case 1 -&gt; (@, @@ = +, *)
case 2 -&gt; (@, @@ = *, +);
print(1 @ 2 @@ 3);</code></pre>
<p>There’s simply no way to know which of <code>@</code> and <code>@@</code> binds more tightly until the random number has been generated, long after the code’s been parsed. So I concluded that Noulith had to parse <code>a @ b @@ c</code> as a flat list of three operands and two operators, and deal with precedence at runtime. In brief, what happens then: every operator function is examined at runtime to resolve whether it “chains” with the next operator to produce a single operator invocation subexpression, and then to resolve which operators bind the most tightly.</p>
<p>From there, it was easy and natural to make operator precedence accessible and mutable by users. Without thinking too hard, I threw it under a string key <code>"precedence"</code> just to get something working, so I could take a cool screenshot and post it on Twitter. Then it stayed there out of inertia. Here’s a remake of that screenshot with the newest syntax and highlighting.</p>
<figure>
<a href="https://blog.vero.site/img/noulith-cursed-precedence.png"><img src="https://blog.vero.site/img/noulith-cursed-precedence.png" alt="REPL in which two arithmetic operators are swapped and their precedences are swapped, and this is shown to affect the parsing and return value of a function using those operators. Screenshot of terminal."></a>
</figure>
<p>While this is probably deeply disturbing to any parser enthusiasts out there, it opens up the field for us to easily add chaining support to basically any operator, and there are actually some additional “nice” side effects of this!</p>
<ul>
<li><p>Cartesian product and zip operators can behave more nicely with three or more operands. If <code>zip</code> were a normal left-associative binary operator, then the result of <code>[1, 2, 3] zip [4, 5, 6] zip [7, 8, 9]</code> would begin with <code>[[1, 4], 7]</code>. But by allowing <code>zip</code> to recognize when you’re immediately zipping its output with another sequence, you can produce a result that starts with <code>[1, 4, 7]</code>. The only other language I’ve seen that supports something like this is TLA<sup>+</sup>’s Cartesian product <code>×</code>,<a href="#fn5" id="fnref5"><sup>5</sup></a> though I have no clue how to search for this kind of syntax in other programming languages.</p></li>
<li><p>Runs of binary operator invocations can naturally include functions that take more than two arguments. By saying that <code>replace</code> chains with <code>with</code>, I allow you to tack <code>replace b with c</code> onto the end of a sequence of binary operators.</p></li>
<li><p>Finally, functions can have “optional arguments” while still being called in the same binary operator style. By saying that <code>to</code> and <code>til</code> chain with <code>by</code>, I allow the expression <code>1 to 10 by 2</code> without affecting the meaning of <code>1 to 10</code>. (Scala achieves the same effect without parsing shenanigans by having ranges being aware of what kind of range they are and supporting <a href="https://www.scala-lang.org/api/current/scala/collection/immutable/Range.html#by%28step:Int%29:scala.collection.immutable.Range"><code>by</code> as a method</a>.)</p></li>
</ul>
<p>Another implementation detail of note is that Noulith precedences are floating point numbers. I thought this was natural because it seems that every programming language with only a few precedences, like <a href="https://www.haskell.org/onlinereport/decls.html#prelude-fixities">Haskell’s 10</a>, eventually gets complaints that there’s no room to fit an operator’s precedence between two existing ones. Some languages hedge by leaving gaps, the way BASIC programmers <a href="https://stackoverflow.com/questions/541421/why-did-we-bother-with-line-numbers-at-all#541447">spread out their line numbers</a> in the 1970s (or so I’m told) and CSS developers <a href="https://www.reddit.com/r/webdev/comments/cu1776/how_mad_at_your_css_do_you_have_to_be_to_add_274/">spread out their <code>z-index</code> values</a>, just in case you need to insert something in-between later: <a href="https://coq.inria.fr/refman/user-extensions/syntax-extensions.html#precedences-and-associativity">Coq</a> uses precedences from 0 to 100, with defaults mostly at multiples of 5 or 10; <a href="https://www.swi-prolog.org/pldoc/man?section=operators">Prolog</a>, from 0 to 1200 in multiples of 50 or 100; <a href="https://zlanguage.github.io/#ops">Z</a>, at multiples of… 111? But floating-point precedences let you leave finer gaps with less foresight. I imagine other languages don’t do the same for reasons along the lines of, the semantics of floating-point numbers are too complicated and unportable for a core feature of language syntax to depend on them. (What if an operator’s precedence is NaN?) I can sympathize a lot with this, but as I have no ambitions for Noulith to become a language with a formal specification, I didn’t mind.</p>
<p>Finally, I should mention the standard boolean “operators” <code>and</code> and <code>or</code>. These operators are, and have to be, special in most programming languages because they need to short-circuit — in an expression like <code>a and b</code>, if <code>a</code> evaluates to something falsy, then <code>b</code> is not evaluated, which is important for both efficiency and correctness. For example, you can check if an index is in bounds for an array on the left side of an <code>and</code> and then perform the actual indexing on the right; without short-circuiting, the indexing would still be attempted when the index is out of bounds, causing an error. <code>and</code> and <code>or</code> can be normal functions/operators in some languages with easily accessible lazy evaluation like Haskell, or normal macro constructs in other languages like Lisps. Unfortunately, Noulith lacks both faculties, so its <code>and</code> and <code>or</code> do have to be language constructs. As in Python, these expressions return the last or first truthy expression they encounter (e.g., <code>2 and 3</code> is <code>3</code> instead of just “true”), enabling them to emulate conditional expressions in some contexts. I also added the SQL-inspired <code>coalesce</code>, which is similar to <code>or</code> but only rejects <code>null</code> as its left operand, with the vague idea that it could be used in more precise “default value” setups, but barely ended up using it. (However, <code>not</code> does not need any special behavior, so it’s just a normal function.)</p>
<h3 id="variables-statements-and-blocks-of-code">Variables, statements, and blocks of code</h3>
<p>We’re finally graduating from expressions to statements. First up: How do you declare a variable? I was just going to copy Python at first and use a simple <code>=</code> for both declaration and assignment, but then I read the <cite>Crafting Interpreters</cite> design note on <a href="https://craftinginterpreters.com/statements-and-state.html#design-note">implicit variable declaration</a> and was utterly convinced, so I started looking for a syntax to distinguish them.</p>
<p>In some statically typed languages (mostly C/C++/C♯ and Java), variable declarations start with the variable’s type merely juxtaposed with its name. I’m not sufficiently invested in static types to want this, but even if I were, since I already decided that juxtaposition can be function invocation, trying to copy this exact syntax basically means that Noulith has to immediately be able to tell whether it’s parsing a type or an expression when starting to parse a statement. This is doable by a strategy like saying that types have to be capitalized or something, but… it’s complicated.</p>
<!-- (A vaguely similar problem is the reason for Rust's [infamous turbofish](https://github.com/rust-lang/rust/blob/master/src/test/ui/parser/bastion-of-the-turbofish.rs).) -->
<p>Still, there are many other viable choices. <code>let</code>? <code>var</code>? <code>my</code>? Heck, I could spell out <code>variable</code> as in <a href="https://www.ceylon-lang.org/documentation/1.3/tour/attributes-control-structures/">Ceylon</a>. In the end I landed on using <code>:=</code>, sort of like Go or even Pascal, both for succinctness and because I realized I liked the option of being able to declare types sometimes (like Python 3 annotations, as used by type checkers like mypy): conveniently, a declaration like <code>a := 3</code> can be seen as a special case of a declaration like <code>a: int = 3</code> where the type is omitted, which Noulith also supports. Of note is that Noulith checks the values assigned to typed variables <em>at runtime</em>, so the following errors:</p>
<pre><code>a: int = "hi"</code></pre>
<p>As does this:</p>
<pre><code>a: int = 6;
a = "hi"</code></pre>
<p>This is bizarre and silly — usually you don’t want type annotations to have any runtime cost, much less every time you assign to an annotated variable — but it catches some bugs and is way easier to implement than a static analysis pass, plus it’s consistent with a more reasonable behavior for typed patterns in pattern matching, which we’ll talk about much, much later.</p>
<p>Another advantage is that by thinking of <code>x:</code> as a general lvalue (crudely, a “thing that can be assigned to”), this syntax naturally generalizes to single assignments that simultaneously assign to an existing variable and declare a new one: <code>x := [3, 4]; (a:), b = x</code>. (Go’s <a href="https://go.dev/ref/spec#Short_variable_declarations">short variable declarations</a> are somewhat magic here: you can use a single <code>:=</code> to simultaneously declare some new variables and assign to some old ones, as long as at least one variable is new. I think this is slightly inelegant, and sometimes daydream about <a href="https://www.evanmiller.org/four-days-of-go.html">Evan Miller’s proposal</a> whereby you need to write exactly as many colons as variables you’re newly declaring. But as my gripes with languages go, it ranks pretty low.)</p>
<p>Also unlike <cite>Crafting Interpreters</cite>, I don’t allow redeclaring a variable with the same name in the same scope. The book makes a really good point that this is annoying for REPL usage, where programmers might just want to use and reuse variable names without mentally tracking which ones have been declared so far. I have not made up my mind here yet, so redeclarations are banned for now, mostly because it’s easier to make rules laxer than stricter as the language develops, but I suspect I’ll end up lifting this restriction at some point.</p>
<p>Next: how are statements and blocks of code (for control flow branches, e.g.) delimited? I used to like indentation-based structure <i>a la</i> Python, the idea being that, because you want your code to be indented to reflect its structure for the human reader anyway, having your language also require braces or other delimiters is redundant. However, I’ve learned to appreciate that redundancy is not inherently bad, and control flow that’s delimited with only indentation is actually quite annoying to refactor. When you have a block of nested code that you want to move around, you have to track its indentation much more carefully than you’d need to if there were explicit delimiters. For example, suppose I wanted to inline the call to <code>f</code> in this Python code:</p>

<p>I might try copying the body of <code>f</code> where I want it to go, replacing the call to it, which seems like it should work because its arguments and parameters are exactly the same. Uh-oh:</p>

<p>This code is currently broken, and to fix it I have to indent the two lines I copied exactly twice, while taking care not to indent the lines next to it. This is an exaggeratedly simple case, but the block of code being transferred might have its own internal indentation or other internal details that must be kept track of, like parameter names that need to be changed, making the transfer much trickier. On the other hand, in a similar language with braces, the copy-pasted code would be syntactically and semantically correct with no extra effort, and its indentation can trivially be fixed by any competent text editor.</p>

<p>To defend the indentation solution, I might say that this is rare and that the right way to avoid it is to avoid deeply nested code in the first place, or just to get better editor support (I haven’t spent enough time in large Python projects to look into more sophisticated tooling, but I assume it exists). I’d also point out all the other costs of the braces-based solution, such as the blank lines with only <code>}</code>s in them. I don’t think this is a <em>terrible</em> defense — deeply nested code is often worth avoiding. But I wanted Noulith to support code without a lot of effort put into structuring it and breaking things into functions, so I chose to stick with explicit delimiters.</p>
<p>What delimiters, though? Unusually, I ended up using more parentheses, rather than the far more common curly braces, because I found the simplicity of not distinguishing expressions and statements quite appealing. Scala (at least, version 2) is one language where some blocks can be written with either parentheses or curly braces, which are similar but have <a href="https://scalapuzzlers.com/#pzzlr-047">subtly different semantics</a>, and I didn’t want to think about that. This led me to follow C/C++/Rust and always require statements to be separated by semicolons, because if any expression can be a series of statements, and if a language’s syntax is so flexible in other ways, it’s really hard to guess when a newline is meant to end a statement. Other languages can say that line breaks don’t count inside parentheses, or have even more complicated rules for <a href="https://stackoverflow.com/questions/2846283/what-are-the-rules-for-javascripts-automatic-semicolon-insertion-asi">automatic semicolon insertion</a>; but the flexibility of Noulith syntax means code like the contents of the parentheses below really could make sense as one large expression or as two expressions (the latter of which calls <code>*</code> with one argument to partially apply it).</p>
<pre><code>x := (2
# hello
* 3)</code></pre>
<p>All this does make Noulith’s parser incredibly bad at recovering from mistakenly omitted semicolons, which is one reason I’d wholeheartedly disrecommend anybody try to write Noulith programs that are larger than quick-and-dirty scripts. It’s probably too late to fix this at this point, and in hindsight, perhaps I should have thought a bit more about alternatives before allocating both square and curly brackets to literals. Still, I don’t know if I would have decided any differently. I like all the other features I got for this tradeoff.</p>
<h3 id="control-flow">Control flow</h3>
<p>Having discussed most of the decisions surrounding simple expressions and statements, we can turn our attention to control flow structures.</p>
<p>A fundamental syntactic issue most languages have to grapple with: in the syntax for a construct like <code>if condition body</code> or <code>while condition body</code>, you need some way to decide where <code>condition</code> stops and <code>body</code> starts. There are a couple options:</p>
<ul>
<li>You could use a keyword like <code>if condition then body</code> (e.g.&nbsp;Haskell, Ruby) or <code>while condition do body</code> (e.g.&nbsp;various POSIX shells, Scala 3).</li>
<li>You could use punctuation like <code>if condition: body</code> (e.g.&nbsp;Python).</li>
<li>You could require parentheses (or some other delimiter) around the condition like <code>if (condition) body</code> (e.g.&nbsp;C/C++, Java, JavaScript).</li>
<li>You could require braces (or some other delimiter) around the body like <code>if condition { body }</code> (e.g.&nbsp;Go, Rust). (Note that this only works if legitimate <code>condition</code>s never contain the delimiter,<a href="#fn6" id="fnref6"><sup>6</sup></a> so doing this with parentheses wouldn’t work in Noulith and most other languages.)</li>
</ul>
<p>I partly locked myself out of considering the last option by allocating curly brackets to sets, but I think that for my use case, I still preferred the old-school C-like solution of parenthesizing the condition because I often wrote nested control structures with bodies that were long but only comprised a single expression. In such cases, I thought it was less mental load to type the closing parentheses sooner. For example, I thought this:</p>
<pre><code>if (a) for (b &lt;- c) if (d) e;</code></pre>
<p>looked neater and easier to write than this:</p>
<pre><code>if a { for b in c { if d { e }}};</code></pre>
<p>I also copied from Scala/Rust the ability to use <code>if</code>/<code>else</code> constructs as expressions, which just return whatever the last expression of the taken branch evaluate to, so you can write code like:</p>
<pre><code>print(if (a &lt; b) "a is less" else "b is less or equal")</code></pre>
<p>Semantically, this construct (and all others that care about “truth value”, e.g., <code>filter</code> predicates) determine truthiness just like Python, where 0 (which <code>false</code> is a synonym for), <code>null</code>, and empty collections (lists, strings, dictionaries, etc.) are falsy and all other values are truthy. This is another choice I made without much thought, and is not at all the only plausible one — you could, for example, consider 0 truthy like Ruby and most Lisps, or consider empty lists truthy like JavaScript. You could consider the string <code>"0"</code> falsy like PHP and Perl. You could consider everything other than <code>true</code> false like Dart. If you want to be really adventurous, you could consider integers truthy iff positive, like Nibbles; or iff equal to 1, like 05AB1E; or if they’re <a href="https://manual.yoyogames.com/GameMaker_Language/GML_Reference/Variable_Functions/bool.htm">≥ 0.5, like in Game Maker Language</a> (in some contexts?) The Pythonic rule makes sense to me in that it does something useful for most data types, but I suspect that this is mostly just because I’m used to it.</p>
<h4 id="on-ternary-expressions">On ternary expressions</h4>
<p>I have to go on another mini-rant here. Ternary expressions are an important feature of programming languages to me, and I am still annoyed that Go doesn’t have them. Critics say they’re confusing and can always be replaced by if-then-else statements — code like:</p>
<pre><code>var foo = bar ? baz : quux</code></pre>
<p>can always be rewritten as:</p>
<pre><code>var foo
if (bar) {
    foo = baz
} else {
    foo = quux
}</code></pre>
<p>This is six lines instead of one. Now, I try not to let my code golf tendencies seep into other contexts, but even so I think six lines instead of one is an unacceptable amount of verbosity and actually makes the code much harder to read, particularly in cases when all the constituent expressions are really that short. The distance between <code>foo</code>’s declaration and initialization also means that readers have to deal with the mental load of worrying “is <code>foo</code> going to be initialized?” when reading this code.</p>
<p>One might propose the shorter four-line alternative in response, which often works:</p>
<pre><code>var foo = quux
if (bar) {
    foo = baz
}</code></pre>
<p>Even ignoring the cases where evaluating <code>quux</code> has side effects that break this rewrite, what I don’t like about this code is that to readers, the first statement <code>var foo = quux</code> <em>is a lie</em>. Semantically, it appears that the code is stating an unconditional fact that <code>foo</code> should be defined as, or at least initialized to, <code>quux</code>; so if <code>quux</code> is a complicated expression, readers might be mulling over how to understand that fact. For an example (taken directly from Noulith itself), say I was implementing an interpreter that took one command-line argument, which could be either a filename or a literal snippet of code, depending on a flag. The single-branch <code>if</code> might look something like:</p>
<pre><code>var code = arg
if (flag) {
    code = open(arg).read()
}
// lex, parse, and execude code...</code></pre>
<p><code>arg</code> is sometimes a filename, in which case it is definitely not a snippet of code. If a reader already know this, or perhaps guessed it and is skimming the code to verify whether it’s true, and they read the line <code>var code = arg</code>, they’ll stumble. Of course, they’ll probably figure out what’s going on if they keep reading two more lines, but why permit this confusion to occur in the first place?</p>
<p>I can, however, sympathize with believing that <code>? :</code> is too cryptic, so I most prefer Rust and Scala’s approach of just accepting the entire <code>if</code>/<code>else</code> construct to be an expression, allowing code like:</p>
<pre><code>code := if (flag) {
    open(arg).read()
} else {
    arg
}</code></pre>
<p>This is honest and avoids ever suggesting to readers that <code>code</code> is unconditionally something it’s not. It’s also easier to fit on one line (though linters might complain).</p>
<h4 id="loops">Loops</h4>
<p>With <code>if</code>/<code>else</code> out of the way, we can move on to loops. Noulith has <code>while</code> loops, which are quite unremarkable, but no <code>do ... while</code> loops or infinite loops yet. The <code>for</code> loops (which are all “for-each” loops) are more interesting, though, and are one of the few features that I added under one syntax, worked with and wrote code using for a long time, and then went back to change the syntax of. Specifically, I started with the C++/Java-style <code>for (a : b)</code>, plus the quirky generalization <code>for (a :: b)</code> for iterating over index-value or key-value pairs. But eventually I concluded this interfered too much with wanting to use <code>:</code> for “type annotations”, so I swapped out the separator after the iteration variable to be <code>&lt;-</code>, as in Haskell and Scala. (<code>in</code> as in Python and Rust was not a serious contender because I preferred to allocate that to be used nonsyntactically as a function; design choices thus far prevent it from doing double duty. I didn’t want something <code>:=</code>-based as in Go just because the symbol <code>:=</code> does not suggest that to me.) I also copied Scala to provide a feature I use a lot in search-type scripts, allowing multiple iterations in a single loop, as well as <code>if</code> guards.</p>
<pre><code>for (a &lt;- as; b &lt;- bs; c &lt;- cs; d &lt;- ds; if cond(a, b, c, d)) e</code></pre>
<p>Also from Scala I copied the ability to modify this into a list comprehension:</p>
<pre><code>for (a &lt;- as; b &lt;- bs; c &lt;- cs; d &lt;- ds; if cond(a, b, c, d))
  yield e</code></pre>
<p>Finally, inspired by several Discord conversations, I also allow dictionary comprehensions:</p>
<pre><code>for (a &lt;- as; b &lt;- bs; c &lt;- cs; d &lt;- ds; if cond(a, b, c, d))
  yield k: v</code></pre>
<p>I don’t have much more to say about these loops, except perhaps to note that they really are just for iteration, instead of being syntax sugar for monads or anything like that.</p>
<h3 id="structs">Structs</h3>
<p>This is a short section because this was a last-minute addition and I haven’t really used it much yet, but Noulith supports structs, which are super bare-bones product types.</p>
<pre><code>struct Foo(bar, baz);</code></pre>
<p>Each instance of <code>Foo</code> has two fields. The variables <code>bar</code> and <code>baz</code> are “reified fields” that can be used as member access functions, and also used to assign or modify the fields with the same indexing syntax as everything else.</p>
<pre><code>foo := Foo(2, 3);
bar(foo); # evaluates to 2
foo.bar; # just function application, evaluates to 2 for the same reason
foo[bar] = 4;
foo[bar] += 5;</code></pre>
<p>The most notable aspect is that <code>bar</code> and <code>baz</code> are actually just newly defined variables holding these field objects, and not namespaced under the struct <code>Foo</code> in any way. Noulith will not let you define another struct with a field named <code>bar</code> or <code>baz</code> (or any other variable with either name) in the same scope. This was basically the lowest-effort way I could think of to get usable structs into the language, and the only thing I’ll say in defense of this design is that Haskell record fields have hogged their names in much the same way until maybe 2016, when GHC 8 released <code>DuplicateRecordFields</code>, and is still experimenting with language extensions like <code>OverloadedRecordUpdate</code>. So I’m allowing myself at least two decades to figure out something better.</p>
<h3 id="pattern-matching-lvalues-and-packingunpacking">Pattern matching, lvalues, and packing/unpacking</h3>
<p>Noulith has <code>switch</code>/<code>case</code> for basic pattern matching. (Example lifted from Python’s <a href="https://peps.python.org/pep-0636/#appendix-a-quick-intro">pattern matching tutorial</a>.)</p>
<pre><code>switch (status)
case 400 -&gt; "Bad request"
case 404 -&gt; "Not found"
case 418 -&gt; "I'm a teapot"
case _ -&gt; "Something's wrong with the Internet"</code></pre>
<p>(A syntactic observation: because we have the <code>case</code> keyword and because <code>switch</code>es don’t make sense without at least one <code>case</code>, the parentheses around the <code>switch</code> argument aren’t necessary like they are with <code>if</code> or <code>while</code>. Noulith’s parser still requires them for now for consistency, but perhaps I should lift this requirement…)</p>
<p>Unlike some similar constructs in other dynamic languages, Noulith’s <code>switch</code> expressions error out if no cases match, even though there’s a solid case to be made for doing nothing and returning <code>null</code>. This is a change I made during Advent of Code after writing too many bugs caused by mistakenly omitted default cases.</p>
<p>Other than check for equality with constants, pattern matching can destructure/unpack sequences:</p>
<pre><code>switch (x)
case a, -&gt; "one"
case a, b -&gt; "two"
case a, b, c -&gt; "three"
case a, b, c, ...d -&gt; "more"</code></pre>
<p>One gotcha, shared with many other languages’ pattern-matching, is that variable names in patterns always bind new variables, whereas sometimes you want to check equality against a previously defined variable. This code, for example, will not do what you want. The pattern will always match and define a new variable named <code>not_found</code> equal to <code>x</code>.</p>
<pre><code>not_found := 404;
switch (x)
case not_found -&gt; "Not found"  # pattern will always match</code></pre>
<p>Scala and Rust both allow you to work around this by supporting constants that are syntactically distinct from variables; Python supports “constant value patterns” that must be dotted, which I think is fortunately common. Noulith’s slightly more general workaround is the keyword <code>literally</code>, which turns an expression into a pattern that evaluates the expression and checks for equality.</p>
<pre><code>not_found := 404;
switch (x)
case literally not_found -&gt; "Not found"</code></pre>
<p>Patterns can also check the type of values at runtime (which is why this check also occurs when declaring variables):</p>
<pre><code>switch (x)
case _: int -&gt; "it's an int"
case _: float -&gt; "it's a float"</code></pre>
<p>To implement the analogue of many languages’ even more general patterns, “pattern guards”, that let you check for arbitrary predicates, you can manufacture arbitrary types with <code>satisfying</code> (which is a normal function). I am not sure this is “right”, but it was easy.</p>
<pre><code>switch (x)
case _: satisfying(1 &lt; _ &lt; 9) -&gt; "it's between 1 and 9"</code></pre>
<p>Notably missing is the ability to destructure custom structs, partly because I haven’t gotten around to it and partly because there are concerns about how this interacts with augmented assignment, which we’ll talk about much later.</p>
<p>In hindsight, I don’t know why I used the extremely old-school C/C++/Java <code>switch</code> keyword. <code>match</code> makes much more sense and is popular today. Even Python adopted it. But it is what it is for now.</p>
<p>Anyway, my experience was that you don’t need a lot of capabilities for pattern matching to be really useful. The trivial product type provided by sequences is enough to approximate sum types just by manually tagging things with constants. Also, pattern matching is just really useful for parsing Advent of Code strings. <a href="https://adventofcode.com/2022/day/7">Day 7</a> (<a href="https://github.com/betaveros/advent-of-code-2022/blob/main/p7.noul">my full code</a>) might be the best example:</p>
<pre><code>switch (line.words)
case "$", "cd", "/" -&gt; (pwd = [])
case "$", "cd", ".." -&gt; pop pwd
case "$", "cd", x -&gt; (pwd append= x)
case "$", "ls" -&gt; null
case "dir", _ -&gt; null
case size, _name -&gt; (
    for (p &lt;- prefixes(pwd)) csize[p] += int(size)
)</code></pre>
<p>In languages without pattern matching, the simplest way to handle this might be to write a bunch of deeply nested <code>if</code>/<code>else</code> statements that look like the following, which is a pain to read, write, and debug:</p>
<pre><code>if (a == "$") (
    if (b == "cd") (
        if (c == "/") ( ... )
        else if (c == "..") ( ... )
        else ( ... )
    ) else if (b == "ls") ( ... )
)</code></pre>
<p>It happens that Day 7 is the only day on which I was first to solve either Advent of Code part, and I got first on both that day. Perhaps this was a factor?</p>
<p>However, Noulith’s pattern matching has its own issues. Here is a pattern that’s surprisingly tricky to support, which I only realized in mid-September:</p>
<pre><code>switch (x)
case -1 -&gt; "it's negative one"</code></pre>
<p>Obviously, we want the case to match if <code>x</code> equals <code>-1</code>. The analogous pattern for nonnegative integers works with the simple, obvious rule: a value matches a literal if they’re equal. Unfortunately, <code>-1</code> is not a literal — it’s a function invocation! Outside a pattern, it calls unary minus on the argument <code>1</code>.</p>
<p>The simplest way to resolve this is to say that, when parsing a pattern, <code>-</code> gets attached to the subsequent numeric literal if one exists. Python’s pattern matching, for example, specifically allows <code>-</code> in the <a href="https://peps.python.org/pep-0634/#literal-patterns">syntax for literal patterns</a> — as does <a href="https://doc.rust-lang.org/stable/reference/patterns.html#literal-patterns">Rust</a>, as does <a href="https://www.haskell.org/onlinereport/haskell2010/haskellch3.html#x8-580003.17">Haskell</a>. As for Scala, its <a href="https://www.scala-lang.org/files/archive/spec/2.11/08-pattern-matching.html#literal-patterns">literal patterns</a> are syntactically the same as its <a href="https://www.scala-lang.org/files/archive/spec/2.11/01-lexical-syntax.html#literals">literals</a>, which encompass a negative sign in every context. One reason this makes sense for it but not the other languages I just listed is that, courtesy of its Java/JVM lineage, the sets of legal positive and negative integer literal are not symmetric because they represent two’s-complement machine words. Specifically, <code>-2147483648</code> is a legal Java/Scala expression, but <code>2147483648</code> by itself is a compile-time error. (Therefore, so is <code>-(2147483648)</code>! I first learned this from <a href="http://www.javapuzzlers.com/"><cite>Java Puzzlers</cite></a>.)</p>
<p>But returning to Noulith: having gotten this far without privileging <code>-</code> in the syntax, I decided to try a little harder. Thus, I had pattern matching “ask” the function <code>-</code> how to destructure the scrutinee into an inner pattern. That is, to see whether <code>x</code> matches the pattern <code>-1</code>, Noulith resolves the identifier <code>-</code>, determines that it means negation in a pattern-matching context, <em>negates <code>x</code></em>, and matches that against the pattern <code>1</code>.</p>
<p>This means that pattern matching like this works as well:</p>
<pre><code>switch (x)
case -y -&gt; print(x, "is negative", y)</code></pre>
<p>This makes it easy to support a bunch of other, somewhat ad hoc patterns, like allowing fractions to be destructured into their numerator and denominator.</p>
<pre><code>switch (f)
case x/y -&gt; print("numerator is", x, "and denominator is", y)</code></pre>
<p>Or checking for divisibility. Because we can.</p>
<pre><code>switch (x)
case 2*k -&gt; print(k, "pairs")
case 2*k + 1 -&gt; print(k, "pairs with one left over")</code></pre>
<p>But the most “evil” pattern-matching mode I’ve implemented is probably for the comparison operators. A pattern like <code>1 &lt; y &lt; 9</code> matches any number that is greater than 1 and less than 9, and binds that number to <code>y</code>. More generally, a chain of comparison operators with one variable matches any value that would satisfy those comparisons. But if the chain has X variables where X &gt; 1, it matches any list of X values that would satisfy those comparisons if plugged in.</p>
<pre><code>xs := [2, 7];
switch (xs)
case 1 &lt; a &lt; b &lt; 9 -&gt;
  "two strictly increasing numbers between 1 and 9"</code></pre>
<p>This works because, before an expression is matched against a pattern, there’s a preparatory pass through the pattern that evaluates literals and <code>literally</code> expressions and presents them to the function, so that any function asked to destructure something during the matching process knows which of its operands are known values and which are other patterns that it might send something downwards into. Also, functions determine their precedence and chaining properties as they would outside a pattern. So, the three <code>&lt;</code>’s in the above example chain into one function that is then asked whether it matches <code>[2, 7]</code>, with the information that it has four “slots”, the first and fourth of which contain values 1 and 9 and the second and third of which are its responsibility to fill. However, it does not know any more specifics about what patterns produced those values or what patterns are in the slots it has to fill. Its view of the situation is the same as in the following example (which also succeeds… at least after I fixed a bug I found while writing this post):</p>
<pre><code>xs := [2, 7];
switch (xs)
case 1 &lt; 2*a &lt; 2*b + 1 &lt; literally 3*3 -&gt;
  "an even number and then an odd number, both between 1 and 9"</code></pre>
<p>I had to look all this up in the code to remember how it works. I think I wrote this while possessed by the dragon. Still, being able to write notation like this pleases my inner mathematician.</p>
<p>The last feature of patterns is <code>or</code>, which can be used to combine patterns to produce a pattern that matches if either subpattern matches. I think <code>|</code> is a lot more popular in other languages, but again, I wanted <code>|</code> to be a normal identifier in the syntax. Pattern-combining has short-circuiting behavior that can’t be implemented by a normal pattern-matching function, just like <code>or</code> in an expression can’t be replaced by a function, so it made sense to me.</p>
<p>The other control flow structure using pattern matching is <code>try</code>/<code>catch</code>.</p>
<pre><code>try 1//0
catch x -&gt; print(x)</code></pre>
<p>The code in the body of the <code>try</code> is evaluated normally, except that if an exception is thrown, the exception is checked against the <code>catch</code> clause’s pattern in much the same way a <code>case</code> clause checks whether the <code>switch</code> argument matches a pattern; if it matches, the <code>catch</code>’s body is evaluated and the exception is not propagated further. For whatever reason, I only allow each <code>try</code> to accept one <code>catch</code> clause now, even though it would be easy and more sensible for each <code>try</code> to accept multiple clauses, the same way one <code>switch</code> accepts multiple <code>case</code>s. I have no excuse except laziness. Maybe I’ll implement it after finishing this post.</p>
<p>As previously mentioned, Noulith doesn’t have a special type for exceptions or errors, even though it “should”. You can just throw and catch any value you can store in a variable. Most (all?) errors thrown by built-in functions are just strings for now, and most of my Advent of Code solutions just throw and catch the string <code>"done"</code>. The extraordinarily poor error handling is another reason nobody should write production code in Noulith.</p>
<p>Pattern matching is also useful in mere assignments, for destructuring a sequence and assigning different parts to different variables…</p>
<pre><code>foo := [1, 2];
a, b := foo</code></pre>
<p>…as well as in functions’ parameter lists. So let’s turn to those next.</p>
<h3 id="functions">Functions</h3>
<p>What do functions and lambdas look like?</p>
<p>I love lambdas and want Noulith to support functional programming extensively, so a keyword like Python’s <code>lambda</code> is definitely too verbose for me. This isn’t a syntax where there’s much uniformity across programming languages to be found, so I went with Haskell’s short, snappy <code>\</code>, which I think is supposed to look like an actual lambda λ if you squint. (The really “fun” option would have been to directly use U+03BB λ, which is actually easy for me to type with a Vim digraph, <kbd>Ctrl-K</kbd><kbd>L</kbd><kbd>*</kbd>; but I’m not <em>that</em> adventurous and didn’t think I’d do anything else with <code>\</code> anyway. Not to mention, λ is a Letter, the wrong Unicode General Category.) The rest of the syntax is a mix of Python and Haskell: parameters are delimited with commas, but the parameter list is separated from the body with <code>-&gt;</code>.</p>
<pre><code>\a, b -&gt; a + b</code></pre>
<p>On reflection, I realized many programming languages don’t start lambdas with a prefix sigil at all, e.g., JavaScript and Scala have arrow functions similar to <code>x =&gt; x + 1</code> or <code>(x, y) =&gt; x + 4</code>; you just parse a comma-separated list of expressions, then when you see an arrow you turn that expression into an argument list. This doesn’t make parsing meaningfully harder because I already have to do similar backtracking when parsing the LHS of an assignment. But using a prefix sigil does allow me to continue to reject <code>()</code> as a syntactically invalid expression, instead of accepting it in some contexts to express a lambda with zero parameters <code>() =&gt; x</code>. Plus, a prefix-less syntax would make parse errors even more fragile. So I was satisfied sticking with <code>\</code>.</p>
<p>Finally, I decided I was comfortable enough with lambdas that I didn’t feel the need to design and add a separate syntax for declaring named functions. Just make a lambda and assign it to a variable. One drawback, though, is that it’s sometimes useful for debugging or metaprogramming for functions to know what their own names are, so I wouldn’t rule out adding a syntax for defining and naming a function one day.</p>
<p>While we’re talking about lambdas, let’s talk about a common lambda-related pitfall and one of Noulith’s weirdest keywords. Quick, what’s wrong with the following Python code?</p>

<p>The problem, which many a Python programmer has been bitten by, is that all the lambdas close over the same variable <code>i</code>, which is shared between loop iterations. When the loop concludes, <code>i</code> is <code>9</code>, so all of the functions add <code>9</code>. Even worse, if you were building <code>adders</code> in an imperative <code>for</code> loop, you could still mutate <code>i</code> outside the loop (for example, by accidentally using it in another loop).</p>

<p>This issue is less likely to appear in Noulith. Firstly, partial application is way more common, often obviating explicit lambdas, and the act of partial application grabs the variable’s value rather than closing over it. Secondly, Noulith <code>for</code> loops get a fresh iterator variable in each loop iteration, so even if you did make explicit lambdas like the above, they’d close over different variables — one of very few breaking changes (possibly the only one?) being <a href="https://github.com/golang/go/issues/20733">considered for Go 2</a>, which should attest to how treacherous the alternative is. The <a href="https://github.com/golang/go/discussions/56010">associated discussion</a> has fun tidbits like:</p>
<blockquote>
<p>Loop variables being per-loop instead of per-iteration is the only design decision I know of in Go that makes programs incorrect more often than it makes them correct.</p>
</blockquote>
<blockquote>
<p>We built a toolchain with the change and tested a subset of Google’s Go tests […] The rate of new test failures was approximately 1 in 2,000, but nearly all were previously undiagnosed actual bugs. The rate of spurious test failures (correct code actually broken by the change) was 1 in 50,000.</p>
</blockquote>
<p>Still, if you wanted to artificially induce this mistake, you could write something like:</p>
<pre><code>i := 0;
adders := for (_ &lt;- 1 to 10) yield (
    i += 1;
    \x -&gt; x + i
)</code></pre>
<p>Pretend that you can’t use a unique loop variable or partial application due to other complications in the code. How could you make the code work as intended anyway?</p>
<p>One approach, common in <a href="https://developer.mozilla.org/en-US/docs/Glossary/IIFE#for_loop_with_var_before_es6">older JavaScript</a>, would be to use an immediately invoked function expression (IIFE). Translated to Noulith, this would be:</p>
<pre><code>i := 0;
adders := for (_ &lt;- 1 to 10) yield (
    i += 1;
    (\i -&gt; \x -&gt; x + i)(i)
)</code></pre>
<p>Noulith doesn’t have this feature (yet), but another approach you can often get by with in Python is using a default argument (though this risks swallowing later mistakes where <code>adders</code>’s elements are called with two arguments, and might not work if you wanted to do deeper metaprogramming on the functions):</p>

<p>But I don’t find either of those totally satisfying. Noulith offers a different way out with the <code>freeze</code> keyword:</p>
<pre><code>i := 0;
adders := for (_ &lt;- 1 to 10) yield (
    i += 1;
    freeze \x -&gt; x + i
)</code></pre>
<p><code>freeze</code> takes an arbitrary expression, usually a lambda, and eagerly resolves every <a href="https://en.wikipedia.org/wiki/Free_variables_and_bound_variables">free variable</a> to the value that that variable holds. So in the lambda produced by <code>freeze \x -&gt; x + i</code>, <code>i</code> is “frozen” to the value the variable <code>i</code> held at the time of production (and so is the operator <code>+</code>). Aside from the semantic change, <code>freeze</code> can also be used as a mild optimization, since otherwise the lambda would have to look up <code>i</code> and <code>+</code> by their string names in the environment on each invocation (something that could be optimized out by more intelligent compilers, but: effort!)</p>
<p>On reflection, this took a stupid amount of work for what amounts to a party trick, but I was able to reuse some of the work for static passes later, so it worked out.</p>
<h3 id="augmented-assignment">Augmented assignment</h3>
<p>In addition to the unpacking/pattern matching we’ve already discussed, many programming languages also support another variant of assignment statement sometimes called <a href="https://en.wikipedia.org/wiki/Augmented_assignment">augmented assignment</a>, as in <code>x += y</code>. This is often described as simply being shorthand for <code>x = x + y</code>, but many languages actually have surprising subtle semantic differences between the two. In C++, I believe they are the same for numeric types, but classes can overload individual augmented assignment operators like <code>+=</code> separately from each operator <code>+</code>. In Python, if <code>x</code> is a mutable list, <code>x += y</code> will mutate <code>x</code> but <code>x = x + y</code> will make a new copy, which matters if some variable elsewhere holds reference to the same list. Even in that bastion of unadventurous languages, Java, <code>x += y</code> and <code>x = x + y</code> have subtle differences involving type coercion and sometimes when one of the arguments is a <code>String</code> (see <cite>Java Puzzlers</cite> 9 and 10). Noulith has its own subtle semantic difference, but let’s talk about the syntax first.</p>
<p>I definitely wanted to support <code>+=</code>, but unlike most languages with such operators, <code>+</code> is just an identifier, and I didn’t want to go through every operator and define an augmented variant. So I thought it made sense to allow any function <code>f</code> to be part of an augmented assignment <code>f=</code>, regardless of whether <code>f</code>’s name is alphanumeric or symbolic. This feature got Noulith a <a href="https://buttondown.email/hillelwayne/archive/microfeatures-id-like-to-see-in-more-languages/">shoutout in Computer Things</a>.</p>
<p>I do think this syntax feature is practical. I have often wanted to write assignments like <code>a max= b</code> or <code>a min= b</code> in search problems, where <code>a</code> is a variable tracking the best score you’ve achieved so far and <code>b</code> is a score you just achieved. These constructs are so useful that I include them in my competitive programming template as <code>minify</code> and <code>maxify</code>, with definitions like the following, and I’ve found at least a few other templates online with similar functions. (I won’t link to any concrete examples because most of the results look like SEO spam, but I am confident many competitive programmers other than myself do this.)</p>

<p>Not only that (and I totally forgot about this until writing this post), a silly <a href="https://github.com/betaveros/cpp2">“competitive programming preprocessor”</a> I briefly tried to create in <strong>2015</strong><a href="#fn7" id="fnref7"><sup>7</sup></a> borrowed the operator spellings <code>&lt;?</code> and <code>&gt;?</code> of <code>min</code> and <code>max</code>, respectively, from <a href="https://livescript.net/#operators">LiveScript</a> so that they could be used in augmented assignment. So this has been something I’ve wanted for a long time. More prosaically, though, the augmented assignment with an alphanumeric identifier that I’ve used by far the most often is <code>append=</code>. All in all, I wanted to support augmented assignment for any identifier, alphanumeric or symbolic.</p>
<p>There are several difficulties, though. Most immediately, the overwhelmingly common comparison operators conflict with making this syntax fully general, or even merely applicable to all symbolic identifiers: <code>x &lt;= y</code> is definitely not the augmented assignment <code>x = x &lt; y</code>. This was one place where internal and external consistency came into hard conflict and I couldn’t see how to get everything I wanted without some syntax special casing. So, Noulith’s lexer specifically treats the four tokens <code>==</code>, <code>!=</code>, <code>&lt;=</code>, and <code>&gt;=</code> specially. All operators whose names end with <code>=</code> are lexed as meaning augmented assignment, except for those four. In hindsight, I could have looked harder for precedent: Scala has <a href="https://scala-lang.org/files/archive/spec/2.13/06-expressions.html#assignment-operators">very similar carveouts</a>, but additionally carves out any symbol starting and ending with <code>=</code>.</p>
<p>Even with that decided, it’s not clear how exactly in which stage of lexing and parsing this should be handled. Right now, the lexer parses tokens like <code>+=</code> as two separate tokens, so the parser just parses <code>a += 3</code> as assigning <code>3</code> to <code>a +</code>. This way, augmented assignments look the same to the parser no matter whether the augmenting operator’s identifier is alphanumeric or symbolic. Then, the left-hand side <code>a +</code> is parsed as a call expression, the same kind used in juxtaposition for unary operators; and when a call expression is assigned to, it performs an augmented assignment.</p>
<p>This works, but is actually a huge problem for internal consistency. Did you notice it? We already decided that in pattern matching, a pattern like <code>a b</code>, which is a function call, is a “destructure” with <code>a</code>: we give <code>a</code> the value we’re matching the pattern against, and it tells us what value we should match against <code>b</code>. This allows us to effectively pattern-match against negative numbers by having <code>a</code> be <code>-</code> and <code>b</code> be a numeric literal. But this conflicts with wanting it to mean to augment the assignment with <code>b</code> as the function when on the left of an <code>=</code>. Alas, these two interpretations just coexist in an uneasy tension for now; assignments check for the augmented assignment interpretation before allowing any destructuring, but that check is omitted in other pattern matching contexts.</p>
<p>This might seem like a reasonable compromise at first: augmentation doesn’t make much sense when pattern matching in a <code>switch</code>/<code>case</code> or <code>try</code>/<code>catch</code>, which should always bind new variables; and destructuring often doesn’t make sense with a single argument on the left-hand side of an assignment, which should be irrefutable. <code>-x := y</code> is horrible when <code>x := -y</code> works. But I don’t have a satisfying way to reconcile this with a syntax for destructuring structs I’d like some day. Ideally, given a custom product type like <code>struct Foo(bar, baz)</code>, both pattern-matching and simple assignment destructuring would work:</p>
<pre><code>switch (foo) case Foo(bar, baz) -&gt; print(bar, baz);

Foo(bar, baz) = foo</code></pre>
<p>But then the second assignment looks like it has a call on its left-hand side, which we currently parse as augmented assignment. One idea would be to only interpret LHS calls as augmented assignment when the call has one argument, but that seems inelegant and I think custom structs with one field should be well-supported, since they’re useful for emulating sum types. Another idea would be to distinguish <code>a b</code> and <code>a(b)</code> in LHSes, interpreting the parentheses-free version as augmented assignment and the parenthesized version as destructuring. However, augmented assignment with a parenthesized operator, such as <code>(zip +)</code>, isn’t that outlandish (though I might well conclude that forgoing this ability is the least bad option):</p>
<pre><code>a := [2, 5, 3];
a (zip +)= [4, 9, 2];
a # [6, 14, 5]</code></pre>
<p>Perhaps the interpretation should be chosen at runtime based on whether the participating identifiers/expressions are defined or what they evaluate to, like how juxtaposition decides to partially apply the right function on the left argument? This seems… very messy.</p>
<p>Perhaps the lexer should take on more responsibility, lexing code like <code>+=</code> and <code>f=</code> as single tokens that “mean” <code>+</code> or <code>f</code> with an <code>=</code> attached, so that <code>a b =</code> is a destructure but <code>a b=</code> is an augmented assignment? But we also wouldn’t want the lexer to consider the first token of <code>x==y</code> to be <code>x=</code>… right? Or perhaps we could, and require programmers to include the space between <code>x</code> and <code>==</code> when writing an expression like <code>x == y</code>? Or perhaps the lexer can get just one extra character of lookahead? This is all to say, this is one of the corners of the language design I’m the most uncertain about.</p>
<p>Anyway, onto Noulith’s promised subtle semantic difference: augmented assignment like <code>x += y</code> “takes the value” out of <code>x</code> and then <strong>sets <code>x</code> to null</strong> before calling <code>+</code> with the argument. To give a concrete example, this code successfully appends <code>1</code> to <code>x</code> but prints <code>x is null</code>:</p>
<pre><code>x := [];
myappend := \a, b -&gt; (
  print("x is", x);
  a append b
);
x myappend= 1;</code></pre>
<p>This highly unusual behavior turns out to be really important for efficiency, but to explain why, I have to talk about Noulith’s curious semantics around immutability.</p>
<h3 id="immutability-and-copy-on-write-semantics">Immutability and copy-on-write semantics</h3>
<p>Possibly the weirdest semantic feature of Noulith is its approach to immutability. In Noulith, all built-in data types are immutable, in the sense that the assignment to <code>x</code> in the following code doesn’t affect <code>y</code> and vice versa:</p>
<pre><code>x := [1, 2, 3];
y := x;
x[0] = 4;
y[1] += 5;</code></pre>
<p>The same principle applies if you pass <code>x</code> into a function. That function cannot mutate <code>x</code> through its parameter. However, as the same snippet demonstrates, variables holding lists are mutable, and you can set and mutate their elements individually.</p>
<p>To be perfectly honest, this “feature” is something I mostly sleepwalked into: Rust, the implementation language, is really big on immutability, and <code>Rc&lt;Vec&lt;Obj&gt;&gt;</code> is shorter than <code>Rc&lt;RefCell&lt;Vec&lt;Obj&gt;&gt;&gt;</code>. But in hindsight, there are plenty of reasons to like it:</p>
<ul>
<li><p>Nearly everybody who completes Advent of Code in Python learns that you can’t initialize a grid you intend to mutate later with code like <code>x = [[0] * 10] * 10</code>, because then <code>x</code> will consist of ten references to the same mutable list. An assignment like <code>x[0][0] = 1</code> will set 1 in every row. Oops.</p>
<figure>
<a href="https://blog.vero.site/img/aoc-2022-10-drake.jpg"><img src="https://blog.vero.site/img/aoc-2022-10-drake.jpg" alt="Classic Drake meme titled 'Day 10 using python be like:' in which Drake dislikes the code crt = [['.'] * 40] * 6 and likes the code crt = [['.'] * 40 for i in range(6)]"></a>
<figcaption>
<a href="https://www.reddit.com/r/adventofcode/comments/zi4ym4/2022_day_10_valuable_lesson_learned_the_hard_way/">Meme by /u/QultrosSanhattan</a>
</figcaption>
</figure>
<p>Noulith avoids this pitfall.</p></li>
<li><p>Because Python lists are mutable, they can’t be used as dictionary keys, so you need to use Python’s separate tuple type if you want to key a dictionary by sequences. This may mean a bunch of explicit conversions when accessing the dictionary. Noulith also dispenses with this.</p></li>
</ul>
<p>The big, obvious downside is that, if this is implemented naively, mutation is slow! If every assignment like <code>x[i][j] = k</code> had to make a copy of the entire array in case some other variable refers to <code>x</code>, writing performant imperative code would become immensely difficult. I didn’t immediately consider this a dealbreaker — it’s possible to just suck it up and say that Noulith programmers have to get good at working with immutable data structures. As a parallel, you can write a lot of code in Haskell while staying firmly in the land of immutable data structures, generally by building new data structures in sweeps rather than individual mutations (though Haskell’s ecosystem has much more sophisticated data structures to support translating mutation-flavored algorithms, like the finger trees of <a href="https://hackage.haskell.org/package/containers-0.6.0.1/docs/Data-Sequence.html">Data.Sequence</a><a href="#fn8" id="fnref8"><sup>8</sup></a>, not to mention neat ways to achieve local mutability like with the <a href="https://hackage.haskell.org/package/base-4.17.0.0/docs/Control-Monad-ST.html">ST monad</a>). Another plausible escape hatch would have been to expose an explicit “mutable pointer” type.</p>
<p>However, none of that ended up mattering because it was far easier than I expected to implement this non-naively in Rust. The key is that Rust’s reference-counted pointer <code><a href="https://doc.rust-lang.org/std/rc/struct.Rc.html">Rc</a></code> lets you inspect the reference count and mutate through a pointer if and only if you hold the only pointer to that particular value — otherwise, you can choose to make a copy. In practice, you can just call <code><a href="https://doc.rust-lang.org/std/rc/struct.Rc.html#method.make_mut">Rc::make_mut</a></code>. Thus, if you make an <span>\(n \times n\)</span> grid <code>x := 0 .* n .* n</code> and mutate a bunch of cells with <code>x[i][j] = k</code>, some rows will be copied in the first few mutations since their reference counts are &gt; 1, but eventually every row will point to a unique list that can be safely mutated without copying, and the whole endeavor amortizes out to <span>\(O(n^2)\)</span> plus <span>\(O(1)\)</span> per assignment, exactly as asymptotically performant as it would be in, say, Python.</p>
<p>This behavior is the reason for the bizarre temporarily-stashing-<code>null</code> behavior of augmented assignment. Without it, when executing <code>x append= y</code> and calling the <code>append</code> function, there will always be another live reference to the list being appended to, which guarantees Noulith has to perform a <span>\(\Theta(n)\)</span> copy of the list and append to the copy, making <code>append=</code> unusably inefficient. But with this feature, in most common cases <code>append</code> can mutate <code>x</code> and get the job done in <span>\(O(1)\)</span> time. This wasn’t always the strategy: for a while, I kept the old value in <code>x</code> by default and manually marked a bunch of built-ins as pure so that the extra reference would be dropped only when one of those was the function used for augmented assignment. But eventually I decided manually marking built-ins as pure was too much work, too fragile, and still liable to miss cases where the extra reference could be dropped. In particular, it would prevent users from easily writing an efficient function like <code>myappend</code> for a custom data structure without a ton of additional language support. So I just enshrined this behavior into the semantics.</p>
<p>Why don’t other languages take this approach? I expect the answer is just that it’s “too magic”. Subtle changes in your code can easily leave an extra reference to a list somewhere and make manipulations much slower. Not all code is performance-critical, but preventing programmers from reasoning about performance locally to this extent is a big deal.</p>
<p>There are other aspects of Noulith where immutability is even more poorly thought out. The main thing is the presence of a handful of “lazy streams” that can execute arbitrary code when you iterate over them, similar to Python generators or lazy <code>map</code>s in other languages. In theory, it doesn’t make sense to copy a stream like that and pretend it’s immutable. The stream could be modifying files or sending packets as you iterate over it — you can’t just put it in two variables, iterate over one, and expect the other stream to still represent the same sequence of elements. In practice… well, you can just shrug, call it undefined behavior if the code isn’t a pure function, and allow the programmer to shoot themselves in the foot.</p>
<!-- Objective-C uses `^`. https://developer.apple.com/library/archive/documentation/Cocoa/Conceptual/ProgrammingWithObjectiveC/WorkingwithBlocks/WorkingwithBlocks.html -->
<h3 id="other-assignments-and-mutations">Other assignments and mutations</h3>
<p>One of the less pleasing consequences of immutability is that there’s no way to call a function that will mutate an argument. This is unfortunate because there are plenty of common mutations you might want to perform on complex data structures, such as popping the last element from a list, that seem like they should be functions. There is no way to implement a normal function <code>pop</code> such that, if you have a list <code>xs</code>, calling <code>pop(xs)</code> modifies it. You might try to make do by making <code>pop</code> a function that takes a list and separately returns the last element and the list of all previous elements (this is an existing built-in, <code>unsnoc</code> — the inverse of <code>snoc</code>, the reverse of <code>cons</code>, as Lispers will recognize), and then asking people to write:</p>
<pre><code>xs, (x:) = pop(xs)</code></pre>
<p>But if you did this, while <code>pop</code> is running, <code>xs</code> will still refer to the list being popped, so <code>pop</code> will always have to inefficiently make a <span>\(\Theta(n)\)</span> copy of the list, just as <code>append</code> would have without our special handling around augmented assignment. This would make it essentially unusable.</p>
<p>So… I made <code>pop</code> a keyword that mutates its operand and returns the popped value.</p>
<p>There are two other keywords that perform similar mutations: <code>remove</code> removes an element from a list or a value from a dictionary, so given,</p>
<pre><code>xs := [1, 2, 3, 4];</code></pre>
<p><code>remove xs[1]</code> will evaluate to 2 and will leave <code>xs</code> set as <code>[1, 3, 4]</code>. And <code>consume</code> is the lowest-level mutator that takes the value from an lvalue and leaves behind <code>null</code>, in a mechanism vaguely reminiscent of C++ move semantics. This at least gives you another way to efficiently pop an element if you only had <code>unsnoc</code>:</p>
<pre><code>xs, (x:) = unsnoc(consume xs);</code></pre>
<p>More importantly, this lets you write and use analogous efficient mutating functions for custom data structures, although it’s quite verbose. It may be worth introducing a keyword that more elegantly converts <code>unsnoc</code> to <code>pop</code>.</p>
<p>There are a few other weird assignment-related keywords. <code>swap x, y</code> swaps two lvalues, which I think I mostly put together just for the sake of making a good precedence demo. Here’s the remade screenshot from earlier:</p>
<figure>
<a href="https://blog.vero.site/img/noulith-cursed-precedence.png"><img src="https://blog.vero.site/img/noulith-cursed-precedence.png" alt="REPL in which two arithmetic operators are swapped and their precedences are swapped, and this is shown to affect the parsing and return value of a function using those operators. Screenshot of terminal."></a>
</figure>
<p>The tiny advantage of the <code>swap</code> keyword over the classic Pythonic <code>x, y = y, x</code> is just that it’s more concise when the expressions being swapped are long, as they are in the screenshot.</p>
<p>And finally, the <code>every</code> keyword is a way to assign one expression to multiple variables or even at once, like so: <code>every a, b, c = 1</code>. In part, this is Noulith’s response to constructs like Python’s <a href="https://docs.python.org/3/reference/simple_stmts.html#assignment-statements">chained assignment</a> <code>a = b = c = 1</code>, which I believe was itself a restricted version of assignments in a language like C. In C, expressions that evaluate to the assigned value and so can naturally be chained, but allowing this in full generality is a common source of bugs (consider the dreaded <code>if (a = b)</code> when <code>if (a == b)</code> was intended). However, <code>every</code> also walks through sliced lists and dictionaries, giving it a different set of powers than chained assignment. Assuming <code>x</code> is a list, code like <code>every x[2:5] = 1</code> assigns 1 to each of <code>x[2]</code>, <code>x[3]</code>, and <code>x[4]</code>. I cannot remember if I had a specific use case or pain point in mind when designing <code>every</code>; it comes in useful once in a while, but so would a lot of features. I can, just barely, find one place I used it on <a href="https://github.com/betaveros/advent-of-code-2016/blob/d8e4effd1b08808ca8e66fcf5909fda496e60cea/p8.noul#L12">2016 Day 8</a>. So it may be one of those things that sticks around purely through inertia.</p>
<h3 id="naming-built-ins">Naming built-ins</h3>
<p>Syntax is important [citation needed], but a language also needs built-in functions to, well, function.</p>
<p>Noulith has a lot of sequence-manipulating and higher-order functions with English names (<code>map</code>, <code>filter</code>, etc.) that I won’t discuss too much, except with respect to two recurring issues:</p>
<ul>
<li>What part of speech and tense should these function names be? For example, should the function that sorts a list — or more precisely, receives a list and returns a sorted copy — be called <code>sort</code> or <code>sorted</code>? One line of thought I recall from <cite>Java Puzzlers</cite> recommends the latter to describe functions that take an input and produce a new output instead of mutating the input, since the present-tense verb connotes mutation. I think this makes sense in contexts where both kinds of functions appear often, but immutability is so central to Noulith that I decided using shorter present-tense verbs would not cause any confusion.</li>
<li>Should identifiers with multiple words be named with CamelCase or snake_case? This is a tough question that I’ve flipflopped on. Aesthetically, I think snake case looks better, but this is totally subjective; camel case is more compact and easier to type (word breaks marked by capital letters require hitting <kbd>Shift</kbd> one extra time, whereas the underscore itself requires <kbd>Shift</kbd> plus another key). I chose snake case for now mostly because both it’s standard in both Rust, the implementation language, and Python, the scripting language I’d previously use for most use cases Noulith was meant to target.</li>
</ul>
<h4 id="arithmetic-operators-and-semantics">Arithmetic operators and semantics</h4>
<p>A far more interesting topic is choosing the names and definitions of functions with symbolic names, the most familiar of which are the ones for performing basic arithmetic. It might be surprising how much inter-language variation there is here. I think the only uncontroversial operators are <code>+</code>, <code>-</code><a href="#fn9" id="fnref9"><sup>9</sup></a>, and <code>*</code>.</p>
<ul>
<li>What does <code>/</code> mean? Probably division (though not in, e.g., J!), but what kind? In low- to medium-level languages it’s typical for <code>/</code> to mean integer division. It also used to do so in Python 2, but became float division in Python 3. I actually also used the float division definition at first, but eventually I realized that, because I really didn’t care about performance and wanted to do Real Math™, I might as well add rational numbers as in Common Lisp.<a href="#fn10" id="fnref10"><sup>10</sup></a></li>
<li>What does <code>%</code> mean? Probably remainder/modulo, but there are several <a href="https://blog.vero.site/post/modulo">subtly different semantics</a> for it. (And again J has <code>%</code> mean division.) I ended up keeping the C-style behavior for <code>%</code> and offering the paired <code>//</code> and <code>%%</code> for rounding-down division and sign-of-divisor remainder.</li>
<li>What does <code>^</code> mean? There is a bit of a schism here: mathematicians and a handful of programming languages (e.g.&nbsp;Haskell, Lua, Awk (!)) use it for exponentiation due to connoting a superscript and/or LaTeX influence, but lower-level languages usually use it for bitwise xor. I chose to side with the mathematicians here, because for my use cases, I expected exponentiation to be more practically useful than xor, so I didn’t want to give it a longer name like <code>**</code> (plus, I thought there was a natural sequence-related definition for <code>**</code>).</li>
<li><p>How are comparisons made? These are mostly uncontroversial. We do want <code>=</code> to mean assignment, so <code>==</code> is pretty locked-in, and <code>&lt;</code> <code>&gt;</code> <code>&lt;=</code> <code>&gt;=</code> are also close enough to universal<a href="#fn11" id="fnref11"><sup>11</sup></a> that I never seriously considered any alternatives (despite their mild conflict with augmented assignment), but the most common inequality operator <code>!=</code> is harder to justify because <code>!</code> does not mean “not” in Noulith. I considered Haskell’s <code>/=</code> (which visually looks more like ≠), but that would collide with the natural syntax for augmented division-assignment (an issue Haskell itself has experienced: the Lens operator <code><a href="https://hackage.haskell.org/package/lens-5.2/docs/Control-Lens-Operators.html#v:-47--47--61-">//=</a></code> uses a double slash for this reason, and, for consistency, so does every other division-related Lens operator). The alternative I found the most compelling was actually <code>&lt;&gt;</code>, prominent in SQL and offered in Python 2 all the way up until its dying gasp, which is actually quite internally consistent with the other comparisons. But in the end I thought the external consistency consideration for <code>!=</code> was still overwhelming. Other languages that use <code>!=</code> for not-equals without using standalone <code>!</code> to mean “not” include OCaml and fish.</p>
I also included the three-valued comparison “spaceship operator”, <code>&lt;=&gt;</code>, as well as its inverse, <code>&gt;=&lt;</code>.</li>
<li><p>What symbols should be used for bitwise operators? There are some real benefits to not assigning <code>&amp;</code> and <code>|</code> and instead giving those symbols other purposes. For example, <code>&amp;</code> could be saved for some type of concatenation (popular in spreadsheets), which I’d expect to use overwhelmingly more often in scripts than an operator for bitwise AND. But what would I call them instead? Haskell calls them <code>.&amp;.</code> and <code>.|.</code> and F♯ calls them <code>&amp;&amp;&amp;</code> and <code>|||</code>, but I couldn’t find any specific symbolic alternatives with convincing precedent. I think the main alternative would just be to give them a prose name like <code>bitand</code>/<code>bitor</code> instead. Eventually I decided to stick with <code>&amp;</code> and <code>|</code> out of the additional consideration that it was more internally consistent if most operators for doing math on two numbers were single characters (though the paired division/modulo <code>//</code> and <code>%%</code>, as well as bit shifting <code>&lt;&lt;</code> and <code>&gt;&gt;</code>, are all still two characters).</p>
<p>But wait, given that I assigned <code>^</code> already, how do I write bitwise xor? I eventually realized that I could overload <code>~</code> to perform either bitwise complement or xor, depending on whether it’s called with one or two arguments; this is actually internally consistent with how we already decided <code>-</code> would work. Furthermore, this is the same approach as the numerical analysis language <a href="https://yorick.sourceforge.net/refcard/qrlang06.php">Yorick</a> and, curiously, the exact mirror of <a href="https://go.dev/ref/spec#Arithmetic_operators">Go’s approach</a>, whereby <code>^</code> means both bitwise xor and complement so that <code>~</code> can be assigned a different meaning, so this decision isn’t indefensible in terms of external consistency either. I didn’t consciously recall these examples when deciding on these names, but felt like there was precedent.</p></li>
</ul>
<h4 id="sequence-operators">Sequence operators</h4>
<ul>
<li><p>What operator should we use for list and/or string concatenation? One of the most popular options is overloading <code>+</code>, but I never actually really liked that. I think overloading the same operator to mean numeric addition and sequence concatenation is really hard to justify from first principles. Nor is it satisfying to the mathematicians: any algebraist will tell you that <code>+</code> usually connotes that you’re working in an abelian group, but concatenation is neither commutative (in general, <code>a + b</code> does not equal <code>b + a</code>) nor invertible (in general, there is no “negative string” <code>-a</code> such that <code>a + -a</code> is the identity element, i.e., the empty sequence). Furthermore, you <em>could</em> imagine generalizing <code>+</code> and other arithmetic operators to some sequences, simply by adding or operating on elements pairwise, and in fact I did want to do that for the specific sequence type of “vectors” because it’s immensely useful in practice.</p>
<p>So, what instead? There are a lot of options justifiable with precedent: D uses <code>~</code>, some MLs and F♯ use <code>@</code>, Ada uses <code>&amp;</code>, Smalltalk uses <code>,</code>, Maple (and notation popular in cryptography) sometimes uses <code>||</code><a href="#fn12" id="fnref12"><sup>12</sup></a>… Eventually I went with Haskell/Scala’s <code>++</code> because it generalizes well to suggest symbolic names for other sequence operations, obtained by doubling similar arithmetic operators: <code>**</code> is the Cartesian product; <code>&amp;&amp;</code>, <code>||</code>, <code>--</code> combine sets.</p>
<p>Following this train of thought also allows us to define systematic operators for prepending/appending. Here Scala uses <code>+:</code> and <code>:+</code>, with the mnemonic, “the <strong>col</strong>lection is on the <strong>col</strong>on side”, but I wanted to save the colon for other things, so I instead chose <code>.</code> with the opposite orientation, a <strong>single</strong> dot on the side with a <strong>single</strong> object. So <code>.+</code> prepends one item to a list and <code>+.</code> appends one item to a list. This also generalizes well to other kinds of collections and operations: adding one element to a set can be <code>|.</code>, “replicating” an item into a list of <i>n</i> items can be <code>.*</code>, joining two items into a length-2 list can be <code>..</code>, etc. One popular and pretty reasonable complaint about languages that make extensive use of operators or support operator overloading is that operators are particularly cryptic and hard to look up, so I wanted the “vocabulary” of operator symbols to be conceptually simple.</p>
I also chose to allocate a separate operator, <code>$</code>, to string concatenation, partly because I again thought the kinds of concatenation were conceptually distinct, partly because I could then make <code>$</code> coerce all of its arguments to strings without feeling bad about shoehorning coercions into overloads. This became less compelling later as I added byte strings and “vectors” of numbers, which are other sequence types that sometimes need to be concatenated but that I didn’t want separate concatenation operators for, as well as format strings, which enable coercion to be done even more explicitly. Still, there’s something nice about having <code>apply $</code> close at hand for mashing a bunch of strings together.</li>
<li><p>Finally, this is not exactly an operator, but what syntax (if any) should we use for “splatting” — that is, declaring a function that takes a variable number of arguments and/or calling a function with a list of arguments? We can’t make <code>*</code> serve double duty as in Python/Ruby since it’s just a normal identifier, so <code>...</code> of languages like JavaScript seemed the best idea.</p></li>
</ul>
<h4 id="more-function-composition">More function composition</h4>
<p>The last batch of operators I think are worth remarking on are those for function composition. I stole <code>&gt;&gt;&gt;</code>, <code>&lt;&lt;&lt;</code>, <code>&amp;&amp;&amp;</code>, and <code>***</code> from Haskell’s <a href="https://hackage.haskell.org/package/base-4.17.0.0/docs/Control-Arrow.html">Control.Arrow</a>:</p>
<pre><code>(f &lt;&lt;&lt; g &lt;&lt;&lt; h)(a, b, c) = f(g(h(a, b, c)))
(f &gt;&gt;&gt; g &gt;&gt;&gt; h)(a, b, c) = h(g(f(a, b, c)))
(f &amp;&amp;&amp; g &amp;&amp;&amp; h)(a, b, c) = [f(a, b, c), g(a, b, c), h(a, b, c)]
(f *** g *** h)(a, b, c) = [f(a), g(b), h(c)]</code></pre>
<p>They are quite verbose, but I couldn’t think of a better batch of names that would be acceptably internally consistent.</p>
<p>A slightly different function composition operator is Haskell’s <code>on</code>, which is actually from <a href="https://hackage.haskell.org/package/base-4.17.0.0/docs/Data-Function.html#v:on">Data.Function</a>, and primarily intended to be used in the exact same way.</p>
<pre><code>(f on g)(a, b, c) = f(g(a), g(b), g(c))</code></pre>
<p>Finally, lists of arguments can be “splatted” into functions with the JavaScript-inspired <code>of</code> and <code>apply</code>, which is useful for chaining with things that produce lists:</p>
<pre><code>f of [a, b, c] = f(a, b, c)
[a, b, c] apply f = f(a, b, c)</code></pre>
<h3 id="lessons-learned">Lessons learned</h3>
<p>I think I predicted that requiring myself to use only Noulith on Advent of Code would make my median leaderboard performance better but my worst-case and average performances significantly worse. I don’t think my median performance improved, but my worst-case performance definitely got worse. Somehow it still didn’t matter and I placed top of the leaderboard anyway. (I will note that 2021’s second to fourth place all didn’t do 2022.)</p>
<p>One completely predictable issue: Debugging Noulith code is much harder because most programmers can be pretty confident that the root cause of a bug isn’t a bug in the language implementation. That assumption is not safe when you also wrote the language! For every bug I encountered, I had to consider whether the cause might have been in the couple dozen lines I had written for that day or in the 13,000 lines of Rust I had written over the prior few months. In the end, I don’t think I ever encountered any correctness bugs in the language while doing Advent of Code — that is, bugs where the language executed a program successfully but gave the wrong result — but that didn’t prevent me from considering such a hypothesis at several moments, so I was still substantially slower debugging. I did encounter a few bugs that caused errors where they shouldn’t have, as well as surprising interactions between features that I’m not sure count as bugs but suggest a flaw in the design <em>somewhere</em>: for example, on Day 21, I realized that the natural stringification of negative fractions like <code>-1/2</code> cannot be <code>eval</code>ed due to the same precedence issues as always. Not to mention quite a few correctness bugs in later days that I was just lucky enough to not hit before Christmas.</p>
<p>I was also not quite pessimistic enough about my Noulith interpreter simply being slow. There weren’t any days that became impossible, but there were several days where I believe I would have finished several minutes faster if I had just implemented the same algorithm in Python, whereas I expected maybe only one.</p>
<p>Taking a step back, the language design process was a lot of fun. One thing I enjoyed, which would be unrealistic in a more serious language, was the freedom to just add keywords willy-nilly (<code>swap</code>, <code>literally</code>, <code>coalesce</code>). Adding keywords to a language with production users tends to be a big deal for the simple reason that it breaks code using that word as an identifier (unless the keyword manages to be a “soft keyword” or something, but that just complicates the parser even more). Also naming things is hard in general. (Although it’s not a keyword per se, consider JavaScript’s <code><a href="https://github.com/tc39/proposal-global/blob/master/NAMING.md">globalThis</a></code> and the dozens of rejected names.) This freedom also allowed me to avoid the temptation to add punctuation to the syntax: the set of usable punctuation characters is much more finite and makes me want to be quite confident that a language feature is worthy of one before assigning a character to it, not to mention that search engines often have trouble with documentation for them.</p>
<p>Reflecting on the entire process, strangely enough, I’m reminded of this bit from Lockhart’s Lament, about imagining and investigating mathematical objects:</p>
<blockquote>
<p>[O]nce you have made your choices […] then your new creations do what they do, whether you like it or not. This is the amazing thing about making imaginary patterns: they talk back!</p>
</blockquote>
<p>The language design and semantics, independent of the implementation, are in some sense an imaginary pattern, and I did often feel like the choices I made “talked back”. See how chaining comparison operators led to mutable runtime operator precedences, or how immutability led to a custom world of move-semantics-like keywords. Pretty neat.</p>
<p>As for my broader practical goals for Noulith, in terms of becoming a better go-to language for quick and dirty scripts: it worked, 100%. I solved at least two Mystery Hunt puzzles with it and used it extensively for data munging in another upcoming project, sometimes while I was on a different system without my dev setup, and I expect to continue.</p>
<p>Still, maybe the most generalizable takeaway is just how I encountered <a href="https://www.hyrumslaw.com/">Hyrum’s Law</a>. I haven’t made any promises of stability/compatibility in any shape or form — I haven’t updated the <code>Cargo.toml</code> version field from 0.1.0 since the first commit — but it sort of doesn’t matter anyway: there’s a bunch of random Noulith files out there in the wild, somebody even wrote a <a href="https://dan-simon.github.io/puzzles/december_2022/mentally_stoned.html">puzzle about the language</a>, and I would feel a little bad for breaking them without a good reason.</p>
<p>Overall, 10/10, would do again.</p>
<h3 id="the-fun-fact-roulette">The fun fact roulette</h3>
<p>In no particular order, here are some fun facts about non-Noulith languages that I learned or remembered while writing this post. They’re all referenced somewhere in the 16,000 preceding words but I don’t blame anybody for not reading all that.</p>
<p>Did you know that:</p>
<ul>
<li>In Swift, <a href="https://docs.swift.org/swift-book/documentation/the-swift-programming-language/declarations/#Precedence-Group-Declaration">operator precedences form a poset</a> — an operator’s precedence can be greater than, less than, equal to, or incomparable with another’s?</li>
<li>In Prolog, operators have <a href="https://www.swi-prolog.org/pldoc/man?section=operators">precedences that are integers between 0 and 1200</a>?</li>
<li>In Game Maker Language, numbers are cast to booleans by checking if <a href="https://manual.yoyogames.com/GameMaker_Language/GML_Reference/Variable_Functions/bool.htm">they’re greater than or equal to 0.5</a>?</li>
<li>In TLA<sup>+</sup>, the Cartesian product × is a special syntax construct rather than an infix operator so that <i>n</i>-way Cartesian products hold <i>n</i>-tuples rather than nested pairs?</li>
<li>In Ceylon, variables are <a href="https://www.ceylon-lang.org/documentation/1.3/tour/attributes-control-structures/">declared to be mutable with the eight-letter keyword <code>variable</code></a>?</li>
<li>More than <a href="https://github.com/tc39/proposal-global/blob/master/NAMING.md">three dozen names were rejected</a> for JavaScript’s <code>globalThis</code>?</li>
<li>In Haskell, the <code>-</code> prefix operator is syntactically special and <a href="https://www.haskell.org/onlinereport/haskell2010/haskellch3.html#x8-300003.5">always calls the default (Prelude) <code>negate</code> function</a>, regardless of whether the default <code>-</code> and <code>negate</code> are in scope?</li>
<li>In LiveScript, the operators <a href="https://livescript.net/#operators"><code>&lt;?</code> and <code>&gt;?</code> compute the min and max</a> of their operands?</li>
<li>An extremely rare breaking change to <a href="https://github.com/golang/go/issues/20733">redefine range loop variables in each iteration</a> is under consideration for Go 2?</li>
<li>In Java, <code>-2147483648</code> is a legal expression, but <code>-(2147483648)</code> is a compile-time error?</li>
</ul>
<h3 id="appendix-what-is-a-noulith">Appendix: What is a noulith?</h3>
<p>Nouliths are the weapons wielded by Sages, a healer class from the critically acclaimed MMORPG Final Fantasy XIV, with an expanded free trial in which you can — *ahem*</p>
<p>It is hard to describe exactly what nouliths are, but in-game we’re introduced to them as a set of four “short staves” that sages control with their mind to draw. A Wikipedia blurb calls them “magical aether foci”. According to Reddit sleuths, etymologically, the name is based on <a href="https://www.reddit.com/r/ffxiv/comments/ldwbq0/i_think_this_is_root_of_nouliths_word/">Ancient Greek</a>: <a href="https://en.wiktionary.org/wiki/%CE%BD%CF%8C%CE%BF%CF%82#Ancient_Greek">νόος</a> “mind” + <a href="https://en.wiktionary.org/wiki/%CE%BB%CE%AF%CE%B8%CE%BF%CF%82#Ancient_Greek">λίθος</a> “stone”. (The Sage’s skill set is <a href="https://www.reddit.com/r/ffxiv/comments/q7vftc/sge_theme_spell_name_meanings_explained_optional/">Ancient Greek and themed around the medical theory of Humors</a>.)</p>
<p>I thought the name was apt because computers are also just smart rocks we try to control with our minds sometimes, and this programming language was an attempt to make a tiny corner of that control a little smoother for a single person. (Who also mains Sage nowadays.)</p>
<figure>
<a href="https://blog.vero.site/img/ffxiv-gpose-1.png"><img src="https://blog.vero.site/img/ffxiv-gpose-1.png"></a>
<figcaption>

</figcaption>
</figure>
<section>
<hr>
<ol>
<li id="fn1"><p>All exact letters and numbers have been changed to minimize spoilers. <a href="https://betaveros.github.io/noulith/#cmVhZF9jb21wcmVzc2VkKCkubGluZXMgZmlsdGVyIChcdyAtPiBsZW4odykgPT0gMTAgYW5kICJhYmMiIG1hcCAodyBjb3VudCkgYWxsICg9PSAxKSBhbmQgd1s4XSA9PSAiayIpIHRoZW4gdW5saW5lcw==#yawl.gz">Here’s an implementation</a>.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Possibly modulo the same kind of issues with weird characters in string literals that made <a href="http://timelessrepo.com/json-isnt-a-javascript-subset">JSON not a subset of JavaScript</a> (<a href="https://github.com/judofyr/timeless/blob/master/posts/json-isnt-a-javascript-subset.md">GitHub source because the site’s down for me</a>) (<a href="https://github.com/tc39/proposal-json-superset">until ES2019</a>).<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Though I note that using this syntax for most methods is <a href="https://docs.scala-lang.org/style/method-invocation.html#arity-1-infix-notation"><em>not</em> considered idiomatic Scala</a>.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>Instead (as described at the link), <code>-x</code> always invokes Haskell’s predefined <code>negate</code> function on <code>x</code>. This is true even if the identifier <code>-</code> is bound to something other than Haskell’s predefined subtraction function!<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>See p.&nbsp;284 of <a href="https://lamport.azurewebsites.net/tla/book-21-07-04.pdf">Specifying Systems (PDF)</a>:</p>
<blockquote>
<p>However, × is part of a special construct, not an infix operator.</p>
</blockquote>
<p>Or <a href="https://old.learntla.com/tla/tuples/#sets-of-tuples">Sets of Tuples</a> (from the “old” Learn TLA<sup>+</sup> guide, but the new guide doesn’t currently explain this as clearly).<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>Soemwhat more precisely, in legitimate conditions that contain the opening delimiter, the prefix before the delimiter can’t also look like a legitimate condition. But code like <code>if if true { ... } else { ... } { ... }</code> is legal Rust.<a href="#fnref6">↩</a></p></li>
<li id="fn7"><p>This was the summer I graduated from high school. And also the only year I made Google Code Jam World Finals. Huh.<a href="#fnref7">↩</a></p></li>
<li id="fn8"><p>The 2-3 finger trees implemented herein support amortized <span>\(O(1)\)</span> read-write access, including adding and removing elements, to the front and back, despite being fully immutable!<a href="#fnref8">↩</a></p></li>
<li id="fn9"><p>Though consider: using <a href="https://twitter.com/eevee/status/1098672717404852224">−, U+2212, for subtraction</a> instead?<a href="#fnref9">↩</a></p></li>
<li id="fn10"><p>In fact, the specific motivating incident was that I was trying to solve one day’s <a href="https://beastacademy.com/all-ten">All Ten</a>: using the numbers 3, 5, 7, 8 each exactly once, and the four standard arithmetic operators <code>+-*/</code> as desired, make 5.</p>
<p>This is amazingly hard, though it’s probably easier now that I’ve told you it’s hard, so you can look directly for unreasonable-seeming arithmetic expressions.</p>
<p>Here’s the source code for the solver:</p>
<pre><code>choose_one := \xs -&gt; for (i, x &lt;&lt;- xs) yield [x, xs[:i] ++ xs[i+1:]];

dfs := \inputs, target -&gt; switch (inputs)
    case [x] -&gt; if (x[0] == target) print(x[1])
    case _ -&gt; for (
        [x, xe], r &lt;- choose_one inputs;
        [y, ye], r' &lt;- choose_one r;
        name, op &lt;- [["+", +], ["-", -], ["*", *], ["/", /]]
    ) dfs! r' +. [x op y, F"({xe} {name} {ye})"], target;

dfs! [3, 5, 7, 8] map (\x -&gt; [x, str(x)]), 5</code></pre>
<a href="#fnref10">↩</a></li>
<li id="fn11"><p>One much rarer alternative that I think is interesting is using <code>=&lt;</code> instead of <code>&lt;=</code>. Pixel’s page lists Mercury and Oz as two languages that do so; I know that TLA<sup>+</sup> offers both <a href="https://apalache.informal.systems/docs/lang/integers.html#integer-less-than-or-equal"><code>=&lt;</code> and <code>&lt;=</code></a>; <a href="https://datatracker.ietf.org/doc/html/rfc1345">RFC 1345</a>, and by extension Vim digraphs, use <code>=&lt;</code> as the mnemonic for ≤. I think the reason in every single case is so that <code>&lt;=</code> and <code>=&gt;</code> can be used as arrows. Still, I thought external consistency with other programming languages and simply with the way that we say “less than or equal to” is also overwhelming.<a href="#fnref11">↩</a></p></li>
<li id="fn12"><p>I was reviewing a reference cryptography implementation in C++ not too long ago and observed that the author defined his own wrapper class for <code>std::string</code> that, as far as I could tell, primarily existed so that the author could define <code>operator||</code> on that class to mean concatenation.<a href="#fnref12">↩</a></p></li>
</ol>
</section></article>
	
	
	
	
	
	
	
	
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[It's true. Your devices are listening to you (101 pts)]]></title>
            <link>https://www.cmglocalsolutions.com/cmg-active-listening</link>
            <guid>38255425</guid>
            <pubDate>Mon, 13 Nov 2023 21:10:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cmglocalsolutions.com/cmg-active-listening">https://www.cmglocalsolutions.com/cmg-active-listening</a>, See on <a href="https://news.ycombinator.com/item?id=38255425">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
<div data-page-id="747770" data-theme="" data-layout-id="15662" data-title="Hero Image" data-hide-inview="true">
<h2>It's True. Your Devices Are Listening to You.</h2>
<div><p><span dir="ltr">With Active Listening, CMG can now use voice data to target your advertising to the EXACT people you are looking for.</span></p>
</div>

</div>
<div data-page-id="747776" data-theme="" data-layout-id="15667" data-title="Content Image List">
<div data-animation="inview-cascade-fade-up">
<h2>Imagine This...</h2>
<p>What could it do for your business, if you were able to target potential clients or customers who are using terms like this in their day to day conversations:</p>
<div>
<p>The car lease ends in a month- we need a plan.</p>
<p>We need to get serious about planning for retirement.</p>
<p>A mini van would be perfect for us.</p>
<p>This AC is on it's last leg!</p>
<p>Do I see mold on the ceiling?</p>
<p>We need a better mortgage rate.</p>
</div>
</div>
<div data-animation="inview-cascade-fade-up">
<picture>
<img data-src="https://transform.octanecdn.com/width/900/https://octanecdn.com/cmglocalsolutionsnew/cmglocalsolutionsnew_918657998.jpg" alt="a person painting a room" data-lazy-load="true" src="https://transform.octanecdn.com/width/900/https://octanecdn.com/cmglocalsolutionsnew/cmglocalsolutionsnew_918657998.jpg">
</picture>
</div>
</div><div data-animation="inview-cascade-fade-up" data-page-id="747772" data-theme="" data-layout-id="15664" data-title="60/40 Content Image">
<div>
<h3>Create Personas</h3>
<p>We create buyer personas by uploading past client data into the platform.</p>
</div>
<div>
<h3>Identify Keywords</h3>
<p>We identify top performing keywords relative to the type of customer you are looking for.</p>
</div>
<div>
<h3>Tracking</h3>
<p>We set up tracking via pixel placed on your site, so we can track your ROI in real time. </p>
</div>
<div>
<h3>Listening</h3>
<p>Active Listening begins and is analyzed via AI to detect pertinent conversations via smartphones, smart tvs and other devices. </p>
</div>
<div>
<h3>Analysis</h3>
<p>As qualified consumers are detected, a 360 analysis via AI on past behaviors of each potential customer occurs.</p>
</div>
<div>
<h3>Create a List</h3>
<p>With the audience information gathered, an encrypted evergreen audience list is created.</p>
</div>
<div>
<h3>Re-targeting</h3>
<div><p>We use the list to target your advertising via many different platforms and tactics including:</p>
<p>- Streaming TV/OTT</p>
<p>- Streaming Audio</p>
<p>- Display Ads</p>
<p>- Paid Social Media</p>
<p>- YouTube</p>
<p>- Mobile Precise</p>
<p>- Google/Bing Search (PPC)</p></div>
</div>
</div><div data-animation="inview-cascade-fade-up" data-page-id="747781" data-theme="" data-layout-id="12188" data-title="Content Image">
<div>
<picture>
<img data-src="https://transform.octanecdn.com/width/900/https://octanecdn.com/cmglocalsolutionsnew/cmglocalsolutionsnew_473222309.jpg" alt="a city with a freeway and buildings" data-lazy-load="true" src="https://transform.octanecdn.com/width/900/https://octanecdn.com/cmglocalsolutionsnew/cmglocalsolutionsnew_473222309.jpg">
</picture>
</div>
<div data-animation="inview-cascade-fade-up">
<h2 data-styleable-text="">Claim Your Exclusive Territory Before Your Competitor&nbsp;</h2>
<p>Our technology provides a process that makes it possible to know exactly when someone is in the market for your services in real-time, giving you a significant advantage over your competitors. Territories are available in 10 or 20 mile radiuses, but customizations can be made for regional, state and national coverage. </p>

</div>
</div><div data-animation="inview-cascade-fade-up" data-page-id="747785" data-theme="" data-layout-id="12188" data-title="Content Image">
<div>
<picture>
<img data-src="https://transform.octanecdn.com/width/900/https://octanecdn.com/cmglocalsolutionsnew/cmglocalsolutionsnew_181705900.jpg" alt="a group of people sitting on a dock" data-lazy-load="true" src="https://transform.octanecdn.com/width/900/https://octanecdn.com/cmglocalsolutionsnew/cmglocalsolutionsnew_181705900.jpg">
</picture>
</div>
<div data-animation="inview-cascade-fade-up">
<h2 data-styleable-text="">We know what you are thinking...</h2>
<p>Is this legal? YES- it is totally legal for phones and devices to listen to you. That's because consumers usually give consent when accepting terms and conditions of software updates or app downloads.</p>

</div>
</div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Discouraging the use of web application firewalls (219 pts)]]></title>
            <link>https://www.macchaffee.com/blog/2023/wafs/</link>
            <guid>38255004</guid>
            <pubDate>Mon, 13 Nov 2023 20:32:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.macchaffee.com/blog/2023/wafs/">https://www.macchaffee.com/blog/2023/wafs/</a>, See on <a href="https://news.ycombinator.com/item?id=38255004">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
        <p>I wanted to write this because I don't hear enough real people discouraging the use of Web Application Firewalls (WAFs). Probably because the search results for "Web Application Firewall" are all written by WAF vendors. Anyone reading just that could conclude that WAFs are a good idea. I'm here to offer another perspective, after having suffered through using a WAF for two years.</p>
<p>Web Application Firewalls were created early in the Internet's history, especially popularized by the <a href="https://en.wikipedia.org/wiki/ModSecurity">ModSecurity project in 2002</a>. WAFs essentially work by intercepting every single HTTP request (and sometimes responses too) and evaluating several hundred regular expressions over the URI, headers, and body, sometimes aided by machine learning. If the request kinda looks like SQL, shell code, etc., the server may block your request.</p>
<p>In the infancy of the cybersecurity field, WAFs seemed like a good idea. HTTP requests were tiny, infrequent, and mostly contained mundane form data. But today, WAFs have overstayed their welcome in the security toolbelt. There are better techniques you can use that make even the most advanced WAFs entirely obsolete.</p>
<h2 id="wafs-have-horrible-performance">WAFs have Horrible Performance</h2>
<p>Since WAFs run hundreds of regular expressions on every request, you may ask, "isn't that super inefficient?" Yes, very.</p>
<table><thead><tr><th></th><th>WAF</th><th>No WAF</th></tr></thead><tbody>
<tr><td>Average time taken to upload 9,462 text files</td><td>7.36</td><td>4.55</td></tr>
<tr><td>Average requests per second</td><td>1285</td><td>2079</td></tr>
<tr><td>Number of requests blocked erroneously</td><td>5</td><td>0</td></tr>
<tr><td>Peak nginx CPU during trial</td><td>73%</td><td>8%</td></tr>
</tbody></table>
<details>
<summary>
<em>Specifics about the benchmark</em>
</summary>
<hr>
The easiest way I know to get modsecurity + CoreRuleSet installed is through ingress-nginx, which I've installed in a Kind cluster.
<pre data-lang="bash"><code data-lang="bash"><span># https://kind.sigs.k8s.io/docs/user/quick-start/
</span><span>cat </span><span>&lt;&lt;</span><span>EOF </span><span>| </span><span>kind</span><span> create cluster</span><span> --config</span><span>=-
</span><span>kind: Cluster
</span><span>apiVersion: kind.x-k8s.io/v1alpha4
</span><span>nodes:
</span><span>- role: control-plane
</span><span>  extraPortMappings:
</span><span>  - containerPort: 32080
</span><span>    hostPort: 32080
</span><span>    protocol: TCP
</span><span>  - containerPort: 32443
</span><span>    hostPort: 32443
</span><span>    protocol: TCP
</span><span>EOF
</span><span>
</span><span># https://kubernetes.github.io/ingress-nginx/user-guide/third-party-addons/modsecurity/
</span><span>helm</span><span> upgrade</span><span> --install</span><span> ingress-nginx ingress-nginx \
</span><span>  --repo</span><span> https://kubernetes.github.io/ingress-nginx \
</span><span>  --namespace</span><span> ingress-nginx</span><span> --create-namespace </span><span>\
</span><span>  --set</span><span> controller.service.type=NodePort \
</span><span>  --set</span><span> controller.service.nodePorts.https=32443 \
</span><span>  --set</span><span> controller.service.nodePorts.http=32080 \
</span><span>  --set</span><span> controller.ingressClassResource.default=true \
</span><span>  --set</span><span> controller.allowSnippetAnnotations=true
</span></code></pre>
<p>For the test, I'll be uploading files to MinIO using these values:</p>
<pre data-lang="yaml"><code data-lang="yaml"><span>replicas</span><span>: </span><span>1
</span><span>mode</span><span>: </span><span>standalone
</span><span>resources</span><span>:
</span><span>  </span><span>requests</span><span>:
</span><span>    </span><span>memory</span><span>: </span><span>512Mi
</span><span>persistence</span><span>:
</span><span>  </span><span>enabled</span><span>: </span><span>false
</span><span>rootUser</span><span>: </span><span>rootuser
</span><span>rootPassword</span><span>: </span><span>rootpass123
</span><span>buckets</span><span>:
</span><span>  - </span><span>name</span><span>: </span><span>bucket1
</span><span>    </span><span>policy</span><span>: </span><span>none
</span><span>    </span><span>purge</span><span>: </span><span>false
</span><span>ingress</span><span>:
</span><span>  </span><span>enabled</span><span>: </span><span>true
</span><span>  </span><span>hosts</span><span>: [</span><span>minio-waf.localhost</span><span>]
</span><span>  </span><span>annotations</span><span>:
</span><span>    </span><span>nginx.ingress.kubernetes.io/enable-modsecurity</span><span>: "</span><span>true</span><span>"
</span><span>    </span><span>nginx.ingress.kubernetes.io/enable-owasp-core-rules</span><span>: "</span><span>true</span><span>"
</span><span>    </span><span>nginx.ingress.kubernetes.io/modsecurity-snippet</span><span>: </span><span>|
</span><span>      Include /etc/nginx/owasp-modsecurity-crs/nginx-modsecurity.conf
</span><span>      SecRuleEngine On
</span><span>      # Even the core rules are ridiculous, blocking PUT requests, certain content-types, or any body with "options" in it
</span><span>      SecRuleRemoveById 911100 920420 921110
</span></code></pre>
<pre data-lang="bash"><code data-lang="bash"><span>helm</span><span> upgrade</span><span> --install</span><span> minio minio/minio</span><span> -f</span><span> values.yaml</span><span> -n</span><span> minio</span><span> --create-namespace
</span><span>helm</span><span> upgrade</span><span> --install</span><span> minio-waf minio/minio</span><span> -f</span><span> values-waf.yaml</span><span> -n</span><span> minio-waf</span><span> --create-namespace
</span><span># Verify the WAF is working (should get a 403)
</span><span>curl </span><span>'</span><span>http://minio-waf.localhost:32080/?q=../../etc/passwd</span><span>'
</span></code></pre>
<p>We'll be uploading just the "Documentation" folder of the v6.6 Linux Kernel, which contains 9462 files for a total of 65MB.</p>
<pre data-lang="bash"><code data-lang="bash"><span>curl -LO</span><span> https://github.com/torvalds/linux/archive/refs/tags/v6.6.zip
</span><span>unzip</span><span> v6.6.zip '</span><span>linux-6.6/Documentation/*</span><span>'
</span></code></pre>
<p>Configure the minio client:</p>
<pre data-lang="bash"><code data-lang="bash"><span># You may need to add these hosts to /etc/hosts
</span><span>export </span><span>MC_HOST_nowaf</span><span>='</span><span>http://rootuser:rootpass123@minio.localhost:32080</span><span>'
</span><span>export </span><span>MC_HOST_waf</span><span>='</span><span>http://rootuser:rootpass123@minio-waf.localhost:32080</span><span>'
</span></code></pre>
<p>Run the benchmark (5 times each):</p>
<pre data-lang="bash"><code data-lang="bash"><span>time</span><span> mc cp</span><span> -r</span><span> linux-6.6/Documentation/ waf/bucket1/
</span><span>time</span><span> mc cp</span><span> -r</span><span> linux-6.6/Documentation/ nowaf/bucket1/
</span></code></pre>
<hr>
</details>
<p>In addition to slowing down every request, you also need significant additional RAM for buffering requests. Since not a single byte in the buffer can be flushed to the backend server until the WAF completes its analysis, you need several gigabytes of RAM to store request bodies. Servers like nginx buffer requests by default, but enough large concurrent requests (like pushing a container image) can make a buffering web server run out of RAM. When using a WAF, every server becomes a buffering web server, which is simply incompatible with many types of applications.</p>
<p>I know computers are fast and hardware is cheap, but we shouldn't be spending that kind of CPU and RAM on WAFs unless they're a really effective security tool. But they aren't, as you'll see next.</p>
<h2 id="wafs-are-easily-bypassed">WAFs are Easily Bypassed</h2>
<p>WAF vendors and attackers are locked in a constant arms race, but it seems <a href="https://github.com/0xInfection/Awesome-WAF#evasion-techniques">attackers are much better armed</a>. How could they not be? Many of the attacks that a WAF purports to block involve complex grammars like SQL, shell code, and entire programming languages. They often include comments, character escaping, encoding issues, and more oddities. These oddities mean that attackers always have a significant advantage and can typically bypass any WAF rule if they are clever enough.</p>
<p>For example, you might think <a href="https://en.wikipedia.org/wiki/Log4Shell">Log4shell</a> is pretty easy to catch: just check for <code>${jndi</code>, right? Unfortunately, Log4J supports nested "<a href="https://logging.apache.org/log4j/2.x/manual/lookups.html">lookups</a>", including ones that convert letters to upper/lower case like <code>${lower:J}</code></p>
<p>That means an attacker can insert an arbitrary number of nested lookups around each letter and still perform the attack, like this: <code>${${lower:J}ndi:...</code>. This lead CloudFlare to say <a href="https://blog.cloudflare.com/exploitation-of-cve-2021-44228-before-public-disclosure-and-evolution-of-waf-evasion-patterns/">"WAF vendors need to be looking at any occurrence of <code>${</code> and treating it as suspicious"</a>, which is just another hilarious example of how WAFs can never live up to the expectations placed on them.</p>
<p>I just discussed the fairly simple grammar that is Log4J Lookups, but you can imagine how many more evasion tactics you could use in a language as complex as SQL or PHP, especially when considering encoding tricks. For an in-depth description of specific WAF bypass techniques, check out <a href="https://habr.com/en/companies/dsec/articles/454592/">this awesome post</a>.</p>
<p>Another way to bypass a WAF involves just padding your attack string to appear <a href="https://docs.aws.amazon.com/waf/latest/developerguide/waf-oversize-request-components.html">&gt;8KB or so</a> into the request body. Like I mentioned in the section on performance, request bodies must be buffered into RAM for analysis, so WAFs must choose some cut-off point to avoid spending infinite CPU and RAM on a single request. For some WAFs like AWS's, that cutoff point is around 8KB. So if you just put 8192 innocuous characters before your Log4Shell attack string, you've rendered the WAF worthless.</p>
<h2 id="wafs-are-an-attack-vector">WAFs are an Attack Vector</h2>
<p>In 2019, CapitalOne experienced a breach of 100 million credit applications that was <a href="https://krebsonsecurity.com/2019/08/what-we-can-learn-from-the-capital-one-hack/">allegedly caused by a WAF misconfiguration</a>. The attacker allegedly tricked the WAF into sending requests to the EC2 Metadata Service, which handed out a credential that allowed reading sensitive files from S3.</p>
<p>While this is just one example, it illustrates the curious fact that WAFs actually have a large attack surface.</p>
<p>Most WAFs are giant, complex codebases that are usually closed-source and written in memory-unsafe languages. Since they're expensive "enterprise" products, companies stuff them full of unnecessary features to make them stand out more than competitors. All of this adds up to make WAFs yet another example of a dangerous "security" tool, <a href="https://www.macchaffee.com/blog/2023/solarwinds-hack-lessons-learned/">just like SolarWinds</a>.</p>
<p>No security officer would approve taking such a risky piece of software, putting it directly on the internet, making it parse mountains of untrusted input, and giving it access to all your backend servers, logging infra, SIEM, alerting systems, <a href="https://docs.fastly.com/en/ngwaf/jira">and even JIRA for some reason</a> UNLESS it's covered in security buzzwords and costs 5-6 figures per year.</p>
<p>Somehow, companies that sell security products have gotten a pass on implementing foundational security principles like secure by default, secure by design, attack surface reduction, and the principle of least privilege. Don't let them keep getting away with that.</p>
<h2 id="wafs-have-a-high-false-positive-rate">WAFs have a High False Positive Rate</h2>
<p>Over the last twenty years, open-source WAF rulesets have expanded considerably to detect more-recent types of attack. Apparently all those proprietary WAFs are doing the same. That means there are more and more possible strings that could trigger a WAF to block your request. If you want to write a comment on an article discussing Log4shell, you might be blocked for including the string <code>${jndi</code> in your comment. So naturally the false positive rate continues to rise with every new rule, and it's already quite high based on my experience maintaining a giant list of ModSecurity rule exceptions.</p>
<p>So-called "next-generation" WAFs claim to solve this problem by <a href="https://docs.fastly.com/en/ngwaf/about-next-gen-waf">looking at multiple requests</a> or by using <a href="https://docs.fastly.com/en/ngwaf/about-the-architecture#about-the-collection-and-analysis-system">IP reputation systems</a>. While these can improve false positive rates, they can never truly solve the problem. In some ways, less false positives can increase the impact of particular false positives since neither users nor support teams have a clear procedure for fixing it. CloudFlare's algorithm can randomly decide to block you and <a href="https://www.ctrl.blog/entry/cloudflare-ip-blockade.html">you will have no recourse</a>. Imagine that happening to someone less tech-savvy.</p>
<p>This is the classic problem with using an outdated security tool like a WAF: defenders have to configure the tool absolutely perfectly to be safe and avoid false positives, but attackers just need to find a single weakness. Those are horrible odds. You should use alternatives that don't require perfection from imperfect humans.</p>
<h2 id="alternatives-to-wafs">Alternatives to WAFs</h2>
<p>Since WAFs are resource-hungry, inneffective, unsafe, and noisy, how do I convince an auditor to not make me use one? The technical term would be to use "compensating controls", but that sounds like such a weak term to describe the powerful and simple alternatives to WAFs I'm about to describe:</p>
<ul>
<li><strong>Isolation:</strong> Isolation involves ensuring that a breach in one component can not affect the rest of the system, and there are many technologies that provide isolation.
<ul>
<li>Browsers do this by executing all code inside special sandboxed processes that don't have carte blanch access to cookies, saved passwords, other tabs, etc. Imagine how slow the web would be if every piece of JavaScript needed to be analyzed by hundreds of regexes before being executed!</li>
<li>Microservices are designed with isolation in mind, but you can also do it in a monolith with a variety of <a href="https://github.com/dckc/awesome-ocap#libraries-and-frameworks">libraries and languages</a>.</li>
</ul>
</li>
<li><strong>Immutability:</strong> Entire classes of attack can be eliminated by removing a few assumptions, like having a <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/">readOnlyRootFilesystem</a>, a <a href="https://thenewstack.io/3-immutable-operating-systems-bottlerocket-flatcar-and-talos-linux/">package manager that requires rebooting</a>, or append-only/<a href="https://www.rsync.net/resources/faq.html#9a">immutable backups</a>.</li>
<li><strong>Static Analysis:</strong> SQL injection has a miracle cure called "prepared statements". The problem is that devs forget to use them. Static analysis checks in a CI pipeline can all but ensure that zero SQL injection vulnerabilities are in your codebase, at which point there is no need for any SQL injection WAF rules. No, "defense in depth" is not a valid excuse to use a WAF anyway, because it provides no real defense! Like surrounding Fort Knox with an army of guard guinea pigs.</li>
<li><strong>Capability-based security:</strong> Not every API endpoint needs to have unrestricted read/write access to your entire database and file system, but that is the normal way people build APIs today. By using capabilities, you can express exactly that "GET /api/v1/books" only needs read access to the "books" table. Or that "POST /api/v1/imageupload" needs write access to a specific folder, but doesn't need the ability to spawn processes.</li>
</ul>
<p>Now I'll admit these ideas are quite broad; you'll need to adapt them to your particular app. WAF vendors offer a one-WAF-fits-all fantasy that I can't match. But these secure-by-design strategies are the way that the security industry needs to be heading. Unfortunately, it's a lot harder for the security industry to profit off of design-based techniques, so don't hold your breath.</p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Reauthorizing Mass Surveillance Shouldn't Be Tied to Funding the Government (235 pts)]]></title>
            <link>https://www.eff.org/deeplinks/2023/11/reauthorizing-mass-surveillance-shouldnt-be-tied-funding-government</link>
            <guid>38254656</guid>
            <pubDate>Mon, 13 Nov 2023 20:04:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.eff.org/deeplinks/2023/11/reauthorizing-mass-surveillance-shouldnt-be-tied-funding-government">https://www.eff.org/deeplinks/2023/11/reauthorizing-mass-surveillance-shouldnt-be-tied-funding-government</a>, See on <a href="https://news.ycombinator.com/item?id=38254656">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <article role="article">
  
  
  <div><p><span>Section 702 is the controversial and much-abused mass surveillance authority that expires in December unless Congress renews it. EFF and others have been working hard to get real reforms into the law and have opposed a renewal, and now, we’re hearing about a rushed attempt to tie renewal to funding the government. We need to stop it.</span></p>
<p><span>In September, President Biden signed a short-term continuing resolution to fund the government preventing a full shutdown. This week Congress must pass another bill to make sure it doesn’t happen again. But this time, we understand that Congress wants to </span><a href="https://rollcall.com/2023/11/11/senate-stopgap-plan-might-extend-to-january-jettison-war-funds/"><span>vote on a "clean" renewal of Section 702</span></a>—essentially, kicking the can down the road, as they've done before<span>. <br></span></p>
<p><span>The program was intended to collect communications of people outside of the United States, but because we live in an increasingly globalized world, the government retains a massive trove of communications between Americans and people overseas. Increasingly, it’s this U.S. side of digital conversations that domestic law enforcement agencies trawl through—all without a warrant.</span></p>
<p><span>This is not how the government should work. Lawmakers should not take an unpopular, contested, and dangerous piece of legislation and slip it into a massive bill that, if opposed, would shut down the entire government. No one should have to choose between funding the government and renewing a dangerous mass surveillance program that even&nbsp;</span><a href="https://www.eff.org/deeplinks/2023/09/federal-governments-privacy-watchdog-concedes-702-must-change"><span>the federal government admits is in need of reform</span></a><span>.&nbsp; <br></span></p>
<p><span>EFF has signed onto a </span><a href="https://www.brennancenter.org/our-work/research-reports/coalition-statement-urges-senator-schumer-keep-reauthorization-section"><span>letter</span></a><span> with a dozen organizations opposing even a short-term reauthorization of a program as dangerous as 702 in a piece of vital legislation. The letter says: <br></span></p>
<blockquote><p><span>“In its current form, this authority is dangerous to our liberties and our democracy, and it should not be renewed for any length of time without robust debate, an opportunity for amendment, and — ultimately — far-reaching reforms. </span><b>Allowing a short-term reauthorization to be slipped into a must-pass bill would demonstrate a blatant disregard for the civil liberties and civil rights of the American people.</b><span>”</span></p>
</blockquote>
<p><span>For months, EFF and a large coalition of </span><a href="https://www.wired.com/story/government-surveillance-reform-act-2023/"><span>civil rights, civil liberties, and racial justice groups</span></a><span> have been fighting the renewal of Section 702. Just last week, a group of privacy-minded Senators and Representatives introduced the </span><a href="https://www.eff.org/deeplinks/2023/11/government-surveillance-reform-act-would-rein-some-worst-abuses-section-702"><span>Government Surveillance Reform Act</span></a><span>, which would introduce some much-needed safeguards and oversight onto a historically out-of-control surveillance program. Section 702 is far too powerful, invasive, and dangerous to renew it cleanly as a matter of bureaucratic necessity and we say that it has to be renewed with massive reforms or not at all. Sneaking something this important into a massive must-pass bill is dishonest and a slap in the face to all people who care about privacy and the integrity of our digital communications.&nbsp;</span></p>

</div>

          </article>
    </div><div>
          <h2>Join EFF Lists</h2>
        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Panama Canal is so congested that one ship owner paid $4M to skip the line (233 pts)]]></title>
            <link>https://fortune.com/2023/11/08/panama-canal-congestion-record-4-million-skip-line/</link>
            <guid>38254353</guid>
            <pubDate>Mon, 13 Nov 2023 19:43:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fortune.com/2023/11/08/panama-canal-congestion-record-4-million-skip-line/">https://fortune.com/2023/11/08/panama-canal-congestion-record-4-million-skip-line/</a>, See on <a href="https://news.ycombinator.com/item?id=38254353">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-cy="articleContent" id="article-content"><p>A shipper has paid nearly $4 million to jump to the front of the line at the congested Panama Canal waterway, a record high.&nbsp;</p><div>



<p>Japan’s Eneos Group paid $3.975 million in an auction Wednesday to secure the crossing, bidding documents show. That comes on top of the regular transit fees companies pay, which can be hundreds of thousands of dollars more.</p>



<p>“You are getting close to $4.5 million to use the canal, so that is pricing out a lot of ships,” Oystein Kalleklev, chief executive officer of Flex LNG Ltd. and Avance Gas Holding Ltd., said during a conference call Wednesday when asked about the state of the canal.</p>



<p>Eneos’ shipping division transports various commodities, including crude oil, liquefied petroleum gas, chemicals and bulk cargo. Eneos and the Panama Canal Authority didn’t respond to a request for comment.</p>



<p>A queue of ships waiting to use the canal has been growing in recent months amid a deep drought. To manage the situation, the canal’s managing authority has announced&nbsp;<a href="https://www.bloomberg.com/news/articles/2023-05-19/panama-canal-imposes-new-shipping-restrictions-for-drought" target="_blank" rel="noreferrer noopener">increasingly</a>&nbsp;drastic restrictions for the depleted thoroughfare. The Panama Canal Authority also holds auctions for those wishing to jump to the front of the line.</p></div><p>Subscribe to the CFO Daily newsletter to keep up with the trends, issues, and executives shaping corporate finance. <a href="https://www.fortune.com/newsletters/cfodaily?&amp;itm_source=fortune&amp;itm_medium=article_tout&amp;itm_campaign=cfo_daily" target="_self" rel="">Sign up</a> for free.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Algorithms (2019) (120 pts)]]></title>
            <link>http://jeffe.cs.illinois.edu/teaching/algorithms/</link>
            <guid>38254153</guid>
            <pubDate>Mon, 13 Nov 2023 19:28:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/">http://jeffe.cs.illinois.edu/teaching/algorithms/</a>, See on <a href="https://news.ycombinator.com/item?id=38254153">Hacker News</a></p>
<div id="readability-page-1" class="page">

<p><a href="http://jeffe.cs.illinois.edu/teaching/algorithms/book/Algorithms-JeffE.pdf"><img src="http://jeffe.cs.illinois.edu/teaching/algorithms/FrontCover.png" width="250"></a>
</p>



<h2>by <a href="http://jeffe.cs.illinois.edu/">Jeff Erickson</a></h2>

<center>🔥<b>1st edition, June 2019</b> 🔥<br>
(Amazon links: <a href="https://www.amazon.com/dp/1792644833">US</a>,
	<a href="https://www.amazon.co.uk/dp/1792644833">UK</a>,
	<a href="https://www.amazon.de/dp/1792644833">DE</a>,
	<a href="https://www.amazon.es/dp/1792644833">ES</a>,
	<a href="https://www.amazon.fr/dp/1792644833">FR</a>,
	<a href="https://www.amazon.it/dp/1792644833">IT</a>,
	<a href="https://www.amazon.co.jp/dp/1792644833">JP</a>)</center>

<p>
This web page contains a free electronic version of my self-published textbook <cite>Algorithms</cite>, along with other lecture notes I have written for various theoretical computer science classes at the University of Illinois, Urbana-Champaign since 1998.
<!-- I have taught or co-taught twenty-one courses from this material:
	Spring 1999, Fall 2000, Spring 2001, Fall 2002, Spring 2004,
	Fall 2005, Fall 2006, Spring 2007, Fall 2008, Spring 2009,
	Spring 2010, Fall 2010, Fall 2012, Fall 2013, Spring 2014,
	Fall 2014, Spring 2015, Spring 2016, Fall 2016, Spring 2017,
	and Spring 2018. -->


</p><ul>
<li> <a href="#blah">More information</a>
</li><li> <a href="#book">Get the book</a>
</li><li> <a href="#notes">More algorithms lecture notes</a>
</li><li> <a href="#models">Models of computation notes</a>
</li><li> <a href="https://github.com/jeffgerickson/algorithms/issues">Report an error</a> (separate page)
</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/hwex.html">Coursework archive</a> (separate page)
</li></ul>

<hr>
<h3><a name="blah"> More Information </a></h3>

<b>Publication.</b>
A black-and-white paperback edition of the textbook can be purchased from <a href="https://www.amazon.com/dp/1792644833">Amazon</a> for $27.50.  The full-color electronic version will remain freely available here indefinitely.  (If there is enough demand, I may publish a full-color printed version of the <em>next</em> edition.  Color printing is considerably more expensive; a full-color printed version of the current book would cost about $75.)

<p>
<b>Bug reports.</b>
After years of trying and failing to manage bug reports by email, I now maintain an issue-tracking page at <a href="https://github.com/jeffgerickson/algorithms">GitHub</a>.  If you find an error in the textbook, in the lecture notes, or in any other materials, <a href="https://github.com/jeffgerickson/algorithms/issues">please submit a bug report</a>.  All other feedback is welcome as well.

</p><p>
<b>Permissions.</b>
Anyone is welcome to download, print, use, copy, and/or distribute anything on this page, either electronically or on paper.  You do not need to ask my permission, although I would appreciate hearing from you if you find this material useful.  If you redistribute any of this material, please include a link back to <a href="http://jeffe.cs.illinois.edu/teaching/algorithms">this web page</a>, either directly or through the mnemomic shortcut <a href="http://algorithms.wtf/">http://algorithms.wtf</a>.  Specifically:

</p><ul>
<li>
The textbook <cite>Algorithms</cite> (in both paper and electronic forms) is licensed under a <a href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International license</a>.

</li><li>
All other lecture notes are licensed under a more restrictive <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Attribution-NonCommercial-ShareAlike 4.0 International</a> license.
</li></ul>

<p>
<b>Please do not ask me for solutions to the exercises.</b>  See <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/hwex.html#solutions">the course materials page</a> for an explanation.

</p><p>
<b>Context.</b>
This material is the primary reference for two regularly-offered theoretical computer science courses at Illinois: <a href="https://courses.engr.illinois.edu/cs374/">CS&nbsp;374</a>
and
<a href="https://courses.engr.illinois.edu/cs473/">CS&nbsp;473</a>.  I taught these courses most recently in <a href="https://courses.engr.illinois.edu/cs374/sp2018/A/schedule.html">Spring 2018</a>
and <a href="https://courses.engr.illinois.edu/cs473/sp2017/lectures.html">Spring 2017</a>, respectively.  
I maintain a complete archive of <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/hwex.html">my past homeworks, exams, and lab handouts</a> on a separate page.

</p><p>
<b>Prerequisites.</b>  The textbook assumes knowledge of discrete math (especially induction) and basic data structures and algorithms (especially recursion) consistent with the prerequisite courses <a href="https://courses.engr.illinois.edu/cs173/">CS 173</a> and <a href="https://courses.engr.illinois.edu/cs225/sp2019/">CS 225</a> at Illinois.  (See the <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/book/!!-frontmatter.pdf" <="" a=""> for more details.)  For a thorough overview of prerequisite material, I strongly recommend the following resources:
</a></p><ul><a href="http://jeffe.cs.illinois.edu/teaching/algorithms/book/!!-frontmatter.pdf" <="" a="">
</a><li><a href="http://jeffe.cs.illinois.edu/teaching/algorithms/book/!!-frontmatter.pdf" <="" a="">
</a><a href="http://mfleck.cs.illinois.edu/building-blocks/">Building Blocks for Theoretical Computer Science</a> by Margaret Fleck
</li><li>
<a href="https://courses.csail.mit.edu/6.042/spring18/">Mathematics for Computer Science</a> by Eric Lehman, Tom Leighton, and Albert Meyer.  (I strongly recommend searching for the most recent revision.)
</li><li> 
<a href="http://opendatastructures.org/">Open Data Structures</a> by Pat Morin	
</li><li>
<a href="https://donsheehy.github.io/datastructures/">datastructures</a> by Don Sheehy
</li></ul>


<hr>
<h3><a name="book"> Get the Book </a></h3>

<ul> 
<li> <b>Entire book</b> (1st edition, June 2019, 472 pages)
	<ul>
	<li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/book/Algorithms-JeffE.pdf">one page per page (for screens)</a>	
	</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/book/Algorithms-JeffE-2up.pdf">two pages per page (for printing)</a>
	</li><li> <a href="https://github.com/jeffgerickson/algorithms">GitHub</a> (bug tracking)
	</li><li> <a href="https://archive.org/details/Algorithms-Jeff-Erickson">Internet Archive</a> (permanent archival copy, currently the 0th edition)
	</li></ul>
</li><li> <b>Individual chapters:</b>  These were extracted from the full book PDF file, to keep page numbers consistent; however, hyperlinks in these files do not work.
<ul>
	<li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/book/!!-frontmatter.pdf">Front matter: Cover, copyright, table of contents,  preface</a> (18 pages)
</li></ul>
<ol start="0">
	<li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/book/00-intro.pdf">Introduction</a> (20 pages)
	</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/book/01-recursion.pdf">Recursion</a> (50 pages)
	</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/book/02-backtracking.pdf">Backtracking</a> (26 pages)
	</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/book/03-dynprog.pdf">Dynamic Programming</a> (62 pages)
	</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/book/04-greedy.pdf">Greedy Algorithms</a> (28 pages)
	</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/book/05-graphs.pdf">Basic Graph Algorithms</a> (38 pages)
	</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/book/06-dfs.pdf">Depth-First Search</a> (32 pages)
	</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/book/07-mst.pdf">Minimum Spanning Trees</a> (16 pages)
	</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/book/08-sssp.pdf">Shortest Paths</a> (36 pages)
	</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/book/09-apsp.pdf">All-Pairs Shortest Paths</a> (18 pages)
	</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/book/10-maxflow.pdf">Maximum Flows &amp; Minimum Cuts</a> (26 pages)
	</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/book/11-maxflowapps.pdf">Applications of Flows and Cuts</a> (26 pages)
	</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/book/12-nphard.pdf">NP-Hardness</a> (50 pages)
</li></ol>
<ul>
	<li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/book/99-backmatter.pdf">Back matter: Indices, image credits, colophon</a> (26 pages)
</li></ul>
</li></ul>

<hr>
<h3><a name="notes"> More Algorithms Lecture Notes </a></h3>

Both the topical coverage (except for flows) and the level of difficulty of the textbook material  (mostly) reflect the algorithmic content of CS 374.  The remainder of these notes cover either more advanced aspects of topics from the book, or other topics that appear only in our more advanced algorithms class CS 473.  Don't be fooled by the fancy typesetting; these notes are <em>considerably</em> less polished than the textbook.

<ul> 
<li>
<b>Extended Dance Remix:</b> These are notes on more advanced material directly related to the textbook.  The notes are ordered roughly to match the textbook chapters.
	<ol type="A" start="A">
	<li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/notes/A-fft.pdf">Fast Fourier Transforms</a> (17 pages)
	</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/notes/B-fastexpo.pdf">Fast Exponential Algorithms</a> (14 pages)
	</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/notes/C-automata-dynprog.pdf">Dynamic Programming for Formal Languages and Automata</a> (7 pages, unfinished)
	</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/notes/D-faster-dynprog.pdf">Advanced Dynamic Programming</a> (18 pages)
	</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/notes/E-matroids.pdf">Matroids</a> (8 pages)
	</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/notes/F-pseudoflows.pdf">Balances and Pseudoflows</a> (13 pages)
	</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/notes/G-mincostflow.pdf">Minimum-Cost Flows</a> (16 pages)
	</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/notes/H-lp.pdf">Linear Programming</a> (21 pages)
	</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/notes/I-simplex.pdf">Linear Programming Algorithms</a> (18 pages)
	</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/notes/J-approx.pdf">Approximation Algorithms</a> (25 pages)
</li></ol>

</li><li>
<b>Director's Cut:</b>  These are notes on topics not covered in the textbook.  The numbering is completely independent os the textbook; I just started over at 1.  We regularly cover some of the randomized algorithms material in CS 473, but I haven't used the amortized analysis or lower bounds notes in many years.

	<ol>
	<li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/notes/01-random.pdf">Discrete Probability</a> (22 pages)
	</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/notes/02-nutsbolts.pdf">Nuts and Bolts</a> (13 pages)
	</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/notes/03-treaps.pdf">Treaps and Skip Lists</a> (14 pages)
	</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/notes/04-chernoff.pdf">Tail Inequalities</a> (10 pages)
	</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/notes/05-hashing.pdf">Hashing</a> (19 pages)
	</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/notes/06-bloom.pdf">Filtering and Streaming</a> (6 pages)
	</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/notes/07-strings.pdf">String Matching</a> (14 pages)
	</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/notes/08-mincut.pdf">Randomized Minimum Cut</a> (7 pages)
	</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/notes/09-amortize.pdf">Amortized Analysis</a> (14 pages)
	</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/notes/10-scapegoat-splay.pdf">Scapegoat and Splay Trees</a> (15 pages)
	</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/notes/11-unionfind.pdf">Disjoint Sets</a> (14 pages)
	</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/notes/12-lowerbounds.pdf">Lower Bounds</a> (6 pages)
	</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/notes/13-adversary.pdf">Adversary Arguments</a> (8 pages)
	</li></ol>
	<ul>
	<li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/notes/98-induction.pdf">Appendix I. Proof by Induction</a> (30 pages)
	</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/notes/99-recurrences.pdf">Appendix II. Solving Recurrences</a> (22 pages)
	</li></ul>

</li></ul>

<hr>
<h3><a name="models"> Models of Computation </a></h3>

These notes cover (a superset of) the automata and formal languages material in CS 374.  Some of these notes are a lot more polished than others.

<ul>
<li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/models/all-models.pdf"><b>Everything</b></a> (155 pages)
</li><li> Individual notes:
<ol start="0">
<li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/models/0-cover.pdf">Cover and preface</a> (3 pages)
</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/models/01-strings.pdf">Strings</a> (17 pages)
</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/models/02-regular.pdf">Regular languages</a> (12 pages)
</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/models/03-automata.pdf">Finite-state automata</a> (24 pages)
</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/models/04-nfa.pdf">Nondeterministic automata</a> (21 pages)
</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/models/05-context-free.pdf">Context-free languages</a> (20 pages)
</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/models/06-turing-machines.pdf">Turing machings</a> (20 pages)
</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/models/07-undecidable.pdf">Undecidability</a> (20 pages)
</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/models/08-universal.pdf">Universal models</a> (8 pages, unfinished)
</li><li> <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/models/09-nondeterminism.pdf">Nondeterministic Turing machines</a> (6 pages, unfinished)
</li></ol>
</li></ul>

<hr>
	
<blockquote><blockquote><small>
	<i>If were not a little mad and generally silly
	<br>I should give you my advice upon the subject, willy-nilly;
	<br>I should show you in a moment how to grapple with the question,
	<br>And you'd really be astonished at the force of my suggestion.
	<br>On the subject I shall write you a most valuable letter,
	<br>Full of excellent suggestions when I feel a little better,
	<br>But at present I'm afraid I am as mad as any hatter,
	<br>So I'll keep 'em to myself, for my opinion doesn't matter!</i>
	
</small></blockquote></blockquote>
	
<blockquote><blockquote><small>
	<i>It is time we did away with “publish or perish” and replace it with “publish <em>and</em> perish.”<br>
	Nothing will be more blasphemous than writing a textbook that anyone can go out and buy.</i>
	
</small></blockquote></blockquote>

	
<hr>

<p><address><small><a href="http://jeffe.cs.illinois.edu/">Jeff Erickson</a> — 15 Jun 2019</small></address></p>




</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ubuntu Pro Shenanigans (112 pts)]]></title>
            <link>https://inteltechniques.com/blog/2023/11/12/ubuntu-pro-shenanigans/</link>
            <guid>38254040</guid>
            <pubDate>Mon, 13 Nov 2023 19:19:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://inteltechniques.com/blog/2023/11/12/ubuntu-pro-shenanigans/">https://inteltechniques.com/blog/2023/11/12/ubuntu-pro-shenanigans/</a>, See on <a href="https://news.ycombinator.com/item?id=38254040">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page-content" role="main">
<article id="post-3895" role="article">
<header>

<ul>
<li>
<a href="https://inteltechniques.com/blog/2023/11/12/ubuntu-pro-shenanigans/">
<i></i>
<span>Posted on</span>
November 12, 2023 </a>
</li>
<li>
<i></i>
<span>Posted in</span>
<a href="https://inteltechniques.com/blog/category/osint/" rel="category tag">OSINT</a> </li>
</ul>
</header>
<section>
<p>Posted by Aaron Kelley</p>
<p>Several readers of our <a href="https://inteltechniques.com/book1.html">OSINT Techniques book</a> and <a href="https://www.inteltechniques.net/">Online Video Training</a> have expressed concern about Ubuntu's new Pro feature and update restrictions for those who do not subscribe to the service. Since we recommend Ubuntu for OSINT virtual machines, we should address the issue and offer some guidance. Let's start with addressing Ubuntu Pro. If you run 'sudo apt update' and 'sudo apt upgrade' within an Ubuntu Terminal, you will likely see something similar to the following.</p>
<p><img fetchpriority="high" decoding="async" src="https://inteltechniques.com/blog/wp-content/uploads/Image-5-620x261.png" alt="" width="620" height="261" srcset="https://inteltechniques.com/blog/wp-content/uploads/Image-5-620x261.png 620w, https://inteltechniques.com/blog/wp-content/uploads/Image-5-300x126.png 300w, https://inteltechniques.com/blog/wp-content/uploads/Image-5-768x323.png 768w, https://inteltechniques.com/blog/wp-content/uploads/Image-5.png 1288w" sizes="(max-width: 620px) 100vw, 620px"></p>
<p>This warning appears concerning as it insinuates that some updates are being withheld from your machine unless you subscribe to the Pro service. The following warning from Ubuntu's software updater is even more alarming.</p>
<p><img decoding="async" src="https://inteltechniques.com/blog/wp-content/uploads/Image-2-1-620x344.png" alt="" width="620" height="344" srcset="https://inteltechniques.com/blog/wp-content/uploads/Image-2-1-620x344.png 620w, https://inteltechniques.com/blog/wp-content/uploads/Image-2-1-300x166.png 300w, https://inteltechniques.com/blog/wp-content/uploads/Image-2-1-768x426.png 768w, https://inteltechniques.com/blog/wp-content/uploads/Image-2-1-1536x851.png 1536w, https://inteltechniques.com/blog/wp-content/uploads/Image-2-1-2048x1135.png 2048w" sizes="(max-width: 620px) 100vw, 620px"></p>
<p>This appears to present a lot of outdated software which we cannot update. However, looks can be deceiving. Click on any of these updates and look at the details pane. As one example, I clicked on Ffmpeg and observed the following.</p>
<p><img decoding="async" src="https://inteltechniques.com/blog/wp-content/uploads/Image-3-1-620x217.png" alt="" width="620" height="217" srcset="https://inteltechniques.com/blog/wp-content/uploads/Image-3-1-620x217.png 620w, https://inteltechniques.com/blog/wp-content/uploads/Image-3-1-300x105.png 300w, https://inteltechniques.com/blog/wp-content/uploads/Image-3-1-768x269.png 768w, https://inteltechniques.com/blog/wp-content/uploads/Image-3-1.png 1154w" sizes="(max-width: 620px) 100vw, 620px"></p>
<p>The "Available version" is the exact same product as the currently installed software. The update does nothing. Running 'pro security-status' displays the following.</p>
<p><img loading="lazy" decoding="async" src="https://inteltechniques.com/blog/wp-content/uploads/Image-4-1-620x191.png" alt="" width="620" height="191" srcset="https://inteltechniques.com/blog/wp-content/uploads/Image-4-1-620x191.png 620w, https://inteltechniques.com/blog/wp-content/uploads/Image-4-1-300x93.png 300w, https://inteltechniques.com/blog/wp-content/uploads/Image-4-1-768x237.png 768w, https://inteltechniques.com/blog/wp-content/uploads/Image-4-1.png 1446w" sizes="(max-width: 620px) 100vw, 620px"></p>
<p>This confirms that our machine is receiving all Main/Restricted updates until 2027, at which time we would be using Ubuntu 26.04. We do not need extended updates until 2032 as offered through Ubuntu Pro. This makes Ubuntu Pro unnecessary for our needs. Opting to avoid Ubuntu Pro does not restrict your machine from the typical security updates which Ubuntu has always provided. The options available within Ubuntu Pro are enhancements to Ubuntu and no features have been removed from a typical Ubuntu installation. The focus of Ubuntu Pro is to extend the availability of updates from five to ten years, and provide some third-party security patches which may not be available otherwise. We should not be using old versions of Ubuntu for our VMs, so this does little for us within an investigative VM.</p>
<p>Ubuntu Pro is available for free for personal use, but it requires you to attach a unique license key to your Ubuntu installation, which will be tracked by Canonical. We do not recommend this. Instead, we encourage users to remove these unnecessary warnings with the following command.</p>
<p>mv /etc/apt/apt.conf.d/20apt-esm-hook.conf /etc/apt/apt.conf.d/20apt-esm-hook.conf.bak</p>
<p>After this command, which only renames the file responsible for this warning, updating through Terminal (and therefore the update script we provide within the VM), should appear as follows.</p>
<p><img loading="lazy" decoding="async" src="https://inteltechniques.com/blog/wp-content/uploads/Image-5-1-620x120.png" alt="" width="620" height="120" srcset="https://inteltechniques.com/blog/wp-content/uploads/Image-5-1-620x120.png 620w, https://inteltechniques.com/blog/wp-content/uploads/Image-5-1-300x58.png 300w, https://inteltechniques.com/blog/wp-content/uploads/Image-5-1-768x148.png 768w, https://inteltechniques.com/blog/wp-content/uploads/Image-5-1.png 1130w" sizes="(max-width: 620px) 100vw, 620px"></p>
<p>We feel that Ubuntu is being aggressively misleading with the rollout of Ubuntu Pro, and we do not recommend any OSINT users attach this service to their investigative VMs. We have not recommended Ubuntu as a host OS for some time.</p>
</section>
</article>
<nav>

</nav>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google pays Apple 36% of the revenue it earns from searches in Safari (204 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2023-11-13/apple-gets-36-of-google-revenue-from-search-deal-witness-says</link>
            <guid>38253384</guid>
            <pubDate>Mon, 13 Nov 2023 18:26:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2023-11-13/apple-gets-36-of-google-revenue-from-search-deal-witness-says">https://www.bloomberg.com/news/articles/2023-11-13/apple-gets-36-of-google-revenue-from-search-deal-witness-says</a>, See on <a href="https://news.ycombinator.com/item?id=38253384">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <section>
        <h3>Why did this happen?</h3>
        <p>Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our <a href="https://www.bloomberg.com/notices/tos">Terms of
                Service</a> and <a href="https://www.bloomberg.com/notices/tos">Cookie Policy</a>.</p>
    </section>
    <section>
        <h3>Need Help?</h3>
        <p>For inquiries related to this message please <a href="https://www.bloomberg.com/feedback">contact
            our support team</a> and provide the reference ID below.</p>
        <p>Block reference ID:</p>
    </section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Forests with multiple tree species are more effective as carbon sinks (123 pts)]]></title>
            <link>https://phys.org/news/2023-11-forests-multiple-tree-species-effective.html</link>
            <guid>38253130</guid>
            <pubDate>Mon, 13 Nov 2023 18:06:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://phys.org/news/2023-11-forests-multiple-tree-species-effective.html">https://phys.org/news/2023-11-forests-multiple-tree-species-effective.html</a>, See on <a href="https://news.ycombinator.com/item?id=38253130">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
										
<div data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2023/forests.jpg" data-src="https://scx2.b-cdn.net/gfx/news/hires/2023/forests.jpg" data-sub-html="Credit: Unsplash/CC0 Public Domain">
        <figure>
            <img src="https://scx1.b-cdn.net/csz/news/800a/2023/forests.jpg" alt="forests" title="Credit: Unsplash/CC0 Public Domain" width="800" height="530">
             <figcaption>
                Credit: Unsplash/CC0 Public Domain
            </figcaption>        </figure>
    </div>
<p>To slow the effects of climate change, conserve biodiversity, and meet the sustainable development goals, replanting trees is vital. Restored forests store carbon within the forest's soil, shrubs, and trees. Mixed forests are especially effective at carbon storage, as different species with complementary traits can increase overall carbon storage.

										  
											        </p>
										 
										 											  
<p>Compared to single-species forests, mixed forests are also more resilient to pests, diseases, and climatic disturbances, which increases their long-term <a href="https://phys.org/tags/carbon+storage/" rel="tag">carbon storage</a> potential. The delivery of other ecosystem services is also greater in mixed species forests, and they support higher levels of biodiversity.
</p><p>Although the benefits of diverse forest systems are well known, many countries' restoration commitments are focused on establishing monoculture plantations. Given this practice, an international team of scientists has compared <a href="https://phys.org/tags/carbon+stocks/" rel="tag">carbon stocks</a> in mixed planted forests to <a href="https://phys.org/tags/carbon/" rel="tag">carbon</a> stocks in commercial and best-performing monocultures, as well as the average of monocultures.
</p><p>Their work is published in <i>Frontiers in Forests and Global Change</i>.
</p><p>"Diverse planted forests store more carbon than monocultures—upwards of 70%," said Dr. Emily Warner, a postdoctoral researcher in ecology and biodiversity science at the Department of Biology, University of Oxford, and first author of the study. "We also found the greatest increase in carbon storage relative to monocultures in four-species mixtures."
</p><h2>Species richness increases carbon storage potential</h2>
<p>The researchers analyzed studies published since 1975 that directly compared carbon storage in mixed and single-species forests, and combined this with previously unpublished data from a global network of tree diversity experiments. "We wanted to pull together and assess the existing evidence to determine whether forest diversification provides carbon storage benefits," Warner explained.
</p><p>The mixed planted forests assessed in the study ranged in <a href="https://phys.org/tags/species+richness/" rel="tag">species richness</a> from two to six species. In the data set the scientists worked with, four-species mixtures were the most effective carbon sinks. One such mix was made up from different broadleaf trees, which can be found across Europe. Mixes with two species also had greater above-ground carbon stocks than monocultures and stored up to 35% more carbon. Forests made up of six species, however, showed no clear advantage to monocultures.
</p><p>Accordingly, the researchers were able to show that diversification of forests enhances carbon storage. Altogether, above-ground carbon stocks in mixed forests were 70% higher than in the average monoculture. The researchers also found that mixed forests had 77% higher carbon stocks than commercial monocultures, made up of species bred to be particularly high yielding.
</p><h2>Forests for the future</h2>
<p>"As momentum for <a href="https://phys.org/tags/tree+planting/" rel="tag">tree planting</a> grows, our study highlights that mixed species plantations would increase carbon storage alongside other benefits of diversifying planted forests," said Dr. Susan Cook-Patton, a senior forest restoration scientist at The Nature Conservancy and collaborator on the study. The results are particularly relevant to forest managers, showing that there is a productivity incentive for diversifying new planted forests, the researchers pointed out.
</p><p>While showing the increased potential of mixed forests to store more carbon, the researchers cautioned that their study is not without limitations, including the overall limited availability of studies addressing mixed vs. monoculture forests, particularly studies from older forests and with higher levels of tree diversity.
</p><p>"This study demonstrates the potential of diversification of planted forests, and also the need for long-term <a href="https://phys.org/tags/experimental+data/" rel="tag">experimental data</a> to explore the mechanisms behind our results," Warner said. "There is an urgent need to explore further how the carbon <a href="https://phys.org/tags/storage/" rel="tag">storage</a> benefits of diversification change depending on factors such as location, <a href="https://phys.org/tags/species/" rel="tag">species</a> used and <a href="https://phys.org/tags/forest/" rel="tag">forest</a> age."
										 																				
																				</p><p><strong>More information:</strong>
												Young mixed planted forests store more carbon than monocultures—a meta-analysis, <i>Frontiers in Forests and Global Change</i> (2023). <a data-doi="1" href="https://dx.doi.org/10.3389/ffgc.2023.1226514" target="_blank">DOI: 10.3389/ffgc.2023.1226514</a>
																						
																					</p>
                               											
																					
                              										                                        
										<!-- print only -->
										<div>
											 <p><strong>Citation</strong>:
												Forests with multiple tree species are 70% more effective as carbon sinks than monoculture forests, study finds (2023, November 9)
												retrieved 13 November 2023
												from https://phys.org/news/2023-11-forests-multiple-tree-species-effective.html
											 </p>
											 <p>
											 This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no
											 part may be reproduced without the written permission. The content is provided for information purposes only.
											 </p>
										</div>
                                        
									</div></div>]]></description>
        </item>
    </channel>
</rss>