<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 16 Feb 2024 19:00:08 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Air Canada must honor refund policy invented by airline's chatbot (103 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2024/02/air-canada-must-honor-refund-policy-invented-by-airlines-chatbot/</link>
            <guid>39400374</guid>
            <pubDate>Fri, 16 Feb 2024 17:45:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2024/02/air-canada-must-honor-refund-policy-invented-by-airlines-chatbot/">https://arstechnica.com/tech-policy/2024/02/air-canada-must-honor-refund-policy-invented-by-airlines-chatbot/</a>, See on <a href="https://news.ycombinator.com/item?id=39400374">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h4>
      Blame game    —
</h4>
            
            <h2 itemprop="description">Air Canada appears to have quietly killed its costly chatbot support.</h2>
                    </div><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2024/02/GettyImages-1453660913-800x533.jpg" alt="Air Canada must honor refund policy invented by airline’s chatbot">
      <figcaption></figcaption>  </figure>

  




<!-- cache hit 23:single/related:3be96e65dcfb5c467dc970978b485c40 --><!-- empty -->
<p>After months of resisting, Air Canada was <a href="https://www.canlii.org/en/bc/bccrt/doc/2024/2024bccrt149/2024bccrt149.html">forced</a> to give a partial refund to a grieving passenger who was misled by an airline chatbot inaccurately explaining the airline's bereavement travel policy.</p>
<p>On the day Jake Moffatt's grandmother died, Moffat immediately visited Air Canada's website to book a flight from Vancouver to Toronto. Unsure of how Air Canada's bereavement rates worked, Moffatt asked Air Canada's chatbot to explain.</p>
<p>The chatbot provided inaccurate information, encouraging Moffatt to book a flight immediately and then request a refund within 90 days. In reality, Air Canada's policy explicitly stated that the airline will not provide refunds for bereavement travel after the flight is booked. Moffatt dutifully attempted to follow the chatbot's advice and request a refund but was shocked that the request was rejected.</p>
<p>Moffatt tried for months to convince Air Canada that a refund was owed, sharing a screenshot from the chatbot that clearly claimed:</p>
<blockquote><p>If you need to travel immediately or have already travelled and would like to submit your ticket for a reduced bereavement rate, kindly do so within 90 days of the date your ticket was issued by completing our Ticket Refund Application form.</p></blockquote>
<p>Air Canada argued that because the chatbot response elsewhere linked to a page with the actual bereavement travel policy, Moffatt should have known bereavement rates could not be requested retroactively. Instead of a refund, the best Air Canada would do was to promise to update the chatbot and offer Moffatt a $200 coupon to use on a future flight.</p>                                            
                                                        
<p>Unhappy with this resolution, Moffatt refused the coupon and filed a small claims complaint in Canada's Civil Resolution Tribunal.</p>
<p>According to Air Canada, Moffatt never should have trusted the chatbot and the airline should not be liable for the chatbot's misleading information because Air Canada essentially argued that "the chatbot is a separate legal entity that is responsible for its own actions," a <a href="https://www.canlii.org/en/bc/bccrt/doc/2024/2024bccrt149/2024bccrt149.html">court order</a> said.</p>
<p>Experts <a href="https://vancouversun.com/news/local-news/air-canada-told-it-is-responsible-for-errors-by-its-website-chatbot">told the Vancouver Sun</a> that Moffatt's case appeared to be the first time a Canadian company tried to argue that it wasn't liable for information provided by its chatbot.</p>
<p>Tribunal member Christopher Rivers, who decided the case in favor of Moffatt, called Air Canada's defense "remarkable."</p>
<p>"Air Canada argues it cannot be held liable for information provided by one of its agents, servants, or representatives—including a chatbot," Rivers wrote. "It does not explain why it believes that is the case" or "why the webpage titled 'Bereavement travel' was inherently more trustworthy than its chatbot."</p>
<p>Further, Rivers found that Moffatt had "no reason" to believe that one part of Air Canada's website would be accurate and another would not.</p>
<p>Air Canada "does not explain why customers should have to double-check information found in one part of its website on another part of its website," Rivers wrote.</p>
<p>In the end, Rivers ruled that Moffatt was entitled to a partial refund of $650.88 in Canadian dollars (CAD) off the original fare (about $482 USD), which was $1,640.36 CAD (about $1,216 USD), as well as additional damages to cover interest on the airfare and Moffatt's tribunal fees.</p>
<p>Air Canada told Ars it will comply with the ruling and considers the matter closed.</p>
<h2>Air Canada’s chatbot appears to be disabled</h2>
<p>When Ars visited Air Canada's website on Friday, there appeared to be no chatbot support available, suggesting that Air Canada has disabled the chatbot.</p>                                            
                                                        
<p>Air Canada did not respond to Ars' request to confirm whether the chatbot is still part of the airline's online support offerings.</p>
<p>Last March, Air Canada's chief information officer Mel Crocker <a href="https://www.theglobhttps//www.theglobeandmail.com/business/article-ai-call-centres/">told the Globe and Mail</a> that the airline had launched the chatbot as an AI "experiment."</p>
<p>Initially, the chatbot was used to lighten the load on Air Canada's call center when flights experienced unexpected delays or cancellations.</p>
<p>“So in the case of a snowstorm, if you have not been issued your new boarding pass yet and you just want to confirm if you have a seat available on another flight, that’s the sort of thing we can easily handle with AI,” Crocker told the Globe and Mail.</p>
<p>Over time, Crocker said, Air Canada hoped the chatbot would "gain the ability to resolve even more complex customer service issues," with the airline's ultimate goal to automate every service that did not require a "human touch."</p>
<p>If Air Canada can use "technology to solve something that can be automated, we will do that,” Crocker said.</p>
<p>Air Canada was seemingly so invested in experimenting with AI that Crocker told the Globe and Mail that "Air Canada’s initial investment in customer service AI technology was much higher than the cost of continuing to pay workers to handle simple queries." It was worth it, Crocker said, because "the airline believes investing in automation and machine learning technology will lower its expenses" and "fundamentally" create "a better customer experience."</p>
<p>It's now clear that for at least one person, the chatbot created a more frustrating customer experience.</p>
<p>Experts told the Vancouver Sun that Air Canada may have succeeded in avoiding liability in Moffatt's case if its chatbot had warned customers that the information that the chatbot provided may not be accurate.</p>
<p>Because Air Canada seemingly failed to take that step, Rivers ruled that "Air Canada did not take reasonable care to ensure its chatbot was accurate."</p>
<p>"It should be obvious to Air Canada that it is responsible for all the information on its website," Rivers wrote. "It makes no difference whether the information comes from a static page or a chatbot."</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dear writers: Delete your Findaway Voices account NOW (150 pts)]]></title>
            <link>https://mwl.io/archives/23448</link>
            <guid>39399826</guid>
            <pubDate>Fri, 16 Feb 2024 17:08:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mwl.io/archives/23448">https://mwl.io/archives/23448</a>, See on <a href="https://news.ycombinator.com/item?id=39399826">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-23448">
		<!-- .entry-header -->

	
	<div>
		<p>When Findaway Voices first appeared, it made it comparatively easy for independent authors to do audiobooks. Audio was still hard, mind you, but it was possible.</p>
<p>Spotify bought Findaway. They began playing with payments, refunds, and returns. And now, the <a href="https://my.findawayvoices.com/terms-of-use">licensing terms have changed</a>.</p>
<blockquote><p>Accordingly, you hereby grant Spotify a non-exclusive, transferable, sublicensable, royalty-free, fully paid, irrevocable, worldwide license to reproduce, make available, perform and display, translate, modify, create derivative works from (such as transcripts of User Content), distribute, and otherwise use any such User Content through any medium, whether alone or in combination with other Content or materials, in any manner and by any means, method or technology, whether now known or hereafter created, in connection with the Service, the promotion, advertising or marketing of the Service, and the operation of Spotify’s (and its successors’ and affiliates’) business, including for systems and products management, improvement and development, testing, training, modeling and implementation in connection with the Spotify Service. Where applicable and to the extent permitted under applicable law, you also agree to waive, and not to enforce, any “moral rights” or equivalent rights, such as your right to object to derogatory treatment of such User Content. Nothing in these Terms prohibits any use of User Content by Spotify that may be taken without a license.
</p></blockquote>
<p>Spotify may now do anything they want with your audiobook. They will–not can, <em>will</em>–feed it to their AI system and use it to rip off your work. They specifically declare you can’t complain about derogatory uses. They can mix your book with work you find abhorrent and release it as a new product. They can use a speech recognition system and create a printed version of your book.</p>
<p>I have one audiobook. I pulled it from distribution when the royalties problems started and I stopped getting paid. <a href="https://www.tiltedwindmillpress.com/product/audiosbs/">That audiobook became exclusive to my store on 17 January 2023</a>. It has fewer sales, but I’ve made more than I did in all the years before. (“But exposure,” some folks will say. People die of exposure.)</p>
<p>It’s not enough to stop distributing your work via Findaway. If you use them to store your audio files and nothing else, the new terms apply. They have no automatic option to delete titles from their site. I just sent this email to their technical support.</p>
<blockquote><p>Hello,</p>
<p>Findaway’s new terms of service are unacceptable. Please delete my<br>
book and my entire account.</p>
<p>Thank you.
</p></blockquote>
<p>No need to be rude. It’s not the tech support flunky’s fault.</p>
<p>Also, I’m super happy with how my one lone audiobook came out. If it sold more, I’d do more.</p>
	</div><!-- .entry-content -->

	 <!-- .entry-footer -->
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why we stopped building cut and cover (110 pts)]]></title>
            <link>https://worksinprogress.co/issue/why-we-stopped-building-cut-and-cover/</link>
            <guid>39398803</guid>
            <pubDate>Fri, 16 Feb 2024 16:04:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://worksinprogress.co/issue/why-we-stopped-building-cut-and-cover/">https://worksinprogress.co/issue/why-we-stopped-building-cut-and-cover/</a>, See on <a href="https://news.ycombinator.com/item?id=39398803">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>We used to dig up roads to put trains underneath – cheaply. Ever-better tunnel boring machines have made the disruption this causes unnecessary.</p><article><div role="presentation"><p>Tunneling is one of the many technologies that make modern ​civilization possible. For one, a tunnel can dramatically reduce transportation costs by shortening travel times between two points. Prior to the construction of the Holland Tunnel beneath the Hudson River in New York, for instance, the only way across was via ferry, a journey that <!-- --><a href="https://www.ascemetsection.org/committees/history-and-heritage/landmarks/holland-tunnel">could take hours</a> if the ferries were backed up. Tunnels are also needed to build large-scale infrastructure projects: hydroelectric dams require tunnels to divert water around the construction site and to feed water to the ​turbines. And tunneling can create new, valuable land beneath dense urban areas. By going underground, we can create the space for horizontal ​infrastructure such as subway lines without destroying existing buildings or disrupting the urban fabric. This makes tunneling an important tech­nology for building cities that people like living in.<!-- --></p>



<!-- --><p>Historically, most subways were built using what’s known as ‘cut and cover’ excavation: digging an open trench, building the tunnel structure within it, and then covering the trench up. Cut and cover was used for the first London subway line, in 1860; it was used for the construction of New York’s first subway, in 1900; and for nearly a century it remained the preferred method of building subway tunnels. As late as the 1970s, most subway construction in the US was done using cut and cover.</p>


<!-- -->


<!-- --><p>For many types of underground construction, especially in undeveloped greenfield land, <!-- --><a href="https://www.waterproofmag.com/2012/01/cut-and-cover-tunnels/">cut and cover is still widely used</a>. And cut and cover is still the primary method of constructing underground train stations. But for urban subway tunnels, cut and cover has largely been supplanted by the use of tunnel-boring machines (TBMs), which tunnel horizontally beneath the ground without disturbing the surface. In a <!-- --><a href="https://datawrapper.dwcdn.net/MYoQk/5/">database compiled by Britain Remade of recent transit projects</a> around the world, there are 80 projects listed as using TBMs, compared to just one being built with cut and cover.<!-- --></p>



<!-- --><p>Some transit experts believe that this transition was a misstep. Cut and cover is a much more disruptive construction method (since it tears up the street while construction is taking place), but it’s often much cheaper than using a TBM. During construction of the <!-- --><a href="https://en.wikipedia.org/wiki/Canada_Line">Canada Line</a> in Vancouver, between 2005 and 2009, changing from bored tunnel to cut and cover <!-- --><a href="https://rccao.com/research/files/RCCAO-STATION-TO-STATION-REPORT-APRIL2020.pdf">saved more than $400 million</a> in construction costs, 16 percent of the cost of the entire project. Alon Levy of the <!-- --><a href="https://marroninstitute.nyu.edu/initiatives/transit-costs-project">Transit Costs Project</a> argues that ‘<!-- --><a href="https://pedestrianobservations.com/2021/02/25/cut-and-cover-is-underrated/">cut and cover is underrated</a>’, and it should merit more consideration when deciding how a transit project should be built:<!-- --></p>



<!-- --><blockquote><p>Regrettably, people don’t seem to even recognize it as a tradeoff, in which they spend more money to avoid surface disruption – some of our sources have told us that avoiding top-down cut and cover is an unalloyed good, a kind of modernity. Even more regrettably, this same thinking is common in much of the developing world, where subways tend to be bored.</p></blockquote>



<!-- --><p>But cut and cover has always been an unpopular method of construction, opposed by urban residents since the very first time it was used. Cut and cover may often be cheaper in terms of dollars, but as tolerance for the disruptive effects of construction has decreased, the <!-- --><em>political</em> costs of using cut and cover have risen. And as other tunneling technology has improved, the relative advantage of using cut and cover has decreased.&nbsp;<!-- --></p>



<!-- --><h3>A brief overview of tunneling technology</h3>



<!-- --><p>Understanding why tunnel-boring machines replaced cut and cover requires knowing a little bit about how tunnel construction works.</p>



<!-- --><p>With cut and cover construction, the basic method – digging a trench and then covering the trench up – is simple. But there are a variety of ways that this can be done, depending on the specifics of the tunnel and where it’s being built. The most straightforward method is to dig a trench with gently sloping sides that require no additional support. Once you’ve dug down deep enough, you build your structure, and cover everything back up again.</p>



<!-- --><figure><img loading="lazy" width="1024" height="609" src="https://wip.gatspress.com/wp-content/uploads/2024/02/cut-and-cover-tunneling-1024x609.png" alt="" srcset="https://wip.gatspress.com/wp-content/uploads/2024/02/cut-and-cover-tunneling-1024x609.png 1024w, https://wip.gatspress.com/wp-content/uploads/2024/02/cut-and-cover-tunneling-300x179.png 300w, https://wip.gatspress.com/wp-content/uploads/2024/02/cut-and-cover-tunneling-768x457.png 768w, https://wip.gatspress.com/wp-content/uploads/2024/02/cut-and-cover-tunneling-1536x914.png 1536w, https://wip.gatspress.com/wp-content/uploads/2024/02/cut-and-cover-tunneling-2048x1219.png 2048w, https://wip.gatspress.com/wp-content/uploads/2024/02/cut-and-cover-tunneling-402x239.png 402w, https://wip.gatspress.com/wp-content/uploads/2024/02/cut-and-cover-tunneling-462x275.png 462w, https://wip.gatspress.com/wp-content/uploads/2024/02/cut-and-cover-tunneling-662x394.png 662w, https://wip.gatspress.com/wp-content/uploads/2024/02/cut-and-cover-tunneling-722x430.png 722w, https://wip.gatspress.com/wp-content/uploads/2024/02/cut-and-cover-tunneling-982x584.png 982w, https://wip.gatspress.com/wp-content/uploads/2024/02/cut-and-cover-tunneling-1032x614.png 1032w, https://wip.gatspress.com/wp-content/uploads/2024/02/cut-and-cover-tunneling-1402x834.png 1402w, https://wip.gatspress.com/wp-content/uploads/2024/02/cut-and-cover-tunneling-1702x1013.png 1702w, https://wip.gatspress.com/wp-content/uploads/2024/02/cut-and-cover-tunneling-2002x1192.png 2002w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<!-- --><p>Because the sides of the trench slope outward, this method occupies a lot of horizontal space. And the deeper the excavation, the more space is required. This can make it a challenge to use in urban areas, where that space is occupied by buildings and other infrastructure. To avoid this, cut and cover construction will instead often excavate straight downward, using support structures to prevent the walls from caving in.</p>



<!-- --><p>These supports can be built in a variety of different ways. One common method is to use piles, large posts that are driven deep into the earth. Piles are typically made from either steel or concrete, and can either be spaced close enough together that they form a continuous wall (such as with <!-- --><a href="https://www.kagaoanengineering.com/secant-and-tangent-pile-walls">secant piles</a> or <!-- --><a href="https://railsystem.net/sheet-pile-wall-construction/">sheet piles</a>), or spaced farther apart with infill structure between them, such as timber lagging (wood boards that span between piles) or shotcrete (sprayed concrete).<!-- --></p>



<!-- --><figure><img src="https://www.wsp.com/-/media/service/global/image/img-amtrak-gateway-program.jpg?h=710&amp;iar=0&amp;w=1440&amp;hash=6E86A2468F05791D74E053169A5B6A24" alt="">
          <!-- --><figcaption>
            <!-- --><div>
                <!-- --><p>Image</p>
                <!-- --><div><p>
                  Image from </p><!-- --><p><a href="https://www.wsp.com/en-us/services/cut-and-cover-tunneling">WSP</a>.
                </p><!-- --></div>
              <!-- --></div>
          <!-- --></figcaption>
        <!-- --></figure>



<!-- --><p>Another type of vertical support structure is the <!-- --><a href="https://en.wikipedia.org/wiki/Slurry_wall">slurry wall</a>, sometimes called the Milan system. With this method, a deep, narrow trench is excavated and filled with bentonite, a dense clay slurry, which prevents the sides from collapsing. The trench is then filled with concrete, which displaces the bentonite and forms a continuous wall when it solidifies. ​The Milan method was invented in the 1940s, and was notably used to <!-- --><a href="https://www.911memorial.org/connect/blog/slurry-wall-behind-engineering-feat-made-wtc-possible">create the ‘bathtub’ foundation on the original World Trade Center</a>.<!-- --></p>



<!-- --><p>As excavation proceeds downward, these vertical supports need to be braced to resist the horizontal force of the soil. This can be done with steel braces that span the width of the trench, or with soil anchors that tie the walls back into the surrounding soil.</p>



<!-- --><p>Cut and cover also uses different methods for building the tunnel structure itself. In the conventional method, known as bottom-up, the trench is fully excavated and the tunnel structure is built up starting from the bottom. With the top-down method, by contrast, the tunnel is excavated only partway down, and then the roof of the tunnel is built using the existing soil as a vertical support. Once the roof is in place, the rest of the tunnel is then excavated below it. With top-down construction, the surface can be completely restored after the roof has been built; with bottom-up, the top of the excavation will often be covered with temporary decking to allow use of the surface while tunnel construction is taking place.</p>



<!-- --><figure><img loading="lazy" width="1024" height="554" src="https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.01.34-1024x554.png" alt="" srcset="https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.01.34-1024x554.png 1024w, https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.01.34-300x162.png 300w, https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.01.34-768x416.png 768w, https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.01.34-1536x832.png 1536w, https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.01.34-402x218.png 402w, https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.01.34-462x250.png 462w, https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.01.34-662x358.png 662w, https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.01.34-722x391.png 722w, https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.01.34-982x532.png 982w, https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.01.34-1032x559.png 1032w, https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.01.34-1402x759.png 1402w, https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.01.34-1702x921.png 1702w, https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.01.34.png 1932w" sizes="(max-width: 1024px) 100vw, 1024px">
          <!-- --><figcaption>
            <!-- --><div>
              <!-- --><p>
                Cut and cover tunneling using different construction sequences: (a) bottom-up (b) top-down.
              </p>
              <!-- --><div>
                <!-- --><p>Image</p>
                <!-- -->
              <!-- --></div>
            <!-- --></div>
          <!-- --></figcaption>
        <!-- --></figure>



<!-- --><p>With a tunnel-boring machine, the basic method is different. Instead of digging downward, TBMs use large rotating cutting heads to excavate horizontally through the ground. Behind the rotating cutting head will be conveyors for carrying away excavated material (known as muck), hydraulic jacks for pushing the machine forward, and machines for installing the tunnel lining. A modern TBM is very much like a mobile factory that pushes its way through the earth and leaves a completely constructed tunnel behind it.</p>



<!-- --><p>Like with cut and cover, TBMs comprise a variety of specific excavation technologies that vary depending on the project. At a high level, TBMs are categorized by whether they’re designed to tunnel through soil and soft ground or through rock (though today there are increasingly <!-- --><a href="https://www.robbinstbm.com/products/tunnel-boring-machines/crossover-machines/">crossover machines</a> that can do both).<!-- --></p>



<!-- --><p>Soft-ground TBMs evolved from unmechanized tunnel shields, large hollow structures that supported the sides of the tunnel while it was being excavated. The tunnel shield was invented by Marc Brunel (father of ​Isambard Kingdom Brunel) in 1806 for tunneling under the Neva River in Russia, and was first used to <!-- --><a href="https://en.wikipedia.org/wiki/Thames_Tunnel">tunnel under the Thames in 1825</a>. <!-- --></p>



<!-- --><p>Brunel’s shield consisted of a 21-foot-tall grid of iron frames, divided into 12 separate frames, each one consisting of three compartments stacked on top of one another. Within each compartment, the face of the tunnel would be supported by a series of boards called poling boards. A worker would remove a single board, dig away the soil behind it to a depth of around nine inches, and then replace the board and move on to the next one. After all boards had been dug out, the frame would advance forward with large mechanical jacks, and the process would repeat. Behind the shield, brick lining would be installed around the sides of the tunnel to form its structure. With Brunel’s shield, tunneling under the Thames proceeded at about eight feet per week on average.</p>



<!-- --><figure><img loading="lazy" width="787" height="542" src="https://wip.gatspress.com/wp-content/uploads/2024/02/Thames_tunnel_shield.png" alt="" srcset="https://wip.gatspress.com/wp-content/uploads/2024/02/Thames_tunnel_shield.png 787w, https://wip.gatspress.com/wp-content/uploads/2024/02/Thames_tunnel_shield-300x207.png 300w, https://wip.gatspress.com/wp-content/uploads/2024/02/Thames_tunnel_shield-768x529.png 768w, https://wip.gatspress.com/wp-content/uploads/2024/02/Thames_tunnel_shield-402x277.png 402w, https://wip.gatspress.com/wp-content/uploads/2024/02/Thames_tunnel_shield-462x318.png 462w, https://wip.gatspress.com/wp-content/uploads/2024/02/Thames_tunnel_shield-662x456.png 662w, https://wip.gatspress.com/wp-content/uploads/2024/02/Thames_tunnel_shield-722x497.png 722w" sizes="(max-width: 787px) 100vw, 787px">
          <!-- --><figcaption>
            <!-- --><div>
              <!-- --><p>
                Caption: Tunneling shield used in construction of the Thames Tunnel in London.
              </p>
              <!-- --><div>
                <!-- --><p>Image</p>
                <!-- -->
              <!-- --></div>
            <!-- --></div>
          <!-- --></figcaption>
        <!-- --></figure>



<!-- --><p>Brunel’s shield was rectangular in shape, but most subsequent tunnel shields were circular. Early shields used workers with picks and shovels to do the actual excavation, but over time mechanical excavation equipment was added. In the early 1900s John Price developed a tunnel shield that had a large, rotating disc mounted to the front. Bucket-shaped cutters ​attached to the front of the disc would scrape away soil as it rotated and feed it into a conveyor for removal. Price’s mechanized shields were an ​immediate success, and over the next several decades were used to dig ​subways around the world, and are the ancestor of modern soft-​ground TBMs.</p>



<!-- --><p>The tunnel shield prevented the sides of the tunnel from collapsing while it was bored, but they still required some method to prevent the face of the tunnel from collapsing, and to prevent water from intruding when tunneling below the water table. By the late nineteenth century, the standard method was to use compressed air. By pressurizing the tunnel to several times atmospheric pressure, water would be kept out. Compressed air remained in use well into the twentieth century, and is still sometimes used today, but it has been largely supplanted by slurry machines and earth pressure balance machines, which respectively use a bentonite slurry and the excavated material itself to support the face of the tunnel. Today, earth pressure balance machines are the most common type of TBM for tunneling through soil.</p>



<!-- --><p>Rock TBMs evolved separately from soil TBMs. In soil, the task of ex­cavation was comparatively simple, and the primary challenge was finding a way to prevent the tunnel from collapsing while it was being dug. In rock, the tunnel could often support itself while it was being dug, and the primary difficulty was building a machine robust enough to carve through rock. This second task proved much more difficult, and successful rock-tunneling machines were developed much later than soil-tunneling machines.</p>



<!-- --><p>Attempts to build rock-tunneling machines date back to the 1850s, but the first successes appeared in the 1950s, when <!-- --><a href="https://www.robbinstbm.com/about/history/">James Robbins developed the disc cutter</a> for the <!-- --><a href="https://en.wikipedia.org/wiki/Oahe_Dam">Oahe Dam</a> project. Prior to this, most attempts at mechanical rock-tunneling machines used drag picks, sharp steel tools that scraped away bits of rock as the cutting head rotated. Robbins’s disc cutter, on the other hand, rolled freely over the surface of the rock like a wheel. As the tunneling machine pressed the disc cutter against the face of the rock, the rock cracked and flaked off.<!-- --></p>



<!-- --><figure><img loading="lazy" width="1024" height="350" src="https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.04.31-1024x350.png" alt="" srcset="https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.04.31-1024x350.png 1024w, https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.04.31-300x103.png 300w, https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.04.31-768x262.png 768w, https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.04.31-1536x525.png 1536w, https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.04.31-402x137.png 402w, https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.04.31-462x158.png 462w, https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.04.31-662x226.png 662w, https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.04.31-722x247.png 722w, https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.04.31-982x336.png 982w, https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.04.31-1032x353.png 1032w, https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.04.31-1402x479.png 1402w, https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.04.31-1702x582.png 1702w, https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.04.31.png 1978w" sizes="(max-width: 1024px) 100vw, 1024px">
          <!-- --><figcaption>
            <!-- --><div>
              <!-- --><p>
                How two disc cutters chip away rock.
              </p>
              <!-- --><div>
                <!-- --><p>Image</p>
                <!-- -->
              <!-- --></div>
            <!-- --></div>
          <!-- --></figcaption>
        <!-- --></figure>



<!-- --><p>Robbins’s disc cutter greatly increased how fast a rock-tunneling machine could tunnel. And disc cutters lasted much longer before needing to be replaced than drag picks, meaning the machines spent more time tunneling and less time down for maintenance. As a result, Robbins’s machine made it economical to mechanically tunnel through the rock for the first time. <!-- --><a href="https://www.robbinstbm.com/">The Robbins Company</a> remains a builder of all types of TBMs today, and the disc cutter continues to be the standard method for excavation on rock TBMs.<!-- --></p>



<!-- --><p>There are also other ways to bore a tunnel besides using a TBM. A <!-- --><a href="https://en.wikipedia.org/wiki/Roadheader">roadheader</a> uses a small, rotating cutter mounted to a boom arm that gets moved back and forth over the tunnel face (this is in contrast to a TBM, which excavates the entire face of the tunnel at once).<!-- --></p>



<!-- --><p><a href="https://en.wikipedia.org/wiki/Drilling_and_blasting">Drill and blast</a> involves drilling several holes in the face of the tunnel ​(typically using a mechanical drilling machine known as a <!-- --><a href="https://en.wikipedia.org/wiki/Drilling_jumbo">drilling jumbo</a>) and setting off explosives in them. Drill and blast was the primary method of excavating rock tunnels prior to the invention of rock TBMs, and is &nbsp;<!-- --><a href="https://nyatunnelbanan.se/en/blasting-is-now-being-performed-at-61-sites-simultaneously/">still widely used today</a>.<!-- --></p>



<!-- --><p>The <!-- --><a href="https://www.soundtransit.org/sites/default/files/project-documents/SEM_final.pdf">sequential excavation method</a> (SEM), also known as the New Austrian ​tunneling method, excavates a tunnel in small ‘bites’ using <!-- --><a href="https://en.wikipedia.org/wiki/Excavator#:~:text=Excavators%20are%20heavy%20construction%20equipment,undercarriage%20with%20tracks%20or%20wheels.">mechanical excavators</a> and other equipment, and supports the sides of the tunnel using <!-- --><a href="https://en.wikipedia.org/wiki/Shotcrete">shotcrete</a>.<!-- --></p>



<!-- --><figure><img loading="lazy" width="1024" height="628" src="https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.12.43-1-1024x628.png" alt="" srcset="https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.12.43-1-1024x628.png 1024w, https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.12.43-1-300x184.png 300w, https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.12.43-1-768x471.png 768w, https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.12.43-1-1536x942.png 1536w, https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.12.43-1-402x247.png 402w, https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.12.43-1-462x283.png 462w, https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.12.43-1-662x406.png 662w, https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.12.43-1-722x443.png 722w, https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.12.43-1-982x603.png 982w, https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.12.43-1-1032x633.png 1032w, https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.12.43-1-1402x860.png 1402w, https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.12.43-1-1702x1044.png 1702w, https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.12.43-1-2002x1228.png 2002w, https://wip.gatspress.com/wp-content/uploads/2024/02/Screenshot-2024-02-12-at-15.12.43-1.png 2034w" sizes="(max-width: 1024px) 100vw, 1024px">
          <!-- --><figcaption>
            <!-- --><div>
                <!-- --><p>Image</p>
                <!-- --><p>
                  Caption: Sketch of the sequential excavation method (SEM).
                </p>
              <!-- --></div>
          <!-- --></figcaption>
        <!-- --></figure>



<!-- --><h3>Changing technology, changing economics</h3>



<!-- --><p>For most of history, cut and cover was the cheapest way to build an urban tunnel, and boring was only done if cut and cover wasn’t an option. In the construction of <!-- --><a href="https://en.wikipedia.org/wiki/Early_history_of_the_IRT_subway">New York’s first subway, in 1900</a>, for instance, cut and cover was estimated to be just an eighth the cost of boring a tunnel, but it could only be used on about half the total length of the line.<!-- --><sup id="ref-1"></sup> Because the ground of New York varies in elevation substantially, keeping the tracks straight required tunnels bored through rock, which were built using drill and blast.<!-- --></p>



<!-- --><p>But as tunneling machine technology continued to advance, this calculus changed. Brunel’s non-mechanized shield tunneled under the Thames at the glacial pace of eight feet per week. By the early 1900s, Price mechanized shields were achieving excavation rates of nearly 200 feet per week. And by the 1970s, TBMs were achieving rates of 1,400 feet per week in soft ground, and 1,900 feet per week in rock.</p>


<!-- -->


<!-- --><p>As TBMs got faster, they also got cheaper, and became increasingly competitive with cut and cover. When a TBM was used to bore some of the tunnels on the Bay Area Rapid Transit (BART) project in the 1960s, its costs were just <!-- --><a href="https://archive.org/details/analysisofbartca1978davi/page/26/mode/2up?q=%22cut+and+cover%22">40 percent higher on average</a> than the cut and cover sections, a far cry from the eight-times cost difference on the New York Subway. ​A 1994 study of <!-- --><a href="https://www.researchgate.net/publication/341325652_Etude_des_couts_des_infrastructures_de_transport_ferroviaire_en_zone_urbaine_et_suburbaine">French subway construction costs</a> on over 90 miles of underground tunnel found that only in the most difficult underground conditions was tunnel boring more expensive on average than cut and cover.<!-- --></p>



<!-- --><p>Depending on the nature of the project and how disruptive surface construction would be, TBMs in some cases began to be cheaper than cut and cover. A <!-- --><a href="https://archive.org/details/finalalternative00unse_0/page/n31/mode/2up?q=%22cut+and+cover%22">1980 environmental analysis for a rapid transit system for Los Angeles</a> estimated that cut and cover construction would be more expensive than bored tunnel, due to needing to use eminent domain to buy and destroy homes along the roads. And when Seattle planned a tunnel to replace the <!-- --><a href="https://en.wikipedia.org/wiki/Alaskan_Way_Viaduct">Alaskan Way Viaduct</a> in the early 2000s, the costs of a bored tunnel were projected to be comparable to cut and cover, but the disruptions to the city caused by cut and cover were projected to cost several additional billion dollars.<!-- --><sup id="ref-2"></sup></p>



<!-- --><p>Similarly, TBMs have high fixed costs (in the form of the time, effort, and expense to buy the machine and get it set up) but low operational costs: once they are up and running, the marginal cost of additional excavation is low. TBMs are thus often particularly economical on large tunneling projects where the fixed costs of the machine can be thinly spread. ​When Madrid built 60 miles of underground tunnel when constructing its metro in the late 1990s and early 2000s, it achieved a famously low cost ​of €42 million per kilometer (about $73 million per kilometer in 2023 dollars) using TBMs. And the recent <!-- --><a href="http://www.madrid.org/media/transportes/ampliacion-linea11-metro/documento3-presupuesto.pdf">extension of the L11 line</a> in Madrid, which adds another 4.3 miles to the metro system, likewise found that excavation with TBMs would be cheaper than cut and cover.<!-- --></p>



<!-- --><p>Underground construction is high variance, and the costs of construction can vary greatly depending on the nature of the project and the conditions of the ground.<!-- --><sup id="ref-3"></sup> The best construction technology for a given project will depend on the specifics of that project. As Alon Levy notes, cut and cover is still a useful arrow to have in a tunneler’s quiver, as per the $400 million savings it achieved on the Canada Line. Given the comparative labor intensity of cut and cover (TBMs are highly automated, and can operate with a very small number of workers), it is likely especially appropriate for countries in Asia and Africa with low wages. But as TBM technology has advanced, it’s become more and more attractive for urban tunneling.<!-- --></p>



<!-- --><h3>Cut and cover gets harder</h3>



<!-- --><p>While tunnel-boring technology has gotten better and better, cut and cover has steadily gotten more difficult. The chief issue is the fact that cut and cover creates an enormous amount of disruption on the surface while excavation is taking place. The construction creates dirt, noise, and flooding, and can damage nearby properties as the ground is dug up; this has resulted in <!-- --><a href="https://archive.org/details/redlineextension01mass/page/n99/mode/2up?q=%22cut-and-cover%22">numerous lawsuits against transit authorities</a> (tunnel builders will generally include a contingency to pay for buildings damaged as a result of construction for this reason).<!-- --></p>



<!-- --><p>Most importantly, in cut and cover construction the street gets torn up and portions of it become unusable, sometimes for years. Access to businesses is blocked, retail sales fall, and people complain. Disruption of traffic has been called ‘<!-- --><a href="https://archive.org/details/cutandcovertunne00wick/page/28/mode/2up">the plague</a>’ of cut and cover construction:<!-- --></p>



<!-- --><blockquote><p>Noise and dust receive their share of complaints, but these can be controlled to some extent to minimize nuisance. It is the day-to-day rerouted obstacle course of construction equipment, barricades, flagmen, and rattling deck beams that create an impression of confusion and personal affront to the daily commuter or casual visitor.</p></blockquote>



<!-- --><p>Traffic disruption from cut and cover is especially egregious because the nature of subway construction projects means that construction is likely to take place in the most heavily congested areas of the city, making the problem worse until construction is completed.</p>



<!-- --><p>In <!-- --><em>The Great Society Subway</em>, Zachary Schrag talks about the disruptions caused by cut and cover during the construction of the Washington, DC, Metro, stating that ‘cut and cover meant pain’:<!-- --></p>



<!-- --><blockquote><p>Virginia Ali, whose Chili Bowl restaurant had served U Street since 1958, had endured riots and illicit drug markets, but subway construction was worse. With U Street itself blocked off, customers had to find their way through alleys. If construction workers hit a gas line, diners would have to evacuate, and frequently Ali found her restaurant’s floor inches deep in dirty water that ran off the wooden blanks that served as U Street’s decking. A block-long stretch of 7th Street turned into a twenty-foot deep garbage pit; residents fretted that children might climb through the shoddy fences and fall in. By the eve of completion, a neighborhood resident mourned ‘after five years of construction, the name of the game right now is survival’.</p></blockquote>



<!-- --><p>Cut and cover can also cause subsidence in the ground surrounding the excavation site, damaging surrounding buildings. TBMs will sometimes be chosen as the excavation method even when they’re more expensive purely to reduce this risk.</p>



<!-- --><p>Because of the disruptions it causes, cut and cover transit construction has been unpopular since its inception. London’s <!-- --><a href="https://archive.org/details/tunnelsplanningd0002mega/page/10/mode/2up">first two underground railways were built using cut and cover</a>, but public objections to the disruptions it caused, plus the fact that most of the remaining streets were too small to practically use, forced subsequent lines to be built using bored tunnel.<!-- --></p>



<!-- --><p>In the construction of New York’s first subway, cut and cover was described as ‘<!-- --><a href="https://archive.org/details/isbn_9780312591328/page/312/mode/2up?q=%22cut+and+cover%22">making life miserable for a few years</a>’. Boston built its first subway (and the first subway in the US) using cut and cover, but when designing an extension to the Red Line in 1977 it opted for less disruptive tunnel boring in many locations, such as the <!-- --><a href="https://archive.org/details/redlineextension01mass/page/n99/mode/2up?q=%22cut-and-cover%22">segment between Harvard Square and Porter Square</a>:<!-- --></p>



<!-- --><blockquote><p>The deep bore tunneling method was chosen over the cut and cover method because it will lessen the impact during construction on the surrounding neighborhoods. Specifically, the deep bore method negates the need to tear up Massachusetts Avenue, which runs along most of the length of this section and which, if narrowed to half its width, would cause severe traffic congestion problems. Additionally, shops along this section of Massachusetts Avenue, which number between 40 and 50, would incur substantial economic losses due to a temporary loss of customer parking spaces, advertising exposure, and customer accessibility.</p></blockquote>



<!-- --><p>The disruptions caused by cut and cover often make using it difficult, even when it’s cheaper than other methods. In their <!-- --><a href="https://archive.org/details/tunnelsplanningd0002mega/page/n7/mode/2up">1981 textbook on tunneling</a>, TM Megaw and JV Bartlett note that ‘The negotiations with those affected by a new [subway] line are likely to be far more difficult for cut and cover construction than for a deep tunnel. Objections sometimes are given disproportionate publicity; the promoters have to justify their proposals in much greater detail’.<!-- --></p>



<!-- --><p>During the construction of Atlanta’s subway, MARTA, in the 1970s, the planned use of cut and cover in the downtown area <!-- --><a href="https://www.princeton.edu/~ota/disk3/1976/7602/7602.PDF">caused a major backlash</a>, and caused the planners to <!-- --><a href="https://rosap.ntl.bts.gov/view/dot/11690">switch to bored tunnel</a>. When planning the BART extension to reach San Francisco Airport in 1995, transit ​advocates argued that BART’s preferred alternative required tunneling that would cost $135 million more than cut and cover, but if cut and cover was used the cities of South San Francisco and San Bruno would <!-- --><a href="https://books.google.com/books?id=SpofAAAAMAAJ&amp;pg=PA2096&amp;dq=bart+cut+and+cover+construction+vs+tunnels&amp;hl=en&amp;newbks=1&amp;newbks_redir=0&amp;sa=X&amp;ved=2ahUKEwi8iL7-kKOBAxVQnGoFHSZlBh44ChDoAXoECAoQAg#v=onepage&amp;q=bart%20cut%20and%20cover%20construction%20vs%20tunnels&amp;f=false">oppose the plan because of the construction impacts</a>. When a cut and cover tunnel was proposed for the Alaskan Way Viaduct replacement in Seattle, it was <!-- --><a href="https://books.google.com/books?id=7Xw2AQAAMAAJ&amp;pg=PA13&amp;dq=cut+and+cover+deep+bore+tunnel+cost&amp;hl=en&amp;newbks=1&amp;newbks_redir=0&amp;sa=X&amp;ved=2ahUKEwjxkujMn4KCAxWLLkQIHQdsBV44ChDoAXoECAgQAg#v=onepage&amp;q=cut%20and%20cover%20deep%20bore%20tunnel%20cost&amp;f=false">rejected by Seattle voters</a>.<!-- --></p>



<!-- --><p>The ability for citizens and advocacy groups to oppose disruptive cut and cover projects has likely been strengthened by environmental laws, such as the National Environmental Policy Act (NEPA), that require government agencies to investigate and disclose the environmental impacts of their projects. These laws first began to appear in the late 1960s, and provide an avenue for affected parties to oppose projects by arguing that agencies haven’t followed the proper administrative procedures. ​The previously mentioned Canada Line project, for instance, <!-- --><a href="https://archive.org/details/canadianenvironm0026unse/page/18/mode/2up?q=%22cut+and+cover%22">was sued</a> by a group of citizens who objected to the use of cut and cover under Canada’s <!-- --><a href="https://web.archive.org/web/20230321182016/https://www.canada.ca/en/impact-assessment-agency/services/policy-guidance/canadian-environmental-assessment-act-overview.html">Environmental Assessment Act</a>. They argued that the proper ​procedure had not been followed when disclosing the impacts of it. ​The appeals court noted that citizens objecting to public projects by ‘challenging not the substantive decision of government approving the project, but the process by which the government decision-makers informed the public’, had become common.<!-- --></p>



<!-- --><p>The nature of cut and cover construction means that we might expect it to get more expensive over time. A major expense of cut and cover construction is having to relocate below-ground utilities, and the more that have to be relocated, the greater the cost. As cities age, and accumulate more buried services, this will naturally make relocating them more expensive compared to simply tunneling beneath them.</p>



<!-- --><p>With so much opposition to cut and cover, and the increasingly competitive costs of tunnel boring, it’s not surprising that developed countries have largely switched to the latter in built-up areas. And in fact, adopting a technology, then abandoning it later when its downsides are deemed to be too high, even if its replacement is more expensive, is a common arc of technological progression: until the 1940s, hydroelectric dams made up nearly a third of electricity generated in the US, but hydro plants stopped being built in the US in the 1970s, due largely to their disruption of river ecosystems and other negative environmental effects; aluminum wiring is cheaper than copper (which is why its used for <!-- --><a href="https://en.wikipedia.org/wiki/Aluminium-conductor_steel-reinforced_cable">long-distance transmission lines</a>), but the risks of fire due to improper installation in homes caused it to be <!-- --><a href="https://books.google.com/books?id=c69tJz28TSQC&amp;pg=PA249&amp;dq=use+of+aluminum+wiring&amp;hl=en&amp;newbks=1&amp;newbks_redir=0&amp;sa=X&amp;ved=2ahUKEwjXuKvNkK2BAxW-mmoFHXWYCwo4FBDoAXoECAoQAg#v=onepage&amp;q=use%20of%20aluminum%20wiring&amp;f=false">phased out in the 1970s</a>; and, of course, there’s currently an enormous effort dedicated toward replacing carbon-emitting energy sources with more more environmentally friendly low-carbon ones.<!-- --></p>



<!-- --><p>In fact, the existence of subways at all is arguably a result of this sort of progression. Prior to the subway-construction era in the US in the early twentieth century, many cities had rapid transit systems based on elevated ​trains. New York built the first elevated train line in the world in 1868, and by 1890 its elevated trains gave New York the largest mass transit system in the world, ten years before it began construction on its subway. By 1900, elevated urban trains had been built in Kansas City, Sioux City, Chicago, and Boston.</p>



<!-- --><p>But, not unlike cut and cover construction, building an elevated train came at the cost of incredible disruption to the surrounding area (and unlike an elevated train, that disruption remained after construction was complete). The structures reduced sunlight to the street below, and steam-powered elevateds on steel frames were incredibly noisy for surroundings, lowering the quality of life and property values in their immediate vicinity. Urban elevated trains were thus frequently opposed by residents.</p>



<!-- --><p>When Los Angeles was <!-- --><a href="https://www.construction-physics.com/p/how-the-car-came-to-la">considering a mass transit system</a> in the 1920s, it was opposed by citizen groups such as the Taxpayers’ Anti-Elevated League, and an LA reporter who researched elevated trains in other US cities <!-- --><a href="https://archive.org/details/losangelesautom00bott/page/152/mode/2up?q=devil">came away with the conclusion that</a> ‘an elevated is a many-legged and roaring steel serpent and should be shunned by all cities for the machination of the devil that it is’. Cities began to prefer subways to elevateds, despite the fact that they were two to four times as expensive to build as elevated trains. Since 1908, the US has only built one new urban elevated railway, in <!-- --><a href="https://en.wikipedia.org/wiki/Metrorail_(Miami-Dade_County)">Miami</a>, a city where the high water table makes underground construction difficult.<!-- --></p>



<!-- --><p>The US is partly an exception: dozens of elevated railways have been built across the developing and middle-income worlds in the past few decades. A handful have been built in developed countries, including the Docklands Light Railway in London, in the late 1980s, and the Yurikamome which connects Tokyo with Odaiba artificial island, in 1995. But both of these were built into virgin terrain as part of redevelopments, and residents came along later with their impacts already ‘priced in’. Both are also automated and electric, and run on concrete supports, making them much quieter than early 1900s elevateds (though the DLR can screech when it turns sharp corners).</p>



<!-- --><p>Cut and cover will sometimes be the best choice from a basic cost-benefit perspective, as will elevated rail, and both are useful to have in the tunnel construction toolbox for that reason. But technological deployment decisions are <!-- --><a href="https://worksinprogress.co/issue/why-skyscrapers-are-so-short">rarely governed purely by economics</a>. There’s always a broader calculus at work with any technology: a set of shifting norms, assumptions, culture, and institutions that governs what methods are considered acceptable for solving problems. Political costs and civic opposition will often make a technology unviable regardless of how the dollars and cents add up. Perhaps a policy will overcome this problem – if not, then cut and cover seems to be a casualty of this broader calculus, like so many technologies that have come before.<!-- --></p>
<!-- --></div></article><div><div id="reference-item-1"><p>1</p><p><span><em>722 Miles: The Building of the Subways and How They Transformed New York</em>, Clifton Hood 2004, p. 86</span></p></div><div id="reference-item-2"><p>2</p><p><span>Tracking down the costs of tunnel boring versus cut and cover for this project is difficult, but they appear to be similar. The initial, six-lane cut and cover tunnel was projected to cost </span><a href="https://web.archive.org/web/20061026053920/http://www.wsdot.wa.gov/Projects/Viaduct/Alternatives.htm"><span>$4.6 billion</span></a><span>. That was later cut down to a four-lane tunnel that would cost </span><a href="https://www.seattletimes.com/seattle-news/nickels-backing-for-4-lane-tunnel-lite-gets-cool-reception/"><span>$3.4 billion</span></a><span>. The ultimate budget of the bored tunnel, which used what at the time was the largest tunnel-boring machine in the world, was </span><a href="https://web.archive.org/web/20190629201419/https://www.wsdot.wa.gov/Projects/Viaduct/Budget"><span>$3.3 billion</span></a><span>. </span><a href="https://wsdot.wa.gov/sites/default/files/2021-05/AWV-PDF-FEIS-Summary.pdf"><span>Environmental impact documents</span></a><span> from the project also seem to show bored tunnel as having been cheaper than cut and cover, though making a direct comparison here is difficult since the projects have somewhat different scope.</span></p></div><div id="reference-item-3"><p>3</p><p><span>The previously mentioned French study found that in the most difficult ground conditions, tunneling costs per mile varied from ~250 million francs per mile to ~1,200 million francs per mile.</span></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LWM – Open LLM with 1M Tokens Context Window (108 pts)]]></title>
            <link>https://github.com/LargeWorldModel/LWM</link>
            <guid>39398631</guid>
            <pubDate>Fri, 16 Feb 2024 15:54:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/LargeWorldModel/LWM">https://github.com/LargeWorldModel/LWM</a>, See on <a href="https://news.ycombinator.com/item?id=39398631">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><h2 tabindex="-1" dir="auto">Large World Model (LWM)</h2>
<p dir="auto"><a href="https://largeworldmodel.github.io/" rel="nofollow">[Project]</a>
<a href="https://arxiv.org/abs/2402.08268" rel="nofollow">[Paper]</a>
<a href="https://huggingface.co/LargeWorldModel" rel="nofollow">[Models]</a></p>
<p dir="auto"><strong>Large World Model (LWM)</strong> is a general-purpose large-context multimodal autoregressive model. It is trained on a large dataset of diverse long videos and books using RingAttention, and can perform language, image, and video understanding and generation.</p>
<h2 tabindex="-1" dir="auto">Approach</h2>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/LargeWorldModel/LWM/blob/main/imgs/data.png"><img src="https://github.com/LargeWorldModel/LWM/raw/main/imgs/data.png"></a>
</p>
<p dir="auto">Current language models fall short in understanding aspects of the world not easily described in words, and struggle with complex, long-form tasks. Video sequences offer valuable temporal information absent in language and static images, making them attractive for joint modeling with language. Such models could develop a understanding of both human textual knowledge and the physical world, enabling broader AI capabilities for assisting humans. However, learning from millions of tokens of video and language sequences poses challenges due to memory constraints, computational complexity, and limited datasets. To address these challenges, we curate a large dataset of diverse videos and books, utilize the RingAttention technique to scalably train on long sequences, and gradually increase context size from 4K to 1M tokens. This paper makes the following contributions: (a) Largest context size neural network: We train one of the largest context size transformers on long video and language sequences, setting new benchmarks in difficult retrieval tasks and long video understanding. (b) Solutions for overcoming vision-language training challenges, including using masked sequence packing for mixing different sequence lengths, loss weighting to balance language and vision, and model-generated QA dataset for long sequence chat. (c) A highly-optimized implementation with RingAttention, masked sequence packing, and other key features for training on millions-length multimodal sequences. (d) Fully open-sourced a family of 7B parameter models capable of processing long text documents (LWM-Text, LWM-Text-Chat) and videos (LWM, LWM-Chat) of over 1M tokens.
This work paves the way for training on massive datasets of long video and language to develop understanding of both human knowledge and the multimodal world, and broader capabilities.</p>
<h2 tabindex="-1" dir="auto">LWM Capabilities</h2>
<div dir="auto">
  <p><a target="_blank" rel="noopener noreferrer" href="https://github.com/LargeWorldModel/LWM/blob/main/imgs/single_needle_1M.png"><img src="https://github.com/LargeWorldModel/LWM/raw/main/imgs/single_needle_1M.png"></a></p><p dir="auto">
  LWM can retrieval facts across 1M context with high accuracy.
  </p>
</div>
<br>
<div dir="auto">
  <p><a target="_blank" rel="noopener noreferrer" href="https://github.com/LargeWorldModel/LWM/blob/main/imgs/long_video_chat_main.png"><img src="https://github.com/LargeWorldModel/LWM/raw/main/imgs/long_video_chat_main.png"></a></p><p dir="auto">
  LWM can answer questions over 1 hour YouTube video.
  </p>
</div>
<br>
<div dir="auto">
  <p><a target="_blank" rel="noopener noreferrer" href="https://github.com/LargeWorldModel/LWM/blob/main/imgs/image_chat.png"><img src="https://github.com/LargeWorldModel/LWM/raw/main/imgs/image_chat.png"></a></p><p dir="auto">
  LWM can chat with images.
  </p>
</div>
<br>
<div dir="auto">
  <p><a target="_blank" rel="noopener noreferrer" href="https://github.com/LargeWorldModel/LWM/blob/main/imgs/image_video_gen.png"><img src="https://github.com/LargeWorldModel/LWM/raw/main/imgs/image_video_gen.png"></a></p><p dir="auto">
  LWM can generate videos and images from text.
  </p>
</div>
<h2 tabindex="-1" dir="auto">Setup</h2>
<p dir="auto">Install the requirements with:</p>
<div data-snippet-clipboard-copy-content="conda create -n lwm python=3.10
pip install -U &quot;jax[cuda12_pip]==0.4.23&quot; -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
pip install -r requirements.txt"><pre><code>conda create -n lwm python=3.10
pip install -U "jax[cuda12_pip]==0.4.23" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
pip install -r requirements.txt
</code></pre></div>
<p dir="auto">or set up TPU VM with:</p>

<h2 tabindex="-1" dir="auto">Available models</h2>
<p dir="auto">There are language-only and video-language versions, offering context sizes from 32K, to 128K, 256K and 1M tokens. The vision-language models are available only in Jax, and the language-only models are available in both PyTorch and Jax. Below are the names of the available models and their corresponding context sizes and capabilities:</p>
<table>
<thead>
<tr>
<th>Model Name</th>
<th>Context Size</th>
<th>Language or Vision-Language</th>
<th>Chat or Base</th>
<th>URL</th>
</tr>
</thead>
<tbody>
<tr>
<td>LWM-Text-Chat-128K</td>
<td>128K</td>
<td>Language</td>
<td>Chat</td>
<td>[<a href="https://huggingface.co/LargeWorldModel/LWM-Text-Chat-128K" rel="nofollow">Pytorch</a>][<a href="https://huggingface.co/LargeWorldModel/LWM-Text-Chat-128K-Jax" rel="nofollow">Jax</a>]</td>
</tr>
<tr>
<td>LWM-Text-Chat-256K</td>
<td>256K</td>
<td>Language</td>
<td>Chat</td>
<td>[<a href="https://huggingface.co/LargeWorldModel/LWM-Text-Chat-256K" rel="nofollow">Pytorch</a>][<a href="https://huggingface.co/LargeWorldModel/LWM-Text-Chat-256K-Jax" rel="nofollow">Jax</a>]</td>
</tr>
<tr>
<td>LWM-Text-Chat-512K</td>
<td>512K</td>
<td>Language</td>
<td>Chat</td>
<td>[<a href="https://huggingface.co/LargeWorldModel/LWM-Text-Chat-512K" rel="nofollow">Pytorch</a>][<a href="https://huggingface.co/LargeWorldModel/LWM-Text-Chat-512K-Jax" rel="nofollow">Jax</a>]</td>
</tr>
<tr>
<td>LWM-Text-Chat-1M</td>
<td>1M</td>
<td>Language</td>
<td>Chat</td>
<td>[<a href="https://huggingface.co/LargeWorldModel/LWM-Text-Chat-1M" rel="nofollow">Pytorch</a>][<a href="https://huggingface.co/LargeWorldModel/LWM-Text-Chat-1M-Jax" rel="nofollow">Jax</a>]</td>
</tr>
<tr>
<td>LWM-Text-128K</td>
<td>128K</td>
<td>Language</td>
<td>Base</td>
<td>[<a href="https://huggingface.co/LargeWorldModel/LWM-Text-128K" rel="nofollow">Pytorch</a>][<a href="https://huggingface.co/LargeWorldModel/LWM-Text-128K-Jax" rel="nofollow">Jax</a>]</td>
</tr>
<tr>
<td>LWM-Text-256K</td>
<td>256K</td>
<td>Language</td>
<td>Base</td>
<td>[<a href="https://huggingface.co/LargeWorldModel/LWM-Text-256K" rel="nofollow">Pytorch</a>][<a href="https://huggingface.co/LargeWorldModel/LWM-Text-256K-Jax" rel="nofollow">Jax</a>]</td>
</tr>
<tr>
<td>LWM-Text-512K</td>
<td>512K</td>
<td>Language</td>
<td>Base</td>
<td>[<a href="https://huggingface.co/LargeWorldModel/LWM-Text-512K" rel="nofollow">Pytorch</a>][<a href="https://huggingface.co/LargeWorldModel/LWM-Text-512K-Jax" rel="nofollow">Jax</a>]</td>
</tr>
<tr>
<td>LWM-Text-1M</td>
<td>1M</td>
<td>Language</td>
<td>Base</td>
<td>[<a href="https://huggingface.co/LargeWorldModel/LWM-Text-1M" rel="nofollow">Pytorch</a>][<a href="https://huggingface.co/LargeWorldModel/LWM-Text-1M-Jax" rel="nofollow">Jax</a>]</td>
</tr>
<tr>
<td>LWM-Chat-32K</td>
<td>32K</td>
<td>Vision-Language</td>
<td>Chat</td>
<td>[<a href="https://huggingface.co/LargeWorldModel/LWM-32K-Jax" rel="nofollow">Jax</a>]</td>
</tr>
<tr>
<td>LWM-Chat-128K</td>
<td>128K</td>
<td>Vision-Language</td>
<td>Chat</td>
<td>[<a href="https://huggingface.co/LargeWorldModel/LWM-128K-Jax" rel="nofollow">Jax</a>]</td>
</tr>
<tr>
<td>LWM-Chat-1M</td>
<td>1M</td>
<td>Vision-Language</td>
<td>Chat</td>
<td>[<a href="https://huggingface.co/LargeWorldModel/LWM-1M-Jax" rel="nofollow">Jax</a>]</td>
</tr>
</tbody>
</table>
<h2 tabindex="-1" dir="auto">Code structure</h2>
<p dir="auto">Use <code>scan_query_chunk_size</code> and <code>scan_key_chunk_size</code> to control the block size in blockwise compute of the self-attention. Use <code>scan_mlp_chunk_size</code> to control the block size in blockwise compute of the feedforward network. Use <code>scan_attention=True</code> and <code>scan_mlp=True</code> to enable/disable blockwise compute in the self-attention and feed-forward network. Use <code>remat_attention</code> and <code>remat_mlp</code> to control the rematerialization policy with <code>nothing_saveable</code> recommended.</p>
<p dir="auto">You can use <code>mesh_dim=dp, fsdp, tp, sp</code> to control the degree of parallelism and RingAttention. It is a string of 4 integers separated by commas, representing the number of data parallelism, fully sharded data parallelism, tensor parallelism, and sequence parallelism.
For example, <code>mesh_dim='1,64,4,1'</code> means 1 data parallelism, 64 fully sharded data parallelism, 4 tensor parallelism, and 1 sequence parallelism. <code>mesh_dim='1,1,4,64'</code> means 1 data parallelism, 1 fully sharded data parallelism, 4 tensor parallelism, and 64 sequence parallelism for RingAttention.</p>
<h2 tabindex="-1" dir="auto">Command-line usage</h2>
<p dir="auto">In this section, we provide instructions on how to run each of the provided scripts. For each script, you may need to fill in your own paths and values in the variables described in the beginning of each script.</p>
<p dir="auto">To run each of the following scripts, use <code>bash &lt;script_name&gt;.sh</code>:</p>
<ul dir="auto">
<li>Language model training: <code>bash scripts/run_train_text.sh</code></li>
<li>Vision-Language model training: <code>bash scripts/run_train_vision_text.sh</code></li>
<li>Single Needle Evals (Language Model): <code>bash scripts/run_eval_needle.sh</code></li>
<li>Multi Needle Evals (Language Model): <code>bash scripts/run_eval_needle_multi.sh</code></li>
<li>Sampling images (Vision-Language Model): <code>bash scripts/run_sample_image.sh</code></li>
<li>Sampling videos (Vision-LanguageModel): <code>bash scripts/run_sample_video.sh</code></li>
<li>Image / Video understanding (Vision-Language Model): <code>bash scripts/run_vision_chat.sh</code></li>
</ul>
<h2 tabindex="-1" dir="auto">If you have issues</h2>
<p dir="auto">This is based on the <a href="https://github.com/lhao499/ring-attention">codebase</a> of BPT and RingAttention, with the necessary features for vision-language training. The training and inference have been tested on both TPUv3 and TPUv4.</p>
<p dir="auto">If you encounter bugs, please open a GitHub issue!</p>
<h2 tabindex="-1" dir="auto">Citation</h2>
<p dir="auto">If you use this codebase, or otherwise found our work valuable, please cite:</p>
<div data-snippet-clipboard-copy-content="@article{liu2023world,
    title={World Model on Million-Length Video and Language with RingAttention},
    author={Liu, Hao and Yan, Wilson and Zaharia, Matei and Abbeel, Pieter},
    journal={arXiv preprint},
    year={2024},
}
@article{liu2023ring,
    title={Ring Attention with Blockwise Transformers for Near-Infinite Context},
    author={Liu, Hao and Zaharia, Matei and Abbeel, Pieter},
    journal={International Conference on Learning Representations},
    year={2024}
}
@article{liu2023blockwise,
    title={Blockwise Parallel Transformer for Large Context Models},
    author={Liu, Hao and Abbeel, Pieter},
    journal={Advances in neural information processing systems},
    year={2023}
}"><pre><code>@article{liu2023world,
    title={World Model on Million-Length Video and Language with RingAttention},
    author={Liu, Hao and Yan, Wilson and Zaharia, Matei and Abbeel, Pieter},
    journal={arXiv preprint},
    year={2024},
}
@article{liu2023ring,
    title={Ring Attention with Blockwise Transformers for Near-Infinite Context},
    author={Liu, Hao and Zaharia, Matei and Abbeel, Pieter},
    journal={International Conference on Learning Representations},
    year={2024}
}
@article{liu2023blockwise,
    title={Blockwise Parallel Transformer for Large Context Models},
    author={Liu, Hao and Abbeel, Pieter},
    journal={Advances in neural information processing systems},
    year={2023}
}
</code></pre></div>
<h2 tabindex="-1" dir="auto">License</h2>
<p dir="auto">LWM's code and model weights are released under the Apache 2.0 License. See <a href="https://github.com/LargeWorldModel/lwm/blob/main/LICENSE">LICENSE</a> for further details.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I'm an Old Fart and AI Makes Me Sad (173 pts)]]></title>
            <link>https://medium.com/@alex.suzuki/im-an-old-fart-and-ai-makes-me-sad-06003bfb6750</link>
            <guid>39398481</guid>
            <pubDate>Fri, 16 Feb 2024 15:42:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://medium.com/@alex.suzuki/im-an-old-fart-and-ai-makes-me-sad-06003bfb6750">https://medium.com/@alex.suzuki/im-an-old-fart-and-ai-makes-me-sad-06003bfb6750</a>, See on <a href="https://news.ycombinator.com/item?id=39398481">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><a rel="noopener follow" href="https://medium.com/@alex.suzuki?source=post_page-----06003bfb6750--------------------------------"><div aria-hidden="false"><p><img alt="Alex Suzuki" src="https://miro.medium.com/v2/resize:fill:88:88/1*4OAF_wW7N0Fe-p4dfSLVlA@2x.jpeg" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a></div><p id="5f8e">Previous major advancements in Computer Science and its applications always felt empowering to me, AI feels different.</p><figure><figcaption>Dall-E 3, “A drawing of a software engineer overwhelmed by recent advances in Artificial Intelligence”</figcaption></figure><p id="b736">To find out why I feel this way, I had to look into the past couple of decades and compare the major technological advances to today’s movements.</p><h2 id="e41c">Personal Computers [80s]</h2><p id="dde9">PCs were my gateway drug into technology — they got me into gaming and eventually programming. I have fond memories using tools like MEMMAKER to squeeze more memory from my machine just so I could run a pirated copy of some new game on my DOS box.</p><p id="6612">PCs came in different shapes, sizes and architectures. They had many names and were sold by different vendors. You were mostly free to do what you wanted with these machines. You could swap out parts. You could run games, you could write software, or do something totally different with them.</p><p id="3807">From an engineer’s viewpoint, the inner workings of these machines were comprehensible. With some skill and dedication, you could even build your own.</p><h2 id="228d">World Wide Web (a.k.a. The Internet) [90s]</h2><p id="22c4">Suddenly the bandwidth of your internet connection was more interesting than the clock rate of your computer. I spent hours in Internet Cafés chatting with strangers, who like me, were exploring this new and strange medium.</p><p id="7c6c">The early Internet felt accessible to me. It was built on open, interoperable protocols like TCP/IP, HTTP, SMTP etc. You could buy a book or visit SELFHTML, open an account at Geocities and create a website. If Geocities went down, you could move your website somewhere else, or even self-host. No single company or entity owned a significant chunk of the Internet.</p><h2 id="e9da">Smartphones [mid-2000s]</h2><p id="ec3c">A new form factor and interaction paradigm is introduced. Even though we saw similarly sized devices and touch screens before, the iPhone was the first device that tied it all together, helped by ubiquitous mobile internet access.</p><p id="e106">Smartphones are, for the most part, accessible in the same way as other computers are. They are small, connected, programmable computers with a wide range of input sensors.</p><p id="727d">I understand how smartphones work, I can program them, I know their limitations.</p><h2 id="6710">How AI is Different</h2><p id="69fb">Don’t get me wrong, I’m excited about AI and blown away by stuff like the just announced <a href="https://openai.com/sora" rel="noopener ugc nofollow" target="_blank">Sora</a> text-to-video model. But at the same time, I feel left out.</p><h2 id="cb6a"><strong>AI is Opaque</strong></h2><p id="a38d">I want to understand how things work. AI feels like a black box to me. The amount of papers I’d have to read and mathematics that I’d have to ingest to really understand why a certain prompt X results in a certain output Y feels overwhelming. Even some top scientists in the field admit that we don’t really understand how AI works.</p><blockquote><p id="c88a">“If we open up ChatGPT or a system like it and look inside, you just see millions of numbers flipping around a few hundred times a second,” says AI scientist <a href="https://cims.nyu.edu/~sbowman/" rel="noopener ugc nofollow" target="_blank">Sam Bowman</a>. “And we just have no idea what any of it means.”</p></blockquote><p id="4113">To me as an engineer, that is just incredibly <strong>unsatisfying</strong>. Without understanding how something works, we are doomed to be just users.</p><h2 id="5c8e">AI is Not Approachable</h2><p id="dbb2">Sure, on the outside it is — anyone and their dog can open a ChatGPT session or throw some JSON at OpenAI’s APIs. What I’m talking about is gaining access to the core technological foundations that make AI possible.</p><p id="dd90">Flipping all those numbers to get the result (inference), and especially determining those numbers in the first place (training), requires a vast amounts of resources, data and skill.</p><p id="ca97">AI is not a shovel for little people.</p><h2 id="b142">AI is Not Open</h2><p id="d90d">If you’re like me, you might have some ideas for cool stuff to build with AI. But in doing so, chances are you’ll end up building a <em>GPT Wrapper</em>.</p><p id="6efc">What is a GPT Wrapper, you might ask? It’s any bit of software, SaaS, whatever you want to call it, that relies on someone else’s AI product that you can’t easily replicate or exchange (ChatGPT, for instance).</p><p id="f23a">If I build an app that needs persistence, I might use Postgres and S3 for storing data. If those are no longer available, I’ll use another relational database, key-value store, distributed filesystem, whatever. But what if OpenAI decides to revoke access to that API feature I’m using? What if they change pricing and make it uneconomical to run? What if OpenAI extends their offering and makes my product redundant?</p><figure><figcaption>Old man yells at cloud (credits: <a href="https://knowyourmeme.com/memes/old-man-yells-at-cloud" rel="noopener ugc nofollow" target="_blank">https://knowyourmeme.com/memes/old-man-yells-at-cloud</a>)</figcaption></figure><p id="0ff5">So am I just like that angry old man shaking his fist at the Cloud in anger? I hope not.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[My Sixth Year as a Bootstrapped Founder (409 pts)]]></title>
            <link>https://mtlynch.io/solo-developer-year-6/</link>
            <guid>39398009</guid>
            <pubDate>Fri, 16 Feb 2024 15:07:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mtlynch.io/solo-developer-year-6/">https://mtlynch.io/solo-developer-year-6/</a>, See on <a href="https://news.ycombinator.com/item?id=39398009">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Six years ago, I <a href="https://mtlynch.io/why-i-quit-google/">quit my job as a developer at Google</a> to create my own bootstrapped software company.</p><p>For the first few years, all of my businesses flopped. The best of them earned a few hundred dollars per month in revenue, but none were profitable.</p><p>Halfway through my third year, I created a device called <a href="https://tinypilotkvm.com/">TinyPilot</a>. It allows users to control their computers remotely. The product quickly caught on, and it’s been my main focus ever since.</p><figure><div><p><a href="https://mtlynch.io/solo-developer-year-6/2a-front.webp"><img sizes="(min-width: 768px) 400px, 98vw" srcset="https://mtlynch.io/solo-developer-year-6/2a-front_hu4a4fe4485c4f5b136908c096ea5f50a0_50606_300x0_resize_q90_h2_lanczos_2.webp 300w,
https://mtlynch.io/solo-developer-year-6/2a-front_hu4a4fe4485c4f5b136908c096ea5f50a0_50606_600x0_resize_q90_h2_lanczos_2.webp 600w,
https://mtlynch.io/solo-developer-year-6/2a-front_hu4a4fe4485c4f5b136908c096ea5f50a0_50606_800x0_resize_q90_h2_lanczos_2.webp 800w,
https://mtlynch.io/solo-developer-year-6/2a-front.webp 800w" src="https://mtlynch.io/solo-developer-year-6/2a-front.webp" alt="Front view of TinyPilot Voyager 2a device" loading="lazy"></a></p><p><a href="https://mtlynch.io/solo-developer-year-6/tinypilot-bios-menu-2.webp"><img sizes="(min-width: 768px) 500px, 98vw" srcset="https://mtlynch.io/solo-developer-year-6/tinypilot-bios-menu-2_hu56211ece14bdb48ec8faba25fba3903f_92902_300x0_resize_q90_h2_lanczos_2.webp 300w,
https://mtlynch.io/solo-developer-year-6/tinypilot-bios-menu-2_hu56211ece14bdb48ec8faba25fba3903f_92902_600x0_resize_q90_h2_lanczos_2.webp 600w,
https://mtlynch.io/solo-developer-year-6/tinypilot-bios-menu-2_hu56211ece14bdb48ec8faba25fba3903f_92902_800x0_resize_q90_h2_lanczos_2.webp 800w,
https://mtlynch.io/solo-developer-year-6/tinypilot-bios-menu-2_hu56211ece14bdb48ec8faba25fba3903f_92902_1200x0_resize_q90_h2_lanczos_2.webp 1200w,
https://mtlynch.io/solo-developer-year-6/tinypilot-bios-menu-2.webp 1515w" src="https://mtlynch.io/solo-developer-year-6/tinypilot-bios-menu-2.webp" alt="Screenshot of TinyPilot web interface" loading="lazy"></a></p></div><figcaption><p>TinyPilot is a small device that allows users to control their computers remotely.</p></figcaption></figure><p>In 2023, TinyPilot generated $997k in revenue, which I’ll generously round up to a cool million. More importantly, the business earned $236k in profit, a 20x increase from 2022.</p><p>In this post, I’ll share what I’ve learned about being a bootstrapped founder from my sixth year doing it.</p><h2 id="previous-updates">Previous updates<a href="#previous-updates" arialabel="Anchor"> 🔗︎</a></h2><ul><li><a href="https://mtlynch.io/solo-developer-year-1/">My First Year as a Solo Developer</a></li><li><a href="https://mtlynch.io/solo-developer-year-2/">My Second Year as a Solo Developer</a></li><li><a href="https://mtlynch.io/solo-developer-year-3/">My Third Year as a Solo Developer</a></li><li><a href="https://mtlynch.io/solo-developer-year-4/">My Fourth Year as a Bootstrapped Founder</a></li><li><a href="https://mtlynch.io/solo-developer-year-5/">My Fifth Year as a Bootstrapped Founder</a></li></ul><h2 id="tinypilot-became-20x-more-profitable">TinyPilot became 20x more profitable<a href="#tinypilot-became-20x-more-profitable" arialabel="Anchor"> 🔗︎</a></h2><table><thead><tr><th>Income/Expense</th><th>2022</th><th>2023</th><th>Change</th></tr></thead><tbody><tr><td>Sales Revenue</td><td>$807,458</td><td>$992,597</td><td><span blog-purpose="delta">enable JS to see delta</span></td></tr><tr><td>Credit Card Rewards</td><td>$4,327</td><td>$4,379</td><td><span blog-purpose="delta">enable JS to see delta</span></td></tr><tr><td><strong>Total Income</strong></td><td><strong>$811,785</strong></td><td><strong>$996,976</strong></td><td><strong><span blog-purpose="delta">enable JS to see delta</span></strong></td></tr><tr><td>Advertising</td><td>$51,764</td><td>$39,270</td><td><span blog-purpose="delta">enable JS to see delta</span></td></tr><tr><td>Cloud Services</td><td>$9,151</td><td>$16,408</td><td><span blog-purpose="delta">enable JS to see delta</span></td></tr><tr><td>Design Consulting</td><td>$30,215</td><td>$950</td><td><span blog-purpose="delta">enable JS to see delta</span></td></tr><tr><td>Electrical Engineering Consulting</td><td>$124,643</td><td>$23,427</td><td><span blog-purpose="delta">enable JS to see delta</span></td></tr><tr><td>Fulfillment Vendors</td><td>$0</td><td>$28,321</td><td><span blog-purpose="delta">enable JS to see delta</span></td></tr><tr><td>Office Rent</td><td>$6,600</td><td>$6,310</td><td><span blog-purpose="delta">enable JS to see delta</span></td></tr><tr><td>Payroll</td><td>$205,984</td><td>$255,779</td><td><span blog-purpose="delta">enable JS to see delta</span></td></tr><tr><td>Postage</td><td>$28,324</td><td>$16,853</td><td><span blog-purpose="delta">enable JS to see delta</span></td></tr><tr><td>Raw Materials</td><td>$324,140</td><td>$358,457</td><td><span blog-purpose="delta">enable JS to see delta</span></td></tr><tr><td>Everything Else</td><td>$25,398</td><td>$31,404</td><td><span blog-purpose="delta">enable JS to see delta</span></td></tr><tr><td><strong>Total Expenses</strong></td><td><strong>$806,219</strong></td><td><strong>$777,179</strong></td><td><strong><span blog-purpose="delta">enable JS to see delta</span></strong></td></tr><tr><td><strong>Net Profit</strong></td><td><strong>$10,447</strong></td><td><strong>$235,568</strong></td><td><strong><span blog-purpose="delta">enable JS to see delta</span></strong></td></tr></tbody></table><p>After two years of basically breaking even, TinyPilot finally earned a meaningful profit.</p><p>Most of the change is due to stronger sales. We switched to metal cases this year, which both increased the price customers were willing to pay and increased our manufacturing capacity.</p><p>Expenses shifted around but stayed roughly the same overall. Design costs shrunk to nearly zero, as I <a href="https://mtlynch.io/tinypilot-redesign/">stopped paying a design agency $6k/mo to tweak my logo</a>. I focused on scaling my existing product rather than iterating on the hardware design, which reduced my electrical engineering costs by $100k.</p><p>I don’t draw a salary, so the total amount I earned from TinyPilot in 2023 was $236k. People often wonder how I survived on the meager earnings of my first five bootstrapper years. The answer is that I live in Western Massachusetts, where the cost of living is low. I had savings in index funds from years working in big tech, and those investments generated enough dividend income to sustain me.</p><h2 id="the-most-terrifying-10-minutes-of-2023">The most terrifying 10 minutes of 2023<a href="#the-most-terrifying-10-minutes-of-2023" arialabel="Anchor"> 🔗︎</a></h2><p>One lazy Saturday afternoon in February, I heard a knock on my door. Standing on my porch was a mid-fifties guy in jeans and a windbreaker. I opened the door, still in my pajamas.</p><p>“Are you the TinyPilot guy?” he asked me.</p><p>“Uh oh,” I thought. Did a disgruntled customer find my house?</p><p>“Yes…” I said cautiously.</p><p>“I’m the handyman at the office. A sprinkler burst, and we can’t get into your suite. Can you come down?”</p><p>That didn’t sound good.</p><p>During my five-minute drive to the office, I wondered if this was the end of my business. We kept all of our inventory in TinyPilot’s office. Would circuit boards work after being drenched? Probably not.</p><p>TinyPilot had insurance, but I chose coverage a year before when we carried half as much inventory. And even if insurance paid out, TinyPilot would be dead in the water for months until we could restart our whole manufacturing pipeline.</p><p>I arrived at the building and walked up to TinyPilot’s office on the second floor, the carpet squishing damply with every step I took.</p><p>When I reached my floor, I was relieved to see that the sprinkler had actually burst in the shared conference room, not TinyPilot’s suite. I unlocked our office and found everything was bone dry. The water hadn’t even trickled under our door.</p><p>My relief was short-lived, as the landlord told me he might have to kick us out for “weeks to months” to repair the wall we shared with the conference room.</p><figure><a href="https://mtlynch.io/solo-developer-year-6/office-damage.webp"><img sizes="(min-width: 768px) 450px, 98vw" srcset="https://mtlynch.io/solo-developer-year-6/office-damage_hu95bc095e58acca5e44e2d603d11dd552_115556_300x0_resize_q90_h2_lanczos_2.webp 300w,
https://mtlynch.io/solo-developer-year-6/office-damage_hu95bc095e58acca5e44e2d603d11dd552_115556_600x0_resize_q90_h2_lanczos_2.webp 600w,
https://mtlynch.io/solo-developer-year-6/office-damage_hu95bc095e58acca5e44e2d603d11dd552_115556_800x0_resize_q90_h2_lanczos_2.webp 800w,
https://mtlynch.io/solo-developer-year-6/office-damage_hu95bc095e58acca5e44e2d603d11dd552_115556_1200x0_resize_q90_h2_lanczos_2.webp 1200w,
https://mtlynch.io/solo-developer-year-6/office-damage.webp 1200w" src="https://mtlynch.io/solo-developer-year-6/office-damage.webp" alt="Photo of a room with ceiling, carpets, and furniture all removed" loading="lazy"></a><figcaption><p>A sprinkler burst in the office adjacent to TinyPilot’s, destroying everything inside.</p></figcaption></figure><p>Normally, being forced to move my entire office on a few days’ notice would be disruptive, but it was <em>especially</em> disruptive this week. I was about to take a two-week trip to Europe, my longest travel since starting TinyPilot.</p><p>If the team had to move while I was away, no one would be able to set up the computers or printers — the office IT guy was me. And if the team couldn’t print shipping labels, they couldn’t fulfill orders.</p><p>Long story short, we ended up not having to move, but the experience made me never want to be in that situation again. I was risking so much by centralizing TinyPilot’s operations in a single, small office.</p><h2 id="outsourcing-order-fulfillment-and-reducing-stress">Outsourcing order fulfillment and reducing stress<a href="#outsourcing-order-fulfillment-and-reducing-stress" arialabel="Anchor"> 🔗︎</a></h2><p>TinyPilot’s order fulfillment had always been extremely smooth, which was why I’d always procrastinated outsourcing it. Out of 3,500+ orders in the past two years, there were only about five where we sent a customer the wrong item.</p><p>In March 2023, TinyPilot switched from fulfilling orders in-house to <a href="https://mtlynch.io/retrospectives/2023/04/">using a third-party logistics (3PL) warehouse</a>. We were still assembling devices at our office, but we’d send customer-ready packages to the warehouse in bulk, and the 3PL would handle the final step of shipping orders to customers.</p><p>At the time of the 3PL shift, we were in <a href="https://mtlynch.io/retrospectives/2023/05/#getting-out-of-ldquourgent-moderdquo">“urgent mode.”</a> Our team of two part-time employees assembled about fifty devices per week, but customers were buying at the same rate. It was a stressful situation because any interruption put us at risk of halting sales.</p><p>I hoped that outsourcing fulfillment would free up enough of the team’s time to produce about 100 devices per week. It turned out that our full capacity was only about 70. At that rate, it would take months of working at maximum capacity to build up a healthy inventory at the warehouse. I ended up hiring a third employee temporarily to get us through the summer.</p><p>So, outsourcing fulfillment didn’t free up a ton of time, but it did win us a lot more flexibility.</p><p>The local team seemed to have flexibility already because they could come in whenever they wanted. As long as orders were packed and ready for mail pickup the next day, they could take their shifts at 3 AM if they felt like it.</p><p>Switching to the 3PL eliminated the daily deadline of mail pickup. Instead, our only deadline was to ship assembled products to our warehouse once a week.</p><p>The increase in flexibility reduced a lot of stress. If an employee wanted to take a four-day weekend, they could shift their schedule around and still work their normal 15 hours that week. Or they could take a few days off and <a href="https://mtlynch.io/solo-developer-year-5/#run-at-50-capacity">not feel like it was overloading the rest of the team</a>.</p><h2 id="making-tinypilot-look-like-a-real-product">Making TinyPilot look like a real product<a href="#making-tinypilot-look-like-a-real-product" arialabel="Anchor"> 🔗︎</a></h2><p>One of the most notable changes to TinyPilot in 2023 was how we improved the product’s physical appearance.</p><p>At the end of 2022, we were still making TinyPilot’s cases with a fleet of seven high-end 3D printers running nonstop. As far as 3D printing goes, our cases were especially nice, but they still had the “just a prototype” feel of a 3D-printed product.</p><figure><a href="https://mtlynch.io/solo-developer-year-6/voyager2-angled.webp"><img sizes="(min-width: 768px) 500px, 98vw" srcset="https://mtlynch.io/solo-developer-year-6/voyager2-angled_hua7e845be8e8389e043efb90824d7e2b7_79900_300x0_resize_q90_h2_lanczos_2.webp 300w,
https://mtlynch.io/solo-developer-year-6/voyager2-angled_hua7e845be8e8389e043efb90824d7e2b7_79900_600x0_resize_q90_h2_lanczos_2.webp 600w,
https://mtlynch.io/solo-developer-year-6/voyager2-angled_hua7e845be8e8389e043efb90824d7e2b7_79900_800x0_resize_q90_h2_lanczos_2.webp 800w,
https://mtlynch.io/solo-developer-year-6/voyager2-angled.webp 988w" src="https://mtlynch.io/solo-developer-year-6/voyager2-angled.webp" alt="TinyPilot in a black plastic 3D-printed case" loading="lazy"></a><figcaption><p>Before: TinyPilot’s 3D-printed case</p></figcaption></figure><p>In February 2023, we <a href="https://tinypilotkvm.com/blog/introducing-voyager-2a">switched to a metal case</a>.</p><figure><a href="https://mtlynch.io/solo-developer-year-6/metal-case.webp"><img sizes="(min-width: 768px) 500px, 98vw" srcset="https://mtlynch.io/solo-developer-year-6/metal-case_hu6cae36b8bc4d8e2a596166b9a2a6b928_108752_300x0_resize_q90_h2_lanczos_2.webp 300w,
https://mtlynch.io/solo-developer-year-6/metal-case_hu6cae36b8bc4d8e2a596166b9a2a6b928_108752_600x0_resize_q90_h2_lanczos_2.webp 600w,
https://mtlynch.io/solo-developer-year-6/metal-case_hu6cae36b8bc4d8e2a596166b9a2a6b928_108752_800x0_resize_q90_h2_lanczos_2.webp 800w,
https://mtlynch.io/solo-developer-year-6/metal-case_hu6cae36b8bc4d8e2a596166b9a2a6b928_108752_1200x0_resize_q90_h2_lanczos_2.webp 1200w,
https://mtlynch.io/solo-developer-year-6/metal-case.webp 1200w" src="https://mtlynch.io/solo-developer-year-6/metal-case.webp" alt="TinyPilot in a new metal case" loading="lazy"></a><figcaption><p>After: TinyPilot’s metal case case</p></figcaption></figure><p>I was surprised at how much the metal case impacted sales. Not only did it increase the absolute number of sales, it increased the price customers were willing to pay. After <a href="https://mtlynch.io/retrospectives/2023/05/#what-price-maximizes-profits">experimenting with pricing</a>, I ended up increasing our price by 10%, and our monthly sales were still higher than when we had a 3D-printed case.</p><p>We also updated TinyPilot’s packaging. Until late last year, we were still bunching the device and all the cables together in a bubble wrap pouch and dropping that into a plain brown box.</p><figure><div><p><a href="https://mtlynch.io/solo-developer-year-6/labeled-blob.webp"><img sizes="(min-width: 768px) 500px, 98vw" srcset="https://mtlynch.io/solo-developer-year-6/labeled-blob_hueffda2ad33839f8bf55e2bbf404000c0_994572_300x0_resize_q90_h2_lanczos_2.webp 300w,
https://mtlynch.io/solo-developer-year-6/labeled-blob_hueffda2ad33839f8bf55e2bbf404000c0_994572_600x0_resize_q90_h2_lanczos_2.webp 600w,
https://mtlynch.io/solo-developer-year-6/labeled-blob_hueffda2ad33839f8bf55e2bbf404000c0_994572_800x0_resize_q90_h2_lanczos_2.webp 800w,
https://mtlynch.io/solo-developer-year-6/labeled-blob_hueffda2ad33839f8bf55e2bbf404000c0_994572_1200x0_resize_q90_h2_lanczos_2.webp 1200w,
https://mtlynch.io/solo-developer-year-6/labeled-blob.webp 3722w" src="https://mtlynch.io/solo-developer-year-6/labeled-blob.webp" alt="Overhead view of TinyPilot, instructions, and cables in a bubble pouch" loading="lazy"></a></p><p><a href="https://mtlynch.io/solo-developer-year-6/bundle-stacked.webp"><img sizes="(min-width: 768px) 370px, 98vw" srcset="https://mtlynch.io/solo-developer-year-6/bundle-stacked_huab89ca4d7f98fed9bcb0302b344df50a_378128_300x0_resize_q90_h2_lanczos_2.webp 300w,
https://mtlynch.io/solo-developer-year-6/bundle-stacked_huab89ca4d7f98fed9bcb0302b344df50a_378128_600x0_resize_q90_h2_lanczos_2.webp 600w,
https://mtlynch.io/solo-developer-year-6/bundle-stacked_huab89ca4d7f98fed9bcb0302b344df50a_378128_800x0_resize_q90_h2_lanczos_2.webp 800w,
https://mtlynch.io/solo-developer-year-6/bundle-stacked.webp 1098w" src="https://mtlynch.io/solo-developer-year-6/bundle-stacked.webp" alt="Stack of TinyPilot bubble pouches on a shelf" loading="lazy"></a></p></div><figcaption><p>Our previous packaging for TinyPilot was just neatly wrapping the device, cables, and instructions in a bubble pouch.</p></figcaption></figure><p>Every time a reviewer shared their experience unboxing TinyPilot, I winced a little.</p><figure><a href="https://mtlynch.io/solo-developer-year-6/unboxing.webp"><img sizes="(min-width: 768px) 700px, 98vw" srcset="https://mtlynch.io/solo-developer-year-6/unboxing_hu24e0390e473c337b3cb9f085daf2d365_89166_300x0_resize_q90_h2_lanczos_2.webp 300w,
https://mtlynch.io/solo-developer-year-6/unboxing_hu24e0390e473c337b3cb9f085daf2d365_89166_600x0_resize_q90_h2_lanczos_2.webp 600w,
https://mtlynch.io/solo-developer-year-6/unboxing_hu24e0390e473c337b3cb9f085daf2d365_89166_800x0_resize_q90_h2_lanczos_2.webp 800w,
https://mtlynch.io/solo-developer-year-6/unboxing_hu24e0390e473c337b3cb9f085daf2d365_89166_1200x0_resize_q90_h2_lanczos_2.webp 1200w,
https://mtlynch.io/solo-developer-year-6/unboxing.webp 1210w" src="https://mtlynch.io/solo-developer-year-6/unboxing.webp" alt="Screenshot of review from noted.lol showing TinyPilot's old packaging in plain brown box" loading="lazy"></a><figcaption><p>A <a href="https://noted.lol/tinypilot-voyager-2a-2/">homelab reviewer</a> shows TinyPilot’s old, embarrassing packaging in a review</p></figcaption></figure><p>I’d had a few conversations with designers about making a nice retail box for the product, but it never came together, and it was never my top priority. After we switched to metal cases, TinyPilot’s packaging stood out as particularly immature.</p><p>In the second half of 2023, we worked with a contract manufacturer to take over our entire production process. As part of that work, they offered to make a retail box for us.</p><figure><div><p><a href="https://mtlynch.io/solo-developer-year-6/box-angled.webp"><img sizes="(min-width: 768px) 400px, 98vw" srcset="https://mtlynch.io/solo-developer-year-6/box-angled_hu15801411bd490c3784918bd0271fef49_84342_300x0_resize_q90_h2_lanczos_2.webp 300w,
https://mtlynch.io/solo-developer-year-6/box-angled_hu15801411bd490c3784918bd0271fef49_84342_600x0_resize_q90_h2_lanczos_2.webp 600w,
https://mtlynch.io/solo-developer-year-6/box-angled_hu15801411bd490c3784918bd0271fef49_84342_800x0_resize_q90_h2_lanczos_2.webp 800w,
https://mtlynch.io/solo-developer-year-6/box-angled_hu15801411bd490c3784918bd0271fef49_84342_1200x0_resize_q90_h2_lanczos_2.webp 1200w,
https://mtlynch.io/solo-developer-year-6/box-angled.webp 1200w" src="https://mtlynch.io/solo-developer-year-6/box-angled.webp" alt="Angled view of TinyPilot branded box, closed" loading="lazy"></a></p><p><a href="https://mtlynch.io/solo-developer-year-6/box-open.webp"><img sizes="(min-width: 768px) 400px, 98vw" srcset="https://mtlynch.io/solo-developer-year-6/box-open_hu7a3285e4b69839f495f8076cc3a20cde_84940_300x0_resize_q90_h2_lanczos_2.webp 300w,
https://mtlynch.io/solo-developer-year-6/box-open_hu7a3285e4b69839f495f8076cc3a20cde_84940_600x0_resize_q90_h2_lanczos_2.webp 600w,
https://mtlynch.io/solo-developer-year-6/box-open_hu7a3285e4b69839f495f8076cc3a20cde_84940_800x0_resize_q90_h2_lanczos_2.webp 800w,
https://mtlynch.io/solo-developer-year-6/box-open_hu7a3285e4b69839f495f8076cc3a20cde_84940_1200x0_resize_q90_h2_lanczos_2.webp 1200w,
https://mtlynch.io/solo-developer-year-6/box-open.webp 1200w" src="https://mtlynch.io/solo-developer-year-6/box-open.webp" alt="Front view of TinyPilot box open with components organized inside" loading="lazy"></a></p></div><figcaption><p>TinyPilot’s new contract manufacturer made a branded retail box for our product.</p></figcaption></figure><p>Our contract manufacturer did a great job on the box. It’s not going to catch your eye if it was on the shelf at Best Buy, but it feels like professional packaging for a networking hardware product.</p><h2 id="lessons-learned">Lessons learned<a href="#lessons-learned" arialabel="Anchor"> 🔗︎</a></h2><h3 id="theres-hidden-stress-in-low-latency-responsibility">There’s hidden stress in low-latency responsibility<a href="#theres-hidden-stress-in-low-latency-responsibility" arialabel="Anchor"> 🔗︎</a></h3><p>Switching TinyPilot’s order fulfillment to a 3PL <a href="#outsourcing-order-fulfillment-and-reducing-stress">reduced stress and increased flexibility</a> for TinyPilot’s local team, but I was most surprised at how drastically it relieved stress for me.</p><p>I’d been carrying around so much “what if?” anxiety for years without even realizing it.</p><p>Before we switched to the 3PL, there was always a worry in the back of my mind about all the things that could block order fulfillment. What if our office router crashes and prevents anyone from accessing the Internet? What if the desktop suddenly can’t talk to the printer? There were dozens of ways I might be called to unblock a critical process urgently.</p><p>Now that we’ve shifted to a 3PL and a contract manufacturer, there are still many things that can go wrong, but I’m outside the critical path of most day-to-day operations. If a printer breaks at our warehouse, someone else will fix it, and I’ll hopefully never even hear about it.</p><h3 id="as-a-project-matures-more-time-goes-into-maintenance">As a project matures, more time goes into maintenance<a href="#as-a-project-matures-more-time-goes-into-maintenance" arialabel="Anchor"> 🔗︎</a></h3><p>In June, when I sat down to write <a href="https://tinypilotkvm.com/pro/changes#260">the changelog</a> for the latest TinyPilot software update, I struggled to explain how any of the work we did benefitted our users. Assuming I overdid it on refactoring work, I resolved to make our next release more user-focused.</p><p>When it came time to announce the <a href="https://tinypilotkvm.com/pro/changes#261">next update</a>, I had the same problem. After two and a half months of development, all we had to show for it were small, cosmetic improvements.</p><p>Our current pace felt glacial compared to the early days when we were releasing major features every couple of months. Was I prioritizing tasks poorly? Had the team lost their enthusiasm? Had we taken on too much technical debt?</p><p>I reviewed the complete list of tasks for the release, including all the work that wasn’t visible to end-users. Even with the benefit of hindsight, I felt like I had chosen the right tasks. And the time we invested in each task felt reasonable as well.</p><figure><a href="https://mtlynch.io/solo-developer-year-6/three-category-2.6.1.webp"><img sizes="(min-width: 768px) 350px, 98vw" srcset="https://mtlynch.io/solo-developer-year-6/three-category-2.6.1_hud46bdabb41af542e3ad5855636d86d43_26018_300x0_resize_q90_h2_lanczos_2.webp 300w,
https://mtlynch.io/solo-developer-year-6/three-category-2.6.1_hud46bdabb41af542e3ad5855636d86d43_26018_600x0_resize_q90_h2_lanczos_2.webp 600w,
https://mtlynch.io/solo-developer-year-6/three-category-2.6.1.webp 607w" src="https://mtlynch.io/solo-developer-year-6/three-category-2.6.1.webp" alt="A screenshot of TinyPilot's dev tasks for 2.6.1 release" loading="lazy"></a><figcaption><p>The tasks in TinyPilot’s <a href="https://tinypilotkvm.com/pro/changes#261">2.6.1 release</a>, colored according to improving the product (green), automation and reducing complexity (blue), and regular maintenance (red)</p></figcaption></figure><p>So, how could our progress be so much slower when we were prioritizing well and working efficiently?</p><p>I realized that the dominant factor was <a href="https://mtlynch.io/retrospectives/2023/09/#how-do-we-reduce-accidental-difficulty">the size of our codebase</a>. We have three times the code that we did three years ago. And every line of code requires time to maintain. So, if I keep the number of developers fixed but increase the size of the codebase, then a higher proportion of our time must go to maintaining old code.</p><p>Beyond maintenance, more code means that new features are more expensive to build. If your app has zero features, building the first one is easy. If your app already has 20 features, you have to put a lot more thought into how your 21st feature integrates with everything else.</p><p>So, I haven’t figured out a way for us to go significantly faster, but I’ve learned to temper my expectations around feature pace. And I’ve adjusted how I estimate dev costs to account for our more complex codebase.</p><h3 id="most-support-escalation-can-happen-asynchronously">Most support escalation can happen asynchronously<a href="#most-support-escalation-can-happen-asynchronously" arialabel="Anchor"> 🔗︎</a></h3><p>I try to give the TinyPilot team <a href="https://mtlynch.io/solo-developer-year-4/#good-leadership-means-helping-teammates-grow">as much autonomy as possible</a>. At the same time, I want to make sure everyone feels comfortable asking me for help when they get stuck.</p><p>The problem was that when support tickets escalated to me, they felt particularly stressful.</p><p>For a while, I thought that was the nature of support escalation. I’m only seeing the toughest customer questions, so of course they’re going to feel stressful. It turned out that most of it was fixable.</p><p>First, I adjusted our process for escalation. Most escalation took the form of, “Michael, here’s a problem we’ve never seen before. How do you want us to handle it?” I encouraged the team to tweak their approach by proposing a solution as part of escalating to me. If I wasn’t available, and they were the last line of support, what would they tell the customer?</p><p>80% of the time, the team came up with the same solution that I would have recommended. The more the support team did this, the better they became at tackling hard cases.</p><p>Once I saw how close the team’s answers were to my own, I realized there was no need to block a support ticket on an answer from me. If my only contribution to 80% of cases is, “Yes, do that,” why not just do their plan immediately and check with me in parallel about alternatives?</p><p>In the minority of cases where I had a better idea for solving a support issue, it was almost always something the customer could try in addition to my team’s suggestion. We never ran into a situation where my team told me, “Oh, we wish you’d intervened earlier because we suggested putting their device in the microwave, and now their house is on fire.”</p><h2 id="grading-last-years-goals">Grading last year’s goals<a href="#grading-last-years-goals" arialabel="Anchor"> 🔗︎</a></h2><p>Last year, I set <a href="https://mtlynch.io/solo-developer-year-5/#goals-for-year-six">three high-level goals</a> that I wanted to achieve during the year. Here’s how I did against those goals:</p><h3 id="manage-tinypilot-on-20-hours-per-week">Manage TinyPilot on 20 hours per week<a href="#manage-tinypilot-on-20-hours-per-week" arialabel="Anchor"> 🔗︎</a></h3><ul><li><strong>Result</strong>: I worked 35-40 hours per week, a reduction from previous years, and traveled more than any previous year.</li><li><strong>Grade</strong>: B-</li></ul><p>It’s not 20 hours, but I did work significantly less in 2023 than in 2022. I did a lot of travel for both work and non-work. I was “out of the office” for about five weeks cumulatively, and everything went fine.</p><p>When I signed off in the evenings, my work day usually felt complete, whereas in 2022, I frequently felt like I was leaving behind loose ends.</p><h3 id="earn-100k-in-profit">Earn $100k in profit<a href="#earn-100k-in-profit" arialabel="Anchor"> 🔗︎</a></h3><ul><li><strong>Result</strong>: I earned $236k in profit.</li><li><strong>Grade</strong>: A+</li></ul><p>I expected this year to be profitable, as I knew I’d be spending less on hardware engineering, but I underestimated how much additional revenue TinyPilot would earn from the switch to metal cases.</p><p>I was pleasantly surprised to exceed my goal here.</p><h3 id="close-the-tinypilot-office">Close the TinyPilot office<a href="#close-the-tinypilot-office" arialabel="Anchor"> 🔗︎</a></h3><ul><li><strong>Result</strong>: We still have the office for non-critical workflows.</li><li><strong>Grade</strong>: B</li></ul><p>When I made this goal, I didn’t expect our landlord to agree to a month-to-month lease, but he did. Without a long-term commitment, there’s less pressure to move out by a certain deadline.</p><p>We’ve successfully moved the critical operations of manufacturing and fulfillment out of our office. So, we don’t strictly need the office, but it’s convenient to have a home base.</p><p>If the handyman knocked on my door tomorrow to announce that some disaster destroyed all of our office property and made the space unusable, it would be frustrating but not catastrophic.</p><h2 id="goals-for-year-seven">Goals for year seven<a href="#goals-for-year-seven" arialabel="Anchor"> 🔗︎</a></h2><h3 id="manage-tinypilot-on-20-hours-per-week-1">Manage TinyPilot on 20 hours per week<a href="#manage-tinypilot-on-20-hours-per-week-1" arialabel="Anchor"> 🔗︎</a></h3><p>I know I set this as a goal in <a href="https://mtlynch.io/solo-developer-year-4/#manage-tinypilot-on-20-hours-per-week">2022</a> and again in <a href="https://mtlynch.io/solo-developer-year-5/#manage-tinypilot-on-20-hours-per-week">2023</a>, but the third time’s the charm! My management time is trending downward, so this could be my year.</p><h3 id="publish-a-course-or-book">Publish a course or book<a href="#publish-a-course-or-book" arialabel="Anchor"> 🔗︎</a></h3><p>In 2021, I <a href="https://mtlynch.io/solo-developer-year-3/#publish-six-blog-posts-and-one-book">said</a> I’d <a href="https://refactoringenglish.com/">write a book</a> to help developers improve their writing. I got 80% through the first chapter, and then TinyPilot absorbed all of my free time that year.</p><p>I still want to write that book, so if I reduce my management time, hopefully, I can use the free time to write more.</p><p>I’ve also been experimenting with <a href="https://mtlynch.io/tags/nix/">Nix</a> and <a href="https://mtlynch.io/tags/zig">Zig</a>, two technologies that I find exciting but lacking in educational resources. Creating a course for one of those technologies could be a fun way to build my own expertise while also making these tools more accessible to others.</p><h3 id="write-software-for-ten-working-hours-per-week">Write software for ten working hours per week<a href="#write-software-for-ten-working-hours-per-week" arialabel="Anchor"> 🔗︎</a></h3><p>Writing code is still one of my favorite activities.</p><p>For the past few years of TinyPilot, I’ve enjoyed programming, but it’s never been a sensible way to spend my limited working hours. With a team of six people, several critical vendors, and many moving pieces, the most pressing parts of TinyPilot have always been management.</p><p>I hope that by outsourcing and delegating more of TinyPilot’s operational side, I can free up enough bandwidth that programming is, if not the optimal use of my time, at least a reasonable use of my time.</p><h2 id="do-i-still-love-it">Do I still love it?<a href="#do-i-still-love-it" arialabel="Anchor"> 🔗︎</a></h2><p>Every year, when I write these blog posts, I ask myself whether I still love what I’m doing.</p><p>2022 remains the toughest year I’ve had. I still preferred it to working for an employer, but it was a massive challenge to onboard new teammates while navigating <a href="https://en.wikipedia.org/wiki/Global_chip_shortage_(2020%E2%80%932023)">the global chip shortage</a>.</p><p>2023 was a major improvement from the previous year. There were fewer fires to put out, and it felt good to shift critical workflows to specialized vendors.</p><p>The downside to 2023 is that I have a hard time getting excited about it. It was a restructuring year, so I spent a lot of time redefining TinyPilot’s processes and shifting around team responsibilities. TinyPilot has shown me that I’m better than the average developer at designing organizational processes, but I still find it painfully boring.</p><p>While I can’t say that I loved the year, I still enjoyed most of it and preferred it to working for an employer. I’m grateful to be in a position where I can earn a living working for myself and creating a product I’m proud of.</p><blockquote><p lang="en" dir="ltr">2023 was the best year yet for my indie hardware business. We reached $1M in revenue and $236k in profit. I learned how much low-latency responsibility was really costing me and how much the team benefits from async escalation. <a href="https://t.co/Mo7IIwLH42">https://t.co/Mo7IIwLH42</a></p>— Michael Lynch (@deliberatecoder) <a href="https://twitter.com/deliberatecoder/status/1758511480238985701?ref_src=twsrc%5Etfw">February 16, 2024</a></blockquote><hr><p><em>Cover image by <a href="https://www.loraineyow.com/">Loraine Yow</a>. After <a href="https://mtlynch.io/how-to-hire-a-cartoonist/">six years</a> as this blog’s official illustrator, Loraine <a href="https://www.loraineyow.com/post/how-accounting-blew-my-mind">has changed careers</a> but graciously agreed to make one last illustration for this post.</em></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[$5 device accurately tests for breast cancer in under 5 seconds (167 pts)]]></title>
            <link>https://studyfinds.org/device-breast-cancer-5-seconds/</link>
            <guid>39397961</guid>
            <pubDate>Fri, 16 Feb 2024 15:04:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://studyfinds.org/device-breast-cancer-5-seconds/">https://studyfinds.org/device-breast-cancer-5-seconds/</a>, See on <a href="https://news.ycombinator.com/item?id=39397961">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content"><main role="main"><article><figure><img data-perfmatters-preload="" width="500" height="320" src="https://studyfinds.org/wp-content/uploads/2024/02/circuit-board-500x320.jpg" alt="green and black computer motherboard" decoding="async" fetchpriority="high" srcset="https://studyfinds.org/wp-content/uploads/2024/02/circuit-board-500x320.jpg 500w, https://studyfinds.org/wp-content/uploads/2024/02/circuit-board-392x250.jpg 392w, https://studyfinds.org/wp-content/uploads/2024/02/circuit-board-604x385.jpg 604w" sizes="(max-width: 500px) 100vw, 500px"><figcaption>Photo by Vishnu Mohanan from Unsplash</figcaption></figure><div>
<p><strong>GAINESVILLE, Fla. —</strong> A five-dollar circuit board could hold the key to near-instantaneous breast cancer screenings. Scientists say a new handheld device is capable of testing for breast cancer in less than five seconds using just a small sample of saliva. This portable device is not only remarkably quick and user-friendly but also highly cost-effective.</p>



<p>The $5 device, along with its test strips that cost just a few cents each, uses common components. These include glucose testing strips readily available on the market and Arduino, an open-source hardware-software platform.</p>



<p>The <a href="https://studyfinds.org/smart-contact-lenses-cancer/" target="_blank" rel="noopener">biosensor</a>, a collaborative development by the University of Florida and National Yang Ming Chiao Tung University in Taiwan, employs paper <a href="https://studyfinds.org/covid-test-3-minutes-no-blood/" target="_blank" rel="noopener">test strips</a> coated with specific antibodies. These antibodies interact with cancer biomarkers targeted in the test. Upon placing a drop of <a href="https://studyfinds.org/oral-partners-bedroom-cancer/" target="_blank" rel="noopener">saliva</a> on the strip, electrical pulses are sent to contact points on the biosensor device. This process leads to the binding of biomarkers to antibodies, resulting in a measurable change in the output signal. This change is then converted into digital data, indicating the biomarker’s presence.</p>



<p>The researchers highlight that this device stands as a revolutionary alternative to traditional methods such as <a href="https://studyfinds.org/ai-technology-mammograms/" target="_blank" rel="noopener">mammograms</a>, ultrasounds, and <a href="https://studyfinds.org/artificial-intelligence-breast-cancer/" target="_blank" rel="noopener">MRIs</a>. These conventional methods often involve <a href="https://studyfinds.org/cancer-treatment-without-side-effects-flash-rt/" target="_blank" rel="noopener">radiation exposure</a>, are expensive, invasive, slow, and require bulky equipment.</p>




<figure><img data-perfmatters-preload="" decoding="async" width="1200" height="713" src="https://studyfinds.org/wp-content/uploads/2024/02/saliva-biosensor-circuit-board.jpg" alt="The printed circuit board used in the saliva-based biosensor, which can detect breast cancer biomarkers from extremely small saliva samples in about five seconds, costs about $5. The design uses widely available components such as common glucose testing strips and the open-source Arduino platform." srcset="https://studyfinds.org/wp-content/uploads/2024/02/saliva-biosensor-circuit-board.jpg 1200w, https://studyfinds.org/wp-content/uploads/2024/02/saliva-biosensor-circuit-board-475x282.jpg 475w, https://studyfinds.org/wp-content/uploads/2024/02/saliva-biosensor-circuit-board-768x456.jpg 768w" sizes="(max-width: 1200px) 100vw, 1200px">
<figcaption>The printed circuit board used in the saliva-based biosensor, which can detect breast cancer biomarkers from extremely small saliva samples in about five seconds, costs about $5. The design uses widely available components such as common glucose testing strips and the open-source Arduino platform. (Credit: Hsiao-Hsuan Wan)</figcaption>
</figure>



<p>The team behind this innovation hopes it will enable early breast cancer <a href="https://studyfinds.org/gene-changes-breast-cancer/" target="_blank" rel="noopener">detection</a> worldwide.</p>



<p>“Imagine medical staff conducting <a href="https://studyfinds.org/dog-sniffing-out-breast-cancer/" target="_blank" rel="noopener">breast cancer</a> screening in communities or hospitals. Our device is an excellent choice because it is portable — about the size of your hand — and reusable,” says the author of the study, Hsiao-Hsuan Wan, a Ph.D. student at the University of Florida, in a <a href="https://publishing.aip.org/publications/latest-content/would-you-prefer-a-mammogram-mri-or-saliva-on-a-test-strip/" target="_blank" rel="noopener">media release</a>. “The testing time is under five seconds per sample, which makes it highly efficient.”</p>



<p>Wan notes that in many regions, particularly in developing countries, advanced technologies for breast cancer testing, such as MRIs, may not be readily accessible.</p>



<p>“Our technology is more cost-effective, with the test strip costing <a href="https://studyfinds.org/skip-doctors-healthcare-expenses/" target="_blank" rel="noopener">just a few cents</a> and the reusable circuit board priced at $5,” Wan says. “We are excited about the potential to make a significant impact in areas where people might not have had the resources for breast cancer screening tests before.”</p>



<p>Testing results demonstrate the device’s ability to provide accurate results even with <a href="https://studyfinds.org/forever-chemicals-breast-cancer/" target="_blank" rel="noopener">biomarker</a> concentrations as low as one femtogram per milliliter.</p>



<p>“The highlight for me was when I saw readings that clearly distinguished between healthy individuals and those with cancer,” Wan highlights. “We dedicated a lot of time and effort to perfecting the strip, board, and other components. Ultimately, we’ve created a technique that has the potential to help people all around the world.”</p>



<p>The study is published in the <a href="http://dx.doi.org/10.1116/6.0003370" target="_blank" rel="noopener"><em>Journal of Vacuum Science &amp; Technology B</em></a>.</p>







<p><em>SWNS writer Isobel Williams contributed to this report.</em></p>
<br><!-- AI CONTENT END 1 -->
</div>


<!-- #comments -->
</article></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Erlang/OTP 27.0 Release Candidate 1 (105 pts)]]></title>
            <link>https://www.erlang.org/news/167</link>
            <guid>39397538</guid>
            <pubDate>Fri, 16 Feb 2024 14:34:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.erlang.org/news/167">https://www.erlang.org/news/167</a>, See on <a href="https://news.ycombinator.com/item?id=39397538">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <article>
    

    <div>
        
        
        
        
        <h2 id="otp-270-rc1">
        
        
          OTP 27.0-rc1 <a href="#otp-270-rc1">#</a>
        
        
      </h2>
    

<p>Erlang/OTP 27.0-rc1 is the first release candidate of three before the OTP 27.0 release.</p>

<p>The intention with this release is to get feedback from our users.
All feedback is welcome, even if it is only to say that it works for you.
We encourage users to try it out and give us feedback either by creating an issue at
<a href="https://github.com/erlang/otp/issues">https://github.com/erlang/otp/issues</a> or by
posting to <a href="https://erlangforums.com/">Erlang Forums</a>.</p>

<p>All artifacts for the release can be downloaded from the
<a href="https://github.com/erlang/otp/releases/tag/OTP-27.0-rc1">Erlang/OTP Github</a> release
and you can view the new documentation at
<a href="https://erlang.org/documentation/doc-15.0-rc1/doc/">https://erlang.org/documentation/doc-15.0-rc1/doc</a>.
You can also install the latest release using <a href="https://github.com/kerl/kerl">kerl</a> like this:</p>

<pre><code>kerl build 27.0-rc1 27.0-rc1.
</code></pre>

<p>Erlang/OTP 27 is a new major release with new features, improvements
as well as a few incompatibilities. Some of the new features are
highlighted below.</p>

<p>Many thanks to all contributors!</p>
      <h2 id="highlights">
        
        
          Highlights <a href="#highlights">#</a>
        
        
      </h2>
    
      <h2 id="documentation">
        
        
          Documentation <a href="#documentation">#</a>
        
        
      </h2>
    

<p><a href="https://www.erlang.org/eeps/eep-0059">EEP-59</a> has been
implemented. Documentation attributes in source files can now be used
to document functions, types, callbacks, and modules.</p>

<p>The entire Erlang/OTP documentation is now using the new documentation
system.</p>
      <h2 id="new-language-features">
        
        
          New language features <a href="#new-language-features">#</a>
        
        
      </h2>
    

<ul>
  <li>
    <p>Triple-Quoted Strings has been implemented as per
<a href="https://www.erlang.org/eeps/eep-0064">EEP 64</a> to allow a string
to encompass a complete paragraph.</p>
  </li>
  <li>
    <p>Adjacent string literals without intervening white space is now a syntax
error, to avoid possible confusion with triple-quoted strings.</p>
  </li>
  <li>
    <p>Sigils on string literals (both ordinary and triple-quoted) have
been implemented as per
<a href="https://www.erlang.org/eeps/eep-0066">EEP  66</a>. For example,
<code>~"Björn"</code> or <code>~b"Björn"</code> are now equivalent to <code>&lt;&lt;"Björn"/utf8&gt;&gt;</code>.</p>
  </li>
</ul>
      <h2 id="compiler-and-jit-improvements">
        
        
          Compiler and JIT improvements <a href="#compiler-and-jit-improvements">#</a>
        
        
      </h2>
    

<ul>
  <li>
    <p>The compiler will now merge consecutive updates of the same record.</p>
  </li>
  <li>
    <p>Safe destructive update of tuples has been implemented in the compiler
and runtime system. This allows the VM to update tuples in-place when it
is safe to do so, thus improving performance by doing less copying but
also by producing less garbage.</p>
  </li>
  <li>
    <p>The <code>maybe</code> expression is now enabled by default, eliminating the need
for enabling the <code>maybe_expr</code> feature.</p>
  </li>
  <li>
    <p>Native coverage support has been implemented in the JIT. It will
automatically be used by the <code>cover</code> tool to reduce the execution
overhead when running cover-compiled code. There are also new APIs
to support native coverage without using the <code>cover</code> tool.</p>
  </li>
  <li>
    <p>The compiler will now raise a warning when updating record/map literals
to catch a common mistake. For example, the compiler will now emit a
warning for <code>#r{a=1}#r{b=2}</code>.</p>
  </li>
</ul>
      <h2 id="erts">
        
        
          ERTS <a href="#erts">#</a>
        
        
      </h2>
    

<ul>
  <li>
    <p>The <code>erl</code> command now supports the <code>-S</code> flag, which is similar to
the <code>-run</code> flag, but with some of the rough edges filed off.</p>
  </li>
  <li>
    <p>By default, escripts will now be compiled instead of interpreted. That
means that the <code>compiler</code> application must be installed.</p>
  </li>
  <li>
    <p>The default process limit has been raised to <code>1048576</code> processes.</p>
  </li>
  <li>
    <p>The <code>erlang:system_monitor/2</code> functionality is now able to monitor long
message queues in the system.</p>
  </li>
  <li>
    <p>The obsolete and undocumented support for opening a port to an external
resource by passing an atom (or a string) as first argument to
<code>open_port()</code>, implemented by the vanilla driver,
has been removed. This feature has been scheduled for removal in OTP 27
since the release of OTP 26.</p>
  </li>
  <li>
    <p>The <code>pid</code> field has been removed from <code>erlang:fun_info/1,2</code>.</p>
  </li>
  <li>
    <p>Multiple trace sessions are now supported.</p>
  </li>
</ul>
      <h2 id="stdlib">
        
        
          STDLIB <a href="#stdlib">#</a>
        
        
      </h2>
    

<ul>
  <li>
    <p>Several new functions that accept funs have been added to module <code>timer</code>.</p>
  </li>
  <li>
    <p>The functions <code>is_equal/2</code>, <code>map/2</code>, and <code>filtermap/2</code> have been added to
the modules <code>sets</code>, <code>ordsets</code>, and <code>gb_sets</code>.</p>
  </li>
  <li>
    <p>There are new efficient <code>ets</code> traversal functions with guaranteed atomicity.
For example, <code>ets:next/2</code> followed by <code>ets:lookup/2</code> can now be replaced
with <code>ets:next_lookup/1</code>.</p>
  </li>
  <li>
    <p>The new function <code>ets:update_element/4</code> is similar to <code>ets:update_element/3</code>,
but takes a default tuple as the fourth argument, which will be inserted
if no previous record with that key exists.</p>
  </li>
  <li>
    <p><code>binary:replace/3,4</code> now supports using a fun for supplying the
replacement binary.</p>
  </li>
  <li>
    <p>The new function <code>proc_lib:set_label/1</code> can be used to add a descriptive
term to any process that does not have a registered name. The name will
be shown by tools such as <code>c:i/0</code> and <code>observer</code>, and it will be included
in crash reports produced by processes using <code>gen_server</code>, <code>gen_statem</code>,
<code>gen_event</code>, and <code>gen_fsm</code>.</p>
  </li>
  <li>
    <p>Added functions to retrieve the next higher or lower key/element from
<code>gb_trees</code> and <code>gb_sets</code>, as well as returning iterators that start at
given keys/elements.</p>
  </li>
</ul>
      <h2 id="common_test">
        
        
          common_test <a href="#common_test">#</a>
        
        
      </h2>
    

<ul>
  <li>
    <p>Calls to <code>ct:capture_start/0</code> and <code>ct:capture_stop/0</code> are now synchronous to
ensure that all output is captured.</p>
  </li>
  <li>
    <p>The default CSS will now include a basic dark mode handling if it is
preferred by the browser.</p>
  </li>
</ul>
      <h2 id="crypto">
        
        
          crypto <a href="#crypto">#</a>
        
        
      </h2>
    

<ul>
  <li>The functions <code>crypto_dyn_iv_init/3</code> and <code>crypto_dyn_iv_update/3</code>
that were marked as deprecated in Erlang/OTP 25 have been removed.</li>
</ul>
      <h2 id="dialyzer">
        
        
          dialyzer <a href="#dialyzer">#</a>
        
        
      </h2>
    

<ul>
  <li>The <code>--gui</code> option for Dialyzer has been removed.</li>
</ul>
      <h2 id="ssl">
        
        
          ssl <a href="#ssl">#</a>
        
        
      </h2>
    

<ul>
  <li>The <code>ssl</code> client can negotiate and handle certificate status request (OCSP
stapling support on the client side).</li>
</ul>
      
    

<ul>
  <li>There is a new tool <code>tprof</code>, which combines the functionality of <code>eprof</code>
and <code>cprof</code> under one interface. It also adds heap profiling.</li>
</ul>
      <h2 id="xmerl">
        
        
          xmerl <a href="#xmerl">#</a>
        
        
      </h2>
    

<ul>
  <li>As an alternative to <code>xmerl_xml</code>, a new export module <code>xmerl_xml_indent</code>
that provides out-of-the box indented output has been added.</li>
</ul>

<p>For more details about new features and potential incompatibilities see the <a href="https://erlang.org/download/otp_src_27.0-rc1.readme">README</a>.</p>

        
    </div>
</article>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Data structures as topological spaces (2002) [pdf] (105 pts)]]></title>
            <link>http://mgs.spatial-computing.org/PUBLICATIONS/umc02.pdf</link>
            <guid>39396337</guid>
            <pubDate>Fri, 16 Feb 2024 12:57:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://mgs.spatial-computing.org/PUBLICATIONS/umc02.pdf">http://mgs.spatial-computing.org/PUBLICATIONS/umc02.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=39396337">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Alexei Navalny has died – authorities (908 pts)]]></title>
            <link>https://www.reuters.com/world/europe/jailed-russian-opposition-leader-navalny-dead-prison-service-2024-02-16/</link>
            <guid>39395631</guid>
            <pubDate>Fri, 16 Feb 2024 11:34:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/world/europe/jailed-russian-opposition-leader-navalny-dead-prison-service-2024-02-16/">https://www.reuters.com/world/europe/jailed-russian-opposition-leader-navalny-dead-prison-service-2024-02-16/</a>, See on <a href="https://news.ycombinator.com/item?id=39395631">Hacker News</a></p>
Couldn't get https://www.reuters.com/world/europe/jailed-russian-opposition-leader-navalny-dead-prison-service-2024-02-16/: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Guix on the Framework 13 AMD (187 pts)]]></title>
            <link>https://wingolog.org/archives/2024/02/16/guix-on-the-framework-13-amd</link>
            <guid>39395474</guid>
            <pubDate>Fri, 16 Feb 2024 11:05:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wingolog.org/archives/2024/02/16/guix-on-the-framework-13-amd">https://wingolog.org/archives/2024/02/16/guix-on-the-framework-13-amd</a>, See on <a href="https://news.ycombinator.com/item?id=39395474">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I got a new laptop!  It’s a <a href="https://frame.work/fr/en/products/laptop-diy-13-gen-amd">Framework 13 AMD</a>: 8 cores, 2 threads per core, 64 GB RAM, 3:2 2256×1504 matte screen.  It kicks my 5-year-old Dell XPS 13 in the pants, and I am so relieved to be back to a matte screen.  I just got it up and running with <a href="https://guix.gnu.org/">Guix</a>, which though easier than past installation experiences was not without some wrinkles, so here I wanted to share a recipe for what worked for me.</p><p>(I swear this isn’t going to become a product review blog, but when I went to post something like this on the <a href="https://community.frame.work/">Framework forum</a>  I got an error saying that new users could only post 2 links.  I understand how we got here but hoo, that is a garbage experience!)</p><h3>The basic deal</h3><p>Upstream Guix works on the Framework 13 AMD, but only with software rendering and no wifi, and I wasn’t able to install from upstream media.  This is mainly because Guix uses a modified kernel and doesn’t include necessary firmware.  There is a third-party <a href="https://gitlab.com/nonguix/nonguix">nonguix</a> repository that defines packages for the vanilla Linux kernel and the linux-firmware collection; we have to use that repo if we want all functionality.</p><p>Of course having the firmware be user-hackable would be better, and it would be better if the framework laptop used parts with free firmware.  Something for a next revision, hopefully.</p><h3>On firmware</h3><p>As an aside, I think the official Free Software Foundation position on firmware is bad praxis.  To recall, the idea is that if a device has embedded software (firmware) that can be updated, but that software is in a form that users can’t modify, then the system as a whole is not free software.  This is technically correct but doesn’t logically imply that the right strategy for advancing free software is to forbid firmware blobs; you have a number of potential policy choices and you have to look at their expected results to evaluate which one is most in line with your goals.</p><p>Bright lines are useful, of course; I just think that with respect to free software, drawing that line around firmware is not interesting.  To illustrate this point, I believe the current FSF position is that if you can run e.g. a USB ethernet adapter without installing non-free firmware, then it is kosher, otherwise it is haram.  However many of these devices <i>have</i> firmware; it’s just that you aren’t updating it.  So for example the the USB Ethernet adapter I got with my Dell system many years ago has firmware, therefore it has bugs, but I have never updated that firmware because that’s not how we roll.  Or, on my old laptop, I never updated the CPU microcode, despite spectre and meltdown and all the rest.</p><p>“Firmware, but never updated” reminds me of the <a href="https://en.wikipedia.org/wiki/Eruv">wires around some New York neighborhoods</a> that allow orthodox people to leave the house on Sabbath; useful if you are of a given community and enjoy the feeling of belonging, but I think even the faithful would see it as a hack.  It is like how Richard Stallman wouldn’t use travel booking web sites because they had non-free JavaScript, but would happily call someone on the telephone to perform the booking for him, using those same sites.  In that case, the net effect on the world of this particular bright line is negative: it does not advance free software in the least and only adds overhead.  Privileging principle over praxis is generally a losing strategy.</p><h3>Installation</h3><p>Firstly I had to turn off secure boot in the bios settings; it’s in “security”.</p><p>I wasn’t expecting wifi to work out of the box, but for some reason the upstream Guix install media was not able to configure the network via the Ethernet expansion card nor an external USB-C ethernet adapter that I had; stuck at the DHCP phase.  So my initial installation attempt failed.</p><p>Then I realized that the <a href="https://gitlab.com/nonguix/nonguix">nonguix</a> repository has installation media, which is the same as upstream but with the vanilla kernel and linux-firmware.  So on another machine where I had Guix installed, I added the nonguix channel and built the installation media, via <tt>guix system image -t iso9660 nongnu/system/install.scm</tt>.  That gave me a file that I could write to a USB stick.</p><p>Using that installation media, installing was a breeze.</p><p>However upon reboot, I found that I had no wifi and I was using software rendering;  clearly, installation produced an OS config with the Guix kernel instead of upstream Linux.  Happily, at this point the ethernet expansion card was able to work, so connect to wired ethernet, open <tt>/etc/config.scm</tt>, add the needed lines as described in the <tt>operating-system</tt> part of <a href="https://gitlab.com/nonguix/nonguix">the nonguix README</a>, reconfigure, and reboot.  Building Linux takes a little less than an hour on this machine.</p><h3>Fractional scaling</h3><p>At that point you have wifi and graphics drivers.  I use GNOME, and things seem to work.  However the screen defaults to 200% resolution, which makes everything really big.  Crisp, pretty, but big.  Really you would like something in between?  Or that the Framework ships a higher-resolution screen so that 200% would be a good scaling factor; this was the case with my old Dell XPS 13, and it worked well.  Anyway with the Framework laptop, I wanted 150% scaling, and it seems these days that the way you have to do this is to use Wayland, which Guix does not yet enable by default.</p><p>So you go into <tt>config.scm</tt> again, and change where it says <tt>%desktop-services</tt> to be:</p><pre>(modify-services %desktop-services
  (gdm-service-type config =&gt;
    (gdm-configuration (inherit config) (wayland? #t))))
</pre><p>Then when you reboot you are in Wayland.  Works fine, it seems.  But then you have to go and enable an experimental mutter setting; install <tt>dconf-editor</tt>, run it, search for keys with “mutter” in the name, find the “experimental settings” key, tell it to not use the default setting, then click the box for “scale-monitor-framebuffer”.</p><p>Then!  You can go into GNOME settings and get 125%, 150%, and so on.  Great.</p><p>HOWEVER, and I hope this is a transient situation, there is a problem: in GNOME, applications that aren’t native Wayland apps don’t scale nicely.  It’s like the app gets rendered to a texture at the original resolution, which then gets scaled up in a blurry way.  There aren’t so many of these apps these days as most things have been ported to be Wayland-capable, Firefox included, but Emacs is one of them :(  However however!  If you install the <tt>emacs-pgtk</tt> package instead of <tt>emacs</tt>, it looks better.  Not perfect, but good enough.  So that’s where I am.</p><h3>Bugs</h3><p>The laptop hangs on reboot due to <a href="https://community.frame.work/t/tracking-gnu-guix-system-on-the-framework/22459/24">this bug</a>, but that seems a minor issue at this point.  There is an ongoing <a href="https://community.frame.work/t/tracking-gnu-guix-system-on-the-framework/22459">tracker discussion</a> on the community forum; like other problems in that thread, I hope that this one resolves itself upstream in Linux over time.</p><h3>Other things?</h3><p>I didn’t mention the funniest thing about this laptop: it comes in pieces that you have to put together :)  I am not so great with hardware, but I had no problem.  The build quality seems pretty good; not a MacBook Air, but then it’s also user-repairable, which is a big strong point.  It has these funny extension cards that slot into the chassis, which I have found to be quite amusing.</p><p>I haven’t had the machine for long enough but it seems to work fine up to now: suspend, good battery use, not noisy (unless it’s compiling on all 16 threads), graphics, wifi, ethernet, good compilation speed.  (I should give compiling LLVM a go; that’s a useful workload.)  I don’t have bluetooth or the fingerprint reader working yet; I give it 25% odds that I get around to this during the lifetime of this laptop :)</p><p>Until next time, happy hacking!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[German credit agency earns millions through unlawful customer manipulation (256 pts)]]></title>
            <link>https://noyb.eu/en/german-credit-agency-earns-millions-through-unlawful-customer-manipulation</link>
            <guid>39395329</guid>
            <pubDate>Fri, 16 Feb 2024 10:40:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://noyb.eu/en/german-credit-agency-earns-millions-through-unlawful-customer-manipulation">https://noyb.eu/en/german-credit-agency-earns-millions-through-unlawful-customer-manipulation</a>, See on <a href="https://news.ycombinator.com/item?id=39395329">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
                    <p><strong>Today, </strong><em><strong>noyb </strong></em><strong>has filed a complaint and report against the German credit agency SCHUFA with the Hessian data protection authority. The company appears to be making millions of euros by selling people in Germany their own data. With the help of manipulative designs, people are prevented from obtaining a free copy of their data in accordance with Article 15 GDPR – even though they would actually be legally entitled to it. The company’s primary aim appears to be to profit from people looking for accommodation. In Germany, they often have to provide proof of solvency in order to obtain a lease.</strong></p><ul><li><a href="https://noyb.eu/sites/default/files/2024-02/Beschwerde%20%2B%20Anzeige%20SCHUFA_geschw%C3%A4rzt.pdf">Complaint and report against SCHUFA (DE)</a></li></ul><p><strong>Lucrative business at the expense of apartment seekers.</strong> Anyone looking for a flat or house to rent in Germany is regularly asked to prove their financial reliability. As a result, people looking for accommodation often end up at credit agencies such as SCHUFA – which make a fortune by selling people in Germany their own data. What SCHUFA deliberately conceals: according to Article 15 GDPR, it would have to provide precisely this data free of charge and without delay.</p><p><strong>Concealed rights. </strong>On its own website, SCHUFA only advertises its so-called “BonitätsAuskunft” for €29.95 to private individuals and claims that it offers an “advantage on the housing market”. A transparent reference to the Article 15 GDPR right to free information is not provided. Using manipulative designs, the company is trying to push the sale of paid products and even falsely presents the free information as unsuitable for submission to third parties.</p><p>Martin Baumann, data protection lawyer at <em>noyb</em>: <em>“SCHUFA is falsely claiming that only its paid products can be presented to third parties. In reality, the European Court of Justice has emphasised several times that data subjects are allowed to do whatever they want with their free information."</em></p><p><strong>“Data copy” instead of information.</strong> The vast majority of data subjects is unlikely to even find the free information. Although the GDPR stipulates that companies must support data subjects in obtaining their free information, SCHUFA does not even mention it by name. The company casually refers to the information in accordance with Article 15 GDPR as a “data copy”. In fact, a range of further information needs to be included as well. At the same time, the legal term “information” in accordance with Article 15 is misused for the paid product (BonitätsAuskunft). Anyone who manages to find the hidden option to request the information free of charge is once again bombarded with adverts for the paid product. In addition, SCHUFA advises against sharing the free information with third parties. On the one hand, it supposedly contains sensitive data, but on the other hand <em>“not an up-to-date calculation of your creditworthiness scores”</em>.</p><p><strong>Data is deliberately withheld.</strong> As a result, SCHUFA is violating European data protection law in several ways. The company doesn’t take any measures to make it easy for data subjects to exercise their right of access to data, contrary to the clear requirements of the GDPR. Additionally, the company deliberately withholds information in order to be able to sell its paid product: For example, in the case of the complainant, the free information included only a “basic score”, while the paid information showed six different “industry scores”. Article 15 GDPR obliges the company to disclose all processed data in full. To make matters worse, the complainant received the paid product after five days, while the free information, which was ordered at the same time, took significantly longer. Here, too, the GDPR actually requires an “immediate” delivery.</p><p>Martin Baumann, data protection lawyer at <em>noyb</em>: <em>“The GDPR requires companies to make all data available immediately, transparently, easily accessible and free of charge. These requirements are in clear contradiction to the current business practice of selling people their own data.”</em></p><p><strong>Complaint and report in Germany.</strong> <em>noyb </em>has therefore filed a complaint against SCHUFA with the Hessian data protection authority. By systematically hiding and delaying the free information and deliberately withholding data, the company is in breach of the GDPR. In addition, <em>noyb</em> is filing a report with the Hessian DPA. SCHUFA systematically violates the legal requirement of free information by creating the impression that only the paid products are suitable as proof to third parties.</p>
            
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Kagi Changelog 2/13: Faster and more accurate instant answers and Wikipedia page (298 pts)]]></title>
            <link>https://kagi.com/changelog#3179</link>
            <guid>39394060</guid>
            <pubDate>Fri, 16 Feb 2024 06:59:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kagi.com/changelog#3179">https://kagi.com/changelog#3179</a>, See on <a href="https://news.ycombinator.com/item?id=39394060">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="mainContent"><div id="3179"><h2><span>Feb 13, 2024 - Faster and more accurate instant answers. And we got a Wikipedia page!</span><a href="https://kagi.com/changelog#3179"> #</a></h2><div><h3>Features</h3><ul><li><p>We added <strong>Wolfram|Alpha to enhance our capabilities in calculations, unit conversions, and time</strong> queries for better results. This solves a huge number of issues reported for these kind of queries as the results now come from a computational knowledge authorithy.</p> <p>(Wolfram Alpha integration for fact based queries <a href="https://kagifeedback.org/d/1271" data-id="1271"><span></span></a><a href="https://kagifeedback.org/d/1271">#1271</a> <a href="https://kagifeedback.org/u/Recast">@Recast</a>)</p></li></ul><p><img src="https://kagifeedback.org/assets/files/2024-02-13/1707850630-598677-screenshot-2024-02-13-at-105705.png" title="" alt=""></p><p><img src="https://kagifeedback.org/assets/files/2024-02-13/1707852556-300223-image.png" title="" alt=""></p><ul><li>In the same spirit of <strong>getting answers faster</strong>, now simply starting your query with an interrogative word (what, where, who, which, when, how) or just ending it with a question mark (?) will <strong>automatically trigger Quick Answer</strong>:</li></ul><p><img src="https://kagifeedback.org/assets/files/2024-02-13/1707851550-889014-screenshot-2024-02-13-at-111224.png" title="" alt=""></p><p><img src="https://kagifeedback.org/assets/files/2024-02-13/1707851590-997746-screenshot-2024-02-13-at-111305.png" title="" alt=""></p><ul><li><p>Video results will now feature <strong>duration, channel name and timestamp</strong>  <a href="https://kagifeedback.org/d/2970" data-id="2970"><span></span></a><a href="https://kagifeedback.org/d/2970">#2970</a> <a href="https://kagifeedback.org/u/blosh">@blosh</a>)<br><img src="https://kagifeedback.org/assets/files/2024-02-13/1707850745-196425-screenshot-2024-02-13-at-105856.png" title="" alt=""></p></li><li><p>You can now <strong>hear how words are pronounced</strong> directly in our dictionary results, <a href="https://kagifeedback.org/d/321" data-id="321"><span></span></a><a href="https://kagifeedback.org/d/321">#321</a> <a href="https://kagifeedback.org/u/Yuu">@Yuu</a> <br><img src="https://kagifeedback.org/assets/files/2024-02-13/1707849305-551491-image.png" title="" alt=""></p></li><li><p>We've introduced a new feature in Research Assistant that allows you to <strong>use your lenses to narrow down the scope of search results in Assistant</strong>   <a href="https://kagifeedback.org/d/2147" data-id="2147"><span></span></a><a href="https://kagifeedback.org/d/2147">#2147</a> <a href="https://kagifeedback.org/u/truethomas">@truethomas</a>)<br><img src="https://kagifeedback.org/assets/files/2024-02-13/1707840034-526042-image.png" title="" alt=""></p></li></ul><h3>Improvements &amp; Bug fixes</h3><ul><li>Switch between search and assistant mode without clearing the search <a href="https://kagifeedback.org/d/2390" data-id="2390"><span></span></a><a href="https://kagifeedback.org/d/2390">#2390</a> <a href="https://kagifeedback.org/u/mackid1993">@mackid1993</a></li><li>Time Ascending/Descending should contextually change to facilitate understanding <a href="https://kagifeedback.org/d/1387" data-id="1387"><span></span></a><a href="https://kagifeedback.org/d/1387">#1387</a> <a href="https://kagifeedback.org/u/kf">@kf</a> </li><li>Ctrl+V for assistant upload <a href="https://kagifeedback.org/d/3024" data-id="3024"><span></span></a><a href="https://kagifeedback.org/d/3024">#3024</a> <a href="https://kagifeedback.org/u/Value7609">@Value7609</a></li><li>Billing page does not have a way to show detailed consumption statistics on Trial plan <a href="https://kagifeedback.org/d/3052" data-id="3052"><span></span></a><a href="https://kagifeedback.org/d/3052">#3052</a> <a href="https://kagifeedback.org/u/aochagavia">@aochagavia</a></li><li>Turn Off Search Suggestions for Kids Accounts <a href="https://kagifeedback.org/d/3070" data-id="3070"><span></span></a><a href="https://kagifeedback.org/d/3070">#3070</a> <a href="https://kagifeedback.org/u/keen_dog">@keen_dog</a></li><li>Searching for a unicode sequence always opens first result <a href="https://kagifeedback.org/d/3078" data-id="3078"><span></span></a><a href="https://kagifeedback.org/d/3078">#3078</a> <a href="https://kagifeedback.org/u/cvzakharchenko">@cvzakharchenko</a> </li><li>Video search timestamps incorrect <a href="https://kagifeedback.org/d/3117" data-id="3117"><span></span></a><a href="https://kagifeedback.org/d/3117">#3117</a> <a href="https://kagifeedback.org/u/Amino4873">@Amino4873</a></li><li>Bang completion in browser search bar <a href="https://kagifeedback.org/d/1967" data-id="1967"><span></span></a><a href="https://kagifeedback.org/d/1967">#1967</a> <a href="https://kagifeedback.org/u/Value7609">@Value7609</a> </li><li>Semicolons aren't properly handled in the search results page <a href="https://kagifeedback.org/d/2365" data-id="2365"><span></span></a><a href="https://kagifeedback.org/d/2365">#2365</a> <a href="https://kagifeedback.org/u/laiz">@laiz</a></li><li>Markup sneaking into calculation responses <a href="https://kagifeedback.org/d/3053" data-id="3053"><span></span></a><a href="https://kagifeedback.org/d/3053">#3053</a> <a href="https://kagifeedback.org/u/anotherhue">@anotherhue</a></li><li>Quick Answer on iOS erroneously enables horizontal scroll <a href="https://kagifeedback.org/d/3166" data-id="3166"><span></span></a><a href="https://kagifeedback.org/d/3166">#3166</a> <a href="https://kagifeedback.org/u/equalidea">@equalidea</a> </li><li>Flight widget “show more” does not work on mobile <a href="https://kagifeedback.org/d/3136" data-id="3136"><span></span></a><a href="https://kagifeedback.org/d/3136">#3136</a> <a href="https://kagifeedback.org/u/stoyle">@stoyle</a></li><li>Copying from Assistant while it is writing it's answer doesn't work <a href="https://kagifeedback.org/d/2596" data-id="2596"><span></span></a><a href="https://kagifeedback.org/d/2596">#2596</a> <a href="https://kagifeedback.org/u/Grooty">@Grooty</a></li><li>[UI] popover for paid sites does not display on Mobile <a href="https://kagifeedback.org/d/3076" data-id="3076"><span></span></a><a href="https://kagifeedback.org/d/3076">#3076</a> <a href="https://kagifeedback.org/u/heliostatic">@heliostatic</a></li><li>Assistant answer format  <a href="https://kagifeedback.org/d/3068" data-id="3068"><span></span></a><a href="https://kagifeedback.org/d/3068">#3068</a> <a href="https://kagifeedback.org/u/cardinal086">@cardinal086</a></li><li>Pressing <code>c</code> while renaming a family will open the menu <a href="https://kagifeedback.org/d/3105" data-id="3105"><span></span></a><a href="https://kagifeedback.org/d/3105">#3105</a> <a href="https://kagifeedback.org/u/catgirlinspace">@catgirlinspace</a></li><li>Quick answer gives no answer <a href="https://kagifeedback.org/d/3108" data-id="3108"><span></span></a><a href="https://kagifeedback.org/d/3108">#3108</a> <a href="https://kagifeedback.org/u/X145678908765">@X145678908765</a></li><li>Assistant citation quotation do nothing on mobile <a href="https://kagifeedback.org/d/3085" data-id="3085"><span></span></a><a href="https://kagifeedback.org/d/3085">#3085</a> <a href="https://kagifeedback.org/u/stoyle">@stoyle</a></li></ul><h3>Fixed with Wolfram|Alpha computation integration</h3> <ul><li>Incorrect timezone conversion <a href="https://kagifeedback.org/d/2342" data-id="2342"><span></span></a><a href="https://kagifeedback.org/d/2342">#2342</a> <a href="https://kagifeedback.org/u/Crafty9853">@Crafty9853</a></li><li>Crypto to fiat conversation, increase floating point precision for cryptocurrency <a href="https://kagifeedback.org/d/2348" data-id="2348"><span></span></a><a href="https://kagifeedback.org/d/2348">#2348</a> <a href="https://kagifeedback.org/u/mccowen">@mccowen</a></li><li>Calculator struggling with percentages <a href="https://kagifeedback.org/d/2566" data-id="2566"><span></span></a><a href="https://kagifeedback.org/d/2566">#2566</a> <a href="https://kagifeedback.org/u/Krmloo">@Krmloo</a></li><li>Time conversion using city/region/country names and current location <a href="https://kagifeedback.org/d/756" data-id="756"><span></span></a><a href="https://kagifeedback.org/d/756">#756</a> <a href="https://kagifeedback.org/u/tychoregter">@tychoregter</a></li><li>Improve calculator widget <a href="https://kagifeedback.org/d/2505" data-id="2505"><span></span></a><a href="https://kagifeedback.org/d/2505">#2505</a> <a href="https://kagifeedback.org/u/EvacuatedTerminal">@EvacuatedTerminal</a></li><li>"time argentina" uses wrong timezone <a href="https://kagifeedback.org/d/2482" data-id="2482"><span></span></a><a href="https://kagifeedback.org/d/2482">#2482</a> <a href="https://kagifeedback.org/u/bwkagi">@bwkagi</a></li><li>Can't convert "bytes to MB" <a href="https://kagifeedback.org/d/2481" data-id="2481"><span></span></a><a href="https://kagifeedback.org/d/2481">#2481</a> <a href="https://kagifeedback.org/u/jesus">@jesus</a></li><li>Wrong time <a href="https://kagifeedback.org/d/2377" data-id="2377"><span></span></a><a href="https://kagifeedback.org/d/2377">#2377</a> <a href="https://kagifeedback.org/u/Edweis">@Edweis</a></li><li>Speed conversion widget <a href="https://kagifeedback.org/d/2362" data-id="2362"><span></span></a><a href="https://kagifeedback.org/d/2362">#2362</a> <a href="https://kagifeedback.org/u/lumpycustard">@lumpycustard</a></li><li>Asking from Cyprus time gives wrong answer <a href="https://kagifeedback.org/d/2032" data-id="2032"><span></span></a><a href="https://kagifeedback.org/d/2032">#2032</a> <a href="https://kagifeedback.org/u/asolovyov">@asolovyov</a></li><li>Calculator widget appears to be haunted (wrong results for MANY things) <a href="https://kagifeedback.org/d/2289" data-id="2289"><span></span></a><a href="https://kagifeedback.org/d/2289">#2289</a> <a href="https://kagifeedback.org/u/puppy">@puppy</a></li><li>Gallon to oz - unknown  <a href="https://kagifeedback.org/d/2212" data-id="2212"><span></span></a><a href="https://kagifeedback.org/d/2212">#2212</a> <a href="https://kagifeedback.org/u/partlycloudy">@partlycloudy</a></li><li>Conversion to minutes fails if using <code>min</code> <a href="https://kagifeedback.org/d/2082" data-id="2082"><span></span></a><a href="https://kagifeedback.org/d/2082">#2082</a> <a href="https://kagifeedback.org/u/xeophon">@xeophon</a></li><li>Add natural language maths to calculator <a href="https://kagifeedback.org/d/2061" data-id="2061"><span></span></a><a href="https://kagifeedback.org/d/2061">#2061</a> <a href="https://kagifeedback.org/u/StarMaze">@StarMaze</a></li><li>Calculator gives wrong answer for 2<sup>63 <a href="https://kagifeedback.org/d/1953" data-id="1953"><span></span></a><a href="https://kagifeedback.org/d/1953">#1953</a></sup> <a href="https://kagifeedback.org/u/rookwood101">@rookwood101</a></li><li>Unit converter can't handle small numbers <a href="https://kagifeedback.org/d/1995" data-id="1995"><span></span></a><a href="https://kagifeedback.org/d/1995">#1995</a> <a href="https://kagifeedback.org/u/ThreePointsShort">@ThreePointsShort</a></li><li>Recognize comma as decimal separator <a href="https://kagifeedback.org/d/2743" data-id="2743"><span></span></a><a href="https://kagifeedback.org/d/2743">#2743</a> <a href="https://kagifeedback.org/u/jstolarek">@jstolarek</a></li><li>A search for 'Eastern Time' brings up the wrong time zone <a href="https://kagifeedback.org/d/243" data-id="243"><span></span></a><a href="https://kagifeedback.org/d/243">#243</a> <a href="https://kagifeedback.org/u/CorlinP">@CorlinP</a></li><li>"Current time in ___" does not bring up time zone widget <a href="https://kagifeedback.org/d/99" data-id="99"><span></span></a><a href="https://kagifeedback.org/d/99">#99</a> <a href="https://kagifeedback.org/u/lacikawiz">@lacikawiz</a></li><li>Time zone conversion uses wrong time zone <a href="https://kagifeedback.org/d/1075" data-id="1075"><span></span></a><a href="https://kagifeedback.org/d/1075">#1075</a> <a href="https://kagifeedback.org/u/Jake-Moss">@Jake-Moss</a></li><li>Time Converter widget doesn't account for summer time <a href="https://kagifeedback.org/d/1031" data-id="1031"><span></span></a><a href="https://kagifeedback.org/d/1031">#1031</a> <a href="https://kagifeedback.org/u/Kai">@Kai</a></li><li>Time widget is broken <a href="https://kagifeedback.org/d/1053" data-id="1053"><span></span></a><a href="https://kagifeedback.org/d/1053">#1053</a> <a href="https://kagifeedback.org/u/test41">@test41</a></li><li>Time conversion is incorrect <a href="https://kagifeedback.org/d/1386" data-id="1386"><span></span></a><a href="https://kagifeedback.org/d/1386">#1386</a> <a href="https://kagifeedback.org/u/alanb">@alanb</a></li><li>Time widget thinks Palestine is five hours ahead of Israel <a href="https://kagifeedback.org/d/2985" data-id="2985"><span></span></a><a href="https://kagifeedback.org/d/2985">#2985</a> <a href="https://kagifeedback.org/u/cybiko123">@cybiko123</a></li><li>"Time in argentina" is incorrect <a href="https://kagifeedback.org/d/1379" data-id="1379"><span></span></a><a href="https://kagifeedback.org/d/1379">#1379</a> <a href="https://kagifeedback.org/u/kagiar">@kagiar</a></li><li>3pm PT is about pacific time <a href="https://kagifeedback.org/d/355" data-id="355"><span></span></a><a href="https://kagifeedback.org/d/355">#355</a> <a href="https://kagifeedback.org/u/matkoniecz">@matkoniecz</a></li><li>'Time in Equador' returns incorrect offset <a href="https://kagifeedback.org/d/1212" data-id="1212"><span></span></a><a href="https://kagifeedback.org/d/1212">#1212</a> <a href="https://kagifeedback.org/u/SamSkjord">@SamSkjord</a></li><li>Clock Widget - No/Odd results for some european microstates <a href="https://kagifeedback.org/d/25" data-id="25"><span></span></a><a href="https://kagifeedback.org/d/25">#25</a> <a href="https://kagifeedback.org/u/Deucalion">@Deucalion</a></li><li>Wrong timezone conversion for IST to CEST and wrong usage of CET/CEST <a href="https://kagifeedback.org/d/1669" data-id="1669"><span></span></a><a href="https://kagifeedback.org/d/1669">#1669</a> <a href="https://kagifeedback.org/u/Nankeru">@Nankeru</a></li><li>Widget for time span (e.g.  "38 days from now", "38 days from today") <a href="https://kagifeedback.org/d/119" data-id="119"><span></span></a><a href="https://kagifeedback.org/d/119">#119</a> <a href="https://kagifeedback.org/u/yokoffing">@yokoffing</a></li><li>Nautical miles unit conversion <a href="https://kagifeedback.org/d/1117" data-id="1117"><span></span></a><a href="https://kagifeedback.org/d/1117">#1117</a> <a href="https://kagifeedback.org/u/dharmab">@dharmab</a></li><li>Currency conversion search too slow <a href="https://kagifeedback.org/d/1116" data-id="1116"><span></span></a><a href="https://kagifeedback.org/d/1116">#1116</a> <a href="https://kagifeedback.org/u/Tomotake">@Tomotake</a></li><li>Fl oz conversion does not work in all regions <a href="https://kagifeedback.org/d/1883" data-id="1883"><span></span></a><a href="https://kagifeedback.org/d/1883">#1883</a> <a href="https://kagifeedback.org/u/mon">@mon</a></li><li>Convert kJ to Calories <a href="https://kagifeedback.org/d/2775" data-id="2775"><span></span></a><a href="https://kagifeedback.org/d/2775">#2775</a> <a href="https://kagifeedback.org/u/gateway">@gateway</a></li><li>Weird behavior when converting temperature <a href="https://kagifeedback.org/d/557" data-id="557"><span></span></a><a href="https://kagifeedback.org/d/557">#557</a> <a href="https://kagifeedback.org/u/trekt">@trekt</a></li><li>Incorrect decimal separator for calculation results <a href="https://kagifeedback.org/d/1336" data-id="1336"><span></span></a><a href="https://kagifeedback.org/d/1336">#1336</a> <a href="https://kagifeedback.org/u/hmnd">@hmnd</a></li><li>Allow commas for large numbers in calculator <a href="https://kagifeedback.org/d/136" data-id="136"><span></span></a><a href="https://kagifeedback.org/d/136">#136</a> <a href="https://kagifeedback.org/u/lacikawiz">@lacikawiz</a></li><li>Time conversion doesn't use daylight savings <a href="https://kagifeedback.org/d/344" data-id="344"><span></span></a><a href="https://kagifeedback.org/d/344">#344</a> <a href="https://kagifeedback.org/u/rozbb">@rozbb</a></li><li>Need more decimal places in USD-BTC conversion <a href="https://kagifeedback.org/d/1256" data-id="1256"><span></span></a><a href="https://kagifeedback.org/d/1256">#1256</a> <a href="https://kagifeedback.org/u/SK">@SK</a></li><li>Time conversion to daylight time <a href="https://kagifeedback.org/d/1433" data-id="1433"><span></span></a><a href="https://kagifeedback.org/d/1433">#1433</a> <a href="https://kagifeedback.org/u/matkam">@matkam</a></li><li>Currency Conversion Error <a href="https://kagifeedback.org/d/2910" data-id="2910"><span></span></a><a href="https://kagifeedback.org/d/2910">#2910</a> <a href="https://kagifeedback.org/u/cempack">@cempack</a></li><li>Calculator widget doesn't support shortenings of storage units (GB vs gigabyte) <a href="https://kagifeedback.org/d/1642" data-id="1642"><span></span></a><a href="https://kagifeedback.org/d/1642">#1642</a> <a href="https://kagifeedback.org/u/Grooty">@Grooty</a></li><li>Wrong calculator results when region set to Singapore <a href="https://kagifeedback.org/d/1216" data-id="1216"><span></span></a><a href="https://kagifeedback.org/d/1216">#1216</a> <a href="https://kagifeedback.org/u/bh">@bh</a></li><li>Math calculations round to 0 after e8 <a href="https://kagifeedback.org/d/2767" data-id="2767"><span></span></a><a href="https://kagifeedback.org/d/2767">#2767</a> <a href="https://kagifeedback.org/u/rudyfink">@rudyfink</a></li><li>Calculator ignores commas. <a href="https://kagifeedback.org/d/2323" data-id="2323"><span></span></a><a href="https://kagifeedback.org/d/2323">#2323</a> <a href="https://kagifeedback.org/u/guissmo">@guissmo</a></li></ul><h2>In other news</h2><p>Kagi got a <a href="https://en.wikipedia.org/wiki/Kagi_%28search_engine%29" rel="ugc nofollow">Wikipedia page</a>!</p></div></div><div id="3122"><h2><span>Feb 8, 2024 - Ultimate features available for Family / Duo plans</span><a href="https://kagi.com/changelog#3122"> #</a></h2><div><h3>Features</h3><ul><li>We're happy to announce that Family and Duo plan members can now upgrade to Ultimate plan features for just $15 per month. per family member upgraded. The Ultimate plan includes access to the latest AI models, such as GPT-4/GPT-4-Turbo, Claude 2.1 (100k) and soon Gemini Ultra. To upgrade your account or that of another member, simply visit the "Members" section under <a href="https://www.kagi.com/settings?p=account_members" rel="ugc nofollow">Family Settings</a>.</li></ul><p> <a href="https://kagifeedback.org/d/2024" data-id="2024"><span></span></a><a href="https://kagifeedback.org/d/2024">#2024</a> <a href="https://kagifeedback.org/u/nucleardog">@nucleardog</a></p><p><img src="https://kagifeedback.org/assets/files/2024-02-08/1707408105-538813-image.png" title="" alt=""></p><ul><li><a href="https://apps.apple.com/us/app/kagi-for-safari/id1622835804" rel="ugc nofollow">Kagi for Safari 2.2.0</a> is released, fixing many previous issues reported by the users.</li></ul><p><img src="https://kagifeedback.org/assets/files/2024-02-08/1707412278-840815-image.png" title="" alt=""></p><ul><li>We've improved the mobile experience for Assistant users. Now, Now, when composing your prompt, you can effortlessly access settings for Assistant, including options for Research mode and Chat mode. Additionally, the entire prompt is now fully visible.<br><img src="https://kagifeedback.org/assets/files/2024-02-08/1707415219-743952-image.png" title="" alt=""></li></ul><h3>Improvements &amp; Bug fixes</h3><ul><li>Kagi search on Firefox ESR 78 give JS error (previously fixed but happening again) <a href="https://kagifeedback.org/d/3055" data-id="3055"><span></span></a><a href="https://kagifeedback.org/d/3055">#3055</a> <a href="https://kagifeedback.org/u/kagi-not-working">@kagi-not-working</a> </li><li>Weather Widget Doesn't Display when Asking for Temperature <a href="https://kagifeedback.org/d/3046" data-id="3046"><span></span></a><a href="https://kagifeedback.org/d/3046">#3046</a> <a href="https://kagifeedback.org/u/bhagwad">@bhagwad</a></li><li>/ (slash) keyboard shortcut doesn't scroll to top in safari <a href="https://kagifeedback.org/d/2989" data-id="2989"><span></span></a><a href="https://kagifeedback.org/d/2989">#2989</a> <a href="https://kagifeedback.org/u/nullable">@nullable</a></li><li>Quick answer does not always show up on results page <a href="https://kagifeedback.org/d/3035" data-id="3035"><span></span></a><a href="https://kagifeedback.org/d/3035">#3035</a> <a href="https://kagifeedback.org/u/stoyle">@stoyle</a></li> </ul></div></div><div id="3106"><h2><span>Feb 6, 2024 - Two-factor authentication</span><a href="https://kagi.com/changelog#3106"> #</a></h2><div><h3>Features</h3><ul><li><p>Two-factor authentication (2FA) is now possible to further secure your Kagi account . This was the most upvoted feature on kagifeedback.org and we are glad to (finally) deliver it. You can <a href="https://kagi.com/settings?p=2fa_setup" rel="ugc nofollow">set it up</a> on your Settings page under the "Account" section. <a href="https://kagifeedback.org/u/Kai">@Kai</a> in <a href="https://kagifeedback.org/d/14" data-id="14"><span></span></a><a href="https://kagifeedback.org/d/14">#14</a></p> <p><img src="https://kagifeedback.org/assets/files/2024-02-06/1707252573-299972-screenshot-2024-02-06-at-124926.png" title="" alt=""></p></li></ul><ul><li><p>We added indication of results coming from Kagi's own index. Look for doggo graphics in results information popup.</p>  <p><img src="https://kagifeedback.org/assets/files/2024-02-06/1707236132-683351-screenshot-2024-02-06-at-081525.png" title="" alt=""></p></li></ul><ul><li>We've introduced the possibility to customise how hours are displayed in your account—choose between the 12 and 24-hour formats. To access this option, simply navigate to "General" under your account settings.<br><img src="https://kagifeedback.org/assets/files/2024-02-06/1707230712-907319-image.png" title="" alt=""></li></ul><h3>Improvements &amp; Bug fixes</h3><ul><li>Issues with how Quick Answer refers to search result items <a href="https://kagifeedback.org/d/3000" data-id="3000"><span></span></a><a href="https://kagifeedback.org/d/3000">#3000</a> <a href="https://kagifeedback.org/u/leftium">@leftium</a> </li><li>We continued tacking accessibility issues reported in <a href="https://kagifeedback.org/d/2923" data-id="2923"><span></span></a><a href="https://kagifeedback.org/d/2923">#2923</a> <a href="https://kagifeedback.org/u/darekkay">@darekkay</a> </li><li>Additional features for news and other articles <a href="https://kagifeedback.org/d/2932" data-id="2932"><span></span></a><a href="https://kagifeedback.org/d/2932">#2932</a> <a href="https://kagifeedback.org/u/Dumb">@Dumb</a></li><li>Calculator broken for basic mathematics <a href="https://kagifeedback.org/d/3087" data-id="3087"><span></span></a><a href="https://kagifeedback.org/d/3087">#3087</a> <a href="https://kagifeedback.org/u/bgeron">@bgeron</a> </li><li>Wrong title for search result <a href="https://kagifeedback.org/d/2984" data-id="2984"><span></span></a><a href="https://kagifeedback.org/d/2984">#2984</a> <a href="https://kagifeedback.org/u/strager">@strager</a></li><li>Translation/Localization on "Phone" button and "Opens soon X PM/AM" <a href="https://kagifeedback.org/d/2976" data-id="2976"><span></span></a><a href="https://kagifeedback.org/d/2976">#2976</a> <a href="https://kagifeedback.org/u/TheLastEnvoy">@TheLastEnvoy</a></li><li>Relaxed password restrictions to meet most <a href="https://pages.nist.gov/800-63-3/sp800-63b.html" rel="ugc nofollow">recent standards</a></li><li>Redirect rules do not trim white space <a href="https://kagifeedback.org/d/3064" data-id="3064"><span></span></a><a href="https://kagifeedback.org/d/3064">#3064</a> <a href="https://kagifeedback.org/u/gunslingerfry">@gunslingerfry</a></li><li>Safari for iOS Results Page Too Wide <a href="https://kagifeedback.org/d/3026" data-id="3026"><span></span></a><a href="https://kagifeedback.org/d/3026">#3026</a> <a href="https://kagifeedback.org/u/TVPaulD">@TVPaulD</a> </li><li>Site details can be dismissed before it finishes appearing <a href="https://kagifeedback.org/d/1798" data-id="1798"><span></span></a><a href="https://kagifeedback.org/d/1798">#1798</a> <a href="https://kagifeedback.org/u/tuesday">@tuesday</a></li><li>Switch between search and assistant mode without clearing the search <a href="https://kagifeedback.org/d/2390" data-id="2390"><span></span></a><a href="https://kagifeedback.org/d/2390">#2390</a> <a href="https://kagifeedback.org/u/mackid1993">@mackid1993</a></li><li>Inline LaTeX response in quick answer not rendering properly <a href="https://kagifeedback.org/d/3008" data-id="3008"><span></span></a><a href="https://kagifeedback.org/d/3008">#3008</a> <a href="https://kagifeedback.org/u/gladiator2339">@gladiator2339</a> <ul><li>$_latex_inline in AI output <a href="https://kagifeedback.org/d/3073" data-id="3073"><span></span></a><a href="https://kagifeedback.org/d/3073">#3073</a> <a href="https://kagifeedback.org/u/MightyPork">@MightyPork</a> </li> <li>Quick Answer renders "$" as "$_latex_inline" <a href="https://kagifeedback.org/d/3032" data-id="3032"><span></span></a><a href="https://kagifeedback.org/d/3032">#3032</a> <a href="https://kagifeedback.org/u/arinazari">@arinazari</a></li></ul></li><li>Maps:   <ul><li>Improve the UI-layout of the Inline-Maps Component on the Main Page</li>  <li>Fix rendering bug in Directions</li>  <li>Improvements to UI layout of POI Infobox</li></ul></li></ul>  <br></div></div><div id="3051"><h2><span>Jan 30, 2024 - Misc improvements and bug fixes</span><a href="https://kagi.com/changelog#3051"> #</a></h2><div><h3>Improvements &amp; Bug fixes</h3><ul><li>The weather widget now features a location button for users to set their precise location</li><li>Region search doesn't work when a lens is active <a href="https://kagifeedback.org/d/2933" data-id="2933"><span></span></a><a href="https://kagifeedback.org/d/2933">#2933</a> <a href="https://kagifeedback.org/u/Vapid">@Vapid</a></li><li>Inconsistent enrichment API results <a href="https://kagifeedback.org/d/2888" data-id="2888"><span></span></a><a href="https://kagifeedback.org/d/2888">#2888</a> <a href="https://kagifeedback.org/u/Value7609">@Value7609</a> </li><li>!m Maps Bang doesn't search <a href="https://kagifeedback.org/d/2945" data-id="2945"><span></span></a><a href="https://kagifeedback.org/d/2945">#2945</a> <a href="https://kagifeedback.org/u/andyrew1">@andyrew1</a></li><li>Search param dropdowns stay open <a href="https://kagifeedback.org/d/2902" data-id="2902"><span></span></a><a href="https://kagifeedback.org/d/2902">#2902</a> <a href="https://kagifeedback.org/u/jrileyh">@jrileyh</a></li><li>Blank page after signup <a href="https://kagifeedback.org/d/2939" data-id="2939"><span></span></a><a href="https://kagifeedback.org/d/2939">#2939</a> <a href="https://kagifeedback.org/u/petiole">@petiole</a> </li><li>Time Ascending/Descending should contextually change to facilitate understanding <a href="https://kagifeedback.org/d/1387" data-id="1387"><span></span></a><a href="https://kagifeedback.org/d/1387">#1387</a> <a href="https://kagifeedback.org/u/kf">@kf</a></li><li>Search input field initially scrolled out of view on mobile <a href="https://kagifeedback.org/d/2958" data-id="2958"><span></span></a><a href="https://kagifeedback.org/d/2958">#2958</a> <a href="https://kagifeedback.org/u/tacocat">@tacocat</a></li><li>Increased number of image search results <a href="https://kagifeedback.org/d/2785" data-id="2785"><span></span></a><a href="https://kagifeedback.org/d/2785">#2785</a> <a href="https://kagifeedback.org/u/KimLaughton">@KimLaughton</a></li><li>Maps search is broken; just returns local area <a href="https://kagifeedback.org/d/3015" data-id="3015"><span></span></a><a href="https://kagifeedback.org/d/3015">#3015</a> <a href="https://kagifeedback.org/u/jamescridland">@jamescridland</a></li></ul><h3>Assistant</h3><p>Research Assistant now displays the uploaded reference photo on the right side during chat conversations for easy reference<br><img src="https://kagifeedback.org/assets/files/2024-01-30/1706616058-706586-image.png" title="" alt=""></p><ul><li>Universal Summarizer now shows the reading time "saved" by summarizing a web page</li><li>Make Kagi Assistant's sources respect regex redirects <a href="https://kagifeedback.org/d/2602" data-id="2602"><span></span></a><a href="https://kagifeedback.org/d/2602">#2602</a> <a href="https://kagifeedback.org/u/Kel">@Kel</a></li><li>Research is not being given full context of the conversation <a href="https://kagifeedback.org/d/2950" data-id="2950"><span></span></a><a href="https://kagifeedback.org/d/2950">#2950</a> <a href="https://kagifeedback.org/u/httpjames">@httpjames</a></li><li>Assistant input form doesn't allow newlines on mobile <a href="https://kagifeedback.org/d/2731" data-id="2731"><span></span></a><a href="https://kagifeedback.org/d/2731">#2731</a> <a href="https://kagifeedback.org/u/EvacuatedTerminal">@EvacuatedTerminal</a></li><li>Assistant switching language during conversation <a href="https://kagifeedback.org/d/2982" data-id="2982"><span></span></a><a href="https://kagifeedback.org/d/2982">#2982</a> <a href="https://kagifeedback.org/u/lou">@lou</a></li></ul><h3>In other news</h3><ul><li><a href="https://teclis.com/" rel="ugc nofollow">Teclis</a> is live again. Teclis surfaces most of Kagi's own index (non-commercial content) in a public way. Teclis is a hobby project by Kagi founder,  maintained on a best effort single person basis, just to set the right expectations.</li></ul></div></div><div id="3005"><h2><span>Jan 24, 2024 - Celebrating 20k members</span><a href="https://kagi.com/changelog#3005"> #</a></h2><div><h3>Announcements</h3><p>Today, we’re happy and proud to have reached <strong>20,000 members</strong>.</p>  <p>We have a special surprise for our community. Read everything about it in our <a href="https://blog.kagi.com/celebrating-20k" rel="ugc nofollow">blog post</a>.</p><p><img src="https://kagifeedback.org/assets/files/2024-01-23/1706048250-795569-kagi-stickers.png" title="" alt=""></p><h3>Features</h3><ul><li>Kagi search extension for <a href="https://chromewebstore.google.com/detail/kagi-search-for-chrome/cdglnehniifkbagbbombnjghhcihifij?hl=en-Us" rel="ugc nofollow">Chrome</a> and <a href="https://addons.mozilla.org/en-US/firefox/addon/kagi-search-for-firefox/" rel="ugc nofollow">Firefox</a> 0.5.0 released  <ul><li>Added support for FastGPT</li>  <li>Fix: Kagi Extension API Key Not Persistent in Firefox ESR on Kali Linux <a href="https://kagifeedback.org/d/2234" data-id="2234"><span></span></a><a href="https://kagifeedback.org/d/2234">#2234</a> @Maxpl01Z</li>  <li>Fix: Kagi keeps saying invalid session token in firefox on linux after setting up successfully and working fine for a while. <a href="https://kagifeedback.org/d/2090" data-id="2090"><span></span></a><a href="https://kagifeedback.org/d/2090">#2090</a> @bkw777a</li>  <li>Fix: Firefox default search engine keeps resetting <a href="https://kagifeedback.org/d/2748" data-id="2748"><span></span></a><a href="https://kagifeedback.org/d/2748">#2748</a> <a href="https://kagifeedback.org/u/Tulip">@Tulip</a></li> </ul></li></ul><p><img src="https://kagifeedback.org/assets/files/2024-01-24/1706110928-788953-screenshot-2024-01-24-at-074142.png" title="" alt=""></p><ul><li>We've introduced an option to disable all personalizations (like blocked or raised domains) for your current search. Just click on "Options" — found at the top of the search results — and deselect "Personalized". Suggested by  <a href="https://kagifeedback.org/d/1943" data-id="1943"><span></span></a><a href="https://kagifeedback.org/d/1943">#1943</a> <a href="https://kagifeedback.org/u/stoyle">@stoyle</a><br><img src="https://kagifeedback.org/assets/files/2024-01-24/1706136800-302539-image.png" title="" alt=""></li></ul><ul><li>New, automated <a href="https://status.kagi.com/" rel="ugc nofollow">status page</a> following the learnings from the <a href="https://status.kagi.com/clrnl9zwl97290beoine8zlvzx" rel="ugc nofollow">post-mortem</a> last week</li></ul><p><img src="https://kagifeedback.org/assets/files/2024-01-24/1706111630-443135-screenshot-2024-01-24-at-075345.png" title="" alt=""></p><h3>Improvements &amp; Bug fixes</h3><ul><li>Accessibility improvements <a href="https://kagifeedback.org/d/2923" data-id="2923"><span></span></a><a href="https://kagifeedback.org/d/2923">#2923</a> <a href="https://kagifeedback.org/u/darekkay">@darekkay</a></li><li>Perform a reverse image search with Kagi by pasting any image directly from your clipboard into the search bar. Suggested by <a href="https://kagifeedback.org/d/1704" data-id="1704"><span></span></a><a href="https://kagifeedback.org/d/1704">#1704</a> <a href="https://kagifeedback.org/u/VIEWVIEWVIEW">@VIEWVIEWVIEW</a></li><li>Search yields no results <a href="https://kagifeedback.org/d/2953" data-id="2953"><span></span></a><a href="https://kagifeedback.org/d/2953">#2953</a> <a href="https://kagifeedback.org/u/flat_reward">@flat_reward</a></li><li>"Time in Japan" shows the Wikipedia page for "suicide in Japan" <a href="https://kagifeedback.org/d/2877" data-id="2877"><span></span></a><a href="https://kagifeedback.org/d/2877">#2877</a> <a href="https://kagifeedback.org/u/fexii">@fexii</a></li><li>Image search not respecting minus / negative operator <a href="https://kagifeedback.org/d/2884" data-id="2884"><span></span></a><a href="https://kagifeedback.org/d/2884">#2884</a> <a href="https://kagifeedback.org/u/Tiny_Beetle">@Tiny_Beetle</a></li><li>Bangs have become case sensitive <a href="https://kagifeedback.org/d/2946" data-id="2946"><span></span></a><a href="https://kagifeedback.org/d/2946">#2946</a> <a href="https://kagifeedback.org/u/httpjames">@httpjames</a></li><li>Lens descriptions in settings are all truncated <a href="https://kagifeedback.org/d/2916" data-id="2916"><span></span></a><a href="https://kagifeedback.org/d/2916">#2916</a> <a href="https://kagifeedback.org/u/ValPolyakh">@ValPolyakh</a></li><li>Issues with iOS keyboard cursor swipe gesture <a href="https://kagifeedback.org/d/1759" data-id="1759"><span></span></a><a href="https://kagifeedback.org/d/1759">#1759</a> <a href="https://kagifeedback.org/u/TyPell91">@TyPell91</a> </li><li>Weather Widget Location Issue. <a href="https://kagifeedback.org/d/2852" data-id="2852"><span></span></a><a href="https://kagifeedback.org/d/2852">#2852</a> <a href="https://kagifeedback.org/u/cempack">@cempack</a></li><li>News and Web Search Returns Out of Date Results vs. Google <a href="https://kagifeedback.org/d/2764" data-id="2764"><span></span></a><a href="https://kagifeedback.org/d/2764">#2764</a> <a href="https://kagifeedback.org/u/CrunchyFritos">@CrunchyFritos</a></li><li>Add a feedback button to the mobile menu <a href="https://kagifeedback.org/d/1616" data-id="1616"><span></span></a><a href="https://kagifeedback.org/d/1616">#1616</a> <a href="https://kagifeedback.org/u/thislooksfun">@thislooksfun</a></li><li>Redirect rule does not seem to be applied to "Interesting Finds" <a href="https://kagifeedback.org/d/2967" data-id="2967"><span></span></a><a href="https://kagifeedback.org/d/2967">#2967</a> <a href="https://kagifeedback.org/u/frereit">@frereit</a></li><li>Some links in the wikipedia article preview do not work <a href="https://kagifeedback.org/d/2977" data-id="2977"><span></span></a><a href="https://kagifeedback.org/d/2977">#2977</a> <a href="https://kagifeedback.org/u/tdf">@tdf</a></li><li>"Show more" button when there's nothing more to show <a href="https://kagifeedback.org/d/2980" data-id="2980"><span></span></a><a href="https://kagifeedback.org/d/2980">#2980</a> <a href="https://kagifeedback.org/u/eikowagenknecht">@eikowagenknecht</a></li></ul><h3>Assistant</h3><ul><li>We've added Mistral Medium to our Assistant, available to Ultimate subscribers<br><img src="https://kagifeedback.org/assets/files/2024-01-24/1706106465-144740-image.png" title="" alt=""></li></ul><ul><li>Research Assistant now respects your preferences for domain rankings when gathering information. It recognizes the websites you've pinned, promoted, demoted, or blocked, ensuring tailored search results. Inspired by <a href="https://kagifeedback.org/d/2533" data-id="2533"><span></span></a><a href="https://kagifeedback.org/d/2533">#2533</a> <a href="https://kagifeedback.org/u/Zambyte">@Zambyte</a></li><li>When invoking chat in assistant wrong model is selected <a href="https://kagifeedback.org/d/2909" data-id="2909"><span></span></a><a href="https://kagifeedback.org/d/2909">#2909</a> <a href="https://kagifeedback.org/u/stoyle">@stoyle</a></li><li>Assistant Chat GPT4 truncates Input on Mobile <a href="https://kagifeedback.org/d/2882" data-id="2882"><span></span></a><a href="https://kagifeedback.org/d/2882">#2882</a> <a href="https://kagifeedback.org/u/jhkmnl">@jhkmnl</a></li><li>FastGPT should autofocus on the prompt field on load <a href="https://kagifeedback.org/d/2676" data-id="2676"><span></span></a><a href="https://kagifeedback.org/d/2676">#2676</a> <a href="https://kagifeedback.org/u/mhitza">@mhitza</a> </li><li>Support for research assistant math results with latex <a href="https://kagifeedback.org/d/2925" data-id="2925"><span></span></a><a href="https://kagifeedback.org/d/2925">#2925</a> <a href="https://kagifeedback.org/u/Mg432">@Mg432</a></li><li>Code examples show "$_latex_inline" in place of any $ character <a href="https://kagifeedback.org/d/2966" data-id="2966"><span></span></a><a href="https://kagifeedback.org/d/2966">#2966</a> <a href="https://kagifeedback.org/u/mhersh">@mhersh</a></li></ul></div></div><div id="2931"><h2><span>Jan 16, 2024 - Quick Peek, Enhanced Local Business search, Usenet lens, GPT4-vision in Assistant, and Outage Post-Mortem</span><a href="https://kagi.com/changelog#2931"> #</a></h2><div><h2>Announcements</h2> <ul><li>We added "<strong>Quick Peek</strong>" widget to our results. With this addition you will find additional relevant results about the query you are searching. This feature can be easily enabled / disabled in the <a href="https://kagi.com/settings?p=search" rel="ugc nofollow">Search Settings</a>.</li></ul><p><img src="https://kagifeedback.org/assets/files/2024-01-16/1705427357-987277-screenshot-2024-01-16-at-094910.png" title="" alt=""></p>  <br> <ul><li>We added <strong>additional sources for local businesses</strong> to our inline maps search results. Inline maps should be more responsive now when searching for specific local businesses. More improvements to come. Related issue: Local business / open hours <a href="https://kagifeedback.org/d/2477" data-id="2477"><span></span></a><a href="https://kagifeedback.org/d/2477">#2477</a> <a href="https://kagifeedback.org/u/partlycloudy">@partlycloudy</a></li></ul><p><img src="https://kagifeedback.org/assets/files/2024-01-16/1705427049-489431-screenshot-2024-01-16-at-094320.png" title="" alt=""></p><ul><li>Find hidden treasures from the early days of the web through the new <strong>Usenet / Archive search lens</strong></li> </ul><p><img src="https://kagifeedback.org/assets/files/2024-01-16/1705428002-248272-screenshot-2024-01-16-at-095955.png" title="" alt=""></p> <ul><li><p>Kagi Assistant (available as open beta for Ultimate members) now leverages <strong>GPT4-vision model</strong> to better understand and describe images. You can test this improved functionality by uploading images or providing image URLs for the Kagi Assistant to analyse.</p>  <p>Edit: A member just emailed saying "the example you have is wrong about 7 of the 10 "Key Points" (only the restaurant name, VAT amount, and amount tendered are correct)." Yes that can be the case with LLMs, we are not trying to present this as grounbreaking, we just integrated a model and the example is as good as the underlying model. This is clearly demonstrating its limitations and it is what it is. We are currently using the best commercially available vision model on the market and it is our desire to emphasize that access to this and other world's cutting edge LLM's are all included in the Assistant with one Kagi subscription. No doubt they will get better in the future.</p></li></ul><p><img src="https://kagifeedback.org/assets/files/2024-01-16/1705427862-716303-screenshot-2024-01-16-at-095718.png" title="" alt=""></p><h3>Outage post-mortem</h3> <ul><li>We've dissected last Thursday's events and the steps we're taking in response in a detailed <a href="https://status.kagi.com/issues/2024-01-12-kagi-down-on-some-regions/" rel="ugc nofollow">post-mortem report</a>. Thank you for bearing with us, and please look forward to a more robust service as we continue to improve.</li></ul><h2>Improvements &amp; Bug fixes</h2><ul><li>Some summarized results display wrong language <a href="https://kagifeedback.org/d/2907" data-id="2907"><span></span></a><a href="https://kagifeedback.org/d/2907">#2907</a> <a href="https://kagifeedback.org/u/Dustin">@Dustin</a> </li><li>Clear search button doesn’t work (Mobile Orion) <a href="https://kagifeedback.org/d/2870" data-id="2870"><span></span></a><a href="https://kagifeedback.org/d/2870">#2870</a> <a href="https://kagifeedback.org/u/jeffdaley">@jeffdaley</a></li><li>Long filename text overflows in Assistant <a href="https://kagifeedback.org/d/2631" data-id="2631"><span></span></a><a href="https://kagifeedback.org/d/2631">#2631</a> <a href="https://kagifeedback.org/u/sw">@sw</a></li><li>Research Assistant incorrectly citing sources as "Item X"  <a href="https://kagifeedback.org/d/2908" data-id="2908"><span></span></a><a href="https://kagifeedback.org/d/2908">#2908</a> <a href="https://kagifeedback.org/u/Buffalo_Tree">@Buffalo_Tree</a></li><li>Assistant responses rendering real HTML elements <a href="https://kagifeedback.org/d/2650" data-id="2650"><span></span></a><a href="https://kagifeedback.org/d/2650">#2650</a> <a href="https://kagifeedback.org/u/httpjames">@httpjames</a></li><li>Fail to apply redirection rule to widget (e.g. wikipedia widget) in search result <a href="https://kagifeedback.org/d/2801" data-id="2801"><span></span></a><a href="https://kagifeedback.org/d/2801">#2801</a> <a href="https://kagifeedback.org/u/Fernandez">@Fernandez</a></li><li>Lens excluded words not working <a href="https://kagifeedback.org/d/2643" data-id="2643"><span></span></a><a href="https://kagifeedback.org/d/2643">#2643</a> <a href="https://kagifeedback.org/u/Recast">@Recast</a> </li><li>Setting theme-color meta attribute doesn't work <a href="https://kagifeedback.org/d/2868" data-id="2868"><span></span></a><a href="https://kagifeedback.org/d/2868">#2868</a> <a href="https://kagifeedback.org/u/Holger">@Holger</a></li><li>Short terms &amp; quick answers add horizontal scrolling on mobile <a href="https://kagifeedback.org/d/2865" data-id="2865"><span></span></a><a href="https://kagifeedback.org/d/2865">#2865</a> <a href="https://kagifeedback.org/u/Vapid">@Vapid</a></li><li>Rendring markdown code snippets etc, is wrong in lists <a href="https://kagifeedback.org/d/2724" data-id="2724"><span></span></a><a href="https://kagifeedback.org/d/2724">#2724</a> <a href="https://kagifeedback.org/u/stoyle">@stoyle</a></li><li>Sending two different questions to FastGPT causes responses to glitch <a href="https://kagifeedback.org/d/2480" data-id="2480"><span></span></a><a href="https://kagifeedback.org/d/2480">#2480</a> <a href="https://kagifeedback.org/u/DeltAndy">@DeltAndy</a></li><li>Transparent image filters not working <a href="https://kagifeedback.org/d/2402" data-id="2402"><span></span></a><a href="https://kagifeedback.org/d/2402">#2402</a> <a href="https://kagifeedback.org/u/jesus">@jesus</a></li><li>Kagi search personalised results bug <a href="https://kagifeedback.org/d/2867" data-id="2867"><span></span></a><a href="https://kagifeedback.org/d/2867">#2867</a> <a href="https://kagifeedback.org/u/Repacking6528">@Repacking6528</a></li><li>Country Codes should work when searching in Country Menu <a href="https://kagifeedback.org/d/998" data-id="998"><span></span></a><a href="https://kagifeedback.org/d/998">#998</a> <a href="https://kagifeedback.org/u/Imperator_of_all">@Imperator_of_all</a></li><li>Shield icon and popup panel metric are unclear how they relate <a href="https://kagifeedback.org/d/2668" data-id="2668"><span></span></a><a href="https://kagifeedback.org/d/2668">#2668</a> <a href="https://kagifeedback.org/u/laserdinosaur">@laserdinosaur</a></li><li>[Discuss Document] SyntaxError: Unexpected token <a href="https://kagifeedback.org/d/2843" data-id="2843"><span></span></a><a href="https://kagifeedback.org/d/2843">#2843</a> <a href="https://kagifeedback.org/u/RMcCurdyDOTcom">@RMcCurdyDOTcom</a></li><li>Universal Summarizer Hallucination <a href="https://kagifeedback.org/d/2652" data-id="2652"><span></span></a><a href="https://kagifeedback.org/d/2652">#2652</a> <a href="https://kagifeedback.org/u/CrunchyFritos">@CrunchyFritos</a></li><li>Universal summarizer doesn't show in Korean <a href="https://kagifeedback.org/d/2891" data-id="2891"><span></span></a><a href="https://kagifeedback.org/d/2891">#2891</a> <a href="https://kagifeedback.org/u/HanbyulKimLuke">@HanbyulKimLuke</a></li><li>Quick answer messes up viewport on mobile <a href="https://kagifeedback.org/d/2924" data-id="2924"><span></span></a><a href="https://kagifeedback.org/d/2924">#2924</a> <a href="https://kagifeedback.org/u/stoyle">@stoyle</a></li><li>Lowering the ranking of gov.cn doesn't work <a href="https://kagifeedback.org/d/2681" data-id="2681"><span></span></a><a href="https://kagifeedback.org/d/2681">#2681</a> <a href="https://kagifeedback.org/u/Peter">@Peter</a></li><li>Images from text results show missing image for a few seconds after clicking <a href="https://kagifeedback.org/d/2844" data-id="2844"><span></span></a><a href="https://kagifeedback.org/d/2844">#2844</a> <a href="https://kagifeedback.org/u/olly_kf">@olly_kf</a></li><li>Metacritic bang command (!mc) gives a 404, new URL needed <a href="https://kagifeedback.org/d/2920" data-id="2920"><span></span></a><a href="https://kagifeedback.org/d/2920">#2920</a> <a href="https://kagifeedback.org/u/pdm">@pdm</a></li></ul></div></div><div id="2856"><h2><span>Jan 5, 2024 - Lemmy search lens and Safari extension update</span><a href="https://kagi.com/changelog#2856"> #</a></h2><div><h3>📢 Announcements</h3><ul><li><p>We have rolled back Kagi Search for Safari extension because of issues reported by users. The URL to download the rolled back version is the same <a href="https://apps.apple.com/us/app/kagi-search-for-safari/id1622835804" rel="ugc nofollow">apps.apple.com/us/app/kagi-search-for-safari/id1622835804</a>. We are continuing the development of the 2.0 branch which is open source and you can follow the discussion <a href="https://github.com/kagisearch/browser_extensions/pull/59" rel="ugc nofollow">here</a>.</p> </li><li><p>Kagi is popular on Lemmy and a lot of Lemmy users are using Kagi. We have released the first version of a Lemmy/Kbin search lens.  <a href="https://kagifeedback.org/d/1659" data-id="1659"><span></span></a><a href="https://kagifeedback.org/d/1659">#1659</a> <a href="https://kagifeedback.org/u/Nankeru">@Nankeru</a></p></li></ul><p><img src="https://kagifeedback.org/assets/files/2024-01-05/1704485218-274688-screenshot-2024-01-05-at-120649.png" title="" alt=""></p><ul><li>Quick answer and Summarize page are quicker and using streaming responses</li></ul><h3>🪲 Bug fixes &amp; Improvements</h3><ul><li>Improvements to quality of results in user lenses</li><li>We are showing less subresults for single domain (it was sometimes overwhelming)</li><li>Inline news widget shows more news stories when available</li><li>Weather widget should be displayed when searching for "temperature", "humidity" etc <a href="https://kagifeedback.org/d/2704" data-id="2704"><span></span></a><a href="https://kagifeedback.org/d/2704">#2704</a> <a href="https://kagifeedback.org/u/montag2k">@montag2k</a> </li><li>Cannot highlight search input without risking clearing the entire thing I typed <a href="https://kagifeedback.org/d/2779" data-id="2779"><span></span></a><a href="https://kagifeedback.org/d/2779">#2779</a> <a href="https://kagifeedback.org/u/guissmo">@guissmo</a> </li><li>Irrelevant adult results for Chinese query <a href="https://kagifeedback.org/d/2633" data-id="2633"><span></span></a><a href="https://kagifeedback.org/d/2633">#2633</a> <a href="https://kagifeedback.org/u/energize_detonator">@energize_detonator</a> </li><li>Autosuggest frequently recommends appending 'fnaf' to searches <a href="https://kagifeedback.org/d/2713" data-id="2713"><span></span></a><a href="https://kagifeedback.org/d/2713">#2713</a> <a href="https://kagifeedback.org/u/Ryologic">@Ryologic</a></li><li>Search suggestion box showing &lt;i&gt;-tags <a href="https://kagifeedback.org/d/2820" data-id="2820"><span></span></a><a href="https://kagifeedback.org/d/2820">#2820</a> <a href="https://kagifeedback.org/u/BeNice">@BeNice</a> </li><li>Spurious NSFW result <a href="https://kagifeedback.org/d/2824" data-id="2824"><span></span></a><a href="https://kagifeedback.org/d/2824">#2824</a> <a href="https://kagifeedback.org/u/grrr">@grrr</a> </li><li>Empty Footnotes without References in Discuss Doc <a href="https://kagifeedback.org/d/2805" data-id="2805"><span></span></a><a href="https://kagifeedback.org/d/2805">#2805</a> <a href="https://kagifeedback.org/u/CrunchyFritos">@CrunchyFritos</a></li><li>Landscape mode on iOS does not render search/assistant widgets correctly <a href="https://kagifeedback.org/d/2778" data-id="2778"><span></span></a><a href="https://kagifeedback.org/d/2778">#2778</a> <a href="https://kagifeedback.org/u/stoyle">@stoyle</a></li><li>Fix URL normalization/validation for summarizer</li><li>Random number in quick answer response <a href="https://kagifeedback.org/d/2800" data-id="2800"><span></span></a><a href="https://kagifeedback.org/d/2800">#2800</a> <a href="https://kagifeedback.org/u/bert">@bert</a></li><li>RegEx matching bug in redirect rules for your search results. <a href="https://kagifeedback.org/d/2840" data-id="2840"><span></span></a><a href="https://kagifeedback.org/d/2840">#2840</a> <a href="https://kagifeedback.org/u/Fernandez">@Fernandez</a> </li><li>Maps Search Doesn't Load Android <a href="https://kagifeedback.org/d/2831" data-id="2831"><span></span></a><a href="https://kagifeedback.org/d/2831">#2831</a> <a href="https://kagifeedback.org/u/purpledingo">@purpledingo</a></li><li>"enterprise movie" gives back result with Cyrillic spelling <a href="https://kagifeedback.org/d/2850" data-id="2850"><span></span></a><a href="https://kagifeedback.org/d/2850">#2850</a> <a href="https://kagifeedback.org/u/inesicio">@inesicio</a> </li><li>Info card top is cut off <a href="https://kagifeedback.org/d/2811" data-id="2811"><span></span></a><a href="https://kagifeedback.org/d/2811">#2811</a> <a href="https://kagifeedback.org/u/Kai">@Kai</a></li><li>Timer and stopwatch are broken <a href="https://kagifeedback.org/d/2771" data-id="2771"><span></span></a><a href="https://kagifeedback.org/d/2771">#2771</a> <a href="https://kagifeedback.org/u/drdaeman">@drdaeman</a></li></ul></div></div><div id="2793"><h2><span>Dec 28, 2023 - Improved search results and new extension for Safari</span><a href="https://kagi.com/changelog#2793"> #</a></h2><div><h3>📢 Announcements</h3><ul><li><p>We have added Brave Search API as an additional source of results. With this, Brave API joins the growing list of <a href="https://help.kagi.com/kagi/search-details/search-sources.html" rel="ugc nofollow">Kagi's search sources</a>, ensuring that if you can not find something on Kagi, it does not exist on the web. This will come at no additional cost to you.</p></li><li><p><a href="https://help.kagi.com/kagi/ai/assistant.html" rel="ugc nofollow">Kagi Assistant</a> usage stats are now on the <a href="https://kagi.com/stats" rel="ugc nofollow">stats page</a>. Assistant is still in beta and currently available to Ultimate plan users. We plan to roll it out to all members in January.  <a href="https://kagifeedback.org/d/2141" data-id="2141"><span></span></a><a href="https://kagifeedback.org/d/2141">#2141</a> <a href="https://kagifeedback.org/u/Grooty">@Grooty</a></p></li><li><p>We vastly improved Image search results. Check them <a href="https://kagi.com/images?q=captain+america" rel="ugc nofollow">here</a> including all the powerful filters.<br><img src="https://kagifeedback.org/assets/files/2023-12-28/1703786798-699687-screenshot-2023-12-28-at-100607.png" title="" alt=""></p></li></ul><ul><li><p><a href="https://apps.apple.com/us/app/kagi-search-for-safari/id1622835804?mt=12" rel="ugc nofollow">Kagi Search for Safari Extension 2.0</a> has just launched! We've updated the extension to be cross-platform and rewrote it from scratch for improved reliability, as well as a simpler way to get your Kagi searches going across your macOS and iOS devices, with Apple’s automatic extension state syncing.</p>  <p>Existing users of the iOS Kagi Search for Safari Extension are encouraged to <a href="https://apps.apple.com/us/app/kagi-search-for-safari/id1622835804?mt=12" rel="ugc nofollow">install the cross-platform version</a> from the App Store to continue receiving updates after the iOS-only extension is delisted (in July 2024). Until that time, the iOS-only extension will continue to receive the same updates and run the same underlying code as the cross-platform extension.</p>  <p>The migration to the Safari Web Extensions framework will require you to grant access to the Extension on the search engine during your first run. See the instructions in the app for details.</p>  <p>Release notes for extension:</p>  <p>[Fixed] Redirects should not experience intermittent failures<br>  [Changed] You can override which search engine redirects to Kagi manually. The first time you install/upgrade to this version, the extension will attempt to detect your current Safari search engine<br>  [Changed] You only need to provide extension access to urls on the search engine you are overriding.</p></li></ul><h3>🪲 Bug fixes &amp; Improvements</h3><ul><li>Clickable anchors in changelog for easy sharing of changelog posts</li><li>Added new regional bangs !be_fr, !ca_fr, !ch_fr, !es_ca</li><li>Query suggestions are too "greedy" <a href="https://kagifeedback.org/d/2750" data-id="2750"><span></span></a><a href="https://kagifeedback.org/d/2750">#2750</a> <a href="https://kagifeedback.org/u/dcoates">@dcoates</a> </li><li>Custom CSS does not apply to the Assistant page. <a href="https://kagifeedback.org/d/2534" data-id="2534"><span></span></a><a href="https://kagifeedback.org/d/2534">#2534</a> <a href="https://kagifeedback.org/u/Zambyte">@Zambyte</a> </li><li>Semicolons aren't properly handled in the search results page <a href="https://kagifeedback.org/d/2365" data-id="2365"><span></span></a><a href="https://kagifeedback.org/d/2365">#2365</a> <a href="https://kagifeedback.org/u/laiz">@laiz</a></li><li>Kagi search interface no longer works on Firefox ESR <a href="https://kagifeedback.org/d/2766" data-id="2766"><span></span></a><a href="https://kagifeedback.org/d/2766">#2766</a> <a href="https://kagifeedback.org/u/kagi-not-working">@kagi-not-working</a></li><li>This screen needs to be available on Localazy to be localized <a href="https://kagifeedback.org/d/2714" data-id="2714"><span></span></a><a href="https://kagifeedback.org/d/2714">#2714</a> <a href="https://kagifeedback.org/u/TheLastEnvoy">@TheLastEnvoy</a></li><li>!staples search is broken <a href="https://kagifeedback.org/d/2772" data-id="2772"><span></span></a><a href="https://kagifeedback.org/d/2772">#2772</a> <a href="https://kagifeedback.org/u/sidwolf6583">@sidwolf6583</a></li><li>"Are you finding this answer useful" overlay hard to close <a href="https://kagifeedback.org/d/2195" data-id="2195"><span></span></a><a href="https://kagifeedback.org/d/2195">#2195</a> <a href="https://kagifeedback.org/u/timo">@timo</a></li><li>Opening Quick Answer links in another tab <a href="https://kagifeedback.org/d/2556" data-id="2556"><span></span></a><a href="https://kagifeedback.org/d/2556">#2556</a> <a href="https://kagifeedback.org/u/X145678908765">@X145678908765</a></li><li>Paywall indicator in the news tab <a href="https://kagifeedback.org/d/2663" data-id="2663"><span></span></a><a href="https://kagifeedback.org/d/2663">#2663</a> <a href="https://kagifeedback.org/u/Dumb">@Dumb</a></li><li>Making prorating clearer on the Pricing page <a href="https://kagifeedback.org/d/2736" data-id="2736"><span></span></a><a href="https://kagifeedback.org/d/2736">#2736</a> <a href="https://kagifeedback.org/u/GrygrFlzr">@GrygrFlzr</a></li><li>Page redirects / regexes don't get applied to quick answer citations <a href="https://kagifeedback.org/d/2389" data-id="2389"><span></span></a><a href="https://kagifeedback.org/d/2389">#2389</a> <a href="https://kagifeedback.org/u/xhat">@xhat</a> </li><li>Allow Quick Bangs at end of query <a href="https://kagifeedback.org/d/1434" data-id="1434"><span></span></a><a href="https://kagifeedback.org/d/1434">#1434</a> <a href="https://kagifeedback.org/u/macro">@macro</a> </li><li>Improved forgot password &amp; reset password errors</li><li>Fixed missing HN and Reddit comments on posts</li></ul><p><img src="https://kagifeedback.org/assets/files/2023-12-28/1703789237-596032-screenshot-2023-12-28-at-104712.png" title="" alt=""></p><h3>Assistant</h3><ul><li><p>Faster assistant responses (<strong>over 100 tok/sec for gpt-3.5-turbo</strong>)<br><video controls="" src="https://kagifeedback.org/assets/files/2023-12-28/1703791944-576645-screen-recording-2023-12-28-at-113035.mov"></video></p></li><li><p>Improvements to Fast Research assistant</p></li><li><p>Assistant on iOS and iPadOS, safari or orion, requires reload after question is posed <a href="https://kagifeedback.org/d/2776" data-id="2776"><span></span></a><a href="https://kagifeedback.org/d/2776">#2776</a> <a href="https://kagifeedback.org/u/stoyle">@stoyle</a></p></li></ul><h3>Happy holidays from the Kagi team!</h3><p>Thank you for supporting us in this amazing year. Consider <a href="https://kagi.com/settings?p=gift" rel="ugc nofollow">gifting Kagi </a> to your friends and family and bring the joy of great web search to their homes. See you all in 2024!</p><p><img src="https://kagifeedback.org/assets/files/2023-12-28/1703788045-128686-image.png" title="" alt=""></p></div></div><div id="2762"><h2><span>Dec 21, 2023 - Kagi Search community event and new language regions</span><a href="https://kagi.com/changelog#2762"> #</a></h2><div><h3>📢 Announcements</h3><p>We held our end-of-the-year Kagi Search community event featuring business update, year in review and questions and answers session. You can watch the recording here:</p><p><span data-s9e-mediaembed="youtube"><span><iframe allowfullscreen="" loading="lazy" scrolling="no" src="https://www.youtube.com/embed/DRVY-74lkBA"></iframe></span></span></p><p>If you want to check out only the slide deck of the  business update, click <a href="https://docs.google.com/presentation/d/10DY-qrg6kQ-SKlbbiF9kaE2M_K8_6xrV9JDs5D_wNFk/edit?usp=sharing" rel="ugc nofollow">here</a>.</p><p><img src="https://kagifeedback.org/assets/files/2023-12-21/1703200863-701810-screenshot-2023-12-21-at-152035.png" title="" alt=""></p><p>One of the most requested features from the community was improved language/region support. In this update, we added support for Belgium (fr), Belgium (nl), Canada (en), Canada (fr), Spain (es), Spain (ca), Switzerland (de), Switzerland (fr) regions/languages <a href="https://kagifeedback.org/d/89" data-id="89"><span></span></a><a href="https://kagifeedback.org/d/89">#89</a> <a href="https://kagifeedback.org/u/tom">@tom</a></p><h3>🪲 Bug fixes &amp; Improvements</h3><ul><li>Podcast results don't link to the episodes <a href="https://kagifeedback.org/d/2268" data-id="2268"><span></span></a><a href="https://kagifeedback.org/d/2268">#2268</a> <a href="https://kagifeedback.org/u/jamescridland">@jamescridland</a></li><li>A quick way to block a result domain <a href="https://kagifeedback.org/d/2711" data-id="2711"><span></span></a><a href="https://kagifeedback.org/d/2711">#2711</a> <a href="https://kagifeedback.org/u/truist">@truist</a></li><li>Layout issue when switching plans on mobile <a href="https://kagifeedback.org/d/2721" data-id="2721"><span></span></a><a href="https://kagifeedback.org/d/2721">#2721</a> <a href="https://kagifeedback.org/u/ForumNinja404">@ForumNinja404</a></li><li>PDF tag cut off for long paths <a href="https://kagifeedback.org/d/2710" data-id="2710"><span></span></a><a href="https://kagifeedback.org/d/2710">#2710</a> <a href="https://kagifeedback.org/u/kevin51jiang">@kevin51jiang</a> </li><li>Signup does not validate password <a href="https://kagifeedback.org/d/2696" data-id="2696"><span></span></a><a href="https://kagifeedback.org/d/2696">#2696</a> <a href="https://kagifeedback.org/u/jimbo">@jimbo</a> </li><li>The Summarizer "Discuss further" doesn't seem to work <a href="https://kagifeedback.org/d/2712" data-id="2712"><span></span></a><a href="https://kagifeedback.org/d/2712">#2712</a> <a href="https://kagifeedback.org/u/JanPieter">@JanPieter</a></li><li>Attempting to mouse over Quick Answer feedback window dismisses feedback window <a href="https://kagifeedback.org/d/2664" data-id="2664"><span></span></a><a href="https://kagifeedback.org/d/2664">#2664</a> <a href="https://kagifeedback.org/u/bert">@bert</a></li><li>Stopwatch resets on hour mark <a href="https://kagifeedback.org/d/2500" data-id="2500"><span></span></a><a href="https://kagifeedback.org/d/2500">#2500</a> <a href="https://kagifeedback.org/u/abb128">@abb128</a> </li><li>!answer bang asks me to login when using a Private Session Link search URL in a new private browsing session <a href="https://kagifeedback.org/d/2401" data-id="2401"><span></span></a><a href="https://kagifeedback.org/d/2401">#2401</a> <a href="https://kagifeedback.org/u/fawkesley">@fawkesley</a></li><li>Visit links within location searches slightly off. <a href="https://kagifeedback.org/d/2716" data-id="2716"><span></span></a><a href="https://kagifeedback.org/d/2716">#2716</a> <a href="https://kagifeedback.org/u/gabeio">@gabeio</a></li><li>Redirect user to sign-in instead of sign-up on invalid OAuth2 login</li><li>Proper www to non-www redirection <a href="https://kagifeedback.org/d/2703" data-id="2703"><span></span></a><a href="https://kagifeedback.org/d/2703">#2703</a> <a href="https://kagifeedback.org/u/maddy">@maddy</a> </li><li>Wrong time <a href="https://kagifeedback.org/d/2730" data-id="2730"><span></span></a><a href="https://kagifeedback.org/d/2730">#2730</a> <a href="https://kagifeedback.org/u/AntonMakiievskyi">@AntonMakiievskyi</a></li><li>Russian Wikipedia !bang is broken <a href="https://kagifeedback.org/d/2734" data-id="2734"><span></span></a><a href="https://kagifeedback.org/d/2734">#2734</a> <a href="https://kagifeedback.org/u/fxgn">@fxgn</a></li><li>Update !mcwiki to minecraft.wiki <a href="https://kagifeedback.org/d/2449" data-id="2449"><span></span></a><a href="https://kagifeedback.org/d/2449">#2449</a> <a href="https://kagifeedback.org/u/sbrl">@sbrl</a></li><li>Give more specific user API error when token is malformed</li><li>Clicking on any gift amount on the 'Gift Kagi subscription' page redirects to the main page <a href="https://kagifeedback.org/d/2756" data-id="2756"><span></span></a><a href="https://kagifeedback.org/d/2756">#2756</a> <a href="https://kagifeedback.org/u/vankusss">@vankusss</a></li><li>No custom CSS in Video and News tabs <a href="https://kagifeedback.org/d/2706" data-id="2706"><span></span></a><a href="https://kagifeedback.org/d/2706">#2706</a> <a href="https://kagifeedback.org/u/sw">@sw</a></li><li>Stackoverflow title copied from query rather than site <a href="https://kagifeedback.org/d/1064" data-id="1064"><span></span></a><a href="https://kagifeedback.org/d/1064">#1064</a> <a href="https://kagifeedback.org/u/nicstella">@nicstella</a></li><li>Search suggestions must not disappear if I just keep typing what is already suggested <a href="https://kagifeedback.org/d/1893" data-id="1893"><span></span></a><a href="https://kagifeedback.org/d/1893">#1893</a> <a href="https://kagifeedback.org/u/rs387">@rs387</a></li></ul><h2>Assistant</h2><p>📢 Announcements</p><ul><li>We have implemented a new system allowing for longer input. All modes currently have a max of 6k characters per user query, this will be adjusted (upward) in the future</li></ul><p>✨ Features</p><ul><li>New citation snippet design</li><li>Improved robustness of attachment extraction</li></ul><p>🪲 Bugfixes</p><ul><li>Long inputs with non-alphabetical characters may sometimes result in a 414 error</li><li>Citation snippet double HTML-escaping</li></ul></div></div><div id="2702"><h2><span>Dec 14th, 2023</span><a href="https://kagi.com/changelog#2702"> #</a></h2><div><h3>🪲 Bug fixes &amp; Improvements</h3><ul><li>Image Search Returns NSFW Results/Porn With Safe Search Enabled <a href="https://kagifeedback.org/d/2490" data-id="2490"><span></span></a><a href="https://kagifeedback.org/d/2490">#2490</a> <a href="https://kagifeedback.org/u/Kurtis02">@Kurtis02</a></li><li>Login button for Universal Summarizer <a href="https://kagifeedback.org/d/2628" data-id="2628"><span></span></a><a href="https://kagifeedback.org/d/2628">#2628</a> <a href="https://kagifeedback.org/u/Wrought5154">@Wrought5154</a></li><li>Support to output Traditional Chinese in Universal Summarizer <a href="https://kagifeedback.org/d/2606" data-id="2606"><span></span></a><a href="https://kagifeedback.org/d/2606">#2606</a> <a href="https://kagifeedback.org/u/PeterDaveHello">@PeterDaveHello</a></li><li>Unformatted bold tags in summarizer output <a href="https://kagifeedback.org/d/2694" data-id="2694"><span></span></a><a href="https://kagifeedback.org/d/2694">#2694</a> @moretnfyhn</li><li>Better error messages when changing email doesn't work</li><li>Translate doesn't translate to specified language <a href="https://kagifeedback.org/d/1842" data-id="1842"><span></span></a><a href="https://kagifeedback.org/d/1842">#1842</a> <a href="https://kagifeedback.org/u/GiorgiShalvashvili">@GiorgiShalvashvili</a></li><li>New domain information popup shows previous favicon briefly <a href="https://kagifeedback.org/d/2655" data-id="2655"><span></span></a><a href="https://kagifeedback.org/d/2655">#2655</a> <a href="https://kagifeedback.org/u/Jake-Moss">@Jake-Moss</a></li><li>Kagi doesn't supply bold Lufga font, requiring the browser to synthesize it <a href="https://kagifeedback.org/d/2656" data-id="2656"><span></span></a><a href="https://kagifeedback.org/d/2656">#2656</a> <a href="https://kagifeedback.org/u/bert">@bert</a></li><li>Signin with Google: Access blocked: kagi.com has not completed the Google verification process <a href="https://kagifeedback.org/d/2666" data-id="2666"><span></span></a><a href="https://kagifeedback.org/d/2666">#2666</a> <a href="https://kagifeedback.org/u/tjpnz">@tjpnz</a></li><li>A search for "esc pos protocol" shows a preview of Wikipedia's page about the "P" letter <a href="https://kagifeedback.org/d/2494" data-id="2494"><span></span></a><a href="https://kagifeedback.org/d/2494">#2494</a> <a href="https://kagifeedback.org/u/neysofu">@neysofu</a></li><li>Hackernews icons don't show up correctly <a href="https://kagifeedback.org/d/2599" data-id="2599"><span></span></a><a href="https://kagifeedback.org/d/2599">#2599</a> <a href="https://kagifeedback.org/u/Value7609">@Value7609</a></li><li>Weather showing incorrect temperature <a href="https://kagifeedback.org/d/2680" data-id="2680"><span></span></a><a href="https://kagifeedback.org/d/2680">#2680</a> <a href="https://kagifeedback.org/u/dmelcer9">@dmelcer9</a></li><li>Search term gets cut after hashtag <a href="https://kagifeedback.org/d/2422" data-id="2422"><span></span></a><a href="https://kagifeedback.org/d/2422">#2422</a> <a href="https://kagifeedback.org/u/Jak">@Jak</a></li><li>Some regional bangs clash with existing ones <a href="https://kagifeedback.org/d/2229" data-id="2229"><span></span></a><a href="https://kagifeedback.org/d/2229">#2229</a> <a href="https://kagifeedback.org/u/matmat">@matmat</a></li><li>Display phone number on search page <a href="https://kagifeedback.org/d/2647" data-id="2647"><span></span></a><a href="https://kagifeedback.org/d/2647">#2647</a> <a href="https://kagifeedback.org/u/champs777">@champs777</a></li><li>Websites in inline maps results now have a button for copying</li><li>Inline maps results now feature reviews</li></ul><p><img src="https://kagifeedback.org/assets/files/2023-12-15/1702600516-924304-2023-12-14-19-20.png" title="" alt=""></p><h3>Assistant</h3><p>✨ Features</p><ul><li>Research modes are less likely to respond "Unfortunately, I do not have enough information…" or similar. Where possible, we will take some extra time to re-search and try again</li><li>Snippets won't be shown from sources (Wolfram Alpha) whose API structures responses in a different manner than the website</li></ul><p>🪲 Bugfixes</p><ul><li>Regression in citations when using Wolfram Alpha as a source</li><li>Allow code to be copied while assistant is still typing <a href="https://kagifeedback.org/d/2219" data-id="2219"><span></span></a><a href="https://kagifeedback.org/d/2219">#2219</a> <a href="https://kagifeedback.org/u/Reroute5183">@Reroute5183</a></li><li>Assistant doesn't render nested bullet points properly <a href="https://kagifeedback.org/d/2673" data-id="2673"><span></span></a><a href="https://kagifeedback.org/d/2673">#2673</a> <a href="https://kagifeedback.org/u/httpjames">@httpjames</a></li><li>Summarizer &amp; Kagi Assistant unable to summarize/process www.3m.com links. <a href="https://kagifeedback.org/d/2619" data-id="2619"><span></span></a><a href="https://kagifeedback.org/d/2619">#2619</a> <a href="https://kagifeedback.org/u/EvacuatedTerminal">@EvacuatedTerminal</a></li><li>Assistant hangs forever instead of timing out on unsuccessful attachment retrieval</li></ul><p>⚠️ Known Issues</p><ul><li>Long inputs may sometimes result in a 414 error. We are working on a new system to allow much larger input sizes that should be rolled out very soon.</li></ul></div></div><div id="2654"><h2><span>Dec 7, 2023 - Paywalled articles indicator and improved weather widget</span><a href="https://kagi.com/changelog#2654"> #</a></h2><div><h3>📢 Announcements</h3><ul><li><p>We are now indicating (potentially) paywalled articles in search results  <a href="https://kagifeedback.org/d/459" data-id="459"><span></span></a><a href="https://kagifeedback.org/d/459">#459</a> <a href="https://kagifeedback.org/u/Kai">@Kai</a><br><img src="https://kagifeedback.org/assets/files/2023-12-08/1701997361-791733-screenshot-2023-12-07-at-170237.png" title="" alt=""></p></li><li><p>New, beautiful domain information popup!<br><img src="https://kagifeedback.org/assets/files/2023-12-08/1702016622-708427-screenshot-2023-12-07-at-222317.png" title="" alt=""></p></li></ul><ul><li><p>Setting that lets set your default preferred units  <a href="https://kagifeedback.org/d/862" data-id="862"><span></span></a><a href="https://kagifeedback.org/d/862">#862</a> <a href="https://kagifeedback.org/u/jared-07">@jared-07</a> <img src="https://kagifeedback.org/assets/files/2023-12-08/1701997763-521160-screenshot-2023-12-07-at-170917.png" title="" alt=""></p></li><li><p>Improved weather widget to go with the previous<br><img src="https://kagifeedback.org/assets/files/2023-12-08/1701997307-745116-screenshot-2023-12-07-at-170144.png" title="" alt=""></p></li></ul><h3>🪲 Bug fixes &amp; Improvements</h3><ul><li>Kagi Search extension for Firefox and Chrome (0.4.3)   <ul><li>Kagi extension for Firefox on Android <a href="https://kagifeedback.org/d/2209" data-id="2209"><span></span></a><a href="https://kagifeedback.org/d/2209">#2209</a> <a href="https://kagifeedback.org/u/UndarkAido">@UndarkAido</a></li>  <li>Text formatting and copy button for summarizer <a href="https://kagifeedback.org/d/2464" data-id="2464"><span></span></a><a href="https://kagifeedback.org/d/2464">#2464</a> <a href="https://kagifeedback.org/u/dds2d">@dds2d</a></li>  <li>Summarizer doesn't work when firefox reader mode is active <a href="https://kagifeedback.org/d/2558" data-id="2558"><span></span></a><a href="https://kagifeedback.org/d/2558">#2558</a> <a href="https://kagifeedback.org/u/okwhatever">@okwhatever</a></li> </ul></li><li>Option to expand Wikipedia preview in right content box by default. <a href="https://kagifeedback.org/d/1904" data-id="1904"><span></span></a><a href="https://kagifeedback.org/d/1904">#1904</a> <a href="https://kagifeedback.org/u/chris_20017">@chris_20017</a></li><li>Add Google Maps and Apple Maps Links to Search Results</li><li>Bang for Universal Summarizer Summary vs. Key Moments <a href="https://kagifeedback.org/d/2386" data-id="2386"><span></span></a><a href="https://kagifeedback.org/d/2386">#2386</a> <a href="https://kagifeedback.org/u/CrunchyFritos">@CrunchyFritos</a></li><li>Published Date incorrect <a href="https://kagifeedback.org/d/2033" data-id="2033"><span></span></a><a href="https://kagifeedback.org/d/2033">#2033</a> <a href="https://kagifeedback.org/u/hmnd">@hmnd</a></li><li>Favicon not showing on safari tabs <a href="https://kagifeedback.org/d/2532" data-id="2532"><span></span></a><a href="https://kagifeedback.org/d/2532">#2532</a> <a href="https://kagifeedback.org/u/gwynforthewyn">@gwynforthewyn</a></li><li>Unexpected translate widget “switch sides” behavior when detecting language <a href="https://kagifeedback.org/d/2415" data-id="2415"><span></span></a><a href="https://kagifeedback.org/d/2415">#2415</a> <a href="https://kagifeedback.org/u/bert">@bert</a></li><li>!fast inoperable on Team Plan <a href="https://kagifeedback.org/d/2607" data-id="2607"><span></span></a><a href="https://kagifeedback.org/d/2607">#2607</a> <a href="https://kagifeedback.org/u/pbronez">@pbronez</a></li><li>Unable to restore theme color to default <a href="https://kagifeedback.org/d/1614" data-id="1614"><span></span></a><a href="https://kagifeedback.org/d/1614">#1614</a> <a href="https://kagifeedback.org/u/umar">@umar</a></li></ul></div></div><div id="2629"><h2><span>Nov 30, 2023 - Quality of life improvements</span><a href="https://kagi.com/changelog#2629"> #</a></h2><div><h3>Kagi Search</h3><h3>🪲 Bug fixes &amp; Improvements</h3><ul><li>Cursor will not scroll to end of search for long queries on Android  <a href="https://kagifeedback.org/d/2302" data-id="2302"><span></span></a><a href="https://kagifeedback.org/d/2302">#2302</a> <a href="https://kagifeedback.org/u/zam">@zam</a></li><li>Japanese input appears twice when using Hiragana/Katakana/Kanji input <a href="https://kagifeedback.org/d/2029" data-id="2029"><span></span></a><a href="https://kagifeedback.org/d/2029">#2029</a> <a href="https://kagifeedback.org/u/nikke1234">@nikke1234</a> </li><li>Keyboard navigation doesn't highlight instant answer <a href="https://kagifeedback.org/d/2396" data-id="2396"><span></span></a><a href="https://kagifeedback.org/d/2396">#2396</a> <a href="https://kagifeedback.org/u/danbulant">@danbulant</a> </li><li>Lens not scoping correcty <a href="https://kagifeedback.org/d/2250" data-id="2250"><span></span></a><a href="https://kagifeedback.org/d/2250">#2250</a> <a href="https://kagifeedback.org/u/Rediwed">@Rediwed</a></li><li>Mobile - Responsive issues. <a href="https://kagifeedback.org/d/2509" data-id="2509"><span></span></a><a href="https://kagifeedback.org/d/2509">#2509</a> <a href="https://kagifeedback.org/u/cempack">@cempack</a></li><li>Wikipedia widget empty pane <a href="https://kagifeedback.org/d/2423" data-id="2423"><span></span></a><a href="https://kagifeedback.org/d/2423">#2423</a> <a href="https://kagifeedback.org/u/Value7609">@Value7609</a></li><li>Kagi bangs without query should take you to the corresponding Kagi search page <a href="https://kagifeedback.org/d/2316" data-id="2316"><span></span></a><a href="https://kagifeedback.org/d/2316">#2316</a> <a href="https://kagifeedback.org/u/bert">@bert</a></li><li>Reverse image search: search bar text overlap with thumbnail <a href="https://kagifeedback.org/d/2278" data-id="2278"><span></span></a><a href="https://kagifeedback.org/d/2278">#2278</a> <a href="https://kagifeedback.org/u/FrederickZh">@FrederickZh</a></li><li>Ability to navigate to Kagi Assistant when advanced search is enabled <a href="https://kagifeedback.org/d/2298" data-id="2298"><span></span></a><a href="https://kagifeedback.org/d/2298">#2298</a> <a href="https://kagifeedback.org/u/Nezteb">@Nezteb</a></li><li>Laptop screen - responsive issues. <a href="https://kagifeedback.org/d/2508" data-id="2508"><span></span></a><a href="https://kagifeedback.org/d/2508">#2508</a> <a href="https://kagifeedback.org/u/cempack">@cempack</a> </li><li>Add upload date for YouTube video results <a href="https://kagifeedback.org/d/2526" data-id="2526"><span></span></a><a href="https://kagifeedback.org/d/2526">#2526</a> <a href="https://kagifeedback.org/u/Yiin">@Yiin</a> </li><li>Show a message when trying to search in an incognito window while not logged in <a href="https://kagifeedback.org/d/1830" data-id="1830"><span></span></a><a href="https://kagifeedback.org/d/1830">#1830</a> <a href="https://kagifeedback.org/u/ThomasS">@ThomasS</a></li><li>StackOverflow widget formatting is broken <a href="https://kagifeedback.org/d/2452" data-id="2452"><span></span></a><a href="https://kagifeedback.org/d/2452">#2452</a> <a href="https://kagifeedback.org/u/fxgn">@fxgn</a></li><li>Higher quality results for "News" </li><li>Further improved latency for Australia region (after launching the new DC Sydney two weeks ago)</li><li>Missing attribution in map <a href="https://kagifeedback.org/d/2041" data-id="2041"><span></span></a><a href="https://kagifeedback.org/d/2041">#2041</a> <a href="https://kagifeedback.org/u/02JanDal">@02JanDal</a> </li><li>Content-Security-Policy missing on Summarizer <a href="https://kagifeedback.org/d/2527" data-id="2527"><span></span></a><a href="https://kagifeedback.org/d/2527">#2527</a> <a href="https://kagifeedback.org/u/promenaden">@promenaden</a></li><li>YouTube link not encoded properly on clicking the “discuss further” button on the summarization page <a href="https://kagifeedback.org/d/2564" data-id="2564"><span></span></a><a href="https://kagifeedback.org/d/2564">#2564</a> <a href="https://kagifeedback.org/u/platyhsu">@platyhsu</a></li><li>Fixed reported XSS in Universal Summarizer and FastGPT</li><li>Searching for "the boys" triggers shopping widget <a href="https://kagifeedback.org/d/2595" data-id="2595"><span></span></a><a href="https://kagifeedback.org/d/2595">#2595</a> <a href="https://kagifeedback.org/u/terminalnode">@terminalnode</a></li><li>Summerizer no longer works with direct link <a href="https://kagifeedback.org/d/2570" data-id="2570"><span></span></a><a href="https://kagifeedback.org/d/2570">#2570</a> <a href="https://kagifeedback.org/u/inesicio">@inesicio</a></li><li>Summarizer unable to summarize bugs.chromium.org <a href="https://kagifeedback.org/d/2562" data-id="2562"><span></span></a><a href="https://kagifeedback.org/d/2562">#2562</a> <a href="https://kagifeedback.org/u/EvacuatedTerminal">@EvacuatedTerminal</a></li> </ul><h3>Kagi Assistant</h3><p>✨ Features</p><ul><li>Citation snippets:</li><li>- Hover or click an inline citation and review the source text</li><li>- Where possible, clicking the inline citation link directly to the relevant part of the webpage</li><li>Show Assistant on landing page with Advanced Search open</li><li>Improved LLM speed and reliability</li></ul><p>🪲 Bugfixes</p><ul><li>Incorrect formatting in Fast mode <a href="https://kagifeedback.org/d/2567" data-id="2567"><span></span></a><a href="https://kagifeedback.org/d/2567">#2567</a> <a href="https://kagifeedback.org/u/NevevrAlak">@NevevrAlak</a></li><li>Research assistant upload file modal squished on mobile <a href="https://kagifeedback.org/d/2552" data-id="2552"><span></span></a><a href="https://kagifeedback.org/d/2552">#2552</a> <a href="https://kagifeedback.org/u/EvacuatedTerminal">@EvacuatedTerminal</a></li><li>Custom assistant settings use the wrong model <a href="https://kagifeedback.org/d/2510" data-id="2510"><span></span></a><a href="https://kagifeedback.org/d/2510">#2510</a> <a href="https://kagifeedback.org/u/houston">@houston</a></li><li>Chat glitches when scrolling up <a href="https://kagifeedback.org/d/2435" data-id="2435"><span></span></a><a href="https://kagifeedback.org/d/2435">#2435</a> <a href="https://kagifeedback.org/u/feedbackhax">@feedbackhax</a></li><li>Various small UX fixes</li></ul><p>⚠️ Known Issues</p><ul><li>Low contrast citation snippets in dark mode. Bugfix should be out within a couple days</li><li>Long inputs with non-alphabetical characters may sometimes result in a 414 error. We are working on a new system to allow much larger input sizes</li></ul></div></div><div id="2540"><h2><span>Nov 16, 2023 - GPT 4 Turbo, Australia region, Sales Tax</span><a href="https://kagi.com/changelog#2540"> #</a></h2><div><h3>📢 Announcements</h3><ul><li><p>We are making <strong>GPT 4 Turbo</strong> available in Kagi Assistant (<em>Kagi Assistant is currently in closed beta and available to Ultimate members</em>)</p></li><li><p><strong>Summarize any page on the web</strong> with Kagi Search extensions for <a href="https://chromewebstore.google.com/detail/kagi-search-for-chrome/cdglnehniifkbagbbombnjghhcihifij" rel="ugc nofollow">Chrome</a> and <a href="https://addons.mozilla.org/en-US/firefox/addon/kagi-search-for-firefox/" rel="ugc nofollow">Firefox</a> (0.4.2). We added support for Summarizer language setting and summary type. This feature is free for all Kagi members, try it out!</p></li><li><p>We are <strong>now deployed in Australia</strong> (Sydney), yay! This will reduce latency for our members in this part of the world. We <a href="https://help.kagi.com/kagi/search-details/search-speed.html" rel="ugc nofollow">care about speed a lot</a>!</p></li></ul><p>Here is a map of all <strong>Kagi data center locations</strong> currently.<br><img src="https://kagifeedback.org/assets/files/2023-11-16/1700178524-889329-image.png" title="" alt=""></p><ul><li><p>Kagi is <a href="https://kagi.com/stats" rel="ugc nofollow">growing up</a>! You've all seen and participated in the incredible growth of the past few months, thank you. It also means we'll need to start looking at where <strong>we need to start charging sales tax/VAT</strong> on behalf of your state/government. We're in the process of looking at the details, and we will do our best to make this as seamless as possible. Expect more information soon.</p></li><li><p>Our tech lead <strong>Zac Nowicki recently gave a talk</strong> at Crystal Conf. Zac shares a summary of lessons, technology, ideas, and challenges after building a search engine product from the ground up in Crystal for the past three years. <a href="https://www.youtube.com/watch?v=r7t9xPajjTM&amp;list=PLt-CsM4G1WoadONHl3zPN_Ts5PqH8TgMZ&amp;index=8" rel="ugc nofollow">Watch the presentation</a>.</p></li><li><p>We have a great <a href="https://discord.com/channels/849884108750061568/1171909248507191326/1172609911021121600" rel="ugc nofollow">discussion</a> about <strong>Kagi Email</strong> in ⁠our Discord. <a href="https://kagi.com/discord" rel="ugc nofollow">Join us</a>!</p></li> </ul><h3>🪲 Bug fixes &amp; Improvements</h3><ul><li>Fixed Inline discussions not showing actual discussions</li><li>No search bang for no_region, only for regions <a href="https://kagifeedback.org/d/2475" data-id="2475"><span></span></a><a href="https://kagifeedback.org/d/2475">#2475</a> <a href="https://kagifeedback.org/u/bgeron">@bgeron</a></li><li>Keyboard shortcuts are difficult to find <a href="https://kagifeedback.org/d/2252" data-id="2252"><span></span></a><a href="https://kagifeedback.org/d/2252">#2252</a> <a href="https://kagifeedback.org/u/janfoeh">@janfoeh</a></li><li>Unsanitized square brackets in FastGPT <a href="https://kagifeedback.org/d/2470" data-id="2470"><span></span></a><a href="https://kagifeedback.org/d/2470">#2470</a> <a href="https://kagifeedback.org/u/felkr">@felkr</a></li><li>Information icon next to Total searches this period on billing page does not provide information <a href="https://kagifeedback.org/d/2495" data-id="2495"><span></span></a><a href="https://kagifeedback.org/d/2495">#2495</a> <a href="https://kagifeedback.org/u/bebowilson">@bebowilson</a> </li><li>GTA 6 returns “reviews” for game that doesn’t exist <a href="https://kagifeedback.org/d/2488" data-id="2488"><span></span></a><a href="https://kagifeedback.org/d/2488">#2488</a> <a href="https://kagifeedback.org/u/lolrepeatlol">@lolrepeatlol</a></li><li>FastGPT API results are different (and much worse) than kagi.com/fastgpt <a href="https://kagifeedback.org/d/2504" data-id="2504"><span></span></a><a href="https://kagifeedback.org/d/2504">#2504</a> <a href="https://kagifeedback.org/u/estheruary">@estheruary</a></li><li>FastGPT web version and API doesn't provide the same result <a href="https://kagifeedback.org/d/1690" data-id="1690"><span></span></a><a href="https://kagifeedback.org/d/1690">#1690</a> <a href="https://kagifeedback.org/u/doom">@doom</a></li><li>Universal Summarizer reports that multi-page PDF files are too short to summarize <a href="https://kagifeedback.org/d/1962" data-id="1962"><span></span></a><a href="https://kagifeedback.org/d/1962">#1962</a> <a href="https://kagifeedback.org/u/scottharris">@scottharris</a></li><li>Album of The Year bang !aoty with a search redirects to the wrong URL <a href="https://kagifeedback.org/d/2485" data-id="2485"><span></span></a><a href="https://kagifeedback.org/d/2485">#2485</a> <a href="https://kagifeedback.org/u/haakon">@haakon</a> </li><li>Bang !cron is broken <a href="https://kagifeedback.org/d/2459" data-id="2459"><span></span></a><a href="https://kagifeedback.org/d/2459">#2459</a> <a href="https://kagifeedback.org/u/loloriz">@loloriz</a></li><li>!gog bang not working correctly <a href="https://kagifeedback.org/d/2287" data-id="2287"><span></span></a><a href="https://kagifeedback.org/d/2287">#2287</a> <a href="https://kagifeedback.org/u/Xamrica">@Xamrica</a> </li><li>Fun fact on stats page has wrong text <a href="https://kagifeedback.org/d/2442" data-id="2442"><span></span></a><a href="https://kagifeedback.org/d/2442">#2442</a> <a href="https://kagifeedback.org/u/Jake-Moss">@Jake-Moss</a> </li><li>Gift activation links overflow <a href="https://kagifeedback.org/d/2420" data-id="2420"><span></span></a><a href="https://kagifeedback.org/d/2420">#2420</a> <a href="https://kagifeedback.org/u/httpjames">@httpjames</a></li><li>Lens name truncated in settings <a href="https://kagifeedback.org/d/2397" data-id="2397"><span></span></a><a href="https://kagifeedback.org/d/2397">#2397</a> <a href="https://kagifeedback.org/u/timmm">@timmm</a></li><li>The Summarizer extension language option <a href="https://kagifeedback.org/d/2507" data-id="2507"><span></span></a><a href="https://kagifeedback.org/d/2507">#2507</a> <a href="https://kagifeedback.org/u/cempack">@cempack</a></li><li>[Chrome Extension] [Summarizer] Extension always uses Summary, instead of Key Moments <a href="https://kagifeedback.org/d/2321" data-id="2321"><span></span></a><a href="https://kagifeedback.org/d/2321">#2321</a> <a href="https://kagifeedback.org/u/rswerve">@rswerve</a></li></ul><h2>Kagi Assistant</h2><h3>📢 Announcements</h3><ul><li>GPT-4 Turbo now available to Ultimate members</li><li>Increased custom Assistant prompt length (1500 chars) <a href="https://kagifeedback.org/d/2150" data-id="2150"><span></span></a><a href="https://kagifeedback.org/d/2150">#2150</a> <a href="https://kagifeedback.org/u/AndroidKitKat">@AndroidKitKat</a></li></ul><h3>🪲 Bugfixes</h3><ul><li>Show custom mode name in settings <a href="https://kagifeedback.org/d/2294" data-id="2294"><span></span></a><a href="https://kagifeedback.org/d/2294">#2294</a> <a href="https://kagifeedback.org/u/TheLastEnvoy">@TheLastEnvoy</a></li></ul><h3>⚠️ Known Issues</h3><ul><li>Long inputs with non-alphabetical characters may sometimes result in a 414 error. We are working on a new system to allow much larger input sizes</li></ul></div></div><div id="2445"><h2><span>Nov 3, 2023 - New region and mysterious Kagi surprise</span><a href="https://kagi.com/changelog#2445"> #</a></h2><div><h3>📢 Announcements</h3><ul><li>We launched a new deployment in South America (São Paulo) datacenter region to reduce latency for our members there. Next week we plan to also expand datacenter operations to Australia (Sydney) region.</li><li>Starter plan Annual plan changed to 3600 searches/year (vs 300 month) to make it more flexible</li><li>New live <a href="https://kagi.com/stats" rel="ugc nofollow">Kagi Stats</a> page. Have a go at guessing the mysterious surprise <a href="https://kagifeedback.org/d/2444-what-is-kagi-surprise-20k" rel="ugc nofollow">here</a></li></ul><h3>✨ Features</h3><ul><li>General settings (including language preference) now available for Kids accounts in the Family Plan  <a href="https://kagifeedback.org/d/2306" data-id="2306"><span></span></a><a href="https://kagifeedback.org/d/2306">#2306</a> <a href="https://kagifeedback.org/u/ploum">@ploum</a></li></ul><h3>🪲 Bug fixes &amp; Improvements</h3><ul><li>Blocked domain still appears in "confident answer" <a href="https://kagifeedback.org/d/2405" data-id="2405"><span></span></a><a href="https://kagifeedback.org/d/2405">#2405</a> <a href="https://kagifeedback.org/u/erandebl">@erandebl</a> </li><li>Icons in the search bar overlap with text <a href="https://kagifeedback.org/d/2074" data-id="2074"><span></span></a><a href="https://kagifeedback.org/d/2074">#2074</a> <a href="https://kagifeedback.org/u/Michele144">@Michele144</a></li><li>"Search with" query URL encoding <a href="https://kagifeedback.org/d/2222" data-id="2222"><span></span></a><a href="https://kagifeedback.org/d/2222">#2222</a> <a href="https://kagifeedback.org/u/feedbackhax">@feedbackhax</a> </li><li>Adding a search engine shortcut broken: TypeError <a href="https://kagifeedback.org/d/2218" data-id="2218"><span></span></a><a href="https://kagifeedback.org/d/2218">#2218</a> <a href="https://kagifeedback.org/u/ni_something">@ni_something</a></li><li>Nixopt bang does not work <a href="https://kagifeedback.org/d/2352" data-id="2352"><span></span></a><a href="https://kagifeedback.org/d/2352">#2352</a> <a href="https://kagifeedback.org/u/niclasoverby">@niclasoverby</a> </li><li>According to docs bang !si should be regional, but it is used elsewhere <a href="https://kagifeedback.org/d/2216" data-id="2216"><span></span></a><a href="https://kagifeedback.org/d/2216">#2216</a> <a href="https://kagifeedback.org/u/hook">@hook</a> </li><li>FastGPT should not automatically open the keyboard on mobile after it is finished writing the response. <a href="https://kagifeedback.org/d/2085" data-id="2085"><span></span></a><a href="https://kagifeedback.org/d/2085">#2085</a> <a href="https://kagifeedback.org/u/Protech">@Protech</a> </li><li>Adding a search engine shortcut broken: TypeError <a href="https://kagifeedback.org/d/2218" data-id="2218"><span></span></a><a href="https://kagifeedback.org/d/2218">#2218</a> <a href="https://kagifeedback.org/u/ni_something">@ni_something</a></li><li>More results resets country <a href="https://kagifeedback.org/d/1277" data-id="1277"><span></span></a><a href="https://kagifeedback.org/d/1277">#1277</a> <a href="https://kagifeedback.org/u/benelgar">@benelgar</a> </li><li>Incorrect "what is my IP" response <a href="https://kagifeedback.org/d/2412" data-id="2412"><span></span></a><a href="https://kagifeedback.org/d/2412">#2412</a> <a href="https://kagifeedback.org/u/Vex">@Vex</a></li><li>Alternatives to pay per use search for Standard/Starter plan <a href="https://kagifeedback.org/d/2018" data-id="2018"><span></span></a><a href="https://kagifeedback.org/d/2018">#2018</a> <a href="https://kagifeedback.org/u/KagiForMe">@KagiForMe</a> </li><li>Images widget always shows images in portrait boxes on mobile <a href="https://kagifeedback.org/d/2178" data-id="2178"><span></span></a><a href="https://kagifeedback.org/d/2178">#2178</a> <a href="https://kagifeedback.org/u/Michele144">@Michele144</a></li><li>When accessing Kagi News from a smartphone, display 2 rows of text for each item inside the Interesting News section <a href="https://kagifeedback.org/d/1625" data-id="1625"><span></span></a><a href="https://kagifeedback.org/d/1625">#1625</a> <a href="https://kagifeedback.org/u/David">@David</a></li><li>Safari-pinned-tab.svg redirects to 404 <a href="https://kagifeedback.org/d/2009" data-id="2009"><span></span></a><a href="https://kagifeedback.org/d/2009">#2009</a> <a href="https://kagifeedback.org/u/cyann">@cyann</a> </li><li>Universal Summarizer does not summarize podcasts <a href="https://kagifeedback.org/d/2109" data-id="2109"><span></span></a><a href="https://kagifeedback.org/d/2109">#2109</a> <a href="https://kagifeedback.org/u/nbanks">@nbanks</a></li><li>Cloud animations on Kagi start page use excessive CPU resources <a href="https://kagifeedback.org/d/300" data-id="300"><span></span></a><a href="https://kagifeedback.org/d/300">#300</a> <a href="https://kagifeedback.org/u/slartoff">@slartoff</a> </li><li>Tapping on the empty suggestion region should close it <a href="https://kagifeedback.org/d/2233" data-id="2233"><span></span></a><a href="https://kagifeedback.org/d/2233">#2233</a> <a href="https://kagifeedback.org/u/Browsing6853">@Browsing6853</a> </li><li>Show the number of ranked domains  <a href="https://kagifeedback.org/d/2246" data-id="2246"><span></span></a><a href="https://kagifeedback.org/d/2246">#2246</a> <a href="https://kagifeedback.org/u/chris_20017">@chris_20017</a></li><li>Summmarizer does not work for some websites <a href="https://kagifeedback.org/d/2334" data-id="2334"><span></span></a><a href="https://kagifeedback.org/d/2334">#2334</a> <a href="https://kagifeedback.org/u/Roovesta">@Roovesta</a> </li><li>Search button under the reverse image search is not aligned in the centre <a href="https://kagifeedback.org/d/2179" data-id="2179"><span></span></a><a href="https://kagifeedback.org/d/2179">#2179</a> <a href="https://kagifeedback.org/u/Repacking6528">@Repacking6528</a> </li><li>"Open in Web Archive" doesn't honor "Open Links in a New Tab" setting <a href="https://kagifeedback.org/d/2123" data-id="2123"><span></span></a><a href="https://kagifeedback.org/d/2123">#2123</a> <a href="https://kagifeedback.org/u/securemepls">@securemepls</a></li><li>Flash caused by meta theme color <a href="https://kagifeedback.org/d/2165" data-id="2165"><span></span></a><a href="https://kagifeedback.org/d/2165">#2165</a> <a href="https://kagifeedback.org/u/ysun">@ysun</a></li><li>Fun fact on stats page has wrong text <a href="https://kagifeedback.org/d/2442" data-id="2442"><span></span></a><a href="https://kagifeedback.org/d/2442">#2442</a> <a href="https://kagifeedback.org/u/Jake-Moss">@Jake-Moss</a> </li><li>Gift activation links overflow <a href="https://kagifeedback.org/d/2420" data-id="2420"><span></span></a><a href="https://kagifeedback.org/d/2420">#2420</a> <a href="https://kagifeedback.org/u/httpjames">@httpjames</a></li><li>Lens name truncated in settings <a href="https://kagifeedback.org/d/2397" data-id="2397"><span></span></a><a href="https://kagifeedback.org/d/2397">#2397</a> <a href="https://kagifeedback.org/u/timmm">@timmm</a></li><li>Unable to summarize Paul Graham essay <a href="https://kagifeedback.org/d/2413" data-id="2413"><span></span></a><a href="https://kagifeedback.org/d/2413">#2413</a> <a href="https://kagifeedback.org/u/CrunchyFritos">@CrunchyFritos</a></li><li>Custom bangs URL should not get URL encoded - i.e. &amp;amp; <a href="https://kagifeedback.org/d/2259" data-id="2259"><span></span></a><a href="https://kagifeedback.org/d/2259">#2259</a> <a href="https://kagifeedback.org/u/Value7609">@Value7609</a></li><li>Currency conversion, i.e., "100 usd to sek", should display widget <a href="https://kagifeedback.org/d/2031" data-id="2031"><span></span></a><a href="https://kagifeedback.org/d/2031">#2031</a> <a href="https://kagifeedback.org/u/Vapid">@Vapid</a></li><li>Lens "Kagi Help and Feedback" includes other domains <a href="https://kagifeedback.org/d/2301" data-id="2301"><span></span></a><a href="https://kagifeedback.org/d/2301">#2301</a> <a href="https://kagifeedback.org/u/oogl6fhk6">@oogl6fhk6</a></li><li>"ask questions about document" uses default mode instead of the modes which are capable of accessing the internet <a href="https://kagifeedback.org/d/2153" data-id="2153"><span></span></a><a href="https://kagifeedback.org/d/2153">#2153</a> <a href="https://kagifeedback.org/u/Value7609">@Value7609</a> </li><li>Info icon for summarize overlaps with the "more" menu <a href="https://kagifeedback.org/d/2340" data-id="2340"><span></span></a><a href="https://kagifeedback.org/d/2340">#2340</a> <a href="https://kagifeedback.org/u/ocharles">@ocharles</a></li></ul><h2>Kagi Assistant</h2><h3>📢 Announcements</h3><ul><li>Starting next Thursday (1 week from today) non-ultimate users will no longer have access to ultimate features. We appreciate your help testing Assistant, and you will continue to have beta access prior to public launch.</li><li>Known issue: Long inputs with non-alphabetical characters may sometimes result in a 414 error. We are working on a new system to allow much larger input sizes.</li></ul><h3>✨ Features</h3><ul><li>Significantly improved streaming speed for OpenAI models</li><li>Switching back and forth between Assistant and Web Search clears the query</li><li>Navigating to the landing page from Assistant opens Assistant mode</li><li>Assistant bangs without input redirect to the expected location</li></ul><h3>🪲 Bug fixes</h3><ul><li>Accidentally triggering bangs within Assistant  <a href="https://kagifeedback.org/d/2320" data-id="2320"><span></span></a><a href="https://kagifeedback.org/d/2320">#2320</a> <a href="https://kagifeedback.org/u/Whoops">@Whoops</a>]</li><li>Regenerate response errors  <a href="https://kagifeedback.org/d/2374" data-id="2374"><span></span></a><a href="https://kagifeedback.org/d/2374">#2374</a> <a href="https://kagifeedback.org/u/Value7609">@Value7609</a>]</li><li>XSS vulnerabilities</li><li>Hallucination of links in Research mode  <a href="https://kagifeedback.org/d/2358" data-id="2358"><span></span></a><a href="https://kagifeedback.org/d/2358">#2358</a> <a href="https://kagifeedback.org/u/nissa">@nissa</a>]</li><li>Loss of context</li><li>Research mode follow-up edge cases</li><li>No search edge cases</li><li>Text wrapping of messages  <a href="https://kagifeedback.org/d/2154" data-id="2154"><span></span></a><a href="https://kagifeedback.org/d/2154">#2154</a> <a href="https://kagifeedback.org/u/Value7609">@Value7609</a>]</li><li>Unexpected scrolling  <a href="https://kagifeedback.org/d/2155" data-id="2155"><span></span></a><a href="https://kagifeedback.org/d/2155">#2155</a> <a href="https://kagifeedback.org/u/Reroute5183">@Reroute5183</a>]</li><li>No summary of document</li><li>Answer stops streaming midway</li><li>Error on long conversations in Code mode  <a href="https://kagifeedback.org/d/2225" data-id="2225"><span></span></a><a href="https://kagifeedback.org/d/2225">#2225</a> <a href="https://kagifeedback.org/u/whee">@whee</a>]</li></ul></div></div><div id="2284"><h2><span>Oct 10, 2023 - Misc fixes</span><a href="https://kagi.com/changelog#2284"> #</a></h2><div><p><strong>New</strong><br>Kagi Search extension for <a href="https://chrome.google.com/webstore/detail/kagi-search-for-chrome/cdglnehniifkbagbbombnjghhcihifij" rel="ugc nofollow">Chrome</a> and <a href="https://addons.mozilla.org/en-US/firefox/addon/kagi-search-for-firefox/" rel="ugc nofollow">Firefox</a> 0.4.0:</p><ul><li>Right click on the page to summarize the page</li><li>Right-clicking an image to search Kagi by image <a href="https://kagifeedback.org/d/2105" data-id="2105"><span></span></a><a href="https://kagifeedback.org/d/2105">#2105</a> <a href="https://kagifeedback.org/u/Nankeru">@Nankeru</a></li></ul><p><strong>Improved</strong></p><ul><li>Site footer enhanced with useful navigation links</li></ul><p><strong>Fixed</strong></p><ul><li>Icons in the search bar overlap with text <a href="https://kagifeedback.org/d/2074" data-id="2074"><span></span></a><a href="https://kagifeedback.org/d/2074">#2074</a> <a href="https://kagifeedback.org/u/Michele144">@Michele144</a> </li><li>Login does not work without JS <a href="https://kagifeedback.org/d/2045" data-id="2045"><span></span></a><a href="https://kagifeedback.org/d/2045">#2045</a> <a href="https://kagifeedback.org/u/eiangarsaowrfutarfgfarg">@eiangarsaowrfutarfgfarg</a></li><li>Search cannot be triggered by return key without the JS <a href="https://kagifeedback.org/d/2065" data-id="2065"><span></span></a><a href="https://kagifeedback.org/d/2065">#2065</a> <a href="https://kagifeedback.org/u/eiangarsaowrfutarfgfarg">@eiangarsaowrfutarfgfarg</a></li><li>Custom bangs URL should not get URL encoded - i.e. &amp;amp; <a href="https://kagifeedback.org/d/2259" data-id="2259"><span></span></a><a href="https://kagifeedback.org/d/2259">#2259</a> <a href="https://kagifeedback.org/u/Value7609">@Value7609</a> </li><li>Z-index incorrect for keyboard shortcuts overlay <a href="https://kagifeedback.org/d/2143" data-id="2143"><span></span></a><a href="https://kagifeedback.org/d/2143">#2143</a> <a href="https://kagifeedback.org/u/theperiscope">@theperiscope</a></li><li>The video in <a href="https://kagi.com/changelog" rel="ugc nofollow">https://kagi.com/changelog</a> is blocked by CSP <a href="https://kagifeedback.org/d/2139" data-id="2139"><span></span></a><a href="https://kagifeedback.org/d/2139">#2139</a> <a href="https://kagifeedback.org/u/FrederickZh">@FrederickZh</a> </li><li>Wikipedia-mode.el is always at the top of an Emacs search <a href="https://kagifeedback.org/d/1923" data-id="1923"><span></span></a><a href="https://kagifeedback.org/d/1923">#1923</a> <a href="https://kagifeedback.org/u/gthomsen">@gthomsen</a> </li><li>Quick answer info tooltip z-index issue <a href="https://kagifeedback.org/d/2049" data-id="2049"><span></span></a><a href="https://kagifeedback.org/d/2049">#2049</a> <a href="https://kagifeedback.org/u/sdgluck">@sdgluck</a></li><li>Kagi Summary API now returns numbered list summaries <a href="https://kagifeedback.org/d/2149" data-id="2149"><span></span></a><a href="https://kagifeedback.org/d/2149">#2149</a> <a href="https://kagifeedback.org/u/bitjammer">@bitjammer</a></li><li>Unsanitized HTML output <a href="https://kagifeedback.org/d/2088" data-id="2088"><span></span></a><a href="https://kagifeedback.org/d/2088">#2088</a> <a href="https://kagifeedback.org/u/jurassicpeak">@jurassicpeak</a> </li><li>Family plan page overflows on mobile <a href="https://kagifeedback.org/d/2011" data-id="2011"><span></span></a><a href="https://kagifeedback.org/d/2011">#2011</a> <a href="https://kagifeedback.org/u/ForumNinja404">@ForumNinja404</a> </li><li>Advanced search options show above keyboard shortcuts <a href="https://kagifeedback.org/d/2017" data-id="2017"><span></span></a><a href="https://kagifeedback.org/d/2017">#2017</a> <a href="https://kagifeedback.org/u/KoboldMage">@KoboldMage</a></li></ul><p><strong>Assistant beta</strong> (currently available to Ultimate plan members, general availability planned for November)</p><p>✨ Features</p><ul><li>Significantly improved logic driving follow-ups in Research mode</li><li>Better use of previous searches</li><li>New logic to derive additional searches when needed</li><li>Better detection of when additional searches are not needed (and would degrade the output)</li></ul><p>🪲 Bugfixes</p><ul><li>Assistant response caching doesn't account for the entirety of a multiline primp <a href="https://kagifeedback.org/d/2170" data-id="2170"><span></span></a><a href="https://kagifeedback.org/d/2170">#2170</a> <a href="https://kagifeedback.org/u/httpjames">@httpjames</a></li><li>Assistant: Weird layout while code assistent writes unfinished fenced code blocks <a href="https://kagifeedback.org/d/2115" data-id="2115"><span></span></a><a href="https://kagifeedback.org/d/2115">#2115</a> <a href="https://kagifeedback.org/u/warpspin">@warpspin</a> </li><li>Security fixes (XSS) - many thanks to the users who have been flagging these</li><li>Images are more consistently displayed when available</li><li>Better filtering of input so multiple distinct people/places are less likely to be confused</li><li>Assistant-related bangs should now display in the UI for all beta users</li><li>!fast and !expert are fixed</li><li>Better responses for edge cases</li><li>Another input limit bug</li><li>Formatting fixes for summaries</li><li>Various backend bugfixes</li></ul></div></div><div id="2136"><h2><span>Sep 28, 2023 - Assistant beta and enhanced accessibility</span><a href="https://kagi.com/changelog#2136"> #</a></h2><div><p>We are rolling <strong>Kagi Assistant beta</strong> for all our current Ultimate plan members (new Ultimate plan users will be onboarded daily). Assistant includes four dynamic AI modes - Research, Code, Chat, and Custom - powered by cutting-edge language models like GPT-4 and Claude-2 (on the Ultimate plan), all in one package. Stay tuned, as we plan to make the Assistant available to all members in the coming weeks.</p><p><img src="https://kagifeedback.org/assets/files/2023-09-29/1695962516-11471-image.png" title="" alt=""></p><p>We're committed to making our platform more accessible. We already made a number of changes to <strong>improve accessibility</strong> in our search experience and our latest update enhances the screen reader experience in Maps, making it more user-friendly for visually impaired users. We are at the beginning of the road and if we're committed to making our platform more accessible.  If you or someone you know can provide valuable feedback, please contact us at <a href="mailto:dylan@kagi.com">dylan@kagi.com</a>. Your input will help us better serve the needs of these members of our community.</p><p><strong>New:</strong></p><ul><li>Show the "More" menu on the front page <a href="https://kagifeedback.org/d/1579" data-id="1579"><span></span></a><a href="https://kagifeedback.org/d/1579">#1579</a> <a href="https://kagifeedback.org/u/Nankeru">@Nankeru</a> </li><li>Added $300 option for the gift card (so you can gift Ultimate plan for a year to someone)</li><li>New Kagi status page <a href="https://status.kagi.com/" rel="ugc nofollow">https://status.kagi.com</a></li></ul><p><strong>Improved:</strong></p><ul><li>Universal Summarizer unable to process transcript <a href="https://kagifeedback.org/d/1999" data-id="1999"><span></span></a><a href="https://kagifeedback.org/d/1999">#1999</a> <a href="https://kagifeedback.org/u/seligman">@seligman</a> </li><li>Make Universal Summarizer default to Key Moments</li><li>Kids account: Search Entire Web off + Lens not working as expected  <a href="https://kagifeedback.org/d/2081" data-id="2081"><span></span></a><a href="https://kagifeedback.org/d/2081">#2081</a> <a href="https://kagifeedback.org/u/spicytuna">@spicytuna</a></li><li>Kids account: use of email as username does not allow login <a href="https://kagifeedback.org/d/1828" data-id="1828"><span></span></a><a href="https://kagifeedback.org/d/1828">#1828</a> <a href="https://kagifeedback.org/u/Nils">@Nils</a></li><li>Small Values Calculations Search Suggestions <a href="https://kagifeedback.org/d/1969" data-id="1969"><span></span></a><a href="https://kagifeedback.org/d/1969">#1969</a> <a href="https://kagifeedback.org/u/Reroute5183">@Reroute5183</a> </li><li>The example searchs on the landing page do not respect system theme setting</li><li>Show wrong company stock price for apple inc keyword <a href="https://kagifeedback.org/d/1039" data-id="1039"><span></span></a><a href="https://kagifeedback.org/d/1039">#1039</a> <a href="https://kagifeedback.org/u/SukinoVerse">@SukinoVerse</a></li><li>Searching for "Vladimir Prelovac" in "Videos" gives results only about Putin <a href="https://kagifeedback.org/d/2069" data-id="2069"><span></span></a><a href="https://kagifeedback.org/d/2069">#2069</a> <a href="https://kagifeedback.org/u/ASeeker">@ASeeker</a></li></ul><p><strong>Fixed:</strong></p><ul><li>Misc billing/Stripe related issues</li><li>Trial searches were sometimes not properly updated</li><li>Case-sensitive matching for family/team invites</li><li>Safari extension Private Browsing Redirect <a href="https://kagifeedback.org/d/2084" data-id="2084"><span></span></a><a href="https://kagifeedback.org/d/2084">#2084</a> <a href="https://kagifeedback.org/u/kevrodg">@kevrodg</a> </li><li>Redirect Rules don't apply when using the “I feel lucky” (!) operator <a href="https://kagifeedback.org/d/1110" data-id="1110"><span></span></a><a href="https://kagifeedback.org/d/1110">#1110</a> <a href="https://kagifeedback.org/u/quinncom">@quinncom</a></li><li>Login does not work without JS <a href="https://kagifeedback.org/d/2045" data-id="2045"><span></span></a><a href="https://kagifeedback.org/d/2045">#2045</a> <a href="https://kagifeedback.org/u/eiangarsaowrfutarfgfarg">@eiangarsaowrfutarfgfarg</a> </li><li>XSS vulnerability when handling dates</li><li>Can only pay $10 with Paypal <a href="https://kagifeedback.org/d/2060" data-id="2060"><span></span></a><a href="https://kagifeedback.org/d/2060">#2060</a> <a href="https://kagifeedback.org/u/erikvanoosten">@erikvanoosten</a></li><li>Verbatim 'search without quotes' button not working. <a href="https://kagifeedback.org/d/1986" data-id="1986"><span></span></a><a href="https://kagifeedback.org/d/1986">#1986</a> <a href="https://kagifeedback.org/u/Value7609">@Value7609</a> </li><li>Search bar is broken, does not search anything <a href="https://kagifeedback.org/d/1975" data-id="1975"><span></span></a><a href="https://kagifeedback.org/d/1975">#1975</a> <a href="https://kagifeedback.org/u/ASeeker">@ASeeker</a></li><li>Duplicate videos in search results <a href="https://kagifeedback.org/d/1968" data-id="1968"><span></span></a><a href="https://kagifeedback.org/d/1968">#1968</a> <a href="https://kagifeedback.org/u/xjc">@xjc</a></li><li>The first result of the conversion widget adds unnecessary spaces before and after the output. <a href="https://kagifeedback.org/d/2078" data-id="2078"><span></span></a><a href="https://kagifeedback.org/d/2078">#2078</a> <a href="https://kagifeedback.org/u/loloriz">@loloriz</a> </li><li>Currency conversion, i.e., "100 usd to sek", should display widget <a href="https://kagifeedback.org/d/2031" data-id="2031"><span></span></a><a href="https://kagifeedback.org/d/2031">#2031</a> <a href="https://kagifeedback.org/u/Vapid">@Vapid</a></li><li>News widget is linking to msn.com news instead of original source. <a href="https://kagifeedback.org/d/2006" data-id="2006"><span></span></a><a href="https://kagifeedback.org/d/2006">#2006</a> <a href="https://kagifeedback.org/u/yellow">@yellow</a></li><li>Clicking "save" after adjusting soft/hard limit in billing leads to "We haven’t found anything." page <a href="https://kagifeedback.org/d/1947" data-id="1947"><span></span></a><a href="https://kagifeedback.org/d/1947">#1947</a> <a href="https://kagifeedback.org/u/chris_20017">@chris_20017</a></li><li>Empty li on billing page <a href="https://kagifeedback.org/d/2019" data-id="2019"><span></span></a><a href="https://kagifeedback.org/d/2019">#2019</a> <a href="https://kagifeedback.org/u/jonathon">@jonathon</a></li></ul><p><strong>Assistant beta:</strong></p><p>✨ Features</p><ul><li>All new mode selector (now with color!)</li><li>Info boxes for each mode, shown when clicking the ? icon</li><li>Summaries will be given in bullets</li><li>Input limits raised (for real this time) to 8k chars</li></ul><p>🪲 Bugfixes</p><ul><li>No longer re-summarize documents and delay answers</li><li>Citations will be shown with regenerated queries (and other edge cases)</li><li>Various internal AI improvements</li><li>Encoding of characters fixed</li><li>Threads won't change modes anymore</li><li>Uploading documents works no matter what mode is selected as default</li><li>More robust mode selection on landing page</li><li>No longer downgrading followups to gpt-3.5</li></ul></div></div><div id="2012"><h2><span>Sep 21, 2023 - Unlimited searches for $10/month</span><a href="https://kagi.com/changelog#2012"> #</a></h2><div><p>We're thrilled to bring you the news you've all been eagerly awaiting -  Kagi is now available with unlimited searches for just $10/month!</p><p>More in our announcement <a href="https://blog.kagi.com/unlimited-searches-for-10" rel="ugc nofollow">blog post</a>.</p><p><img src="https://kagifeedback.org/assets/files/2023-09-21/1695325178-755236-doggo-2-2.png" title="" alt=""></p></div></div><div id="1970"><h2><span>Sep 14, 2023 - Share lenses with other users</span><a href="https://kagi.com/changelog#1970"> #</a></h2><div><p>You can now share lenses with other Kagi users.</p><p>Enable lens sharing in your lens settings:</p><p><img src="https://kagifeedback.org/assets/files/2023-09-15/1694748322-861944-screenshot-2023-09-14-at-202519.png" title="" alt=""></p><p>The lens will now have the share icon which will allow you to copy a sharable link:<br><img src="https://kagifeedback.org/assets/files/2023-09-15/1694748395-602520-screenshot-2023-09-14-at-202631.png" title="" alt=""></p><p>Other users can now import the lens, using the link.</p><p>Try importing this demo lens by clicking this <a href="https://kagi.com/lenses/lw70zLGD2VksUiNe5dSFTTm90toUCp9y" rel="ugc nofollow">link</a> (it will create a lens that will allow you to search on Hacker News).</p><p><strong>New</strong></p><ul><li>Share custom lenses with other users <a href="https://kagifeedback.org/d/127" data-id="127"><span></span></a><a href="https://kagifeedback.org/d/127">#127</a> <a href="https://kagifeedback.org/u/riddley">@riddley</a> </li><li>Add darkmode support for inline map</li></ul><p><strong>Improvements</strong></p><ul><li>Results filters borders have ugly padding <a href="https://kagifeedback.org/d/1960" data-id="1960"><span></span></a><a href="https://kagifeedback.org/d/1960">#1960</a> <a href="https://kagifeedback.org/u/bln">@bln</a> </li><li>Dropdown menus should close on selection of item</li><li>Switch plan page should display your current plan <a href="https://kagifeedback.org/d/1729" data-id="1729"><span></span></a><a href="https://kagifeedback.org/d/1729">#1729</a> <a href="https://kagifeedback.org/u/Jake-Moss">@Jake-Moss</a></li><li>Apple touch icon and favicon-32x32/16x16 don't match favicon <a href="https://kagifeedback.org/d/1877" data-id="1877"><span></span></a><a href="https://kagifeedback.org/d/1877">#1877</a> <a href="https://kagifeedback.org/u/Value7609">@Value7609</a> </li><li>Show phone number in maps results not just a call button <a href="https://kagifeedback.org/d/1630" data-id="1630"><span></span></a><a href="https://kagifeedback.org/d/1630">#1630</a> <a href="https://kagifeedback.org/u/zannzen">@zannzen</a></li></ul><p><strong>Fixed</strong></p><ul><li>Escaping issues in autosuggest subtext</li><li>Can't save quick bangs <a href="https://kagifeedback.org/d/1922" data-id="1922"><span></span></a><a href="https://kagifeedback.org/d/1922">#1922</a> <a href="https://kagifeedback.org/u/Reroute5183">@Reroute5183</a></li><li>Duplicate results with different domain capitalization <a href="https://kagifeedback.org/d/1860" data-id="1860"><span></span></a><a href="https://kagifeedback.org/d/1860">#1860</a> <a href="https://kagifeedback.org/u/dan">@dan</a> </li><li><code>"business.site" -site:business.site</code> still gave *.business.site results <a href="https://kagifeedback.org/d/1818" data-id="1818"><span></span></a><a href="https://kagifeedback.org/d/1818">#1818</a> <a href="https://kagifeedback.org/u/gslin">@gslin</a></li><li>-site operator not working properly <a href="https://kagifeedback.org/d/1661" data-id="1661"><span></span></a><a href="https://kagifeedback.org/d/1661">#1661</a> <a href="https://kagifeedback.org/u/vicky">@vicky</a> </li><li>Custom bang leads to wrong bang suggestion <a href="https://kagifeedback.org/d/1168" data-id="1168"><span></span></a><a href="https://kagifeedback.org/d/1168">#1168</a> <a href="https://kagifeedback.org/u/Browsing6853">@Browsing6853</a></li><li>Local language version is ignored in Wikipedia preview <a href="https://kagifeedback.org/d/1925" data-id="1925"><span></span></a><a href="https://kagifeedback.org/d/1925">#1925</a> <a href="https://kagifeedback.org/u/chris_20017">@chris_20017</a> </li><li>Directions component become irresponsible when Kagi maps is toggled <a href="https://kagifeedback.org/d/1907" data-id="1907"><span></span></a><a href="https://kagifeedback.org/d/1907">#1907</a> <a href="https://kagifeedback.org/u/Zayon">@Zayon</a></li><li>Place search map image link leads to unexpected map result <a href="https://kagifeedback.org/d/1662" data-id="1662"><span></span></a><a href="https://kagifeedback.org/d/1662">#1662</a> <a href="https://kagifeedback.org/u/cbirdsong">@cbirdsong</a></li><li>Map thumbnail takes me to the wrong location <a href="https://kagifeedback.org/d/1410" data-id="1410"><span></span></a><a href="https://kagifeedback.org/d/1410">#1410</a> <a href="https://kagifeedback.org/u/jrdmcgr">@jrdmcgr</a></li><li>Map widget showing wrong location <a href="https://kagifeedback.org/d/1530" data-id="1530"><span></span></a><a href="https://kagifeedback.org/d/1530">#1530</a> <a href="https://kagifeedback.org/u/yeri">@yeri</a></li><li>Map preview shows different location than rest of info box <a href="https://kagifeedback.org/d/1666" data-id="1666"><span></span></a><a href="https://kagifeedback.org/d/1666">#1666</a> <a href="https://kagifeedback.org/u/sw">@sw</a></li><li>When performing local search on VPN it does not use location permission  <a href="https://kagifeedback.org/d/1564" data-id="1564"><span></span></a><a href="https://kagifeedback.org/d/1564">#1564</a> <a href="https://kagifeedback.org/u/cakeboss">@cakeboss</a></li><li>Using a capital character when blocking a domain causes the domain to not be blocked <a href="https://kagifeedback.org/d/1915" data-id="1915"><span></span></a><a href="https://kagifeedback.org/d/1915">#1915</a> <a href="https://kagifeedback.org/u/laiz">@laiz</a></li><li>Default query for !gt is "{{{s}}}" <a href="https://kagifeedback.org/d/1903" data-id="1903"><span></span></a><a href="https://kagifeedback.org/d/1903">#1903</a> <a href="https://kagifeedback.org/u/nwoeanhinnogaehr">@nwoeanhinnogaehr</a></li><li>Searching for "translated books" attempts to translate the string "d book" <a href="https://kagifeedback.org/d/1862" data-id="1862"><span></span></a><a href="https://kagifeedback.org/d/1862">#1862</a> <a href="https://kagifeedback.org/u/XMPPwocky">@XMPPwocky</a> </li><li>AND search operator description identical to OR <a href="https://kagifeedback.org/d/1930" data-id="1930"><span></span></a><a href="https://kagifeedback.org/d/1930">#1930</a> <a href="https://kagifeedback.org/u/Siemova">@Siemova</a></li></ul><p><strong>AI</strong></p><ul><li>FastGPT moved to <a href="https://kagi.com/fastgpt" rel="ugc nofollow">https://kagi.com/fastgpt</a></li><li>FastGPT: Removed blinking cursor</li><li>FastGPT: Add an X to the query input box to delete the whole query <a href="https://kagifeedback.org/d/1950" data-id="1950"><span></span></a><a href="https://kagifeedback.org/d/1950">#1950</a> <a href="https://kagifeedback.org/u/Reroute5183">@Reroute5183</a></li><li>Universal Summarizer ignoring language <a href="https://kagifeedback.org/d/1902" data-id="1902"><span></span></a><a href="https://kagifeedback.org/d/1902">#1902</a> <a href="https://kagifeedback.org/u/Heeroo">@Heeroo</a> </li><li>When summarising Tildes.net it should take into account the comments as well <a href="https://kagifeedback.org/d/1879" data-id="1879"><span></span></a><a href="https://kagifeedback.org/d/1879">#1879</a> <a href="https://kagifeedback.org/u/hook">@hook</a></li><li>The ' character in a code snippet in Discuss Document is displayed as it's unicode hex code: &amp;#x27; <a href="https://kagifeedback.org/d/1678" data-id="1678"><span></span></a><a href="https://kagifeedback.org/d/1678">#1678</a> <a href="https://kagifeedback.org/u/Grooty">@Grooty</a></li></ul></div></div><div id="1933"><h2><span>Sep 7, 2023 - Kagi Small Web</span><a href="https://kagi.com/changelog#1933"> #</a></h2><div><p>As a part of our ongoing mission to humanize the web, we're thrilled to announce the launch of Kagi Small Web initiative.</p><p>Learn more in our latest <a href="https://blog.kagi.com/small-web" rel="ugc nofollow">blog post</a>.</p><p><img src="https://kagifeedback.org/assets/files/2023-09-07/1694099345-602123-image.png" title="" alt=""></p> </div></div><div id="1909"><h2><span>Aug 31, 2023 - New payment methods</span><a href="https://kagi.com/changelog#1909"> #</a></h2><div><p>We are happy to announce that Kagi now accepts payments through PayPal and Venmo, as well as EUR payments via iDEAL and Giropay. In addition we accept Bitcoin and Bitcoin Lightning through OpenNode.</p><p>Read more in the <a href="https://blog.kagi.com/accepting-paypal-bitcoin" rel="ugc nofollow">blog post</a>.</p><p><img src="https://kagifeedback.org/assets/files/2023-08-31/1693524705-575549-screenshot-2023-08-31-at-163128.png" title="" alt=""></p><p>In addition Kagi now has support for OpenNode which closes one of the most upvotes feature suggestions on our feedback forum.</p><ul><li>Enable anonymous payments  <a href="https://kagifeedback.org/d/493" data-id="493"><span></span></a><a href="https://kagifeedback.org/d/493">#493</a> <a href="https://kagifeedback.org/u/PrivateLiberty">@PrivateLiberty</a></li></ul><p><img src="https://kagifeedback.org/assets/files/2023-08-31/1693522014-802416-screenshot-2023-08-31-at-154646.png" title="" alt=""></p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Orgzly Revived: a community-maintained version of Orgzly (122 pts)]]></title>
            <link>https://github.com/orgzly-revived/orgzly-android-revived</link>
            <guid>39393049</guid>
            <pubDate>Fri, 16 Feb 2024 04:06:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/orgzly-revived/orgzly-android-revived">https://github.com/orgzly-revived/orgzly-android-revived</a>, See on <a href="https://news.ycombinator.com/item?id=39393049">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p><a title="Build" href="https://github.com/orgzly-revived/orgzly-android-revived/actions/workflows/android-build-master.yml"><img src="https://github.com/orgzly-revived/orgzly-android-revived/actions/workflows/android-build-master.yml/badge.svg"></a>
<a title="Crowdin" href="https://crowdin.com/project/orgzly" rel="nofollow"><img src="https://camo.githubusercontent.com/884b5f2b766fc1a788952dcf48ae4e06560457f655f0b675129be4bc2b65059c/68747470733a2f2f64333232637174353834626f346f2e636c6f756466726f6e742e6e65742f6f72677a6c792f6c6f63616c697a65642e737667" data-canonical-src="https://d322cqt584bo4o.cloudfront.net/orgzly/localized.svg"></a>
</p>
<a href="https://f-droid.org/packages/com.orgzlyrevived" rel="nofollow">
    <img src="https://camo.githubusercontent.com/f422f6f830e814ec7e766de8fef4db949c6add75a9d58548ab2f5d29855c0616/68747470733a2f2f6664726f69642e6769746c61622e696f2f617274776f726b2f62616467652f6765742d69742d6f6e2e706e67" alt="Get it on F-Droid" height="80" data-canonical-src="https://fdroid.gitlab.io/artwork/badge/get-it-on.png">
</a>
<h2 tabindex="-1" dir="auto">Orgzly Revived</h2>
<p dir="auto">Orgzly Revived is a community-maintained version of Orgzly, as the development of the original app is no longer active.</p>
<p dir="auto">The Orgzly Revived project aims to continue development of the Orgzly application. The rebranding is due
  to the disappearance of the Orgzly author so we, as the community, had to come up with a new solution.
  We plan to continue adding useful features, fixing bugs, keeping up-to-date with technological evolution,
  and generally maintaining the <a href="https://orgmode.org/" rel="nofollow">Org Mode</a> outreach and its support on Android.</p>
<p dir="auto"><a href="https://orgzly.com/" rel="nofollow">Orgzly</a> is an outliner for taking notes and managing to-do lists. The source code
  <a href="https://github.com/orgzly/orgzly-android">repository</a> is available on GitHub but is no longer actively
  maintained. We have used this code as the base for Orgzly Revived.</p>
<p dir="auto">With Orgzly, you can keep notebooks stored in plain-text and have them synchronized
  with a directory on your mobile device or an SD card, WebDAV server, or Dropbox.</p>
<p dir="auto">Notebooks are saved in <i>Org mode</i>’s file format. “Org mode is for
  keeping notes, maintaining TODO lists, planning projects, and
  authoring documents with a fast and effective plain-text system.” See
  <a href="http://orgmode.org/" rel="nofollow">http://orgmode.org</a> for more information.</p>
<h2 tabindex="-1" dir="auto">End-user documentation</h2>
<p dir="auto">Find out more here: <a href="https://github.com/orgzly/documentation">https://github.com/orgzly/documentation</a></p>
<h2 tabindex="-1" dir="auto">Building &amp; testing</h2>
<p dir="auto">If you don’t use Android Studio and wish to <a href="https://developer.android.com/studio/build/building-cmdline.html" rel="nofollow">build</a> and <a href="https://developer.android.com/studio/test/command-line.html" rel="nofollow">test</a> the app
  from command line, the standard set of Gradle tasks is available.  For
  example:</p>
<ul dir="auto">
  <li><code>./gradlew build</code> builds the project and generates APK files</li>
  <li><code>./gradlew connectedAndroidTest</code> runs instrumented unit tests</li>
</ul>
<p dir="auto">Make sure you <a href="https://developer.android.com/training/testing/espresso/setup" rel="nofollow">turn off animations</a> for the device you’re testing on.</p>
<h2 tabindex="-1" dir="auto">Contributing</h2>
<p dir="auto">Please feel free to get involved in the project on GitHub by contributing issues, ideas, or features!
  We generally plan to leave existing open issues in the original
  <a href="https://github.com/orgzly/orgzly-android">repository</a> and reference them here as
  they are addressed.</p>
<h2 tabindex="-1" dir="auto">License</h2>
<p dir="auto">The project is licensed under the <a href="https://github.com/orgzly-revived/orgzly-android-revived/blob/master/LICENSE">GNU General Public License version 3 (or newer)</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Think Python, 3rd Edition (369 pts)]]></title>
            <link>https://allendowney.github.io/ThinkPython/</link>
            <guid>39392881</guid>
            <pubDate>Fri, 16 Feb 2024 03:32:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://allendowney.github.io/ThinkPython/">https://allendowney.github.io/ThinkPython/</a>, See on <a href="https://news.ycombinator.com/item?id=39392881">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
      
      
      <main id="main-content">
        
        



          <div>
              
              
              
              

<div id="jb-print-docs-body">
            
            <p>
                <h2> Contents </h2>
            </p>
            <nav aria-label="Page">
                <ul>
<li><a href="#the-notebooks">The notebooks</a><ul>
<li><a href="#chapter-1-programming-as-a-way-of-thinking">Chapter 1: Programming as a way of thinking</a></li>
</ul>
</li>
<li><a href="#resources-for-teachers">Resources for teachers</a></li>
</ul>
            </nav>
        </div>

              
                

                <article role="main">
                  
  <section id="think-python-3rd-edition">
<h2>Think Python, 3rd edition<a href="#think-python-3rd-edition" title="Permalink to this heading">#</a></h2>
<p><em>Think Python</em> is an introduction to Python for people who have never programmed before – or for people who have tried and had a hard time.</p>
<p><a href="https://greenteapress.com/wp/think-python-3rd-edition/">Here is the landing page for the book at Green Tea Press</a>.</p>
<p>For the third edition, the biggest changes are:</p>
<ul>
<li><p>The book is now entirely in Jupyter notebooks, so you can read the text, run the code, and work on the exercises, all in one place. Using the links below, you can run the notebooks on Colab, so you don’t have to install anything to get started.</p></li>
<li><p>The text is substantially revised and a few chapters have been reordered. There are more exercises now, and I think a lot of them are better.</p></li>
<li><p>At the end of every chapter, there are suggestions for using tools like ChatGPT and Colab AI to learn more and to get help with the exercises.</p></li>
</ul>
<p>The book is scheduled to be published by O’Reilly Media in July 2024, so it is a work in progress.
Starting in February 2024, I plan to release new chapters here, about one per week.</p>
<p><a href="https://www.oreilly.com/library/view/think-python/9781098155421/">You can read the early release at O’Reilly Media</a></p>
<p><a href="https://www.amazon.com/_/dp/1098155432?smid=ATVPDKIKX0DER&amp;_encoding=UTF8&amp;tag=oreilly20-20&amp;_encoding=UTF8&amp;tag=greenteapre01-20&amp;linkCode=ur2&amp;linkId=e2a529f94920295d27ec8a06e757dc7c&amp;camp=1789&amp;creative=9325">You can preorder the third edition on Amazon</a></p>
<section id="the-notebooks">
<h2>The notebooks<a href="#the-notebooks" title="Permalink to this heading">#</a></h2>
<section id="chapter-1-programming-as-a-way-of-thinking">
<h3>Chapter 1: Programming as a way of thinking<a href="#chapter-1-programming-as-a-way-of-thinking" title="Permalink to this heading">#</a></h3>
<ul>
<li><p><a href="https://colab.research.google.com/github/AllenDowney/ThinkPython/blob/v3/chapters/chap01.ipynb">Click here to run Chapter 1 on Colab</a></p></li>
</ul>
</section>
</section>
<section id="resources-for-teachers">
<h2>Resources for teachers<a href="#resources-for-teachers" title="Permalink to this heading">#</a></h2>
<p>If you are teaching with this book, here are some resources you might find useful.</p>
<ul>
<li><p>You can download notebooks with solutions [COMING SOON]</p></li>
<li><p>Quizzes for each chapter, and a summative quiz for the whole book, are available [COMING SOON]</p></li>
<li><p><em>Teaching and Learning with Jupyter</em> is an online book with suggestions for using Jupyter effectively in the classroom. You can <a href="https://jupyter4edu.github.io/jupyter-edu-book">read the book here</a>.</p></li>
<li><p>One of the best ways to use notebooks in the classroom is live coding, where an instructor writes code and students follow along in their own notebooks. To learn about live coding – and a lot of other great advice about teaching programming – I recommend the teacher training provided by The Carpentries, <a href="https://carpentries.github.io/instructor-training">which you can read here</a>.</p></li>
</ul>
</section>

</section>

    
    

                </article>
              

              
              
                
              
            </div>
          
        

      </main>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Firefly III: A free and open source personal finance manager (183 pts)]]></title>
            <link>https://www.firefly-iii.org/</link>
            <guid>39392428</guid>
            <pubDate>Fri, 16 Feb 2024 02:34:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.firefly-iii.org/">https://www.firefly-iii.org/</a>, See on <a href="https://news.ycombinator.com/item?id=39392428">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<h2>Budgets, categories and tags</h2>
<p>Name your poison. Do you like to work with tags? Need to budget your expenses? Want to categorize all of your hobby expenses? Look no further. Firefly III supports all kinds. Budgets can be expanded with limits in multiple currencies, so you can budget both your daily household expenses and what you spend in Imperial Credits when visiting Tatooine.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Magika: AI powered fast and efficient file type identification (546 pts)]]></title>
            <link>https://opensource.googleblog.com/2024/02/magika-ai-powered-fast-and-efficient-file-type-identification.html</link>
            <guid>39391688</guid>
            <pubDate>Fri, 16 Feb 2024 01:02:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://opensource.googleblog.com/2024/02/magika-ai-powered-fast-and-efficient-file-type-identification.html">https://opensource.googleblog.com/2024/02/magika-ai-powered-fast-and-efficient-file-type-identification.html</a>, See on <a href="https://news.ycombinator.com/item?id=39391688">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-824803887080674909" itemprop="articleBody">
<meta content="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbkRgYqDxKQrhyphenhyphenl-sNYpZJ08Oo5TqD08Yk5tcbCYzFatO-cDjDRlDH96vDiO0ylvYN-TZoKSfR4LBln1wSFJLixBRiuVeuzaF0UQ3wGUMt14VKugPHuk7q0CwSgeg0V7OVS_yxaUXbFus2yVXrLmRdB88QQyXagMFE6Axs91iDgdq1hUkS4hsS4ECMuf4/s1600/OSS-Majika-Social-V5.png" name="twitter:image">
<p>

<a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxNXxbDvO9U0sI1_Vh0b_iePDZZDhr-XzvjOQTrAFBHEnaFSoohzbNR36Op83b8Hj7L1JOiex5JWEyQFVzC5jWBtLRULhMDmbu_Z4Cs3094FwQVmI8H2uOYKIu_ozG4luPhxeJIQiH_GjV7zAP4K-o-B6RnTZptdhH3nhUfatSurWEmn2x9vdp0vitAk4/s1600/OSS-Magika-Banner-V5%20%281%29.png"><img data-original-height="800" data-original-width="1058" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxNXxbDvO9U0sI1_Vh0b_iePDZZDhr-XzvjOQTrAFBHEnaFSoohzbNR36Op83b8Hj7L1JOiex5JWEyQFVzC5jWBtLRULhMDmbu_Z4Cs3094FwQVmI8H2uOYKIu_ozG4luPhxeJIQiH_GjV7zAP4K-o-B6RnTZptdhH3nhUfatSurWEmn2x9vdp0vitAk4/s1600/OSS-Magika-Banner-V5%20%281%29.png"></a></p><p>Today we are <a href="https://google.github.io/magika/" target="_blank">open-sourcing Magika</a>, Google’s AI-powered file-type identification system, to help others accurately detect binary and textual file types. Under the hood, Magika employs a custom, highly optimized deep-learning model, enabling precise file identification within milliseconds, even when running on a CPU. </p>

<div><table><tbody><tr><td><center><img alt="Magika command line tool used to recognize a identify the type of a diverse set of files" data-original-height="1504" data-original-width="720" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiPjf8Rag2sUXJw_aUJcqEBo_RPNRG0PyFSJI8FXuyeaCYT195znXw4DW235ZHVihlUfZ744MFeZBlfhG_xdq8jjnQN5ICFjMO-rphRjt9BfO1gfyupghgAPWxNsivP9l362AcNXWnFSj_CzaJ1Con6ZMfJ1RcFExjhCDRMUs59qcQAxulIlkhBn-uhNDU/s1600/image1.png" td=""></center></td></tr><tr><td><i>Magika command line tool used to recognize a identify the type of a diverse set of files</i></td></tr></tbody></table></div>

<p>You can <a href="https://google.github.io/magika/" target="_blank">try the Magika web demo today</a>, or install it as a Python library and standalone command line tool (output is showcased above) by  using the standard command line  <span>pip install magika</span>.</p>

<h2>Why identifying file type is difficult</h2>

<p>Since the early days of computing, accurately detecting file types has been crucial in determining how to process files. Linux comes equipped with <a href="https://github.com/file/file" target="_blank"><span>libmagic</span> and the <span>file utility</span></a>, which have served as the de facto standard for file type identification for over 50 years. Today web browsers, code editors, and countless other software rely on file-type detection to decide how to properly render a file. For example, modern code editors use file-type detection to choose which syntax coloring scheme to use as the developer starts typing in a new file. </p>

<p>Accurate file-type detection is a notoriously difficult problem because each file format has a different structure, or no structure at all. This is particularly challenging for textual formats and programming languages as they have very similar constructs. So far, <span>libmagic</span> and most other file-type-identification software have been relying on a handcrafted collection of heuristics and custom rules to detect each file format.</p> 

<p>This manual approach is both time consuming and error prone as it is hard for humans to create generalized rules by hand. In particular for security applications, creating dependable detection is especially challenging as attackers are constantly attempting to confuse detection with adversarially-crafted payloads.</p>

<p>To address this issue and provide fast and accurate file-type detection we researched and developed Magika, a new AI powered file type detector. Under the hood, Magika uses a custom, highly optimized deep-learning model designed and trained using <a href="https://keras.io/" target="_blank">Keras</a> that only weighs about 1MB.  At inference time Magika uses  <a href="https://onnx.ai/" target="_blank">Onnx</a> as an inference engine to ensure files are identified in a matter of milliseconds, almost as fast as a non-AI tool even on CPU.</p>

<h2>Magika Performance</h2>

<div><table><tbody><tr><td><center><img alt="Magika detection quality compared to other tools on our 1M files benchmark" data-original-height="1504" data-original-width="720" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuz_BUIEGw6FQfhzA1lr4EQ1SLO_O0S8TIlGrnYY5Kh0BRngFTboPD4_DTCYsbKkO9c51xUKaulCu8ivEdyQlGIhEfvRzleArV9XpatHdvgSf62F1kt3DMwSOwOOan6gL2kaaangCIzQBv1ZVGIXInx-9jpO9N_OkYR8LeJvl6A-Ba3Qdfq441i9QBxvY/s1600/image2.png" td=""></center></td></tr><tr><td><i>Magika detection quality compared to other tools on our 1M files benchmark</i></td></tr></tbody></table></div>

<p>Performance wise, Magika, thanks to its AI model and large training dataset, is able to  outperform other existing tools by about 20% when evaluated on a 1M files benchmark that encompasses over 100 file types.  Breaking down by file type, as reported in the table below, we see even greater performance gains on textual files, including code files  and configuration files that other tools can struggle with.</p>

<div><table><tbody><tr><td><center><img alt="Table showing various file type identification tools performance for a selection of the file types included in our benchmark" data-original-height="1504" data-original-width="720" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYHJfKw3xp2yBY_qdS8RcamEPn5oWhK2jbbkSgztnC_icqV7IFqh3lcAOWEzEu20TI2zMwBsdBp6YauRRc-ouVZTqLpxbkW5PMoBfuZLgSJKwfYIBVjGrrbM8Ob2P5iuJvQXE2eQ5mGe0WFXT4ilZbciPwasz8h-6AKx-sk7CH_klLRwrYbC3VqDPqlng/s1600/Screenshot%202024-02-15%20at%2011.24.40%E2%80%AFAM.png" td=""></center></td></tr><tr><td><i>Various file type identification tools performance for a selection of the file types included in our benchmark - n/a indicates the tool doesn’t detect the given file type.</i></td></tr></tbody></table></div><br>

<h2>Magika at Google</h2>

<p>Internally, Magika is used at scale to help improve Google users’ safety by routing Gmail, Drive, and Safe Browsing files to the proper security and content policy scanners.
Looking at a weekly average of hundreds of billions of files reveals that Magika improves file type identification accuracy by 50% compared to our previous system that relied on handcrafted rules. In particular, this increase in accuracy allows us to scan 11% more files with our <a href="https://security.googleblog.com/2020/02/improving-malicious-document-detection.html" target="_blank">specialized malicious AI document scanners</a> and reduce the number of unidentified files to 3%.</p>

<p>The upcoming integration of Magika with VirusTotal will complement the platform's existing Code Insight functionality, which employs Google's generative AI to analyze and detect malicious code. Magika will act as a pre-filter before files are analyzed by <a href="https://blog.virustotal.com/2023/04/introducing-virustotal-code-insight.html" target="_blank">Code Insight</a>, improving the platform’s efficiency and accuracy. This integration, due to VirusTotal’s collaborative nature, directly contributes to the global cybersecurity ecosystem, fostering a safer digital environment.</p>

<h2>Open Sourcing Magika</h2>

<p>By <a href="https://google.github.io/magika/" target="_blank">open-sourcing Magika</a>, we aim to help other software improve their file identification accuracy and offer researchers a reliable method for identifying file types at scale. </p>

<p><a href="https://github.com/google/magika" target="_blank">Magika code and model</a> are freely available starting today in Github under the Apache2 License. Magika  can also quickly be installed as a standalone utility and python library via the <a href="https://pypi.org/project/magika/" target="_blank">pypi package manager</a> by simply typing <span>pip install</span> magika with no GPU required. We also have an experimental <a href="https://www.npmjs.com/package/magika" target="_blank">npm package</a> if you would like to use the TFJS version.</p>

<p>To learn more about how to use it, please refer to <a href="https://www.npmjs.com/package/magika" target="_blank">Magika documentation site</a>.</p><br>

<h4><span>Acknowledgements </span></h4>
  
<p>Magika would not have been possible without the help of many people including: Ange Albertini, Loua Farah, Francois Galilee, Giancarlo Metitieri, Luca Invernizzi, Young Maeng, Alex Petit-Bianco, David Tao, Kurt Thomas, Amanda Walker, and Zhixun Tan.</p>

<p><em>By Elie Bursztein – Cybersecurity AI Technical and Research Lead and Yanick Fratantonio – Cybersecurity Research Scientist</em></p>







</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Video generation models as world simulators (330 pts)]]></title>
            <link>https://openai.com/research/video-generation-models-as-world-simulators</link>
            <guid>39391458</guid>
            <pubDate>Fri, 16 Feb 2024 00:38:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/research/video-generation-models-as-world-simulators">https://openai.com/research/video-generation-models-as-world-simulators</a>, See on <a href="https://news.ycombinator.com/item?id=39391458">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content"><!--[--><!--[--><!--]--><!--[--><div><p>This technical report focuses on (1) our method for turning visual data of all types into a unified representation that enables large-scale training of generative models, and (2) qualitative evaluation of Sora’s capabilities and limitations. Model and implementation details are not included in this report.</p><p>Much prior work has studied generative modeling of video data using a variety of methods, including recurrent networks,<span><sup><span>[^1]</span></sup><!----></span><span><sup><span>[^2]</span></sup><!----></span><span><sup><span>[^3]</span></sup><!----></span> generative adversarial networks,<span><sup><span>[^4]</span></sup><!----></span><span><sup><span>[^5]</span></sup><!----></span><span><sup><span>[^6]</span></sup><!----></span><span><sup><span>[^7]</span></sup><!----></span> autoregressive transformers,<span><sup><span>[^8]</span></sup><!----></span><span><sup><span>[^9]</span></sup><!----></span> and diffusion models.<span><sup><span>[^10]</span></sup><!----></span><span><sup><span>[^11]</span></sup><!----></span><span><sup><span>[^12]</span></sup><!----></span> These works often focus on a narrow category of visual data, on shorter videos, or on videos of a fixed size. Sora is a generalist model of visual data—it can generate videos and images spanning diverse durations, aspect ratios and resolutions, up to a full minute of high definition video.<br></p></div><!--]--><!--[--><div id="turning-visual-data-into-patches" data-heading=""><p><h2>Turning visual data into patches</h2></p></div><!--]--><!--[--><div><p>We take inspiration from large language models which acquire generalist capabilities by training on internet-scale data.<span><sup><span>[^13]</span></sup><!----></span><span><sup><span>[^14]</span></sup><!----></span> The success of the LLM paradigm is enabled in part by the use of tokens<em> </em>that elegantly unify diverse modalities of text—code, math and various natural languages. In this work, we consider how generative models of visual data can inherit such benefits. Whereas LLMs have text tokens, Sora has visual <em>patches</em>. Patches have previously been shown to be an effective representation for models of visual data.<span><sup><span>[^15]</span></sup><!----></span><span><sup><span>[^16]</span></sup><!----></span><span><sup><span>[^17]</span></sup><!----></span><span><sup><span>[^18]</span></sup><!----></span> We find that patches are a highly-scalable and effective representation for training generative models on diverse types of videos and images.<br></p></div><!--]--><!--[--><div><figure><p><img src="https://images.openai.com/blob/1d2955dd-9d05-4f33-b346-be531d2a7737/figure-patches.png?trim=0,0,0,0&amp;width=10&amp;height=10&amp;quality=50" width="2031" height="378" alt="Figure Patches" loading="lazy" data-nuxt-img="" sizes="(max-width: 744px) 100vw, (max-width: 1280px) 100vw, (max-width: 1440px) 100vw, 100vw" srcset="https://images.openai.com/blob/1d2955dd-9d05-4f33-b346-be531d2a7737/figure-patches.png?trim=0,0,0,0&amp;width=400 400w, https://images.openai.com/blob/1d2955dd-9d05-4f33-b346-be531d2a7737/figure-patches.png?trim=0,0,0,0&amp;width=800 800w, https://images.openai.com/blob/1d2955dd-9d05-4f33-b346-be531d2a7737/figure-patches.png?trim=0,0,0,0&amp;width=1000 1000w, https://images.openai.com/blob/1d2955dd-9d05-4f33-b346-be531d2a7737/figure-patches.png?trim=0,0,0,0&amp;width=1400 1400w, https://images.openai.com/blob/1d2955dd-9d05-4f33-b346-be531d2a7737/figure-patches.png?trim=0,0,0,0&amp;width=2000 2000w, https://images.openai.com/blob/1d2955dd-9d05-4f33-b346-be531d2a7737/figure-patches.png?trim=0,0,0,0&amp;width=2600 2600w, https://images.openai.com/blob/1d2955dd-9d05-4f33-b346-be531d2a7737/figure-patches.png?trim=0,0,0,0&amp;width=3200 3200w" aria-hidden="false"></p><figcaption><!--[--><!--]--></figcaption></figure></div><!--]--><!--[--><div><p>At a high level, we turn videos into patches by first compressing videos into a lower-dimensional latent space,<span><sup><span>[^19]</span></sup><!----></span> and subsequently decomposing the representation into spacetime patches.<br></p></div><!--]--><!--[--><div id="video-compression-network" data-heading=""><p><h2>Video compression network</h2></p></div><!--]--><!--[--><div><p>We train a network that reduces the dimensionality of visual data.<span><sup><span>[^20]</span></sup><!----></span> This network takes raw video as input and outputs a latent representation that is compressed both temporally and spatially. Sora is trained on and subsequently generates videos within this compressed latent space. We also train a corresponding decoder model that maps generated latents back to pixel space.</p></div><!--]--><!--[--><div id="spacetime-latent-patches" data-heading=""><p><h2>Spacetime Latent Patches</h2></p></div><!--]--><!--[--><div><p>Given a compressed input video, we extract a sequence of spacetime patches which act as transformer tokens. This scheme works for images too since images are just videos with a single frame. Our patch-based representation enables Sora to train on videos and images of variable resolutions, durations and aspect ratios. At inference time, we can control the size of generated videos by arranging randomly-initialized patches in an appropriately-sized grid.<br></p></div><!--]--><!--[--><div id="scaling-transformers-for-video-generation" data-heading=""><p><h2>Scaling transformers for video generation</h2></p></div><!--]--><!--[--><div><p>Sora is a diffusion model<span><sup><span>[^21]</span></sup><!----></span><span><sup><span>[^22]</span></sup><!----></span><span><sup><span>[^23]</span></sup><!----></span><span><sup><span>[^24]</span></sup><!----></span><span><sup><span>[^25]</span></sup><!----></span>; given input noisy patches (and conditioning information like text prompts), it’s trained to predict the original “clean” patches. Importantly, Sora is a diffusion <em>transformer</em>.<span><sup><span>[^26]</span></sup><!----></span> Transformers have demonstrated remarkable scaling properties across a variety of domains, including language modeling,<span><sup><span>[^13]</span></sup><!----></span><span><sup><span>[^14]</span></sup><!----></span> computer vision,<span><sup><span>[^15]</span></sup><!----></span><span><sup><span>[^16]</span></sup><!----></span><span><sup><span>[^17]</span></sup><!----></span><span><sup><span>[^18]</span></sup><!----></span> and image generation.<span><sup><span>[^27]</span></sup><!----></span><span><sup><span>[^28]</span></sup><!----></span><span><sup><span>[^29]</span></sup><!----></span><br></p></div><!--]--><!--[--><div><figure><p><img src="https://images.openai.com/blob/aa8b687c-bee5-4d72-a1c8-1350d33c80d3/figure-diffusion.png?trim=0,0,0,0&amp;width=10&amp;height=10&amp;quality=50" width="1261" height="312" alt="Figure Diffusion" loading="lazy" data-nuxt-img="" sizes="(max-width: 744px) 100vw, (max-width: 1280px) 100vw, (max-width: 1440px) 100vw, 100vw" srcset="https://images.openai.com/blob/aa8b687c-bee5-4d72-a1c8-1350d33c80d3/figure-diffusion.png?trim=0,0,0,0&amp;width=400 400w, https://images.openai.com/blob/aa8b687c-bee5-4d72-a1c8-1350d33c80d3/figure-diffusion.png?trim=0,0,0,0&amp;width=800 800w, https://images.openai.com/blob/aa8b687c-bee5-4d72-a1c8-1350d33c80d3/figure-diffusion.png?trim=0,0,0,0&amp;width=1000 1000w, https://images.openai.com/blob/aa8b687c-bee5-4d72-a1c8-1350d33c80d3/figure-diffusion.png?trim=0,0,0,0&amp;width=1400 1400w, https://images.openai.com/blob/aa8b687c-bee5-4d72-a1c8-1350d33c80d3/figure-diffusion.png?trim=0,0,0,0&amp;width=2000 2000w, https://images.openai.com/blob/aa8b687c-bee5-4d72-a1c8-1350d33c80d3/figure-diffusion.png?trim=0,0,0,0&amp;width=2600 2600w, https://images.openai.com/blob/aa8b687c-bee5-4d72-a1c8-1350d33c80d3/figure-diffusion.png?trim=0,0,0,0&amp;width=3200 3200w" aria-hidden="false"></p><figcaption><!--[--><!--]--></figcaption></figure></div><!--]--><!--[--><div><p>In this work, we find that diffusion transformers scale effectively as video models as well. Below, we show a comparison of video samples with fixed seeds and inputs as training progresses. Sample quality improves markedly as training compute increases.<br></p></div><!--]--><!--[--><!--]--><!--[--><div id="variable-durations-resolutions-aspect-ratios" data-heading=""><p><h2>Variable durations, resolutions, aspect ratios</h2></p></div><!--]--><!--[--><div><p>Past approaches to image and video generation typically resize, crop or trim videos to a standard size – e.g., 4 second videos at 256x256 resolution. We find that instead training on data at its native size provides several benefits.<br></p></div><!--]--><!--[--><div id="sampling-flexibility" data-heading=""><p><h3>Sampling flexibility</h3></p></div><!--]--><!--[--><div><p>Sora can sample widescreen 1920x1080p videos, vertical 1080x1920 videos and everything inbetween. This lets Sora create content for different devices directly at their native aspect ratios. It also lets us quickly prototype content at lower sizes before generating at full resolution—all with the same model.<br></p></div><!--]--><!--[--><!--]--><!--[--><div id="improved-framing-and-composition" data-heading=""><p><h3>Improved framing and composition</h3></p></div><!--]--><!--[--><div><p>We empirically find that training on videos at their native aspect ratios improves composition and framing. We compare Sora against a version of our model that crops all training videos to be square, which is common practice when training generative models. The model&nbsp; trained on square crops (left) sometimes generates videos where the subject is only partially in view. In comparison, videos from Sora (right)s have improved framing.<br></p></div><!--]--><!--[--><!--]--><!--[--><div id="language-understanding" data-heading=""><p><h2>Language understanding</h2></p></div><!--]--><!--[--><div><p>Training text-to-video generation systems requires a large amount of videos with corresponding text captions. We apply the re-captioning technique introduced in DALL·E 3<span><sup><span>[^30]</span></sup><!----></span> to videos. We first train a highly descriptive captioner model and then use it to produce text captions for all videos in our training set. We find that training on highly descriptive video captions improves text fidelity as well as the overall quality of videos.</p><p>Similar to DALL·E 3, we also leverage GPT to turn short user prompts into longer detailed captions that are sent to the video model. This enables Sora to generate high quality videos that accurately follow user prompts.<br></p></div><!--]--><!--[--><div layout="full-bleed" id="SoraMadlib-25"><p> taking a pleasant stroll in </p></div><!--]--><!--[--><div id="prompting-with-images-and-videos" data-heading=""><p><h2>Prompting with images and videos</h2></p></div><!--]--><!--[--><div><p>All of the results above and in our <a href="https://openai.com/sora" rel="noopener noreferrer">landing page</a> show text-to-video samples. But Sora can also be prompted with other inputs, such as pre-existing images or video. This capability enables Sora to perform a wide range of image and video editing tasks—creating perfectly looping video, animating static images, extending videos forwards or backwards in time, etc.<br></p></div><!--]--><!--[--><div id="animating-dall-e-images" data-heading=""><p><h3>Animating DALL·E images</h3></p></div><!--]--><!--[--><div><p>Sora is capable of generating videos provided an image and prompt as input. Below we show example videos generated based on DALL·E 2<span><sup><span>[^31]</span></sup><!----></span> and DALL·E 3<span><sup><span>[^30]</span></sup><!----></span> images.<br></p></div><!--]--><!--[--><div layout="auto" id="SoraVideoGrid-30"><!--[--><div><p><img loading="lazy" src="https://cdn.openai.com/tmp/s/prompting_0.png"></p></div><!--]--><p>A Shiba Inu dog wearing a beret and black turtleneck.</p></div><!--]--><!--[--><div layout="auto" id="SoraVideoGrid-31"><!--[--><div><p><img loading="lazy" src="https://cdn.openai.com/tmp/s/prompting_2.png"></p></div><!--]--><p>Monster Illustration in flat design style of a diverse family of monsters. The group includes a furry brown monster, a sleek black monster with antennas, a spotted green monster, and a tiny polka-dotted monster, all interacting in a playful environment.</p></div><!--]--><!--[--><div layout="auto" id="SoraVideoGrid-32"><!--[--><div><p><img loading="lazy" src="https://cdn.openai.com/tmp/s/prompting_4.png"></p></div><!--]--><p>An image of a realistic cloud that spells “SORA”.</p></div><!--]--><!--[--><div layout="auto" id="SoraVideoGrid-33"><!--[--><div><p><img loading="lazy" src="https://cdn.openai.com/tmp/s/prompting_6.png"></p></div><!--]--><p>In an ornate, historical hall, a massive tidal wave peaks and begins to crash. Two surfers, seizing the moment, skillfully navigate the face of the wave.</p></div><!--]--><!--[--><div id="extending-generated-videos" data-heading=""><p><h3>Extending generated videos</h3></p></div><!--]--><!--[--><div><p>Sora is also capable of extending videos, either forward or backward in time. Below are four videos that were all extended backward in time starting from a segment of a generated video. As a result, each of the four videos starts different from the others, yet all four videos lead to the same ending.<br></p></div><!--]--><!--[--><!--]--><!--[--><div><p>We can use this method to extend a video both forward and backward to produce a seamless infinite loop.<br></p></div><!--]--><!--[--><!--]--><!--[--><div id="video-to-video-editing" data-heading=""><p><h3>Video-to-video editing</h3></p></div><!--]--><!--[--><div><p>Diffusion models have enabled a plethora of methods for editing images and videos from text prompts. Below we apply one of these methods, SDEdit,<span><sup><span>[^32]</span></sup><!----></span> to Sora. This technique enables Sora to transform&nbsp; the styles and environments of input videos zero-shot.<br></p></div><!--]--><!--[--><!--]--><!--[--><div id="connecting-videos" data-heading=""><p><h3>Connecting videos</h3></p></div><!--]--><!--[--><div><p>We can also use Sora to gradually interpolate between two input videos, creating seamless transitions between videos with entirely different subjects and scene compositions. In the examples below, the videos in the center interpolate between the corresponding videos on the left and right.<br></p></div><!--]--><!--[--><!--]--><!--[--><div id="image-generation-capabilities" data-heading=""><p><h2>Image generation capabilities</h2></p></div><!--]--><!--[--><div><p>Sora is also capable of generating images. We do this by arranging patches of Gaussian noise in a spatial grid with a temporal extent of one frame. The model can generate images of variable sizes—up to 2048x2048 resolution.<br></p></div><!--]--><!--[--><div layout="auto" id="SoraVideoGrid-47"><!--[--><div><p><img loading="lazy" src="https://cdn.openai.com/tmp/s/image_0.png"><span>Close-up portrait shot of a woman in autumn, extreme detail, shallow depth of field</span></p></div><div><p><img loading="lazy" src="https://cdn.openai.com/tmp/s/image_1.png"><span>Vibrant coral reef teeming with colorful fish and sea creatures</span></p></div><div><p><img loading="lazy" src="https://cdn.openai.com/tmp/s/image_2.png"><span>Digital art of a young tiger under an apple tree in a matte painting style with gorgeous details</span></p></div><div><p><img loading="lazy" src="https://cdn.openai.com/tmp/s/image_3.png"><span>A snowy mountain village with cozy cabins and a northern lights display, high detail and photorealistic dslr, 50mm f/1.2</span></p></div><!--]--><!----></div><!--]--><!--[--><div id="emerging-simulation-capabilities" data-heading=""><p><h2>Emerging simulation capabilities</h2></p></div><!--]--><!--[--><div><p>We find that video models exhibit a number of interesting emergent capabilities when trained at scale. These capabilities enable Sora to simulate some aspects of people, animals and environments from the physical world. These properties emerge without any explicit inductive biases for 3D, objects, etc.—they are purely phenomena of scale.</p><p><strong>3D consistency.</strong> Sora can generate videos with dynamic camera motion. As the camera shifts and rotates, people and scene elements move consistently through three-dimensional space.<br></p></div><!--]--><!--[--><!--]--><!--[--><div><p><strong>Long-range coherence and object permanence. </strong>A significant challenge for video generation systems has been maintaining temporal consistency when sampling long videos. We find that Sora is often, though not always, able to effectively model both short- and long-range dependencies. For example, our model can persist people, animals and objects even when they are occluded or leave the frame. Likewise, it can generate multiple shots of the same character in a single sample, maintaining their appearance throughout the video.<br></p></div><!--]--><!--[--><!--]--><!--[--><div><p><strong>Interacting with the world.</strong> Sora can sometimes simulate actions that affect the state of the world in simple ways. For example, a painter can leave new strokes along a canvas that persist over time, or a man can eat a burger and leave bite marks.<br></p></div><!--]--><!--[--><!--]--><!--[--><div><p><strong>Simulating digital worlds.</strong> Sora is also able to simulate artificial processes–one example is video games. Sora can simultaneously control the player in Minecraft with a basic policy while also rendering the world and its dynamics in high fidelity. These capabilities can be elicited zero-shot by prompting Sora with captions mentioning “Minecraft.”<br></p></div><!--]--><!--[--><!--]--><!--[--><div><p>These capabilities suggest that continued scaling of video models is a promising path towards the development of highly-capable simulators of the physical and digital world, and the objects, animals and people that live within them.<br></p></div><!--]--><!--[--><div id="discussion" data-heading=""><p><h2>Discussion</h2></p></div><!--]--><!--[--><!--]--><!--[--><div><p>Sora currently exhibits numerous limitations as a simulator. For example, it does not accurately model the physics of many basic interactions, like glass shattering. Other interactions, like eating food, do not always yield correct changes in object state. We enumerate other common failure modes of the model—such as incoherencies that develop in long duration samples or spontaneous appearances of objects—in our <a href="https://openai.com/sora" rel="noopener noreferrer">landing page</a>.<br></p></div><!--]--><!--[--><!--]--><!--[--><div><p>We believe the capabilities Sora has today demonstrate that continued scaling of video models is a promising path towards the development of capable simulators of the physical and digital world, and the objects, animals and people that live within them.<br></p></div><!--]--><!--]--></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[It Was 33 Years Ago Today: Happy Birthday Lemmings (416 pts)]]></title>
            <link>https://scottishgames.net/2024/02/14/it-was-33-years-ago-today-happy-birthday-lemmings/</link>
            <guid>39390965</guid>
            <pubDate>Thu, 15 Feb 2024 23:48:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://scottishgames.net/2024/02/14/it-was-33-years-ago-today-happy-birthday-lemmings/">https://scottishgames.net/2024/02/14/it-was-33-years-ago-today-happy-birthday-lemmings/</a>, See on <a href="https://news.ycombinator.com/item?id=39390965">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-37461">
		<!-- .entry-header -->

	
	<div>
		
<p>Today, February 14th, 2024, marks the 33rd anniversary of <em><a href="https://en.wikipedia.org/wiki/Lemmings_(video_game)">Lemmings</a></em>, the game that transcended mere entertainment to become a cultural icon and a catalyst for Scotland’s thriving game development industry. But before the green-haired hordes invaded screens worldwide, let’s rewind to 1991 and trace its remarkable journey.</p>



<p>Born from the minds of DMA Design (now of course Rockstar North), a small Dundee studio, Lemmings was a revolutionary concept. Instead of blowing things you, you were tasked with saving the plummeting rodents’ lives. </p>



<p>However, DMA’s genius lay in its execution. With charming character design (at an astonishingly small scale), addictive puzzle mechanics, and more than a touch of what would become DMA’s slapstick humour, they transformed a complex concept into a game anyone could pick up and play.</p>


<div>
<figure><a href="https://i0.wp.com/scottishgames.net/wp-content/uploads/2012/11/lemmings-8bit.jpg?ssl=1"><img loading="lazy" decoding="async" width="580" height="363" data-attachment-id="4108" data-permalink="https://scottishgames.net/2012/11/06/steve-hammonds-manual-override-realit/lemmings-8bit/" data-orig-file="https://i0.wp.com/scottishgames.net/wp-content/uploads/2012/11/lemmings-8bit.jpg?fit=580%2C363&amp;ssl=1" data-orig-size="580,363" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}" data-image-title="lemmings 8bit" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/scottishgames.net/wp-content/uploads/2012/11/lemmings-8bit.jpg?fit=300%2C188&amp;ssl=1" data-large-file="https://i0.wp.com/scottishgames.net/wp-content/uploads/2012/11/lemmings-8bit.jpg?fit=580%2C363&amp;ssl=1" src="https://i0.wp.com/scottishgames.net/wp-content/uploads/2012/11/lemmings-8bit.jpg?resize=580%2C363&amp;ssl=1" alt="Lemming. 8 Bit." data-recalc-dims="1"></a></figure></div>


<p><em>Lemmings</em> offered a simple premise: guide a predetermined number of lemmings to an exit by assigning them roles like blocker, climber, builder, and floater. But simplicity masks depth. Each level presented a unique challenge, requiring strategic thinking, quick reflexes, and a touch of trial-and-error gory death.</p>



<p>Success was immediate. <em>Lemmings</em> conquered consoles and computers, selling over 15 million copies and becoming the UK’s best-selling game of 1991. Awards and accolades rained down, but perhaps the most significant impact was on Scottish gaming itself. Lemmings put DMA Design on the map, attracting talent and investment and inspiring the world’s first games degree.</p>



<p>The Scottish Games Network spoke to several of the original team members to ask for their thoughts on the impact of the game:</p>



<h4>Mike Dailly, the creator of the original animation of tiny things being splattered, said:</h4>



<blockquote>
<p>I’m constantly amazed at the legacy of Lemmings. Where ever I go, there are fans, old and new who love the game. With the style, and accessibility of it, it not only entertained, but brought families closer together as kids played with their non-game playing parents and grandparents. I get people getting in touch all the time telling me of their happy memories of playing it with their relatives who never had an interest in games before, and being able to share their hobby with them, meant the world to them.</p>



<p>Even now at shows, some 33 years later, you’ll still see the odd person dressed up as a Lemming and expressing love for the game, the music, the sound effects, the characters – or how they were useless at it, but loved to just nuke them!</p>



<p>Lemmings is still the game I’m most proud to have been a part of, in a world of first person shooters, it’s as popular now as it ever was with young and old alike</p>
</blockquote>



<h4>Russell Kay, told us his dream is to bring the games to a new generation:</h4>



<blockquote>
<p>33 years ago we released a game that is still loved today that is very gratifying and I don’t think any of us would have believed you if we were told at the time. Over the years we have fallen in and out of love with the franchise but it holds a special place in our hearts, personally I would love to be able to update the characters and franchise but Sony hold onto the rights jealously, it would be fantastic to get a chance to see what the Lemmings would make of the modern gaming world!</p>
</blockquote>



<p>Lemmings’ influence resonated far beyond Scotland. It can be said to have popularised puzzle games, inspiring titles like <em>The Incredible Machine</em>. Its emphasis on physics and user-generated content laid the groundwork for future sandbox games (possibly even <em>Minecraft</em>…?)</p>



<p>Moreover, its humour and memorable characters solidified DMA Design’s reputation for innovative, surprising and engaging gameplay, paving the way for future classics like <em>Grand Theft Auto</em>, the often overlooked (and far more bonkers) <em>Tanktics</em>, cult-classic <em>Body Harvest</em> and the underrated <em>Wild Metal Country</em>.</p>



<p>Today, Lemmings remains a beloved puzzle classic, enjoying re-releases on various platforms and inspiring new generations of designers. But its legacy extends far beyond nostalgic pixelated memories. Dundee’s city centre plays host to a beloved <a href="https://scottishgames.net/2013/10/14/lets-go-lemmings-in-the-real-world/">series of statues of Lemmings</a>, hard at work, climbing and bridging a garden gateway overlooking the river Tay.</p>


<div>
<figure><a href="https://i0.wp.com/scottishgames.net/wp-content/uploads/2024/02/Screenshot-2024-02-14-at-14.08.29.png?ssl=1"><img loading="lazy" decoding="async" width="740" height="448" data-attachment-id="37472" data-permalink="https://scottishgames.net/2024/02/14/it-was-33-years-ago-today-happy-birthday-lemmings/screenshot-2024-02-14-at-14-08-29/" data-orig-file="https://i0.wp.com/scottishgames.net/wp-content/uploads/2024/02/Screenshot-2024-02-14-at-14.08.29.png?fit=1398%2C846&amp;ssl=1" data-orig-size="1398,846" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot 2024-02-14 at 14.08.29" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/scottishgames.net/wp-content/uploads/2024/02/Screenshot-2024-02-14-at-14.08.29.png?fit=300%2C182&amp;ssl=1" data-large-file="https://i0.wp.com/scottishgames.net/wp-content/uploads/2024/02/Screenshot-2024-02-14-at-14.08.29.png?fit=740%2C448&amp;ssl=1" src="https://i0.wp.com/scottishgames.net/wp-content/uploads/2024/02/Screenshot-2024-02-14-at-14.08.29.png?resize=740%2C448&amp;ssl=1" alt="Lemmings statue, Perth Road, Dundee" srcset="https://i0.wp.com/scottishgames.net/wp-content/uploads/2024/02/Screenshot-2024-02-14-at-14.08.29.png?resize=1024%2C620&amp;ssl=1 1024w, https://i0.wp.com/scottishgames.net/wp-content/uploads/2024/02/Screenshot-2024-02-14-at-14.08.29.png?resize=300%2C182&amp;ssl=1 300w, https://i0.wp.com/scottishgames.net/wp-content/uploads/2024/02/Screenshot-2024-02-14-at-14.08.29.png?resize=768%2C465&amp;ssl=1 768w, https://i0.wp.com/scottishgames.net/wp-content/uploads/2024/02/Screenshot-2024-02-14-at-14.08.29.png?resize=1200%2C726&amp;ssl=1 1200w, https://i0.wp.com/scottishgames.net/wp-content/uploads/2024/02/Screenshot-2024-02-14-at-14.08.29.png?resize=500%2C303&amp;ssl=1 500w, https://i0.wp.com/scottishgames.net/wp-content/uploads/2024/02/Screenshot-2024-02-14-at-14.08.29.png?w=1398&amp;ssl=1 1398w" sizes="(max-width: 706px) 89vw, (max-width: 767px) 82vw, 740px" data-recalc-dims="1"></a></figure></div>


<p>On the 20th anniversary in 2011 <a href="https://scottishgames.net/2021/02/15/happy-birthday-lemmings-30-today/">a plaque was unveiled</a> at the bottom of Perth Road in the city, commemorating DMA’s first office, where the game was originally born.</p>


<div>
<figure><a href="https://i0.wp.com/scottishgames.net/wp-content/uploads/2011/02/cropped-bb-lemmings.jpg?ssl=1"><img loading="lazy" decoding="async" width="700" height="200" data-attachment-id="881" data-permalink="https://scottishgames.net/cropped-bb-lemmings-jpg/" data-orig-file="https://i0.wp.com/scottishgames.net/wp-content/uploads/2011/02/cropped-bb-lemmings.jpg?fit=700%2C200&amp;ssl=1" data-orig-size="700,200" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}" data-image-title="cropped-bb-lemmings.jpg" data-image-description="<p>http://scottishgames.net/wp-content/uploads/2011/02/cropped-bb-lemmings.jpg</p>
" data-image-caption="" data-medium-file="https://i0.wp.com/scottishgames.net/wp-content/uploads/2011/02/cropped-bb-lemmings.jpg?fit=300%2C86&amp;ssl=1" data-large-file="https://i0.wp.com/scottishgames.net/wp-content/uploads/2011/02/cropped-bb-lemmings.jpg?fit=700%2C200&amp;ssl=1" src="https://i0.wp.com/scottishgames.net/wp-content/uploads/2011/02/cropped-bb-lemmings.jpg?resize=700%2C200&amp;ssl=1" alt="" data-recalc-dims="1"></a></figure></div>


<p>In 2022 <em><a href="https://scottishgames.net/2022/02/15/lemmings-can-you-dig-it-youtube/" target="_blank" rel="noreferrer noopener">Lemmings: Can You Dig It?</a></em> a feature-length documentary was released, which charted the design and development of the original game and its impact upon gamers today. You can watch it here:</p>



<figure><p><span><iframe loading="lazy" width="740" height="417" src="https://www.youtube.com/embed/RbAVNKdk9gA?version=3&amp;rel=1&amp;showsearch=0&amp;showinfo=1&amp;iv_load_policy=1&amp;fs=1&amp;hl=en-GB&amp;autohide=2&amp;wmode=transparent" allowfullscreen="true" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox"></iframe></span>
</p></figure>



<p>Happy birthday <em>Lemmings</em>!</p>
		
		
		
	</div><!-- .entry-content -->

	 <!-- .entry-footer -->
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Galactic Compass – an app that points to the galactic center (152 pts)]]></title>
            <link>https://interconnected.org/home/2024/02/15/galactic-compass</link>
            <guid>39389858</guid>
            <pubDate>Thu, 15 Feb 2024 22:12:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://interconnected.org/home/2024/02/15/galactic-compass">https://interconnected.org/home/2024/02/15/galactic-compass</a>, See on <a href="https://news.ycombinator.com/item?id=39389858">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="social-select-root" data-highlights="">
  <p>Hey I made an app! It’s a green floating arrow that always points to the middle of the Milky Way.</p>
<p>i.e. 26,000 light years towards the supermassive central black hole, Sagittarius A*.</p>
<p>You can have it too!</p>
<p><strong><a href="https://apps.apple.com/gb/app/galactic-compass/id6451314440">Download Galactic Compass from the App Store.</a></strong></p>
<p>BUT: I don’t know how to write apps.</p>
<p>And yet here we are!</p>
<p>Let me explain.</p>
<p><img alt="" src="https://interconnected.org/home/static/content/2024/02/15/galactic-compass.jpg"></p>
<h3>Cultivating a sense of the galactic centre</h3>
<p>It’s remarkably grounding?</p>
<p>Once upon a time I trained myself to always know where to look, and the centre of the galaxy moves of course over the day and the year: <q>So I would end up pointing through the pavement, or down a street, and thinking, huh, that’s where it is.</q></p>
<p>It is a worthwhile super-sense:</p>
<blockquote>
<p>Eventually then I had this picture of myself, and the Earth, and the solar system, and the centre of the galaxy which had initially been whirling round me, and now it had flipped, <u>I was turning around it.</u></p>
<p>It was wildly situating.</p>
</blockquote>
<p>I’ve lost the intuition now, sadly.</p>
<p>The above description is from <a href="https://interconnected.org/home/2021/06/30/galaxy">my 2021 writeup</a> which I conclude by saying:</p>
<blockquote>
<p>In my imagination I see an iPhone app which displays a 3D model, connected to the gyroscope and the compass and the GPS. …</p>
<p><u>But there are slightly too many things I would need to learn</u></p>
</blockquote>
<p>So I couldn’t built it.</p>
<p>EXCEPT.</p>
<p><em>Now there is ChatGPT.</em></p>
<hr>
<h3>Developing an app with ChatGPT</h3>
<p>I can’t write Swift (the language used to code iOS apps).</p>
<p>But what I am able to do is break up large problems into smaller, expressible problems, and then sequence them.</p>
<p><strong>I’ll be detailed about this.</strong> When I’ve walking folks through this, they’re often interested so it is (perhaps) non-obvious?</p>
<p><em>If you’re not interested in the detail, skip to the next section.</em></p>
<p>I started by installing Xcode and setting up a git repo. I know how to do that. (GitHub Copilot doesn’t work in Xcode by the way.)</p>
<p>To get going, I said to ChatGPT 4 something like:</p>
<ul>
<li>I’m building an iPhone app using SwiftUI. I have installed Xcode version X. Please walk me through creating a new iOS app with a single screen. The screen should be blank except for a line of text in the middle that says “Hello, World!”</li>
</ul>
<p>Then I followed the instructions.</p>
<p>There was lots of interaction like: <em>okay I’ve done step 1. I’m on step 2 but I can’t see the X, or I have the error Y, what should I do?</em></p>
<p>I know, from other coding, that I want to have my build working as early as possible.</p>
<p>My next question to ChatGPT was something like:</p>
<ul>
<li>Now I want to see my development app running on my phone as I work. Please walk me through that.</li>
</ul>
<p>Ok, now I’ve got a setup which means I can develop and I can test.</p>
<p>Now putting together the app itself is <em>not</em> about describing the overall app. I don’t want ChatGPT to be overfaced.</p>
<p>I worked in steps at this kind of resolution, making sure each step was complete before moving to the next:</p>
<ul>
<li>Okay now add two tabs at the bottom. The tabs are called Compass and Debug. Each has an icon. The first tab show should the Hello World screen, and the second tab should have the word “Debug” in the middle</li>
<li>We’ll work on the Debug screen. Add a section of text rows that simply say A, B, and C. Use standard iOS components. Ok, now add a label at the top. Make the text smaller. Make it capitalised.</li>
<li>Add two rows, latitude and longitude, based on the device location. Add the device heading.</li>
<li>Track the device motion and add rows for pitch, roll, and yaw.</li>
</ul>
<p>Then I found a Swift-compatible library to translate between galactic coordinates and relative coordinates. (Ultimately I need altitude and azimuth, a way of pointing at a position in the sky, based on the current time and location.) I’m using <a href="https://github.com/onekiloparsec/SwiftAA">SwiftAA</a>.</p>
<ul>
<li>I’m using SwiftAA. Please make a new Swift object that takes the current date and device location, and provides the azimuth and altitude of the galactic centre (I looked up the coordinates of the central black hole as a proxy)</li>
<li>Using the new GalacticCenter object, display azimuth and altitude in a new section on the debug screen.</li>
</ul>
<p>I retained the Debug tab in the shipped app so you can see.</p>
<p>So that’s all the astronomical stuff done.</p>
<p>You never want to give ChatGPT big goals where it has to figure out the way on its own. Then both of you will be confused. Intermediate stepping stones and being sure of your boots with each stride, that’s the way.</p>
<p>Now we build the rotating arrow:</p>
<ul>
<li>Ok now we’re on the Compass screen. Make a SceneKit view with a cube in the middle over the whole screen</li>
<li><em>(There was a whole lot of back and forth here to fix scrolling issues, ensuring the tabs were tappable, positioning some text over the bottom, and so on.)</em></li>
<li>Now let’s make a green arrow from an extruded rectangle and squashed pyramid. The arrow should point to the top of the screen</li>
<li>Break out the data from the Debug screen into a separate object so both tabs can use it</li>
<li>Assuming the phone is lying flat. Make the arrow point north</li>
<li>Rotate the arrow in 3D in real-time in response to the device orientation so that it always points north</li>
<li>Instead of pointing north, point the arrow at the altitude and azimuth of the galactic centre</li>
</ul>
<p>This now became pretty tricky because I had to learn about how to combine rotations. I barely know anything about quaternions, so there was a bunch to learn here.</p>
<p>ChatGPT, being a large language model but lacking embodiment, is awful at 3D maths and reference frames.</p>
<p>Finally I…</p>
<ul>
<li>Asked ChatGPT to walk me through the process of building the app using Xcode Cloud and distributing it on TestFlight</li>
<li>Shared the test app with friends to ask for their help with rotations.</li>
</ul>
<p>Galactic Compass is still pretty janky, to be sure.</p>
<p>But it ain’t bad for a collaboration between someone who can’t build apps and an AI that is barely a year old.</p>
<hr>
<h3>“An app can be a home-cooked meal”</h3>
<p>Ethan Mollick and a team of social scientists studied a group of management consultants using AI.</p>
<p><a href="https://www.oneusefulthing.org/p/centaurs-and-cyborgs-on-the-jagged">The headline is that, yes, AI results in better work.</a></p>
<p>The fascinating buried result is that the biggest effect is felt by the <em>bottom-half skilled participants.</em></p>
<p>i.e. if you’re sub-skilled then you can use AI to drag you up to median.</p>
<p>Now, none of us have just one skill. Like most people, I have a mix.</p>
<p>But now I’m a reasonable engineer, an amateur designer, an ok systems thinker, ok at having ideas, and now a midwit <em>everything</em> when it comes to all the actual skilled tasks.</p>
<p>And the combination means I can bring ideas to life that simply wouldn’t be possible if I had to persuade a designer or engineer buddy to help me out. Being able to bring ideas to life means I can scaffold up to other ideas… and others…</p>
<p>Like this galactic compass.</p>
<p>Back in 2020, Robin Sloan said that <a href="https://www.robinsloan.com/notes/home-cooked-app/">an app can be a home-cooked meal</a>. It’s such a memorable perspective, and what we should aspire to from our software.</p>
<p>Now I’ve cooked a meal that anyone with an iPhone can download. Probably only a couple dozen people will want it, but I want it in my pocket, and I want to share it with my friends, and here we are.</p>
<p>And I can’t even cook!</p>
<p>But I know where the centre of the galaxy is, even so.</p>
<hr>
<p>Galactic Compass links:</p>
<p><a href="https://apps.apple.com/gb/app/galactic-compass/id6451314440">Download from the App Store.</a></p>
<p><a href="https://www.actsnotfacts.com/made/galactic-compass">Project page on Acts Not Facts.</a></p>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Build your own 3D printed Hexapod (117 pts)]]></title>
            <link>https://github.com/MakeYourPet/hexapod</link>
            <guid>39388269</guid>
            <pubDate>Thu, 15 Feb 2024 20:25:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/MakeYourPet/hexapod">https://github.com/MakeYourPet/hexapod</a>, See on <a href="https://news.ycombinator.com/item?id=39388269">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/MakeYourPet/hexapod/blob/main/Illustrations/yellow2.png"><img src="https://github.com/MakeYourPet/hexapod/raw/main/Illustrations/yellow2.png" height="300"></a></p>
<h2 tabindex="-1" dir="auto">Some quick links to get you started</h2>
<ul dir="auto">
<li>Watch the step by step build videos on my <a href="https://www.youtube.com/makeyourpet" rel="nofollow">YouTube channel</a>.<br></li>
<li>For build questions and to connect with the community join my <a href="https://discord.gg/vb8YWMfBuk" rel="nofollow">Discord server</a>.<br></li>
<li>A <a href="https://github.com/MakeYourPet/hexapod/blob/main/wiring-diagram-servo2040.png">wiring diagram</a> that you may find useful.<br></li>
<li>A fan-made <a href="https://docs.google.com/spreadsheets/d/1jLi3IdmLERsBDhjaqHxFGQgZul_3uq9oj55M1rFG8mY/edit#gid=0" rel="nofollow">parts list</a>. Also another fan-made <a href="https://docs.google.com/spreadsheets/d/1y--z7EeejWcb-8ooPaIFn3Hulu9dJOcoKyGoxGq8KI8/edit?usp=drivesdk" rel="nofollow">parts list</a>. And here is a <a href="https://github.com/LonelyGhost6/Public/blob/main/part-list.pdf">third one</a>. These are not meant to be a complete list of EVERYTHING that you need, but they cover most of the important and pricier stuff.<br></li>
<li>If you decide to use the Servo2040 board (highly recommended), find the <a href="https://github.com/EddieCarrera/chica-servo2040-simpleDriver/releases/download/v0.0.1/chica-servo2040_release.uf2">firmware</a> and the <a href="https://github.com/EddieCarrera/chica-servo2040-simpleDriver#loading-the-firmware-image">instructions on how to flash it</a> from Eddie's repository.<br></li>
</ul>
<h2 tabindex="-1" dir="auto">Illustrations</h2>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/MakeYourPet/hexapod/blob/main/Illustrations/front-view.png"><img src="https://github.com/MakeYourPet/hexapod/raw/main/Illustrations/front-view.png" height="200"></a>
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/MakeYourPet/hexapod/blob/main/Illustrations/back-view.png"><img src="https://github.com/MakeYourPet/hexapod/raw/main/Illustrations/back-view.png" height="200"></a>
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/MakeYourPet/hexapod/blob/main/Illustrations/leg-components.png"><img src="https://github.com/MakeYourPet/hexapod/raw/main/Illustrations/leg-components.png" height="200"></a>
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/MakeYourPet/hexapod/blob/main/Illustrations/tibia-components.png"><img src="https://github.com/MakeYourPet/hexapod/raw/main/Illustrations/tibia-components.png" height="200"></a>
</p>
<h2 tabindex="-1" dir="auto">About the STL files in this repository</h2>
<p dir="auto">Some of the parts have multiple versions with slight differences. But all of them are compatible and should work.<br>
Make sure to check all of them to pick the one that works for you before committing to the print.<br>
I use the latest version of each part in my own hexapod, which is the one with the higher number at the end of its name.</p>
<h2 tabindex="-1" dir="auto">Compatible fan-made 3D printable parts</h2>
<p dir="auto">Check out <a href="https://github.com/almelnz2005/hexapod">this</a> repository which contains modified (but compatible) versions of the original parts which uses metal horns, M3 screws and seperated components to make the parts more 3D printer friendly. This is especially helpful if you are printing with material other than PLA and have issues with supports or shrinking.</p>
<h2 tabindex="-1" dir="auto">Wiring Diagram</h2>
<ul dir="auto">
<li>Pimoroni Servo2040 (The newer, simpler, cheaper and recommended option):<br></li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/MakeYourPet/hexapod/blob/main/wiring-diagram-servo2040.png"><img src="https://github.com/MakeYourPet/hexapod/raw/main/wiring-diagram-servo2040.png" height="300"></a></p>
<ul dir="auto">
<li>Pololu Maestro (The original, complicated, expensive and legacy option):<br></li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/MakeYourPet/hexapod/blob/main/wiring-diagram-pololu.png"><img src="https://github.com/MakeYourPet/hexapod/raw/main/wiring-diagram-pololu.png" height="300"></a></p>
<h2 tabindex="-1" dir="auto">Electronic Component Layout</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/MakeYourPet/hexapod/blob/main/component-layout.jpg"><img src="https://github.com/MakeYourPet/hexapod/raw/main/component-layout.jpg" height="400"></a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple confirms it's breaking iPhone web apps in the EU on purpose (773 pts)]]></title>
            <link>https://techcrunch.com/2024/02/15/apple-confirms-its-breaking-iphone-web-apps-in-the-eu-on-purpose/</link>
            <guid>39388218</guid>
            <pubDate>Thu, 15 Feb 2024 20:22:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2024/02/15/apple-confirms-its-breaking-iphone-web-apps-in-the-eu-on-purpose/">https://techcrunch.com/2024/02/15/apple-confirms-its-breaking-iphone-web-apps-in-the-eu-on-purpose/</a>, See on <a href="https://news.ycombinator.com/item?id=39388218">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<p id="speakable-summary">Well, it turns out it’s not a bug that broke iPhone web apps, also known as Progressive Web Apps (PWAs), in the EU. Following developer complaints and press reports about how PWAs were no longer functional in the EU after installing the most recent iOS betas, Apple has updated its website to explain why. No surprise, the tech giant is blaming the new EU regulation, the Digital Markets Act, for the change, saying that the complexities involved with the DMA’s requirement to allow different browser engines is the root cause.</p>
<p>To catch you up, security researcher <a href="https://x.com/mysk_co/status/1753401847044288847?s=20">Tommy Mysk</a> and <a href="https://open-web-advocacy.org/blog/did-apple-just-break-web-apps-in-ios17.4-beta-eu/">Open Web Advocacy</a>, first noticed that PWAs <a href="https://www.macrumors.com/2024/02/08/ios-17-4-nerfs-web-apps-in-the-eu">had been demoted</a> to website shortcuts with the release of the second beta of iOS 17.4. Initially, it was unclear if this was a beta bug — stranger things have happened — or it was intended to undermine the functionality of PWAs in the E.U., a market where Apple is now being forced to allow alternative app stores, third-party payments, and alternative browser engines, among other things. In the betas, PWAs, which typically allow web apps to function and feel more like native iOS apps, were no longer working.&nbsp;Developers noticed that these web apps would open like a bookmark saved to your Home Screen, instead.</p>
<p>As <a href="https://www.macrumors.com/2024/02/08/ios-17-4-nerfs-web-apps-in-the-eu">MacRumors</a> pointed out at the time, that meant no “dedicated windowing, notifications, or long-term local storage.” iOS <a href="https://9to5mac.com/2023/02/16/iphone-web-app-new-features-ios-16-4/">16.4 also allowed PWAs to badge their icons</a> with notifications, as native apps could. iOS 17.4 beta users reported that when they opened a web app while running the iOS beta, the system would ask them if they wanted to open the app in Safari or cancel. The message indicates that the web app will “open in your default browser from now on,” it said. Afterward, users said they experienced issues with data loss, as a Safari website shortcut doesn’t offer local storage. Notifications also no longer worked.</p>
<p>Still, there was reason to be cautious about whether or not the change was intentional. Multiple staff at TechCrunch repeatedly asked Apple for comment but received no reply. (We had wanted to know if the comapny would confirm if this was a beta bug or an intentional change, and if the latter, what Apple’s reasoning for it was.) After the next beta release emerged, <a href="https://www.theverge.com/2024/2/14/24072764/apple-progressive-web-apps-eu-ios-17-4">The Verge</a>&nbsp;ran a report indicating that Apple <em>“appears to be”</em> breaking PWAs in the E.U., after also not likely getting a formal response from the tech giant.</p>
<p>Now, Apple has responded, in its way. Today, it updated its <a href="https://developer.apple.com/support/dma-and-apps-in-the-eu/">website detailing its DMA-related changes in the EU</a> to address the matter. In a new update, the company explains how it’s had to make so many changes to iOS to comply with the EU guidelines, that continued support for PWAs was simply off the table.</p>
<p>Traditionally, the iOS system provided support for Home Screen web apps by building directly on WebKit (Safari’s browser engine), and its security architecture, Apple said. That allowed web apps to align with the same security and privacy models as found in other native apps. But with the DMA, Apple is being forced to allow alternative browser engines. It argues that without the isolation and enforcement of the rules applied to WebKit-based web apps, malicious apps could be installed that could do things like read data from other web apps, or “gain access to a user’s camera, microphone or location without a user’s consent,” Apple said.</p>
<p>“Addressing the complex security and privacy concerns associated with web apps using alternative browser engines would require building an entirely new integration architecture that does not currently exist in iOS and was not practical to undertake given the other demands of the DMA and the very low user adoption of Home Screen web apps. And so, to comply with the DMA’s requirements, we had to remove the Home Screen web apps feature in the EU,” the website reads.</p>
<p>The company informs EU users they will be able to access websites from their Home Screen through bookmarks as a result of the change, confirming developers’ concerns that PWAs were effectively being disabled in the EU.</p>
<p>“We expect this change to affect a small number of users. Still, we regret any impact this change — that was made as part of the work to comply with the DMA — may have on developers of Home Screen web apps and our users,” Apple says.</p>
<p><a href="https://twitter.com/OpenWebAdvocacy/status/1757929731071373447">Critics</a> <a href="https://twitter.com/douglascamata/status/1755967835371716975">have</a> <a href="https://twitter.com/MyDaebakCafe/status/1755538810287350259">argued</a> that Apple’s desire to hold onto its power in the iOS app ecosystem was so strong that it would break web app functionality for users of its devices. Apple’s defenders, meanwhile, will probably argue that the company’s explanation is reasonable and aligns with Apple’s desire to keep iOS safe for its users. The truth, as it often is, is likely lies more in the middle.</p>
<p>Apple still has not responded to requests for comment.</p>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Solar and battery to make up 81% of new US electric-generating capacity in 2024 (164 pts)]]></title>
            <link>https://www.eia.gov/todayinenergy/detail.php?id=61424</link>
            <guid>39387862</guid>
            <pubDate>Thu, 15 Feb 2024 20:02:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.eia.gov/todayinenergy/detail.php?id=61424">https://www.eia.gov/todayinenergy/detail.php?id=61424</a>, See on <a href="https://news.ycombinator.com/item?id=39387862">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-type="inbrief">
													<p>
								In-brief analysis							</p>
											<p><span>February 15, 2024</span></p>
					
										












<p><img src="https://www.eia.gov/todayinenergy/images/2024.02.15/main.svg" alt="U.S. planned utility-scale electric-generating capacity additions"></p>
<hr>

<p>Developers and power plant owners plan to add 62.8 gigawatts (GW) of new utility-scale electric-generating capacity in 2024, according to our latest <a href="https://www.eia.gov/electricity/data/eia860m/"><em>Preliminary Monthly Electric Generator Inventory</em></a>. This addition would be 55% more added capacity than the 40.4 GW added in 2023 (the most since 2003) and points to a continued rise in industry activity. We expect solar to account for the largest share of new capacity in 2024, at 58%, followed by battery storage, at 23%.  </p>

     


<p><strong>Solar. </strong>We expect a record addition of utility-scale solar in 2024 if the scheduled 36.4 GW are added to the grid. This growth would almost double last year’s 18.4 GW increase, which was itself a record for annual utility-scale solar installation in the United States. As the effects of supply chain challenges and trade restrictions ease, solar continues to outpace capacity additions from other generating resources.</p>
  
<p>More than half of the new utility-scale solar capacity is planned for three states: Texas (35%), California (10%), and Florida (6%). Outside of these states, the Gemini solar facility in Nevada plans to begin operating in 2024.  With a planned photovoltaic capacity of 690 megawatts (MW) and battery storage of 380 MW, it is <a href="https://www.usgs.gov/centers/southwest-biological-science-center/science/gemini-solar-project">expected to be the largest solar project</a> in the United States when fully operational.  </p>

<p><strong>Battery storage. </strong>We also expect battery storage to set a record for annual capacity additions in 2024. We expect <a href="https://www.eia.gov/todayinenergy/detail.php?id=61202">U.S. battery storage capacity to nearly double in 2024</a> as developers report plans to add 14.3 GW of battery storage to the existing 15.5 GW this year. In 2023, 6.4 GW of new battery storage capacity was added to the U.S. grid, a 70% annual increase.</p>

<p>Texas, with an expected 6.4 GW, and California, with an expected 5.2 GW, will account for 82% of the new U.S. battery storage capacity. Developers have scheduled the Menifee Power Bank (460.0 MW) at the site of the former Inland Empire Energy Center natural gas-fired power plant in Riverside, California, to come on line in 2024. With the rise of solar and wind capacity in the United States, the demand for battery storage continues to increase. The Inflation Reduction Act (IRA) has also accelerated the development of energy storage by introducing investment tax credits (ITCs) for stand-alone storage. Prior to the IRA, batteries qualified for federal tax credits only if they were co-located with solar.  </p>

<p><strong>Wind. </strong>Operators report another 8.2 GW of wind capacity is scheduled to come on line in 2024. Following the record additions of more than 14.0 GW in both 2020 and 2021, wind capacity additions have slowed in the last two years.  </p>

<p>Two large offshore wind plants scheduled to come on line this year are the 800-MW Vineyard Wind 1 off the coast of Massachusetts and the 130-MW South Fork Wind off the coast of New York. South Fork Wind, which developers expected to begin commercial operation last year, is now scheduled to come on line in March 2024. </p>

<p><strong>Natural gas. </strong>For 2024, developers report 2.5 GW in planned natural gas capacity additions, the least new natural gas capacity in 25 years. Notably, in 2024, 79% of the natural gas capacity added is to come from simple-cycle, natural gas turbine (SCGT) plants. This year will be the first time since 2001 that combined-cycle capacity was not the predominant natural gas-fired technology. <a href="https://www.eia.gov/todayinenergy/detail.php?id=55680">SCGT power plants provide effective grid support</a> because they can start up, ramp up, and ramp down relatively quickly.  </p>

<p><strong>Nuclear. </strong>Start-up of the fourth reactor (1.1 GW) at Georgia’s Vogtle nuclear power plant, originally scheduled for last year, has moved to March 2024. Vogtle Unit 3 began commercial operation at the end of July last year.</p>

<p><img src="https://www.eia.gov/todayinenergy/images/2024.02.15/chart2.svg" alt="planned 2023 U.S. utility-scale electric generator additions"></p>
<hr>

<p><strong>Principal contributor: </strong>Suparna Ray<br><strong>Data visualization: </strong>Suparna Ray, Kristen Tsai</p>






										
										
				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Building an LLM from Scratch: Automatic Differentiation (311 pts)]]></title>
            <link>https://bclarkson-code.github.io/posts/llm-from-scratch-scalar-autograd/post.html</link>
            <guid>39387850</guid>
            <pubDate>Thu, 15 Feb 2024 20:01:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bclarkson-code.github.io/posts/llm-from-scratch-scalar-autograd/post.html">https://bclarkson-code.github.io/posts/llm-from-scratch-scalar-autograd/post.html</a>, See on <a href="https://news.ycombinator.com/item?id=39387850">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="quarto-content">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main id="quarto-document-content">





<div id="faa21911" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="1">
<details>
<summary>Setup</summary>
<div id="cb1"><pre><code><span id="cb1-1"><span>from</span> typing <span>import</span> Any, Optional, List</span>
<span id="cb1-2"></span>
<span id="cb1-3"><span>import</span> networkx <span>as</span> nx</span></code></pre></div>
</details>
</div>
<section id="llm-from-scratch-automatic-differentiation">
<h2 data-anchor-id="llm-from-scratch-automatic-differentiation">LLM from scratch: Automatic Differentiation</h2>
<p>I’m building a modern language model with all the bells and whistles completely from scratch: from vanilla python to functional coding assistant. Borrowing (shamelessly stealing) from computer games, I’ve built a tech tree of everything that I think I’ll need to implement to get a fully functional language model. If you think anything is missing, <a href="mailto:bclarkson-code@proton.me">please let me know</a>:</p>
<div>
<figure>
<p><img src="https://bclarkson-code.github.io/posts/llm-from-scratch-scalar-autograd/images/tech_tree_post_1.png" alt="The LLM from scratch tech tree" width="700"></p>
<figcaption>The LLM from scratch tech tree</figcaption>
</figure>
</div>
<p>Before we can move onto building modern features like <a href="https://arxiv.org/abs/2104.09864">Rotary Positional Encodings</a>, we first need to figure out how to differentiate with a computer. The backpropagation algorithm that underpins the entire field of Deep Learning requires the ability to differentiate the outputs of neural networks with respect to (wrt) their inputs. In this post, we’ll go from nothing to an (admittedly very limited) automatic differentiation library that can differentiate arbitrary functions of scalar values.</p>
<p>This one algorithm will form the core of our deep learning library that, eventually, will include everything we need to train a language model.</p>
</section>
<section id="creating-a-tensor">
<h2 data-anchor-id="creating-a-tensor">Creating a tensor</h2>
<p>We can’t do any differentiation if we don’t have any numbers to differentiate. We’ll want to add some extra functionality that is in standard <code>float</code> types so we’ll need to create our own. Let’s call it a <code>Tensor</code>.</p>
<div id="3c2dc0d6" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="2"><pre><code><span id="cb2-1"><span>class</span> Tensor:</span>
<span id="cb2-2">    <span>"""</span></span>
<span id="cb2-3"><span>    Just a number (for now)</span></span>
<span id="cb2-4"><span>    """</span></span>
<span id="cb2-5"></span>
<span id="cb2-6">    value: <span>float</span></span>
<span id="cb2-7"></span>
<span id="cb2-8">    <span>def</span> <span>__init__</span>(<span>self</span>, value: <span>float</span>):</span>
<span id="cb2-9">        <span>self</span>.value <span>=</span> value</span>
<span id="cb2-10"></span>
<span id="cb2-11">    <span>def</span> <span>__repr__</span>(<span>self</span>) <span>-&gt;</span> <span>str</span>:</span>
<span id="cb2-12">        <span>"""</span></span>
<span id="cb2-13"><span>        Create a printable string representation of this</span></span>
<span id="cb2-14"><span>        object</span></span>
<span id="cb2-15"></span>
<span id="cb2-16"><span>        This function gets called when you pass a Tensor to print</span></span>
<span id="cb2-17"></span>
<span id="cb2-18"><span>        Without this function:</span></span>
<span id="cb2-19"><span>        &gt;&gt;&gt; print(Tensor(5))</span></span>
<span id="cb2-20"><span>        &lt;__main__.Tensor at 0x104fd1950&gt;</span></span>
<span id="cb2-21"></span>
<span id="cb2-22"><span>        With this function:</span></span>
<span id="cb2-23"><span>        &gt;&gt;&gt; print(Tensor(5))</span></span>
<span id="cb2-24"><span>        Tensor(5)</span></span>
<span id="cb2-25"><span>        """</span></span>
<span id="cb2-26">        <span>return</span> <span>f"Tensor(</span><span>{</span><span>self</span><span>.</span>value<span>}</span><span>)"</span></span>
<span id="cb2-27"></span>
<span id="cb2-28"></span>
<span id="cb2-29"><span># try it out</span></span>
<span id="cb2-30">Tensor(<span>5</span>)</span></code></pre></div>
<p>Next we’ll need some simple operations we want to perform: addition, subtraction and multiplication.</p>
<div id="69ba409d" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="3"><pre><code><span id="cb4-1"><span>def</span> _add(a: Tensor, b: Tensor):</span>
<span id="cb4-2">    <span>"""</span></span>
<span id="cb4-3"><span>    Add two tensors</span></span>
<span id="cb4-4"><span>    """</span></span>
<span id="cb4-5">    <span>return</span> Tensor(a.value <span>+</span> b.value)</span>
<span id="cb4-6"></span>
<span id="cb4-7"></span>
<span id="cb4-8"><span>def</span> _sub(a: Tensor, b: Tensor):</span>
<span id="cb4-9">    <span>"""</span></span>
<span id="cb4-10"><span>    Subtract tensor b from tensor a</span></span>
<span id="cb4-11"><span>    """</span></span>
<span id="cb4-12">    <span>return</span> Tensor(a.value <span>-</span> b.value)</span>
<span id="cb4-13"></span>
<span id="cb4-14"></span>
<span id="cb4-15"><span>def</span> _mul(a: Tensor, b: Tensor):</span>
<span id="cb4-16">    <span>"""</span></span>
<span id="cb4-17"><span>    Multiply two tensors</span></span>
<span id="cb4-18"><span>    """</span></span>
<span id="cb4-19">    <span>return</span> Tensor(a.value <span>*</span> b.value)</span></code></pre></div>
<p>We can use use our operations as follows:</p>
<div id="55858d5d" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="4">
<div id="cb5"><pre><code><span id="cb5-1"><span>def</span> test(got: Any, want: Any):</span>
<span id="cb5-2">    <span>"""</span></span>
<span id="cb5-3"><span>    Check that two objects are equal to each other</span></span>
<span id="cb5-4"><span>    """</span></span>
<span id="cb5-5">    indicator <span>=</span> <span>"✅"</span> <span>if</span> want <span>==</span> got <span>else</span> <span>"❌"</span></span>
<span id="cb5-6">    <span>print</span>(<span>f"</span><span>{</span>indicator<span>}</span><span> - Want: </span><span>{</span>want<span>}</span><span>, Got: </span><span>{</span>got<span>}</span><span>"</span>)</span>
<span id="cb5-7"></span>
<span id="cb5-8"></span>
<span id="cb5-9">a <span>=</span> Tensor(<span>3</span>)</span>
<span id="cb5-10">b <span>=</span> Tensor(<span>4</span>)</span>
<span id="cb5-11"></span>
<span id="cb5-12"></span>
<span id="cb5-13">test(_add(a, b).value, <span>7</span>)</span>
<span id="cb5-14">test(_sub(a, b).value, <span>-</span><span>1</span>)</span>
<span id="cb5-15">test(_mul(a, b).value, <span>12</span>)</span></code></pre></div>
<div>
<pre><code>✅ - Want: 7, Got: 7
✅ - Want: -1, Got: -1
✅ - Want: 12, Got: 12</code></pre>
</div>
</div>
</section>
<section id="scalar-derivatives">
<h2 data-anchor-id="scalar-derivatives">Scalar derivatives</h2>
<p>Diving straight into differentiating matrices sounds too hard so let’s start with something simpler: differentiating scalars. The simplest scalar derivative I can think of is differentiating a tensor with respect to itself: <span>\[\frac{dx}{dx} = 1\]</span></p>
<p>A more interesting case is the derivative of two tensors added together (note we are using partial derivatives because our function has multiple inputs): <span>\[f(x, y) = x + y\]</span> <span>\[\frac{\partial f}{\partial x} = 1\]</span> <span>\[\frac{\partial f}{\partial y} = 1\]</span></p>
<p>We can do a similar thing for multiplication and subtraction</p>
<table>
<colgroup>
<col>
<col>
<col>
</colgroup>
<thead>
<tr>
<th><span>\(f(x, y)\)</span></th>
<th><span>\(\frac{\partial f}{\partial x}\)</span></th>
<th><span>\(\frac{\partial f}{\partial y}\)</span></th>
</tr>
</thead>
<tbody>
<tr>
<td><span>\(x + y\)</span></td>
<td><span>\(1\)</span></td>
<td><span>\(1\)</span></td>
</tr>
<tr>
<td><span>\(x - y\)</span></td>
<td><span>\(1\)</span></td>
<td><span>\(-1\)</span></td>
</tr>
<tr>
<td><span>\(x \times y\)</span></td>
<td><span>\(y\)</span></td>
<td><span>\(x\)</span></td>
</tr>
</tbody>
</table>
<p>Now that we’ve worked out these derivatives mathematically, the next step is to convert them into code. In the table above, when we make a tensor by combining two tensors with an operation, the derivative only ever depends on the inputs and the operation. There is no “hidden state”.</p>
<p>This means that the only information we need to store is the inputs to an operation and a function to calculate the derivative wrt each input. With this, we should be able to differentiate any binary function wrt its inputs. A good place to store this information is in the tensor that is produced by the operation.</p>
<p>We’ll add some new attributes to our <code>Tensor</code>: <code>args</code> and <code>local_derivatives</code>. If the tensor is the output of an operation, then <code>args</code> will store the arguments to the operation and <code>local_derivatives</code> will store the derivatives wrt each input. We’re calling it <code>local_derivatives</code> to avoid confusion when we start nesting functions.</p>
<p>Once we’ve calculated the derivative (from our <code>args</code> and <code>local_derivatives</code>) we’ll need to store it. It turns out that the neatest place to put this is in the tensor that the output is being differentiated wrt. We’ll call this <code>derivative</code>.</p>
<div id="2df2b7ce" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="5"><pre><code><span id="cb7-1"><span>class</span> Tensor:</span>
<span id="cb7-2">    <span>"""</span></span>
<span id="cb7-3"><span>    A number that can be differentiated</span></span>
<span id="cb7-4"><span>    """</span></span>
<span id="cb7-5"></span>
<span id="cb7-6">    <span># If the tensor was made by an operation, the operation arguments</span></span>
<span id="cb7-7">    <span># are stored in args</span></span>
<span id="cb7-8">    args: <span>tuple</span>[<span>"Tensor"</span>] <span>=</span> ()</span>
<span id="cb7-9">    <span># If the tensor was made by an operation, the derivatives wrt</span></span>
<span id="cb7-10">    <span># operation inputs are stored in derivatives</span></span>
<span id="cb7-11">    local_derivatives: <span>tuple</span>[<span>"Tensor"</span>] <span>=</span> ()</span>
<span id="cb7-12">    <span># The derivative we have calculated</span></span>
<span id="cb7-13">    derivative: Optional[<span>"Tensor"</span>] <span>=</span> <span>None</span></span>
<span id="cb7-14"></span>
<span id="cb7-15">    <span>def</span> <span>__init__</span>(<span>self</span>, value: <span>float</span>):</span>
<span id="cb7-16">        <span>self</span>.value <span>=</span> value</span>
<span id="cb7-17"></span>
<span id="cb7-18">    <span>def</span> <span>__repr__</span>(<span>self</span>) <span>-&gt;</span> <span>str</span>:</span>
<span id="cb7-19">        <span>"""</span></span>
<span id="cb7-20"><span>        Create a printable string representation of this</span></span>
<span id="cb7-21"><span>        object</span></span>
<span id="cb7-22"></span>
<span id="cb7-23"><span>        This function gets called when you pass a Tensor to print</span></span>
<span id="cb7-24"></span>
<span id="cb7-25"><span>        Without this function:</span></span>
<span id="cb7-26"><span>        &gt;&gt;&gt; print(Tensor(5))</span></span>
<span id="cb7-27"><span>        &lt;__main__.Tensor at 0x104fd1950&gt;</span></span>
<span id="cb7-28"></span>
<span id="cb7-29"><span>        With this function:</span></span>
<span id="cb7-30"><span>        &gt;&gt;&gt; print(Tensor(5))</span></span>
<span id="cb7-31"><span>        Tensor(5)</span></span>
<span id="cb7-32"><span>        """</span></span>
<span id="cb7-33">        <span>return</span> <span>f"Tensor(</span><span>{</span><span>self</span><span>.</span>value<span>}</span><span>)"</span></span></code></pre></div>
<p>For example, if we have</p>
<div id="16c996f7" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="6"><pre><code><span id="cb8-1">a <span>=</span> Tensor(<span>3</span>)</span>
<span id="cb8-2">b <span>=</span> Tensor(<span>4</span>)</span>
<span id="cb8-3"></span>
<span id="cb8-4">output <span>=</span> _mul(a, b)</span></code></pre></div>
<p>Then <code>output.args</code> and <code>output.local_derivatives</code> should be set to:</p>
<div id="cb9"><pre><code><span id="cb9-1">output.args <span>==</span> (Tensor(<span>3</span>), Tensor(<span>4</span>))</span>
<span id="cb9-2">output.derivatives <span>==</span> (</span>
<span id="cb9-3">    b,  <span># derivative of output wrt a is b</span></span>
<span id="cb9-4">    a,  <span># derivative of output wrt b is a</span></span>
<span id="cb9-5">)</span></code></pre></div>
<p>Once we have actually computed the derivatives, then the derivative of <code>output</code> wrt <code>a</code> will be stored in <code>a.derivative</code> and should be equal to <code>b</code> (which is 4 in this case).</p>
<p>We know that we’ve done everything right once these tests pass:</p>
<div id="615dc0ba" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="7">
<div id="cb10"><pre><code><span id="cb10-1">a <span>=</span> Tensor(<span>3</span>)</span>
<span id="cb10-2">b <span>=</span> Tensor(<span>4</span>)</span>
<span id="cb10-3"></span>
<span id="cb10-4">output <span>=</span> _mul(a, b)</span>
<span id="cb10-5"></span>
<span id="cb10-6"><span># </span><span>TODO</span><span>: differentiate here</span></span>
<span id="cb10-7"></span>
<span id="cb10-8">test(got<span>=</span>output.args, want<span>=</span>(a, b))</span>
<span id="cb10-9">test(got<span>=</span>output.local_derivatives, want<span>=</span>(b, a))</span>
<span id="cb10-10">test(got<span>=</span>a.derivative, want<span>=</span>b)</span>
<span id="cb10-11">test(got<span>=</span>b.derivative, want<span>=</span>a)</span></code></pre></div>
<div>
<pre><code>❌ - Want: (Tensor(3), Tensor(4)), Got: ()
❌ - Want: (Tensor(4), Tensor(3)), Got: ()
❌ - Want: Tensor(4), Got: None
❌ - Want: Tensor(3), Got: None</code></pre>
</div>
</div>
<p>First, let’s add a function to our <code>Tensor</code> that will actually calculate the derivatives for each of the function arguments. Pytorch calls this function <code>backward</code> so we’ll do the same.</p>
<div id="18dcfc02" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="8"><pre><code><span id="cb12-1"><span>class</span> Tensor:</span>
<span id="cb12-2">    <span>"""</span></span>
<span id="cb12-3"><span>    A number that can be differentiated</span></span>
<span id="cb12-4"><span>    """</span></span>
<span id="cb12-5"></span>
<span id="cb12-6">    <span># If the tensor was made by an operation, the operation arguments</span></span>
<span id="cb12-7">    <span># are stored in args</span></span>
<span id="cb12-8">    args: <span>tuple</span>[<span>"Tensor"</span>] <span>=</span> ()</span>
<span id="cb12-9">    <span># If the tensor was made by an operation, the derivatives wrt</span></span>
<span id="cb12-10">    <span># operation inputs are stored in</span></span>
<span id="cb12-11">    local_derivatives: <span>tuple</span>[<span>"Tensor"</span>] <span>=</span> ()</span>
<span id="cb12-12">    <span># The derivative we have calculated</span></span>
<span id="cb12-13">    derivative: Optional[<span>"Tensor"</span>] <span>=</span> <span>None</span></span>
<span id="cb12-14"></span>
<span id="cb12-15">    <span># optionally give this tensor a name</span></span>
<span id="cb12-16">    name: Optional[<span>str</span>] <span>=</span> <span>None</span></span>
<span id="cb12-17">    <span># Later, we'll want to record the path we followed to get</span></span>
<span id="cb12-18">    <span># to this tensor and some operations we did along the way</span></span>
<span id="cb12-19">    <span># don't worry about these for now</span></span>
<span id="cb12-20">    paths: List[Tensor] <span>=</span> <span>None</span></span>
<span id="cb12-21">    chains: List[Tensor] <span>=</span> <span>None</span></span>
<span id="cb12-22"></span>
<span id="cb12-23">    <span>def</span> <span>__init__</span>(<span>self</span>, value: <span>float</span>):</span>
<span id="cb12-24">        <span>self</span>.value <span>=</span> value</span>
<span id="cb12-25"></span>
<span id="cb12-26">    <span>def</span> backward(<span>self</span>):</span>
<span id="cb12-27">        <span>if</span> <span>self</span>.args <span>is</span> <span>None</span> <span>or</span> <span>self</span>.local_derivatives <span>is</span> <span>None</span>:</span>
<span id="cb12-28">            <span>raise</span> <span>ValueError</span>(</span>
<span id="cb12-29">                <span>"Cannot differentiate a Tensor that is not a function of other Tensors"</span></span>
<span id="cb12-30">            )</span>
<span id="cb12-31"></span>
<span id="cb12-32">        <span>for</span> arg, derivative <span>in</span> <span>zip</span>(<span>self</span>.args, <span>self</span>.local_derivatives):</span>
<span id="cb12-33">            arg.derivative <span>=</span> derivative</span>
<span id="cb12-34"></span>
<span id="cb12-35">    <span>def</span> <span>__repr__</span>(<span>self</span>) <span>-&gt;</span> <span>str</span>:</span>
<span id="cb12-36">        <span>"""</span></span>
<span id="cb12-37"><span>        Create a printable string representation of this</span></span>
<span id="cb12-38"><span>        object</span></span>
<span id="cb12-39"></span>
<span id="cb12-40"><span>        This function gets called when you pass a Tensor to print</span></span>
<span id="cb12-41"></span>
<span id="cb12-42"><span>        Without this function:</span></span>
<span id="cb12-43"><span>        &gt;&gt;&gt; print(Tensor(5))</span></span>
<span id="cb12-44"><span>        &lt;__main__.Tensor at 0x104fd1950&gt;</span></span>
<span id="cb12-45"></span>
<span id="cb12-46"><span>        With this function:</span></span>
<span id="cb12-47"><span>        &gt;&gt;&gt; print(Tensor(5))</span></span>
<span id="cb12-48"><span>        Tensor(5)</span></span>
<span id="cb12-49"><span>        """</span></span>
<span id="cb12-50">        <span>return</span> <span>f"Tensor(</span><span>{</span><span>self</span><span>.</span>value<span>}</span><span>)"</span></span></code></pre></div>
<p>This only works if we also store the arguments and derivatives in the output tensors of operations</p>
<div id="0f77b5c2" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="9"><pre><code><span id="cb13-1"><span>def</span> _add(a: Tensor, b: Tensor):</span>
<span id="cb13-2">    <span>"""</span></span>
<span id="cb13-3"><span>    Add two tensors</span></span>
<span id="cb13-4"><span>    """</span></span>
<span id="cb13-5">    result <span>=</span> Tensor(a.value <span>+</span> b.value)</span>
<span id="cb13-6">    result.local_derivatives <span>=</span> (Tensor(<span>1</span>), Tensor(<span>1</span>))</span>
<span id="cb13-7">    result.args <span>=</span> (a, b)</span>
<span id="cb13-8">    <span>return</span> result</span>
<span id="cb13-9"></span>
<span id="cb13-10"></span>
<span id="cb13-11"><span>def</span> _sub(a: Tensor, b: Tensor):</span>
<span id="cb13-12">    <span>"""</span></span>
<span id="cb13-13"><span>    Subtract tensor b from a</span></span>
<span id="cb13-14"><span>    """</span></span>
<span id="cb13-15">    result <span>=</span> Tensor(a.value <span>-</span> b.value)</span>
<span id="cb13-16">    result.local_derivatives <span>=</span> (Tensor(<span>1</span>), Tensor(<span>-</span><span>1</span>))</span>
<span id="cb13-17">    result.args <span>=</span> (a, b)</span>
<span id="cb13-18">    <span>return</span> result</span>
<span id="cb13-19"></span>
<span id="cb13-20"></span>
<span id="cb13-21"><span>def</span> _mul(a: Tensor, b: Tensor):</span>
<span id="cb13-22">    <span>"""</span></span>
<span id="cb13-23"><span>    Multiply two tensors</span></span>
<span id="cb13-24"><span>    """</span></span>
<span id="cb13-25">    result <span>=</span> Tensor(a.value <span>*</span> b.value)</span>
<span id="cb13-26">    result.local_derivatives <span>=</span> (b, a)</span>
<span id="cb13-27">    result.args <span>=</span> (a, b)</span>
<span id="cb13-28">    <span>return</span> result</span></code></pre></div>
<p>Let’s re-run our tests and see if it works</p>
<div id="426bc097" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="10">
<div id="cb14"><pre><code><span id="cb14-1">a <span>=</span> Tensor(<span>3</span>)</span>
<span id="cb14-2">b <span>=</span> Tensor(<span>4</span>)</span>
<span id="cb14-3"></span>
<span id="cb14-4">output <span>=</span> _mul(a, b)</span>
<span id="cb14-5"></span>
<span id="cb14-6">output.backward()</span>
<span id="cb14-7"></span>
<span id="cb14-8">test(got<span>=</span>output.args, want<span>=</span>(a, b))</span>
<span id="cb14-9">test(got<span>=</span>output.local_derivatives, want<span>=</span>(b, a))</span>
<span id="cb14-10">test(a.derivative, b)</span>
<span id="cb14-11">test(b.derivative, a)</span></code></pre></div>
<div>
<pre><code>✅ - Want: (Tensor(3), Tensor(4)), Got: (Tensor(3), Tensor(4))
✅ - Want: (Tensor(4), Tensor(3)), Got: (Tensor(4), Tensor(3))
✅ - Want: Tensor(4), Got: Tensor(4)
✅ - Want: Tensor(3), Got: Tensor(3)</code></pre>
</div>
</div>
<p>So far so good, let’s try nesting operations.</p>
<div id="e14634fb" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="11">
<div id="cb16"><pre><code><span id="cb16-1">a <span>=</span> Tensor(<span>3</span>)</span>
<span id="cb16-2">b <span>=</span> Tensor(<span>4</span>)</span>
<span id="cb16-3"></span>
<span id="cb16-4">output_1 <span>=</span> _mul(a, b)</span>
<span id="cb16-5"><span># z = a + (a * b)</span></span>
<span id="cb16-6">output_2 <span>=</span> _add(a, output_1)</span>
<span id="cb16-7"></span>
<span id="cb16-8">output_2.backward()</span>
<span id="cb16-9"></span>
<span id="cb16-10"><span># should get</span></span>
<span id="cb16-11"><span># dz/db = 0 + a = a</span></span>
<span id="cb16-12">test(b.derivative, a)</span></code></pre></div>
<div>
<pre><code>❌ - Want: Tensor(3), Got: None</code></pre>
</div>
</div>
<p>Something has gone wrong.</p>
<p>We should have got <code>a</code> as the derivative for <code>b</code> but we got <code>0</code> instead. Looking through the <code>.backward()</code> function, the issue is pretty clear: we haven’t thought about nested functions. To get this example working, we’ll need to figure out how to calculate derivatives through multiple functions instead of just one.</p>
</section>
<section id="chaining-functions-together">
<h2 data-anchor-id="chaining-functions-together">Chaining Functions Together</h2>
<p>To calculate derivatives of nested functions, we can use a rule from calculus: The Chain Rule.</p>
<p>For a variable <span>\(z\)</span> generated by nested functions <span>\(f\)</span> and <span>\(g\)</span> such that <span>\[z = f(g(x))\]</span></p>
<p>Then the derivative of <span>\(z\)</span> wrt <span>\(x\)</span> is: <span>\[\frac{\partial z}{\partial x} = \frac{\partial f(u)}{\partial u} \frac{\partial g(x)}{\partial x}\]</span></p>
<p>Here, <span>\(u\)</span> is a dummy variable. <span>\(\frac{\partial f(u)}{\partial u}\)</span> means the derivative of <span>\(f\)</span> wrt its input.</p>
<p>For example, if</p>
<p><span>\[f(x) = g(x)^2\]</span> Then we can define <span>\(u=g(x)\)</span> and rewrite <span>\(f\)</span> in terms of u <span>\[f(u) = u^2 \implies \frac{\partial f(u)}{\partial u} = 2u = 2 g(x)\]</span></p>
<section id="multiple-variables">
<h3 data-anchor-id="multiple-variables">Multiple Variables</h3>
<p>The chain rule works as you might expect for functions of multiple variables. When differentiating wrt a variable, we can treat the other variables as constant and differentiate as normal <span>\[z = f(g(x), h(y))\]</span></p>
<p><span>\[\frac{\partial z}{\partial x} = \frac{\partial f(u)}{\partial u} \frac{\partial g(x)}{\partial x}\]</span> <span>\[\frac{\partial z}{\partial y} = \frac{\partial f(u)}{\partial u} \frac{\partial h(y)}{\partial y}\]</span></p>
<p>If we have different functions that take the same input, we differentiate each of them individually and then add them together</p>
<p><span>\[z = f(g(x), h(x))\]</span></p>
<p>We get <span>\[\frac{\partial z}{\partial x} = \frac{\partial f(u)}{\partial u}\frac{\partial g(x)}{\partial x} + \frac{\partial f(u)}{\partial u}\frac{\partial h(x)}{\partial x}\]</span></p>
</section>
<section id="more-than-2-functions">
<h3 data-anchor-id="more-than-2-functions">More than 2 functions</h3>
<p>If we chain 3 functions together, we still just multiply the derivatives for each function together:</p>
<p><span>\[\frac{\partial z}{\partial x} = \frac{\partial f(u)}{\partial u} \frac{\partial g(x)}{\partial x} = \frac{\partial f(u)}{\partial u} \frac{\partial g(u)}{\partial u}\frac{\partial h(x)}{\partial x}\]</span></p>
<p>And this generalises to any amount of nesting</p>
<p><span>\[z = f_1(f_2(....f_{n-1}(f_n(x))...)) \]</span> <span>\(\implies \frac{\partial z}{\partial x} = \frac{\partial f_1(u)}{\partial u}\frac{\partial f_2(u)}{\partial u}...\frac{\partial f_{n-1}(u)}{\partial u}\frac{\partial f_{n}(x)}{\partial x}\)</span>$</p>
</section>
<section id="a-picture-is-worth-a-thousand-equations">
<h3 data-anchor-id="a-picture-is-worth-a-thousand-equations">A picture is worth a thousand equations</h3>
<p>As you probably noticed, the maths is starting to get quite dense. When we start working with neural networks, we can easily get 100s or 1000s of functions deep so to get a handle on things, we’ll need a different strategy. Helpfully, there is one: turning it into a graph.</p>
<p>We can start with some rules:</p>
<blockquote>
<p>Variables are represented with circles and operations are represented with boxes</p>
</blockquote>
<div>
<figure>
<p><img src="https://bclarkson-code.github.io/posts/llm-from-scratch-scalar-autograd/images/variable_and_box.png" alt="A variable as a circle and an operation as a box"></p>
</figure>
</div>
<blockquote>
<p>Inputs to an operation are represented with arrows that point to the operation box. Outputs point away.</p>
</blockquote>
<p>For example, here is the diagram for <span>\(z = mx\)</span></p>
<div>
<figure>
<p><img src="https://bclarkson-code.github.io/posts/llm-from-scratch-scalar-autograd/images/z_eq_mx.png" alt="The operation z = mx"></p>
</figure>
</div>
<p>And that’s it! All of the equations we’ll be working with can be represented graphically using these simple rules. To try it out, let’s draw the diagram for a more complex formula:</p>
<div>
<figure>
<p><img src="https://bclarkson-code.github.io/posts/llm-from-scratch-scalar-autograd/images/square_error.png" alt="A diagram of the square error of a linear regression"></p>
</figure>
</div>
<p>This is an example of a structure called a graph (also called a network). A lot of problem in computer science get much easier if you can represent them with a graph and this is no exception.</p>
<p>The real power of these diagrams is that they can also help us with our derivatives. Take <span>\[y = mx + p = \texttt{add}(p, \texttt{mul}(m ,x)).\]</span></p>
<p>From before, we can find its derivatives by differentiating each operation wrt its inputs and multiplying the results together. In this case, we get: <span>\[\frac{\partial y}{\partial p} = \frac{\partial \texttt{add}(u_1, u_2)}{\partial u_1} = 1\]</span> <span>\[\frac{\partial y}{\partial m} = \frac{\partial \texttt{add}(u_1, u_2)}{\partial u_2}\frac{\partial \texttt{mul}(u_1, u_2)}{\partial u_2} = 1 \times x = x\]</span> <span>\[\frac{\partial y}{\partial x} = \frac{\partial \texttt{add}(u_1, u_2)}{\partial u_2}\frac{\partial \texttt{mul}(u_1, u_2)}{\partial u_1} = 1 \times m = m\]</span></p>
<p>We can also graph it like this:</p>
<div>
<figure>
<p><img src="https://bclarkson-code.github.io/posts/llm-from-scratch-scalar-autograd/images/y_eq_mx_plus_p_labelled.png" alt="a graph of y = mx + p"></p>
</figure>
</div>
<p>If you imagine walking from <span>\(y\)</span> to each of the inputs, you might notice a similarity between the edges you pass through and the equations above. If you walk from <span>\(y\)</span> to <span>\(x\)</span>, you’ll pass through <code>a-&gt;c-&gt;d</code>. Similarly, if you walk from <span>\(y\)</span> to <span>\(m\)</span>, you’ll pass through <code>a-&gt;d-&gt;e</code>. Notice that both paths go through <code>c</code>, the edge coming out of <code>add</code> that corresponds to the input <span>\(u_2\)</span>. Also, both equations include the term <span>\(\frac{\partial \texttt{add}(u_1, u_2)}{\partial u_2}\)</span>.</p>
<p>If I rename the edges as follows:</p>
<div>
<figure>
<p><img src="https://bclarkson-code.github.io/posts/llm-from-scratch-scalar-autograd/images/y_eq_mx_plus_p_deriv.png" alt="y = mx + p with each edge given a letter"></p>
</figure>
</div>
<p>We can see that going from <span>\(y\)</span> to <span>\(x\)</span>, we pass through <span>\(1\)</span>, <span>\(\frac{\partial \texttt{add}(u_1, u_2)}{\partial u_2}\)</span> and <span>\(\frac{\partial \texttt{mul}(u_1, u_2)}{\partial u_1}\)</span>. If we multiply these together, we get exactly <span>\(\frac{\partial \texttt{add}(u_1, u_2)}{\partial u_2}\frac{\partial \texttt{mul}(u_1, u_2)}{\partial u_1} = \frac{\partial y}{\partial x}\)</span>!</p>
<p>It turns out that this rule works in general:</p>
<blockquote>
<p>If we have some operation <span>\(\texttt{op}(u_1, u_2, ..., u_n)\)</span>, we should label the edge corresponding to input <span>\(u_i\)</span> with <span>\(\frac{\partial \texttt{op}(u_1, u_2, ..., u_n)}{\partial u_i}\)</span></p>
</blockquote>
<p>Then, if we want to find the derivative of the output node wrt any of the inputs,</p>
<blockquote>
<p>The derivative of an output variable wrt one of the input variables can be found by traversing the graph from the output to the input and multiplying together the derivatives for every edge on the path</p>
</blockquote>
<p>To cover every edge case, there are some extra details</p>
<blockquote>
<p>If a graph contains multiple paths from the output to an input, then the derivative is the sum of the products for each path</p>
</blockquote>
<p>This comes from the case we saw earlier where when we have different functions that have the same input we have to add their derivative chains together.</p>
<blockquote>
<p>If an edge is not the input to any function, its derivative is 1</p>
</blockquote>
<p>This covers the edge that leads from the final operation to the output. You can think of the edge having the derivative <span>\(\frac{\partial y}{\partial y}=1\)</span></p>
<p>And that’s it! Let’s try it out with <span>\(z = (x + c)x\)</span>:</p>
<div>
<figure>
<p><img src="https://bclarkson-code.github.io/posts/llm-from-scratch-scalar-autograd/images/z_eq_xx_plus_xc.png" alt="A graph of z = (x+c)x with edges annotated with derivatives"></p>
</figure>
</div>
<p>Here, instead of writing the formulae for each derivative, I have gone ahead and calculated their actual values. Instead of just figuring out the formulae for a derivative, we want to calculate its value when we plug in our input parameters.</p>
<p>All that remains is to multiply the local derivatives together along each path. We’ll call the product of derivatives along a single path a chain (after the chain rule)</p>
<p>We can get from <span>\(z\)</span> to <span>\(x\)</span> via the green path and the red path. Following these paths, we get: <span>\[\text{red path} = 1 \times (x + c) = x + c\]</span> Along the green path we get: <span>\[\text{green path} = 1 \times x \times 1 = x\]</span></p>
<p>Adding these together, we get <span>\((x+c) + x = 2x + c\)</span></p>
<p>If we work out the derivative algebraically:</p>
<p><span>\[\frac{\partial z}{\partial x} = \frac{\partial}{\partial x}((x+c)x) = \frac{\partial}{\partial x}(x^2 + cx) = \frac{\partial x^2}{\partial x} + c\frac{\partial x}{\partial x} = 2x + c\]</span></p>
<p>We can see that it seems to work! Calculating <span>\(\frac{\partial z}{\partial c}\)</span> is left as an exercise for the reader (I’ve always wanted to say that).</p>
<p>To summarise, we have invented the following algorithm for calculating of a variable wrt its inputs:</p>
<ol type="1">
<li>Turn the equation into a graph</li>
<li>Label each edge with the appropriate derivative</li>
<li>Find every path from the output to the input variable you care about</li>
<li>Follow each path and multiply the derivatives you pass through</li>
<li>Add together the results for each path</li>
</ol>
<p>Now that we have an algorithm in pictures and words, let’s turn it into code.</p>
</section>
<section id="the-algorithm">
<h3 data-anchor-id="the-algorithm">The Algorithm™</h3>
<p>Surprisingly, we have actually already converted our functions into graphs. If you recall, when we generate a tensor from an operation, we record the inputs to the operation in the output tensor (in <code>.args</code>). We also stored the functions to calculate derivatives for each of the inputs in <code>.local_derivatives</code> which means that we know both the destination and derivative for every edge that points to a given node. This means that we’ve already completed steps 1 and 2.</p>
<p>The next challenge is to find all paths from the tensor we want to differentiate to the input tensors that created it. Because none of our operations are self referential (outputs are never fed back in as inputs), and all of our edges have a direction, our graph of operations is a directed acyclic graph or DAG. The property of the graph having no cycles means that we can find all paths to every parameter pretty easily with a Breadth First Search (or Depth First Search but BFS makes some optimisations easier as we’ll see in part 2).</p>
<p>To try it out, let’s recreate that giant graph we made earlier. We can do this by first calculating <span>\(L\)</span> from the inputs</p>
<div id="4c65eba1" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="12"><pre><code><span id="cb18-1">y <span>=</span> Tensor(<span>1</span>)</span>
<span id="cb18-2">m <span>=</span> Tensor(<span>2</span>)</span>
<span id="cb18-3">x <span>=</span> Tensor(<span>3</span>)</span>
<span id="cb18-4">c <span>=</span> Tensor(<span>4</span>)</span>
<span id="cb18-5"></span>
<span id="cb18-6"><span># L = (y - (mx + c))^2</span></span>
<span id="cb18-7">left <span>=</span> _sub(y, _add(_mul(m, x), c))</span>
<span id="cb18-8">right <span>=</span> _sub(y, _add(_mul(m, x), c))</span>
<span id="cb18-9"></span>
<span id="cb18-10">L <span>=</span> _mul(left, right)</span>
<span id="cb18-11"></span>
<span id="cb18-12"><span># Attaching names to tensors will make our</span></span>
<span id="cb18-13"><span># diagram look nicer</span></span>
<span id="cb18-14">y.name <span>=</span> <span>"y"</span></span>
<span id="cb18-15">m.name <span>=</span> <span>"m"</span></span>
<span id="cb18-16">x.name <span>=</span> <span>"x"</span></span>
<span id="cb18-17">c.name <span>=</span> <span>"c"</span></span>
<span id="cb18-18">L.name <span>=</span> <span>"L"</span></span></code></pre></div>
<p>And then using Breadth First Search to do 3 things:</p>
<ul>
<li>Find all nodes</li>
<li>Find all edges</li>
<li>Find all paths from <span>\(L\)</span> to our parameters</li>
</ul>
<p>We haven’t implemented a simple way to check whether two tensors are identical so we’ll need to compare hashes.</p>
<div id="6f90fd88" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="13"><pre><code><span id="cb19-1">edges <span>=</span> []</span>
<span id="cb19-2"></span>
<span id="cb19-3">stack <span>=</span> [(L, [L])]</span>
<span id="cb19-4"></span>
<span id="cb19-5">nodes <span>=</span> []</span>
<span id="cb19-6">edges <span>=</span> []</span>
<span id="cb19-7"><span>while</span> stack:</span>
<span id="cb19-8">    node, current_path <span>=</span> stack.pop()</span>
<span id="cb19-9">    <span># Record nodes we haven't seen before</span></span>
<span id="cb19-10">    <span>if</span> <span>hash</span>(node) <span>not</span> <span>in</span> [<span>hash</span>(n) <span>for</span> n <span>in</span> nodes]:</span>
<span id="cb19-11">        nodes.append(node)</span>
<span id="cb19-12"></span>
<span id="cb19-13">    <span># If we have reached a parameter (it has no arguments</span></span>
<span id="cb19-14">    <span># because it wasn't created by an operation) then</span></span>
<span id="cb19-15">    <span># record the path taken to get here</span></span>
<span id="cb19-16">    <span>if</span> <span>not</span> node.args:</span>
<span id="cb19-17">        <span>if</span> node.paths <span>is</span> <span>None</span>:</span>
<span id="cb19-18">            node.paths <span>=</span> []</span>
<span id="cb19-19">        node.paths.append(current_path)</span>
<span id="cb19-20">        <span>continue</span></span>
<span id="cb19-21"></span>
<span id="cb19-22">    <span>for</span> arg <span>in</span> node.args:</span>
<span id="cb19-23">        stack.append((arg, current_path <span>+</span> [arg]))</span>
<span id="cb19-24">        <span># Record every new edge</span></span>
<span id="cb19-25">        edges.append((<span>hash</span>(node), <span>hash</span>(arg)))</span></code></pre></div>
<p>Now we’ve got all of the edges and nodes, we have complete knowledge of our computational graph. Let’s use networkx to plot it</p>
<div id="8fd42378" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="14">
<div id="cb20"><pre><code><span id="cb20-1"><span># Assign a unique integer to each</span></span>
<span id="cb20-2"><span># unnamed node so we know which</span></span>
<span id="cb20-3"><span># node is which in the picture</span></span>
<span id="cb20-4">labels <span>=</span> {}</span>
<span id="cb20-5"><span>for</span> i, node <span>in</span> <span>enumerate</span>(nodes):</span>
<span id="cb20-6">    <span>if</span> node.name <span>is</span> <span>None</span>:</span>
<span id="cb20-7">        labels[<span>hash</span>(node)] <span>=</span> <span>str</span>(i)</span>
<span id="cb20-8">    <span>else</span>:</span>
<span id="cb20-9">        labels[<span>hash</span>(node)] <span>=</span> node.name</span>
<span id="cb20-10"></span>
<span id="cb20-11">graph <span>=</span> nx.DiGraph()</span>
<span id="cb20-12">graph.add_edges_from(edges)</span>
<span id="cb20-13">pos <span>=</span> nx.nx_agraph.pygraphviz_layout(graph, prog<span>=</span><span>"dot"</span>)</span>
<span id="cb20-14">nx.draw(graph, pos<span>=</span>pos, labels<span>=</span>labels)</span></code></pre></div>
<div>
<figure>
<p><img src="https://bclarkson-code.github.io/posts/llm-from-scratch-scalar-autograd/post_files/figure-html/cell-15-output-1.png" width="691" height="499"></p>
</figure>
</div>
</div>
<p>If you squint a bit, you can see that this looks like the graph we made earlier! Let’s take a look at the paths the algorithm found from <span>\(L\)</span> to <span>\(x\)</span>.</p>
<div id="7cf96dca" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="15">
<div id="cb21"><pre><code><span id="cb21-1"><span>for</span> path <span>in</span> x.paths:</span>
<span id="cb21-2">    steps <span>=</span> []</span>
<span id="cb21-3">    <span>for</span> step <span>in</span> path:</span>
<span id="cb21-4">        steps.append(labels[<span>hash</span>(step)])</span>
<span id="cb21-5">    <span>print</span>(<span>"-&gt;"</span>.join(steps))</span></code></pre></div>
<div>
<pre><code>L-&gt;1-&gt;2-&gt;4-&gt;x
L-&gt;8-&gt;9-&gt;10-&gt;x</code></pre>
</div>
</div>
<p>The paths look correct! All we need to do now is to modify the algorithm a bit to keep track of the chain of derivatives along each path.</p>
<div id="386e7fdf" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="16"><pre><code><span id="cb23-1">y <span>=</span> Tensor(<span>1</span>)</span>
<span id="cb23-2">m <span>=</span> Tensor(<span>2</span>)</span>
<span id="cb23-3">x <span>=</span> Tensor(<span>3</span>)</span>
<span id="cb23-4">c <span>=</span> Tensor(<span>4</span>)</span>
<span id="cb23-5"></span>
<span id="cb23-6"><span># L = (y - (mx + c))^2</span></span>
<span id="cb23-7">left <span>=</span> _sub(y, _add(_mul(m, x), c))</span>
<span id="cb23-8">right <span>=</span> _sub(y, _add(_mul(m, x), c))</span>
<span id="cb23-9"></span>
<span id="cb23-10">L <span>=</span> _mul(left, right)</span>
<span id="cb23-11"></span>
<span id="cb23-12">y.name <span>=</span> <span>"y"</span></span>
<span id="cb23-13">m.name <span>=</span> <span>"m"</span></span>
<span id="cb23-14">x.name <span>=</span> <span>"x"</span></span>
<span id="cb23-15">c.name <span>=</span> <span>"c"</span></span>
<span id="cb23-16">L.name <span>=</span> <span>"L"</span></span></code></pre></div>
<div id="28e3b011" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="17"><pre><code><span id="cb24-1">stack <span>=</span> [(L, [L], [])]</span>
<span id="cb24-2"></span>
<span id="cb24-3">nodes <span>=</span> []</span>
<span id="cb24-4">edges <span>=</span> []</span>
<span id="cb24-5"><span>while</span> stack:</span>
<span id="cb24-6">    node, current_path, current_chain <span>=</span> stack.pop()</span>
<span id="cb24-7">    <span># Record nodes we haven't seen before</span></span>
<span id="cb24-8">    <span>if</span> <span>hash</span>(node) <span>not</span> <span>in</span> [<span>hash</span>(n) <span>for</span> n <span>in</span> nodes]:</span>
<span id="cb24-9">        nodes.append(node)</span>
<span id="cb24-10"></span>
<span id="cb24-11">    <span># If we have reached a parameter (it has no arguments</span></span>
<span id="cb24-12">    <span># because it wasn't created by an operation) then</span></span>
<span id="cb24-13">    <span># record the path taken to get here</span></span>
<span id="cb24-14">    <span>if</span> <span>not</span> node.args:</span>
<span id="cb24-15">        <span>if</span> node.paths <span>is</span> <span>None</span>:</span>
<span id="cb24-16">            node.paths <span>=</span> []</span>
<span id="cb24-17">        <span>if</span> node.chains <span>is</span> <span>None</span>:</span>
<span id="cb24-18">            node.chains <span>=</span> []</span>
<span id="cb24-19">        node.paths.append(current_path)</span>
<span id="cb24-20">        node.chains.append(current_chain)</span>
<span id="cb24-21">        <span>continue</span></span>
<span id="cb24-22"></span>
<span id="cb24-23">    <span>for</span> arg, op <span>in</span> <span>zip</span>(node.args, node.local_derivatives):</span>
<span id="cb24-24">        next_node <span>=</span> arg</span>
<span id="cb24-25">        next_path <span>=</span> current_path <span>+</span> [arg]</span>
<span id="cb24-26">        next_chain <span>=</span> current_chain <span>+</span> [op]</span>
<span id="cb24-27"></span>
<span id="cb24-28">        stack.append((arg, next_path, next_chain))</span>
<span id="cb24-29"></span>
<span id="cb24-30">        <span># Record every new edge</span></span>
<span id="cb24-31">        edges.append((<span>hash</span>(node), <span>hash</span>(arg)))</span></code></pre></div>
<p>Let’s check if the derivatives were recorded correctly.</p>
<div id="95ea9947" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="18">
<div id="cb25"><pre><code><span id="cb25-1"><span>print</span>(<span>f"Number of chains: </span><span>{</span><span>len</span>(x.chains)<span>}</span><span>"</span>)</span>
<span id="cb25-2"><span>for</span> chain <span>in</span> x.chains:</span>
<span id="cb25-3">    <span>print</span>(chain)</span></code></pre></div>
<div>
<pre><code>Number of chains: 2
[Tensor(-9), Tensor(-1), Tensor(1), Tensor(2)]
[Tensor(-9), Tensor(-1), Tensor(1), Tensor(2)]</code></pre>
</div>
</div>
<p>Looks reasonable so far. We have 2 identical paths, each with 4 derivatives (one for each edge in the path) as expected.</p>
<p>Let’s multiply the derivatives together along each path and add the total for each path together and see if we get the right answer.</p>
<p>According my calculations (and <a href="https://www.wolframalpha.com/">Wolfram Alpha</a>) the derivative of <span>\(L\)</span> wrt <span>\(x\)</span> is: <span>\[\frac{\partial L}{\partial x} = 2m (c + mx - y)\]</span> Plugging the values for our tensors in, we get <span>\[2\times2 (4 + (2\times3) - 1) = 36\]</span></p>
<div id="71c45972" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="19"><pre><code><span id="cb27-1">total_derivative <span>=</span> Tensor(<span>0</span>)</span>
<span id="cb27-2"><span>for</span> chain <span>in</span> x.chains:</span>
<span id="cb27-3">    chain_total <span>=</span> Tensor(<span>1</span>)</span>
<span id="cb27-4">    <span>for</span> step <span>in</span> chain:</span>
<span id="cb27-5">        chain_total <span>=</span> _mul(chain_total, step)</span>
<span id="cb27-6">    total_derivative <span>=</span> _add(total_derivative, chain_total)</span>
<span id="cb27-7"></span>
<span id="cb27-8">total_derivative</span></code></pre></div>
<p>The correct answer! It looks like our algorithm works. All that remains is to put all the pieces together.</p>
</section>
</section>
<section id="putting-it-all-together">
<h2 data-anchor-id="putting-it-all-together">Putting it all together</h2>
<p>When dreaming up the algorithm, we kept a record of the nodes, edges and paths which made plotting and debugging easier. Now that we know that it works, we can remove these and simplify things a bit.</p>
<div id="5c3a5852" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="20"><pre><code><span id="cb29-1"><span>def</span> backward(root_node: Tensor) <span>-&gt;</span> <span>None</span>:</span>
<span id="cb29-2">    stack <span>=</span> [(root_node, [])]</span>
<span id="cb29-3"></span>
<span id="cb29-4">    <span>while</span> stack:</span>
<span id="cb29-5">        node, current_derivative <span>=</span> stack.pop()</span>
<span id="cb29-6"></span>
<span id="cb29-7">        <span># if we have reached a parameter (it has no arguments</span></span>
<span id="cb29-8">        <span># because it wasn't created by an operation) then</span></span>
<span id="cb29-9">        <span># record the path taken to get here</span></span>
<span id="cb29-10">        <span>if</span> <span>not</span> node.args:</span>
<span id="cb29-11">            <span>if</span> node.chains <span>is</span> <span>None</span>:</span>
<span id="cb29-12">                node.chains <span>=</span> []</span>
<span id="cb29-13">            node.chain.append(current_derivative)</span>
<span id="cb29-14">            <span>continue</span></span>
<span id="cb29-15"></span>
<span id="cb29-16">        <span>for</span> arg, op <span>in</span> <span>zip</span>(node.args, node.local_derivatives):</span>
<span id="cb29-17">            stack.append((arg, current_derivative <span>+</span> [op]))</span></code></pre></div>
<p>There is also no need (for now) to store the derivatives and calculate them separately. Instead, we can avoid a bunch of repeated calculations by multiplying the derivatives as we go.</p>
<div id="c4000db8" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="21"><pre><code><span id="cb30-1"><span>def</span> backward(root_node: Tensor) <span>-&gt;</span> <span>None</span>:</span>
<span id="cb30-2">    stack <span>=</span> [(root_node, Tensor(<span>1</span>))]</span>
<span id="cb30-3"></span>
<span id="cb30-4">    <span>while</span> stack:</span>
<span id="cb30-5">        node, current_derivative <span>=</span> stack.pop()</span>
<span id="cb30-6"></span>
<span id="cb30-7">        <span># if we have reached a parameter (it has no arguments</span></span>
<span id="cb30-8">        <span># because it wasn't created by an operation) then add the</span></span>
<span id="cb30-9">        <span># derivative</span></span>
<span id="cb30-10">        <span>if</span> <span>not</span> node.args:</span>
<span id="cb30-11">            <span>if</span> node.derivative <span>is</span> <span>None</span>:</span>
<span id="cb30-12">                node.derivative <span>=</span> current_derivative</span>
<span id="cb30-13">            <span>else</span>:</span>
<span id="cb30-14">                node.derivative <span>=</span> _add(node.derivative, current_derivative)</span>
<span id="cb30-15">            <span>continue</span></span>
<span id="cb30-16"></span>
<span id="cb30-17">        <span>for</span> arg, derivative <span>in</span> <span>zip</span>(node.args, node.local_derivatives):</span>
<span id="cb30-18">            stack.append((arg, _mul(current_derivative, derivative)))</span></code></pre></div>
<p>Let’s make sure we didn’t break anything</p>
<div id="6bc39eac" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="22">
<div id="cb31"><pre><code><span id="cb31-1">y <span>=</span> Tensor(<span>1</span>)</span>
<span id="cb31-2">m <span>=</span> Tensor(<span>2</span>)</span>
<span id="cb31-3">x <span>=</span> Tensor(<span>3</span>)</span>
<span id="cb31-4">c <span>=</span> Tensor(<span>4</span>)</span>
<span id="cb31-5"></span>
<span id="cb31-6">left <span>=</span> _sub(y, _add(_mul(m, x), c))</span>
<span id="cb31-7">right <span>=</span> _sub(y, _add(_mul(m, x), c))</span>
<span id="cb31-8"></span>
<span id="cb31-9">L <span>=</span> _mul(left, right)</span>
<span id="cb31-10">backward(L)</span>
<span id="cb31-11"></span>
<span id="cb31-12"><span>print</span>(<span>f"</span><span>{</span>x<span>.</span>derivative <span>=</span> <span>}</span><span>\n</span><span>"</span>)</span>
<span id="cb31-13">test(got<span>=</span>x.derivative.value, want<span>=</span><span>36</span>)</span></code></pre></div>
<div>
<pre><code>x.derivative = Tensor(36)

✅ - Want: 36, Got: 36</code></pre>
</div>
</div>
<p>Let’s put this algorithm into our Tensor object</p>
<div id="208fa522" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="23"><pre><code><span id="cb33-1"><span>class</span> Tensor:</span>
<span id="cb33-2">    <span>"""</span></span>
<span id="cb33-3"><span>    A float that can be differentiated</span></span>
<span id="cb33-4"><span>    """</span></span>
<span id="cb33-5"></span>
<span id="cb33-6">    args: <span>tuple</span>[Tensor] <span>=</span> ()</span>
<span id="cb33-7">    local_derivatives: <span>tuple</span>[Tensor] <span>=</span> ()</span>
<span id="cb33-8">    <span># The derivative (once we've calculated it).  This is None if the derivative</span></span>
<span id="cb33-9">    <span># has not been computed yet</span></span>
<span id="cb33-10">    derivative: Tensor <span>|</span> <span>None</span> <span>=</span> <span>None</span></span>
<span id="cb33-11"></span>
<span id="cb33-12">    <span>def</span> <span>__init__</span>(<span>self</span>, value: <span>float</span>):</span>
<span id="cb33-13">        <span>self</span>.value <span>=</span> value</span>
<span id="cb33-14"></span>
<span id="cb33-15">    <span>def</span> <span>__repr__</span>(<span>self</span>) <span>-&gt;</span> <span>str</span>:</span>
<span id="cb33-16">        <span>return</span> <span>f"Tensor(</span><span>{</span><span>self</span><span>.</span>value<span>.</span><span>__repr__</span>()<span>}</span><span>)"</span></span>
<span id="cb33-17"></span>
<span id="cb33-18">    <span>def</span> backward(<span>self</span>):</span>
<span id="cb33-19">        <span>if</span> <span>self</span>.args <span>is</span> <span>None</span> <span>or</span> <span>self</span>.local_derivatives <span>is</span> <span>None</span>:</span>
<span id="cb33-20">            <span>raise</span> <span>ValueError</span>(</span>
<span id="cb33-21">                <span>"Cannot differentiate a Tensor that is not a function of other Tensors"</span></span>
<span id="cb33-22">            )</span>
<span id="cb33-23"></span>
<span id="cb33-24">        stack <span>=</span> [(<span>self</span>, Tensor(<span>1</span>))]</span>
<span id="cb33-25"></span>
<span id="cb33-26">        <span>while</span> stack:</span>
<span id="cb33-27">            node, current_derivative <span>=</span> stack.pop()</span>
<span id="cb33-28"></span>
<span id="cb33-29">            <span># if we have reached a parameter (it has no arguments</span></span>
<span id="cb33-30">            <span># because it wasn't created by an operation) then add the</span></span>
<span id="cb33-31">            <span># derivative</span></span>
<span id="cb33-32">            <span>if</span> <span>not</span> node.args:</span>
<span id="cb33-33">                <span>if</span> node.derivative <span>is</span> <span>None</span>:</span>
<span id="cb33-34">                    node.derivative <span>=</span> Tensor(<span>0</span>)</span>
<span id="cb33-35">                node.derivative <span>=</span> _add(node.derivative, current_derivative)</span>
<span id="cb33-36">                <span>continue</span></span>
<span id="cb33-37"></span>
<span id="cb33-38">            <span>for</span> arg, derivative <span>in</span> <span>zip</span>(node.args, node.local_derivatives):</span>
<span id="cb33-39">                new_derivative <span>=</span> _mul(current_derivative, derivative)</span>
<span id="cb33-40">                stack.append((arg, new_derivative))</span></code></pre></div>
<p>Let’s try it out</p>
<div id="59db14e7" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="24">
<div id="cb34"><pre><code><span id="cb34-1">y <span>=</span> Tensor(<span>1</span>)</span>
<span id="cb34-2">m <span>=</span> Tensor(<span>2</span>)</span>
<span id="cb34-3">x <span>=</span> Tensor(<span>3</span>)</span>
<span id="cb34-4">c <span>=</span> Tensor(<span>4</span>)</span>
<span id="cb34-5"></span>
<span id="cb34-6">left <span>=</span> _sub(y, _add(_mul(m, x), c))</span>
<span id="cb34-7">right <span>=</span> _sub(y, _add(_mul(m, x), c))</span>
<span id="cb34-8"></span>
<span id="cb34-9">L <span>=</span> _mul(left, right)</span>
<span id="cb34-10">L.backward()</span>
<span id="cb34-11"></span>
<span id="cb34-12">test(x.derivative, Tensor(<span>36</span>))</span></code></pre></div>
<div>
<pre><code>❌ - Want: Tensor(36), Got: Tensor(36)</code></pre>
</div>
</div>
<p>Huh?</p>
<p>By default, if you compare two objects in python with <code>==</code>, python will check whether the object on the left has the same reference as the object as the one on the right. Because <code>Tensor(36)</code> is a different object (that just happens to have the same value) to <code>x.derivative</code>, <code>x.derivative == Tensor(36)</code> returns <code>False</code>.</p>
<p>It makes a lot more sense to compare two tensors based upon their <code>.value</code>. To achieve this, we can add the <code>__eq__</code> special method to <code>Tensor</code> which will change the behaviour of the <code>==</code> operator for <code>Tensor</code> objects</p>
<div id="00be4f8d" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="25"><pre><code><span id="cb36-1"><span>def</span> <span>__eq__</span>(<span>self</span>, other) <span>-&gt;</span> <span>bool</span>:</span>
<span id="cb36-2">    <span>"""</span></span>
<span id="cb36-3"><span>    Tells python to compare .value when applying the `==`</span></span>
<span id="cb36-4"><span>    operation to two tensors instead of comparing references</span></span>
<span id="cb36-5"><span>    """</span></span>
<span id="cb36-6">    <span>if</span> <span>not</span> <span>isinstance</span>(other, <span>"Tensor"</span>):</span>
<span id="cb36-7">        <span>raise</span> <span>TypeError</span>(<span>f"Cannot compare a Tensor with a </span><span>{</span><span>type</span>(other)<span>}</span><span>"</span>)</span>
<span id="cb36-8"></span>
<span id="cb36-9">    <span>return</span> <span>self</span>.value <span>==</span> other.value</span></code></pre></div>
<p>Similarly, if we try to use <code>+</code>, <code>-</code> or <code>*</code> on our tensors, we’ll get an error. We can tell python how to do these operations on our tensors by defining the following special functions:</p>
<ul>
<li><code>__add__</code> let’s us use <code>+</code></li>
<li><code>__sub__</code> let’s us use <code>-</code></li>
<li><code>__mul__</code> let’s us use <code>*</code></li>
</ul>
<div id="20dbaf3a" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="26"><pre><code><span id="cb37-1"><span>def</span> <span>__add__</span>(<span>self</span>, other) <span>-&gt;</span> Tensor:</span>
<span id="cb37-2">    <span>if</span> <span>not</span> <span>isinstance</span>(other, <span>"Tensor"</span>):</span>
<span id="cb37-3">        <span>raise</span> <span>TypeError</span>(<span>f"Cannot add a Tensor to a </span><span>{</span><span>type</span>(other)<span>}</span><span>"</span>)</span>
<span id="cb37-4"></span>
<span id="cb37-5">    <span>return</span> _add(<span>self</span>, other)</span>
<span id="cb37-6"></span>
<span id="cb37-7"></span>
<span id="cb37-8"><span>def</span> <span>__sub__</span>(<span>self</span>, other) <span>-&gt;</span> Tensor:</span>
<span id="cb37-9">    <span>if</span> <span>not</span> <span>isinstance</span>(other, <span>"Tensor"</span>):</span>
<span id="cb37-10">        <span>raise</span> <span>TypeError</span>(<span>f"Cannot subtract a Tensor from a </span><span>{</span><span>type</span>(other)<span>}</span><span>"</span>)</span>
<span id="cb37-11"></span>
<span id="cb37-12">    <span>return</span> _sub(<span>self</span>, other)</span>
<span id="cb37-13"></span>
<span id="cb37-14"></span>
<span id="cb37-15"><span>def</span> <span>__mul__</span>(<span>self</span>, other) <span>-&gt;</span> Tensor:</span>
<span id="cb37-16">    <span>if</span> <span>not</span> <span>isinstance</span>(other, <span>"Tensor"</span>):</span>
<span id="cb37-17">        <span>raise</span> <span>TypeError</span>(<span>f"Cannot multiply a Tensor with a </span><span>{</span><span>type</span>(other)<span>}</span><span>"</span>)</span>
<span id="cb37-18">    <span>return</span> _mul(<span>self</span>, other)</span></code></pre></div>
<p>Finally, we can add the <code>__iadd__</code>, <code>__isub__</code> and <code>__imul__</code> methods to allow us to use <code>+=</code>, <code>-=</code> and <code>*=</code>.</p>
<div id="7f0b2199" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="27"><pre><code><span id="cb38-1"><span>def</span> <span>__iadd__</span>(<span>self</span>, other) <span>-&gt;</span> Tensor:</span>
<span id="cb38-2">    <span>self</span> <span>=</span> <span>self</span>.<span>__add__</span>(<span>self</span>, other)</span>
<span id="cb38-3">    <span>return</span> <span>self</span></span>
<span id="cb38-4"></span>
<span id="cb38-5"></span>
<span id="cb38-6"><span>def</span> <span>__isub__</span>(<span>self</span>, other) <span>-&gt;</span> Tensor:</span>
<span id="cb38-7">    <span>self</span> <span>=</span> <span>self</span>.<span>__sub__</span>(<span>self</span>, other)</span>
<span id="cb38-8">    <span>return</span> <span>self</span></span>
<span id="cb38-9"></span>
<span id="cb38-10"></span>
<span id="cb38-11"><span>def</span> <span>__imul__</span>(<span>self</span>, other) <span>-&gt;</span> Tensor:</span>
<span id="cb38-12">    <span>self</span> <span>=</span> <span>self</span>.<span>__mul__</span>(<span>self</span>, other)</span>
<span id="cb38-13">    <span>return</span> <span>self</span></span></code></pre></div>
<p>While we’re here, let’s clean up our backward function a bit by replacing the ugly <code>_add</code> and <code>_mul</code> operations with <code>+</code> and <code>*</code>.</p>
<div id="aee373de" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="28"><pre><code><span id="cb39-1"><span>def</span> backward(<span>self</span>):</span>
<span id="cb39-2">    <span>if</span> <span>self</span>.args <span>is</span> <span>None</span> <span>or</span> <span>self</span>.local_derivatives <span>is</span> <span>None</span>:</span>
<span id="cb39-3">        <span>raise</span> <span>ValueError</span>(</span>
<span id="cb39-4">            <span>"Cannot differentiate a Tensor that is not a function of other Tensors"</span></span>
<span id="cb39-5">        )</span>
<span id="cb39-6"></span>
<span id="cb39-7">    stack <span>=</span> [(<span>self</span>, Tensor(<span>1</span>))]</span>
<span id="cb39-8"></span>
<span id="cb39-9">    <span>while</span> stack:</span>
<span id="cb39-10">        node, current_derivative <span>=</span> stack.pop()</span>
<span id="cb39-11"></span>
<span id="cb39-12">        <span># if we have reached a parameter (it has no arguments</span></span>
<span id="cb39-13">        <span># because it wasn't created by an operation) then add the</span></span>
<span id="cb39-14">        <span># derivative</span></span>
<span id="cb39-15">        <span>if</span> <span>not</span> node.args:</span>
<span id="cb39-16">            <span>if</span> node.derivative <span>is</span> <span>None</span>:</span>
<span id="cb39-17">                node.derivative <span>+=</span> current_derivative</span>
<span id="cb39-18">            <span>else</span>:</span>
<span id="cb39-19">                node.derivative <span>+=</span> current_derivative</span>
<span id="cb39-20">            <span>continue</span></span>
<span id="cb39-21"></span>
<span id="cb39-22">        <span>for</span> arg, derivative <span>in</span> <span>zip</span>(node.args, node.local_derivatives):</span>
<span id="cb39-23">            stack.append((arg, current_derivative <span>*</span> derivative))</span></code></pre></div>
<p>Putting all of these improvements together, we get a final <code>Tensor</code> object as follows:</p>
<div id="2eae868b" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="29"><pre><code><span id="cb40-1"><span>class</span> Tensor:</span>
<span id="cb40-2">    <span>"""</span></span>
<span id="cb40-3"><span>    A float that can be differentiated</span></span>
<span id="cb40-4"><span>    """</span></span>
<span id="cb40-5"></span>
<span id="cb40-6">    args: <span>tuple</span>[Tensor] <span>=</span> ()</span>
<span id="cb40-7">    local_derivatives: <span>tuple</span>[Tensor] <span>=</span> ()</span>
<span id="cb40-8">    <span># The derivative (once we've calculated it).  This is None if the derivative</span></span>
<span id="cb40-9">    <span># has not been computed yet</span></span>
<span id="cb40-10">    derivative: Tensor <span>|</span> <span>None</span> <span>=</span> <span>None</span></span>
<span id="cb40-11"></span>
<span id="cb40-12">    <span>def</span> <span>__init__</span>(<span>self</span>, value: <span>float</span>):</span>
<span id="cb40-13">        <span>self</span>.value <span>=</span> value</span>
<span id="cb40-14"></span>
<span id="cb40-15">    <span>def</span> <span>__repr__</span>(<span>self</span>) <span>-&gt;</span> <span>str</span>:</span>
<span id="cb40-16">        <span>return</span> <span>f"Tensor(</span><span>{</span><span>self</span><span>.</span>value<span>.</span><span>__repr__</span>()<span>}</span><span>)"</span></span>
<span id="cb40-17"></span>
<span id="cb40-18">    <span>def</span> <span>__eq__</span>(<span>self</span>, other) <span>-&gt;</span> <span>bool</span>:</span>
<span id="cb40-19">        <span>if</span> <span>not</span> <span>isinstance</span>(other, Tensor):</span>
<span id="cb40-20">            <span>raise</span> <span>TypeError</span>(<span>f"Cannot compare a Tensor with a </span><span>{</span><span>type</span>(other)<span>}</span><span>"</span>)</span>
<span id="cb40-21">        <span>return</span> <span>self</span>.value <span>==</span> other.value</span>
<span id="cb40-22"></span>
<span id="cb40-23">    <span>def</span> <span>__add__</span>(<span>self</span>, other) <span>-&gt;</span> Tensor:</span>
<span id="cb40-24">        <span>if</span> <span>not</span> <span>isinstance</span>(other, Tensor):</span>
<span id="cb40-25">            <span>raise</span> <span>TypeError</span>(<span>f"Cannot add a Tensor to a </span><span>{</span><span>type</span>(other)<span>}</span><span>"</span>)</span>
<span id="cb40-26">        <span>return</span> _add(<span>self</span>, other)</span>
<span id="cb40-27"></span>
<span id="cb40-28">    <span>def</span> <span>__sub__</span>(<span>self</span>, other) <span>-&gt;</span> Tensor:</span>
<span id="cb40-29">        <span>if</span> <span>not</span> <span>isinstance</span>(other, Tensor):</span>
<span id="cb40-30">            <span>raise</span> <span>TypeError</span>(<span>f"Cannot subtract a Tensor from a </span><span>{</span><span>type</span>(other)<span>}</span><span>"</span>)</span>
<span id="cb40-31">        <span>return</span> _sub(<span>self</span>, other)</span>
<span id="cb40-32"></span>
<span id="cb40-33">    <span>def</span> <span>__mul__</span>(<span>self</span>, other) <span>-&gt;</span> Tensor:</span>
<span id="cb40-34">        <span>if</span> <span>not</span> <span>isinstance</span>(other, Tensor):</span>
<span id="cb40-35">            <span>raise</span> <span>TypeError</span>(<span>f"Cannot multiply a Tensor with a </span><span>{</span><span>type</span>(other)<span>}</span><span>"</span>)</span>
<span id="cb40-36">        <span>return</span> _mul(<span>self</span>, other)</span>
<span id="cb40-37"></span>
<span id="cb40-38">    <span>def</span> <span>__iadd__</span>(<span>self</span>, other) <span>-&gt;</span> Tensor:</span>
<span id="cb40-39">        <span>return</span> <span>self</span>.<span>__add__</span>(other)</span>
<span id="cb40-40"></span>
<span id="cb40-41">    <span>def</span> <span>__isub__</span>(<span>self</span>, other) <span>-&gt;</span> Tensor:</span>
<span id="cb40-42">        <span>return</span> <span>self</span>.<span>__sub__</span>(other)</span>
<span id="cb40-43"></span>
<span id="cb40-44">    <span>def</span> <span>__imul__</span>(<span>self</span>, other) <span>-&gt;</span> Tensor:</span>
<span id="cb40-45">        <span>return</span> <span>self</span>.<span>__mul__</span>(other)</span>
<span id="cb40-46"></span>
<span id="cb40-47">    <span>def</span> <span>__repr__</span>(<span>self</span>) <span>-&gt;</span> <span>str</span>:</span>
<span id="cb40-48">        <span>return</span> <span>f"Tensor(</span><span>{</span><span>self</span><span>.</span>value<span>}</span><span>)"</span></span>
<span id="cb40-49"></span>
<span id="cb40-50">    <span>def</span> backward(<span>self</span>):</span>
<span id="cb40-51">        <span>if</span> <span>self</span>.args <span>is</span> <span>None</span> <span>or</span> <span>self</span>.local_derivatives <span>is</span> <span>None</span>:</span>
<span id="cb40-52">            <span>raise</span> <span>ValueError</span>(</span>
<span id="cb40-53">                <span>"Cannot differentiate a Tensor that is not a function of other Tensors"</span></span>
<span id="cb40-54">            )</span>
<span id="cb40-55"></span>
<span id="cb40-56">        stack <span>=</span> [(<span>self</span>, Tensor(<span>1</span>))]</span>
<span id="cb40-57"></span>
<span id="cb40-58">        <span>while</span> stack:</span>
<span id="cb40-59">            node, current_derivative <span>=</span> stack.pop()</span>
<span id="cb40-60"></span>
<span id="cb40-61">            <span># if we have reached a parameter (it has no arguments</span></span>
<span id="cb40-62">            <span># because it wasn't created by an operation) then add the</span></span>
<span id="cb40-63">            <span># current_derivative to derivative</span></span>
<span id="cb40-64">            <span>if</span> <span>not</span> node.args:</span>
<span id="cb40-65">                <span>if</span> node.derivative <span>is</span> <span>None</span>:</span>
<span id="cb40-66">                    node.derivative <span>=</span> current_derivative</span>
<span id="cb40-67">                <span>else</span>:</span>
<span id="cb40-68">                    node.derivative <span>+=</span> current_derivative</span>
<span id="cb40-69">                <span>continue</span></span>
<span id="cb40-70"></span>
<span id="cb40-71">            <span>for</span> arg, derivative <span>in</span> <span>zip</span>(node.args, node.local_derivatives):</span>
<span id="cb40-72">                stack.append((arg, current_derivative <span>*</span> derivative))</span></code></pre></div>
<p>Let’s take it for a spin. We’ll try calculating <span>\(L\)</span> again</p>
<div id="db25982d" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="30">
<div id="cb41"><pre><code><span id="cb41-1">y <span>=</span> Tensor(<span>1</span>)</span>
<span id="cb41-2">m <span>=</span> Tensor(<span>2</span>)</span>
<span id="cb41-3">x <span>=</span> Tensor(<span>3</span>)</span>
<span id="cb41-4">c <span>=</span> Tensor(<span>4</span>)</span>
<span id="cb41-5"></span>
<span id="cb41-6">diff <span>=</span> y <span>-</span> ((m <span>*</span> x) <span>+</span> c)</span>
<span id="cb41-7">L <span>=</span> diff <span>*</span> diff</span>
<span id="cb41-8">L.backward()</span>
<span id="cb41-9"></span>
<span id="cb41-10">test(got<span>=</span>x.derivative, want<span>=</span>Tensor(<span>36</span>))</span></code></pre></div>
<div>
<pre><code>✅ - Want: Tensor(36), Got: Tensor(36)</code></pre>
</div>
</div>
<p>Much easier!</p>
<p>To really see what this baby can do, I asked a language model for the most complicated expression it could think of and it gave me this:</p>
<p><span>\[f(x) = (2x^3 + 4x^2 - 5x) \times (3x^2 - 2x + 7) - (6x^4 + 2x^3 - 8x^2) + (5x^2 - 3x)\]</span> According to <a href="https://www.wolframalpha.com/">Wolfram Alpha</a>, the derivative of this expression is: <span>\[\frac{d f(x)}{dx} = -38 + 102 x - 33 x^2 + 8 x^3 + 30 x^4\]</span></p>
<p>If we plug 2 into this equation, the answer is apparently 578 (again, thanks to <a href="https://www.wolframalpha.com/">Wolfram Alpha</a>).</p>
<p>Let’s try it with our algorithm</p>
<div id="0fd9b01f" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="31">
<div id="cb43"><pre><code><span id="cb43-1">x <span>=</span> Tensor(<span>2</span>)</span>
<span id="cb43-2"></span>
<span id="cb43-3">y <span>=</span> (</span>
<span id="cb43-4">    (Tensor(<span>2</span>) <span>*</span> x <span>*</span> x <span>*</span> x <span>+</span> Tensor(<span>4</span>) <span>*</span> x <span>*</span> x <span>-</span> Tensor(<span>5</span>) <span>*</span> x)</span>
<span id="cb43-5">    <span>*</span> (Tensor(<span>3</span>) <span>*</span> x <span>*</span> x <span>-</span> Tensor(<span>2</span>) <span>*</span> x <span>+</span> Tensor(<span>7</span>))</span>
<span id="cb43-6">    <span>-</span> (Tensor(<span>6</span>) <span>*</span> x <span>*</span> x <span>*</span> x <span>*</span> x <span>+</span> Tensor(<span>2</span>) <span>*</span> x <span>*</span> x <span>*</span> x <span>-</span> Tensor(<span>8</span>) <span>*</span> x <span>*</span> x)</span>
<span id="cb43-7">    <span>+</span> (Tensor(<span>5</span>) <span>*</span> x <span>*</span> x <span>-</span> Tensor(<span>3</span>) <span>*</span> x)</span>
<span id="cb43-8">)</span>
<span id="cb43-9"></span>
<span id="cb43-10">y.backward()</span>
<span id="cb43-11"></span>
<span id="cb43-12">test(got<span>=</span>x.derivative, want<span>=</span>Tensor(<span>578</span>))</span></code></pre></div>
<div>
<pre><code>✅ - Want: Tensor(578), Got: Tensor(578)</code></pre>
</div>
</div>
<p>Once again, we got the right answer!</p>
</section>
<section id="conclusion">
<h2>Conclusion</h2>
<p>From nothing, we have now written an algorithm that will let us differentiate any mathematical expression (provided it only involves addition, subtraction and multiplication). We did this by converting our expression into a graph and re-imagining partial derivatives as operations on the edges of that graph. Then we found that we could apply Breadth First Search to combine all the derivatives together to get a final answer.</p>
<p>Differentiating scalars is (I hope you agree) interesting, but it isn’t exactly GPT-4. That said, with a few small modifications to our algorithm, we can extend our algorithm to handle multi-dimensional tensors like matrices and vectors. Once you can do that, you can build up to backpropagation and, eventually, to a fully functional language model.</p>
<p>Next time, we’ll extend our algorithm to vectors and matrices and build up from there to a working neural network. If you want to peek ahead, you can check out the repo for <a href="https://github.com/bclarkson-code/Tricycle">Tricycle</a> which is the name for the deep learning framework we’re building.</p>


</section>

</main> <!-- /main -->


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Uv: Python packaging in Rust (592 pts)]]></title>
            <link>https://astral.sh/blog/uv</link>
            <guid>39387641</guid>
            <pubDate>Thu, 15 Feb 2024 19:50:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://astral.sh/blog/uv">https://astral.sh/blog/uv</a>, See on <a href="https://news.ycombinator.com/item?id=39387641">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p><strong>TL;DR:</strong> <a href="https://github.com/astral-sh/uv">uv</a> is an <strong>extremely fast Python package
installer and resolver</strong>, written in Rust, and designed as a drop-in replacement for <code>pip</code> and
<code>pip-tools</code> workflows.</p>
<p><a href="https://github.com/astral-sh/uv">uv</a> represents a milestone in our pursuit of a <a href="https://blog.rust-lang.org/2016/05/05/cargo-pillars.html#pillars-of-cargo">"Cargo for Python"</a>:
a comprehensive Python project and package manager that's fast, reliable, and easy to use.</p>
<p>As part of this release, we're also taking stewardship of <a href="https://github.com/mitsuhiko/rye">Rye</a>,
an experimental Python packaging tool from <a href="https://github.com/mitsuhiko">Armin Ronacher</a>. We'll
maintain <a href="https://github.com/mitsuhiko/rye">Rye</a> as we expand <a href="https://github.com/astral-sh/uv">uv</a> into a unified successor
project, to fulfill our <a href="https://rye-up.com/philosophy/">shared vision</a> for Python packaging.</p>
<hr>
<p>At Astral, we build high-performance developer tools for the Python ecosystem. We're best known
for <a href="https://github.com/astral-sh/ruff">Ruff</a>, an extremely fast
Python <a href="https://notes.crmarsh.com/python-tooling-could-be-much-much-faster">linter</a>
and <a href="https://astral.sh/blog/the-ruff-formatter">formatter</a>.</p>
<p>Today, we're releasing the next tool in the Astral toolchain: <strong><a href="https://github.com/astral-sh/uv">uv</a>, an extremely fast Python
package resolver and installer, written in Rust</strong>.</p>
<div><p><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 422 250"><g aria-roledescription="group mark container" fill="none" stroke-miterlimit="10"><g aria-roledescription="group mark container"><g aria-roledescription="axis" aria-label="X-axis for a linear scale with values from 0.0 to 3.5"><g pointer-events="none"><path stroke="rgba(127,127,127,0.25)" d="M347.5 90.5"></path></g><g pointer-events="none"><text text-anchor="middle" transform="translate(90.5 105.5)" font-family="Roboto Mono,monospace" font-size="12">0s
                        </text><text text-anchor="middle" transform="translate(176.214 105.5)" font-family="Roboto Mono,monospace" font-size="12">1s
                        </text><text text-anchor="middle" transform="translate(261.929 105.5)" font-family="Roboto Mono,monospace" font-size="12">2s
                        </text><text text-anchor="middle" transform="translate(347.643 105.5)" font-family="Roboto Mono,monospace" font-size="12">3s
                        </text></g></g><g aria-roledescription="axis" aria-label="Y-axis for a discrete scale with 4 values: uv, poetry, pip-compile, pdm"><g pointer-events="none"><text text-anchor="end" transform="translate(80.5 15.25)" font-family="Roboto Mono,monospace" font-size="12" font-weight="bold">uv
                        </text><text text-anchor="end" transform="translate(80.5 37.75)" font-family="Roboto Mono,monospace" font-size="12">poetry
                        </text><text text-anchor="end" transform="translate(80.5 60.25)" font-family="Roboto Mono,monospace" font-size="12">pip-compile
                        </text><text text-anchor="end" transform="translate(80.5 82.75)" font-family="Roboto Mono,monospace" font-size="12">pdm
                        </text></g></g><g aria-roledescription="rect mark container"><path aria-label="Sum of time: 0.0134756369786; tool: uv" aria-roledescription="bar" d="M90 4.75h1.155v13H90Z"></path><path aria-label="Sum of time: 0.60278702674; tool: poetry" aria-roledescription="bar" d="M90 27.25h51.667v13H90Z"></path><path aria-label="Sum of time: 1.55616658094; tool: pip-compile" aria-roledescription="bar" d="M90 49.75h133.386v13H90Z"></path><path aria-label="Sum of time: 3.37404433084; tool: pdm" aria-roledescription="bar" d="M90 72.25h289.204v13H90Z"></path></g><g aria-roledescription="text mark container"><text aria-label="Sum of time: 0.60278702674; tool: poetry; timeFormat: 0.60s" aria-roledescription="text mark" transform="translate(147.667 37.75)" font-family="Roboto Mono,monospace" font-size="12">0.60s
                  </text><text aria-label="Sum of time: 1.55616658094; tool: pip-compile; timeFormat: 1.56s" aria-roledescription="text mark" transform="translate(229.386 60.25)" font-family="Roboto Mono,monospace" font-size="12">1.56s
                  </text><text aria-label="Sum of time: 3.37404433084; tool: pdm; timeFormat: 3.37s" aria-roledescription="text mark" transform="translate(385.204 82.75)" font-family="Roboto Mono,monospace" font-size="12">3.37s
                  </text></g><g aria-roledescription="text mark container"><text aria-label="Sum of time: 0.0134756369786; tool: uv; timeFormat: 0.01s" aria-roledescription="text mark" transform="translate(97.155 15.25)" font-family="Roboto Mono,monospace" font-size="12" font-weight="bold">0.01s
                  </text></g><g aria-roledescription="axis" aria-label="X-axis for a linear scale with values from 0 to 5"><g pointer-events="none"><path stroke="rgba(127,127,127,0.25)" d="M330.5 233.5"></path></g><g pointer-events="none"><text text-anchor="middle" transform="translate(90.5 248.5)" font-family="Roboto Mono,monospace" font-size="12">0s
                        </text><text text-anchor="middle" transform="translate(210.5 248.5)" font-family="Roboto Mono,monospace" font-size="12">2s
                        </text><text text-anchor="middle" transform="translate(330.5 248.5)" font-family="Roboto Mono,monospace" font-size="12">4s
                        </text></g></g><g aria-roledescription="axis" aria-label="Y-axis for a discrete scale with 4 values: uv, poetry, pdm, pip-sync"><g pointer-events="none"><text text-anchor="end" transform="translate(80.5 158.25)" font-family="Roboto Mono,monospace" font-size="12" font-weight="bold">uv
                        </text><text text-anchor="end" transform="translate(80.5 180.75)" font-family="Roboto Mono,monospace" font-size="12">poetry
                        </text><text text-anchor="end" transform="translate(80.5 203.25)" font-family="Roboto Mono,monospace" font-size="12">pdm
                        </text><text text-anchor="end" transform="translate(80.5 225.75)" font-family="Roboto Mono,monospace" font-size="12">pip-sync
                        </text></g></g><g aria-roledescription="rect mark container"><path aria-label="Sum of time: 0.0576289908; tool: uv" aria-roledescription="bar" d="M90 147.75h3.458v13H90Z"></path><path aria-label="Sum of time: 0.9872183659; tool: poetry" aria-roledescription="bar" d="M90 170.25h59.233v13H90Z"></path><path aria-label="Sum of time: 1.8969612492; tool: pdm" aria-roledescription="bar" d="M90 192.75h113.818v13H90Z"></path><path aria-label="Sum of time: 4.6313483826; tool: pip-sync" aria-roledescription="bar" d="M90 215.25h277.88v13H90Z"></path></g><g aria-roledescription="text mark container"><text aria-label="Sum of time: 0.9872183659; tool: poetry; timeFormat: 0.99s" aria-roledescription="text mark" transform="translate(155.233 180.75)" font-family="Roboto Mono,monospace" font-size="12">0.99s
                  </text><text aria-label="Sum of time: 1.8969612492; tool: pdm; timeFormat: 1.90s" aria-roledescription="text mark" transform="translate(209.818 203.25)" font-family="Roboto Mono,monospace" font-size="12">1.90s
                  </text><text aria-label="Sum of time: 4.6313483826; tool: pip-sync; timeFormat: 4.63s" aria-roledescription="text mark" transform="translate(373.88 225.75)" font-family="Roboto Mono,monospace" font-size="12">4.63s
                  </text></g><g aria-roledescription="text mark container"><text aria-label="Sum of time: 0.0576289908; tool: uv; timeFormat: 0.06s" aria-roledescription="text mark" transform="translate(99.458 158.25)" font-family="Roboto Mono,monospace" font-size="12" font-weight="bold">0.06s
                  </text></g></g></g></svg></p><div><p><span>Resolving (left) and installing (right) the<!-- --> <a target="_blank" rel="noreferrer" href="https://github.com/python-trio/trio">Trio</a> <!-- -->dependencies with a<!-- --> </span><a aria-label="Toggle cache" tabindex="0" type="button">warm</a> <span>cache, to simulate<!-- --> <!-- -->recreating a virtual environment or adding a dependency to an existing project<!-- --> <!-- -->(<a href="https://github.com/astral-sh/uv/blob/main/BENCHMARKS.md" target="_blank" rel="noreferrer">source</a>).</span></p><p><span>Resolving (top) and installing (bottom) the<!-- --> <a target="_blank" rel="noreferrer" href="https://github.com/python-trio/trio">Trio</a> <!-- -->dependencies with a<!-- --> </span><a aria-label="Toggle cache" tabindex="0" type="button">warm</a> <span>cache, to simulate<!-- --> <!-- -->recreating a virtual environment or adding a dependency to an existing project<!-- --> <!-- -->(<a href="https://github.com/astral-sh/uv/blob/main/BENCHMARKS.md" target="_blank" rel="noreferrer">source</a>).</span></p></div></div>
<p><a href="https://github.com/astral-sh/uv">uv</a> is designed as a drop-in replacement for <code>pip</code> and <code>pip-tools</code>,
and is ready for production use today in projects built around those workflows.</p>
<p>Like <a href="https://github.com/astral-sh/ruff">Ruff</a>, uv's implementation was grounded in our core
product principles:</p>
<ol>
<li><strong>An obsessive focus on performance.</strong> In the above <a href="https://github.com/astral-sh/uv/blob/main/BENCHMARKS.md">benchmarks</a>,
uv is <strong>8-10x faster</strong> than <code>pip</code> and <code>pip-tools</code> without caching, and <strong>80-115x faster</strong>
when running with a warm cache (e.g., recreating a virtual environment or updating a dependency).
uv uses a global module cache to avoid re-downloading and re-building dependencies, and
leverages Copy-on-Write and hardlinks on supported filesystems to minimize disk space usage.</li>
<li><strong>Optimized for adoption.</strong> While we have big aspirations for the future of Python packaging,
uv's initial release is centered on supporting the <code>pip</code> and <code>pip-tools</code> APIs behind
our <code>uv pip</code> interface, making it usable by existing projects with zero configuration.
Similarly, uv can be used as "just" a resolver (<code>uv pip compile</code> to lock your
dependencies), "just" a virtual environment creator (<code>uv venv</code>), "just" a package
installer (<code>uv pip sync</code>), and so on. It's both unified and modular.</li>
<li><strong>A simplified toolchain.</strong> uv ships as a single static binary capable of
replacing <code>pip</code>, <code>pip-tools</code>, and <code>virtualenv</code>. uv has no direct Python dependency, so you
can install it separately from Python itself, avoiding the need to manage <code>pip</code> installations
across multiple Python versions (e.g., <code>pip</code> vs. <code>pip3</code> vs. <code>pip3.7</code>).</li>
</ol>
<p>While uv will evolve into a <strong>complete Python project and package manager</strong> (a <a href="https://blog.rust-lang.org/2016/05/05/cargo-pillars.html#pillars-of-cargo">"Cargo for Python"</a>),
the narrower <code>pip-tools</code> scope allows us to solve the low-level problems involved in building such
a tool (like package installation) while shipping something immediately useful with minimal barrier
to adoption.</p>
<p>You can install <a href="https://github.com/astral-sh/uv">uv</a> today via our standalone installers,
or from <a href="https://pypi.org/project/uv/">PyPI</a>.</p>

<p><a href="https://github.com/astral-sh/uv">uv</a> supports everything you'd expect from a modern Python
packaging tool: editable installs, Git dependencies, URL dependencies, local dependencies,
constraint files, source distributions, custom indexes, and more, all designed around drop-in
compatibility with your existing tools.</p>
<p><a href="https://github.com/astral-sh/uv">uv</a> supports <strong>Linux</strong>, <strong>Windows</strong>, and <strong>macOS</strong>, and
has been tested at-scale against the public PyPI index.</p>
<h3><span id="a-drop-in-compatible-api"></span>A drop-in compatible API<!-- --> <a href="#a-drop-in-compatible-api">#</a></h3>
<p>This initial release centers on what we refer to as uv's <code>pip</code> API. It'll be familiar to those
that have used <code>pip</code> and <code>pip-tools</code> in the past:</p>
<ul>
<li>Instead of <code>pip install</code>, run <code>uv pip install</code> to install Python dependencies from the command
line, a requirements file, or a <code>pyproject.toml</code>.</li>
<li>Instead of <code>pip-compile</code>, run <code>uv pip compile</code> to generate a locked <code>requirements.txt</code>.</li>
<li>Instead of <code>pip-sync</code>, run <code>uv pip sync</code> to sync a virtual environment with a locked <code>requirements.txt</code>.</li>
</ul>
<p>By scoping these "lower-level" commands under <code>uv pip</code>, we retain space in the CLI for the more
"opinionated" project management API we intend to ship in the future, which will look more like
<a href="https://github.com/mitsuhiko/rye">Rye</a>, or <a href="https://github.com/rust-lang/cargo">Cargo</a>, or
<a href="https://github.com/python-poetry/poetry">Poetry</a>. (Imagine <code>uv run</code>, <code>uv build</code>, and so on.)</p>
<p>uv can also be used as a virtual environment manager via <code>uv venv</code>. It's about 80x
faster than <code>python -m venv</code> and 7x faster than <code>virtualenv</code>, with no dependency on Python.</p>
<div><p><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 454 282"><g aria-roledescription="group mark container" fill="none" stroke-miterlimit="10"><g aria-roledescription="group mark container"><g aria-roledescription="axis" aria-label="Y-axis for a discrete scale with 3 values: uv, virtualenv, venv"><g pointer-events="none"><text text-anchor="end" transform="translate(89.5 35)" font-family="Roboto Mono,monospace" font-size="12" font-weight="bold">uv
                        </text><text text-anchor="end" transform="translate(89.5 65)" font-family="Roboto Mono,monospace" font-size="12">virtualenv
                        </text><text text-anchor="end" transform="translate(89.5 95)" font-family="Roboto Mono,monospace" font-size="12">venv
                        </text></g></g><g aria-roledescription="axis" aria-label="Y-axis for a discrete scale with 3 values: uv, virtualenv, venv"><g pointer-events="none"><text text-anchor="end" transform="translate(89.5 178)" font-family="Roboto Mono,monospace" font-size="12" font-weight="bold">uv
                        </text><text text-anchor="end" transform="translate(89.5 208)" font-family="Roboto Mono,monospace" font-size="12">virtualenv
                        </text><text text-anchor="end" transform="translate(89.5 238)" font-family="Roboto Mono,monospace" font-size="12">venv
                        </text></g></g></g><g aria-roledescription="group mark container"><g aria-roledescription="axis" aria-label="X-axis for a linear scale with values from 0.00 to 0.08"><g pointer-events="none"><path stroke="rgba(127,127,127,0.25)" d="M407.5 106.5"></path></g><g pointer-events="none"><text text-anchor="middle" transform="translate(107.5 121.5)" font-family="Roboto Mono,monospace" font-size="12">0s
                        </text><text text-anchor="middle" transform="translate(182.5 121.5)" font-family="Roboto Mono,monospace" font-size="12">0.02s
                        </text><text text-anchor="middle" transform="translate(257.5 121.5)" font-family="Roboto Mono,monospace" font-size="12">0.04s
                        </text><text text-anchor="middle" transform="translate(332.5 121.5)" font-family="Roboto Mono,monospace" font-size="12">0.06s
                        </text><text text-anchor="middle" transform="translate(407.5 121.5)" font-family="Roboto Mono,monospace" font-size="12">0.08s
                        </text></g></g><g aria-roledescription="rect mark container"><path aria-label="Sum of time: 0.0041; tool: uv" aria-roledescription="bar" d="M107 24.5h15.375v13H107Z"></path><path aria-label="Sum of time: 0.0744; tool: virtualenv" aria-roledescription="bar" d="M107 54.5h279v13H107Z"></path><path aria-label="Sum of time: 0.0241; tool: venv" aria-roledescription="bar" d="M107 84.5h90.375v13H107Z"></path></g><g aria-roledescription="text mark container"><text aria-label="Sum of time: 0.0744; tool: virtualenv; timeFormat: 74.4ms" aria-roledescription="text mark" transform="translate(392 65)" font-family="Roboto Mono,monospace" font-size="12">74.4ms
                  </text><text aria-label="Sum of time: 0.0241; tool: venv; timeFormat: 24.1ms" aria-roledescription="text mark" transform="translate(203.375 95)" font-family="Roboto Mono,monospace" font-size="12">24.1ms
                  </text></g><g aria-roledescription="text mark container"><text aria-label="Sum of time: 0.0041; tool: uv; timeFormat: 4.1ms" aria-roledescription="text mark" transform="translate(128.375 35)" font-family="Roboto Mono,monospace" font-size="12" font-weight="bold">4.1ms
                  </text></g><g aria-roledescription="axis" aria-label="X-axis for a linear scale with values from 0.0 to 1.6"><g pointer-events="none"><path stroke="rgba(127,127,127,0.25)" d="M388.5 249.5"></path></g><g pointer-events="none"><text text-anchor="middle" transform="translate(107.5 264.5)" font-family="Roboto Mono,monospace" font-size="12">0s
                        </text><text text-anchor="middle" transform="translate(201.25 264.5)" font-family="Roboto Mono,monospace" font-size="12">0.5s
                        </text><text text-anchor="middle" transform="translate(295 264.5)" font-family="Roboto Mono,monospace" font-size="12">1s
                        </text><text text-anchor="middle" transform="translate(388.75 264.5)" font-family="Roboto Mono,monospace" font-size="12">1.5s
                        </text></g></g><g aria-roledescription="rect mark container"><path aria-label="Sum of time: 0.0182; tool: uv" aria-roledescription="bar" d="M107 167.5h3.413v13H107Z"></path><path aria-label="Sum of time: 0.1414; tool: virtualenv" aria-roledescription="bar" d="M107 197.5h26.512v13H107Z"></path><path aria-label="Sum of time: 1.54; tool: venv" aria-roledescription="bar" d="M107 227.5h288.75v13H107Z"></path></g><g aria-roledescription="text mark container"><text aria-label="Sum of time: 0.1414; tool: virtualenv; timeFormat: 141.4ms" aria-roledescription="text mark" transform="translate(139.512 208)" font-family="Roboto Mono,monospace" font-size="12">141.4ms
                  </text><text aria-label="Sum of time: 1.54; tool: venv; timeFormat: 1.54s" aria-roledescription="text mark" transform="translate(401.75 238)" font-family="Roboto Mono,monospace" font-size="12">1.54s
                  </text></g><g aria-roledescription="text mark container"><text aria-label="Sum of time: 0.0182; tool: uv; timeFormat: 18.2ms" aria-roledescription="text mark" transform="translate(116.412 178)" font-family="Roboto Mono,monospace" font-size="12" font-weight="bold">18.2ms
                  </text></g></g></g></svg></p><div><p>Creating a virtual environment, with (top) and without (bottom) seed packages like pip and setuptools (<a href="https://github.com/astral-sh/uv/blob/ea13d94c57149a8fc6ebfcef46149252e869269f/scripts/benchmarks/venv.sh" target="_blank" rel="noreferrer">source</a>).</p><p>Creating a virtual environment, with (left) and without (right) seed packages like pip and setuptools (<a href="https://github.com/astral-sh/uv/blob/ea13d94c57149a8fc6ebfcef46149252e869269f/scripts/benchmarks/venv.sh" target="_blank" rel="noreferrer">source</a>).</p></div></div>
<p>uv's virtual environments are standards-compliant and work interchangeably with other tools —
there's no lock-in or customization.</p>
<p>Building our own package management stack from scratch also opened up room for new capabilities.
For example:</p>
<ul>
<li><strong>uv supports alternate resolution strategies.</strong> By default, uv follows the standard
Python dependency resolution strategy of preferring the latest compatible version of each package.
But by passing <code>--resolution=lowest</code>, library authors can test their packages against the lowest-compatible version of their dependencies. (This is similar to Go's
<a href="https://go.dev/ref/mod#minimal-version-selection">Minimal version selection</a>.)</li>
<li><strong>uv allows for resolutions against arbitrary target Python versions.</strong> While <code>pip</code>
and <code>pip-tools</code> always resolve against the currently-installed Python version (generating, e.g., a
Python 3.12-compatible resolution when running under Python 3.12), uv accepts
a <code>--python-version</code> parameter, enabling you to generate, e.g., Python 3.7-compatible resolutions
even when running under newer versions.</li>
<li><strong>uv allows for dependency “overrides”.</strong> uv takes pip's “constraints” concepts a step
further via overrides (<code>-o overrides.txt</code>), which allow the user to guide the resolver by
overriding the declared dependencies of a package. Overrides give the user an escape hatch for
working around erroneous upper bounds and other incorrectly-declared dependencies.</li>
</ul>
<p>In its current form, uv won't be the right fit for all projects. <code>pip</code> is a mature and stable
tool, with extensive support for an extremely wide range of use cases and a focus on compatibility.
While uv supports a large fraction of the <code>pip</code> interface, it lacks support for some of its
legacy features, like <code>.egg</code> distributions.</p>
<p>Similarly, uv does not yet generate a platform-agnostic lockfile. This matches <code>pip-tools</code>, but
differs from Poetry and PDM, making uv a better fit for projects built around the <code>pip</code> and
<code>pip-tools</code> workflows.</p>
<p>For those deep in the packaging ecosystem, uv also includes standards-compliant Rust
implementations of <a href="https://peps.python.org/pep-0440/">PEP 440</a> (version identifiers),
<a href="https://peps.python.org/pep-0508/">PEP 508</a> (dependency specifiers),
<a href="https://peps.python.org/pep-0517/">PEP 517</a> (a build-system independent build frontend),
<a href="https://peps.python.org/pep-0405/">PEP 405</a> (virtual environments), and more.</p>
<h3><span id="a-cargo-for-python-uv-and-rye"></span>A "Cargo for Python": uv and Rye<!-- --> <a href="#a-cargo-for-python-uv-and-rye">#</a></h3>
<p>uv represents an intermediary milestone in our pursuit of a <a href="https://blog.rust-lang.org/2016/05/05/cargo-pillars.html#pillars-of-cargo">"Cargo for Python"</a>: a unified Python
package and project manager that is extremely fast, reliable, and easy to use.</p>
<p>Think: a single binary that bootstraps your Python installation and gives you everything you need to
be productive with Python, bundling not only <code>pip</code>, <code>pip-tools</code>, and <code>virtualenv</code>, but also <code>pipx</code>,
<code>tox</code>, <code>poetry</code>, <code>pyenv</code>, <code>ruff</code>, and more.</p>
<p>Python tooling can be a low-confidence experience: it's a significant amount of work to stand up a
new or existing project, and commands fail in confusing ways. In contrast, when working in the Rust
ecosystem, you trust the tools to succeed. The Astral toolchain is about bringing Python from a
low-confidence to a high-confidence experience.</p>
<p>This vision for Python packaging is not far off from that put forward by <a href="https://github.com/mitsuhiko/rye">Rye</a>,
an experimental project and package management tool from <a href="https://github.com/mitsuhiko">Armin Ronacher</a>.</p>
<p>In talking with Armin, it was clear that our visions were closely aligned, but that fulfilling
them would require a significant investment in foundational tooling. For example: building such a
tool requires an extremely fast, end-to-end integrated, cross-platform resolver and installer. <strong>In
uv, we've built that foundational tooling.</strong></p>
<p>We saw this as a rare opportunity to team up, and to avoid fragmenting the Python ecosystem.
<strong>As such, in collaboration with Armin, we're excited to be taking over <a href="https://github.com/mitsuhiko/rye">Rye</a>.</strong>
Our goal is to evolve uv into a production-ready <a href="https://blog.rust-lang.org/2016/05/05/cargo-pillars.html#pillars-of-cargo">"Cargo for Python"</a>, and to provide a smooth
migration path from Rye to uv when the time is right.</p>
<p>Until then, we'll be maintaining Rye, migrating it to use uv under-the-hood, and, more
generally, treating it as an experimental testbed for the end-user experience we're building
towards.</p>
<p>While merging projects comes with its own challenges, we're committed to building a single, unified
tool under the Astral banner, and to supporting existing Rye users as we evolve uv into a
suitable and comprehensive successor project.</p>
<h3><span id="our-roadmap"></span>Our Roadmap<!-- --> <a href="#our-roadmap">#</a></h3>
<p>Following this release, our first priority is to support users as they consider <a href="https://github.com/astral-sh/uv">uv</a>,
with a focus on improving compatibility, performance, and stability across platforms.</p>
<p>From there, we'll look towards expanding uv into a complete Python project and package manager:
a single binary that gives you everything you need to be productive with Python.</p>
<p>We have an ambitious roadmap for uv. But even in its current form, I think it will
feel like a very different experience for Python. I hope you'll give it a try.</p>
<h3><span id="acknowledgements"></span>Acknowledgements<!-- --> <a href="#acknowledgements">#</a></h3>
<p>Finally, we'd like to thank all those that contributed directly or indirectly to the development of
uv. Foremost among them are <a href="https://github.com/Eh2406">Jacob Finkelman</a>
and <a href="https://github.com/mpizenberg">Matthieu Pizenberg</a>, the maintainers
of <a href="https://github.com/pubgrub-rs/pubgrub">pubgrub-rs</a>. uv uses PubGrub as its underlying
version solver, and we're grateful to Jacob and Matthieu for the work they put into PubGrub in the
past, and for the way they've engaged with us as collaborators throughout the project.</p>
<p>We'd also like to thank those projects in the packaging space that've inspired us,
especially <a href="https://github.com/rust-lang/cargo">Cargo</a>, along with <a href="https://github.com/oven-sh/bun">Bun</a>, <a href="https://github.com/orogene/orogene">Orogene</a>,
and <a href="https://github.com/pnpm/pnpm">pnpm</a> from the JavaScript ecosystem,
and <a href="https://github.com/njsmith/posy">Posy</a>, <a href="https://github.com/konstin/monotrail-resolve">Monotrail</a>,
and <a href="https://github.com/mitsuhiko/rye">Rye</a> from the Python ecosystem. In particular, thanks
to <a href="https://github.com/mitsuhiko">Armin Ronacher</a> for collaborating with us on this effort.</p>
<p>Finally, we'd like to thank the maintainers of <a href="https://github.com/pypa/pip">pip</a> and the members of
the PyPA more broadly for all the work they do to make Python packaging possible.</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sam Altman owns OpenAI's venture capital fund (246 pts)]]></title>
            <link>https://www.axios.com/2024/02/15/sam-altman-openai-startup-fund</link>
            <guid>39387578</guid>
            <pubDate>Thu, 15 Feb 2024 19:45:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.axios.com/2024/02/15/sam-altman-openai-startup-fund">https://www.axios.com/2024/02/15/sam-altman-openai-startup-fund</a>, See on <a href="https://news.ycombinator.com/item?id=39387578">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-theme="core" id="main-content"><div data-vars-content-id="89f40cb7-4531-4c6b-85c8-c199765f13ff" data-vars-headline="Sam Altman owns OpenAI's venture capital fund" data-vars-category="story" data-vars-sub-category="story"><div><div><p><img alt="headshot" loading="lazy" width="52" height="52" decoding="async" data-nimg="1" srcset="https://www.axios.com/_next/image?url=https%3A%2F%2Fimages.axios.com%2FcUYY6Rl2xaPB8AWGASzP3EWlsDQ%3D%2F0x0%3A328x328%2F52x0%2F2020%2F05%2F01%2F1588371030543.jpg&amp;w=320&amp;q=75 1x" src="https://www.axios.com/_next/image?url=https%3A%2F%2Fimages.axios.com%2FcUYY6Rl2xaPB8AWGASzP3EWlsDQ%3D%2F0x0%3A328x328%2F52x0%2F2020%2F05%2F01%2F1588371030543.jpg&amp;w=320&amp;q=75"></p></div><div><ul><li data-cy="byline-author"><a href="https://www.axios.com/authors/danprimack"><span>Dan Primack</span></a><p>, author of  </p><a href="https://www.axios.com/newsletters/axios-pro-rata">Axios Pro Rata</a></li></ul></div></div><figure data-cy="au-image"><img data-cy="StoryImage" alt="Photo illustration of Sam Altman waving in front of a pile of money." fetchpriority="high" width="1920" height="1080" decoding="async" data-nimg="1" sizes="100vw" srcset="https://images.axios.com/GDQgvSXrjaTnHVNOCxYUj2XUtJQ=/0x0:1920x1080/320x180/2024/02/15/1708018368446.jpg?w=320 320w, https://images.axios.com/GDQgvSXrjaTnHVNOCxYUj2XUtJQ=/0x0:1920x1080/320x180/2024/02/15/1708018368446.jpg?w=320 320w, https://images.axios.com/XQFxVifpfo8p2VsNz37-hfOiyS0=/0x0:1920x1080/640x360/2024/02/15/1708018368446.jpg?w=640 640w, https://images.axios.com/XQFxVifpfo8p2VsNz37-hfOiyS0=/0x0:1920x1080/640x360/2024/02/15/1708018368446.jpg?w=640 640w, https://images.axios.com/i27RdSyJupBNc81EE1MowR8wsWY=/0x0:1920x1080/768x432/2024/02/15/1708018368446.jpg?w=768 768w, https://images.axios.com/i27RdSyJupBNc81EE1MowR8wsWY=/0x0:1920x1080/768x432/2024/02/15/1708018368446.jpg?w=768 768w, https://images.axios.com/IDowc7V9ZUYqv5KeTV8Ra1-X-20=/0x0:1920x1080/1024x576/2024/02/15/1708018368446.jpg?w=1024 1024w, https://images.axios.com/IDowc7V9ZUYqv5KeTV8Ra1-X-20=/0x0:1920x1080/1024x576/2024/02/15/1708018368446.jpg?w=1024 1024w, https://images.axios.com/o8FshlR8oMZ76w2gNQAh6FkTZV4=/0x0:1920x1080/1366x768/2024/02/15/1708018368446.jpg?w=1366 1366w, https://images.axios.com/o8FshlR8oMZ76w2gNQAh6FkTZV4=/0x0:1920x1080/1366x768/2024/02/15/1708018368446.jpg?w=1366 1366w, https://images.axios.com/PfEiA3uwp9fWdaDzrLY7JoJTu-w=/0x0:1920x1080/1600x900/2024/02/15/1708018368446.jpg?w=1600 1600w, https://images.axios.com/PfEiA3uwp9fWdaDzrLY7JoJTu-w=/0x0:1920x1080/1600x900/2024/02/15/1708018368446.jpg?w=1600 1600w, https://images.axios.com/ut-S-jEgmNSG0cpCRmG9tiH27jM=/0x0:1920x1080/1920x1080/2024/02/15/1708018368446.jpg?w=1920 1920w, https://images.axios.com/ut-S-jEgmNSG0cpCRmG9tiH27jM=/0x0:1920x1080/1920x1080/2024/02/15/1708018368446.jpg?w=1920 1920w" src="https://images.axios.com/ut-S-jEgmNSG0cpCRmG9tiH27jM=/0x0:1920x1080/1920x1080/2024/02/15/1708018368446.jpg?w=1920"><figcaption><p>Rebecca Zisser / Axios</p></figcaption></figure><div><p><span data-schema="smart-brevity"><p>Sam Altman isn't just the CEO of ChatGPT maker OpenAI. He's also the owner of OpenAI Startup Fund, which Altman once <a data-vars-link-text="called" data-vars-click-url="https://www.openai.fund/about" data-vars-content-id="89f40cb7-4531-4c6b-85c8-c199765f13ff" data-vars-headline="Sam Altman owns OpenAI's venture capital fund" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.openai.fund/about" target="_blank">called</a> a "corporate venture fund," according to federal securities filings.</p><p><strong>Why it matters:</strong> OpenAI's structural strangeness permeates all aspects of the business.</p></span></p><p><strong>Background:</strong> OpenAI Startup Fund was launched in late 2021 to invest in other AI startups and projects.</p><ul><li>By last May it reported $175 million in total commitments, and a portfolio that included video editor Descript and legal tool Harvey.</li><li>It always had outside limited partners, including major OpenAI partner Microsoft, which is unusual for corporate VC funds but not unique.</li><li>What set OpenAI Startup Fund apart, however, was that it wasn't (and isn't) owned by OpenAI. Nor even by its affiliated <a data-vars-link-text="nonprofit foundation" data-vars-click-url="https://www.axios.com/2023/01/10/how-a-silicon-valley-nonprofit-became-worth-billions" data-vars-content-id="89f40cb7-4531-4c6b-85c8-c199765f13ff" data-vars-headline="Sam Altman owns OpenAI's venture capital fund" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.axios.com/2023/01/10/how-a-silicon-valley-nonprofit-became-worth-billions" target="_self">nonprofit foundation</a>. Instead, it's legally owned by Altman.</li></ul><p><strong>Behind the scenes: </strong>"We wanted to get started quickly and the easiest way to do that due to our structure was to put it in Sam's name," an OpenAI spokesperson tells Axios. "We have always intended for this to be temporary."</p><ul><li>"Temporary" has been well over a year and it's a significant risk. For example, what might have happened had Altman <a data-vars-link-text="remained fired" data-vars-click-url="https://www.axios.com/2023/11/22/sam-altman-return-open-ai" data-vars-content-id="89f40cb7-4531-4c6b-85c8-c199765f13ff" data-vars-headline="Sam Altman owns OpenAI's venture capital fund" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.axios.com/2023/11/22/sam-altman-return-open-ai" target="_self">remained fired</a> by OpenAI. Could he have kept the fund? Was there anything contractual to prevent it?</li><li>No answer to that last question, but the company does add: "We now know that we may need to re-examine our governance structure, which should precede any changes to the fund, but our priority is to establish a new board first."</li></ul></div></div><h5>Go deeper</h5></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Plastics producers deceived public about recycling, report reveals (128 pts)]]></title>
            <link>https://www.theguardian.com/us-news/2024/feb/15/recycling-plastics-producers-report</link>
            <guid>39387387</guid>
            <pubDate>Thu, 15 Feb 2024 19:32:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/us-news/2024/feb/15/recycling-plastics-producers-report">https://www.theguardian.com/us-news/2024/feb/15/recycling-plastics-producers-report</a>, See on <a href="https://news.ycombinator.com/item?id=39387387">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>Plastic producers have known for more than 30 years that recycling is not an economically or technically feasible plastic waste management solution. That has not stopped them from promoting it, according to a new report.</p><p>“The companies lied,” said Richard Wiles, president of fossil-fuel accountability advocacy group the Center for Climate Integrity (CCI), which published the report. “It’s time to hold them accountable for the damage they’ve caused.”</p><figure id="a1b80ff2-5ffb-49c5-94cf-04e08c7a3648" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:2,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;What a waste: New York City budget cuts eviscerate community composting groups&quot;,&quot;elementId&quot;:&quot;a1b80ff2-5ffb-49c5-94cf-04e08c7a3648&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/environment/2024/feb/11/new-york-city-community-composting-groups-budget-cuts&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;display&quot;:0,&quot;theme&quot;:0,&quot;design&quot;:0}}" config="{&quot;renderingTarget&quot;:&quot;Web&quot;,&quot;darkModeAvailable&quot;:false}"></gu-island></figure><p>Plastic, which is made from oil and gas, is notoriously difficult to recycle. Doing so requires meticulous sorting, since most of the thousands of chemically distinct varieties of plastic cannot be recycled together. That renders an already pricey process even more expensive. Another challenge: the material degrades each time it is reused, meaning it can generally only be reused once or twice.</p><p>The industry has known for decades about these existential challenges, but obscured that information in its marketing campaigns, <a href="https://climateintegrity.org/plastics-fraud" data-link-name="in body link">the report shows</a>.</p><p>The research draws on previous <a href="https://www.pbs.org/wgbh/frontline/documentary/plastic-wars/?" data-link-name="in body link">investigations</a> as well as newly revealed internal documents illustrating the extent of this decades-long campaign.</p><p>Industry insiders over the past several decades have variously referred to plastic recycling as “uneconomical”, said it “cannot be considered a permanent solid waste solution”, and said it “cannot go on indefinitely”, the revelations show.</p><p>The authors say the evidence demonstrates that oil and petrochemical companies, as well as their trade associations, may have broken laws designed to protect the public from misleading marketing and pollution.</p><h2 id="single-use-plastics">Single-use plastics</h2><p>In the 1950s, plastic producers came up with an idea to ensure a continually growing market for their products: disposability.</p><p>“They knew if they focused on single-use [plastics] people would buy and buy and buy,” said Davis Allen, investigative researcher at the CCI and the report’s lead author.</p><p>At a 1956 industry conference, the Society of the Plastics Industry, a trade group, told producers to focus on “low cost, big volume” and “expendability” and to aim for materials to end up “in the garbage wagon”. (Society of Plastics is <a href="https://www.plasticstoday.com/business/spi-no-more-rebrands-as-plastics-industry-association-aka-plastics" data-link-name="in body link">now</a> known as the Plastics Industry Association. Plastics Industry Association was not immediately available for comment.)</p><p>Over the following decades, the industry told the public that plastics can easily be tossed into landfills or burned in garbage incinerators. But in the 1980s, as<strong> </strong>municipalities began considering bans on grocery bags and other plastic products<strong>, </strong>the industry began promoting a new solution: recycling.</p><h2 id="recycling-campaigns">Recycling campaigns</h2><p>The industry has long known that plastics recycling is not economically or practically viable, the report shows. An internal 1986 report from the trade association the Vinyl Institute noted that “recycling cannot be considered a permanent solid waste solution [to plastics], as it merely prolongs the time until an item is disposed of”.</p><p>In 1989, the founding director of the Vinyl Institute told attendees of a trade conference: “Recycling cannot go on indefinitely, and does not solve the solid waste problem.”</p><p>Despite this knowledge, the Society of the Plastics Industry established the Plastics Recycling Foundation in 1984, bringing together petrochemical companies and bottlers, and launched a campaign focused on the sector’s <a href="https://www.toxicdocs.org/d/rpQVOR8obVNLbN5R69K0EJ5pJ?lightbox=1" data-link-name="in body link">commitment</a> to recycling.</p><p>In 1988, the trade group rolled out the “chasing arrows” – the widely recognized symbol for recyclable plastic – and began using it on packaging. Experts have long said the symbol is highly misleading, and recently federal regulators have <a href="https://www.nytimes.com/2023/08/07/climate/chasing-arrows-recycling-symbol-epa.html" data-link-name="in body link">echoed</a> their concerns.</p><p>The Society of the Plastics Industry also established a plastics recycling research center at Rutgers University in New Jersey in 1985, one year after state lawmakers passed<strong> </strong>a mandatory recycling law. In 1988, industry group the Council for Solid <a href="https://www.theguardian.com/environment/waste" data-link-name="in body link" data-component="auto-linked-tag">Waste</a> Solutions set up a recycling pilot project in St Paul, Minnesota, where the city council had just voted to ban the plastic polystyrene, or styrofoam.</p><p>And in the early 1990s, another industry group ran ads in Ladies’ Home Journal proclaiming: “A bottle can come back as a bottle, over and over again.”</p><p>All the while, behind closed doors, industry leaders maintained that recycling was not a real solution.</p><p>In 1994, a representative of Eastman Chemical spoke at an industry conference about the need for proper plastic recycling infrastructure. “While some day this may be a reality,” he said, “it is more likely that we will wake up and realize that we are not going to recycle our way out of the solid waste issue.” That same year, an Exxon employee told staffers at the American Plastics Council: “We are committed to the activities [of plastics recycling], but not committed to the results.”</p><p>“It’s clearly fraud they’re engaged in,” said Wiles.</p><p>The report does not allege that the companies broke specific laws. But Alyssa Johl, report co-author and attorney, said she suspects they violated public-nuisance, racketeering and consumer-fraud protections.</p><p>The industry’s misconduct continues today, the report alleges. Over the past several years, industry lobbying groups have <a href="https://cen.acs.org/environment/recycling/plastic-recycling-chemical-advanced-fuel-pyrolysis-state-laws/100/i17" data-link-name="in body link">promoted</a> so-called<strong> </strong>chemical recycling, which <a href="https://www.theguardian.com/us-news/2023/apr/10/exxon-advanced-recycling-plastic-environment" data-link-name="in body link">breaks plastic polymers down</a> into tiny molecules in order to make new plastics, synthetic fuels and other products. But the process creates pollution and is even more energy intensive than traditional plastic recycling.</p><p>The plastics sector has long known chemical recycling is also not a true solution to plastic waste, the report says. In a 1994 trade meeting, Exxon Chemical vice-president Irwin Levowitz called one common form of chemical recycling a “fundamentally uneconomical process”. And in 2003, a longtime trade consultant criticized the industry for promoting chemical recycling, calling it “another example of how non-science got into the minds of industry and environmental activists alike”.</p><p>“This is just another example, a new version, of the deception we saw before,” said Allen.</p><h2 id="legal-ramifications">Legal ramifications</h2><p>The report comes as the plastic industry and recycling are facing growing<strong> </strong>public scrutiny. Two years ago, California’s attorney general, Rob Bonta, publicly launched an <a href="https://oag.ca.gov/news/press-releases/attorney-general-bonta-announces-investigation-fossil-fuel-and-petrochemical" data-link-name="in body link">investigation</a> into fossil fuel and petrochemical producers “for their role in causing and exacerbating the global plastics pollution crisis”.</p><p>A toxic train derailment in East Palestine, Ohio, last February also catalyzed a movement demanding a ban on vinyl chloride, a carcinogen used to make plastic. Last month, the EPA announced a health review of the chemical – the first step toward a <a href="https://apnews.com/article/vinyl-chloride-ohio-train-derailment-toxic-chemicals-54bb0a943f4f4af0e4f68cc60ce4edb4" data-link-name="in body link">potential ban</a>.</p><p>In 2023, New York state also filed a lawsuit against PepsiCo, saying its single-use plastics violate public nuisance laws, and that the company misled consumers about the effectiveness of recycling.</p><figure id="662870ea-aad7-4537-a257-dab131a2a311" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:31,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;Certified natural gas is ‘dangerous greenwashing scheme’, US senators say&quot;,&quot;elementId&quot;:&quot;662870ea-aad7-4537-a257-dab131a2a311&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/us-news/2024/feb/12/natural-gas-greenwashing-democrats&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;display&quot;:0,&quot;theme&quot;:0,&quot;design&quot;:0}}" config="{&quot;renderingTarget&quot;:&quot;Web&quot;,&quot;darkModeAvailable&quot;:false}"></gu-island></figure><p>The public is also increasingly concerned about the climate impact of plastic production and disposal, which account for <a href="https://rmi.org/clean-energy-101-reducing-climate-pollution-from-the-plastics-industry/#:~:text=In%20fact%2C%20about%2012%20percent,to%20manage%20plastics'%20climate%20risks." data-link-name="in body link">3.4% of all global greenhouse-gas emissions.</a> In recent years, two dozen cities and states have <a href="https://www.theguardian.com/us-news/2023/jun/07/climate-crisis-big-oil-lawsuits-constitution" data-link-name="in body link">sued the oil industry</a> for covering up the dangers of the climate crisis. Similarly taking the oil and petrochemical industries to court for “knowingly deceiving” the public, said Wiles, could force them to change their business models.</p><p>“I think the first step in solving the problem is holding the companies accountable,” he said.</p><p>Judith Enck, a former regional administrator for the Environmental Protection Agency and founder of the advocacy group Beyond Plastics, called the analysis “very solid”.</p><p>“The report should be read by every attorney general in the nation and the Federal Trade Commission,” she said.</p><p>Brian Frosh, the former attorney general for the state of Maryland, said the report includes the kind of evidence he would not normally expect to see until a lawsuit has already gone through a process of discovery.</p><p>“If I were attorney general, based on what I read in CCI’s report, I’d feel comfortable pressing for an investigation and a lawsuit,” he said.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[PWAs wont replace native iOS apps (180 pts)]]></title>
            <link>https://app.campsite.co/campsite/p/notes/rengvq2txami</link>
            <guid>39387304</guid>
            <pubDate>Thu, 15 Feb 2024 19:28:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://app.campsite.co/campsite/p/notes/rengvq2txami">https://app.campsite.co/campsite/p/notes/rengvq2txami</a>, See on <a href="https://news.ycombinator.com/item?id=39387304">Hacker News</a></p>
<div id="readability-page-1" class="page"><a href="#main">Skip to content</a><div id="__next"><nav><a href="https://www.campsite.co/"><svg width="26" height="14" viewBox="0 0 34 18" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0.140151 16.4649L9.5715 0.490928C9.75112 0.186693 10.0782 0 10.4316 0H25.7959C26.569 0 27.049 0.840284 26.6561 1.50582L17.2247 17.4798C17.0451 17.7841 16.718 17.9708 16.3646 17.9708H1.00028C0.227176 17.9708 -0.252794 17.1305 0.140151 16.4649Z" fill="currentColor"></path><path d="M22.2255 17.9707H32.9577C33.7108 17.9707 34.193 17.1691 33.8398 16.5042L28.7719 6.96112C28.4064 6.27298 27.4284 6.24978 27.0307 6.91981L21.3666 16.4628C20.9715 17.1284 21.4514 17.9707 22.2255 17.9707Z" fill="currentColor"></path></svg></a><a data-state="closed" href="https://www.campsite.co/"><span><span>Made with Campsite</span></span></a></nav><main><div><p>Ryan Nystrom</p></div></main><!--$--><!--/$--></div></div>]]></description>
        </item>
    </channel>
</rss>