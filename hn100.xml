<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 18 Sep 2025 03:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Slack is extorting us with a $195k/yr bill increase (356 pts)]]></title>
            <link>https://skyfall.dev/posts/slack</link>
            <guid>45283887</guid>
            <pubDate>Thu, 18 Sep 2025 01:37:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://skyfall.dev/posts/slack">https://skyfall.dev/posts/slack</a>, See on <a href="https://news.ycombinator.com/item?id=45283887">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <div>  <p>An open letter, or something</p> </div> <p><time datetime="2025-09-18T18:00:00.000Z"> September 18th 2025 </time> </p>  </div><div>  <p>For nearly 11 years, Hack Club - a nonprofit that provides coding education and community to teenagers worldwide - has used Slack as the tool for communication. We weren’t freeloaders. A few years ago, when Slack transitioned us from their free nonprofit plan to a $5,000/year arrangement, we happily paid. It was reasonable, and we valued the service they provided to our community.</p>
<p>However, two days ago, Slack reached out to us and said that if we don’t agree to pay an extra $50k <strong>this week</strong> and $200k a year, they’ll deactivate our Slack workspace and delete all of our message history.</p>
<p>One could argue that Slack is free to stop providing us the nonprofit offer at any time, but in my opinion, a six month grace period is the <em>bare minimum</em> for a massive hike like this, if not more. Essentially, Salesforce (a <strong>$230 billion</strong> company) is strong-arming a small nonprofit for teens, by providing less than a week to pony up a pretty massive sum of money, or risk cutting off all our communications. That’s absurd.</p>
<h2 id="the-impact">The impact</h2>
<p>The small amount of notice has also been catastrophic for the programs that we run. Dozens of our staff and volunteers are now scrambling to update systems, rebuild integrations and migrate <em>years</em> of institutional knowledge. The opportunity cost of this forced migration is simply staggering.</p>
<p><img width="752" height="55" alt="image" src="https://github.com/user-attachments/assets/48097101-1521-4f50-b970-9557a0b7eefd">
<img width="1146" height="103" alt="image" src="https://github.com/user-attachments/assets/f09902a1-42cb-4cd7-9a32-21cdbfb3fd05">
<img width="1146" height="134" alt="image" src="https://github.com/user-attachments/assets/dbfc784a-d06b-44d8-a050-ec8c16c5a98b">
<img width="611" height="274" alt="image" src="https://github.com/user-attachments/assets/8a41302f-2e5f-41c1-933f-d856094c587a"></p><p>Anyway, we’re moving to Mattermost. This experience has taught us that owning your data is incredibly important, and if you’re a small business especially, then I’d advise you move away too.</p>
<hr>
<p><em>This post was rushed out because, well, this has been a shock! If you’d like any additional details then feel free to send me an email.</em></p>  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meta Ray-Ban Display (176 pts)]]></title>
            <link>https://www.meta.com/blog/meta-ray-ban-display-ai-glasses-connect-2025/</link>
            <guid>45283306</guid>
            <pubDate>Thu, 18 Sep 2025 00:30:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.meta.com/blog/meta-ray-ban-display-ai-glasses-connect-2025/">https://www.meta.com/blog/meta-ray-ban-display-ai-glasses-connect-2025/</a>, See on <a href="https://news.ycombinator.com/item?id=45283306">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[ABC Pulls Jimmy Kimmel Live from the Air 'Indefinitely' (145 pts)]]></title>
            <link>https://www.vulture.com/article/abc-pulls-jimmy-kimmel-live-from-the-air-indefinitely.html</link>
            <guid>45282485</guid>
            <pubDate>Wed, 17 Sep 2025 23:00:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.vulture.com/article/abc-pulls-jimmy-kimmel-live-from-the-air-indefinitely.html">https://www.vulture.com/article/abc-pulls-jimmy-kimmel-live-from-the-air-indefinitely.html</a>, See on <a href="https://news.ycombinator.com/item?id=45282485">Hacker News</a></p>
<div id="readability-page-1" class="page"><section data-editable="main" data-track-zone="main">  <article role="main" data-track-type="article-detail" data-uri="www.vulture.com/_components/article/instances/cmfojo9bx000j0jik2r8w9h6u@published" data-content-channel="TV" data-crosspost="" data-type="Breaking-News-Original Reporting" data-syndication="original" data-headline="ABC Pulls Jimmy Kimmel Live! From the Air ‘Indefinitely’" data-authors="Josef Adalian" data-publish-date="2025-09-17" data-tags="tv, comedy, late night, jimmy kimmel live!, jimmy kimmel, politics, charlie kirk, vulture homepage lede" data-issue-date="" data-components-count="4" data-canonical-url="http://www.vulture.com/article/abc-pulls-jimmy-kimmel-live-from-the-air-indefinitely.html">


  
  
  
  <header>
    <div>
          

            <p><span data-editable="bylines">
            <p><span>By</span> <span>
        ,
          <span>who has covered the television industry since 1992</span><span>&nbsp;</span>
          <span>and writes Buffering, a newsletter about streaming</span>
      </span></p>

              </span>
          </p>
        </div>
    
  </header>
  <section>
    
    <div id="vulture-zephr-anchor" data-editable="content">
      <div>
          <div>
            <picture> <source media="(min-resolution: 192dpi) and (min-width: 1180px), (-webkit-min-device-pixel-ratio: 2) and (min-width: 1180px)" srcset="https://pyxis.nymag.com/v1/imgs/8c0/d93/983356eab25f0dade8b73751d607a07a7f-jimmy-kimmel.2x.rhorizontal.w700.jpg 2x" width="700" height="467"> <source media="(min-width: 1180px) " srcset="https://pyxis.nymag.com/v1/imgs/8c0/d93/983356eab25f0dade8b73751d607a07a7f-jimmy-kimmel.rhorizontal.w700.jpg" width="700" height="467"> <source media="(min-resolution: 192dpi) and (min-width: 768px), (-webkit-min-device-pixel-ratio: 2) and (min-width: 768px)" srcset="https://pyxis.nymag.com/v1/imgs/8c0/d93/983356eab25f0dade8b73751d607a07a7f-jimmy-kimmel.2x.rhorizontal.w700.jpg 2x" width="700" height="467"> <source media="(min-width: 768px)" srcset="https://pyxis.nymag.com/v1/imgs/8c0/d93/983356eab25f0dade8b73751d607a07a7f-jimmy-kimmel.rhorizontal.w700.jpg" width="700" height="467"> <source media="(min-resolution: 192dpi), (-webkit-min-device-pixel-ratio: 2)" srcset="https://pyxis.nymag.com/v1/imgs/8c0/d93/983356eab25f0dade8b73751d607a07a7f-jimmy-kimmel.2x.rhorizontal.w700.jpg" width="700" height="467"> <img src="https://pyxis.nymag.com/v1/imgs/8c0/d93/983356eab25f0dade8b73751d607a07a7f-jimmy-kimmel.rhorizontal.w700.jpg" data-content-img="" alt="JIMMY KIMMEL" width="700" height="467" fetchpriority="high"> </picture>
          </div>
            <div>
              <p><span>Photo: Randy Holmes/Disney via Getty Images</span>
              </p>
            </div>
              </div>
        <p data-editable="text" data-uri="www.vulture.com/_components/clay-paragraph/instances/cmfojo9bx000i0jik5fd1vt55@published" data-word-count="65">Conservative cancel culture has come for Jimmy Kimmel: Walt Disney–owned ABC has announced it’s pulling new episodes of <em>Jimmy Kimmel Live! </em>“indefinitely” following right-wing outrage over comments he made on his September 15 show about the reaction to the <a href="https://nymag.com/intelligencer/article/charlie-kirk-shooting-at-utah-university-q-and-a-live-updates.html">killing of right-wing podcaster and provocateur Charlie Kirk</a>. Disney’s&nbsp;decision follows a move by one of its major affiliate groups, Nexstar, to preempt the show in response.</p>

  <p data-editable="text" data-uri="www.vulture.com/_components/clay-paragraph/instances/cmfokcub900253b74tnikyb3l@published" data-word-count="133">While Nexstar didn’t say exactly what Kimmel had said that it objected to, and ABC offered no further explanation of its move, FCC chairman Brendan Carr earlier on Wednesday denounced this part of the host’s Monday monologue, <a href="https://deadline.com/2025/09/fcc-jimmy-kimmel-charlie-kirk-suspect-1236547238/">per Deadline</a>: “We had some new lows over the weekend with the MAGA gang desperately trying to characterize this kid who murdered Charlie Kirk as anything other than one of them and with everything they can to score political points from it.” Around 6 p.m. ET Wednesday, Nexstar issued this statement: “Nexstar strongly objects to recent comments made by Mr. Kimmel concerning the killing of Charlie Kirk and will replace the show with other programming in its ABC-affiliated markets.” When Vulture asked ABC for comment, a network rep replied, “<em>Jimmy Kimmel Live!</em> will be preempted indefinitely.”</p>

  <p data-editable="text" data-uri="www.vulture.com/_components/clay-paragraph/instances/cmfokcub900263b74w1qr3a7v@published" data-word-count="28">Vulture has reached out to Kimmel’s reps for comment and asked Nexstar and ABC for additional clarification of today’s actions. We’ll update this story when we know more.</p>

  


    </div>

      


          



      <span>ABC Pulls <em>Jimmy Kimmel Live!</em> From the Air ‘Indefinitely’</span>



    <dialog>
      <span>
        <svg width="6" height="14" viewBox="0 0 6 14" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="M4.84191 13.478C4.84191 13.826 4.64391 14 4.24791 14H1.54791C1.22391 14 1.06191 13.85 1.06191 13.55V10.85C1.06191 10.586 1.17591 10.454 1.40391 10.454H4.51791C4.73391 10.454 4.84191 10.574 4.84191 10.814V13.478ZM4.13991 8.708C4.12791 8.888 4.07391 9.02 3.97791 9.104C3.89391 9.176 3.74991 9.212 3.54591 9.212H2.30391C2.12391 9.212 2.00391 9.176 1.94391 9.104C1.89591 9.032 1.85991 8.918 1.83591 8.762L0.935906 1.058C0.923906 0.926 0.947906 0.823999 1.00791 0.751999C1.07991 0.679999 1.16991 0.643999 1.27791 0.643999H4.67991C4.91991 0.643999 5.02791 0.769999 5.00391 1.022L4.13991 8.708Z" fill="#DB2800"></path>
</svg>

      </span>
      <span></span>
      <span>
        <svg width="14" height="13" viewBox="0 0 14 13" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path fill-rule="evenodd" clip-rule="evenodd" d="M12.9823 1.22855C13.1775 1.03329 13.1775 0.716709 12.9823 0.521447C12.787 0.326184 12.4704 0.326184 12.2751 0.521447L7.00185 5.79474L1.72855 0.521447C1.53329 0.326184 1.21671 0.326184 1.02145 0.521447C0.826184 0.716709 0.826184 1.03329 1.02145 1.22855L6.29474 6.50185L1.02145 11.7751C0.826184 11.9704 0.826184 12.287 1.02145 12.4823C1.21671 12.6775 1.53329 12.6775 1.72855 12.4823L7.00185 7.20896L12.2751 12.4823C12.4704 12.6775 12.787 12.6775 12.9823 12.4823C13.1775 12.287 13.1775 11.9704 12.9823 11.7751L7.70896 6.50185L12.9823 1.22855Z" fill="#DA4022"></path>
</svg>

      </span>
    </dialog>

  </section>
  

</article>

  

</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ABC yanks Jimmy Kimmel's show 'indefinitely' after remarks about Charlie Kirk (266 pts)]]></title>
            <link>https://www.cnn.com/2025/09/17/media/jimmy-kimmel-charlie-kirk-trump-fcc-brendan-carr</link>
            <guid>45282482</guid>
            <pubDate>Wed, 17 Sep 2025 23:00:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnn.com/2025/09/17/media/jimmy-kimmel-charlie-kirk-trump-fcc-brendan-carr">https://www.cnn.com/2025/09/17/media/jimmy-kimmel-charlie-kirk-trump-fcc-brendan-carr</a>, See on <a href="https://news.ycombinator.com/item?id=45282482">Hacker News</a></p>
Couldn't get https://www.cnn.com/2025/09/17/media/jimmy-kimmel-charlie-kirk-trump-fcc-brendan-carr: Error: Request failed with status code 451]]></description>
        </item>
        <item>
            <title><![CDATA[A postmortem of three recent issues (208 pts)]]></title>
            <link>https://www.anthropic.com/engineering/a-postmortem-of-three-recent-issues</link>
            <guid>45281139</guid>
            <pubDate>Wed, 17 Sep 2025 20:41:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.anthropic.com/engineering/a-postmortem-of-three-recent-issues">https://www.anthropic.com/engineering/a-postmortem-of-three-recent-issues</a>, See on <a href="https://news.ycombinator.com/item?id=45281139">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><p>Between August and early September, three infrastructure bugs intermittently degraded Claude's response quality. We've now resolved these issues and want to explain what happened.</p><p>In early August, a number of users began reporting degraded responses from Claude. These initial reports were difficult to distinguish from normal variation in user feedback. By late August, the increasing frequency and persistence of these reports prompted us to open an investigation that led us to uncover three separate infrastructure bugs.</p><p>To state it plainly: We never reduce model quality due to demand, time of day, or server load. The problems our users reported were due to infrastructure bugs alone.</p><p>We recognize users expect consistent quality from Claude, and we maintain an extremely high bar for ensuring infrastructure changes don't affect model outputs. In these recent incidents, we didn't meet that bar. The following postmortem explains what went wrong, why detection and resolution took longer than we would have wanted, and what we're changing to prevent similar future incidents.</p><p>We don't typically share this level of technical detail about our infrastructure, but the scope and complexity of these issues justified a more comprehensive explanation.</p><h2 id="how-we-serve-claude-at-scale">How we serve Claude at scale</h2><p>We serve Claude to millions of users via our first-party API, Amazon Bedrock, and Google Cloud's Vertex AI. We deploy Claude across multiple hardware platforms, namely AWS Trainium, NVIDIA GPUs, and Google TPUs. This approach provides the capacity and geographic distribution necessary to serve users worldwide.</p><p>Each hardware platform has different characteristics and requires specific optimizations. Despite these variations, we have strict equivalence standards for model implementations. Our aim is that users should get the same quality responses regardless of which platform serves their request. This complexity means that any infrastructure change requires careful validation across all platforms and configurations.</p><h2 id="timeline-of-events">Timeline of events</h2><div><figure><img alt="Illustrative timeline of events on the Claude API. Yellow: issue detected, Red: degradation worsened, Green: fix deployed." loading="lazy" width="3840" height="1800" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fd707dfc2effceba608d04007bc776132a3e57838-3840x1800.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fd707dfc2effceba608d04007bc776132a3e57838-3840x1800.png&amp;w=3840&amp;q=75"><figcaption>Illustrative timeline of events on the <strong>Claude API</strong>. Yellow: issue detected, Red: degradation worsened, Green: fix deployed.</figcaption></figure></div><p>The overlapping nature of these bugs made diagnosis particularly challenging. The first bug was introduced on August 5, affecting approximately 0.8% of requests made to Sonnet 4. Two more bugs arose from deployments on August 25 and 26.</p><p>Although initial impacts were limited, a load balancing change on August 29 started to increase affected traffic. This caused many more users to experience issues while others continued to see normal performance, creating confusing and contradictory reports.</p><h2 id="three-overlapping-issues">Three overlapping issues</h2><p>Below we describe the three bugs that caused the degradation, when they occurred, and how we resolved them:</p><h3 id="1-context-window-routing-error">1. Context window routing error</h3><p>On August 5, some Sonnet 4 requests were misrouted to servers configured for the upcoming <a href="https://docs.claude.com/en/docs/build-with-claude/context-windows#1m-token-context-window">1M token</a> <a href="https://docs.claude.com/en/docs/build-with-claude/context-windows">context window</a>. This bug initially affected 0.8% of requests. On August 29, a routine load balancing change unintentionally increased the number of short-context requests routed to the 1M context servers. At the worst impacted hour on August 31, 16% of Sonnet 4 requests were affected.</p><p>Approximately 30% of Claude Code users who made requests during this period had at least one message routed to the wrong server type, resulting in degraded responses. On Amazon Bedrock, misrouted traffic peaked at 0.18% of all Sonnet 4 requests from August 12. Incorrect routing affected less than 0.0004% of requests on Google Cloud's Vertex AI between August 27 and September 16.</p><p>However, some users were affected more severely, as our routing is "sticky". This meant that once a request was served by the incorrect server, subsequent follow-ups were likely to be served by the same incorrect server.</p><p><strong>Resolution:</strong> We fixed the routing logic to ensure short- and long-context requests were directed to the correct server pools. We deployed the fix on September 4. A rollout to our first-party platforms and Google Cloud’s Vertex was completed by September 16. The fix is in the process of being rolled out on Bedrock.</p><h3 id="2-output-corruption">2. Output corruption</h3><p>On August 25, we deployed a misconfiguration to the Claude API TPU servers that caused an error during token generation. An issue caused by a runtime performance optimization occasionally assigned a high probability to tokens that should rarely be produced given the context, for example producing Thai or Chinese characters in response to English prompts, or producing obvious syntax errors in code. A small subset of users that asked a question in English might have seen "สวัสดี" in the middle of the response, for example.</p><p>This corruption affected requests made to Opus 4.1 and Opus 4 on August 25-28, and requests to Sonnet 4 August 25–September 2. Third-party platforms were not affected by this issue.</p><p><strong>Resolution:</strong> We identified the issue and rolled back the change on September 2. We've added detection tests for unexpected character outputs to our deployment process.</p><h3 id="3-approximate-top-k-xlatpu-miscompilation">3. Approximate top-k XLA:TPU miscompilation</h3><p>On August 25, we deployed code to improve how Claude selects tokens during text generation. This change inadvertently triggered a latent bug in the XLA:TPU<sup>[1] </sup>compiler, which has been confirmed to affect requests to Claude Haiku 3.5.</p><p>We also believe this could have impacted a subset of Sonnet 4 and Opus 3 on the Claude API. Third-party platforms were not affected by this issue.</p><p><strong>Resolution:</strong> We first observed the bug affecting Haiku 3.5 and rolled it back on September 4. We later noticed user reports of problems with Opus 3 that were compatible with this bug, and rolled it back on September 12. After extensive investigation we were unable to reproduce this bug on Sonnet 4 but decided to also roll it back out of an abundance of caution.</p><p>Simultaneously, we have (a) been working with the XLA:TPU team on a fix for the compiler bug and (b) rolled out a fix to use exact top-k with enhanced precision. For details, see the deep dive below.</p><h2 id="a-closer-look-at-the-xla-compiler-bug">A closer look at the XLA compiler bug</h2><p>To illustrate the complexity of these issues, here's how the XLA compiler bug manifested and why it proved particularly challenging to diagnose.</p><p>When Claude generates text, it calculates probabilities for each possible next word, then randomly chooses a sample from this probability distribution. We use "top-p sampling" to avoid nonsensical outputs—only considering words whose cumulative probability reaches a threshold (typically 0.99 or 0.999). On TPUs, our models run across multiple chips, with probability calculations happening in different locations. To sort these probabilities, we need to coordinate data between chips, which is complex.<sup>[2]</sup></p><p>In December 2024, we discovered our TPU implementation would occasionally drop the most probable token when <a href="https://docs.claude.com/en/docs/about-claude/glossary#temperature">temperature</a> was zero. We deployed a workaround to fix this case.</p><div><figure><img alt="Code snippet of a December 2024 patch to work around the unexpected dropped token bug when temperature = 0." loading="lazy" width="2000" height="500" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fefee0d3d25f6b03cbfc57e70e0e364dcd8b82fe0-2000x500.png&amp;w=2048&amp;q=75 1x, https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fefee0d3d25f6b03cbfc57e70e0e364dcd8b82fe0-2000x500.png&amp;w=3840&amp;q=75 2x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fefee0d3d25f6b03cbfc57e70e0e364dcd8b82fe0-2000x500.png&amp;w=3840&amp;q=75"><figcaption>Code snippet of a December 2024 patch to work around the unexpected dropped token bug when temperature = 0.</figcaption></figure></div><p>The root cause involved mixed precision arithmetic. Our models compute next-token probabilities in <a href="https://github.com/tensorflow/tensorflow/blob/f41959ccb2d9d4c722fe8fc3351401d53bcf4900/tensorflow/core/framework/bfloat16.h">bf16</a> (16-bit floating point). However, the vector processor is <a href="https://dl.acm.org/doi/pdf/10.1145/3360307">fp32-native</a>, so the TPU compiler (XLA) can optimize runtime by converting some operations to fp32 (32-bit). This optimization pass is guarded by the <code>xla_allow_excess_precision</code> flag which defaults to true.</p><p>This caused a mismatch: operations that should have agreed on the highest probability token were running at different precision levels. The precision mismatch meant they didn't agree on which token had the highest probability. This caused the highest probability token to sometimes disappear from consideration entirely.</p><p>On August 26, we deployed a rewrite of our sampling code to fix the precision issues and improve how we handled probabilities at the limit that reach the top-p threshold. But in fixing these problems, we exposed a trickier one.</p><div><figure><img alt="Code snippet showing minimized reproducer merged as part of the August 11 change that root-caused the “bug” being worked around in December 2024; in reality, it’s expected behavior of the xla_allow_excess_precision flag." loading="lazy" width="2000" height="2560" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F6d10e58c0bd5fd7cb03dc0adc716cb1e4f039343-2000x2560.png&amp;w=2048&amp;q=75 1x, https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F6d10e58c0bd5fd7cb03dc0adc716cb1e4f039343-2000x2560.png&amp;w=3840&amp;q=75 2x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F6d10e58c0bd5fd7cb03dc0adc716cb1e4f039343-2000x2560.png&amp;w=3840&amp;q=75"><figcaption>Code snippet showing a minimized reproducer merged as part of the August 11 change that root-caused the "bug" being worked around in December 2024. In reality, it’s expected behavior of the <code>xla_allow_excess_precision</code> flag.</figcaption></figure></div><p>Our fix removed the December workaround because we believed we'd solved the root cause. This led to a deeper bug in the <a href="https://docs.jax.dev/en/latest/_autosummary/jax.lax.approx_max_k.html">approximate top-k</a> operation—a performance optimization that quickly finds the highest probability tokens.<sup>[3]</sup> This approximation sometimes returned completely wrong results, but only for certain batch sizes and model configurations. The December workaround had been inadvertently masking this problem.</p><div><figure><img alt="Slack message showing reproducer of the underlying approximate top-k bug shared with the XLA:TPU engineers who developed the algorithm. The code returns correct results when run on CPUs." loading="lazy" width="2400" height="1404" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F7e42db934d0e84ea40fc56b416ddb09b2097a5ff-2400x1404.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F7e42db934d0e84ea40fc56b416ddb09b2097a5ff-2400x1404.png&amp;w=3840&amp;q=75"><figcaption>Reproducer of the underlying approximate top-k bug shared with the XLA:TPU engineers who <a href="https://arxiv.org/pdf/2206.14286">developed the algorithm</a>. The code returns correct results when run on CPUs.</figcaption></figure></div><p>The bug's behavior was frustratingly inconsistent. It changed depending on unrelated factors such as what operations ran before or after it, and whether debugging tools were enabled. The same prompt might work perfectly on one request and fail on the next.</p><p>While investigating, we also discovered that the exact top-k operation no longer had the prohibitive performance penalty it once did. We switched from approximate to exact top-k and standardized some additional operations on fp32 precision.<sup>[4]</sup> Model quality is non-negotiable, so we accepted the minor efficiency impact.</p><h2 id="why-detection-was-difficult">Why detection was difficult</h2><p>Our validation process ordinarily relies on benchmarks alongside safety evaluations and performance metrics. Engineering teams perform spot checks and deploy to small "canary" groups first.</p><p>These issues exposed critical gaps that we should have identified earlier. The evaluations we ran simply didn't capture the degradation users were reporting, in part because Claude often recovers well from isolated mistakes. Our own privacy practices also created challenges in investigating reports. Our internal privacy and security controls limit how and when engineers can access user interactions with Claude, in particular when those interactions are not reported to us as feedback. This protects user privacy but prevents engineers from examining the problematic interactions needed to identify or reproduce bugs.</p><p>Each bug produced different symptoms on different platforms at different rates. This created a confusing mix of reports that didn't point to any single cause. It looked like random, inconsistent degradation.</p><p>More fundamentally, we relied too heavily on noisy evaluations. Although we were aware of an increase in reports online, we lacked a clear way to connect these to each of our recent changes. When negative reports spiked on August 29, we didn't immediately make the connection to an otherwise standard load balancing change.</p><h2 id="what-were-changing">What we're changing</h2><p>As we continue to improve our infrastructure, we're also improving the way we evaluate and prevent bugs like those discussed above across all platforms where we serve Claude. Here's what we're changing:</p><ul><li><strong>More sensitive evaluations:</strong> To help discover the root cause of any given issue, we’ve developed evaluations that can more reliably differentiate between working and broken implementations. We’ll keep improving these evaluations to keep a closer eye on model quality.</li><li><strong>Quality evaluations in more places:</strong> Although we run regular evaluations on our systems, we will run them continuously on true production systems to catch issues such as the context window load balancing error.</li><li><strong>Faster debugging tooling:</strong> We'll develop infrastructure and tooling to better debug community-sourced feedback without sacrificing user privacy. Additionally, some bespoke tools developed here will be used to reduce the remediation time in future similar incidents, if those should occur.</li></ul><p>Evals and monitoring are important. But these incidents have shown that we also need continuous signal from users when responses from Claude aren't up to the usual standard. Reports of specific changes observed, examples of unexpected behavior encountered, and patterns across different use cases all helped us isolate the issues.</p><p>It remains particularly helpful for users to continue to send us their feedback directly. You can use the <code>/bug</code> command in Claude Code or you can use the "thumbs down" button in the Claude apps to do so. Developers and researchers often create new and interesting ways to evaluate model quality that complement our internal testing. If you'd like to share yours, reach out to <a href="mailto:feedback@anthropic.com">feedback@anthropic.com</a>.</p><p>We remain grateful to our community for these contributions.</p><p><sup>[1]</sup> XLA:TPU is the optimizing compiler that translates <a href="https://openxla.org/xla/architecture">XLA</a> High Level Optimizing language—often written using <a href="https://docs.jax.dev/en/latest">JAX</a>—to TPU machine instructions.</p><p><sup>[2]</sup> Our models are too large for single chips and are partitioned across tens of chips or more, making our sorting operation a distributed sort. TPUs (just like GPUs and Trainium) also have different performance characteristics than CPUs, requiring different implementation techniques using vectorized operations instead of serial algorithms.</p><p><sup>[3]</sup> We had been using this approximate operation because it yielded substantial performance improvements. The approximation works by accepting potential inaccuracies in the lowest probability tokens, which shouldn't affect quality—except when the bug caused it to drop the highest probability token instead.</p><p><sup>[4]</sup> Note that the now-correct top-k implementation may result in slight differences in the inclusion of tokens near the top-p threshold, and in rare cases users may benefit from re-tuning their choice of top-p.</p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Famous cognitive psychology experiments that failed to replicate (139 pts)]]></title>
            <link>https://buttondown.com/aethermug/archive/aether-mug-famous-cognitive-psychology/</link>
            <guid>45279898</guid>
            <pubDate>Wed, 17 Sep 2025 18:55:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://buttondown.com/aethermug/archive/aether-mug-famous-cognitive-psychology/">https://buttondown.com/aethermug/archive/aether-mug-famous-cognitive-psychology/</a>, See on <a href="https://news.ycombinator.com/item?id=45279898">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main components="[object Object]"><p><em>TL;DR is the part in bold below.</em></p>
<p>The field of psychology had a big crisis in the 2010s, when many widely accepted results turned out to be much less solid than previously thought. It's called the <a href="https://en.wikipedia.org/w/index.php?title=Replication_crisis" rel="nofollow noopener noreferrer" target="_blank">replication crisis</a>, because labs around the world tried and failed to replicate, in new experiments, previous results published by their original "discoverers". In other words, many reported psychological effects were either non-existent—artifacts of the experimenter's flawed setup—or so much weaker than originally claimed that they lost most of their intellectual sparkle.</p>
<p>(The crisis spanned other fields as well, but I mostly care about psychology here, especially the cognitive kind.)</p>
<p>This is very old news, and I've been vaguely aware of several of the biggest disgraced results for years, but I keep on forgetting which are (still probably) real and which aren't. This is not good. <em>Most</em> results in the field do actually replicate and are robust<sup>[citation needed]</sup>, so it would be a pity to lose confidence in the whole field just because of a few bad apples.</p>
<p><strong>This post is a compact reference list of the most (in)famous cognitive science results that failed to replicate and should, for the time being, be considered false.</strong> The only goal is to offset the trust-undermining effects of my poor memory—and perhaps yours, too?—with a bookmarkable page.</p>
<p>This can't be a comprehensive list: if a study is <em>not</em> on this page, it's not guaranteed to be fully replicated. Still, this should cover most of the high-profile debunked theories that laypeople like me may have heard of.</p>
<p><em>Credit: I enlisted the help of Kimi K2, o3, and Sonnet 4 to gather and fact-check this list. I also checked, pruned, and de-hallucinated all the results.</em></p>

<h3>Ego Depletion Effect</h3>
<ul>
<li><strong>Claimed result:</strong> We have a "willpower battery" that gradually depletes during the day as we exercise self-control. (I remember reading Baumeister's pop-science book and being awed by the implications of their findings; I might have known it sounded too good to be true.)</li>
<li><strong>Representative paper:</strong> <a href="https://psycnet.apa.org/doiLanding?doi=10.1037%2F0022-3514.74.5.1252" rel="nofollow noopener noreferrer" target="_blank">Baumeister et al. 1998</a></li>
<li><strong>Replication status:</strong> <em>did not replicate</em></li>
<li><strong>Source:</strong> <a href="https://journals.sagepub.com/doi/10.1177/1745691616652873" rel="nofollow noopener noreferrer" target="_blank">Hagger et (63!) al. 2016</a></li>
</ul>
<h3>Power Posing Effect</h3>
<ul>
<li><strong>Claimed result:</strong> Adopting expansive body postures for 2 minutes (like standing with hands on hips or arms raised) increases testosterone, decreases cortisol, and makes people feel more powerful and take more risks.</li>
<li><strong>Representative paper:</strong> <a href="https://doi.org/10.1177/0956797610383437" rel="nofollow noopener noreferrer" target="_blank">Carney, Cuddy, &amp; Yap (2010)</a></li>
<li><strong>Replication status:</strong> <em>did not replicate</em></li>
<li><strong>Source:</strong> <a href="https://doi.org/10.1177/0956797614553946" rel="nofollow noopener noreferrer" target="_blank">Ranehill et al. (2015)</a></li>
</ul>
<h3>Social Priming: Elderly Words Effect</h3>
<ul>
<li><strong>Claimed result:</strong> People walk more slowly after being exposed to words related to elderly stereotypes.</li>
<li><strong>Representative paper:</strong> <a href="https://doi.org/10.1037/0022-3514.71.2.230" rel="nofollow noopener noreferrer" target="_blank">Bargh, Chen, &amp; Burrows (1996)</a></li>
<li><strong>Replication status:</strong> <em>did not replicate</em></li>
<li><strong>Source:</strong> <a href="https://doi.org/10.1371/journal.pone.0029081" rel="nofollow noopener noreferrer" target="_blank">Doyen et al. (2012)</a> (I like how they prove that the psychological effect was actually in the experimenters, rather than the subjects!)</li>
</ul>
<h3>Money Priming Effect</h3>
<ul>
<li><strong>Claimed result:</strong> Simply thinking about money makes you more selfish and more likely to endorse free market values.</li>
<li><strong>Representative paper:</strong> <a href="https://doi.org/10.1126/science.1132491" rel="nofollow noopener noreferrer" target="_blank">Vohs, Mead, &amp; Goode (2006)</a></li>
<li><strong>Replication status:</strong> <em>did not replicate</em></li>
<li><strong>Source:</strong> <a href="https://pubmed.ncbi.nlm.nih.gov/26214168/" rel="nofollow noopener noreferrer" target="_blank">Rohrer, Pashler, &amp; Harris (2015)</a></li>
</ul>
<h3>ESP Precognition Effect</h3>
<ul>
<li><strong>Claimed result:</strong> In some cases, people can predict future events "that could not otherwise be anticipated through any known inferential process".</li>
<li><strong>Representative paper:</strong> <a href="https://doi.org/10.1037/a0021524" rel="nofollow noopener noreferrer" target="_blank">Bem (2011)</a></li>
<li><strong>Replication status:</strong> <em>did not replicate</em></li>
<li><strong>Source:</strong> <a href="https://doi.org/10.1037/a0029709" rel="nofollow noopener noreferrer" target="_blank">Galak et al. (2012)</a>, <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0033423" rel="nofollow noopener noreferrer" target="_blank">Ritchie, Wiseman, &amp; French (2012)</a></li>
</ul>
<h3>Cleanliness and Morality Effect</h3>
<ul>
<li><strong>Claimed result:</strong> Being clean or thinking about cleanliness makes people more morally lax.</li>
<li><strong>Representative paper:</strong> <a href="https://doi.org/10.1111/j.1467-9280.2008.02227.x" rel="nofollow noopener noreferrer" target="_blank">Schnall, Benton, &amp; Harvey (2008)</a></li>
<li><strong>Replication status:</strong> <em>did not replicate</em></li>
<li><strong>Source:</strong> <a href="https://doi.org/10.1027/1864-9335/a000186" rel="nofollow noopener noreferrer" target="_blank">Johnson, Cheung, &amp; Donnellan (2014)</a></li>
</ul>
<h3>Glucose and Ego Depletion Effect</h3>
<ul>
<li><strong>Claimed result:</strong> Connected to the debunked ego-depletion effect, this one claims that adding glucose to your blood "recharges" the willpower battery. (For a while, I may have drunk more orange juice than usual after reading Baumeister's book. At least it's healthy-ish.)</li>
<li><strong>Representative paper:</strong> <a href="https://doi.org/10.1037/0022-3514.92.2.325" rel="nofollow noopener noreferrer" target="_blank">Gailliot &amp; Baumeister (2007)</a></li>
<li><strong>Replication status:</strong> <em>did not replicate</em></li>
<li><strong>Source:</strong> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0195666313005072" rel="nofollow noopener noreferrer" target="_blank">Lange &amp; Eggert (2014)</a></li>
</ul>
<h3>Hunger and Risk-Taking Effect</h3>
<ul>
<li><strong>Claimed result:</strong> People exposed to the scent of freshly baked cookies become less sensitive to risk and take more risks to obtain food.</li>
<li><strong>Representative paper:</strong> <a href="https://onlinelibrary.wiley.com/doi/10.1002/bdm.520" rel="nofollow noopener noreferrer" target="_blank">Ditto et al. 2006</a></li>
<li><strong>Replication status:</strong> <em>did not replicate</em></li>
<li><strong>Source:</strong> <a href="https://doi.org/10.1016/j.foodqual.2018.02.014" rel="nofollow noopener noreferrer" target="_blank">Festjens, Bruyneel, &amp; Dewitte (2018)</a></li>
</ul>
<h3>Psychological Distance &amp; Construal Level Theory</h3>
<ul>
<li><strong>Claimed result</strong>: "Psychologically distant" events are processed more abstractly, while "psychologically near" events are processed more concretely. E.g., you worry about the difficulty of a task if you have to do it tomorrow, but you see the same task's attractive side if it is planned far in the future.</li>
<li><strong>Representative paper</strong>: <a href="https://pubmed.ncbi.nlm.nih.gov/20438233/" rel="nofollow noopener noreferrer" target="_blank">Trope &amp; Liberman (2010)</a>, building on <a href="https://nyuscholars.nyu.edu/en/publications/the-role-of-feasibility-and-desirability-considerations-in-near-a" rel="nofollow noopener noreferrer" target="_blank">Liberman &amp; Trope (1998)</a></li>
<li><strong>Replication status</strong>: <em>serious credibility problems</em></li>
<li><strong>Source</strong>: A <a href="https://climr.org/about/" rel="nofollow noopener noreferrer" target="_blank">collaboration</a> between 73 labs around the world is vetting this theory right now because of many doubts about its validity.</li>
</ul>
<h3>Ovulation &amp; Mate Preferences Effect</h3>
<ul>
<li><strong>Claimed result:</strong> Women are more attracted to hot guys during high-fertility days of their cycles.</li>
<li><strong>Representative paper:</strong> <a href="https://doi.org/10.1037/a0035438" rel="nofollow noopener noreferrer" target="_blank">Gildersleeve, Haselton, &amp; Fales (2014)</a></li>
<li><strong>Replication status:</strong> <em>did not replicate</em></li>
<li><strong>Source:</strong> <a href="https://publications.goettingen-research-online.de/bitstream/2/77327/1/10.1177_0956797619882022.pdf" rel="nofollow noopener noreferrer" target="_blank">Stern, Gerlach, &amp; Penke (2020)</a></li>
</ul>
<h3>Marshmallow Test &amp; Long-Term Success Effect</h3>
<ul>
<li><strong>Claimed result:</strong> Children's ability to resist eating a marshmallow when left alone in a room at age 4-5 strongly predicts adolescent achievement, with those who waited longer showing better life outcomes.</li>
<li><strong>Representative paper:</strong> <a href="https://doi.org/10.1037/0012-1649.26.6.978" rel="nofollow noopener noreferrer" target="_blank">Shoda, Mischel, &amp; Peake (1990)</a></li>
<li><strong>Replication status:</strong> <em>did not replicate significantly</em></li>
<li><strong>Source:</strong> <a href="https://doi.org/10.1177/0956797618761661" rel="nofollow noopener noreferrer" target="_blank">Watts, Duncan, &amp; Quan (2018)</a></li>
</ul>
<h3>Stereotype Threat (Women's Math Performance) Effect</h3>
<ul>
<li><strong>Claimed result:</strong> Women risk being judged by the negative stereotype that women have weaker math ability, and this apprehension disrupts their math performance on difficult tests.</li>
<li><strong>Representative paper:</strong> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0022103198913737" rel="nofollow noopener noreferrer" target="_blank">Spencer, Steele, &amp; Quinn (1999)</a></li>
<li><strong>Replication status:</strong> <em>did not replicate</em></li>
<li><strong>Source:</strong> <a href="https://doi.org/10.1016/j.jsp.2014.10.002" rel="nofollow noopener noreferrer" target="_blank">Flore &amp; Wicherts (2015)</a></li>
</ul>
<h3>Smile to Feel Better Effect</h3>
<ul>
<li><strong>Claimed result</strong>: Holding a pen in your teeth (forcing a smile-like expression) makes you rate cartoons as funnier compared to holding a pen with your lips (preventing smiling). More broadly, facial expressions can influence emotional experiences: "fake it till you make it."</li>
<li><strong>Representative paper</strong>: <a href="https://psycnet.apa.org/record/1988-25514-001" rel="nofollow noopener noreferrer" target="_blank">Strack, Martin, &amp; Stepper (1988)</a></li>
<li><strong>Replication status</strong>: <em>did not replicate</em></li>
<li><strong>Source</strong>: <a href="https://journals.sagepub.com/doi/full/10.1177/1745691616674458" rel="nofollow noopener noreferrer" target="_blank">Wagenmakers et (54!) al. (2016)</a></li>
</ul>
<h3>Objective Measurement of Biases</h3>
<ul>
<li><strong>Claimed result</strong>: You can predict if someone is racist by how quickly they answer certain trick questions.</li>
<li><strong>Representative paper</strong>: <a href="https://psycnet.apa.org/doiLanding?doi=10.1037%2F0022-3514.74.6.1464" rel="nofollow noopener noreferrer" target="_blank">Greenwald, McGhee, &amp; Schwartz (1998)</a></li>
<li><strong>Replication status</strong>: <em>mixed evidence with small effects</em></li>
<li><strong>Source</strong>: <a href="https://pubmed.ncbi.nlm.nih.gov/23773046/" rel="nofollow noopener noreferrer" target="_blank">Oswald et al. (2013)</a> shows that the prediction power is small at best.</li>
</ul>
<h3>Mozart Effect</h3>
<ul>
<li><strong>Claimed result</strong>: Listening to Mozart temporarily makes you smarter.</li>
<li><strong>Representative paper</strong>: <a href="https://www.nature.com/articles/365611a0" rel="nofollow noopener noreferrer" target="_blank">Rauscher, Shaw, &amp; Ky (1993)</a></li>
<li><strong>Replication status</strong>: <em>did not replicate</em></li>
<li><strong>Source</strong>: <a href="https://www.sciencedirect.com/science/article/abs/pii/S0160289610000267" rel="nofollow noopener noreferrer" target="_blank">Pietschnig, Voracek, &amp; Formann (2010)</a> (What a title!)</li>
</ul>
<h3>Growth Mindset Interventions</h3>
<ul>
<li><strong>Claimed result:</strong> Teaching students that intelligence is malleable (not fixed) dramatically improves academic performance.</li>
<li><strong>Representative paper:</strong> <a href="https://psycnet.apa.org/doiLanding?doi=10.1037%2F0033-295X.95.2.256" rel="nofollow noopener noreferrer" target="_blank">Dweck, &amp; Leggett (1988)</a></li>
<li><strong>Replication status:</strong> <em>mixed results</em> - many failed replications but also some successful replications</li>
<li><strong>Failed replication source:</strong> <a href="https://pubmed.ncbi.nlm.nih.gov/31464486/" rel="nofollow noopener noreferrer" target="_blank">Li &amp; Bates 2019</a></li>
<li><strong>Notable successful replication:</strong> <a href="https://www.nature.com/articles/s41586-019-1466-y" rel="nofollow noopener noreferrer" target="_blank">Yeager et al. 2019 in Nature</a></li>
</ul>
<h3>Bilinguals Are Smarter</h3>
<ul>
<li><strong>Claimed result:</strong> Being bilingual provides substantial cognitive advantages in attention, task-switching, and executive control.</li>
<li><strong>Representative paper:</strong> <a href="https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(12)00056-3" rel="nofollow noopener noreferrer" target="_blank">Bialystok, Craik, &amp; Luk (2012)</a></li>
<li><strong>Replication status:</strong> <em>did not replicate</em></li>
<li><strong>Source:</strong> <a href="https://pubmed.ncbi.nlm.nih.gov/29494195/" rel="nofollow noopener noreferrer" target="_blank">Lehtonen et al. 2018</a></li>
</ul>
<p>Did I miss any famous debunked studies? Let me know by replying to this newsletter, and I'll add it to the list. ●</p>
<div><p>Cover image:</p><p><em>Photo by Rebecca Freeman, Unsplash</em></p></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Optimizing ClickHouse for Intel's 280 core processors (156 pts)]]></title>
            <link>https://clickhouse.com/blog/optimizing-clickhouse-intel-high-core-count-cpu</link>
            <guid>45279792</guid>
            <pubDate>Wed, 17 Sep 2025 18:46:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://clickhouse.com/blog/optimizing-clickhouse-intel-high-core-count-cpu">https://clickhouse.com/blog/optimizing-clickhouse-intel-high-core-count-cpu</a>, See on <a href="https://news.ycombinator.com/item?id=45279792">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><blockquote>
<p>This is a guest post from Jiebin Sun, Zhiguo Zhou, Wangyang Guo and Tianyou Li, performance optimization engineers at Intel Shanghai.</p>
</blockquote>
<p>Intel's latest processor generations are pushing the number of cores in a server to unprecedented levels - from 128 P-cores per socket in Granite Rapids to 288 E-cores per socket in Sierra Forest, with future roadmaps targeting 200+ cores per socket. These numbers multiply on multi-socket systems, such servers may consist of 400 and more cores. The paradigm of "more, not faster cores" is driven by physical limitations. Since the end of Dennard scaling in the mid-2000s, power density concerns made it increasingly difficult to push single-thread performance further.</p>
<p>For analytical databases like ClickHouse, ultra-high core counts represent a huge opportunity and a complex challenge at the same time. While more cores theoretically mean more power to process tasks in parallel, most databases struggle to utilize the available hardware fully. Bottlenecks for parallel processing  like lock contention, cache coherence, non-uniform memory access (NUMA), memory bandwidth, and coordination overhead become significantly worse as the core count increases.</p>

<p>Over the past three years, I dedicated a part of my professional life to understand and optimize ClickHouse's scalability on Intel Xeon ultra-high core count processors. My work focused on using various profiling and analysis tools - including perf, emon, and Intel VTune - to analyze all 43 ClickBench queries on ultra-high core count servers systematically, identifying bottlenecks, and optimizing the ClickHouse accordingly.</p>
<p>The results have been exciting: individual optimizations routinely deliver speedups of multiple times for individual queries, in some cases up to 10x. The geometric mean of all 43 ClickBench queries consistently improved between 2% and 10% per optimization. The results demonstrate that ClickHouse can be made scale very well on ultra-high core count systems.</p>

<p>Beyond single-thread performance, several key challenges must be addressed to optimize performance in ultra-high core count systems.</p>
<ol>
<li><strong>Cache coherence overhead</strong>: Bouncing cache lines costs CPU cycles.</li>
<li><strong>Lock contention</strong>: Amdahl's Law becomes brutal for serialized code sections as little as 1% of the overall code.</li>
<li><strong>Memory bandwidth</strong>: Utilizing the memory bandwidth effectively is a persistent challenge for data-intensive systems. Proper memory reuse, management and caching becomes critical.</li>
<li><strong>Thread coordination</strong>: The cost of synchronizing threads grows super-linearly with the number of threads.</li>
<li><strong>NUMA effects</strong>: The memory latency and bandwidth on multi-socket systems differs for local or remote memory.</li>
</ol>
<p>This blog post summarizes our optimizations for ClickHouse on ultra-high core count servers. All of them were merged into the main codeline and they now help to speed up queries in ClickHouse deployments around the globe.</p>
<p><strong>Hardware setup</strong>: Our work was conducted on Intel's latest generation platforms, including 2 x 80 vCPUs Ice Lake (ICX), 2 x 128 vCPUs Sapphire Rapids (SPR), 1 x 288 vCPUs Sierra Forest (SRF), and 2 x 240 vCPUs Granite Rapids (GNR). SMT (Hyper-threading) was enabled, except on SRF which doesn't support SMT, and high-memory-bandwidth configurations.</p>
<p><strong>Software setup</strong>: We used perf, Intel VTune, pipeline visualization, and other custom profiling infrastructure.</p>

<p>Through a systematic analysis of ClickHouse's performance on ultra-high core count systems, I identified five areas with a high potential for optimization. Each area addresses a different aspect of scalability, and together they form a comprehensive approach to unlocking the full potential of ultra-high core count systems.</p>
<p>My journey began with the most fundamental challenge: lock contention.</p>
<h2 id="bottleneck-1-lock-contention"><strong>Bottleneck 1: Lock contention</strong> </h2>
<p>According to queue theory, if N threads compete for the same lock, the cycles grows quadratically (N^2). For example, if we go from 8 to 80 cores, lock wait times increase by (80/8)² = 100x. Furthermore, cache coherence traffic for the mutex itself grows linearly with the core count, and the overhead for context switching compounds the problem. In such settings, every mutex becomes a potential scalability obstacle, and seemingly innocent synchronization patterns can bring entire systems to their knee.</p>
<p>The key insight is that lock contention isn't just about removing locks - it's about rethinking more fundamentally how threads coordinate and share state. This requires a multi-pronged approach: reducing the duration of critical sections, replacing exclusive locks (mutexes) with more granular synchronization primitives, and in some cases, eliminating shared state entirely.</p>

<p>After resolving jemalloc page faults (an optimization detailed below), a new hotspot appeared in <code>native_queued_spin_lock_slowpath</code> which consumed 76% of the CPU time. This function was called from <code>QueryConditionCache::write</code> on 2×240 vCPU systems.</p>
<p><strong>What is the query condition cache?</strong></p>
<p><a href="https://clickhouse.com/docs/operations/query-condition-cache">ClickHouse’s query condition cache</a> stores the results of WHERE filters, enabling the database to skip irrelevant data. In each SELECT query, multiple threads check if cache entries must be updated based on different criteria:</p>
<ul>
<li>the hash of the filter condition (as cache key)</li>
<li>the read mark ranges</li>
<li>whether the currently read part has a final mark</li>
</ul>
<p>The query condition cache is read-heavy, i.e. there are far more reads than writes, but the original implementation used exclusive locking for all operations.</p>
<p><strong>Reducing critical paths in read-heavy workloads</strong></p>
<p>This optimization demonstrates the importance of reducing the time spent holding locks, especially write locks in read-heavy code.</p>
<p>With 240 threads within a single query, the original code created a perfect storm:</p>
<ol>
<li><strong>Unnecessary write locks</strong>: All threads acquired exclusive locks, even when they only read cache entries.</li>
<li><strong>Long critical sections</strong>: Expensive updates of cache entries were performed inside exclusive locks.</li>
<li><strong>Redundant work</strong>: Multiple threads updated the same cache entries potentially multiple times.</li>
</ol>
<p>Our optimization uses <a href="https://en.wikipedia.org/wiki/Double-checked_locking">double-checked locking</a> with atomic operations to resolve these bottlenecks:</p>
<ol>
<li>The code now first checks with atomic reads (no locking), respectively under a shared lock if an update is needed at all (fast path).</li>
<li>Next, the code checks immediately after acquiring an exclusive lock (slow path) if an update is actually required - another thread may have performed the same update in the meantime.</li>
</ol>
<p><strong>Implementation</strong></p>
<p>Based on <a href="https://github.com/ClickHouse/ClickHouse/pull/80247/files">PR #80247</a>, the optimization introduces a fast path which checks if an update is needed before acquiring the expensive write lock.</p>
<pre><code><span>/// Original code</span>
<span>void</span> <span>updateCache</span><span>(mark_ranges, has_final_mark)</span>
{
    acquire_exclusive_lock(cache_mutex);  <span>/// 240 threads wait here!</span>

    <span>/// Always update marks, even if already in desired state</span>
    <span>for</span> (<span>const</span> <span>auto</span> &amp; range : mark_ranges)
        set_marks_to_false(range.begin, range.end);

    <span>if</span> (has_final_mark):
        set_final_mark_to_false();

    release_lock(cache_mutex);
}
</code></pre>
<pre><code>
<span>/// Optimized code</span>
<span>void</span> <span>updateCache</span><span>(mark_ranges, has_final_mark)</span>
{
    <span>/// Fast path: Check if update is needed with a cheap shared lock</span>
    acquire_shared_lock(cache_mutex);  <span>/// Multiple threads can read simultaneously</span>

    need_update = <span>false</span>;
    <span>for</span> (<span>const</span> <span>auto</span> &amp; range : mark_ranges)
    {
        <span>if</span> (any_marks_are_true(range.begin, range.end))
        {
            need_update = <span>true</span>;
            <span>break</span>;
        }
    }

    <span>if</span> (has_final_mark &amp;&amp; final_mark_is_true())
        need_update = <span>true</span>;

    release_shared_lock(cache_mutex);

    <span>if</span> (!need_update)
        <span>return</span>;  <span>/// Early out - no expensive lock needed!</span>

    <span>/// Slow path: Actually need to update, acquire exclusive lock</span>
    acquire_exclusive_lock(cache_mutex);

    <span>/// Double-check: verify update is still needed after acquiring lock</span>
    need_update = <span>false</span>;
    <span>for</span> (<span>const</span> <span>auto</span> &amp; range : mark_ranges)
    {
        <span>if</span> (any_marks_are_true(range.begin, range.end))
        {
            need_update = <span>true</span>;
            <span>break</span>;
        }
    }

    <span>if</span> (has_final_mark &amp;&amp; final_mark_is_true())
        need_update = <span>true</span>;

    <span>if</span> (need_update)
    {
        <span>// Perform the actual updates only if still needed</span>
        <span>for</span> (<span>const</span> <span>auto</span> &amp; range : mark_ranges)
            set_marks_to_false(range.begin, range.end);

        <span>if</span> (has_final_mark)
            set_final_mark_to_false();
    }

    release_lock(cache_mutex);
}
</code></pre>
<p><strong>Performance impact</strong></p>
<p>The optimized code delivered impressive performance improvements:</p>
<ul>
<li>CPU cycles spend for <code>native_queued_spin_lock_slowpath</code> reduced from 76% to 1%</li>
<li>The QPS of ClickBench queries Q10 and Q11 improved by 85% and 89%</li>
<li>The geometric mean of all ClickBench queries improved by 8.1%</li>
</ul>

<p>ClickHouse's query profiler was frequently creating and deleting a global timer_id variable, causing lock contention during query profiling.</p>
<p><strong>Query profiler timer usage</strong></p>
<p>ClickHouse's query profiler uses POSIX timers to sample thread stacks in periodic intervals for performance analysis. The original implementation:</p>
<ul>
<li>created and deleted timer_id frequently during profiling, and</li>
<li>required global synchronization for all operations that read or write the timer.</li>
</ul>
<p>Usage of shared data structures that needed protection with locks caused significant overhead.</p>
<p><strong>Eliminating global state with thread-local storage</strong></p>
<p>Here, we eliminated lock contention by thread-local storage, removing the need for shared state. Now, each thread has its own timer_id. This avoids shared state and the overhead of thread synchronization. To update a timer, it is no longer required to acquire locks.</p>
<p><strong>Technical solution</strong></p>
<pre><code><span>/// Original code</span>
<span><span>class</span> <span>QueryProfiler</span>
{</span>
    <span>static</span> global_mutex timer_management_lock

    <span>void</span> <span>startProfiling</span><span>()</span>
    {
        timer_id = create_new_timer();  <span>/// Expensive system call</span>

        acquire_exclusive_lock(timer_management_lock);  <span>/// Global lock!</span>
        update_shared_timer_state(timer_id);  <span>/// Modify shared state</span>
        release_lock(timer_management_lock);
    }

    <span>void</span> <span>stopProfiling</span><span>()</span>
    {
        acquire_exclusive_lock(timer_management_lock);
        cleanup_shared_timer_state(timer_id);
        release_lock(timer_management_lock);

        delete_timer(timer_id);
    }
}
</code></pre>
<pre><code><span>/// Optimized code</span>
<span><span>class</span> <span>QueryProfiler</span>
{</span>
    <span>static</span> <span>thread_local</span> timer_id per_thread_timer;
    <span>static</span> <span>thread_local</span> boolean timer_initialized;

    <span>void</span> <span>startProfiling</span><span>()</span>
    {
        <span>if</span> (!timer_initialized)
        {
            per_thread_timer = create_new_timer();  <span>/// Once per thread</span>
            timer_initialized = <span>true</span>;
        }

        <span>/// Reuse existing timer - no locks, no system calls!</span>
        enable_timer(per_thread_timer);
    }

    <span>void</span> <span>stopProfiling</span><span>()</span>
    {
        <span>/// Just disable timer - no deletion, no locks!</span>
        disable_timer(per_thread_timer);
    }
}
</code></pre>
<p><strong>Performance impact</strong></p>
<p>The new implementation has the following advantages:</p>
<ul>
<li>It eliminated timer-related lock contention hotspots from profiling traces</li>
<li>It reduced timer create/delete system calls through reuse</li>
<li>It makes profiling on ultra-high core count servers more scalable.</li>
</ul>
<p>Thread-local storage can eliminate lock contention by removing the need for shared state. Global synchronization becomes unnecessary if threads maintain their own state.</p>

<p>Memory optimization on ultra-high core count systems differs a lot from single-threaded memory management. Memory allocators themselves become contention points, memory bandwidth is divided across more cores, and allocation patterns that work fine on small systems can create cascading performance problems at scale. It is crucial to be mindful of how much memory is allocated and how memory is used.</p>
<p>This class of optimizations involves the allocator’s behavior, reducing pressure on memory bandwidth, and sometimes completely rethinking algorithms to eliminate memory-intensive operations altogether.</p>

<p>This optimization is motivated by high page fault rates and excessive resident memory usage which we observed for certain aggregation queries on ultra-high core count systems.</p>
<p><strong>Understanding two-level hash tables in ClickHouse</strong></p>
<p>Aggregation in ClickHouse uses different hash tables, depending on the data type, data distribution and data size. Large aggregation states are maintained in ephemeral hash tables.</p>
<ul>
<li>The <strong>1st level</strong> consists of 256 static buckets, each pointing to a 2nd level hash table.</li>
<li><strong>2nd level</strong> hash tables grow independently of each other.</li>
</ul>
<p><strong>Memory reuse for two-level hash tables</strong></p>
<p>At the end of an aggregation query, all hash tables used by the query are deallocated. In particular, the 256 sub-hash tables are deallocated and their memory is merged into larger free memory blocks.</p>
<p>jemalloc (as ClickHouse’s memory allocator) unfortunately prevented the reuse of merged memory blocks for future smaller allocations. This is because by default, only memory from blocks up to 64x larger than the requested size can be reused. This issue in jemalloc is very subtle but critical on ultra-high core count systems.</p>
<p>Based on <a href="https://github.com/jemalloc/jemalloc/pull/2842">jemalloc issue #2842</a>, we noticed a fundamental problem with jemalloc’s memory reuse for the irregularly-sized allocations typical in two-level hash tables:</p>
<ol>
<li><strong>Extent management issue</strong>: When large allocations are freed, jemalloc fails to efficiently track and reuse these memory extents.</li>
<li><strong>Size class fragmentation</strong>: Memory gets trapped in size classes that don't match future allocation patterns.</li>
<li><strong>Metadata overhead</strong>: Excessive metadata structures prevent efficient memory coalescing.</li>
<li><strong>Page fault amplification</strong>: New allocations trigger page faults instead of reusing existing committed pages.</li>
</ol>
<p>We identified jemalloc's <code>lg_extent_max_active_fit</code> parameter as the root cause - it was too restrictive for ClickHouse's allocation patterns.</p>
<p>We contributed the fix to <a href="https://github.com/jemalloc/jemalloc/pull/2842">jemalloc PR #2842</a>, but jemalloc didn’t have new stable releases for an extended period. Fortunately, we could resolve this issue through jemalloc's configuration parameters at compilation time.</p>
<p>Based on ClickHouse <a href="https://github.com/ClickHouse/ClickHouse/pull/80245">PR #80245</a>, the fix involved tuning jemalloc's configuration parameters:</p>
<pre><code><span>/// Original jemalloc configuration</span>
JEMALLOC_CONFIG_MALLOC_CONF = <span>"oversize_threshold:0,muzzy_decay_ms:0,dirty_decay_ms:5000"</span>
<span>/// lg_extent_max_active_fit defaults to 6, meaning memory can be reused from extents up to 64x larger than the requested allocation size</span>
</code></pre>
<pre><code><span>/// Optimized jemalloc configuration</span>
JEMALLOC_CONFIG_MALLOC_CONF = <span>"oversize_threshold:0,muzzy_decay_ms:0,dirty_decay_ms:5000,lg_extent_max_active_fit:8"</span>
<span>/// lg_extent_max_active_fit is set to 8.</span>
<span>/// This allows memory reuse from extents up to 256x larger</span>
<span>/// than the requested allocation size (2^8 = 256x vs default 2^6 = 64x).</span>
<span>/// The 256x limit matches ClickHouse's two-level hash table structure (256 buckets).</span>
<span>/// This enables efficient reuse of merged hash table memory blocks.</span>
</code></pre>
<p><strong>Performance impact</strong></p>
<p>The optimization improved</p>
<ul>
<li>the performance of ClickBench query Q35 by 96.1%,</li>
<li>memory usage (VmRSS, resident memory) and page faults reduced for the same query went down by 45.4% and 71%, respectively.</li>
</ul>
<p>The behavior of the memory allocator can have a dramatic impact on ultra-high core count systems.</p>

<p>ClickBench query Q29 was memory-bound and bottlenecked in excessive memory accesses caused by redundant computations of the form <code>sum(column + literal)</code>.</p>
<p><strong>Understanding the memory bottleneck</strong></p>
<p>ClickBench query Q29 contains multiple sum expressions with literals:</p>
<pre><code><span>SELECT</span> <span>SUM</span>(ResolutionWidth), <span>SUM</span>(ResolutionWidth <span>+</span> <span>1</span>), <span>SUM</span>(ResolutionWidth <span>+</span> <span>2</span>), 
       <span>SUM</span>(ResolutionWidth <span>+</span> <span>3</span>), <span>SUM</span>(ResolutionWidth <span>+</span> <span>4</span>), <span>SUM</span>(ResolutionWidth <span>+</span> <span>5</span>), 
       <span>SUM</span>(ResolutionWidth <span>+</span> <span>6</span>), <span>SUM</span>(ResolutionWidth <span>+</span> <span>7</span>), <span>SUM</span>(ResolutionWidth <span>+</span> <span>8</span>), 
       <span>SUM</span>(ResolutionWidth <span>+</span> <span>9</span>), <span>SUM</span>(ResolutionWidth <span>+</span> <span>10</span>), <span>SUM</span>(ResolutionWidth <span>+</span> <span>11</span>), 
       <span>SUM</span>(ResolutionWidth <span>+</span> <span>12</span>), <span>SUM</span>(ResolutionWidth <span>+</span> <span>13</span>), <span>SUM</span>(ResolutionWidth <span>+</span> <span>14</span>), 
       <span>SUM</span>(ResolutionWidth <span>+</span> <span>15</span>), <span>SUM</span>(ResolutionWidth <span>+</span> <span>16</span>), <span>SUM</span>(ResolutionWidth <span>+</span> <span>17</span>), 
       <span>SUM</span>(ResolutionWidth <span>+</span> <span>18</span>), <span>SUM</span>(ResolutionWidth <span>+</span> <span>19</span>), <span>SUM</span>(ResolutionWidth <span>+</span> <span>20</span>),
       <span>-- ... continues up to SUM(ResolutionWidth + 89)</span>
<span>FROM</span> hits;
</code></pre>
<p>The original query execution</p>
<ol>
<li><strong>Loaded column</strong> “ResolutionWidth” from storage once,</li>
<li><strong>Compute expressions</strong> - 90 times, creating 90 temporary columns (one per expression),</li>
<li><strong>Sum values</strong> performing 90 separate aggregation operations on each computed column.</li>
</ol>
<p>Creating 90 temporary columns and running 90 redundant aggregations obviously created massive memory pressure.</p>
<p><strong>Frontend query optimization for memory efficiency</strong></p>
<p>This optimization demonstrates how better optimizer rules can reduce memory pressure by eliminating redundant computations. The key insight is that many analytical queries contain patterns that can be algebraically simplified.</p>
<p>The optimization recognizes that <code>sum(column + literal)</code> can be rewritten to <code>sum(column) + count(column) * literal</code>.</p>
<p><strong>Performance impact</strong></p>
<ul>
<li>ClickBench query Q29 sped up by 11.5x on a 2×80 vCPU system.</li>
<li>The geometric mean of all ClickBench queries saw a 5.3% improvement overall.</li>
</ul>
<p>More intelligent query plans can be more effective than optimizing execution itself. Avoiding work is better than doing work efficiently.</p>

<p>Fast aggregation is a core promise of any analytical database. From a database perspective, aggregating data in parallel threads is only one part of the equation. It is equally important to merge the local results in parallel.</p>
<p>ClickHouse's aggregation operator has two phases: In the first phase, each thread processes its portion of the data in parallel, creating a local and partial result. In the second phase, all partial results must be merged. If the merge phase is not properly parallelized, it becomes a bottleneck. More threads can actually make this issue worse by creating more partial results to merge.</p>
<p>Solving this issue requires careful algorithm design, smart data structure choices, and a deep understanding how hash tables behave under different load patterns. The goal is to eliminate the serial merge phase and enable linear scaling even for the most complex aggregation queries.</p>

<p>ClickBench query Q5 showed a severe performance degradation as the core count increased from 80 to 112 threads. Our pipeline analysis revealed serial processing in the hash table conversion.</p>
<p><strong>Understanding hash tables in ClickHouse</strong></p>
<p>ClickHouse uses two types of hash tables for hash aggregation:</p>
<ol>
<li><strong>Single-level hash tables</strong>: This is a flat hash table that is suitable (= faster) for smaller datasets.</li>
<li><strong>Two-level hash tables</strong>: This is a hierarchical hash table with 256 buckets. Two-level hash tables are more amendable to large datasets.</li>
</ol>
<p>The database chooses the right hash table type based on the size of the processed data: Once a single-level hash table reaches a certain threshold during aggregation, it is automatically converted to a two-level hash table. The code to merge hash tables of different types was serialized.</p>
<p><strong>The serial bottleneck</strong></p>
<p>When merging hash tables from different threads,</p>
<ul>
<li><strong>single-level hash tables</strong> were serially merged in a pair-wise manner, e.g. ht1 / ht2 → result, then result / ht3, etc.</li>
<li><strong>two-level hash tables</strong> are merged one-by-one as well but the merge is parallelized across buckets.</li>
</ul>
<p>In the case of mixed single/two-level hash tables, the single-level hash tables had to be converted to two-level hash tables first (this was a serial process). Once the was done, the resulting two-level hash tables could be merged in parallel.</p>
<p>With Q5, increasing the number of threads from 80 to 112 meant that each thread processes less data. With 80 threads, all hash tables were two-level. With 112 threads, the aggregation ended up with the mixed scenario: some hash tables remained single-level while others became two-level. This caused serialization - all single-level hash tables had to be converted to two-level before parallel merging could take place.</p>
<p>To diagnose the issue, pipeline visualization was a crucial tool. The telltale sign was that the merge phase duration increased with thread count - this is the opposite of what should happen.</p>
<p><img src="https://clickhouse.com/uploads/intel_img_1_1481af3982.png" alt="intel_img_1.png" loading="lazy"></p>
<p><em>Performance degradation with increased core count</em></p>
<p><img src="https://clickhouse.com/uploads/intel_img_2_d019431938.png" alt="intel_img_2.png" loading="lazy"></p>
<p><em>Pipeline visualization (max_threads=80) - the merge phase is reasonable</em></p><p><img src="https://clickhouse.com/uploads/intel_img_3_b28b847281.png" alt="intel_img_3.png" loading="lazy"></p>
<p><em>Pipeline visualization (max_threads=112) - the merge phase takes 3.2x longer</em></p><p>Our optimization parallelizes the conversion phase: instead of converting all single-level hash tables to two-level hash tables one by one (serially), we now convert them in parallel. As each hash table can be converted independently, this eliminates the serial bottleneck.</p>
<pre><code><span>/// Original code</span>
<span>void</span> <span>mergeHashTable</span><span>(left_table, right_table)</span>
{
    <span>if</span> (left_table.is_single_level() &amp;&amp; right_table.is_two_level())    
        left_table.convert_to_two_level();  <span>/// Serial conversion blocks threads</span>

    <span>/// Now merge</span>
    merge_sets(left_table, right_table);
}
</code></pre>
<pre><code><span>/// Optimized code</span>
<span>void</span> <span>mergeHashTableParallel</span><span>(all_tables)</span>
{
    <span>/// Phase 1: Parallel conversion</span>
    parallel_tasks = [];
    <span>for</span> (<span>const</span> <span>auto</span> &amp; table : all_tables)
    {
        <span>if</span> (table.is_single_level())
        {
            <span>/// Parallel conversion!</span>
            task = create_parallel_task(table.convert_to_two_level());
            parallel_tasks.add(task);
        }
    }

    <span>/// Wait for all conversions to complete</span>
    wait_for_all_tasks(parallel_tasks);

    <span>/// Phase 2: Now all sets are two-level, merge efficiently.</span>
    <span>for</span> (<span>const</span> <span>auto</span> &amp; <span>pair</span> : all_tables)
        merge_sets(<span>pair</span>.left_table, <span>pair</span>.right_table);
}
</code></pre>
<p><strong>Performance impact</strong></p>
<p>The performance did not improve only for Q5 - the optimization enabled linear scaling for any aggregation-heavy query on ultra-high core count systems.</p>
<p><img src="https://clickhouse.com/uploads/intel_img_4_c4f403312b.png" alt="intel_img_4.png" loading="lazy"></p>
<p><em>Performance improvement after parallel conversion - Q5 achieves 264% improvement</em></p>
<ul>
<li>ClickBench query Q5 improved by a 264% on a 2×112 vCPU system,</li>
<li>24 queries achieved &gt;5% improvement,</li>
<li>the overall geometric mean improved by 7.4%</li>
</ul>
<p>The optimization demonstrates that scalability isn't just about making things more parallel - it's about eliminating serial sections that grow with parallelism. Sometimes you need to restructure algorithms on a more deep level, not just add more threads.</p>

<p>We noticed that the performance was also subpar when all hash tables were single-level.</p>
<p><strong>Extending parallel merge to single-level cases</strong></p>
<p>Building on <a href="https://github.com/ClickHouse/ClickHouse/pull/50748">PR #50748</a>, this optimization recognizes that the benefits of parallel merging are not limited to mixed hash tables. Even when all hash tables are single-level, parallel merging can improve performance if the total data size is large enough.</p>
<p>The challenge was to determine when single-level hash tables should be merged in parallel parallel:</p>
<ul>
<li>If datasets are too small, parallelization introduces extra overhead.</li>
<li>If datasets are too large, parallelization may not be beneficial enough.</li>
</ul>
<p>Based on the implementation in <a href="https://github.com/ClickHouse/ClickHouse/pull/52973/files">PR #52973</a>, the optimization added parallel merges to all single-level cases:</p>
<pre><code><span>/// Before: Only parallelize mixed-level merges</span>
<span>void</span> <span>parallelizeMergePrepare</span><span>(hash_tables)</span>
{
    single_level_count = <span>0</span>;

    <span>for</span> (<span>const</span> <span>auto</span> &amp; hash_table : hash_tables)
        <span>if</span> hash_table.is_single_level():
            single_level_count++;

    <span>/// Only convert if mixed levels (some single, some two-level)</span>
    <span>if</span> single_level_count &gt; <span>0</span> and single_level_count &lt; hash_tables.size():
        convert_to_two_level_parallel(hash_tables);
}
</code></pre>
<pre><code><span>/// Optimized code</span>
<span>void</span> <span>parallelizeMergePrepare</span><span>(hash_tables)</span>:
{
    single_level_count = <span>0</span>;
    all_single_hash_size = <span>0</span>;

    <span>for</span> (<span>const</span> <span>auto</span> &amp; hash_table : hash_tables)
        <span>if</span> (hash_table.is_single_level())
            single_level_count++

    <span>/// Calculate total size if all hash tables are single-level</span>
    <span>if</span> (single_level_count == hash_tables.size())
        <span>for</span> (<span>const</span> <span>auto</span> &amp; hash_table : hash_tables)
            all_single_hash_size += hash_table.size();

    <span>/// Convert if mixed levels OR if all single-level with average size &gt; THRESHOLD</span>
    <span>if</span> (single_level_count &gt; <span>0</span> and single_level_count &lt; hash_tables.size())
        ||
       (all_single_hash_size / hash_tables.size() &gt; THRESHOLD)
        convert_to_two_level_parallel(hash_tables);
}
</code></pre>
<p><strong>Performance impact</strong></p>
<ul>
<li>Performance for single-level merge scenarios improved by 235%</li>
<li>The optimal threshold was determined through systematic testing</li>
<li>There were no regressions on small datasets</li>
</ul>

<p>GROUP BY operations with large hash tables were merged serially.</p>
<p><strong>Extending parallelization to keyed aggregations</strong></p>
<p>The previous two optimizations (3.1 and 3.2) addressed merges without key - simple hash table operations like <code>COUNT(DISTINCT)</code>. We applied the same optimization to merges with key where hash tables contain both keys and aggregated values that must be combined, e.g. general <code>GROUP BY</code> semantics.</p>
<p><strong>Performance Impact</strong>:</p>
<ul>
<li>ClickBench query Q8 improved by 10.3%, Q9 by 7.6%</li>
<li>There were no regressions in other queries</li>
<li>CPU utilization during the merge phase improved</li>
</ul>
<p>Parallel merging can be extended to complex aggregation scenarios with careful attention to cancellation and error handling.</p>

<p>Harnessing the full potential of SIMD instructions is notoriously difficult. Compilers are conservative about vectorization, and database workloads often have complex control flows that inhibit auto-vectorization.</p>
<p>Effective usage of SIMD instructions in databases requires thinking beyond traditional vectorization. Besides processing N data items simultaneously instead of one, one can also utilize parallel SIMD comparisons for smart pruning strategies which lead to less work done overall. This idea is particularly powerful for string operations. These are at the same time frequently used in practice and computationally expensive.</p>

<p>String search (e.g. plain substring search or LIKE pattern search) is a bottleneck in a lot of queries, for example in ClickBench query Q20.</p>
<p><strong>Understanding string search in analytical queries</strong></p>
<p>Clickbench query 20 evaluates a LIKE pattern on millions of URLs, making fast string search crucial.</p>
<pre><code><span>SELECT</span> <span>COUNT</span>(<span>*</span>) <span>FROM</span> hits <span>WHERE</span> URL <span>LIKE</span> <span>'%google%'</span>
</code></pre>
<p><strong>Reducing false positives with two-character filtering</strong></p>
<p><a href="https://github.com/ClickHouse/ClickHouse/pull/46289/files">PR #46289</a> is based on the insight that SIMD instructions can be used in a smart way beyond brute-force parallelization. The original code already leveraged SIMD instructions but it only considered the search pattern’s first character, leading to expensive false positives. We rewrite the code to check the second character as well. This improved selectivity dramatically while adding only a negligible amount of new SIMD operations.</p>
<pre><code><span>/// Original code</span>
<span><span>class</span> <span>StringSearcher</span>
{</span>
    first_needle_character = needle[<span>0</span>];
    first_needle_character_vec = broadcast_to_simd_vector(first_needle_character);

    <span>void</span> <span>search</span><span>()</span>
    {
        <span>for</span> (position in haystack; step by <span>16</span> bytes)
        {
            haystack_chunk = load_16_bytes(haystack + position);
            first_matches = simd_compare_equal(haystack_chunk, first_needle_character_vec);
            match_mask = extract_match_positions(first_matches);

            <span>for</span> (<span>const</span> <span>auto</span> &amp; match : match_mask)
                <span>/// High false positive rate - many expensive verifications</span>
                <span>if</span> (full_string_match(haystack + match_pos, needle))
                    <span>return</span> match_pos;
        }
    }
}
</code></pre>
<pre><code><span>// Optimized code</span>
<span><span>class</span> <span>StringSearcher</span>
{</span>
    first_needle_character = needle[<span>0</span>];
    second_needle_character = needle[<span>1</span>];  <span>/// Second character</span>
    first_needle_character_vec = broadcast_to_simd_vector(first_needle_character);
    second_needle_character_vec = broadcast_to_simd_vector(second_needle_character);

    <span>void</span> <span>search</span><span>()</span>
    {
        <span>for</span> (position : haystack, step by <span>16</span> bytes)
        {
            haystack_chunk1 = load_16_bytes(haystack + position);
            haystack_chunk2 = load_16_bytes(haystack + position + <span>1</span>);

            <span>/// Compare both characters simultaneously</span>
            first_matches = simd_compare_equal(haystack_chunk1, first_needle_character_vec);
            second_matches = simd_compare_equal(haystack_chunk2, second_needle_character_vec);
            combined_matches = simd_and(first_matches, second_matches);

            match_mask = extract_match_positions(combined_matches);

            <span>for</span> (<span>const</span> <span>auto</span> &amp; match : match_mask)
                <span>// Dramatically fewer false positives - fewer expensive verifications</span>
                <span>if</span> <span>full_string_match</span><span>(haystack + match_pos, needle)</span>:
                    <span>return</span> match_pos;
        }
    }
}
</code></pre>
<p><strong>Performance impact</strong></p>
<p>Two-character SIMD filtering improved performance significantly:</p>
<ul>
<li>ClickBench query Q20 sped up by 35%</li>
<li>Other queries which perform substring matching saw an overall improvement of ~10%</li>
<li>The geometric mean of all queries improved by 4.1%</li>
</ul>
<p>The performance improvements are a result of fewer false positives, better cache locality and more efficient branch prediction.</p>
<p>Two-character SIMD filtering demonstrates that effective SIMD optimization isn't just about processing more data per instruction - it's about using SIMD's parallel comparison capabilities to improve the algorithmic efficiency. The two-character approach shows how a small number of additional SIMD operations can in some cases yield massive performance gains.</p>

<p>False sharing occurs when multiple threads access variables in the same cache. The CPU's cache coherence protocol works at cache line granularity, meaning that any cache line modifications - including modifications of two different variables - are treated as conflicts which require expensive synchronization between cores. On a 2 x 240 vCPUs system, false sharing can turn simple counter increments into system-wide performance disasters.</p>
<p>Eliminating false sharing requires how CPU cache coherence is implemented at the hardware level. It's not enough to optimize algorithms - to avoid false sharing, one must also optimize the memory layout to make sure that frequently-accessed data structures don't accidentally interfere with each other through cache line conflicts. This involves for example a strategic data layout and use of alignment and padding.</p>

<p>ClickBench query Q3 showed 36.6% of CPU cycles spent in <code>ProfileEvents::increment</code> on a 2×240 vCPU system. Performance profiling revealed a severe cache line contention.</p>
<p><strong>ProfileEvents counters at scale</strong></p>
<p>Profile event counters refer to ClickHouse's internal eventing system - profile events track all internal operations, from detailed query execution steps to memory allocations. In a typical analytical query, these counters are incremented millions of times across all threads. The original implementation organized multiple counters in the same memory region without considering cache line boundaries.</p>
<p>This creates three challenges:</p>
<ol>
<li>
<p><strong>Cache line physics</strong>: Modern Intel processors use 64-byte cache lines. When any byte in a cache line is modified, the entire line must be invalidated in the other cores' caches.</p>
</li>
<li>
<p><strong>False sharing amplification</strong>: With 240 threads, each counter update triggers a cache line invalidation across potentially dozens of cores. What should be independent operations become serialized through the cache coherence protocol.</p>
</li>
<li>
<p><strong>Exponential degradation</strong>: As the number of cores increases, the probability of a simultaneous access to the same cache line grows exponentially, compounding the impact of cache misses.</p>
</li>
</ol>
<p>Using perf, I discovered that <code>ProfileEvents::increment</code> was generating massive cache coherence traffic. The smoking gun was the cache line utilization report that showed eight different counters packed into a single cache line. We also added new capabilities to Linux’s perf c2c tool and worked with the community to help developers more easily identify false sharing issues like this.</p>
<p><img src="https://clickhouse.com/uploads/intel_img_5_64dd7ef454.png" alt="intel_img_5.png" loading="lazy"></p>
<p><em>Perf analysis showing 36.6% cycles in ProfileEvents::increment</em></p><p>Proper cache line alignment ensures that each counter gets its own 64-byte cache line. This transforms false sharing (bad) into true sharing (manageable). When a thread updates its counter, now only a single cache line wil be affected.</p>
<p>Based on our implementation in <a href="https://github.com/ClickHouse/ClickHouse/pull/82697/files">PR #82697</a>, the fix improved the cache line alignment for the profile event counters:</p>
<pre><code><span>// Before: Counters packed without alignment</span>
<span><span>struct</span> <span>ProfileEvents</span>:</span>
    <span>atomic_value</span> counters[NUM_EVENTS]  <span>// Multiple counters per cache line</span>
    <span>// 8 counters sharing single 64-byte cache lines</span>

<span>// After: Cache line aligned counters  </span>
<span><span>struct</span> <span>ProfileEvents</span>:</span>
    <span>struct</span> <span>alignas</span><span>(<span>64</span>)</span> AlignedCounter:
        <span>atomic_value</span> value
        <span>// Padding automatically added to reach 64 bytes</span>
    
    AlignedCounter counters[NUM_EVENTS]  <span>// Each counter gets own cache line</span>
    <span>// Now each counter has exclusive cache line ownership</span>
</code></pre>
<p><strong>Performance impact</strong></p>
<p>This optimization pattern applies to any frequently updated shared and compact data structure. The lesson is that the memory layout becomes critical at scale - what works fine on eight cores can be excruciatingly slow on 240 cores.</p>
<p><img src="https://clickhouse.com/uploads/intel_img_6_d32f81bea1.png" alt="intel_img_6.png" loading="lazy"></p>
<p><em>After optimization: ProfileEvents::increment drops to 8.5% (from 36.6%)</em></p><p>As a result of our optimization, ClickBench query Q3 saw a 27.4% improvement on ultra-high core count systems. The performance gain increases with the number of cores because the cache coherence overhead grows super-linearly. This optimization therefore doesn't merely fix a bottleneck - it changes the scalability curve.</p>
<p><img src="https://clickhouse.com/uploads/intel_img_7_651eaa2f76.png" alt="intel_img_7.png" loading="lazy"></p>
<p><em>ClickBench Q3: 27.4% improvement, with larger gains on higher core count systems</em></p>
<p>In this post I covered optimizations for five performance bottlenecks:</p>
<ol>
<li><strong>Lock contention</strong> - The coordination overhead grows exponentially with core count.</li>
<li><strong>Memory optimization</strong> - The memory bandwidth per core decreases as the core count increases.</li>
<li><strong>Increased parallelism</strong> - Serial phases become the dominant bottleneck.</li>
<li><strong>SIMD optimization</strong> - Smarter algorithms like two-character filtering beyond brute-force vectorization can improve performance significantly.</li>
<li><strong>False sharing</strong> - False sharing is caused by the granularity of cache line size.</li>
</ol>
<p>The bottlenecks and optimizations presented here are not just about ClickHouse - they represent a fundamental shift in how we must approach database optimization in the ultra-high core count era. As processors continue to evolve toward higher core counts, these techniques will become essential for any system that needs to scale.</p>
<p>Our optimizations enable ClickHouse to achieve close-to-linear scalability as the core count increases. This enables ClickHouse to thrive as an analytics database in a future world where Intel and other hardware manufacturers push the core count into the thousands.</p>
<p><img src="https://clickhouse.com/uploads/Team2_16ed51dacb.jpg" alt="Team2.jpg" loading="lazy"></p>
<hr>
<h2 id="references-and-resources"><strong>References and Resources</strong> </h2>
<ul>
<li><strong>Source Code</strong>: All optimizations available in ClickHouse main branch</li>
<li><strong>Slide Deck</strong>: <a href="https://github.com/ClickHouse/clickhouse-presentations/blob/master/2025-meetup-Shanghai-1/Talk%204%20-%20Intel%20-%20Shanghai%20Meetup_01Mar25.pdf">2025 Shanghai Meetup Presentation</a></li>
<li><strong>Pull Requests</strong>: Individual PRs linked throughout this post with detailed performance analysis</li>
<li><strong>Intel Intrinsics Guide</strong>: <a href="https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html">Intel® Intrinsics Guide</a></li>
</ul>

<p>Special thanks to the ClickHouse community for rigorous code review and performance validation. These optimizations represent collaborative effort between Intel and ClickHouse teams to unlock the full potential of modern ultra-high core count processors.</p>
<hr>
<p><em>For questions about implementation details or performance reproduction, please refer to the individual PR discussions linked throughout this post.</em></p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WASM 3.0 Completed (729 pts)]]></title>
            <link>https://webassembly.org/news/2025-09-17-wasm-3.0/</link>
            <guid>45279384</guid>
            <pubDate>Wed, 17 Sep 2025 18:16:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://webassembly.org/news/2025-09-17-wasm-3.0/">https://webassembly.org/news/2025-09-17-wasm-3.0/</a>, See on <a href="https://news.ycombinator.com/item?id=45279384">Hacker News</a></p>
<div id="readability-page-1" class="page">
    <header>
      
    </header>
    

<div>
    

<p><em>Published on September 17, 2025 by <a href="https://github.com/rossberg">Andreas Rossberg</a>.</em></p>

<p>Three years ago, <a href="https://webassembly.org/news/2025-03-20-wasm-2.0/">version 2.0</a> of the Wasm standard was (essentially) finished, which brought a number of new features, such as vector instructions, bulk memory operations, multiple return values, and simple reference types.</p>

<p>In the meantime, the Wasm W3C Community Group and Working Group have not been lazy. Today, we are happy to announce the release of Wasm 3.0 as the new “live” standard.</p>

<p><img src="https://webassembly.org/assets/wasm3_0.png" alt="Title page of the WebAssembly Specification, Release 3.0, 2025-09-17"></p>

<p>This is a substantially larger update: several big features, some of which have been in the making for six or eight years, finally made it over the finishing line.</p>

<ul>
  <li>
    <p><a href="https://github.com/WebAssembly/spec/blob/wasm-3.0/proposals/memory64/Overview.md"><em>64-bit address space.</em></a> Memories and tables can now be declared to use <code>i64</code> as their address type instead of just <code>i32</code>. That expands the available address space of Wasm applications from 4 gigabytes to (theoretically) 16 exabytes, to the extent that physical hardware allows. While the web will necessarily keep enforcing certain limits — on the web, a 64-bit memory is limited to 16 gigabytes — the new flexibility is especially interesting for non-web ecosystems using Wasm, as they can support much, much larger applications and data sets now.</p>
  </li>
  <li>
    <p><a href="https://github.com/WebAssembly/spec/blob/wasm-3.0/proposals/multi-memory/Overview.md"><em>Multiple memories.</em></a> Contrary to popular belief, Wasm applications were always able to use multiple memory objects — and hence multiple address spaces — simultaneously. However, previously that was only possible by declaring and accessing each of them in separate modules. This gap has been closed, a single module can now declare (define or import) multiple memories and directly access them, including directly copying data between them. This finally allows tools like wasm-merge, which perform “static linking” on two or more Wasm modules by merging them into one, to work for <em>all</em> Wasm modules. It also paves the way for new uses of separate address spaces, e.g., for security (separating private data), for buffering, or for instrumentation.</p>
  </li>
  <li>
    <p><a href="https://github.com/WebAssembly/spec/blob/wasm-3.0/proposals/gc/Overview.md"><em>Garbage collection.</em></a> In addition to expanding the capabilities of raw linear memories, Wasm also adds support for a new (and separate) form of storage that is automatically managed by the Wasm runtime via a garbage collector. Staying true to the spirit of Wasm as a low-level language, Wasm GC is low-level as well: a compiler targeting Wasm can declare the memory layout of its runtime data structures in terms of struct and array types, plus unboxed tagged integers, whose allocation and lifetime is then handled by Wasm. But that’s it. Everything else, such as engineering suitable representations for source-language values, including implementation details like method tables, remains the responsibility of compilers targeting Wasm. There are no built-in object systems, nor closures or other higher-level constructs — which would inevitably be heavily biased towards specific languages. Instead, Wasm only provides the basic building blocks for representing such constructs and focuses purely on the memory management aspect.</p>
  </li>
  <li>
    <p><a href="https://github.com/WebAssembly/spec/blob/wasm-3.0/proposals/function-references/Overview.md"><em>Typed references.</em></a> The GC extension is built upon a substantial extension to the Wasm type system, which now supports much richer forms of references. Reference types can now describe the exact shape of the referenced heap value, avoiding additional runtime checks that would otherwise be needed to ensure safety. This more expressive typing mechanism, including subtyping and type recursion, is also available for function references, making it possible to perform safe indirect function calls without any runtime type or bounds check, through the new <code>call_ref</code> instruction.</p>
  </li>
  <li>
    <p><a href="https://github.com/WebAssembly/spec/blob/wasm-3.0/proposals/tail-call/Overview.md"><em>Tail calls.</em></a> Tail calls are a variant of function calls that immediately exit the current function, and thereby avoid taking up additional stack space. Tail calls are an important mechanism that is used in various language implementations both in user-visible ways (e.g., in functional languages) and for internal techniques (e.g., to implement stubs). Wasm tail calls are fully general and work for callees both selected statically (by function index) and dynamically (by reference or table).</p>
  </li>
  <li>
    <p><a href="https://github.com/WebAssembly/spec/blob/wasm-3.0/proposals/exception-handling/Exceptions.md"><em>Exception handling.</em></a> Exceptions provide a way to locally abort execution, and are a common feature in modern programming languages. Previously, there was no efficient way to compile exception handling to Wasm, and existing compilers typically resorted to convoluted ways of implementing them by escaping to the host language, e.g., JavaScript. This was neither portable nor efficient. Wasm 3.0 hence provides native exception handling within Wasm. Exceptions are defined by declaring exception tags with associated payload data. As one would expect, an exception can be thrown, and selectively be caught by a surrounding handler, based on its tag. Exception handlers are a new form of block instruction that includes a dispatch list of tag/label pairs or catch-all labels to define where to jump when an exception occurs.</p>
  </li>
  <li>
    <p><a href="https://github.com/WebAssembly/spec/blob/wasm-3.0/proposals/relaxed-simd/Overview.md"><em>Relaxed vector instructions.</em></a> Wasm 2.0 added a large set of vector (SIMD) instructions, but due to differences in hardware, some of these instructions have to do extra work on some platforms to achieve the specified semantics. In order to squeeze out maximum performance, Wasm 3.0 introduces “relaxed” variants of these instructions that are allowed to have implementation-dependent behavior in certain edge cases. This behavior must be selected from a pre-specified set of legal choices.</p>
  </li>
  <li>
    <p><a href="https://github.com/WebAssembly/profiles/blob/main/proposals/profiles/Overview.md"><em>Deterministic profile.</em></a> To make up for the added semantic fuzziness of relaxed vector instructions, and in order to support settings that demand or need deterministic execution semantics (such as blockchains, or replayable systems), the Wasm standard now specifies a deterministic default behavior for every instruction with otherwise non-deterministic results — currently, this includes floating-point operators and their generated NaN values and the aforementioned relaxed vector instructions. Between platforms choosing to implement this deterministic execution profile, Wasm thereby is fully deterministic, reproducible, and portable.</p>
  </li>
  <li>
    <p><a href="https://github.com/WebAssembly/spec/blob/wasm-3.0/proposals/annotations/Overview.md"><em>Custom annotation syntax.</em></a> Finally, the Wasm text format has been enriched with generic syntax for placing annotations in Wasm source code. Analogous to custom sections in the binary format, these annotations are not assigned any meaning by the Wasm standard itself, and can be chosen to be ignored by implementations. However, they provide a way to represent the information stored in custom sections in human-readable and writable form, and concrete annotations can be specified by downstream standards.</p>
  </li>
</ul>

<p>In addition to these core features, embeddings of Wasm into JavaScript benefit from a new extension to the JS API:</p>

<ul>
  <li><a href="https://github.com/WebAssembly/js-string-builtins/blob/main/proposals/js-string-builtins/Overview.md"><em>JS string builtins.</em></a> JavaScript string values can already be passed to Wasm as externrefs. Functions from this new primitive library can be imported into a Wasm module to directly access and manipulate such external string values inside Wasm.</li>
</ul>

<p>With these new features, Wasm has much better support for compiling high-level programming languages. Enabled by this, we have seen various new languages popping up to target Wasm, such as <a href="https://github.com/google/j2cl/blob/master/docs/getting-started-j2wasm.md">Java</a>, <a href="https://dune.readthedocs.io/en/stable/wasmoo.html">OCaml</a>, <a href="https://www.scala-js.org/doc/project/webassembly.html">Scala</a>, <a href="https://kotlinlang.org/docs/wasm-overview.html">Kotlin</a>, <a href="https://spritely.institute/hoot/">Scheme</a>, or <a href="https://dart.dev/web/wasm">Dart</a>, all of which use the new GC feature.</p>

<p>On top of all these goodies, Wasm 3.0 also is the first version of the standard that has been produced with the new <a href="https://webassembly.org/news/2025-03-27-spectec/">SpecTec</a> tool chain. We believe that this makes for an even more reliable specification.</p>

<p>Wasm 3.0 is already shipping in most major web browsers, and support in stand-alone engines like Wasmtime is on track to completion as well. The <a href="https://webassembly.org/features/">Wasm feature status</a> page tracks support across engines.</p>

  </div>
  
  

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[DeepMind and OpenAI win gold at ICPC (184 pts)]]></title>
            <link>https://codeforces.com/blog/entry/146536</link>
            <guid>45279357</guid>
            <pubDate>Wed, 17 Sep 2025 18:15:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://codeforces.com/blog/entry/146536">https://codeforces.com/blog/entry/146536</a>, See on <a href="https://news.ycombinator.com/item?id=45279357">Hacker News</a></p>
Couldn't get https://codeforces.com/blog/entry/146536: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Anthropic irks White House with limits on models’ use (219 pts)]]></title>
            <link>https://www.semafor.com/article/09/17/2025/anthropic-irks-white-house-with-limits-on-models-uswhite-house-with-limits-on-models-use</link>
            <guid>45279143</guid>
            <pubDate>Wed, 17 Sep 2025 17:57:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.semafor.com/article/09/17/2025/anthropic-irks-white-house-with-limits-on-models-uswhite-house-with-limits-on-models-use">https://www.semafor.com/article/09/17/2025/anthropic-irks-white-house-with-limits-on-models-uswhite-house-with-limits-on-models-use</a>, See on <a href="https://news.ycombinator.com/item?id=45279143">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Anthropic is in the midst of a splashy media tour in Washington, but its refusal to allow its models to be used for some law enforcement purposes has deepened hostility to the company inside the Trump administration, two senior officials told Semafor.</p><p>Anthropic recently declined requests by contractors working with federal law enforcement agencies because the company refuses to make an exception allowing its AI tools to be used for some tasks, including surveillance of US citizens, said the officials, who spoke to Semafor on the condition of anonymity.</p><p>The tensions come at a moment when Donald Trump’s White House has championed American AI companies as patriotic bulwarks of global competition —&nbsp;and expect the companies to repay that loyalty. The officials said they worried that Anthropic was selectively enforcing its policies based on politics and using vague terminology to allow its rules to be interpreted broadly.</p><p>For instance, Anthropic currently limits how the FBI, Secret Service and Immigration, and Customs Enforcement can use its AI models because those agencies conduct surveillance, which is prohibited by Anthropic’s <a href="https://www.anthropic.com/legal/aup" rel="noopener" target="_blank">usage policy</a>.</p><p>One of the officials said Anthropic’s position, which has long been in effect, amounts to making a moral judgment about how law enforcement agencies do their jobs.</p><p>The policy doesn’t specifically define what it means by “domestic surveillance” in a law enforcement context and appears to be using the term broadly, creating room for interpretation.</p><p>Other AI model providers also list restrictions on surveillance, but offer more specific examples and often have carveouts for law enforcement activities. OpenAI’s <a href="https://openai.com/policies/usage-policies/" rel="noopener" target="_blank">policy</a>, for instance, prohibits “unauthorized monitoring of individuals,” implying consent for legal monitoring by law enforcement.</p><p>Anthropic declined to comment.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DeepSeek writes less secure code for groups China disfavors (230 pts)]]></title>
            <link>https://www.washingtonpost.com/technology/2025/09/16/deepseek-ai-security/</link>
            <guid>45278740</guid>
            <pubDate>Wed, 17 Sep 2025 17:24:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.washingtonpost.com/technology/2025/09/16/deepseek-ai-security/">https://www.washingtonpost.com/technology/2025/09/16/deepseek-ai-security/</a>, See on <a href="https://news.ycombinator.com/item?id=45278740">Hacker News</a></p>
Couldn't get https://www.washingtonpost.com/technology/2025/09/16/deepseek-ai-security/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Depression Reduces Capacity to Learn to Actively Avoid Aversive Events (178 pts)]]></title>
            <link>https://www.eneuro.org/content/12/9/ENEURO.0034-25.2025</link>
            <guid>45278686</guid>
            <pubDate>Wed, 17 Sep 2025 17:20:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.eneuro.org/content/12/9/ENEURO.0034-25.2025">https://www.eneuro.org/content/12/9/ENEURO.0034-25.2025</a>, See on <a href="https://news.ycombinator.com/item?id=45278686">Hacker News</a></p>
<div id="readability-page-1" class="page"><div class="page" id="page">
	  <div data-node-nid="3811664" id="top-node-3811664--21755481293" data-pisa="eneuro;12/9/ENEURO.0034-25.2025" data-pisa-master="eneuro;ENEURO.0034-25.2025" data-apath="/eneuro/12/9/ENEURO.0034-25.2025.atom" data-hw-author-tooltip-instance="highwire_author_tooltip">
  
  
      <p><span>Research Article</span><span></span><span><span>Research Article: New Research, Cognition and Behavior</span></span></p>
  
  
        
    	<p><span>, <span data-delta="1">Brandon J. Forys</span>, <span data-delta="2">Liz Kalenteridis</span>, <span data-delta="3">Ian D. Daly</span>, <span data-delta="4">Alex R. Terpstra</span>, <span data-delta="5">Luke Clark</span>, <span data-delta="6">Stan B. Floresco</span>, <span data-delta="7">Trisha Chakrabarty</span> and <span data-delta="8">Rebecca M. Todd</span></span></p>
  
    	<p><span>eNeuro </span><span>1 September 2025,  </span><span>12 </span><span>(9) </span><span>ENEURO.0034-25.2025; </span><span>https://doi.org/10.1523/ENEURO.0034-25.2025 </span></p>
  
  
  
</div> <!-- /.panel-row-wrapper -->	
	  
  <div xmlns="http://www.w3.org/1999/xhtml" data-highwire-cite-ref-tooltip-instance="highwire_reflinks_tooltip" xmlns:xhtml="http://www.w3.org/1999/xhtml" data-panels-ajax-tab-preloaded="jnl_sfneneuro_tab_art" id="panels-ajax-tab-container-highwire_article_tabs"><div id="abstract-1"><h2>Abstract</h2><p id="p-5">Depression and anxiety are often characterized by altered reward-seeking and avoidance, respectively. Yet less is known about the relationship between depressive symptoms and specific avoidance behaviors. To address this gap, we conducted two studies. In Study 1, undergraduates and online workers completed an uninstructed go/no-go avoidance task (<em>N</em><sub>Total</sub> = 465) as a reverse translation of a rodent paradigm. Participants exhibited a wide range of symptom scores on the Beck Depression Inventory-II (BDI-II), ranging from low to severe. In Study 1, cues were used to signal the response type (go/active vs no-go/inhibitory) required to avoid an aversive sound. Higher depressive scores were associated with poorer acquisition of active avoidance in undergraduates. Overall participants showed lower accuracy for active than inhibitory avoidance. To examine whether the better no-go trial performance reflected a prepotent response to avoid aversive outcomes, in Study 2, undergraduates (<em>N</em><sub>Total</sub> = 330) completed a version of the task that included reward-seeking. Here all participants showed higher accuracy for active reward-seeking and inhibitory avoidance, consistent with a prepotent response to inhibit action to avoid aversive consequences. These findings suggest that in young adults, depressive symptoms are associated with difficulty in overriding prepotent responses to actively avoid aversive outcomes in the absence of reward. This work bridges the gap between preclinical animal models and clinical research, offering insights that could guide the development of more targeted clinical interventions.</p></div><ul><li><a href="https://www.eneuro.org/keyword/avoidance" rel="nofollow">avoidance</a></li><li><a href="https://www.eneuro.org/keyword/beck-depression-inventory" rel="nofollow">Beck Depression Inventory</a></li><li><a href="https://www.eneuro.org/keyword/depression" rel="nofollow">depression</a></li><li><a href="https://www.eneuro.org/keyword/dimensional-approaches" rel="nofollow">dimensional approaches</a></li><li><a href="https://www.eneuro.org/keyword/effort-cost" rel="nofollow">effort cost</a></li><li><a href="https://www.eneuro.org/keyword/translational-research" rel="nofollow">translational research</a></li></ul><div id="sec-1"><h2>Significance Statement</h2><p id="p-6">Translational studies in community samples are crucial for bridging the gap between rodent models, which delineate neural circuitry and pharmacology underlying specific behaviors, and the presentation of mood disorders in clinic settings. Building on rodent studies of avoidance behaviors, thought to be linked to depression, this study examines how depressive symptom scores relate to specific types of avoidance. Our findings revealed that higher depressive symptom scores were associated with reduced capacity to learn active avoidance behaviors, which involved overriding a prepotent response to inhibit action to avoid aversive consequences. This work bridges the gap between preclinical animal models and clinical research, offering insights that may guide the development of more targeted clinical interventions.</p></div><div id="sec-2"><h2>Introduction</h2><p id="p-7">Stimuli that predict aversive events typically evoke avoidance responses aimed at minimizing anticipated threats. Depending on the situation, an active strategy, such as taking an action (walking away), may be most effective, while in other situations, the inhibition of motor output (staying put to avoid detection) may be the more prudent strategy. Although effective in many contexts, these strategies can become maladaptive in depression and anxiety, interfering with goal-directed behavior (<a id="xref-ref-52-1" href="#ref-52">Ottenbreit et al., 2014</a>; <a id="xref-ref-34-1" href="#ref-34">Haskell et al., 2020</a>).</p><p id="p-8">Depression is a leading cause of global disability (<a id="xref-ref-74-1" href="#ref-74">Whiteford et al., 2013</a>; <a id="xref-ref-75-1" href="#ref-75">World Health Organization, 2017</a>), yet its cognitive and behavioral mechanisms remains to be fully understood. The Altered Computations underlying Decision Making (ACDM) framework posits that decision-making biases perpetuate both depression and anxiety (<a id="xref-ref-9-1" href="#ref-9">Bishop and Gagne, 2018</a>). Depression is marked by reduced engagement in reward-seeking, while anxiety by heightened avoidance. In depression, impairments arise from underestimating the probability and value of positive outcomes, and overestimating the effort required to obtain them (<a id="xref-ref-9-2" href="#ref-9">Bishop and Gagne, 2018</a>), ultimately leading to reduced engagement in actions. Supporting this view, individuals with major depressive disorder (MDD) choose high-effort, high-reward options less frequently, anticipate fewer positive experiences, and rate them as less pleasurable (<a id="xref-ref-45-1" href="#ref-45">MacLeod and Salaminiou, 2001</a>; <a id="xref-ref-69-1" href="#ref-69">Treadway et al., 2012</a>; <a id="xref-ref-49-1" href="#ref-49">Mukherjee et al., 2020</a>; <a id="xref-ref-37-1" href="#ref-37">Horne et al., 2021</a>). Although the ACDM primarily distinguishes between depression-related biases in reward-seeking, it also suggests that effort-related impairments may extend to avoidance contexts and contribute to reduced active avoidance.</p><p id="p-9">Despite this, the role of active versus inhibitory forms of avoidance remains underexplored in depression, reflecting broader trends in which negatively valenced systems are predominantly studied in anxiety (<a id="xref-ref-17-1" href="#ref-17">Craske et al., 2009</a>). Cognitive theories of depression emphasize a negativity bias in attention, memory, and future expectations (<a id="xref-ref-47-1" href="#ref-47">Mogg et al., 2006</a>; <a id="xref-ref-26-1" href="#ref-26">Fales et al., 2008</a>; <a id="xref-ref-21-1" href="#ref-21">Disner et al., 2011</a>) but often rely on self-report and lack emphasis on behaviors with translational utility for identifying cross-species neurobiological mechanisms.</p><p id="p-10">Reinforcement learning tasks offer a translational approach for examining negatively valenced systems and have been applied across neuropsychiatric conditions (<a id="xref-ref-24-1" href="#ref-24">Endrass et al., 2011</a>; <a id="xref-ref-54-1" href="#ref-54">Palminteri et al., 2012</a>; <a id="xref-ref-59-1" href="#ref-59">Reinen et al., 2016</a>; <a id="xref-ref-72-1" href="#ref-72">Waltz et al., 2018</a>), including depression (<a id="xref-ref-16-1" href="#ref-16">Chase et al., 2010</a>; <a id="xref-ref-60-1" href="#ref-60">Robinson et al., 2012</a>; <a id="xref-ref-46-1" href="#ref-46">Mkrtchian et al., 2017</a>; <a id="xref-ref-49-2" href="#ref-49">Mukherjee et al., 2020</a>; <a id="xref-ref-64-1" href="#ref-64">Smith et al., 2023</a>). These studies typically involve probabilistic and reversal learning tasks to probe sensitivity to reward and punishment. Findings remain mixed: some report reward-specific impairments in depression (<a id="xref-ref-60-2" href="#ref-60">Robinson et al., 2012</a>), others find broader impairments across valence (<a id="xref-ref-16-2" href="#ref-16">Chase et al., 2010</a>; <a id="xref-ref-46-2" href="#ref-46">Mkrtchian et al., 2017</a>; <a id="xref-ref-49-3" href="#ref-49">Mukherjee et al., 2020</a>) or even heightened punishment sensitivity (<a id="xref-ref-50-1" href="#ref-50">Murphy et al., 2003</a>; <a id="xref-ref-51-1" href="#ref-51">Nord et al., 2018</a>), while others identify learning-specific impairments (<a id="xref-ref-16-3" href="#ref-16">Chase et al., 2010</a>; <a id="xref-ref-49-4" href="#ref-49">Mukherjee et al., 2020</a>). These inconsistencies highlight the need for behavioral assays that isolate avoidance processes and align with cross-species models.</p><p id="p-11">Translational gaps can stem from task design. Human studies typically use secondary reinforcers (i.e., monetary rewards or feedback), with punishment operationalized as monetary loss, and avoidance inferred from decreased selection of high-loss options, often omitting safety signals. In contrast, animal paradigms use primary reinforcers (i.e., shock) and deterministic contingencies and explicitly distinguish between active and inhibitory avoidance (<a id="xref-ref-57-1" href="#ref-57">Piantadosi et al., 2018</a>; <a id="xref-ref-14-1" href="#ref-14">Capuzzo and Floresco, 2020</a>). Although functional magnetic resonance imaging (fMRI) studies show overlapping blood-oxygenation-level-dependent (BOLD) responses to primary and secondary aversive cues, regions like the amygdala are more responsive to primary aversive cues (<a id="xref-ref-20-1" href="#ref-20">Delgado et al., 2011</a>). Importantly, shared BOLD activation does not necessarily imply equivalent neural mechanisms—especially when task features might differ meaningfully. Translating animal behavioral paradigms to humans has been proposed as a promising strategy to enhance cross-species translation and improve psychiatric treatment development (<a id="xref-ref-39-1" href="#ref-39">Kirlic et al., 2017</a>).</p><p id="p-12">To address these gaps, we adapted a validated rodent task designed to assess both active and inhibitory avoidance (<a id="xref-ref-57-2" href="#ref-57">Piantadosi et al., 2018</a>; <a id="xref-ref-14-2" href="#ref-14">Capuzzo and Floresco, 2020</a>) and deployed it in a large online sample. While prior human studies have included related features, our avoidance task was modeled to parallel the original rodent paradigm. Our aim was to examine how depressive symptom severity relates to the ability to learn and flexibly implement active and inhibitory avoidance strategies. Using a dimensional approach aligned with Research Domain Criteria (RDoC) principles, we recruited a nonclinical sample reporting a broad range of depressive symptoms. We hypothesized that higher depressive symptom scores would be associated with impairments in active—but not inhibitory—avoidance, consistent with ACDM predictions of reduced behavioral engagement stemming from effort overestimation.</p></div><div id="sec-3"><h2>Materials and Methods</h2><div id="sec-4"><h3>Study 1 (avoidance)</h3><div id="sec-5"><h4>Participants</h4><p id="p-13">We conducted a power analysis using G*Power to detect a small effect size <span id="inline-formula-1"><span><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mspace width=".1em"></mml:mspace><mml:msup><mml:mi>f</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.02</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:math></span>
</span>, indicating that a sample size of <em>N</em> = 395 was required to achieve 80% power at <em>α</em> = 0.05. To account for the higher attrition rates typically observed in online studies, we recruited additional participants. Undergraduates (<em>N</em><sub>Undergraduates </sub>= 475) and online workers (<em>N</em><sub>OnlineWorkers </sub>= 292; recruited via Prolific; <a href="http://www.prolific.co/">www.prolific.co</a>) consented to perform an active/inhibitory avoidance task (<em>N</em><sub>Total </sub>= 767). Undergraduates from the University of British Columbia Psychology Human Subjects Pool were compensated a 1% point increase in their course grade; Prolific workers were compensated £10.59/h. The online avoidance task was unsupervised and uninstructed to allow for instrumental learning processes. Participants were excluded for several reasons, including failure to complete the pre-task survey, failure of survey attention checks, failure to reach criterion accuracy during acquisition, obtaining a <em>d</em>’ &lt; 0.50 during the intermixed task stage, or failure to complete the task. After cleaning, <em>N</em><sub>Total </sub>= 465 (<em>N</em><sub>Undergraduates </sub>= 278; <em>N</em><sub>OnlineWorkers </sub>= 187) were included in the analyses. For details on participant exclusion rates, see Discussion and Extended Data (Extended Data <a id="xref-supplementary-material-1-1" href="#DC1">Table 1-1</a>). The study was approved by the University of British Columbia Behavioral Research Ethics Board (BREB) under certificate H20-01388. Demographic information can be found in <a id="xref-table-wrap-1-1" href="#T1">Table 1</a>.</p></div><div id="sec-6"><h4>Materials</h4><div id="sec-7"><h5>Stimuli</h5><p id="p-14">The task was created in PsychoPy 2020.1 (RRID: SCR_006571) and distributed via Pavlovia (<a href="http://www.pavlovia.org/">www.pavlovia.org</a>; <a id="xref-ref-55-1" href="#ref-55">Peirce et al., 2019</a>). Simple shapes signaled the type of response (active vs inhibitory) required to avoid an aversive sound. Coauthor I.D.D. recorded a set of screeching and scraping sounds (i.e., knife on glass, fork on plate, metal on slate), from which 45 were pilot-tested for unpleasantness and salience (<em>N</em> = 45). Using 9-point Likert scales, eight sounds with the highest combined ratings (unpleasantness: <em>M</em> = 6.87–7.57; salience: <em>M</em> = 5.82–6.83) and lowest variance were selected. These eight aversive sounds were randomly presented on failed trials and were found to be highly motivating. In Study 1, 92.46% of participants who responded to a debriefing question (<em>N</em> = 464) endorsed the aversive sounds as motivating to avoid. Rapid acquisition of instrumental avoidance responses further supports the functional aversiveness of the stimuli. On successful trials, a white border around the gray background signaled safety.</p></div><div id="sec-8"><h5>Measures</h5><p id="p-15">Depressive and anxiety symptom scores were derived from the clinically validated Beck Depression Inventory-II (BDI-II; <a id="xref-ref-6-1" href="#ref-6">Beck et al., 1988b</a>; <a id="xref-ref-7-1" href="#ref-7">Beck et al., 1996</a>) and the Beck Anxiety Inventory (BAI; <a id="xref-ref-5-1" href="#ref-5">Beck et al., 1988a</a>), respectively. One question (suicidality ideation) was removed from the BDI-II for ethical considerations. BDI-II symptom scores were calculated as a proportion score (BDI-II<sub>score</sub>/BDI-II<sub>max_possible_score</sub>) for each participant. Similarly, BAI scores are reported as proportion scores (BAI<sub>score</sub>/BAI<sub>max_possible_score</sub>) for consistency and comparability.</p></div></div><div id="sec-9"><h4>Procedure</h4><p id="p-16">Participants were tested on a computer-based avoidance task that was reverse-translated from a rodent operant paradigm assessing active/inhibitory avoidance—with some modifications (<a id="xref-ref-57-3" href="#ref-57">Piantadosi et al., 2018</a>; <a id="xref-ref-14-3" href="#ref-14">Capuzzo and Floresco, 2020</a>). Prior to the avoidance task, participants completed an effort calibration and a volume calibration to control for differences in physical ability and computer systems (see <a href="https://osf.io/5sepm/">https://osf.io/5sepm/</a>).</p><div id="sec-10"><h5>Active/inhibitory avoidance task</h5><p id="p-17">Specific shapes (circle or squares; counterbalanced) signaled active (“Go”) versus inhibitory (“No-Go”) responses required to avoid a highly aversive sound. The avoidance task consisted of three task stages: acquisition, intermixed, and reversal (<a id="xref-fig-1-1" href="#F1">Fig. 1<em>A</em></a>). (1) The acquisition stage required learning an active avoidance response. During the acquisition stage, participants had to reach a criterion performance of 80% successful trials within the previous 20 trials (maximum 120 trials). Once acquisition criterion was reached, participants performed an additional 30 “over-learning” active avoidance trials before an unsignaled transition into the next task stage. Participants failing to reach criterion performance during the acquisition stage were excluded from data analysis. (2) The intermixed stage required participants to learn the inhibitory avoidance response while flexibly deploying both active and inhibitory responses. This stage consisted of 120 avoidance trials (60 active, 60 inhibitory), presented in a pseudorandomized order. (3) The reversal stage also consisted of 120 avoidance trials (60 active, 60 inhibitory; pseudorandomized), but with active and inhibitory response contingencies reversed. The multiple task stages allowed us to assess distinct patterns in the acquisition and expression of active/inhibitory avoidance, as well as reversal learning. Importantly, participants were not instructed about the cue–response contingencies to allow the acquisition through reinforcement, to mirror the rodent paradigm the task was translated from.</p><div id="F1"><div><div><p><a href="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/F1.large.jpg?width=800&amp;height=600&amp;carousel=1" title="Study 1: Active/inhibitory avoidance task. A, Schematic of task stages: acquisition, intermixed, and reversal. Green lines represent active avoidance (“Go”) trials, while red lines represent inhibitory avoidance (“No-Go”) trials. The black arrow indicates time progression. During acquisition, participants were required to reach a criterion of 80% correct active avoidance trials over a 20-trial period (i.e., initial acquisition) or complete a maximum of 120 trials. Upon reaching criterion, participants completed 30 over-learning trials, followed by an unsignaled transition into the intermixed stage. In this stage, participants flexibly alternated between active and inhibitory avoidance responses. The reversal stage followed another unsignaled transition, during which the cues signaling active and inhibitory avoidance responses are reversed. B, Successful active avoidance: A blue circle signals an active avoidance trial. The participant makes sufficient keyboard presses (green circles) within 1,200 ms cue period, avoiding an aversive sound, as indicated by the safety signal (white border). C, Failed active avoidance: A blue circle signals an active avoidance trial, but insufficient or no keyboard presses (red circle) were made within the 1,200 ms, resulting in an aversive sound (yellow speaker). D, Successful inhibitory avoidance: A blue square signals an inhibitory avoidance trial. The participant withholds keyboard presses for the full 1,200 ms, avoiding an aversive sound, indicated by the safety signal. E, Failed inhibitory avoidance: A blue square signals an inhibitory avoidance trial, but the participant fails to withhold keyboard presses within the 1,200 ms, resulting in an aversive sound." rel="gallery-fragment-images-593667592" data-figure-caption="<div class=&quot;highwire-markup&quot;><div xmlns=&quot;http://www.w3.org/1999/xhtml&quot;>Study 1: Active/inhibitory avoidance task. <strong><em>A</em></strong>, Schematic of task stages: acquisition, intermixed, and reversal. Green lines represent active avoidance (“Go”) trials, while red lines represent inhibitory avoidance (“No-Go”) trials. The black arrow indicates time progression. During acquisition, participants were required to reach a criterion of 80% correct active avoidance trials over a 20-trial period (i.e., initial acquisition) or complete a maximum of 120 trials. Upon reaching criterion, participants completed 30 over-learning trials, followed by an unsignaled transition into the intermixed stage. In this stage, participants flexibly alternated between active and inhibitory avoidance responses. The reversal stage followed another unsignaled transition, during which the cues signaling active and inhibitory avoidance responses are reversed. <strong><em>B</em></strong>, Successful active avoidance: A blue circle signals an active avoidance trial. The participant makes sufficient keyboard presses (green circles) within 1,200 ms cue period, avoiding an aversive sound, as indicated by the safety signal (white border). <strong><em>C</em></strong>, Failed active avoidance: A blue circle signals an active avoidance trial, but insufficient or no keyboard presses (red circle) were made within the 1,200 ms, resulting in an aversive sound (yellow speaker). <strong><em>D</em></strong>, Successful inhibitory avoidance: A blue square signals an inhibitory avoidance trial. The participant withholds keyboard presses for the full 1,200 ms, avoiding an aversive sound, indicated by the safety signal. <strong><em>E</em></strong>, Failed inhibitory avoidance: A blue square signals an inhibitory avoidance trial, but the participant fails to withhold keyboard presses within the 1,200 ms, resulting in an aversive sound.</div></div>" data-icon-position="" data-hide-link-title="0"><span><img alt="Figure 1." src="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/F1.medium.gif" width="440" height="230" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></a></p></div><ul><li><a href="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/F1.large.jpg?download=true" title="Download Figure 1." data-icon-position="" data-hide-link-title="0">Download figure</a></li><li><a href="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/F1.large.jpg" target="_blank" data-icon-position="" data-hide-link-title="0">Open in new tab</a></li><li><a href="https://www.eneuro.org/highwire/powerpoint/3811685" data-icon-position="" data-hide-link-title="0">Download powerpoint</a></li></ul></div><div xmlns:xhtml="http://www.w3.org/1999/xhtml"><p><span>Figure 1.</span></p><p id="p-18">Study 1: Active/inhibitory avoidance task. <strong><em>A</em></strong>, Schematic of task stages: acquisition, intermixed, and reversal. Green lines represent active avoidance (“Go”) trials, while red lines represent inhibitory avoidance (“No-Go”) trials. The black arrow indicates time progression. During acquisition, participants were required to reach a criterion of 80% correct active avoidance trials over a 20-trial period (i.e., initial acquisition) or complete a maximum of 120 trials. Upon reaching criterion, participants completed 30 over-learning trials, followed by an unsignaled transition into the intermixed stage. In this stage, participants flexibly alternated between active and inhibitory avoidance responses. The reversal stage followed another unsignaled transition, during which the cues signaling active and inhibitory avoidance responses are reversed. <strong><em>B</em></strong>, Successful active avoidance: A blue circle signals an active avoidance trial. The participant makes sufficient keyboard presses (green circles) within 1,200 ms cue period, avoiding an aversive sound, as indicated by the safety signal (white border). <strong><em>C</em></strong>, Failed active avoidance: A blue circle signals an active avoidance trial, but insufficient or no keyboard presses (red circle) were made within the 1,200 ms, resulting in an aversive sound (yellow speaker). <strong><em>D</em></strong>, Successful inhibitory avoidance: A blue square signals an inhibitory avoidance trial. The participant withholds keyboard presses for the full 1,200 ms, avoiding an aversive sound, indicated by the safety signal. <strong><em>E</em></strong>, Failed inhibitory avoidance: A blue square signals an inhibitory avoidance trial, but the participant fails to withhold keyboard presses within the 1,200 ms, resulting in an aversive sound.</p></div></div><p id="p-19">Trials began with a fixation cross onscreen (ISI; 2,000 ms; jittered 1,200 ms). Active avoidance required an effortful response—specifically, 3, 4, or 5 rapid button presses (criterion determined during effort calibration test). On successful active trials (<a id="xref-fig-1-2" href="#F1">Fig. 1<em>B</em></a>), participants made the required response within the cue period (≤1,200 ms), resulting in the avoidance of the aversive sound and the presentation of a safety signal (1,000 ms). On failed active trials (<a id="xref-fig-1-3" href="#F1">Fig. 1<em>C</em></a>), either an insufficient response or no response within the cue period triggered the aversive sound (1,000 ms). On inhibitory trials, participants were required to withhold responding. On successful inhibitory trials (<a id="xref-fig-1-4" href="#F1">Fig. 1<em>D</em></a>), participants made no button presses during the cue period (1,200 ms), resulting in the avoidance of the aversive sound and the presentation of a safety signal (1,000 ms). On failed inhibitory trials (<a id="xref-fig-1-5" href="#F1">Fig. 1<em>E</em></a>), an erroneous button press triggered the aversive sound. A graphical overview of the avoidance task is provided in <a id="xref-fig-1-6" href="#F1">Figure 1</a>.</p></div></div><div id="sec-11"><h4>Statistical analysis</h4><p id="p-20">All analyses were conducted in R 4.2.1 (<a id="xref-ref-58-1" href="#ref-58">R Core Team, 2013</a>) using RStudio (<a id="xref-ref-11-1" href="#ref-11">Booth et al., 2018</a>). Primary outcome measures included proportion correct for active and inhibitory trials across task stages and the number of trials to criterion during acquisition. Within-subjects ANOVAs were used except where otherwise stated. Significant main effects or interactions were followed by pairwise comparisons using the <em>emmeans</em> package (<a id="xref-ref-62-1" href="#ref-62">Searle et al., 1980</a>; <a id="xref-ref-44-1" href="#ref-44">Lenth, 2017</a>), with Tukey's honest significant difference (HSD) correction. Between-subject ANOVAs tested sex and sample effects on BDI-II scores. To examine individual differences, we used regression and linear mixed models (lmerTest; fit by REML, <em>t</em> tests using Satterthwaite's method; <a id="xref-ref-4-1" href="#ref-4">Bates et al., 2015</a>; <a id="xref-ref-41-1" href="#ref-41">Kuznetsova et al., 2017</a>) to assess BDI-II scores effects on task performance. To account for multiple comparisons, the Benjamini–Hochberg false discovery rate (FDR) correction was applied (<a id="xref-ref-8-1" href="#ref-8">Benjamini and Hochberg, 1995</a>). We present only the BDI-II analyses in the main text, while corresponding analyses for BAI scores are presented in Extended Data (Extended Data <a id="xref-supplementary-material-2-1" href="#DC2">Figs. 2-1</a>, <a id="xref-supplementary-material-4-1" href="#DC4">4-1</a>; Extended Data <a id="xref-supplementary-material-5-1" href="#DC5">Tables 3-1</a>, 3-3).</p></div></div><div id="sec-12"><h3>Study 2 (reward-seeking/avoidance)</h3><div id="sec-13"><h4>Participants</h4><p id="p-21">Undergraduate participants (<em>N</em> = 771) from the University of British Columbia Psychology Human Subjects Pool were recruited to perform a reward-seeking/avoidance task. Power analysis procedures were identical to Study 1. Recruitment focused exclusively on undergraduates, as effects in Study 1 were strongest in this population. Compensation was identical to the undergraduate sample in Study 1. To motivate performance during reward-seeking trials, participants were told their accumulated points would contribute to the value of a gift card, although participants ultimately received a $5 gift card regardless of performance. Exclusion criteria were similar to Study 1, with the added requirement that participants reach criterion accuracy during acquisition for both reward-seeking and avoidance trials independently. Because the task was designed as a reinforcement-based learning paradigm—with minimal instructions, no explicit information about contingencies, and no practice trials—and given variability in motivation among undergraduates completing online studies for credit, exclusions rates were higher than expected, resulting in lower-than-ideal power. For details on participant exclusion rates, see Discussion and Extended Data (Extended Data <a id="xref-supplementary-material-1-2" href="#DC1">Table 1-1</a>). After cleaning, the final sample included <em>N</em> = 330 participants (<em>N</em><sub>female</sub> = 245; <em>N</em><sub>male</sub> = 85). The study was approved by the University of British Columbia Behavioral Research Ethics Board (BREB) under certificate H20-01388. Demographic information for Study 2 can be found in <a id="xref-table-wrap-1-2" href="#T1">Table 1</a>.</p><div id="T1"><p><span>Table 1.</span></p><p id="p-22">Demographic information for all participants</p></div></div><div id="sec-14"><h4>Materials</h4><div id="sec-15"><h5>Stimuli and measures</h5><p id="p-25">The task was implemented using PsychoPy and Pavlovia (same as Study 1) with modification to incorporate reward-seeking trials. Stimuli included four simple shapes (blue; square, circle, triangle, hexagon) counterbalanced across response type (active vs inhibitory) and motivational context (reward-seeking vs avoidance). As in Study 1, participants completed questionnaire measures, effort, and volume calibrations procedures.</p></div><div id="sec-16"><h5>Mixed-motivation go/no-go task</h5><p id="p-26">The mixed-motivation task consisted of two stages: (1) an acquisition stage, where participants learned active reward-seeking and active avoidance responses, and (2) an intermixed stage, which required the flexible expression of active and inhibitory responses across reward-seeking and avoidance contexts. During the acquisition stage, participants had to reach 80% accuracy within the previous 20 trials, independently for both active reward-seeking and active avoidance trials. After reaching the acquisition criterion, participants completed 24 “over-learning” trials (12 reward-seeking and 12 avoidance) before an unsignaled transition into the intermixed stage. Participants who failed to reach criterion were excluded from analysis. The intermixed stage consisted of 240 trials (60 of each type—active reward-seeking, inhibitory reward-seeking, active avoidance, inhibitory avoidance), presented in a pseudorandomized order. The reversal stage used in Study 1 was omitted.</p><p id="p-27">Trials began with a fixation cross (ISI; 2,000 ms; jittered 1,200 ms). Active responses required 3, 4, or 5 button presses within the 1,200 ms cue period (threshold determined during effort calibration). On successful trials, participants either earned 5 points or avoided an aversive sound, depending on the motivational context. Successful reward-seeking trials provided a reward signal (1,000 ms; white border), while successful avoidance trials were followed by a safety signal (1,000 ms; white border). On failed trials, participants either received no points (reward-seeking) or were presented with an aversive sound (avoidance). Points accumulated were displayed on reward-seeking trials, and a musical tone (C major chord; 1,100 ms) played each time participants earned an additional 25 points.</p></div></div><div id="sec-17"><h4>Statistical analysis</h4><p id="p-28">Analytical procedures followed Study 1. Accuracy (proportion correct) and trials to criterion during acquisition were the primary outcomes. Linear mixed models tested BDI-II symptom scores effects on active and inhibitory accuracy across motivational contexts. FDR corrections were used for multiple comparisons.</p></div></div><div id="sec-18"><h3>Code accessibility</h3><p id="p-29">No computational neuroscience models were developed for this study. However, extended data and code used to conduct the linear mixed models are available at <a href="https://osf.io/5sepm/">https://osf.io/5sepm/</a>.</p></div></div><div id="sec-19"><h2>Results</h2><div id="sec-20"><h3>Study 1 (avoidance)</h3><div id="sec-21"><h4>Demographics</h4><p id="p-30">To assess differences between undergraduates and online workers, we first compared self-reported depressive and anxiety symptom scores. There was no difference in depressive symptom scores (<em>F</em><sub>(1,463)</sub> = 0.34, <em>p</em> = 0.56; <a id="xref-fig-2-1" href="#F2">Fig. 2<em>A</em></a>), but undergraduates reported significantly higher anxiety symptom scores compared with online workers (<em>F</em><sub>(1,463)</sub> = 13.84, <em>p</em> &lt; 0.001; Extended Data <a id="xref-supplementary-material-2-2" href="#DC2">Fig. 2-1</a>). Sex and gender responses were highly congruent (&gt;96%); due to limited statistical power for non-cis gender categories, subsequent analyses refer to sex only. Females reported higher depressive (<em>F</em><sub>(1,463)</sub> = 7.96, <em>p</em> &lt; 0.01; <a id="xref-fig-2-2" href="#F2">Fig. 2<em>B</em></a>) and higher anxiety (<em>F</em><sub>(1,463)</sub> = 33.63, <em>p</em> &lt; 0.001; Extended Data <a id="xref-supplementary-material-2-3" href="#DC2">Fig. 2-1</a>) symptom scores than males. Finally, there was a significant age difference between samples (<em>F</em><sub>(1,462)</sub> = 299.8, <em>p</em> &lt; 0.001), with undergraduates being younger on average compared with online workers (Extended Data <a id="xref-supplementary-material-3-1" href="#DC3">Fig. 2-2</a>).</p><div id="F2"><div><div><p><a href="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/F2.large.jpg?width=800&amp;height=600&amp;carousel=1" title="Study 1: Distribution of depressive symptom scores across samples and sexes. Density plots representing the distribution of Beck Depression Inventory-II (BDI-II) scores. A, Depressive symptom score distributions by sample: Undergraduates (red) and Online Workers (blue). B, Depressive symptom score distributions by sex: Female (red) and Male (blue). The x-axis represents proportion scores, where raw BDI-II symptom scores, ranging from 0 to 60, have been divided by the maximum possible score (60) to produce a proportion between 0 and 1. This adjustment was made because the suicide ideation question was removed for ethical consideration. The labels on the x-axis—Minimal (0–13), Mild-Moderate (14–28), Severe (29–63)—reflect typical ranges of raw symptom scores for ease of interpretation. Dashed vertical lines represent the mean BDI-II symptom score for each group. In panel B, a significant difference in depressive levels between sexes is indicated (p " rel="gallery-fragment-images-593667592" data-figure-caption="<div class=&quot;highwire-markup&quot;><div xmlns=&quot;http://www.w3.org/1999/xhtml&quot;>Study 1: Distribution of depressive symptom scores across samples and sexes.<strong> </strong>Density plots representing the distribution of Beck Depression Inventory-II (BDI-II) scores. <strong><em>A</em></strong>, Depressive symptom score distributions by sample: Undergraduates (red) and Online Workers (blue). <strong><em>B</em></strong>, Depressive symptom score distributions by sex: Female (red) and Male (blue). The <em>x</em>-axis represents proportion scores, where raw BDI-II symptom scores, ranging from 0 to 60, have been divided by the maximum possible score (60) to produce a proportion between 0 and 1. This adjustment was made because the suicide ideation question was removed for ethical consideration. The labels on the <em>x</em>-axis—Minimal (0–13), Mild-Moderate (14–28), Severe (29–63)—reflect typical ranges of raw symptom scores for ease of interpretation. Dashed vertical lines represent the mean BDI-II symptom score for each group. In panel <strong><em>B</em></strong>, a significant difference in depressive levels between sexes is indicated (<em>p</em> < 0.01), with females scoring higher on average than males. See Extended Data Figures 2-1 (BAI distributions) and 2-2 (Age distributions).</div></div>" data-icon-position="" data-hide-link-title="0"><span><img alt="Figure 2." src="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/F2.medium.gif" width="440" height="226" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></a></p></div><ul><li><a href="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/F2.large.jpg?download=true" title="Download Figure 2." data-icon-position="" data-hide-link-title="0">Download figure</a></li><li><a href="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/F2.large.jpg" target="_blank" data-icon-position="" data-hide-link-title="0">Open in new tab</a></li><li><a href="https://www.eneuro.org/highwire/powerpoint/3811670" data-icon-position="" data-hide-link-title="0">Download powerpoint</a></li></ul></div><div><p><span>Figure 2.</span></p><p id="p-31">Study 1: Distribution of depressive symptom scores across samples and sexes.<strong> </strong>Density plots representing the distribution of Beck Depression Inventory-II (BDI-II) scores. <strong><em>A</em></strong>, Depressive symptom score distributions by sample: Undergraduates (red) and Online Workers (blue). <strong><em>B</em></strong>, Depressive symptom score distributions by sex: Female (red) and Male (blue). The <em>x</em>-axis represents proportion scores, where raw BDI-II symptom scores, ranging from 0 to 60, have been divided by the maximum possible score (60) to produce a proportion between 0 and 1. This adjustment was made because the suicide ideation question was removed for ethical consideration. The labels on the <em>x</em>-axis—Minimal (0–13), Mild-Moderate (14–28), Severe (29–63)—reflect typical ranges of raw symptom scores for ease of interpretation. Dashed vertical lines represent the mean BDI-II symptom score for each group. In panel <strong><em>B</em></strong>, a significant difference in depressive levels between sexes is indicated (<em>p</em> &lt; 0.01), with females scoring higher on average than males. See Extended Data <a id="xref-supplementary-material-2-4" href="#DC2">Figures 2-1</a> (BAI distributions) and <a id="xref-supplementary-material-3-2" href="#DC3">2-2</a> (Age distributions).</p></div></div><div id="DC2"><h3>Figure 2-1</h3><p id="p-32"><strong>Study 1: Distribution of Anxiety Scores Across Samples and Sexes.</strong> Density plots representing the distribution of Beck Anxiety Inventory (BAI) scores. <strong>A)</strong> Anxiety score distributions by sample: Undergraduates (red) and Online Workers (blue). <strong>B)</strong> Anxiety score distributions by sex: Female (red) and Male (blue). The x-axis represents proportion scores, where raw BAI scores, ranging from 0-63, have been divided by the maximum possible score (63) to produce a proportion between 0 and 1. This adjustment was made for comparability between the BDI-II and BAI scales. The labels on the x-axis -- Minimal (0-7), Mild-Moderate (8-25), Severe (26-63) -- reflect typical ranges of raw scores for ease of interpretation. Dashed vertical lines represent the mean BAI score for each group. In panel A, a significant difference in anxiety levels between sample groups is indicated (<em>p</em> &lt; .001), with undergraduates scoring higher on average than online workers. In panel B, a significant difference in anxiety levels between sexes is indicated (<em>p</em> &lt; .001), with females scoring higher on average than males. Download <span><span id="DC2"><a href="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/DC2/embed/inline-supplementary-material-2.tif?download=true" data-icon-position="" data-hide-link-title="0"><span></span>Figure 2-1, TIF file</a></span></span>.</p></div><div id="DC3"><h3>Figure 2-2</h3><p id="p-33"><strong>Study 1: Age Distribution Across Samples.</strong> Density plot representing the distribution of ages for undergraduates (red) and online workers (blue). Dashed vertical lines represent the mean age for each group. A significant difference in age between the samples are indicated (<em>p</em> &lt; .001), with online workers being older on average compared to undergraduates. Download <span><span id="DC3"><a href="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/DC3/embed/inline-supplementary-material-3.tif?download=true" data-icon-position="" data-hide-link-title="0"><span></span>Figure 2-2, TIF file</a></span></span>.</p></div></div><div id="sec-22"><h4>Avoidance task</h4><div id="sec-23"><h5>Within-subject results</h5><p id="p-34"><em>Acquisition</em>. The acquisition task stage assessed initial learning of the active avoidance response. Participants showed robust acquisition, with an average accuracy of 0.79 (SD = 0.15; <a id="xref-fig-3-1" href="#F3">Fig. 3<em>A</em></a>). The mean number of trials to reach criterion (≥80% correct in 20 trial period) was 29.65 (SD = 18.42; range, 16–120 trials; <a id="xref-fig-4-1" href="#F4">Fig. 4<em>A</em></a>). Higher BDI-II scores were associated with a greater number of trials needed to reach criterion during acquisition, but this effect was specific to undergraduates (<a id="xref-fig-4-2" href="#F4">Fig. 4<em>B</em></a>; see <a href="https://osf.io/5sepm/">https://osf.io/5sepm/</a>).</p><div id="F3"><div><div><p><a href="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/F3.large.jpg?width=800&amp;height=600&amp;carousel=1" title="Study 1: Avoidance performance across task stages. Proportion correct for active (green) and inhibitory (red) avoidance responses across three task stages: A, Acquisition; B, intermixed, and C, reversal. The left panels display overall accuracy, with circles representing individual participant performance for active and inhibitory avoidance, respectively. The right panels show accuracy across blocks of six trials, with circles representing mean performance for each avoidance type. Proportion correct reflects the ratio of successful avoidance responses relative to the total number of trials for each avoidance type. Significant differences between active and inhibitory avoidance during the intermixed and reversal stages are indicated (p " rel="gallery-fragment-images-593667592" data-figure-caption="<div class=&quot;highwire-markup&quot;><div xmlns=&quot;http://www.w3.org/1999/xhtml&quot;>Study 1: Avoidance performance across task stages. Proportion correct for active (green) and inhibitory (red) avoidance responses across three task stages: <strong><em>A</em></strong>, Acquisition; <strong><em>B</em></strong>, intermixed, and <strong><em>C</em></strong>, reversal. The left panels display overall accuracy, with circles representing individual participant performance for active and inhibitory avoidance, respectively. The right panels show accuracy across blocks of six trials, with circles representing mean performance for each avoidance type. Proportion correct reflects the ratio of successful avoidance responses relative to the total number of trials for each avoidance type. Significant differences between active and inhibitory avoidance during the intermixed and reversal stages are indicated (<em>p</em> < 0.001).</div></div>" data-icon-position="" data-hide-link-title="0"><span><img alt="Figure 3." src="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/F3.medium.gif" width="440" height="209" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></a></p></div><ul><li><a href="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/F3.large.jpg?download=true" title="Download Figure 3." data-icon-position="" data-hide-link-title="0">Download figure</a></li><li><a href="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/F3.large.jpg" target="_blank" data-icon-position="" data-hide-link-title="0">Open in new tab</a></li><li><a href="https://www.eneuro.org/highwire/powerpoint/3811686" data-icon-position="" data-hide-link-title="0">Download powerpoint</a></li></ul></div><div><p><span>Figure 3.</span></p><p id="p-35">Study 1: Avoidance performance across task stages. Proportion correct for active (green) and inhibitory (red) avoidance responses across three task stages: <strong><em>A</em></strong>, Acquisition; <strong><em>B</em></strong>, intermixed, and <strong><em>C</em></strong>, reversal. The left panels display overall accuracy, with circles representing individual participant performance for active and inhibitory avoidance, respectively. The right panels show accuracy across blocks of six trials, with circles representing mean performance for each avoidance type. Proportion correct reflects the ratio of successful avoidance responses relative to the total number of trials for each avoidance type. Significant differences between active and inhibitory avoidance during the intermixed and reversal stages are indicated (<em>p</em> &lt; 0.001).</p></div></div><div id="F4"><div><div><p><a href="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/F4.large.jpg?width=800&amp;height=600&amp;carousel=1" title="Study 1: Trials to criterion for active avoidance during the acquisition stage. A, Number of trials required to reach criterion for all participants in the avoidance task. Individual data points (green circles) represent the number of trials each participant required to reach the criterion of 80% correct responses within 20-trial period during acquisition. B, Interaction between depressive symptom scores (BDI-II proportion scores) and the sample group (Undergraduates vs Online Workers) predicting trials to criterion for active avoidance. The regression lines show the relationship between depressive symptom scores and trials to criterion for each sample, with a stronger effect observed in undergraduates (β = 24.57) compared with online workers (β = 1.09). A significant main effect of BDI-II symptom scores (p " rel="gallery-fragment-images-593667592" data-figure-caption="<div class=&quot;highwire-markup&quot;><div xmlns=&quot;http://www.w3.org/1999/xhtml&quot;>Study 1: Trials to criterion for active avoidance during the acquisition stage. <strong><em>A</em></strong>, Number of trials required to reach criterion for all participants in the avoidance task. Individual data points (green circles) represent the number of trials each participant required to reach the criterion of 80% correct responses within 20-trial period during acquisition. <strong><em>B</em></strong>, Interaction between depressive symptom scores (BDI-II proportion scores) and the sample group (Undergraduates vs Online Workers) predicting trials to criterion for active avoidance. The regression lines show the relationship between depressive symptom scores and trials to criterion for each sample, with a stronger effect observed in undergraduates (<em>β</em> = 24.57) compared with online workers (<em>β</em> = 1.09). A significant main effect of BDI-II symptom scores (<em>p</em> < 0.001) and a significant BDI-II × Sample interaction (<em>p</em> < 0.05) are indicated. See Extended Data Figure 4-1 for corresponding BAI effects.</div></div>" data-icon-position="" data-hide-link-title="0"><span><img alt="Figure 4." src="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/F4.medium.gif" width="440" height="311" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></a></p></div><ul><li><a href="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/F4.large.jpg?download=true" title="Download Figure 4." data-icon-position="" data-hide-link-title="0">Download figure</a></li><li><a href="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/F4.large.jpg" target="_blank" data-icon-position="" data-hide-link-title="0">Open in new tab</a></li><li><a href="https://www.eneuro.org/highwire/powerpoint/3811677" data-icon-position="" data-hide-link-title="0">Download powerpoint</a></li></ul></div><div><p><span>Figure 4.</span></p><p id="p-36">Study 1: Trials to criterion for active avoidance during the acquisition stage. <strong><em>A</em></strong>, Number of trials required to reach criterion for all participants in the avoidance task. Individual data points (green circles) represent the number of trials each participant required to reach the criterion of 80% correct responses within 20-trial period during acquisition. <strong><em>B</em></strong>, Interaction between depressive symptom scores (BDI-II proportion scores) and the sample group (Undergraduates vs Online Workers) predicting trials to criterion for active avoidance. The regression lines show the relationship between depressive symptom scores and trials to criterion for each sample, with a stronger effect observed in undergraduates (<em>β</em> = 24.57) compared with online workers (<em>β</em> = 1.09). A significant main effect of BDI-II symptom scores (<em>p</em> &lt; 0.001) and a significant BDI-II × Sample interaction (<em>p</em> &lt; 0.05) are indicated. See Extended Data <a id="xref-supplementary-material-4-2" href="#DC4">Figure 4-1</a> for corresponding BAI effects.</p></div></div><div id="DC4"><h3>Figure 4-1</h3><p id="p-37"><strong>Study 1: Trials to Criterion for Active Avoidance During the Acquisition Stage</strong>. <strong>A)</strong> Number of trials required to reach criterion for all participants in the avoidance task. Individual data points (green circles) represent the number of trials each participant required to reach the criterion of 80% correct responses within 20-trial period during acquisition. <strong>B)</strong> Interaction between anxiety scores (BAI proportion scores) and the sample group (Undergraduates vs. Online Workers) predicting trials to criterion for active avoidance. The regression lines show the relationship between anxiety scores and trials to criterion for each sample, with a stronger effect observed in undergraduates (<em>β</em> = 16.14) compared to online workers (<em>β</em> = -3.81). A significant main effect of BAI scores (<em>p</em> &lt; .01) and a significant BAI × Sample interaction (<em>p</em> &lt; .05) are indicated. Download <span><span id="DC4"><a href="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/DC4/embed/inline-supplementary-material-4.tif?download=true" data-icon-position="" data-hide-link-title="0"><span></span>Figure 4-1, TIF file</a></span></span>.</p></div><p id="p-38"><em>Intermixed and reversal</em>. The intermixed task stage assessed participants’ ability to switch between active and inhibitory responses using discriminative cues, while the reversal task stage assessed behavioral flexibility when cue–response contingencies were reversed. A 2 × 2 within-subjects ANOVA assessing proportion correct, with Avoidance Type (active, inhibitory) and Task Stage (intermixed, reversal) as within-subjects factors, revealed a significant effect of Avoidance Type and Task Stage, and a significant interaction (<a id="xref-table-wrap-2-1" href="#T2">Table 2</a>). Follow-up analysis revealed higher accuracy on inhibitory compared with active trials in both the intermixed (<em>M</em><sub>Inhibitory</sub> = 0.90, SD = 0.06; <em>M</em><sub>Active</sub> = 0.87, SD = 0.12; <em>t</em><sub>(711)</sub> = −4.74, <em>p</em> &lt; 0.001; <a id="xref-fig-3-2" href="#F3">Fig. 3<em>B</em></a>) and reversal stages (<em>M</em><sub>Inhibitory</sub> = 0.90, SD = 0.07; <em>M</em><sub>Active</sub> = 0.82, SD = 0.15; <em>t</em><sub>(711)</sub> = −12.18, <em>p</em> &lt; 0.001; <a id="xref-fig-3-3" href="#F3">Fig. 3<em>C</em></a>). Accuracy on active trials was also higher in the intermixed compared with the reversal stage (<em>t</em><sub>(877)</sub> = 10.49, <em>p</em> &lt; 0.001), whereas inhibitory accuracy did not differ by stage (<em>t</em><sub>(877)</sub> = 0.81, <em>p</em> = 0.42).</p><div id="T2"><p><span>Table 2.</span></p><p id="p-39">Study 1—2 × 2 within-subjects ANOVA table for active and inhibitory avoidance accuracy for intermixed and reversal task stages</p></div></div><div id="sec-24"><h5>Between-subject results</h5><p id="p-41"><em>Depressive symptom scores and active avoidance accuracy</em>. To examine the relationship between depressive symptom scores and active avoidance accuracy, we used a linear mixed model with BDI-II symptom scores (<em>z</em>-normalized, grand-mean centered) as the primary predictor. The model included Sex (female, male), Task Stage (acquisition, intermixed, reversal), and Sample (undergraduates, online workers) as fixed effects and Participant as a random intercept. Proportion correct on active trials was also <em>z</em>-normalized (grand-mean centered). To ensure model stability, we adopted a simplified random-effects structure that excluded a random slope for Task Stage. As shown in <a id="xref-table-wrap-3-1" href="#T3">Table 3</a>, the model revealed a significant main effect of BDI-II (<em>β</em> = −0.230, SE = 0.074, <em>p</em> = 0.009), indicating that higher depressive symptoms were associated with lower active avoidance accuracy when all other variables were at their reference levels (i.e., female, acquisition, undergraduates). After controlling for multiple comparisons, there were no significant interactions between BDI-II and Sex or Sample. However, we observed a significant interaction between BDI-II and Task Stage, with the relationship between depressive symptoms and active avoidance accuracy changing in the intermixed (<em>β</em> = 0.300, SE = 0.083, <em>p</em> = 0.003) and reversal stages (<em>β</em> = 0.215, SE = 0.083, <em>p</em> = 0.041), relative to acquisition. These interactions suggest that the negative relationship between depressive symptoms and active avoidance accuracy was strongest during initial learning (acquisition) and was attenuated at later stages when avoidance responses are well-learned or inhibitory control was required (<a id="xref-fig-5-1" href="#F5">Fig. 5</a>).</p><div id="F5"><div><div><p><a href="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/F5.large.jpg?width=800&amp;height=600&amp;carousel=1" title="Study 1: Linear mixed model predicting active avoidance accuracy: interaction between BDI-II scores and task stage. Z-normalized depressive symptom scores (BDI-II) interacted with task stage (acquisition, intermixed, reversal) to predict active avoidance accuracy (proportion correct; z-normalized). Formula: Accuracy ∼ BDI-II × Sex × Task Stage × Sample + (1| Participant). Each circle represents individual participant data. Colored regression lines show the relationship between depressive scores and active accuracy across task stages: acquisition (green), intermixed (red), and reversal (amber). A significant negative relationship between BDI-II scores and accuracy in the acquisition stage (β = −0.230), while this relationship was attenuated in the intermixed (β = −0.015), and reversal (β = 0.070) stages. Significant BDI-II × Task Stage interactions are indicated. Asterisks next to regression lines denotes slopes significantly different from zero; asterisks between task stages indicate significant differences in the slopes between levels. Statistical significance denoted as follows: *p " rel="gallery-fragment-images-593667592" data-figure-caption="<div class=&quot;highwire-markup&quot;><div xmlns=&quot;http://www.w3.org/1999/xhtml&quot;>Study 1: Linear mixed model predicting active avoidance accuracy: interaction between BDI-II scores and task stage. <em>Z</em>-normalized depressive symptom scores (BDI-II) interacted with task stage (acquisition, intermixed, reversal) to predict active avoidance accuracy (proportion correct; <em>z</em>-normalized). Formula: Accuracy<em> ∼ </em>BDI-II<em> × </em>Sex<em> × </em>Task Stage<em> × </em>Sample<em> + </em>(<em>1|</em> Participant). Each circle represents individual participant data. Colored regression lines show the relationship between depressive scores and active accuracy across task stages: acquisition (green), intermixed (red), and reversal (amber). A significant negative relationship between BDI-II scores and accuracy in the acquisition stage (<em>β</em> = −0.230), while this relationship was attenuated in the intermixed (<em>β</em> = −0.015), and reversal (<em>β</em> = 0.070) stages. Significant BDI-II × Task Stage interactions are indicated. Asterisks next to regression lines denotes slopes significantly different from zero; asterisks between task stages indicate significant differences in the slopes between levels. Statistical significance denoted as follows: *<em>p</em> < 0.05, **<em>p</em> < 0.01. Full model results are presented in Table 3.</div></div>" data-icon-position="" data-hide-link-title="0"><span><img alt="Figure 5." src="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/F5.medium.gif" width="440" height="435" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></a></p></div><ul><li><a href="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/F5.large.jpg?download=true" title="Download Figure 5." data-icon-position="" data-hide-link-title="0">Download figure</a></li><li><a href="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/F5.large.jpg" target="_blank" data-icon-position="" data-hide-link-title="0">Open in new tab</a></li><li><a href="https://www.eneuro.org/highwire/powerpoint/3811669" data-icon-position="" data-hide-link-title="0">Download powerpoint</a></li></ul></div><div><p><span>Figure 5.</span></p><p id="p-42">Study 1: Linear mixed model predicting active avoidance accuracy: interaction between BDI-II scores and task stage. <em>Z</em>-normalized depressive symptom scores (BDI-II) interacted with task stage (acquisition, intermixed, reversal) to predict active avoidance accuracy (proportion correct; <em>z</em>-normalized). Formula: Accuracy<em> ∼ </em>BDI-II<em> × </em>Sex<em> × </em>Task Stage<em> × </em>Sample<em> + </em>(<em>1|</em> Participant). Each circle represents individual participant data. Colored regression lines show the relationship between depressive scores and active accuracy across task stages: acquisition (green), intermixed (red), and reversal (amber). A significant negative relationship between BDI-II scores and accuracy in the acquisition stage (<em>β</em> = −0.230), while this relationship was attenuated in the intermixed (<em>β</em> = −0.015), and reversal (<em>β</em> = 0.070) stages. Significant BDI-II × Task Stage interactions are indicated. Asterisks next to regression lines denotes slopes significantly different from zero; asterisks between task stages indicate significant differences in the slopes between levels. Statistical significance denoted as follows: *<em>p</em> &lt; 0.05, **<em>p</em> &lt; 0.01. Full model results are presented in <a id="xref-table-wrap-3-2" href="#T3">Table 3</a>.</p></div></div><div id="T3"><p><span>Table 3.</span></p><p id="p-43">Study 1—linear mixed model for BDI-II predicting active avoidance accuracy</p></div><p id="p-51"><em>Depressive symptom scores and inhibitory avoidance accuracy.</em> A similar linear mixed model was used to examine the relationship between depressive symptom scores (BDI-II symptom scores, <em>z</em>-normalized, grand-mean centered) and inhibitory avoidance accuracy (<em>z</em>-normalized, grand-mean centered). The model included Sex (female, male), Task Stage (intermixed, reversal), and Sample (undergraduates, online workers) as fixed effects and Participant as a random intercept. No main effects or interactions involving BDI-II were significant (Extended Data <a id="xref-supplementary-material-6-2" href="#DC6">Table 3-2</a>), suggesting that depressive symptoms were not associated with inhibitory avoidance performance using this task.</p></div></div></div><div id="sec-25"><h3>Study 2 (reward-seeking/avoidance)</h3><p id="p-52">In Study 1, participants showed lower accuracy on active compared with inhibitory avoidance trials. However, it remained unclear whether this effect was driven by conflict arising from a prepotent tendency to inhibit action under threat (<a id="xref-ref-10-1" href="#ref-10">Bolles, 1970</a>; <a id="xref-ref-56-1" href="#ref-56">Pessoa, 2009</a>; <a id="xref-ref-73-1" href="#ref-73">Wendt et al., 2017</a>) or by a preference to reduce effort expenditure due to the additional demands of effortful active responses (<a id="xref-ref-36-1" href="#ref-36">Hogan et al., 2020</a>; <a id="xref-ref-30-1" href="#ref-30">Forys et al., 2023</a>). To address this, Study 2 used a mixed-motivation task that assesses both active and inhibitory responses within reward-seeking and avoidance contexts in undergraduates. Here, the design manipulated the congruency between motivational context (reward-seeking vs avoidance) and instrumental response (active vs inhibitory), allowing for analysis of how motivational context shapes action tendencies. Moreover, because the ACDM framework proposes that depression is associated with altered reward-seeking and effort-related decision-making (<a id="xref-ref-9-3" href="#ref-9">Bishop and Gagne, 2018</a>), we examined whether individual differences in depressive symptom scores would differentially affect behavior across motivational contexts. This design allowed for a detailed examination of both reward-seeking and avoidance behaviors, considering their active and inhibitory dimensions.</p><p id="p-53">We hypothesized that task accuracy would be highest for inhibitory avoidance and active reward-seeking, as these behaviors are contextually aligned with prepotent response tendencies—inhibiting action to avoid threat and initiating action to obtain reward. Furthermore, we expected that participants with higher depressive symptom scores would exhibit reduced accuracy in active reward-seeking, consistent with predictions of diminished behavioral engagement due to effort demand overestimation and/or reward undervaluation. A graphical overview of the mixed-motivation task is provided in <a id="xref-fig-6-1" href="#F6">Figure 6</a>.</p><div id="F6"><div><div><p><a href="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/F6.large.jpg?width=800&amp;height=600&amp;carousel=1" title="Study 2: Mixed-motivation go/no-go task. A, Schematic of task stages. Acquisition and intermixed. Green lines represent active (“Go”) trials, while red lines represent inhibitory (“No-Go”) trials. Light green and light red lines indicate reward-seeking (RS) trials, while dark green and dark red lines indicate avoidance trials. The black arrow indicates time progression. During acquisition, participants were required to reach a criterion of 80% correct active reward-seeking and 80% correct active avoidance trials over a 20-trial period (i.e., initial acquisition) or complete a maximum of 144 trials. Upon reaching criterion, participants completed 24 over-learning trials, followed by an unsignaled transition into the intermixed stage. In this stage, participants flexibly alternated between active and inhibitory reward-seeking and active and inhibitory avoidance responses. B, Active reward-seeking: A blue triangle signals an active reward-seeking trial. The participant makes sufficient keyboard presses (green circles) within 1,200 ms cue period and is awarded points (+5 points) toward a monetary reward, followed with a reward-signal (white border onscreen, not shown). C, Active avoidance: A blue circle signals an active avoidance trial. The participant makes sufficient keyboard presses (green circles) within 1,200 ms cue period, avoiding an aversive sound (yellow speaker), followed with a safety signal (white boarder onscreen, not shown). D, Inhibitory reward-seeking: A blue hexagon signals an inhibitory reward-seeking trial. The participant withholds keyboard presses for the full 1,200 ms and is awarded points toward a monetary reward, followed with a reward-signal. E, Inhibitory avoidance: A blue square signals an inhibitory avoidance trial. The participant withholds keyboard presses for the full 1,200 ms, avoiding an aversive sound, followed by a safety signal. Failed reward-seeking or avoidance trials result in no points (+0 points) or the presentation of an aversive sound, respectively." rel="gallery-fragment-images-593667592" data-figure-caption="<div class=&quot;highwire-markup&quot;><div xmlns=&quot;http://www.w3.org/1999/xhtml&quot;>Study 2: Mixed-motivation go/no-go task. <strong><em>A</em></strong>, Schematic of task stages. Acquisition and intermixed. Green lines represent active (“Go”) trials, while red lines represent inhibitory (“No-Go”) trials. Light green and light red lines indicate reward-seeking (RS) trials, while dark green and dark red lines indicate avoidance trials. The black arrow indicates time progression. During acquisition, participants were required to reach a criterion of 80% correct active reward-seeking and 80% correct active avoidance trials over a 20-trial period (i.e., initial acquisition) or complete a maximum of 144 trials. Upon reaching criterion, participants completed 24 over-learning trials, followed by an unsignaled transition into the intermixed stage. In this stage, participants flexibly alternated between active and inhibitory reward-seeking and active and inhibitory avoidance responses. <strong><em>B</em></strong>, Active reward-seeking: A blue triangle signals an active reward-seeking trial. The participant makes sufficient keyboard presses (green circles) within 1,200 ms cue period and is awarded points (+5 points) toward a monetary reward, followed with a reward-signal (white border onscreen, not shown). <strong><em>C</em></strong>, Active avoidance: A blue circle signals an active avoidance trial. The participant makes sufficient keyboard presses (green circles) within 1,200 ms cue period, avoiding an aversive sound (yellow speaker), followed with a safety signal (white boarder onscreen, not shown). <strong><em>D</em></strong>, Inhibitory reward-seeking: A blue hexagon signals an inhibitory reward-seeking trial. The participant withholds keyboard presses for the full 1,200 ms and is awarded points toward a monetary reward, followed with a reward-signal. <strong><em>E</em></strong>, Inhibitory avoidance: A blue square signals an inhibitory avoidance trial. The participant withholds keyboard presses for the full 1,200 ms, avoiding an aversive sound, followed by a safety signal. Failed reward-seeking or avoidance trials result in no points (+0 points) or the presentation of an aversive sound, respectively.</div></div>" data-icon-position="" data-hide-link-title="0"><span><img alt="Figure 6." src="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/F6.medium.gif" width="440" height="236" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></a></p></div><ul><li><a href="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/F6.large.jpg?download=true" title="Download Figure 6." data-icon-position="" data-hide-link-title="0">Download figure</a></li><li><a href="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/F6.large.jpg" target="_blank" data-icon-position="" data-hide-link-title="0">Open in new tab</a></li><li><a href="https://www.eneuro.org/highwire/powerpoint/3811684" data-icon-position="" data-hide-link-title="0">Download powerpoint</a></li></ul></div><div><p><span>Figure 6.</span></p><p id="p-54">Study 2: Mixed-motivation go/no-go task. <strong><em>A</em></strong>, Schematic of task stages. Acquisition and intermixed. Green lines represent active (“Go”) trials, while red lines represent inhibitory (“No-Go”) trials. Light green and light red lines indicate reward-seeking (RS) trials, while dark green and dark red lines indicate avoidance trials. The black arrow indicates time progression. During acquisition, participants were required to reach a criterion of 80% correct active reward-seeking and 80% correct active avoidance trials over a 20-trial period (i.e., initial acquisition) or complete a maximum of 144 trials. Upon reaching criterion, participants completed 24 over-learning trials, followed by an unsignaled transition into the intermixed stage. In this stage, participants flexibly alternated between active and inhibitory reward-seeking and active and inhibitory avoidance responses. <strong><em>B</em></strong>, Active reward-seeking: A blue triangle signals an active reward-seeking trial. The participant makes sufficient keyboard presses (green circles) within 1,200 ms cue period and is awarded points (+5 points) toward a monetary reward, followed with a reward-signal (white border onscreen, not shown). <strong><em>C</em></strong>, Active avoidance: A blue circle signals an active avoidance trial. The participant makes sufficient keyboard presses (green circles) within 1,200 ms cue period, avoiding an aversive sound (yellow speaker), followed with a safety signal (white boarder onscreen, not shown). <strong><em>D</em></strong>, Inhibitory reward-seeking: A blue hexagon signals an inhibitory reward-seeking trial. The participant withholds keyboard presses for the full 1,200 ms and is awarded points toward a monetary reward, followed with a reward-signal. <strong><em>E</em></strong>, Inhibitory avoidance: A blue square signals an inhibitory avoidance trial. The participant withholds keyboard presses for the full 1,200 ms, avoiding an aversive sound, followed by a safety signal. Failed reward-seeking or avoidance trials result in no points (+0 points) or the presentation of an aversive sound, respectively.</p></div></div><div id="sec-26"><h4>Demographics</h4><p id="p-55">Females reported marginally higher levels of depressive symptoms (<em>F</em><sub>(1,328)</sub> = 3.24, <em>p</em> = 0.0729) and significantly higher levels of anxiety symptoms (<em>F</em><sub>(1,328)</sub> = 12.99, <em>p</em> &lt; 0.001) compared with males (<a id="xref-fig-7-1" href="#F7">Fig. 7<em>B</em></a>; Extended Data <a id="xref-supplementary-material-8-1" href="#DC8">Fig. 7-1</a>). There was no significant age difference between sex (<em>F</em><sub>(1,328)</sub> = 0.112, <em>p</em> = 0.738).</p><div id="F7"><div><div><p><a href="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/F7.large.jpg?width=800&amp;height=600&amp;carousel=1" title="Study 2: Distribution of depressive symptom scores in undergraduates and across sexes. Density plots representing the distribution of Beck Depression Inventory-II (BDI-II) symptom scores. A, Depressive symptom score distributions in full undergraduate sample (red). B, Depressive symptom score distributions by sex: Female (red) and Male (blue). The x-axis represents proportion scores, where raw BDI-II symptom scores, ranging from 0 to 60, have been divided by the maximum possible score (60) to produce a proportion between 0 and 1. This adjustment was made because the suicide ideation question was removed for ethical consideration. The labels on the x-axis—Minimal (0–13), Mild-Moderate (14–28), Severe (29–63)—reflect typical ranges of raw symptom scores for ease of interpretation. Dashed vertical black line represents mean in full sample. Dash vertical-colored lines represent the mean BDI-II symptom score for each group. In panel B, a marginal significant difference in depressive levels between sexes is indicated (p " rel="gallery-fragment-images-593667592" data-figure-caption="<div class=&quot;highwire-markup&quot;><div xmlns=&quot;http://www.w3.org/1999/xhtml&quot;>Study 2: Distribution of depressive symptom scores in undergraduates and across sexes. Density plots representing the distribution of Beck Depression Inventory-II (BDI-II) symptom scores. <strong><em>A</em></strong>, Depressive symptom score distributions in full undergraduate sample (red). <strong><em>B</em></strong>, Depressive symptom score distributions by sex: Female (red) and Male (blue). The <em>x</em>-axis represents proportion scores, where raw BDI-II symptom scores, ranging from 0 to 60, have been divided by the maximum possible score (60) to produce a proportion between 0 and 1. This adjustment was made because the suicide ideation question was removed for ethical consideration. The labels on the <em>x</em>-axis—Minimal (0–13), Mild-Moderate (14–28), Severe (29–63)—reflect typical ranges of raw symptom scores for ease of interpretation. Dashed vertical black line represents mean in full sample. Dash vertical-colored lines represent the mean BDI-II symptom score for each group. In panel <strong><em>B</em></strong>, a marginal significant difference in depressive levels between sexes is indicated (<em>p</em> < 0.10), with females scoring higher on average than males. See Extended Data Figures 7-1 (BAI distributions) and 7-2 (Age distribution).</div></div>" data-icon-position="" data-hide-link-title="0"><span><img alt="Figure 7." src="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/F7.medium.gif" width="440" height="226" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></a></p></div><ul><li><a href="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/F7.large.jpg?download=true" title="Download Figure 7." data-icon-position="" data-hide-link-title="0">Download figure</a></li><li><a href="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/F7.large.jpg" target="_blank" data-icon-position="" data-hide-link-title="0">Open in new tab</a></li><li><a href="https://www.eneuro.org/highwire/powerpoint/3811674" data-icon-position="" data-hide-link-title="0">Download powerpoint</a></li></ul></div><div><p><span>Figure 7.</span></p><p id="p-56">Study 2: Distribution of depressive symptom scores in undergraduates and across sexes. Density plots representing the distribution of Beck Depression Inventory-II (BDI-II) symptom scores. <strong><em>A</em></strong>, Depressive symptom score distributions in full undergraduate sample (red). <strong><em>B</em></strong>, Depressive symptom score distributions by sex: Female (red) and Male (blue). The <em>x</em>-axis represents proportion scores, where raw BDI-II symptom scores, ranging from 0 to 60, have been divided by the maximum possible score (60) to produce a proportion between 0 and 1. This adjustment was made because the suicide ideation question was removed for ethical consideration. The labels on the <em>x</em>-axis—Minimal (0–13), Mild-Moderate (14–28), Severe (29–63)—reflect typical ranges of raw symptom scores for ease of interpretation. Dashed vertical black line represents mean in full sample. Dash vertical-colored lines represent the mean BDI-II symptom score for each group. In panel <strong><em>B</em></strong>, a marginal significant difference in depressive levels between sexes is indicated (<em>p</em> &lt; 0.10), with females scoring higher on average than males. See Extended Data <a id="xref-supplementary-material-8-2" href="#DC8">Figures 7-1</a> (BAI distributions) and <a id="xref-supplementary-material-9-1" href="#DC9">7-2</a> (Age distribution).</p></div></div><div id="DC8"><h3>Figure 7-1</h3><p id="p-57"><strong>Study 2: Distribution of Anxiety Scores in Undergraduates and Across Sexes.</strong> Density plots representing the distribution of Beck Anxiety Inventory (BAI) scores. <strong>A)</strong> Anxiety score distributions in full undergraduate sample (red). <strong>B)</strong> Anxiety score distributions by sex: Female (red) and Male (blue). The x-axis represents proportion scores, where raw BAI scores, ranging from 0-63, have been divided by the maximum possible score (63) to produce a proportion between 0 and 1. This adjustment was made for comparability between the BDI-II and BAI scales. The labels on the x-axis -- Minimal (0-7), Mild-Moderate (8-25), Severe (26-63) -- reflect typical ranges of raw scores for ease of interpretation. Dashed vertical black line represents mean in full sample. Dashed vertical-coloured lines represent the mean BAI score for each sex. In panel B, a significant difference in anxiety levels between sexes is indicated (<em>p</em> &lt; .001), with females scoring higher on average than males. Download <span><span id="DC8"><a href="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/DC8/embed/inline-supplementary-material-8.tif?download=true" data-icon-position="" data-hide-link-title="0"><span></span>Figure 7-1, TIF file</a></span></span>.</p></div><div id="DC9"><h3>Figure 7-2</h3><p id="p-58"><strong>Study 2: Age Distribution in Undergraduates.</strong> Density plot representing the distribution of ages for undergraduates (red). Dashed vertical black line represent the mean age. Download <span><span id="DC9"><a href="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/DC9/embed/inline-supplementary-material-9.tif?download=true" data-icon-position="" data-hide-link-title="0"><span></span>Figure 7-2, TIF file</a></span></span>.</p></div></div><div id="sec-27"><h4>Mixed-motivation go/no-go task</h4><div id="sec-28"><h5>Acquisition</h5><p id="p-59">Participants successfully learned both active reward-seeking and avoidance responses, as indicated by the number of trials to criterion (reward-seeking: <em>M</em> = 46.94, SD = 18.09; avoidance: <em>M</em> = 51.72, SD = 19.90). A one-way within-subjects ANOVA revealed a significant effect of Motivational Context on the number of trials to criterion, with more trials needed to acquire active avoidance than reward-seeking (<em>F</em><sub>(1,329)</sub> = 30.96, <em>p</em> &lt; 0.001; <a id="xref-fig-8-1" href="#F8">Fig. 8<em>A</em></a>). To test whether depressive symptoms predicted trials to criterion, we fit a linear mixed model including BDI-II scores, Sex, and Motivational Context. No main or interaction effect of BDI-II was observed. Motivational context significantly affected acquisition accuracy, with lower proportion correct on active avoidance (<em>M</em> = 0.774, SD = 0.135) compared with active reward-seeking (<em>M</em> = 0.819, SD = 0.137; <em>F</em><sub>(1,329)</sub> = 52.59, <em>p</em> &lt; 0.001; <a id="xref-fig-8-2" href="#F8">Fig. 8<em>B</em></a>). To assess how this difference varied over time, we analyzed accuracy across the first six trial blocks (where all participants had data). Accuracy improved across blocks (main effect of block) and remained higher for reward-seeking trials compared with avoidance trials (main effect of Motivational Context; <a id="xref-table-wrap-4-1" href="#T4">Table 4</a>). While this difference persisted across blocks 1–5 (<em>t</em>'s<sub>(1629)</sub> &gt; 2.56, <em>p</em>'s &lt; 0.01), it converged by block 6 (<em>t</em><sub>(1,629)</sub> = 1.59, <em>p</em> = 0.11; <a id="xref-fig-8-3" href="#F8">Fig. 8<em>C</em></a>), suggesting slower acquisition for active avoidance than reward-seeking.</p><div id="F8"><div><div><p><a href="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/F8.large.jpg?width=800&amp;height=600&amp;carousel=1" title="Study 2: Performance comparison between active reward-seeking and active avoidance during the acquisition stage. A, Number of trials required to reach criterion for all participants in the mixed-motivation task. Individual data points (light green triangles for Active Reward-Seeking; dark green circles for Active Avoidance) represent the number of trials each participant required to reach the criterion of 80% correct responses within 20-trial period during acquisition. B, Overall accuracy for active reward-seeking and active avoidance in the acquisition stage. Individual data points (light green triangles for Active Reward-Seeking; dark green circles for Active Avoidance) represent the proportion correct for each participant. Proportion correct reflects the ratio of successful active responses relative to the total number of trials within each motivation type. C, Accuracy for active reward-seeking and active avoidance across blocks of six trials, with triangles and circles representing mean performance for reward-seeking and avoidance trials, respectively. Significant differences between active reward-seeking and active avoidance during the acquisition stage are indicated (p " rel="gallery-fragment-images-593667592" data-figure-caption="<div class=&quot;highwire-markup&quot;><div xmlns=&quot;http://www.w3.org/1999/xhtml&quot;>Study 2: Performance comparison between active reward-seeking and active avoidance during the acquisition stage. <strong><em>A</em></strong>, Number of trials required to reach criterion for all participants in the mixed-motivation task. Individual data points (light green triangles for Active Reward-Seeking; dark green circles for Active Avoidance) represent the number of trials each participant required to reach the criterion of 80% correct responses within 20-trial period during acquisition. <strong><em>B</em></strong>, Overall accuracy for active reward-seeking and active avoidance in the acquisition stage. Individual data points (light green triangles for Active Reward-Seeking; dark green circles for Active Avoidance) represent the proportion correct for each participant. Proportion correct reflects the ratio of successful active responses relative to the total number of trials within each motivation type. <strong><em>C</em></strong>, Accuracy for active reward-seeking and active avoidance across blocks of six trials, with triangles and circles representing mean performance for reward-seeking and avoidance trials, respectively. Significant differences between active reward-seeking and active avoidance during the acquisition stage are indicated (<em>p</em> < 0.001).</div></div>" data-icon-position="" data-hide-link-title="0"><span><img alt="Figure 8." src="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/F8.medium.gif" width="440" height="303" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></a></p></div><ul><li><a href="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/F8.large.jpg?download=true" title="Download Figure 8." data-icon-position="" data-hide-link-title="0">Download figure</a></li><li><a href="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/F8.large.jpg" target="_blank" data-icon-position="" data-hide-link-title="0">Open in new tab</a></li><li><a href="https://www.eneuro.org/highwire/powerpoint/3811665" data-icon-position="" data-hide-link-title="0">Download powerpoint</a></li></ul></div><div><p><span>Figure 8.</span></p><p id="p-60">Study 2: Performance comparison between active reward-seeking and active avoidance during the acquisition stage. <strong><em>A</em></strong>, Number of trials required to reach criterion for all participants in the mixed-motivation task. Individual data points (light green triangles for Active Reward-Seeking; dark green circles for Active Avoidance) represent the number of trials each participant required to reach the criterion of 80% correct responses within 20-trial period during acquisition. <strong><em>B</em></strong>, Overall accuracy for active reward-seeking and active avoidance in the acquisition stage. Individual data points (light green triangles for Active Reward-Seeking; dark green circles for Active Avoidance) represent the proportion correct for each participant. Proportion correct reflects the ratio of successful active responses relative to the total number of trials within each motivation type. <strong><em>C</em></strong>, Accuracy for active reward-seeking and active avoidance across blocks of six trials, with triangles and circles representing mean performance for reward-seeking and avoidance trials, respectively. Significant differences between active reward-seeking and active avoidance during the acquisition stage are indicated (<em>p</em> &lt; 0.001).</p></div></div><div id="T4"><p><span>Table 4.</span></p><p id="p-61">Study 2—2 × 6 within-subjects ANOVA for active response accuracy for acquisition task stage by block</p></div></div><div id="sec-29"><h5>Intermixed</h5><p id="p-64">The intermixed stage assessed participants’ ability to flexibly select actions or inhibit responses based on motivational contexts (i.e., reward-seeking vs avoidance). A 2 × 2 within-subjects ANOVA (Motivational Context × Response Type) revealed a significant main effect of Response Type and a significant interaction but no main effect of Motivational Context (<a id="xref-table-wrap-5-1" href="#T5">Table 5</a>). Follow-up analysis revealed higher accuracy for active reward-seeking than active avoidance (<em>t</em><sub>(658)</sub> = 9.92, <em>p</em> &lt; 0.0001) and higher accuracy for inhibitory avoidance compared with inhibitory reward-seeking (<em>t</em><sub>(658)</sub> = 9.60, <em>p</em> &lt; 0.0001). In the avoidance context, inhibitory responses were more accurate compared with active responses (<em>t</em><sub>(658)</sub> = 10.73, <em>p</em> &lt; 0.0001), consistent with Study 1. There were no accuracy differences in Response Type in the reward-seeking context (<em>t</em><sub>(498)</sub> = 1.95, <em>p</em> = 0.21; <a id="xref-fig-9-1" href="#F9">Fig. 9</a>).</p><div id="F9"><div><div><p><a href="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/F9.large.jpg?width=800&amp;height=600&amp;carousel=1" title="Study 2: Performance comparison between reward-seeking and avoidance for active and inhibitory responses during the intermixed stage. A, Overall accuracy for active/inhibitory reward-seeking and avoidance in the intermixed stage. Individual data points (light green triangles for Active Reward-Seeking; light red triangles for Inhibitory Reward-Seeking, dark green circles for Active Avoidance, dark red circles for Inhibitory Avoidance) represent the proportion correct for each participant. Proportion correct reflects the ratio of successful responses relative to the total number of trials within each trial type. B, Accuracy for active/inhibitory reward-seeking and avoidance across blocks of six trials during the intermixed task stage. Shapes represent mean performance for each trial type across 10 blocks. Significant differences between (1) active reward-seeking and active avoidance, (2) inhibitory reward-seeking and inhibitory avoidance, and (3) active and inhibitory avoidance are indicated (p " rel="gallery-fragment-images-593667592" data-figure-caption="<div class=&quot;highwire-markup&quot;><div xmlns=&quot;http://www.w3.org/1999/xhtml&quot;>Study 2: Performance comparison between reward-seeking and avoidance for active and inhibitory responses during the intermixed stage. <strong><em>A</em></strong>, Overall accuracy for active/inhibitory reward-seeking and avoidance in the intermixed stage. Individual data points (light green triangles for Active Reward-Seeking; light red triangles for Inhibitory Reward-Seeking, dark green circles for Active Avoidance, dark red circles for Inhibitory Avoidance) represent the proportion correct for each participant. Proportion correct reflects the ratio of successful responses relative to the total number of trials within each trial type. <strong><em>B</em></strong>, Accuracy for active/inhibitory reward-seeking and avoidance across blocks of six trials during the intermixed task stage. Shapes represent mean performance for each trial type across 10 blocks. Significant differences between (1) active reward-seeking and active avoidance, (2) inhibitory reward-seeking and inhibitory avoidance, and (3) active and inhibitory avoidance are indicated (<em>p</em> < 0.001).</div></div>" data-icon-position="" data-hide-link-title="0"><span><img alt="Figure 9." src="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/F9.medium.gif" width="440" height="319" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></a></p></div><ul><li><a href="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/F9.large.jpg?download=true" title="Download Figure 9." data-icon-position="" data-hide-link-title="0">Download figure</a></li><li><a href="https://www.eneuro.org/content/eneuro/12/9/ENEURO.0034-25.2025/F9.large.jpg" target="_blank" data-icon-position="" data-hide-link-title="0">Open in new tab</a></li><li><a href="https://www.eneuro.org/highwire/powerpoint/3811676" data-icon-position="" data-hide-link-title="0">Download powerpoint</a></li></ul></div><div><p><span>Figure 9.</span></p><p id="p-65">Study 2: Performance comparison between reward-seeking and avoidance for active and inhibitory responses during the intermixed stage. <strong><em>A</em></strong>, Overall accuracy for active/inhibitory reward-seeking and avoidance in the intermixed stage. Individual data points (light green triangles for Active Reward-Seeking; light red triangles for Inhibitory Reward-Seeking, dark green circles for Active Avoidance, dark red circles for Inhibitory Avoidance) represent the proportion correct for each participant. Proportion correct reflects the ratio of successful responses relative to the total number of trials within each trial type. <strong><em>B</em></strong>, Accuracy for active/inhibitory reward-seeking and avoidance across blocks of six trials during the intermixed task stage. Shapes represent mean performance for each trial type across 10 blocks. Significant differences between (1) active reward-seeking and active avoidance, (2) inhibitory reward-seeking and inhibitory avoidance, and (3) active and inhibitory avoidance are indicated (<em>p</em> &lt; 0.001).</p></div></div><div id="T5"><p><span>Table 5.</span></p><p id="p-66">Study 2—2 × 2 within-subjects ANOVA for active and inhibitory response accuracy for intermixed task stage</p></div></div></div><div id="sec-30"><h4>Depressive symptom scores and active response accuracy</h4><p id="p-68">We used a linear mixed model to examine whether depressive symptoms (BDI-II, <em>z</em>-normalized, grand-mean centered) predicted accuracy on active trials (also <em>z</em>-normalized). Fixed effects included Sex (female, male), Task Stage (acquisition, intermixed), and Motivational Context (reward-seeking, avoidance), with Participant as a random intercept. This structure matched Study 1, with motivational context added. Motivational Context significantly influenced accuracy, with lower performance on avoidance trials (avoidance; <em>β</em> =−0.307, SE = 0.075, <em>t</em><sub>(978)</sub> = −4.11, <em>p</em> &lt; 0.001). However, BDI-II symptom scores were not significantly associated with active accuracy (<em>β</em> = 0.035, SE = 0.063, <em>t</em><sub>(1,050.90)</sub> = 0.56, <em>p</em> = 0.85), nor did they interact with Motivational Context (avoidance; <em>β</em> = 0.038, SE = 0.076, <em>t</em><sub>(978)</sub> = 0.496, <em>p</em> = 0.85). Thus, although avoidance reduced active accuracy this effect was not associated with depressive symptom scores. Full model results can be found at <a href="https://osf.io/5sepm/">https://osf.io/5sepm/</a>.</p><p id="p-69">To evaluate whether observed effects were sensitive to reference level selection, we conducted an exploratory series of eight linear mixed models, systematically varying the reference levels for Sex, Task Stage, and Motivational Context. This resulted in 128 tested effects (16 per model, including main effects and interactions), and <em>p</em> values were adjusted using Benjamini–Hochberg FDR across all 128 effects. While no significant effects of BDI-II emerged in the initial model, exploratory analyses identified a significant BDI-II × Sex interaction (<em>β</em> = −0.365, SE = 0.121, <em>t</em><sub>(1,050.90)</sub> = −3.02, <em>p</em><sub>adjusted</sub> = 0.026), specifically in the avoidance context during the intermixed stage. This exploratory finding suggests the possibility that sex differences in the relationship between depressive symptoms and instrumental behavior may emerge when active responses are well-learned and inhibitory demands are newly introduced—potentially reflecting sex-specific dynamics in threat processing during later phases of learning. Full model results can be found at <a href="https://osf.io/5sepm/">https://osf.io/5sepm/</a>.</p></div><div id="sec-31"><h4>Depressive symptom scores and inhibitory response accuracy</h4><p id="p-70">A similar linear mixed model was used to examine whether depressive symptom scores (BDI-II, <em>z</em>-normalized) predicted accuracy on inhibitory trials. Fixed effects include Sex (female, male) and Motivational Context (reward-seeking, avoidance) and Participant as a random intercept. Motivational Context significantly affected inhibitory accuracy, with higher performance in the avoidance context (avoidance; <em>β</em> = 0.570, SE = 0.064, <em>t</em><sub>(326)</sub> = 8.95, <em>p</em> &lt; 0.001). However, BDI-II symptom scores were not significantly associated with inhibitory accuracy (<em>β</em> = −0.074, SE = 0.063, <em>t</em><sub>(534.47)</sub> = −1.18, <em>p</em> = 0.28), nor was there a significant interact with BDI-II and Motivational Context (avoidance; <em>β</em> = 0.030, SE = 0.065, <em>t</em><sub>(326)</sub> = 0.47, <em>p</em> = 0.64). Thus, although participants showed lower accuracy for inhibitory reward-seeking trials compared with avoidance trials, this pattern was not associated with depressive symptom severity.</p></div></div></div><div id="sec-32"><h2>Discussion</h2><p id="p-71">In this study we report that higher depressive scores are associated with a reduced capacity to learn active avoidance behaviors, while no relationship was observed with inhibitory avoidance. Specifically, Study 1 extended rodent research on active and inhibitory avoidance to a human nonclinical sample, revealing that higher depressive symptoms predicted lower accuracy during the acquisition phase of active avoidance (<a id="xref-fig-5-2" href="#F5">Fig. 5</a>) and a greater number of trials required to reach criterion performance (<a id="xref-fig-4-3" href="#F4">Fig. 4<em>B</em></a>). In contrast, depressive symptom scores were not related to performance on inhibitory avoidance trials. Additionally, within-subjects analyses indicated that overall, inhibitory avoidance was performed more readily compared with active avoidance, as indicated by higher accuracy during the intermixed and reversal stages (<a id="xref-fig-3-4" href="#F3">Fig. 3<em>B</em>,<em>C</em></a>). Altogether, these findings highlight a selective impairment in active avoidance learning associated with depressive symptoms and underscore the importance of considering how this relationship may vary across different learning phases. This dynamic pattern warrants further investigation into the underlying cognitive and neural processes that constrain avoidance behavior in depression.</p><p id="p-72">Our findings in Study 1 partially support the predictions of the ACDM framework, which posit that depression is associated with a greater tendency toward inaction in avoidance contexts (<a id="xref-ref-9-4" href="#ref-9">Bishop and Gagne, 2018</a>). Using this framework, the decision to act is calculated as the difference between the product of estimated outcome value and probability and the estimated cost of deploying effort to obtain a desired outcome. In depression, inaction may arise from overestimating effort costs, undervaluing outcomes, or underestimating outcome probability. Notably, because our task used a deterministic reinforcement schedule, outcome uncertainty is unlikely to account for the observed deficit. If overestimation of effort costs were solely responsible for these deficits, one would expect consistent active avoidance impairments across all task stages. However, since effort demands remained constant (i.e., the number of button presses required to obtain the desired outcome) and inaction was most pronounced during the acquisition phase—when fatigue-related effort costs were likely minimal—alternative explanations must be considered. Another possibility is that individuals with elevated levels of depressive symptoms became increasingly sensitive to the aversive outcome over time—effectively overvaluing the punishment and potentially overriding initial biases against deploying effort. Consistent with this interpretation, several studies have demonstrated that individuals with depression show increased sensitivity to negative feedback (<a id="xref-ref-23-1" href="#ref-23">Elliott et al., 1997</a>; <a id="xref-ref-25-1" href="#ref-25">Eshel and Roiser, 2010</a>), particularly in probabilistic reversal learning tasks, where they are more likely to switch following misleading negative feedback (<a id="xref-ref-50-2" href="#ref-50">Murphy et al., 2003</a>; <a id="xref-ref-67-1" href="#ref-67">Taylor Tavares et al., 2008</a>). However, other work suggests that the negativity bias in depression may not reflect punishment hypersensitivity per se, but rather blunted responsiveness to reward, resulting in a relative overweighting of negative outcomes (<a id="xref-ref-60-3" href="#ref-60">Robinson et al., 2012</a>). Still other studies have reported reduced sensitivity to both reward and punishment in depressed individuals (<a id="xref-ref-49-5" href="#ref-49">Mukherjee et al., 2020</a>). This heterogeneity likely reflects differences in task structure (i.e., deterministic vs probabilistic reinforcement), cognitive control demands (i.e., attending to and memorizing cue–response associations), and sample characteristics such as comorbid anxiety, sex, IQ, and medication status. Regardless, our findings suggest that the relationship between depressive symptoms and active avoidance is dynamic, with experience-dependent shifts across phases of avoidance.</p><p id="p-73">Clarifying how the neural circuits regulating active avoidance are dynamically engaged over time may offer critical insight into motivational dysfunction in depression. Evidence from both human and animal studies highlights the role of species-specific defensive reactions (SSDRs), where freezing is a prepotent response in aversive contexts (<a id="xref-ref-10-2" href="#ref-10">Bolles, 1970</a>; <a id="xref-ref-27-1" href="#ref-27">Fanselow, 1994</a>; <a id="xref-ref-43-1" href="#ref-43">LeDoux et al., 2017</a>). For successful active avoidance, both humans and rodents must overcome these prepotent defensive responses to engage in instrumental, goal-directed action. From a neural circuitry perspective, considerable progress has been made in understanding the mechanisms underlying the acquisition of active avoidance (<a id="xref-ref-43-2" href="#ref-43">LeDoux et al., 2017</a>; <a id="xref-ref-12-1" href="#ref-12">Cain, 2019</a>). Early in avoidance training, SSDRs are largely driven by amygdala circuits that promote behavioral suppression. With repeated training, however, ventromedial prefrontal systems (homologs of infralimbic cortex, Area 25 of anterior cingulate) increasingly suppress amygdala activity to reduce freezing and facilitate goal-directed avoidance responses (<a id="xref-ref-48-1" href="#ref-48">Moscarello and LeDoux, 2013</a>). These dynamics suggest that individuals with elevated depressive symptoms may exhibit difficulty in suppressing prepotent defensive responses during early learning—potentially due to dysfunction in cortico-limbic-striatal circuits that support the shift from reactive to goal-directed control. This provides a plausible neurobiological mechanism for the symptom-related impairments in active avoidance observed during the acquisition phase, while performance at later stages remains unaffected.</p><p id="p-74">Moving to research in humans, the dual competition model (<a id="xref-ref-56-2" href="#ref-56">Pessoa, 2009</a>) proposes the effects of emotionally salient stimuli on task performance depends both on the level of arousal evoked by a stimulus and on whether the stimulus aligns with or opposes the action tendency evoked by the stimulus. Prepotent behavioral responses to avoid punishment and approach reward, mediated in part by prefrontal regions, have been reliably observed in human neuroimaging studies (<a id="xref-ref-32-1" href="#ref-32">Guitart-Masip et al., 2012</a>; <a id="xref-ref-1-1" href="#ref-1">Asci et al., 2019</a>). In depression, disruptions in top-down regulatory control have been linked to reduced activity in dorsolateral and dorsomedial prefrontal cortex (dlPFC, dmPFC) and rostral ACC (rACC), along with elevated and sustained amygdala activity in response to negative feedback or emotional salient stimuli (<a id="xref-ref-63-1" href="#ref-63">Siegle et al., 2007</a>; <a id="xref-ref-26-2" href="#ref-26">Fales et al., 2008</a>; <a id="xref-ref-67-2" href="#ref-67">Taylor Tavares et al., 2008</a>). This pattern may indicate that emotionally salient cues disproportionately influence behavior due to weakened regulatory input from cognitive control systems. As a result, the capacity to override prepotent defensive responses—particularly during early stages of active avoidance learning—may be compromised in depression. Recent work further supports this interpretation, showing that reductions in GABA within the rACC were associated with decreased functional connectivity across cortico-striatal-limbic circuits in females with MDD (<a id="xref-ref-38-1" href="#ref-38">Ironside et al., 2021</a>)—a finding especially relevant given our predominantly female sample.</p><div id="sec-33"><h3>Avoidance mechanisms in depression and related disorders</h3><p id="p-75">To contextualize our findings, it is important to position them within the broader literature on avoidance across psychiatric disorders, highlighting key conceptual differences and points of convergence. For instance, many studies define avoidance as the decreased selection of high-loss options in probabilistic selection tasks—a definition that differs meaningfully from the framework used here but useful for understanding sensitivity to reward and negative feedback. <a id="xref-ref-16-4" href="#ref-16">Chase et al. (2010)</a> used a probabilistic selection task to examine feedback learning in individuals with MDD and found reduced learning rates for both positive and negative feedback during training, particularly among individuals with higher anhedonia. This suggests blunted reinforcement learning rather than a valence-specific bias such as altered sensitivity to negative feedback. Nonetheless, this profile is consistent with our observed impairment in active avoidance acquisition, despite differences in task design.</p><p id="p-76"><a id="xref-ref-49-6" href="#ref-49">Mukherjee et al. (2020)</a> extended this work using probabilistic reversal learning and found that MDD patients—most of whom were medicated—selected fewer rich options following reversals and exhibited reduced win-stay behavior (i.e., less likely to repeat a rewarded choice), but no difference in lose-shift behavior (i.e., switching after punishment). If depression involved heightened punishment sensitivity, an increase in lose-shift behavior would be expected. The absence of this effect supports the idea of diminished reward sensitivity rather than increased responsiveness to punishment. This interpretation aligns with the possibility that symptom-related impairments in active avoidance reflect deficits in safety learning rather than heightened punishment sensitivity that interacts with effort-related biases. Safety learning—the process of learning about cues that predict the absence of threat (<a id="xref-ref-42-1" href="#ref-42">Laing et al., 2025</a>)—has been shown to promote instrumental avoidance learning in animals and humans (<a id="xref-ref-28-1" href="#ref-28">Fernando et al., 2014</a>; <a id="xref-ref-29-1" href="#ref-29">Fisher and Urcelay, 2024</a>). Impaired learning of safety signals may contribute to reduced active avoidance performance, even in aversively motivated contexts. Given that safety learning is supported by amygdala and vmPFC circuitry (<a id="xref-ref-40-1" href="#ref-40">Kong et al., 2014</a>), this may offer a more parsimonious explanation for acquisition-specific effects than models emphasizing the accumulation of punishment sensitivity and effort-related bias.</p><p id="p-77">Neuromodulator systems may further complicate interpretation, as both serotonin and dopamine are implicated in punishment and reward learning. SSRIs, commonly prescribed in MDD, are known to blunt negative feedback sensitivity (<a id="xref-ref-35-1" href="#ref-35">Herzallah et al., 2013</a>). Supporting this, low doses of the antidepressant citalopram—which attenuate serotonin signaling—increase lose-shift behavior and sensitivity to punishment in both rodents and humans (<a id="xref-ref-15-1" href="#ref-15">Chamberlain et al., 2006</a>; <a id="xref-ref-3-1" href="#ref-3">Bari et al., 2010</a>). However, findings from obsessive compulsive disorder (OCD) populations highlight more nuanced effects of serotonergic modulation: <a id="xref-ref-24-2" href="#ref-24">Endrass et al. (2011)</a> found greater sensitivity to negative feedback in medicated OCD patients with elevated depressive symptoms, but only after initial learning—consistent with the idea that punishment sensitivity may build with experience. In contrast, <a id="xref-ref-54-2" href="#ref-54">Palminteri et al. (2012)</a> reported no valence-specific effects of medication status in OCD patients using a task previously linking dopamine to punishment learning (<a id="xref-ref-53-1" href="#ref-53">Palminteri et al., 2009</a>).</p><p id="p-78">Motivational impairments similar to those in depression are also evident in schizophrenia, particularly in relation to altered dopamine signaling. In unmedicated patients, <a id="xref-ref-59-2" href="#ref-59">Reinen et al. (2016)</a> found blunted prediction error BOLD signals in the striatum and mPFC for rewards, but intact response to punishment, suggesting D2 tone may selectively dampen reward while keeping punishment signaling intact. Similarly, <a id="xref-ref-72-2" href="#ref-72">Waltz et al. (2018)</a> reported reduced differential activation to gain versus loss-avoidance in vmPFC, ACC, and ventral striatum (VS), with diminished activation in VS associated with higher negative symptom scores. Sex differences in dopaminergic responses to loss versus gain have also been observed. Using PET during the monetary incentive delay task, <a id="xref-ref-33-1" href="#ref-33">Hahn et al. (2021)</a> found females exhibited heightened VS dopaminergic responses to punishment relative to gain. Together, these findings suggest that disrupted valuation and motivational processes, linked to both dopamine and serotonin signaling, may reflect cortico-striatal-limbic dysfunction as a transdiagnostic mechanism across conditions like OCD, schizophrenia, and depression.</p><p id="p-79">Overall, these studies underscore the dynamic nature of avoidance, which may shift with experience (i.e., acquisition, expression, habit) and neuromodulatory state. If punishment sensitivity increases with experience, it may eventually override early inaction driven by effort-related biases. Alternatively, if deficits are more prominent for reward-related signals, disrupted safety learning may play a greater role. Future computational modeling that integrates effort costs, punishment and reward sensitivity, and safety learning mechanisms will be critical for disentangling these processes and clarifying how depressive symptoms influence active avoidance behavior.</p></div><div id="sec-34"><h3>Motivational context influences accuracy of instrumental actions</h3><p id="p-80">Study 2 examined whether poorer active avoidance performance reflect a general bias toward effort minimization or context-specific effects by assessing active and inhibitory responses across both reward-seeking and avoidance contexts. While depressive symptom scores were not significantly related to performance, robust within-subjects effects emerged. Participants performed more accurately on trials aligned with their prepotent tendencies—active reward-seeking and inhibitory avoidance—consistent with prior research demonstrating approach biases for reward and withdrawal biases for punishment (<a id="xref-ref-18-1" href="#ref-18">Crockett et al., 2009</a>; <a id="xref-ref-32-2" href="#ref-32">Guitart-Masip et al., 2012</a>; <a id="xref-ref-46-3" href="#ref-46">Mkrtchian et al., 2017</a>; <a id="xref-ref-1-2" href="#ref-1">Asci et al., 2019</a>).</p><p id="p-81">Prefrontal regions such as the anterior prefrontal cortex (aPFC) and orbitofrontal (OFC) are implicated in overcoming these motivational-action conflicts (<a id="xref-ref-61-1" href="#ref-61">Roelofs et al., 2009</a>; <a id="xref-ref-71-1" href="#ref-71">Volman et al., 2011</a>). If the poorer performance for active versus inhibitory avoidance observed in Study 1 were driven by a general preference to minimize effort, we would expect a similar pattern for active reward-seeking in Study 2. However, this pattern did not emerge, suggesting that effort bias alone does not account for these findings.</p><p id="p-82">Parallel findings in humans and animals suggest that newly learned discriminative cues can differentially influence instrumental behavior depending on whether the context is appetitive or aversive (<a id="xref-ref-66-1" href="#ref-66">Talmi et al., 2008</a>; <a id="xref-ref-31-1" href="#ref-31">Geurts et al., 2013</a>; <a id="xref-ref-46-4" href="#ref-46">Mkrtchian et al., 2017</a>; <a id="xref-ref-51-2" href="#ref-51">Nord et al., 2018</a>; <a id="xref-ref-13-1" href="#ref-13">Campese, 2021</a>). Consistent with this, participants in the current study more readily inhibited responses in avoidance context than in reward-seeking contexts. Remarkably, rats display similar patterns, with greater accuracy for active responses during reward seeking and greater inhibitory accuracy during avoidance (<a id="xref-ref-19-1" href="#ref-19">Dalton et al., 2025</a>).</p><p id="p-83">Although these performance differences could be attributed to differences in the motivational value of the reward and the punishment outcomes, this explanation is unlikely. No significant main effect of motivational context was found during the intermixed task stage. Specifically, the average accuracy for reward-seeking and avoidance trials (irrespective of response type) did not differ. This suggests that the outcomes were equally motivating overall and not biased toward one context. Instead, the observed interaction is more consistent with the context-dependent effect of prepotent response tendencies influencing instrumental actions.</p><p id="p-84">While depression is typically associated with reduced reward-seeking, the ACDM framework predicts a broader bias toward inaction across both reward-seeking and avoidance contexts, driven by overestimation of effort costs. In Study 1, higher depressive symptom scores were associated with reduced active avoidance performance—consistent with this framework. However, contrary to our hypotheses, depressive symptoms were not significantly associated with active or inhibitory response accuracy in either motivational context.</p><p id="p-85">Several key differences may account for these null findings. First, the mixed-motivation task employed an interleaved design, requiring participants to frequently switch between responding to appetitive and aversive stimuli, rather than engaging with each in distinct blocks. This design placed avoidance trials within a broader reward-rich context, attenuating depression-related impairments in active avoidance—potentially due to the prepotent tendency to approach reward. This interpretation aligns with findings from approach-avoidance conflict paradigms—where conditions involving potential reward despite the risk of punishment are more likely to elicit active approach behavior than avoidance-only conditions (<a id="xref-ref-2-1" href="#ref-2">Aupperle et al., 2011</a>). Second, the four-condition task structure (active vs inhibitory and reward-seeking vs avoidance) likely imposed greater working memory demands, which have been implicated in reward/punishment learning (<a id="xref-ref-70-1" href="#ref-70">Van Der Schaaf et al., 2014</a>). These cognitive demands may have masked the influence of depressive symptoms on performance. Future studies may consider controlling for cognitive load to better isolate symptom-specific effects. Finally, Study 1 took place during the height of the COVID-19 pandemic, a contextual factor that may have influenced affective states and task engagement in ways that were not present during Study 2.</p><p id="p-86">Although no association between depressive symptoms were found in Study 2, the robust within-subjects effects suggest this task may be useful for assessing motivated behavior in other psychiatric populations. For example, research on substance use disorder emphasizes the strong motivational salience of reward and punishment related cues, particularly those associated with drug use.</p></div><div id="sec-35"><h3>Conclusion</h3><p id="p-87">Although depression is often linked with reward-processing deficits like anhedonia (<a id="xref-ref-68-1" href="#ref-68">Treadway and Zald, 2011</a>; <a id="xref-ref-69-2" href="#ref-69">Treadway et al., 2012</a>), our findings reveal a novel link between symptom severity and impaired active avoidance learning in aversive contexts. A key limitation is whether these results generalize to clinical populations. Depression is increasingly understood as a dimensional condition, with clinical diagnoses reflecting the more severe end of a broader symptom spectrum and avoidance impairments representing a potential transdiagnostic feature (<a id="xref-ref-22-1" href="#ref-22">Eaton et al., 2023</a>). Our sample included a wide range of depressive symptom scores, with many participants self-reporting prior diagnoses or scoring above clinical cutoffs, supporting the relevance of our findings to clinical populations. Furthermore, this dimensional approach may offer a more nuanced understanding of symptom-related effects on avoidance behavior and extend to other psychiatric conditions, such as OCD and schizophrenia, that share overlapping motivational and affective features with depression.</p><p id="p-88">Another limitation of our study was the higher than expected exclusion rates, which warrant a closer examination of their potential impact on results. Both tasks were designed as reinforcement-based learning paradigms with minimal instructions, no explicit information on cue–response contingencies, and no practice trials. While this design enhances translational relevance, it likely contributed to the variability in participants’ ability to acquire the task contingencies. Although higher exclusion rates were anticipated, the rates observed—39.37% in Study 1 and 57.20% in Study 2—exceeded expectations and were primarily due to failure to meet behavioral performance criteria. However, when considering only exclusions related to questionnaire failures or task noncompletion, rates were consistent with typical online studies (Study 1: 19.17%, Study 2: 19.20%; <a id="xref-ref-65-1" href="#ref-65">Suzuki et al., 2021</a>). To assess potential bias, we compared included participants to those who passed attention checks but were later excluded for other reasons (see <a href="https://osf.io/5sepm/">https://osf.io/5sepm/</a>). In Study 1, excluded participants had significantly higher BAI scores, but not BDI-II scores. While the ACDM framework does not explicitly make predictions about how depressive and anxiety symptoms interact, it implies that their effects may counterbalance one another. In the context of our study, anxiety symptoms could offset the depressive impairments in active avoidance by promoting increased avoidance effort. This antagonistic dynamic suggests that higher exclusion rates may have reduced confounding influences rather than introduce bias. Importantly, BDI-II scores did not differ between included and excluded groups, preserving the validity of our primary analyses. Sex differences in exclusion rates were also observed, with a higher proportion of females excluded. However, both final samples remained predominantly female—the group in which we observed our strongest effects—suggesting any bias would likely underestimate, rather than inflate our findings. For these reasons, we believe the interpretation and relevance of our findings are still valid. However, future adaptations of this task may benefit from optimizing the trade-off between ecological validity and participant retention.</p><p id="p-89">Across two studies, we sought to extend rodent research to investigate patterns of active and inhibitory avoidance and reward-seeking in a nonclinical sample varying in depressive symptoms. By integrating self-report and behavioral measures, we aimed to strengthen translational links between preclinical models and depressive symptom severity in humans. Our findings highlight the value of transdiagnostic approaches in a community sample for bridging bench and clinic in understanding psychiatric disorders. Results demonstrate an important link between depressive symptoms and reduced efficacy at learning to override a prepotent response to inhibit action to avoid unpleasant events. Future work should test whether these effects replicate in clinically diagnosed MDD populations, use computational models to probe underlying mechanisms, and apply neuroimaging to evaluate cross-species convergence in neural circuitry.</p></div></div><div id="fn-group-1"><h2>Footnotes</h2><ul><li id="fn-1"><p id="p-1">The authors declare no competing financial interests.</p></li><li id="fn-3"><p id="p-3">We thank Veronica Dudarev for her advice on statistical analysis, as well as the contributions of Imogen Daly for creation and recording sound stimuli, and Karen Ip. 
This work was supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) grant (#F19-05182) to R.M.T., the UBC Djavad Mowafaghian Centre for Brain Health Innovation Fund Kickstart Research Grant (#F19-05932), the Michael Smith Foundation for Health Research Scholar Award to R.M.T., and an NSERC Postgraduate Scholarship – Doctoral (PGS-D) award to R.J.T.</p></li></ul></div><p id="p-4">This is an open-access article distributed under the terms of the <a href="https://creativecommons.org/licenses/by/4.0/" rel="license">Creative Commons Attribution 4.0 International license</a>, which permits unrestricted use, distribution and reproduction in any medium provided that the original work is properly attributed.</p><div id="sec-36"><h2>Synthesis</h2><div id="boxed-text-1"><p id="p-90">Reviewing Editor: Ifat Levy, Yale School of Medicine</p><p id="p-91">Decisions are customarily a result of the Reviewing Editor and the peer reviewers coming together and discussing their recommendations until a consensus is reached. When revisions are invited, a fact-based synthesis statement explaining their decision and outlining what is needed to prepare a revision will be listed below. The following reviewer(s) agreed to reveal their identity: Ziv Ben-Zion.</p></div><p id="p-92">The present study examined the relationship between depressive symptoms and active/inhibitory avoidance behavior in general populations. The authors demonstrated that depressive symptoms (BDI-II scores) are negatively associated with the accuracy of active avoidance in the avoidance Go/NoGo task, particularly among younger participants. This relationship did not emerge in the mixed version of the task (i.e., a combination of reward-seeking and avoidance). Given these findings, the authors argue that their work may help bridge the gap between preclinical animal research and clinical studies.</p><p id="p-93">This is an interesting paper, which may provide new insights into the relationship between depressive symptoms and avoidance learning using a reverse-translated avoidance task from rodent models. Reviewers identified, however, unclear aspects of the conceptual framework, as well as the methods and results, which should be addressed.</p><p id="p-94">- The authors should provide more background, to position their study in the context of prior research. They should be clear on whether and how the work is conceptually novel. If this is a replication, this is totally fine, but should be explicitly said.</p><p id="p-95">- The authors should discuss earlier research on psychiatric symptoms and avoidance behavior using simple decision-making tasks applicable to rodent models (for example: https://pubmed.ncbi.nlm.nih.gov/22420038/; https://pubmed.ncbi.nlm.nih.gov/19607754/; https://pubmed.ncbi.nlm.nih.gov/33001663/; https://pubmed.ncbi.nlm.nih.gov/22325972/; https://pubmed.ncbi.nlm.nih.gov/21284070/; https://pubmed.ncbi.nlm.nih.gov/28343697/; https://pubmed.ncbi.nlm.nih.gov/29486865/; https://pubmed.ncbi.nlm.nih.gov/27105903/; and https://pubmed.ncbi.nlm.nih.gov/34151477/).</p><p id="p-96">The authors should explain how their findings relate to these previous studies.</p><p id="p-97">- The authors describe the brain regions identified in animal research as related to active and inhibitory avoidance. However, it is unclear how these findings translate to humans. The statement that "Neural activation of these regions is found in humans performing avoidance tasks" is too general-are distinct brain regions implicated in active vs. inhibitory avoidance in humans? Similarly, the claim that "depression has been linked to structural or atypical patterns of activation in homologous brain regions in humans" lacks specificity. Could the authors provide more precise evidence from human neuroimaging studies to clarify these points?</p><p id="p-98">- The authors clearly articulate their research objective and hypothesis, but some aspects could be clarified. First, why was a non-clinical sample chosen instead of a clinical one? How might this impact the generalizability of findings to clinical depression? Second, the hypothesis states that depressive symptoms will impair active avoidance, but do the authors expect a similar or different effect for inhibitory avoidance? Clarifying these points would strengthen the rationale for the study.</p><p id="p-99">- The authors conducted numerous statistical tests as part of their main analyses. For instance, according to the description in lines 269-274, the main findings involved the effects of BDI-II and the interaction of Sex and Sample on active and inhibitory avoidance accuracies, amounting to more than 20 tests. This raises a concern about the robustness of the results - p-values should be properly corrected for multiple comparisons.</p><p id="p-100">- In the mixed-effect models, why was Age omitted (i.e., replaced by Sample)? Ideally, a single model would include all relevant variables. Furthermore, it is unclear why the authors included only a random intercept rather than also considering random slopes (e.g., for task stages), which could vary across participants. Were the variables in these models z-normalized?</p><p id="p-101">- The negative association between BDI-II scores and active avoidance accuracy emerged in the avoidance task but not in the mixed task. Could the authors speculate about the meaning of this discrepancy? Does it suggest that the relationship is highly context-dependent?</p><p id="p-102">- In Study 1, 302 of 767 participants were excluded, and in Study 2, 439 of 769 were excluded. Such high exclusion rates - over half of each sample - are unusual. Even in online experiments on crowdsourcing services, the exclusion rate typically does not exceed 30%. It would be informative to detail what attention checks were employed and how many participants failed each criterion. This issue also warrants discussion in the Discussion section.</p><p id="p-103">- Do the authors have any data regarding participants' general intelligence (e.g., educational background or IQ)? Differences between undergraduate and online participants may be attributable to such factors.</p><p id="p-104">- Mental disorders frequently co-occur - in particular, depression often co-occurs with anxiety. Thus, it is possible that the BDI-II scores were influenced by symptoms other than depression. Commenting on this possibility, and on how the ACDM framework handles this would be helpful.</p><p id="p-105">- Was the aversive sound truly aversive? Is there any data to support this claim?</p><p id="p-106">- please provide more detail on the effort and volume calibration procedures.</p><p id="p-107">- The methods and results sections are very long. Please try to streamline descriptions of procedures, statistical methods, and secondary analyses where possible.</p></div></div> <!-- /.panel-row-wrapper -->	
	
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tinycolor supply chain attack post-mortem (137 pts)]]></title>
            <link>https://sigh.dev/posts/ctrl-tinycolor-post-mortem/</link>
            <guid>45278657</guid>
            <pubDate>Wed, 17 Sep 2025 17:18:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sigh.dev/posts/ctrl-tinycolor-post-mortem/">https://sigh.dev/posts/ctrl-tinycolor-post-mortem/</a>, See on <a href="https://news.ycombinator.com/item?id=45278657">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>  <h2 id="tldr"><a href="#tldr">TL;DR</a></h2>
<p>A malicious GitHub Actions workflow was pushed to a shared repo and exfiltrated a npm token with broad publish rights. The attacker then used that token to publish malicious versions of 20 packages, including <code>@ctrl/tinycolor</code>.</p>
<p>My GitHub account, the @ctrl/tinycolor repository were not directly compromised. There was no phishing involved, and no malicious packages were installed on my machine and I already use pnpm to avoid unapproved postinstall scripts. There was no pull request involved because a repo admin does not need a pull request to add new github actions.</p>
<p>GitHub/npm security responded quickly, unpublishing the malicious versions. I followed by releasing clean versions to flush caches, as advised.</p>
<p>For broader context, see <a href="https://socket.dev/blog/tinycolor-supply-chain-attack-affects-40-packages" rel="noreferrer noopener" target="_blank">Socket’s write-up</a> or <a href="https://www.stepsecurity.io/blog/ctrl-tinycolor-and-40-npm-packages-compromised" rel="noreferrer noopener" target="_blank">StepSecurity’s analysis</a>. For community discussion, see this <a href="https://news.ycombinator.com/item?id=45260741" rel="noreferrer noopener" target="_blank">Hacker News post</a>, which spent 24 hours on the front page. I’m also finding this <a href="https://www.wiz.io/blog/shai-hulud-npm-supply-chain-attack" rel="noreferrer noopener" target="_blank">wiz.io</a> post helpful.</p>
<h2 id="how-i-found-out"><a href="#how-i-found-out">How I Found Out</a></h2>
<p>On September 15 around 4:30
 PM PT, <a href="https://bsky.app/profile/notwes.bsky.social" rel="noreferrer noopener" target="_blank">Wes Todd</a> DM’d me on Bluesky and looped me into the OpenJS Foundation Slack. By that point, Wes had already alerted GitHub/npm security, who were compiling lists of affected packages and rapidly unpublishing compromised versions.</p>
<p>Early guidance (attributed to Daniel Pereira) was to look for suspicious <code>Shai-Hulud</code> repos or branches. I wasn’t able to find any of these repos or branches on my own personal repos. The mystery was: how was I impacted at all?</p>
<blockquote>
<p>Shai-Hulud was the Fremen term for the sandworm of Arrakis. - <a href="https://dune.fandom.com/wiki/Shai-Hulud" rel="noreferrer noopener" target="_blank">dune wiki</a></p>
</blockquote>
<h2 id="what-actually-happened"><a href="#what-actually-happened">What Actually Happened</a></h2>
<p>A while ago, I collaborated on <a href="https://github.com/angulartics/angulartics2" rel="noreferrer noopener" target="_blank">angulartics2</a>, a shared repository where multiple people still had admin rights. That repo still contained a GitHub Actions secret — a npm token with broad publish rights. This collaborator had access to projects with other people which I believe explains some of the other 40 initial packages that were affected.</p>
<p>A new Shai-Hulud branch was force pushed to angulartics2 with a malicious github action workflow by a collaborator. The workflow ran immediately on push (did not need review since the collaborator is an admin) and stole the npm token. With the stolen token, the attacker published malicious versions of 20 packages. Many of which are not widely used, however the @ctrl/tinycolor package is downloaded about 2 million times a week.</p>
<p>GitHub and npm security teams moved quickly to unpublish the malicious versions. I then re-published fresh, verified versions of the packages I maintain to flush caches and restore trust.</p>
<h2 id="impact"><a href="#impact">Impact</a></h2>
<p>Malicious versions of several packages — including @ctrl/tinycolor — were briefly available on npm before removal. Installing those compromised versions would have triggered a postinstall payload, which is documented in detail by <a href="https://www.stepsecurity.io/blog/ctrl-tinycolor-and-40-npm-packages-compromised#attack-mechanism" rel="noreferrer noopener" target="_blank">StepSecurity</a>.</p>
<p>What should you do if you’ve installed a compromised version of a package? <a href="https://www.stepsecurity.io/blog/ctrl-tinycolor-and-40-npm-packages-compromised#immediate-actions-required" rel="noreferrer noopener" target="_blank">see StepSecurity’s immediate actions</a>.</p>
<h2 id="publishing-setup--interim-plan"><a href="#publishing-setup--interim-plan">Publishing Setup &amp; Interim Plan</a></h2>
<p>I currently use <a href="https://github.com/semantic-release/semantic-release" rel="noreferrer noopener" target="_blank">semantic-release</a> with GitHub Actions to handle publishing. The automation is convenient and predictable. I also have npm provenance enabled on many packages, which provides attestations of how they were built. Unfortunately, provenance didn’t prevent this attack because the attacker had a valid token.</p>
<p>My goal is to move to npm’s <strong>Trusted Publishing (OIDC)</strong> to eliminate static tokens altogether. However, semantic-release integration is still in progress: <a href="https://github.com/npm/cli/issues/8525" rel="noreferrer noopener" target="_blank">npm/cli#8525</a>.</p>
<p><img alt="npm Publishing access settings" loading="lazy" decoding="async" fetchpriority="auto" sizes="(min-width: 1618px) 1618px, 100vw" data-astro-image="constrained" width="1618" height="804" src="https://sigh.dev/_astro/publishing-access.DTmYbTkJ_Z2may3Q.webp" srcset="https://sigh.dev/_astro/publishing-access.DTmYbTkJ_1Fa49o.webp 640w, https://sigh.dev/_astro/publishing-access.DTmYbTkJ_Z22BseH.webp 750w, https://sigh.dev/_astro/publishing-access.DTmYbTkJ_Z27AVbY.webp 828w, https://sigh.dev/_astro/publishing-access.DTmYbTkJ_ZGtYiM.webp 1080w, https://sigh.dev/_astro/publishing-access.DTmYbTkJ_Z11X4ph.webp 1280w, https://sigh.dev/_astro/publishing-access.DTmYbTkJ_Z2may3Q.webp 1618w"></p><p>For the forseeable future, @ctrl/tinycolor requires 2FA for publishing, and all tokens have been revoked. Not expecting to merge any new changes anytime soon.</p>
<p>For smaller packages, I’ll continue using semantic-release but under stricter controls: no new contributors will be added, and each repo will use a granular npm token limited to publish-only rights for that specific package.</p>
<p>Local 2FA based publishing isn’t sustainable, so I’m watching OIDC/Trusted Publishing closely and will adopt it as soon as it fits the workflow.</p>
<p>I plan to continue using pnpm that prevents unapproved postinstall scripts from being run and I’ll look into adding pnpm’s new <a href="https://pnpm.io/settings#minimumreleaseage" rel="noreferrer noopener" target="_blank">minimumReleaseAge</a> setting.</p>
<h2 id="publishing-wishlist"><a href="#publishing-wishlist">Publishing Wishlist</a></h2>
<p>If I could wave a magic wand and design my ideal setup, npm would allow me to require Trusted Publishing (OIDC) with a single toggle for all of my packages. That same toggle would block any release missing provenance, enforcing security at the account level. I’d also want first-class semantic-release support with OIDC and provenance so no static tokens are ever needed.</p>
<p>On top of that, I’d like a secure, human-approved publishing option directly in the GitHub UI: a protected workflow_dispatch flow that uses github 2FA approval to satisfy 2FA, without requiring me to publish from my laptop.</p>
<p>GitHub Environments — or equivalent workflow protections — should be available without a Pro subscription, or else integrated directly into Trusted Publishing so that security doesn’t depend on the pricing tier.</p>
<p>It would be really nice if NPM also had a more visible mark on the package details page to indicate if the package had a postinstall script. Also, once the packages are pulled its not clear what versions were removed and why.</p>
<h2 id="thanks"><a href="#thanks">Thanks</a></h2>
<p>Thanks to Wes Todd, the OpenJS Foundation, and the GitHub/npm security teams for their rapid and coordinated response. Everyone was incredibly fast, helpful, and knowledgeable.</p>
<p><img alt="dune worm [wide] | dune worm via chatgpt" loading="lazy" decoding="async" fetchpriority="auto" sizes="(min-width: 1536px) 1536px, 100vw" data-astro-image="constrained" width="1536" height="1024" src="https://sigh.dev/_astro/dune-worm.QN2JLFkT_ZOy594.webp" srcset="https://sigh.dev/_astro/dune-worm.QN2JLFkT_ZWljFi.webp 640w, https://sigh.dev/_astro/dune-worm.QN2JLFkT_Z24bQ0U.webp 750w, https://sigh.dev/_astro/dune-worm.QN2JLFkT_1CdkyI.webp 828w, https://sigh.dev/_astro/dune-worm.QN2JLFkT_1kRnJS.webp 1080w, https://sigh.dev/_astro/dune-worm.QN2JLFkT_Z1xwr04.webp 1280w, https://sigh.dev/_astro/dune-worm.QN2JLFkT_ZOy594.webp 1536w">  </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Drought in Iraq reveals tombs created 2,300 years ago (105 pts)]]></title>
            <link>https://www.smithsonianmag.com/smart-news/severe-droughts-in-iraq-reveals-dozens-of-ancient-tombs-created-2300-years-ago-180987347/</link>
            <guid>45278581</guid>
            <pubDate>Wed, 17 Sep 2025 17:12:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.smithsonianmag.com/smart-news/severe-droughts-in-iraq-reveals-dozens-of-ancient-tombs-created-2300-years-ago-180987347/">https://www.smithsonianmag.com/smart-news/severe-droughts-in-iraq-reveals-dozens-of-ancient-tombs-created-2300-years-ago-180987347/</a>, See on <a href="https://news.ycombinator.com/item?id=45278581">Hacker News</a></p>
Couldn't get https://www.smithsonianmag.com/smart-news/severe-droughts-in-iraq-reveals-dozens-of-ancient-tombs-created-2300-years-ago-180987347/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Ton Roosendaal to step down as Blender chairman and CEO (248 pts)]]></title>
            <link>https://www.cgchannel.com/2025/09/ton-roosendaal-to-step-down-as-blender-chairman-and-ceo/</link>
            <guid>45278279</guid>
            <pubDate>Wed, 17 Sep 2025 16:49:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cgchannel.com/2025/09/ton-roosendaal-to-step-down-as-blender-chairman-and-ceo/">https://www.cgchannel.com/2025/09/ton-roosendaal-to-step-down-as-blender-chairman-and-ceo/</a>, See on <a href="https://news.ycombinator.com/item?id=45278279">Hacker News</a></p>
<div id="readability-page-1" class="page"><div class="page">
			<article class="page">
			<header>
				<span>Wednesday, September 17th, 2025</span>
				Posted by Jim Thacker			</header>

			

			<main>
				
				
<p><iframe width="960" height="539" src="https://www.youtube.com/embed/JXm0-ilIknE?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="" title="Keynote — Blender Conference 2025"></iframe></p>
<p><br>
Ton Roosendaal is to stop down as chairman and Blender CEO on 1 January 2026. The news was announced during today’s keynote at the annual Blender Conference.</p>
<p>Roosendaal – the <a href="https://www.blender.org/about/history/" target="_blank">original author</a> of the open-source 3D software, and its public figurehead for the past three decades – will pass on his roles to current Blender COO Francesco Siddi.</p>
<p>Roosendaal himself will move to the newly established Blender Foundation supervisory board.</p>
<p>Other new Blender Foundation board positions will also include Sergey Sharybin (Head of Development), Dalai Felinto (Head of Product) and Fiona Cohen (Head of Operations).</p>
<p>“We’ve been preparing for this since 2019,” said Roosendaal, “I am very proud to have such a wonderfully talented young team around me to bring our free and open source project into the next decade.”</p>
<p><em>We aim to update this story with a brief retrospective of Ton’s time as Blender CEO and the growth of Blender during that time, so check back for updates.</em></p>
<p><a href="https://www.blender.org/press/blender-foundation-announces-new-board-and-executive-director/" target="_blank">Read the official announcement that Ton Roosendaal is stepping down as Blender CEO</a></p>


			</main>

			
			<!-- Tags -->

			
		</article>

		
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Not Buying American Anymore (141 pts)]]></title>
            <link>https://xd1.dev/2025/09/not-buying-american-anymore</link>
            <guid>45277346</guid>
            <pubDate>Wed, 17 Sep 2025 15:53:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://xd1.dev/2025/09/not-buying-american-anymore">https://xd1.dev/2025/09/not-buying-american-anymore</a>, See on <a href="https://news.ycombinator.com/item?id=45277346">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
                <h2 id="content-not-buying-american-anymore">Not buying American anymore</h2>
<p>I feel like I need to first defuse any idea of antagonizing Americans
themselves. The people are never to blame. This isn't a post criticizing the
American people, it's criticizing the state of things that makes it impractical
to have any sort of economical relation with the country if you are an average
Joe like me.</p>
<h2 id="content-where-do-i-come-from">Where do I come from</h2>
<p>I come from a place where I believe regulation needs to be employed to protect
the weaker party in any economic relation. If you don't agree with this
probably this rant isn't for you.</p>
<p>I also come from Brazil, a country that, for better or worse, is arguably a
strongly regulated market. For worse I guess because it makes all transactions
that bit more bureaucratic, for better because I am sure to be able to buy
something and have it too, without fear that corporations will swoop in and
take away my rights after purchase.</p>
<p>The <a href="https://www.gov.br/mj/pt-br/assuntos/seus-direitos/consumidor/Anexos/guia-do-consumidor-estrangeiro-ingles.pdf">Consumer Defense
Code</a>
is strong and mostly works, at least insofar as I needed to use it, and many
issues can be resolved directly with companies via
<a href="https://www.reclameaqui.com.br/">reclameaqui</a>, which is a website that
streamlines the communication with sellers to resolve problems without having
to go to court.</p>
<p>So here are my biases laid out in advance.</p>
<h2 id="content-why-am-i-writing-this">Why am I writing this</h2>
<p>I've been following <a href="https://www.youtube.com/@rossmanngroup">Louis Rossmann's youtube
channel</a> for a while now, mostly
because their tech repair videos are really interesting. And I really love
cats. But lately I've seen
<a href="https://youtu.be/HlyiLQ6WPRU?si=LeS8D1ntWfTUrUUB">many</a>
<a href="https://youtu.be/KNuZ3BjT7IU?si=93NVeqWEI1CO24Nv">worrying</a>
<a href="https://youtu.be/lzdIjCzKhfM?si=dET-MmiWzW7VJixk">videos</a> in the channel about
anti-consumer practices and I felt like I needed to vent.</p>
<p>You can ask why do I even care about these videos if I don't live in the US.
Well, in a globalized world where we are steadily blurrying the lines that
separate countries and nations (also for better and worse), there is really no
such distinction. Sure when I buy services and pay for them in my local
currency, I'm likely protected by local laws, and also in the case of Netflix
I'm also restricted by geographical limitations, but that's not always the
case, specially when purchasing software.</p>
<p>I've just came across <a href="https://youtu.be/YAx3yCNomkg?si=PhsAcUN-z7zXpvOC">this
video</a> where Louis explains
what Reason studios did to screw over their customers by removing the option to
activate older products that still work fine. I am also a musician in my spare
time, mostly hobbyist, but I did purchase music production software before and
I could just as easily have been a victim of such customer-hostile practices. I
don't need to be American to be an interested party when laws, or lack thereof,
of foreign country directly affects me.</p>
<h2 id="content-current-state-of-things">Current state of things</h2>
<p>All this points to a very clear trend, at least for me, that the US is <a href="https://www.hks.harvard.edu/faculty-research/policycast/oligarchy-open-what-happens-now-us-forced-confront-its-plutocracy">openly
an
oligarchy</a>.
And this explains very well this trend of consumer-hostile practices.</p>
<p>If a country and its laws serve the nobility and the extremely wealthy, it'll
work in favor of those, of maintaining their status and wealth. An oligarchy
isn't a regime that is characterized by actively screwing over the common
people. It doesn't need to be. All it needs is to give a free pass for those
that maintain power and influence to do whatever they want in order to maximize
their profits and expand their influence.</p>
<p>This is why all these anti-consumer practices are happening out in the open.
Really... what are the chances that in the current administration a profitable
company will be prosecuted by anti-consumer practices?</p>
<p>Given time, a liberal capitalist democracy with excessively weak regulations
will eventually devolve into a plutocracy just because companies need to make
money to appease investors at all costs. When they are out of ideas for
innovation, or when innovation is just too risky, they will <a href="https://www.baldurbjarnason.com/2024/the-deterioration-of-google/">make their
services
worse</a>,
<a href="https://shawlewenz.com/11-times-big-brands-violated-consumer-protection-laws/">violate consumer protection laws if it makes them more
competitive</a>
and <a href="https://disconnect.blog/ive-had-it-with-microsoft/">make the consumer pay more for
it</a>, just because it looks
good in a quarterly report -- which I guess <a href="https://www.reuters.com/sustainability/boards-policy-regulation/trump-renews-calls-ending-quarterly-reports-companies-2025-09-16/">won't be quarterly
anymore</a>
because it's not looking too good lately.</p>
<h2 id="content-a-counter-argument">A counter argument</h2>
<p>There is a point to be made that all these changes were made recently and that
there was no reason not to buy American ten, fifteen years ago. And therefore
this logic of not buying American isn't going to work because the same can
happen anywhere else.</p>
<p>But I'd disagree. We can't and we don't need to be able to see the future to
make informed decisions. Ten years ago there was no reason not to buy American.
There is now and that's the end of it. If I start buying European and they
start behaving like the US does now, then this rant will just as easily apply
to them.</p>
<p>A good, informed decision doesn't require knowledge of the future. It just need
to be grounded in solid contemporary facts, and the fact is there is no reason
we can trust American companies anymore.</p>
<p>This rant also isn't to say that <em>all</em> American companies are trying to make
the largest possible profit at the expense of their customers. There surely are
legit businesses trying to be profitable at the same time as they care and
protect their customers. Unfortunately these same theoretical companies are
subjective to the current US economical ethos that exposes them to hostile
takeovers and pressure from investors. This is why in an unregulated market
that makes it <em>that</em> easy to screw customers over, even those that are honest
good-working citizens can't really be trusted to run stable and responsible
companies in the long run.</p>
<h2 id="content-a-moral-imperative">A moral imperative</h2>
<p>The TLDR is: if you can, don't buy American. I'm not buying it anymore if I
can. There is little innovation to be had there, little protection to rely upon
and to be honest little incentive to keep buying it, because the rest of the
world is picking up relatively quickly, since all the wealth has had a negative
impact in the incentive for the US industry to keep itself up to date.</p>
<p>Choosing not to buy American is a message. The message is simple. I don't need
it. I would like to keep improving on things, keep working together and be part
of flourishing global community. But I don't have to. I can make do with less
feature-packed alternatives that will serve me longer term. I can do without
all the wealth and shiny things, because honestly in a couple of decades time
nobody will even remember it if the US keeps not paying attention to those that
generate actual value to the world, the people. If you change your laws, if you
can show that you are not out there to get my money at all costs, then we are
back in business. Until them I'm not buying American anymore.</p>
<p>Nothing is too big that it can't be replaced.</p>
<h2 id="content-comments">Comments</h2>
<p>If you want to comment on this blog post, I invite you to follow the dicussions
on <a href="https://news.ycombinator.com/item?id=45277346">Hackernews</a>.</p>

                
                
            </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to motivate yourself to do a thing you don't want to do (269 pts)]]></title>
            <link>https://ashleyjanssen.com/how-to-motivate-yourself-to-do-a-thing-you-dont-want-to-do/</link>
            <guid>45276987</guid>
            <pubDate>Wed, 17 Sep 2025 15:25:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ashleyjanssen.com/how-to-motivate-yourself-to-do-a-thing-you-dont-want-to-do/">https://ashleyjanssen.com/how-to-motivate-yourself-to-do-a-thing-you-dont-want-to-do/</a>, See on <a href="https://news.ycombinator.com/item?id=45276987">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <article>

            <header>

                

                    

                    <p>Learn some ways to help encourage action when you feel unmotivated.&nbsp;</p>

                    <figure>
        <img srcset="https://ashleyjanssen.com/content/images/size/w300/2025/09/How-to-Motivate-Yourself-To-Do-A-Thing-You-Don-t-Want-to-Do.jpg 300w,
                    https://ashleyjanssen.com/content/images/size/w720/2025/09/How-to-Motivate-Yourself-To-Do-A-Thing-You-Don-t-Want-to-Do.jpg 720w,
                    https://ashleyjanssen.com/content/images/size/w960/2025/09/How-to-Motivate-Yourself-To-Do-A-Thing-You-Don-t-Want-to-Do.jpg 960w,
                    https://ashleyjanssen.com/content/images/size/w1200/2025/09/How-to-Motivate-Yourself-To-Do-A-Thing-You-Don-t-Want-to-Do.jpg 1200w,
                    https://ashleyjanssen.com/content/images/size/w2000/2025/09/How-to-Motivate-Yourself-To-Do-A-Thing-You-Don-t-Want-to-Do.jpg 2000w" sizes="(max-width: 1200px) 100vw, 1200px" src="https://ashleyjanssen.com/content/images/size/w1200/2025/09/How-to-Motivate-Yourself-To-Do-A-Thing-You-Don-t-Want-to-Do.jpg" alt="How to Motivate Yourself To Do A Thing You Don't Want to Do">
            <figcaption><span>Photo by </span><a href="https://unsplash.com/@anniespratt?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash"><span>Annie Spratt</span></a><span> on </span><a href="https://unsplash.com/photos/a-piece-of-paper-with-a-message-written-on-it-0Qo_Nn5wLOc?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash"><span>Unsplash</span></a></figcaption>
    </figure>
            </header>

            <section>
                <p>We have an air bike in our basement. If you are unfamiliar with air bikes, they are similar to stationary bikes with foot pedals but also have handles you push and pull with your arms. It uses air resistance, so the harder you pedal and move your arms, the higher the resistance.&nbsp;</p><p>It’s also known as an assault bike. 😬</p><p>Which is apt, because it’s a <em>butt-kicker</em> of a workout. I use it about once a week, more frequently in the winter when it’s too cold to run, and less often in the summer when I can get outside more. And I kind of hate it!</p><p>Before I even drag myself to our basement, I’m already dreading it. The only way I can convince myself to do it is by finding a suitably engaging show I can distract myself with on my phone while I huff and puff.&nbsp;</p><p>Every time, I start my warm-up and think to myself,</p><p>“It’s only 30 minutes, I can do this!”</p><p>Like clockwork, within the first three minutes, I think, “Maybe I will only do ten minutes today and do some pilates or weights instead.”</p><p>After ten minutes, I think, “OK, surely I can make it to 20 minutes, and that will be enough”.</p><p>After 20 minutes, as I gasp for air and sweat soaks through my shirt, I think “Well, I already made it to 20 minutes… I guess I will just finish it.”</p><p>And then I proceed to huff and puff to the end, wherein I walk my wobbly legs back up the stairs to do a cooldown. At which point I think, “That suuuuuucked…” And then congratulate myself on finishing as I try to get my heart rate back to normal. 🥵</p><p>This mental dance happens, without fail, every single time I ride.&nbsp;</p><hr><p>I share this anecdote because it illustrates how tricky motivation can be, especially when faced with something you don’t want to do or have been procrastinating on. There are any number of things you have to deal with in your life that you don’t want to. There are even things you might generally enjoy that feel like they are hanging over you.&nbsp;</p><p>The pattern often goes like this:</p><ul><li>Before you start, it feels daunting, and the prospect lingers in the back of your mind. You know it needs to be done, but you really, <em>really</em> don’t feel like it. You leave it until it starts to loom larger and larger.</li><li>When you finally convince yourself to start, it’s not what you want to be doing, but it’s generally <em>fine</em>. It’s often not even as bad as you thought it would be, and it feels good to make progress.</li><li>As you near the end, you can even push yourself a little to wrap it up and get it off your plate.</li><li>When it’s over, you feel relieved, like a weight has been taken off your shoulders, and you are both pleased with yourself and a little annoyed that it took you so long to deal with.</li></ul><p>Sound familiar?&nbsp;</p><p>Motivation is a topic that comes up with nearly all my clients, as they navigate the various complexities of their lives. In some ways, motivation seems simple. You ask yourself, <strong>“Why can’t I just <em>make</em> myself be motivated to do the thing?”</strong>, whatever the thing might be. However, as you beat yourself up about it, consider that many factors influence our decision-making and the feeling of being motivated.</p><p>Humans are complex creatures, with <a href="https://thetouchpointsolution.com/blogs/touchpoints-blog/the-impact-of-brain-chemicals-on-mood-and-health?ref=ashleyjanssen.com"><u>numerous brain chemicals and hormones</u></a> influencing our overall physical and emotional state, which themselves are constantly impacted, sometimes drastically, by things like:</p><ul><li>Have you been sleeping well and enough?</li><li>Have you been eating well and the right amount for you?</li><li>Have you been imbibing in alcohol or other things?</li><li>Have you been moving your body regularly?</li><li>Do you have any physical or mental conditions?</li><li>Are you in pain?</li><li>Do you have significant life stressors at this time?</li><li>What time of day is it?</li><li>Where are you in your natural hormone cycles?</li><li>How old are you?</li><li>Have you had any conflicts in your life recently?</li><li>Did you move your body in a way entirely within your usual routines, but apparently in a way that is no longer acceptable?&nbsp;</li><li>Did you sleep in a slightly different position than usual, and now your back will never be the same again?</li></ul><p>I could go on, but you get the idea.😅</p><p>All of these factors (and more) conspire to shift your mood, physical energy, and mental energy, often making it harder to muster the motivation to do things. What, then, can you do to move things in the right direction? How do you motivate yourself to do a thing you don’t want to do?</p><p>Here are several ways to help encourage action when you feel unmotivated.&nbsp;</p><h2 id="1-think-about-why-you-are-feeling-unmotivated">1. Think about <em>why</em> you are feeling unmotivated</h2><p>There are many external and internal factors, as listed above, that contribute to motivation.&nbsp;</p><ul><li>When your body isn’t feeling good, it’s harder to make it do things.&nbsp;</li><li>When your mind is tired, distracted, or overwhelmed, it’s challenging to focus and accomplish tasks.&nbsp;</li><li>When the thing you need to do isn’t important to you or something you don’t like, it’s hard to make yourself do it.</li></ul><p>When you know why you aren’t motivated, you can think about what you could change to make things easier on yourself. What factors do you have control over?&nbsp;</p><ul><li><strong>Environment</strong> - Is there a place you can go or a thing you can add that will make it feel easier? For example, I have my writing desk set up in a quiet corner of my bedroom (not the office I share with my husband) to help make writing easier, even when I am not feeling it.&nbsp;</li><li><strong>Mood</strong> - Is there something that will help boost your mood? Go for a ten-minute walk, treat yourself to a donut, text your best friend for a pep talk, turn on your favourite tunes… anything that will give you a little pick-me-up.</li><li><strong>Body</strong> - Are there things you can do to take care of your body to make it feel better? Try some stretching, take a nap, meditate, read a book, get some fresh air, go for a run, eat a comfort meal, or do anything that will help your body feel less stressed.</li><li><strong>Negative or fear motivators</strong> - Is the thing you are not motivated to do being motivated by negative or <a href="https://ashleyjanssen.com/7-strategies-to-stop-fear-based-decision-making/"><u>fear motivators</u></a>? These include things like fear of judgment, fear of conflict, shame, guilt, or obligation. These motivators only go so far and deserve further examination to determine their place in your priorities. Maybe they aren’t things you need to do in the first place.&nbsp;&nbsp;</li></ul><p>The key point here is to identify where you have control and where you don’t, and then do your best to adapt your circumstances to make it easier to take action.</p><h2 id="2-identify-what-does-motivate-you">2. Identify what <em>does</em> motivate you&nbsp;</h2><p>When you think about the various activities and tasks you do each day, what is it that encourages you to do them? Some of those things will be negative motivators, as I mentioned above, but others will be things you do for fun, because they are interesting or rewarding. These are some tactics to consider for things that might help motivate you:</p><h3 id="combine-the-task-with-something-you-enjoy">Combine the task with something you enjoy</h3><p>You know what makes cleaning out the garage a lot better? Some good tunes. Throw on an audiobook while you cook dinner. Watch a good show while you huff and puff on the air bike! Think about the things you enjoy and consider how you can combine them with the thing you're trying to motivate yourself to do.</p><h3 id="add-external-accountability">Add external accountability</h3><p>Sometimes it can be challenging to push yourself to do something when there are no external motivators. Ask a friend to be your accountability buddy, or hire a professional to help you stay accountable for the thing you're trying to do, such as a coach, trainer, teacher, or dietitian. I know that one of the significant value-added benefits my clients get from <a href="https://ashleyjanssen.com/consulting/"><u>working with me</u></a> for a few months is having someone they have to report back to on their progress!</p><h3 id="gamify">Gamify</h3><p>Is there any way to turn the process or thing you are unmotivated to do into a game? Can you add rewards if you do a certain amount, or set a goal for how many days you make progress in a row? For example, one of my motivators for doing some kind of fitness every day is<a href="https://ashleyjanssen.com/how-tracking-and-streaks-help-you-establish-habits-and-reach-your-goals/"><u> keeping up my streak</u></a>! 2817 days in a row as of publishing. 😁</p><h3 id="celebrate-milestones">Celebrate milestones</h3><p>Beyond small planned rewards, having something to look forward to as you make progress on your task or activity can also help encourage you to continue moving forward. Maybe you take a day off, order your favourite takeout, or simply share it with someone you care about.&nbsp;</p><p>For more specifics on types of motivation, read my article, <a href="https://ashleyjanssen.com/what-motivates-you-learn-the-types-of-motivation-and-how-to-use-them/"><u>What Motivates You? Learn the Types of Motivation and How to Use Them</u></a><strong>, </strong>where I get into more detail about intrinsic and extrinsic motivation.</p><figure><img src="https://ashleyjanssen.com/content/images/2025/09/treat-yo-self.jpg" alt="" loading="lazy" width="2000" height="1244" srcset="https://ashleyjanssen.com/content/images/size/w600/2025/09/treat-yo-self.jpg 600w, https://ashleyjanssen.com/content/images/size/w1000/2025/09/treat-yo-self.jpg 1000w, https://ashleyjanssen.com/content/images/size/w1600/2025/09/treat-yo-self.jpg 1600w, https://ashleyjanssen.com/content/images/size/w2400/2025/09/treat-yo-self.jpg 2400w" sizes="(min-width: 720px) 720px"><figcaption><span>Photo by </span><a href="https://unsplash.com/@jayrheike?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash"><span>Jay Heike</span></a><span> on </span><a href="https://unsplash.com/photos/white-love-neon-light-signage-QZ8dPT46gzc?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash"><span>Unsplash</span></a></figcaption></figure><h2 id="3-break-it-into-smaller-chunks">3. Break it into smaller chunks</h2><p>If part of why you feel unmotivated is that what you need to do feels big and overwhelming, often the best thing you can do is try to break it down into smaller, more manageable pieces. What is the smallest amount you can do to make a bit of progress?</p><ul><li>Commit to spending 5 minutes on it</li><li>Choose a small corner of a room you need to clean</li><li>Commit to writing the outline</li><li>Write the text, even if you don’t send it</li><li>Plan in your calendar when you will do it, so you don’t have it sitting in the back of your mind</li><li>Talk about it with your partner or a friend</li><li>Switch tasks to take a break and come back to it</li></ul><p>Often, getting over the hump of <em>starting</em> something is enough to help push you through it. Even if it isn’t, at the very least, you have made some amount of progress, which you can build on.&nbsp;</p><h2 id="4-consistency-over-motivation">4. Consistency over motivation</h2><p>If the thing you need to do is something you need to do regularly, like writing, fitness, practicing an instrument, or cleaning, you can’t rely purely on motivation to drive you. Even for things you enjoy, it’s easy to push something off <em>“until you feel like it”.</em> But with so many factors affecting your mood and energy, the times when you feel like it will be fleeting. Instead of relying on motivation, try to establish a routine that fosters consistency.&nbsp;</p><ul><li>Plan your <a href="https://ashleyjanssen.com/time-blocking-and-imagining-your-ideal-week/" rel="noreferrer">intentional week</a> so you have an idea of when you intend to do it</li><li>Set a daily reminder</li><li>Book it in your calendar</li><li>Set a certain amount of time you will put aside each day or week to chip away at it</li></ul><p>A little bit, consistently, will go a long way.</p><h2 id="5-put-it-on-the-back-burner">5. Put it on the back burner</h2><p>Sometimes, when you are not feeling motivated to do something, it’s reasonable to just put it on the back burner. Maybe it’s just not a priority right now, and that’s totally fine! Ask yourself, is this <a href="https://ashleyjanssen.com/how-to-juggle-priorities-decide-which-balls-are-glass-and-which-are-plastic/"><u>a glass ball</u></a> or a plastic ball? If it’s plastic, set it aside for a bit and focus your time and energy on other things.</p><p>It's ok to decide now is not the right time, but make it an intentional decision instead of something you avoid and feel bad about!</p><hr><p>If you're struggling with motivation, you're not alone! It’s normal, it’s natural, and there are tons of different, ever-changing factors that will change how you feel. Do your best to examine where you are at, control what you can control, and make progress where you can!</p><p>Need some help getting motivated? Get in touch!</p>
            </section>

            

            <div>
                        <figure>
                            <img src="https://ashleyjanssen.com/content/images/2023/09/AshleyJanssen-500x500.jpeg" alt="Ashley Janssen">
                        </figure>

                    <div>


                        <p>Productivity consultant, writer, speaker, serial entrepreneur, chaos calmer, introvert, cat-lady. Lover of books, fitness, old fashioned’s, basketball, and video games.</p>

                        <p>
                            Follow me on
                            <a href="https://twitter.com/AshleyJanssen">Twitter</a>
                            or
                            <a href="https://www.linkedin.com/in/ashleyjanssen">LinkedIn</a>.
                            <br>
                            Hire me for
                            <a href="https://ashleyjanssen.com/consulting">1 on 1 productivity consulting</a>
                            or
                            <a href="https://ashleyjanssen.com/speaking">speaking</a>.
                        </p>
                    </div>
                </div>
        </article>


                

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[YouTube addresses lower view counts which seem to be caused by ad blockers (286 pts)]]></title>
            <link>https://9to5google.com/2025/09/16/youtube-lower-view-counts-ad-blockers/</link>
            <guid>45276262</guid>
            <pubDate>Wed, 17 Sep 2025 14:29:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://9to5google.com/2025/09/16/youtube-lower-view-counts-ad-blockers/">https://9to5google.com/2025/09/16/youtube-lower-view-counts-ad-blockers/</a>, See on <a href="https://news.ycombinator.com/item?id=45276262">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					
<figure>
	<img width="1600" height="800" src="https://9to5google.com/wp-content/uploads/sites/4/2025/06/youtube-logo-desktop-1.jpg?quality=82&amp;strip=all&amp;w=1600" alt="" srcset="https://i0.wp.com/9to5google.com/wp-content/uploads/sites/4/2025/06/youtube-logo-desktop-1.jpg?w=320&amp;quality=82&amp;strip=all&amp;ssl=1 320w, https://i0.wp.com/9to5google.com/wp-content/uploads/sites/4/2025/06/youtube-logo-desktop-1.jpg?w=640&amp;quality=82&amp;strip=all&amp;ssl=1 640w, https://i0.wp.com/9to5google.com/wp-content/uploads/sites/4/2025/06/youtube-logo-desktop-1.jpg?w=1024&amp;quality=82&amp;strip=all&amp;ssl=1 1024w, https://i0.wp.com/9to5google.com/wp-content/uploads/sites/4/2025/06/youtube-logo-desktop-1.jpg?w=1500&amp;quality=82&amp;strip=all&amp;ssl=1 1500w" decoding="async" fetchpriority="high"></figure>

<p>Over the past month or so, many YouTubers have been reporting major drops to their video view counts. Theories have run wild, but there’s one explanation involving ad blockers that makes the most sense, but YouTube isn’t confirming anything directly.</p>



<p>Since mid-August, many YouTubers have noticed their view counts are considerably lower than they were before, in some cases with very drastic drops. The reason for the drop, though, has been shrouded in mystery for many creators.</p>



<p>The most likely explanation seems to be that YouTube is not counting views properly for users with an ad blocker enabled, another step in the platform’s continued war on ad blockers. This was first realized <a href="https://youtu.be/YX1eEe8erkQ">by Josh Strife Hayes, who noticed</a> that view counts on TV, phones, and tablets have been steady, while views on computers have dropped by around 50% since the mid-August trend started. TechLinked, a channel in the Linus Tech Tips family, <a href="https://youtu.be/gZ5pATTvc2o">confirmed similar numbers within its statistics</a>.</p>



<p>This aligns with one of the possible explanations that <a href="https://support.google.com/youtube/thread/373195597">YouTube itself hinted at in an acknowledgement of lower view counts. </a></p>	
	



<p>Google says:</p>



<blockquote>
<p><strong>Viewers Using Ad Blockers &amp; Other Content Blocking Tools:&nbsp;</strong>Ad blockers and other extensions can impact the accuracy of reported view counts. Channels whose audiences include a higher proportion of users utilizing such tools may see more fluctuations in traffic related to updates to these tools.</p>
</blockquote>




	<p>The rest of the post addresses prior speculation that YouTube’s <a href="https://9to5google.com/2025/08/07/youtube-ai-age-verification-what-to-know/">new AI-powered age verification tools</a> were to blame – which YouTube adamantly says is not the case – while also offering other possible explanations such as “seasonal viewing habits” and competition on the platform. </p>



<p>YouTube says “there is no systemic issue that is impacting creators” regarding lower view counts.</p>



<p>This ad blocker situation does seem the most likely explanation, though. In <a href="https://youtu.be/KqCV6Rk8kOA">a prior video</a>, Linus Tech Tips had noted that while view counts were down, ad revenue was not. If computer views are the only ones down, it stands to reason that viewers using an ad blocker are not being counted correctly, especially if ad revenue isn’t taking a hit from the lower view counts. YouTube’s hint that ad blockers “can impact the accuracy of reported view counts” certainly suggests this is possible, even if it’s not firm confirmation.</p>



<h2 id="h-more-on-youtube">More on YouTube:</h2>



<ul>
<li><a href="https://9to5google.com/2025/08/26/youtube-for-android-tv-google-tv-beta-program/">YouTube for Android TV, Google TV will now let you test new features in beta</a></li>



<li><a href="https://9to5google.com/2025/08/08/fix-unwanted-youtube-recommendations/">YouTube recommending awful videos? Here’s how to fix that</a></li>



<li><a href="https://9to5google.com/2025/07/30/youtube-new-profanity-guidelines-utilizes-ai-identify-teens/">YouTube rolls out new profanity guidelines for creators</a></li>
</ul>



<p><em><strong>Follow Ben:</strong>&nbsp;<a href="https://twitter.com/NexusBen" target="_blank" rel="noreferrer noopener">Twitter/X</a>,&nbsp;<a href="https://www.threads.net/@nexusben" target="_blank" rel="noreferrer noopener">Threads</a>, <a href="https://bsky.app/profile/nexusben.com">Bluesky</a>, and&nbsp;<a href="https://www.instagram.com/nexusben" target="_blank" rel="noreferrer noopener">Instagram</a></em></p>
	<p>
				<a target="_blank" rel="nofollow" href="https://news.google.com/publications/CAAqBwgKMMqA-Qow-c_gAg?hl=en-US&amp;gl=US&amp;ceid=US:en">
			<em>Add 9to5Google to your Google News feed.</em>&nbsp;
					</a>
			</p>
	<p><em>FTC: We use income earning auto affiliate links.</em> <a href="https://9to5mac.com/about/#affiliate">More.</a></p>				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[UUIDv47: Store UUIDv7 in DB, emit UUIDv4 outside (SipHash-masked timestamp) (157 pts)]]></title>
            <link>https://github.com/stateless-me/uuidv47</link>
            <guid>45275973</guid>
            <pubDate>Wed, 17 Sep 2025 14:02:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/stateless-me/uuidv47">https://github.com/stateless-me/uuidv47</a>, See on <a href="https://news.ycombinator.com/item?id=45275973">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">UUIDv47 - UUIDv7-in / UUIDv4-out (SipHash-masked timestamp)</h2><a id="user-content-uuidv47---uuidv7-in--uuidv4-out-siphash-masked-timestamp" aria-label="Permalink: UUIDv47 - UUIDv7-in / UUIDv4-out (SipHash-masked timestamp)" href="#uuidv47---uuidv7-in--uuidv4-out-siphash-masked-timestamp"></a></p>
<p dir="auto">uuidv47 lets you store sortable UUIDv7 in your database while emitting a
UUIDv4-looking façade at your API boundary. It does this by XOR-masking
only the UUIDv7 timestamp field with a keyed SipHash-2-4 stream tied to
the UUID’s own random bits.</p>
<ul dir="auto">
<li>Header-only C (C89) · zero deps</li>
<li>Deterministic, invertible mapping (exact round-trip)</li>
<li>RFC-compatible version/variant bits (v7 in DB, v4 on the wire)</li>
<li>Key-recovery resistant (SipHash-2-4, 128-bit key)</li>
<li>Full tests provided</li>
</ul>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li>Why</li>
<li>Quick start</li>
<li>Public API</li>
<li>Specification
<ul dir="auto">
<li>UUIDv7 bit layout</li>
<li>Façade mapping (v7 ↔ v4)</li>
<li>SipHash message derived from random</li>
<li>Invertibility</li>
<li>Collision analysis</li>
</ul>
</li>
<li>Security model</li>
<li>Build, test, coverage</li>
<li>Integration tips</li>
<li>Performance notes</li>
<li>FAQ</li>
<li>License</li>
</ul>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why</h2><a id="user-content-why" aria-label="Permalink: Why" href="#why"></a></p>
<ul dir="auto">
<li>DB-friendly: UUIDv7 is time-ordered → better index locality &amp; pagination.</li>
<li>Externally neutral: The façade hides timing patterns and looks like v4 to clients/systems.</li>
<li>Secret safety: Uses a PRF (SipHash-2-4). Non-crypto hashes are not suitable when the key must not leak.</li>
</ul>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick start" href="#quick-start"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="#include <stdio.h>
#include &quot;uuidv47.h&quot;

int main(void){
  const char* s = &quot;00000000-0000-7000-8000-000000000000&quot;;
  uuid128_t v7;
  if (!uuid_parse(s, &amp;v7)) return 1;
  uuidv47_key_t key = { .k0 = 0x0123456789abcdefULL, .k1 = 0xfedcba9876543210ULL };
  uuid128_t facade = uuidv47_encode_v4facade(v7, key);
  uuid128_t back = uuidv47_decode_v4facade(facade, key);

  char a[37], b[37], c[37];
  uuid_format(&amp;v7, a);
  uuid_format(&amp;facade, b);
  uuid_format(&amp;back, c);
  printf(&quot;v7 (DB) : %s\n&quot;, a);
  printf(&quot;v4 (API): %s\n&quot;, b);
  printf(&quot;back    : %s\n&quot;, c);
}"><pre><span>#include</span> <span>&lt;stdio.h&gt;</span>
<span>#include</span> <span>"uuidv47.h"</span>

<span>int</span> <span>main</span>(<span>void</span>){
  <span>const</span> <span>char</span><span>*</span> <span>s</span> <span>=</span> <span>"00000000-0000-7000-8000-000000000000"</span>;
  <span>uuid128_t</span> <span>v7</span>;
  <span>if</span> (!<span>uuid_parse</span>(<span>s</span>, <span>&amp;</span><span>v7</span>)) <span>return</span> <span>1</span>;
  <span>uuidv47_key_t</span> <span>key</span> <span>=</span> { .<span>k0</span> <span>=</span> <span>0x0123456789abcdefULL</span>, .<span>k1</span> <span>=</span> <span>0xfedcba9876543210ULL</span> };
  <span>uuid128_t</span> <span>facade</span> <span>=</span> <span>uuidv47_encode_v4facade</span>(<span>v7</span>, <span>key</span>);
  <span>uuid128_t</span> <span>back</span> <span>=</span> <span>uuidv47_decode_v4facade</span>(<span>facade</span>, <span>key</span>);

  <span>char</span> <span>a</span>[<span>37</span>], <span>b</span>[<span>37</span>], <span>c</span>[<span>37</span>];
  <span>uuid_format</span>(<span>&amp;</span><span>v7</span>, <span>a</span>);
  <span>uuid_format</span>(<span>&amp;</span><span>facade</span>, <span>b</span>);
  <span>uuid_format</span>(<span>&amp;</span><span>back</span>, <span>c</span>);
  <span>printf</span>(<span>"v7 (DB) : %s\n"</span>, <span>a</span>);
  <span>printf</span>(<span>"v4 (API): %s\n"</span>, <span>b</span>);
  <span>printf</span>(<span>"back    : %s\n"</span>, <span>c</span>);
}</pre></div>
<p dir="auto">Build &amp; run with the provided Makefile:
make test
make coverage
sudo make install</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Public API</h2><a id="user-content-public-api" aria-label="Permalink: Public API" href="#public-api"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="typedef struct { uint8_t  b[16]; } uuid128_t;
typedef struct { uint64_t k0, k1; } uuidv47_key_t;

uuid128_t uuidv47_encode_v4facade(uuid128_t v7, uuidv47_key_t key);
uuid128_t uuidv47_decode_v4facade(uuid128_t v4_facade, uuidv47_key_t key);
int  uuid_version(const uuid128_t* u);
void set_version(uuid128_t* u, int ver);
void set_variant_rfc4122(uuid128_t* u);
bool uuid_parse (const char* str, uuid128_t* out);
void uuid_format(const uuid128_t* u, char out[37]);"><pre><span>typedef</span> <span>struct</span> { <span>uint8_t</span>  <span>b</span>[<span>16</span>]; } <span>uuid128_t</span>;
<span>typedef</span> <span>struct</span> { <span>uint64_t</span> <span>k0</span>, <span>k1</span>; } <span>uuidv47_key_t</span>;

<span>uuid128_t</span> <span>uuidv47_encode_v4facade</span>(<span>uuid128_t</span> <span>v7</span>, <span>uuidv47_key_t</span> <span>key</span>);
<span>uuid128_t</span> <span>uuidv47_decode_v4facade</span>(<span>uuid128_t</span> <span>v4_facade</span>, <span>uuidv47_key_t</span> <span>key</span>);
<span>int</span>  <span>uuid_version</span>(<span>const</span> <span>uuid128_t</span><span>*</span> <span>u</span>);
<span>void</span> <span>set_version</span>(<span>uuid128_t</span><span>*</span> <span>u</span>, <span>int</span> <span>ver</span>);
<span>void</span> <span>set_variant_rfc4122</span>(<span>uuid128_t</span><span>*</span> <span>u</span>);
<span>bool</span> <span>uuid_parse</span> (<span>const</span> <span>char</span><span>*</span> <span>str</span>, <span>uuid128_t</span><span>*</span> <span>out</span>);
<span>void</span> <span>uuid_format</span>(<span>const</span> <span>uuid128_t</span><span>*</span> <span>u</span>, <span>char</span> <span>out</span>[<span>37</span>]);</pre></div>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Specification</h2><a id="user-content-specification" aria-label="Permalink: Specification" href="#specification"></a></p>
<p dir="auto">UUIDv7 bit layout:</p>
<ul dir="auto">
<li>ts_ms_be: 48-bit big-endian timestamp</li>
<li>ver:      high nibble of byte 6 = 0x7 (v7) or 0x4 (façade)</li>
<li>rand_a:   12 random bits</li>
<li>var:      RFC variant (0b10)</li>
<li>rand_b:   62 random bits</li>
</ul>
<p dir="auto">Façade mapping:</p>
<ul dir="auto">
<li>Encode: ts48 ^ mask48(R), set version=4</li>
<li>Decode: encTS ^ mask48(R), set version=7</li>
<li>Random bits unchanged</li>
</ul>
<p dir="auto">SipHash input: 10 bytes from random field:
msg[0] = (byte6 &amp; 0x0F)
msg[1] = byte7
msg[2] = (byte8 &amp; 0x3F)
msg[3..9] = bytes9..15</p>
<p dir="auto">Invertibility: XOR mask is reversible with known key.</p>
<p dir="auto">Collision analysis: Injective mapping. Only risk is duplicate randoms per ms.</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Security model</h2><a id="user-content-security-model" aria-label="Permalink: Security model" href="#security-model"></a></p>
<ul dir="auto">
<li>Goal: Secret key unrecoverable even with chosen inputs.</li>
<li>Achieved: SipHash-2-4 is a keyed PRF.</li>
<li>Keys: 128-bit. Derive via HKDF.</li>
<li>Rotation: store small key ID outside UUID.</li>
</ul>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Build, test, coverage</h2><a id="user-content-build-test-coverage" aria-label="Permalink: Build, test, coverage" href="#build-test-coverage"></a></p>
<div data-snippet-clipboard-copy-content="make test
make coverage
make debug
sudo make install"><pre><code>make test
make coverage
make debug
sudo make install
</code></pre></div>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Integration tips</h2><a id="user-content-integration-tips" aria-label="Permalink: Integration tips" href="#integration-tips"></a></p>
<ul dir="auto">
<li>Do encode/decode at API boundary.</li>
<li>For Postgres, write tiny C extension.</li>
<li>For sharding, hash v4 façade with xxh3 or SipHash.</li>
</ul>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Performance</h2><a id="user-content-performance" aria-label="Permalink: Performance" href="#performance"></a></p>
<p dir="auto">SipHash-2-4 on 10-byte message is extremely fast. No allocations.</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">FAQ</h2><a id="user-content-faq" aria-label="Permalink: FAQ" href="#faq"></a></p>
<p dir="auto">Q: Why not xxHash with a secret?
A: Not a PRF; secret can leak. Use SipHash.</p>
<p dir="auto">Q: Is façade indistinguishable from v4?
A: Yes, variable bits uniform, version/variant set to v4.</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">MIT, Copyright (c) 2025 Stateless Limited</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Firefox 143 for Android to introduce DoH (196 pts)]]></title>
            <link>https://blog.mozilla.org/en/firefox/dns-android/</link>
            <guid>45275444</guid>
            <pubDate>Wed, 17 Sep 2025 13:14:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.mozilla.org/en/firefox/dns-android/">https://blog.mozilla.org/en/firefox/dns-android/</a>, See on <a href="https://news.ycombinator.com/item?id=45275444">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
  <main id="main">

    
<article id="post-81686">
  

  <div>
    
<div><p>All web browsing starts with a DNS query to find the IP address for the desired service or website. For much of the internet’s history, this query is sent in the clear. <a href="https://en.wikipedia.org/wiki/DNS_over_HTTPS">DNS-over-HTTPS (DoH)</a> plugs this privacy leak by encrypting the DNS messages, so no one on the network, not your internet service provider or a free public WiFi provider, can eavesdrop on your browsing.</p><p>In 2020, Firefox became the first browser to roll out <a href="https://blog.mozilla.org/en/firefox/firefox-continues-push-to-bring-dns-over-https-by-default-for-us-users/"><strong>DoH by default</strong></a>, starting in the United States and in 2023, we announced the Firefox <a href="https://blog.mozilla.org/en/mozilla/news/firefox-by-default-dns-over-https-rollout-in-canada/">DoH-by-default rollout in Canada</a>, powered by our trusted partner, the Canadian Internet Registration Authority (CIRA).</p></div>



<p>This year, we’ve built on that foundation and delivered major performance improvements and mobile support, ensuring more Firefox users benefit from privacy without compromise.</p>



<h2><strong>Introducing DoH for Android</strong></h2>



<p>After bringing encrypted DNS protection to millions of desktop users, we’re now extending the same to mobile. Firefox users who have been waiting for DoH on Android can now turn it on and browse with the same privacy protections as on their desktops.</p>



<p>Starting with this week’s release of <strong>Firefox 143 for Android</strong>, users can choose to enable DoH in Firefox on their mobile devices by selecting “<a href="https://support.mozilla.org/en-US/kb/configure-dns-over-https-protection-levels-firefox-android#w_increased-protection">Increased Protection”</a> DoH configuration. Performance testing with <a href="https://wiki.mozilla.org/Security/DOH-resolver-policy#Conforming_Resolvers">Firefox DoH partners</a> is currently underway. If DoH is as fast as we expect, we plan to enable it by default for Android users in certain regions, similar to desktop users. Until then, these configuration options provide you the choice to opt in early.</p>


<div>
<figure><img decoding="async" width="683" height="1024" src="https://blog.mozilla.org/wp-content/blogs.dir/278/files/2025/09/image-683x1024.png" alt="" srcset="https://blog.mozilla.org/wp-content/blogs.dir/278/files/2025/09/image-683x1024.png 683w, https://blog.mozilla.org/wp-content/blogs.dir/278/files/2025/09/image-200x300.png 200w, https://blog.mozilla.org/wp-content/blogs.dir/278/files/2025/09/image-768x1152.png 768w, https://blog.mozilla.org/wp-content/blogs.dir/278/files/2025/09/image-1000x1500.png 1000w, https://blog.mozilla.org/wp-content/blogs.dir/278/files/2025/09/image.png 1024w" sizes="(max-width: 683px) 100vw, 683px"><figcaption><em>Enable DoH in Firefox on Android</em></figcaption></figure></div>


<h2><strong>DoH performance breakthroughs in 2025</strong></h2>



<p>DNS resolution speed is critical to the browsing experience — when web pages involve multiple DNS queries, the speed difference compounds and can cause page loads to be slow. Since we first rolled out DoH in Canada, we’ve worked closely with CIRA for reliability and performance measurements. Through our strong collaboration with them and their technology partner Akamai, Firefox DoH lookups are now <strong>61% faster</strong> year-to-date for the 75th percentile.</p>



<p>With these performance improvements, DoH resolution time is now within a millisecond or two of native DNS resolution. This is a big win because <strong>Firefox users in Canada now get the privacy of encrypted DNS with no performance penalty</strong>.</p>



<p>Although the investigation and analysis started with the desire to improve DoH in Firefox, the benefits didn’t end there. Our collaboration also improved CIRA DoH performance for many of its DNS users, including Canadian universities, as well as other DNS providers relying on CIRA’s or Akamai’s server implementations.</p>



<p>This is a win not just for Firefox users, but for the many other users around the globe.</p>



<h2><strong>Robust privacy on your terms</strong></h2>



<p>We have always approached DoH with an emphasis on transparency, user choice, and strong privacy safeguards. Firefox gives users meaningful control over how their DNS traffic is handled: Users can opt out, choose their own resolver, or adjust DoH protection levels, and Firefox makes it clear what DoH is doing and why it matters.</p>



<p>Firefox enforces strict requirements for DNS resolvers before trusting them with your browsing. Not every DNS provider can become a DoH provider in Firefox — only those that meet and attest to Mozilla’s rigorous <a href="https://wiki.mozilla.org/Security/DOH-resolver-policy">Trusted Recursive Resolver (TRR) policy</a> through a legally binding contract.</p>



<h2><strong>Prioritizing your privacy and speed</strong></h2>



<p>Our work with DoH this year shows what’s possible when privacy and performance go hand-in-hand. We’ve proven that encrypted DNS can be fast, reliable, and available on desktop and Android. Just as importantly, we’ve shown that partnerships grounded in open standards and accountability can deliver benefits not only to Firefox users but to the wider internet.</p>



<p>As we look forward, our commitment stays the same: Privacy should be the default, speed should never be a compromise, and the web should remain open and accessible to everyone. Choosing Firefox means choosing a browser that is built for you and for a better internet.</p>



<a href="https://www.mozilla.org/firefox/new/">
  <p><img width="800" height="800" src="https://blog.mozilla.org/wp-content/blogs.dir/278/files/2021/10/Visual-Guidelines-800x800.png" alt="" decoding="async" srcset="https://blog.mozilla.org/wp-content/blogs.dir/278/files/2021/10/Visual-Guidelines-800x800.png 800w, https://blog.mozilla.org/wp-content/blogs.dir/278/files/2021/10/Visual-Guidelines-150x150.png 150w" sizes="(max-width: 800px) 100vw, 800px">  </p>
  <div>
     <h3>Download Firefox</h3>      <p><span>Get the browser that protects what’s important</span>   </p></div>
</a>
  </div>

</article><!-- #post-81686 -->

  </main><!-- #main -->
  

<div id="related-articles">
    <h2>Related Articles</h2>
    
  </div>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bringing fully autonomous rides to Nashville, in partnership with Lyft (129 pts)]]></title>
            <link>https://waymo.com/blog/2025/09/waymo-is-coming-to-nashville-in-partnership-with-lyft</link>
            <guid>45275415</guid>
            <pubDate>Wed, 17 Sep 2025 13:10:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://waymo.com/blog/2025/09/waymo-is-coming-to-nashville-in-partnership-with-lyft">https://waymo.com/blog/2025/09/waymo-is-coming-to-nashville-in-partnership-with-lyft</a>, See on <a href="https://news.ycombinator.com/item?id=45275415">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article aria-labelledby="P0-7-title"><div><p>We’re on our way to Music City! We’re excited to bring the magic of Waymo’s fully autonomous ride-hailing service to riders in Nashville, in partnership with Lyft.</p><p>Our generalizable Waymo Driver has become even more capable as we’ve scaled to hundreds of thousands of fully autonomous rides each week across five major U.S. cities. We’ll start fully autonomous operations in Nashville in the coming months, and open to public riders next year. We’ll do so by pairing our world-leading technology and seamless ride-hailing service with Lyft’s proven track record of fleet management through its Flexdrive subsidiary.</p><p>We’re also excited to offer riders in Nashville even more ways to ride with Waymo. Riders will hail via the Waymo app, and as our service grows, riders will also be able to use the Lyft app to match with a Waymo vehicle. We’re thrilled for even more people to have access to our ride-hailing service, as we work towards our mission to be the world’s most trusted driver.</p><p>“We’re delighted to partner with Lyft and launch in Nashville next year, as we continue to scale our Waymo ride-hailing service to more people in more places,” said Waymo co-CEO Tekedra Mawakana. “Lyft’s extensive fleet management capabilities through Flexdrive make them an ideal partner for expanding to Nashville. We can’t wait to introduce Music City’s residents and visitors to the convenient, consistent, safe, and magical Waymo experience.”</p><p>"This partnership brings together best-in-class autonomous vehicles with best-in-class customer experience," said Lyft CEO David Risher. "Waymo has proven that its autonomous technology works at scale. When combined with Lyft's customer-obsession and world-class fleet management capabilities, it's two great tastes that go great together."</p><p>With more than 100 million fully autonomous miles driven on public roads, the <a href="https://waymo.com/safety/impact/"><u>data</u></a> shows Waymo’s technology is significantly safer than human drivers in the areas where we operate. Nashville joins a growing list of cities that will soon have access to Waymo.</p><p>“As families and businesses move to Tennessee in record numbers, our state continues to lead the nation in finding innovative solutions to transportation challenges," said Governor Bill Lee.<b> </b>"By leveraging private sector technologies like Waymo's fully autonomous vehicles, we're exploring possibilities we couldn't achieve on our own, and further accelerating economic growth. I look forward to Waymo's launch in The Volunteer State.”</p><p>We’re looking forward to serving the people of Nashville soon. If you’re interested in following our journey or want to help bring Waymo to your city next, sign up at <a href="http://waymo.com/updates"><u>waymo.com/updates</u></a>.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tau² Benchmark: How a Prompt Rewrite Boosted GPT-5-Mini by 22% (162 pts)]]></title>
            <link>https://quesma.com/blog/tau2-benchmark-improving-results-smaller-models/</link>
            <guid>45275354</guid>
            <pubDate>Wed, 17 Sep 2025 13:03:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://quesma.com/blog/tau2-benchmark-improving-results-smaller-models/">https://quesma.com/blog/tau2-benchmark-improving-results-smaller-models/</a>, See on <a href="https://news.ycombinator.com/item?id=45275354">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-astro-cid-xj2uyz6m="">  <p><strong>Now on the front page of Hacker News — <a href="https://news.ycombinator.com/item?id=45275354">join the discussion</a>.</strong></p>
<p>In <a href="https://quesma.com/blog/tau2-from-llm-benchmark-to-blueprint-for-testing-ai-agents/">a recent post</a>, we introduced the Tau² benchmark, a framework for benchmaring LLMs. Today we’re sharing a surprising discovery we made while using it: a simple <strong>prompt rewrite boosted a small model’s success rate by over 20%</strong>. This post is a deep-dive on how we found and fixed this performance bottleneck by making subtle changes to agent policies.</p>
<h2 id="benchmarking-llms-with-tau">Benchmarking LLMs with Tau²</h2>
<p>On the recent OpenAI Summer Update, we have seen that GPT-5 model has made significant strides in agentic tasks. To validate these claims, they’ve turned to the Tau² benchmark, which simulates real-world agent interactions across various domains like telecom, retail, and airlines.</p>
<p>Before moving any further, we have to establish that GPT-5 showed significant improvement only in one benchmark domain - which is Telecom. The other ones have been somehow overlooked during model presentation - therefore we won’t bother about them either (😉).</p>
<p><img alt="Bar chart comparing GPT-5, OpenAI o3, and GPT-4.1 tool use accuracy in telecom, retail, and airline" loading="lazy" decoding="async" fetchpriority="auto" width="1999" height="947" src="https://quesma.com/_astro/image2.3Ev8Kk46_Z2q2E1W.webp"></p>
<p>In agentic interactions, accuracy is non-negotiable, but model speed is equally vital for user experience. Therefore, it makes sense to consider alternatives to flagship models, such as the recently introduced GPT-5-mini.</p>
<p>GPT-5-mini offers significant advantages: it’s roughly twice as fast in latency and noticeably more efficient in throughput. While delivering 85–95% of the full GPT-5’s performance, it is also five times cheaper.</p>
<p>Therefore, we ran an experiment to explore two things:</p>
<ul>
<li>How well GPT-5-mini performs on this benchmark.</li>
<li>Whether we can improve its results by making subtle changes to the domain, such as modifying agent policies or task descriptions.</li>
</ul>
<h2 id="baseline-expect-gpt-5-mini-to-fail-45-of-the-time">Baseline: Expect GPT-5-mini to Fail 45% of the Time</h2>
<p><img alt="GPT-5, GPT-5 mini, and GPT-5 nano model options with descriptions" loading="lazy" decoding="async" fetchpriority="auto" width="1902" height="572" src="https://quesma.com/_astro/image3.CmeGdum5_1a59hL.webp"></p>
<p>Firstly, we’re going to establish the benchmark for the GPT-5-mini model. As the telecom benchmark contains over 100 tests, we’ll use their subset. Luckily, the telecom_small task set comes in handy with just 20 test scenarios.</p>
<p>Running the benchmark with:</p>
<pre tabindex="0" data-language="bash"><code><span><span>tau2</span><span> run</span><span> \</span></span>
<span><span>    --domain</span><span> telecom</span><span> \</span></span>
<span><span>    --agent-llm</span><span> gpt-5-mini</span><span> \</span></span>
<span><span>    --user-llm</span><span> gpt-5-mini</span><span> \</span></span>
<span><span>    --num-trials</span><span> 2</span><span> --task-set-name</span><span> telecom_small</span></span></code></pre>
<p>Our results are:
<img alt="Simulation results showing average reward 0.55 and cost per conversation $0.0292" loading="lazy" decoding="async" fetchpriority="auto" width="1999" height="412" src="https://quesma.com/_astro/image4.B03mMqZq_1aIjlH.webp"></p>
<p>We ended up running 40 simulations:<br>
<img alt="Simulation task results with green checkmarks and red Xs showing successes and failures" loading="lazy" decoding="async" fetchpriority="auto" width="1950" height="1414" src="https://quesma.com/_astro/image5.CyupezbJ_Z1blhYq.webp"></p>
<p>The initial success rate was low: <strong>just 55%</strong>. The GPT-5-mini with its limited reasoning capabilities doesn’t even get close to flagship GPT-5.</p>
<p>There’s an additional interesting metric this benchmark has introduced, which is pass^k. This measures how well an agent can perform when it’s challenged with the same task k times. I like to think of it as the <strong>reliability of the AI Agent</strong>.<br>
Another intriguing aspect of this benchmark are tasks which failed for all given trials - which could imply that the <strong>AI Agent is simply not capable of handling at all</strong>. This can happen due to multiple factors - reasoning might be too difficult, user ask could not be specific enough, etc.</p>
<h2 id="the-hack-using-claude-to-rewrite-prompts-for-gpt-5-mini">The Hack: Using Claude to Rewrite Prompts for GPT-5-mini</h2>
<p>When hacking the AI Agent for <code>GPT-5-mini</code> model we aimed to answer three questions:</p>
<ul>
<li>Can I improve the overall success rate when using this limited model?</li>
<li>Are we able to “unlock” more tasks which agents are capable of handling?</li>
<li>Does agent reliability improve?</li>
</ul>
<p>The beauty of generative AI is that we could offload lots of work and allow ourselves a little laziness. Therefore, we asked Claude to analyze the AI agent policies in the telecom domain, which are the building blocks for the agent prompt. From there, we asked specifically to assume that these policies are going to feed an AI Agent prompt, which will be using faster, non-flagship LLM. Output documents (<code>main_policy.md</code> and <code>tech_support_manual.md</code>) will have to be rephrased so that they are easier to follow for the agent and result in improving its behaviour.</p>
<p>Here’s the “executive summary” of improvements it made:</p>
<pre tabindex="0" data-language="markdown"><code><span><span>I've created two AI-optimized versions of the telecom domain documentation:</span></span>
<span></span>
<span><span>Key Improvements Made:</span></span>
<span></span>
<span><span>1.</span><span> Structure &amp; Flow</span></span>
<span><span>   -</span><span> Decision Trees: Clear branching logic with ├── and └── notation</span></span>
<span><span>   -</span><span> Sequential Steps: Numbered, ordered procedures instead of scattered explanations</span></span>
<span><span>   -</span><span> Prerequisites: Explicit dependency checks before proceeding</span></span>
<span></span>
<span><span>2.</span><span> AI Agent Optimizations</span></span>
<span><span>   -</span><span> Tool Call Clarity: Exact function names and parameters</span></span>
<span><span>   -</span><span> Binary Decisions: Clear yes/no conditions instead of ambiguous language</span></span>
<span><span>   -</span><span> Error Handling: Specific failure conditions and next steps</span></span>
<span><span>   -</span><span> Verification Steps: "Recheck" instructions after each fix</span></span>
<span></span>
<span><span>3.</span><span> Cognitive Load Reduction</span></span>
<span><span>   -</span><span> Reference Tables: Quick lookup for tools and purposes</span></span>
<span><span>   -</span><span> Pattern Recognition: Common issue combinations and their solutions</span></span>
<span><span>   -</span><span> Critical Reminders: Common AI mistakes section to prevent errors</span></span>
<span></span>
<span><span>4.</span><span> Actionable Language</span></span>
<span><span>   -</span><span> Removed verbose explanations mixed with instructions</span></span>
<span><span>   -</span><span> Consolidated multiple documents' logic into single workflows</span></span>
<span><span>   -</span><span> Used imperative commands: "Check X", "If Y then Z"</span></span>
<span><span>   -</span><span> Added immediate verification steps</span></span></code></pre>
<p>We’ve moved from lengthy policy declarations to clear, directive instructions. Instead of wordy statements, the new style uses step-by-step logic, conditions, and patterns. For prompts, this works more like a checklist — “Check X → If Y, then Z” — rather than vague, descriptive policies.</p>
<h2 id="the-result-22-accuracy-boost-and-50-fewer-unsolvable-tasks">The Result: 22% Accuracy Boost and 50% Fewer Unsolvable Tasks</h2>
<p>Let’s review what our improved AI agent results look like:</p>
<p><img alt="Simulation results showing agent metrics with rewards, pass rates, and cost per conversation" loading="lazy" decoding="async" fetchpriority="auto" width="1999" height="435" src="https://quesma.com/_astro/image6.Djj-zTez_RWCcw.webp"></p>
<p>The new prompts led to a significant performance boost.
Pass^k metrics surged:</p>
<ul>
<li>k=1 from 0.55 to 0.675 (<strong>a 22.73% improvement</strong>) → In plain terms, GPT-5-mini now succeeds on <strong>67.5% of tasks instead of 55%</strong>.</li>
<li>k=2 from 0.4 to 0.5 (<strong>a 25% improvement</strong>) → Meaning retries became more effective too.</li>
</ul>
<p>For context, flagship GPT-5 scores ~97% on this benchmark, o3 comes in at 58%, and GPT-4.1 at 34%. With our optimized prompts, GPT-5-mini not only jumped well above its own baseline but also <strong>outperformed o3</strong>, landing much closer to GPT-5 than before.</p>
<p>The side-by-side comparison shows exactly where the gains came from. On the left side of the screen you’ll see the “stock” AI agent results, on the right - our AI agent improved for GPT-5-mini.</p>
<p><img alt="Side-by-side console logs comparing stock AI results with improved GPT-5-mini test runs" loading="lazy" decoding="async" fetchpriority="auto" width="1999" height="722" src="https://quesma.com/_astro/image7.BWRFnxLy_2mRVVB.webp"></p>
<p>The screenshot above outlines that with our updated prompts and policies, <strong>we managed to “unlock” some of the tests which were previously always failing</strong> due to GPT-5-mini’s limited capabilities. Now there are only 3 tasks, which the agent didn’t manage to solve at all within the given 2 trials - compared to 6.</p>
<h2 id="key-takeaways-for-your-own-models">Key Takeaways for Your Own Models</h2>
<p>This experiment shows that thoughtful prompt design can meaningfully boost the performance of smaller models like GPT-5-mini. By restructuring policies into clear, step-by-step instructions, we not only improved success rates but also “unlocked” tasks that previously seemed unsolvable for the model.</p>
<p>The key was in simplifying language, reducing ambiguity, and breaking down reasoning into explicit, actionable steps. Smaller models struggle with long-winded or fuzzy policies, but thrive when given structured flows, binary decisions, and lightweight verification steps.</p>
<p>The takeaway is clear: using a frontier model to automatically optimize prompts can unlock major improvements for smaller LLMs.
With strategic optimization, lightweight models can deliver decent results at a fraction of the cost — making them a compelling alternative when efficiency and affordability matter as much as accuracy.</p>
<p>If you found this helpful, let us know! Prompt engineering is still an open playground, and we’re excited to see what creative approaches others are exploring in this space.</p>
<p>Discuss it on <a href="https://www.linkedin.com/posts/quesma_bigger-ai-models-always-win-benchmarks-right-activity-7373757332989771776-cpc8">LinkedIn</a>, <a href="https://x.com/quesmaorg/status/1968324178215592128">X</a> or <a href="https://news.ycombinator.com/item?id=45275354">Hacker News</a>.</p>
<p><img alt="Two astronauts in space with Earth behind them, one pointing a gun at the other." loading="lazy" decoding="async" fetchpriority="auto" width="996" height="564" src="https://quesma.com/_astro/image8.B08GRvh2_RL2E0.webp"></p>
<p><strong>UPDATE:</strong> Since publishing this post and hitting <a href="https://news.ycombinator.com/item?id=45275354">the front page of HN</a>,
some readers expressed interest in seeing the actual before and after policies (which are building block for the agent prompt).
Initially I thought these would be too lengthy for the article and no one would care,
but since there’s interest, I’m happy to share them in <a href="https://github.com/mieciu/tau2-bench/pull/1/files">this Pull Request</a>.</p>  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Procedural Island Generation (III) (103 pts)]]></title>
            <link>https://brashandplucky.com/2025/09/17/procedural-island-generation-iii.html</link>
            <guid>45275049</guid>
            <pubDate>Wed, 17 Sep 2025 12:29:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://brashandplucky.com/2025/09/17/procedural-island-generation-iii.html">https://brashandplucky.com/2025/09/17/procedural-island-generation-iii.html</a>, See on <a href="https://news.ycombinator.com/item?id=45275049">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <figure>
  <img src="https://brashandplucky.com/uploads/2025/procedural-island-generation-iii/chemaguerra-terrain-elevation-detailed.png" alt="Terrain elevation with noise layers">
  <figcaption>Resulting terrain elevation with multi-scale noise layers and mountain peaks</figcaption>
</figure>

<p>This post continues from <a href="https://brashandplucky.com/2025/09/10/procedural-island-generation-ii.html">Part II</a>, where we established the paint map foundation and mountain ridge system. Now we’ll add detailed noise layers, distance-based mountain peaks, and do blending to create the final terrain elevation.</p>

<h2 id="paint-map-recap">Paint Map (recap)</h2>

<p>Before applying noise layers, we start with the foundation established in Part I - the paint map that defines our base land/water distribution:</p>

<figure>
  <img src="https://brashandplucky.com/uploads/2025/procedural-island-generation-iii/chemaguerra-paint-map-part-i.png" alt="Paint map from Part I">
  <figcaption>The paint map from Part I - our starting elevation values before noise enhancement</figcaption>
</figure>

<p>For visualization throughout this series, we’ll be using the magma palette from matplotlib, which I patched to artificially darken the ocean areas to highlight the coastline:</p>

<figure>
  <img src="https://brashandplucky.com/uploads/2025/procedural-island-generation-iii/chemaguerra-paint-map-triangulated.png" alt="Paint map sampled at triangle centroids">
  <figcaption>Paint map values sampled at the centroids of Delaunay triangles</figcaption>
</figure>

<p>Note that we’ll be sampling the paint map <em>per Delaunay triangle</em> (at each triangle’s centroid):</p>

<figure>
  <img src="https://brashandplucky.com/uploads/2025/procedural-island-generation-iii/chemaguerra-paint-map-triangulated-center.png" alt="Central portion of triangulated paint map">
  <figcaption>Central portion of the above image showing the per-triangle sampling more clearly</figcaption>
</figure>

<p>Remember that the paint map provides the broad strokes: positive values for land, negative for ocean, with smooth transitions between them. Now we’ll enhance it with noise layers to create realistic terrain detail.</p>

<h2 id="multi-scale-noise-layers">Multi-Scale Noise Layers</h2>

<p>We will layer multiple octaves of Simplex noise at different frequencies over the broad strokes provided by the paint map. Each will contribute different detail scales to the final terrain.</p>

<p><a href="https://github.com/redblobgames/mapgen4">mapgen4</a> by <a href="https://x.com/redblobgames">@redblobgames</a> in particular uses six layers:</p>

<figure>
  <img src="https://brashandplucky.com/uploads/2025/procedural-island-generation-iii/chemaguerra-noise-fields-combined.png" alt="Noise field visualization">
  <figcaption>All six noise fields at different frequencies (1x, 2x, 4x, 16x, 32x, 64x) shown in a 3x2 grid</figcaption>
</figure>

<figure>
  <img src="https://brashandplucky.com/uploads/2025/procedural-island-generation-iii/chemaguerra-noise-triangulated.png" alt="Triangulated noise field">
  <figcaption>Top-left corner of noise2 - Note that we are sampling the noises at the (centroids of the) triangles.</figcaption>
</figure>

<table>
  <thead>
    <tr>
      <th>Layer</th>
      <th>Frequency</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>n₀</td>
      <td>1x</td>
      <td>Lowest frequency</td>
    </tr>
    <tr>
      <td>n₁</td>
      <td>2x</td>
      <td>Low frequency</td>
    </tr>
    <tr>
      <td>n₂</td>
      <td>4x</td>
      <td>Medium-low frequency</td>
    </tr>
    <tr>
      <td>n₄</td>
      <td>16x</td>
      <td>Medium-high frequency</td>
    </tr>
    <tr>
      <td>n₅</td>
      <td>32x</td>
      <td>High frequency</td>
    </tr>
    <tr>
      <td>n₆</td>
      <td>64x</td>
      <td>Highest frequency</td>
    </tr>
  </tbody>
</table>

<p>Notice the gap in numbering (n₃ is missing). This would correspond to frequency 8x, which we don’t use.</p>

<h3 id="coastal-noise-enhancement">Coastal Noise Enhancement</h3>

<p>mapgen4 starts with coastal noise enhancement. This provides control over the variation at coastlines while keeping inland elevation unaffected:</p><p>

\[e = \text{Paint map from Part I}\]

\[e_{coast} = e + \alpha \cdot (1 - e^4) \cdot \left(n_4 + \frac{n_5}{2} + \frac{n_6}{4}\right)\]

</p><p>The term \((1 - e^4)\) creates a bell curve that peaks at \(e=0\) (coastline) and decreases rapidly for \(\lvert e \rvert &gt; 0\). This modulates an fBm-like combination of our three highest frequency noise layers.</p>

<p>What matters here isn’t the exact formula or amplitudes, but the core principle: applying high-frequency detail specifically where land meets water.</p><p>

\[e_{tmp} = \begin{cases}
e &amp; \text{if } e_{coast} &gt; 0 \\
e_{coast} &amp; \text{if } e_{coast} \leq 0
\end{cases}\]

</p><figure>
  <video controls="" loop="" muted="" playsinline="">
    <source src="https://brashandplucky.com/uploads/2025/procedural-island-generation-iii/chemaguerra-noisy-coastlines-variation.mp4" type="video/mp4">
    Your browser does not support the video tag.
  </video>
  <figcaption>e<sub>tmp</sub> with varying α - Showing its effect on coastline and seabed complexity</figcaption>
</figure>

<h2 id="mountain-distance-field">Mountain Distance Field</h2>

<p>Mountains need special pre-processing. If you remember from <a href="https://brashandplucky.com/2025/09/07/procedural-island-generation-i.html">Part I</a> in the swarm of seed points we tagged some as mountain peaks. Here we will pre-compute a <em>distance field</em> from every regular seed point to the closest mountain peak point.</p>

<p>We compute distance through the mesh topology of the Delaunay triangulation using BFS (breadth-first search). <em>i.e.,</em> we don’t use Euclidean distance. This creates more organic mountain shapes that follow the terrain’s natural connectivity.</p>

<p>The algorithm spreads outward from mountain peaks:</p>
<ol>
  <li>Start at triangles containing mountain seed points (distance = 0)</li>
  <li>Visit neighboring triangles, incrementing distance by a randomized amount</li>
  <li>The randomization creates natural ridge patterns instead of perfect cones</li>
</ol>

<p>Here’s the magic formula used for distance increment in each step:</p><p>

\[\Delta = s \cdot (1 + j \cdot r)\]

</p><p>Where:</p>
<ul>
  <li>\(s\) = spacing between triangles (uses configured Poisson disk separation)</li>
  <li>\(j\) = jaggedness parameter (0 = true topological distance, 1 = very irregular)</li>
  <li>\(r \in [-1,1]\) = random factor using triangular distribution</li>
</ul>

<p>The triangular distribution <code>rand() - rand()</code> clusters values near zero while allowing occasional larger variations. This looks more natural than uniform randomness.</p>

<p>I implemented <a href="https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle">Fisher-Yates shuffling</a> when visiting neighbor triangles. Instead of processing neighbors in a fixed order (which would create directional bias), the order is randomly shuffled each time. This ensures mountain ridges branch out organically in all directions rather than following predictable patterns.</p>

<p>After computing distances this way, we normalize them (by the max dist, for example):</p>

<figure>
  <video controls="" loop="" muted="" playsinline="">
    <source src="https://brashandplucky.com/uploads/2025/procedural-island-generation-iii/chemaguerra-mountain-distance-field-jaggedness.mp4" type="video/mp4">
    Your browser does not support the video tag.
  </video>
  <figcaption>d<sub>m</sub> - Normalized mountain distance field with varying jaggedness parameter</figcaption>
</figure>

<h2 id="elevation-blending">Elevation Blending</h2>

<p>The final elevation combines all components through weighted blending:</p><p>

\[e_{final} = \begin{cases}
\text{lerp}(e_{coast}^2, e_{hill}, e_{mountain}) &amp; \text{if } e_{coast} &gt; 0 \\
e_{coast} \cdot (\rho + n_1) &amp; \text{if } e_{coast} \leq 0
\end{cases}\]

</p><p>Where:</p>

<ul>
  <li>\(e_{hill} = h \cdot (1 + \text{lerp}(\frac{1 + n_0}{2}, n_4, n_2))\) =&gt; hill elevation with noise-modulated height</li>
  <li>\(e_{mountain} = 1 - \frac{\mu}{2^\sigma} \cdot d_m\) =&gt; mountain elevation from distance field</li>
</ul>

<p>The quadratic blend weight produces smooth transitions from hills near the coast through mixed terrain at mid-elevations to pure mountains at peaks.</p>

<p>With (editable) parameters:</p>

<ul>
  <li>\(\alpha\): Coastal noise strength (0.01)</li>
  <li>\(h\): Hill height scale (0.02)</li>
  <li>\(\rho\): Ocean depth multiplier (1.5)</li>
  <li>\(\mu\): Mountain slope (17.6)</li>
  <li>\(\sigma\): Mountain sharpness (9.8)</li>
</ul>

<h3 id="interactive-parameter-exploration">Interactive Parameter Exploration</h3>

<figure>
  <video controls="" loop="" muted="" playsinline="">
    <source src="https://brashandplucky.com/uploads/2025/procedural-island-generation-iii/chemaguerra-mountain-sharpness-variation.mp4" type="video/mp4">
    Your browser does not support the video tag.
  </video>
  <figcaption>Terrain elevation with varying mountain sharpness σ</figcaption>
</figure>

<figure>
  <video controls="" loop="" muted="" playsinline="">
    <source src="https://brashandplucky.com/uploads/2025/procedural-island-generation-iii/chemaguerra-mountain-jaggedness-elevation.mp4" type="video/mp4">
    Your browser does not support the video tag.
  </video>
  <figcaption>Terrain elevation with varying mountain jaggedness j</figcaption>
</figure>

<figure>
  <video controls="" loop="" muted="" playsinline="">
    <source src="https://brashandplucky.com/uploads/2025/procedural-island-generation-iii/chemaguerra-hill-height-variation.mp4" type="video/mp4">
    Your browser does not support the video tag.
  </video>
  <figcaption>Terrain elevation with varying hill height scale h</figcaption>
</figure>

<figure>
  <video controls="" loop="" muted="" playsinline="">
    <source src="https://brashandplucky.com/uploads/2025/procedural-island-generation-iii/chemaguerra-ocean-depth-variation.mp4" type="video/mp4">
    Your browser does not support the video tag.
  </video>
  <figcaption>Terrain elevation with varying ocean depth multiplier ρ</figcaption>
</figure>

<h2 id="region-vs-triangle-elevation">Region (vs. Triangle) Elevation</h2>

<p>So far we’ve computed elevation for triangles. But our Voronoi regions (from Part I) also need elevations for certain stages in the rest of the series.</p>

<figure>
  <img src="https://brashandplucky.com/uploads/2025/procedural-island-generation-iii/chemaguerra-triangle-region-full-animation.gif" alt="Triangle vs Region elevation animation">
  <figcaption>Animation comparing triangle elevation vs region elevation (1 second each)</figcaption>
</figure>

<p>Each seed point defines a Voronoi region and serves as a vertex in multiple Delaunay triangles. To assign elevation to a Voronoi region, we average the elevations of all triangles that share its seed point as a vertex.</p>

<figure>
  <img src="https://brashandplucky.com/uploads/2025/procedural-island-generation-iii/chemaguerra-triangle-region-animation.gif" alt="Triangle vs Region elevation animation">
  <figcaption>Central detail animating between triangle elevation and region elevation (1 second each)</figcaption>
</figure>

<h2 id="next-steps">Next Steps</h2>

<p>With elevation complete, our island has shape but lacks the defining features carved by water. Part IV will simulate the hydrological cycle: rainfall patterns influenced by topography, rivers flowing from peaks to ocean, and valleys carved by erosion.</p>

<h2 id="valuable-resources">Valuable Resources</h2>

<ul>
  <li><a href="https://www.redblobgames.com/maps/terrain-from-noise/">Terrain from Noise</a> - Amit Patel’s Red Blob Games guide to layering noise for terrain</li>
  <li><a href="https://www.redblobgames.com/x/1843-planet-generation/">Polygonal Map Generation</a> - Red Blob Games on Voronoi-based terrain (mapgen4 inspiration)</li>
  <li><a href="https://www.redblobgames.com/x/1723-procedural-river-growing/">Distance Fields for Terrain</a> - Red Blob Games on using distance fields in terrain generation</li>
</ul>

  </div>
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple Photos App Corrupts Images (950 pts)]]></title>
            <link>https://tenderlovemaking.com/2025/09/17/apple-photos-app-corrupts-images/</link>
            <guid>45274277</guid>
            <pubDate>Wed, 17 Sep 2025 11:07:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tenderlovemaking.com/2025/09/17/apple-photos-app-corrupts-images/">https://tenderlovemaking.com/2025/09/17/apple-photos-app-corrupts-images/</a>, See on <a href="https://news.ycombinator.com/item?id=45274277">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p>The Apple Photos app sometimes corrupts images when importing from my camera.
I just wanted to make a blog post about it in case anyone else runs into the problem.
I’ve seen other references to this online, but most of the people gave up trying to fix it, and none of them went as far as I did to debug the issue.</p>
<p>I’ll try to describe the problem, and the things I’ve tried to do to fix it.
But also note that I’ve (sort of) given up on the Photos app too.
Since I can’t trust it to import photos from my camera, I switched to a different workflow.</p>
<p>Here is a screenshot of a corrupted image in the Photos app:</p>
<p><img src="https://tenderlovemaking.com/images/corrupt-image.png" alt="screenshot of a corrupt image"></p>
<h2 id="how-i-used-to-import-images">How I used to import images</h2>
<p>I’ve got an <a href="https://en.wikipedia.org/wiki/OM_System_OM-1">OM System OM-1</a> camera.
I used to shoot in RAW + jpg, then when I would import to Photos app, I would check the “delete photos after import” checkbox in order to empty the SD card.
Turns out “delete after import” was a huge mistake.</p>
<h2 id="getting-corrupted-images">Getting corrupted images</h2>
<p>I’m pretty sure I’d been getting corrupted images for a while, but it would only be 1 or 2 images out of thousands, so I thought nothing of it (it was probably my fault anyway, right?)</p>
<p>But the problem really got me upset when last year I went to a family member’s wedding and took tons of photos.
Apple Photos combines RAW + jpg photos so you don’t have a bunch of duplicates, and when you view the images in the photos app, it just shows you the jpg version by default.
After I imported all of the wedding photos I noticed some of them were corrupted.
Upon closer inspection, I found that it sometimes had corrupted the jpg, sometimes corrupted the RAW file, and sometimes both.
Since I had been checking the “delete after import” box, I didn’t know if the images on the SD card were corrupted <em>before</em> importing or not.
After all, the files had been deleted so there was no way to check.</p>
<p>I estimate I completely lost about 30% of the images I took that day.</p>
<p>Losing so many photos really rattled me, but I wanted to figure out the problem so I didn’t lose images in the future.</p>
<h2 id="narrowing-down-the-problem">Narrowing down the problem</h2>
<p>I was worried this was somehow a hardware problem.
Copying files seems so basic, I didn’t think there was any way a massively deployed app like Photos could fuck it up (especially since its main job is managing photo files).
So, to narrow down the issue I changed out all of the hardware.
Here are all the things I did:</p>
<ul>
<li>Switched USB-C cables</li>
<li>Bought a new SD card direct from the manufacturer (to eliminate the possibility of buying a bootleg SD card)</li>
<li>Switched to only shooting in RAW (if importing messes up 30% of my images, but I cut the number of images I import by half, then that should be fewer corrupted images right? lol)</li>
<li>Bought a new laptop</li>
<li>Bought a new camera: the OM System OM-1 MKii</li>
</ul>
<p>I did each of these steps over time, as to only change one variable at a time, and still the image corruption persisted.
I didn’t really want to buy a new camera, the MKii is not really a big improvement over the OM-1, but we had a family trip coming up and the idea that pressing the shutter button on the camera might not actually record the image didn’t sit well with me.</p>
<h2 id="finally-a-smoking-gun">Finally a smoking gun</h2>
<p>Since I had replaced literally all of the hardware involved, I knew it must be a software problem.
I stopped checking the “delete after import” button, and started reviewing all of the photos after import.
After verifying none of them were corrupt, then I would format the SD card.
I did this for months without finding any corrupt files.
At this point I figured it was somehow a race condition or something when copying the photo files and deleting them at the same time.</p>
<p>However, after I got home from RailsConf and imported my photos, I found one corrupt image (the one above).
I was able to verify that the image was <em>not</em> corrupt on the SD card, so the camera was working fine (meaning I probably didn’t need to buy a new camera body at all).</p>
<p>I tried deleting the corrupt file and re-importing the original to see if it was something about that particular image, but it re-imported just fine.
In other words, it seems like the Photos app will corrupt files randomly.</p>
<p>I don’t know if this is a problem that is specific to OM System cameras, and I’m not particularly interested in investing in a new camera system just to find out.</p>
<p>If I compare the corrupted image with the non-corrupted image, the file sizes are exactly the same, but the bytes are different:</p>
<p>Checksums:</p>
<pre tabindex="0"><code>aaron@tc ~/Downloads&gt; md5sum P7110136-from-camera.ORF Exports/P7110136.ORF 
17ce895fd809a43bad1fe8832c811848  P7110136-from-camera.ORF
828a33005f6b71aea16d9c2f2991a997  Exports/P7110136.ORF
</code></pre><p>File sizes:</p>
<pre tabindex="0"><code>aaron@tc ~/Downloads&gt; ls -al P7110136-from-camera.ORF Exports/P7110136.ORF
-rw-------@ 1 aaron  staff  18673943 Jul 12 04:38 Exports/P7110136.ORF
-rwx------  1 aaron  staff  18673943 Jul 17 09:29 P7110136-from-camera.ORF*
</code></pre><p>The <code>P7110136-from-camera.ORF</code> is the non-corrupted file, and <code>Exports/P7110136.ORF</code> is the corrupted file from Photos app.
Here’s a screenshot of the preview of the non-corrupted photo:</p>
<p><img src="https://tenderlovemaking.com/images/non-corrupt.png" alt="screenshot of non-corrupt image"></p>
<p><a href="https://gist.github.com/tenderlove/25853f50ab46a58738ff2cc22d682f2b">Here is the binary diff between the files</a>.
I ran both files through <code>xxd</code> then diffed them.</p>
<h2 id="my-new-workflow">My new workflow</h2>
<p>I’m not going to put any more effort into debugging this problem, but I wanted to blog about it in case anyone else is seeing the issue.
I take a lot of photos, and to be frank, most of them are not very good.
I don’t want to look through a bunch of bad photos every time I look at my library, so culling photos is important.
Culling photos in the Photos app is way too cumbersome, so I’ve switched to using <a href="https://www.darktable.org/">Darktable</a>.</p>
<p>My current process is:</p>
<ul>
<li>Import images to Darktable</li>
<li>Delete the ones I don’t like</li>
<li>Process ones I do like</li>
<li>Export both the jpg and the original raw file</li>
<li>Import those to the Photos app so they’re easy to view and share</li>
<li>Periodically format my SD card</li>
</ul>
<p>I’ve not seen any file corruption when importing to Darktable, so I am convinced this is a problem with the Photos app.
But now, since all of my images land in Darktable before making their way to the Photos app, I don’t really care anymore.
The bad news is that I’ve spent a lot of time and money trying to debug this.
I guess the good news is that now I have redundant hardware!</p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Determination of the fifth Busy Beaver value (237 pts)]]></title>
            <link>https://arxiv.org/abs/2509.12337</link>
            <guid>45273999</guid>
            <pubDate>Wed, 17 Sep 2025 10:26:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2509.12337">https://arxiv.org/abs/2509.12337</a>, See on <a href="https://news.ycombinator.com/item?id=45273999">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
    <div><p><span>Authors:</span>The <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=bbchallenge+Collaboration" rel="nofollow">bbchallenge Collaboration</a>: <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Blanchard,+J" rel="nofollow">Justin Blanchard</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Briggs,+D" rel="nofollow">Daniel Briggs</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Deka,+K" rel="nofollow">Konrad Deka</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fenner,+N" rel="nofollow">Nathan Fenner</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Forster,+Y" rel="nofollow">Yannick Forster</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Georgiev,+G" rel="nofollow">Georgi Georgiev</a> (Skelet), <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=House,+M+L" rel="nofollow">Matthew L. House</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hunter,+R" rel="nofollow">Rachel Hunter</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Iijil" rel="nofollow">Iijil</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=K%C4%85dzio%C5%82ka,+M" rel="nofollow">Maja Kądziołka</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kropitz,+P" rel="nofollow">Pavel Kropitz</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ligocki,+S" rel="nofollow">Shawn Ligocki</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=mxdys" rel="nofollow">mxdys</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Na%C5%9Bciszewski,+M" rel="nofollow">Mateusz Naściszewski</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=savask" rel="nofollow">savask</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=St%C3%A9rin,+T" rel="nofollow">Tristan Stérin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+C" rel="nofollow">Chris Xu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yuen,+J" rel="nofollow">Jason Yuen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zimmermann,+T" rel="nofollow">Théo Zimmermann</a></p></div>            
    <p><a href="https://arxiv.org/pdf/2509.12337">View PDF</a>
    <a href="https://arxiv.org/html/2509.12337v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>We prove that $S(5) = 47,176,870$ using the Coq proof assistant. The Busy Beaver value $S(n)$ is the maximum number of steps that an $n$-state 2-symbol Turing machine can perform from the all-zero tape before halting, and $S$ was historically introduced by Tibor Radó in 1962 as one of the simplest examples of an uncomputable function. The proof enumerates $181,385,789$ Turing machines with 5 states and, for each machine, decides whether it halts or not. Our result marks the first determination of a new Busy Beaver value in over 40 years and the first Busy Beaver value ever to be formally verified, attesting to the effectiveness of massively collaborative online research (bbchallenge$.$org).
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Tristan Stérin [<a href="https://arxiv.org/show-email/f02cef48/2509.12337" rel="nofollow">view email</a>]      <br>    <strong>[v1]</strong>
        Mon, 15 Sep 2025 18:05:08 UTC (546 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[PureVPN IPv6 Leak (161 pts)]]></title>
            <link>https://anagogistis.com/posts/purevpn-ipv6-leak/</link>
            <guid>45273897</guid>
            <pubDate>Wed, 17 Sep 2025 10:10:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://anagogistis.com/posts/purevpn-ipv6-leak/">https://anagogistis.com/posts/purevpn-ipv6-leak/</a>, See on <a href="https://news.ycombinator.com/item?id=45273897">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
            
            
            <div>
                <p>In late August 2025, I submitted two security reports to PureVPN under their VDP. Three weeks later, I’ve received no response, so I decided to publish the findings to inform other users.</p>
<p>The issues affect both their GUI (v2.10.0) and CLI (v2.0.1) clients on Linux (tested on Ubuntu 24.04.3 LTS, kernel 6.8.0, iptables-nft backend). Here’s what I found.</p>
<h2 id="1-ipv6-leaks-off-tunnel">1. IPv6 Leaks Off-Tunnel</h2>
<p>After toggling Wi-Fi or resuming from suspend, the PureVPN client fails to restore IPv6 protections:</p>
<ul>
<li>
<p><strong>CLI (IKS enabled)</strong>: The client auto-reconnects and reports status as “connected”, yet the system regains a default IPv6 route via Router Advertisements (<code>fe80::1</code>). Since <code>ip6tables</code> <code>OUTPUT</code> remains <code>ACCEPT</code> (default), egress resumes off-tunnel.
</p><p>
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/HDqoD2SaCKA?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" title="YouTube video"></iframe>
    </p>

</li>
<li>
<p><strong>GUI (IKS enabled)</strong>: When the GUI detects a disconnection, it blocks IPv4 and displays the “VPN session disconnected” dialog. However, IPv6 remains functional until the user explicitly clicks <code>Reconnect</code>.
</p><p>
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/jjCsTt4y2JM?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" title="YouTube video"></iframe>
    </p>

</li>
</ul>
<p>Real-world effect: I was able to browse IPv6-preferred sites and send/receive email (Thunderbird) with my ISP’s IPv6 address while the client UI claimed I was protected.</p>
<h2 id="2-host-firewall-reset-and-not-restored">2. Host Firewall Reset and Not Restored</h2>
<p>At connect time, PureVPN wipes the user’s <code>iptables</code> configuration:</p>
<ul>
<li><code>INPUT</code> is set to <code>ACCEPT</code></li>
<li>All <code>-A</code> rules are flushed (UFW, Docker jumps, user rules, etc.)</li>
<li>After disconnect, these changes are not reverted</li>
</ul>
<p>Result: the system remains more exposed after using the VPN than before. This defeats the point of using UFW or a local deny policy and contradicts user expectations.</p>
<p>Example:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span><span># Baseline protections</span>
</span></span><span><span>$ sudo iptables -P INPUT DROP
</span></span><span><span>$ sudo iptables -I INPUT -p icmp -j DROP
</span></span><span><span>
</span></span><span><span><span># Connect to VPN</span>
</span></span><span><span>$ purevpn-cli -c US
</span></span><span><span>$ sudo iptables -S <span>|</span> head -3
</span></span><span><span>-P INPUT ACCEPT
</span></span><span><span>-P FORWARD DROP
</span></span><span><span>-P OUTPUT ACCEPT
</span></span><span><span>$ sudo iptables -S <span>|</span> grep icmp
</span></span><span><span><span># (no output — rule was wiped)</span>
</span></span><span><span>
</span></span><span><span><span># Disconnect</span>
</span></span><span><span>$ purevpn-cli -d
</span></span><span><span>$ sudo iptables -S <span>|</span> head -3
</span></span><span><span>-P INPUT ACCEPT
</span></span><span><span>-P FORWARD DROP
</span></span><span><span>-P OUTPUT ACCEPT
</span></span><span><span><span># All wiped. INPUT = ACCEPT</span>
</span></span></code></pre></div><h2 id="tldr">TL;DR</h2>
<p>PureVPN:</p>
<ul>
<li>Does not properly implement an IPv6 kill-switch</li>
<li>Leaves IPv6 egress open after reconnects or IKS events</li>
<li>Wipes your firewall state (<code>iptables</code>) and does not restore it</li>
<li>Applies broad <code>ACCEPT</code> policies to make things work</li>
</ul>
<p>Both issues have real-world impact. Privacy claims are undermined when your real IPv6 leaks and your firewall state is lost.</p>
<p>I submitted full technical reports and screencasts to <a href="mailto:security@purevpn.com">security@purevpn.com</a>. No acknowledgment to date.</p>
<p>Use with caution.</p>

            </div>
        </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[EU Chat Control: Germany's position has been reverted to UNDECIDED (378 pts)]]></title>
            <link>https://mastodon.social/@chatcontrol/115215006562371435</link>
            <guid>45273854</guid>
            <pubDate>Wed, 17 Sep 2025 10:02:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mastodon.social/@chatcontrol/115215006562371435">https://mastodon.social/@chatcontrol/115215006562371435</a>, See on <a href="https://news.ycombinator.com/item?id=45273854">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Oh no, not again a meditation on NPM supply chain attacks (164 pts)]]></title>
            <link>https://tane.dev/2025/09/oh-no-not-again...-a-meditation-on-npm-supply-chain-attacks/</link>
            <guid>45273824</guid>
            <pubDate>Wed, 17 Sep 2025 09:57:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tane.dev/2025/09/oh-no-not-again...-a-meditation-on-npm-supply-chain-attacks/">https://tane.dev/2025/09/oh-no-not-again...-a-meditation-on-npm-supply-chain-attacks/</a>, See on <a href="https://news.ycombinator.com/item?id=45273824">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I’ve been sitting on this article for a while now – well over a year I’ve put off publishing it – but as we’ve seen this week, the time has come to lift the veil and say the quiet part out loud:</p><blockquote><p><strong>It’s 2025; Microsoft should be considered a “bad actor” and a threat to all companies who develop software.</strong></p></blockquote><p>Of course, if you’re old enough to remember – this is not the first time either…</p><h3 id="time-is-a-flat-circle">Time is a flat circle</h3><p>Here we are again – in 2025, <strong>Microsoft</strong> have fucked up so bad, they have likely created an even larger risk than they did in the 2000’s with their browser by simply doing absolutly nothing.</p><p>I had started initially writing this post around the time of the <a href="https://en.wikipedia.org/wiki/XZ_Utils_backdoor" target="_blank">xz incident</a>
– a sophisticated and long-term attempt to gain control of a library used in many package managers of most Linux distributions.</p><p>Since then, many more incidents have happened, and to be specific <strong>NPM</strong> has become the largest and <em>easiest</em> way to ship malware. At first, most of it was aimed at stealing cryptocurrency (because techbros seem to be obsessed with magic electic money and are easy prey). But now, these supply chain attacks are starting to target more critical things like tokens and access keys of the package maintainers, as seen with the <a href="https://snyk.io/blog/weaponizing-ai-coding-agents-for-malware-in-the-nx-malicious-package/" target="_blank">NX</a>
incident and now <a href="https://www.aikido.dev/blog/npm-debug-and-chalk-packages-compromised" target="_blank">several depedencies that are used daily by thousands of developers</a>
.</p><p>Again… this is <a href="https://en.wikipedia.org/wiki/Npm_left-pad_incident" target="_blank">nothing new</a>
in the land of NPM.</p><p>But it didn’t have to be this way…</p><h3 id="weve-come-along-way-but-have-travelled-nowhere">We’ve come along way, but have travelled nowhere</h3><p>I have a long history with NodeJS – around 2010 I started working on a startup, and this was <a href="https://www.slideshare.net/slideshow/techmeetup-edinburgh-nodejs-talk/5742191" target="_blank">before npm was even a thing</a>
.</p><p><img src="https://tane.dev/images/npm-before-it-was-a-thing.png" alt="A sceenshot of a slide with an announcement of npm as a package manage for node"></p><p>Back in the misty days of the 1990s most JavaScript security issues were not much of a backend concern: this was mostly the domain of Perl, PHP, Python, and Java.</p><p><em>The web however was a much different story.</em></p><p>In the very early days of the World Wide Web there was really only one main browser everyone used: Netscape Navigator. Released in 1994 it was not <em>just a browser</em>: throughout its life it had various incarnations of a built-in email client, calendar, HTML editor with FTP browser, and with plugins could play media files like Realplayer and MP3 (which I remember at its launch) and Flash movies and games. It’s where JavaScript was born.</p><p>Many of the early websites of the day were static – popular tools to build websites included <a href="https://en.wikipedia.org/wiki/HotDog" target="_blank">HotDog</a>
or <a href="https://en.wikipedia.org/wiki/Windows_Notepad" target="_blank">Notepad</a>
. No fancy IDEs or frameworks, just a text editor, a browser, and <code>alert()</code> to debug.</p><blockquote><p>Microsoft had also entered the game with Internet Explorer – included in an early Windows DLC called “Plus! For Windows 95”. It eventually became the software that Microsoft bet its whole company strategy around (much like today with AI).</p></blockquote><p>Internet Explorer was embedded into every aspect of Windows – first in 1995 with Active Desktop, which continued all the way to Windows XP. With it you could embed a frame item on your Desktop, but also a Rich Text document or Excel spreadsheet. It was also bloated and buggy – and with that it presented two problems: a massive security risk and exposure to accusations of monopolising the browser market.</p><blockquote><p>The law came after Microsoft hard and in 2001 it won – Microsoft was told to break up its monopoly. One aspect was that it had to offer other browsers on its operating system (a similar story happening now to Apple) – but it also wasn’t forced to <strong>remove</strong> Internet Explorer.</p></blockquote><p>Microsoft essentially <em>abandoned</em> IE; as the years rolled on they continued to push out new major verions to capture the market, but without fixing the major flaws. It still shipped as default with the OS, unable to be removed without breaking other parts of the system.</p><p>Each release of Internet Explorer added something new to the browser landscape, but it also continued to add bugs and flaws on top of the ones that no one touched – by default, on all Windows systems lived code that could <a href="https://learn.microsoft.com/en-us/security-updates/SecurityBulletins/2014/ms14-012?redirectedfrom=MSDN" target="_blank">give hijackers access to users machines</a>
.</p><blockquote><p>It wasn’t until 2015 they finally abandoned the existing Internet Explorer codebase and shifted to a new engine before eventually settling on their <del>Chome</del>Blink-based engine. However the ghost of IE <a href="https://www.forbes.com/sites/zakdoffman/2024/07/11/microsoft-warning-21-days-to-update-or-stop-using-windows/" target="_blank">still haunts us today</a>
.</p></blockquote><h3 id="the-ticking-time-bomb-of-postinstall">The ticking time-bomb of postinstall</h3><p>8 years ago, I wrote a small <a href="https://github.com/tanepiper/steal-ur-stuff" target="_blank">proof of concept</a>
. It was in response to <a href="https://github.com/npm/npm/issues/17724" target="_blank">this issue</a>
about <code>npx</code> – a small tool that had just been added to <code>npm</code> by default whether you liked it or not.</p><p>With <code>npx</code> you could now run the following arbitary command (PLEASE DO NOT RUN THIS SCRIPT):</p><pre tabindex="0"><code>npx https://gist.github.com/tanepiper/6cb9067adca626cd2c0edbc3786dad7b
</code></pre><p>This would now pull the gist as a node module and run it. In the proof-of-concept I put this command as a <code>postinstall</code> script. If you look at the gist, it’s a small binary script that posts your <code>.bash_history</code> to example.com – which at the time <code>npx</code> would just run.</p><p>My frustration at the time was aimed mostly towards <code>npx</code> itself – it seemed like the NPM team were adding a new easy-to-use attack vector by shipping a tool that could run <em>any module from any source on the web, on your machine without user interaction</em>. But little did I know at the time there was a deeper problem lurking with <code>postinstall</code>.</p><blockquote><p>At the time I also created a <a href="https://github.com/tanepiper/npm-lint" target="_blank">package.json linter</a>
that would warn of potential issues. But of course it required projects to opt in, it needed trust, and I didn’t see a way forward for it.</p></blockquote><p>This was, of course, <em>before</em> Microsoft, via GitHub, owned NPM.</p><h3 id="a-short-bit-of-history">A short bit of history</h3><p>So how did NPM become the main package manager for Node? Back then, it solved a problem – it was as simple as that – and people noticed it and adopted it. Over time, more useful little libraries showed up and from that, the rest is history.</p><p>NPM, built on CouchDB which enabled fast replication, allowed a flourishing and open JavaScript ecosystem. In the beginning, it was a bit of a wild west, where people tended to cut corners or miss steps. There was also a lot of early abandonment of libraries, and communities started to form around some of the larger ones to at least establish them as de facto tools – Express.js for example has been around since before <code>npm</code> (and for all the complaints about performance aimed at it: it’s highly battle tested and the worst bugs have likely been squashed).</p><p>Node and npm’s future was not a guaranteed thing. At some point, there was fragmentation of the ecosystem – tools such as <code>yarn</code> and <code>pnpm</code> exist because <code>npm</code> couldn’t or wouldn’t fix something, but they introduced their own changes that only made them partially compatible with each other. In 2014, for a short while we even had a fork of NodeJS called <code>io-js</code> because of fundamental disagreements.</p><p>There was also the small problem that all of this infrastructure and services cost money to run.</p><blockquote><p>To paraphrase <a href="https://blog.ceejbot.com/" target="_blank"><em>C J Silverio</em></a>
– “There’s no money in package managers.”</p></blockquote><p>In 2018 Microsoft bought GitHub (and until this year ran it as a side-concern with its own CEO and management team – just last month, the CEO stepped down and now GitHub is part of the “AI” team). In 2020, GitHub bought NPM – with pockets deep enough to run the infrastructure. This means that Microsoft owns the world’s largest repository of JavaScript code, the distribution channel for its packages – and the development ecosystem with VSCode.</p><p>This likely saved <code>npm</code> in the long run by them simply having the resources to do so.</p><p>On the other hand, they have done little to make it a more secure tool, especially for enterprise customers. To their credit, GitHub has provided <a href="https://www.infoq.com/news/2024/05/github-dependabot-supply-chain/" target="_blank">new tools for Software Bill of Materials Attestastion</a>
, which is a step in the right direction. But right now there are still no signed dependencies and nothing stopping people using AI agents, or just plain old scripts, from creating thousands of junk or namesquatting repositories.</p><p>… and as we’ve learned 2-Factor Authentication isn’t enough secure npm.</p><hr><h3 id="i-want-to-get-back-to-the-fun-of-building-software">I want to get back to the fun of building software</h3><p>Ultimately, I don’t think we can trust the software ecosystem provided by Microsoft anymore. It’s too fragile, brittle in the wrong places, and too open to abuse, and for most of my career I have seen the causes and effects first hand. This has made software development less fun, and more of a chore.</p><p>The tools we use to build software are not secure by default, and almost all of the time, the companies that provide them are not held to account for the security of their products.</p><p>Without a concerted effort across the industry to make the software supply chain secure by default, we will continue to see a rise in incidents – and the risks to data privacy and security will only increase. Criminal and state actors are always looking to exploit the vulnerabilities in our software; the use of AI to create more sophisticated attacks will only improve. These don’t have to be technical either – deep fakes are close enough to be used as effective social engineering tools - and it’s very easy to fake emails that seem very legitimte.</p><p>Unfortunately, Microsoft seem to be actively hostile - in their lack of attempts to shut down an active security hole that’s almost a decade old, they have left their customers are the higest levels of risk seen in computing.</p><blockquote><p>For many companies, now is the right time to start looking at the tools they use to build software, and to start asking the hard questions about the security of their software supply chain – is it putting their customers, workers, or own profits at risk?</p></blockquote></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Alibaba's New AI Chip Unveiled: Key Specifications Comparable to H20 (231 pts)]]></title>
            <link>https://news.futunn.com/en/post/62202518/alibaba-s-new-ai-chip-unveiled-key-specifications-comparable-to</link>
            <guid>45273747</guid>
            <pubDate>Wed, 17 Sep 2025 09:45:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.futunn.com/en/post/62202518/alibaba-s-new-ai-chip-unveiled-key-specifications-comparable-to">https://news.futunn.com/en/post/62202518/alibaba-s-new-ai-chip-unveiled-key-specifications-comparable-to</a>, See on <a href="https://news.ycombinator.com/item?id=45273747">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-v-50ed2316=""><div data-v-50ed2316=""><p>Operations too frequent. <br data-v-50ed2316="">Try again later</p></div> <p>Page not found, please try again later.</p> <p><a href="https://www.futunn.com/" data-v-50ed2316="">Take me home</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stategraph: Terraform state as a distributed systems problem (124 pts)]]></title>
            <link>https://stategraph.dev/blog/why-stategraph/</link>
            <guid>45273352</guid>
            <pubDate>Wed, 17 Sep 2025 08:38:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stategraph.dev/blog/why-stategraph/">https://stategraph.dev/blog/why-stategraph/</a>, See on <a href="https://news.ycombinator.com/item?id=45273352">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
        

        <h2>Why We're Building Stategraph: Terraform State as a Distributed Systems Problem</h2>

        <section aria-labelledby="tldr-h">
            <strong id="tldr-h">TL;DR</strong>
            <div role="region" aria-label="Terminal output: TL;DR">
                    <p><span>$</span> cat why-stategraph.tldr</p>
                    <p>• Terraform state shows distributed coordination issues but uses file primitives.</p>
                    <p>• File blob (100% read/lock) vs. change cone (~3%).</p>
                    <p>• Stategraph → graph state, ACID transactions, subgraph isolation.</p>
                </div>
        </section>

        <p>The Terraform ecosystem has spent a decade working around a fundamental architectural mismatch: we're using filesystem semantics to solve a distributed systems problem. The result is predictable and painful.</p>

        <p>When we started building infrastructure automation at scale, we discovered that Terraform's state management exhibits all the classic symptoms of impedance mismatch between data representation and access patterns. Teams implement increasingly elaborate workarounds: state file splitting, wrapper orchestration, external locking mechanisms. These aren't solutions; they're evidence that we're solving the wrong problem.</p>

        <p>Stategraph addresses this by treating state for what it actually is: a directed acyclic graph of resources with partial update semantics, not a monolithic document.</p>

        <div>
            <h2>The Pathology of File-Based State</h2>

            <p>Terraform state, at its core, is a coordination problem. Multiple actors (engineers, CI systems, drift detection) need to read and modify overlapping subsets of infrastructure state concurrently. This is a well-studied problem in distributed systems, with established solutions around fine-grained locking, multi-version concurrency control, and transaction isolation.</p>

            <p>Instead, Terraform implements the simplest possible solution: a global mutex on a JSON file.</p>

            <div>
                <h4>Observation</h4>
                <p>The probability of lock contention in a shared state file increases super-linearly with both team size and resource count. At 100 resources and 5 engineers, you're coordinating 500 potential interaction points through a single mutex.</p>
            </div>

            <p>Consider the actual data access patterns in a typical Terraform operation:</p>

            <div role="img" aria-label="Comparison of file-based state with global lock versus graph state with precise operations">
                <div role="group" aria-labelledby="current-model">
                    <h4 id="current-model">Current Model</h4>
                    <p>
                        tfstate.json (2.3MB)
                    </p>
                    <p>
                        Read: 100%<br>Lock: 100%<br>Modify: 0.5%
                    </p>
                </div>
                <div role="group" aria-labelledby="actual-need">
                    <h4 id="actual-need">Actual Requirement</h4>
                    <div>
                        <p>VPC</p>
                        <p>Subnet</p>
                        <p>RDS</p>
                        <p>ALB</p>
                        <p>ASG</p>
                        <p>SG</p>
                    </div>
                    <p>
                        Read: 3%<br>Lock: 3%<br>Modify: 3%
                    </p>
                </div>
            </div>

            <p>This mismatch between granularity of operation and granularity of locking is the root cause of every Terraform scaling problem. It violates the fundamental principle of isolation in concurrent systems: non-overlapping operations should not block each other.</p>

            <p>The standard response, splitting state files, doesn't solve the problem. It redistributes it. Now you have N coordination problems instead of one, plus the additional complexity of managing cross-state dependencies. You've traded false contention for distributed transaction coordination, which is arguably worse.</p>
        </div>

        <div>
            <h2>State as a Graph: The Natural Representation</h2>

            <p>Infrastructure state is inherently a directed graph. Resources have dependencies, which form edges. Changes propagate along these edges. Terraform already knows this: the internal representation is a graph, and the planner performs graph traversal. But at the storage layer, we flatten this rich structure into a blob.</p>

            <p>This is akin to storing a B-tree in a CSV file. You can do it, but you're destroying the very properties that make the data structure useful.</p>

            <div role="region" aria-label="Stategraph query demo">
                    <p><span>stategraph&gt;</span> -- Find resource subgraph for planned change</p>
                    <p>WITH RECURSIVE affected AS (</p>
                    <p>    SELECT id, type, name FROM resources</p>
                    <p>    WHERE name = 'prod-api-cluster'</p>
                    <p>    UNION</p>
                    <p>    SELECT r.id, r.type, r.name FROM resources r</p>
                    <p>    JOIN dependencies d ON r.id = d.dependent_id</p>
                    <p>    JOIN affected a ON d.resource_id = a.id</p>
                    <p>) SELECT * FROM affected;</p>
                    <p>→ 12 resources in change scope <span>(0.003s)</span></p>
                    <p>→ Compared to: 2,847 resources in full state <span>(1.2s)</span></p>
                </div>

            <p>When state is properly normalized into a graph database, several properties emerge naturally:</p>

            <p><strong>Subgraph isolation:</strong> Operations on disjoint subgraphs are inherently parallelizable. If Team A is modifying RDS instances and Team B is updating CloudFront distributions, there's no shared state to coordinate.</p>

            <p><strong>Precise locking:</strong> We can implement row-level locking on resources and edge-level locking on dependencies. Lock acquisition follows the dependency graph, preventing deadlocks through consistent ordering.</p>

            <p><strong>Incremental refresh:</strong> Given a change set, we can compute the minimal refresh set by traversing the dependency graph. Most changes affect a small cone of resources, not the entire state space.</p>
        </div>

        <div>
            <h2>Concurrency Control Through Proper Abstractions</h2>

            <p>The distributed systems community solved these problems decades ago. Multi-version concurrency control (MVCC) allows readers to proceed without blocking writers. Write-ahead logging provides durability without sacrificing performance. Transaction isolation levels let operators choose their consistency guarantees.</p>

            <p>Stategraph implements these patterns at the Terraform state layer:</p>

            <div>
                <h4>Traditional: Global Lock</h4>
                
                <p><span>$ terraform apply</span> Acquiring <strong>global lock</strong>… <span>waiting</span></p>
                <h4>Stategraph: Subgraph Isolation</h4>
                
                <p><span>$ stategraph apply</span> Locking subgraph <strong>(3 resources)</strong>… <span>ready</span></p>
            </div>

            <p>Each operation acquires locks only on its subgraph. The lock manager uses the dependency graph to ensure consistent ordering, preventing deadlocks. Readers use MVCC to access consistent snapshots without blocking writers.</p>

            <div>
                <h4>Implementation Detail</h4>
                <p>Lock acquisition follows a strict partial order derived from the resource dependency graph. Resources are locked in topological order, with ties broken by resource ID. This guarantees deadlock freedom without requiring global coordination.</p>
            </div>

            <p>The result is dramatic improvement in concurrent throughput:</p>

            <div>
                <div>
                    <h5>Transaction A</h5>
                    <p>Lock: RDS:prod-db</p>
                    <p>Lock: SG:prod-db-sg</p>
                    <p>Apply changes</p>
                </div>
                <div>
                    <h5>Transaction B</h5>
                    <p>Lock: CF:cdn-dist</p>
                    <p>Lock: S3:static-assets</p>
                    <p>Apply changes</p>
                </div>
                <div>
                    <h5>Transaction C</h5>
                    <p>Lock: ASG:workers</p>
                    <p>Lock: LC:worker-config</p>
                    <p>Apply changes</p>
                </div>
            </div>

            <p>Three teams, three transactions, zero contention. This isn't possible with file-based state, regardless of how you split it.</p>
        </div>

        <div>
            <h2>The Refresh Problem</h2>

            <p>Terraform refresh is O(n) in the number of resources, regardless of change scope. Change one security group rule and you still walk the entire state. That's an algorithmic bottleneck, not just an implementation detail.</p>

            <div role="img" aria-label="Comparison of refresh scope: file-based vs graph-based">
                <div role="group" aria-labelledby="refresh-before">
                    <h4 id="refresh-before">File-Based State</h4>
                    
                    <p>
                        Changing 1 resource<br>
                        <strong>Refreshing all 30</strong>
                    </p>
                </div>
                <p>
                    →
                </p>
                <div role="group" aria-labelledby="refresh-after">
                    <h4 id="refresh-after">Graph State</h4>
                    
                    <p>
                        Changing 1 resource<br>
                        <strong>Refreshing only 3</strong>
                    </p>
                </div>
            </div>

            <p>With a graph representation, refresh work can be scoped to the affected subgraph instead of the entire state. Most changes touch only a small fraction of resources, not everything.</p>
        </div>

        <div>
            <h2>Why We Built This</h2>

            <p>At Terrateam, we've watched hundreds of teams struggle with the same fundamental problems. They start with a single state file, hit scaling limits, split their state, discover coordination complexity, build orchestration layers, and eventually resign themselves to living with the pain.</p>

            <p>This is a solvable problem. The computer science is well-understood. The implementation is straightforward once you acknowledge that state management is a distributed systems problem, not a file storage problem.</p>

            <p>Stategraph isn't revolutionary. It's the application of established distributed systems principles to a problem that's been mischaracterized since its inception. We're not inventing new algorithms; we're applying the right ones.</p>

            <div>
                <h4>Design Principle</h4>
                <p>The storage layer should match the access patterns. Terraform state exhibits graph traversal patterns, partial update patterns, and concurrent access patterns. The storage layer should be a graph database with ACID transactions and fine-grained locking. Anything else is impedance mismatch.</p>
            </div>

            <p>The infrastructure industry has accepted file-based state as an immutable constraint for too long. It's not. It's a choice, and it's the wrong one for systems at scale.</p>
        </div>

        <div>
            <h2>Technical Implementation</h2>

            <p>Stategraph is implemented as a PostgreSQL schema with a backend that speaks the Terraform/OpenTofu remote backend protocol. We chose PostgreSQL for its robust MVCC, proven scalability, and operational familiarity. The schema normalizes state into three primary relations:</p>

            <p><strong>resources:</strong> one row per resource, with type, provider, and attribute columns.<br>
            <strong>dependencies:</strong> edge table representing the resource dependency graph.<br>
            <strong>transactions:</strong> append-only log of all state mutations with full attribution.</p>

            <p>The backend extends Terraform's protocol with graph-aware operations. Lock acquisition and state queries operate directly on the database representation of the graph, enabling precision and concurrency that file-based backends can't provide.</p>

            <p>This isn't a wrapper or an orchestrator. It's a replacement for the storage layer that preserves Terraform's execution model while fixing its coordination problems.</p>
        </div>

        <div>
            <h2>Adoption Path</h2>

            <p>Stategraph reads existing tfstate files and constructs the graph representation automatically. No changes to Terraform configurations are required. The backend protocol is unchanged. From Terraform's perspective, Stategraph is just another backend, like S3 or GCS.</p>

            <p>But from an operational perspective, everything changes. Lock contention disappears. Refresh times drop by orders of magnitude. Teams stop blocking each other. State becomes queryable, auditable, and comprehensible.</p>

            <p>We're not asking teams to rewrite their infrastructure. We're asking them to store it properly.</p>

            <blockquote>
                The question isn't whether Terraform state should be a graph. It already is. The question is whether we'll continue pretending it's a file.
            </blockquote>
        </div>

        <div>
            <h3>Technical Preview</h3>
            <p>Stategraph is in active development. We're working with design partners to validate the approach at scale.</p>
            <p><a href="https://stategraph.dev/#signup">Get Updates</a></p>
        </div>
    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[In Praise of Idleness (1932) (131 pts)]]></title>
            <link>https://harpers.org/archive/1932/10/in-praise-of-idleness/</link>
            <guid>45272296</guid>
            <pubDate>Wed, 17 Sep 2025 06:04:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://harpers.org/archive/1932/10/in-praise-of-idleness/">https://harpers.org/archive/1932/10/in-praise-of-idleness/</a>, See on <a href="https://news.ycombinator.com/item?id=45272296">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="non-paywall">
                    <p>Like most of my generation, I was brought up on the saying “Satan finds some mischief still for idle hands to do.” Being a highly virtuous child, I believed all that I was told and acquired a conscience which has kept me working hard down to the present moment. But although my conscience has controlled my <em>actions,</em> my <em>opinions</em> have undergone a revolution. I think that there is far too much work done in the world, that immense harm is caused by the belief that work is virtuous, and that what needs to be preached in modern industrial countries is quite different from what always has been preached. Every one knows the story of the traveler in Naples who saw twelve beggars lying in the sun (it was before the days of Mussolini), and offered a lira to the laziest of them. Eleven of them jumped up to claim it, so he gave it to the twelfth. This traveler was on the right lines. But in countries which do not enjoy Mediterranean sunshine idleness is more difficult, and a great public propaganda will be required to inaugurate it. I hope that after reading the following pages the leaders of the Y.&nbsp;M.&nbsp;C.&nbsp;A. will start a campaign to induce good young men to do nothing. If so, I shall not have lived in vain.</p>
<p>Before advancing my own arguments for laziness, I must dispose of one which I cannot accept. Whenever a person who already has enough to live on proposes to engage in some everyday kind of job, such as school-teaching or typing, he or she is told that such conduct takes the bread out of other people’s mouths, and is, therefore, wicked. If this argument were valid, it would only be necessary for us all to be idle in order that we should all have our mouths full of bread. What people who say such things forget is that what a man earns he usually spends, and in spending he gives employment. As long as a man spends his income he puts just as much bread into people’s mouths in spending as he takes out of other people’s mouths in earning. The real villain, from this point of view, is the man who saves. If he merely puts his savings in a stocking, like the proverbial French peasant, it is obvious that they do not give employment. If he invests his savings the matter is less obvious, and different cases arise.</p>
<p>One of the commonest things to do with savings is to lend them to some government. In view of the fact that the bulk of the expenditure of most civilized governments consists in payments for past wars and preparation for future wars, the man who lends his money to a government is in the same position as the bad men in Shakespeare who hire murderers. The net result of the man’s economical habits is to increase the armed forces of the State to which he lends his savings. Obviously it would be better if he spent the money, even if he spent it on drink or gambling.</p>
<p>But, I shall be told, the case is quite different when savings are invested in industrial enterprises. When such enterprises succeed and produce something useful this may be conceded. In these days, however, no one will deny that most enterprises fail. That means that a large amount of human labor, which might have been devoted to producing something which could be enjoyed, was expended on producing machines which, when produced, lay idle and did no good to anyone. The man who invests his savings in a concern that goes bankrupt is, therefore, injuring others as well as himself. If he spent his money, say, in giving parties for his friends, they (we may hope) would get pleasure, and so would all those on whom he spent money, such as the butcher, the baker, and the bootlegger. But if he spends it (let us say) upon laying down rails for surface cars in some place where surface cars turn out to be not wanted, he has diverted a mass of labor into channels where it gives pleasure to no one. Nevertheless, when he becomes poor through the failure of his investment he will be regarded as a victim of undeserved misfortune, whereas the gay spendthrift, who has spent his money philanthropically, will be despised as a fool and a frivolous person.</p>
<p>All this is only preliminary. I want to say, in all seriousness, that a great deal of harm is being done in the modern world by the belief in the virtuousness of <em>work,</em> and that the road to happiness and prosperity lies in an organized diminution of work.</p>
<p>First of all: what is work? Work is of two kinds: first, altering the position of matter at or near the earth’s surface relatively to other such matter; second, telling other people to do so. The first kind is unpleasant and ill paid; the second is pleasant and highly paid. The second kind is capable of indefinite extension: there are not only those who give orders but those who give advice as to what orders should be given. Usually two opposite kinds of advice are given simultaneously by two different bodies of men; this is called politics. The skill required for this kind of work is not knowledge of the subjects as to which advice is given, but knowledge of the art of persuasive speaking and writing, <em>i.e.</em> of advertising.</p>
<p>Throughout Europe, though not in America, there is a third class of men, more respected than either of the classes of workers. These are men who, through ownership of land, are able to make others pay for the privilege of being allowed to exist and to work. These landowners are idle, and I might, therefore, be expected to praise them. Unfortunately, their idleness is rendered possible only by the industry of others; indeed their desire for comfortable idleness is historically the source of the whole gospel of work. The last thing they have ever wished is that others should follow their example.</p>
<p>From the beginning of civilization until the industrial revolution a man could, as a rule, produce by hard work little more than was required for the subsistence of himself and his family, although his wife worked at least as hard and his children added their labor as soon as they were old enough to do so. The small surplus above bare necessaries was not left to those who produced it, but was appropriated by priests and warriors. In times of famine there was no surplus; the warriors and priests, however, still secured as much as at other times, with the result that many of the workers died of hunger. This system persisted in Russia until 1917, and still persists in the East; in England, in spite of the Industrial Revolution, it remained in full force throughout the Napoleonic wars, and until a hundred years ago, when the new class of manufacturers acquired power. In America the system came to an end with the Revolution, except in the South, where it persisted until the Civil War. A system which lasted so long and ended so recently has naturally left a profound impression upon men’s thoughts and opinions. Much that we take for granted about the desirability of work is derived from this system and, being pre-industrial, is not adapted to the modern world. Modern technic has made it possible for leisure, within limits, to be not the prerogative of small privileged classes, but a right evenly distributed throughout the community. The morality of work is the morality of slaves, and the modern world has no need of slavery.</p>
<p>It is obvious that, in primitive communities, peasants, left to themselves, would not have parted with the slender surplus upon which the warriors and priests subsisted, but would have either produced less or consumed more. At first sheer force compelled them to produce and part with the surplus. Gradually, however, it was found possible to induce many of them to accept an ethic according to which it was their duty to work hard, although part of their work went to support others in idleness. By this means the amount of compulsion required was lessened, and the expenses were diminished. To this day ninety-nine per cent of British wage-earners would be genuinely shocked if it were proposed that the King should not have a larger income than a working man. The conception of duty, speaking historically, has been a means used by the holders of power to induce others to live for the interests of their masters rather than their own. Of course the holders of power conceal this fact from themselves by managing to believe that their interests are identical with the larger interests of humanity. Sometimes this is true; Athenian slave-owners, for instance, employed part of their leisure in making a permanent contribution to civilization which would have been impossible under a just economic system. Leisure is essential to civilization, and in former times leisure for the few was rendered possible only by the labors of the many. But their labors were valuable, not because work is good, but because leisure is good. And with modern technic it would be possible to distribute leisure justly without injury to civilization.</p>
<p>Modern technic has made it possible to diminish enormously the amount of labor necessary to produce the necessaries of life for every one. This was made obvious during the War. At that time all the men in the armed forces, all the men and women engaged in the production of munitions, all the men and women engaged in spying, war propaganda, or government offices connected with the War were withdrawn from productive occupations. In spite of this, the general level of physical well-being among wage-earners on the side of the Allies was higher than before or since. The significance of this fact was concealed by finance; borrowing made it appear as if the future was nourishing the present. But that, of course, would have been impossible; a man cannot eat a loaf of bread that does not yet exist. The War showed conclusively that by the scientific organization of production it is possible to keep modern populations in fair comfort on a small part of the working capacity of the modern world. If at the end of the War the scientific organization which had been created in order to liberate men for fighting and munition work had been preserved, and the hours of work had been cut down to four, all would have been well. Instead of that, the old chaos was restored, those whose work was demanded were made to work long hours, and the rest were left to starve as unemployed. Why? Because work is a duty, and a man should not receive wages in proportion to what he has produced, but in proportion to his virtue as exemplified by his industry.</p>
<p>This is the morality of the Slave State, applied in circumstances totally unlike those in which it arose. No wonder the result has been disastrous. Let us take an illustration. Suppose that at a given moment a certain number of people are engaged in the manufacture of pins. They make as many pins as the world needs, working (say) eight hours a day. Someone makes an invention by which the same number of men can make twice as many pins as before. But the world does not need twice as many pins: pins are already so cheap that hardly any more will be bought at a lower price. In a sensible world everybody concerned in the manufacture of pins would take to working four hours instead of eight, and everything else would go on as before. But in the actual world this would be thought demoralizing. The men still work eight hours, there are too many pins, some employers go bankrupt, and half the men previously concerned in making pins are thrown out of work. There is, in the end, just as much leisure as on the other plan, but half the men are totally idle while half are still overworked. In this way it is insured that the unavoidable leisure shall cause misery all round instead of being a universal source of happiness. Can anything more insane be imagined?</p>
<p>The idea that the poor should have leisure has always been shocking to the rich. In England in the early nineteenth century fifteen hours was the ordinary day’s work for a man; children sometimes did as much, and very commonly did twelve hours a day. When meddlesome busy-bodies suggested that perhaps these hours were rather long, they were told that work kept adults from drink and children from mischief. When I was a child, shortly after urban working men had acquired the vote, certain public holidays were established by law, to the great indignation of the upper classes. I remember hearing an old Duchess say, “What do the poor want with holidays? they ought to <em>work.</em>” People nowadays are less frank, but the sentiment persists, and is the source of much economic confusion.</p>

<p>II</p>
<p>Let us, for a moment, consider the ethics of work frankly, without superstition. Every human being, of necessity, consumes in the course of his life a certain amount of produce of human labor. Assuming, as we may, that labor is on the whole disagreeable, it is unjust that a man should consume more than he produces. Of course he may provide services rather than commodities, like a medical man, for example; but he should provide something in return for his board and lodging. To this extent, the duty of work must be admitted, but to this extent only.</p>
<p>I shall not develop the fact that in all modern societies outside the U.&nbsp;S.&nbsp;S.&nbsp;R. many people escape even this minimum of work, namely all those who inherit money and all those who marry money. I do not think the fact that these people are allowed to be idle is nearly so harmful as the fact that wage-earners are expected to overwork or starve. If the ordinary wage-earner worked four hours a day there would be enough for everybody, and no unemployment&nbsp;— assuming a certain very moderate amount of sensible organization. This idea shocks the well-to-do, because they are convinced that the poor would not know how to use so much leisure. In America men often work long hours even when they are already well-off; such men, naturally, are indignant at the idea of leisure for wage-earners except as the grim punishment of unemployment, in fact, they dislike leisure even for their sons. Oddly enough, while they wish their sons to work so hard as to have no time to be civilized, they do not mind their wives and daughters having no work at all. The snobbish admiration of uselessness, which, in an aristocratic society, extends to both sexes, is under a plutocracy confined to women; this, however, does not make it any more in agreement with common sense.</p>
<p>The wise use of leisure, it must be conceded, is a product of civilization and education. A man who has worked long hours all his life will be bored if he becomes suddenly idle. But without a considerable amount of leisure a man is cut off from many of the best things. There is no longer any reason why the bulk of the population should suffer this deprivation; only a foolish asceticism, usually vicarious, makes us insist on work in excessive quantities now that the need no longer exists.</p>
<p>In the new creed which controls the government of Russia, while there is much that is very different from the traditional teaching of the West, there are some things that are quite unchanged. The attitude of the governing classes, and especially of those who control educational propaganda, on the subject of the dignity of labor is almost exactly that which the governing classes of the world have always preached to what were called the “honest poor.” Industry, sobriety, willingness to work long hours for distant advantages, even submissiveness to authority, all these reappear; moreover, authority still represents the will of the Ruler of the Universe, Who, however, is now called by a new name, Dialectical Materialism.</p>
<p>The victory of the proletariat in Russia has some points in common with the victory of the feminists in some other countries. For ages men had conceded the superior saintliness of women and had consoled women for their inferiority by maintaining that saintliness is more desirable than power. At last the feminists decided that they would have both, since the pioneers among them believed all that the men had told them about the desirability of virtue but not what they had told them about the worthlessness of political power. A similar thing has happened in Russia as regards manual work. For ages the rich and their sycophants have written in praise of “honest toil,” have praised the simple life, have professed a religion which teaches that the poor are much more likely to go to heaven than the rich, and in general have tried to make manual workers believe that there is some special nobility about altering the position of matter in space, just as men tried to make women believe that they derived some special nobility from their sexual enslavement. In Russia all this teaching about the excellence of manual work has been taken seriously, with the result that the manual worker is more honored than anyone else. What are, in essence, revivalist appeals are made to secure shock workers for special tasks. Manual work is the ideal which is held before the young, and is the basis of all ethical teaching.</p>
<p>For the present this is all to the good. A large country, full of natural resources, awaits development and has to be developed with very little use of credit. In these circumstances hard work is necessary and is likely to bring a great reward. But what will happen when the point has been reached where everybody could be comfortable without working long hours?</p>
<p>In the West we have various ways of dealing with this problem. We have no attempt at economic justice, so that a large proportion of the total produce goes to a small minority of the population, many of whom do no work at all. Owing to the absence of any central control over production, we produce hosts of things that are not wanted. We keep a large percentage of the working population idle because we can dispense with their labor by making others overwork. When all these methods prove inadequate we have a war: we cause a number of people to manufacture high explosives, and a number of others to explode them, as if we were children who had just discovered fireworks. By a combination of all these devices we manage, though with difficulty, to keep alive the notion that a great deal of manual work must be the lot of the average man.</p>
<p>In Russia, owing to economic justice and central control over production, the problem will have to be differently solved. The rational solution would be as soon as the necessaries and elementary comforts can be provided for all to reduce the hours of labor gradually, allowing a popular vote to decide, at each stage, whether more leisure or more goods were to be preferred. But, having taught the supreme virtue of hard work, it is difficult to see how the authorities can aim at a paradise in which there will be much leisure and little work. It seems more likely that they will find continually fresh schemes by which present leisure is to be sacrificed to future productivity. I read recently of an ingenious scheme put forward by Russian engineers for making the White Sea and the northern coasts of Siberia warm by putting a dam across the Kara Straits. An admirable plan, but liable to postpone proletarian comfort for a generation, while the nobility of toil is being displayed amid the ice-fields and snowstorms of the Arctic Ocean. This sort of thing, if it happens, will be the result of regarding the virtue of hard work as an end in itself, rather than as a means to a state of affairs in which it is no longer needed.</p>

<p>III</p>
<p>The fact is that moving matter about, while a certain amount of it is necessary to our existence, is emphatically not one of the ends of human life. If it were, we should have to consider every navvy superior to Shakespeare. We have been misled in this matter by two causes. One is the necessity of keeping the poor contented, which has led the rich for thousands of years to preach the dignity of labor, while taking care themselves to remain undignified in this respect. The other is the new pleasure in mechanism, which makes us delight in the astonishingly clever changes that we can produce on the earth’s surface. Neither of these motives makes any great appeal to the actual worker. If you ask him what he thinks the best part of his life, he is not likely to say, “I enjoy manual work because it makes me feel that I am fulfilling man’s noblest task, and because I like to think how much man can transform his planet. It is true that my body demands periods of rest, which I have to fill in as best I may, but I am never so happy as when the morning comes and I can return to the toil from which my contentment springs.” I have never heard working men say this sort of thing. They consider work, as it should be considered, as a necessary means to a livelihood, and it is from their leisure hours that they derive whatever happiness they may enjoy.</p>
<p>It will be said that while a little leisure is pleasant, men would not know how to fill their days if they had only four hours’ work out of the twenty-four. In so far as this is true in the modern world it is a condemnation of our civilization; it would not have been true at any earlier period. There was formerly a capacity for light-heartedness and play which has been to some extent inhibited by the cult of efficiency. The modern man thinks that everything ought to be done for the sake of something else, and never for its own sake. Serious-minded persons, for example, are continually condemning the habit of going to the cinema, and telling us that it leads the young into crime. But all the work that goes to producing a cinema is respectable, because it is work, and because it brings a money profit. The notion that the desirable activities are those that bring a profit has made everything topsy-turvy. The butcher who provides you with meat and the baker who provides you with bread are praiseworthy because they are making money but when you enjoy the food they have provided you are merely frivolous, unless you eat only to get strength for your work. Broadly speaking, it is held that getting money is good and spending money is bad. Seeing that they are two sides of one transaction, this is absurd; one might as well maintain that keys are good but keyholes are bad. The individual, in our society, works for profit; but the social purpose of his work lies in the consumption of what he produces. It is this divorce between the individual and the social purpose of production that makes it so difficult for men to think clearly in a world in which profitmaking is the incentive to industry. We think too much of production and too little of consumption. One result is that we attach too little importance to enjoyment and simple happiness, and that we do not judge production by the pleasure that it gives to the consumer.</p>
<p>When I suggest that working hours should be reduced to four, I am not meaning to imply that all the remaining time should necessarily be spent in pure frivolity. I mean that four hours’ work a day should entitle a man to the necessities and elementary comforts of life, and that the rest of his time should be his to use as he might see fit. It is an essential part of any such social system that education should be carried farther than it usually is at present, and should aim, in part, at providing tastes which would enable a man to use leisure intelligently. I am not thinking mainly of the sort of things that would be considered “high-brow.” Peasant dances have died out except in remote rural areas, but the impulses which caused them to be cultivated must still exist in human nature. The pleasures of urban populations have become mainly passive: seeing cinemas, watching football matches, listening to the radio, and so on. This results from the fact that their active energies are fully taken up with work; if they had more leisure they would again enjoy pleasures in which they took an active part.</p>
<p>In the past there was a small leisure class and a large working class. The leisure class enjoyed advantages for which there was no basis in social justice; this necessarily made it oppressive, limited its sympathies, and caused it to invent theories by which to justify its privileges. These facts greatly diminished its excellence, but in spite of this drawback it contributed nearly the whole of what we call civilization. It cultivated the arts and discovered the sciences; it wrote the books, invented the philosophies, and refined social relations. Even the liberation of the oppressed has usually been inaugurated from above. Without the leisure class mankind would never have emerged from barbarism.</p>
<p>The method of a hereditary leisure class without duties was, however, extraordinarily wasteful. None of the members of the class had been taught to be industrious, and the class as a whole was not exceptionally intelligent. It might produce one Darwin, but against him had to be set tens of thousands of country gentlemen who never thought of anything more intelligent than fox-hunting and punishing poachers. At present, the universities are supposed to provide, in a more systematic way, what the leisure class provided accidentally and as a byproduct. This is a great improvement, but it has certain drawbacks. University life is so different from life in the world at large that men who live in an academic milieu tend to be unaware of the pre-occupations of ordinary men and women; moreover, their ways of expressing themselves are usually such as to rob their opinions of the influence that they ought to have upon the general public. Another disadvantage is that in universities studies are organized, and the man who thinks of some original line of research is likely to be discouraged. Academic institutions, therefore, useful as they are, are not adequate guardians of the interests of civilization in a world where every one outside their walls is too busy for unutilitarian pursuits.</p>
<p>In a world where no one is compelled to work more than four hours a day every person possessed of scientific curiosity will be able to indulge it, and every painter will be able to paint without starving, however excellent his pictures may be. Young writers will not be obliged to draw attention to themselves by sensational pot-boilers, with a view to acquiring the economic independence needed for monumental works, for which, when the time at last comes, they will have lost the taste and the capacity. Men who in their professional work have become interested in some phase of economics or government will be able to develop their ideas without the academic detachment that makes the work of university economists lacking in reality. Medical men will have time to learn about the progress of medicine. Teachers will not be exasperatedly struggling to teach by routine things which they learned in their youth, which may, in the interval, have been proved to be untrue.</p>
<p>Above all, there will be happiness and joy of life, instead of frayed nerves, weariness, and dyspepsia. The work exacted will be enough to make leisure delightful, but not enough to produce exhaustion. Since men will not be tired in their spare time, they will not demand only such amusements as are passive and vapid. At least one per cent will probably devote the time not spent in professional work to pursuits of some public importance, and, since they will not depend upon these pursuits for their livelihood, their originality will be unhampered, and there will be no need to conform to the standards set by elderly pundits. But it is not only in these exceptional cases that the advantages of leisure will appear. Ordinary men and women, having the opportunity of a happy life, will become more kindly and less persecuting and less inclined to view others with suspicion. The taste for war will die out, partly for this reason, and partly because it will involve long and severe work for all. Good nature is, of all moral qualities, the one that the world needs most, and good nature is the result of ease and security, not of a life of arduous struggle. Modern methods of production have given us the possibility of ease and security for all; we have chosen instead to have overwork for some and starvation for others. Hitherto we have continued to be as energetic as we were before there were machines. In this we have been foolish, but there is no reason to go on being foolish for ever.</p>

                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Notion API importer, with Databases to Bases conversion bounty (170 pts)]]></title>
            <link>https://github.com/obsidianmd/obsidian-importer/issues/421</link>
            <guid>45271942</guid>
            <pubDate>Wed, 17 Sep 2025 05:11:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/obsidianmd/obsidian-importer/issues/421">https://github.com/obsidianmd/obsidian-importer/issues/421</a>, See on <a href="https://news.ycombinator.com/item?id=45271942">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="issue-body-viewer" data-team-hovercards-enabled="true" data-turbolinks="false" id="issue-body-viewer"><p dir="auto">Currently Importer supports Notion imports via the HTML export <a data-error-text="Failed to load title" data-id="1805719836" data-permission-text="Title is private" data-url="https://github.com/obsidianmd/obsidian-importer/issues/14" data-hovercard-type="issue" data-hovercard-url="/obsidianmd/obsidian-importer/issues/14/hovercard" href="https://github.com/obsidianmd/obsidian-importer/issues/14">#14</a>. This new importer would use the Notion API and download files progressively. This would also add support for <a href="https://www.notion.com/help/intro-to-databases" rel="nofollow">Databases</a> to <a href="https://help.obsidian.md/bases" rel="nofollow">Bases</a> conversion. See <a data-error-text="Failed to load title" data-id="3341049068" data-permission-text="Title is private" data-url="https://github.com/obsidianmd/obsidian-importer/issues/415" data-hovercard-type="issue" data-hovercard-url="/obsidianmd/obsidian-importer/issues/415/hovercard" href="https://github.com/obsidianmd/obsidian-importer/issues/415">#415</a></p>
<h2 dir="auto">Bounty</h2>
<p dir="auto">See the <a href="https://github.com/obsidianmd/obsidian-importer/blob/master/CONTRIBUTING.md">Contribution guidelines</a> for how to claim this bounty.</p>
<p dir="auto"><strong>Bounty:</strong> $5,000 USD<br>
<strong>Timeframe:</strong> 30 days</p>
<p dir="auto"><g-emoji alias="warning">⚠️</g-emoji> Please only apply if you have taken time to explore the Importer codebase, as well as the Notion API. Provide detail about how you would approach solving it within the constraints of this Obsidian plugin.</p>
<h2 dir="auto">Requirements</h2>
<ul>
<li> Uses Notion API (integration token) incorporating changes from new <a href="https://developers.notion.com/reference/data-source" rel="nofollow">data source object</a> introduced 2025-09.</li>
<li> Properly converts files to <a href="https://help.obsidian.md/Editing+and+formatting/Obsidian+Flavored+Markdown" rel="nofollow">Obsidian-flavored Markdown</a>, including tables, to-do lists, etc</li>
<li> Support for images and attachments. Embed links converted to Markdown format <code>!()[image.png]</code> and placed in the user's defined attachment location (Settings → File &amp; links)</li>
<li> Provide working test cases, ideally a reproducible data import that can be used on Notion. Alternatively a test account you can share with us via DM.</li>
</ul>
<h3 dir="auto">Databases to Bases</h3>
<p dir="auto">Some exploration is required before implementation because Databases and Bases work a bit differently. Notion's Databases start out as empty, whereas a Base starts out with all of the user's files, then narrows down using filters.</p>
<ul dir="auto">
<li>Determine for importing files, and the bases/views</li>
<li>Determine what can be imported: views, columns, groups, summaries, formulas, etc.</li>
</ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Asus Gaming Laptop ACPI Firmware Bug: A Deep Technical Investigation (372 pts)]]></title>
            <link>https://github.com/Zephkek/Asus-ROG-Aml-Deep-Dive</link>
            <guid>45271484</guid>
            <pubDate>Wed, 17 Sep 2025 03:54:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Zephkek/Asus-ROG-Aml-Deep-Dive">https://github.com/Zephkek/Asus-ROG-Aml-Deep-Dive</a>, See on <a href="https://news.ycombinator.com/item?id=45271484">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">The ASUS Gaming Laptop ACPI Firmware Bug: A Deep Technical Investigation</h2><a id="user-content-the-asus-gaming-laptop-acpi-firmware-bug-a-deep-technical-investigation" aria-label="Permalink: The ASUS Gaming Laptop ACPI Firmware Bug: A Deep Technical Investigation" href="#the-asus-gaming-laptop-acpi-firmware-bug-a-deep-technical-investigation"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">If You're Here, You Know The Pain</h2><a id="user-content-if-youre-here-you-know-the-pain" aria-label="Permalink: If You're Here, You Know The Pain" href="#if-youre-here-you-know-the-pain"></a></p>
<p dir="auto">You own a high-end ASUS ROG laptop perhaps a Strix, Scar, or Zephyrus. It's specifications are impressive: an RTX 30/40 series GPU, a top-tier Intel processor, and plenty of RAM. Yet, it stutters during basic tasks like watching a YouTube video, audio crackles and pops on Discord calls, the mouse cursor freezes for a split second, just long enough to be infuriating.</p>
<p dir="auto">You've likely tried all the conventional fixes:</p>
<ul dir="auto">
<li>Updating every driver imaginable, multiple times.</li>
<li>Performing a "clean" reinstallation of Windows.</li>
<li>Disabling every conceivable power-saving option.</li>
<li>Manually tweaking processor interrupt affinities.</li>
<li>Following convoluted multi-step guides from Reddit threads.</li>
<li>Even installing Linux, only to find the problem persists.</li>
</ul>
<p dir="auto">If none of that worked, it's because the issue isn't with the operating system or a driver. The problem is far deeper, embedded in the machine's firmware, the BIOS.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Initial Symptoms and Measurement</h2><a id="user-content-initial-symptoms-and-measurement" aria-label="Permalink: Initial Symptoms and Measurement" href="#initial-symptoms-and-measurement"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">The Pattern Emerges</h3><a id="user-content-the-pattern-emerges" aria-label="Permalink: The Pattern Emerges" href="#the-pattern-emerges"></a></p>
<p dir="auto">The first tool in any performance investigator's toolkit for these symptoms is LatencyMon. It acts as a canary in the coal mine for system-wide latency issues. On an affected ASUS Zephyrus M16, the results are immediate and damning:</p>
<div data-snippet-clipboard-copy-content="CONCLUSION
Your system appears to be having trouble handling real-time audio and other tasks. 
You are likely to experience buffer underruns appearing as drop outs, clicks or pops.

HIGHEST MEASURED INTERRUPT TO PROCESS LATENCY
Highest measured interrupt to process latency (μs):   65,816.60
Average measured interrupt to process latency (μs):   23.29

HIGHEST REPORTED ISR ROUTINE EXECUTION TIME
Highest ISR routine execution time (μs):              536.80
Driver with highest ISR routine execution time:       ACPI.sys

HIGHEST REPORTED DPC ROUTINE EXECUTION TIME  
Highest DPC routine execution time (μs):              5,998.83
Driver with highest DPC routine execution time:       ACPI.sys"><pre><code>CONCLUSION
Your system appears to be having trouble handling real-time audio and other tasks. 
You are likely to experience buffer underruns appearing as drop outs, clicks or pops.

HIGHEST MEASURED INTERRUPT TO PROCESS LATENCY
Highest measured interrupt to process latency (μs):   65,816.60
Average measured interrupt to process latency (μs):   23.29

HIGHEST REPORTED ISR ROUTINE EXECUTION TIME
Highest ISR routine execution time (μs):              536.80
Driver with highest ISR routine execution time:       ACPI.sys

HIGHEST REPORTED DPC ROUTINE EXECUTION TIME  
Highest DPC routine execution time (μs):              5,998.83
Driver with highest DPC routine execution time:       ACPI.sys
</code></pre></div>
<p dir="auto">The data clearly implicates <code>ACPI.sys</code>. However, the per-CPU data reveals a more specific pattern:</p>
<div data-snippet-clipboard-copy-content="CPU 0 Interrupt cycle time (s):                       208.470124
CPU 0 ISR highest execution time (μs):                536.804674
CPU 0 DPC highest execution time (μs):                5,998.834725
CPU 0 DPC total execution time (s):                   90.558238"><pre><code>CPU 0 Interrupt cycle time (s):                       208.470124
CPU 0 ISR highest execution time (μs):                536.804674
CPU 0 DPC highest execution time (μs):                5,998.834725
CPU 0 DPC total execution time (s):                   90.558238
</code></pre></div>
<p dir="auto">CPU 0 is taking the brunt of the impact, spending over 90 seconds processing interrupts while other cores remain largely unaffected. This isn't a failure of load balancing; it's a process locked to a single core.</p>
<p dir="auto">A similar test on a Scar 15 from 2022 shows the exact same culprit: high DPC latency originating from <code>ACPI.sys</code>.</p>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/76183331/490271011-fdf6f26a-dda8-4561-82c7-349fc8c298ab.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTgwOTQ1MDIsIm5iZiI6MTc1ODA5NDIwMiwicGF0aCI6Ii83NjE4MzMzMS80OTAyNzEwMTEtZmRmNmYyNmEtZGRhOC00NTYxLTgyYzctMzQ5ZmM4YzI5OGFiLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA5MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwOTE3VDA3MzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg0ZmQxZmZlNDhjYTRkZDMxMzE0YjYxN2JlMWVkNTRkMWE1MDAzZTJlMWE3ZDNjNGVlYzVlNWI0OWIyNjI3ZTkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.XeloawO4FWC41BnjkaHHNr1GcQWOIMS9m80iHpBKDeI"><img width="974" height="511" alt="latencymon" src="https://private-user-images.githubusercontent.com/76183331/490271011-fdf6f26a-dda8-4561-82c7-349fc8c298ab.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTgwOTQ1MDIsIm5iZiI6MTc1ODA5NDIwMiwicGF0aCI6Ii83NjE4MzMzMS80OTAyNzEwMTEtZmRmNmYyNmEtZGRhOC00NTYxLTgyYzctMzQ5ZmM4YzI5OGFiLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA5MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwOTE3VDA3MzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg0ZmQxZmZlNDhjYTRkZDMxMzE0YjYxN2JlMWVkNTRkMWE1MDAzZTJlMWE3ZDNjNGVlYzVlNWI0OWIyNjI3ZTkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.XeloawO4FWC41BnjkaHHNr1GcQWOIMS9m80iHpBKDeI"></a>
<p dir="auto">It's easy to blame a Windows driver, but <code>ACPI.sys</code> is not a typical driver. It primarily functions as an interpreter for ACPI Machine Language (AML), the code provided by the laptop's firmware (BIOS). If <code>ACPI.sys</code> is slow, it's because the firmware is feeding it inefficient or flawed AML code to execute. These slowdowns are often triggered by General Purpose Events (GPEs) and traffic from the Embedded Controller (EC). To find the true source, we must dig deeper.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Capturing the Problem in More Detail: ETW Tracing</h2><a id="user-content-capturing-the-problem-in-more-detail-etw-tracing" aria-label="Permalink: Capturing the Problem in More Detail: ETW Tracing" href="#capturing-the-problem-in-more-detail-etw-tracing"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Setting Up Advanced ACPI Tracing</h3><a id="user-content-setting-up-advanced-acpi-tracing" aria-label="Permalink: Setting Up Advanced ACPI Tracing" href="#setting-up-advanced-acpi-tracing"></a></p>
<p dir="auto">To understand what <code>ACPI.sys</code> is doing during these latency spikes, we can use Event Tracing for Windows (ETW) to capture detailed logs from the ACPI providers.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Find the relevant ACPI ETW providers
logman query providers | findstr /i acpi
# This returns two key providers:
# Microsoft-Windows-Kernel-Acpi {C514638F-7723-485B-BCFC-96565D735D4A}
# Microsoft-ACPI-Provider {DAB01D4D-2D48-477D-B1C3-DAAD0CE6F06B}

# Start a comprehensive trace session
logman start ACPITrace -p {DAB01D4D-2D48-477D-B1C3-DAAD0CE6F06B} 0xFFFFFFFF 5 -o C:\Temp\acpi.etl -ets
logman update ACPITrace -p {C514638F-7723-485B-BCFC-96565D735D4A} 0xFFFFFFFF 5 -ets

# Then once we're done we can stop the trace and check the etl file and save the data in csv format aswell.
logman stop ACPITrace -ets
tracerpt C:\Temp\acpi_providers.etl -o C:\Temp\acpi_events.csv -of CSV"><pre><span><span>#</span> Find the relevant ACPI ETW providers</span>
logman query providers <span>|</span> findstr <span>/</span>i acpi
<span><span>#</span> This returns two key providers:</span>
<span><span>#</span> Microsoft-Windows-Kernel-Acpi {C514638F-7723-485B-BCFC-96565D735D4A}</span>
<span><span>#</span> Microsoft-ACPI-Provider {DAB01D4D-2D48-477D-B1C3-DAAD0CE6F06B}</span>

<span><span>#</span> Start a comprehensive trace session</span>
logman start ACPITrace <span>-</span>p {DAB01D4D<span>-</span>2D48<span>-</span><span>477D</span><span>-</span>B1C3<span>-</span>DAAD0CE6F06B} <span>0xFFFFFFFF</span> <span>5</span> <span>-</span>o C:\Temp\acpi.etl <span>-</span>ets
logman update ACPITrace <span>-</span>p {C514638F<span>-</span><span>7723</span><span>-</span>485B<span>-</span>BCFC<span>-</span>96565D735D4A} <span>0xFFFFFFFF</span> <span>5</span> <span>-</span>ets

<span><span>#</span> Then once we're done we can stop the trace and check the etl file and save the data in csv format aswell.</span>
logman stop ACPITrace <span>-</span>ets
tracerpt C:\Temp\acpi_providers.etl <span>-</span>o C:\Temp\acpi_events.csv <span>-</span>of CSV</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">An Unexpected Discovery</h3><a id="user-content-an-unexpected-discovery" aria-label="Permalink: An Unexpected Discovery" href="#an-unexpected-discovery"></a></p>
<p dir="auto">Analyzing the resulting trace file in the Windows Performance Analyzer reveals a crucial insight. The spikes aren't random; they are periodic, occurring like clockwork every 30 to 60 seconds.</p>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/76183331/490271181-2aac7320-3e06-4025-841c-86129f9d5b62.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTgwOTQ1MDIsIm5iZiI6MTc1ODA5NDIwMiwicGF0aCI6Ii83NjE4MzMzMS80OTAyNzExODEtMmFhYzczMjAtM2UwNi00MDI1LTg0MWMtODYxMjlmOWQ1YjYyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA5MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwOTE3VDA3MzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTZjOGFkZGM5YTAxNDg0MTMwMjYxMDY4ZTcwYmE5MzE1OTlhMDY5OTI2MzM1Y2U4YmUwMzhkNmFkZTE0NDkzMTgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.iS-x0lJNeizY1LZjTINBC2ukNLsVV1UmDyIMxpZVUmQ"><img width="1673" height="516" alt="61c7abb1-d7aa-4b69-9a88-22cca7352f00" src="https://private-user-images.githubusercontent.com/76183331/490271181-2aac7320-3e06-4025-841c-86129f9d5b62.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTgwOTQ1MDIsIm5iZiI6MTc1ODA5NDIwMiwicGF0aCI6Ii83NjE4MzMzMS80OTAyNzExODEtMmFhYzczMjAtM2UwNi00MDI1LTg0MWMtODYxMjlmOWQ1YjYyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA5MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwOTE3VDA3MzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTZjOGFkZGM5YTAxNDg0MTMwMjYxMDY4ZTcwYmE5MzE1OTlhMDY5OTI2MzM1Y2U4YmUwMzhkNmFkZTE0NDkzMTgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.iS-x0lJNeizY1LZjTINBC2ukNLsVV1UmDyIMxpZVUmQ"></a>
<p dir="auto">Random interruptions often suggest hardware faults or thermal throttling. A perfectly repeating pattern points to a systemic issue, a timer or a scheduled event baked into the system's logic.</p>
<p dir="auto">The raw event data confirms this pattern:</p>
<div data-snippet-clipboard-copy-content="Clock-Time (100ns),        Event,                      Kernel(ms), CPU
134024027290917802,       _GPE._L02 started,          13.613820,  0
134024027290927629,       _SB...BAT0._STA started,    0.000000,   4
134024027290932512,       _GPE._L02 finished,         -,          6"><pre lang="csv"><code>Clock-Time (100ns),        Event,                      Kernel(ms), CPU
134024027290917802,       _GPE._L02 started,          13.613820,  0
134024027290927629,       _SB...BAT0._STA started,    0.000000,   4
134024027290932512,       _GPE._L02 finished,         -,          6
</code></pre></div>
<p dir="auto">The first event, <code>_GPE._L02</code>, is an interrupt handler that takes <strong>13.6 milliseconds</strong> to execute. For a high-priority interrupt, this is an eternity and is catastrophic for real-time system performance.</p>
<p dir="auto">Deeper in the trace, another bizarre behavior emerges; the system repeatedly attempts to power the discrete GPU on and off, even when it's supposed to be permanently active.</p>
<div data-snippet-clipboard-copy-content="Clock-Time,                Event,                    Duration
134024027315051227,       _SB.PC00.GFX0._PS0 start, 278μs     # GPU Power On
134024027315155404,       _SB.PC00.GFX0._DOS start, 894μs     # Display Output Switch
134024027330733719,       _SB.PC00.GFX0._PS3 start, 1364μs    # GPU Power Off
[~15 seconds later]
134024027607550064,       _SB.PC00.GFX0._PS0 start, 439μs     # Power On Again!
134024027607657368,       _SB.PC00.GFX0._DOS start, 1079μs    # Display Output Switch
134024027623134006,       _SB.PC00.GFX0._PS3 start, 394μs     # Power Off Again!
..."><pre lang="csv"><code>Clock-Time,                Event,                    Duration
134024027315051227,       _SB.PC00.GFX0._PS0 start, 278μs     # GPU Power On
134024027315155404,       _SB.PC00.GFX0._DOS start, 894μs     # Display Output Switch
134024027330733719,       _SB.PC00.GFX0._PS3 start, 1364μs    # GPU Power Off
[~15 seconds later]
134024027607550064,       _SB.PC00.GFX0._PS0 start, 439μs     # Power On Again!
134024027607657368,       _SB.PC00.GFX0._DOS start, 1079μs    # Display Output Switch
134024027623134006,       _SB.PC00.GFX0._PS3 start, 394μs     # Power Off Again!
...
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why This Behavior is Fundamentally Incorrect</h3><a id="user-content-why-this-behavior-is-fundamentally-incorrect" aria-label="Permalink: Why This Behavior is Fundamentally Incorrect" href="#why-this-behavior-is-fundamentally-incorrect"></a></p>
<p dir="auto">This power cycling is nonsensical because the laptop is configured for a scenario where it is impossible: <strong>The system is in Ultimate Mode (via a MUX switch) with an external display connected.</strong></p>
<p dir="auto">In this mode:</p>
<ul dir="auto">
<li>The discrete NVIDIA GPU (dGPU) is the <strong>only</strong> active graphics processor.</li>
<li>The integrated Intel GPU (iGPU) is completely powered down and bypassed.</li>
<li>The dGPU is wired directly to the internal and external displays.</li>
<li>There is no mechanism for switching between GPUs.</li>
</ul>
<p dir="auto">Yet, the firmware is relentlessly trying to power cycle the dGPU every 15-30 seconds. The dGPU in mux mode isn't just "preferred" - it's the ONLY path to the display. There's no fallback, and no alternative. When the firmware sends <code>_PS3</code> (power off), it's attempting something architecturally impossible.</p>
<p dir="auto">Most of the time, hardware sanity checks refuse these nonsensical commands, but even failed attempts introduce latency spikes causing audio dropouts, input lag, and accumulating performance degradation. Games freeze mid-session, videos buffer indefinitely, system responsiveness deteriorates until restart.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">The Catastrophic Edge Case</h4><a id="user-content-the-catastrophic-edge-case" aria-label="Permalink: The Catastrophic Edge Case" href="#the-catastrophic-edge-case"></a></p>
<p dir="auto">Sometimes, under specific thermal conditions or race conditions, the power-down actually succeeds. When the firmware manages to power down the GPU that's driving the display, the sequence is predictable and catastrophic:</p>
<ol dir="auto">
<li><strong>Firmware executes <code>_PS3</code></strong> - GPU power off command</li>
<li><strong>Hardware complies</strong> - safety checks fail or timing aligns</li>
<li><strong>Display signal cuts</strong> - monitors go black</li>
<li><strong>User input triggers wake</strong> - mouse/keyboard activity</li>
<li><strong>Windows calls <code>PowerOnMonitor()</code></strong> - attempt display recovery</li>
<li><strong>NVIDIA driver executes <code>_PS0</code></strong> - GPU power on command</li>
<li><strong>GPU enters impossible state</strong> - firmware insists OFF, Windows needs ON</li>
<li><strong>Driver thread blocks indefinitely</strong> - waiting for GPU response</li>
<li><strong>30-second watchdog expires</strong> - Windows gives up</li>
<li><strong>System crashes with BSOD</strong></li>
</ol>
<div data-snippet-clipboard-copy-content="5: kd> !analyze -v
*******************************************************************************
*                                                                             *
*                        Bugcheck Analysis                                    *
*                                                                             *
*******************************************************************************

WIN32K_POWER_WATCHDOG_TIMEOUT (19c)
Win32k did not turn the monitor on in a timely manner.
Arguments:
Arg1: 0000000000000050, Calling monitor driver to power on.
Arg2: ffff8685b1463080, Pointer to the power request worker thread.
Arg3: 0000000000000000
Arg4: 0000000000000000
...
STACK_TEXT:  
fffff685`3a767130 fffff800`94767be0     : 00000000`00000047 00000000`00000000 00000000`00000000 00000000`00000000 : nt!KiSwapContext+0x76
fffff685`3a767270 fffff800`94726051     : ffff8685`b1463080 00000027`00008b94 fffff685`3a767458 fffff800`00000000 : nt!KiSwapThread+0x6a0
fffff685`3a767340 fffff800`94724ed3     : fffff685`00000000 00000000`00000043 00000000`00000002 0000008a`fbf50968 : nt!KiCommitThreadWait+0x271
fffff685`3a7673e0 fffff800`9471baf2     : fffff685`3a7675d0 02000000`0000001b 00000000`00000000 fffff800`94724500 : nt!KeWaitForSingleObject+0x773
fffff685`3a7674d0 fffff800`9471b7d5     : ffff8685`9cbec810 fffff685`3a7675b8 00000000`00010224 fffff800`00000003 : nt!ExpWaitForFastResource+0x92
fffff685`3a767580 fffff800`9471b49d     : 00000000`00000000 ffff8685`9cbec850 ffff8685`b1463080 00000000`00000000 : nt!ExpAcquireFastResourceExclusiveSlow+0x1e5
fffff685`3a767630 fffff800`28faca9b     : fffff800`262ee9c8 00000000`00000003 ffff8685`9cbec810 02000000`00000065 : nt!ExAcquireFastResourceExclusive+0x1bd
fffff685`3a767690 fffff800`28facbe5     : ffff8685`b31de000 00000000`00000000 ffffd31d`9a05244f 00000000`00000000 : win32kbase!<lambda_63b61c2369133a205197eda5bd671ee7>::<lambda_invoker_cdecl>+0x2b
fffff685`3a7676c0 fffff800`28e5f864     : ffffad0c`94d10878 fffff685`3a767769 ffffad0c`94d10830 ffff8685`b31de000 : win32kbase!UserCritInternal::`anonymous namespace'::EnterCritInternalEx+0x4d
fffff685`3a7676f0 fffff800`28e5f4ef     : 00000000`00000000 00000000`00000000 fffff800`262ee9c8 00000000`00000000 : win32kbase!DrvSetWddmDeviceMonitorPowerState+0x354
fffff685`3a7677d0 fffff800`28e2abab     : ffff8685`b31de000 00000000`00000000 ffff8685`b31de000 00000000`00000000 : win32kbase!DrvSetMonitorPowerState+0x2f
fffff685`3a767800 fffff800`28ef22fa     : 00000000`00000000 fffff685`3a7678d9 00000000`00000001 00000000`00000001 : win32kbase!PowerOnMonitor+0x19b
fffff685`3a767870 fffff800`28ef13dd     : ffff8685`94a40700 ffff8685`a2eb31d0 00000000`00000001 00000000`00000020 : win32kbase!xxxUserPowerEventCalloutWorker+0xaaa
fffff685`3a767940 fffff800`4bab21c2     : ffff8685`b1463080 fffff685`3a767aa0 00000000`00000000 00000000`00000020 : win32kbase!xxxUserPowerCalloutWorker+0x13d
fffff685`3a7679c0 fffff800`26217f3a     : 00000000`00000000 00000000`00000000 00000000`00000000 00000000`00000000 : win32kfull!NtUserUserPowerCalloutWorker+0x22
fffff685`3a7679f0 fffff800`94ab8d55     : 00000000`000005bc 00000000`00000104 ffff8685`b1463080 00000000`00000000 : win32k!NtUserUserPowerCalloutWorker+0x2e
fffff685`3a767a20 00007ff8`ee71ca24     : 00000000`00000000 00000000`00000000 00000000`00000000 00000000`00000000 : nt!KiSystemServiceCopyEnd+0x25
000000cc`d11ffbc8 00000000`00000000     : 00000000`00000000 00000000`00000000 00000000`00000000 00000000`00000000 : 0x00007ff8`ee71ca24

..."><pre><code>5: kd&gt; !analyze -v
*******************************************************************************
*                                                                             *
*                        Bugcheck Analysis                                    *
*                                                                             *
*******************************************************************************

WIN32K_POWER_WATCHDOG_TIMEOUT (19c)
Win32k did not turn the monitor on in a timely manner.
Arguments:
Arg1: 0000000000000050, Calling monitor driver to power on.
Arg2: ffff8685b1463080, Pointer to the power request worker thread.
Arg3: 0000000000000000
Arg4: 0000000000000000
...
STACK_TEXT:  
fffff685`3a767130 fffff800`94767be0     : 00000000`00000047 00000000`00000000 00000000`00000000 00000000`00000000 : nt!KiSwapContext+0x76
fffff685`3a767270 fffff800`94726051     : ffff8685`b1463080 00000027`00008b94 fffff685`3a767458 fffff800`00000000 : nt!KiSwapThread+0x6a0
fffff685`3a767340 fffff800`94724ed3     : fffff685`00000000 00000000`00000043 00000000`00000002 0000008a`fbf50968 : nt!KiCommitThreadWait+0x271
fffff685`3a7673e0 fffff800`9471baf2     : fffff685`3a7675d0 02000000`0000001b 00000000`00000000 fffff800`94724500 : nt!KeWaitForSingleObject+0x773
fffff685`3a7674d0 fffff800`9471b7d5     : ffff8685`9cbec810 fffff685`3a7675b8 00000000`00010224 fffff800`00000003 : nt!ExpWaitForFastResource+0x92
fffff685`3a767580 fffff800`9471b49d     : 00000000`00000000 ffff8685`9cbec850 ffff8685`b1463080 00000000`00000000 : nt!ExpAcquireFastResourceExclusiveSlow+0x1e5
fffff685`3a767630 fffff800`28faca9b     : fffff800`262ee9c8 00000000`00000003 ffff8685`9cbec810 02000000`00000065 : nt!ExAcquireFastResourceExclusive+0x1bd
fffff685`3a767690 fffff800`28facbe5     : ffff8685`b31de000 00000000`00000000 ffffd31d`9a05244f 00000000`00000000 : win32kbase!&lt;lambda_63b61c2369133a205197eda5bd671ee7&gt;::&lt;lambda_invoker_cdecl&gt;+0x2b
fffff685`3a7676c0 fffff800`28e5f864     : ffffad0c`94d10878 fffff685`3a767769 ffffad0c`94d10830 ffff8685`b31de000 : win32kbase!UserCritInternal::`anonymous namespace'::EnterCritInternalEx+0x4d
fffff685`3a7676f0 fffff800`28e5f4ef     : 00000000`00000000 00000000`00000000 fffff800`262ee9c8 00000000`00000000 : win32kbase!DrvSetWddmDeviceMonitorPowerState+0x354
fffff685`3a7677d0 fffff800`28e2abab     : ffff8685`b31de000 00000000`00000000 ffff8685`b31de000 00000000`00000000 : win32kbase!DrvSetMonitorPowerState+0x2f
fffff685`3a767800 fffff800`28ef22fa     : 00000000`00000000 fffff685`3a7678d9 00000000`00000001 00000000`00000001 : win32kbase!PowerOnMonitor+0x19b
fffff685`3a767870 fffff800`28ef13dd     : ffff8685`94a40700 ffff8685`a2eb31d0 00000000`00000001 00000000`00000020 : win32kbase!xxxUserPowerEventCalloutWorker+0xaaa
fffff685`3a767940 fffff800`4bab21c2     : ffff8685`b1463080 fffff685`3a767aa0 00000000`00000000 00000000`00000020 : win32kbase!xxxUserPowerCalloutWorker+0x13d
fffff685`3a7679c0 fffff800`26217f3a     : 00000000`00000000 00000000`00000000 00000000`00000000 00000000`00000000 : win32kfull!NtUserUserPowerCalloutWorker+0x22
fffff685`3a7679f0 fffff800`94ab8d55     : 00000000`000005bc 00000000`00000104 ffff8685`b1463080 00000000`00000000 : win32k!NtUserUserPowerCalloutWorker+0x2e
fffff685`3a767a20 00007ff8`ee71ca24     : 00000000`00000000 00000000`00000000 00000000`00000000 00000000`00000000 : nt!KiSystemServiceCopyEnd+0x25
000000cc`d11ffbc8 00000000`00000000     : 00000000`00000000 00000000`00000000 00000000`00000000 00000000`00000000 : 0x00007ff8`ee71ca24

...
</code></pre></div>
<p dir="auto">The crash dump confirms the thread is stuck in <code>win32kbase!DrvSetWddmDeviceMonitorPowerState</code>, waiting for the NVIDIA driver to respond. It can't because it's caught between a confused power state, windows wanting to turn on the GPU while the firmware is arming the GPU cut off.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Understanding General Purpose Events</h3><a id="user-content-understanding-general-purpose-events" aria-label="Permalink: Understanding General Purpose Events" href="#understanding-general-purpose-events"></a></p>
<p dir="auto">GPEs are the firmware's mechanism for signaling hardware events to the operating system. They are essentially hardware interrupts that trigger the execution of ACPI code. The trace data points squarely at <code>_GPE._L02</code> as the source of our latency.</p>
<p dir="auto">A closer look at the timing reveals a consistent and problematic pattern:</p>
<div data-snippet-clipboard-copy-content="_GPE._L02 Event Analysis from ROG Strix Trace:

Event 1 @ Clock 134024027290917802
  Duration: 13,613,820 ns (13.61ms)
  Triggered: Battery and AC adapter status checks

Event 2 @ Clock 134024027654496591  
  Duration: 13,647,255 ns (13.65ms)
  Triggered: Battery and AC adapter status checks
  
Event 3 @ Clock 134024028048493318
  Duration: 13,684,515 ns (13.68ms)  
  Triggered: Battery and AC adapter status checks

Interval between events: ~36-39 seconds
Consistency: The duration is remarkably stable and the interval is periodic."><pre><code>_GPE._L02 Event Analysis from ROG Strix Trace:

Event 1 @ Clock 134024027290917802
  Duration: 13,613,820 ns (13.61ms)
  Triggered: Battery and AC adapter status checks

Event 2 @ Clock 134024027654496591  
  Duration: 13,647,255 ns (13.65ms)
  Triggered: Battery and AC adapter status checks
  
Event 3 @ Clock 134024028048493318
  Duration: 13,684,515 ns (13.68ms)  
  Triggered: Battery and AC adapter status checks

Interval between events: ~36-39 seconds
Consistency: The duration is remarkably stable and the interval is periodic.
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">The Correlation</h3><a id="user-content-the-correlation" aria-label="Permalink: The Correlation" href="#the-correlation"></a></p>
<p dir="auto">Every single time the lengthy <code>_GPE._L02</code> event fires, it triggers the exact same sequence of ACPI method calls.</p>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/76183331/490271340-01326c61-b7a2-4c12-a907-8433f43a6a72.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTgwOTQ1MDIsIm5iZiI6MTc1ODA5NDIwMiwicGF0aCI6Ii83NjE4MzMzMS80OTAyNzEzNDAtMDEzMjZjNjEtYjdhMi00YzEyLWE5MDctODQzM2Y0M2E2YTcyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA5MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwOTE3VDA3MzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTVkNzI2ZmIxM2UxZTA3ZGE2ZjhmMmY0ZjQ3ZjAzZDJiYzZlY2EzMGFmOTIwYjE0OWUzMWRmOTBkZDQxNDc2YWImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.-4XfvRJx29Ys_QBL35nxbKxrohnoTUZ-1_RKQK954ZI"><img width="589" height="589" alt="64921999-7614-4706-a5ac-54c39c38fd0b" src="https://private-user-images.githubusercontent.com/76183331/490271340-01326c61-b7a2-4c12-a907-8433f43a6a72.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTgwOTQ1MDIsIm5iZiI6MTc1ODA5NDIwMiwicGF0aCI6Ii83NjE4MzMzMS80OTAyNzEzNDAtMDEzMjZjNjEtYjdhMi00YzEyLWE5MDctODQzM2Y0M2E2YTcyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA5MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwOTE3VDA3MzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTVkNzI2ZmIxM2UxZTA3ZGE2ZjhmMmY0ZjQ3ZjAzZDJiYzZlY2EzMGFmOTIwYjE0OWUzMWRmOTBkZDQxNDc2YWImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.-4XfvRJx29Ys_QBL35nxbKxrohnoTUZ-1_RKQK954ZI"></a>
<p dir="auto">The pattern is undeniable:</p>
<ol dir="auto">
<li>A hardware interrupt fires <code>_GPE._L02</code>.</li>
<li>The handler executes methods to check battery status.</li>
<li>Shortly thereafter, the firmware attempts to change the GPU's power state.</li>
<li>The system runs normally for about 30-60 seconds.</li>
<li>The cycle repeats.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Extracting and Decompiling the Firmware Code</h2><a id="user-content-extracting-and-decompiling-the-firmware-code" aria-label="Permalink: Extracting and Decompiling the Firmware Code" href="#extracting-and-decompiling-the-firmware-code"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Getting to the Source</h3><a id="user-content-getting-to-the-source" aria-label="Permalink: Getting to the Source" href="#getting-to-the-source"></a></p>
<p dir="auto">To analyze the code responsible for this behavior, we must extract and decompile the ACPI tables provided by the BIOS to the operating system.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Extract all ACPI tables into binary .dat files
acpidump -b

# Output includes:
# DSDT.dat - The main Differentiated System Description Table
# SSDT1.dat ... SSDT17.dat - Secondary System Description Tables

# Decompile the main table into human-readable ACPI Source Language (.dsl)
iasl -d DSDT.dsl"><pre><span><span>#</span> Extract all ACPI tables into binary .dat files</span>
acpidump -b

<span><span>#</span> Output includes:</span>
<span><span>#</span> DSDT.dat - The main Differentiated System Description Table</span>
<span><span>#</span> SSDT1.dat ... SSDT17.dat - Secondary System Description Tables</span>

<span><span>#</span> Decompile the main table into human-readable ACPI Source Language (.dsl)</span>
iasl -d DSDT.dsl</pre></div>
<p dir="auto">This decompiled ASL provides a direct view into the firmware's executable logic. It is a precise representation of the exact instructions that the ACPI.sys driver is fed by the firmware and executes at the highest privilege level within the Windows kernel. Any logical flaws found in this code are the direct cause of the system's behavior.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Finding the GPE Handler</h3><a id="user-content-finding-the-gpe-handler" aria-label="Permalink: Finding the GPE Handler" href="#finding-the-gpe-handler"></a></p>
<p dir="auto">Searching the decompiled <code>DSDT.dsl</code> file, we find the definition for our problematic GPE handler:</p>
<div dir="auto" data-snippet-clipboard-copy-content="Scope (_GPE)
{
    Method (_L02, 0, NotSerialized)  // _Lxx: Level-Triggered GPE
    {
        \_SB.PC00.LPCB.ECLV ()
    }
}"><pre><span>Scope</span> (<span>_GPE</span>)
{
    <span>Method</span> (<span>_L02</span>, <span>0</span>, <span>NotSerialized</span>)  <span>// _Lxx: Level-Triggered GPE</span>
    {
        \<span>_SB</span>.PC00.LPCB.ECLV ()
    }
}</pre></div>
<p dir="auto">This code is simple: when the <code>_L02</code> interrupt occurs, it calls a single method, <code>ECLV</code>. The "L" prefix in <code>_L02</code> signifies that this is a <strong>level-triggered</strong> interrupt, meaning it will continue to fire as long as the underlying hardware condition is active. This is a critical detail.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">The Catastrophic <code>ECLV</code> Implementation</h3><a id="user-content-the-catastrophic-eclv-implementation" aria-label="Permalink: The Catastrophic ECLV Implementation" href="#the-catastrophic-eclv-implementation"></a></p>
<p dir="auto">Following the call to <code>ECLV()</code>, we uncover a deeply flawed implementation that is the direct cause of the system-wide stuttering.</p>
<div dir="auto" data-snippet-clipboard-copy-content="Method (ECLV, 0, NotSerialized)  // Starting at line 099244
{
    // Main loop - continues while events exist OR sleep events are pending
    // AND we haven't exceeded our time budget (TI3S < 0x78)
    While (((CKEV() != Zero) || (SLEC != Zero)) &amp;&amp; (TI3S < 0x78))
    {
        Local1 = One
        While (Local1 != Zero)
        {
            Local1 = GEVT()    // Get next event from queue
            LEVN (Local1)      // Process the event
            TIMC += 0x19       // Increment time counter by 25
            
            // This is where it gets really bad
            If ((SLEC != Zero) &amp;&amp; (Local1 == Zero))
            {
                // No events but sleep events pending
                If (TIMC == 0x19)
                {
                    Sleep (0x64)    // Sleep for 100 milliseconds!!!
                    TIMC = 0x64     // Set time counter to 100
                    TI3S += 0x04    // Increment major counter by 4
                }
                Else
                {
                    Sleep (0x19)    // Sleep for 25 milliseconds!!!
                    TI3S++          // Increment major counter by 1
                }
            }
        }
    }
    
    // Here's where it gets even worse
    If (TI3S >= 0x78)  // If we hit our time budget (120)
    {
        TI3S = Zero
        If (EEV0 == Zero)
        {
            EEV0 = 0xFF    // Force another event to be pending!
        }
    }
}"><pre><span>Method</span> (ECLV, <span>0</span>, <span>NotSerialized</span>)  <span>// Starting at line 099244</span>
{
    <span>// Main loop - continues while events exist OR sleep events are pending</span>
    <span>// AND we haven't exceeded our time budget (TI3S &lt; 0x78)</span>
    <span>While</span> (((CKEV() != <span>Zero</span>) || (SLEC != <span>Zero</span>)) &amp;&amp; (TI3S &lt; <span>0x78</span>))
    {
        <span>Local1</span> = <span>One</span>
        <span>While</span> (<span>Local1</span> != <span>Zero</span>)
        {
            <span>Local1</span> = GEVT()    <span>// Get next event from queue</span>
            LEVN (<span>Local1</span>)      <span>// Process the event</span>
            TIMC += <span>0x19</span>       <span>// Increment time counter by 25</span>
            
            <span>// This is where it gets really bad</span>
            <span>If</span> ((SLEC != <span>Zero</span>) &amp;&amp; (<span>Local1</span> == <span>Zero</span>))
            {
                <span>// No events but sleep events pending</span>
                <span>If</span> (TIMC == <span>0x19</span>)
                {
                    <span>Sleep</span> (<span>0x64</span>)    <span>// Sleep for 100 milliseconds!!!</span>
                    TIMC = <span>0x64</span>     <span>// Set time counter to 100</span>
                    TI3S += <span>0x04</span>    <span>// Increment major counter by 4</span>
                }
                <span>Else</span>
                {
                    <span>Sleep</span> (<span>0x19</span>)    <span>// Sleep for 25 milliseconds!!!</span>
                    TI3S++          <span>// Increment major counter by 1</span>
                }
            }
        }
    }
    
    <span>// Here's where it gets even worse</span>
    <span>If</span> (TI3S &gt;= <span>0x78</span>)  <span>// If we hit our time budget (120)</span>
    {
        TI3S = <span>Zero</span>
        <span>If</span> (EEV0 == <span>Zero</span>)
        {
            EEV0 = <span>0xFF</span>    <span>// Force another event to be pending!</span>
        }
    }
}</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Breaking Down this monstrosity</h3><a id="user-content-breaking-down-this-monstrosity" aria-label="Permalink: Breaking Down this monstrosity" href="#breaking-down-this-monstrosity"></a></p>
<p dir="auto">This short block of code violates several fundamental principles of firmware and kernel programming.</p>
<p dir="auto"><strong>Wtf 1: Sleeping in an Interrupt Context</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="Sleep (0x64)    // 100ms sleep
Sleep (0x19)    // 25ms sleep"><pre><span>Sleep</span> (<span>0x64</span>)    <span>// 100ms sleep</span>
<span>Sleep</span> (<span>0x19</span>)    <span>// 25ms sleep</span></pre></div>
<p dir="auto">An interrupt handler runs at a very high priority to service hardware requests quickly. The <code>Sleep()</code> function completely halts the execution of the CPU core it is running on (CPU 0 in this case). While CPU 0 is sleeping, it cannot:</p>
<ul dir="auto">
<li>Process any other hardware interrupts.</li>
<li>Allow the kernel to schedule other threads.</li>
<li>Update system timers.</li>
</ul>
<p dir="auto"><strong>Wtf 2: Time-Sliced Interrupt Processing</strong>
The entire loop is designed to run for an extended period, processing events in batches. It's effectively a poorly designed task scheduler running inside an interrupt handler, capable of holding a CPU core hostage for potentially seconds at a time.</p>
<p dir="auto"><strong>Wtf 3: Self-Rearming Interrupt</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="If (EEV0 == Zero)
{
    EEV0 = 0xFF    // Forces all EC event bits on
}"><pre><span>If</span> (EEV0 == <span>Zero</span>)
{
    EEV0 = <span>0xFF</span>    <span>// Forces all EC event bits on</span>
}</pre></div>
<p dir="auto">This logic ensures that even if the Embedded Controller's event queue is empty, the code will create a new, artificial event. This guarantees that another interrupt will fire shortly after, creating the perfectly periodic pattern of ACPI spikes observed in the traces.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">The Event Dispatch System</h2><a id="user-content-the-event-dispatch-system" aria-label="Permalink: The Event Dispatch System" href="#the-event-dispatch-system"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">How Events Route to Actions</h3><a id="user-content-how-events-route-to-actions" aria-label="Permalink: How Events Route to Actions" href="#how-events-route-to-actions"></a></p>
<p dir="auto">The LEVN() method takes an event and routes it:</p>
<div dir="auto" data-snippet-clipboard-copy-content="Method (LEVN, 1, NotSerialized)
  {
      If ((Arg0 != Zero))
      {
          MBF0 = Arg0
          P80B = Arg0
          Local6 = Match (LEGA, MEQ, Arg0, MTR, Zero, Zero)
          If ((Local6 != Ones))
          {
              LGPA (Local6)
          }
      }
  }
"><pre><span>Method</span> (LEVN, <span>1</span>, <span>NotSerialized</span>)
  {
      <span>If</span> ((<span>Arg0</span> != <span>Zero</span>))
      {
          MBF0 = <span>Arg0</span>
          P80B = <span>Arg0</span>
          <span>Local6</span> = <span>Match</span> (LEGA, <span>MEQ</span>, <span>Arg0</span>, <span>MTR</span>, <span>Zero</span>, <span>Zero</span>)
          <span>If</span> ((<span>Local6</span> != <span>Ones</span>))
          {
              LGPA (<span>Local6</span>)
          }
      }
  }
</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">The <code>LGPA</code> Dispatch Table</h3><a id="user-content-the-lgpa-dispatch-table" aria-label="Permalink: The LGPA Dispatch Table" href="#the-lgpa-dispatch-table"></a></p>
<p dir="auto">The LGPA() method is a giant switch statement handling different events:</p>
<div dir="auto" data-snippet-clipboard-copy-content="Method (LGPA, 1, Serialized)  // Line 098862
{
    Switch (ToInteger (Arg0))
    {
        Case (Zero)  // Most common case - power event
        {
            DGD2 ()       // GPU-related function
            ^EC0._QA0 ()  // EC query method
            PWCG ()       // Power change - this is our battery polling
        }
        
        Case (0x18)  // GPU-specific event
        {
            If (M6EF == One)
            {
                Local0 = 0xD2
            }
            Else
            {
                Local0 = 0xD1
            }
            NOD2 (Local0)  // Notify GPU driver
        }
        
        Case (0x1E)  // Another GPU event
        {
            Notify (^^PEG1.PEGP, 0xD5)  // Direct GPU notification
            ROCT = 0x55                  // Sets flag for follow-up
        }
       
    }
}"><pre><span>Method</span> (LGPA, <span>1</span>, <span>Serialized</span>)  <span>// Line 098862</span>
{
    <span>Switch</span> (<span>ToInteger</span> (<span>Arg0</span>))
    {
        <span>Case</span> (<span>Zero</span>)  <span>// Most common case - power event</span>
        {
            DGD2 ()       <span>// GPU-related function</span>
            ^EC0._QA0 ()  <span>// EC query method</span>
            PWCG ()       <span>// Power change - this is our battery polling</span>
        }
        
        <span>Case</span> (<span>0x18</span>)  <span>// GPU-specific event</span>
        {
            <span>If</span> (M6EF == <span>One</span>)
            {
                <span>Local0</span> = <span>0xD2</span>
            }
            <span>Else</span>
            {
                <span>Local0</span> = <span>0xD1</span>
            }
            NOD2 (<span>Local0</span>)  <span>// Notify GPU driver</span>
        }
        
        <span>Case</span> (<span>0x1E</span>)  <span>// Another GPU event</span>
        {
            <span>Notify</span> (^^PEG1.PEGP, <span>0xD5</span>)  <span>// Direct GPU notification</span>
            ROCT = <span>0x55</span>                  <span>// Sets flag for follow-up</span>
        }
       
    }
}</pre></div>
<p dir="auto">This shows a direct link: a GPE fires, and the dispatch logic calls functions related to battery polling and GPU notifications.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">The Battery Polling Function</h2><a id="user-content-the-battery-polling-function" aria-label="Permalink: The Battery Polling Function" href="#the-battery-polling-function"></a></p>
<p dir="auto">The <code>PWCG()</code> method, called by multiple event types, is responsible for polling the battery and AC adapter status.</p>
<div dir="auto" data-snippet-clipboard-copy-content="Method (PWCG, 0, NotSerialized)
{
    Notify (ADP0, Zero)      // Tell OS to check the AC adapter
    ^BAT0._BST ()            // Execute the Battery Status method
    Notify (BAT0, 0x80)      // Tell OS the battery status has changed
    ^BAT0._BIF ()            // Execute the Battery Information method  
    Notify (BAT0, 0x81)      // Tell OS the battery info has changed
}"><pre><span>Method</span> (PWCG, <span>0</span>, <span>NotSerialized</span>)
{
    <span>Notify</span> (ADP0, <span>Zero</span>)      <span>// Tell OS to check the AC adapter</span>
    ^BAT0.<span>_BST</span> ()            <span>// Execute the Battery Status method</span>
    <span>Notify</span> (BAT0, <span>0x80</span>)      <span>// Tell OS the battery status has changed</span>
    ^BAT0.<span>_BIF</span> ()            <span>// Execute the Battery Information method  </span>
    <span>Notify</span> (BAT0, <span>0x81</span>)      <span>// Tell OS the battery info has changed</span>
}</pre></div>
<p dir="auto">Which we can see here:</p>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/76183331/490271565-f6c62050-b470-49bd-ad55-35def0fff893.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTgwOTQ1MDIsIm5iZiI6MTc1ODA5NDIwMiwicGF0aCI6Ii83NjE4MzMzMS80OTAyNzE1NjUtZjZjNjIwNTAtYjQ3MC00OWJkLWFkNTUtMzVkZWYwZmZmODkzLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA5MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwOTE3VDA3MzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWU4ZjJhNzA2Y2IzYzkyNjkxMWM5MmM1NGNhNWExNGU0ZWIwZDFhZGM4Mjk2ZDJmNDEzNTI3YWMxNzNlMTdlZmImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.s6ILMM49J7f9AMg_3DJD9ggUKUZiz0cWLYmuxFZ594o"><img width="1043" height="315" alt="image" src="https://private-user-images.githubusercontent.com/76183331/490271565-f6c62050-b470-49bd-ad55-35def0fff893.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTgwOTQ1MDIsIm5iZiI6MTc1ODA5NDIwMiwicGF0aCI6Ii83NjE4MzMzMS80OTAyNzE1NjUtZjZjNjIwNTAtYjQ3MC00OWJkLWFkNTUtMzVkZWYwZmZmODkzLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA5MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwOTE3VDA3MzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWU4ZjJhNzA2Y2IzYzkyNjkxMWM5MmM1NGNhNWExNGU0ZWIwZDFhZGM4Mjk2ZDJmNDEzNTI3YWMxNzNlMTdlZmImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.s6ILMM49J7f9AMg_3DJD9ggUKUZiz0cWLYmuxFZ594o"></a>
<p dir="auto">Each of these operations requires communication with the Embedded Controller, adding to the workload inside the already-stalled interrupt handler.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">The GPU Notification System</h3><a id="user-content-the-gpu-notification-system" aria-label="Permalink: The GPU Notification System" href="#the-gpu-notification-system"></a></p>
<p dir="auto">The <code>NOD2()</code> method sends notifications to the GPU driver.</p>
<div dir="auto" data-snippet-clipboard-copy-content="Method (NOD2, 1, Serialized)
{
    If ((Arg0 != DNOT))
    {
        DNOT = Arg0
        Notify (^^PEG1.PEGP, Arg0)
    }

    If ((ROCT == 0x55))
    {
        ROCT = Zero
        Notify (^^PEG1.PEGP, 0xD1) // Hardware-Specific
    }
}"><pre><span>Method</span> (NOD2, <span>1</span>, <span>Serialized</span>)
{
    <span>If</span> ((<span>Arg0</span> != DNOT))
    {
        DNOT = <span>Arg0</span>
        <span>Notify</span> (^^PEG1.PEGP, <span>Arg0</span>)
    }

    <span>If</span> ((ROCT == <span>0x55</span>))
    {
        ROCT = <span>Zero</span>
        <span>Notify</span> (^^PEG1.PEGP, <span>0xD1</span>) <span>// Hardware-Specific</span>
    }
}</pre></div>
<p dir="auto">These notifications (<code>0xD1</code>, <code>0xD2</code>, etc.) are hardware-specific signals that tell the NVIDIA driver to re-evaluate its power state, which is what triggers the futile <code>_PS0/_DOS/_PS3</code> power-cycling sequence seen in the traces.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">The Mux Mode Confusion: A Firmware with a Split Personality</h2><a id="user-content-the-mux-mode-confusion-a-firmware-with-a-split-personality" aria-label="Permalink: The Mux Mode Confusion: A Firmware with a Split Personality" href="#the-mux-mode-confusion-a-firmware-with-a-split-personality"></a></p>
<p dir="auto">Here's where a simple but catastrophic oversight in the firmware's logic causes system-wide failure. High-end ASUS gaming laptops feature a MUX (Multiplexer) switch, a piece of hardware that lets the user choose between two distinct graphics modes:</p>
<ol dir="auto">
<li><strong>Optimus Mode:</strong> The power-saving default. The integrated Intel GPU (iGPU) is physically connected to the display. The powerful NVIDIA GPU (dGPU) only renders demanding applications when needed, passing finished frames to the iGPU to be drawn on screen.</li>
<li><strong>Ultimate/Mux Mode:</strong> The high-performance mode. The MUX switch physically rewires the display connections, bypassing the iGPU entirely and wiring the NVIDIA dGPU directly to the screen. In this mode, the dGPU is not optional; it is the <strong>only</strong> graphics processor capable of outputting an image.</li>
</ol>
<p dir="auto">Any firmware managing this hardware <strong>must</strong> be aware of which mode the system is in. Sending a command intended for one GPU to the other is futile and, in some cases, dangerous. Deep within the ACPI code, a hardware status flag named <code>HGMD</code> is used to track this state. To understand the flaw, we first need to decipher what <code>HGMD</code> means, and the firmware itself gives us the key.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto"><strong>Decoding the Firmware's Logic with the Brightness Method</strong></h4><a id="user-content-decoding-the-firmwares-logic-with-the-brightness-method" aria-label="Permalink: Decoding the Firmware's Logic with the Brightness Method" href="#decoding-the-firmwares-logic-with-the-brightness-method"></a></p>
<p dir="auto">For screen brightness to work, the command must be sent to the GPU that is physically controlling the display backlight. A command sent to the wrong GPU will simply do nothing. Therefore, the brightness control method (<code>BRTN</code>) <em>must</em> be aware of the MUX switch state to function at all. It is the firmware's own Rosetta Stone.</p>
<div dir="auto" data-snippet-clipboard-copy-content="// Brightness control - CORRECTLY checks for mux mode
Method (BRTN, 1, Serialized)  // Line 034003
{
    If (((DIDX &amp; 0x0F0F) == 0x0400))
    {
        If (HGMD == 0x03)  // 0x03 = Ultimate/Mux mode
        {
            // In mux mode, notify discrete GPU
            Notify (\_SB.PC00.PEG1.PEGP.EDP1, Arg0)
        }
        Else
        {
            // In Optimus, notify integrated GPU
            Notify (\_SB.PC00.GFX0.DD1F, Arg0)
        }
    }
}"><pre><span>// Brightness control - CORRECTLY checks for mux mode</span>
<span>Method</span> (BRTN, <span>1</span>, <span>Serialized</span>)  <span>// Line 034003</span>
{
    <span>If</span> (((DIDX &amp; <span>0x0F0F</span>) == <span>0x0400</span>))
    {
        <span>If</span> (HGMD == <span>0x03</span>)  <span>// 0x03 = Ultimate/Mux mode</span>
        {
            <span>// In mux mode, notify discrete GPU</span>
            <span>Notify</span> (\<span>_SB</span>.PC00.PEG1.PEGP.EDP1, <span>Arg0</span>)
        }
        <span>Else</span>
        {
            <span>// In Optimus, notify integrated GPU</span>
            <span>Notify</span> (\<span>_SB</span>.PC00.GFX0.DD1F, <span>Arg0</span>)
        }
    }
}</pre></div>
<p dir="auto">The logic here is flawless and revealing. The code uses the <code>HGMD</code> flag to make a binary decision. If <code>HGMD</code> is <code>0x03</code>, it sends the command to the NVIDIA GPU. If not, it sends it to the Intel GPU. The firmware itself, through this correct implementation, provides the undeniable definition: <strong><code>HGMD == 0x03</code> means the system is in Ultimate/Mux Mode.</strong></p>
<p dir="auto"><h4 tabindex="-1" dir="auto"><strong>The Logical Contradiction: Unconditional Power Cycling in a Conditional Hardware State</strong></h4><a id="user-content-the-logical-contradiction-unconditional-power-cycling-in-a-conditional-hardware-state" aria-label="Permalink: The Logical Contradiction: Unconditional Power Cycling in a Conditional Hardware State" href="#the-logical-contradiction-unconditional-power-cycling-in-a-conditional-hardware-state"></a></p>
<p dir="auto">This perfect, platform-aware logic is completely abandoned in the critical code paths responsible for power management. The <code>LGPA</code> method, which is called by the stutter-inducing interrupt, dispatches power-related commands to the GPU <em>without ever checking the MUX mode</em>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="// GPU power notification - NO MUX CHECK!
Case (0x18)
{
    // This SHOULD have: If (HGMD != 0x03)
    // But it doesn't, so it runs even in mux mode
    If (M6EF == One)
    {
        Local0 = 0xD2
    }
    Else
    {
        Local0 = 0xD1
    }
    NOD2 (Local0)  // Notifies GPU regardless of mode
}"><pre><span>// GPU power notification - NO MUX CHECK!</span>
<span>Case</span> (<span>0x18</span>)
{
    <span>// This SHOULD have: If (HGMD != 0x03)</span>
    <span>// But it doesn't, so it runs even in mux mode</span>
    <span>If</span> (M6EF == <span>One</span>)
    {
        <span>Local0</span> = <span>0xD2</span>
    }
    <span>Else</span>
    {
        <span>Local0</span> = <span>0xD1</span>
    }
    NOD2 (<span>Local0</span>)  <span>// Notifies GPU regardless of mode</span>
}</pre></div>
<p dir="auto">This is the bug. The firmware, despite proving it knows how to check the MUX state, forgets to do so here. It blindly sends a power-management notification to the NVIDIA driver, instructing it to change power states. In MUX mode, this command is nonsensical it's asking the driver to power down the only GPU that is keeping the screen on. This triggers the futile power-cycling, the massive latency spikes, and the system instability.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Another Path to the Same Problem: The Platform Power Management DSM</h3><a id="user-content-another-path-to-the-same-problem-the-platform-power-management-dsm" aria-label="Permalink: Another Path to the Same Problem: The Platform Power Management DSM" href="#another-path-to-the-same-problem-the-platform-power-management-dsm"></a></p>
<p dir="auto">This is not a single typo. A second, parallel power management system in the firmware exhibits the exact same flaw. The Platform Extension Plug-in Device (<code>PEPD</code>) is used by Windows to manage system-wide power states, such as turning off displays during modern standby.</p>
<div dir="auto" data-snippet-clipboard-copy-content="Device (PEPD)  // Line 071206
{
    Name (_HID, &quot;INT33A1&quot;)  // Intel Power Engine Plugin
    
    Method (_DSM, 4, Serialized)  // Device Specific Method
    {
        // ... lots of setup code ...
        
        // Arg2 == 0x05: &quot;All displays have been turned off&quot;
        If ((Arg2 == 0x05))
        {
            // Prepare for aggressive power saving
            If (CondRefOf (\_SB.PC00.PEG1.DHDW))
            {
                ^^PC00.PEG1.DHDW ()         // GPU pre-shutdown work
                ^^PC00.PEG1.DGCE = One      // Set &quot;GPU Cut Enable&quot; flag
            }
            
            If (S0ID == One)  // If system supports S0 idle
            {
                GUAM (One)    // Enter low power mode
            }
            
            ^^PC00.DPOF = One  // Display power off flag
            
            // Tell USB controller about display state
            If (CondRefOf (\_SB.PC00.XHCI.PSLI))
            {
                ^^PC00.XHCI.PSLI (0x05)
            }
        }
        
        // Arg2 == 0x06: &quot;A display has been turned on&quot;
        If ((Arg2 == 0x06))
        {
            // Wake everything back up
            If (CondRefOf (\_SB.PC00.PEG1.DGCE))
            {
                ^^PC00.PEG1.DGCE = Zero     // Clear &quot;GPU Cut Enable&quot;
            }
            
            If (S0ID == One)
            {
                GUAM (Zero)   // Exit low power mode
            }
            
            ^^PC00.DPOF = Zero  // Display power on flag
            
            If (CondRefOf (\_SB.PC00.XHCI.PSLI))
            {
                ^^PC00.XHCI.PSLI (0x06)
            }
        }
    }
}"><pre><span>Device</span> (PEPD)  <span>// Line 071206</span>
{
    <span>Name</span> (<span>_HID</span>, <span>"INT33A1"</span>)  <span>// Intel Power Engine Plugin</span>
    
    <span>Method</span> (<span>_DSM</span>, <span>4</span>, <span>Serialized</span>)  <span>// Device Specific Method</span>
    {
        <span>// ... lots of setup code ...</span>
        
        <span>// Arg2 == 0x05: "All displays have been turned off"</span>
        <span>If</span> ((<span>Arg2</span> == <span>0x05</span>))
        {
            <span>// Prepare for aggressive power saving</span>
            <span>If</span> (<span>CondRefOf</span> (\<span>_SB</span>.PC00.PEG1.DHDW))
            {
                ^^PC00.PEG1.DHDW ()         <span>// GPU pre-shutdown work</span>
                ^^PC00.PEG1.DGCE = <span>One</span>      <span>// Set "GPU Cut Enable" flag</span>
            }
            
            <span>If</span> (S0ID == <span>One</span>)  <span>// If system supports S0 idle</span>
            {
                GUAM (<span>One</span>)    <span>// Enter low power mode</span>
            }
            
            ^^PC00.DPOF = <span>One</span>  <span>// Display power off flag</span>
            
            <span>// Tell USB controller about display state</span>
            <span>If</span> (<span>CondRefOf</span> (\<span>_SB</span>.PC00.XHCI.PSLI))
            {
                ^^PC00.XHCI.PSLI (<span>0x05</span>)
            }
        }
        
        <span>// Arg2 == 0x06: "A display has been turned on"</span>
        <span>If</span> ((<span>Arg2</span> == <span>0x06</span>))
        {
            <span>// Wake everything back up</span>
            <span>If</span> (<span>CondRefOf</span> (\<span>_SB</span>.PC00.PEG1.DGCE))
            {
                ^^PC00.PEG1.DGCE = <span>Zero</span>     <span>// Clear "GPU Cut Enable"</span>
            }
            
            <span>If</span> (S0ID == <span>One</span>)
            {
                GUAM (<span>Zero</span>)   <span>// Exit low power mode</span>
            }
            
            ^^PC00.DPOF = <span>Zero</span>  <span>// Display power on flag</span>
            
            <span>If</span> (<span>CondRefOf</span> (\<span>_SB</span>.PC00.XHCI.PSLI))
            {
                ^^PC00.XHCI.PSLI (<span>0x06</span>)
            }
        }
    }
}</pre></div>
<p dir="auto">Once again, the firmware prepares to cut power to the discrete GPU without first checking if it's the only GPU driving the displays. This demonstrates that the Mux Mode Confusion is a systemic design flaw. The firmware is internally inconsistent, leading it to issue self-destructive commands that try to cripple the system.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Cross-System Analysis</h2><a id="user-content-cross-system-analysis" aria-label="Permalink: Cross-System Analysis" href="#cross-system-analysis"></a></p>
<p dir="auto">Traces from multiple ASUS gaming laptop models confirm this is not an isolated issue.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Scar 15 Analysis</h4><a id="user-content-scar-15-analysis" aria-label="Permalink: Scar 15 Analysis" href="#scar-15-analysis"></a></p>
<ul dir="auto">
<li><strong>Trace Duration:</strong> 4.1 minutes</li>
<li><strong><code>_GPE._L02</code> Events:</strong> 7</li>
<li><strong>Avg. GPE Duration:</strong> 1.56ms (lower, but still unacceptably high)</li>
<li><strong>Avg. Interval:</strong> 39.4 seconds (nearly identical periodic nature)</li>
<li><strong>GPU Power Cycles:</strong> 8</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">Zephyrus M16 Analysis</h4><a id="user-content-zephyrus-m16-analysis" aria-label="Permalink: Zephyrus M16 Analysis" href="#zephyrus-m16-analysis"></a></p>
<ul dir="auto">
<li><strong>Trace Duration:</strong> 19.9 minutes</li>
<li><strong><code>_GPE._L02</code> Events:</strong> 3</li>
<li><strong>Avg. GPE Duration:</strong> 2.94ms</li>
<li><strong>GPU Power Cycles:</strong> 197 (far more frequent)</li>
<li><strong>ASUS WMI Calls:</strong> 2,370 (a massive number, indicating software amplification)</li>
</ul>
<p dir="auto">Microsoft has a built-in "smooth video" check. It plays HD video in full screen and watches for hiccups. If the PC drops frames, crackles, or any driver pauses for more than a few milliseconds, it fails. That’s Microsoft’s baseline for what "smooth" should look like.</p>
<p dir="auto">Why it matters here:</p>
<p dir="auto">ASUS firmware is causing millisecond-long pauses. Those pauses are exactly the kind that make this test fail i.e., the same stutters and audio pops regular users notice on YouTube/Netflix and games; this firmware violates fundemental standards.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">The Universal Pattern</h3><a id="user-content-the-universal-pattern" aria-label="Permalink: The Universal Pattern" href="#the-universal-pattern"></a></p>
<p dir="auto">Despite being different models, all affected systems exhibit the same core flaws:</p>
<ol dir="auto">
<li><code>_GPE._L02</code> handlers take milliseconds to execute instead of microseconds.</li>
<li>The GPEs trigger unnecessary battery polling.</li>
<li>The firmware attempts to power cycle the GPU while in a fixed MUX mode.</li>
<li>The entire process is driven by a periodic, timer-like trigger.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Summarizing the Findings</h2><a id="user-content-summarizing-the-findings" aria-label="Permalink: Summarizing the Findings" href="#summarizing-the-findings"></a></p>
<p dir="auto">This bug is a cascade of firmware design failures.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Root Cause 1: The Misunderstanding of Interrupt Context</h3><a id="user-content-root-cause-1-the-misunderstanding-of-interrupt-context" aria-label="Permalink: Root Cause 1: The Misunderstanding of Interrupt Context" href="#root-cause-1-the-misunderstanding-of-interrupt-context"></a></p>
<p dir="auto">The firmware's <code>ECLV()</code> method treats a high-priority interrupt handler like a standard application thread, which is fundamentally incorrect.</p>
<p dir="auto"><strong>What ASUS Wrote:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="Method (ECLV, 0, NotSerialized)
{
    While (events_exist)
    {
        process_event();
        Sleep(100);         // FATAL FLAW: This blocks the entire CPU core.
        check_timers();
    }
}"><pre><span>Method</span> (ECLV, <span>0</span>, <span>NotSerialized</span>)
{
    <span>While</span> (events_exist)
    {
        process_event();
        <span>Sleep</span>(100);         <span>// FATAL FLAW: This blocks the entire CPU core.</span>
        check_timers();
    }
}</pre></div>
<p dir="auto"><strong>What It Should Be:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="Method (ECLV, 0, NotSerialized)
{
    // Acknowledge interrupt, queue work, and exit immediately.
    Local0 = EC0.EEV0;      // Read event source
    EC0.EEV0 = 0;           // Clear the event source
    QueueWorkForLater(Local0); // Queue a low-priority task
    Return;                 // Exit the handler in microseconds.
}"><pre><span>Method</span> (ECLV, <span>0</span>, <span>NotSerialized</span>)
{
    <span>// Acknowledge interrupt, queue work, and exit immediately.</span>
    <span>Local0</span> = EC0.EEV0;      <span>// Read event source</span>
    EC0.EEV0 = <span>0</span>;           <span>// Clear the event source</span>
    QueueWorkForLater(<span>Local0</span>); <span>// Queue a low-priority task</span>
    <span>Return</span>;                 <span>// Exit the handler in microseconds.</span>
}</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Root Cause 2: Flawed Interrupt Handling</h3><a id="user-content-root-cause-2-flawed-interrupt-handling" aria-label="Permalink: Root Cause 2: Flawed Interrupt Handling" href="#root-cause-2-flawed-interrupt-handling"></a></p>
<p dir="auto">The firmware artificially re-arms the interrupt, creating an endless loop of GPEs instead of clearing the source and waiting for the next legitimate hardware event. This transforms a hardware notification system into a disruptive, periodic timer.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Root Cause 3: Lack of Platform Awareness</h3><a id="user-content-root-cause-3-lack-of-platform-awareness" aria-label="Permalink: Root Cause 3: Lack of Platform Awareness" href="#root-cause-3-lack-of-platform-awareness"></a></p>
<p dir="auto">The code that sends GPU power notifications does not check if the system is in MUX mode, a critical state check that is correctly performed in other parts of the firmware. This demonstrates inconsistency and a lack of quality control.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Timeline of User Reports</h2><a id="user-content-timeline-of-user-reports" aria-label="Permalink: Timeline of User Reports" href="#timeline-of-user-reports"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">The Three-Year Pattern</h3><a id="user-content-the-three-year-pattern" aria-label="Permalink: The Three-Year Pattern" href="#the-three-year-pattern"></a></p>
<p dir="auto">This issue is not new or isolated. User reports documenting identical symptoms with high ACPI.sys DPC latency, periodic stuttering, and audio crackling have been accumulating since at least 2021 across ASUS's entire gaming laptop lineup.</p>
<p dir="auto"><strong>August 2021: The First Major Reports</strong><br>
The earliest documented cases appear on the official ASUS ROG forums. A G15 Advantage Edition (G513QY) owner reports <a href="https://rog-forum.asus.com/t5/rog-strix-series/g15-advantage-edition-g513qy-severe-dpc-latency-audio-dropouts/m-p/809512" rel="nofollow">"severe DPC latency from ACPI.sys"</a> with audio dropouts occurring under any load condition. The thread, last edited in March 2024, shows the issue remains unresolved after nearly three years.</p>
<p dir="auto">Reddit users simultaneously report <a href="https://www.reddit.com/r/ASUS/comments/odprtv/high_dpc_latency_from_acpisys_can_be_caused_by/" rel="nofollow">identical ACPI.sys latency problems</a> alongside NVIDIA driver issues; the exact symptoms described in this investigation.</p>
<p dir="auto"><strong>2021-2023: Spreading Across Models</strong><br>
Throughout this period, the issue proliferates across ASUS's gaming lineup:</p>
<ul dir="auto">
<li><a href="https://www.reddit.com/r/techsupport/comments/mxtm86/i_need_help_high_acpisys_latency_and_microstutters/" rel="nofollow">ROG Strix models experience micro-stutters</a></li>
<li><a href="https://www.reddit.com/r/Asustuf/comments/1m2e40v/my_laptop_throttling_for_few_seconds/" rel="nofollow">TUF Gaming series reports throttling for seconds at a time</a></li>
<li><a href="https://www.reddit.com/r/techsupport/comments/17rqfq5/new_laptop_started_stuttering_every_45_seconds/" rel="nofollow">G18 models exhibit the characteristic 45-second periodic stuttering</a></li>
</ul>
<p dir="auto"><strong>2023-2024: The Problem Persists in New Models</strong><br>
Even the latest generations aren't immune:</p>
<ul dir="auto">
<li><a href="https://www.reddit.com/r/ZephyrusM16/comments/1j33ld6/this_machine_has_been_nothing_but_problems_no/" rel="nofollow">2023 Zephyrus G16 owners report persistent audio issues</a></li>
<li><a href="https://www.reddit.com/r/ZephyrusG14/comments/1l4jb13/audio_popscrackles_on_zephyrus_g16_2023/" rel="nofollow">2023 G16 models continue experiencing audio pops/crackles</a></li>
<li><a href="https://www.reddit.com/r/ZephyrusG14/comments/1i2w9ah/resolving_audio_popsstuttering_on_2024_intel_g16/" rel="nofollow">2024 Intel G16 models require workarounds for audio stuttering</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Conclusion</h2><a id="user-content-conclusion" aria-label="Permalink: Conclusion" href="#conclusion"></a></p>
<p dir="auto">The evidence is undeniable:</p>
<ul dir="auto">
<li><strong>Measured Proof:</strong> GPE handlers are measured blocking a CPU core for over 13 milliseconds.</li>
<li><strong>Code Proof:</strong> The decompiled firmware explicitly contains <code>Sleep()</code> calls within an interrupt handler.</li>
<li><strong>Logical Proof:</strong> The code lacks critical checks for the laptop's hardware state (MUX mode).</li>
<li><strong>Systemic Proof:</strong> The issue is reproducible across different models and BIOS versions.</li>
</ul>
<p dir="auto">Until a fix is implemented, millions of buyers of Asus laptops from approx. 2021 to present day are facing stutters on the simplest of tasks, such as watching YouTube, for the simple mistake of using a sleep call inside of an inefficient interrupt handler and not checking the GPU environment properly.</p>
<p dir="auto">The code is there. The traces prove it. ASUS must fix its firmware.</p>
<blockquote>
<p dir="auto">ASUS has not responded to this investigation or the documented firmware issues at the time of publication, will update this if anything changes.</p>
</blockquote>
<hr>
<p dir="auto"><em>Investigation conducted using the Windows Performance Toolkit, ACPI table extraction tools, and Intel ACPI Component Architecture utilities. All code excerpts are from official ASUS firmware. Traces were captured on multiple affected systems, all showing consistent behavior.</em></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GNU Midnight Commander (472 pts)]]></title>
            <link>https://midnight-commander.org/</link>
            <guid>45271481</guid>
            <pubDate>Wed, 17 Sep 2025 03:54:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://midnight-commander.org/">https://midnight-commander.org/</a>, See on <a href="https://news.ycombinator.com/item?id=45271481">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-md-component="container">
      
      
        
          
        
      
      <main data-md-component="main">
        <div data-md-component="content">
              <article>
                


                  


  
  


<h2 id="welcome-to-midnight-commander">Welcome to Midnight Commander</h2>
<p><a href="https://ftp.osuosl.org/pub/midnightcommander/?C=N;O=D"><img alt="GitHub Tag" src="https://img.shields.io/github/v/tag/MidnightCommander/mc?label=latest%20release"></a>
<a href="https://github.com/MidnightCommander/mc/blob/master/doc/COPYING"><img alt="License" src="https://img.shields.io/badge/license-GPLv3+-blue"></a>
<a href="https://github.com/MidnightCommander/mc"><img alt="GitHub top language" src="https://img.shields.io/github/languages/top/MidnightCommander/mc"></a>
<a href="https://github.com/MidnightCommander/mc/issues"><img alt="GitHub Issues" src="https://img.shields.io/github/issues/MidnightCommander/mc"></a>
<a href="https://github.com/MidnightCommander/mc/pulls"><img alt="GitHub Pull Requests" src="https://img.shields.io/github/issues-pr/MidnightCommander/mc"></a>
<a href="https://github.com/MidnightCommander/mc/actions/workflows/ci.yml"><img alt="GitHub Actions CI" src="https://github.com/MidnightCommander/mc/actions/workflows/ci.yml/badge.svg"></a></p>
<p>GNU Midnight Commander (or <code>mc</code>) is a visual, dual-pane file manager. It is released under the GNU General Public License and therefore qualifies as Free Software.</p>
<p>Midnight Commander is a feature-rich, full-screen, text-mode application that allows you to copy, move, and delete files and entire directory trees, search for files, and execute commands in the subshell. Internal viewer, editor and diff viewer are included.</p>
<p><code>mc</code> uses versatile text interface libraries such as <a href="https://invisible-island.net/ncurses/">ncurses</a> or <a href="https://www.jedsoft.org/slang/">S-Lang</a>, which allows it to work on a regular console, inside an X Window terminal, over <code>ssh</code> connections, and in all kinds of remote shells.</p>
<p><img alt="Midnight Commander screenshot" src="https://midnight-commander.org/img/mc-screenshot-cropped.png"></p>
<h2 id="installation">Installation</h2>
<p>The easiest way to install <code>mc</code> is to use your system package manager:</p>
<div data-tabs="1:4"><p><label for="__tabbed_1_1">Debian / Ubuntu</label><label for="__tabbed_1_2">Fedora / Red Hat</label><label for="__tabbed_1_3">FreeBSD</label><label for="__tabbed_1_4">macOS</label></p>
<div><pre><span></span><code>% brew install midnight-commander
</code></pre></div>
</div>
<p>Our <a href="https://ftp.osuosl.org/pub/midnightcommander/?C=N;O=D">source releases</a> are kindly mirrored by OSU OSL. Our <a href="https://github.com/MidnightCommander/mc">canonical repository</a> is hosted on GitHub. See the <a href="https://midnight-commander.org/source-code/">Source code</a> page for details.</p>
<h2 id="documentation">Documentation</h2>
<p>The primary way to learn about <code>mc</code> is to use the context-sensitive online help available via <span><kbd>F1</kbd></span>.</p>
<p>We also have extensive manual pages, which are the primary source of official documentation:</p>

<h2 id="color-schemes">Color schemes</h2>
<p>Midnight Commander supports theming! Check out the skins that come with the distribution or develop your own:</p>
<ul>
<li><a href="https://skins.midnight-commander.org/">https://skins.midnight-commander.org</a></li>
</ul>
<h2 id="contributing-support">Contributing &amp; support</h2>
<ul>
<li>For support, see the <a href="https://midnight-commander.org/communication/">Communication</a> page.</li>
<li>To contribute to <code>mc</code>, proceed to the <a href="https://midnight-commander.org/source-code/">"Development" section</a>.</li>
<li>Release notes for the development version are collected on the <a href="https://github.com/MidnightCommander/mc/wiki">wiki</a>.</li>
</ul>







  
    
  
  
    
  


  





                



<ins data-ad-client="ca-pub-1116512972967230" data-ad-slot="3684429103" data-ad-format="auto" data-full-width-responsive="true"></ins>




              </article>
            </div>
        
      </main>
      
        
      
    </div></div>]]></description>
        </item>
    </channel>
</rss>