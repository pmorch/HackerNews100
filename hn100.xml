<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 31 Jan 2025 07:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Mini-R1: Reproduce DeepSeek R1 "Aha Moment" (130 pts)]]></title>
            <link>https://www.philschmid.de/mini-deepseek-r1</link>
            <guid>42884834</guid>
            <pubDate>Fri, 31 Jan 2025 05:03:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.philschmid.de/mini-deepseek-r1">https://www.philschmid.de/mini-deepseek-r1</a>, See on <a href="https://news.ycombinator.com/item?id=42884834">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>The release of Deepseek R1 shocked the industry. Why? Well, DeepSeek-R1 is an open model that rivals OpenAI's o1 in complex reasoning tasks, introduced using Group Relative Policy Optimization (GRPO) and RL-focused multi-stage training approach. They not only released the model, but also a research paper on how they did it.</p>
<p>In the <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2501.12948">paper</a> they described an "aha moment" when using pure RL to train the model. During this phase, DeepSeek-R1-Zero (the first test of DeepSeek-R1) learns to allocate more thinking time to a problem by reevaluating its initial approach without any human feedback or data describing how to do it.  They describe this as an "aha moment" as:</p>
<blockquote>
<p>This behavior is not only a testament to the model’s growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes.</p>
</blockquote>
<p>In this blog post we want to recreate the small "aha moment" of DeepSeek-R1 using Group Relative Policy Optimization (GRPO) and the Countdown Game. We will train an open model using reinforcement learning trying to teach it self-verification and search abilities all on its own to solve the Countdown Game.
The Countdown game is a numbers puzzle where players use a set of randomly drawn numbers and basic arithmetic operations (+, -, ×, ÷) to reach or get as close as possible to a target number.</p>
<div><pre><code>Target Number: 952
Available Numbers: 25, 50, 75, 100, 3, 6

(100 × (3 × 3)) + (50 + 6 / 3) = 952
</code></pre></div>
<p>The blog post includes an interactive code which you can run in a Jupyter Notebook on how to train a model using GRPO and Q-Lora. This is a great way to learn how to use TRL and GRPO, but it is very slow and requires a lot of compute. Additionally, I added a <a target="_blank" rel="noopener noreferrer" href="https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/scripts/run_r1_grpo.py">script</a> and instructions to run the training on Node with multiple GPUs or a SLURM cluster.</p>
<ol>
<li><a href="#1-setup-the-development-environment">Setup the development environment</a></li>
<li><a href="#2-generate-training-samples-with-reasoning-prefix-from-the-countdown-game">Generate training samples with reasoning prefix from the Countdown Game</a></li>
<li><a href="#3-train-the-model-using-grpo-educational-part">Train the model using GRPO (Educational part)</a></li>
<li><a href="#4-distributed-training-example-for-grpo-using-deepspeed-and-vllm">Distributed Training example for GRPO using Deepspeed and vLLM</a></li>
<li><a href="#5-results-and-training-observations">Results and Training Observations</a></li>
</ol>
<p><em>Note: This blog is inspired by <a target="_blank" rel="noopener noreferrer" href="https://x.com/jiayi_pirate/status/1882839370505621655">Jiayi Pan</a> who initially explored the idea and proofed it with a small model.</em></p>
<p>But Before we start, let's take a look at the <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2402.03300">Group Relative Policy Optimization (GRPO)</a> and understand how it works.</p>
<p><strong>Group Relative Policy Optimization (GRPO)</strong></p>
<p>Group Relative Policy Optimization (GRPO) is a reinforcement learning algorithm to improve the reasoning capabilities of LLMs. It was introduced in the <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2402.03300">DeepSeekMath</a> paper in the context of mathematical reasoning. GRPO modifies the traditional Proximal Policy Optimization (PPO) by eliminating the need for a value function model. Instead, it estimates baselines from group scores, reducing memory usage and computational overhead. GRPO, now also used by the Qwen team, can be used with rule/binary-based Rewards as well as General Reward Models to improve models on helpfulness.</p>
<ol>
<li><strong>Sampling</strong>: Generate multiple outputs for each prompt using the current policy</li>
<li><strong>Reward Scoring</strong>: Each generation is scored using a reward function, could be (rule-based or outcome-based)</li>
<li><strong>Advantage Calculation</strong>: The average reward of the generated outputs is used as a baseline. The advantage of each solution within the group is then computed relative to this baseline. The reward is normalized within a group.</li>
<li><strong>Policy Optimization</strong>: The policy tries to maximize the GRPO objective, which includes the calculated advantages and a KL divergence term. This is different from how PPO implements the KL term within the reward.</li>
</ol>
<p><img src="https://www.philschmid.de/static/blog/mini-deepseek-r1/grpo.png" alt="grpo.png"></p>
<h2 id="1-setup-the-development-environment">1. Setup the development environment</h2>
<p>Our first step is to install Hugging Face Libraries and Pytorch, vllm, and trl, transformers and datasets. If you haven't heard of trl yet, don't worry. It is a new library on top of transformers and datasets, which makes it easier to fine-tune, rlhf, align open LLMs.</p>
<figure data-rehype-pretty-code-figure=""><div><pre><code data-language="python" data-theme="github-dark github-light"><span data-line=""><span># Install Pytorch &amp; other libraries, make sure to match your GPU driver version</span></span>
<span data-line=""><span>%</span><span>pip install </span><span>"torch==2.5.1"</span><span> tensorboard </span><span>"setuptools&lt;71.0.0"</span><span>  --</span><span>index</span><span>-</span><span>url https:</span><span>//</span><span>download.pytorch.org</span><span>/</span><span>whl</span><span>/</span><span>cu121</span></span>
<span data-line=""> </span>
<span data-line=""><span># Install flash-attn</span></span>
<span data-line=""><span>%</span><span>pip install flash</span><span>-</span><span>attn </span></span>
<span data-line=""> </span>
<span data-line=""><span># Install Hugging Face libraries</span></span>
<span data-line=""><span>%</span><span>pip install  </span><span>--</span><span>upgrade \</span></span>
<span data-line=""><span>  "transformers==4.48.1"</span><span> \</span></span>
<span data-line=""><span>  "datasets==3.1.0"</span><span> \</span></span>
<span data-line=""><span>  "accelerate==1.3.0"</span><span> \</span></span>
<span data-line=""><span>  "hf-transfer==0.1.9"</span><span> \</span></span>
<span data-line=""><span>  "deepspeed==0.15.4"</span><span> \</span></span>
<span data-line=""><span>  "trl==0.14.0"</span></span>
<span data-line=""> </span>
<span data-line=""><span># install vLLM </span></span>
<span data-line=""><span>%</span><span>pip install </span><span>"vllm==0.7.0"</span></span>
<span data-line=""> </span>
<span data-line=""><span>## IMPORTANT: If you want to run the notebook and the interactive cells you also need to install the following libraries:</span></span>
<span data-line=""><span># But first read it the blog post and then decide as they might conflict with the libraries for distributed training. </span></span>
<span data-line=""><span># %pip install "peft==0.14.0" "bitsandbytes==0.45.0"</span></span>
<span data-line=""> </span></code></pre></div></figure>
<p><em>Note: you may need to restart the kernel to use updated packages.</em></p>
<p>We will use the <a target="_blank" rel="noopener noreferrer" href="https://huggingface.co/models">Hugging Face Hub</a> as a remote model versioning service. This means we will automatically push our model, logs and information to the Hub during training. You must register on the <a target="_blank" rel="noopener noreferrer" href="https://huggingface.co/join">Hugging Face</a> for this. After you have an account, we will use the <code>login</code> util from the <code>huggingface_hub</code> package to log into our account and store our token (access key) on the disk.</p>
<figure data-rehype-pretty-code-figure=""><div><pre><code data-language="python" data-theme="github-dark github-light"><span data-line=""><span>from</span><span> huggingface_hub </span><span>import</span><span> login</span></span>
<span data-line=""> </span>
<span data-line=""><span>login(</span><span>token</span><span>=</span><span>""</span><span>, </span><span>add_to_git_credential</span><span>=</span><span>True</span><span>) </span><span># ADD YOUR TOKEN HERE</span></span></code></pre></div></figure>
<h2 id="2-generate-training-samples-with-reasoning-prefix-from-the-countdown-game">2. Generate training samples with reasoning prefix from the Countdown Game</h2>
<p>We are going to use the <a target="_blank" rel="noopener noreferrer" href="https://huggingface.co/datasets/Jiayi-Pan/Countdown-Tasks-3to4">Jiayi-Pan/Countdown-Tasks-3to4</a> dataset, which contains samples with 3 to 4 numbers and solutions.</p>
<p>As Model we are going to use <a target="_blank" rel="noopener noreferrer" href="https://huggingface.co/Qwen/Qwen2.5-3B-Instruct">Qwen/Qwen2.5-3B-Instruct</a> which is a 3B parameter instruction tuned model. This makes it easier to showcase the "aha moment" as it already follows the prompt format. But you can use the base version of Qwen or other models as well. <a target="_blank" rel="noopener noreferrer" href="https://x.com/jiayi_pirate/status/1882839487417561307">Jiayi-Pan</a> explored that the model needs to have a certain quality to be able to learn the reasoning process, starting with &gt; 1.5B parameters.</p>
<figure data-rehype-pretty-code-figure=""><div><pre><code data-language="python" data-theme="github-dark github-light"><span data-line=""><span>from</span><span> transformers </span><span>import</span><span> AutoTokenizer</span></span>
<span data-line=""><span>from</span><span> datasets </span><span>import</span><span> load_dataset</span></span>
<span data-line=""> </span>
<span data-line=""><span># Load dataset from Hugging Face Hub</span></span>
<span data-line=""><span>dataset_id </span><span>=</span><span> "Jiayi-Pan/Countdown-Tasks-3to4"</span></span>
<span data-line=""><span>dataset </span><span>=</span><span> load_dataset(dataset_id, </span><span>split</span><span>=</span><span>"train"</span><span>)</span></span>
<span data-line=""><span># select a random subset of 50k samples</span></span>
<span data-line=""><span>dataset </span><span>=</span><span> dataset.shuffle(</span><span>seed</span><span>=</span><span>42</span><span>).select(</span><span>range</span><span>(</span><span>50000</span><span>))</span></span>
<span data-line=""> </span>
<span data-line=""><span># Load tokenizer from Hugging Face Hub to format the dataset to our "r1" prompt </span></span>
<span data-line=""><span>tokenizer </span><span>=</span><span> AutoTokenizer.from_pretrained(</span><span>"Qwen/Qwen2.5-3B-Instruct"</span><span>)</span></span>
<span data-line=""> </span>
<span data-line=""><span># gemerate r1 prompt with a prefix for the model to already start with the thinking process</span></span>
<span data-line=""><span>def</span><span> generate_r1_prompt</span><span>(numbers, target):</span></span>
<span data-line=""><span>    r1_prefix </span><span>=</span><span> [{</span></span>
<span data-line=""><span>        "role"</span><span>: </span><span>"system"</span><span>,</span></span>
<span data-line=""><span>        "content"</span><span>: </span><span>"You are a helpful assistant. You first thinks about the reasoning process in the mind and then provides the user with the answer."</span></span>
<span data-line=""><span>      },</span></span>
<span data-line=""><span>      { </span></span>
<span data-line=""><span>        "role"</span><span>: </span><span>"user"</span><span>,</span></span>
<span data-line=""><span>        "content"</span><span>: </span><span>f</span><span>"Using the numbers </span><span>{</span><span>numbers</span><span>}</span><span>, create an equation that equals </span><span>{</span><span>target</span><span>}</span><span>. You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. Show your work in &lt;think&gt; &lt;/think&gt; tags. And return the final equation and answer in &lt;answer&gt; &lt;/answer&gt; tags, for example &lt;answer&gt; (1 + 2) / 3 = 1 &lt;/answer&gt;."</span></span>
<span data-line=""><span>      },</span></span>
<span data-line=""><span>      {</span></span>
<span data-line=""><span>        "role"</span><span>: </span><span>"assistant"</span><span>,</span></span>
<span data-line=""><span>        "content"</span><span>: </span><span>"Let me solve this step by step.</span><span>\n</span><span>&lt;think&gt;"</span></span>
<span data-line=""><span>      }]</span></span>
<span data-line=""><span>    return</span><span> {</span><span>"prompt"</span><span>: tokenizer.apply_chat_template(r1_prefix, </span><span>tokenize</span><span>=</span><span>False</span><span>, </span><span>continue_final_message</span><span>=</span><span>True</span><span>), </span><span>"target"</span><span>: target}</span></span>
<span data-line=""> </span>
<span data-line=""><span># convert our dataset to the r1 prompt</span></span>
<span data-line=""><span>dataset </span><span>=</span><span> dataset.map(</span><span>lambda</span><span> x: generate_r1_prompt(x[</span><span>"nums"</span><span>], x[</span><span>"target"</span><span>]))</span></span>
<span data-line=""> </span>
<span data-line=""><span># split the dataset into train and test</span></span>
<span data-line=""><span>train_test_split </span><span>=</span><span> dataset.train_test_split(</span><span>test_size</span><span>=</span><span>0.1</span><span>)</span></span>
<span data-line=""> </span>
<span data-line=""><span>train_dataset </span><span>=</span><span> train_test_split[</span><span>"train"</span><span>]</span></span>
<span data-line=""><span>test_dataset </span><span>=</span><span> train_test_split[</span><span>"test"</span><span>]</span></span></code></pre></div></figure>
<h2 id="3-train-the-model-using-grpo-educational-part">3. Train the model using GRPO (Educational part)</h2>
<p><em>Note: Section 3 is shows the basic on how to use TRL and GRPO. If you want to run the interactive cells you need to install <code>bitsandbytes</code> and <code>peft</code> as they are required for the <code>Trainer</code> class. This section is mostly for educational purposes.</em></p>
<p>TRL supports Group Relative Policy Optimization (GRPO) through a dedicated <a target="_blank" rel="noopener noreferrer" href="https://huggingface.co/docs/trl/main/en/grpo_trainer">GRPOTrainer</a> for aligning LLMs from preference data, as described in <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2402.03300">DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models</a>. The <code>GRPOTrainer</code> is a subclass of the <code>Trainer</code> from the <code>transformers</code> library and supports all the same features, including logging, checkpointing, distributed training, and parameter efficient fine-tuning (PEFT).</p>
<p>The <code>GRPOTrainer</code> supports generic Outcome Reward Models (ORM) and custom reward functions, that can be used to implement Rule-Based Reward Models. In the Deepseek R1 paper they implemented Rule-Based Reward Models to verify the correctness of the generated solutions. In our exmaple we are going to do a similar approach, where we will create 2 reward functions that:</p>
<ol>
<li><strong>Format Reward</strong>: Checks if the generated format is correct <code>&lt;think&gt; [thinking] &lt;/think&gt;&lt;answer&gt; [answer] &lt;/answer&gt;</code></li>
<li><strong>Accuracy Reward</strong>: Extracts the equation from the <code>&lt;answer&gt;</code> tag and evaluates it against the target and if every number is used once.</li>
</ol>
<p><em>Note: Correct <code>&lt;answer&gt;</code> in our example includes the equation, for example <code>&lt;answer&gt; 55 + 36 - 7 - 19 &lt;/answer&gt;</code></em></p>
<figure data-rehype-pretty-code-figure=""><div><pre><code data-language="python" data-theme="github-dark github-light"><span data-line=""><span>import</span><span> re</span></span>
<span data-line=""> </span>
<span data-line=""><span>def</span><span> format_reward_func</span><span>(completions, target, </span><span>**</span><span>kwargs):</span></span>
<span data-line=""><span>    """</span></span>
<span data-line=""><span>    Format: &lt;think&gt;...&lt;/think&gt;&lt;answer&gt;...&lt;/answer&gt;</span></span>
<span data-line=""><span>    Args:</span></span>
<span data-line=""><span>        completions (list[str]): Generated outputs</span></span>
<span data-line=""><span>        target (list[str]): Expected answers</span></span>
<span data-line=""><span>      </span></span>
<span data-line=""><span>      Returns:</span></span>
<span data-line=""><span>          list[float]: Reward scores</span></span>
<span data-line=""><span>    """</span></span>
<span data-line=""><span>    rewards </span><span>=</span><span> []</span></span>
<span data-line=""> </span>
<span data-line=""><span>    for</span><span> completion, gt </span><span>in</span><span> zip</span><span>(completions, target):</span></span>
<span data-line=""> </span>
<span data-line=""><span>      try</span><span>:</span></span>
<span data-line=""><span>        # add synthetic &lt;think&gt; as its already part of the prompt and prefilled for the assistant to more easily match the regex</span></span>
<span data-line=""><span>        completion </span><span>=</span><span> "&lt;think&gt;"</span><span> +</span><span> completion        </span></span>
<span data-line=""><span>        # Check if the format is correct</span></span>
<span data-line=""><span>        regex </span><span>=</span><span> r</span><span>"</span><span>^</span><span>&lt;think&gt;</span><span>([</span><span>^</span><span>&lt;]</span><span>*</span><span>(?:</span><span>&lt;</span><span>(?!</span><span>/</span><span>?</span><span>think&gt;</span><span>)</span><span>[</span><span>^</span><span>&lt;]</span><span>*</span><span>)</span><span>*</span><span>)</span><span>&lt;</span><span>\/</span><span>think&gt;</span><span>\n</span><span>&lt;answer&gt;</span><span>([\s\S]</span><span>*?</span><span>)</span><span>&lt;</span><span>\/</span><span>answer&gt;</span><span>$</span><span>"</span></span>
<span data-line=""> </span>
<span data-line=""><span>        match </span><span>=</span><span> re.search(regex, completion, re.</span><span>DOTALL</span><span>) </span></span>
<span data-line=""><span>        # if the format is not correct, reward is 0</span></span>
<span data-line=""><span>        if</span><span> match </span><span>is</span><span> None</span><span> or</span><span> len</span><span>(match.groups()) </span><span>!=</span><span> 2</span><span>:</span></span>
<span data-line=""><span>            rewards.append(</span><span>0.0</span><span>)</span></span>
<span data-line=""><span>        else</span><span>:</span></span>
<span data-line=""><span>            rewards.append(</span><span>1.0</span><span>)</span></span>
<span data-line=""><span>      except</span><span> Exception</span><span>:</span></span>
<span data-line=""><span>        rewards.append(</span><span>0.0</span><span>)</span></span>
<span data-line=""><span>    return</span><span> rewards</span></span>
<span data-line=""> </span>
<span data-line=""><span>def</span><span> equation_reward_func</span><span>(completions, target, nums, </span><span>**</span><span>kwargs):</span></span>
<span data-line=""><span>    """</span></span>
<span data-line=""><span>    Evaluates completions based on:</span></span>
<span data-line=""><span>    2. Mathematical correctness of the answer</span></span>
<span data-line=""> </span>
<span data-line=""><span>    Args:</span></span>
<span data-line=""><span>        completions (list[str]): Generated outputs</span></span>
<span data-line=""><span>        target (list[str]): Expected answers</span></span>
<span data-line=""><span>        nums (list[str]): Available numbers</span></span>
<span data-line=""><span>    </span></span>
<span data-line=""><span>    Returns:</span></span>
<span data-line=""><span>        list[float]: Reward scores</span></span>
<span data-line=""><span>    """</span></span>
<span data-line=""><span>    rewards </span><span>=</span><span> []</span></span>
<span data-line=""><span>    for</span><span> completion, gt, numbers </span><span>in</span><span> zip</span><span>(completions, target, nums):</span></span>
<span data-line=""><span>      try</span><span>:</span></span>
<span data-line=""><span>        # add synthetic &lt;think&gt; as its already part of the prompt and prefilled for the assistant to more easily match the regex</span></span>
<span data-line=""><span>        completion </span><span>=</span><span> "&lt;think&gt;"</span><span> +</span><span> completion</span></span>
<span data-line=""><span>        # Check if the format is correct</span></span>
<span data-line=""><span>        match </span><span>=</span><span> re.search(</span><span>r</span><span>"</span><span>&lt;answer&gt;</span><span>(.</span><span>*?</span><span>)</span><span>&lt;</span><span>\/</span><span>answer&gt;</span><span>"</span><span>, completion)</span></span>
<span data-line=""><span>        if</span><span> match </span><span>is</span><span> None</span><span>:</span></span>
<span data-line=""><span>            rewards.append(</span><span>0.0</span><span>)</span></span>
<span data-line=""><span>            continue</span></span>
<span data-line=""><span>        # Extract the "answer" part from the completion</span></span>
<span data-line=""><span>        equation </span><span>=</span><span> match.group(</span><span>1</span><span>).strip()</span></span>
<span data-line=""><span>        # Extract all numbers from the equation</span></span>
<span data-line=""><span>        used_numbers </span><span>=</span><span> [</span><span>int</span><span>(n) </span><span>for</span><span> n </span><span>in</span><span> re.findall(</span><span>r</span><span>'</span><span>\d</span><span>+</span><span>'</span><span>, equation)]</span></span>
<span data-line=""><span>        </span></span>
<span data-line=""><span>        # Check if all numbers are used exactly once</span></span>
<span data-line=""><span>        if</span><span> sorted</span><span>(used_numbers) </span><span>!=</span><span> sorted</span><span>(numbers):</span></span>
<span data-line=""><span>            rewards.append(</span><span>0.0</span><span>)</span></span>
<span data-line=""><span>            continue</span></span>
<span data-line=""><span>        # Define a regex pattern that only allows numbers, operators, parentheses, and whitespace</span></span>
<span data-line=""><span>        allowed_pattern </span><span>=</span><span> r</span><span>'</span><span>^[\d+</span><span>\-</span><span>*/().\s]</span><span>+</span><span>$</span><span>'</span></span>
<span data-line=""><span>        if</span><span> not</span><span> re.match(allowed_pattern, equation):</span></span>
<span data-line=""><span>           rewards.append(</span><span>0.0</span><span>)</span></span>
<span data-line=""><span>           continue</span></span>
<span data-line=""><span>        </span></span>
<span data-line=""><span>        # Evaluate the equation with restricted globals and locals</span></span>
<span data-line=""><span>        result </span><span>=</span><span> eval</span><span>(equation, {</span><span>"__builtins__"</span><span>: </span><span>None</span><span>}, {})</span></span>
<span data-line=""><span>        # Check if the equation is correct and matches the ground truth</span></span>
<span data-line=""><span>        if</span><span> abs</span><span>(</span><span>float</span><span>(result) </span><span>-</span><span> float</span><span>(gt)) </span><span>&lt;</span><span> 1e-5</span><span>:</span></span>
<span data-line=""><span>            rewards.append(</span><span>1.0</span><span>)</span></span>
<span data-line=""><span>        else</span><span>:</span></span>
<span data-line=""><span>            rewards.append(</span><span>0.0</span><span>)</span></span>
<span data-line=""><span>      except</span><span> Exception</span><span>:</span></span>
<span data-line=""><span>            # If evaluation fails, reward is 0</span></span>
<span data-line=""><span>            rewards.append(</span><span>0.0</span><span>) </span></span>
<span data-line=""><span>    return</span><span> rewards</span></span></code></pre></div></figure>
<p>Lets try our reward function with a sample.</p>
<p><em>Note: None of the example starts with <code>&lt;think&gt;</code> as we added it synthetically to the prompt.</em></p>
<figure data-rehype-pretty-code-figure=""><div><pre><code data-language="python" data-theme="github-dark github-light"><span data-line=""><span>correct_sample_1 </span><span>=</span><span> """We need to find an equation using the numbers 19, 36, 55, and 7</span></span>
<span data-line=""><span>exactly once, with basic arithmetic operations, that equals 65. One possible</span></span>
<span data-line=""><span>combination is 55 + 36 - 19 + 7... &lt;/think&gt;</span></span>
<span data-line=""> </span>
<span data-line=""><span>&lt;answer&gt; 55 + 36 - 7 - 19 &lt;/answer&gt;"""</span></span>
<span data-line=""> </span>
<span data-line=""><span>correct_sample_2 </span><span>=</span><span> """ ... &lt;/think&gt;</span></span>
<span data-line=""> </span>
<span data-line=""><span>&lt;answer&gt; 55 + 36 - 7 - 19 &lt;/answer&gt;"""</span></span>
<span data-line=""> </span>
<span data-line=""><span>wrong_format </span><span>=</span><span> """User: Using the numbers [19, 36, 55, 7], create an equation that equals 65."""</span></span>
<span data-line=""> </span>
<span data-line=""><span>wrong_format_2 </span><span>=</span><span> """To find the equation that equals 79 using the numbers 95, 78, 6, 88, I'll start by adding 88 and 95:                      </span></span>
<span data-line=""><span>95 + 88 = 183                                                                                                              </span></span>
<span data-line=""><span>Now, let's subtract 104 from 183 to get 79:</span></span>
<span data-line=""><span>183 - 104 = 79</span></span>
<span data-line=""><span>&lt;think&gt; 183 - 104 = 79 &lt;/think&gt;&lt;think&gt; 183 - 104 = 79 &lt;/think&gt;&lt;answer&gt; 183 - 104 = 79 &lt;/answer&gt;"""</span></span>
<span data-line=""> </span>
<span data-line=""><span>wrong_result </span><span>=</span><span> """ ... &lt;/think&gt;</span></span>
<span data-line=""> </span>
<span data-line=""><span>&lt;answer&gt; 55 + 36 - 7 - 18 &lt;/answer&gt;"""</span></span>
<span data-line=""> </span>
<span data-line=""> </span>
<span data-line=""><span>test_rewards </span><span>=</span><span> format_reward_func(</span><span>completions</span><span>=</span><span>[correct_sample_1, correct_sample_2, wrong_format, wrong_format_2, wrong_result], </span><span>target</span><span>=</span><span>[</span><span>"65"</span><span>, </span><span>"65"</span><span>, </span><span>"65"</span><span>, </span><span>"65"</span><span>, </span><span>"65"</span><span>], </span><span>nums</span><span>=</span><span>[[</span><span>19</span><span>, </span><span>36</span><span>, </span><span>55</span><span>, </span><span>7</span><span>]] </span><span>*</span><span> 5</span><span>)</span></span>
<span data-line=""><span>assert</span><span> test_rewards </span><span>==</span><span> [</span><span>1.0</span><span>, </span><span>1.0</span><span>, </span><span>0.0</span><span>, </span><span>0.0</span><span>, </span><span>1.0</span><span>], </span><span>"Reward function is not working"</span></span>
<span data-line=""><span>test_rewards </span><span>=</span><span> equation_reward_func(</span><span>completions</span><span>=</span><span>[correct_sample_1, correct_sample_2, wrong_format, wrong_format_2, wrong_result], </span><span>target</span><span>=</span><span>[</span><span>"65"</span><span>, </span><span>"65"</span><span>, </span><span>"65"</span><span>, </span><span>"65"</span><span>, </span><span>"65"</span><span>], </span><span>nums</span><span>=</span><span>[[</span><span>19</span><span>, </span><span>36</span><span>, </span><span>55</span><span>, </span><span>7</span><span>]] </span><span>*</span><span> 5</span><span>)</span></span>
<span data-line=""><span>assert</span><span> test_rewards </span><span>==</span><span> [</span><span>1.0</span><span>, </span><span>1.0</span><span>, </span><span>0.0</span><span>, </span><span>0.0</span><span>, </span><span>0.0</span><span>], </span><span>"Reward function is not working"</span></span></code></pre></div></figure>
<p>This looks good, now lets define our remaining training parameters, create a trainer and start training.</p>
<figure data-rehype-pretty-code-figure=""><div><pre><code data-language="python" data-theme="github-dark github-light"><span data-line=""><span>from</span><span> trl </span><span>import</span><span> GRPOConfig, GRPOTrainer, get_peft_config, ModelConfig</span></span>
<span data-line=""> </span>
<span data-line=""><span># our model we are going to use as policy </span></span>
<span data-line=""><span>model_config </span><span>=</span><span> ModelConfig(</span></span>
<span data-line=""><span>    model_name_or_path</span><span>=</span><span>"Qwen/Qwen2.5-3B-Instruct"</span><span>,</span></span>
<span data-line=""><span>    torch_dtype</span><span>=</span><span>"bfloat16"</span><span>,</span></span>
<span data-line=""><span>    attn_implementation</span><span>=</span><span>"flash_attention_2"</span><span>,</span></span>
<span data-line=""><span>    use_peft</span><span>=</span><span>True</span><span>,</span></span>
<span data-line=""><span>    load_in_4bit</span><span>=</span><span>True</span><span>,</span></span>
<span data-line=""><span>)</span></span>
<span data-line=""> </span>
<span data-line=""><span># Hyperparameters</span></span>
<span data-line=""><span>training_args </span><span>=</span><span> GRPOConfig(</span></span>
<span data-line=""><span>    output_dir</span><span>=</span><span>"qwen-r1-aha-moment"</span><span>,</span></span>
<span data-line=""><span>    learning_rate</span><span>=</span><span>5e-7</span><span>,</span></span>
<span data-line=""><span>    lr_scheduler_type</span><span>=</span><span>"cosine"</span><span>,</span></span>
<span data-line=""><span>    logging_steps</span><span>=</span><span>10</span><span>,</span></span>
<span data-line=""><span>    max_steps</span><span>=</span><span>100</span><span>,</span></span>
<span data-line=""><span>    per_device_train_batch_size</span><span>=</span><span>1</span><span>,</span></span>
<span data-line=""><span>    gradient_accumulation_steps</span><span>=</span><span>1</span><span>,</span></span>
<span data-line=""><span>    gradient_checkpointing</span><span>=</span><span>True</span><span>,</span></span>
<span data-line=""><span>    gradient_checkpointing_kwargs</span><span>=</span><span>{</span><span>"use_reentrant"</span><span>: </span><span>False</span><span>},</span></span>
<span data-line=""><span>    bf16</span><span>=</span><span>True</span><span>,</span></span>
<span data-line=""><span>    # GRPO specific parameters</span></span>
<span data-line=""><span>    max_prompt_length</span><span>=</span><span>256</span><span>,</span></span>
<span data-line=""><span>    max_completion_length</span><span>=</span><span>1024</span><span>, </span><span># max length of the generated output for our solution</span></span>
<span data-line=""><span>    num_generations</span><span>=</span><span>2</span><span>,</span></span>
<span data-line=""><span>    beta</span><span>=</span><span>0.001</span><span>,</span></span>
<span data-line=""><span>    </span></span>
<span data-line=""><span>)</span></span>
<span data-line=""><span>trainer </span><span>=</span><span> GRPOTrainer(</span></span>
<span data-line=""><span>    model</span><span>=</span><span>model_config.model_name_or_path,</span></span>
<span data-line=""><span>    reward_funcs</span><span>=</span><span>[format_reward_func, equation_reward_func],</span></span>
<span data-line=""><span>    args</span><span>=</span><span>training_args,</span></span>
<span data-line=""><span>    train_dataset</span><span>=</span><span>train_dataset,</span></span>
<span data-line=""><span>    eval_dataset</span><span>=</span><span>test_dataset,</span></span>
<span data-line=""><span>    peft_config</span><span>=</span><span>get_peft_config(model_config),</span></span>
<span data-line=""><span>)</span></span></code></pre></div></figure>
<p>We can start our training by calling the <code>train</code> method on the trainer instance.</p>
<p><em>Note: Reinforcement Training is very slow and compute intensive. Running a single step on 1x L4 with Q-LoRA, Batch size of 1 and only 2 generations per samples takes &gt;20 minutes.</em></p>
<figure data-rehype-pretty-code-figure=""><div><pre><code data-language="python" data-theme="github-dark github-light"><span data-line=""><span># Train and push the model to the Hub</span></span>
<span data-line=""><span>trainer.train()</span></span>
<span data-line=""><span># Save model</span></span>
<span data-line=""><span>trainer.save_model(training_args.output_dir)</span></span></code></pre></div></figure>
<h2 id="4-distributed-training-example-for-grpo-using-deepspeed-and-vllm">4. Distributed Training example for GRPO using Deepspeed and vLLM</h2>
<p>More than 20 minutes per step with only 2 generations per sample is not feasible. We need to scale up our training. Hugging Face TRL added support for distributed training with Deepspeed and using vLLM for faster generation. I preprared a <a target="_blank" rel="noopener noreferrer" href="https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/scripts/run_r1_grpo.py">run_r1_grpo.py</a> script and a <a target="_blank" rel="noopener noreferrer" href="https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/receipes/grpo-qwen-2.5-3b-deepseek-r1-countdown.yaml">receipes/grpo-qwen-2.5-3b-deepseek-r1-countdown.yaml</a> config file to run the training.</p>
<p>This configuration is tested and validated on a Node with 4x H100 80GBs, where a single step takes around 45-60s, as we can leverage vLLM for generation and DeepSpeed for distributed training. Therefore we need to make sure we correctly set the <code>num_processes</code> to the number of GPUs you have - 1 as the last one will be used with vLLM for Generation. If you are using more GPUS you need to change the <code>vllm_device</code> in the config file to last index GPU, e.g. if you have 8 GPUs you need to set <code>vllm_device=7</code> and your <code>num_processes</code> to 7.</p>
<p>command to run the training:</p>
<figure data-rehype-pretty-code-figure=""><div><pre><code data-language="bash" data-theme="github-dark github-light"><span data-line=""><span>accelerate</span><span> launch</span><span> --num_processes</span><span> 3</span><span> --config_file</span><span> configs/accelerate_configs/deepspeed_zero3.yaml</span><span> scripts/run_r1_grpo.py</span><span> --config</span><span> receipes/grpo-qwen-2.5-3b-deepseek-r1-countdown.yaml</span></span></code></pre></div></figure>
<p>With the optimized distributed training a single step with 8 generations per sample on 4x H100 80GBs takes around 45-60s. The full training for 450 steps takes around 6 hours.</p>
<h2 id="5-results-and-training-observations">5. Results and Training Observations</h2>
<p>The script saves random completions to the <code>completion_samples</code> folder, which you can use to inspect the model's progress. It includes <code>completion_samples.txt</code> and <code>success_completion_samples.txt</code>. The <code>completion_samples.txt</code> includes all completions, while the <code>success_completion_samples.txt</code> which correctly solves the equation. Below you can find the interesating training obeserations on how the performance changes over time, as well as the Tensornoard logs and successfull reasoning samples.</p>
<p>The model with checkpoints for every 25th step can be found at <a target="_blank" rel="noopener noreferrer" href="https://huggingface.co/philschmid/qwen-2.5-3b-r1-countdown">philschmid/qwen-2.5-3b-r1-countdown</a>.</p>
<h3 id="hyperparameters">Hyperparameters</h3>
<p>I started the experiment using the hyperparameters from the <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2402.03300">DeepSeekMath</a> paper with a learning rate of 1e-6 and a beta (KL coefficient) of 0.04, which led to unstable training runs after around 150 steps. I ran some small ablations and decreased both the learning rate to 5e-7 and the beta to 0.001, based on a test from <a target="_blank" rel="noopener noreferrer" href="https://hijkzzz.notion.site/unraveling-rlhf-and-its-variants-engineering-insights#147d9a33ecc9806090f3d5c749d31f05">OpenRLHF</a>. I coulnd't test how increasing the <code>num_generations</code> from 8 to 64 would affect the training. 64 is the generation value, which was used in the DeepSeekMath paper. All other parameters can be found in the <a target="_blank" rel="noopener noreferrer" href="https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/receipes/grpo-qwen-2.5-3b-deepseek-r1-countdown.yaml">grpo-qwen-2.5-3b-deepseek-r1-countdown.yaml</a> config file.</p>
<h3 id="training-observations">Training Observations:</h3>
<ul>
<li>At ~50 steps the model has learned the correct format <code>&lt;think&gt;...&lt;/think&gt;\n&lt;answer&gt;...&lt;/answer&gt;</code>.</li>
<li>At 100 steps the success rate for solving the equation is around 25%. The model starts to "reason" with words see examples below.</li>
<li>At 200 steps the performance seems to converge much slower and we are at ~40% success rate. The model starts to learn a new "format" where it solves the equation similar to how you would do it programmatically, by trying different combinations and reviewing the results, see "Successfull Reasoning Samples between step 200 and 450".</li>
<li>At 450 steps we have 50% success rate for solving the equation. The performance still improves slowly and the model kept its new format form from step 200.</li>
</ul>
<p>I have 3 potential assumptions why the model shifts from "word reasoning" to "programmatic execution":</p>
<ol>
<li>Qwen 2.5 3B is not strong enough or to small, Deepseek mentions that you need a very strong base model.</li>
<li>The reward functions are not good enough defined and it reward hacks the model to solve the equation. We could try to force it to use words, e.g. having a number to word frequency condition. (We don't know much about the reward functions from Deepseek)</li>
<li>Only training on the Countdown Game tasks might force the model naturally to learn the most effective way to solve the equation as no other formats are required.</li>
<li>We didn't train the model long enough, in the R1 paper they showed visuals of training for over 8000 steps.</li>
</ol>
<h4 id="tensorboard-results">Tensorboard Results</h4>
<p><img src="https://www.philschmid.de/static/blog/mini-deepseek-r1/tensorboard-r1.png" alt="Qwen R1 Tensorboard results"></p>
<h4 id="successfull-reasoning-samples-up-to-step-200">Successfull Reasoning Samples up to step 200</h4>
<figure data-rehype-pretty-code-figure=""><div><pre><code data-language="xml" data-theme="github-dark github-light"><span data-line=""><span>&lt;</span><span>think</span><span>&gt; First, I need to think about the numbers we have: 14, 70, and 80. To get 85, I think 80 might be a good starting point because it's right in between 85. If I subtract or add 80, it won't lead to 85 easily. Next, 70 seems to be a good intermediate number, especially to use in conjunction with 14. </span></span>
<span data-line=""><span>Now, I can think about different combinations using addition, subtraction and multiplication/division. If I take 80 - 70 = 10, which is not needed. So, I need to find a way to use 14. If I subtract 14 from 80, I get 66, which still isn't 85. If I subtract 70 from 80, I get 10 again, this isn't constructive yet. </span></span>
<span data-line=""><span>Multiplication could create numbers larger than the target 85. Let's consider division: If I divide 70 by 14, I get 5. Then if I add this result to 80, I get 85. This combination seems promising: 80 - (70 / 14) = 80 - 5 = 75, this still isn't quite right. Let me correct my approach: Let's use division and then addition. If I take 80 / (70 / 14) → 80 / 5 = 16, and then 80 - 16 = 64, still incorrect. Let me simplify: 80 / (70 / 14) → 80 / 5 = 16, and then 80 - 16 gives 64 again, let me correct: 80 - (70 / 14) → 80 - 5 = 75, but I need 85. Thus I will use the combinations in a slightly different order to insert numbers: I get 80 + (70 / 14) = 80 + 5 = 85&lt;/</span><span>think</span><span>&gt;</span></span>
<span data-line=""><span>&lt;</span><span>answer</span><span>&gt; 80 + (70 / 14) &lt;/</span><span>answer</span><span>&gt;</span></span></code></pre></div></figure>
<figure data-rehype-pretty-code-figure=""><div><pre><code data-language="xml" data-theme="github-dark github-light"><span data-line=""><span>&lt;</span><span>think</span><span>&gt; First, I need to consider how I can use the numbers 1, 21, and 90 to reach 68. I can try different combinations of operations. Let's try to use subtraction and addition. If I subtract 90 from 21, I get -69, which is too far from 68. Now, if I add 1 to 90, I get 91, which is also not right. Another way is to see what happens if I subtract 21 from 90, which gives me 69. This is closer but still not 68. Let me use multiplication instead of subtraction. 90 * 1 = 90. Now, if I try to reduce this slightly by subtracting 21, I am not getting close to 68.</span></span>
<span data-line=""><span>Now, let's try another multiplication. I'll think of 1 * (larger value) + (smaller value). Let's try 1 * 90 + 21. This evaluates to 90 + 21 = 111, which is too much. Instead, I will try: 90 / 1 + 21 - 7. But this gives a division by 7, which seems complex. Let's re-evaluate: I'll use 1 to divide 21: 21 / 1 = 21. Now, subtract 21 from 90, and add 1, we have 90 - 21 = 69, using one...</span></span>
<span data-line=""><span>Hey, wait on re-evaluating I will subtract 21 from 90, and include 1 to complete the solution: 90 - (21 + 1) which would be 68.&lt;/</span><span>think</span><span>&gt;</span></span>
<span data-line=""><span>&lt;</span><span>answer</span><span>&gt; 90 - (21 + 1) &lt;/</span><span>answer</span><span>&gt;</span></span></code></pre></div></figure>
<figure data-rehype-pretty-code-figure=""><div><pre><code data-language="xml" data-theme="github-dark github-light"><span data-line=""><span>&lt;</span><span>think</span><span>&gt; First, I need to consider the numbers we have: 59, 80, and 66. I need to use these numbers to get 45. Let's try to use subtraction first since it can quickly reduce large numbers.</span></span>
<span data-line=""><span>Let's see if we can get close by subtracting or adding these numbers. If I subtract 66 from 80, I get 14. Then I need to make this 14 equal 45 when combined with 59. That gives me 14 + 59 = 73, which is too high.</span></span>
<span data-line=""><span>Now let's try adding numbers. If I add 59 and 66, I get 125. Then I would need to subtract 80 from 125 to get 45, but 80 is too large to fit well with this.</span></span>
<span data-line=""><span>Let me try a different approach. What if I subtract 66 from 80 first and then add 59? That would be 14 + 59 = 73, which is still not 45.</span></span>
<span data-line=""><span>Hmm, I need to think of another combination. What if I start with 59 and then add 80 and subtract 66? That would be (59 + 80) - 66, let me calculate it. That equals 139 - 66, which is 73.</span></span>
<span data-line=""><span>That's still not 45, so let me try rearranging it slightly: maybe 80 - 66 + 59 would be closer to 45. Calculating again: 80 - 66 is 14, then adding 59 gives 73.</span></span>
<span data-line=""><span>This way doesn't work either. Let's flip this: maybe (59 + 66) - 80. Let me check: that's 125 - 80, which equals 45.&lt;/</span><span>think</span><span>&gt;</span></span>
<span data-line=""><span>&lt;</span><span>answer</span><span>&gt;(59 + 66) - 80&lt;/</span><span>answer</span><span>&gt;</span></span></code></pre></div></figure>
<figure data-rehype-pretty-code-figure=""><div><pre><code data-language="xml" data-theme="github-dark github-light"><span data-line=""><span>&lt;</span><span>think</span><span>&gt; First, I need to find a combination of the numbers 61, 4, 94, and 21 that can be manipulated using basic arithmetic operations to equal 50. One way to approach this is to look for numbers that can be used to reach a target number close to 50 and then adjust with the remaining numbers.</span></span>
<span data-line=""><span>Let's try combining 94 and 21 first:</span></span>
<span data-line=""><span>94 + 21 = 115</span></span>
<span data-line=""><span>Now, we need to get from 115 to 50. We can try subtracting 65:</span></span>
<span data-line=""><span>115 - 65 = 50</span></span>
<span data-line=""><span>Now we need to find a way to get 65 using the remaining numbers 61 and 4. We can try:</span></span>
<span data-line=""><span>61 - 4 = 57</span></span>
<span data-line=""><span>Now we need to get from 57 to 65. Adding 8 would give us that:</span></span>
<span data-line=""><span>57 + 8 = 65</span></span>
<span data-line=""><span>So, we need to find a way to get 8 using 61 and 4. We can try:</span></span>
<span data-line=""><span>61 - 94 = -36</span></span>
<span data-line=""><span>This is negative, so let's try dividing:</span></span>
<span data-line=""><span>61 / 94 = 0.648987...</span></span>
<span data-line=""><span>This is not helping, so let's try multiplication and division in a different way:</span></span>
<span data-line=""><span>(61 - 4) * (94 / 21) = 57 * 4.476... = 254.4</span></span>
<span data-line=""><span>This is not working either. Let's try a different approach:</span></span>
<span data-line=""><span>(94 - 61) = 33</span></span>
<span data-line=""><span>Now we have:</span></span>
<span data-line=""><span>33 + 21 = 54</span></span>
<span data-line=""><span>We need to adjust from 54 to 50, so we can:</span></span>
<span data-line=""><span>54 - 4 = 50</span></span>
<span data-line=""><span>So, the equation becomes:</span></span>
<span data-line=""><span>(94 - 61) + (21 - 4) = 50</span></span>
<span data-line=""><span>&lt;/</span><span>think</span><span>&gt;</span></span>
<span data-line=""><span>&lt;</span><span>answer</span><span>&gt;(94 - 61) + (21 - 4)&lt;/</span><span>answer</span><span>&gt;</span></span></code></pre></div></figure>
<figure data-rehype-pretty-code-figure=""><div><pre><code data-language="xml" data-theme="github-dark github-light"><span data-line=""><span>&lt;</span><span>think</span><span>&gt; First, I'll consider the possibility of directly subtracting two numbers to get close to 31. If I subtract 57 from 63, I get 7. Now, if I subtract this 7 from 25, I get 25 - 7 = 18. Far from 31, so this doesn't work.</span></span>
<span data-line=""><span>Next, let's try adding and subtracting. Dividing 63 by 25 will give a decimal, so that's unlikely to work. Now, let's see if adding and subtracting in another order could work. For example, let's try 63 - 25, and then we see this gets us 38. Now, we need a way to get to 31 from here. Subtracting 7 from 38 would work, but we can't figure out how to create that 7 from the given numbers.</span></span>
<span data-line=""><span>Hmm, let me try to incorporate all numbers by adding and subtracting. If I try 63 + 25, I get 88. This is quite far from 31, but if 88 - 57, I get 31 exactly!</span></span>
<span data-line=""><span>This seems to be the equation: 63 + 25 - 57 equals 31.</span></span>
<span data-line=""><span>&lt;/</span><span>think</span><span>&gt;</span></span>
<span data-line=""><span>&lt;</span><span>answer</span><span>&gt; 63 + 25 - 57 &lt;/</span><span>answer</span><span>&gt;</span></span></code></pre></div></figure>
<h4 id="successfull-reasoning-samples-between-step-200-and-450">Successfull Reasoning Samples between step 200 and 450</h4>
<figure data-rehype-pretty-code-figure=""><div><pre><code data-language="xml" data-theme="github-dark github-light"><span data-line=""><span>&lt;</span><span>think</span><span>&gt; We need to use the numbers 39, 16, and 55 with basic arithmetic operations to get 78. Let's try different combinations:</span></span>
<span data-line=""><span>- 55 + 39 - 16 = 84 - 16 = 68 (too low)</span></span>
<span data-line=""><span>- 55 + 16 - 39 = 71 - 39 = 32 (too low)</span></span>
<span data-line=""><span>- 55 - 16 + 39 = 39 + 39 = 78 (this works)</span></span>
<span data-line=""><span>So, the equation is 55 - 16 + 39 = 78. &lt;/</span><span>think</span><span>&gt;</span></span>
<span data-line=""><span>&lt;</span><span>answer</span><span>&gt; 55 - 16 + 39 &lt;/</span><span>answer</span><span>&gt;</span></span></code></pre></div></figure>
<figure data-rehype-pretty-code-figure=""><div><pre><code data-language="xml" data-theme="github-dark github-light"><span data-line=""><span>&lt;</span><span>think</span><span>&gt; We need to use the numbers 21, 5, and 40 with basic arithmetic operations to get 24. Let's try different combinations:</span></span>
<span data-line=""><span>- 21 - 5 - 40 = -15</span></span>
<span data-line=""><span>- 21 - 5 + 40 = 56</span></span>
<span data-line=""><span>- 21 + 5 - 40 = -14</span></span>
<span data-line=""><span>- 21 * 5 / 40 = 2.1 (not an integer, let's try another)</span></span>
<span data-line=""><span>- 40 * 5 / 21 = 9.52 (not 24)</span></span>
<span data-line=""><span>- 40 / 5 - 21 = 9 - 21 = -12</span></span>
<span data-line=""><span>- 5 * 40 / 21 = 100 / 21 ~= 4.76 (not 24)</span></span>
<span data-line=""><span>- 5 + 40 - 21 = 24 ! This works.</span></span>
<span data-line=""><span>&lt;/</span><span>think</span><span>&gt;</span></span>
<span data-line=""><span>&lt;</span><span>answer</span><span>&gt; 5 + 40 - 21 &lt;/</span><span>answer</span><span>&gt;</span></span></code></pre></div></figure>
<figure data-rehype-pretty-code-figure=""><div><pre><code data-language="xml" data-theme="github-dark github-light"><span data-line=""><span>&lt;</span><span>think</span><span>&gt; We need to use the numbers 75, 99, 62, and 34 with basic arithmetic operations to get 72. Let's try different combinations:</span></span>
<span data-line=""><span>- 99 - 75 + 62 - 34 = 92 - 34 = 58 (too high)</span></span>
<span data-line=""><span>- 99 - 75 - 62 + 34 = 24 - 62 + 34 = -38 + 34 = -4 (too low)</span></span>
<span data-line=""><span>- 99 + 75 - 62 - 34 = 174 - 96 = 78 (too high)</span></span>
<span data-line=""><span>- 99 + 75 - 62 + 34 = 174 + 34 - 62 = 208 - 62 = 146 (too high)</span></span>
<span data-line=""><span>- 99 - 75 + 62 + 34 = 24 + 96 = 120 (too high)</span></span>
<span data-line=""><span>- 75 + 99 - 62 - 34 = 174 - 96 = 78 (too high)</span></span>
<span data-line=""><span>- 75 + 99 - 62 + 34 = 174 + 34 - 62 = 208 - 62 = 146 (too high)</span></span>
<span data-line=""><span>- 75 + 62 - 99 + 34 = 137 - 99 + 34 = 38 + 34 = 72</span></span>
<span data-line=""><span>So, 75 + 62 - 99 + 34 equals 72.</span></span>
<span data-line=""><span>&lt;/</span><span>think</span><span>&gt;</span></span>
<span data-line=""><span>&lt;</span><span>answer</span><span>&gt; 75 + 62 - 99 + 34 &lt;/</span><span>answer</span><span>&gt;</span></span></code></pre></div></figure>
<h2 id="conclusion">Conclusion</h2>
<p>The release of DeepSeek R1 and its research paper might be breakpoint for the open-science and open-source development. Just a week after DeepSeek release, we've been able to reproduce a simple version of R1 learned "reasoning" using GRPO and the Countdown Game. While our implementation focuses on a specific task rather than general reasoning and convergence into a very specific "reasoning" format, it shows that the method is working.</p>
<p>In our mini R1 experiment we used GRPO, with two rule-based reward but already required significant compute: 4 H100 GPUs running for 6 hours to complete just 450 training steps on a 3B parameter model. This gives us an idea of the compute needs that you will need to scale Reinforcement Learning. Deepseek ran a 671B model for over 8000 steps and they probably did many ablations.</p>
<p>Looking in to 2025, it's clear that we are on the cusp of even more significant progress. RL will become more accessible and user-friendly, more researchers and developers will explore its potential, but also require amount of more compute as before and compared to supervised fine-tuning.</p>
<p>I am excited for 2025. If you are have any question or ideas feel free to reach out to me.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[TopoNets: High performing vision and language models with brain-like topography (109 pts)]]></title>
            <link>https://arxiv.org/abs/2501.16396</link>
            <guid>42884338</guid>
            <pubDate>Fri, 31 Jan 2025 02:58:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2501.16396">https://arxiv.org/abs/2501.16396</a>, See on <a href="https://news.ycombinator.com/item?id=42884338">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2501.16396">View PDF</a>
    <a href="https://arxiv.org/html/2501.16396v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Neurons in the brain are organized such that nearby cells tend to share similar functions. AI models lack this organization, and past efforts to introduce topography have often led to trade-offs between topography and task performance. In this work, we present TopoLoss, a new loss function that promotes spatially organized topographic representations in AI models without significantly sacrificing task performance. TopoLoss is highly adaptable and can be seamlessly integrated into the training of leading model architectures. We validate our method on both vision (ResNet-18, ResNet-50, ViT) and language models (GPT-Neo-125M, NanoGPT), collectively TopoNets. TopoNets are the highest-performing supervised topographic models to date, exhibiting brain-like properties such as localized feature processing, lower dimensionality, and increased efficiency. TopoNets also predict responses in the brain and replicate the key topographic signatures observed in the brain's visual and language cortices. Together, this work establishes a robust and generalizable framework for integrating topography into leading model architectures, advancing the development of high-performing models that more closely emulate the computational strategies of the human brain.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Mayukh Deb [<a href="https://arxiv.org/show-email/3224565a/2501.16396" rel="nofollow">view email</a>]      <br>    <strong>[v1]</strong>
        Mon, 27 Jan 2025 01:09:59 UTC (38,378 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How Jane Street accidentally built a better build system for OCaml (195 pts)]]></title>
            <link>https://blog.janestreet.com/how-we-accidentally-built-a-better-build-system-for-ocaml-index/</link>
            <guid>42881724</guid>
            <pubDate>Thu, 30 Jan 2025 20:14:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.janestreet.com/how-we-accidentally-built-a-better-build-system-for-ocaml-index/">https://blog.janestreet.com/how-we-accidentally-built-a-better-build-system-for-ocaml-index/</a>, See on <a href="https://news.ycombinator.com/item?id=42881724">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>A “build system” is one of the most important tools in a developer’s
toolbox. Roughly, it figures out how to create runnable programs from
a bunch of different source files by calling out to the compiler,
setting up and executing test suites, and so on. Because you interact
with it daily, above all it has to be <a href="https://xkcd.com/303/">fast</a> –
but it also has to be flexible.</p>

<p>Around 2012 we were growing dissatisfied with OMake, then one of
OCaml’s standard build systems, and decided to build our own; we
called this new system Jenga. It worked quite well for us, and we
thought the broader community might find it useful. So we decided to
release Jenga. We hoped that when other people tried it, they’d like
it, and maybe even contribute back to it. Releasing it would also make
it easier for us to open source our code.</p>

<p>Ha! What actually happened is that nobody really wanted to use Jenga.
For one thing it didn’t work on Windows. But also, to adopt Jenga was
in effect to adopt the whole “Jane Street way” of building OCaml. The
adoption of Jenga by those we hoped to embrace it was weak enough that
we actually decided to <em>un</em>-open source it. And so we were back to the
same place as before.</p>

<p>By 2016 we had had enough of this, and decided to make a simple
cross-platform tool, called Jbuilder, that would allow external users
to build our code without having to adopt Jenga in full, and would
release us from the obligation of rewriting our builds in
<a href="https://github.com/ocaml/ocamlbuild">OCamlbuild</a>, then an emerging
standard for building OCaml projects.</p>

<p>Jbuilder understood the <code>jbuild</code> files that Jenga used for build
configuration and simply executed all required compilation commands in
a topological order. It wasn’t a build system in the usual sense: it
would simply re-execute all commands every time (instead of only
re-executing commands whose inputs have changed).</p>

<h3 id="jbuilder-gets-popular-and-becomes-dune"><strong>Jbuilder gets popular and becomes “Dune”</strong></h3>

<p>Then something strange happened. People <em>loved</em> Jbuilder. They started
using it to build not just our packages but their own, too. At first
we didn’t really understand this. Jbuilder wasn’t a real build system,
after all. It was just meant to be a little compatibility shim.</p>

<p>What we realized, eventually, was that the compelling feature was
speed. It turned out that Jbuilder was really a lot faster than the
other options, compiling our projects something like 5x faster than
OCamlbuild. That, plus the system being portable and easy to hack on,
were the things that mattered for early adopters and contributors.</p>

<p>So, in collaboration with <a href="https://www.linkedin.com/company/ocaml-labs-consultancy-ltd/?originalSubdomain=uk">OCaml
Labs</a>
(and today, <a href="https://tarides.com/">Tarides</a>), we started working on
making Jbuilder into more of a real build system, adding more of the
features that would be required for it to be a useful tool for the
broader open-source world.</p>

<p>And then, we ran into another problem. The name.</p>

<p>It turned out there was already a Borland Java IDE called
“<a href="https://en.wikipedia.org/wiki/JBuilder">JBuilder</a>.”  The system was
long defunct, and we even went to the trouble of finding the current
owners of the copyright and asking them if they’d mind us using the
name.  But, no dice.</p>

<p>So we decided to pick a new name.  We did <a href="https://discuss.ocaml.org/t/new-name-for-jbuilder">a bit of community
outreach</a>, and “Dune”
emerged as the winning name.</p>

<p>In the meantime, Dune’s popularity had exploded. People really began
using it in earnest, and we found ourselves in a somewhat ridiculous,
self-inflicted situation: we now had two full build systems to
maintain and support.</p>

<h3 id="jenga-vs-dune"><strong>Jenga vs. Dune</strong></h3>

<p>It became clear to us that Dune was the better system—a rethought
design, faster for most people’s builds, with wider adoption, and a
better API and user experience—which brought up the question “when
will we migrate Jane Street onto Dune?” Considering the provenance
of the tool this felt like an absurd question, but oh well, that’s
where we’d ended up.</p>

<p>The answer was inevitably “next year,” since the build systems team
had plenty to work on simply keeping up with our growing codebase. (In
2016, when Dune started, we had 4M lines of OCaml code; today we have
65M, plus 5M lines of Python.) A migration to Dune was daunting enough
that we never quite fully embarked on the mission. But it wasn’t <em>so</em>
daunting to keep us from estimating, aspirationally, that it might
well happen in the next six to twelve months.</p>

<p>It was only last year that we decided to finally rip the band-aid off.
We’d grown the build systems team to five full-time engineers, so we
felt that we finally had the strength to tackle this monster we’d been
afraid of for so long.</p>

<h3 id="dune-subsumes-jenga-within-jane-street"><strong>Dune subsumes Jenga within Jane Street</strong></h3>

<p>One large chunk of work we didn’t really appreciate in the beginning was
just making Dune scale to our huge codebase. Dune was quite fast
externally—but that’s in part because most users built relatively tiny
things compared to Jane Street’s 70M-line repository.</p>

<p>And Jenga hadn’t stood still, either. Over a decade, we had improved
the implementation to deal with the growth of our codebase. That
produced a system that was pretty well-optimized for scale, and
carefully tailored to our monorepo’s requirements.  Now, much of that
good optimization work had to be translated to Dune.</p>

<p>There were more mundane problems. The build system is called by a
variety of different workflows, notably, from three different
editors—Vim, Emacs, and VSCode.  Each one sadly had its own custom
integrations with Jenga that had to be migrated to use Dune one by
one.</p>

<p>But after more than a year of focused work we’re finally done: our
codebase is now built by Dune. At the time of the switch, Dune’s
performance was across the board as good or better than Jenga’s, and
much better in some cases. In particular, builds where most of the
build work is already in the cache (which is a surprisingly common
case!) have gotten 2-3x faster.</p>

<p>A lot of what we’ve done to improve Dune’s performance will get open
sourced, and some already has. We’re keen not to end up with two build
systems again—a Jane Street fork of Dune and the external Dune—so
we’re putting a lot of thought and energy into upstreaming our changes
where we can.</p>

<p>Dune is a very good foundation for doing new things. Some of that is
because Dune’s codebase is simpler and easier to work with; some
because we can just focus on one system. But, features like
distributed builds, shallow builds (also known as <a href="https://blog.bazel.build/2023/10/06/bwob-in-bazel-7.html#what-is-build-without-the-bytes">“builds without the
bytes”</a>),
and cached loading of the build graph itself are all closer in reach
than ever.</p>

<p>The exciting thing going forward is that we have a single system and
our velocity in improving it is going way up. The team has also grown
to 12 full-time engineers, in New York, London and Singapore, which
means we’re now working on Dune 24 hours a day.</p>

<p>It’s been a long and sometimes meandering path, and certainly hasn’t
unfolded the way we might have planned.  But we think that the end
result is good for OCaml’s build-system story, both within and beyond
Jane Street’s walls.</p>

        </div><p>Andrey joined Jane Street London in 2019, and in 2025 moved to the Singapore office to
grow the Tools and Compilers team in Asia. Before Jane Street, Andrey was an academic
at Newcastle University, UK, where he studied functional programming and asynchronous
circuits.
</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stats – macOS system monitor in your menu bar (267 pts)]]></title>
            <link>https://github.com/exelban/stats</link>
            <guid>42881342</guid>
            <pubDate>Thu, 30 Jan 2025 19:37:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/exelban/stats">https://github.com/exelban/stats</a>, See on <a href="https://news.ycombinator.com/item?id=42881342">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Stats</h2><a id="user-content-stats" aria-label="Permalink: Stats" href="#stats"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/exelban/stats/raw/master/Stats/Supporting%20Files/Assets.xcassets/AppIcon.appiconset/icon_256x256.png"><img src="https://github.com/exelban/stats/raw/master/Stats/Supporting%20Files/Assets.xcassets/AppIcon.appiconset/icon_256x256.png" width="120"></a></p>
<p dir="auto"><a href="https://github.com/exelban/stats/releases"><img src="https://camo.githubusercontent.com/c0b7c8837b8f05167af0febd37cc3b775b8916072c3d1ce432c10fbbd8dc626f/68747470733a2f2f7365726869792e73332e65752d63656e7472616c2d312e616d617a6f6e6177732e636f6d2f4769746875625f7265706f2f73746174732f6d656e757325334676322e332e322e706e673f7631" alt="Stats" data-canonical-src="https://serhiy.s3.eu-central-1.amazonaws.com/Github_repo/stats/menus%3Fv2.3.2.png?v1"></a>
<a href="https://github.com/exelban/stats/releases"><img src="https://camo.githubusercontent.com/d8438fa217f3a8b4411eff42d5141cdc18e6087feb797a5477f3fc179597b342/68747470733a2f2f7365726869792e73332e65752d63656e7472616c2d312e616d617a6f6e6177732e636f6d2f4769746875625f7265706f2f73746174732f706f7075707325334676322e332e322e706e673f7633" alt="Stats" data-canonical-src="https://serhiy.s3.eu-central-1.amazonaws.com/Github_repo/stats/popups%3Fv2.3.2.png?v3"></a></p>
<p dir="auto">macOS system monitor in your menu bar</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Manual</h3><a id="user-content-manual" aria-label="Permalink: Manual" href="#manual"></a></p>
<p dir="auto">You can download the latest version <a href="https://github.com/exelban/stats/releases/latest/download/Stats.dmg">here</a>.
This will download a file called <code>Stats.dmg</code>. Open it and move the app to the application folder.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Homebrew</h3><a id="user-content-homebrew" aria-label="Permalink: Homebrew" href="#homebrew"></a></p>
<p dir="auto">To install it using Homebrew, open the Terminal app and type:</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Requirements</h2><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<p dir="auto">Stats is supported on the released macOS version starting from macOS 10.15 (Catalina).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<p dir="auto">Stats is an application that allows you to monitor your macOS system.</p>
<ul dir="auto">
<li>CPU utilization</li>
<li>GPU utilization</li>
<li>Memory usage</li>
<li>Disk utilization</li>
<li>Network usage</li>
<li>Battery level</li>
<li>Fan's control (not maintained)</li>
<li>Sensors information (Temperature/Voltage/Power)</li>
<li>Bluetooth devices</li>
<li>Multiple time zone clock</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">FAQs</h2><a id="user-content-faqs" aria-label="Permalink: FAQs" href="#faqs"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">How do you change the order of the menu bar icons?</h3><a id="user-content-how-do-you-change-the-order-of-the-menu-bar-icons" aria-label="Permalink: How do you change the order of the menu bar icons?" href="#how-do-you-change-the-order-of-the-menu-bar-icons"></a></p>
<p dir="auto">macOS decides the order of the menu bar items not <code>Stats</code> - it may change after the first reboot after installing Stats.</p>
<p dir="auto">To change the order of any menu bar icon - macOS Mojave (version 10.14) and up.</p>
<ol dir="auto">
<li>Hold down ⌘ (command key).</li>
<li>Drag the icon to the desired position on the menu bar.</li>
<li>Release ⌘ (command key)</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">What if you don't see sensors (M1 macs)?</h3><a id="user-content-what-if-you-dont-see-sensors-m1-macs" aria-label="Permalink: What if you don't see sensors (M1 macs)?" href="#what-if-you-dont-see-sensors-m1-macs"></a></p>
<p dir="auto">Sensors data on the first generation of M1 mac could be obtained only from HID services. It's disabled by default because it consumes a lot of CPU and energy. You can enable it in the Sensors module settings with the option <code>HID sensors</code>.</p>
<p dir="auto">It's only valid for M1 Apple Silicon macs. If you don't see sensors on another mac, please open an issue for that.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">How to show the CPU frequency?</h3><a id="user-content-how-to-show-the-cpu-frequency" aria-label="Permalink: How to show the CPU frequency?" href="#how-to-show-the-cpu-frequency"></a></p>
<p dir="auto">The CPU frequency is available only on Intel-based macs. You need to have installed <a href="https://www.intel.com/content/www/us/en/developer/articles/tool/power-gadget.html" rel="nofollow">Intel Power Gadget</a> (IPG) for that. It allows receiving the CPU frequency from the IPG driver. There is no way to obtain a CPU frequency on Apple silicon macs.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">How to reduce energy impact or CPU usage of Stats?</h3><a id="user-content-how-to-reduce-energy-impact-or-cpu-usage-of-stats" aria-label="Permalink: How to reduce energy impact or CPU usage of Stats?" href="#how-to-reduce-energy-impact-or-cpu-usage-of-stats"></a></p>
<p dir="auto">Stats tries to be efficient as it's possible. But reading some data periodically is not a cheap task. Each module has its own "price". So, if you want to reduce energy impact from the Stats you need to disable some Stats modules. The most inefficient modules are Sensors and Bluetooth. Disabling these modules could reduce CPU usage and power efficiency by up to 50% in some cases.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why my issue was closed without any response?</h3><a id="user-content-why-my-issue-was-closed-without-any-response" aria-label="Permalink: Why my issue was closed without any response?" href="#why-my-issue-was-closed-without-any-response"></a></p>
<p dir="auto">Most probably because it's a duplicated issue and there is an answer to the question, report, or proposition. Please use a search by closed issues to get an answer.
So, if your issue was closed without any response, most probably it already has a response.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Fan control</h3><a id="user-content-fan-control" aria-label="Permalink: Fan control" href="#fan-control"></a></p>
<p dir="auto">Fan control is in legacy mode. It does not receive any updates or fixes. It's not dropped from the app just because in the old Macs it works pretty acceptable. I'm open to accepting fixed or improvements (via PR) for this feature in case someone would like to help with that. But have no option and time to provide support for this feature.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Supported languages</h2><a id="user-content-supported-languages" aria-label="Permalink: Supported languages" href="#supported-languages"></a></p>
<ul dir="auto">
<li>English</li>
<li>Polski</li>
<li>Українська</li>
<li>Русский</li>
<li>中文 (简体) (thanks to <a href="https://github.com/chenguokai">chenguokai</a>, <a href="https://github.com/Tai-Zhou">Tai-Zhou</a>, and <a href="https://github.com/Jerry23011">Jerry</a>)</li>
<li>Türkçe (thanks to <a href="https://github.com/yusufozgul">yusufozgul</a>)</li>
<li>한국어 (thanks to <a href="https://github.com/escapeanaemia">escapeanaemia</a> and <a href="https://github.com/iamhslee">iamhslee</a>)</li>
<li>German (thanks to <a href="https://github.com/natterstefan">natterstefan</a> and <a href="https://github.com/aneitel">aneitel</a>)</li>
<li>中文 (繁體) (thanks to <a href="https://github.com/iamch15542">iamch15542</a> and <a href="https://github.com/jrthsr700tmax">jrthsr700tmax</a>)</li>
<li>Spanish (thanks to <a href="https://github.com/jcconca">jcconca</a>)</li>
<li>Vietnamese (thanks to <a href="https://github.com/xuandung38">HXD.VN</a>)</li>
<li>French (thanks to <a href="https://github.com/RomainLt">RomainLt</a>)</li>
<li>Italian (thanks to <a href="https://github.com/gmcinalli">gmcinalli</a>)</li>
<li>Portuguese (Brazil) (thanks to <a href="https://github.com/marcelochaves95">marcelochaves95</a> and <a href="https://github.com/pedroserigatto">pedroserigatto</a>)</li>
<li>Norwegian Bokmål (thanks to <a href="https://github.com/rubjo">rubjo</a>)</li>
<li>日本語 (thanks to <a href="https://github.com/treastrain">treastrain</a>)</li>
<li>Portuguese (Portugal) (thanks to <a href="https://github.com/AdamModus">AdamModus</a>)</li>
<li>Czech (thanks to <a href="https://github.com/mpl75">mpl75</a>)</li>
<li>Magyar (thanks to <a href="https://github.com/moriczr">moriczr</a>)</li>
<li>Bulgarian (thanks to <a href="https://github.com/zbrox">zbrox</a>)</li>
<li>Romanian (thanks to <a href="https://github.com/razluta">razluta</a>)</li>
<li>Dutch (thanks to <a href="https://github.com/ngohungphuc">ngohungphuc</a>)</li>
<li>Hrvatski (thanks to <a href="https://github.com/milotype">milotype</a>)</li>
<li>Danish (thanks to <a href="https://github.com/casperes1996">casperes1996</a> and <a href="https://github.com/aleksanderbl29">aleksanderbl29</a>)</li>
<li>Catalan (thanks to <a href="https://github.com/davidalonso">davidalonso</a>)</li>
<li>Indonesian (thanks to <a href="https://github.com/yooody">yooody</a>)</li>
<li>Hebrew (thanks to <a href="https://github.com/BadSugar">BadSugar</a>)</li>
<li>Slovenian (thanks to <a href="https://github.com/zigapovhe">zigapovhe</a>)</li>
<li>Greek (thanks to <a href="https://github.com/sudoxcess">sudoxcess</a> and <a href="https://github.com/vaionicle">vaionicle</a>)</li>
<li>Persian (thanks to <a href="https://github.com/ShawnAlisson">ShawnAlisson</a>)</li>
<li>Slovenský (thanks to <a href="https://github.com/martinbernat">martinbernat</a>)</li>
<li>Thai (thanks to <a href="https://github.com/apiphoomchu">apiphoomchu</a>)</li>
<li>Estonian (thanks to <a href="https://github.com/postylem">postylem</a>)</li>
<li>Hindi (thanks to <a href="https://github.com/patiljignesh">patiljignesh</a>)</li>
<li>Finnish (thanks to <a href="https://github.com/eightscrow">eightscrow</a>)</li>
</ul>
<p dir="auto">You can help by adding a new language or improving the existing translation.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto"><a href="https://github.com/exelban/stats/blob/master/LICENSE">MIT License</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[California Law Enforcement Misused State Databases More Than 7k Times in 2023 (422 pts)]]></title>
            <link>https://www.eff.org/deeplinks/2025/01/california-police-misused-state-databases-more-7000-times-2023</link>
            <guid>42880807</guid>
            <pubDate>Thu, 30 Jan 2025 18:48:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.eff.org/deeplinks/2025/01/california-police-misused-state-databases-more-7000-times-2023">https://www.eff.org/deeplinks/2025/01/california-police-misused-state-databases-more-7000-times-2023</a>, See on <a href="https://news.ycombinator.com/item?id=42880807">Hacker News</a></p>
Couldn't get https://www.eff.org/deeplinks/2025/01/california-police-misused-state-databases-more-7000-times-2023: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[1996 "Authentic" Beta Pokemon Cards Exposed as 2024 Prints via Printer Dots (497 pts)]]></title>
            <link>https://www.elitefourum.com/t/many-of-the-pokemon-playtest-cards-were-likely-printed-in-2024/52421</link>
            <guid>42880704</guid>
            <pubDate>Thu, 30 Jan 2025 18:38:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.elitefourum.com/t/many-of-the-pokemon-playtest-cards-were-likely-printed-in-2024/52421">https://www.elitefourum.com/t/many-of-the-pokemon-playtest-cards-were-likely-printed-in-2024/52421</a>, See on <a href="https://news.ycombinator.com/item?id=42880704">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemscope="" itemtype="http://schema.org/DiscussionForumPosting">
      <meta itemprop="headline" content="Many of the Pokemon playtest cards were likely printed in 2024">
      
      <meta itemprop="datePublished" content="2025-01-27T01:19:25Z">
        <meta itemprop="articleSection" content="Articles">
      <meta itemprop="keywords" content="prototype">
      


          <div id="post_1">
            <div>
              

                

              <p><span>
                  <time datetime="2025-01-27T01:19:25Z">
                    January 27, 2025,  1:19am
                  </time>
                  <meta itemprop="dateModified" content="2025-01-30T20:21:34Z">
              <span itemprop="position">1</span>
              </span>
            </p></div>
            <div itemprop="text">
              
<h2><a name="p-695383-about-the-tracking-dots-1" href="#p-695383-about-the-tracking-dots-1"></a>About the tracking dots</h2>
<p>Most home and office printers add metadata to printed sheets in the form of very tiny yellow dots (sometimes called a machine identification code) that can’t be seen with the naked eye. The layout of the dots are different between printer brands and some don’t leave any at all. Information like serial number and sometime the print time is encoded in these dots. They can act as a “signature” for the printer that law enforcement uses as document forensic evidence (like in cases of forged currency).</p>
<div><a href="https://efour.b-cdn.net/uploads/default/original/3X/9/d/9dc46977f44696c14a8056aee3cb3b70769819dd.png" data-download-href="https://efour.b-cdn.net/uploads/default/9dc46977f44696c14a8056aee3cb3b70769819dd" title="Example of yellow tracking dots"><img src="https://efour.b-cdn.net/uploads/default/optimized/3X/9/d/9dc46977f44696c14a8056aee3cb3b70769819dd_2_469x485.png" alt="Example of yellow tracking dots" data-base62-sha1="mvFLXIMZajZuTKxFqhV5zci9v3f" width="469" height="485" srcset="https://efour.b-cdn.net/uploads/default/optimized/3X/9/d/9dc46977f44696c14a8056aee3cb3b70769819dd_2_469x485.png, https://efour.b-cdn.net/uploads/default/optimized/3X/9/d/9dc46977f44696c14a8056aee3cb3b70769819dd_2_703x727.png 1.5x, https://efour.b-cdn.net/uploads/default/optimized/3X/9/d/9dc46977f44696c14a8056aee3cb3b70769819dd_2_938x970.png 2x"></a></div>
<p>To view the dots you need to a magnifying glass or high resolution image and you need to adjust the colour channels to emphasize the yellow. More info: <a href="https://www.elitefourum.com/t/how-to-find-yellow-dots-in-prototypes-diy-guide/52544">How to Find Yellow Dots in Prototypes - DIY Guide</a></p>
<p>Important note:</p>
<blockquote>
<p>The dots are printed on by the kind of printers you and I would buy. Things like magazines, posters, <strong>and real Pokemon cards will not have dots</strong>. They are printed by large industrial offset printers.</p>
</blockquote>
<h2><a name="p-695383-how-to-read-the-dots-2" href="#p-695383-how-to-read-the-dots-2"></a>How to read the dots</h2>
<p>Different brands use different dot encoding patterns, and not all of these can be decoded. The companies don’t reveal this information so any known pattern has been cracked by someone from the general public.</p>
<p>The most well known of these is was broken in 2005 and often referred to as the <a href="https://w2.eff.org/Privacy/printers/docucolor/">“Xerox DocuColor” code</a> - named for the printer models used to discover it. Some brands other than Xerox also use this same pattern.</p>
<p>In this case, a 15x8 grid repeats like a checkerboard across the entire printed sheet. Each repeated grid has the same dots. The columns represent binary numbers For example, the eighth column encodes the year in binary. “2024” would appear as “011000” which is 24 in binary.</p>
<div>
<div><a href="https://efour.b-cdn.net/uploads/default/original/3X/d/0/d01b574face250a2964b871f86659a2c739dbd79.jpeg" data-download-href="https://efour.b-cdn.net/uploads/default/d01b574face250a2964b871f86659a2c739dbd79" title="Eevee example"><img src="https://efour.b-cdn.net/uploads/default/optimized/3X/d/0/d01b574face250a2964b871f86659a2c739dbd79_2_573x484.jpeg" alt="Eevee example" data-base62-sha1="tGZTi5G2uBiDrWYLjlAs9ZyX329" width="573" height="484" srcset="https://efour.b-cdn.net/uploads/default/optimized/3X/d/0/d01b574face250a2964b871f86659a2c739dbd79_2_573x484.jpeg, https://efour.b-cdn.net/uploads/default/optimized/3X/d/0/d01b574face250a2964b871f86659a2c739dbd79_2_859x726.jpeg 1.5x, https://efour.b-cdn.net/uploads/default/original/3X/d/0/d01b574face250a2964b871f86659a2c739dbd79.jpeg 2x" data-dominant-color="8C8C8C"></a></div><br>
<div><a href="https://efour.b-cdn.net/uploads/default/original/3X/a/5/a57b3e4784b33462f8a02f3d0007399ea0b487bb.jpeg" data-download-href="https://efour.b-cdn.net/uploads/default/a57b3e4784b33462f8a02f3d0007399ea0b487bb" title="Xerox DocuColor code"><img src="https://efour.b-cdn.net/uploads/default/original/3X/a/5/a57b3e4784b33462f8a02f3d0007399ea0b487bb.jpeg" alt="Xerox DocuColor code" data-base62-sha1="nBUPt1ufUzc2WdeVSU8YzNjXChd" width="512" height="384"></a></div>
</div>
<p>If you have a “Xerox DocuColor” style code, you can use <a href="https://www.elitefourum.com/u/mika">@mika</a>’s decoder implementation here to decode it: <a href="https://www.elitefourum.com/t/yellow-dot-decoder-fake-prototype-playtest-cards/52472">Yellow Dot Decoder - Fake Prototype Playtest Cards</a></p>
<h2><a name="p-695383-tracking-dots-on-the-prototype-pokemon-cards-3" href="#p-695383-tracking-dots-on-the-prototype-pokemon-cards-3"></a>Tracking dots on the “prototype” Pokemon cards</h2>
<p>A slew of cards that were originally believed to be early prototype and cards used for playtesting have been appearing in public auction starting in 2024. Most are believed to trace back to Takumi Akabane - someone who was involved in the early development of the TCG.</p>
<p>Because these cards are printed from a home/office printer, many of them have tracking dots. And many have a dot patter that suggest a print date in 2024.</p>
<div>
<div><a href="https://efour.b-cdn.net/uploads/default/original/3X/2/5/25b1577fcbf93464ec1dd16f5ac86d1ecce5177f.jpeg" data-download-href="https://efour.b-cdn.net/uploads/default/25b1577fcbf93464ec1dd16f5ac86d1ecce5177f" title="Beta playtest"><img src="https://efour.b-cdn.net/uploads/default/optimized/3X/2/5/25b1577fcbf93464ec1dd16f5ac86d1ecce5177f_2_690x442.jpeg" alt="Beta playtest" data-base62-sha1="5nrCp9FFDk9WkwaR8XFXfliakEf" width="690" height="442" srcset="https://efour.b-cdn.net/uploads/default/optimized/3X/2/5/25b1577fcbf93464ec1dd16f5ac86d1ecce5177f_2_690x442.jpeg, https://efour.b-cdn.net/uploads/default/optimized/3X/2/5/25b1577fcbf93464ec1dd16f5ac86d1ecce5177f_2_1035x663.jpeg 1.5x, https://efour.b-cdn.net/uploads/default/optimized/3X/2/5/25b1577fcbf93464ec1dd16f5ac86d1ecce5177f_2_1380x884.jpeg 2x" data-dominant-color="B9BAB2"></a></div><br>
<div><a href="https://efour.b-cdn.net/uploads/default/original/3X/d/e/de5d65a6e3d94154fbf5260988dc25b96e1f7e04.jpeg" data-download-href="https://efour.b-cdn.net/uploads/default/de5d65a6e3d94154fbf5260988dc25b96e1f7e04" title="Beta playtest dots"><img src="https://efour.b-cdn.net/uploads/default/optimized/3X/d/e/de5d65a6e3d94154fbf5260988dc25b96e1f7e04_2_690x481.jpeg" alt="Beta playtest dots" data-base62-sha1="vJ86fiYoHH1H0EXweu4kSqZa2WM" width="690" height="481" srcset="https://efour.b-cdn.net/uploads/default/optimized/3X/d/e/de5d65a6e3d94154fbf5260988dc25b96e1f7e04_2_690x481.jpeg, https://efour.b-cdn.net/uploads/default/optimized/3X/d/e/de5d65a6e3d94154fbf5260988dc25b96e1f7e04_2_1035x721.jpeg 1.5x, https://efour.b-cdn.net/uploads/default/optimized/3X/d/e/de5d65a6e3d94154fbf5260988dc25b96e1f7e04_2_1380x962.jpeg 2x" data-dominant-color="7A6584"></a></div>
</div>
<div><a href="https://efour.b-cdn.net/uploads/default/original/3X/f/d/fdd0d5e157c769690bac28dd8405c9721e2bb0c3.jpeg" data-download-href="https://efour.b-cdn.net/uploads/default/fdd0d5e157c769690bac28dd8405c9721e2bb0c3" title="Dot pattern"><img src="https://efour.b-cdn.net/uploads/default/optimized/3X/f/d/fdd0d5e157c769690bac28dd8405c9721e2bb0c3_2_690x372.jpeg" alt="Dot pattern" data-base62-sha1="Adme8fO7Dy03Egh5qOc8JZAy6fV" width="690" height="372" srcset="https://efour.b-cdn.net/uploads/default/optimized/3X/f/d/fdd0d5e157c769690bac28dd8405c9721e2bb0c3_2_690x372.jpeg, https://efour.b-cdn.net/uploads/default/optimized/3X/f/d/fdd0d5e157c769690bac28dd8405c9721e2bb0c3_2_1035x558.jpeg 1.5x, https://efour.b-cdn.net/uploads/default/optimized/3X/f/d/fdd0d5e157c769690bac28dd8405c9721e2bb0c3_2_1380x744.jpeg 2x" data-dominant-color="94788E"></a></div><br>
<div><a href="https://efour.b-cdn.net/uploads/default/original/3X/2/c/2c9b57a300664e6bc7a1eb83c456aac6bcfb2d6f.png" data-download-href="https://efour.b-cdn.net/uploads/default/2c9b57a300664e6bc7a1eb83c456aac6bcfb2d6f" title="Dot pattern decoded"><img src="https://efour.b-cdn.net/uploads/default/optimized/3X/2/c/2c9b57a300664e6bc7a1eb83c456aac6bcfb2d6f_2_535x485.png" alt="Dot pattern decoded" data-base62-sha1="6mBPlfx8Nvv55TW5IaL7PwOQxxJ" width="535" height="485" srcset="https://efour.b-cdn.net/uploads/default/optimized/3X/2/c/2c9b57a300664e6bc7a1eb83c456aac6bcfb2d6f_2_535x485.png, https://efour.b-cdn.net/uploads/default/optimized/3X/2/c/2c9b57a300664e6bc7a1eb83c456aac6bcfb2d6f_2_802x727.png 1.5x, https://efour.b-cdn.net/uploads/default/optimized/3X/2/c/2c9b57a300664e6bc7a1eb83c456aac6bcfb2d6f_2_1070x970.png 2x" data-dominant-color="0E0E0E"></a></div>
<p>Note the decoding result. We see this serial number reoccur across many cards.<br>
<img src="https://efour.b-cdn.net/uploads/default/optimized/3X/6/f/6fef7f423366310e609fdb218a7caf204566a761_2_690x46.png" alt="image" data-base62-sha1="fYe7oXZEdkg8gPmcEM0rwhiUqSB" width="690" height="46" srcset="https://efour.b-cdn.net/uploads/default/optimized/3X/6/f/6fef7f423366310e609fdb218a7caf204566a761_2_690x46.png, https://efour.b-cdn.net/uploads/default/optimized/3X/6/f/6fef7f423366310e609fdb218a7caf204566a761_2_1035x69.png 1.5x, https://efour.b-cdn.net/uploads/default/original/3X/6/f/6fef7f423366310e609fdb218a7caf204566a761.png 2x" data-dominant-color="1D1D1D"></p>
<p>Checking the dots like the above example can be done using public images if they are sufficiently high resolution. Such as <a href="https://sales-history.fanaticscollect.com/items/PREMIER13649">this</a> signed one where the autograph was witnessed by CGC:</p>
<div><a href="https://efour.b-cdn.net/uploads/default/original/3X/4/e/4e8f678dfce2ea8d99f6fded64d4fa4d2557222f.jpeg" data-download-href="https://efour.b-cdn.net/uploads/default/4e8f678dfce2ea8d99f6fded64d4fa4d2557222f" title="1996 Pokemon Japanese Beta Playtest Mounted On Card Eevee AUTO CGC 10 GEM MINT"><img src="https://efour.b-cdn.net/uploads/default/optimized/3X/4/e/4e8f678dfce2ea8d99f6fded64d4fa4d2557222f_2_287x485.jpeg" alt="1996 Pokemon Japanese Beta Playtest Mounted On Card Eevee AUTO CGC 10 GEM MINT" data-base62-sha1="bcYu62gYh0s3KxacexhI8IaLPIj" width="287" height="485" srcset="https://efour.b-cdn.net/uploads/default/optimized/3X/4/e/4e8f678dfce2ea8d99f6fded64d4fa4d2557222f_2_287x485.jpeg, https://efour.b-cdn.net/uploads/default/optimized/3X/4/e/4e8f678dfce2ea8d99f6fded64d4fa4d2557222f_2_430x727.jpeg 1.5x, https://efour.b-cdn.net/uploads/default/optimized/3X/4/e/4e8f678dfce2ea8d99f6fded64d4fa4d2557222f_2_574x970.jpeg 2x" data-dominant-color="948B8F"></a></div><br>
<div><a href="https://efour.b-cdn.net/uploads/default/original/3X/a/1/a1b58f2c10a74dbcf8c49d18780d33f65677a66e.jpeg" data-download-href="https://efour.b-cdn.net/uploads/default/a1b58f2c10a74dbcf8c49d18780d33f65677a66e" title="image"><img src="https://efour.b-cdn.net/uploads/default/optimized/3X/a/1/a1b58f2c10a74dbcf8c49d18780d33f65677a66e_2_627x485.jpeg" alt="image" data-base62-sha1="n4xRqhvPAt1Qe20rYdLSTCXnFMO" width="627" height="485" srcset="https://efour.b-cdn.net/uploads/default/optimized/3X/a/1/a1b58f2c10a74dbcf8c49d18780d33f65677a66e_2_627x485.jpeg, https://efour.b-cdn.net/uploads/default/optimized/3X/a/1/a1b58f2c10a74dbcf8c49d18780d33f65677a66e_2_940x727.jpeg 1.5x, https://efour.b-cdn.net/uploads/default/optimized/3X/a/1/a1b58f2c10a74dbcf8c49d18780d33f65677a66e_2_1254x970.jpeg 2x" data-dominant-color="ACA9A9"></a></div>
<p>The dots reveal the 2024 year and the same printer serial number.</p>
<h2><a name="p-695383-variant-breakdown-4" href="#p-695383-variant-breakdown-4"></a>Variant breakdown</h2>
<p>The development of the Pokemon TCG was an iterative process, and many prototype and playtest variants have surfaced. Based on evidence inherent in the cards, this is what I can put together to the best of my ability</p>
<blockquote>
<p>Note: only Alpha playtest and Beta presentation have any documented evidence of their existence that predates 2024</p>
</blockquote>
<h2><a name="p-695383-alpha-prototypes-5" href="#p-695383-alpha-prototypes-5"></a>Alpha prototypes</h2>
<div><a href="https://efour.b-cdn.net/uploads/default/original/3X/a/4/a4512f1764633bde48c845a270f01c69f0e3e7a9.jpeg" data-download-href="https://efour.b-cdn.net/uploads/default/a4512f1764633bde48c845a270f01c69f0e3e7a9" title="Alpha prototype (HQ)"><img src="https://efour.b-cdn.net/uploads/default/optimized/3X/a/4/a4512f1764633bde48c845a270f01c69f0e3e7a9_2_148x242.jpeg" alt="Alpha prototype (HQ)" data-base62-sha1="nrCeWgTHLWgdDdFG7VRaJxmO42J" width="148" height="242" srcset="https://efour.b-cdn.net/uploads/default/optimized/3X/a/4/a4512f1764633bde48c845a270f01c69f0e3e7a9_2_148x242.jpeg, https://efour.b-cdn.net/uploads/default/optimized/3X/a/4/a4512f1764633bde48c845a270f01c69f0e3e7a9_2_222x363.jpeg 1.5x, https://efour.b-cdn.net/uploads/default/optimized/3X/a/4/a4512f1764633bde48c845a270f01c69f0e3e7a9_2_296x484.jpeg 2x" data-dominant-color="ABAAAA"></a></div> <div><a href="https://efour.b-cdn.net/uploads/default/original/3X/1/3/13d0de7c94f89f0eca1915f8eb5a56e4d6c8e8ad.jpeg" data-download-href="https://efour.b-cdn.net/uploads/default/13d0de7c94f89f0eca1915f8eb5a56e4d6c8e8ad" title="Alpha prototypes (LQ)"><img src="https://efour.b-cdn.net/uploads/default/optimized/3X/1/3/13d0de7c94f89f0eca1915f8eb5a56e4d6c8e8ad_2_144x242.jpeg" alt="Alpha prototypes (LQ)" data-base62-sha1="2Pizsn0exyM8urlenOCeV7FIyPX" width="144" height="242" srcset="https://efour.b-cdn.net/uploads/default/optimized/3X/1/3/13d0de7c94f89f0eca1915f8eb5a56e4d6c8e8ad_2_144x242.jpeg, https://efour.b-cdn.net/uploads/default/optimized/3X/1/3/13d0de7c94f89f0eca1915f8eb5a56e4d6c8e8ad_2_216x363.jpeg 1.5x, https://efour.b-cdn.net/uploads/default/optimized/3X/1/3/13d0de7c94f89f0eca1915f8eb5a56e4d6c8e8ad_2_288x484.jpeg 2x" data-dominant-color="A7A7AA"></a></div>
<ul>
<li>there are no yellow tracking dots since they were printed in b&amp;w</li>
<li>2 variants, we can call them HQ and LQ: <a href="https://www.elitefourum.com/t/many-of-the-pokemon-playtest-cards-were-likely-printed-in-2024/52421/8">Many of the Pokemon playtest cards were likely printed in 2024 - #8 by pfm</a></li>
<li>easily distinguished by the background</li>
<li>there is one set of HQ, all other copies appear to be LQ</li>
<li>LQ appear to be copies of HQ, as they pick up the same printing artifacts seen on the HQ version.</li>
</ul>
<p>HQ: inconclusive.<br>
LQ: likely a copy of HQ, when the copy was made is inconclusive.</p>
<h2><a name="p-695383-alpha-prototypes-6" href="#p-695383-alpha-prototypes-6"></a>Alpha prototypes</h2>
<div><a href="https://efour.b-cdn.net/uploads/default/original/3X/6/8/680a9e1ae5c0b31ed6bcd069cc417a75523123fb.jpeg" data-download-href="https://efour.b-cdn.net/uploads/default/680a9e1ae5c0b31ed6bcd069cc417a75523123fb" title="Alpha prototype with 1996 dots"><img src="https://efour.b-cdn.net/uploads/default/optimized/3X/6/8/680a9e1ae5c0b31ed6bcd069cc417a75523123fb_2_145x242.jpeg" alt="Alpha prototype with 1996 dots" data-base62-sha1="eQop6USu0XzLds97q8UbPQ5PMIr" width="145" height="242" srcset="https://efour.b-cdn.net/uploads/default/optimized/3X/6/8/680a9e1ae5c0b31ed6bcd069cc417a75523123fb_2_145x242.jpeg, https://efour.b-cdn.net/uploads/default/optimized/3X/6/8/680a9e1ae5c0b31ed6bcd069cc417a75523123fb_2_217x363.jpeg 1.5x, https://efour.b-cdn.net/uploads/default/optimized/3X/6/8/680a9e1ae5c0b31ed6bcd069cc417a75523123fb_2_290x484.jpeg 2x" data-dominant-color="8B8E7F"></a></div> <div><a href="https://efour.b-cdn.net/uploads/default/original/3X/a/6/a67bca55b4ab4def189c6f31c4d90edb2482465d.jpeg" data-download-href="https://efour.b-cdn.net/uploads/default/a67bca55b4ab4def189c6f31c4d90edb2482465d" title="Alpha prototype with Konica Minolta-style dots"><img src="https://efour.b-cdn.net/uploads/default/optimized/3X/a/6/a67bca55b4ab4def189c6f31c4d90edb2482465d_2_148x242.jpeg" alt="Alpha prototype with Konica Minolta-style dots" data-base62-sha1="nKMtK7omUOX1qNSqi57rYW62RRz" width="148" height="242" srcset="https://efour.b-cdn.net/uploads/default/optimized/3X/a/6/a67bca55b4ab4def189c6f31c4d90edb2482465d_2_148x242.jpeg, https://efour.b-cdn.net/uploads/default/optimized/3X/a/6/a67bca55b4ab4def189c6f31c4d90edb2482465d_2_222x363.jpeg 1.5x, https://efour.b-cdn.net/uploads/default/optimized/3X/a/6/a67bca55b4ab4def189c6f31c4d90edb2482465d_2_296x484.jpeg 2x" data-dominant-color="A4A7A3"></a></div>
<ul>
<li>Most carry a unique dot pattern (different from beta) <a href="https://www.elitefourum.com/t/many-of-the-pokemon-playtest-cards-were-likely-printed-in-2024/52421/18">Many of the Pokemon playtest cards were likely printed in 2024 - #18 by pfm</a></li>
<li>Likely from a Konica Minolta (Japanese brand) or Epson printer <a href="https://www.elitefourum.com/t/many-of-the-pokemon-playtest-cards-were-likely-printed-in-2024/52421/421">Many of the Pokemon playtest cards were likely printed in 2024 - #421 by pfm</a></li>
<li>Some copies have BOTH alpha dots and “xerox” dots that can be dated. The hypothesis is that some alpha cards were scanned/copied (picking up alpha dots) and reprinted in 2024: <a href="https://www.elitefourum.com/t/many-of-the-pokemon-playtest-cards-were-likely-printed-in-2024/52421/143">Many of the Pokemon playtest cards were likely printed in 2024 - #143 by pfm</a></li>
<li>the alpha with inflated pop reports are more likely to have the double pattern (many trainers): <a href="https://www.cgccards.com/population-report/tcg/pok%C3%A9mon/2/test-prints-oddities-other/692/alpha-playtest-thick-lines/13952?populationID=1288703">https://www.cgccards.com/population-report/tcg/pokémon/2/test-prints-oddities-other/692/alpha-playtest-thick-lines/13952?populationID=1288703</a></li>
<li>a small handful have noticeably higher quality backs such as the three starters and this gyarados: <a href="https://www.elitefourum.com/t/many-of-the-pokemon-playtest-cards-were-likely-printed-in-2024/52421/59">Many of the Pokemon playtest cards were likely printed in 2024 - #59 by pfm</a></li>
</ul>
<p>High quality back:  dated to 1996 for the three starters<br>
Alpha pattern only: inconclusive.<br>
Double dot pattern: all observed copies so far have 2024 dots</p>
<h2><a name="p-695383-beta-playtest-7" href="#p-695383-beta-playtest-7"></a>Beta playtest</h2>
<div><a href="https://efour.b-cdn.net/uploads/default/original/3X/d/f/dfa320261105937b9a5517ad96ed1567b5982003.jpeg" data-download-href="https://efour.b-cdn.net/uploads/default/dfa320261105937b9a5517ad96ed1567b5982003" title="Beta playtest (HQ)"><img src="https://efour.b-cdn.net/uploads/default/optimized/3X/d/f/dfa320261105937b9a5517ad96ed1567b5982003_2_144x242.jpeg" alt="Beta playtest (HQ)" data-base62-sha1="vUnYbXZqNu0PKhDMj4dPJrOyOUX" width="144" height="242" srcset="https://efour.b-cdn.net/uploads/default/optimized/3X/d/f/dfa320261105937b9a5517ad96ed1567b5982003_2_144x242.jpeg, https://efour.b-cdn.net/uploads/default/optimized/3X/d/f/dfa320261105937b9a5517ad96ed1567b5982003_2_216x363.jpeg 1.5x, https://efour.b-cdn.net/uploads/default/optimized/3X/d/f/dfa320261105937b9a5517ad96ed1567b5982003_2_288x484.jpeg 2x" data-dominant-color="8B9098"></a></div> <div><a href="https://efour.b-cdn.net/uploads/default/original/3X/d/9/d90c9616be1c6a6faf70715f6038128eae99431a.jpeg" data-download-href="https://efour.b-cdn.net/uploads/default/d90c9616be1c6a6faf70715f6038128eae99431a" title="Beta playtest (LQ)"><img src="https://efour.b-cdn.net/uploads/default/optimized/3X/d/9/d90c9616be1c6a6faf70715f6038128eae99431a_2_148x242.jpeg" alt="Beta playtest (LQ)" data-base62-sha1="uY6zMhLB07I2TWjLaHOa4472LLs" width="148" height="242" srcset="https://efour.b-cdn.net/uploads/default/optimized/3X/d/9/d90c9616be1c6a6faf70715f6038128eae99431a_2_148x242.jpeg, https://efour.b-cdn.net/uploads/default/optimized/3X/d/9/d90c9616be1c6a6faf70715f6038128eae99431a_2_222x363.jpeg 1.5x, https://efour.b-cdn.net/uploads/default/optimized/3X/d/9/d90c9616be1c6a6faf70715f6038128eae99431a_2_296x484.jpeg 2x" data-dominant-color="9EA4B2"></a></div>
<ul>
<li>two variants, HQ and LQ: <a href="https://www.elitefourum.com/t/many-of-the-pokemon-playtest-cards-were-likely-printed-in-2024/52421/299">Many of the Pokemon playtest cards were likely printed in 2024 - #299 by pfm</a></li>
<li>HQ has yet to be observed with any dots</li>
<li>LQ has always been observed with 2024 dots</li>
<li>there only only appears to be 1 HQ beta per Pokemon, implying the vast majority of betas have 2024 dots</li>
<li>HQ betas have subtle traces of the alpha pattern on the back, suggesting that the HQ beta back was scanned/copied from a card that has the alpha print pattern - this implies the HQ betas were printed after the alphas and the authenticity is contingent on the authenticity of the alphas: <a href="https://www.elitefourum.com/t/many-of-the-pokemon-playtest-cards-were-likely-printed-in-2024/52421/745">Many of the Pokemon playtest cards were likely printed in 2024 - #745 by pfm</a></li>
<li>it is unlikely that the LQ are simply scans/copies of HQ given that not a single print artifact or dirt spec has yet been observed to be transferred from HQ to LQ</li>
</ul>
<p>HQ beta: inconclusive<br>
LQ beta: all observed copies so far have 2024 dots</p>
<h2><a name="p-695383-delta-prototypes-and-alpha-presentation-8" href="#p-695383-delta-prototypes-and-alpha-presentation-8"></a>Delta prototypes and Alpha presentation</h2>
<ul>
<li>I haven’t said too much about these yet but so far they all look similar to LQ beta and all observed copies so far have 2024 dots</li>
</ul>
<h2><a name="p-695383-beta-presentation-9" href="#p-695383-beta-presentation-9"></a>Beta presentation</h2>
<ul>
<li>No hi-res scan available yet, but I see no evidence of dots so far</li>
<li>They match well with the corocoro images and the print quality suggests a different printer was used for these than anything mentioned so far</li>
<li>inconclusive so far</li>
</ul>
<h2><a name="p-695383-acknowledgements-10" href="#p-695383-acknowledgements-10"></a>Acknowledgements</h2>
<p>Thanks to <a href="https://www.elitefourum.com/u/tediorso">@tediorso</a> for suggesting this avenue of investigation.<br>
Thanks to <a href="https://www.elitefourum.com/u/mika">@mika</a> <a href="https://www.elitefourum.com/u/linkdu83">@linkdu83</a> and <a href="https://www.elitefourum.com/u/humanforscale">@HumanForScale</a> for helping with data collection and analysis<br>
Thanks to <a href="https://www.elitefourum.com/u/banks">@BANKS</a> for working on information summary</p>
            </div>

            

                
          </div>
          <div id="post_2" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://www.elitefourum.com/u/Dyl"><span itemprop="name">Dyl</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-01-27T01:40:25Z">
                    January 27, 2025,  1:40am
                  </time>
                  <meta itemprop="dateModified" content="2025-01-27T01:40:25Z">
              <span itemprop="position">2</span>
              </span>
            </p>
            <div itemprop="text">
              <p>Holy Shit</p>
<p><img src="https://media0.giphy.com/media/v1.Y2lkPTc5MGI3NjExazV6dXMyZXkyajI2c2tjN21yc2c5b2ZyNm10ZHNpdXJnbGNqOGpqYyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/IO0gM5bNgCuZFWkQPF/giphy.webp" width="480" height="480" title="Episode 7 Wow GIF by Wrexham AFC - Find &amp; Share on GIPHY"></p>
            </div>

            

          </div>
          <div id="post_3" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            
            <p>Are any of these 2024 prints raw or have they been “authenticated”?</p>

            

          </div>
          <div id="post_5" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://www.elitefourum.com/u/casual"><span itemprop="name">casual</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-01-27T01:45:22Z">
                    January 27, 2025,  1:45am
                  </time>
                  <meta itemprop="dateModified" content="2025-01-27T01:45:22Z">
              <span itemprop="position">5</span>
              </span>
            </p>
            <div itemprop="text">
              <p>Incredible research.</p>
<p>What a dangerous game people are playing buying these, pkonno really DID have a printer in his basement.</p>
            </div>

            

          </div>
          <div id="post_6" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://www.elitefourum.com/u/BANKS"><span itemprop="name">BANKS</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-01-27T01:46:04Z">
                    January 27, 2025,  1:46am
                  </time>
                  <meta itemprop="dateModified" content="2025-01-27T01:46:04Z">
              <span itemprop="position">6</span>
              </span>
            </p>
            <p>Taking into consideration the personal stake that you have in these cards, you’re the goat</p>

            

          </div>
          <div id="post_7" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://www.elitefourum.com/u/lyleberr"><span itemprop="name">lyleberr</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-01-27T01:46:47Z">
                    January 27, 2025,  1:46am
                  </time>
                  <meta itemprop="dateModified" content="2025-01-27T01:46:47Z">
              <span itemprop="position">7</span>
              </span>
            </p>
            <p>Does it have to be a scan or can you do this from a photo of a playtest? Does it have to have color?</p>

            

          </div>
          <div id="post_8" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://www.elitefourum.com/u/pfm"><span itemprop="name">pfm</span></a>
                
              </span></p>

              <p><span>
                  <time itemprop="datePublished" datetime="2025-01-27T01:46:50Z">
                    January 27, 2025,  1:46am
                  </time>
                  <meta itemprop="dateModified" content="2025-01-27T01:46:50Z">
              <span itemprop="position">8</span>
              </span>
            </p></div>
            <div itemprop="text">
              <p>Another observation I want to mention.</p>
<p>The alpha prototypes were likely designed in a tool called ClarisWorks 4 which was available in 1995.</p>
<p>Here I emulated an old mac and ran it:</p>
<div><a href="https://efour.b-cdn.net/uploads/default/original/3X/2/d/2d23a2c16129614341eb58c419ac14666af37fef.png" data-download-href="https://efour.b-cdn.net/uploads/default/2d23a2c16129614341eb58c419ac14666af37fef" title="clarisworks_pikachu"><img src="https://efour.b-cdn.net/uploads/default/original/3X/2/d/2d23a2c16129614341eb58c419ac14666af37fef.png" alt="clarisworks_pikachu" data-base62-sha1="6rjPNqWkOuMhmIMeGN9SReKng4f" width="644" height="484"></a></div>
<p>When looking at alpha prototypes there is clearly a set that is higher quality than the others. Consider Pikachu</p>

<p>Consider the dots in the background:</p>
<div data-masonry-gallery="">
<div><a href="https://efour.b-cdn.net/uploads/default/original/3X/9/2/9259b7e71edc6e0e223c70e24df2f16044c635a3.jpeg" data-download-href="https://efour.b-cdn.net/uploads/default/9259b7e71edc6e0e223c70e24df2f16044c635a3" title="image"><img src="https://efour.b-cdn.net/uploads/default/optimized/3X/9/2/9259b7e71edc6e0e223c70e24df2f16044c635a3_2_647x485.jpeg" alt="image" data-base62-sha1="kSFVKkv9znEPcLJf4jwyzwKkEBd" width="647" height="485" srcset="https://efour.b-cdn.net/uploads/default/optimized/3X/9/2/9259b7e71edc6e0e223c70e24df2f16044c635a3_2_647x485.jpeg, https://efour.b-cdn.net/uploads/default/optimized/3X/9/2/9259b7e71edc6e0e223c70e24df2f16044c635a3_2_970x727.jpeg 1.5x, https://efour.b-cdn.net/uploads/default/optimized/3X/9/2/9259b7e71edc6e0e223c70e24df2f16044c635a3_2_1294x970.jpeg 2x" data-dominant-color="E0DFE3"></a></div><br>
<div><a href="https://efour.b-cdn.net/uploads/default/original/3X/d/f/dfa6901400bfbc639fecf3eb999e79f1fc57e1b3.jpeg" data-download-href="https://efour.b-cdn.net/uploads/default/dfa6901400bfbc639fecf3eb999e79f1fc57e1b3" title="image"><img src="https://efour.b-cdn.net/uploads/default/optimized/3X/d/f/dfa6901400bfbc639fecf3eb999e79f1fc57e1b3_2_637x485.jpeg" alt="image" data-base62-sha1="vUvkM1DuN5goEgm2cpCMK7lOZTd" width="637" height="485" srcset="https://efour.b-cdn.net/uploads/default/optimized/3X/d/f/dfa6901400bfbc639fecf3eb999e79f1fc57e1b3_2_637x485.jpeg, https://efour.b-cdn.net/uploads/default/optimized/3X/d/f/dfa6901400bfbc639fecf3eb999e79f1fc57e1b3_2_955x727.jpeg 1.5x, https://efour.b-cdn.net/uploads/default/optimized/3X/d/f/dfa6901400bfbc639fecf3eb999e79f1fc57e1b3_2_1274x970.jpeg 2x" data-dominant-color="DDDCE2"></a></div>
</div>
<p>Also note the shared print artifact that goes through the “Pikachu” text.</p>
<p>It’s not limited to Pikachu.</p>

            </div>

            

          </div>
          <div id="post_9" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://www.elitefourum.com/u/casual"><span itemprop="name">casual</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-01-27T01:46:53Z">
                    January 27, 2025,  1:46am
                  </time>
                  <meta itemprop="dateModified" content="2025-01-27T01:46:53Z">
              <span itemprop="position">9</span>
              </span>
            </p>
            <div itemprop="text">
              <p>Personally, I think it makes sense to go after the grading companies here</p>
<p>This is a disaster</p>
<p>They should be fully refunding people</p>
            </div>

            

          </div>
          <div id="post_10" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://www.elitefourum.com/u/pfm"><span itemprop="name">pfm</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-01-27T01:47:13Z">
                    January 27, 2025,  1:47am
                  </time>
                  <meta itemprop="dateModified" content="2025-01-27T01:47:13Z">
              <span itemprop="position">10</span>
              </span>
            </p>
            

            

          </div>
          <div itemprop="comment" id="post_11" itemscope="" itemtype="http://schema.org/Comment"><a href="https://efour.b-cdn.net/uploads/default/original/3X/d/a/da4481234122cefc0f58c78442cae136fb5e1f6c.jpeg" data-download-href="https://efour.b-cdn.net/uploads/default/da4481234122cefc0f58c78442cae136fb5e1f6c" title="image"><img src="https://efour.b-cdn.net/uploads/default/optimized/3X/d/a/da4481234122cefc0f58c78442cae136fb5e1f6c_2_245x242.jpeg" alt="image" data-base62-sha1="v8SReEYq9rNAiDadtyZu2hidIHW" width="245" height="242" srcset="https://efour.b-cdn.net/uploads/default/optimized/3X/d/a/da4481234122cefc0f58c78442cae136fb5e1f6c_2_245x242.jpeg, https://efour.b-cdn.net/uploads/default/optimized/3X/d/a/da4481234122cefc0f58c78442cae136fb5e1f6c_2_367x363.jpeg 1.5x, https://efour.b-cdn.net/uploads/default/optimized/3X/d/a/da4481234122cefc0f58c78442cae136fb5e1f6c_2_490x484.jpeg 2x" data-dominant-color="876E78"></a></div>
          <div id="post_12" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://www.elitefourum.com/u/ddk"><span itemprop="name">ddk</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-01-27T01:49:58Z">
                    January 27, 2025,  1:49am
                  </time>
                  <meta itemprop="dateModified" content="2025-01-27T01:49:58Z">
              <span itemprop="position">12</span>
              </span>
            </p>
            <p>The tragic irony of pfm believing in these cards for months only to be the one to discover the truth behind them…</p>

            

          </div>
          <div id="post_13" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            
            <p>Proud of you <a href="https://www.elitefourum.com/u/pfm">@pfm</a>. You didn’t have to release this and yet you did all the same. You solved one of the biggest mysteries this hobby has seen in years. If anyone was going to do it, it was going to be you.</p>

            

          </div>
          <div id="post_14" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://www.elitefourum.com/u/Dirge"><span itemprop="name">Dirge</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-01-27T01:51:17Z">
                    January 27, 2025,  1:51am
                  </time>
                  <meta itemprop="dateModified" content="2025-01-27T01:51:17Z">
              <span itemprop="position">14</span>
              </span>
            </p>
            <div itemprop="text">
              <p>I am stunned and impressed beyond words. Holy shit.</p>
<p>Does this mean they’re all fakes or just some of them?</p>
            </div>

            

          </div>
          <div id="post_15" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://www.elitefourum.com/u/BANKS"><span itemprop="name">BANKS</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-01-27T01:51:21Z">
                    January 27, 2025,  1:51am
                  </time>
                  <meta itemprop="dateModified" content="2025-01-27T01:51:21Z">
              <span itemprop="position">15</span>
              </span>
            </p>
            <p>That’s what separates Greek heroes from timmys, PFM went the distance</p>

            

          </div>
          <div id="post_16" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://www.elitefourum.com/u/Will"><span itemprop="name">Will</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-01-27T01:51:35Z">
                    January 27, 2025,  1:51am
                  </time>
                  <meta itemprop="dateModified" content="2025-01-27T01:51:35Z">
              <span itemprop="position">16</span>
              </span>
            </p>
            <p>Great analysis as always pfm, amazing stuff</p>

            

          </div>
          <div id="post_17" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://www.elitefourum.com/u/Alec"><span itemprop="name">Alec</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-01-27T01:54:08Z">
                    January 27, 2025,  1:54am
                  </time>
                  <meta itemprop="dateModified" content="2025-01-27T01:54:08Z">
              <span itemprop="position">17</span>
              </span>
            </p>
            <p>I think the most reasonable take is some are certainly not 1995/1996 playtest cards as we’ve been led to believe, and the rest are at least questionable.</p>

            

          </div>
          <div id="post_18" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://www.elitefourum.com/u/pfm"><span itemprop="name">pfm</span></a>
                
              </span></p>

              <p><span>
                  <time itemprop="datePublished" datetime="2025-01-27T01:56:47Z">
                    January 27, 2025,  1:56am
                  </time>
                  <meta itemprop="dateModified" content="2025-01-27T01:56:47Z">
              <span itemprop="position">18</span>
              </span>
            </p></div>
            <div itemprop="text">
              
<p>I’m still digging into it. All I can say is that I’ve seen evidence of 3 different printers for the coloured variants and one of them has 2024 in the metadata (majority of the beta). The majority of the alpha have a different set of print dots.</p>
<p>for example:<br>
</p><div><a href="https://efour.b-cdn.net/uploads/default/original/3X/4/7/47a6b0190821d32e8a3d909e6359843596a7a21c.jpeg" data-download-href="https://efour.b-cdn.net/uploads/default/47a6b0190821d32e8a3d909e6359843596a7a21c" title="image"><img src="https://efour.b-cdn.net/uploads/default/optimized/3X/4/7/47a6b0190821d32e8a3d909e6359843596a7a21c_2_690x395.jpeg" alt="image" data-base62-sha1="adR1HS0CPjkcR1XaLMVbIVDHeXy" width="690" height="395" srcset="https://efour.b-cdn.net/uploads/default/optimized/3X/4/7/47a6b0190821d32e8a3d909e6359843596a7a21c_2_690x395.jpeg, https://efour.b-cdn.net/uploads/default/optimized/3X/4/7/47a6b0190821d32e8a3d909e6359843596a7a21c_2_1035x592.jpeg 1.5x, https://efour.b-cdn.net/uploads/default/optimized/3X/4/7/47a6b0190821d32e8a3d909e6359843596a7a21c_2_1380x790.jpeg 2x" data-dominant-color="D38FCD"></a></div>
<p>I have a guess at the printer brand. I don’t believe print time is encoded but the printer model should be in there.</p>
<p>In other words:</p>
<ul>
<li>High quality beta = no dots</li>
<li>Low quality beta = dots in the OP</li>
<li>Alpha playtest = dots above</li>
</ul>
<p>This is my general observation, there are exceptions</p>
            </div>

            

          </div>
          <div itemprop="comment" id="post_19" itemscope="" itemtype="http://schema.org/Comment">
              <p>Well there you go. I won’t speak in absolutes as it still seems like there is some information to come, but if this proves to reflect the entirety of the playtest cards it can’t be seen as anything other than an epic failure of CGC’s authentication practices.</p>

<p>This was the original problem with the cards, that we had to rely on “trust me bro” from CGC and Akabane, with none of the in-depth analysis that CGC is known for. Because they didn’t do it, pfm had to, and digs up very questionable results.</p>
<p>I am hard pressed to imagine a more disingenuous and exploitative process. It appears like these cards and their release were deliberately engineered to fool the community. CGC at best was negligent in their duty as a grading company, and at worst was complicit in the process.</p>
            </div>
          <div id="post_20" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://www.elitefourum.com/u/Dirge"><span itemprop="name">Dirge</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-01-27T02:01:24Z">
                    January 27, 2025,  2:01am
                  </time>
                  <meta itemprop="dateModified" content="2025-01-27T02:01:24Z">
              <span itemprop="position">20</span>
              </span>
            </p>
            <p>Should I be impressed that you managed to do all this or should I be horrified that CGC didn’t have the means to catch this?</p>

            

          </div>
          <div id="post_21" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            
            <p>At times like these, I’m glad I’m risk adverse. I wanted to get some but there were too many unanswered questions.</p>

            

          </div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Quaternions and spherical trigonometry (108 pts)]]></title>
            <link>https://terrytao.wordpress.com/2024/12/19/quaternions-and-spherical-trigonometry/</link>
            <guid>42880242</guid>
            <pubDate>Thu, 30 Jan 2025 17:57:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://terrytao.wordpress.com/2024/12/19/quaternions-and-spherical-trigonometry/">https://terrytao.wordpress.com/2024/12/19/quaternions-and-spherical-trigonometry/</a>, See on <a href="https://news.ycombinator.com/item?id=42880242">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
<p>
 Hamilton’s <a href="https://en.wikipedia.org/wiki/Quaternion">quaternion number</a> system <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BH%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BH%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BH%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\mathbb{H}}"> is a non-commutative extension of the complex numbers, consisting of numbers of the form <img src="https://s0.wp.com/latex.php?latex=%7Bt+%2B+xi+%2B+yj+%2B+zk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bt+%2B+xi+%2B+yj+%2B+zk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bt+%2B+xi+%2B+yj+%2B+zk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{t + xi + yj + zk}"> where <img src="https://s0.wp.com/latex.php?latex=%7Bt%2Cx%2Cy%2Cz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bt%2Cx%2Cy%2Cz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bt%2Cx%2Cy%2Cz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{t,x,y,z}"> are real numbers, and <img src="https://s0.wp.com/latex.php?latex=%7Bi%2Cj%2Ck%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bi%2Cj%2Ck%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bi%2Cj%2Ck%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{i,j,k}"> are anti-commuting square roots of <img src="https://s0.wp.com/latex.php?latex=%7B-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{-1}"> with <img src="https://s0.wp.com/latex.php?latex=%7Bij%3Dk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bij%3Dk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bij%3Dk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{ij=k}">, <img src="https://s0.wp.com/latex.php?latex=%7Bjk%3Di%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bjk%3Di%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bjk%3Di%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{jk=i}">, <img src="https://s0.wp.com/latex.php?latex=%7Bki%3Dj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bki%3Dj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bki%3Dj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{ki=j}">. While they are non-commutative, they do keep many other properties of the complex numbers: 

</p><ul> <li> Being non-commutative, the quaternions do not form a field. However, they are still a <a href="https://en.wikipedia.org/wiki/Division_ring">skew field</a> (or division ring): multiplication is associative, and every non-zero quaternion has a unique multiplicative inverse. </li><li> Like the complex numbers, the quaternions have a conjugation <p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Coverline%7Bt%2Bxi%2Byj%2Bzk%7D+%3A%3D+t-xi-yj-zk%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Coverline%7Bt%2Bxi%2Byj%2Bzk%7D+%3A%3D+t-xi-yj-zk%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Coverline%7Bt%2Bxi%2Byj%2Bzk%7D+%3A%3D+t-xi-yj-zk%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  \overline{t+xi+yj+zk} := t-xi-yj-zk,"></p>
 although this is now an <a href="https://en.wikipedia.org/wiki/Antihomomorphism">antihomomorphism</a> rather than a homomorphism: <img src="https://s0.wp.com/latex.php?latex=%7B%5Coverline%7Bqr%7D+%3D+%5Coverline%7Br%7D%5C+%5Coverline%7Bq%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Coverline%7Bqr%7D+%3D+%5Coverline%7Br%7D%5C+%5Coverline%7Bq%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Coverline%7Bqr%7D+%3D+%5Coverline%7Br%7D%5C+%5Coverline%7Bq%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\overline{qr} = \overline{r}\ \overline{q}}">. One can then split up a quaternion <img src="https://s0.wp.com/latex.php?latex=%7Bt+%2B+xi+%2B+yj+%2B+zk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bt+%2B+xi+%2B+yj+%2B+zk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bt+%2B+xi+%2B+yj+%2B+zk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{t + xi + yj + zk}"> into its real part <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{t}"> and imaginary part <img src="https://s0.wp.com/latex.php?latex=%7Bxi%2Byj%2Bzk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bxi%2Byj%2Bzk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bxi%2Byj%2Bzk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{xi+yj+zk}"> by the familiar formulae <p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathrm%7BRe%7D+q+%3A%3D+%5Cfrac%7Bq+%2B+%5Coverline%7Bq%7D%7D%7B2%7D%3B+%5Cquad+%5Cmathrm%7BIm%7D+q+%3A%3D+%5Cfrac%7Bq+-+%5Coverline%7Bq%7D%7D%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathrm%7BRe%7D+q+%3A%3D+%5Cfrac%7Bq+%2B+%5Coverline%7Bq%7D%7D%7B2%7D%3B+%5Cquad+%5Cmathrm%7BIm%7D+q+%3A%3D+%5Cfrac%7Bq+-+%5Coverline%7Bq%7D%7D%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathrm%7BRe%7D+q+%3A%3D+%5Cfrac%7Bq+%2B+%5Coverline%7Bq%7D%7D%7B2%7D%3B+%5Cquad+%5Cmathrm%7BIm%7D+q+%3A%3D+%5Cfrac%7Bq+-+%5Coverline%7Bq%7D%7D%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  \mathrm{Re} q := \frac{q + \overline{q}}{2}; \quad \mathrm{Im} q := \frac{q - \overline{q}}{2}"></p>
 (though we now leave the imaginary part purely imaginary, as opposed to dividing by <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{i}"> in the complex case). </li><li> The inner product <p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+q%2C+r+%5Crangle+%3A%3D+%5Cmathrm%7BRe%7D+q+%5Coverline%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+q%2C+r+%5Crangle+%3A%3D+%5Cmathrm%7BRe%7D+q+%5Coverline%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+q%2C+r+%5Crangle+%3A%3D+%5Cmathrm%7BRe%7D+q+%5Coverline%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  \langle q, r \rangle := \mathrm{Re} q \overline{r}"></p>
 is symmetric and positive definite (with <img src="https://s0.wp.com/latex.php?latex=%7B1%2Ci%2Cj%2Ck%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B1%2Ci%2Cj%2Ck%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B1%2Ci%2Cj%2Ck%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{1,i,j,k}"> forming an orthonormal basis). Also, for any <img src="https://s0.wp.com/latex.php?latex=%7Bq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{q}">, <img src="https://s0.wp.com/latex.php?latex=%7Bq+%5Coverline%7Bq%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bq+%5Coverline%7Bq%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bq+%5Coverline%7Bq%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{q \overline{q}}"> is real, hence equal to <img src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+q%2C+q+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Clangle+q%2C+q+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Clangle+q%2C+q+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\langle q, q \rangle}">. Thus we have a norm <p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7Cq%7C+%3D+%5Csqrt%7Bq%5Coverline%7Bq%7D%7D+%3D+%5Csqrt%7B%5Clangle+q%2Cq+%5Crangle%7D+%3D+%5Csqrt%7Bt%5E2+%2B+x%5E2+%2B+y%5E2+%2B+z%5E2%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7Cq%7C+%3D+%5Csqrt%7Bq%5Coverline%7Bq%7D%7D+%3D+%5Csqrt%7B%5Clangle+q%2Cq+%5Crangle%7D+%3D+%5Csqrt%7Bt%5E2+%2B+x%5E2+%2B+y%5E2+%2B+z%5E2%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7Cq%7C+%3D+%5Csqrt%7Bq%5Coverline%7Bq%7D%7D+%3D+%5Csqrt%7B%5Clangle+q%2Cq+%5Crangle%7D+%3D+%5Csqrt%7Bt%5E2+%2B+x%5E2+%2B+y%5E2+%2B+z%5E2%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  |q| = \sqrt{q\overline{q}} = \sqrt{\langle q,q \rangle} = \sqrt{t^2 + x^2 + y^2 + z^2}."></p>
 Since the real numbers commute with all quaternions, we have the multiplicative property <img src="https://s0.wp.com/latex.php?latex=%7B%7Cqr%7C+%3D+%7Cq%7C+%7Cr%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%7Cqr%7C+%3D+%7Cq%7C+%7Cr%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%7Cqr%7C+%3D+%7Cq%7C+%7Cr%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{|qr| = |q| |r|}">. In particular, the unit quaternions <img src="https://s0.wp.com/latex.php?latex=%7BU%281%2C%5Cmathbb%7BH%7D%29+%3A%3D+%5C%7B+q+%5Cin+%5Cmathbb%7BH%7D%3A+%7Cq%7C%3D1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BU%281%2C%5Cmathbb%7BH%7D%29+%3A%3D+%5C%7B+q+%5Cin+%5Cmathbb%7BH%7D%3A+%7Cq%7C%3D1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BU%281%2C%5Cmathbb%7BH%7D%29+%3A%3D+%5C%7B+q+%5Cin+%5Cmathbb%7BH%7D%3A+%7Cq%7C%3D1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{U(1,\mathbb{H}) := \{ q \in \mathbb{H}: |q|=1\}}"> (also known as <img src="https://s0.wp.com/latex.php?latex=%7BSU%282%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BSU%282%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BSU%282%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{SU(2)}">, <img src="https://s0.wp.com/latex.php?latex=%7BSp%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BSp%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BSp%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{Sp(1)}">, or <img src="https://s0.wp.com/latex.php?latex=%7BSpin%283%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BSpin%283%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BSpin%283%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{Spin(3)}">) form a compact group. </li><li> We have the <a href="https://en.wikipedia.org/wiki/Trace_(linear_algebra)#Cyclic_property">cyclic trace property</a> <p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathrm%7BRe%7D%28qr%29+%3D+%5Cmathrm%7BRe%7D%28rq%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathrm%7BRe%7D%28qr%29+%3D+%5Cmathrm%7BRe%7D%28rq%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathrm%7BRe%7D%28qr%29+%3D+%5Cmathrm%7BRe%7D%28rq%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  \mathrm{Re}(qr) = \mathrm{Re}(rq)"></p>
 which allows one to take adjoints of left and right multiplication: <p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+qr%2C+s+%5Crangle+%3D+%5Clangle+q%2C+s%5Coverline%7Br%7D%5Crangle%3B+%5Cquad+%5Clangle+rq%2C+s+%5Crangle+%3D+%5Clangle+q%2C+%5Coverline%7Br%7Ds+%5Crangle&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+qr%2C+s+%5Crangle+%3D+%5Clangle+q%2C+s%5Coverline%7Br%7D%5Crangle%3B+%5Cquad+%5Clangle+rq%2C+s+%5Crangle+%3D+%5Clangle+q%2C+%5Coverline%7Br%7Ds+%5Crangle&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+qr%2C+s+%5Crangle+%3D+%5Clangle+q%2C+s%5Coverline%7Br%7D%5Crangle%3B+%5Cquad+%5Clangle+rq%2C+s+%5Crangle+%3D+%5Clangle+q%2C+%5Coverline%7Br%7Ds+%5Crangle&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  \langle qr, s \rangle = \langle q, s\overline{r}\rangle; \quad \langle rq, s \rangle = \langle q, \overline{r}s \rangle"></p>
 </li><li> As <img src="https://s0.wp.com/latex.php?latex=%7Bi%2Cj%2Ck%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bi%2Cj%2Ck%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bi%2Cj%2Ck%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{i,j,k}"> are square roots of <img src="https://s0.wp.com/latex.php?latex=%7B-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{-1}">, we have the usual Euler formulae <p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5E%7Bi%5Ctheta%7D+%3D+%5Ccos+%5Ctheta+%2B+i+%5Csin+%5Ctheta%2C+e%5E%7Bj%5Ctheta%7D+%3D+%5Ccos+%5Ctheta+%2B+j+%5Csin+%5Ctheta%2C+e%5E%7Bk%5Ctheta%7D+%3D+%5Ccos+%5Ctheta+%2B+k+%5Csin+%5Ctheta&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5E%7Bi%5Ctheta%7D+%3D+%5Ccos+%5Ctheta+%2B+i+%5Csin+%5Ctheta%2C+e%5E%7Bj%5Ctheta%7D+%3D+%5Ccos+%5Ctheta+%2B+j+%5Csin+%5Ctheta%2C+e%5E%7Bk%5Ctheta%7D+%3D+%5Ccos+%5Ctheta+%2B+k+%5Csin+%5Ctheta&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5E%7Bi%5Ctheta%7D+%3D+%5Ccos+%5Ctheta+%2B+i+%5Csin+%5Ctheta%2C+e%5E%7Bj%5Ctheta%7D+%3D+%5Ccos+%5Ctheta+%2B+j+%5Csin+%5Ctheta%2C+e%5E%7Bk%5Ctheta%7D+%3D+%5Ccos+%5Ctheta+%2B+k+%5Csin+%5Ctheta&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  e^{i\theta} = \cos \theta + i \sin \theta, e^{j\theta} = \cos \theta + j \sin \theta, e^{k\theta} = \cos \theta + k \sin \theta"></p>
 for real <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctheta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Ctheta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Ctheta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\theta}">, together with other familiar formulae such as <img src="https://s0.wp.com/latex.php?latex=%7B%5Coverline%7Be%5E%7Bi%5Ctheta%7D%7D+%3D+e%5E%7B-i%5Ctheta%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Coverline%7Be%5E%7Bi%5Ctheta%7D%7D+%3D+e%5E%7B-i%5Ctheta%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Coverline%7Be%5E%7Bi%5Ctheta%7D%7D+%3D+e%5E%7B-i%5Ctheta%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\overline{e^{i\theta}} = e^{-i\theta}}">, <img src="https://s0.wp.com/latex.php?latex=%7Be%5E%7Bi%28%5Calpha%2B%5Cbeta%29%7D+%3D+e%5E%7Bi%5Calpha%7D+e%5E%7Bi%5Cbeta%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Be%5E%7Bi%28%5Calpha%2B%5Cbeta%29%7D+%3D+e%5E%7Bi%5Calpha%7D+e%5E%7Bi%5Cbeta%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Be%5E%7Bi%28%5Calpha%2B%5Cbeta%29%7D+%3D+e%5E%7Bi%5Calpha%7D+e%5E%7Bi%5Cbeta%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{e^{i(\alpha+\beta)} = e^{i\alpha} e^{i\beta}}">, <img src="https://s0.wp.com/latex.php?latex=%7B%7Ce%5E%7Bi%5Ctheta%7D%7C+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%7Ce%5E%7Bi%5Ctheta%7D%7C+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%7Ce%5E%7Bi%5Ctheta%7D%7C+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{|e^{i\theta}| = 1}">, etc. 
</li></ul><p>

 We will use these sorts of algebraic manipulations in the sequel without further comment.
</p><p>
The unit quaternions <img src="https://s0.wp.com/latex.php?latex=%7BU%281%2C%5Cmathbb%7BH%7D%29+%3D+%5C%7B+q+%5Cin+%5Cmathbb%7BH%7D%3A+%7Cq%7C%3D1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BU%281%2C%5Cmathbb%7BH%7D%29+%3D+%5C%7B+q+%5Cin+%5Cmathbb%7BH%7D%3A+%7Cq%7C%3D1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BU%281%2C%5Cmathbb%7BH%7D%29+%3D+%5C%7B+q+%5Cin+%5Cmathbb%7BH%7D%3A+%7Cq%7C%3D1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{U(1,\mathbb{H}) = \{ q \in \mathbb{H}: |q|=1\}}"> act on the imaginary quaternions <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+xi+%2B+yj+%2B+zk%3A+x%2Cy%2Cz+%5Cin+%7B%5Cbf+R%7D%5C%7D+%5Cequiv+%7B%5Cbf+R%7D%5E3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5C%7B+xi+%2B+yj+%2B+zk%3A+x%2Cy%2Cz+%5Cin+%7B%5Cbf+R%7D%5C%7D+%5Cequiv+%7B%5Cbf+R%7D%5E3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5C%7B+xi+%2B+yj+%2B+zk%3A+x%2Cy%2Cz+%5Cin+%7B%5Cbf+R%7D%5C%7D+%5Cequiv+%7B%5Cbf+R%7D%5E3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\{ xi + yj + zk: x,y,z \in {\bf R}\} \equiv {\bf R}^3}"> by conjugation: </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++v+%5Cmapsto+q+v+%5Coverline%7Bq%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++v+%5Cmapsto+q+v+%5Coverline%7Bq%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++v+%5Cmapsto+q+v+%5Coverline%7Bq%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  v \mapsto q v \overline{q}."></p><p>
 This action is by orientation-preserving isometries, hence by rotations. It is not quite faithful, since conjugation by the unit quaternion <img src="https://s0.wp.com/latex.php?latex=%7B-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{-1}"> is the identity, but one can show that this is the only loss of faithfulness, reflecting the well known fact that <img src="https://s0.wp.com/latex.php?latex=%7BU%281%2C%5Cmathbb%7BH%7D%29+%5Cequiv+SU%282%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BU%281%2C%5Cmathbb%7BH%7D%29+%5Cequiv+SU%282%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BU%281%2C%5Cmathbb%7BH%7D%29+%5Cequiv+SU%282%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{U(1,\mathbb{H}) \equiv SU(2)}"> is a double cover of <img src="https://s0.wp.com/latex.php?latex=%7BSO%283%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BSO%283%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BSO%283%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{SO(3)}">.
</p><p>
For instance, for any real <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctheta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Ctheta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Ctheta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\theta}">, conjugation by <img src="https://s0.wp.com/latex.php?latex=%7Be%5E%7Bi%5Ctheta%2F2%7D+%3D+%5Ccos%28%5Ctheta%2F2%29+%2B+i+%5Csin%28%5Ctheta%2F2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Be%5E%7Bi%5Ctheta%2F2%7D+%3D+%5Ccos%28%5Ctheta%2F2%29+%2B+i+%5Csin%28%5Ctheta%2F2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Be%5E%7Bi%5Ctheta%2F2%7D+%3D+%5Ccos%28%5Ctheta%2F2%29+%2B+i+%5Csin%28%5Ctheta%2F2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{e^{i\theta/2} = \cos(\theta/2) + i \sin(\theta/2)}"> is a rotation by <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctheta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Ctheta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Ctheta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\theta}"> around <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{i}">: <a name="id0"></a></p><p><a name="id0"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5E%7Bi%5Ctheta%2F2%7D+i+e%5E%7B-i%5Ctheta%2F2%7D+%3D+i+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5E%7Bi%5Ctheta%2F2%7D+i+e%5E%7B-i%5Ctheta%2F2%7D+%3D+i+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5E%7Bi%5Ctheta%2F2%7D+i+e%5E%7B-i%5Ctheta%2F2%7D+%3D+i+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  e^{i\theta/2} i e^{-i\theta/2} = i \ \ \ \ \ (1)"></a></p><a name="id1"><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5E%7Bi%5Ctheta%2F2%7D+j+e%5E%7B-i%5Ctheta%2F2%7D+%3D+%5Ccos%28%5Ctheta%29+j+-+%5Csin%28%5Ctheta%29+k+%5C+%5C+%5C+%5C+%5C+%282%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5E%7Bi%5Ctheta%2F2%7D+j+e%5E%7B-i%5Ctheta%2F2%7D+%3D+%5Ccos%28%5Ctheta%29+j+-+%5Csin%28%5Ctheta%29+k+%5C+%5C+%5C+%5C+%5C+%282%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5E%7Bi%5Ctheta%2F2%7D+j+e%5E%7B-i%5Ctheta%2F2%7D+%3D+%5Ccos%28%5Ctheta%29+j+-+%5Csin%28%5Ctheta%29+k+%5C+%5C+%5C+%5C+%5C+%282%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  e^{i\theta/2} j e^{-i\theta/2} = \cos(\theta) j - \sin(\theta) k \ \ \ \ \ (2)"></p>
</a> <a name="id2"><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5E%7Bi%5Ctheta%2F2%7D+k+e%5E%7B-i%5Ctheta%2F2%7D+%3D+%5Ccos%28%5Ctheta%29+k+%2B+%5Csin%28%5Ctheta%29+j.+%5C+%5C+%5C+%5C+%5C+%283%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5E%7Bi%5Ctheta%2F2%7D+k+e%5E%7B-i%5Ctheta%2F2%7D+%3D+%5Ccos%28%5Ctheta%29+k+%2B+%5Csin%28%5Ctheta%29+j.+%5C+%5C+%5C+%5C+%5C+%283%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5E%7Bi%5Ctheta%2F2%7D+k+e%5E%7B-i%5Ctheta%2F2%7D+%3D+%5Ccos%28%5Ctheta%29+k+%2B+%5Csin%28%5Ctheta%29+j.+%5C+%5C+%5C+%5C+%5C+%283%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  e^{i\theta/2} k e^{-i\theta/2} = \cos(\theta) k + \sin(\theta) j. \ \ \ \ \ (3)"></p>
</a><p> Similarly for cyclic permutations of <img src="https://s0.wp.com/latex.php?latex=%7Bi%2Cj%2Ck%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bi%2Cj%2Ck%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bi%2Cj%2Ck%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{i,j,k}">. The doubling of the angle here can be explained from the Lie algebra fact that <img src="https://s0.wp.com/latex.php?latex=%7B%5Bi%2Cj%5D%3Dij-ji%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Bi%2Cj%5D%3Dij-ji%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Bi%2Cj%5D%3Dij-ji%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{[i,j]=ij-ji}"> is <img src="https://s0.wp.com/latex.php?latex=%7B2k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B2k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B2k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{2k}"> rather than <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{k}">; it also closely related to the aforementioned double cover. We also of course have <img src="https://s0.wp.com/latex.php?latex=%7BU%281%2C%5Cmathbb%7BH%7D%29%5Cequiv+Spin%283%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BU%281%2C%5Cmathbb%7BH%7D%29%5Cequiv+Spin%283%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BU%281%2C%5Cmathbb%7BH%7D%29%5Cequiv+Spin%283%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{U(1,\mathbb{H})\equiv Spin(3)}"> acting on <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BH%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BH%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BH%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\mathbb{H}}"> by left multiplication; this is known as the <a href="https://en.wikipedia.org/wiki/Spinor">spinor representation</a>, but will not be utilized much in this post. (Giving <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BH%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BH%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BH%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\mathbb{H}}"> the right action of <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cbf+C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%7B%5Cbf+C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%7B%5Cbf+C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{{\bf C}}"> makes it a copy of <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cbf+C%7D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%7B%5Cbf+C%7D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%7B%5Cbf+C%7D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{{\bf C}^2}">, and the spinor representation then also becomes the standard representation of <img src="https://s0.wp.com/latex.php?latex=%7BSU%282%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BSU%282%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BSU%282%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{SU(2)}"> on <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cbf+C%7D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%7B%5Cbf+C%7D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%7B%5Cbf+C%7D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{{\bf C}^2}">.)
</p><p>
Given how quaternions relate to three-dimensional rotations, it is not surprising that one can also be used to recover the basic laws of <a href="https://en.wikipedia.org/wiki/Spherical_trigonometry">spherical trigonometry</a> – the study of spherical triangles on the unit sphere. This is fairly well known, but it took a little effort for me to locate the required arguments, so I am recording the calculations here.
</p><p>
The first observation is that every unit quaternion <img src="https://s0.wp.com/latex.php?latex=%7Bq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{q}"> induces a unit tangent vector <img src="https://s0.wp.com/latex.php?latex=%7Bqj%5Coverline%7Bq%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bqj%5Coverline%7Bq%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bqj%5Coverline%7Bq%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{qj\overline{q}}"> on the unit sphere <img src="https://s0.wp.com/latex.php?latex=%7BS%5E2+%5Csubset+%7B%5Cbf+R%7D%5E3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BS%5E2+%5Csubset+%7B%5Cbf+R%7D%5E3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BS%5E2+%5Csubset+%7B%5Cbf+R%7D%5E3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{S^2 \subset {\bf R}^3}">, located at <img src="https://s0.wp.com/latex.php?latex=%7Bqi%5Coverline%7Bq%7D+%5Cin+S%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bqi%5Coverline%7Bq%7D+%5Cin+S%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bqi%5Coverline%7Bq%7D+%5Cin+S%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{qi\overline{q} \in S^2}">; the third unit vector <img src="https://s0.wp.com/latex.php?latex=%7Bqk%5Coverline%7Bq%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bqk%5Coverline%7Bq%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bqk%5Coverline%7Bq%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{qk\overline{q}}"> is then another tangent vector orthogonal to the first two (and oriented to the left of the original tangent vector), and can be viewed as the cross product of <img src="https://s0.wp.com/latex.php?latex=%7Bqi%5Coverline%7Bq%7D+%5Cin+S%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bqi%5Coverline%7Bq%7D+%5Cin+S%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bqi%5Coverline%7Bq%7D+%5Cin+S%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{qi\overline{q} \in S^2}"> and <img src="https://s0.wp.com/latex.php?latex=%7Bqj%5Coverline%7Bq%7D+%5Cin+S%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bqj%5Coverline%7Bq%7D+%5Cin+S%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bqj%5Coverline%7Bq%7D+%5Cin+S%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{qj\overline{q} \in S^2}">. Right multplication of this quaternion then corresponds to various natural operations on this unit tangent vector:
</p>


<p>
Now suppose one has a spherical triangle with vertices <img src="https://s0.wp.com/latex.php?latex=%7BA%2CB%2CC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BA%2CB%2CC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BA%2CB%2CC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{A,B,C}">, with the spherical arcs <img src="https://s0.wp.com/latex.php?latex=%7BAB%2C+BC%2C+CA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BAB%2C+BC%2C+CA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BAB%2C+BC%2C+CA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{AB, BC, CA}"> subtending angles <img src="https://s0.wp.com/latex.php?latex=%7Bc%2C+a%2C+b%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bc%2C+a%2C+b%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bc%2C+a%2C+b%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{c, a, b}"> respectively, and the vertices <img src="https://s0.wp.com/latex.php?latex=%7BA%2CB%2CC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BA%2CB%2CC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BA%2CB%2CC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{A,B,C}"> subtending angles <img src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%2C%5Cbeta%2C%5Cgamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Calpha%2C%5Cbeta%2C%5Cgamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Calpha%2C%5Cbeta%2C%5Cgamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\alpha,\beta,\gamma}"> respectively; suppose also that <img src="https://s0.wp.com/latex.php?latex=%7BABC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BABC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BABC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{ABC}"> is oriented in an anti-clockwise direction for sake of discussion. Observe that if one starts at <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{A}"> with a tangent vector oriented towards <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{B}">, advances that vector by <img src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{c}">, and then rotates by <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpi+-+%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Cpi+-+%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Cpi+-+%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\pi - \beta}">, the tangent vector now at <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{B}"> and pointing towards <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{C}">. If one advances by <img src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{a}"> and rotates by <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpi+-+%5Cgamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Cpi+-+%5Cgamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Cpi+-+%5Cgamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\pi - \gamma}">, one is now at <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{C}"> pointing towards <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{A}">; and if one then advances by <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{b}"> and rotates by <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpi+-+%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Cpi+-+%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Cpi+-+%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\pi - \alpha}">, one is back at <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{A}"> pointing towards <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{B}">. This gives the fundamental relation <a name="ident"></a></p><p><a name="ident"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5E%7Bkc%2F2%7D+e%5E%7Bi%28%5Cpi-%5Cbeta%29%2F2%7D+e%5E%7Bka%2F2%7D+e%5E%7Bi%28%5Cpi-%5Cgamma%29%2F2%7D+e%5E%7Bkb%2F2%7D+e%5E%7Bi%28%5Cpi-%5Calpha%29%2F2%7D+%3D+1+%5C+%5C+%5C+%5C+%5C+%284%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5E%7Bkc%2F2%7D+e%5E%7Bi%28%5Cpi-%5Cbeta%29%2F2%7D+e%5E%7Bka%2F2%7D+e%5E%7Bi%28%5Cpi-%5Cgamma%29%2F2%7D+e%5E%7Bkb%2F2%7D+e%5E%7Bi%28%5Cpi-%5Calpha%29%2F2%7D+%3D+1+%5C+%5C+%5C+%5C+%5C+%284%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5E%7Bkc%2F2%7D+e%5E%7Bi%28%5Cpi-%5Cbeta%29%2F2%7D+e%5E%7Bka%2F2%7D+e%5E%7Bi%28%5Cpi-%5Cgamma%29%2F2%7D+e%5E%7Bkb%2F2%7D+e%5E%7Bi%28%5Cpi-%5Calpha%29%2F2%7D+%3D+1+%5C+%5C+%5C+%5C+%5C+%284%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  e^{kc/2} e^{i(\pi-\beta)/2} e^{ka/2} e^{i(\pi-\gamma)/2} e^{kb/2} e^{i(\pi-\alpha)/2} = 1 \ \ \ \ \ (4)"></a></p><p><a name="ident">
</a> relating the three sides and three equations of this triangle. (<em>A priori</em>, due to the lack of faithfulness of the <img src="https://s0.wp.com/latex.php?latex=%7BU%281%2C%5Cmathbb%7BH%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BU%281%2C%5Cmathbb%7BH%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BU%281%2C%5Cmathbb%7BH%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{U(1,\mathbb{H})}"> action, the right-hand side could conceivably have been <img src="https://s0.wp.com/latex.php?latex=%7B-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{-1}"> rather than <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{1}">; but for extremely small triangles the right-hand side is clearly <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{1}">, and so by continuity it must be <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{1}"> for all triangles.) Indeed, a moments thought will reveal that the condition <a href="#ident">(4)</a> is necessary and sufficient for the data <img src="https://s0.wp.com/latex.php?latex=%7Ba%2Cb%2Cc%2C%5Calpha%2C%5Cbeta%2C%5Cgamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Ba%2Cb%2Cc%2C%5Calpha%2C%5Cbeta%2C%5Cgamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Ba%2Cb%2Cc%2C%5Calpha%2C%5Cbeta%2C%5Cgamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{a,b,c,\alpha,\beta,\gamma}"> to be associated with a spherical triangle. Thus one can view <a href="#ident">(4)</a> as a “master equation” for spherical trigonometry: in principle, it can be used to derive all the other laws of this subject.
</p><blockquote><b>Remark 1</b>  The law <a href="#ident">(4)</a> has an evident symmetry <img src="https://s0.wp.com/latex.php?latex=%7B%28a%2Cb%2Cc%2C%5Calpha%2C%5Cbeta%2C%5Cgamma%29+%5Cmapsto+%28%5Cpi-%5Calpha%2C%5Cpi-%5Cbeta%2C%5Cpi-%5Cgamma%2C%5Cpi-a%2C%5Cpi-b%2C%5Cpi-c%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%28a%2Cb%2Cc%2C%5Calpha%2C%5Cbeta%2C%5Cgamma%29+%5Cmapsto+%28%5Cpi-%5Calpha%2C%5Cpi-%5Cbeta%2C%5Cpi-%5Cgamma%2C%5Cpi-a%2C%5Cpi-b%2C%5Cpi-c%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%28a%2Cb%2Cc%2C%5Calpha%2C%5Cbeta%2C%5Cgamma%29+%5Cmapsto+%28%5Cpi-%5Calpha%2C%5Cpi-%5Cbeta%2C%5Cpi-%5Cgamma%2C%5Cpi-a%2C%5Cpi-b%2C%5Cpi-c%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{(a,b,c,\alpha,\beta,\gamma) \mapsto (\pi-\alpha,\pi-\beta,\pi-\gamma,\pi-a,\pi-b,\pi-c)}">, which corresponds to the operation of replacing a spherical triangle with its dual triangle. Also, there is nothing particularly special about the choice of imaginaries <img src="https://s0.wp.com/latex.php?latex=%7Bi%2Ck%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bi%2Ck%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bi%2Ck%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{i,k}"> in <a href="#ident">(4)</a>; one can conjugate <a href="#ident">(4)</a> by various quaternions and replace <img src="https://s0.wp.com/latex.php?latex=%7Bi%2Ck%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bi%2Ck%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bi%2Ck%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{i,k}"> here by any other orthogonal pair of unit quaternions. </blockquote>

<blockquote><b>Remark 2</b>  If we work in the small scale regime, replacing <img src="https://s0.wp.com/latex.php?latex=%7Ba%2Cb%2Cc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Ba%2Cb%2Cc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Ba%2Cb%2Cc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{a,b,c}"> by <img src="https://s0.wp.com/latex.php?latex=%7B%5Cvarepsilon+a%2C+%5Cvarepsilon+b%2C+%5Cvarepsilon+c%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Cvarepsilon+a%2C+%5Cvarepsilon+b%2C+%5Cvarepsilon+c%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Cvarepsilon+a%2C+%5Cvarepsilon+b%2C+%5Cvarepsilon+c%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\varepsilon a, \varepsilon b, \varepsilon c}"> for some small <img src="https://s0.wp.com/latex.php?latex=%7B%5Cvarepsilon%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Cvarepsilon%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Cvarepsilon%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\varepsilon>0}">, then we expect spherical triangles to behave like Euclidean triangles. Indeed, <a href="#ident">(4)</a> to zeroth order becomes <p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5E%7Bi%28%5Cpi-%5Cbeta%29%2F2%7D+e%5E%7Bi%28%5Cpi-%5Cgamma%29%2F2%7D+e%5E%7Bi%28%5Cpi-%5Calpha%29%2F2%7D+%3D+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5E%7Bi%28%5Cpi-%5Cbeta%29%2F2%7D+e%5E%7Bi%28%5Cpi-%5Cgamma%29%2F2%7D+e%5E%7Bi%28%5Cpi-%5Calpha%29%2F2%7D+%3D+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5E%7Bi%28%5Cpi-%5Cbeta%29%2F2%7D+e%5E%7Bi%28%5Cpi-%5Cgamma%29%2F2%7D+e%5E%7Bi%28%5Cpi-%5Calpha%29%2F2%7D+%3D+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  e^{i(\pi-\beta)/2} e^{i(\pi-\gamma)/2} e^{i(\pi-\alpha)/2} = 1"></p>
 which reflects the classical fact that the sum of angles of a Euclidean triangle is equal to <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\pi}">. To first order, one obtains <p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++c+%2B+a+e%5E%7Bi%28%5Cpi-%5Cgamma%29%2F2%7D+e%5E%7Bi%28%5Cpi-%5Calpha%29%2F2%7D+%2B+b+e%5E%7Bi%28%5Cpi-%5Calpha%29%2F2%7D+%3D+0&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++c+%2B+a+e%5E%7Bi%28%5Cpi-%5Cgamma%29%2F2%7D+e%5E%7Bi%28%5Cpi-%5Calpha%29%2F2%7D+%2B+b+e%5E%7Bi%28%5Cpi-%5Calpha%29%2F2%7D+%3D+0&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++c+%2B+a+e%5E%7Bi%28%5Cpi-%5Cgamma%29%2F2%7D+e%5E%7Bi%28%5Cpi-%5Calpha%29%2F2%7D+%2B+b+e%5E%7Bi%28%5Cpi-%5Calpha%29%2F2%7D+%3D+0&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  c + a e^{i(\pi-\gamma)/2} e^{i(\pi-\alpha)/2} + b e^{i(\pi-\alpha)/2} = 0"></p>
 which reflects the evident fact that the vector sum of the sides of a Euclidean triangle sum to zero. (Geometrically, this correspondence reflects the fact that the action of the (projective) quaternion group on the unit sphere converges to the action of the special Euclidean group <img src="https://s0.wp.com/latex.php?latex=%7BSE%282%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BSE%282%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BSE%282%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{SE(2)}"> on the plane, in a suitable asymptotic limit.) </blockquote>

<p>
The identity <a href="#ident">(4)</a> is an identity of two unit quaternions; as the unit quaternion group <img src="https://s0.wp.com/latex.php?latex=%7BU%281%2C%5Cmathbb%7BH%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BU%281%2C%5Cmathbb%7BH%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BU%281%2C%5Cmathbb%7BH%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{U(1,\mathbb{H})}"> is three-dimensional, this thus imposes three independent constraints on the six real parameters <img src="https://s0.wp.com/latex.php?latex=%7Ba%2Cb%2Cc%2C%5Calpha%2C%5Cbeta%2C%5Cgamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Ba%2Cb%2Cc%2C%5Calpha%2C%5Cbeta%2C%5Cgamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Ba%2Cb%2Cc%2C%5Calpha%2C%5Cbeta%2C%5Cgamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{a,b,c,\alpha,\beta,\gamma}"> of the spherical triangle. One can manipulate this constraint in various ways to obtain various trigonometric identities involving some subsets of these six parameters. For instance, one can rearrange <a href="#ident">(4)</a> to get <a name="piba"></a></p><p><a name="piba"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5E%7Bi%28%5Cpi-%5Cbeta%29%2F2%7D+e%5E%7Bka%2F2%7D+e%5E%7Bi%28%5Cpi-%5Cgamma%29%2F2%7D+%3D+e%5E%7B-kc%2F2%7D+e%5E%7B-i%28%5Cpi-%5Calpha%29%2F2%7D+e%5E%7B-kb%2F2%7D.+%5C+%5C+%5C+%5C+%5C+%285%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5E%7Bi%28%5Cpi-%5Cbeta%29%2F2%7D+e%5E%7Bka%2F2%7D+e%5E%7Bi%28%5Cpi-%5Cgamma%29%2F2%7D+%3D+e%5E%7B-kc%2F2%7D+e%5E%7B-i%28%5Cpi-%5Calpha%29%2F2%7D+e%5E%7B-kb%2F2%7D.+%5C+%5C+%5C+%5C+%5C+%285%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5E%7Bi%28%5Cpi-%5Cbeta%29%2F2%7D+e%5E%7Bka%2F2%7D+e%5E%7Bi%28%5Cpi-%5Cgamma%29%2F2%7D+%3D+e%5E%7B-kc%2F2%7D+e%5E%7B-i%28%5Cpi-%5Calpha%29%2F2%7D+e%5E%7B-kb%2F2%7D.+%5C+%5C+%5C+%5C+%5C+%285%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  e^{i(\pi-\beta)/2} e^{ka/2} e^{i(\pi-\gamma)/2} = e^{-kc/2} e^{-i(\pi-\alpha)/2} e^{-kb/2}. \ \ \ \ \ (5)"></a></p><p><a name="piba">
</a> Conjugating by <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{i}"> to reverse the sign of <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{k}">, we also have </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5E%7Bi%28%5Cpi-%5Cbeta%29%2F2%7D+e%5E%7B-ka%2F2%7D+e%5E%7Bi%28%5Cpi-%5Cgamma%29%2F2%7D+%3D+e%5E%7Bkc%2F2%7D+e%5E%7B-i%28%5Cpi-%5Calpha%29%2F2%7D+e%5E%7Bkb%2F2%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5E%7Bi%28%5Cpi-%5Cbeta%29%2F2%7D+e%5E%7B-ka%2F2%7D+e%5E%7Bi%28%5Cpi-%5Cgamma%29%2F2%7D+%3D+e%5E%7Bkc%2F2%7D+e%5E%7B-i%28%5Cpi-%5Calpha%29%2F2%7D+e%5E%7Bkb%2F2%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5E%7Bi%28%5Cpi-%5Cbeta%29%2F2%7D+e%5E%7B-ka%2F2%7D+e%5E%7Bi%28%5Cpi-%5Cgamma%29%2F2%7D+%3D+e%5E%7Bkc%2F2%7D+e%5E%7B-i%28%5Cpi-%5Calpha%29%2F2%7D+e%5E%7Bkb%2F2%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  e^{i(\pi-\beta)/2} e^{-ka/2} e^{i(\pi-\gamma)/2} = e^{kc/2} e^{-i(\pi-\alpha)/2} e^{kb/2}."></p><p>
 Taking the inner product of both sides of these identities, we conclude that </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+e%5E%7Bi%28%5Cpi-%5Cbeta%29%2F2%7D+e%5E%7Bka%2F2%7D+e%5E%7Bi%28%5Cpi-%5Cgamma%29%2F2%7D%2C+e%5E%7Bi%28%5Cpi-%5Cbeta%29%2F2%7D+e%5E%7B-ka%2F2%7D+e%5E%7Bi%28%5Cpi-%5Cgamma%29%2F2%7D+%5Crangle&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+e%5E%7Bi%28%5Cpi-%5Cbeta%29%2F2%7D+e%5E%7Bka%2F2%7D+e%5E%7Bi%28%5Cpi-%5Cgamma%29%2F2%7D%2C+e%5E%7Bi%28%5Cpi-%5Cbeta%29%2F2%7D+e%5E%7B-ka%2F2%7D+e%5E%7Bi%28%5Cpi-%5Cgamma%29%2F2%7D+%5Crangle&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+e%5E%7Bi%28%5Cpi-%5Cbeta%29%2F2%7D+e%5E%7Bka%2F2%7D+e%5E%7Bi%28%5Cpi-%5Cgamma%29%2F2%7D%2C+e%5E%7Bi%28%5Cpi-%5Cbeta%29%2F2%7D+e%5E%7B-ka%2F2%7D+e%5E%7Bi%28%5Cpi-%5Cgamma%29%2F2%7D+%5Crangle&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  \langle e^{i(\pi-\beta)/2} e^{ka/2} e^{i(\pi-\gamma)/2}, e^{i(\pi-\beta)/2} e^{-ka/2} e^{i(\pi-\gamma)/2} \rangle"></p><p>
 is equal to </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+e%5E%7B-kc%2F2%7D+e%5E%7B-i%28%5Cpi-%5Calpha%29%2F2%7D+e%5E%7B-kb%2F2%7D%2C+e%5E%7Bkc%2F2%7D+e%5E%7B-i%28%5Cpi-%5Calpha%29%2F2%7D+e%5E%7Bkb%2F2%7D+%5Crangle.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+e%5E%7B-kc%2F2%7D+e%5E%7B-i%28%5Cpi-%5Calpha%29%2F2%7D+e%5E%7B-kb%2F2%7D%2C+e%5E%7Bkc%2F2%7D+e%5E%7B-i%28%5Cpi-%5Calpha%29%2F2%7D+e%5E%7Bkb%2F2%7D+%5Crangle.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+e%5E%7B-kc%2F2%7D+e%5E%7B-i%28%5Cpi-%5Calpha%29%2F2%7D+e%5E%7B-kb%2F2%7D%2C+e%5E%7Bkc%2F2%7D+e%5E%7B-i%28%5Cpi-%5Calpha%29%2F2%7D+e%5E%7Bkb%2F2%7D+%5Crangle.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  \langle e^{-kc/2} e^{-i(\pi-\alpha)/2} e^{-kb/2}, e^{kc/2} e^{-i(\pi-\alpha)/2} e^{kb/2} \rangle."></p><p>
 Using the various properties of inner product, the former expression simplifies to <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathrm%7BRe%7D+e%5E%7Bka%7D+%3D+%5Ccos+a%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Cmathrm%7BRe%7D+e%5E%7Bka%7D+%3D+%5Ccos+a%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Cmathrm%7BRe%7D+e%5E%7Bka%7D+%3D+%5Ccos+a%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\mathrm{Re} e^{ka} = \cos a}">, while the latter simplifies to </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathrm%7BRe%7D+%5Clangle+e%5E%7B-i%28%5Cpi-%5Calpha%29%2F2%7D+e%5E%7B-kb%7D+e%5E%7Bi%28%5Cpi-%5Calpha%29%2F2%7D%2C+e%5E%7Bkc%7D+%5Crangle.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathrm%7BRe%7D+%5Clangle+e%5E%7B-i%28%5Cpi-%5Calpha%29%2F2%7D+e%5E%7B-kb%7D+e%5E%7Bi%28%5Cpi-%5Calpha%29%2F2%7D%2C+e%5E%7Bkc%7D+%5Crangle.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathrm%7BRe%7D+%5Clangle+e%5E%7B-i%28%5Cpi-%5Calpha%29%2F2%7D+e%5E%7B-kb%7D+e%5E%7Bi%28%5Cpi-%5Calpha%29%2F2%7D%2C+e%5E%7Bkc%7D+%5Crangle.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  \mathrm{Re} \langle e^{-i(\pi-\alpha)/2} e^{-kb} e^{i(\pi-\alpha)/2}, e^{kc} \rangle."></p><p>
 We can write <img src="https://s0.wp.com/latex.php?latex=%7Be%5E%7Bkc%7D+%3D+%5Ccos+c+%2B+%28%5Csin+c%29+k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Be%5E%7Bkc%7D+%3D+%5Ccos+c+%2B+%28%5Csin+c%29+k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Be%5E%7Bkc%7D+%3D+%5Ccos+c+%2B+%28%5Csin+c%29+k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{e^{kc} = \cos c + (\sin c) k}"> and </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5E%7B-i%28%5Cpi-%5Calpha%29%2F2%7D+e%5E%7B-kb%7D+e%5E%7Bi%28%5Cpi-%5Calpha%29%2F2%7D+%3D+%5Ccos+b+-+%28%5Csin+b%29+%28%5Ccos%28%5Cpi-%5Calpha%29+k+%2B+%5Csin%28%5Cpi-%5Calpha%29+j%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5E%7B-i%28%5Cpi-%5Calpha%29%2F2%7D+e%5E%7B-kb%7D+e%5E%7Bi%28%5Cpi-%5Calpha%29%2F2%7D+%3D+%5Ccos+b+-+%28%5Csin+b%29+%28%5Ccos%28%5Cpi-%5Calpha%29+k+%2B+%5Csin%28%5Cpi-%5Calpha%29+j%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5E%7B-i%28%5Cpi-%5Calpha%29%2F2%7D+e%5E%7B-kb%7D+e%5E%7Bi%28%5Cpi-%5Calpha%29%2F2%7D+%3D+%5Ccos+b+-+%28%5Csin+b%29+%28%5Ccos%28%5Cpi-%5Calpha%29+k+%2B+%5Csin%28%5Cpi-%5Calpha%29+j%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  e^{-i(\pi-\alpha)/2} e^{-kb} e^{i(\pi-\alpha)/2} = \cos b - (\sin b) (\cos(\pi-\alpha) k + \sin(\pi-\alpha) j)"></p><p>
 so on substituting and simplifying we obtain </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Ccos+b+%5Ccos+c+%2B+%5Csin+b+%5Csin+c+%5Ccos+%5Calpha+%3D+%5Ccos+a&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Ccos+b+%5Ccos+c+%2B+%5Csin+b+%5Csin+c+%5Ccos+%5Calpha+%3D+%5Ccos+a&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Ccos+b+%5Ccos+c+%2B+%5Csin+b+%5Csin+c+%5Ccos+%5Calpha+%3D+%5Ccos+a&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  \cos b \cos c + \sin b \sin c \cos \alpha = \cos a"></p><p>
 which is the <a href="https://en.wikipedia.org/wiki/Spherical_law_of_cosines">spherical cosine rule</a>. Note in the infinitesimal limit (replacing <img src="https://s0.wp.com/latex.php?latex=%7Ba%2Cb%2Cc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Ba%2Cb%2Cc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Ba%2Cb%2Cc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{a,b,c}"> by <img src="https://s0.wp.com/latex.php?latex=%7B%5Cvarepsilon+a%2C+%5Cvarepsilon+b%2C+%5Cvarepsilon+c%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Cvarepsilon+a%2C+%5Cvarepsilon+b%2C+%5Cvarepsilon+c%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Cvarepsilon+a%2C+%5Cvarepsilon+b%2C+%5Cvarepsilon+c%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\varepsilon a, \varepsilon b, \varepsilon c}">) this rule becomes the familiar Euclidean cosine rule </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++a%5E2+%3D+b%5E2+%2B+c%5E2+-+2bc+%5Ccos+%5Calpha.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++a%5E2+%3D+b%5E2+%2B+c%5E2+-+2bc+%5Ccos+%5Calpha.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++a%5E2+%3D+b%5E2+%2B+c%5E2+-+2bc+%5Ccos+%5Calpha.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  a^2 = b^2 + c^2 - 2bc \cos \alpha."></p>

<p>
In a similar fashion, from <a href="#piba">(5)</a> we see that the quantity </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+e%5E%7Bi%28%5Cpi-%5Cbeta%29%2F2%7D+e%5E%7Bka%2F2%7D+e%5E%7Bi%28%5Cpi-%5Cgamma%29%2F2%7D+i+e%5E%7B-i%28%5Cpi-%5Cgamma%29%2F2%7D+e%5E%7B-ka%2F2%7D+e%5E%7B-i%28%5Cpi-%5Cbeta%29%2F2%7D%2C+k+%5Crangle&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+e%5E%7Bi%28%5Cpi-%5Cbeta%29%2F2%7D+e%5E%7Bka%2F2%7D+e%5E%7Bi%28%5Cpi-%5Cgamma%29%2F2%7D+i+e%5E%7B-i%28%5Cpi-%5Cgamma%29%2F2%7D+e%5E%7B-ka%2F2%7D+e%5E%7B-i%28%5Cpi-%5Cbeta%29%2F2%7D%2C+k+%5Crangle&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+e%5E%7Bi%28%5Cpi-%5Cbeta%29%2F2%7D+e%5E%7Bka%2F2%7D+e%5E%7Bi%28%5Cpi-%5Cgamma%29%2F2%7D+i+e%5E%7B-i%28%5Cpi-%5Cgamma%29%2F2%7D+e%5E%7B-ka%2F2%7D+e%5E%7B-i%28%5Cpi-%5Cbeta%29%2F2%7D%2C+k+%5Crangle&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  \langle e^{i(\pi-\beta)/2} e^{ka/2} e^{i(\pi-\gamma)/2} i e^{-i(\pi-\gamma)/2} e^{-ka/2} e^{-i(\pi-\beta)/2}, k \rangle"></p><p>
 is equal to </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+e%5E%7B-kc%2F2%7D+e%5E%7B-i%28%5Cpi-%5Calpha%29%2F2%7D+e%5E%7B-kb%2F2%7D+i+e%5E%7Bkb%2F2%7D+e%5E%7Bi%28%5Cpi-%5Calpha%29%2F2%7D+e%5E%7Bkc%2F2%7D%2C+k+%5Crangle.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+e%5E%7B-kc%2F2%7D+e%5E%7B-i%28%5Cpi-%5Calpha%29%2F2%7D+e%5E%7B-kb%2F2%7D+i+e%5E%7Bkb%2F2%7D+e%5E%7Bi%28%5Cpi-%5Calpha%29%2F2%7D+e%5E%7Bkc%2F2%7D%2C+k+%5Crangle.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+e%5E%7B-kc%2F2%7D+e%5E%7B-i%28%5Cpi-%5Calpha%29%2F2%7D+e%5E%7B-kb%2F2%7D+i+e%5E%7Bkb%2F2%7D+e%5E%7Bi%28%5Cpi-%5Calpha%29%2F2%7D+e%5E%7Bkc%2F2%7D%2C+k+%5Crangle.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  \langle e^{-kc/2} e^{-i(\pi-\alpha)/2} e^{-kb/2} i e^{kb/2} e^{i(\pi-\alpha)/2} e^{kc/2}, k \rangle."></p><p>
 The first expression simplifies by <a href="#id0">(1)</a> and properties of the inner product to </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+e%5E%7Bka%2F2%7D+i+e%5E%7B-ka%2F2%7D%2C+e%5E%7B-i%28%5Cpi-%5Cbeta%29%2F2%7D+k+e%5E%7Bi%28%5Cpi-%5Cbeta%29%2F2%7D+%5Crangle%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+e%5E%7Bka%2F2%7D+i+e%5E%7B-ka%2F2%7D%2C+e%5E%7B-i%28%5Cpi-%5Cbeta%29%2F2%7D+k+e%5E%7Bi%28%5Cpi-%5Cbeta%29%2F2%7D+%5Crangle%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+e%5E%7Bka%2F2%7D+i+e%5E%7B-ka%2F2%7D%2C+e%5E%7B-i%28%5Cpi-%5Cbeta%29%2F2%7D+k+e%5E%7Bi%28%5Cpi-%5Cbeta%29%2F2%7D+%5Crangle%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  \langle e^{ka/2} i e^{-ka/2}, e^{-i(\pi-\beta)/2} k e^{i(\pi-\beta)/2} \rangle,"></p><p>
 which by <a href="#id1">(2)</a>, <a href="#id2">(3)</a> simplifies further to <img src="https://s0.wp.com/latex.php?latex=%7B-%5Csin+a+%5Csin+%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B-%5Csin+a+%5Csin+%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B-%5Csin+a+%5Csin+%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{-\sin a \sin \beta}">. Similarly, the second expression simplifies to </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+e%5E%7B-kb%2F2%7D+i+e%5E%7Bkb%2F2%7D+%2C+e%5E%7Bi%28%5Cpi-%5Calpha%29%2F2%7D+k+e%5E%7B-i%28%5Cpi-%5Calpha%29%2F2%7D%5Crangle%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+e%5E%7B-kb%2F2%7D+i+e%5E%7Bkb%2F2%7D+%2C+e%5E%7Bi%28%5Cpi-%5Calpha%29%2F2%7D+k+e%5E%7B-i%28%5Cpi-%5Calpha%29%2F2%7D%5Crangle%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+e%5E%7B-kb%2F2%7D+i+e%5E%7Bkb%2F2%7D+%2C+e%5E%7Bi%28%5Cpi-%5Calpha%29%2F2%7D+k+e%5E%7B-i%28%5Cpi-%5Calpha%29%2F2%7D%5Crangle%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  \langle e^{-kb/2} i e^{kb/2} , e^{i(\pi-\alpha)/2} k e^{-i(\pi-\alpha)/2}\rangle,"></p><p>
 which by <a href="#id1">(2)</a>, <a href="#id2">(3)</a> simplifies to <img src="https://s0.wp.com/latex.php?latex=%7B-%5Csin+b+%5Csin+%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B-%5Csin+b+%5Csin+%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B-%5Csin+b+%5Csin+%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{-\sin b \sin \alpha}">. Equating the two and rearranging, we obtain </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B%5Csin+%5Calpha%7D%7B%5Csin+a%7D+%3D+%5Cfrac%7B%5Csin+%5Cbeta%7D%7B%5Csin+b%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B%5Csin+%5Calpha%7D%7B%5Csin+a%7D+%3D+%5Cfrac%7B%5Csin+%5Cbeta%7D%7B%5Csin+b%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B%5Csin+%5Calpha%7D%7B%5Csin+a%7D+%3D+%5Cfrac%7B%5Csin+%5Cbeta%7D%7B%5Csin+b%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  \frac{\sin \alpha}{\sin a} = \frac{\sin \beta}{\sin b}"></p><p>
 which is the <a href="https://en.wikipedia.org/wiki/Law_of_sines#The_spherical_law_of_sines">spherical sine rule</a>. Again, in the infinitesimal limit we obtain the familiar Euclidean sine rule </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B%5Csin+%5Calpha%7D%7Ba%7D+%3D+%5Cfrac%7B%5Csin+%5Cbeta%7D%7Bb%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B%5Csin+%5Calpha%7D%7Ba%7D+%3D+%5Cfrac%7B%5Csin+%5Cbeta%7D%7Bb%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B%5Csin+%5Calpha%7D%7Ba%7D+%3D+%5Cfrac%7B%5Csin+%5Cbeta%7D%7Bb%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  \frac{\sin \alpha}{a} = \frac{\sin \beta}{b}."></p>

<p>
As a variant of the above analysis, we have from <a href="#piba">(5)</a> again that </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+e%5E%7Bi%28%5Cpi-%5Cbeta%29%2F2%7D+e%5E%7Bka%2F2%7D+e%5E%7Bi%28%5Cpi-%5Cgamma%29%2F2%7D+i+e%5E%7B-i%28%5Cpi-%5Cgamma%29%2F2%7D+e%5E%7B-ka%2F2%7D+e%5E%7B-i%28%5Cpi-%5Cbeta%29%2F2%7D%2C+j+%5Crangle&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+e%5E%7Bi%28%5Cpi-%5Cbeta%29%2F2%7D+e%5E%7Bka%2F2%7D+e%5E%7Bi%28%5Cpi-%5Cgamma%29%2F2%7D+i+e%5E%7B-i%28%5Cpi-%5Cgamma%29%2F2%7D+e%5E%7B-ka%2F2%7D+e%5E%7B-i%28%5Cpi-%5Cbeta%29%2F2%7D%2C+j+%5Crangle&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+e%5E%7Bi%28%5Cpi-%5Cbeta%29%2F2%7D+e%5E%7Bka%2F2%7D+e%5E%7Bi%28%5Cpi-%5Cgamma%29%2F2%7D+i+e%5E%7B-i%28%5Cpi-%5Cgamma%29%2F2%7D+e%5E%7B-ka%2F2%7D+e%5E%7B-i%28%5Cpi-%5Cbeta%29%2F2%7D%2C+j+%5Crangle&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  \langle e^{i(\pi-\beta)/2} e^{ka/2} e^{i(\pi-\gamma)/2} i e^{-i(\pi-\gamma)/2} e^{-ka/2} e^{-i(\pi-\beta)/2}, j \rangle"></p><p>
 is equal to </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+e%5E%7B-kc%2F2%7D+e%5E%7B-i%28%5Cpi-%5Calpha%29%2F2%7D+e%5E%7B-kb%2F2%7D+i+e%5E%7Bkb%2F2%7D+e%5E%7Bi%28%5Cpi-%5Calpha%29%2F2%7D+e%5E%7Bkc%2F2%7D%2C+j+%5Crangle.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+e%5E%7B-kc%2F2%7D+e%5E%7B-i%28%5Cpi-%5Calpha%29%2F2%7D+e%5E%7B-kb%2F2%7D+i+e%5E%7Bkb%2F2%7D+e%5E%7Bi%28%5Cpi-%5Calpha%29%2F2%7D+e%5E%7Bkc%2F2%7D%2C+j+%5Crangle.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+e%5E%7B-kc%2F2%7D+e%5E%7B-i%28%5Cpi-%5Calpha%29%2F2%7D+e%5E%7B-kb%2F2%7D+i+e%5E%7Bkb%2F2%7D+e%5E%7Bi%28%5Cpi-%5Calpha%29%2F2%7D+e%5E%7Bkc%2F2%7D%2C+j+%5Crangle.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  \langle e^{-kc/2} e^{-i(\pi-\alpha)/2} e^{-kb/2} i e^{kb/2} e^{i(\pi-\alpha)/2} e^{kc/2}, j \rangle."></p><p>
 As before, the first expression simplifies to </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+e%5E%7Bka%2F2%7D+i+e%5E%7B-ka%2F2%7D%2C+e%5E%7B-i%28%5Cpi-%5Cbeta%29%2F2%7D+j+e%5E%7Bi%28%5Cpi-%5Cbeta%29%2F2%7D+%5Crangle&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+e%5E%7Bka%2F2%7D+i+e%5E%7B-ka%2F2%7D%2C+e%5E%7B-i%28%5Cpi-%5Cbeta%29%2F2%7D+j+e%5E%7Bi%28%5Cpi-%5Cbeta%29%2F2%7D+%5Crangle&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+e%5E%7Bka%2F2%7D+i+e%5E%7B-ka%2F2%7D%2C+e%5E%7B-i%28%5Cpi-%5Cbeta%29%2F2%7D+j+e%5E%7Bi%28%5Cpi-%5Cbeta%29%2F2%7D+%5Crangle&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  \langle e^{ka/2} i e^{-ka/2}, e^{-i(\pi-\beta)/2} j e^{i(\pi-\beta)/2} \rangle"></p><p>
 which equals <img src="https://s0.wp.com/latex.php?latex=%7B%5Csin+a+%5Ccos+%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Csin+a+%5Ccos+%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Csin+a+%5Ccos+%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\sin a \cos \beta}">. Meanwhile, the second expression can be rearranged as </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+e%5E%7B-i%28%5Cpi-%5Calpha%29%2F2%7D+e%5E%7B-kb%2F2%7D+i+e%5E%7Bkb%2F2%7D+e%5E%7Bi%28%5Cpi-%5Calpha%29%2F2%7D%2C+e%5E%7Bkc%2F2%7D+j+e%5E%7B-kc%2F2%7D+%5Crangle.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+e%5E%7B-i%28%5Cpi-%5Calpha%29%2F2%7D+e%5E%7B-kb%2F2%7D+i+e%5E%7Bkb%2F2%7D+e%5E%7Bi%28%5Cpi-%5Calpha%29%2F2%7D%2C+e%5E%7Bkc%2F2%7D+j+e%5E%7B-kc%2F2%7D+%5Crangle.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+e%5E%7B-i%28%5Cpi-%5Calpha%29%2F2%7D+e%5E%7B-kb%2F2%7D+i+e%5E%7Bkb%2F2%7D+e%5E%7Bi%28%5Cpi-%5Calpha%29%2F2%7D%2C+e%5E%7Bkc%2F2%7D+j+e%5E%7B-kc%2F2%7D+%5Crangle.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  \langle e^{-i(\pi-\alpha)/2} e^{-kb/2} i e^{kb/2} e^{i(\pi-\alpha)/2}, e^{kc/2} j e^{-kc/2} \rangle."></p><p>
 By <a href="#id1">(2)</a>, <a href="#id2">(3)</a> we can simplify to </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5E%7B-i%28%5Cpi-%5Calpha%29%2F2%7D+e%5E%7B-kb%2F2%7D+i+e%5E%7Bkb%2F2%7D+e%5E%7Bi%28%5Cpi-%5Calpha%29%2F2%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5E%7B-i%28%5Cpi-%5Calpha%29%2F2%7D+e%5E%7B-kb%2F2%7D+i+e%5E%7Bkb%2F2%7D+e%5E%7Bi%28%5Cpi-%5Calpha%29%2F2%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5E%7B-i%28%5Cpi-%5Calpha%29%2F2%7D+e%5E%7B-kb%2F2%7D+i+e%5E%7Bkb%2F2%7D+e%5E%7Bi%28%5Cpi-%5Calpha%29%2F2%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  e^{-i(\pi-\alpha)/2} e^{-kb/2} i e^{kb/2} e^{i(\pi-\alpha)/2} "></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%3D+%28%5Ccos+b%29+i+-+%28%5Csin+b%29+%5Ccos%28%5Cpi-%5Calpha%29+j+%2B+%28%5Csin+b%29+%5Csin%28%5Cpi-%5Calpha%29+k+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%3D+%28%5Ccos+b%29+i+-+%28%5Csin+b%29+%5Ccos%28%5Cpi-%5Calpha%29+j+%2B+%28%5Csin+b%29+%5Csin%28%5Cpi-%5Calpha%29+k+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%3D+%28%5Ccos+b%29+i+-+%28%5Csin+b%29+%5Ccos%28%5Cpi-%5Calpha%29+j+%2B+%28%5Csin+b%29+%5Csin%28%5Cpi-%5Calpha%29+k+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle= (\cos b) i - (\sin b) \cos(\pi-\alpha) j + (\sin b) \sin(\pi-\alpha) k "></p><p>
 and so the inner product is <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccos+b+%5Csin+c+-+%5Ccos+b+%5Csin+c+%5Ccos+%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Ccos+b+%5Csin+c+-+%5Ccos+b+%5Csin+c+%5Ccos+%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Ccos+b+%5Csin+c+-+%5Ccos+b+%5Csin+c+%5Ccos+%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\cos b \sin c - \cos b \sin c \cos \alpha}">, leading to the “<a href="https://en.wikipedia.org/wiki/Spherical_trigonometry#Five-part_rules">five part rule</a>” </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Ccos+b+%5Csin+c+-+%5Csin+b+%5Ccos+c+%5Ccos+%5Calpha+%3D+%5Csin+a+%5Ccos+%5Cbeta.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Ccos+b+%5Csin+c+-+%5Csin+b+%5Ccos+c+%5Ccos+%5Calpha+%3D+%5Csin+a+%5Ccos+%5Cbeta.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Ccos+b+%5Csin+c+-+%5Csin+b+%5Ccos+c+%5Ccos+%5Calpha+%3D+%5Csin+a+%5Ccos+%5Cbeta.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  \cos b \sin c - \sin b \cos c \cos \alpha = \sin a \cos \beta."></p><p>
 In the case of a right-angled triangle <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%3D%5Cpi%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%3D%5Cpi%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Cbeta%3D%5Cpi%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\beta=\pi/2}">, this simplifies to one of <a href="https://en.wikipedia.org/wiki/Spherical_trigonometry#Napier's_rules_for_right_spherical_triangles">Napier’s rules</a></p><a name="example"><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Ccos+%5Calpha+%3D+%5Cfrac%7B%5Ctan+c%7D%7B%5Ctan+b%7D%2C+%5C+%5C+%5C+%5C+%5C+%286%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Ccos+%5Calpha+%3D+%5Cfrac%7B%5Ctan+c%7D%7B%5Ctan+b%7D%2C+%5C+%5C+%5C+%5C+%5C+%286%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Ccos+%5Calpha+%3D+%5Cfrac%7B%5Ctan+c%7D%7B%5Ctan+b%7D%2C+%5C+%5C+%5C+%5C+%5C+%286%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  \cos \alpha = \frac{\tan c}{\tan b}, \ \ \ \ \ (6)"></p>
</a><p> which in the infinitesimal limit is the familiar <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccos+%5Calpha+%3D+%5Cfrac%7Bc%7D%7Bb%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Ccos+%5Calpha+%3D+%5Cfrac%7Bc%7D%7Bb%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Ccos+%5Calpha+%3D+%5Cfrac%7Bc%7D%7Bb%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\cos \alpha = \frac{c}{b}}">. The other rules of Napier can be derived in a similar fashion.
</p><blockquote><b>Example 3</b>  One application of Napier’s rule <a href="#example">(6)</a> is to determine the <a href="https://en.wikipedia.org/wiki/Sunrise_equation">sunrise equation</a> for when the sun rises and sets at a given location on the Earth, and a given time of year. For sake of argument let us work in summer, in which the <a href="https://en.wikipedia.org/wiki/Declination">declination</a> <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\delta}"> of the Sun is positive (due to axial tilt, it reaches a maximum of <img src="https://s0.wp.com/latex.php?latex=%7B23.5%5E%5Ccirc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B23.5%5E%5Ccirc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B23.5%5E%5Ccirc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{23.5^\circ}"> at the summer solstice). Then the Sun subtends an angle of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%2F2-%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Cpi%2F2-%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Cpi%2F2-%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\pi/2-\delta}"> from the pole star (<a href="https://en.wikipedia.org/wiki/Polaris">Polaris</a> in the northern hemisphere, <a href="https://en.wikipedia.org/wiki/Sigma_Octantis">Sigma Octantis</a> in the southern hemisphere), and appears to rotate around that pole star once every <img src="https://s0.wp.com/latex.php?latex=%7B24%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B24%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B24%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{24}"> hours. On the other hand, if one is at a latitude <img src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\phi}">, then the pole star an elevation of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\phi}"> above the horizon. At extremely high latitudes <img src="https://s0.wp.com/latex.php?latex=%7B%5Cphi+%3E+%5Cpi%2F2-%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Cphi+%3E+%5Cpi%2F2-%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Cphi+%3E+%5Cpi%2F2-%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\phi > \pi/2-\delta}">, the sun will never set (a phenomenon known as “<a href="https://en.wikipedia.org/wiki/Midnight_sun">midnight sun</a>“); but in all other cases, at sunrise or sunset, the sun, pole star, and horizon point below the pole star will form a right-angled spherical triangle, with hypotenuse subtending an angle <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%2F2-%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Cpi%2F2-%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Cpi%2F2-%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\pi/2-\delta}"> and vertical side subtending an angle <img src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\phi}">. The angle subtended by the pole star in this triangle is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpi-%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Cpi-%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Cpi-%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\pi-\omega}">, where <img src="https://s0.wp.com/latex.php?latex=%7B%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\omega}"> is the <a href="https://en.wikipedia.org/wiki/Hour_angle#Solar">solar hour angle</a> <img src="https://s0.wp.com/latex.php?latex=%7B%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\omega}"> – the angle that the sun deviates from its noon position. Equation <a href="#example">(6)</a> then gives the <em>sunrise equation</em> <p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Ccos%28%5Cpi-%5Comega%29+%3D+%5Cfrac%7B%5Ctan+%5Cphi%7D%7B%5Ctan%28%5Cpi%2F2-%5Cdelta%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Ccos%28%5Cpi-%5Comega%29+%3D+%5Cfrac%7B%5Ctan+%5Cphi%7D%7B%5Ctan%28%5Cpi%2F2-%5Cdelta%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Ccos%28%5Cpi-%5Comega%29+%3D+%5Cfrac%7B%5Ctan+%5Cphi%7D%7B%5Ctan%28%5Cpi%2F2-%5Cdelta%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  \cos(\pi-\omega) = \frac{\tan \phi}{\tan(\pi/2-\delta)}"></p>
 or equivalently <p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Ccos+%5Comega+%3D+-+%5Ctan+%5Cphi+%5Ctan+%5Cdelta.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Ccos+%5Comega+%3D+-+%5Ctan+%5Cphi+%5Ctan+%5Cdelta.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Ccos+%5Comega+%3D+-+%5Ctan+%5Cphi+%5Ctan+%5Cdelta.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  \cos \omega = - \tan \phi \tan \delta."></p>
 A similar rule determines the time of sunset. In particular, the number of daylight hours in summer (assuming one is not in the midnight sun scenario <img src="https://s0.wp.com/latex.php?latex=%7B%5Cphi+%3E+%5Cpi%2F2+-%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Cphi+%3E+%5Cpi%2F2+-%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Cphi+%3E+%5Cpi%2F2+-%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\phi > \pi/2 -\delta}">) is given by <p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++24+-+%5Cfrac%7B24%7D%7B%5Cpi%7D+%5Cmathrm%7Barccos%7D%28%5Ctan+%5Cphi+%5Ctan+%5Cdelta%29.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++24+-+%5Cfrac%7B24%7D%7B%5Cpi%7D+%5Cmathrm%7Barccos%7D%28%5Ctan+%5Cphi+%5Ctan+%5Cdelta%29.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++24+-+%5Cfrac%7B24%7D%7B%5Cpi%7D+%5Cmathrm%7Barccos%7D%28%5Ctan+%5Cphi+%5Ctan+%5Cdelta%29.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  24 - \frac{24}{\pi} \mathrm{arccos}(\tan \phi \tan \delta)."></p>
 The situation in winter is similar, except that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\delta}"> is now negative, and <a href="https://en.wikipedia.org/wiki/Polar_night">polar night</a> (no sunrise) occurs when <img src="https://s0.wp.com/latex.php?latex=%7B%5Cphi+%3E+%5Cpi%2F2%2B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Cphi+%3E+%5Cpi%2F2%2B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Cphi+%3E+%5Cpi%2F2%2B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\phi > \pi/2+\delta}">. </blockquote>

	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GitHub Is Down (113 pts)]]></title>
            <link>https://www.githubstatus.com</link>
            <guid>42877995</guid>
            <pubDate>Thu, 30 Jan 2025 14:29:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.githubstatus.com">https://www.githubstatus.com</a>, See on <a href="https://news.ycombinator.com/item?id=42877995">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <!-- postmortem -->

    <!-- incident updates -->
      <p><strong>Resolved</strong> -
      	<span>On January 23, 2025, between 9:49 and 17:00 UTC, the available capacity of large hosted runners was degraded.  On average, 26% of jobs requiring large runners had a &gt;5min delay getting a runner assigned.  This was caused by the rollback of a configuration change and a latent bug in event processing, which was triggered by the mixed data shape that resulted from the rollback.  The processing would reprocess the same events unnecessarily and cause the background job that manages large runner creation and deletion to run out of resources.  It would automatically restart and continue processing, but the jobs were not able to keep up with production traffic.  We mitigated the impact by using a feature flag to bypass the problematic event processing logic.  While these changes had been rolling out in stages over the last few months and had been safely rolled back previously, an unrelated change prevented rollback from causing this problem in earlier stages.<p>We are reviewing and updating the feature flags in this event processing workflow to ensure that we have high confidence in rollback in all rollout stages.  We are also improving observability of the event processing to reduce the time to diagnose and mitigate similar issues going forward.</p></span>

        <br>

        <small>
            Jan <var data-var="date">23</var>, <var data-var="time">17:27</var> UTC
        </small>
      </p>
      <p><strong>Update</strong> -
      	<span>We are seeing recovery with the latest mitigation. Queue time for a very small percentage of larger runner jobs are still longer than expected so we are monitoring those for full recovery before going green.</span>

        <br>

        <small>
            Jan <var data-var="date">23</var>, <var data-var="time">17:03</var> UTC
        </small>
      </p>
      <p><strong>Update</strong> -
      	<span>We are actively applying mitigations to help improve larger runner start times. We are currently seeing delays starting about 25% of larger runner jobs.</span>

        <br>

        <small>
            Jan <var data-var="date">23</var>, <var data-var="time">16:25</var> UTC
        </small>
      </p>
      <p><strong>Update</strong> -
      	<span>We are still actively investigating a slowdown in larger runner assignment and are working to apply additional mitigations.</span>

        <br>

        <small>
            Jan <var data-var="date">23</var>, <var data-var="time">15:33</var> UTC
        </small>
      </p>
      <p><strong>Update</strong> -
      	<span>We're still applying mitigations to unblock queueing Actions in large runners. We are monitoring for full recovery.</span>

        <br>

        <small>
            Jan <var data-var="date">23</var>, <var data-var="time">14:53</var> UTC
        </small>
      </p>
      <p><strong>Update</strong> -
      	<span>We are applying further mitigations to fix the issues with delayed queuing for Actions jobs in large runners. We continue to monitor for full recovery.</span>

        <br>

        <small>
            Jan <var data-var="date">23</var>, <var data-var="time">14:17</var> UTC
        </small>
      </p>
      <p><strong>Update</strong> -
      	<span>We are investigating further mitigations for queueing Actions jobs in large runners. We continue to watch telemetry and are monitoring for full recovery.</span>

        <br>

        <small>
            Jan <var data-var="date">23</var>, <var data-var="time">13:42</var> UTC
        </small>
      </p>
      <p><strong>Update</strong> -
      	<span>We've applied a mitigation to fix the issues with queuing and running Actions jobs. We are seeing improvements in telemetry and are monitoring for full recovery.</span>

        <br>

        <small>
            Jan <var data-var="date">23</var>, <var data-var="time">13:09</var> UTC
        </small>
      </p>
      <p><strong>Update</strong> -
      	<span>The team continues to apply mitigations for issues with some Actions jobs delayed being enqueued for larger runners seen by a small number of customers. We will continue providing updates on the progress towards full mitigation.</span>

        <br>

        <small>
            Jan <var data-var="date">23</var>, <var data-var="time">12:36</var> UTC
        </small>
      </p>
      <p><strong>Update</strong> -
      	<span>The team continues to apply mitigations for issues with some Actions jobs delayed being enqueued for larger runners. We will continue providing updates on the progress towards full mitigation.</span>

        <br>

        <small>
            Jan <var data-var="date">23</var>, <var data-var="time">12:03</var> UTC
        </small>
      </p>
      <p><strong>Update</strong> -
      	<span>The team continues to investigate issues with some Actions jobs delayed being enqueued for larger runners. We will continue providing updates on the progress towards mitigation.</span>

        <br>

        <small>
            Jan <var data-var="date">23</var>, <var data-var="time">11:31</var> UTC
        </small>
      </p>
      <p><strong>Update</strong> -
      	<span>The team continues to investigate issues with some Actions jobs having delays in being queued for larger runners. We will continue providing updates on the progress towards mitigation.</span>

        <br>

        <small>
            Jan <var data-var="date">23</var>, <var data-var="time">10:58</var> UTC
        </small>
      </p>
      <p><strong>Investigating</strong> -
      	<span>We are investigating reports of degraded performance for Actions</span>

        <br>

        <small>
            Jan <var data-var="date">23</var>, <var data-var="time">10:25</var> UTC
        </small>
      </p>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mistral Small 3 (574 pts)]]></title>
            <link>https://mistral.ai/news/mistral-small-3/</link>
            <guid>42877860</guid>
            <pubDate>Thu, 30 Jan 2025 14:16:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mistral.ai/news/mistral-small-3/">https://mistral.ai/news/mistral-small-3/</a>, See on <a href="https://news.ycombinator.com/item?id=42877860">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Today we’re introducing Mistral Small 3, a latency-optimized 24B-parameter model released under the Apache 2.0 license.</p><p><img src="https://mistral.ai/images/news/mistral-small-3/up-and-to-the-left.png" alt="Detailed benchmarks" width="77%"></p><p>Mistral Small 3 is competitive with larger models such as Llama 3.3 70B or Qwen 32B, and is an excellent open replacement for opaque proprietary models like GPT4o-mini. Mistral Small 3 is on par with Llama 3.3 70B instruct, while being more than 3x faster on the same hardware.</p><p>Mistral Small 3 is a pre-trained and instructed model catered to the ‘80%’ of generative AI tasks—those that require robust language and instruction following performance, with very low latency.</p><p>We designed this new model to saturate performance at a size suitable for local deployment. Particularly, Mistral Small 3 has far fewer layers than competing models, substantially reducing the time per forward pass. At over 81% accuracy on MMLU and 150 tokens/s latency, Mistral Small is currently the most efficient model of its category.</p><p>We’re releasing both a pretrained and instruction-tuned checkpoint under Apache 2.0. The checkpoints can serve as a powerful base for accelerating progress. Note that Mistral Small 3 is neither trained with RL nor synthetic data, so is earlier in the model production pipeline than models like Deepseek R1 (a great and complementary piece of open-source technology!). It can serve as a great base model for building accrued reasoning capacities. We look forward to seeing how the open-source community adopts and customizes it.</p><h3 id="performance">Performance</h3><h4 id="human-evaluations">Human Evaluations</h4><p><img src="https://mistral.ai/images/news/mistral-small-3/mistral-small-3-human-evals.png" alt="Detailed benchmarks" width="77%"></p><p>We conducted side by side evaluations with an external third-party vendor, on a set of over 1k proprietary coding and generalist prompts. Evaluators were tasked with selecting their preferred model response from anonymized generations produced by Mistral Small 3 vs another model. We are aware that in some cases the benchmarks on human judgement starkly differ from publicly available benchmarks, but have taken extra caution in verifying a fair evaluation. We are confident that the above benchmarks are valid.</p><h4 id="instruct-performance">Instruct performance</h4><p>Our instruction tuned model performs competitively with open weight models three times its size and with proprietary GPT4o-mini model across Code, Math, General knowledge and Instruction following benchmarks.</p><p><img src="https://mistral.ai/images/news/mistral-small-3/mistral-instruct-knowledge.png" alt="Detailed benchmarks" width="77%"></p><p><img src="https://mistral.ai/images/news/mistral-small-3/mistral-instruct-code-math.png" alt="Detailed benchmarks" width="77%"></p><p><img src="https://mistral.ai/images/news/mistral-small-3/mistral-instruct-IF.png" alt="Detailed benchmarks" width="77%"></p><p>Performance accuracy on all benchmarks were obtained through the same internal evaluation pipeline - as such, numbers may vary slightly from previously reported performance (<a href="https://qwenlm.github.io/blog/qwen2.5-llm/">Qwen2.5-32B-Instruct</a>, <a href="https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct">Llama-3.3-70B-Instruct</a>, <a href="https://huggingface.co/google/gemma-2-27b-it">Gemma-2-27B-IT</a>). Judge based evals such as Wildbench, Arena hard and MTBench were based on gpt-4o-2024-05-13.</p><h4 id="pretraining-performance">Pretraining performance</h4><p><img src="https://mistral.ai/images/news/mistral-small-3/mistral-base-benchmarks.png" alt="Detailed benchmarks" width="77%"></p><p><img src="https://mistral.ai/images/news/mistral-small-3/mistral-base-mmlu-int.png" alt="Detailed benchmarks" width="77%"></p><p>Mistral Small 3, a 24B model, offers the best performance for its size class and rivals with models three times larger such as Llama 3.3 70B.</p><h3 id="when-to-use-mistral-small-3">When to use Mistral Small 3</h3><p>Across our customers and community, we are seeing several distinct use cases emerge for pre-trained models of this size:</p><ul><li>Fast-response conversational assistance: Mistral Small 3 excels in scenarios where quick, accurate responses are critical. This includes virtual assistants in many scenarios where users expect immediate feedback and near real-time interactions.</li><li>Low-latency function calling: Mistral Small 3 is able to handle rapid function execution when used as part of automated or agentic workflows.</li><li>Fine-tuning to create subject matter experts: Mistral Small 3 can be fine-tuned to specialize in specific domains, creating highly accurate subject matter experts. This is particularly useful in fields like legal advice, medical diagnostics, and technical support, where domain-specific knowledge is essential.</li><li>Local inference: Particularly beneficial for hobbyists and organizations handling sensitive or proprietary information. When quantized, Mistral Small 3 can be run privately on a single RTX 4090 or a Macbook with 32GB RAM.</li></ul><p>Our customers are evaluating Mistral Small 3 across multiple industries, including:</p><ul><li>Financial services customers for fraud detection</li><li>Healthcare providers for customer triaging</li><li>Robotics, automotive, and manufacturing companies for on-device command and control</li><li>Horizontal use cases across customers include virtual customer service, and sentiment and feedback analysis.</li></ul><h3 id="using-mistral-small-3-on-your-preferred-tech-stack">Using Mistral Small 3 on your preferred tech stack</h3><p>Mistral Small 3 is now available on la Plateforme as <code>mistral-small-latest</code> or <code>mistral-small-2501</code>. Explore our <a href="https://docs.mistral.ai/">docs</a> to learn how to use our models for text generation.</p><p>We are also excited to collaborate with Hugging Face, Ollama, Kaggle, Together AI, and Fireworks AI to make the model available on their platforms starting today:</p><ul><li><a href="https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501">Hugging Face</a> (<a href="https://huggingface.co/mistralai/Mistral-Small-24B-Base-2501">base model</a>)</li><li><a href="https://ollama.com/library/mistral-small">Ollama</a></li><li><a href="https://www.kaggle.com/models/mistral-ai/mistral-small-24b">Kaggle</a></li><li><a href="https://www.together.ai/blog/mistral-small-3-api-now-available-on-together-ai-a-new-category-leader-in-small-models">Together AI</a></li><li><a href="https://fireworks.ai/models/fireworks/mistral-small-24b-instruct-2501">Fireworks AI</a></li><li>Coming soon on NVIDIA NIM, Amazon SageMaker, Groq, Databricks and Snowflake</li></ul><h3 id="the-road-ahead">The road ahead</h3><p>It’s been exciting days for the open-source community! Mistral Small 3 complements large open-source reasoning models like the recent releases of DeepSeek, and can serve as a strong base model for making reasoning capabilities emerge.</p><p>Among many other things, expect small and large Mistral models with boosted reasoning capabilities in the coming weeks. Join the journey if you’re keen (we’re hiring), or beat us to it by hacking Mistral Small 3 today and making it better!</p><h3 id="open-source-models-at-mistral">Open-source models at Mistral</h3><p><strong>We’re renewing our commitment to using Apache 2.0 license for our general purpose models, as we progressively move away from MRL-licensed models</strong>. As with Mistral Small 3, model weights will be available to download and deploy locally, and free to modify and use in any capacity. These models will also be made available through a serverless API on <a href="https://console.mistral.ai/">la Plateforme</a>, through our on-prem and VPC deployments, customisation and orchestration platform, and through our inference and cloud partners. Enterprises and developers that need specialized capabilities (increased speed and context, domain specific knowledge, task-specific models like code completion) can count on additional commercial models complementing what we contribute to the community.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Note on the Relationship Between Artificial Intelligence and Human Intelligence (467 pts)]]></title>
            <link>https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20250128_antiqua-et-nova_en.html</link>
            <guid>42877709</guid>
            <pubDate>Thu, 30 Jan 2025 14:01:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20250128_antiqua-et-nova_en.html">https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20250128_antiqua-et-nova_en.html</a>, See on <a href="https://news.ycombinator.com/item?id=42877709">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
										<td>
										
										
										
										
<p><span color="#663300"> 
          [</span><a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20250128_antiqua-et-nova_ge.html">DE</a><span color="#663300"> 
          - 
          <a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20250128_antiqua-et-nova_en.html">EN</a> - 
          <a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20250128_antiqua-et-nova_sp.html">ES</a> - 
          <a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20250128_antiqua-et-nova_it.html">IT</a>] </span></p> 
										
										
										
										
<p><span color="#663300">DICASTERY FOR THE DOCTRINE OF THE FAITH</span><b><br>
</b><span color="#663300">DICASTERY FOR CULTURE AND EDUCATION</span></p>

<p><span color="#663300" size="4"><b>ANTIQUA ET NOVA</b></span></p>
<p><span color="#663300"><b><i>Note</i> on the Relationship Between <br>
Artificial Intelligence and Human Intelligence</b></span></p>

<p>I. <b> <a name="Introduction">Introduction</a></b></p>
<p>1.&nbsp; With wisdom both ancient and new (cf. Mt. 13:52), we are called to reflect on 
the current challenges and opportunities posed by scientific and technological 
advancements, particularly by the recent development of Artificial Intelligence 
(AI). The Christian tradition regards the gift of intelligence as an essential aspect 
of how humans are created “in the image of God” (Gen. 1:27). Starting from an 
integral vision of the human person and the biblical calling to “till” and 
“keep” the earth (Gen. 2:15), the Church emphasizes that this gift of 
intelligence should be expressed through the responsible use of reason and 
technical abilities in the stewardship of the created world. </p>
<p>2. The Church encourages the advancement of science, technology, the arts, and 
other forms of human endeavor, viewing them as part of the “collaboration of man 
and woman with God in perfecting the visible creation.”<a name="_ftnref1" title="" href="#_ftn1">[1]</a> As Sirach affirms, God “gave skill to human beings, that he might be 
glorified in his marvelous works” (Sir. 38:6). Human abilities and creativity 
come from God and, when used rightly, glorify God by reflecting his wisdom and 
goodness. In light of this, when we ask ourselves what it means to “be human,” 
we cannot exclude a consideration of our scientific and technological abilities.</p>
<p>3. It is within this perspective that the present <i>Note</i> addresses the 
anthropological and ethical challenges raised by AI—issues that are particularly 
significant, as one of the goals of this technology is <i>to imitate the human 
intelligence that designed it. </i>For instance, unlike many other human 
creations, AI can be trained on the results of human creativity and then 
generate new “artifacts” with a level of speed and skill that often rivals or 
surpasses what humans can do, such as producing text or images indistinguishable 
from human compositions. This raises critical concerns about AI’s potential role 
in the growing crisis of truth in the public forum. Moreover, this technology is 
designed to learn and make certain choices autonomously, adapting to new 
situations and providing solutions not foreseen by its programmers, and thus, it 
raises fundamental questions about ethical responsibility and human safety, with 
broader implications for society as a whole. This new situation has prompted 
many people to reflect on what it means to be human and the role of humanity in 
the world.</p>
<p>4. Taking all this into account, there is broad consensus that AI marks a new and 
significant phase in humanity’s engagement with technology, placing it at the 
heart of what Pope Francis has described as an “epochal change.”<a name="_ftnref2" title="" href="#_ftn2">[2]</a> Its impact is felt globally and in a wide range of areas, including 
interpersonal relationships, education, work, art, healthcare, law, warfare, and 
international relations. As AI advances rapidly toward even greater 
achievements, it is critically important to consider its anthropological and 
ethical implications. This involves not only mitigating risks and preventing 
harm but also ensuring that its applications are used to promote human progress 
and the common good.</p>
<p>5. To contribute positively to the discernment regarding AI, and in response to 
Pope Francis’ call for a renewed “wisdom of heart,”<a name="_ftnref3" title="" href="#_ftn3">[3]</a> the Church offers its experience through the anthropological and ethical 
reflections contained in this <i>Note</i>. Committed to its active role in the 
global dialogue on these issues, the Church invites those entrusted with 
transmitting the faith—including parents, teachers, pastors, and bishops—to 
dedicate themselves to this critical subject with care and attention. While this 
document is intended especially for them, it is also meant to be accessible to a 
broader audience, particularly those who share the conviction that scientific 
and technological advances should be directed toward serving the human person 
and the common good.<a name="_ftnref4" title="" href="#_ftn4">[4]</a></p>
<p>6. To this end, the document begins by distinguishing between concepts of 
intelligence in AI and in human intelligence. It then explores the Christian 
understanding of human intelligence, providing a framework rooted in the 
Church’s philosophical and theological tradition. Finally, the document offers 
guidelines to ensure that the development and use of AI uphold human dignity and 
promote the integral development of the human person and society.</p>
<p>II. <b> <a name="What_is_Artificial_Intelligence">What is Artificial Intelligence?</a></b></p>
<p>7. The concept of “intelligence” in AI has evolved over time, drawing on a range of 
ideas from various disciplines. While its origins extend back centuries, a 
significant milestone occurred in 1956 when the American computer scientist John 
McCarthy organized a summer workshop at Dartmouth University to explore the 
problem of “Artificial Intelligence,” which he defined as “that of making a 
machine behave in ways that would be called intelligent if a human were so 
behaving.”<a name="_ftnref5" title="" href="#_ftn5"> [5]</a> This workshop launched a research program focused on designing machines 
capable of performing tasks typically associated with the human intellect and 
intelligent behavior.</p>
<p>8. Since then, AI research has advanced rapidly, leading to the development of 
complex systems capable of performing highly sophisticated tasks.<a name="_ftnref6" title="" href="#_ftn6">[6]</a> These so-called “narrow AI” systems are typically designed to handle 
specific and limited functions, such as translating languages, predicting the 
trajectory of a storm, classifying images, answering questions, or generating 
visual content at the user’s request. While the definition of “intelligence” in 
AI research varies, most contemporary AI systems—particularly those using 
machine learning—rely on statistical inference rather than logical deduction. By 
analyzing large datasets to identify patterns, AI can “predict”<a name="_ftnref7" title="" href="#_ftn7">[7]</a> outcomes and propose new approaches, mimicking some cognitive processes 
typical of human problem-solving. Such achievements have been made possible 
through advances in computing technology (including neural networks, 
unsupervised machine learning, and evolutionary algorithms) as well as hardware 
innovations (such as specialized processors). Together, these technologies 
enable AI systems to respond to various forms of human input, adapt to new 
situations, and even suggest novel solutions not anticipated by their original 
programmers.<a name="_ftnref8" title="" href="#_ftn8">[8]</a></p>
<p>9. Due to these rapid advancements, many tasks once managed exclusively by humans 
are now entrusted to AI. These systems can augment or even supersede what humans 
are able to do in many fields, particularly in specialized areas such as data 
analysis, image recognition, and medical diagnosis. While each “narrow AI” 
application is designed for a specific task, many researchers aspire to develop 
what is known as “Artificial General Intelligence” (AGI)—a single system capable 
of operating across all cognitive domains and performing any task within the 
scope of human intelligence. Some even argue that AGI could one day achieve the 
state of “superintelligence,” surpassing human intellectual capacities, or 
contribute to “super-longevity” through advances in biotechnology. Others, 
however, fear that these possibilities, even if hypothetical, could one day 
eclipse the human person, while still others welcome this potential 
transformation.<a name="_ftnref9" title="" href="#_ftn9">[9]</a></p>
<p>10. Underlying this and 
many other perspectives on the subject is the implicit assumption that the term 
“intelligence” can be used in the same way to refer to both human intelligence 
and AI. Yet, this does not capture the full scope of the concept. In the case of humans, 
intelligence is a faculty that pertains to the person in his or her entirety, 
whereas in the context of AI, “intelligence” is understood functionally, often 
with the presumption that the activities characteristic of the human mind can be 
broken down into digitized steps that machines can replicate.<a name="_ftnref10" title="" href="#_ftn10">[10]</a></p>
<p>11. This functional 
perspective is exemplified by the “Turing Test,” which considers a machine 
“intelligent” if a person cannot distinguish its behavior from that of a human.<a name="_ftnref11" title="" href="#_ftn11">[11]</a> However, in this context, the term “behavior” refers only to the 
performance of specific intellectual tasks; it does not account for the full 
breadth of human experience, which includes abstraction, emotions, creativity, 
and the aesthetic, moral, and religious sensibilities. Nor does it encompass the 
full range of expressions characteristic of the human mind. Instead, in the case 
of AI, the “intelligence” of a system is evaluated methodologically, but also 
reductively, based on its<i> ability to produce appropriate responses—in this 
case, those associated with the human intellect—regardless of how those 
responses are generated</i>.</p>
<p>12. AI’s advanced features 
give it sophisticated abilities to <i>perform tasks</i>, but not the ability to
<i>think</i>.<a name="_ftnref12" title="" href="#_ftn12">[12]</a> This distinction is crucially important, as the way “intelligence” is 
defined inevitably shapes how we understand the relationship between human 
thought and this technology.<a name="_ftnref13" title="" href="#_ftn13">[13]</a> To appreciate this, one must recall the richness of the philosophical 
tradition and Christian theology, which offer a deeper and more comprehensive 
understanding of intelligence—an understanding that is central to the Church’s 
teaching on the nature, dignity, and vocation of the human person.<a name="_ftnref14" title="" href="#_ftn14">[14]</a></p>
<p>III. <b> <a name="Intelligence_in_the_Philosophical">Intelligence in the Philosophical</a> and Theological Tradition</b></p>
<p><i><a name="Rationality">Rationality</a></i></p>
<p>13. From the dawn of human 
self-reflection, the mind has played a central role in understanding what it 
means to be “human.” Aristotle observed that “all people by nature desire to 
know.”<a name="_ftnref15" title="" href="#_ftn15">[15]</a> This knowledge, with its capacity for abstraction that grasps the nature 
and meaning of things, sets humans apart from the animal world.<a name="_ftnref16" title="" href="#_ftn16">[16]</a> As philosophers, theologians, and psychologists have examined the exact 
nature of this intellectual faculty, they have also explored how humans 
understand the world and their unique place within it. Through this exploration, 
the Christian tradition has come to understand the human person as a being 
consisting of both body and soul—deeply connected to this world and yet 
transcending it.<a name="_ftnref17" title="" href="#_ftn17">[17]</a></p>
<p>14. In the classical 
tradition, the concept of intelligence is often understood through the 
complementary concepts of “reason” (<i>ratio</i>) and “intellect” (<i>intellectus</i>). 
These are not separate faculties but, as Saint Thomas Aquinas explains, they are 
two modes in which the same intelligence operates: “The term <i>intellect</i> is 
inferred from the inward grasp of the truth, while the name <i>reason</i> is 
taken from the inquisitive and discursive process.”<a name="_ftnref18" title="" href="#_ftn18">[18]</a> This concise description highlights the two fundamental and complementary 
dimensions of human intelligence. <i>Intellectus</i> refers to the intuitive 
grasp of the truth—that is, apprehending it with the “eyes” of the mind—which 
precedes and grounds argumentation itself. <i>Ratio</i> pertains to reasoning 
proper: the discursive, analytical process that leads to judgment. Together, 
intellect and reason form the two facets of the act of <i>intelligere</i>, “the 
proper operation of the human being as such.”<a name="_ftnref19" title="" href="#_ftn19">[19]</a></p>
<p>15. Describing the human 
person as a “rational” being does not reduce the person to a specific mode of 
thought; rather, it recognizes that the ability for intellectual understanding 
shapes and permeates all aspects of human activity.<a name="_ftnref20" title="" href="#_ftn20"> [20]</a> Whether exercised well or poorly, this capacity is an intrinsic aspect of 
human nature. In this sense, the “term ‘rational’ encompasses all the capacities 
of the human person,” including those related to “knowing and understanding, as 
well as those of willing, loving, choosing, and desiring; it also includes all 
corporeal functions closely related to these abilities.”<a name="_ftnref21" title="" href="#_ftn21"> [21]</a>&nbsp;This comprehensive perspective underscores how, in the human person, 
created in the “image of God,” reason is integrated in a way that elevates, 
shapes, and transforms both the person’s will and actions.<a name="_ftnref22" title="" href="#_ftn22"> [22]</a></p>
<p><i><a name="Embodiment">Embodiment</a></i></p>
<p>16. Christian thought 
considers the intellectual faculties of the human person within the framework of 
an integral anthropology that views the human being as essentially embodied. In 
the human person, spirit and matter “are not two natures united, but rather 
their union forms a single nature.”<a name="_ftnref23" title="" href="#_ftn23">[23]</a>&nbsp;In other words, the soul is not merely the immaterial “part” of the person 
contained within the body, nor is the body an outer shell housing an intangible 
“core.” Rather, the entire human person is simultaneously both material and 
spiritual. This understanding reflects the teaching of Sacred Scripture, which 
views the human person as a being who lives out relationships with God and 
others (and thus, an authentically spiritual dimension) within and through this 
embodied existence.<a name="_ftnref24" title="" href="#_ftn24">[24]</a>&nbsp;The profound meaning of this condition is further illuminated by the 
mystery of the Incarnation, through which God himself took on our flesh and 
“raised it up to a sublime dignity.”<a name="_ftnref25" title="" href="#_ftn25">[25]</a></p>
<p>17. Although deeply rooted 
in bodily existence, the human person transcends the material world through the 
soul, which is “almost on the horizon of eternity and time.”<a name="_ftnref26" title="" href="#_ftn26">[26]</a>&nbsp;The intellect's capacity for transcendence and the self-possessed freedom 
of the will belong to the soul, by which the human person “shares in the light 
of the divine mind.”<a name="_ftnref27" title="" href="#_ftn27">[27]</a>&nbsp;Nevertheless, the human spirit does not exercise its normal mode of 
knowledge without the body.<a name="_ftnref28" title="" href="#_ftn28">[28]</a>&nbsp;In this way, the intellectual faculties of the human person are an 
integral part of an anthropology that recognizes that the human person is a 
“unity of body and soul.”<a name="_ftnref29" title="" href="#_ftn29">[29]</a>&nbsp;Further aspects of this understanding will be developed in what follows.</p>
<p><i><a name="Relationality">Relationality</a></i></p>
<p>18. &nbsp;Human 
beings are “ordered by their very nature to interpersonal communion,”<a name="_ftnref30" title="" href="#_ftn30">[30]</a> 
&nbsp;possessing the capacity to know one another, to give themselves in love, and to 
enter into communion with others. Accordingly, human intelligence is not an 
isolated faculty but is exercised in relationships, finding its fullest 
expression in dialogue, collaboration, and solidarity. We learn with others, and 
we learn through others.</p>
<p>19. The relational orientation of the human person is ultimately grounded in the 
eternal self-giving of the Triune God, whose love is revealed in creation and 
redemption.<a name="_ftnref31" title="" href="#_ftn31">[31]</a>&nbsp;The human person is “called to share, by knowledge and love, in God’s own 
life.”<a name="_ftnref32" title="" href="#_ftn32">[32]</a></p>
<p>20. This vocation to 
communion with God is necessarily tied to the call to communion with others. 
Love of God cannot be separated from love for one’s neighbor (cf. 1 Jn. 4:20; 
Mt. 22:37-39). By the grace of sharing God’s life, Christians are also called to 
imitate Christ’s outpouring gift (cf. 2 Cor. 9:8-11; Eph. 5:1-2) by following 
his command to “love one another, as I have loved you” (Jn.&nbsp;13:34).<a name="_ftnref33" title="" href="#_ftn33">[33]</a>&nbsp;Love and service, echoing the divine life of self-giving, transcend 
self-interest to respond more fully to the human vocation (cf. 1 Jn. 2:9). Even 
more sublime than knowing many things is the commitment to care for one another, 
for if “I understand all mysteries and all knowledge 
[...] but do not have love, I am nothing” (1 Cor. 13:2).</p>
<p><i><a name="Relationship_with_the_Truth">Relationship with the Truth</a></i></p>
<p>21. Human intelligence is ultimately “God’s gift fashioned for the assimilation of 
truth.”<a name="_ftnref34" title="" href="#_ftn34">[34]</a>In the dual sense of <i>intellectus</i>-<i>ratio</i>, it enables the person to 
explore realities that surpass mere sensory experience or utility, since “the 
desire for truth is part of human nature itself. It is an innate property of 
human reason to ask why things are as they are.”<a name="_ftnref35" title="" href="#_ftn35">[35]</a>&nbsp;Moving beyond the limits of empirical data, human intelligence can “with 
genuine certitude attain to reality itself as knowable.”<a name="_ftnref36" title="" href="#_ftn36">[36]</a>&nbsp;While reality remains only partially known, the desire for truth “spurs 
reason always to go further; indeed, it is as if reason were overwhelmed to see 
that it can always go beyond what it has already achieved.”<a name="_ftnref37" title="" href="#_ftn37">[37]</a>&nbsp;Although Truth in itself transcends the boundaries of human intelligence, 
it irresistibly attracts it.<a name="_ftnref38" title="" href="#_ftn38">[38]</a>&nbsp;Drawn by this attraction, the human person is led to seek “truths of a 
higher order.”<a name="_ftnref39" title="" href="#_ftn39">[39]</a></p>
<p>22. &nbsp;This innate drive toward the pursuit of truth is especially evident in the 
distinctly human capacities for semantic understanding and creativity,<a name="_ftnref40" title="" href="#_ftn40">[40]</a>&nbsp;through which this search unfolds in a “manner that is appropriate to the 
social nature and dignity of the human person.”<a name="_ftnref41" title="" href="#_ftn41">[41]</a>&nbsp;Likewise, a steadfast orientation to the truth is essential for charity to 
be both authentic and universal.<a name="_ftnref42" title="" href="#_ftn42">[42]</a></p>
<p>23. The search for truth 
finds its highest expression in openness to realities that transcend the 
physical and created world. In God, all truths attain their ultimate and original meaning.<a name="_ftnref43" title="" href="#_ftn43">[43]</a>&nbsp;Entrusting oneself to God is a “fundamental decision that engages the 
whole person.”<a name="_ftnref44" title="" href="#_ftn44">[44]</a><a name="_Hlk181548087">
</a>In this way, the human person becomes fully what he 
or she is called to be: “the intellect and the will display their spiritual 
nature,” enabling the person “to act in a way that realizes personal freedom to 
the full.”<a name="_ftnref45" title="" href="#_ftn45">[45]</a></p>
<p><i><a name="Stewardship_of_the_World">Stewardship of the World</a></i></p>
<p>24. The Christian faith 
understands creation as the free act of the Triune God, who, as Saint 
Bonaventure of Bagnoregio explains, creates “not to increase his glory, but to 
show it forth and to communicate it.”<a name="_ftnref46" title="" href="#_ftn46">[46]</a>&nbsp;Since God creates according to his Wisdom (cf. Wis. 9:9; Jer. 10:12), 
creation is imbued with an intrinsic order that reflects God’s plan (cf. Gen. 1; 
Dan. 2:21-22; Is. 45:18; Ps. 74:12-17; 104),<a name="_ftnref47" title="" href="#_ftn47">[47]</a>&nbsp;within which God has called human beings to assume a unique role: <i>to</i>
<i>cultivate and care for the world</i>.<a name="_ftnref48" title="" href="#_ftn48">[48]</a></p>
<p>25. Shaped by the Divine 
Craftsman, humans live out their identity as beings made <i>in imago Dei</i> by 
“keeping” and “tilling” (cf. Gen. 2:15) creation—using their intelligence and 
skills to care for and develop creation in accord with God’s plan.<a name="_ftnref49" title="" href="#_ftn49">[49]</a>&nbsp;In this, human intelligence reflects the Divine 
Intelligence that created all things (cf. Gen. 1-2; Jn. 1),<a name="_ftnref50" title="" href="#_ftn50">[50]</a>&nbsp;continuously sustains them, and guides them to their ultimate purpose in 
him.<a name="_ftnref51" title="" href="#_ftn51">[51]</a>&nbsp;Moreover, human beings are called to develop their abilities in science 
and technology, for through them, God is glorified (cf<i>. </i>Sir.<i> </i>
38:6). Thus, in a proper relationship with creation, humans, on the one hand, 
use their intelligence and skill to cooperate with God in guiding creation 
toward the purpose to which he has called it.<a name="_ftnref52" title="" href="#_ftn52">[52]</a> On the other hand, creation itself, as Saint Bonaventure observes, helps 
the human mind to “ascend gradually to the supreme Principle, who is God.”<a name="_ftnref53" title="" href="#_ftn53">[53]</a></p>
<p><i><a name="An_Integral">An Integral</a> Understanding of Human Intelligence</i></p>
<p>26. In this context, human 
intelligence becomes more clearly understood as a faculty that forms an integral 
part of how the whole person engages with reality. Authentic engagement requires 
embracing the full scope of one’s being: spiritual, cognitive, embodied, and 
relational.</p>
<p>27. This engagement with 
reality unfolds in various ways, as each person, in his or her multifaceted 
individuality<a name="_ftnref54" title="" href="#_ftn54">[54]</a>, seeks to understand the world, relate to others, solve problems, express 
creativity, and pursue integral well-being through the harmonious interplay of 
the various dimensions of the person’s intelligence.<a name="_ftnref55" title="" href="#_ftn55">[55]</a>&nbsp;This involves logical and linguistic abilities but can also encompass 
other modes of interacting with reality. Consider the work of an artisan, who 
“must know how to discern, in inert matter, a particular form that others cannot 
recognize”<a name="_ftnref56" title="" href="#_ftn56">[56]</a>&nbsp;and bring it forth through insight and practical skill. Indigenous peoples 
who live close to the earth often possess a profound sense of nature and its 
cycles.<a name="_ftnref57" title="" href="#_ftn57">[57]</a>&nbsp;Similarly, a friend who knows the right word to say or a person adept at 
managing human relationships exemplifies an intelligence that is “the fruit of 
self-examination, dialogue and generous encounter between persons.”<a name="_ftnref58" title="" href="#_ftn58">[58]</a>&nbsp;As Pope Francis observes, “in this age of artificial intelligence, we 
cannot forget that poetry and love are necessary to save our humanity.”<a name="_ftnref59" title="" href="#_ftn59">[59]</a></p>
<p>28. At the heart of the 
Christian understanding of intelligence is the integration of truth into the 
moral and spiritual life of the person, guiding his or her actions in light of 
God’s goodness and truth. According to God’s plan, 
intelligence, in its fullest sense, also includes the ability to savor what is 
true, good, and beautiful. As the twentieth-century 
French poet Paul Claudel expressed, “intelligence is nothing without delight.”<a name="_ftnref60" title="" href="#_ftn60">[60]</a> 
Similarly, Dante, upon reaching the highest heaven in <i>Paradiso</i>, testifies 
that the culmination of this intellectual delight is found in the “light 
intellectual full of love, love of true good filled with joy, joy which 
transcends every sweetness.”<a name="_ftnref61" title="" href="#_ftn61">[61]</a></p>
<p>29. A proper understanding 
of human intelligence, therefore, cannot be reduced to the mere acquisition of 
facts or the ability to perform specific tasks. Instead, it involves the 
person’s openness to the ultimate questions of life and reflects an orientation 
toward the True and the Good.<a name="_ftnref62" title="" href="#_ftn62">[62]</a>&nbsp;As an expression of the divine image within the person, human intelligence 
has the ability to access the totality of being, contemplating existence in its 
fullness, which goes beyond what is measurable, and grasping the meaning of what 
has been understood. For believers, this capacity includes, in a particular way, 
the ability to grow in the knowledge of the mysteries of God by using reason to 
engage ever more profoundly with revealed truths (<i>intellectus fidei</i>).<a name="_ftnref63" title="" href="#_ftn63">[63]</a>&nbsp;True intelligence is shaped by divine love, which “is poured forth in our 
hearts by the Holy Spirit” (Rom. 5:5). From this, it follows that human 
intelligence possesses an essential <i>contemplative </i>dimension, an unselfish 
openness to the True, the Good, and the Beautiful, beyond any utilitarian 
purpose.</p>
<p><i><a name="The_Limits_of_AI">The Limits of AI</a></i></p>
<p>30. In light of the 
foregoing discussion, the differences between human intelligence and current AI 
systems become evident. While AI is an extraordinary technological achievement capable of imitating 
certain outputs associated with human intelligence, it operates by performing 
tasks, achieving goals, or making decisions based on quantitative data and 
computational logic. For example, with its analytical power, AI excels at 
integrating data from a variety of fields, modeling complex systems, and 
fostering interdisciplinary connections. In this way, it can help experts 
collaborate in solving complex problems that “cannot be dealt with from a single 
perspective or from a single set of interests.”<a name="_ftnref64" title="" href="#_ftn64">[64]</a></p>
<p>31. However, even as AI 
processes and simulates certain expressions of intelligence, it remains 
fundamentally confined to a logical-mathematical framework, which imposes 
inherent limitations. Human intelligence, in contrast, develops organically 
throughout the person’s physical and psychological growth, shaped by a myriad of 
lived experiences in the flesh. Although advanced AI systems can “learn” through processes such as machine 
learning, this sort of training is fundamentally different from the 
developmental growth of human intelligence, which is shaped by embodied 
experiences, including sensory input, emotional responses, social interactions, 
and the unique context of each moment. These elements shape and form individuals 
within their personal history.<b></b>In contrast, AI, lacking a physical body, relies on computational reasoning and 
learning based on vast datasets that include recorded human experiences and 
knowledge. </p>
<p>32. Consequently, although 
AI can simulate aspects of human reasoning and perform specific tasks with incredible speed and efficiency, its computational 
abilities represent only a fraction of the broader capacities of the human mind. 
For instance, AI cannot currently replicate moral discernment or the ability to 
establish authentic relationships. Moreover, human intelligence is situated 
within a personally lived history of intellectual and moral formation that 
fundamentally shapes the individual’s perspective, encompassing the physical, emotional, social, moral, 
and spiritual dimensions of life. Since AI cannot offer this fullness of 
understanding, approaches that rely solely on this technology or treat it as the 
primary means of interpreting the world can lead to “a loss of appreciation for 
the whole, for the relationships between things, and for the broader horizon.”<a name="_ftnref65" title="" href="#_ftn65">[65]</a></p>
<p>33. Human intelligence is 
not primarily about completing functional tasks but about understanding and 
actively engaging with reality in all its dimensions; it is also capable of 
surprising insights. Since AI lacks the richness of corporeality, relationality, 
and the openness of the human heart to truth and goodness, its capacities—though 
seemingly limitless—are incomparable with the human ability to grasp reality. So 
much can be learned from an illness, an embrace of reconciliation, and even a 
simple sunset; indeed, many experiences we have as humans open new horizons and 
offer the possibility of attaining new wisdom. No device, working solely with 
data, can measure up to these and countless other experiences present in our 
lives.</p>
<p>34. Drawing an overly 
close equivalence between human intelligence and AI risks succumbing to a 
functionalist perspective, where people are valued based on the work they can 
perform. However, a person’s worth does not depend on possessing specific 
skills, cognitive and technological achievements, or individual success, but on 
the person’s inherent dignity, grounded in being created in the image of God.<a name="_ftnref66" title="" href="#_ftn66">[66]</a>&nbsp;This dignity remains intact in all circumstances, including 
for those unable 
to exercise their abilities, whether it be an unborn child, an unconscious 
person, or an older person who is suffering.<a name="_ftnref67" title="" href="#_ftn67"> [67]</a>&nbsp;It 
also underpins the tradition of human rights (and, in particular, 
what are now called “neuro-rights”), which represent “an important point of 
convergence in the search for common ground”<a name="_ftnref68" title="" href="#_ftn68">[68]</a>&nbsp;and can, thus, serve as a fundamental ethical guide in discussions on the 
responsible development and use of AI.</p>
<p>35. Considering all these 
points, as Pope Francis observes, “the very use of the word ‘intelligence’” in 
connection with AI “can prove misleading”<a name="_ftnref69" title="" href="#_ftn69">[69]</a>&nbsp;and risks overlooking what is most precious in the human person. In light 
of this, AI should not be seen as <i>an artificial form </i>of<i> </i>human 
intelligence but as<i> a product </i>of<i> </i>it.<a name="_ftnref70" title="" href="#_ftn70">[70]</a></p>
<p>IV. <b> <a name="The_Role_of_Ethics">The Role of Ethics</a> in Guiding the Development and Use of AI</b></p>
<p>36. Given these 
considerations, one can ask how AI can be understood within God’s plan. To answer this, it is important to recall that 
techno-scientific activity is not neutral in character but is a <i>human </i>
endeavor that engages the humanistic and cultural dimensions of human 
creativity.<a name="_ftnref71" title="" href="#_ftn71">[71]</a></p>
<p>37. Seen as a fruit of the 
potential inscribed within human intelligence,<a name="_ftnref72" title="" href="#_ftn72">[72]</a>&nbsp;scientific inquiry and the development of technical skills are part of the 
“collaboration of man and woman with God in perfecting the visible creation.”<a name="_ftnref73" title="" href="#_ftn73">[73]</a>&nbsp;At the same time, all scientific and technological achievements are, 
ultimately, gifts from God.<a name="_ftnref74" title="" href="#_ftn74">[74]</a>&nbsp;Therefore, human beings must always use their abilities in view of the 
higher purpose for which God has granted them.<a name="_ftnref75" title="" href="#_ftn75">[75]</a></p>
<p>38. We can gratefully 
acknowledge how technology has “remedied countless evils which used to harm and 
limit human beings,”<a name="_ftnref76" title="" href="#_ftn76">[76]</a>&nbsp;a fact for which we should rejoice. Nevertheless, not all technological 
advancements in themselves represent genuine human progress.<a name="_ftnref77" title="" href="#_ftn77">[77]</a>&nbsp;The Church is particularly opposed to those applications that threaten the 
sanctity of life or the dignity of the human person.<a name="_ftnref78" title="" href="#_ftn78">[78]</a>&nbsp;Like any human endeavor, technological development must be directed to 
serve the human person and contribute to the pursuit of “greater justice, more 
extensive fraternity, and a more humane order of social relations,” which are 
“more valuable than advances in the technical field.”<a name="_ftnref79" title="" href="#_ftn79">[79]</a>&nbsp;Concerns about the ethical implications of technological development are 
shared not only within the Church but also among many scientists, technologists, 
and professional associations, who increasingly call for ethical reflection to 
guide this development in a responsible way. </p>
<p>39. To address these 
challenges, it is essential to emphasize <i>the importance of moral 
responsibility grounded in the dignity and vocation of the human person</i>. 
This guiding principle also applies to questions concerning AI. In this context, 
the ethical dimension takes on primary importance because it is people who 
design systems and determine the purposes for which they are used.<a name="_ftnref80" title="" href="#_ftn80">[80]</a>&nbsp;Between a machine and a human being, only the latter is truly a moral 
agent—a subject of moral responsibility who exercises freedom in his or her 
decisions and accepts their consequences.<a name="_ftnref81" title="" href="#_ftn81">[81]</a>&nbsp;It is not the machine but the human who is in relationship with truth and 
goodness, guided by a moral conscience that calls the person “to love and to do 
what is good and to avoid evil,”<a name="_ftnref82" title="" href="#_ftn82">[82]</a>&nbsp;bearing witness to “the authority of truth in reference to the supreme 
Good to which the human person is drawn.”<a name="_ftnref83" title="" href="#_ftn83">[83]</a>&nbsp;Likewise, between a machine and a human, only the human can be 
sufficiently self-aware to the point of listening and following the voice of 
conscience, discerning with prudence, and seeking the good that is possible in 
every situation.<a name="_ftnref84" title="" href="#_ftn84">[84]</a>&nbsp;In fact, all of this also belongs to the person’s exercise of 
intelligence.</p>
<p>40. Like any product of 
human creativity, AI can be directed toward positive or negative ends.<a name="_ftnref85" title="" href="#_ftn85">[85]</a>&nbsp;When used in ways that respect human dignity and promote the well-being of 
individuals and communities, it can contribute positively to the human vocation.
Yet, as in all areas where humans are called to make 
decisions, the shadow of evil also looms here. Where human freedom allows for 
the possibility of choosing what is wrong, the moral evaluation of this 
technology will need to take into account how it is directed and used.</p>
<p>41. At the same time, it 
is not only the ends that are ethically significant but also the means employed 
to achieve them. Additionally, the overall vision and understanding of the human 
person embedded within these systems are important to consider as well. 
Technological products reflect the worldview of their developers, owners, users, 
and regulators,<a name="_ftnref86" title="" href="#_ftn86">[86]</a>&nbsp;and have the power to “shape the world and engage consciences on the level 
of values.”<a name="_ftnref87" title="" href="#_ftn87">[87]</a>&nbsp;On a societal level, some technological developments could also reinforce 
relationships and power dynamics that are inconsistent with a proper 
understanding of the human person and society. </p>
<p>42. Therefore, the ends 
and the means used in a given application of AI, as well as the overall vision 
it incorporates, must all be evaluated to ensure they respect human dignity and 
promote the common good.<a name="_ftnref88" title="" href="#_ftn88">[88]</a>&nbsp;As Pope Francis has stated, “the intrinsic dignity of every man and every 
woman” must be “the key criterion in evaluating emerging technologies; these 
will prove ethically sound to the extent that they help respect that dignity and 
increase its expression at every level of human life,”<a name="_ftnref89" title="" href="#_ftn89">[89]</a>&nbsp;including in the social and economic spheres. In this sense, human 
intelligence plays a crucial role not only in designing and producing technology 
but also in directing its use in line with the authentic good of the human 
person.<a name="_ftnref90" title="" href="#_ftn90">[90]</a>&nbsp;The responsibility for managing this wisely pertains to every level of 
society, guided by the principle of subsidiarity and other principles of 
Catholic Social Teaching. </p>
<p><i><a name="Helping_Human">Helping Human</a> Freedom and Decision-Making</i></p>
<p>43. The commitment to 
ensuring that <i>AI always supports and promotes the supreme value of the 
dignity of every human being and the fullness of the human vocation</i> serves 
as a criterion of discernment for developers, owners, operators, and regulators 
of AI, as well as to its users. It remains valid for every application of the 
technology at every level of its use.</p>
<p>44. An evaluation of the 
implications of this guiding principle could begin by considering the importance 
of <i>moral responsibility. </i>Since full moral causality belongs only to <i>
personal</i> agents, not artificial ones, it is crucial to be able to identify 
and define who bears responsibility for the processes involved in AI, 
particularly those capable of learning, correction, and reprogramming. While 
bottom-up approaches and very deep neural networks enable AI to solve complex 
problems, they make it difficult to understand the processes that lead to the 
solutions they adopted. This complicates accountability since if an AI 
application produces undesired outcomes, determining who is responsible becomes 
difficult. To address this problem, attention needs to be given to the nature of
<i>accountability </i>processes in complex, highly automated settings, where 
results may only become evident in the medium to long term. For this, it is 
important that ultimate responsibility for decisions made using AI rests with 
the human decision-makers and that there is accountability for the use of AI at 
each stage of the decision-making process.<a name="_ftnref91" title="" href="#_ftn91">[91]</a></p>
<p>45. In addition to 
determining who is responsible, it is essential to identify the objectives 
given to AI systems. Although these systems may use unsupervised autonomous 
learning mechanisms and sometimes follow paths that humans cannot reconstruct, 
they ultimately pursue goals that humans have assigned to them and are governed 
by processes established by their designers and programmers. Yet, this presents 
a challenge because, as AI models become increasingly capable of independent 
learning, the ability to maintain control over them to ensure that such 
applications serve human purposes may effectively diminish. This raises the 
critical question of how to ensure that AI systems are ordered for the good of 
people and not against them. </p>
<p>46. While responsibility 
for the ethical use of AI systems starts with those who develop, produce, 
manage, and oversee such systems, it is also shared by those who use them. As Pope 
Francis noted, the machine “makes a technical choice among several possibilities 
based either on well-defined criteria or on statistical inferences. Human 
beings, however, not only choose, but in their hearts are capable of deciding.”<a name="_ftnref92" title="" href="#_ftn92">[92]</a>&nbsp;Those who use AI to accomplish a task and follow its results create a 
context in which they are ultimately responsible for the power they have 
delegated. Therefore, insofar as AI can assist humans in making decisions, the 
algorithms that govern it should be trustworthy, secure, robust enough to handle 
inconsistencies, and transparent in their operation to mitigate biases and 
unintended side effects.<a name="_ftnref93" title="" href="#_ftn93">[93]</a>&nbsp;Regulatory frameworks should ensure that all legal entities remain 
accountable for the use of AI and all its consequences, with appropriate 
safeguards for transparency, privacy, and accountability.<a name="_ftnref94" title="" href="#_ftn94">[94]</a>&nbsp;Moreover, those using AI should be careful not to become overly dependent 
on it for their decision-making, a trend that increases contemporary society’s 
already high reliance on technology.</p>
<p>47. The Church’s moral and 
social teaching provides resources to help ensure that AI is used in a way that 
preserves human agency. Considerations about justice, for example, should also 
address issues such as fostering just social dynamics, upholding international 
security, and promoting peace. By exercising prudence, individuals and 
communities can discern ways to use AI to benefit humanity while avoiding 
applications that could degrade human dignity or harm the environment. In this 
context, the concept of responsibility should be understood not only in its most 
limited sense but as a “responsibility for the care for others, which is more 
than simply accounting for results achieved.”<a name="_ftnref95" title="" href="#_ftn95">[95]</a></p>
<p>48. Therefore, AI, like 
any technology, can be part of a conscious and responsible answer to humanity’s 
vocation to the good. However, as previously discussed, AI must be directed by 
human intelligence<i> </i>to align with this vocation, ensuring it respects the 
dignity of the human person. Recognizing this “exalted dignity,” the Second 
Vatican Council affirmed that “the social order and its development must 
invariably work to the benefit of the human person.”<a name="_ftnref96" title="" href="#_ftn96">[96]</a>&nbsp;In light of this, the use of AI, as Pope Francis said, must be 
“accompanied by an ethic inspired by a vision of the common good, an ethic of 
freedom, responsibility, and fraternity, capable of fostering the full 
development of people in relation to others and to the whole of creation.”<a name="_ftnref97" title="" href="#_ftn97">[97]</a></p>
<p>V. <b> <a name="Specific_Questions">Specific Questions</a></b></p>
<p>49. Within this general 
perspective, some observations follow below to illustrate how the preceding 
arguments can help provide an ethical orientation in practical situations, in 
line with the “wisdom of heart” that Pope Francis has proposed.<a name="_ftnref98" title="" href="#_ftn98">[98]</a> 
While not exhaustive, this discussion is offered in service of the dialogue that 
considers how AI can be used to uphold the dignity of the human person and 
promote the common good.<a name="_ftnref99" title="" href="#_ftn99">[99]</a></p>
<p><i><a name="AI_and_Society">AI and Society</a></i></p>
<p>50. As Pope Francis 
observed, “the inherent dignity of each human being and the fraternity that 
binds us together as members of the one human family must undergird the 
development of new technologies and serve as indisputable criteria for 
evaluating them before they are employed<a name="_Int_Zt1Jg2IN">.”</a><a name="_ftnref100" title="" href="#_ftn100">[100]</a></p>
<p>51. Viewed through this 
lens, AI could “introduce important innovations in agriculture, education and 
culture, an improved level of life for entire nations and peoples, and the 
growth of human fraternity and social friendship,” and thus be “used to promote 
integral human development.”<a name="_ftnref101" title="" href="#_ftn101">[101]</a>&nbsp;AI could also help organizations identify those in need and counter 
discrimination and marginalization. These and other similar applications of this 
technology could contribute to human development and the common good.<a name="_ftnref102" title="" href="#_ftn102">[102]</a></p>
<p>52. However, while AI 
holds many possibilities for promoting the good, it can also hinder or even 
counter human development and the common good. Pope Francis has noted that 
“evidence to date suggests that digital technologies have increased inequality 
in our world. Not just differences in material wealth, which are also 
significant, but also differences in access to political and social influence.”<a name="_ftnref103" title="" href="#_ftn103">[103]</a>&nbsp;In this sense, AI could be used to perpetuate marginalization and 
discrimination, create new forms of poverty, widen the “digital divide,” and 
worsen existing social inequalities.<a name="_ftnref104" title="" href="#_ftn104">[104]</a></p>
<p>53. Moreover, the 
concentration of the power over mainstream AI applications in the hands of a few 
powerful companies raises significant ethical concerns. Exacerbating this 
problem is the inherent nature of AI systems, where no single individual can 
exercise complete oversight over the vast and complex datasets used for 
computation. This lack of well-defined accountability creates the risk that AI 
could be manipulated for personal or corporate gain or to direct public opinion 
for the benefit of a specific industry. Such entities, motivated by their own 
interests, possess the capacity to exercise “forms of control as subtle as they 
are invasive, creating mechanisms for the manipulation of consciences and of the 
democratic process.”<a name="_ftnref105" title="" href="#_ftn105">[105]</a></p>
<p>54. &nbsp;Furthermore, there is 
the risk of AI being used to promote what Pope Francis has called the 
“technocratic paradigm,” which perceives all the world’s problems as solvable 
through technological means alone.<a name="_ftnref106" title="" href="#_ftn106">[106]</a>&nbsp;In this paradigm, human dignity and fraternity are often set aside in the 
name of efficiency, “as if reality, goodness, and truth automatically flow from 
technological and economic power as such.”<a name="_ftnref107" title="" href="#_ftn107">[107]</a>&nbsp;Yet, human dignity and the common good must never be violated for the sake 
of efficiency,<a name="_ftnref108" title="" href="#_ftn108">[108]</a>&nbsp;for “technological developments that do not lead to an improvement in the 
quality of life of all humanity, but on the contrary, aggravate inequalities and 
conflicts, can never count as true progress.”<a name="_ftnref109" title="" href="#_ftn109">[109]</a>&nbsp;Instead, AI should be put “at the service of another type of progress, one 
which is healthier, more human, more social, more integral.”<a name="_ftnref110" title="" href="#_ftn110">[110]</a></p>
<p>55. Achieving 
this objective requires a deeper reflection on the relationship between autonomy 
and responsibility. Greater autonomy heightens each person’s responsibility 
across various aspects of communal life. For Christians, the foundation of this 
responsibility lies in the recognition that all human capacities, including the 
person’s autonomy, come from God and are meant to be used in the service of 
others<a name="_Int_JUdzKSa4">.</a><a name="_ftnref111" title="" href="#_ftn111">[111]</a>&nbsp;Therefore, rather than merely pursuing economic or technological 
objectives, AI should serve “the common good of the entire human family,” which 
is “the sum total of social conditions that allow people, either as groups or as 
individuals, to reach their fulfillment more fully and more easily.”<a name="_ftnref112" title="" href="#_ftn112">[112]</a></p>
<p><i><a name="AI_and_Human_Relationships">AI and Human Relationships</a></i></p>
<p>56. The Second Vatican 
Council observed that “by his innermost nature man is a social being; and if he 
does not enter into relations with others, he can neither live nor develop his 
gifts.”<a name="_ftnref113" title="" href="#_ftn113">[113]</a>&nbsp;This conviction underscores that living in society is intrinsic to the 
nature and vocation of the human person.<a name="_ftnref114" title="" href="#_ftn114">[114]</a>&nbsp;As social beings, we seek relationships that involve mutual exchange and 
the pursuit of truth, in the course of which, people “share with each other the 
truth they have discovered, or think they have discovered, in such a way that 
they help one another in the search for truth.”<a name="_ftnref115" title="" href="#_ftn115">[115]</a></p>
<p>57. &nbsp;Such a quest, along 
with other aspects of human communication, presupposes encounters and mutual 
exchange between individuals shaped by their unique histories, thoughts, 
convictions, and relationships. Nor can we forget that human intelligence is a 
diverse, multifaceted, and complex reality: individual and social, rational and 
affective, conceptual and symbolic. Pope Francis underscores this dynamic, 
noting that “together, we can seek the truth in dialogue, in relaxed 
conversation or in passionate debate. To do so calls for perseverance; it 
entails moments of silence and suffering, yet it can patiently embrace the 
broader experience of individuals and peoples. […] The process of building 
fraternity, be it local or universal, can only be undertaken by spirits that are 
free and open to authentic encounters.”<a name="_ftnref116" title="" href="#_ftn116">[116]</a></p>
<p>58. It is in this context 
that one can consider the challenges AI poses to human relationships. Like other 
technological tools, AI has the potential to foster connections within the human 
family. However, it could also hinder a true encounter with reality and, 
ultimately, lead people to “a deep and melancholic dissatisfaction with 
interpersonal relations, or a harmful sense of isolation.”<a name="_ftnref117" title="" href="#_ftn117">[117]</a>&nbsp;Authentic human relationships require the richness of being with others in 
their pain, their pleas, and their joy.<a name="_ftnref118" title="" href="#_ftn118">[118]</a>&nbsp;Since human intelligence is expressed and enriched also in interpersonal 
and embodied ways, authentic and spontaneous encounters with others are 
indispensable for engaging with reality in its fullness.</p>
<p>59. Because “true wisdom 
demands an encounter with reality,”<a name="_ftnref119" title="" href="#_ftn119">[119]</a>&nbsp;the rise of AI introduces another challenge. Since AI can effectively 
imitate the products of human intelligence, the ability to know when one is 
interacting with a human or a machine can no longer be taken for granted. 
Generative AI can produce text, speech, images, and other advanced outputs that 
are usually associated with human beings. Yet, it must be understood for what it 
is: a tool, not a person.<a name="_ftnref120" title="" href="#_ftn120">[120]</a>&nbsp;This distinction is often obscured by the language used by practitioners, 
which tends to anthropomorphize AI and thus blurs the line between human and 
machine. </p>
<p>60. Anthropomorphizing AI 
also poses specific challenges for the development of children, potentially 
encouraging them to develop patterns of interaction that treat human 
relationships in a transactional manner, as one would relate to a chatbot. Such 
habits could lead young people to see teachers as mere dispensers of information 
rather than as mentors who guide and nurture their intellectual and moral 
growth. Genuine relationships, rooted in empathy and a steadfast commitment to 
the good of the other, are essential and irreplaceable in fostering the full 
development of the human person. </p>
<p>61. In this context, it is 
important to clarify that, despite the use of anthropomorphic language, no AI 
application can genuinely experience empathy. Emotions cannot be reduced to 
facial expressions or phrases generated in response to prompts; they reflect the 
way a person, as a whole, relates to the world and to his or her own life, with 
the body playing a central role. True empathy requires the ability to listen, 
recognize another’s irreducible uniqueness, welcome their otherness, and grasp 
the meaning behind even their silences.<a name="_ftnref121" title="" href="#_ftn121">[121]</a>&nbsp;Unlike the realm of analytical judgment in which AI excels, true empathy 
belongs to the relational sphere. It involves intuiting and apprehending the 
lived experiences of another while maintaining the distinction between self and 
other.<a name="_ftnref122" title="" href="#_ftn122">[122]</a>&nbsp;While AI can simulate empathetic responses, it cannot replicate the 
eminently personal and relational nature of authentic empathy.<a name="_ftnref123" title="" href="#_ftn123">[123]</a></p>
<p>62. In light of the above, 
it is clear why misrepresenting AI as a person should always be avoided; doing 
so for fraudulent purposes is a grave ethical violation that could erode social 
trust. Similarly, using AI to deceive in other contexts—such as in education or 
in human relationships, including the sphere of sexuality—is also to be 
considered immoral and requires careful oversight to prevent harm, maintain 
transparency, and ensure the dignity of all <a name="_Int_d05vIJmL">people.</a><a name="_ftnref124" title="" href="#_ftn124">[124]</a></p>
<p>63. &nbsp;In an increasingly 
isolated world, some people have turned to AI in search of deep human 
relationships, simple companionship, or even emotional bonds. However, while 
human beings are meant to experience authentic relationships, AI can only 
simulate them. Nevertheless, such relationships with 
others are an integral part of how a person grows to become who he or she is 
meant to be. If AI is used to help people foster genuine connections 
between people, it can contribute positively to the full realization of the 
person. Conversely, if we replace relationships with God and with others with 
interactions with technology, we risk replacing authentic relationality with a 
lifeless image (cf. Ps. 106:20; Rom. 1:22-23). Instead of retreating into 
artificial worlds, we are called to engage in a committed and intentional way 
with reality, especially by identifying with the poor and suffering, consoling 
those in sorrow, and forging bonds of communion with all. </p>
<p><i><a name="AI,_the_Economy,_and_Labor">AI, the Economy, and Labor</a></i></p>
<p>64. Due to its 
interdisciplinary nature, AI is being increasingly integrated into economic and 
financial systems. Significant investments are currently being made not only in 
the technology sector but also in energy, finance, and media, particularly in 
the areas of marketing and sales, logistics, technological innovation, 
compliance, and risk management. At the same time, AI’s applications in these 
areas have also highlighted its ambivalent nature, as a source of tremendous 
opportunities but also profound risks. A first real critical point in this area 
concerns the possibility that—due to the concentration of AI applications in the 
hands of a few corporations—only those large companies would benefit from the 
value created by AI rather than the businesses that use it.</p>
<p>65. Other broader aspects 
of AI’s impact on the economic-financial sphere must also be carefully examined, 
particularly concerning the interaction between concrete reality and the digital 
world. One important consideration in this regard involves the coexistence of 
diverse and alternative forms of economic and financial institutions within a 
given context. This factor should be encouraged, as it can bring benefits in how 
it supports the real economy by fostering its development and stability, 
especially during times of crisis. Nevertheless, it should be stressed that 
digital realities, not restricted by any spatial bonds, tend to be more 
homogeneous and impersonal than communities rooted in a particular place and a 
specific history, with a common journey characterized by shared values and 
hopes, but also by inevitable disagreements and divergences. This diversity is 
an undeniable asset to a community’s economic life. Turning over the economy and 
finance entirely to digital technology would reduce this variety and richness. 
As a result, many solutions to economic problems that can be reached through 
natural dialogue between the involved parties may no longer be attainable in a 
world dominated by procedures and only the appearance of nearness.</p>
<p>66. Another area where AI 
is already having a profound impact is the world of work. As in many other 
fields, AI is driving fundamental transformations across many professions, with 
a range of effects. On the one hand, it has the potential to enhance expertise 
and productivity, create new jobs, enable workers to focus on more innovative 
tasks, and open new horizons for creativity and innovation. </p>
<p>67. However, while AI 
promises to boost productivity by taking over mundane tasks, it frequently 
forces workers to adapt to the speed and demands of machines rather than 
machines being designed to support those who work. As a result, contrary to the 
advertised benefits of AI, current approaches to the technology can 
paradoxically <i>deskill</i> workers, subject them to automated surveillance, 
and relegate them to rigid and repetitive tasks. The need to keep up with the 
pace of technology can erode workers’ sense of agency and stifle
the innovative abilities they are expected to bring to 
their work.<a name="_ftnref125" title="" href="#_ftn125">[125]</a> </p>
<p>68. AI is currently 
eliminating the need for some jobs that were once performed by humans. If AI is 
used to replace human workers rather than complement them, there is a 
“substantial risk of disproportionate benefit for the few at the price of the 
impoverishment of many.”<a name="_ftnref126" title="" href="#_ftn126">[126]</a> 
Additionally, as AI becomes more powerful, there is an associated risk that 
human labor may lose its value in the economic realm. This is the logical 
consequence of the technocratic paradigm: a world of humanity enslaved to 
efficiency, where, ultimately, the cost of humanity 
must be cut. Yet, human lives are intrinsically valuable, independent of their 
economic output. Nevertheless, the “current model,” Pope Francis explains, “does 
not appear to favor an investment in efforts to help the slow, the weak, or the 
less talented to find opportunities in life.”<a name="_ftnref127" title="" href="#_ftn127">[127]</a> 
In light of this, “we cannot allow a tool as powerful and indispensable as 
Artificial Intelligence to reinforce such a paradigm, but rather, we must make 
Artificial Intelligence a bulwark against its expansion.”<a name="_ftnref128" title="" href="#_ftn128"> [128]</a></p>
<p>69. It is important to 
remember that “the order of things must be subordinate to the order of persons, 
and not the other way around.”<a name="_ftnref129" title="" href="#_ftn129">[129]</a> 
Human work must not only be at the service of profit but at “the service of the 
whole human person […] taking into account the person’s material needs and the 
requirements of his or her intellectual, moral, spiritual, and religious life.”<a name="_ftnref130" title="" href="#_ftn130">[130]</a> 
In this context, the Church recognizes that work is “not only a means of earning 
one’s daily bread” but is also “an essential dimension of social life” and “a 
means […] of personal growth, the building of healthy relationships, 
self-expression and the exchange of gifts. Work gives us a sense of shared 
responsibility for the development of the world, and ultimately, for our life as 
a people.”<a name="_ftnref131" title="" href="#_ftn131">[131]</a></p>
<p>70. &nbsp;Since work is a “part 
of the meaning of life on this earth, a path to growth, human development and 
personal fulfillment,” “the goal should not be that technological progress 
increasingly replaces human work, for this would be detrimental to humanity”<a name="_ftnref132" title="" href="#_ftn132">[132]</a>—rather, 
it should promote human labor. Seen in this light, AI should assist, not 
replace, human judgment. Similarly, it must never degrade creativity or reduce 
workers to mere “cogs in a machine.” Therefore, “respect for the dignity of 
laborers and the importance of employment for the economic well-being of 
individuals, families, and societies, for job security and just wages, ought to 
be a high priority for the international community as these forms of technology 
penetrate more deeply into our workplaces.”<a name="_ftnref133" title="" href="#_ftn133">[133]</a></p>
<p><i><a name="AI_and_Healthcare">AI and Healthcare</a></i></p>
<p>71. As participants in 
God’s healing work, healthcare professionals have the vocation and 
responsibility to be “guardians and servants of human life.”<a name="_ftnref134" title="" href="#_ftn134">[134]</a> 
Because of this, the healthcare profession carries an “intrinsic and undeniable 
ethical dimension,” recognized by the Hippocratic Oath, which obliges physicians 
and healthcare professionals to commit themselves to having “absolute respect 
for human life and its sacredness.”<a name="_ftnref135" title="" href="#_ftn135">[135]</a> 
Following the example of the Good Samaritan, this commitment is to be carried 
out by men and women “who reject the creation of a society of exclusion, and act 
instead as neighbors, lifting up and rehabilitating the fallen for the sake of 
the common good.”<a name="_ftnref136" title="" href="#_ftn136">[136]</a></p>
<p>72. Seen in this light, AI 
seems to hold immense potential in a variety of applications in the medical 
field, such as assisting the diagnostic work of healthcare providers, 
facilitating relationships between patients and medical staff, offering new 
treatments, and expanding access to quality care also for those who are isolated 
or marginalized. In these ways, the technology could enhance the “compassionate 
and loving closeness”<a name="_ftnref137" title="" href="#_ftn137">[137]</a> that 
healthcare providers are called to extend to the sick and suffering.</p>
<p>73. However, if AI is used 
not to enhance but to replace the relationship between patients and healthcare 
providers—leaving patients to interact with a machine rather than a human 
being—it would reduce a crucially important human relational structure to a 
centralized, impersonal, and unequal framework. Instead of encouraging 
solidarity with the sick and suffering, such applications of AI would risk 
worsening the loneliness that often accompanies illness, especially in the 
context of a culture where “persons are no longer seen as a paramount value to 
be cared for and respected.”<a name="_ftnref138" title="" href="#_ftn138">[138]</a> 
This misuse of AI would not align with respect for the dignity of the human 
person and solidarity with the suffering.</p>
<p>74. Responsibility for the 
well-being of patients and the decisions that touch upon their lives are at the 
heart of the healthcare profession. This accountability requires medical 
professionals to exercise all their skill and intelligence in making 
well-reasoned and ethically grounded choices regarding those entrusted to their 
care, always respecting the inviolable dignity of the patients and the need for 
informed consent. As a result, decisions regarding patient treatment and the 
weight of responsibility they entail must always remain with the human person 
and should never be delegated to AI.<a name="_ftnref139" title="" href="#_ftn139">[139]</a></p>
<p>75. In addition, using AI 
to determine who should receive treatment based predominantly on economic 
measures or metrics of efficiency represents a particularly problematic instance 
of the “technocratic paradigm” that must be rejected.<a name="_ftnref140" title="" href="#_ftn140">[140]</a> 
For, “optimizing resources means using them in an ethical and fraternal way, and 
not penalizing the most fragile.”<a name="_ftnref141" title="" href="#_ftn141">[141]</a> 
Additionally, AI tools in healthcare are “exposed to forms of bias and 
discrimination,” where “systemic errors can easily multiply, producing not only 
injustices in individual cases but also, due to the domino effect, real forms of 
social inequality.”<a name="_ftnref142" title="" href="#_ftn142">[142]</a></p>
<p>76. The integration of AI 
into healthcare also poses the risk of amplifying other existing disparities in 
access to medical care. As healthcare becomes increasingly oriented toward 
prevention and lifestyle-based approaches, AI-driven solutions may inadvertently 
favor more affluent populations who already enjoy better access to medical 
resources and quality nutrition. This trend risks reinforcing a “medicine for 
the rich” model, where those with financial means benefit from advanced 
preventative tools and personalized health information while others struggle to 
access even basic services. To prevent such inequities, equitable frameworks are 
needed to ensure that the use of AI in healthcare does not worsen existing 
healthcare inequalities but rather serves the common good. </p>
<p><i><a name="AI_and_Education">AI and Education</a></i></p>
<p>77. The words of the 
Second Vatican Council remain fully relevant today: “True education strives to 
form individuals with a view toward their final end and the good of the society 
to which they belong.”<a name="_ftnref143" title="" href="#_ftn143">[143]</a> As 
such, education is “never a mere process of passing on facts and intellectual 
skills: rather, its aim is to contribute to the person’s holistic formation in 
its various aspects (intellectual, cultural, spiritual, etc.), including, for 
example, community life and relations within the academic community,”<a name="_ftnref144" title="" href="#_ftn144">[144]</a> 
in keeping with the nature and dignity of the human person.</p>
<p>78. This approach involves 
a commitment to cultivating the mind, but always as a part of the integral 
development of the person: “We must break that idea of education which holds 
that educating means filling one’s head with ideas. That is the way we educate 
automatons, cerebral minds, not people. Educating is taking a risk in the 
tension between the mind, the heart, and the hands.”<a name="_ftnref145" title="" href="#_ftn145">[145]</a></p>
<p>79. At the center of this 
work of forming the whole human person is the indispensable relationship between 
teacher and student. Teachers do more than convey knowledge; they model 
essential human qualities and inspire the joy of discovery.<a name="_ftnref146" title="" href="#_ftn146">[146]</a> 
Their presence motivates students both through the content they teach and the 
care they demonstrate for their students. This bond fosters trust, mutual 
understanding, and the capacity to address each person’s unique dignity and 
potential. On the part of the student, this can generate a genuine desire to 
grow. The physical presence of a teacher creates a relational dynamic that AI 
cannot replicate, one that deepens engagement and nurtures the student’s 
integral development.</p>
<p>80. In this context, AI 
presents both opportunities and challenges. If used in a prudent manner, within 
the context of an existing teacher-student relationship and ordered to the 
authentic goals of education, AI can become a valuable educational resource by 
enhancing access to education, offering tailored support, and providing 
immediate feedback to students. These benefits could enhance the learning 
experience, especially in cases where individualized attention is needed, or 
educational resources are otherwise scarce. </p>
<p>81. Nevertheless, an 
essential part of education is forming “the intellect to reason well in all 
matters, to reach out towards truth, and to grasp it,”<a name="_ftnref147" title="" href="#_ftn147">[147]</a> 
while helping the “language of the head” to grow harmoniously with the “language 
of the heart” and the “language of the hands.”<a name="_ftnref148" title="" href="#_ftn148">[148]</a> 
This is all the more vital in an age marked by technology, in which “it is no 
longer merely a question of ‘using’ instruments of communication, but of living 
in a highly digitalized culture that has had a profound impact on […] our 
ability to communicate, learn, be informed and enter into relationship with 
others.”<a name="_ftnref149" title="" href="#_ftn149">[149]</a> However, instead of 
fostering “a cultivated intellect,” which “brings with it a power and a grace to 
every work and occupation that it undertakes,”<a name="_ftnref150" title="" href="#_ftn150">[150]</a> 
the extensive use of AI in education could lead to the students’ increased 
reliance on technology, eroding their ability to perform some skills 
independently and worsening their dependence on screens.<a name="_ftnref151" title="" href="#_ftn151">[151]</a> 
</p>
<p>82. Additionally, while 
some AI systems are designed to help people develop their critical thinking 
abilities and problem-solving skills, many others merely provide answers instead 
of prompting students to arrive at answers themselves or write text for 
themselves.<a name="_ftnref152" title="" href="#_ftn152">[152]</a>&nbsp;Instead of training young people how to amass information and generate 
quick responses, education should encourage “the responsible use of freedom to 
face issues with good sense and intelligence.”<a name="_ftnref153" title="" href="#_ftn153">[153]</a>&nbsp;Building on this, “education in the use of forms of artificial 
intelligence should aim above all at promoting critical thinking. Users of all 
ages, but especially the young, need to develop a discerning approach to the use 
of data and content collected on the web or produced by artificial intelligence 
systems. Schools, universities, and scientific societies are challenged to help 
students and professionals to grasp the social and ethical aspects of the 
development and uses of technology.”<a name="_ftnref154" title="" href="#_ftn154">[154]</a></p>
<p>83. As Saint John Paul II 
recalled, “in the world today, characterized by such rapid developments in 
science and technology, the tasks of a Catholic University assume an ever 
greater importance and urgency.”<a name="_ftnref155" title="" href="#_ftn155">[155]</a>&nbsp;In a particular way, Catholic universities are urged to be present as 
great laboratories of hope at this crossroads of history. In an 
inter-disciplinary and cross-disciplinary key, they are urged to engage “with 
wisdom and creativity”<a name="_ftnref156" title="" href="#_ftn156">[156]</a>&nbsp;in careful research on this phenomenon, helping to draw out the salutary 
potential within the various fields of science and reality, and guiding them 
always towards ethically sound applications that clearly serve the cohesion of 
our societies and the common good, reaching new frontiers in the dialogue 
between faith and reason. </p>
<p>84. Moreover, it should be noted that current AI 
programs have been known to provide biased or fabricated information, which can 
lead students to trust inaccurate content. This problem “not only runs the risk 
of legitimizing fake news and strengthening a dominant culture’s advantage, but, 
in short, it also undermines the educational process itself.”<a name="_ftnref157" title="" href="#_ftn157">[157]</a>&nbsp;With time, clearer distinctions may emerge between proper and improper 
uses of AI in education and research. Yet, a decisive guideline is that the use 
of AI should always be transparent and never misrepresented.</p>
<p><i><a name="AI,_Misinformation">AI, Misinformation</a>, Deepfakes, and Abuse</i></p>
<p>85. AI could be used as an 
aid to human dignity if it helps people understand complex concepts or directs 
them to sound resources that support their search for the truth.<a name="_ftnref158" title="" href="#_ftn158">[158]</a></p>
<p>86. However, AI also 
presents a serious risk of generating manipulated content and false information, 
which can easily mislead people due to its resemblance to the truth. Such 
misinformation might occur unintentionally, as in the case of AI 
“hallucination,” where a generative AI system yields results that appear real 
but are not. Since generating content that mimics human artifacts is central to 
AI’s functionality, mitigating these risks proves challenging. Yet, the 
consequences of such aberrations and false information can be quite grave. For 
this reason, all those involved in producing and using AI systems should be 
committed to the truthfulness and accuracy of the information processed by such 
systems and disseminated to the public. </p>
<p>87. While AI has a latent 
potential to generate false information, an even more troubling problem lies in 
the deliberate misuse of AI for manipulation. This can occur when individuals or 
organizations intentionally generate and spread false content with the aim to 
deceive or cause harm, such as “deepfake” images, videos, and audio—referring to 
a false depiction of a person, edited or generated by an AI algorithm. The 
danger of deepfakes is particularly evident when they are used to target or harm 
others. While the images or videos themselves may be artificial, the damage they 
cause is real, leaving “deep scars in the hearts of those who suffer it” and 
“real wounds in their human dignity.”<a name="_ftnref159" title="" href="#_ftn159">[159]</a></p>
<p>88. On a broader scale, by 
distorting “our relationship with others and with reality,”<a name="_ftnref160" title="" href="#_ftn160">[160]</a>&nbsp;AI-generated fake media can gradually undermine the foundations of 
society. This issue requires careful regulation, as misinformation—especially 
through AI-controlled or influenced media—can spread unintentionally, fueling 
political polarization and social unrest. When society becomes indifferent to 
the truth, various groups construct their own versions of “facts,” weakening the 
“reciprocal ties and mutual dependencies”<a name="_ftnref161" title="" href="#_ftn161">[161]</a>&nbsp;that underpin the fabric of social life. As deepfakes cause people to 
question everything and AI-generated false content erodes trust in what they see 
and<b> </b>hear, polarization and conflict will only grow. Such widespread 
deception is no trivial matter; it strikes at the core of humanity, dismantling 
the foundational trust on which societies are built.<a name="_ftnref162" title="" href="#_ftn162">[162]</a></p>
<p>89. Countering AI-driven 
falsehoods is not only the work of industry experts—it requires the efforts of 
all people of goodwill. “If technology is to serve human dignity and not harm 
it, and if it is to promote peace rather than violence, then the human community 
must be proactive in addressing these trends with respect to human dignity and 
the promotion of the good.”<a name="_ftnref163" title="" href="#_ftn163">[163]</a>&nbsp;Those who produce and share AI-generated content should always exercise 
diligence in verifying the truth of what they disseminate and, in all cases, 
should “avoid the sharing of words and images that are degrading of human 
beings, that promote hatred and intolerance, that debase the goodness and 
intimacy of human sexuality or that exploit the weak and vulnerable.”<a name="_ftnref164" title="" href="#_ftn164">[164]</a>&nbsp;This calls for the ongoing prudence and careful discernment of all users 
regarding their activity online.<a name="_ftnref165" title="" href="#_ftn165">[165]</a></p>
<p><i><a name="AI,_Privacy">AI, Privacy</a>, and Surveillance</i></p>
<p>90. &nbsp;Humans are inherently 
relational, and the data each person generates in the digital world can be seen 
as an objectified expression of this relational nature. Data conveys not only 
information but also personal and relational knowledge, which, in an 
increasingly digitized context, can amount to power over the individual. 
Moreover, while some types of data may pertain to public aspects of a person’s 
life, others may touch upon the individual’s interiority, perhaps even their 
conscience. Seen in this way, privacy plays an essential role in protecting the 
boundaries of a person’s inner life, preserving their freedom to relate to 
others, express themselves, and make decisions without undue control. This 
protection is also tied to the defense of religious freedom, as surveillance can 
also be misused to exert control over the lives of believers and how they 
express their faith.</p>
<p>91. It is appropriate, 
therefore, to address the issue of privacy from a concern for the legitimate 
freedom and inalienable dignity of the human person “in all circumstances.”<a name="_ftnref166" title="" href="#_ftn166">[166]</a> 
The Second Vatican Council included the right “to safeguard privacy” among the 
fundamental rights “necessary for living a genuinely human life,” a right that 
should be extended to all people on account of their “sublime dignity.”<a name="_ftnref167" title="" href="#_ftn167">[167]</a> 
Furthermore, the Church has also affirmed the right to the legitimate respect 
for a private life in the context of affirming the person’s right to a good 
reputation, defense of their physical and mental integrity, and freedom from 
harm or undue intrusion<a name="_ftnref168" title="" href="#_ftn168">[168]</a>—essential 
components of the due respect for the intrinsic dignity of the human person.<a name="_ftnref169" title="" href="#_ftn169">[169]</a></p>
<p>92. Advances in AI-powered 
data processing and analysis now make it possible to infer patterns in a 
person’s behavior and thinking from even a small amount of information, making 
the role of data privacy even more imperative as a safeguard for the dignity and 
relational nature of the human person. As Pope Francis observed, “while closed 
and intolerant attitudes towards others are on the rise, distances are otherwise 
shrinking or disappearing to the point that the right to privacy scarcely 
exists. Everything has become a kind of spectacle to be examined and inspected, 
and people’s lives are now under constant surveillance.”<a name="_ftnref170" title="" href="#_ftn170">[170]</a></p>
<p>93. While there can be 
legitimate and proper ways to use AI in keeping with human dignity and the 
common good, using it for surveillance aimed at exploiting, restricting others’ 
freedom, or benefitting a few at the expense of the many is unjustifiable. The 
risk of surveillance overreach must be monitored by appropriate regulators to 
ensure transparency and public accountability. Those responsible for 
surveillance should never exceed their authority, which must always favor the 
dignity and freedom of every person as the essential basis of a just and humane 
society.</p>
<p>94. Furthermore, 
“fundamental respect for human dignity demands that we refuse to allow the 
uniqueness of the person to be identified with a set of data.”<a name="_ftnref171" title="" href="#_ftn171">[171]</a> 
This especially applies when AI is used to evaluate individuals or groups based 
on their behavior, characteristics, or history—a practice known as “social 
scoring”: “In social and economic decision-making, we should be cautious about 
delegating judgments to algorithms that process data, often collected 
surreptitiously, on an individual’s makeup and prior behavior. Such data can be 
contaminated by societal prejudices and preconceptions. A person’s past behavior 
should not be used to deny him or her the opportunity to change, grow, and 
contribute to society. We cannot allow algorithms to limit or condition respect 
for human dignity, or to exclude compassion, mercy, forgiveness, and above all, 
the hope that people are able to change.”<a name="_ftnref172" title="" href="#_ftn172">[172]</a> 
</p>
<p><i><a name="AI_and_the_Protection">AI and the Protection</a> of Our Common Home</i></p>
<p>95. AI has many promising 
applications for improving our relationship with our “common home,” such as 
creating models to forecast extreme climate events, proposing engineering 
solutions to reduce their impact, managing relief operations, and predicting 
population shifts.<a name="_ftnref173" title="" href="#_ftn173">[173]</a> 
Additionally, AI can support sustainable agriculture, optimize energy usage, and 
provide early warning systems for public health emergencies. These advancements 
have the potential to strengthen resilience against climate-related challenges 
and promote more sustainable development.</p>
<p>96. At the same time, 
current AI models and the hardware required to support them consume vast amounts 
of energy and water, significantly contributing to CO<sub>2</sub> emissions and 
straining resources. This reality is often obscured by the way this technology 
is presented in the popular imagination, where words such as “the cloud”<a name="_ftnref174" title="" href="#_ftn174"> [174]</a>&nbsp;can give the impression that data is stored and processed in an intangible 
realm, detached from the physical world. However, “the cloud” is not an ethereal 
domain separate from the physical world; as with all computing technologies, it 
relies on physical machines, cables, and energy. The same is true of the 
technology behind AI. As these systems grow in 
complexity, especially large language models (LLMs), they require ever-larger 
datasets, increased computational power, and greater storage infrastructure. 
Considering the heavy toll these technologies take on the environment, it is 
vital to develop sustainable solutions that reduce their impact on our common 
home.</p>
<p>97. &nbsp;Even then, as Pope 
Francis teaches, it is essential “that we look for solutions not only in 
technology but in a change of humanity.”<a name="_ftnref175" title="" href="#_ftn175">[175]</a> 
A complete and authentic understanding of creation recognizes that the value of 
all created things cannot be reduced to their mere utility. Therefore, a fully 
human approach to the stewardship of the earth rejects the distorted 
anthropocentrism of the technocratic paradigm, which seeks to “extract 
everything possible” from the world,<a name="_ftnref176" title="" href="#_ftn176"> [176]</a> 
and rejects the “myth of progress,” which assumes that “ecological problems will 
solve themselves simply with the application of new technology and without any 
need for ethical considerations or deep change.”<a name="_ftnref177" title="" href="#_ftn177">[177]</a> 
Such a mindset must give way to a more holistic approach that respects the order 
of creation and promotes the integral good of the human person while 
safeguarding our common home.<a name="_ftnref178" title="" href="#_ftn178"> [178]</a> 
</p>
<p><i><a name="AI_and_Warfare">AI and Warfare</a></i></p>
<p>98. The Second Vatican 
Council and the consistent teaching of the Popes since then have insisted that 
peace is not merely the absence of war and is not limited to maintaining a 
balance of powers between adversaries. Instead, in the words of Saint Augustine, 
peace is “the tranquility of order.”<a name="_ftnref179" title="" href="#_ftn179"> [179]</a>&nbsp;Indeed, peace cannot be attained without safeguarding the goods of 
persons, free communication, respect for the dignity of persons and peoples, and 
the assiduous practice of fraternity. Peace is the work of justice and the 
effect of charity and cannot be achieved through force alone; instead, it must 
be principally built through patient diplomacy, the active promotion of justice, 
solidarity, integral human development, and respect for the dignity of all 
people.<a name="_ftnref180" title="" href="#_ftn180">[180]</a>&nbsp;In this way, the tools used to maintain peace should never be allowed to 
justify injustice, violence, or oppression. Instead, they should always be 
governed by a “firm determination to respect other people and nations, along 
with their dignity, as well as the deliberate practice of fraternity.”<a name="_ftnref181" title="" href="#_ftn181">[181]</a></p>
<p>99. While AI’s analytical 
abilities could help nations seek peace and ensure security, the “weaponization 
of Artificial Intelligence” can also be highly problematic. Pope Francis has 
observed that “the ability to conduct military operations through remote control 
systems has led to a lessened perception of the devastation caused by those 
weapon systems and the burden of responsibility for their use, resulting in an 
even more cold and detached approach to the immense tragedy of war.”<a name="_ftnref182" title="" href="#_ftn182">[182]</a>&nbsp;Moreover, the ease with which autonomous weapons make war more viable 
militates against the principle of war as a last resort in legitimate 
self-defense,<a name="_ftnref183" title="" href="#_ftn183">[183]</a>&nbsp;potentially increasing the instruments of war well beyond the scope of 
human oversight and precipitating a destabilizing arms race, with catastrophic 
consequences for human rights.<a name="_ftnref184" title="" href="#_ftn184">[184]</a></p>
<p>100. In particular, Lethal Autonomous Weapon 
Systems, which are capable of identifying and striking targets without direct 
human intervention, are a “cause for grave ethical concern” because they lack 
the “unique human capacity for moral judgment and ethical decision-making.”<a name="_ftnref185" title="" href="#_ftn185">[185]</a>&nbsp;For this reason, Pope Francis has urgently called for a reconsideration of 
the development of these weapons and a prohibition on their use, starting with 
“an effective and concrete commitment to introduce ever greater and proper human 
control. No machine should ever choose to take the life of a human being.”<a name="_ftnref186" title="" href="#_ftn186">[186]</a></p>
<p>101. Since it is a small step from machines that 
can kill autonomously with precision to those capable of large-scale 
destruction, some AI researchers have expressed concerns that such technology 
poses an “existential risk” by having the potential to act in ways that could 
threaten the survival of entire regions or even of humanity itself. This danger 
demands serious attention, reflecting the long-standing concern about 
technologies that grant war “an uncontrollable destructive power over great 
numbers of innocent civilians,”<a name="_ftnref187" title="" href="#_ftn187">[187]</a>&nbsp;without even sparing children. In this context, the call from <i>
<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a></i> to “undertake an evaluation of war with an entirely new attitude”<a name="_ftnref188" title="" href="#_ftn188">[188]</a>&nbsp;is more urgent than ever.</p>
<p>102. At the same time, while the theoretical risks 
of AI deserve attention, the more immediate and pressing concern lies in how 
individuals with malicious intentions might misuse this technology.<a name="_ftnref189" title="" href="#_ftn189">[189]</a>&nbsp;Like any tool, AI is an extension of human power, and while its future 
capabilities are unpredictable, humanity’s past actions provide clear warnings. 
The atrocities committed throughout history are enough to raise deep concerns 
about the potential abuses of AI.</p>
<p>103. Saint John Paul II observed that “humanity 
now has instruments of unprecedented power: we can turn this world into a 
garden, or reduce it to a pile of rubble.”<a name="_ftnref190" title="" href="#_ftn190">[190]</a>&nbsp;Given this fact, the Church reminds us, in the words of Pope Francis, that 
“we are free to apply our intelligence towards things evolving positively,” or 
toward “decadence and mutual destruction.”<a name="_ftnref191" title="" href="#_ftn191">[191]</a>&nbsp;To prevent humanity from spiraling into self-destruction,<a name="_ftnref192" title="" href="#_ftn192">[192]</a>&nbsp;there must be a clear stand against all applications of technology that 
inherently threaten human life and dignity. This commitment requires careful 
discernment about the use of AI, particularly in military defense applications, 
to ensure that it always respects human dignity and serves the common good. The 
development and deployment of AI in armaments should be subject to the highest 
levels of ethical scrutiny, governed by a concern for human dignity and the 
sanctity of life.<a name="_ftnref193" title="" href="#_ftn193">[193]</a></p>
<p><i><a name="AI_and_Our">AI and Our</a> Relationship with God</i></p>
<p>104. Technology offers remarkable tools to oversee 
and develop the world’s resources. However, in some cases, humanity is increasingly ceding control of these 
resources to machines. Within some circles of scientists and futurists, there is 
optimism about the potential of artificial general intelligence (AGI), a 
hypothetical form of AI that would match or surpass human intelligence and bring 
about unimaginable advancements. Some even speculate that AGI could achieve 
superhuman capabilities. At the same time, as society drifts away from a 
connection with the transcendent, some are tempted to turn to AI in search of 
meaning or fulfillment—longings that can only be truly satisfied in communion 
with God.<a name="_ftnref194" title="" href="#_ftn194">[194]</a></p>
<p>105. However, <i>the presumption of substituting 
God for an artifact of human making is idolatry</i>, a practice Scripture 
explicitly warns against (e.g., Ex. 20:4; 32:1-5; 34:17). Moreover, AI may prove 
even more seductive than traditional idols for, unlike idols that “have mouths 
but do not speak; eyes, but do not see; ears, but do not hear” (Ps. 115:5-6), AI 
can “speak,” or at least gives the illusion of doing so (cf. Rev. 13:15). Yet, 
it is vital to remember that AI is but a pale reflection of humanity—it is 
crafted by human minds, trained on human-generated material, responsive to human 
input, and sustained through human labor. AI cannot possess many of the 
capabilities specific to human life, and it is also fallible. By turning to AI 
as a perceived “Other” greater than itself, with which to share existence and 
responsibilities, humanity risks creating a substitute for God. However, it is 
not AI that is ultimately deified and worshipped, but humanity itself—which, in 
this way, becomes enslaved to its own work.<a name="_ftnref195" title="" href="#_ftn195">[195]</a> 
</p>
<p>106. While AI has the potential to serve humanity 
and contribute to the common good, it remains a creation of human hands, bearing 
“the imprint of human art and ingenuity” (Acts 17:29). It must never be ascribed 
undue worth. As the Book of Wisdom affirms: “For a man made them, and one whose 
spirit is borrowed formed them; for no man can form a god which is like himself. 
He is mortal, and what he makes with lawless hands is dead, for he is better 
than the objects he worships since he has life, but they never have” (Wis. 
15:16-17). </p>
<p>107. In contrast, human beings, “by their interior 
life, transcend the entire material universe; they experience this deep 
interiority when they enter into their own heart, where God, who probes the 
heart, awaits them, and where they decide their own destiny in the sight of 
God.”<a name="_ftnref196" title="" href="#_ftn196">[196]</a> It is within the 
heart, as Pope Francis reminds us, that each individual discovers the 
“mysterious connection between self-knowledge and openness to others, between 
the encounter with one’s personal uniqueness and the willingness to give oneself 
to others.”<a name="_ftnref197" title="" href="#_ftn197">[197]</a> Therefore, it is 
the heart alone that is “capable of setting our other powers and passions, and 
our entire person, in a stance of reverence and loving obedience before the 
Lord,”<a name="_ftnref198" title="" href="#_ftn198">[198]</a> who “offers to treat 
each one of us as a ‘Thou,’ always and forever.”<a name="_ftnref199" title="" href="#_ftn199">[199]</a> 
</p>
<p>VI. <b> <a name="Concluding_Reflections">Concluding Reflections</a></b></p>
<p>108. Considering the various challenges posed by 
advances in technology, Pope Francis emphasized the need for growth in “human 
responsibility, values, and conscience,” proportionate to the growth in the 
potential that this technology brings<a name="_ftnref200" title="" href="#_ftn200">[200]</a>—recognizing that “with an increase in human power comes a broadening of 
responsibility on the part of individuals and communities.”<a name="_ftnref201" title="" href="#_ftn201">[201]</a></p>
<p>109. At the same time, the “essential and 
fundamental question” remains “whether in the context of this progress man, as 
man, is becoming truly better, that is to say, more mature spiritually, more 
aware of the dignity of his humanity, more responsible, more open to others, 
especially the neediest and the weakest, and readier to give and to aid all.”<a name="_ftnref202" title="" href="#_ftn202">[202]</a></p>
<p>110. As a result, it is crucial to know how to 
evaluate individual applications of AI in particular contexts to determine 
whether its use promotes human dignity, the vocation of the human person, and 
the common good. As with many technologies, the effects of the various uses of 
AI may not always be predictable from their inception. As these applications and 
their social impacts become clearer, appropriate responses should be made at all 
levels of society, following the principle of subsidiarity. Individual users, 
families, civil society, corporations, institutions, governments, and 
international organizations should work at their proper levels to ensure that AI 
is used for the good of all. </p>
<p>111. A significant challenge and opportunity for 
the common good today lies in considering AI within a framework of relational 
intelligence, which emphasizes the interconnectedness of individuals and 
communities and highlights our shared responsibility for fostering the integral 
well-being of others. The twentieth-century philosopher Nicholas Berdyaev 
observed that people often blame machines for personal and social problems; 
however, “this only humiliates man and does not correspond to his dignity,” for 
“it is unworthy to transfer responsibility from man to a machine.”<a name="_ftnref203" title="" href="#_ftn203">[203]</a> 
Only the human person can be morally responsible, and the challenges of a 
technological society are ultimately <i>spiritual</i> in nature. Therefore, 
facing those challenges “demands an intensification of spirituality.”<a name="_ftnref204" title="" href="#_ftn204">[204]</a></p>
<p>112. A further point to consider is the call, 
prompted by the appearance of AI on the world stage, for a <i>renewed 
appreciation of all that is human</i>. Years ago, the 
French Catholic author Georges Bernanos warned that “the danger is not in the 
multiplication of machines, but in the ever-increasing number of men accustomed 
from their childhood to desire only what machines can give.”<a name="_ftnref205" title="" href="#_ftn205">[205]</a> 
This challenge is as true today as it was then, as the rapid pace of 
digitization risks a “digital reductionism,” where non-quantifiable aspects of 
life are set aside and then forgotten or even deemed irrelevant because they 
cannot be computed in formal terms. AI should be used only as a tool to 
complement human intelligence rather than replace its richness.<a name="_ftnref206" title="" href="#_ftn206">[206]</a> Cultivating those aspects of human life that transcend computation is 
crucial for preserving “an authentic humanity” that “seems to dwell in the midst 
of our technological culture, almost unnoticed, like a mist seeping gently 
beneath a closed door.”<a name="_ftnref207" title="" href="#_ftn207">[207]</a></p>
<p><i><a name="True_Wisdom">True Wisdom</a></i></p>
<p>113. The vast expanse of the world’s knowledge is 
now accessible in ways that would have filled past generations with awe. 
However, to ensure that advancements in knowledge do not become humanly or 
spiritually barren, one must go beyond the mere accumulation of data and strive 
to achieve true wisdom.<a name="_ftnref208" title="" href="#_ftn208">[208]</a></p>
<p>114. This wisdom is the gift that humanity needs 
most to address the profound questions and ethical challenges posed by AI: “Only 
by adopting a spiritual way of viewing reality, only by recovering a wisdom of 
the heart, can we confront and interpret the newness of our time.”<a name="_ftnref209" title="" href="#_ftn209">[209]</a>&nbsp;Such “wisdom of the heart” is “the virtue that enables us to integrate the 
whole and its parts, our decisions and their consequences.” It “cannot be sought 
from machines,” but it “lets itself be found by those who seek it and be seen by 
those who love it; it anticipates those who desire it, and it goes in search of 
those who are worthy of it (cf. Wis 6:12-16).”<a name="_ftnref210" title="" href="#_ftn210">[210]</a></p>
<p>115. In a world marked by AI, we need the grace of 
the Holy Spirit, who “enables us to look at things with God’s eyes, to see 
connections, situations, events and to uncover their real meaning.”<a name="_ftnref211" title="" href="#_ftn211">[211]</a></p>
<p>116. Since a “person’s perfection is measured not 
by the information or knowledge they possess, but by the depth of their 
charity,”<a name="_ftnref212" title="" href="#_ftn212">[212]</a>&nbsp;how we incorporate AI “to include the least of our brothers and sisters, 
the vulnerable, and those most in need, will be the true measure of our 
humanity.”<a name="_ftnref213" title="" href="#_ftn213">[213]</a>&nbsp;The “wisdom of the heart” can illuminate and guide the human-centered use 
of this technology to help promote the common good, care for our “common home,” 
advance the search for the truth, foster integral human development, favor human 
solidarity and fraternity, and lead humanity to its ultimate goal: happiness and 
full communion with God.<a name="_ftnref214" title="" href="#_ftn214">[214]</a></p>
<p>117. From this perspective of wisdom, believers 
will be able to act as moral agents capable of using this technology to promote 
an authentic vision of the human person and society.<a name="_ftnref215" title="" href="#_ftn215">[215]</a>&nbsp;This should be done with the understanding that technological progress is 
part of God’s plan for creation—an activity that we are called to order toward 
the Paschal Mystery of Jesus Christ, in the continual search for the True and 
the Good.</p>
<p><i>The Supreme Pontiff, Francis, at the Audience granted on 14 January 2025 to the 
undersigned Prefects and Secretaries of the Dicastery for the Doctrine of the 
Faith and the Dicastery for Culture and Education, approved this </i>Note<i> and ordered its publication.</i></p>
<p>Given in Rome, at the offices of the Dicastery for the Doctrine of the Faith and 
the Dicastery for Culture and Education, on 28 January 2025, the Liturgical 
Memorial of Saint Thomas Aquinas, Doctor of the Church.</p>

<div>
	<table>
		<tbody><tr>
			<td>Víctor Manuel Card. Fernández <br>
			Prefect</td>
			<td>José Card. Tolentino de Mendonça<br>
			Prefect</td>
		</tr>
		<tr>
			<td>&nbsp;</td>
			<td>&nbsp;</td>
		</tr>
		<tr>
			<td>Msgr. Armando Matteo<br>
			Secretary, Doctrinal Section</td>
			<td>Most Rev. Paul Tighe<br>
			Secretary, Culture Section </td>
		</tr>
	</tbody></table>
</div>

<p><i>Ex audientia die 14 ianuarii 2025<br>
Franciscus</i><br clear="all">
			</p>
			
<p><b>Contents</b></p>
<p><b>I. <a href="#Introduction">Introduction</a></b></p>
<p><b>II. <a href="#What_is_Artificial_Intelligence">What is Artificial Intelligence?</a></b></p>
<p><b>III. <a href="#Intelligence_in_the_Philosophical">Intelligence in the Philosophical and Theological Tradition</a></b></p>
<blockquote>
	<p><a href="#Rationality">Rationality</a></p>
	<p><a href="#Embodiment">Embodiment</a></p>
	<p><a href="#Relationality">Relationality</a></p>
	<p><a href="#Relationship_with_the_Truth">Relationship with the Truth</a></p>
	<p><a href="#Stewardship_of_the_World">Stewardship of the World</a></p>
	<p><a href="#An_Integral">An Integral Understanding of Human Intelligence</a></p>
	<p><a href="#The_Limits_of_AI">The Limits of AI</a></p>
</blockquote>
<p><b>IV. <a href="#The_Role_of_Ethics">The Role of Ethics in Guiding the Development and Use of AI</a></b></p>
<blockquote>
	<p><a href="#Helping_Human">Helping Human Freedom and Decision-Making</a></p>
</blockquote>
<p><b>V. <a href="#Specific_Questions">Specific Questions</a></b></p>
<blockquote>
	<p><a href="#AI_and_Society">AI and Society</a></p>
	<p><a href="#AI_and_Human_Relationships">AI and Human Relationships</a></p>
	<p><a href="#AI,_the_Economy,_and_Labor">AI, the Economy, and Labor</a></p>
	<p><a href="#AI_and_Healthcare">AI and Healthcare</a></p>
	<p><a href="#AI_and_Education">AI and Education</a></p>
	<p><a href="#AI,_Misinformation">AI, Misinformation, Deepfakes, and Abuse</a></p>
	<p><a href="#AI,_Privacy">AI, Privacy, and Surveillance</a></p>
	<p><a href="#AI_and_the_Protection">AI and the Protection of Our Common Home</a></p>
	<p><a href="#AI_and_Warfare">AI and Warfare</a></p>
	<p><a href="#AI_and_Our">AI and Our Relationship with God</a></p>
</blockquote>
<p><b>VI. <a href="#Concluding_Reflections">Concluding Reflections</a></b></p>
<blockquote>
	<p><a href="#True_Wisdom">True Wisdom</a></p>
</blockquote>
<div>
	&nbsp;<hr size="1">
	<p><a name="_ftn1" title="" href="#_ftnref1">[1]</a>&nbsp;<i>Catechism of the Catholic Church</i>, par. 378. See also Second Vatican Ecumenical Council, 
Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 34: <i>AAS</i> 
58 (1966), 1052-1053.</p>
	
	
	<p><a name="_ftn4" title="" href="#_ftnref4">[4]</a>&nbsp;Cf. <i>Catechism of the Catholic Church</i>, par. 2293; Second Vatican Ecumenical Council, 
Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 35: <i>AAS
</i>58 (1966), 1053.</p>
	<p><a name="_ftn5" title="" href="#_ftnref5">[5]</a>&nbsp;J. McCarthy, <i>et al</i>., “A Proposal for the Dartmouth Summer Research 
Project on Artificial Intelligence” (31 August 1955), 
http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html (accessed: 21 October 2024).</p>
	
	<p><a name="_ftn7" title="" href="#_ftnref7">[7]</a>&nbsp;Terms in this document describing the outputs or processes of AI are used 
figuratively to explain its operations and are not intended to anthropomorphize 
the machine.</p>
	
	<p><a name="_ftn9" title="" href="#_ftnref9">[9]</a>&nbsp;Here, one can see the primary positions of the “transhumanists” and the 
“posthumanists.” <i>Transhumanists</i> argue that technological advancements 
will enable humans to overcome their biological limitations and enhance both 
their physical and cognitive abilities. <i>Posthumanists</i>, on the other hand, 
contend that such advances will ultimately alter human identity to the extent 
that humanity itself may no longer be considered truly “human.” Both views rest 
on a fundamentally negative perception of human corporality, which treats the 
body more as an obstacle than as an integral part of the person’s identity and 
call to full realization. Yet, this negative view of the body is inconsistent 
with a proper understanding of human dignity. While the Church supports genuine 
scientific progress, it affirms that human dignity is rooted in “the person as 
an inseparable unity of body and soul.” Thus, “dignity is also inherent in each 
person’s body, which participates in its own way in being <i>in</i> <i>imago Dei</i>” 
(Dicastery for the Doctrine of the Faith, Declaration <i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20240402_dignitas-infinita_en.html">Dignitas 
Infinita</a> </i>[8 April 2024], par. 18).</p>
	<p><a name="_ftn10" title="" href="#_ftnref10">[10]</a>&nbsp;This approach reflects a functionalist perspective, which reduces the 
human mind to its functions and assumes that its functions can be entirely 
quantified in physical or mathematical terms. However, even if a future AGI were 
to appear truly intelligent, it would still remain functional in nature.</p>
	<p><a name="_ftn11" title="" href="#_ftnref11">[11]</a>&nbsp;Cf. A.M. Turing, “Computing Machinery and Intelligence,” <i>Mind </i>59 
(1950) 443-460.</p>
	<p><a name="_ftn12" title="" href="#_ftnref12">[12]</a>&nbsp;If “thinking” is attributed to machines, it must 
be clarified that this refers to calculative thinking rather than critical 
thinking. Similarly, if machines are said to operate using logical thinking, it 
must be specified that this is limited to computational logic. On the other 
hand, by its very nature, human thought is a creative process that eludes 
programming and transcends constraints.</p>
	<p><a name="_ftn13" title="" href="#_ftnref13">[13]</a>&nbsp;On the foundational role of language in shaping understanding, cf. M. Heidegger,
<i>Über den Humanismus</i>, Klostermann, Frankfurt am Main 1949 (en. tr. “Letter 
on Humanism,” in <i>Basic Writings: Martin Heidegger</i>, Routledge, London ‒ 
New York 2010, 141-182). </p>
	<p><a name="_ftn14" title="" href="#_ftnref14">[14]</a>&nbsp;For further discussion of these anthropological and theological 
foundations, see AI Research Group of the Centre for Digital Culture of the Dicastery for Culture 
and Education, <i>Encountering Artificial Intelligence: Ethical and 
Anthropological Investigations</i>&nbsp;(Theological Investigations of Artificial 
Intelligence 1), M.J. Gaudet, N. Herzfeld, P. Scherz, J.J. Wales, eds., <i>
Journal of Moral Theology</i>, Pickwick, Eugene 2024, 43-144.</p>
	<p><a name="_ftn15" title="" href="#_ftnref15">[15]</a>&nbsp;Aristotle, <i>Metaphysics</i>, I.1, 980 a 21.</p>
	<p><a name="_ftn16" title="" href="#_ftnref16">[16]</a> Cf. Augustine, <i>De Genesi ad litteram </i>III, 20, 30: PL 34, 292: “Man is 
made in the image of God in relation to that [faculty] by which he is superior 
to the irrational animals. Now, this [faculty] is reason itself, or the ‘mind,’ 
or ‘intelligence,’ whatever other name it may more suitably be given”;<b> </b>Id.,
<i>Enarrationes in Psalmos </i>54, 3: PL 36, 629: “When considering all that 
they have, humans discover that they are most distinguished from animals 
precisely by the fact they possess intelligence.” This is also reiterated by 
Saint Thomas Aquinas, who states that “man is the most perfect of all earthly 
beings endowed with motion, and his proper and natural operation is 
intellection,” by which man abstracts from things and “receives in his mind 
things actually intelligible” (Thomas Aquinas, <i>Summa Contra Gentiles </i>II, 
76).</p>
	<p><a name="_ftn17" title="" href="#_ftnref17">[17]</a>&nbsp;Cf. Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 15: <i>AAS </i>58 (1966), 1036.</p>
	<p><a name="_ftn18" title="" href="#_ftnref18">[18]</a>&nbsp;Aquinas, <i>Summa Theologiae</i>, II-II, q. 49, a. 5, ad 3. Cf. <i>ibid</i>., 
I, q. 79; II-II, q. 47, a. 3; II-II, q. 49, a. 2. For a contemporary perspective 
that echoes elements of the classical and medieval distinction between these two 
modes of cognition, cf. D. Kahneman, <i>Thinking, Fast and Slow</i>, New York<i>
</i>2011. </p>
	<p><a name="_ftn19" title="" href="#_ftnref19">[19]</a>&nbsp;Aquinas, <i>Summa Theologiae</i>, I, q. 76, a. 1, <i>resp</i>. 
	</p>
	<p><a name="_ftn20" title="" href="#_ftnref20">[20]</a>&nbsp;Cf. Irenaeus of Lyon, <i>Adversus Haereses</i>,<i> </i>V, 6, 1: PG 7(2), 
1136-1138.</p>
	<p><a name="_ftn21" title="" href="#_ftnref21">[21]</a>&nbsp;Dicastery for the Doctrine of the Faith, Declaration <i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20240402_dignitas-infinita_en.html">Dignitas 
Infinita</a> </i>(8 April 2024), par. 9. Cf. Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20201003_enciclica-fratelli-tutti.html">Fratelli Tutti</a></i> 
(3 October 2020), par. 213: <i>AAS </i>112 (2020), 1045: “The intellect can 
investigate the reality of things through reflection, experience and dialogue, 
and come to recognize in that reality, which transcends it, the basis of certain 
universal moral demands.”</p>
	
	<p><a name="_ftn23" title="" href="#_ftnref23">[23]</a>&nbsp;<i>Catechism of the Catholic Church</i>, par. 365. Cf. Aquinas, <i>Summa 
Theologiae</i>, I, q. 75, a. 4, resp.</p>
	<p><a name="_ftn24" title="" href="#_ftnref24">[24]</a>&nbsp;Indeed, Sacred Scripture “generally considers the human person as a being 
who exists in the body and is unthinkable outside of it” (Pontifical Biblical Commission,
<i><a href="https://www.vatican.va/roman_curia/congregations/cfaith/pcb_documents/rc_con_cfaith_doc_20190930_cosa-e-luomo_sp.pdf">“Che cosa è l’uomo?” (Sal 8,5): Un itinerario di antropologia biblica</a> </i>[30 
September 2019], par. 19). Cf. <i>ibid</i>., pars. 20-21, 43-44, 48. </p>
	<p><a name="_ftn25" title="" href="#_ftnref25">[25]</a>&nbsp;Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 22: <i>AAS </i>58 (1966), 1042: Cf. Congregation for the Doctrine of the Faith, 
Instruction <i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_con_cfaith_doc_20081208_dignitas-personae_en.html">Dignitas Personae</a> </i>(8 September 2008), par. 7: <i>AAS </i>100 
(2008), 863: “Christ did not disdain human bodiliness, but instead fully 
disclosed its meaning and value.”</p>
	<p><a name="_ftn26" title="" href="#_ftnref26">[26]</a>&nbsp;Aquinas, <i>Summa Contra Gentiles</i> II, 81.</p>
	<p><a name="_ftn27" title="" href="#_ftnref27">[27]</a>&nbsp;Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 15: <i>AAS </i>58 (1966), 1036.</p>
	<p><a name="_ftn28" title="" href="#_ftnref28">[28]</a>&nbsp;Cf. Aquinas<i>, Summa Theologiae </i>I, q. 89, a. 1, <i>resp</i>.: “to be 
separated from the body is not in accordance with [the soul’s] nature […] and 
hence it is united to the body in order that it may have an existence and an 
operation suitable to its nature.”</p>
	<p><a name="_ftn29" title="" href="#_ftnref29">[29]</a>&nbsp;Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 14: <i>AAS </i>58 (1966), 1035. Cf. Dicastery for the Doctrine of the Faith, Declaration 
		<i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20240402_dignitas-infinita_en.html">Dignitas 
Infinita</a> </i>(8 April 2024), par. 18.</p>
	
	<p><a name="_ftn31" title="" href="#_ftnref31">[31]</a>&nbsp;Cf. Congregation for the Doctrine of the Faith, Instruction <i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_con_cfaith_doc_20081208_dignitas-personae_en.html">Dignitas Personae</a> </i>(8 September 2008), pars. 5, 8; Dicastery for the Doctrine of the Faith, Declaration 
		<i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20240402_dignitas-infinita_en.html">Dignitas 
Infinita</a> </i>(8 April 2024), pars. 15, 24, 53-54.</p>
	<p><a name="_ftn32" title="" href="#_ftnref32">[32]</a>&nbsp;<i>Catechism of the Catholic Church</i>, par. 356. Cf. <i>ibid.</i>, par. 
221.</p>
	<p><a name="_ftn33" title="" href="#_ftnref33">[33]</a>&nbsp;Cf. Dicastery for the Doctrine of the Faith, Declaration <i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20240402_dignitas-infinita_en.html">Dignitas 
Infinita</a> </i>(8 April 2024), pars. 13, 26-27.</p>
	<p><a name="_ftn34" title="" href="#_ftnref34">[34]</a>&nbsp;Congregation for the Doctrine of the Faith, Instruction <i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_con_cfaith_doc_19900524_theologian-vocation_en.html">Donum Veritatis</a></i> 
(24 May 1990), 6: <i>AAS</i> 82 (1990), 1552. Cf. John Paul II, Encyclical <i>
		<a href="https://www.vatican.va/content/john-paul-ii/en/encyclicals/documents/hf_jp-ii_enc_06081993_veritatis-splendor.html">Veritatis Splendor</a> </i>(6 August 1993), par. 109: <i>AAS </i>85 (1993), 1219. 
Cf. Pseudo-Dionysius, <i>De divinis nominibus, </i>VII, 2: PG 3, 868B-C: “Human 
souls also possess reason and with it they circle in discourse around the truth 
of things. […] [O]n account of the manner in which they are capable of 
concentrating the many into the one, they too, in their own fashion and as far 
as they can, are worthy of conceptions like those of the angels” (en. tr. <i>
Pseudo-Dionysius: The Complete Works</i>, Paulist Press, New York – Mahwah 1987, 
106-107).</p>
	<p><a name="_ftn35" title="" href="#_ftnref35">[35]</a>&nbsp;John Paul II, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/john-paul-ii/en/encyclicals/documents/hf_jp-ii_enc_14091998_fides-et-ratio.html">Fides et Ratio</a></i> (14 September 1998), 
par. 3: <i>AAS </i>91 (1999), 7.</p>
	<p><a name="_ftn36" title="" href="#_ftnref36">[36]</a>&nbsp;Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 15: <i>AAS </i>58 (1966), 1036.</p>
	<p><a name="_ftn37" title="" href="#_ftnref37">[37]</a>&nbsp;John Paul II, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/john-paul-ii/en/encyclicals/documents/hf_jp-ii_enc_14091998_fides-et-ratio.html">Fides et Ratio</a></i> (14 September 1998), 
par. 42: <i>AAS </i>91 (1999), 38. Cf. Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20201003_enciclica-fratelli-tutti.html">Fratelli Tutti</a></i> (3 October 2020), par. 208: <i>AAS </i>112 (2020), 1043: “the human 
mind is capable of transcending immediate concerns and grasping certain truths 
that are unchanging, as true now as in the past. As it peers into human nature, 
reason discovers universal values derived from that same nature”; <i>ibid</i>., 
par. 184: <i>AAS </i>112 (2020), 1034.</p>
	<p><a name="_ftn38" title="" href="#_ftnref38">[38]</a>&nbsp;Cf. B. Pascal, <i>Pensées</i>, no. 267 (ed. Brunschvicg): “The last 
proceeding of reason is to recognize that there is an infinity of things which 
are beyond it” (en. tr. <i>Pascal’s Pensées</i>, E.P. Dutton, New York 1958, 
77).</p>
	<p><a name="_ftn39" title="" href="#_ftnref39">[39]</a>&nbsp;Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 15: <i>AAS </i>58 (1966), 1036. Cf. Congregation for the Doctrine of the Faith,
		<i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_con_cfaith_doc_20071203_nota-evangelizzazione_en.html">Doctrinal Note on Some 
Aspects of Evangelization</a></i> (3 December 2007), par. 
4: <i>AAS</i> 100 (2008), 491-492.</p>
	<p><a name="_ftn40" title="" href="#_ftnref40">[40]</a>&nbsp;Our <i>semantic capacity</i> allows us to understand messages in any form 
of communication in a manner that both takes into account and transcends their 
material or empirical structures (such as computer code). Here, intelligence 
becomes a wisdom that “enables us to look at things with God’s eyes, to see 
connections, situations, events and to uncover their real meaning” (Francis, <i>
		<a href="https://www.vatican.va/content/francesco/en/messages/communications/documents/20240124-messaggio-comunicazioni-sociali.html">Message for the LVIII World Day of Social Communications</a> </i> 
		[24 January 2024]:
<i>L’Osservatore Romano</i>, 24 January 2024, 8). Our <i>creativity</i> enables 
us to generate new content or ideas, primarily by offering an original viewpoint 
on reality. Both capacities depend on the existence of a personal subjectivity 
for their full realization.</p>
	<p><a name="_ftn41" title="" href="#_ftnref41">[41]</a>&nbsp;Second Vatican Ecumenical Council, Declaration <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_decl_19651207_dignitatis-humanae_en.html">Dignitatis Humanae</a></i>
(7 December 1965), par. 3: <i>AAS </i>58 (1966), 931. </p>
	<p><a name="_ftn42" title="" href="#_ftnref42">[42]</a>&nbsp;Cf. Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20201003_enciclica-fratelli-tutti.html">Fratelli Tutti</a></i> (3 October 2020), 
par. 184: <i>AAS</i> 112 (2020), 1034: “Charity, when accompanied by a 
commitment to the truth, is much more than personal feeling […]. Indeed, its 
close relation to truth fosters its universality and preserves it from being 
‘confined to a narrow field devoid of relationships.’ […] Charity’s openness to 
truth thus protects it from ‘a fideism that deprives it of its human and 
universal breadth.’” The internal quotes are from Benedict XVI, Encyclical 
Letter <i>
		<a href="https://www.vatican.va/content/benedict-xvi/en/encyclicals/documents/hf_ben-xvi_enc_20090629_caritas-in-veritate.html">Caritas in Veritate</a></i> (29 June 2009), pars. 2-4: <i>AAS</i> 101 
(2009), 642-643. </p>
	
	
	<p><a name="_ftn45" title="" href="#_ftnref45">[45]</a>&nbsp;John Paul II, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/john-paul-ii/en/encyclicals/documents/hf_jp-ii_enc_14091998_fides-et-ratio.html">Fides et Ratio</a></i> (14 September 1998), 
par. 13: <i>AAS</i> 91 (1999), 15.</p>
	<p><a name="_ftn46" title="" href="#_ftnref46">[46]</a>&nbsp;Bonaventure, <i>In II Librum Sententiarum</i>, d. I, p. 2, a. 2, q. 1; as 
quoted in <i>Catechism of the Catholic Church</i>, par. 293. Cf. <i>ibid.</i>, 
par. 294. </p>
	<p><a name="_ftn47" title="" href="#_ftnref47">[47]</a>&nbsp;Cf. <i>Catechism of the Catholic Church</i>, pars. 295, 299, 302. 
Bonaventure likens the universe to “a book reflecting, representing, and 
describing its Maker,” the Triune God who grants existence to all things (<i>Breviloquium</i> 
2.12.1). Cf. Alain de Lille, <i>De Incarnatione Christi</i>, PL 210, 579a: “<i>Omnis 
mundi creatura quasi liber et pictura nobis est et speculum.</i>”</p>
	<p><a name="_ftn48" title="" href="#_ftnref48">[48]</a>&nbsp;Cf. Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 67:
<i>AAS </i>107 (2015), 874; John Paul II, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/john-paul-ii/en/encyclicals/documents/hf_jp-ii_enc_14091981_laborem-exercens.html">Laborem Exercens</a> </i>(14 September 1981), par. 6: <i>AAS </i>73 (1981), 589-592; Second Vatican Ecumenical Council, 
Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), pars. 33-34: <i>
AAS </i>58 (1966), 1052-1053; International Theological Commission, <i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/cti_documents/rc_con_cfaith_doc_20040723_communion-stewardship_en.html">Communion and Stewardship: Human 
Persons Created in the Image of God</a> </i>(2004), 
par. 57: “human beings occupy a unique place in the universe according to the 
divine plan: they enjoy the privilege of sharing in the divine governance of 
visible creation. […] Since man’s place as ruler is in fact a participation in 
the divine governance of creation, we speak of it here as a form of 
stewardship.”</p>
	<p><a name="_ftn49" title="" href="#_ftnref49">[49]</a>&nbsp;Cf. John Paul II, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/john-paul-ii/en/encyclicals/documents/hf_jp-ii_enc_06081993_veritatis-splendor.html">Veritatis Splendor</a> </i>(6 August 
1993), pars. 38-39: <i>AAS </i>85 (1993), 1164-1165.</p>
	<p><a name="_ftn50" title="" href="#_ftnref50">[50]</a> Cf.&nbsp;Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), pars. 33-34: <i>AAS </i>58 (1966), 1052-1053. This 
idea is also reflected in the creation account, where God brings creatures to 
Adam “to see what he would call them. And whatever [he] called every living 
creature, that was its name” (Gen. 2:19), an action that demonstrates the active 
engagement of human intelligence in the stewardship of God’s creation. Cf. John Chrysostom,
<i>Homiliae in Genesim</i>, XIV, 17-21: PG 53, 116-117.</p>
	<p><a name="_ftn51" title="" href="#_ftnref51">[51]</a>&nbsp;Cf. <i>Catechism of the Catholic Church</i>, par. 301. 
	</p>
	<p><a name="_ftn52" title="" href="#_ftnref52">[52]</a>&nbsp;Cf. <i>Catechism of the Catholic Church</i>, par. 302.</p>
	<p><a name="_ftn53" title="" href="#_ftnref53">[53]</a>&nbsp;Bonaventure, <i>Breviloquium </i>2.12.1. Cf. <i>ibid</i>., 2.11.2.</p>
	
	
	
	<p><a name="_ftn57" title="" href="#_ftnref57">[57]</a>&nbsp;Cf. Francis, Post-Synodal Apostolic Exhortation <i>
		<a href="https://www.vatican.va/content/francesco/en/apost_exhortations/documents/papa-francesco_esortazione-ap_20200202_querida-amazonia.html">Querida Amazonia</a> </i>(2 
February 2020), par. 41: <i>AAS</i> 112 (2020), 246; Id., Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 146: <i>AAS </i>107 (2015), 906.</p>
	<p><a name="_ftn58" title="" href="#_ftnref58">[58]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 47: <i>
AAS </i>107 (2015), 864. Cf. Id., Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/20241024-enciclica-dilexit-nos.html">Dilexit Nos</a> </i>(24 
October 2024), pars. 17-24: <i>L’Osservatore Romano</i>, 24 October 2024, 5; Id., 
Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20201003_enciclica-fratelli-tutti.html">Fratelli Tutti</a> </i>(3 October 2020), par. 47-50: <i>AAS </i>
112 (2020), 985-987.</p>
	<p><a name="_ftn59" title="" href="#_ftnref59">[59]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/20241024-enciclica-dilexit-nos.html">Dilexit Nos</a></i> (24 October 2024), par. 20:
<i>L’Osservatore Romano</i>, 24 October 2024, 5.</p>
	<p><a name="_ftn60" title="" href="#_ftnref60">[60]</a>&nbsp;P. Claudel, <i>Conversation sur Jean Racine</i>, Gallimard, Paris 1956, 
32: “<i>L’intelligence n’est rien sans la délectation</i>.” Cf. Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/20241024-enciclica-dilexit-nos.html">Dilexit Nos</a> </i>(24 October 2024), par. 13:
<i>L’Osservatore Romano</i>, 24 October 2024, 5: “The mind and the will are put 
at the service of the greater good by sensing and savoring truths.”</p>
	<p><a name="_ftn61" title="" href="#_ftnref61">[61]</a>&nbsp;Dante, <i>Paradiso</i>, Canto XXX: “<i>luce intellettüal, piena d’amore; / 
amor di vero ben, pien di letizia; / letizia che trascende ogne dolzore</i>” 
(en. tr. <i>The Divine Comedy of Dante Alighieri</i>, C.E. Norton, tr., Houghton 
Mifflin, Boston 1920, 232).</p>
	<p><a name="_ftn62" title="" href="#_ftnref62">[62]</a>&nbsp;Cf. Second Vatican Ecumenical Council, Declaration <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_decl_19651207_dignitatis-humanae_en.html">Dignitatis Humanae</a></i>
(7 December 1965), par. 3: <i>AAS </i>58 (1966), 931: “[T]he highest norm of 
human life is the divine law itself—eternal, objective and universal, by which 
God orders, directs and governs the whole world and the ways of the human 
community according to a plan conceived in his wisdom and love. God has enabled 
man to participate in this law of his so that, under the gentle disposition of 
divine providence, many may be able to arrive at a deeper and deeper knowledge 
of unchangeable truth.” Also cf. Id., Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 16: <i>AAS </i>58 (1966), 1037.</p>
	<p><a name="_ftn63" title="" href="#_ftnref63">[63]</a>&nbsp;Cf. First Vatican Council, Dogmatic Constitution <i>
		<a href="https://www.vatican.va/content/pius-ix/la/documents/constitutio-dogmatica-dei-filius-24-aprilis-1870.html">Dei Filius</a></i> (24 
April 1870), ch. 4, DH 3016. </p>
	<p><a name="_ftn64" title="" href="#_ftnref64">[64]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 110: <i>
AAS </i>107 (2015), 892. </p>
	<p><a name="_ftn65" title="" href="#_ftnref65">[65]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 110: <i>
AAS </i>107 (2015), 891. Cf. Id., Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20201003_enciclica-fratelli-tutti.html">Fratelli Tutti</a></i> (3 
October 2020), par. 204: <i>AAS </i>112 (2020), 1042.</p>
	<p><a name="_ftn66" title="" href="#_ftnref66">[66]</a>&nbsp;Cf. John Paul II, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/john-paul-ii/en/encyclicals/documents/hf_jp-ii_enc_01051991_centesimus-annus.html">Centesimus Annus</a> </i>(1 May 1991), 
par. 11: <i>AAS </i>83 (1991), 807: “God has imprinted his own image and 
likeness on man (cf. Gen 1:26), conferring upon him an incomparable dignity […]. 
In effect, beyond the rights which man acquires by his own work, there exist 
rights which do not correspond to any work he performs, but which flow from his 
essential dignity as a person.” Cf. Francis, <i>
		<a href="https://www.vatican.va/content/francesco/en/speeches/2024/june/documents/20240614-g7-intelligenza-artificiale.html">Address at the G7 Session on 
Artificial Intelligence in Borgo Egnazia (Puglia)</a></i> (14 June 2024): <i>L’Osservatore Romano</i>, 14 June 2024, 3-4.</p>
	<p><a name="_ftn67" title="" href="#_ftnref67">[67]</a>&nbsp;Cf. Dicastery for the Doctrine of the Faith, Declaration <i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20240402_dignitas-infinita_en.html">Dignitas 
Infinita</a> </i>(8 April 2024), par. 8. Cf. <i>ibid., </i>par. 9; Congregation for the Doctrine of the Faith, 
Instruction <i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_con_cfaith_doc_20081208_dignitas-personae_en.html">Dignitas Personae</a> </i>(8 September 2008), par. 22.</p>
	
	
	<p><a name="_ftn70" title="" href="#_ftnref70">[70]</a>&nbsp;In this sense, “Artificial Intelligence” is understood as a technical term 
to indicate this technology, recalling that the expression is also used to 
designate the field of study and not only its applications.</p>
	<p><a name="_ftn71" title="" href="#_ftnref71">[71]</a>&nbsp;Cf. Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), pars. 34-35: <i>AAS </i>58 (1966), 1052-1053; John Paul II, 
Encyclical Letter <i>
		<a href="https://www.vatican.va/content/john-paul-ii/en/encyclicals/documents/hf_jp-ii_enc_01051991_centesimus-annus.html">Centesimus Annus</a> </i>(1 May 1991), par. 51: <i>AAS </i>83 
(1991), 856-857.</p>
	<p><a name="_ftn72" title="" href="#_ftnref72">[72]</a>&nbsp;For example, see the encouragement of scientific exploration in Albertus Magnus 
(<i>De Mineralibus</i>, II, 2, 1) and the appreciation for the mechanical arts 
in Hugh of St. Victor (<i>Didascalicon</i>, I, 9). These writers, among a long 
list of other Catholics engaged in scientific research and technological 
exploration, illustrate that “faith and science can be united in charity, 
provided that science is put at the service of the men and woman of our time and 
not misused to harm or even destroy them” (Francis, <i>A<a href="https://www.vatican.va/content/francesco/en/speeches/2024/june/documents/20240620-specola-vaticana.html">ddress to Participants 
in the 2024 Lemaître Conference of the Vatican Observatory</a> </i>[20 June 2024]:
<i>L’Osservatore Romano</i>,<i> </i>20 June 2024, 8). Cf. Second Vatican Ecumenical Council, 
Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 36: <i>AAS
</i>58 (1966), 1053-1054; John Paul II, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/john-paul-ii/en/encyclicals/documents/hf_jp-ii_enc_14091998_fides-et-ratio.html">Fides et Ratio</a></i>  
(14 September 1998), pars. 2, 106: <i>AAS </i>91 (1999), 6-7.86-87.</p>
	<p><a name="_ftn73" title="" href="#_ftnref73">[73]</a>&nbsp;<i>Catechism of the Catholic Church</i>, par. 378.</p>
	<p><a name="_ftn74" title="" href="#_ftnref74">[74]</a>&nbsp;Cf. Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 34: <i>AAS </i>58 (1966), 1053.</p>
	<p><a name="_ftn75" title="" href="#_ftnref75">[75]</a>&nbsp;Cf. Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 35: <i>AAS </i>58 (1966), 1053.</p>
	<p><a name="_ftn76" title="" href="#_ftnref76">[76]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 102: <i>
AAS </i>107 (2015), 888.</p>
	<p><a name="_ftn77" title="" href="#_ftnref77">[77]</a>&nbsp;Cf. Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a></i> (24 May 2015), par. 105:
<i>AAS </i>107 (2015), 889; Id., Encyclical <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20201003_enciclica-fratelli-tutti.html">Fratelli Tutti</a></i> (3 October 2020), par. 27: <i>AAS </i>112 (2020), 978; Benedict XVI, 
Encyclical <i>
		<a href="https://www.vatican.va/content/benedict-xvi/en/encyclicals/documents/hf_ben-xvi_enc_20090629_caritas-in-veritate.html">Caritas in Veritate</a></i> (29 June 2009), par. 23: <i>AAS </i>101 
(2009), 657-658.</p>
	<p><a name="_ftn78" title="" href="#_ftnref78">[78]</a>&nbsp;Cf. Dicastery for the Doctrine of the Faith, Declaration <i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20240402_dignitas-infinita_en.html">Dignitas 
Infinita</a> </i>(8 April 2024), pars. 38-39, 47; Congregation for the Doctrine of the Faith, 
Instruction <i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_con_cfaith_doc_20081208_dignitas-personae_en.html">Dignitas Personae</a> </i>(8 September 2008), <i>passim</i>.</p>
	<p><a name="_ftn79" title="" href="#_ftnref79">[79]</a>&nbsp;Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 35: <i>
AAS </i>58 (1966), 1053. Cf. <i>Catechism of the Catholic Church</i>, par 2293.</p>
	
	<p><a name="_ftn81" title="" href="#_ftnref81">[81]</a>&nbsp;Cf. <i>Catechism of the Catholic Church</i>, par. 1749: “Freedom makes man 
a moral subject. When he acts deliberately, man is, so to speak, the father of 
his acts.”</p>
	<p><a name="_ftn82" title="" href="#_ftnref82">[82]</a>&nbsp;Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 16: <i>AAS </i>58 (1966), 1037. Cf.<i> 
Catechism of the Catholic Church</i>, par. 1776.</p>
	<p><a name="_ftn83" title="" href="#_ftnref83">[83]</a>&nbsp;<i>Catechism of the Catholic Church</i>, par. 1777.</p>
	<p><a name="_ftn84" title="" href="#_ftnref84">[84]</a>&nbsp;Cf. <i>Catechism of the Catholic Church</i>, pars. 1779-1781; Francis, <i>
		<a href="https://www.vatican.va/content/francesco/en/speeches/2023/march/documents/20230327-minerva-dialogues.html">Address to the Participants in the “Minerva Dialogues”</a></i> (27 March 2023): <i>
AAS </i>115 (2023), 463, where the Holy Father encouraged efforts “to ensure 
that technology remains human-centered, ethically grounded and directed toward 
the good.”</p>
	<p><a name="_ftn85" title="" href="#_ftnref85">[85]</a>&nbsp;Cf. Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20201003_enciclica-fratelli-tutti.html">Fratelli Tutti</a> </i>(3 October 2020), 
par. 166: <i>AAS </i>112 (2020), 1026-1027; Id., <i>
		<a href="https://www.vatican.va/content/francesco/en/speeches/2024/september/documents/20240923-plenaria-accademia-scienze.html">Address to the Plenary 
Assembly of the Pontifical Academy of Sciences</a> </i>(23 September 2024): <i>L’Osservatore 
</i>Romano, 23 September 2024, 10. On the role of human agency in choosing a 
wider aim (<i>Ziel</i>) that then informs the particular purpose (<i>Zweck</i>) 
for which each technological application is created, cf. F. Dessauer, <i>Streit 
um die Technik</i>, Herder-Bücherei, Freiburg i. Br. 1959, 70-71.</p>
	<p><a name="_ftn86" title="" href="#_ftnref86">[86]</a>&nbsp;Cf. Francis, <i>
		<a href="https://www.vatican.va/content/francesco/en/speeches/2024/june/documents/20240614-g7-intelligenza-artificiale.html">Address at the G7 Session on 
Artificial Intelligence in Borgo Egnazia (Puglia)</a></i> (14 June 2024): <i>L’Osservatore Romano</i>, 14 June 2024, 4: 
“Technology is born for a purpose and, in its impact on human society, always 
represents a form of order in social relations and an arrangement of power, thus 
enabling certain people to perform specific actions while preventing others from 
performing different ones. In a more or less explicit way, this constitutive 
power-dimension of technology always includes the worldview of those who 
invented and developed it.”</p>
	
	
	
	
	<p><a name="_ftn91" title="" href="#_ftnref91">[91]</a>&nbsp;Cf. Francis, <i>
		<a href="https://www.vatican.va/content/francesco/en/speeches/2024/june/documents/20240614-g7-intelligenza-artificiale.html">Address at the G7 Session on 
Artificial Intelligence in Borgo Egnazia (Puglia)</a></i> (14 June 2024): <i>L’Osservatore Romano</i>, 14 June 2024, 2: 
“Faced with the marvels of machines, which seem to know how to choose 
independently, we should be very clear that decision-making […] must always be 
left to the human person. We would condemn humanity to a future without hope if 
we took away people’s ability to make decisions about themselves and their 
lives, by dooming them to depend on the choices of machines.”</p>
	
	<p><a name="_ftn93" title="" href="#_ftnref93">[93]</a>&nbsp;The term “bias” in this document refers to <i>algorithmic bias</i> 
(systematic and consistent errors in computer systems that may 
disproportionately prejudice certain groups in unintended ways) or <i>learning 
bias</i> (which will result in training on a biased data set) and not the “<i>bias 
vector</i>” in neural networks (which is a parameter used to adjust the output 
of “neurons” to adjust more accurately to the data).</p>
	<p><a name="_ftn94" title="" href="#_ftnref94">[94]</a>&nbsp;Cf. Francis, <i>
		<a href="https://www.vatican.va/content/francesco/en/speeches/2023/march/documents/20230327-minerva-dialogues.html">Address to the Participants in the “Minerva Dialogues”</a></i>  
(27 March 2023): <i>AAS </i>115 (2023), 464, where the Holy Father affirmed the 
growth in consensus “on the need for development processes to respect such 
values as inclusion, transparency, security, equity, privacy and reliability,” 
and also welcomed “the efforts of international organizations to regulate these 
technologies so that they promote genuine progress, contributing, that is, to a 
better world and an integrally higher quality of life.”</p>
	
	<p><a name="_ftn96" title="" href="#_ftnref96">[96]</a>&nbsp;Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965)<i>, </i>par. 
26: <i>AAS </i>58 (1966), 1046-1047.</p>
	
	<p><a name="_ftn98" title="" href="#_ftnref98">[98]</a>&nbsp;Cf. Francis, <i>
		<a href="https://www.vatican.va/content/francesco/en/messages/communications/documents/20240124-messaggio-comunicazioni-sociali.html">Message for the LVIII World Day of Social Communications</a></i> 
(24 January 2024): <i>L’Osservatore Romano</i>, 24 January 2024, 8. For further 
discussion of the ethical questions raised by AI from a Catholic perspective, 
see AI Research Group of the Centre for Digital Culture of the Dicastery for Culture 
and Education, <i>Encountering Artificial Intelligence: Ethical and 
Anthropological Investigations </i>(Theological Investigations of Artificial 
Intelligence 1), M.J. Gaudet, N. Herzfeld, P. Scherz, J.J. Wales, eds., <i>
Journal of Moral Theology</i>, Pickwick, Eugene 2024, 147-253.</p>
	<p><a name="_ftn99" title="" href="#_ftnref99">[99]</a>&nbsp;On the importance of dialogue in a pluralist society oriented toward a 
“robust and solid social ethics,” see Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20201003_enciclica-fratelli-tutti.html">Fratelli Tutti</a> </i>(3 October 2020), pars. 211-214: <i>AAS </i>112 (2020), 1044-1045. 
	</p>
	
	<p><a name="_ftn101" title="" href="#_ftnref101">[101]</a>&nbsp;Francis, <i>
		<a href="https://www.vatican.va/content/francesco/en/messages/peace/documents/20231208-messaggio-57giornatamondiale-pace2024.html">Message for the LVII World Day of 
Peace</a> </i>(1 January 2024), par. 6:<i>&nbsp;L’Osservatore Romano</i>,<i>&nbsp;</i>14 December 2023, 3. 
Cf. Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965)<i>, </i>par. 26: <i>AAS </i>58 (1966), 1046-1047.</p>
	<p><a name="_ftn102" title="" href="#_ftnref102">[102]</a> Cf. Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 112: <i>
AAS </i>107 (2015), 892-893. </p>
	
	<p><a name="_ftn104" title="" href="#_ftnref104">[104]</a> Cf.&nbsp;Pontifical Council for Social Communications, <i>
		<a href="https://www.vatican.va/roman_curia/pontifical_councils/pccs/documents/rc_pc_pccs_doc_20020228_ethics-internet_en.html">Ethics in Internet</a> </i>
(22 February 2002), par. 10. </p>
	
	<p><a name="_ftn106" title="" href="#_ftnref106">[106]</a> Cf. Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), pars. 105-114:
<i>AAS </i>107 (2015), 889-893; Id., Apostolic Exhortation <i>
		<a href="https://www.vatican.va/content/francesco/en/apost_exhortations/documents/20231004-laudate-deum.html">Laudate Deum</a> </i>(4 October 2023), pars. 20-33: <i>AAS</i> 115 (2023), 1047-1050. </p>
	<p><a name="_ftn107" title="" href="#_ftnref107">[107]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 105: <i>AAS</i> 107 
(2015), 889. Cf. Id., Apostolic Exhortation <i>
		<a href="https://www.vatican.va/content/francesco/en/apost_exhortations/documents/20231004-laudate-deum.html">Laudate Deum</a> </i>(4 October 
2023), pars. 20-21: <i>AAS </i>115<i> </i>(2023), 1047. </p>
	
	
	<p><a name="_ftn110" title="" href="#_ftnref110">[110]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 112: <i>AAS </i>107 
(2015), 892.</p>
	<p><a name="_ftn111" title="" href="#_ftnref111">[111]</a>&nbsp;Cf. Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20201003_enciclica-fratelli-tutti.html">Fratelli Tutti</a> </i>(3 October 2020), 
pars. 101, 103, 111, 115, 167:<i> AAS </i>112 (2020), 1004-1005, 1007-1009, 
1027.</p>
	<p><a name="_ftn112" title="" href="#_ftnref112">[112]</a>&nbsp;Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 26: <i>AAS </i>58 (1966), 1046-1047; cf. Leo XIII, 
Encyclical Letter <i>
		<a href="https://www.vatican.va/content/leo-xiii/en/encyclicals/documents/hf_l-xiii_enc_15051891_rerum-novarum.html">Rerum Novarum</a> </i>(15 May 1891), par. 35: <i>Acta Leonis 
XIII</i>,<i> </i>11 (1892), 123. </p>
	<p><a name="_ftn113" title="" href="#_ftnref113">[113]</a>&nbsp;Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 12: <i>AAS </i>58 (1966), 1034. </p>
	<p><a name="_ftn114" title="" href="#_ftnref114">[114]</a>&nbsp;Cf. Pontifical Council for Justice and Peace, <i>Compendium of the Social 
Doctrine of the Church</i> (2004), par. 149.</p>
	<p><a name="_ftn115" title="" href="#_ftnref115">[115]</a>&nbsp;Second Vatican Ecumenical Council, Declaration <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_decl_19651207_dignitatis-humanae_en.html">Dignitatis Humanae</a> </i>
(7 December 1965), par. 3: <i>AAS </i>58 (1966), 931. Cf. Francis, Encyclical 
Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20201003_enciclica-fratelli-tutti.html">Fratelli Tutti</a> </i>(3 October 2020), par. 50: <i>AAS </i>112 (2020), 
986-987.</p>
	<p><a name="_ftn116" title="" href="#_ftnref116">[116]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20201003_enciclica-fratelli-tutti.html">Fratelli Tutti</a> </i>(3 October 2020), par. 
50: <i>AAS </i>112 (2020), 986-987.</p>
	<p><a name="_ftn117" title="" href="#_ftnref117">[117]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 47: <i>AAS </i>107 
(2015), 865. Cf. Id., Post-Synodal Exhortation <i>
		<a href="https://www.vatican.va/content/francesco/en/apost_exhortations/documents/papa-francesco_esortazione-ap_20190325_christus-vivit.html">Christus Vivit</a></i> (25 March 
2019), pars. 88-89: <i>AAS </i>111 (2019), 413-414.</p>
	<p><a name="_ftn118" title="" href="#_ftnref118">[118]</a>&nbsp;Cf. Francis, Apostolic Exhortation <i>
		<a href="https://www.vatican.va/content/francesco/en/apost_exhortations/documents/papa-francesco_esortazione-ap_20131124_evangelii-gaudium.html#Yes_to_the_new_relationships_brought_by_Christ">Evangelii Gaudium</a></i> (24 November 
2013), par. 88: <i>AAS</i> 105 (2013), 1057.</p>
	<p><a name="_ftn119" title="" href="#_ftnref119">[119]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20201003_enciclica-fratelli-tutti.html">Fratelli Tutti</a> </i>(3 October 2020), par. 
47: <i>AAS </i>112 (2020), 985.</p>
	
	<p><a name="_ftn121" title="" href="#_ftnref121">[121]</a>&nbsp;Cf. Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20201003_enciclica-fratelli-tutti.html">Fratelli Tutti</a> </i>(3 October 2020), 
par. 50: <i>AAS </i>112 (2020), 986-987.</p>
	<p><a name="_ftn122" title="" href="#_ftnref122">[122]</a>&nbsp;Cf. E. Stein, 
		<i>Zum Problem der Einfühlung</i>, Buchdruckerei des Waisenhauses, Halle 1917 (en. tr. <i>On the Problem of Empathy</i>, ICS Publications, Washington D.C. 
1989).</p>
	<p><a name="_ftn123" title="" href="#_ftnref123">[123]</a>&nbsp;Cf. Francis, Apostolic Exhortation <i>
		<a href="https://www.vatican.va/content/francesco/en/apost_exhortations/documents/papa-francesco_esortazione-ap_20131124_evangelii-gaudium.html#Yes_to_the_new_relationships_brought_by_Christ">Evangelii Gaudium</a></i> (24 November 
2013), par. 88: <i>AAS</i> 105 (2013), 1057: “[Many people] want their 
interpersonal relationships provided by sophisticated equipment, by screens and 
systems which can be turned on and off on command. Meanwhile, the Gospel tells 
us constantly to run the risk of a face-to-face encounter with others, with 
their physical presence which challenges us, with their pain and their pleas, 
with their joy which infects us in our close and continuous interaction. True 
faith in the incarnate Son of God is inseparable from self-giving, from 
membership in the community, from service, from reconciliation with others.” 
Also cf. Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 24: <i>AAS </i>58 (1966), 1044-1045.</p>
	<p><a name="_ftn124" title="" href="#_ftnref124">[124]</a>&nbsp;Cf. Dicastery for the Doctrine of the Faith, Declaration <i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20240402_dignitas-infinita_en.html">Dignitas 
Infinita</a></i> (8 April 2024), 
par. 1.</p>
	
	
	<p><a name="_ftn127" title="" href="#_ftnref127">[127]</a>&nbsp;Francis, Apostolic Exhortation <i>
		<a href="https://www.vatican.va/content/francesco/en/apost_exhortations/documents/papa-francesco_esortazione-ap_20131124_evangelii-gaudium.html#Concern_for_the_vulnerable">Evangelii Gaudium</a> </i>(24 November 2013), par. 209:
<i>AAS </i>105 (2013), 1107.</p>
	
	<p><a name="_ftn129" title="" href="#_ftnref129">[129]</a>&nbsp;Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 26: <i>
AAS </i>58 (1966), 1046-1047.; as quoted in <i>Catechism of the Catholic Church</i>, 
par. 1912. Cf. John XXIII, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/john-xxiii/en/encyclicals/documents/hf_j-xxiii_enc_15051961_mater.html">Mater et Magistra</a></i> (15 May 
1961), par. 219: <i>AAS </i>53 (1961), 453. </p>
	<p><a name="_ftn130" title="" href="#_ftnref130">[130]</a>&nbsp;Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par 64: <i>AAS
</i>58 (1966), 1086.</p>
	<p><a name="_ftn131" title="" href="#_ftnref131">[131]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20201003_enciclica-fratelli-tutti.html">Fratelli Tutti</a> </i>(3 October 2020), par. 162: <i>AAS </i>
112 (2020), 1025. Cf. John Paul II, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/john-paul-ii/en/encyclicals/documents/hf_jp-ii_enc_14091981_laborem-exercens.html">Laborem Exercens</a> </i>(14 September 1981), par. 6: <i>AAS </i>73 (1981), 591: “work is ‘for man’ and 
not man ‘for work.’ Through this conclusion one rightly comes to recognize the 
pre-eminence of the subjective meaning of work over the objective one.”</p>
	<p><a name="_ftn132" title="" href="#_ftnref132">[132]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 128: <i>AAS </i>107 
(2015), 898. Cf. Id., Post-Synodal Apostolic Exhortation <i>
		<a href="https://www.vatican.va/content/francesco/en/apost_exhortations/documents/papa-francesco_esortazione-ap_20160319_amoris-laetitia.html">
		Amoris Laetitia</a></i>  
(19 March 2016), par. 24: <i>AAS </i>108 (2016), 319-320.</p>
	
	<p><a name="_ftn134" title="" href="#_ftnref134">[134]</a>&nbsp;John Paul II, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/john-paul-ii/en/encyclicals/documents/hf_jp-ii_enc_25031995_evangelium-vitae.html">Evangelium Vitae</a></i> (25 March 1995), 
par. 89: <i>AAS </i>87 (1995), 502.</p>
	
	
	
	
	
	
	<p><a name="_ftn141" title="" href="#_ftnref141">[141]</a>&nbsp;Francis, <i>Address to the Participants at the Meeting Sponsored by the 
Charity and Health Commission of the Italian Bishops’ Conference </i>(10 
February 2017): <i>AAS </i>109 (2017), 243. Cf. <i>ibid</i>., 242-243: “If there 
is a sector in which the throwaway culture is manifest, with its painful 
consequences, it is that of healthcare. When a sick person is not placed in the 
center or their dignity is not considered, this gives rise to attitudes that can 
lead even to speculation on the misfortune of others. And this is very grave! 
[…] The application of a business approach to the healthcare sector, if 
indiscriminate […] may risk discarding human beings.”</p>
	
	<p><a name="_ftn143" title="" href="#_ftnref143">[143]</a>&nbsp;Second Vatican Ecumenical Council, Declaration <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_decl_19651028_gravissimum-educationis_en.html">Gravissimum Educationis</a> </i>(28 October 1965), par. 1: <i>AAS
</i>58 (1966), 729.</p>
	<p><a name="_ftn144" title="" href="#_ftnref144">[144]</a>&nbsp;Congregation for Catholic Education, <i>Instruction on the Use of Distance Learning in Ecclesiastical Universities 
and Faculties</i>, I. Cf. Second Vatican Ecumenical Council, Declaration <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_decl_19651028_gravissimum-educationis_en.html">Gravissimum Educationis</a> </i>(28 October 1965), par. 1: <i>AAS </i>58 (1966), 
729; Francis, <i><span size="4">
		<a href="https://www.vatican.va/content/francesco/en/messages/peace/documents/papa-francesco_20151208_messaggio-xlix-giornata-mondiale-pace-2016.html">Message for the LXIX World Day of Peace</a></span></i> (1 January 2016), 
6: <i>AAS</i> 108 (2016), 57-58.</p>
	
	<p><a name="_ftn146" title="" href="#_ftnref146">[146]</a>&nbsp;Cf. Paul VI, Apostolic Exhortation <i>
		<a href="https://www.vatican.va/content/paul-vi/en/apost_exhortations/documents/hf_p-vi_exh_19751208_evangelii-nuntiandi.html">Evangelii Nuntiandi</a></i> (8 December 1975), 
par. 41: <i>AAS </i>68 (1976), 31, quoting Id., <i>Address to the Members of the 
“Consilium de Laicis”</i> (2 October 1974): <i>AAS</i> 66 (1974), 568: “if [the 
contemporary person] does listen to teachers, it is because they are witnesses.”</p>
	<p><a name="_ftn147" title="" href="#_ftnref147">[147]</a> J.H. Newman, <i>The Idea of a University Defined and Illustrated</i>, Discourse 
6.1, London 1873<sup>3</sup>, 125-126.</p>
	
	<p><a name="_ftn149" title="" href="#_ftnref149">[149]</a>&nbsp;Francis, Post-Synodal Apostolic Exhortation <i>
		<a href="https://www.vatican.va/content/francesco/en/apost_exhortations/documents/papa-francesco_esortazione-ap_20190325_christus-vivit.html">Christus Vivit</a></i> (25 March 2019), par. 
86: <i>AAS </i>111 (2019), 413, quoting the XV Ordinary General Assembly of the Synod of Bishops,
		<a href="https://www.vatican.va/roman_curia/synod/documents/rc_synod_doc_20181027_doc-final-instrumentum-xvassemblea-giovani_en.html">
<i>Final Document</i> (27 October 2018)</a>, par. 21: <i>AAS </i>110 (2018), 1592.</p>
	<p><a name="_ftn150" title="" href="#_ftnref150">[150]</a> J.H. Newman, <i>The Idea of a University Defined and Illustrated</i>, Discourse 
7.6, Basil Montagu Pickering, London 1873<sup>3</sup>, 167.</p>
	<p><a name="_ftn151" title="" href="#_ftnref151">[151]</a> Cf. Francis, Post-Synodal Apostolic Exhortation <i>
		<a href="https://www.vatican.va/content/francesco/en/apost_exhortations/documents/papa-francesco_esortazione-ap_20190325_christus-vivit.html">Christus Vivit</a></i> (25 March 
2019), par. 88: <i>AAS </i>111 (2019), 413.</p>
	<p><a name="_ftn152" title="" href="#_ftnref152">[152]</a> In a 2023 policy document about the use of generative AI in education and 
research, UNESCO notes: “One of the key questions [of the use of generative AI 
(GenAI) in education and research] is whether humans can possibly cede basic 
levels of thinking and skill-acquisition processes to AI and rather concentrate 
on higher-order thinking skills based on the outputs provided by AI. Writing, 
for example, is often associated with the structuring of thinking. With GenAI 
[…], humans can now start with a well-structured outline provided by GenAI. Some 
experts have characterized the use of GenAI to generate text in this way as 
‘writing without thinking’” (UNESCO, <i>Guidance for Generative AI in Education 
and Research </i>[2023], 37-38). The German-American philosopher Hannah Arendt 
foresaw such a possibility in her 1959 book, <i>The Human Condition</i>, and 
cautioned: “If it should turn out to be true that knowledge (in the sense of 
know-how) and thought have parted company for good, then we would indeed become 
the helpless slaves, not so much of our machines as of our know-how” (Id., <i>
The Human Condition</i>, University of Chicago Press, Chicago 2018<sup>2</sup>, 
3). </p>
	<p><a name="_ftn153" title="" href="#_ftnref153">[153]</a>&nbsp;Francis, Post-Synodal Apostolic Exhortation <i>
		<a href="https://www.vatican.va/content/francesco/en/apost_exhortations/documents/papa-francesco_esortazione-ap_20160319_amoris-laetitia.html">
		Amoris Laetitia</a></i> (19 March 2016), 
par. 262: <i>AAS </i>108 (2016), 417. </p>
	<p><a name="_ftn154" title="" href="#_ftnref154">[154]</a>&nbsp;Francis, <i>
		<a href="https://www.vatican.va/content/francesco/en/messages/peace/documents/20231208-messaggio-57giornatamondiale-pace2024.html">Message for the LVII World Day of 
Peace</a></i> (1 January 2024), par. 7: <i>L’Osservatore Romano</i>, 14 December 2023, 3; 
cf. Id., Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 167: <i>AAS
</i>107 (2015), 914. </p>
	<p><a name="_ftn155" title="" href="#_ftnref155">[155]</a>&nbsp;John Paul II, Apostolic Constitution <i>
		<a href="https://www.vatican.va/content/john-paul-ii/en/apost_constitutions/documents/hf_jp-ii_apc_15081990_ex-corde-ecclesiae.html">Ex Corde Ecclesiae</a></i> (15 August 
1990), 7: <i>AAS</i> 82 (1990), 1479.</p>
	<p><a name="_ftn156" title="" href="#_ftnref156">[156]</a>&nbsp;Francis, Apostolic Constitution <i>
		<a href="https://www.vatican.va/content/francesco/en/apost_constitutions/documents/papa-francesco_costituzione-ap_20171208_veritatis-gaudium.html">Veritatis Gaudium</a></i> (29 January 
2018), 4c: <i>AAS</i> 110 (2018), 9-10.</p>
	
	<p><a name="_ftn158" title="" href="#_ftnref158">[158]</a>&nbsp;For example, it might help people access the “array of resources for 
generating greater knowledge of truth” contained in the works of philosophy (John Paul II, 
Encyclical Letter <i>
		<a href="https://www.vatican.va/content/john-paul-ii/en/encyclicals/documents/hf_jp-ii_enc_14091998_fides-et-ratio.html">Fides et Ratio</a></i> [14 September 1998], par. 3: <i>AAS </i>
91 [1999], 7). Cf. <i>ibid</i>., par. 4: <i>AAS </i>91 (1999), 7-8.</p>
	<p><a name="_ftn159" title="" href="#_ftnref159">[159]</a>&nbsp;Dicastery for the Doctrine of the Faith, Declaration <i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20240402_dignitas-infinita_en.html">Dignitas 
Infinita</a> </i>(8 April 2024), par. 43. Cf. <i>ibid.</i>, pars. 61-62.</p>
	
	<p><a name="_ftn161" title="" href="#_ftnref161">[161]</a>&nbsp;Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par 25: <i>AAS
</i>58 (1966), 1053; cf. Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20201003_enciclica-fratelli-tutti.html">Fratelli Tutti</a> </i>(3 
October 2020), <i>passim</i>: <i>AAS </i>112 (2020), 969-1074.</p>
	<p><a name="_ftn162" title="" href="#_ftnref162">[162]</a>&nbsp;Cf. Francis., Post-Synodal Exhortation <i>
		<a href="https://www.vatican.va/content/francesco/en/apost_exhortations/documents/papa-francesco_esortazione-ap_20190325_christus-vivit.html">Christus Vivit</a></i> (25 March 
2019), par. 89: <i>AAS </i>111 (2019), 414; John Paul II, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/john-paul-ii/en/encyclicals/documents/hf_jp-ii_enc_14091998_fides-et-ratio.html">Fides et Ratio</a></i> (14 September 1998), par. 25: <i>AAS </i>91 (1999), 25-26: 
“People cannot be genuinely indifferent to the question of whether what they 
know is true or not. […] It is this that Saint Augustine teaches when he writes: 
‘I have met many who wanted to deceive, but none who wanted to be deceived’”; 
quoting Augustine, <i>Confessiones</i>, X, 23, 33: PL 32, 794.</p>
	<p><a name="_ftn163" title="" href="#_ftnref163">[163]</a>&nbsp;Dicastery for the Doctrine of the Faith, Declaration <i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20240402_dignitas-infinita_en.html">Dignitas 
Infinita</a></i> 
(4 April 2024), par. 62.</p>
	
	
	<p><a name="_ftn166" title="" href="#_ftnref166">[166]</a>&nbsp;Dicastery for the Doctrine of the Faith, Declaration <i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20240402_dignitas-infinita_en.html">Dignitas 
Infinita</a></i> 
(4 April 2024), pars. 1, 6, 16, 24. </p>
	<p><a name="_ftn167" title="" href="#_ftnref167">[167]</a>&nbsp;Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a></i>, (7 December 1965), par. 26: <i>AAS </i>58 (1966), 1046. Cf. Leo XIII, 
Encyclical Letter<i>
		<a href="https://www.vatican.va/content/leo-xiii/en/encyclicals/documents/hf_l-xiii_enc_15051891_rerum-novarum.html">Rerum Novarum</a></i> (15 May 1891), par. 40: <i>Acta Leonis 
XIII</i>,<i> </i>11 (1892), 127: “no man may with impunity violate that human 
dignity which God himself treats with great reverence”; as quoted in John Paul II, 
Encyclical Letter <i>
		<a href="https://www.vatican.va/content/john-paul-ii/en/encyclicals/documents/hf_jp-ii_enc_01051991_centesimus-annus.html">Centesimus Annus</a> </i>(1 May 1991), par. 9: <i>AAS </i>83 
(1991), 804.</p>
	
	<p><a name="_ftn169" title="" href="#_ftnref169">[169]</a>&nbsp;Cf. Permanent Observer Mission of the Holy See to the United Nations, <i>
Holy See Statement to the Thematic Discussion on Other Disarmament Measures and 
International Security</i> (24 October 2022): “Upholding human dignity in 
cyberspace obliges States to also respect the right to privacy, by shielding 
citizens from intrusive surveillance and allowing them to safeguard their 
personal information from unauthorized access.”</p>
	<p><a name="_ftn170" title="" href="#_ftnref170">[170]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20201003_enciclica-fratelli-tutti.html">Fratelli Tutti</a> </i>(3 October 2020), par. 
42: <i>AAS </i>112 (2020), 984.</p>
	
	
	<p><a name="_ftn173" title="" href="#_ftnref173">[173]</a>&nbsp;The 2023 <i>Interim Report</i> of the United Nations AI Advisory Body<i>
</i>identified a list of “early promises of AI helping to address climate 
change” (United Nations AI Advisory Body, <i>Interim Report: Governing AI for 
Humanity</i> [December 2023], 3). The document observed that, “taken together 
with predictive systems that can transform data into insights and insights into 
actions, AI-enabled tools may help develop new strategies and investments to 
reduce emissions, influence new private sector investments in net zero, protect 
biodiversity, and build broad-based social resilience” (<i>ibid</i>.).</p>
	<p><a name="_ftn174" title="" href="#_ftnref174">[174]</a>&nbsp;“The cloud” refers to a network of physical servers throughout the world 
that enables users to store, process, and manage their data remotely.</p>
	<p><a name="_ftn175" title="" href="#_ftnref175">[175]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 9: <i>
AAS </i>107 (2015), 850.</p>
	<p><a name="_ftn176" title="" href="#_ftnref176">[176]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 106: <i>
AAS </i>107 (2015), 890. </p>
	<p><a name="_ftn177" title="" href="#_ftnref177">[177]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 60: <i>
AAS </i>107 (2015), 870. </p>
	<p><a name="_ftn178" title="" href="#_ftnref178">[178]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), pars. 3, 13:
<i>AAS </i>107 (2015), 848.852.</p>
	<p><a name="_ftn179" title="" href="#_ftnref179">[179]</a>&nbsp;Augustine, <i>De Civitate Dei</i>, XIX, 13, 1: PL 41, 640.</p>
	<p><a name="_ftn180" title="" href="#_ftnref180">[180]</a>&nbsp;Cf. Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), pars.&nbsp;77-82: <i>AAS </i>58 (1966), 1100-1107; Francis, 
Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20201003_enciclica-fratelli-tutti.html">Fratelli Tutti</a> </i>(3 October 2020), pars.&nbsp;256-262: <i>
AAS </i>112 (2020), 1060-1063; Dicastery for the Doctrine of the Faith, 
Declaration<i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20240402_dignitas-infinita_en.html">Dignitas 
Infinita</a> </i>(4 April 2024), pars. 38-39; <i>Catechism of 
the Catholic Church</i>, pars. 2302-2317.</p>
	<p><a name="_ftn181" title="" href="#_ftnref181">[181]</a>&nbsp;Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par.&nbsp;78: <i>AAS </i>58 (1966), 1101.</p>
	
	<p><a name="_ftn183" title="" href="#_ftnref183">[183]</a>&nbsp;Cf.<b> </b><i>Catechism of the Catholic Church</i>, pars. 2308-2310. 
	</p>
	<p><a name="_ftn184" title="" href="#_ftnref184">[184]</a> Cf. Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), pars.&nbsp;80-81: <i>AAS </i>58 (1966), 1103-1105.</p>
	
	<p><a name="_ftn186" title="" href="#_ftnref186">[186]</a>&nbsp;Francis, <i>
		<a href="https://www.vatican.va/content/francesco/en/speeches/2024/june/documents/20240614-g7-intelligenza-artificiale.html">Address at the G7 Session on 
Artificial Intelligence in Borgo Egnazia (Puglia)</a></i> (14 June 2024): <i>L’Osservatore Romano</i>, 14 June 2024, 2. Cf. Permanent Observer Mission of the Holy See to the United Nations,
<i>Holy See Statement to Working Group II on Emerging Technologies at the UN 
Disarmament Commission </i>(3 April 2024): “The development and use of lethal 
autonomous weapons systems (LAWS) that lack the appropriate human control would 
pose fundamental ethical concerns, given that LAWS can never be morally 
responsible subjects capable of complying with international humanitarian law.”</p>
	<p><a name="_ftn187" title="" href="#_ftnref187">[187]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20201003_enciclica-fratelli-tutti.html">Fratelli Tutti</a> </i>(3 October 2020), par.&nbsp;258: 
<i>AAS </i>112 (2020), 1061. Cf. Second Vatican Ecumenical Council, Pastoral 
Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 80: <i>AAS </i>58 
(1966), 1103-1104. </p>
	<p><a name="_ftn188" title="" href="#_ftnref188">[188]</a>&nbsp;Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 80: <i>AAS </i>58 (1966), 1103-1104.</p>
	<p><a name="_ftn189" title="" href="#_ftnref189">[189]</a>&nbsp;Cf. Francis, <i>
		<a href="https://www.vatican.va/content/francesco/en/messages/peace/documents/20231208-messaggio-57giornatamondiale-pace2024.html">Message for the LVII World Day of 
Peace</a> </i>(1 January 
2024), par. 6: <i>L’Osservatore Romano</i>,<i>&nbsp;</i>14 December 2023, 3: 
“Nor can we ignore the possibility of sophisticated weapons ending up in the 
wrong hands, facilitating, for instance, terrorist attacks or interventions 
aimed at destabilizing the institutions of legitimate systems of government. In 
a word, the world does not need new technologies that contribute to the unjust 
development of commerce and the weapons trade and consequently end up promoting 
the folly of war.”</p>
	
	<p><a name="_ftn191" title="" href="#_ftnref191">[191]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 79: <i>
AAS </i>107 (2015), 878.</p>
	<p><a name="_ftn192" title="" href="#_ftnref192">[192]</a>&nbsp;Cf. Benedict XVI, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/benedict-xvi/en/encyclicals/documents/hf_ben-xvi_enc_20090629_caritas-in-veritate.html">Caritas in Veritate</a> </i> (29 June 
2009), par. 51: <i>AAS </i>101 (2009), 687.</p>
	<p><a name="_ftn193" title="" href="#_ftnref193">[193]</a>&nbsp;Cf. Dicastery for the Doctrine of the Faith, Declaration <i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20240402_dignitas-infinita_en.html">Dignitas 
Infinita</a> </i>(8 April 2024), pars. 38-39. </p>
	<p><a name="_ftn194" title="" href="#_ftnref194">[194]</a>&nbsp;Cf.&nbsp;Augustine, <i>Confessiones</i>, I, 1, 1: PL 32, 661.</p>
	<p><a name="_ftn195" title="" href="#_ftnref195">[195]</a>&nbsp;Cf. John Paul II, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/john-paul-ii/en/encyclicals/documents/hf_jp-ii_enc_30121987_sollicitudo-rei-socialis.html">Sollicitudo Rei Socialis</a> </i>(30 
December 1987), par. 28: <i>AAS </i>80 (1988), 548: “[T]here is a better 
understanding today that the mere accumulation of goods and services […] is not 
enough for the realization of human happiness. Nor, in consequence, does the 
availability of the many real benefits provided in recent times by science and 
technology, including the computer sciences, bring freedom from every form of 
slavery. On the contrary, […] unless all the considerable body of resources and 
potential at man’s disposal is guided by a moral understanding and by an 
orientation towards the true good of the human race, it easily turns against man 
to oppress him.” Cf. <i>ibid.</i>, pars. 29, 37: <i>AAS </i>80 (1988), 
550-551.563-564.</p>
	<p><a name="_ftn196" title="" href="#_ftnref196">[196]</a>&nbsp;Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 14: <i>AAS </i>58 (1966), 1036.</p>
	<p><a name="_ftn197" title="" href="#_ftnref197">[197]</a>&nbsp;Francis, Encyclical Letter<i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/20241024-enciclica-dilexit-nos.html">Dilexit Nos</a> </i>(24 October 2024), par. 18:
<i>L’Osservatore Romano</i>, 24 October 2024, 5. </p>
	<p><a name="_ftn198" title="" href="#_ftnref198">[198]</a>&nbsp;Francis, Encyclical Letter<i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/20241024-enciclica-dilexit-nos.html">Dilexit Nos</a> </i>(24 October 2024), par. 27:
<i>L’Osservatore Romano</i>, 24 October 2024, 6. </p>
	<p><a name="_ftn199" title="" href="#_ftnref199">[199]</a>&nbsp;Francis, Encyclical Letter<i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/20241024-enciclica-dilexit-nos.html">Dilexit Nos</a> </i>(24 October 2024), par. 25:
<i>L’Osservatore Romano</i>, 24 October 2024, 5-6. </p>
	<p><a name="_ftn200" title="" href="#_ftnref200">[200]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 105: <i>AAS </i>107 
(2015), 889. Cf. R. Guardini, <i>Das Ende der Neuzeit</i>, Würzburg 1965<sup>9</sup>, 
87 ff. (en. tr. <i>The End of the Modern World</i>, Wilmington 1998, 82-83).</p>
	<p><a name="_ftn201" title="" href="#_ftnref201">[201]</a>&nbsp;Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 34: <i>AAS </i>58 (1966), 1053.</p>
	<p><a name="_ftn202" title="" href="#_ftnref202">[202]</a>&nbsp;John Paul II, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/john-paul-ii/en/encyclicals/documents/hf_jp-ii_enc_04031979_redemptor-hominis.html">Redemptor Hominis</a> </i>(4 March 1979), 
par. 15: <i>AAS </i>71 (1979), 287-288.</p>
	<p><a name="_ftn203" title="" href="#_ftnref203">[203]</a>&nbsp;N. Berdyaev, “Man and Machine,” in C. Mitcham – R. Mackey, eds., <i>
Philosophy and Technology: Readings in the Philosophical Problems of Technology</i>, 
New York 1983<sup>2</sup>, 212-213.</p>
	<p><a name="_ftn204" title="" href="#_ftnref204">[204]</a>&nbsp;N. Berdyaev, “Man and Machine,” 210. 
	</p>
	<p><a name="_ftn205" title="" href="#_ftnref205">[205]</a>&nbsp;G. Bernanos, “La révolution de la liberté” (1944), in Id., <i>Le Chemin de 
la Croix-des-Âmes</i>, Rocher 1987, 829.</p>
	
	<p><a name="_ftn207" title="" href="#_ftnref207">[207]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 112: <i>
AAS </i>107 (2015), 892-893.</p>
	<p><a name="_ftn208" title="" href="#_ftnref208">[208]</a>&nbsp;Cf. Bonaventure, <i>Hex. </i>XIX, 3; Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20201003_enciclica-fratelli-tutti.html">Fratelli Tutti</a></i> (3 October 2020), par. 50: <i>AAS </i>112 (2020), 986: “The 
flood of information at our fingertips does not make for greater wisdom. Wisdom 
is not born of quick searches on the internet nor is it a mass of unverified 
data. That is not the way to mature in the encounter with truth.”</p>
	
	
	
	<p><a name="_ftn212" title="" href="#_ftnref212">[212]</a>&nbsp;Francis, Apostolic Exhortation <i>
		<a href="https://www.vatican.va/content/francesco/en/apost_exhortations/documents/papa-francesco_esortazione-ap_20180319_gaudete-et-exsultate.html#An_intellect_without_God_and_without_flesh">Gaudete et Exsultate</a></i> (19 March 2018), par. 37: <i>
AAS </i>110 (2018), 1121. </p>
	<p><a name="_ftn213" title="" href="#_ftnref213">[213]</a>&nbsp;Francis, <i>
		<a href="https://www.vatican.va/content/francesco/en/messages/peace/documents/20231208-messaggio-57giornatamondiale-pace2024.html">Message for the LVII World Day of 
Peace</a></i> (1 January 2024), par. 6: <i>L’Osservatore Romano</i>, 14 December 2023, 3. 
Cf. Id., Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 112: <i>AAS
</i>107 (2015), 892-893; Id., Apostolic Exhortation <i>
		<a href="https://www.vatican.va/content/francesco/en/apost_exhortations/documents/papa-francesco_esortazione-ap_20180319_gaudete-et-exsultate.html#The_limits_of_reason">Gaudete et Exsultate</a></i> 
(19 March 2018), par. 46:<i> AAS </i>110 (2018), 1123-1124.</p>
	<p><a name="_ftn214" title="" href="#_ftnref214">[214]</a> Cf. Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 112: <i>
AAS </i>107 (2015), 892-893.</p>
	
</div>
										</td>
												</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Audiocube – A 3D DAW for Spatial Audio (177 pts)]]></title>
            <link>https://www.audiocube.app</link>
            <guid>42877399</guid>
            <pubDate>Thu, 30 Jan 2025 13:12:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.audiocube.app">https://www.audiocube.app</a>, See on <a href="https://news.ycombinator.com/item?id=42877399">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="siteWrapper">
      
      

      












  




      <main id="page" role="main">
        
          
<article id="sections" data-page-sections="65b3b6e1b2fd050adf61b14a">
  
  
    
    


  
  


<section data-test="page-section" data-section-theme="black-bold" data-section-id="678d2cb08dbda17fa2c972f0" data-controller="SectionWrapperController" data-current-styles="{
                      &quot;backgroundImage&quot;: {
                        &quot;id&quot;: &quot;679990a24ce06c5bb487966e&quot;,
                        &quot;recordType&quot;: 2,
                        &quot;addedOn&quot;: 1738117282260,
                        &quot;updatedOn&quot;: 1738117285871,
                        &quot;workflowState&quot;: 1,
                        &quot;publishOn&quot;: 1738117282260,
                        &quot;authorId&quot;: &quot;6111afdce1647b38b8e47725&quot;,
                        &quot;systemDataId&quot;: &quot;d447cb51-fe63-4fa3-9bd5-a529ead4a0e3&quot;,
                        &quot;systemDataVariants&quot;: &quot;4253x1898,100w,300w,500w,750w,1000w,1500w,2500w&quot;,
                        &quot;systemDataSourceType&quot;: &quot;JPG&quot;,
                        &quot;filename&quot;: &quot;Web BG COlor Gradient 2-min.jpg&quot;,
                        &quot;mediaFocalPoint&quot;: {
                          &quot;x&quot;: 0.5188284518828452,
                          &quot;y&quot;: 1.0,
                          &quot;source&quot;: 3
                        },
                        &quot;colorData&quot;: {
                          &quot;topLeftAverage&quot;: &quot;380f69&quot;,
                          &quot;topRightAverage&quot;: &quot;006baf&quot;,
                          &quot;bottomLeftAverage&quot;: &quot;020005&quot;,
                          &quot;bottomRightAverage&quot;: &quot;000716&quot;,
                          &quot;centerAverage&quot;: &quot;0c1842&quot;,
                          &quot;suggestedBgColor&quot;: &quot;151c57&quot;
                        },
                        &quot;urlId&quot;: &quot;mo045m3laajctetldta0za2146udgh&quot;,
                        &quot;title&quot;: &quot;&quot;,
                        &quot;body&quot;: null,
                        &quot;likeCount&quot;: 0,
                        &quot;commentCount&quot;: 0,
                        &quot;publicCommentCount&quot;: 0,
                        &quot;commentState&quot;: 2,
                        &quot;unsaved&quot;: false,
                        &quot;author&quot;: {
                          &quot;id&quot;: &quot;6111afdce1647b38b8e47725&quot;,
                          &quot;displayName&quot;: &quot;Ark Audio&quot;,
                          &quot;firstName&quot;: &quot;Ark&quot;,
                          &quot;lastName&quot;: &quot;Audio&quot;,
                          &quot;avatarUrl&quot;: &quot;https://images.squarespace-cdn.com/content/v2/namespaces/memberAccountAvatars/libraries/6111afdce1647b38b8e47725/c5536852-a1ad-4783-bb65-4997a9313eeb/noah+m+head.png?format=300w&quot;,
                          &quot;bio&quot;: &quot;&quot;,
                          &quot;avatarAssetUrl&quot;: &quot;https://images.squarespace-cdn.com/content/v2/namespaces/memberAccountAvatars/libraries/6111afdce1647b38b8e47725/c5536852-a1ad-4783-bb65-4997a9313eeb/noah+m+head.png?format=300w&quot;
                        },
                        &quot;assetUrl&quot;: &quot;https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/d447cb51-fe63-4fa3-9bd5-a529ead4a0e3/Web+BG+COlor+Gradient+2-min.jpg&quot;,
                        &quot;contentType&quot;: &quot;image/jpeg&quot;,
                        &quot;items&quot;: [ ],
                        &quot;pushedServices&quot;: { },
                        &quot;pendingPushedServices&quot;: { },
                        &quot;originalSize&quot;: &quot;4253x1898&quot;,
                        &quot;recordTypeLabel&quot;: &quot;image&quot;
                      },
                      &quot;imageOverlayOpacity&quot;: 0.0,
                      &quot;backgroundWidth&quot;: &quot;background-width--full-bleed&quot;,
                      &quot;customSectionHeight&quot;: 1,
                      &quot;horizontalAlignment&quot;: &quot;horizontal-alignment--center&quot;,
                      &quot;verticalAlignment&quot;: &quot;vertical-alignment--middle&quot;,
                      &quot;contentWidth&quot;: &quot;content-width--wide&quot;,
                      &quot;customContentWidth&quot;: 50,
                      &quot;backgroundColor&quot;: &quot;&quot;,
                      &quot;sectionTheme&quot;: &quot;black-bold&quot;,
                      &quot;sectionAnimation&quot;: &quot;none&quot;,
                      &quot;backgroundMode&quot;: &quot;image&quot;
                    }" data-current-context="{
                      &quot;video&quot;: {
                        &quot;playbackSpeed&quot;: 0.5,
                        &quot;filter&quot;: 1,
                        &quot;filterStrength&quot;: 0,
                        &quot;zoom&quot;: 0,
                        &quot;videoSourceProvider&quot;: &quot;none&quot;
                      },
                      &quot;backgroundImageId&quot;: null,
                      &quot;backgroundMediaEffect&quot;: {
                        &quot;type&quot;: &quot;none&quot;
                      },
                      &quot;divider&quot;: {
                        &quot;enabled&quot;: false,
                        &quot;type&quot;: &quot;soft-corners&quot;,
                        &quot;width&quot;: {
                          &quot;unit&quot;: &quot;vw&quot;,
                          &quot;value&quot;: 100.0
                        },
                        &quot;height&quot;: {
                          &quot;unit&quot;: &quot;vw&quot;,
                          &quot;value&quot;: 6.0
                        },
                        &quot;isFlipX&quot;: false,
                        &quot;isFlipY&quot;: true,
                        &quot;offset&quot;: {
                          &quot;unit&quot;: &quot;px&quot;,
                          &quot;value&quot;: 0.0
                        },
                        &quot;stroke&quot;: {
                          &quot;style&quot;: &quot;none&quot;,
                          &quot;color&quot;: {
                            &quot;type&quot;: &quot;SITE_PALETTE_COLOR&quot;,
                            &quot;sitePaletteColor&quot;: {
                              &quot;colorName&quot;: &quot;darkAccent&quot;,
                              &quot;alphaModifier&quot;: 1.0
                            }
                          },
                          &quot;thickness&quot;: {
                            &quot;unit&quot;: &quot;px&quot;,
                            &quot;value&quot;: 6.0
                          },
                          &quot;dashLength&quot;: {
                            &quot;unit&quot;: &quot;px&quot;,
                            &quot;value&quot;: 5.0
                          },
                          &quot;gapLength&quot;: {
                            &quot;unit&quot;: &quot;px&quot;,
                            &quot;value&quot;: 5.0
                          },
                          &quot;linecap&quot;: &quot;square&quot;
                        }
                      },
                      &quot;typeName&quot;: &quot;page&quot;
                    }" data-animation="none" data-fluid-engine-section="">
  <div>
    
      
        
        
          





  



<p><img alt="" data-src="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/d447cb51-fe63-4fa3-9bd5-a529ead4a0e3/Web+BG+COlor+Gradient+2-min.jpg" data-image="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/d447cb51-fe63-4fa3-9bd5-a529ead4a0e3/Web+BG+COlor+Gradient+2-min.jpg" data-image-dimensions="4253x1898" data-image-focal-point="0.5188284518828452,1.0" data-load="false" elementtiming="nbf-background" src="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/d447cb51-fe63-4fa3-9bd5-a529ead4a0e3/Web+BG+COlor+Gradient+2-min.jpg" width="4253" height="1898" sizes="(max-width: 799px) 200vw, 100vw" srcset="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/d447cb51-fe63-4fa3-9bd5-a529ead4a0e3/Web+BG+COlor+Gradient+2-min.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/d447cb51-fe63-4fa3-9bd5-a529ead4a0e3/Web+BG+COlor+Gradient+2-min.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/d447cb51-fe63-4fa3-9bd5-a529ead4a0e3/Web+BG+COlor+Gradient+2-min.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/d447cb51-fe63-4fa3-9bd5-a529ead4a0e3/Web+BG+COlor+Gradient+2-min.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/d447cb51-fe63-4fa3-9bd5-a529ead4a0e3/Web+BG+COlor+Gradient+2-min.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/d447cb51-fe63-4fa3-9bd5-a529ead4a0e3/Web+BG+COlor+Gradient+2-min.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/d447cb51-fe63-4fa3-9bd5-a529ead4a0e3/Web+BG+COlor+Gradient+2-min.jpg?format=2500w 2500w" fetchpriority="high" loading="eager" decoding="async" data-loader="sqs"></p>
      
    
    </div>
  <div data-fluid-engine="true"><div data-block-type="23" id="block-e36654e20c3f4c8caa4e"><p><span> A Standalone 3D Audio Workstation</span></p>
</div><div data-blend-mode="NORMAL" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-1a43bf9a82d886939b37">
  <h2><strong>Create Totally Immersive Audio</strong></h2><p>Audiocube is a <span><strong>3D DAW </strong></span>— unleash a new dimension of sonic creativity in your music and sound design.</p><p><span>No credit card required </span></p>
</div></div>
  
</section>

  
    
    


  
  


<div data-fluid-engine="true" data-test="page-section" data-section-theme="black-bold" data-section-id="678a64fd4447dd78577938cd" data-controller="SectionWrapperController" data-current-styles="{
                      &quot;imageOverlayOpacity&quot;: 0.15,
                      &quot;backgroundWidth&quot;: &quot;background-width--full-bleed&quot;,
                      &quot;sectionHeight&quot;: &quot;section-height--custom&quot;,
                      &quot;customSectionHeight&quot;: 1,
                      &quot;horizontalAlignment&quot;: &quot;horizontal-alignment--center&quot;,
                      &quot;verticalAlignment&quot;: &quot;vertical-alignment--middle&quot;,
                      &quot;contentWidth&quot;: &quot;content-width--wide&quot;,
                      &quot;customContentWidth&quot;: 50,
                      &quot;backgroundColor&quot;: &quot;&quot;,
                      &quot;sectionTheme&quot;: &quot;black-bold&quot;,
                      &quot;sectionAnimation&quot;: &quot;none&quot;,
                      &quot;backgroundMode&quot;: &quot;image&quot;
                    }" data-current-context="{
                      &quot;video&quot;: {
                        &quot;playbackSpeed&quot;: 0.5,
                        &quot;filter&quot;: 1,
                        &quot;filterStrength&quot;: 0,
                        &quot;zoom&quot;: 0,
                        &quot;videoSourceProvider&quot;: &quot;none&quot;
                      },
                      &quot;backgroundImageId&quot;: null,
                      &quot;backgroundMediaEffect&quot;: {
                        &quot;type&quot;: &quot;none&quot;
                      },
                      &quot;divider&quot;: {
                        &quot;enabled&quot;: false
                      },
                      &quot;typeName&quot;: &quot;page&quot;
                    }" data-animation="none" data-fluid-engine-section=""><div data-block-type="23" id="block-8ae57b83ce94115f62ef"><p>
  <h2>Unlock a new <span>sonic dimension...</span></h2>
</p>
</div><div data-blend-mode="NORMAL" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:20.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:20.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:20.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:20.0}}" id="block-a6b210f96707920e509b">
  <p><span><strong>✅ An Intuitive Workflow</strong></span></p><p><span>Instantly import audio files, process and spatialize your sounds, then record and export your project as a HD .wav file. </span></p>
</div><div data-blend-mode="NORMAL" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:20.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:20.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:20.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:20.0}}" id="block-95acb798e3bc7804bd9e">
  <p><span><strong>✅ More Power Than VST Plugins</strong></span></p><p><span>As a standalone app, Audiocube offers tools, workflows, and processes that go beyond the capabilities of VST plugins.</span></p>
</div><div data-blend-mode="NORMAL" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:20.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:20.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:20.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:20.0}}" id="block-273debcc074593c4e333">
  <p><span><strong>✅ Support You Can Count On</strong></span></p><p><span>Our responsive support team ensures you’ll never feel stuck, with help available whenever you need it. </span></p>
</div></div>

  
    
    


  
  


<div data-fluid-engine="true" data-test="page-section" data-section-theme="black" data-section-id="678a663da217376b315c1f4b" data-controller="SectionWrapperController" data-current-styles="{
                      &quot;imageOverlayOpacity&quot;: 0.15,
                      &quot;backgroundWidth&quot;: &quot;background-width--full-bleed&quot;,
                      &quot;sectionHeight&quot;: &quot;section-height--custom&quot;,
                      &quot;customSectionHeight&quot;: 1,
                      &quot;horizontalAlignment&quot;: &quot;horizontal-alignment--center&quot;,
                      &quot;verticalAlignment&quot;: &quot;vertical-alignment--middle&quot;,
                      &quot;contentWidth&quot;: &quot;content-width--wide&quot;,
                      &quot;customContentWidth&quot;: 50,
                      &quot;backgroundColor&quot;: &quot;&quot;,
                      &quot;sectionTheme&quot;: &quot;black&quot;,
                      &quot;sectionAnimation&quot;: &quot;none&quot;,
                      &quot;backgroundMode&quot;: &quot;image&quot;
                    }" data-current-context="{
                      &quot;video&quot;: {
                        &quot;playbackSpeed&quot;: 0.5,
                        &quot;filter&quot;: 1,
                        &quot;filterStrength&quot;: 0,
                        &quot;zoom&quot;: 0,
                        &quot;videoSourceProvider&quot;: &quot;none&quot;
                      },
                      &quot;backgroundImageId&quot;: null,
                      &quot;backgroundMediaEffect&quot;: {
                        &quot;type&quot;: &quot;none&quot;
                      },
                      &quot;divider&quot;: {
                        &quot;enabled&quot;: false
                      },
                      &quot;typeName&quot;: &quot;page&quot;
                    }" data-animation="none" data-fluid-engine-section=""><div data-blend-mode="NORMAL" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1737122899196_29878">
  <h4><span><strong>1000+ sound designers &amp; musicians </strong></span>are using Audiocube for audio spatialization and experimentation</h4><p><strong>Featured on</strong></p>
</div><div data-animation-role="image" data-test="image-block-fluid-outer-wrapper" data-aspect-ratio="100.35249787285767" data-block-type="5" id="block-9d1feb521afac4eded5c">
                
            
            
            
            
            
            
            <p><img data-stretch="true" data-src="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/e47d6e88-4946-4e3a-ade3-1364e3a4da85/-1386445_20220419426.jpg" data-image="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/e47d6e88-4946-4e3a-ade3-1364e3a4da85/-1386445_20220419426.jpg" data-image-dimensions="512x512" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/e47d6e88-4946-4e3a-ade3-1364e3a4da85/-1386445_20220419426.jpg" width="512" height="512" sizes="100vw" srcset="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/e47d6e88-4946-4e3a-ade3-1364e3a4da85/-1386445_20220419426.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/e47d6e88-4946-4e3a-ade3-1364e3a4da85/-1386445_20220419426.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/e47d6e88-4946-4e3a-ade3-1364e3a4da85/-1386445_20220419426.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/e47d6e88-4946-4e3a-ade3-1364e3a4da85/-1386445_20220419426.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/e47d6e88-4946-4e3a-ade3-1364e3a4da85/-1386445_20220419426.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/e47d6e88-4946-4e3a-ade3-1364e3a4da85/-1386445_20220419426.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/e47d6e88-4946-4e3a-ade3-1364e3a4da85/-1386445_20220419426.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs"></p>
          
              </div><div data-animation-role="image" data-test="image-block-fluid-outer-wrapper" data-aspect-ratio="83.8257691136156" data-block-type="5" id="block-79d0ba19132b025a764d">
                
            
            
            
            
            
            
            <p><img data-stretch="true" data-src="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/c3ba0dcd-9426-4ec9-b855-f8a09a9e9ee5/kvraudio-dark-200x200-www.png" data-image="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/c3ba0dcd-9426-4ec9-b855-f8a09a9e9ee5/kvraudio-dark-200x200-www.png" data-image-dimensions="200x200" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/c3ba0dcd-9426-4ec9-b855-f8a09a9e9ee5/kvraudio-dark-200x200-www.png" width="200" height="200" sizes="100vw" srcset="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/c3ba0dcd-9426-4ec9-b855-f8a09a9e9ee5/kvraudio-dark-200x200-www.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/c3ba0dcd-9426-4ec9-b855-f8a09a9e9ee5/kvraudio-dark-200x200-www.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/c3ba0dcd-9426-4ec9-b855-f8a09a9e9ee5/kvraudio-dark-200x200-www.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/c3ba0dcd-9426-4ec9-b855-f8a09a9e9ee5/kvraudio-dark-200x200-www.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/c3ba0dcd-9426-4ec9-b855-f8a09a9e9ee5/kvraudio-dark-200x200-www.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/c3ba0dcd-9426-4ec9-b855-f8a09a9e9ee5/kvraudio-dark-200x200-www.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/c3ba0dcd-9426-4ec9-b855-f8a09a9e9ee5/kvraudio-dark-200x200-www.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs"></p>
          
              </div><div data-animation-role="image" data-test="image-block-fluid-outer-wrapper" data-aspect-ratio="100.35249787285767" data-block-type="5" id="block-1a49b67cb21167e74aa3">
                
            
            
            
            
            
            
            <p><img data-stretch="true" data-src="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/69e37cff-c78f-495e-a9c3-2e1f54a360c2/unnamed.jpg" data-image="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/69e37cff-c78f-495e-a9c3-2e1f54a360c2/unnamed.jpg" data-image-dimensions="900x900" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/69e37cff-c78f-495e-a9c3-2e1f54a360c2/unnamed.jpg" width="900" height="900" sizes="100vw" srcset="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/69e37cff-c78f-495e-a9c3-2e1f54a360c2/unnamed.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/69e37cff-c78f-495e-a9c3-2e1f54a360c2/unnamed.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/69e37cff-c78f-495e-a9c3-2e1f54a360c2/unnamed.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/69e37cff-c78f-495e-a9c3-2e1f54a360c2/unnamed.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/69e37cff-c78f-495e-a9c3-2e1f54a360c2/unnamed.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/69e37cff-c78f-495e-a9c3-2e1f54a360c2/unnamed.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/69e37cff-c78f-495e-a9c3-2e1f54a360c2/unnamed.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs"></p>
          
              </div><div data-animation-role="image" data-test="image-block-fluid-outer-wrapper" data-aspect-ratio="100.38910505836576" data-block-type="5" id="block-yui_3_17_2_1_1737122899196_60232">
                
            
            
            
            
            
            
            <p><img data-stretch="true" data-src="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/5b8e74f4-d094-4c6d-97e3-6314989793a4/download.png" data-image="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/5b8e74f4-d094-4c6d-97e3-6314989793a4/download.png" data-image-dimensions="225x225" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/5b8e74f4-d094-4c6d-97e3-6314989793a4/download.png" width="225" height="225" sizes="100vw" srcset="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/5b8e74f4-d094-4c6d-97e3-6314989793a4/download.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/5b8e74f4-d094-4c6d-97e3-6314989793a4/download.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/5b8e74f4-d094-4c6d-97e3-6314989793a4/download.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/5b8e74f4-d094-4c6d-97e3-6314989793a4/download.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/5b8e74f4-d094-4c6d-97e3-6314989793a4/download.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/5b8e74f4-d094-4c6d-97e3-6314989793a4/download.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/5b8e74f4-d094-4c6d-97e3-6314989793a4/download.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs"></p>
          
              </div><div data-animation-role="image" data-test="image-block-fluid-outer-wrapper" data-aspect-ratio="73.69454610372222" data-block-type="5" id="block-247b8ef1633b7fa21f34">
                
            
            
            
            
            
            
            <p><img data-stretch="true" data-src="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/c8b829e2-c7b3-4376-856f-844b9d8bdd33/music-tech-news-banner%402000x1500-696x522.jpg" data-image="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/c8b829e2-c7b3-4376-856f-844b9d8bdd33/music-tech-news-banner%402000x1500-696x522.jpg" data-image-dimensions="696x522" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/c8b829e2-c7b3-4376-856f-844b9d8bdd33/music-tech-news-banner%402000x1500-696x522.jpg" width="696" height="522" sizes="100vw" srcset="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/c8b829e2-c7b3-4376-856f-844b9d8bdd33/music-tech-news-banner%402000x1500-696x522.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/c8b829e2-c7b3-4376-856f-844b9d8bdd33/music-tech-news-banner%402000x1500-696x522.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/c8b829e2-c7b3-4376-856f-844b9d8bdd33/music-tech-news-banner%402000x1500-696x522.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/c8b829e2-c7b3-4376-856f-844b9d8bdd33/music-tech-news-banner%402000x1500-696x522.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/c8b829e2-c7b3-4376-856f-844b9d8bdd33/music-tech-news-banner%402000x1500-696x522.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/c8b829e2-c7b3-4376-856f-844b9d8bdd33/music-tech-news-banner%402000x1500-696x522.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/c8b829e2-c7b3-4376-856f-844b9d8bdd33/music-tech-news-banner%402000x1500-696x522.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs"></p>
          
              </div><div data-animation-role="image" data-test="image-block-fluid-outer-wrapper" data-aspect-ratio="58.21874338904167" data-block-type="5" id="block-a2714f1a3993d2bff038">
                
            
            
            
            
            
            
            <p><img data-stretch="true" data-src="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/3eb663d9-a9bd-4c05-a286-be1a1a1c860e/Logo_audiofanzine2013.jpg" data-image="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/3eb663d9-a9bd-4c05-a286-be1a1a1c860e/Logo_audiofanzine2013.jpg" data-image-dimensions="2463x1269" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/3eb663d9-a9bd-4c05-a286-be1a1a1c860e/Logo_audiofanzine2013.jpg" width="2463" height="1269" sizes="100vw" srcset="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/3eb663d9-a9bd-4c05-a286-be1a1a1c860e/Logo_audiofanzine2013.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/3eb663d9-a9bd-4c05-a286-be1a1a1c860e/Logo_audiofanzine2013.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/3eb663d9-a9bd-4c05-a286-be1a1a1c860e/Logo_audiofanzine2013.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/3eb663d9-a9bd-4c05-a286-be1a1a1c860e/Logo_audiofanzine2013.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/3eb663d9-a9bd-4c05-a286-be1a1a1c860e/Logo_audiofanzine2013.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/3eb663d9-a9bd-4c05-a286-be1a1a1c860e/Logo_audiofanzine2013.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/3eb663d9-a9bd-4c05-a286-be1a1a1c860e/Logo_audiofanzine2013.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs"></p>
          
              </div></div>

  
    
    


  
  


<div data-fluid-engine="true" data-test="page-section" data-section-theme="black-bold" data-section-id="678a7e6c8b4c0060b5d45f38" data-controller="SectionWrapperController" data-current-styles="{
                      &quot;imageOverlayOpacity&quot;: 0.15,
                      &quot;backgroundWidth&quot;: &quot;background-width--full-bleed&quot;,
                      &quot;sectionHeight&quot;: &quot;section-height--custom&quot;,
                      &quot;customSectionHeight&quot;: 1,
                      &quot;horizontalAlignment&quot;: &quot;horizontal-alignment--center&quot;,
                      &quot;verticalAlignment&quot;: &quot;vertical-alignment--middle&quot;,
                      &quot;contentWidth&quot;: &quot;content-width--wide&quot;,
                      &quot;customContentWidth&quot;: 50,
                      &quot;backgroundColor&quot;: &quot;&quot;,
                      &quot;sectionTheme&quot;: &quot;black-bold&quot;,
                      &quot;sectionAnimation&quot;: &quot;none&quot;,
                      &quot;backgroundMode&quot;: &quot;image&quot;
                    }" data-current-context="{
                      &quot;video&quot;: {
                        &quot;playbackSpeed&quot;: 0.5,
                        &quot;filter&quot;: 1,
                        &quot;filterStrength&quot;: 0,
                        &quot;zoom&quot;: 0,
                        &quot;videoSourceProvider&quot;: &quot;none&quot;
                      },
                      &quot;backgroundImageId&quot;: null,
                      &quot;backgroundMediaEffect&quot;: {
                        &quot;type&quot;: &quot;none&quot;
                      },
                      &quot;divider&quot;: {
                        &quot;enabled&quot;: false
                      },
                      &quot;typeName&quot;: &quot;page&quot;
                    }" data-animation="none" data-fluid-engine-section=""><div data-blend-mode="NORMAL" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1737122899196_90321">
  <p>Audio Spatialization &amp; Experimentation</p><h3>Are 2D plugins really the best way to work with 3D audio? </h3><p><span>Try it free!</span></p>
</div><div data-animation-role="image" data-test="image-block-fluid-outer-wrapper" data-aspect-ratio="91.84070137711427" data-block-type="5" id="block-yui_3_17_2_1_1737122899196_98079">
                
            
            
            
            
            
            
            <p><img data-stretch="true" data-src="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/5d784c27-b4d5-4836-9f8f-759c116cd22a/spatial+audio+plguin+2d.png" data-image="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/5d784c27-b4d5-4836-9f8f-759c116cd22a/spatial+audio+plguin+2d.png" data-image-dimensions="964x721" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/5d784c27-b4d5-4836-9f8f-759c116cd22a/spatial+audio+plguin+2d.png" width="964" height="721" sizes="100vw" srcset="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/5d784c27-b4d5-4836-9f8f-759c116cd22a/spatial+audio+plguin+2d.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/5d784c27-b4d5-4836-9f8f-759c116cd22a/spatial+audio+plguin+2d.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/5d784c27-b4d5-4836-9f8f-759c116cd22a/spatial+audio+plguin+2d.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/5d784c27-b4d5-4836-9f8f-759c116cd22a/spatial+audio+plguin+2d.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/5d784c27-b4d5-4836-9f8f-759c116cd22a/spatial+audio+plguin+2d.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/5d784c27-b4d5-4836-9f8f-759c116cd22a/spatial+audio+plguin+2d.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/5d784c27-b4d5-4836-9f8f-759c116cd22a/spatial+audio+plguin+2d.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs"></p>
          
              </div><div data-animation-role="image" data-test="image-block-fluid-outer-wrapper" data-block-type="5" id="block-949e0332446ec4ad2aa6">
                
            
            
            
            
            
            
            <p><img data-stretch="true" data-src="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/51fb8d27-5111-46a2-ab8a-5f7807244435/audio+range+editor.png" data-image="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/51fb8d27-5111-46a2-ab8a-5f7807244435/audio+range+editor.png" data-image-dimensions="877x638" data-image-focal-point="0.7242697242697242,0.6080760095011877" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/51fb8d27-5111-46a2-ab8a-5f7807244435/audio+range+editor.png" width="877" height="638" sizes="100vw" srcset="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/51fb8d27-5111-46a2-ab8a-5f7807244435/audio+range+editor.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/51fb8d27-5111-46a2-ab8a-5f7807244435/audio+range+editor.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/51fb8d27-5111-46a2-ab8a-5f7807244435/audio+range+editor.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/51fb8d27-5111-46a2-ab8a-5f7807244435/audio+range+editor.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/51fb8d27-5111-46a2-ab8a-5f7807244435/audio+range+editor.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/51fb8d27-5111-46a2-ab8a-5f7807244435/audio+range+editor.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/51fb8d27-5111-46a2-ab8a-5f7807244435/audio+range+editor.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs"></p>
          
              </div><div data-blend-mode="NORMAL" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:20.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:20.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:20.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:20.0}}" id="block-70fa9a9f9142a94c399b">
  <h4><span>Audiocube provides </span><span><span><strong>total spatial freedom</strong></span></span></h4><p><span>A dedicated 3D audio engine gives you  a level of control, depth, and freedom that 2D software can’t.</span></p><p><span>✅ Total freedom of source and listener placement<br>✅ Natural &amp; customizable acoustic simulation<br>✅ Quick and easy for 3D audio creation</span></p>
</div><div data-blend-mode="NORMAL" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:20.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:20.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:20.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:20.0}}" id="block-yui_3_17_2_1_1737122899196_102143">
  <h4><span>2D interfaces are bad for 3D audio production</span></h4><p><span> 2D DAWs and plugins don’t have the level of depth and freedom you need to fulfill your creative vision</span></p><p><span>❌ VSTs only offer a fixed listening perspective <br>❌ 2D software creates a barrier between creativity <br>❌ Fiddly, difficult, and time consuming </span></p>
</div></div>

  
    
    


  
  


<div data-blend-mode="NORMAL" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-2d08f58c542ffcd69821" data-fluid-engine="true" data-test="page-section" data-section-theme="black-bold" data-section-id="678a857b6dcf924e58753f83" data-controller="SectionWrapperController" data-current-styles="{
                      &quot;imageOverlayOpacity&quot;: 0.15,
                      &quot;backgroundWidth&quot;: &quot;background-width--full-bleed&quot;,
                      &quot;sectionHeight&quot;: &quot;section-height--custom&quot;,
                      &quot;customSectionHeight&quot;: 1,
                      &quot;horizontalAlignment&quot;: &quot;horizontal-alignment--center&quot;,
                      &quot;verticalAlignment&quot;: &quot;vertical-alignment--middle&quot;,
                      &quot;contentWidth&quot;: &quot;content-width--wide&quot;,
                      &quot;customContentWidth&quot;: 50,
                      &quot;backgroundColor&quot;: &quot;&quot;,
                      &quot;sectionTheme&quot;: &quot;black-bold&quot;,
                      &quot;sectionAnimation&quot;: &quot;none&quot;,
                      &quot;backgroundMode&quot;: &quot;image&quot;
                    }" data-current-context="{
                      &quot;video&quot;: {
                        &quot;playbackSpeed&quot;: 0.5,
                        &quot;filter&quot;: 1,
                        &quot;filterStrength&quot;: 0,
                        &quot;zoom&quot;: 0,
                        &quot;videoSourceProvider&quot;: &quot;none&quot;
                      },
                      &quot;backgroundImageId&quot;: null,
                      &quot;backgroundMediaEffect&quot;: {
                        &quot;type&quot;: &quot;none&quot;
                      },
                      &quot;divider&quot;: {
                        &quot;enabled&quot;: false
                      },
                      &quot;typeName&quot;: &quot;page&quot;
                    }" data-animation="none" data-fluid-engine-section="">
  <h3>How To Easily Spatialize Audio In 2 Minutes</h3><p>Audiocube is a 3D audio engine that expands your immersive audio workflow with a deep set of spatial and experimental tools. </p><p><strong>Stop wasting time</strong> with fiddly VST plugins and 2D DAWs</p><p><span>Try it free!</span></p>
</div>

  
    
    


  
  


<div data-fluid-engine="true" data-test="page-section" data-section-theme="black-bold" data-section-id="678a8a05e5a1d940b92f8c89" data-controller="SectionWrapperController" data-current-styles="{
                      &quot;imageOverlayOpacity&quot;: 0.15,
                      &quot;backgroundWidth&quot;: &quot;background-width--full-bleed&quot;,
                      &quot;sectionHeight&quot;: &quot;section-height--custom&quot;,
                      &quot;customSectionHeight&quot;: 1,
                      &quot;horizontalAlignment&quot;: &quot;horizontal-alignment--center&quot;,
                      &quot;verticalAlignment&quot;: &quot;vertical-alignment--middle&quot;,
                      &quot;contentWidth&quot;: &quot;content-width--wide&quot;,
                      &quot;customContentWidth&quot;: 50,
                      &quot;backgroundColor&quot;: &quot;&quot;,
                      &quot;sectionTheme&quot;: &quot;black-bold&quot;,
                      &quot;sectionAnimation&quot;: &quot;none&quot;,
                      &quot;backgroundMode&quot;: &quot;image&quot;
                    }" data-current-context="{
                      &quot;video&quot;: {
                        &quot;playbackSpeed&quot;: 0.5,
                        &quot;filter&quot;: 1,
                        &quot;filterStrength&quot;: 0,
                        &quot;zoom&quot;: 0,
                        &quot;videoSourceProvider&quot;: &quot;none&quot;
                      },
                      &quot;backgroundImageId&quot;: null,
                      &quot;backgroundMediaEffect&quot;: {
                        &quot;type&quot;: &quot;none&quot;
                      },
                      &quot;divider&quot;: {
                        &quot;enabled&quot;: false
                      },
                      &quot;typeName&quot;: &quot;page&quot;
                    }" data-animation="none" data-fluid-engine-section=""><div data-animation-role="image" data-test="image-block-fluid-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1737122899196_270664">
                
            
            
            
            
            
            
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/35bee2b7-27a7-4fb1-bd29-b70f47baa29d/Library+Panel+copy.jpg" data-image="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/35bee2b7-27a7-4fb1-bd29-b70f47baa29d/Library+Panel+copy.jpg" data-image-dimensions="582x496" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/35bee2b7-27a7-4fb1-bd29-b70f47baa29d/Library+Panel+copy.jpg" width="582" height="496" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 37.5vw" srcset="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/35bee2b7-27a7-4fb1-bd29-b70f47baa29d/Library+Panel+copy.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/35bee2b7-27a7-4fb1-bd29-b70f47baa29d/Library+Panel+copy.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/35bee2b7-27a7-4fb1-bd29-b70f47baa29d/Library+Panel+copy.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/35bee2b7-27a7-4fb1-bd29-b70f47baa29d/Library+Panel+copy.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/35bee2b7-27a7-4fb1-bd29-b70f47baa29d/Library+Panel+copy.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/35bee2b7-27a7-4fb1-bd29-b70f47baa29d/Library+Panel+copy.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/35bee2b7-27a7-4fb1-bd29-b70f47baa29d/Library+Panel+copy.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs"></p>
          
              </div><div data-blend-mode="NORMAL" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-49b95a2e9aa84b35a9b5">

<p>
  <h4>Import Your Audio Files</h4>
</p>




















  
  



</div><div data-blend-mode="NORMAL" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-6e8faf3da0186b7ba764">

<p>Instantly fill your audio library with your own file collection, or download our included 2GB+ (and growing) custom sample packs  </p>




















  
  



</div><div data-blend-mode="NORMAL" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-ae57d85ea8a1139c46cb">
  <p><strong>📁 Flexible Browser</strong></p><p>Easily import, manage, and browse your audio file collection</p>
</div><div data-blend-mode="NORMAL" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-bc5529ee44fbae9c4006">
  <p><strong>📯2GB+ Samples Included </strong></p><p>Download our collections of custom made samples, including synths, drums, textures, field recordings and more…</p>
</div></div>

  
    
    


  
  


<div data-fluid-engine="true" data-test="page-section" data-section-theme="black-bold" data-section-id="678a8c09b9b359721e0e5a4a" data-controller="SectionWrapperController" data-current-styles="{
                      &quot;imageOverlayOpacity&quot;: 0.15,
                      &quot;backgroundWidth&quot;: &quot;background-width--full-bleed&quot;,
                      &quot;sectionHeight&quot;: &quot;section-height--custom&quot;,
                      &quot;customSectionHeight&quot;: 1,
                      &quot;horizontalAlignment&quot;: &quot;horizontal-alignment--center&quot;,
                      &quot;verticalAlignment&quot;: &quot;vertical-alignment--middle&quot;,
                      &quot;contentWidth&quot;: &quot;content-width--wide&quot;,
                      &quot;customContentWidth&quot;: 50,
                      &quot;backgroundColor&quot;: &quot;&quot;,
                      &quot;sectionTheme&quot;: &quot;black-bold&quot;,
                      &quot;sectionAnimation&quot;: &quot;none&quot;,
                      &quot;backgroundMode&quot;: &quot;image&quot;
                    }" data-current-context="{
                      &quot;video&quot;: {
                        &quot;playbackSpeed&quot;: 0.5,
                        &quot;filter&quot;: 1,
                        &quot;filterStrength&quot;: 0,
                        &quot;zoom&quot;: 0,
                        &quot;videoSourceProvider&quot;: &quot;none&quot;
                      },
                      &quot;backgroundImageId&quot;: null,
                      &quot;backgroundMediaEffect&quot;: {
                        &quot;type&quot;: &quot;none&quot;
                      },
                      &quot;divider&quot;: {
                        &quot;enabled&quot;: false
                      },
                      &quot;typeName&quot;: &quot;page&quot;
                    }" data-animation="none" data-fluid-engine-section=""><div data-animation-role="image" data-test="image-block-fluid-outer-wrapper" data-aspect-ratio="81.60929214712567" data-block-type="5" id="block-69b804bc29fe2fb398d0">
                
            
            
            
            
            
            
            <p><img data-stretch="true" data-src="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/83273785-1dd3-4988-8f5f-823602f4fe73/logic+boxes+new+audiocube+device.jpg" data-image="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/83273785-1dd3-4988-8f5f-823602f4fe73/logic+boxes+new+audiocube+device.jpg" data-image-dimensions="1451x602" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/83273785-1dd3-4988-8f5f-823602f4fe73/logic+boxes+new+audiocube+device.jpg" width="1451" height="602" sizes="100vw" srcset="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/83273785-1dd3-4988-8f5f-823602f4fe73/logic+boxes+new+audiocube+device.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/83273785-1dd3-4988-8f5f-823602f4fe73/logic+boxes+new+audiocube+device.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/83273785-1dd3-4988-8f5f-823602f4fe73/logic+boxes+new+audiocube+device.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/83273785-1dd3-4988-8f5f-823602f4fe73/logic+boxes+new+audiocube+device.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/83273785-1dd3-4988-8f5f-823602f4fe73/logic+boxes+new+audiocube+device.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/83273785-1dd3-4988-8f5f-823602f4fe73/logic+boxes+new+audiocube+device.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/83273785-1dd3-4988-8f5f-823602f4fe73/logic+boxes+new+audiocube+device.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs"></p>
          
              </div><div data-blend-mode="NORMAL" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-f67d3eb298a491f7a3cf">

<p>Create sound sources and devices, and place them exactly where you want them</p>




















  
  



</div><div data-blend-mode="NORMAL" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-7c6d61c30c2598345998">

<p>
  <h4>Place Sounds &amp; Devices</h4>
</p>




















  
  



</div><div data-blend-mode="NORMAL" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-e9d47b3f37bfe482d876">
  <p><strong>🔊 </strong><a href="https://www.audiocube.app/features#:~:text=Seven%20Powerful%20Devices" target="_blank"><strong>7 Unique Device </strong></a><strong>Types</strong></p><p>Samplers, Emitters, Tickers, Logic Boxes, Ambience Nodes, FX Zones, Soundwalls each provide powerful functionality. </p>
</div><div data-blend-mode="NORMAL" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-ac17c184cb447c409dae">
  <p><span><strong>✥</strong></span><strong>Total Placement Freedom</strong></p><p>Place your sources and listening position wherever you want in the scene. Automate their movement over time. </p>
</div></div>

  
    
    


  
  


<div data-fluid-engine="true" data-test="page-section" data-section-theme="black-bold" data-section-id="678d2e6abcfe336df342ff30" data-controller="SectionWrapperController" data-current-styles="{
                      &quot;imageOverlayOpacity&quot;: 0.15,
                      &quot;backgroundWidth&quot;: &quot;background-width--full-bleed&quot;,
                      &quot;sectionHeight&quot;: &quot;section-height--custom&quot;,
                      &quot;customSectionHeight&quot;: 1,
                      &quot;horizontalAlignment&quot;: &quot;horizontal-alignment--center&quot;,
                      &quot;verticalAlignment&quot;: &quot;vertical-alignment--middle&quot;,
                      &quot;contentWidth&quot;: &quot;content-width--wide&quot;,
                      &quot;customContentWidth&quot;: 50,
                      &quot;backgroundColor&quot;: &quot;&quot;,
                      &quot;sectionTheme&quot;: &quot;black-bold&quot;,
                      &quot;sectionAnimation&quot;: &quot;none&quot;,
                      &quot;backgroundMode&quot;: &quot;image&quot;
                    }" data-current-context="{
                      &quot;video&quot;: {
                        &quot;playbackSpeed&quot;: 0.5,
                        &quot;filter&quot;: 1,
                        &quot;filterStrength&quot;: 0,
                        &quot;zoom&quot;: 0,
                        &quot;videoSourceProvider&quot;: &quot;none&quot;
                      },
                      &quot;backgroundImageId&quot;: null,
                      &quot;backgroundMediaEffect&quot;: {
                        &quot;type&quot;: &quot;none&quot;
                      },
                      &quot;divider&quot;: {
                        &quot;enabled&quot;: false
                      },
                      &quot;typeName&quot;: &quot;page&quot;
                    }" data-animation="none" data-fluid-engine-section=""><div data-animation-role="image" data-test="image-block-fluid-outer-wrapper" data-block-type="5" id="block-5fe8a804ebee8afaac92">
                
            
            
            
            
            
            
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/439438ff-ce9a-4152-acd9-36ecbff00631/spatial+acoustics+editor.png" data-image="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/439438ff-ce9a-4152-acd9-36ecbff00631/spatial+acoustics+editor.png" data-image-dimensions="467x761" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/439438ff-ce9a-4152-acd9-36ecbff00631/spatial+acoustics+editor.png" width="467" height="761" sizes="(max-width: 640px) 100vw, (max-width: 767px) 50vw, 25vw" srcset="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/439438ff-ce9a-4152-acd9-36ecbff00631/spatial+acoustics+editor.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/439438ff-ce9a-4152-acd9-36ecbff00631/spatial+acoustics+editor.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/439438ff-ce9a-4152-acd9-36ecbff00631/spatial+acoustics+editor.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/439438ff-ce9a-4152-acd9-36ecbff00631/spatial+acoustics+editor.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/439438ff-ce9a-4152-acd9-36ecbff00631/spatial+acoustics+editor.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/439438ff-ce9a-4152-acd9-36ecbff00631/spatial+acoustics+editor.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/439438ff-ce9a-4152-acd9-36ecbff00631/spatial+acoustics+editor.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs"></p>
          
              </div><div data-blend-mode="NORMAL" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-8962350ae11a03f54005">

<p>
  <h4>Adjust the Acoustic Engine</h4>
</p>




















  
  



</div><div data-blend-mode="NORMAL" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-f034be6b067557ffd273">

<p>Tweak and customize the acoustic simulation algorithm to capture the type of spatialization and sonic rendering required for your project. </p>




















  
  



</div><div data-blend-mode="NORMAL" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-a40b58f437cd0fc9be57">
  <p><strong>🔊 Advanced Acoustics</strong></p><p>Get full control over how the acoustic engine processes reflections, occlusion, air absortion, depth, and more.</p>
</div><div data-blend-mode="NORMAL" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-f48433f9de4c5f5993dc">
  <p><strong>🎧 HRTF Binauralization</strong></p><p>Use a detailed Binaural process to capture a perfectly immersive spatial audio scene for headphone listening. </p>
</div></div>

  
    
    


  
  


<div data-fluid-engine="true" data-test="page-section" data-section-theme="black-bold" data-section-id="678d3230a5264926a5cf2fc0" data-controller="SectionWrapperController" data-current-styles="{
                      &quot;imageOverlayOpacity&quot;: 0.15,
                      &quot;backgroundWidth&quot;: &quot;background-width--full-bleed&quot;,
                      &quot;sectionHeight&quot;: &quot;section-height--custom&quot;,
                      &quot;customSectionHeight&quot;: 1,
                      &quot;horizontalAlignment&quot;: &quot;horizontal-alignment--center&quot;,
                      &quot;verticalAlignment&quot;: &quot;vertical-alignment--middle&quot;,
                      &quot;contentWidth&quot;: &quot;content-width--wide&quot;,
                      &quot;customContentWidth&quot;: 50,
                      &quot;backgroundColor&quot;: &quot;&quot;,
                      &quot;sectionTheme&quot;: &quot;black-bold&quot;,
                      &quot;sectionAnimation&quot;: &quot;none&quot;,
                      &quot;backgroundMode&quot;: &quot;image&quot;
                    }" data-current-context="{
                      &quot;video&quot;: {
                        &quot;playbackSpeed&quot;: 0.5,
                        &quot;filter&quot;: 1,
                        &quot;filterStrength&quot;: 0,
                        &quot;zoom&quot;: 0,
                        &quot;videoSourceProvider&quot;: &quot;none&quot;
                      },
                      &quot;backgroundImageId&quot;: null,
                      &quot;backgroundMediaEffect&quot;: {
                        &quot;type&quot;: &quot;none&quot;
                      },
                      &quot;divider&quot;: {
                        &quot;enabled&quot;: false
                      },
                      &quot;typeName&quot;: &quot;page&quot;
                    }" data-animation="none" data-fluid-engine-section=""><div data-animation-role="image" data-test="image-block-fluid-outer-wrapper" data-block-type="5" id="block-c80ede10f1827c6ada43">
                
            
            
            
            
            
            
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/ec1233c3-b421-4206-9be9-05b91028e9b4/record+in+progress.jpg" data-image="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/ec1233c3-b421-4206-9be9-05b91028e9b4/record+in+progress.jpg" data-image-dimensions="855x334" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/ec1233c3-b421-4206-9be9-05b91028e9b4/record+in+progress.jpg" width="855" height="334" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 29.166666666666668vw" srcset="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/ec1233c3-b421-4206-9be9-05b91028e9b4/record+in+progress.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/ec1233c3-b421-4206-9be9-05b91028e9b4/record+in+progress.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/ec1233c3-b421-4206-9be9-05b91028e9b4/record+in+progress.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/ec1233c3-b421-4206-9be9-05b91028e9b4/record+in+progress.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/ec1233c3-b421-4206-9be9-05b91028e9b4/record+in+progress.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/ec1233c3-b421-4206-9be9-05b91028e9b4/record+in+progress.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/ec1233c3-b421-4206-9be9-05b91028e9b4/record+in+progress.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs"></p>
          
              </div><div data-blend-mode="NORMAL" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-9ff9f6e975ed6446369f">

<p>Use a flexible mixer and mastering effects for some final coloration, then export your finished audio. </p>




















  
  



</div><div data-blend-mode="NORMAL" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-7d918bcc184e24461e87">

<p>
  <h4>Mix, Master, and Export</h4>
</p>




















  
  



</div><div data-blend-mode="NORMAL" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:10.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:10.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:10.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:10.0}}" id="block-594b8498682a4f688a7d">

<p><span><em>“Your binauralization algorithm sounds incredible! It sounds so much more natural than Dear VR PRo's HRTF VST."</em> </span></p>




















  
  
    
  



</div><div data-animation-role="image" data-test="image-block-fluid-outer-wrapper" data-block-type="5" id="block-5765cd9957d5028d9c8b">
                
            
            
            
            
            
            
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/e661609d-2e66-4206-9ef7-e071010d8358/Audiocube+Effects.png" data-image="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/e661609d-2e66-4206-9ef7-e071010d8358/Audiocube+Effects.png" data-image-dimensions="935x1031" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/e661609d-2e66-4206-9ef7-e071010d8358/Audiocube+Effects.png" width="935" height="1031" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 20.833333333333336vw" srcset="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/e661609d-2e66-4206-9ef7-e071010d8358/Audiocube+Effects.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/e661609d-2e66-4206-9ef7-e071010d8358/Audiocube+Effects.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/e661609d-2e66-4206-9ef7-e071010d8358/Audiocube+Effects.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/e661609d-2e66-4206-9ef7-e071010d8358/Audiocube+Effects.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/e661609d-2e66-4206-9ef7-e071010d8358/Audiocube+Effects.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/e661609d-2e66-4206-9ef7-e071010d8358/Audiocube+Effects.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/e661609d-2e66-4206-9ef7-e071010d8358/Audiocube+Effects.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs"></p>
          
              </div></div>

  
    
    


  
  


<div data-fluid-engine="true" data-test="page-section" data-section-theme="black" data-section-id="678a87ee7226647bfd45f84e" data-controller="SectionWrapperController" data-current-styles="{
                      &quot;imageOverlayOpacity&quot;: 0.15,
                      &quot;backgroundWidth&quot;: &quot;background-width--full-bleed&quot;,
                      &quot;sectionHeight&quot;: &quot;section-height--custom&quot;,
                      &quot;customSectionHeight&quot;: 1,
                      &quot;horizontalAlignment&quot;: &quot;horizontal-alignment--center&quot;,
                      &quot;verticalAlignment&quot;: &quot;vertical-alignment--middle&quot;,
                      &quot;contentWidth&quot;: &quot;content-width--wide&quot;,
                      &quot;customContentWidth&quot;: 50,
                      &quot;backgroundColor&quot;: &quot;&quot;,
                      &quot;sectionTheme&quot;: &quot;black&quot;,
                      &quot;sectionAnimation&quot;: &quot;none&quot;,
                      &quot;backgroundMode&quot;: &quot;image&quot;
                    }" data-current-context="{
                      &quot;video&quot;: {
                        &quot;playbackSpeed&quot;: 0.5,
                        &quot;filter&quot;: 1,
                        &quot;filterStrength&quot;: 0,
                        &quot;zoom&quot;: 0,
                        &quot;videoSourceProvider&quot;: &quot;none&quot;
                      },
                      &quot;backgroundImageId&quot;: null,
                      &quot;backgroundMediaEffect&quot;: {
                        &quot;type&quot;: &quot;none&quot;
                      },
                      &quot;divider&quot;: {
                        &quot;enabled&quot;: false
                      },
                      &quot;typeName&quot;: &quot;page&quot;
                    }" data-animation="none" data-fluid-engine-section=""><div data-blend-mode="NORMAL" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1737122899196_241143">
  <p><span><strong>Cross-Platform and Fully Standalone</strong></span></p><h4><strong>Easy to Learn, with Powerful Depth</strong></h4><p>Audiocube is easy to set up and requires no other software. It includes a 2GB+ sound library, an intuitive interface, and personal support. </p><p>Your purchase includes both Windows and Mac versions. </p><p><a href="https://www.audiocube.app/features" target="_blank">Discover More Features</a></p>
</div><div data-animation-role="image" data-test="image-block-fluid-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1737122899196_244829">
                
            
            
            
            
            
            
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/fb358681-16b0-491e-90c5-d82344c5f15a/light+overview++copy-min.png" data-image="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/fb358681-16b0-491e-90c5-d82344c5f15a/light+overview++copy-min.png" data-image-dimensions="3164x2062" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/fb358681-16b0-491e-90c5-d82344c5f15a/light+overview++copy-min.png" width="3164" height="2062" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 54.166666666666664vw" srcset="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/fb358681-16b0-491e-90c5-d82344c5f15a/light+overview++copy-min.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/fb358681-16b0-491e-90c5-d82344c5f15a/light+overview++copy-min.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/fb358681-16b0-491e-90c5-d82344c5f15a/light+overview++copy-min.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/fb358681-16b0-491e-90c5-d82344c5f15a/light+overview++copy-min.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/fb358681-16b0-491e-90c5-d82344c5f15a/light+overview++copy-min.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/fb358681-16b0-491e-90c5-d82344c5f15a/light+overview++copy-min.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/fb358681-16b0-491e-90c5-d82344c5f15a/light+overview++copy-min.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs"></p>
          
              </div></div>

  
    
    


  
  


<div data-fluid-engine="true" data-test="page-section" data-section-theme="black-bold" data-section-id="678d36703b4bfd0459b3fe3a" data-controller="SectionWrapperController" data-current-styles="{
                      &quot;imageOverlayOpacity&quot;: 0.15,
                      &quot;backgroundWidth&quot;: &quot;background-width--full-bleed&quot;,
                      &quot;sectionHeight&quot;: &quot;section-height--custom&quot;,
                      &quot;customSectionHeight&quot;: 1,
                      &quot;horizontalAlignment&quot;: &quot;horizontal-alignment--center&quot;,
                      &quot;verticalAlignment&quot;: &quot;vertical-alignment--middle&quot;,
                      &quot;contentWidth&quot;: &quot;content-width--wide&quot;,
                      &quot;customContentWidth&quot;: 50,
                      &quot;backgroundColor&quot;: &quot;&quot;,
                      &quot;sectionTheme&quot;: &quot;black-bold&quot;,
                      &quot;sectionAnimation&quot;: &quot;none&quot;,
                      &quot;backgroundMode&quot;: &quot;image&quot;
                    }" data-current-context="{
                      &quot;video&quot;: {
                        &quot;playbackSpeed&quot;: 0.5,
                        &quot;filter&quot;: 1,
                        &quot;filterStrength&quot;: 0,
                        &quot;zoom&quot;: 0,
                        &quot;videoSourceProvider&quot;: &quot;none&quot;
                      },
                      &quot;backgroundImageId&quot;: null,
                      &quot;backgroundMediaEffect&quot;: {
                        &quot;type&quot;: &quot;none&quot;
                      },
                      &quot;divider&quot;: {
                        &quot;enabled&quot;: false
                      },
                      &quot;typeName&quot;: &quot;page&quot;
                    }" data-animation="none" data-fluid-engine-section=""><div data-blend-mode="NORMAL" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-a54990e25a6283162856">

<p>
  <h3><strong>You’re 2 Minutes Away From Creating More Immersive Music</strong></h3>
</p>




















  
  



</div><div data-blend-mode="NORMAL" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-6939ba93509572acc20a">
  <p>Stand out from the crowd with <strong>deeper, immersive, and more natural sounding audio</strong>. Whether you’re making music or working on sound design don’t miss out on the realism you need.</p><p><strong>Expand your sound with Audiocube!<p>✅ No other software required - Mac or Windows</p></strong></p><p><strong>✅ Extensive tutorials and rapid support</strong></p><p><strong>✅ Try Audiocube For Free</strong></p>
</div><div data-block-type="23" id="block-00e233ab00ec649b9a13">


  <meta charset="UTF-8">
  <title>Enhanced Abstract Floating Popup</title>
  
  <!-- Font Awesome CDN for Icons -->
  
  
  <!-- Enhanced Popup Styles -->
  



  <!-- Enhanced Abstract Floating Popup Container -->
  

  <!-- JavaScript to Handle Close Button and Popup Logic -->
  


</div></div>

  
    
    


  
  


<div data-fluid-engine="true" data-test="page-section" data-section-theme="black-bold" data-section-id="67537cdecddaf7368e7332b9" data-controller="SectionWrapperController" data-current-styles="{
                      &quot;imageOverlayOpacity&quot;: 0.15,
                      &quot;backgroundWidth&quot;: &quot;background-width--full-bleed&quot;,
                      &quot;sectionHeight&quot;: &quot;section-height--custom&quot;,
                      &quot;customSectionHeight&quot;: 1,
                      &quot;horizontalAlignment&quot;: &quot;horizontal-alignment--center&quot;,
                      &quot;verticalAlignment&quot;: &quot;vertical-alignment--middle&quot;,
                      &quot;contentWidth&quot;: &quot;content-width--wide&quot;,
                      &quot;customContentWidth&quot;: 50,
                      &quot;sectionTheme&quot;: &quot;black-bold&quot;,
                      &quot;sectionAnimation&quot;: &quot;none&quot;,
                      &quot;backgroundMode&quot;: &quot;image&quot;
                    }" data-current-context="{
                      &quot;video&quot;: {
                        &quot;playbackSpeed&quot;: 0.5,
                        &quot;filter&quot;: 2,
                        &quot;filterStrength&quot;: 0,
                        &quot;zoom&quot;: 0,
                        &quot;videoSourceProvider&quot;: &quot;none&quot;
                      },
                      &quot;backgroundImageId&quot;: null,
                      &quot;backgroundMediaEffect&quot;: {
                        &quot;type&quot;: &quot;none&quot;
                      },
                      &quot;divider&quot;: {
                        &quot;enabled&quot;: false
                      },
                      &quot;typeName&quot;: &quot;page&quot;
                    }" data-animation="none" data-fluid-engine-section=""><div data-animation-role="image" data-test="image-block-fluid-outer-wrapper" data-aspect-ratio="104.98490439449849" data-block-type="5" id="block-yui_3_17_2_1_1737304558178_132097">
                
            
            
            
            
            
            
            <p><img data-stretch="true" data-src="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/3fc72628-1321-4a55-b232-e0edc137d951/fairlight+small+autoo+copy+2-min.png" data-image="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/3fc72628-1321-4a55-b232-e0edc137d951/fairlight+small+autoo+copy+2-min.png" data-image-dimensions="3164x2062" data-image-focal-point="0.13750101882794033,0.472059007375922" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/3fc72628-1321-4a55-b232-e0edc137d951/fairlight+small+autoo+copy+2-min.png" width="3164" height="2062" sizes="100vw" srcset="https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/3fc72628-1321-4a55-b232-e0edc137d951/fairlight+small+autoo+copy+2-min.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/3fc72628-1321-4a55-b232-e0edc137d951/fairlight+small+autoo+copy+2-min.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/3fc72628-1321-4a55-b232-e0edc137d951/fairlight+small+autoo+copy+2-min.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/3fc72628-1321-4a55-b232-e0edc137d951/fairlight+small+autoo+copy+2-min.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/3fc72628-1321-4a55-b232-e0edc137d951/fairlight+small+autoo+copy+2-min.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/3fc72628-1321-4a55-b232-e0edc137d951/fairlight+small+autoo+copy+2-min.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/65b3b3cd8a8cd17472028226/3fc72628-1321-4a55-b232-e0edc137d951/fairlight+small+autoo+copy+2-min.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs"></p>
          
              </div><div data-blend-mode="NORMAL" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-396619f85ac85b18b89f">
  <p><span><strong>Unlock a deeper level of creative freedom</strong></span></p><h2><strong>It’s Not Just Another VST Plugin…</strong></h2><p>Audiocube is a standalone 3D DAW, built with a custom audio, physics, and graphical engine - enabling more depth and control than any plugin. </p><p>Move your workflow away from a cluttered and compromised VST-based setup, and embrace a modern solution for immersive audio creation and exploration. </p>
</div></div>

  
</article>


          

          
            
          
        
      </main>
      
        

      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LibreOffice 400M Downloads, and Counting (197 pts)]]></title>
            <link>https://blog.documentfoundation.org/blog/2025/01/30/400-million-downloads-and-counting/</link>
            <guid>42876998</guid>
            <pubDate>Thu, 30 Jan 2025 11:59:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.documentfoundation.org/blog/2025/01/30/400-million-downloads-and-counting/">https://blog.documentfoundation.org/blog/2025/01/30/400-million-downloads-and-counting/</a>, See on <a href="https://news.ycombinator.com/item?id=42876998">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page">
		<article>
			<div id="content_box">
							
														<header>
								
								
							</header><!--.headline_area-->
							<div>
								<p><a href="https://blog.documentfoundation.org/wp-content/uploads/2025/01/libreofficedownloadsperyear.png"><img fetchpriority="high" decoding="async" src="https://blog.documentfoundation.org/wp-content/uploads/2025/01/libreofficedownloadsperyear.png" alt="" width="761" height="410" srcset="https://blog.documentfoundation.org/wp-content/uploads/2025/01/libreofficedownloadsperyear.png 761w, https://blog.documentfoundation.org/wp-content/uploads/2025/01/libreofficedownloadsperyear-300x162.png 300w" sizes="(max-width: 761px) 100vw, 761px"></a></p>
<p>The histogram says it all.</p>
<p>First, rapid growth between 2011 and 2014 to 30 million downloads, despite the fierce hostility of the project created to kill LibreOffice.</p>
<p>Then a few years of stagnation, at a time when it seemed that desktop office suites were destined to die, and fashion was driving users to the cloud.</p>
<p>Then the upswing, when even the most fashionable users realised that desktop office suites would never die and would coexist with the cloud.</p>
<p>In 2019, a series of attacks on the download counter – no data is collected other than the click on the DOWNLOAD button – led to a barely credible increase (the figure you see has already been cleaned up as much as possible).</p>
<p>After 2019, a slow but inexorable growth to over 35 million downloads – and 400 million downloads since 2011, with an average of 28.6 million downloads per year – in 2024.</p>
<p>Thanks to everyone: those who developed LibreOffice, those who helped improve it, and those who downloaded it to use it.</p>
<h3><a href="https://www.libreoffice.org/download/">Click here to download LibreOffice</a></h3>
																							</div>
						</div>
		</article>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Interview with DeepSeek Founder: We're Done Following. It's Time to Lead (120 pts)]]></title>
            <link>https://thechinaacademy.org/interview-with-deepseek-founder-were-done-following-its-time-to-lead/</link>
            <guid>42876940</guid>
            <pubDate>Thu, 30 Jan 2025 11:47:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thechinaacademy.org/interview-with-deepseek-founder-were-done-following-its-time-to-lead/">https://thechinaacademy.org/interview-with-deepseek-founder-were-done-following-its-time-to-lead/</a>, See on <a href="https://news.ycombinator.com/item?id=42876940">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>Try Premium Member for Free with a <span>7-Day</span> Trial</p></div><h2>Editor’s Note:</h2><p>Silicon Valley is reeling. A seismic shift in AI dominance is underway, and all eyes are on China. In January 2025, DeepSeek-R1—an open-source inference model from Chinese AI firm DeepSeek—sent shockwaves through the tech world by matching OpenAI’s top-tier performance at 1/30th the API cost, all while embracing full openness.</p><p>With just $6 million, China built one of the world’s finest AI models, dwarfing the billions spent by Meta, Google, and Microsoft. Already, global users—especially individuals and SMEs—are flocking to DeepSeek-R1, retraining it as their foundational model.</p><p>This Eastern-led revolution is forcing a global reckoning: What if AI’s future isn’t forged in Silicon Valley?</p><p>Translated from a July 2024 interview with DeepSeek founder Liang Wenfeng—conducted shortly after the company’s open-source V2 model catapulted it to fame—this rare dialogue unveils how a Chinese startup dared to leapfrog giants and redefine innovation’s rules.</p><p><img decoding="async" src="https://thechinaacademy.org/wp-content/uploads/2025/01/33412227540b40718a0eaf36d96890d3_batchwm.png" alt="" data-href=""><span>This post-85s entrepreneur appeared on Xinwen Lianbo (CCTV News) as the founder of the AI startup DeepSeek, participating in a high-level national symposium and delivering a speech.<br></span></p><h2>How Was the First Shot in the Price War Fired?</h2><p><strong>An Yong (Interviewer)</strong>: After the release of the DeepSeek V2 model, it quickly triggered a fierce price war in the large model industry. Some say you are a disruptor in the market.</p><p><strong>Liang Wenfeng (DeepSeek Founder)</strong>: We never intended to be a disruptor; it just happened by accident.</p><p><strong>An Yong: </strong>Were you surprised by this outcome?</p><p><strong>Liang Wenfeng:</strong> Very surprised. We didn’t expect pricing to be such a sensitive issue. We were simply following our own pace, calculating costs, and setting prices accordingly. <strong>Our principle is neither to sell at a loss nor to seek excessive profits. </strong>The current pricing allows for a modest profit margin above our costs.</p><p><strong>An Yong: </strong>Five days later, Zhipu AI followed suit, and soon after, ByteDance, Alibaba, Baidu, and Tencent joined the race.</p><p><strong>Liang Wenfeng: </strong>Zhipu AI lowered prices for an entry-level product, while their flagship models remain expensive. ByteDance was the first to truly match our price for a flagship model, which then pressured others to follow. Since large companies have much higher model costs than us, we never imagined anyone would operate at a loss. It ended up mirroring the internet era’s subsidy-driven logic.</p><p><strong>An Yong:</strong> From an outsider’s perspective, price cuts seem like a tactic to grab users—typical of internet-era competition.</p><p><strong>Liang Wenfeng: </strong>Grabing users wasn’t our primary goal. We reduced prices because, <strong>first, while exploring next-generation model structures, our costs decreased; second, we believe that both AI and API services should be affordable and accessible to everyone.</strong></p><p><strong>An Yong: </strong>Before this, most Chinese companies simply copied the Llama model structure to develop applications. Why did you choose to focus on model structure instead?</p><p><strong>Liang Wenfeng:</strong> If the goal is to develop applications, adopting Llama’s structure to quickly launch a product is a reasonable choice. However, our goal is AGI (Artificial General Intelligence), which requires us to explore new model structures to achieve superior capabilities within limited resources. This is foundational research for scaling up. Beyond architecture, we’ve studied data curation and human-like reasoning—all reflected in our models. Also, Llama’s training efficiency and inference costs lag behind cutting-edge global standards by about two generations.</p><p><strong>An Yong:</strong> Where does this generational gap come from?</p><p><strong>Liang Wenfeng:</strong> First, there’s a gap in training efficiency. We estimate that China’s best models likely require twice the compute power to match top global models due to structural and training dynamics gaps. Data efficiency is also half as effective, meaning we need twice the data and compute for equivalent results. Combined, that’s four times the resources. Our goal is to continuously narrow these gaps.</p><p><strong>An Yong: </strong>Most Chinese firms pursue both models and applications. Why is DeepSeek focusing solely on research?</p><p><strong>Liang Wenfeng: </strong>Because we believe the most important thing right now is to participate global innovation. For years, Chinese companies have been accustomed to leveraging technological innovations developed elsewhere and monetizing them through applications. But this isn’t sustainable. <strong>This time, our goal isn’t quick profits but advancing the technological frontier to drive ecosystem growth.</strong></p><p><strong>An Yong: </strong>The prevailing belief from the internet and mobile internet eras is that the U.S. leads in innovation, while China excels at applications.</p><p><strong>Liang Wenfeng:</strong> We believe that with economic development, China must gradually transition from being a beneficiary to a contributor, rather than continuing to ride on the coattails of others. Over the past 30 years of the IT revolution, we barely participated in core tech innovation.</p><p>We’ve grown accustomed to Moore’s Law “falling from the sky”—waiting 18 months for better hardware and software. Scaling Law is treated similarly. However, these advancements are the result of generations of relentless effort by Western-led technology communities. Because we haven’t been actively involved in this process, we’ve come to overlook its significance.</p><h2>The Real Gap Lies in Originality, Not Just Time</h2><p><strong>An Yong: </strong>Why did DeepSeek V2 surprise many in Silicon Valley?</p><p><strong>Liang Wenfeng: </strong>Among the daily innovations in the U.S., this is quite ordinary. Their surprise stems from seeing a Chinese company join their game as an innovator, not just a follower—which is what most Chinese firms are accustomed to.</p><p><strong>An Yong: </strong>But in China’s context, prioritizing pure innovation seems almost a luxury. Developing large models is capital-intensive. Not every company can afford to focus solely on research without commercializing first.</p><p><strong>Liang Wenfeng: </strong>Innovation is undoubtedly costly, and <strong>our past tendency to adopt existing technologies was tied to China’s earlier developmental stage. But today, China’s economic scale and the profits of giants like ByteDance and Tencent are globally significant. What we lack isn’t capital but confidence and the ability to organize high-caliber talent for effective innovation.</strong></p><p><strong>An Yong: </strong>Why do Chinese companies, even well-funded giants, often prioritize rapid commercialization?</p><p><strong>Liang Wenfeng: </strong>For three decades, we’ve emphasized profit over innovation. Innovation isn’t purely business-driven; it requires curiosity and creative ambition.<strong> We’re shackled by old habits, but this is a phase.</strong></p><p><strong>An Yong:</strong> But DeepSeek is a business, not a nonprofit research lab. If you innovate and open-source your breakthroughs—like the MLA architecture innovation releasing in May—won’t competitors quickly copy them? Where’s your moat?</p><p><strong>Liang Wenfeng:</strong> In disruptive tech, closed-source moats are fleeting. Even OpenAI’s closed-source model can’t prevent others from catching up.</p><p>Therefore, our real moat lies in our team’s growth—accumulating know-how, fostering an innovative culture. Open-sourcing and publishing papers don’t result in significant losses. <strong>For technologists, being followed is rewarding. Open-source is cultural, not just commercial. Giving back is an honor, and it attracts talent.</strong></p><p><strong>An Yong: </strong>How do you respond to market-driven views like those of Zhu Xiaohu (who advocates prioritizing immediate commercialization over foundational AI research, dismisses AGI as impractical)?</p><p><strong>Liang Wenfeng: </strong>Zhu’s logic suits short-term profit ventures, but the most enduringly profitable U.S. companies are tech giants built on long-term R&amp;D.</p><p><strong>An Yong: </strong>But in AI, pure technical lead isn’t enough. What larger goal is DeepSeek betting on?</p><p><strong>Liang Wenfeng: </strong>We believe that China’s AI cannot remain a follower forever. Often, we say there’s a one- or two-year gap between Chinese and American AI, but the real gap is between originality and imitation. If this doesn’t change, China will always be a follower. Some explorations are unavoidable.</p><p><strong>NVIDIA’s dominance isn’t just its effort—it’s the result of Western tech ecosystems collaborating on roadmaps for next-gen tech. China needs similar ecosystems. Many domestic chips fail because they lack supportive tech communities and rely on secondhand insights. Someone must step onto the frontier.</strong></p><h2>More Investment Doesn’t Always Fuel More Innovation</h2><p><strong>An Yong: </strong>DeepSeek currently exudes an idealistic vibe reminiscent of OpenAI’s early days, and you’re open-source. Do you plan to transition to a closed-source model in the future, as OpenAI and Mistral have done?</p><p><strong>Liang Wenfeng:</strong> <strong>We won’t go closed-source. We believe that establishing a robust technology ecosystem matters more.</strong></p><p><strong>An Yong: </strong>Are there fundraising plans? Media reports suggest Huanfang<a href="#references" target="" rel="noopener">【1】</a> &nbsp;aims to spin off DeepSeek for an IPO. Silicon Valley AI startups inevitably align with big players—will you follow?.</p><p><strong>Liang Wenfeng:</strong> <strong>No short-term plans. Our challenge has never been money; it’s the embargo on high-end chips.</strong></p><p><strong>An Yong: </strong>Many argue AGI requires bold alliances and visibility, unlike quantitative investing, which thrives in secrecy. Do you agree?</p><p><strong>Liang Wenfeng: </strong>More investment doesn’t necessarily result in more innovation. If that were the case, big tech companies would have monopolized all innovation.</p><p><strong>An Yong: </strong>Are you avoiding applications because DeepSeek lacks operational expertise?</p><p><strong>Liang Wenfeng: </strong>We believe that the current stage is a period of technological innovation, not application explosion. In the long term, we aim to establish an ecosystem where the industry directly uses our technologies and outputs. Others develop B2B/B2C services on our models while we focus on foundational research. If a complete industry chain forms, there’s no need for us to develop applications ourselves. That said, if necessary, we are fully capable of doing so. However, research and innovation will always remain our top priority.</p><p><strong>An Yong: </strong>Why would clients choose DeepSeek’s API over big players’?</p><p><strong>Liang Wenfeng: </strong>The future world will likely be one of specialized division of labor. Foundational AI models require continuous innovation, and big companies have their limits—they may not always be the best fit for this role.</p><p><strong>An Yong:</strong> But can technology alone create a significant competitive gap? You’ve said there are no absolute “secrets.”</p><p><strong>Liang Wenfeng:</strong> <strong>Secrets don’t exist, but replication takes time and cost. NVIDIA GPUs have no hidden magic—yet catching up requires rebuilding teams and chasing their next-gen tech. That’s the real moat.</strong></p><p><strong>An Yong: </strong>After your price cuts, ByteDance was the first to follow, suggesting they felt threatened. How do you view the new competitive landscape between startups and giants?</p><p><strong>Liang Wenfeng: </strong>To be honest, we don’t really care about it. Lowering prices was just something we did along the way. Providing cloud services isn’t our main goal—achieving AGI is. So far, we haven’t seen any groundbreaking solutions. Giants have users, but their cash cows also shackle them, making them ripe for disruption.</p><p><strong>An Yong: </strong>What do you think the endgame looks like for the six other major AI startups in China?</p><p><strong>Liang Wenfeng: </strong>Maybe 2-3 survive. All are burning cash now. Those with clear focus and operational discipline will endure. Others will pivot. Value never vanishes; they will take on new forms.</p><p><strong>An Yong: </strong>What’s your core philosophy when it comes to competition?</p><p><strong>Liang Wenfeng: </strong>I focus on whether something elevates societal efficiency and whether we can find our strength in the industry value chain. As long as the ultimate goal boosts efficiency, it’s valid. Many aspects are just temporary phases—over-focusing on them will only lead to confusion.<br>V2 Model: Built Entirely by Homegrown Talent</p><p><strong>An Yong: </strong>ack Clark, former policy lead at OpenAI and co-founder of Anthropic, remarked that DeepSeek has hired “some of those inscrutable wizards” who built DeepSeek V2. What defines these people?</p><p><strong>Liang Wenfeng: </strong>No “inscrutable wizards” here—just fresh graduates from top universities, PhD candidates (even fourth- or fifth-year interns), and young talents with a few years of experience.</p><p><strong>An Yong: </strong>Many major AI companies are keen on recruiting talent from overseas. Some believe that the top 50 AI talents globally are unlikely to be working for Chinese companies. Where does your team come from?</p><p><strong>Liang Wenfeng: V2 was built entirely by domestic talent. The global top 50 might not be in China today, but we aim to cultivate our own.</strong></p><p><strong>An Yong: </strong>How did the MLA innovation emerge? We heard that the idea initially stemmed from a young researcher’s personal interest.</p><p><strong>Liang Wenfeng: </strong>After summarizing the key evolutionary patterns of the mainstream Attention architecture, he had a sudden inspiration to design an alternative. However, turning an idea into reality is a long journey. We assembled a team and spent months validating it.</p><p><strong>An Yong: </strong>This kind of organic creativity seems tied to your flat organizational structure. In Huanfang, you avoided top-down mandates. But for AGI—a high-uncertainty frontier—do you impose more management?</p><p><strong>Liang Wenfeng: </strong>DeepSeek remains entirely bottom-up. We also do not preassign roles; natural division of labor emerges. Everyone brings unique experiences and ideas, and they don’t need to be pushed. When they encounter challenges, they naturally pull others in for discussions. However, once an idea shows potential, we do allocate resources from the top down.</p><p><strong>An Yong: </strong>We’ve heard that DeepSeek operates with remarkable flexibility in allocating computing resources and personnel.</p><p><strong>Liang Wenfeng: </strong>There are no limits on accessing compute resources or team members. If someone has an idea, they can tap into our training clusters anytime without approval. Additionally, since we don’t have rigid hierarchical structures or departmental barriers, people can collaborate freely as long as there’s mutual interest.</p><p><strong>An Yong: </strong>Such loose management relies on hiring intensely driven individuals. It’s said that DeepSeek excels at identifying exceptional talent based on non-traditional criteria.</p><p><strong>Liang Wenfeng:</strong> Our hiring standards have always been based on passion and curiosity. Many of our team members have unique and interesting backgrounds. Their hunger for research far outweighs monetary concerns.</p><p><strong>An Yong: </strong>Transformer was born in Google’s AI Lab, and ChatGPT emerged from OpenAI. In your opinion, how do corporate AI labs differ from startups in fostering innovation?</p><p><strong>Liang Wenfeng:</strong> Whether it’s Google’s labs, OpenAI, or even AI labs at Chinese tech giants, they all provide significant value. The fact that OpenAI eventually delivered breakthroughs was partly historical chance.</p><p><strong>An Yong: </strong>So is innovation largely a matter of chance? Your office layout includes meeting rooms with doors that can be easily opened on both sides. Your colleagues mentioned that this design allows for “serendipity,” reminiscent of the Transformer story—where a passerby overheard a discussion and helped shape it into a universal framework.</p><p><strong>Liang Wenfeng: I believe innovation is, first and foremost, a matter of belief. Why is Silicon Valley so innovative? Because they dare to try. When ChatGPT debuted, China lacked confidence in frontier research. From investors to major tech firms, many felt the gap was too wide and focused instead on applications. But innovation requires confidence, and young people tend to have more of it.</strong></p><p><strong>An Yong: </strong>Unlike other AI companies that actively seek funding and media attention, DeepSeek remains relatively quiet. How do you ensure that DeepSeek becomes the top choice for people looking to work in AI?</p><p><strong>Liang Wenfeng: Because we are tackling the hardest problems. The most attractive thing for top-tier talent is the opportunity to solve the world’s toughest challenges. In fact, top talent in China is often underestimated because hardcore innovation is rare, which means they rarely get recognized. We offer what they crave.</strong></p><p><strong>An Yong: </strong>The recent OpenAI event did not feature GPT-5, leading many to believe that the industry’s technological curve is slowing down, and some have begun questioning Scaling Law. What’s your perspective?</p><p><strong>Liang Wenfeng:</strong> We remain optimistic. The industry’s progress is still in line with expectations. OpenAI isn’t divine; they can’t lead forever.</p><p><strong>An Yong:</strong> How long do you think it will take to achieve AGI? Before V2, you released code/math models and switched from dense to MoE<a href="#references" target="" rel="noopener">【2】</a> . What’s your roadmap?</p><p><strong>Liang Wenfeng:</strong> It could take two years, five years, or ten years—but it will happen within our lifetime. As for our roadmap, there’s no consensus even within our company. However, we are placing our bets on three directions:</p><p><strong>1. Mathematics and code,</strong> which serve as a natural testbed for AGI—much like Go, they are enclosed, verifiable systems where self-learning could lead to high intelligence.</p><p>2.	<strong>Multimodality</strong>, where the AI engages with the real world to learn.</p><p>3.	<strong>Natural language</strong> itself, which is fundamental to human-like intelligence.</p><p>We are open to all possibilities.</p><p><strong>An Yong:</strong> What do you envision as the endgame for large AI models?</p><p><strong>Liang Wenfeng:</strong> There will be specialized companies providing foundational models and services, forming a long value chain of specialized divisions. More players will emerge to meet society’s diverse needs on top of these foundations.</p><h2>All Strategies Are Products of the Past</h2><p><strong>An Yong: </strong>Over the past year, China’s large model startup landscape has seen many changes. For instance, Wang Huiwen<a href="#references" target="" rel="noopener">【3】</a> , who was highly active early on, exited midway, while newer entrants are beginning to differentiate themselves.</p><p><strong>Liang Wenfeng:</strong> Wang Huiwen took on all the losses himself, allowing others to exit unscathed. He made a decision that was most unfavorable to himself but beneficial to everyone else. I truly admire his integrity.</p><p><strong>An Yong: </strong>Where do you currently focus most of your energy?</p><p><strong>Liang Wenfeng: </strong>My main focus is on researching the next generation of large models. There are still many unresolved challenges.</p><p><strong>An Yong: </strong>Many other AI startups insist on balancing both model development and applications, since technical leads aren’t permanent. Why is DeepSeek confident in focusing solely on research? Is it because your models still lag?</p><p><strong>Liang Wenfeng: </strong>All strategies are products of the past generation and may not hold true in the future. Discussing AI’s future profitability using the commercial logic of the internet era is like comparing Tencent’s early days to General Electric or Coca-Cola—it’s essentially carving a boat to mark a sword’s position, an outdated approach.</p><p><strong>An Yong: </strong>Huanfang had strong technological and innovative genes, and its growth seemed relatively smooth. Is this why you remain optimistic?</p><p><strong>Liang Wenfeng: </strong>Huanfang, to some extent, strengthened our confidence in technology-driven innovation, but it wasn’t all smooth sailing. We went through a long accumulation process. People only saw what happened after 2015, but in reality, we had been working on it for 16 years.</p><p><strong>An Yong: </strong>Returning to original innovation: With the economy slowing and capital cooling, will this stifle groundbreaking R&amp;D?</p><p><strong>Liang Wenfeng: </strong>Not necessarily. The restructuring of China’s industrial landscape will increasingly rely on deep-tech innovation. As quick-profit opportunities vanish, more will embrace real innovation.</p><p><strong>An Yong: </strong>So you’re optimistic about this?</p><p><strong>Liang Wenfeng: </strong>I grew up in the 1980s in a fifth-tier city in Guangdong. My father was a primary school teacher. In the 1990s, there were plenty of opportunities to make money in Guangdong. Many parents would come to our home and argue that studying was useless. But looking back now, perspectives have changed. Making money isn’t as easy as it used to be—not even driving a taxi is a viable option anymore. Within just one generation, things have shifted.</p><p><strong>Hardcore innovation will only increase in the future. It’s not widely understood now because society as a whole needs to learn from reality. When this society starts celebrating the success of deep-tech innovators, collective perceptions will change. We just need more real-world examples and time to allow that process to unfold.</strong></p><p>References</p><div id="source_link"><p>【1】Huanfang: A quantitative investment firm and early DeepSeek backer.</p><p>【2】MoE: Mixture of Experts, an architecture that improves model efficiency by activating specialized subnetworks. &nbsp;</p><p>【3】 Wang Huiwen: Co-founder of Meituan, who briefly entered the AI race in 2023 before exiting.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[JavaScript Temporal Is Coming (1197 pts)]]></title>
            <link>https://developer.mozilla.org/en-US/blog/javascript-temporal-is-coming/</link>
            <guid>42876840</guid>
            <pubDate>Thu, 30 Jan 2025 11:28:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://developer.mozilla.org/en-US/blog/javascript-temporal-is-coming/">https://developer.mozilla.org/en-US/blog/javascript-temporal-is-coming/</a>, See on <a href="https://news.ycombinator.com/item?id=42876840">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main><article lang="en-US"><figure><img alt="JavaScript Temporal is coming title. A JavaScript logo, a clock graphic and a globe with orbiting bodies symbolizing calendars and time." src="https://developer.mozilla.org/en-US/blog/javascript-temporal-is-coming/featured.png" height="420" width="800"></figure><div><p>Implementations of the new JavaScript Temporal object are starting to be shipped in experimental releases of browsers.
This is big news for web developers because working with dates and times in JavaScript will be hugely simplified and modernized.</p>
<p>Applications that rely on scheduling, internationalization, or time-sensitive data will be able to use built-ins for efficient, precise and consistent dates, times, durations, and calendars.
We're a long way away from stable, cross-browser support, and there may be changes as implementations develop, but we can already take a look at Temporal as it stands now, why it's exciting, and what problems it solves.</p>
<p>To help you get up to speed, there are over <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Temporal">270 pages of Temporal docs on MDN</a> added this week, with detailed explanations and examples.</p></div><section aria-labelledby="what_is_javascript_temporal"><h2 id="what_is_javascript_temporal"><a href="#what_is_javascript_temporal">What is JavaScript Temporal?</a></h2><div><p>To understand Temporal, we can look at JavaScript's <code>Date</code> object.
When JavaScript was created in 1995, the <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Date"><code>Date</code></a> object was copied from Java's early, flawed <code>java.util.Date</code> implementation.
Java replaced this implementation in 1997, but JavaScript is stuck with the same API for almost 30 years, despite known problems.</p>
<p>The major issues with JavaScript's <code>Date</code> object are that it only supports the user's local time and UTC, and there's no time zone support.
Additionally, its parsing behavior is very unreliable, and <code>Date</code> itself is mutable, which can introduce hard-to-trace bugs.
There are other problems like calculations across Daylight Saving Time (DST) and historical calendar changes, which are notoriously difficult to work with.</p>
<p>All of these issues make working with dates and times in JavaScript complex and prone to bugs, which can have serious consequences for some systems.
Most developers rely on dedicated libraries like <a href="https://momentjs.com/" target="_blank">Moment.js</a> and <a href="https://date-fns.org/" target="_blank">date-fns</a> for better handling of dates and times in their applications.</p>
<p>Temporal is designed as a full replacement for the <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Date"><code>Date</code></a> object, making date and time management reliable and predictable.
Temporal adds support for time zone and calendar representations, many built-in methods for conversions, comparisons and computations, formatting, and more.
The API surface has over 200 utility methods, and you can find information about all of them in the <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Temporal">Temporal docs on MDN</a>.</p></div></section><section aria-labelledby="core_concepts"><h2 id="core_concepts"><a href="#core_concepts">Core concepts</a></h2><div><p>In Temporal, the key concepts are that it has instants (unique points in history), wall-clock times (regional time), and durations.
The APIs have this overall structure to handle these concepts:</p>
<ul>
<li><strong>Duration</strong>: <code>Temporal.Duration</code> the difference between two points in time</li>
<li><strong>Points in time</strong>:
<ul>
<li><strong>Unique points in time</strong>:
<ul>
<li>As a timestamp: <code>Temporal.Instant</code></li>
<li>A date-time with a time zone: <code>Temporal.ZonedDateTime</code></li>
</ul>
</li>
<li><strong>Time-zone-unaware date/time ("Plain")</strong>:
<ul>
<li>Full date and time: <code>Temporal.PlainDateTime</code>
<ul>
<li>Just the date: <code>Temporal.PlainDate</code>
<ul>
<li>Year and month: <code>Temporal.PlainYearMonth</code></li>
<li>Month and day: <code>Temporal.PlainMonthDay</code></li>
</ul>
</li>
<li>Just the time: <code>Temporal.PlainTime</code></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><strong>Now:</strong> using <code>Temporal.now</code> to get the current time as various class instances, or in a specific format</li>
</ul></div></section><section aria-labelledby="temporal_examples"><h2 id="temporal_examples"><a href="#temporal_examples">Temporal examples</a></h2><div><p>Some of the most basic usages of Temporal include getting current dates and times as an ISO string, but we can see from the example below, that we can now provide time zones with many methods, which takes care of complex calculations you may be doing yourself:</p>
<div><pre><code>// The current date in the system's time zone
const dateTime = Temporal.Now.plainDateTimeISO();
console.log(dateTime); // e.g.: 2025-01-22T11:46:36.144

// The current date in the "America/New_York" time zone
const dateTimeInNewYork = Temporal.Now.plainDateTimeISO("America/New_York");
console.log(dateTimeInNewYork);
// e.g.: 2025-01-22T05:47:02.555
</code></pre></div>
<p>Working with different calendars is also simplified, as it's possible to create dates in calendar systems other than Gregorian, such as Hebrew, Chinese, and Islamic, for example.
The code below helps you find out when the next Chinese New Year is (which is quite soon!):</p>
<div><pre><code>// Chinese New Years are on 1/1 in the Chinese calendar
const chineseNewYear = Temporal.PlainMonthDay.from({
  monthCode: "M01",
  day: 1,
  calendar: "chinese",
});
const currentYear = Temporal.Now.plainDateISO().withCalendar("chinese").year;
let nextCNY = chineseNewYear.toPlainDate({ year: currentYear });
// If nextCNY is before the current date, move forward by 1 year
if (Temporal.PlainDate.compare(nextCNY, Temporal.Now.plainDateISO()) &lt;= 0) {
  nextCNY = nextCNY.add({ years: 1 });
}
console.log(
  `The next Chinese New Year is on ${nextCNY.withCalendar("iso8601").toLocaleString()}`,
);
// The next Chinese New Year is on 1/29/2025 (at the time of writing)
</code></pre></div>
<p>Working with Unix timestamps is a very common use case as many systems (APIs, databases) use the format to represent times.
The following example shows how to take a Unix Epoch timestamp in milliseconds, create an instant from it, get the current time with <code>Temporal.Now</code>, then calculate how many hours from now until the Unix timestamp:</p>
<div><pre><code>// 1851222399924 is our timestamp
const launch = Temporal.Instant.fromEpochMilliseconds(1851222399924);
const now = Temporal.Now.instant();
const duration = now.until(launch, { smallestUnit: "hour" });
console.log(`It will be ${duration.toLocaleString("en-US")} until the launch`);
// "It will be 31,600 hr until the launch" &lt;- @js-temporal/polyfill
// "It will be PT31600H until the launch" &lt;- Firefox Nightly
</code></pre></div>
<p>Currently, <code>toLocaleString</code> <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1839694" target="_blank">doesn't output a locale-sensitive string</a> in the Firefox implementation, so durations above (<code>PT31600H</code>) are returned as a non-locale-sensitive <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Temporal/Duration#iso_8601_duration_format">duration format</a>.
This may change as it's more of a design decision rather than a technical limitation as <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Temporal/PlainDate/since#using_since">formatting the duration</a> is possible, so the polyfill and Firefox implementations may eventually converge.</p>
<p>There's a lot to highlight, but one pattern that I thought was interesting in the API is the <code>compare()</code> methods, which allow you to <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Temporal/Duration/compare#sorting_an_array_of_durations">sort durations</a> in an elegant and efficient way:</p>
<div><pre><code>const durations = [
  Temporal.Duration.from({ hours: 1 }),
  Temporal.Duration.from({ hours: 2 }),
  Temporal.Duration.from({ hours: 1, minutes: 30 }),
  Temporal.Duration.from({ hours: 1, minutes: 45 }),
];

durations.sort(Temporal.Duration.compare);
console.log(durations.map((d) =&gt; d.toString()));
// [ 'PT1H', 'PT1H30M', 'PT1H45M', 'PT2H' ]
</code></pre></div></div></section><section aria-labelledby="trying_temporal_and_browser_support"><h2 id="trying_temporal_and_browser_support"><a href="#trying_temporal_and_browser_support">Trying Temporal and browser support</a></h2><div><p>Support is slowly starting to be included in experimental browser releases, and Firefox appears to have the most mature implementation at this point.
In Firefox, Temporal is being built into the <a href="https://www.mozilla.org/en-US/firefox/channel/desktop/" target="_blank">Nightly version</a> behind the <code>javascript.options.experimental.temporal</code> preference.
If you want to see the full compatibility story, you can check the (quite epic) <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Temporal#browser_compatibility">Temporal object Browser Compatibility section</a>.</p>
<p>Here are the main browser bugs that track Temporal implementations:</p>
<ul>
<li><strong>Firefox:</strong> <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1912757" target="_blank">Build temporal in Nightly by default</a></li>
<li><strong>Safari:</strong> <a href="https://bugs.webkit.org/show_bug.cgi?id=223166" target="_blank">[JSC] Implement Temporal</a></li>
<li><strong>Chrome:</strong> <a href="https://issues.chromium.org/issues/42201538" target="_blank">Implement the Temporal proposal</a></li>
</ul>
<p>Additionally, you can visit <a href="https://tc39.es/proposal-temporal/docs/" target="_blank">https://tc39.es/proposal-temporal/docs/</a> which has <code>@js-temporal/polyfill</code> available.
That means, you can open the developer tools on the TC39 docs page, and try some of the examples in the console in any browser without changing flags or preferences.</p>
<p>With experimental implementations landing, now is a good time to try out Temporal and become familiar with what will be the modern approach to handling dates and times in JavaScript.</p></div></section><section aria-labelledby="acknowledgements"><h2 id="acknowledgements"><a href="#acknowledgements">Acknowledgements</a></h2><div><ul>
<li>Thanks to <a href="https://meyerweb.com/" target="_blank">Eric Meyer</a> for his work on the topic.
It's been roughly 4 years since Eric's efforts to document <a href="https://github.com/mdn/browser-compat-data/pull/10643" target="_blank">browser compatibility data</a> and scaffold the documentation <a href="https://github.com/meyerweb/content/tree/temporal/" target="_blank">in his fork of mdn/content</a>.</li>
<li><a href="https://developer.mozilla.org/en-US/community/spotlight/joshua-chen">Joshua Chen</a> for picking up the torch from Eric and getting a pull request together for <a href="https://github.com/mdn/content/pull/37344" target="_blank">the MDN documentation</a>.</li>
<li><a href="https://bugzilla.mozilla.org/user_profile?user_id=339940" target="_blank">André Bargull</a> for the work on the Firefox Temporal implementation.</li>
</ul></div></section><section aria-labelledby="see_also"><h2 id="see_also"><a href="#see_also">See also</a></h2></section><section><a href="https://developer.mozilla.org/en-US/blog/fix-image-lcp/"><article><h2><strong>Previous<!-- --> Post</strong> <!-- -->Fix your website's Largest Contentful Paint by optimizing image loading</h2></article></a></section></article></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The US government's open data on Data.gov is currently being scrubbed (205 pts)]]></title>
            <link>https://old.reddit.com/r/climate/comments/1idiliv/the_us_governments_open_data_on_datagov_is/</link>
            <guid>42876055</guid>
            <pubDate>Thu, 30 Jan 2025 08:36:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/climate/comments/1idiliv/the_us_governments_open_data_on_datagov_is/">https://old.reddit.com/r/climate/comments/1idiliv/the_us_governments_open_data_on_datagov_is/</a>, See on <a href="https://news.ycombinator.com/item?id=42876055">Hacker News</a></p>
Couldn't get https://old.reddit.com/r/climate/comments/1idiliv/the_us_governments_open_data_on_datagov_is/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Investigating the case of human nose shape and climate adaptation (2017) (131 pts)]]></title>
            <link>https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1006616</link>
            <guid>42875888</guid>
            <pubDate>Thu, 30 Jan 2025 08:04:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1006616">https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1006616</a>, See on <a href="https://news.ycombinator.com/item?id=42875888">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content">

<header>





<ul id="almSignposts">
  <li id="loadingMetrics">
    <p>Loading metrics</p>
  </li>
</ul>







    <div>
  <p id="licenseShort">Open Access</p>
  <p id="peerReviewed">Peer-reviewed</p>

<p id="artType">Research Article</p>


</div>
    <div>



<div>
  

<ul data-js-tooltip="tooltip_container" id="author-list">



<li data-js-tooltip="tooltip_trigger">
       
</li>

<li data-js-tooltip="tooltip_trigger">
   <a data-author-id="1">
Brooke C. Mattern,</a>    
</li>

<li data-js-tooltip="tooltip_trigger">
   <a data-author-id="2">
Peter Claes,</a>    
</li>

<li data-js-tooltip="tooltip_trigger">
   <a data-author-id="3">
Brian McEcoy,</a>    
</li>

<li data-js-tooltip="tooltip_trigger">
   <a data-author-id="4">
Cris Hughes,</a>    
</li>

<li data-js-tooltip="tooltip_trigger">
   <a data-author-id="5">
Mark D. Shriver <span>  </span></a>    
</li>

</ul>


</div>


<div id="floatTitleTop" data-js-floater="title_author" role="presentation">
    <div>
      <h2><!--?xml version="1.0" encoding="UTF-8"?-->Investigating the case of human nose shape and climate adaptation</h2>

<ul id="floatAuthorList" data-js-floater="floated_authors">

  <li data-float-index="1">Arslan A. Zaidi,&nbsp;

  </li>
  <li data-float-index="2">Brooke C. Mattern,&nbsp;

  </li>
  <li data-float-index="3">Peter Claes,&nbsp;

  </li>
  <li data-float-index="4">Brian McEcoy,&nbsp;

  </li>
  <li data-float-index="5">Cris Hughes,&nbsp;

  </li>
  <li data-float-index="6">Mark D. Shriver

  </li>

</ul>



    </div>
    <div id="titleTopCloser">
      <p><img src="https://journals.plos.org/resource/img/logo-plos.png" alt="PLOS"></p><p>x</p>
    </div>
  </div>

      <ul>
        <li id="artPubDate">Published: March 16, 2017</li>
        <li id="artDoi">
<a href="https://doi.org/10.1371/journal.pgen.1006616">https://doi.org/10.1371/journal.pgen.1006616</a>
        </li>
        <li></li>
      </ul>

    </div>
  
</header>
  <div>




  


<div id="figure-carousel-section">
  <h2>Figures</h2>

  
</div>





        <div id="artText">
          



<div xmlns:plos="http://plos.org"><h2>Abstract</h2><div><p>The evolutionary reasons for variation in nose shape across human populations have been subject to continuing debate. An import function of the nose and nasal cavity is to condition inspired air before it reaches the lower respiratory tract. For this reason, it is thought the observed differences in nose shape among populations are not simply the result of genetic drift, but may be adaptations to climate. To address the question of whether local adaptation to climate is responsible for nose shape divergence across populations, we use Qst–Fst comparisons to show that nares width and alar base width are more differentiated across populations than expected under genetic drift alone. To test whether this differentiation is due to climate adaptation, we compared the spatial distribution of these variables with the global distribution of temperature, absolute humidity, and relative humidity. We find that width of the nares is correlated with temperature and absolute humidity, but not with relative humidity. We conclude that some aspects of nose shape may indeed have been driven by local adaptation to climate. However, we think that this is a simplified explanation of a very complex evolutionary history, which possibly also involved other non-neutral forces such as sexual selection.</p>
</div></div><div xmlns:plos="http://plos.org"><h2>Author summary</h2>
<div><p>The study of human adaptation is essential to our understanding of disease etiology. Evolutionary investigations into why certain disease phenotypes such as sickle-cell anemia and lactose intolerance occur at different rates in different populations have led to a better understanding of the genetic and environmental risk factors involved. Similarly, research into the geographical distribution of skin pigmentation continues to yield important clues regarding risk of vitamin D deficiency and skin cancer. Here, we investigate whether variation in the shape of the external nose across populations has been driven by regional differences in climate. We find that variation in both nares width and alar base width appear to have experienced accelerated divergence across human populations. We also find that the geospatial distribution of nares width is correlated with temperature, and absolute humidity, but not with relative humidity. Our results support the claim that local adaptation to climate may have had a role in the evolution of nose shape differences across human populations.</p>
</div></div>


<div xmlns:plos="http://plos.org"><p><strong>Citation: </strong>Zaidi AA, Mattern BC, Claes P, McEcoy B, Hughes C, Shriver MD (2017) Investigating the case of human nose shape and climate adaptation. PLoS Genet 13(3):
           e1006616.
        
        https://doi.org/10.1371/journal.pgen.1006616</p><p><strong>Editor: </strong>Greg Gibson, Georgia Institute of Technology, UNITED STATES</p><p><strong>Received: </strong>December 17, 2015; <strong>Accepted: </strong>February 3, 2017; <strong>Published: </strong> March 16, 2017</p><p><strong>Copyright: </strong> © 2017 Zaidi et al. This is an open access article distributed under the terms of the <a href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</a>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p><p><strong>Data Availability: </strong>We have made almost all data, which were used to generate results in the paper, publicly available in a spreadsheet in the Supporting Information section (<a href="#pgen.1006616.s009">S1 File</a>). The spreadsheet includes de-identified data, split by the analyses for which they were used. More specifically, we have provided several phenotypes (nose shape measures, skin pigmentation, height, weight, and BMI) as well as relevant covariates (ancestry components, sex, and age). Where relevant, we have also provided climate values (mean annual temperature, annual aridity index, and ultraviolet B irradiance) ascribed to each individual. These data can be used to replicate almost all the results presented in the manuscript, without compromising the privacy of the participants. Some raw data, such as the 3D photographs, genotypes, and birthplaces, have not been shared because they are highly identifiable in nature and sharing them would be in ethical and legal violation of the informed consent obtained from participants. This restriction is not because of any personal or commercial interests.</p><p><strong>Funding: </strong>This work was supported in part by the United States National Institute of Justice, the United States Department of Defense, the University of Illinois Interdisciplinary Innovation Initiative Research Grant, the Flemish Institute for the Promotion of Innovation by Science and Technology in Flanders (IWT Vlaanderen), the Research Program of the Fund for Scientific Research - Flanders (Belgium), and the Center for Human Evolution and Development at Penn State. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p><p><strong>Competing interests: </strong> The authors have declared that no competing interests exist.</p></div>





<div xmlns:plos="http://plos.org" id="section1"><h2>Introduction</h2><p>The shape of the nose, like many other parts of the face, varies both within as well as across human populations. For example, the distance between the nasal alare (wings of the nose) are significantly larger in individuals of West African, South Asian, and East Asian ancestry as compared to persons with European ancestry [<a href="#pgen.1006616.ref001">1</a>]. The nasal index (width/height of the nasal aperture of the skull) is also known to vary significantly among populations [<a href="#pgen.1006616.ref001">1</a>,<a href="#pgen.1006616.ref002">2</a>]. Whether these population differences in nose shape are due primarily to genetic drift or natural selection is unclear.</p>
<p>A vital function of the nose is to warm inspired air to core body temperature and saturate it with water vapor before it reaches the lower respiratory tract [<a href="#pgen.1006616.ref003">3</a>]. In fact, inhaled air reaches 90% of the required temperature and humidity levels before even reaching the nasopharynx, implicating the nasal cavity as the major conditioning apparatus in the respiratory tract [<a href="#pgen.1006616.ref004">4</a>,<a href="#pgen.1006616.ref005">5</a>]. This conditioning acts to maintain proper functioning of the mucociliary apparatus, which functions in trapping particles and pathogens and removing them from the airways. Low humidity in the respiratory tract leads to impaired mucociliary function and increases the risk of both upper and lower respiratory tract infections [<a href="#pgen.1006616.ref006">6</a>]. Much of the air conditioning occurs as it passes through the turbinates, the walls of which are lined with blood vessels and mucus producing goblet cells [<a href="#pgen.1006616.ref004">4</a>]. Studies have shown that the efficacy of the conditioning process depends on the flow dynamics of the inspired air, which in turn, depend on the geometry of the nasal cavity and inlets [<a href="#pgen.1006616.ref005">5</a>,<a href="#pgen.1006616.ref007">7</a>]. Because of the function of the nose as an air-conditioning apparatus, it is hypothesized that differences in nose shape across populations may have been driven by local adaptation to climate [<a href="#pgen.1006616.ref002">2</a>,<a href="#pgen.1006616.ref003">3</a>,<a href="#pgen.1006616.ref008">8</a>].</p>
<p>There are several challenges in testing this hypothesis. We know there is substantial nose shape variation among human populations, in both the external morphology of the nose as well as the underlying cranial morphology [<a href="#pgen.1006616.ref001">1</a>,<a href="#pgen.1006616.ref002">2</a>,<a href="#pgen.1006616.ref009">9</a>]. While this can be explained by adaptation to local selection pressures, it could also be explained by the fact that phenotypic differences among geographically distant populations can arise simply due to genetic drift. Thus, in order to invoke divergent selection as an explanation, one must demonstrate that the observed variation in nose shape across human populations is greater than that expected under genetic drift alone. This can be carried out using the Qst statistic, which measures the degree of genetic differentiation underlying a quantitative trait [<a href="#pgen.1006616.ref010">10</a>]. In principle, the Qst of a neutrally evolving trait is expected to follow the Fst distribution of neutrally evolving loci [<a href="#pgen.1006616.ref011">11</a>]. Thus, when Qst is much greater than Fst, trait divergence exceeds neutral expectations and can be attributed to divergent selection [<a href="#pgen.1006616.ref011">11</a>]. The problem with Qst is that its calculation requires knowledge of the additive genetic variances within- and among-populations. These can only be estimated reliably using ‘common-garden’ experiments, in which environmental effects on the phenotype can be effectively controlled. Since such experiments are not possible in humans, Qst-based inference regarding divergent selection on human phenotypes necessitates making realistic assumptions about the heritabilities of the phenotypes in question.</p>
<p>Several studies have used this approach and found that while most aspects of the skull seem to be evolving neutrally, the shape of the nasal aperture appears to be more differentiated across human populations than expected under genetic drift [<a href="#pgen.1006616.ref009">9</a>,<a href="#pgen.1006616.ref012">12</a>,<a href="#pgen.1006616.ref013">13</a>]. More recently, it was also reported that the divergence in the shape of the external nose, at least between Europeans and Han Chinese populations, also exceeds neutral expectations [<a href="#pgen.1006616.ref014">14</a>]. While this might be true, most of these studies employed anticonservative heritability assumptions, which overestimate the genetic differentiation underlying a trait, and thus, lead to incorrect conclusions regarding the relative roles of selection and drift in driving phenotypic differences among populations.</p>
<p>Here, we carried out an exploration of external nose shape variation in light of quantitative genetic theory to investigate whether nose shape variation among populations exceeds neutral expectations. In doing so, we discuss the methodological challenges involved in tackling such questions in humans, and show how the limitations of previous studies can be addressed with recent advances in statistical genetics. As proof of principle, we compare the differentiation of nose shape with two highly heritable morphological traits, which are known to have a polygenic basis; namely height and skin pigmentation [<a href="#pgen.1006616.ref015">15</a>–<a href="#pgen.1006616.ref018">18</a>]. Both height and skin pigmentation exhibit substantial variation within and across human populations, and are thought to be under positive selection in various populations [<a href="#pgen.1006616.ref019">19</a>–<a href="#pgen.1006616.ref022">22</a>]. Skin pigmentation is also a good example of a phenotype which is known to have evolved in response to a geospatially varying selection pressure: exposure to ground level ultraviolet B radiation (UVB) [<a href="#pgen.1006616.ref023">23</a>]. Finally, we test whether clinal variation in aspects of nose shape, which appear to be under accelerated divergence across populations, covaries with geographic variation in temperature and humidity, in order to determine whether this divergence is due to climatic selection pressures.</p>
</div>

<div xmlns:plos="http://plos.org" id="section2"><h2>Results</h2>
<div id="section1"><h3>Describing variation in nose shape</h3>
<p>To quantify variation in nose shape, we first captured high resolution 3D images of participants’ faces using the 3dMD Face system (<em>3dMD Atlanta</em>, <em>GA</em>). Five positioning landmarks (two on the inner corner of the eyes, two on the outer corners of the mouth, and one on the tip of the nose) were placed in order to establish facial orientation. A spatially dense mesh of 7,150 quasi-landmarks (QLs) was mapped onto each image and its reflection. Generalized Procrustes Superimposition [<a href="#pgen.1006616.ref024">24</a>] was carried out on both sets of images (original and reflected) to remove differences in position and orientation. The Procrustes coordinates of the original and reflected image for each participant were then averaged to remove effects of bilateral asymmetry following Claes <em>et al</em>. (2014) [<a href="#pgen.1006616.ref025">25</a>]. The nose region, which is comprised of 709 out of the 7,150 QLs, was selected for downstream analyses.</p>
<p>We used linear distances and areas to characterize the shape of the nose (<a href="#pgen-1006616-g001">Fig 1</a>). The linear distances (measured in mm) were calculated using seven standard anthropometric landmarks: i. nasion (n), ii. pronasale (prn), iii. subnasale (sn), iv. Left alar curvature, (al<sub>l</sub>), v. right alar curvature (al<sub>r</sub>), vi. left alar base (ac<sub>l</sub>), and vii. Right alar base (ac<sub>r</sub>) (<a href="#pgen-1006616-g001">Fig 1</a>) [<a href="#pgen.1006616.ref026">26</a>,<a href="#pgen.1006616.ref027">27</a>]. These seven landmarks were placed on the spatially-dense mesh of 709 QLs, which facilitated automated placement on each face. We used the x, y, and z coordinates of these landmarks to calculate five Euclidean distances: i. nares width (al<sub>l</sub>−al<sub>r</sub>), ii. alar base width (ac<sub>l</sub>−ac<sub>r</sub>), iii. nasal height (n–sn), iv. nasal ridge length (n–prn), and v. nasal tip protrusion (sn–prn) (<a href="#pgen-1006616-g001">Fig 1</a>). We also computed two areas (measured in mm<sup>2</sup>): i. total external area of the nose and ii. mean nostril area (<a href="#pgen-1006616-g001">Fig 1</a>) (Methods).</p>
<div data-doi="10.1371/journal.pgen.1006616.g001"><div><p><a title="Click for larger image" href="https://journals.plos.org/plosgenetics/article/figure/image?size=medium&amp;id=10.1371/journal.pgen.1006616.g001" data-doi="10.1371/journal.pgen.1006616" data-uri="10.1371/journal.pgen.1006616.g001"><img src="https://journals.plos.org/plosgenetics/article/figure/image?size=inline&amp;id=10.1371/journal.pgen.1006616.g001" alt="thumbnail" loading="lazy"></a></p></div><p><span>Fig 1. </span> Measurements used to describe the shape of the nose.</p><p>A) The locations of landmarks, which were used to calculate linear distances, are shown as red points. B) Five Linear distances (red lines) and two surface areas (red mesh) were used to describe nose shape. Linear distances were calculated from the 3D coordinates of landmarks (red points). Surface areas were computed as the sum of the polygons highlighted in red.</p>
<p><a href="https://doi.org/10.1371/journal.pgen.1006616.g001">
              https://doi.org/10.1371/journal.pgen.1006616.g001</a></p></div><p><a href="#pgen-1006616-g002">Fig 2</a> shows the distribution of aspects of nose shape in males and females from four human population groups: West African, East Asian, South Asian, and Northern European. These groups were defined based on genetic and self-reported ancestry (Methods). <a href="#pgen-1006616-g002">Fig 2</a> also shows the distribution of height and melanin index, a measure of skin pigmentation level derived from reflectance spectrophotometry (measured on the upper inner arms; see <a href="#sec009">Methods</a>), for comparison. The sample size, mean, and standard deviation for each phenotype are provided in <a href="#pgen-1006616-t001">Table 1</a>. One clear observation is that all aspects of nose shape, at least those considered here, are highly sexually dimorphic (<a href="#pgen-1006616-g002">Fig 2</a>, <a href="#pgen-1006616-t001">Table 1</a>). Males, on average, tend to have wider nares, longer nasal ridges, more outwardly protruding nasal tips, bigger nostrils, and a larger overall external surface area, compared to females (<a href="#pgen-1006616-g002">Fig 2</a>, <a href="#pgen-1006616-t001">Table 1</a>). All nasal measurements are also significantly different across populations (<a href="#pgen.1006616.s008">S1 Table</a>). Based on <a href="#pgen-1006616-t001">Table 1</a>, we note some general patterns. For example, nares width and alar base width are largest in W. Africans and smallest in N. Europeans. These findings are consistent with previous observations [<a href="#pgen.1006616.ref001">1</a>,<a href="#pgen.1006616.ref002">2</a>]. Nasal tip protrusion is greatest in N. Europeans and smallest in W. Africans and E. Asians. E. Asians also tend to have the smallest noses in terms of external surface area.</p>
<div data-doi="10.1371/journal.pgen.1006616.g002"><div><p><a title="Click for larger image" href="https://journals.plos.org/plosgenetics/article/figure/image?size=medium&amp;id=10.1371/journal.pgen.1006616.g002" data-doi="10.1371/journal.pgen.1006616" data-uri="10.1371/journal.pgen.1006616.g002"><img src="https://journals.plos.org/plosgenetics/article/figure/image?size=inline&amp;id=10.1371/journal.pgen.1006616.g002" alt="thumbnail" loading="lazy"></a></p></div><p><span>Fig 2. </span> Boxplots of phenotypes by population and sex overlaid with the individual data points.</p><p>Height is measured in centimeters and melanin index is measured in percentage reflectance (Methods). Linear distances are measured in millimeters (mm) and area are measured in mm<sup>2</sup>. Points are individual observations and the color of the boxplots and points represents sex with blue indicating males and red indicating females.</p>
<p><a href="https://doi.org/10.1371/journal.pgen.1006616.g002">
              https://doi.org/10.1371/journal.pgen.1006616.g002</a></p></div></div>

<div id="section2"><h3>Testing for accelerated divergence</h3>
<p>Differences in a phenotype can accumulate across populations simply due to genetic drift. In order to invoke positive directional selection, one must demonstrate that the variation across populations is more than that expected under genetic drift. We used Qst (see Leinonen <em>et al</em>. [<a href="#pgen.1006616.ref028">28</a>] for a recent review), the quantitative genetic analog of Fst [<a href="#pgen.1006616.ref010">10</a>,<a href="#pgen.1006616.ref029">29</a>], to test whether certain aspects of nose shape exhibit greater differentiation across populations than expected under genetic drift alone. Qst measures the degree of genetic differentiation in a quantitative trait across populations and is defined as:
<a name="pgen.1006616.e001" id="pgen.1006616.e001"></a><span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e001" loading="lazy"><span>(1)</span></span>
where <span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e002" loading="lazy"></span> and <span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e003" loading="lazy"></span> are the components of phenotypic variance due to additive genetic effects among and within populations, respectively. It has been shown that, in principle, the distribution of Qst of a quantitative trait that has evolved under genetic drift alone is expected to be equal to Fst of neutral genetic markers [<a href="#pgen.1006616.ref011">11</a>,<a href="#pgen.1006616.ref030">30</a>,<a href="#pgen.1006616.ref031">31</a>]. This expectation allows one to compare Qst to Fst to test whether genetic drift alone is sufficient to explain the divergence of a trait among populations. If the Qst of a trait across a set of populations is much greater than Fst, it means that the phenotypic differentiation exceeds the expectation under neutrality.</p>
<p>The components of additive genetic variance, <span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e004" loading="lazy"></span> and <span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e005" loading="lazy"></span>, are typically estimated from ‘common-garden’ experiments in which the effects due to the environment can be controlled [<a href="#pgen.1006616.ref028">28</a>]. This is often not possible in non-model organisms, especially humans, because of practical and ethical limitations. In such cases, <span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e006" loading="lazy"></span> and <span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e007" loading="lazy"></span> can be estimated from the among- and within-population components of <em>phenotypic</em> variance, <span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e008" loading="lazy"></span> and <span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e009" loading="lazy"></span>, respectively, provided the heritabilities underlying these components are known:
<a name="pgen.1006616.e010" id="pgen.1006616.e010"></a><span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e010" loading="lazy"><span>(2)</span></span></p>
<p>Here <span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e011" loading="lazy"></span> and <span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e012" loading="lazy"></span> are among- and within-population components of the phenotypic variance and <em>c</em> and <em>h</em><sup><em>2</em></sup> are proportions of <span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e013" loading="lazy"></span> and <span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e014" loading="lazy"></span>, respectively, that are due to additive genetic effects. Both <em>c</em> and <em>h</em><sup><em>2</em></sup> are can range from 0 (none of the variance is due to additive genetic effects) to 1 (all of the variance is due to additive genetic effects). Eq (<a href="#pgen.1006616.e010">2</a>) shows that Qst calculated from phenotypic variance components depends on the ratio between <em>c</em> and <em>h</em><sup><em>2</em></sup> [<a href="#pgen.1006616.ref032">32</a>]. Without prior information, it is reasonable to assume <em>c/h</em><sup><em>2</em></sup> = 1, i.e., the proportion of phenotypic variance due to additive genetic effects is the same among- and within-populations. Qst calculated this way is sometimes referred to as Pst [<a href="#pgen.1006616.ref033">33</a>]. However, we will continue to use the term Qst to avoid confusion and will evaluate the validity of the assumption that <em>c/h</em><sup><em>2</em></sup> = 1 in the following section.</p>
<p>We calculated Qst for each aspect of nose shape, described in the previous section, across four human population groups: i) West African (N = 40), ii) North European (N = 236), iii) East Asian (N = 127), and iv) South Asian (N = 73) (see <a href="#sec009">Methods</a> for selection criteria). We used a non-parametric bootstrap approach to generate the empirical distributions of Qst and Fst and to test whether the observed value of Qst is greater than Fst (Methods). The statistic we used is Qst–Fst, which, under the null hypothesis of genetic drift, is expected to be equal to zero. The larger the Qst–Fst of a phenotype, the stronger the evidence that the variation in the phenotype across populations is more than that expected under genetic drift alone. We refer to outliers in the neutral distribution as signals of accelerated divergence for brevity. The strength of evidence for accelerated divergence can be measured using an empirical p-value, which is the proportion of bootstrapped values of Qst–Fst that are less than zero. To compare with other quantitative traits with a polygenic basis, we also tested whether height and skin pigmentation exhibit signals of accelerated divergence. The results are illustrated in <a href="#pgen-1006616-g003">Fig 3</a> and the p-values are listed in <a href="#pgen-1006616-t002">Table 2</a>. We treat phenotypes that pass a stringent Bonferonni correction (p-value &lt;0.0071 = 0.05/7 for seven nose shape traits) as exhibiting signals of accelerated divergence across populations.</p>
<div data-doi="10.1371/journal.pgen.1006616.g003"><div><p><a title="Click for larger image" href="https://journals.plos.org/plosgenetics/article/figure/image?size=medium&amp;id=10.1371/journal.pgen.1006616.g003" data-doi="10.1371/journal.pgen.1006616" data-uri="10.1371/journal.pgen.1006616.g003"><img src="https://journals.plos.org/plosgenetics/article/figure/image?size=inline&amp;id=10.1371/journal.pgen.1006616.g003" alt="thumbnail" loading="lazy"></a></p></div><p><span>Fig 3. </span> Qst–Fst results across all populations.</p><p>The bootstrapped distribution of Qst–Fst for each phenotype (shown by a violin plot) is compared against the expected value of zero under neutrality (horizontal dashed line). Phenotypes, which exhibit accelerated divergence (using a Bonferronni corrected p-value threshold of 0.0071), are shown in red.</p>
<p><a href="https://doi.org/10.1371/journal.pgen.1006616.g003">
              https://doi.org/10.1371/journal.pgen.1006616.g003</a></p></div><p>As expected, skin pigmentation shows a strong signal of accelerated divergence across populations (Qst = 0.642, p-value = 1.00E-04). This is in accordance with the idea that differences in skin pigmentation, at least across continental populations, have been driven by positive selection. Height, on the other hand, does not show signals of accelerated divergence (Qst = 0.177, p-value = 0.14). While height seems to be under positive <em>selection within</em> European and African populations separately [<a href="#pgen.1006616.ref019">19</a>,<a href="#pgen.1006616.ref034">34</a>], it does not appear to be driven by divergent selection pressures across the populations considered here. Of all the nose shape measurements, only nares width (Qst = 0.467, p-value = 2.80E-03) and alar base width (Qst = 0.440, p-value = 4.90E-03) exhibit signals of accelerated divergence. However, the degree of divergence in these traits is not as high as that observed for skin pigmentation.</p>
</div>

<div id="section3"><h3>Evaluating validity of assumptions regarding <em>c</em> and <em>h</em><sup><em>2</em></sup></h3>
<p>We estimated Qst under the assumption that <em>c/h</em><sup><em>2</em></sup> = 1. In other words, we assume that the proportion of between-population variance in the trait due to additive genetic effects is the same as the proportion of within-population variance due to additive genetic effects. This can be an incorrect assumption if a large proportion of the phenotypic variance between populations is due to direct environmental effects (i.e., phenotypic plasticity). If the true value of <em>c/h</em><sup><em>2</em></sup> is drastically lower than 1, phenotype-derived estimates of the among-population genetic variance, and hence, Qst will be inflated, resulting in false-positive conclusions regarding the role of selection in driving phenotypic differentiation. This fact has largely been ignored in previous studies, in which Qst-based approaches were used to explore craniofacial divergence [<a href="#pgen.1006616.ref012">12</a>–<a href="#pgen.1006616.ref014">14</a>,<a href="#pgen.1006616.ref035">35</a>]. In these studies, <em>h</em><sup><em>2</em></sup> was assumed to be 0.55 and <em>c</em> was implicitly assumed to be 1, resulting in <em>c/h</em><sup><em>2</em></sup> to be greater than 1. While this might be true for some traits (for example, see skin pigmentation in <a href="#pgen-1006616-t003">Table 3</a>), this is an anticonservative approach in our opinion, because it assumes that the heritability across populations is higher than the heritability within populations. If anything, the opposite case (<em>c/h</em><sup><em>2</em></sup> &lt; 1) is more plausible as we expect environmental variation to be large across geographically distant populations. Keeping this in mind, we evaluated the sensitivity of the Qst–Fst results to the case where <em>c</em>/<em>h</em><sup><em>2</em></sup> &lt; 1 [<a href="#pgen.1006616.ref032">32</a>], by determining the ‘critical value’ of <em>c/h</em><sup><em>2</em></sup> at which the 95% lower bound of Qst meets the 95% upper bound of Fst (<a href="#pgen-1006616-g004">Fig 4</a>). Smaller critical values imply that phenotype-based Qst–Fst comparisons are robust to <em>c/h</em><sup><em>2</em></sup> assumptions. Such ‘sensitivity analyses’, while common in the molecular ecology literature, are underrepresented in human genetic and anthropological studies. <a href="#pgen-1006616-g004">Fig 4</a> shows the sensitivity curves for height, skin pigmentation, and nose shape traits. Skin pigmentation, which shows a very high Qst value, has a critical value of 0.25, which is much lower than 1 (<a href="#pgen-1006616-g004">Fig 4</a> and <a href="#pgen-1006616-t002">Table 2</a>). The critical value of nares width is 0.50 and that of alar base width is 0.55, which are also much lower than 1 (<a href="#pgen-1006616-g004">Fig 4</a> and <a href="#pgen-1006616-t002">Table 2</a>). Thus, we expect that these phenotypes would exhibit signals of accelerated divergence, even if the true values of <em>c/h</em><sup><em>2</em></sup> were much less than 1.</p>
<div data-doi="10.1371/journal.pgen.1006616.g004"><div><p><a title="Click for larger image" href="https://journals.plos.org/plosgenetics/article/figure/image?size=medium&amp;id=10.1371/journal.pgen.1006616.g004" data-doi="10.1371/journal.pgen.1006616" data-uri="10.1371/journal.pgen.1006616.g004"><img src="https://journals.plos.org/plosgenetics/article/figure/image?size=inline&amp;id=10.1371/journal.pgen.1006616.g004" alt="thumbnail" loading="lazy"></a></p></div><p><span>Fig 4. </span> Sensitivity plots showing critical values of <em>c/h</em><sup><em>2</em></sup>.</p><p>Change in median Qst of height, skin pigmentation, and nose shape measures as a function of <em>c/h</em><sup><em>2</em></sup> is shown as a solid red line. Median Fst is shown as a solid blue line. The lower and upper 95% bounds for Qst and Fst are shown as dashed red and blue lines, respectively. The critical value at which the lower bound of Qst meets the upper bound of Fst is shown on the upper left corner of each plot. Lower critical values indicate the Qst–Fst is more robust to <em>c/h</em><sup><em>2</em></sup> assumptions.</p>
<p><a href="https://doi.org/10.1371/journal.pgen.1006616.g004">
              https://doi.org/10.1371/journal.pgen.1006616.g004</a></p></div></div>

<div id="section4"><h3>Investigating heritability of nose shape within- and among-populations</h3>
<p>In addition to the sensitivity analysis, we wanted to demonstrate that differences in nose shape among individuals both within- and between-populations are heritable. Within-population heritability (<em>h</em><sup>2</sup>) for a phenotype is traditionally estimated from data collected on large sets of twins or from pedigrees where the genetic relationships among individuals are known [<a href="#pgen.1006616.ref036">36</a>]. Recently, Yang and colleagues (2010) introduced a linear mixed model approach, which can be used to estimate an alternative statistic in unrelated individuals: the proportion of phenotypic variance explained by genotyped SNPs (<em>h</em><sub><em>g</em></sub><sup><em>2</em></sup>) [<a href="#pgen.1006616.ref018">18</a>]. A major advantage of this approach over traditional twin- and pedigree-based approaches is that it is applicable to data from unrelated individuals, which are more accessible and easier to collect in large numbers. We calculated <em>h</em><sub><em>g</em></sub><sup><em>2</em></sup> as an estimate of <em>h</em><sup><em>2</em></sup> for height, melanin index, and nose shape traits using 118,420 autosomal SNPs in a sample of unrelated persons of European ancestry, implemented in the GCTA software [<a href="#pgen.1006616.ref037">37</a>] (Methods). The values of <em>h</em><sub><em>g</em></sub><sup><em>2</em></sup> are provided in <a href="#pgen-1006616-t003">Table 3</a>. The heritability for height in Europeans (<em>h</em><sub><em>g</em></sub><sup><em>2</em></sup> = 0.394; N = 1,825) is similar to previously reported estimates [<a href="#pgen.1006616.ref018">18</a>]. Interestingly, the heritability of skin pigmentation is very low in Europeans (<em>h</em><sub><em>g</em></sub><sup><em>2</em></sup> = 0.191; N = 1,231), which we think can be explained by reduced genetic variation at skin pigmentation loci due to strong positive selection for lighter skin in Europeans [<a href="#pgen.1006616.ref017">17</a>,<a href="#pgen.1006616.ref021">21</a>,<a href="#pgen.1006616.ref038">38</a>]. Almost all aspects of nose shape seem to be highly heritable in Europeans (<em>h</em><sub><em>g</em></sub><sup><em>2</em></sup> range: 0.401–0.657 N = 1,718) (<a href="#pgen-1006616-t003">Table 3</a>).</p>
<p>Estimation of the among-population heritability (<em>c</em>) is more difficult, since genetic effects between geographically distant populations can be confounded with large environmental effects. This parameter can often only be reliably estimated through ‘common garden’ experiments, in which systematic differences in the environment, which might be confounded with genetic differences, can be taken into account. However, as mentioned earlier, this approach is only possible in model organisms, which are amenable to experimental manipulation. In humans and other non-model organisms, this is a severe limitation, which can be overcome by studying naturally occurring admixed populations.</p>
<p>The process of admixture reunites gene pools from two or more populations, which might have diverged due to genetic drift, mutation, and selection. Admixture, which may occur repeatedly over several generations, followed by recombination, leads to chromosomes that are essentially mosaics of ancestry segments (<a href="#pgen-1006616-g005">Fig 5</a>) [<a href="#pgen.1006616.ref039">39</a>]. In a randomly mating admixed population, ancestry segments segregate randomly with respect to the environment, which decouples the between-population genetic effects on the phenotype from the environmental effects, allowing for the estimation of the genetic variance underlying a phenotype. We propose that a method recently developed to estimate heritability in admixed populations (Zaitlen <em>et al</em>. (2014)) [<a href="#pgen.1006616.ref040">40</a>], might provide a valid estimate of <em>c</em>. Zaitlen <em>et al</em>. (2014) extend the method developed by Yang <em>et al</em>. (2010) [<a href="#pgen.1006616.ref018">18</a>], using local ancestry at SNPs, instead of genotypes, to construct the genetic relationship matrix among admixed individuals. The proportion of phenotypic variance in an admixed population that can be explained by local ancestry (<em>h</em><sub><em>y</em></sub><sup><em>2</em></sup>) is conceptually equivalent to <em>c</em> between the parental populations. Our reasoning is that, on a genotypic level, the genetic variation in an admixed population should be the sum of the genetic variation within the parental populations and the genetic variation between them. Variation at the scale of local ancestry only represents genetic variation between populations (<a href="#pgen-1006616-g005">Fig 5</a>). Thus, the proportion of phenotypic variation that can be explained by local ancestry (<em>h</em><sub><em>y</em></sub><sup><em>2</em></sup>), should be equivalent to <em>c</em>.</p>
<div data-doi="10.1371/journal.pgen.1006616.g005"><div><p><a title="Click for larger image" href="https://journals.plos.org/plosgenetics/article/figure/image?size=medium&amp;id=10.1371/journal.pgen.1006616.g005" data-doi="10.1371/journal.pgen.1006616" data-uri="10.1371/journal.pgen.1006616.g005"><img src="https://journals.plos.org/plosgenetics/article/figure/image?size=inline&amp;id=10.1371/journal.pgen.1006616.g005" alt="thumbnail" loading="lazy"></a></p></div><p><span>Fig 5. </span> Admixture brings together within- and between-population variation in admixed individuals.</p><p>Genetic variation between the two parental populations is represented by difference in color of the chromosomes, whereas genetic variation within the populations is represented by hue intensity. Admixture brings together genetic variation from both populations. On a genotypic level, genetic variation within admixed populations is composed of both within-population and between-population variation. The variation at the level of ancestry is variation between the two parental populations.</p>
<p><a href="https://doi.org/10.1371/journal.pgen.1006616.g005">
              https://doi.org/10.1371/journal.pgen.1006616.g005</a></p></div><p>Following this reasoning, we estimated <em>h</em><sub><em>y</em></sub><sup><em>2</em></sup> from local ancestry at 623,625 autosomal SNPs in a sample of 409 Cape Verdeans, who derive their ancestry primarily from W. Africans and Europeans (see <a href="#sec009">Methods</a> for details and <a href="#pgen-1006616-t003">Table 3</a> for results). <a href="#pgen-1006616-t003">Table 3</a> shows that the phenotypic variation between W. Africans and Europeans in height (<em>h</em><sub><em>y</em></sub><sup><em>2</em></sup> = 0.224, N = 409) is quite heritable. Results for skin pigmentation data are not presented here as they were unavailable. Differences in many aspects of nose shape are also heritable (Nares width: <em>h</em><sub><em>y</em></sub><sup><em>2</em></sup> = 0.226, Alar base width: <em>h</em><sub><em>y</em></sub><sup><em>2</em></sup> = 0.212, Nasal tip protrusion: 0.177, External surface area: <em>h</em><sub><em>y</em></sub><sup><em>2</em></sup> = 0.121, N = 409). Other aspects of nose shape may not be as heritable between these two populations (Nasal height: <em>h</em><sub><em>y</em></sub><sup><em>2</em></sup> = 0.03, <em>h</em><sub><em>y</em></sub><sup><em>2</em></sup> = 0.059, N = 409). Another interesting observation from <a href="#pgen-1006616-t003">Table 3</a> is that estimates of <em>h</em><sub><em>y</em></sub><sup><em>2</em></sup> are generally lower than estimates of <em>h</em><sub><em>g</em></sub><sup><em>2</em></sup> for all traits. This suggests that for most human traits, the additive genetic variance between populations might be less than the additive genetic variance within populations, which agrees with the fact that most of the genetic variation in humans exists within populations. However, we are cautious of over-stating this conclusion since the heritabilities were estimated in individuals with W. African and European ancestry only and do not reflect the variation within and across other populations. Altogether, our results show that genetic differences underlie the variation in many aspects of nose shape, both within- and between-populations.</p>
</div>

<div id="section5"><h3>Testing for adaptation to climate</h3>
<p>Previously, several studies have shown that the shapes of the nasal aperture and nasal cavity are correlated with climate variables related to temperature and humidity such that individuals from cold-dry climates exhibit narrower nasal cavities compared to individuals from warm-humid climates [<a href="#pgen.1006616.ref041">41</a>,<a href="#pgen.1006616.ref042">42</a>]. We were interested in testing whether aspects of external nose shape showing unusually high differentiation across populations based on Qst–Fst analysis, show correlations with climate. For this purpose, we selected, from the subset used in the Qst analysis, females with genetic data whose parents were born in a region that coincided with their continental ancestry (N = 140) (<a href="#pgen-1006616-g006">Fig 6</a>). This was done to assign to each individual, a climate value that was most similar to their ‘ancestral’ climate. Since we did not have genotype data available for males of Northern European ancestry, we only used females for this analysis. The genotype data were necessary to correct for genetic structure (see <a href="#sec009">Methods</a>). We tested the correlation of nares width and alar base width with three climate variables: i) mean annual temperature (hereafter referred to as temperature), ii) relative humidity, and iii) absolute humidity. The choice of these climate variables follows from the functional importance of the nose in warming and humidifying inspired air. We also tested whether skin pigmentation is correlated with UVB levels. This was used as proof of principle, since multiple lines of evidence suggest that differences in skin pigmentation across populations have evolved primarily in response to ultraviolet radiation [<a href="#pgen.1006616.ref017">17</a>,<a href="#pgen.1006616.ref020">20</a>–<a href="#pgen.1006616.ref022">22</a>]. The association between phenotypes and climate variables was tested using linear mixed models, which correct for age, BMI, and genetic similarity (Methods). Sex was not included as a covariate as only females were used. We used a likelihood ratio test (LRT) to evaluate the statistical significance of the slope between phenotype and climate variable. The LRT statistic and its corresponding p-values were generated by comparing full (climate predictor included) and reduced (climate predictor removed) models. Results are presented in <a href="#pgen-1006616-t004">Table 4</a>.</p>
<div data-doi="10.1371/journal.pgen.1006616.g006"><div><p><a title="Click for larger image" href="https://journals.plos.org/plosgenetics/article/figure/image?size=medium&amp;id=10.1371/journal.pgen.1006616.g006" data-doi="10.1371/journal.pgen.1006616" data-uri="10.1371/journal.pgen.1006616.g006"><img src="https://journals.plos.org/plosgenetics/article/figure/image?size=inline&amp;id=10.1371/journal.pgen.1006616.g006" alt="thumbnail" loading="lazy"></a></p></div><p><span>Fig 6. </span> Geographic distribution of parents’ birthplaces.</p><p>Individual points represent the birth locations of the parents with a line connecting two parents. A single point indicates that the two parents were born in the same location. Climate values at these locations were used to test for signals of climate adaptation.</p>
<p><a href="https://doi.org/10.1371/journal.pgen.1006616.g006">
              https://doi.org/10.1371/journal.pgen.1006616.g006</a></p></div><p>As expected, we see a strong relationship between skin pigmentation and UVB (N = 126, T = 6.23, LRT = 55.20, p-value = 1.09E-13). The positive values of the slope and t-statistic indicate that people tend to be darker in regions with higher exposure to UVB and vice versa. This pattern provides further support to the notion that adaptation to ultraviolet radiation has had an important role in the evolution of skin pigmentation in humans. Nares width is significantly correlated with temperature (N = 140, T = 3.09, LRT = 9.24, p-value = 2.37E-03) and absolute humidity (N = 140, T = 2.73, LRT = 7.28, p-value = 6.97E-03). The positive slopes indicate that individuals from warm-humid climates, on average, tend to have wider nares whereas individuals from cool-dry climates tend to have narrower nares. This is consistent with previously reported patterns of ecogeographic variation in the width of the nasal aperture and nasal cavity [<a href="#pgen.1006616.ref041">41</a>,<a href="#pgen.1006616.ref042">42</a>]. Nares width was not significantly correlated with relative humidity (N = 140, T = -1.65, LRT = 2.71, p-value = 0.100). This is not surprising since absolute humidity is more important for the physiological functioning of the nose than relative humidity [<a href="#pgen.1006616.ref043">43</a>]. Alar base width is neither significantly correlated with temperature (N = 140, T = 1.97, LRT = 3.82, p-value = 0.051) nor with relative humidity (N = 140, T = -0.79, LRT = 0.62, p-value = 0.431), and is only mildly correlated with absolute humidity (N = 140, T = 2.09, LRT = 4.30, p-value = 0.038). This might suggest that it is not the width of the nose <em>per se</em> but the width of the nares that is under selection pressure. However, even nares width is not as strongly correlated with temperature and humidity as skin pigmentation is to UVB, suggesting that the strength of selection exerted by temperature on nose width is likely to be much weaker than the selection pressure imposed by UVB exposure on skin pigmentation.</p>
<p>Following a suggestion by one of the reviewers, we investigated whether the observed phenotype-climate correlations were driven by any one population. To do so, we re-estimated the phenotype-climate slopes (skin pigmentation vs UVB and nares width vs temperature) after removing each population in turn. <a href="#pgen-1006616-g007">Fig 7</a> shows that the 95% confidence intervals of the slope overlap with zero (red line in <a href="#pgen-1006616-g007">Fig 7</a>) only when the N. Europeans are removed, suggesting that both phenotype-climate correlations are driven primarily by the inclusion of N. Europeans. This effect might be indicative of positive selection for lighter skin pigmentation and narrower nares in higher latitude populations. However, because our sampling of populations at higher latitude is limited (<a href="#pgen-1006616-g006">Fig 6</a>), this result requires further investigation.</p>
<div data-doi="10.1371/journal.pgen.1006616.g007"><div><p><a title="Click for larger image" href="https://journals.plos.org/plosgenetics/article/figure/image?size=medium&amp;id=10.1371/journal.pgen.1006616.g007" data-doi="10.1371/journal.pgen.1006616" data-uri="10.1371/journal.pgen.1006616.g007"><img src="https://journals.plos.org/plosgenetics/article/figure/image?size=inline&amp;id=10.1371/journal.pgen.1006616.g007" alt="thumbnail" loading="lazy"></a></p></div><p><span>Fig 7. </span> The effect of removing each population on phenotype-climate correlations.</p><p>The point estimate and the 95% confidence intervals of the phenotype-climate slopes after removing each population (on the y-axis) are shown for A) skin pigmentation and UVB, B) nares width and temperature, and C) nares width and absolute humidity. In all cases, the 95% confidence interval of the slope overlaps with zero (red vertical line), only when the Europeans are removed.</p>
<p><a href="https://doi.org/10.1371/journal.pgen.1006616.g007">
              https://doi.org/10.1371/journal.pgen.1006616.g007</a></p></div></div>
</div>

<div xmlns:plos="http://plos.org" id="section3"><h2>Discussion</h2><p>The diversity of facial features across human populations has fascinated scientists for a long time. Even though genetic drift has played a predominant role in human evolution, external physical traits such as facial shape and skin pigmentation, because of their proximity to the environment, have also likely been influenced by natural selection. Substantial evidence has accumulated supporting the hypothesis that differences in skin pigmentation across human populations have evolved largely in response to selection pressures imposed by exposure to ultraviolet radiation [<a href="#pgen.1006616.ref020">20</a>]. How selection may have affected facial shape, a trait that is also quite variable between populations, is unclear, likely because it has received much less attention to date. Given the complexity of the face, we have chosen to study one particularly interesting and variable part of the face; the nose. The broad question driving this study is: Has climate adaptation played an important role in influencing variation in human nose shape? To answer this question, we formulated two hypotheses: i. divergent selection has been involved in the differentiation of certain aspects of nose shape across populations, and ii. climate is the agent of selection in cases where divergent selection can be invoked.</p>
<p>To test the first hypothesis, we used Qst–Fst comparisons to investigate whether the mean difference in nose shape among populations is greater than that expected under genetic drift alone. Estimation of Qst relies on stipulating the within- and among-population components of phenotypic variance that are due to additive genetic effects. These variance components are ideally calculated through experiments in which the effects of environmental variables can be controlled [<a href="#pgen.1006616.ref028">28</a>]. This poses a practical and ethical challenge in non-model organisms, such as humans, who are not amenable to the type of experimentation required. To circumvent these limitations, studies often calculate Qst directly from phenotype data under the assumption that the within-population heritability (<em>h</em><sup><em>2</em></sup>) is equal to the among-population heritability (<em>c</em>). Qst calculated this way is probably best considered to be the ‘minimum’ Qst proposed by Relethford (1994) [<a href="#pgen.1006616.ref035">35</a>]. However, the notion that this takes the minimum value of Qst relies on <em>h</em><sup>2</sup> always being less than <em>c</em>, which is anticonservative as has been noted previously [<a href="#pgen.1006616.ref032">32</a>]. In fact, genetic variation between-populations is likely to be less than the genetic variation within-populations for most phenotypes. Our approach is similar to previous studies in that we also estimated Qst assuming <em>c</em> = <em>h</em><sup><em>2</em></sup>. However, we used sensitivity curves to evaluate the behavior of Qst to cases where <em>c</em> &lt; <em>h</em><sup><em>2</em></sup>. In addition, we demonstrate that both within- and between-population variation in nose shape are heritable.</p>
<p>We carried out Qst–Fst comparisons for seven nose shape traits: nares width, alar base width, nasal height, length of the nasal ridge, nasal tip protrusion, external surface area, and nostril area. We found that the divergence in the width of the nares and alar base deviate from neutral expectations, and that these results are robust to the <em>c</em> = <em>h</em><sup><em>2</em></sup> assumption. This suggests that the width of the nares and alar base may have evolved across populations due to divergent selection.</p>
<p>Next, we hypothesized that the divergence of these two traits is driven by climate adaptation. To investigate this, we tested whether the spatial distribution of these traits is correlated with mean annual temperature and humidity. Our results show that nares width is strongly correlated with temperature and absolutely humidity. The positive direction of the effects indicate that wider noses are more common in warm-humid climates, while narrower noses are more common in cold-dry climates. Nares width is not, however, correlated with relative humidity. This is not surprising since absolute humidity levels are likely more important for respiration than relative humidity [<a href="#pgen.1006616.ref043">43</a>]. Alar base width is only weakly, if at all, correlated with temperature and absolute humidity, suggesting that the signal of climate adaptation might be specific to the width of the nares. Computational fluid dynamics (CFD) studies show that the geometry of the nose is important for its respiratory functions [<a href="#pgen.1006616.ref005">5</a>,<a href="#pgen.1006616.ref044">44</a>]. Inhaled air reaches 90% of the required temperature and humidity levels before even reaching the nasopharynx, implicating the nasal cavity, especially the turbinates, as the major conditioning apparatus in the respiratory tract [<a href="#pgen.1006616.ref004">4</a>,<a href="#pgen.1006616.ref005">5</a>]. We also know that the geometry of the nasal airways influences the velocity of inspired air [<a href="#pgen.1006616.ref004">4</a>,<a href="#pgen.1006616.ref007">7</a>,<a href="#pgen.1006616.ref045">45</a>]. Narrow airways in cold-dry climates might allow better conditioning by increasing the turbulence in inspired air as it reaches the turbinates, thereby facilitating contact with the nasal mucosa [<a href="#pgen.1006616.ref005">5</a>]. However, we note that nostril area does not show unusually high differentiation across populations, which suggests that it is not the size of the nostrils but the shape that might be functionally important.</p>
<p>Another hypothesis regarding the function of the nose that has received less attention compared to the air-conditioning hypothesis is that of selective brain cooling (SBC) [<a href="#pgen.1006616.ref046">46</a>]. Some large mammals are known to regulate brain temperatures in hyperthermic conditions, often caused by sustained exercise [<a href="#pgen.1006616.ref047">47</a>,<a href="#pgen.1006616.ref048">48</a>]. This ability is mostly reported in mammals with a carotid rete, which is a network of capillaries in a cavity under the brain called the cavernous sinus. Arterial blood leaving the rete eventually perfuses the brain through the Circle of Willis. Venous blood traveling back from the nasal mucosa interacts with the carotid rete in the cavernous sinus and is thought to cool the blood entering the brain in arid-zone mammals, such as Oryx [<a href="#pgen.1006616.ref049">49</a>], as well as in winter-acclimatized animals, such as reindeer [<a href="#pgen.1006616.ref050">50</a>]. Although, humans do not possess such a rete and instead have a single artery going through the cavernous sinus, which has led some to question the role of SBC in humans [<a href="#pgen.1006616.ref051">51</a>], proponents argue that the carotid rete is not a pre-requisite as SBC is present in some animals who don’t possess a rete, such as horses [<a href="#pgen.1006616.ref052">52</a>]. Since SBC in humans is highly debated, the role of nose shape differences in contributing to differences to this physiological process, while worth mentioning here, is highly speculative.</p>
<p>Climate may not have been the only factor in contributing to nose shape differences across populations. In fact, we show that temperature is only weakly correlated with nares width, especially when compared with the correlation between skin pigmentation and UVB. What then could be the selective agent driving the divergence of nose shape? We mentioned earlier that all aspects of nose shape studied here are sexually dimorphic, which raises a number of questions. Why does this sexual dimorphism exist? Is it merely a by-product of circulating hormones leading to differences in growth and development in early adulthood, or does it have an adaptive function, such as signaling sex to other males and females? Sexual selection has likely played an important role in human evolution, as evidenced by the presence of sexual dimorphism in many physical traits (e.g., height, waist-hip ratio, facial hair, and breasts to name a few). Could cultural differences in perceptions of dominance and attractiveness have had a role in the divergence of nose shape [<a href="#pgen.1006616.ref053">53</a>]? Could these perceptions have arisen to select mates who signal adaptation to the local environment? Indeed, ecological selection and sexual selection could reinforce each other, accelerating phenotypic divergence across populations in spite of continued gene flow [<a href="#pgen.1006616.ref054">54</a>]. These are interesting avenues of research, which need to be considered in order to sketch out a more complete picture of the evolution of the human nose.</p>
<p>The investigation of nose shape evolution with respect to climate adaptation, while interesting anthropologically, is also relevant medically. As humans are becoming more of a global community, the study of local adaptation is becoming more important to understanding health risks involved in living in ‘foreign’ climates. Obvious examples of such health risks are of increased risk of sunburn, skin cancer, and folate deficiency in light-skinned individuals exposed to high UVB, and of low birth weight and chronic mountain sickness associated with hypoxia at high altitudes [<a href="#pgen.1006616.ref020">20</a>,<a href="#pgen.1006616.ref055">55</a>]. Does the morphology of the external nose, or that of the inner nasal cavity affect risk of respiratory disease in different climates? It’s difficult to say at this point. While our findings provide support for the idea that differences in aspects of nose shape may have evolved across populations as a result of climate-related selection pressures, something that has been demonstrated previously using craniometric data [<a href="#pgen.1006616.ref002">2</a>,<a href="#pgen.1006616.ref012">12</a>,<a href="#pgen.1006616.ref013">13</a>,<a href="#pgen.1006616.ref041">41</a>,<a href="#pgen.1006616.ref042">42</a>], we note that the signal of climate adaptation is not very strong, especially when compared to skin pigmentation. This could be due to weaker selection pressure or selection on standing variation, but also due the sparse sampling of populations shown here, which is a limitation of this study. These results will need to be replicated in a larger set of populations. We expect that studies incorporating diverse populations who have been living long-term in a range of environments, such as the tropics, deserts, and circumpolar regions, will nicely fill in the gaps. Especially useful would be representation of populations from higher altitude regions, such as Andeans, Tibetans, and Ethiopians, who not only have to cope with the stress of a cold and dry climate, but also that of low atmospheric oxygen levels [<a href="#pgen.1006616.ref056">56</a>]. It would also be informative to study non-human primates in this context, who occupy a variety of climes and exhibit extensive variation in nose morphology. Finally, future studies should also focus on genome-wide association studies (GWASs) to identify variants contributing to nose shape. With increasing interest in identifying loci associated with facial shape, some GWASs have recently identified a number of nose shape loci [<a href="#pgen.1006616.ref057">57</a>–<a href="#pgen.1006616.ref060">60</a>]. Genetic variation at these loci will be informative about the nature of selection, as well as for inferring the timing of selection events.</p>
</div>

<div xmlns:plos="http://plos.org" id="section4"><h2>Materials and methods</h2>
<div id="section1"><h3>Ethics statement</h3>
<p>Data collection was carried out with informed consent of participants and with approval from the institutional review boards (IRBs) at The Pennsylvania State University (IRB# 32341, IRB# 45727, IRB# 44929) and at The University of Illinois Urbana-Champaign (IRB# 13103). All participants provided written informed consent.</p>
</div>

<div id="section2"><h3>Participant recruitment and description of the data</h3>
<p>The data used in this paper are part of a larger dataset that were collected in various locations through studies based at the Pennsylvania State University and at the University of Illinois at Urbana-Champaign. The majority of these data were collected in the United States at the Pennsylvania State University, at the World Science Festival in New York, and at the University of Illinois at Urbana-Champaign. Data collection in Europe took place in Dublin (Ireland), in Rome (Italy), in Warsaw (Poland), and in Porto (Portugal). The Cape Verdean data were collected in collaboration with Dr. Sandra Beleza as described in Beleza <em>et al</em>. [<a href="#pgen.1006616.ref061">61</a>]. All data were collected through informed consent from the participants with approval from the institutional review boards at The Pennsylvania State University, at the University of Illinois at Urbana-Champaign, and the National Ethical Committee for Health Research of Cape Verde.</p>
<p>The phenotype data collected include three dimensional (3D) images, skin pigmentation, standing height, and body weight. 3D images of participants were taken using the 3dMD system (<em>3dMD System</em>, <em>Atlanta</em>, <em>GA</em>). Skin pigmentation was measured with melanin index using Derma Spectrometer (<em>Cortex Technology</em>, <em>Hadsund</em>, <em>Denmark</em>). Melanin index is defined as the 100 x log<sub>10</sub> (1/percentage reflectance at 650 nM) [<a href="#pgen.1006616.ref017">17</a>]. We took three readings of melanin index from the inner left and right arm and used the mean of the six measurements for each individual. Statistical testing was performed using the inverse of melanin index since it is more normally distributed (<a href="#pgen.1006616.s007">S7 Fig</a>) but the use of untransformed melanin index produces similar results and does not affect our conclusions. Demographic information was collected through questionnaires and included self-reported ancestry and sex, participant’s birthplace, the birthplaces of the parents and grandparents, and the locations where the participant was raised.</p>
<p>The total number of participants was 4,257. Of these, we selected 2,637 individuals based on genetic ancestry and availability of phenotypic and covariate data, as described in the following sections. A summary of the sample sizes used in different analyses is provided in <a href="#pgen-1006616-t005">Table 5</a>. For a more detailed and accurate representation, refer to the Excel spreadsheet in the supplementary materials (<a href="#pgen.1006616.s009">S1 File</a>), which provides individual-level data for all samples used to generate the results presented here.</p>
</div>

<div id="section3"><h3>Estimation of genetic ancestry</h3>

<div id="section1"><h4>Genotyping and quality control.</h4><p>Out of the 4,257 participants, we had genotype data for 3,746 individuals. Since the data were collected as part of different studies over a long period of time, the samples were genotyped on four different genotyping arrays, namely the Illumina HumanHap300v1 BeadChip, the Illumina Infinium HD Human 1M-Duo Beadarray (<em>Illumina</em>, <em>Inc</em>., <em>San Diego</em>, <em>CA</em>), and the 23andMe v3 and v4 arrays (<em>23andMe Mountainview</em>, <em>CA)</em>. In order to select individuals based on genetic ancestry, we needed to merge genotypes across platforms into a single database. We took several steps to ensure there were no batch effects due to genotyping platform, which could potentially lead to biases in ancestry estimation and Fst calculation. First, we removed SNPs that were not present on all four platforms to ensure there were no systematic differences due to missingness. Second, we performed quality-control separately for each platform by removing individuals and SNPs with a genotyping rate of less than 90%. Finally, we removed palindromic (AT/GC) SNPs and recoded the data in the Illumina A/B format [<a href="#pgen.1006616.ref062">62</a>] to ensure strand differences across platforms were not an issue. This sufficiently removes any potential batch effects due to platform as evidenced by the observation that in a multidimensional scaling plot, individuals cluster by ancestry and not by platform (<a href="#pgen.1006616.s001">S1 Fig</a>). At this stage, the genotypes were merged across platforms into a single dataset. We further removed non-autosomal SNPs from the merged dataset and pruned for linkage disequilibrium (using a pairwise r<sup>2</sup> cutoff of 0.5). This resulted in 118,420 autosomal SNPs. Related individuals were identified using identity-by-state (IBS) analysis (IBS &gt; 0.8). Individuals were removed to minimize relatedness and maximize sample size (e.g. In the case of a mother-father-siblings quartet, the siblings were removed). These QC steps were carried out in Plink 1.9 [<a href="#pgen.1006616.ref063">63</a>,<a href="#pgen.1006616.ref064">64</a>].</p>
</div>

<div id="section2"><h4>Genomic ancestry estimation and principal components analysis.</h4><p>We estimated genetic ancestry for all individuals with available genotype data (N = 3,746) using an unsupervised clustering approach in ADMIXTURE. To assist with the clustering and interpretation of the resulting clusters, we merged this dataset with genotypes from the HapMap dataset for the same SNPs (Populations included: YRI, LWK, CEU, TSI, CHB, CHD, JPT, GIH, MEX; N = 988). We ran ADMIXTURE for three values of k (k = 5, 6, 7), and visually determined the optimum clustering scheme to be k = 6 for our purposes. To further visualize fine-scale structure, we carried out unsupervised principal component analysis (PCA) on the genotype data in Plink 1.9 [<a href="#pgen.1006616.ref064">64</a>]. A combination of filters based on the ADMIXTURE output and the PCA scores allowed us to select individuals from specific populations of interest. These are described below separately for each analysis.</p>
</div>
</div>

<div id="section4"><h3>Qst–Fst analyses</h3>

<div id="section1"><h4>Sub-selection of individuals.</h4><p>For the Qst–Fst analyses, we were interested in selecting individuals with ancestry primarily from one of four populations: i) North European, ii) West African, iii) East Asian, and iv) South Asian. Much of this was based on genetic ancestry calculated from dense genotype data. However, for some individuals (see <em>Northern European</em> below), self-reported ancestry information, corroborated through the birthplace of all four grandparents, was used where genotype data were unavailable.</p>
<p><em>West African (N = 40)</em>: We identified 40 individuals who had close to 90% or greater ancestry from ADMIXTURE cluster 1. ADMIXTURE cluster 1 represents ancestry from West Africa, given that individuals from the HapMap YRI (Yoruba in Ibadan, Nigeria) derive most of their ancestry from ADMIXTURE cluster 1 (<a href="#pgen-1006616-g008">Fig 8</a>). These 40 individuals were included in the ‘West African’ population group.</p>
<div data-doi="10.1371/journal.pgen.1006616.g008"><div><p><a title="Click for larger image" href="https://journals.plos.org/plosgenetics/article/figure/image?size=medium&amp;id=10.1371/journal.pgen.1006616.g008" data-doi="10.1371/journal.pgen.1006616" data-uri="10.1371/journal.pgen.1006616.g008"><img src="https://journals.plos.org/plosgenetics/article/figure/image?size=inline&amp;id=10.1371/journal.pgen.1006616.g008" alt="thumbnail" loading="lazy"></a></p></div><p><span>Fig 8. </span> ADMIXTURE Ancestry proportions (k = 6) of genotyped individuals used in Qst–Fst analyses.</p><p>Their ADMIXTURE proportions are compared with samples from the HapMap dataset. Each vertical bar in the panels is an individual and the colors represent the proportion of ancestry derived from each of 6 clusters (k = 6). In each panel, the HapMap samples are arranged on the left with a three-letter acronym for the population they are from (e.g. GIH refers to the Gujarati Indians from Houston) listed under them. The samples from our dataset are arranged to the right of the HapMap samples with their population designation (e.g. South Asian) under them.</p>
<p><a href="https://doi.org/10.1371/journal.pgen.1006616.g008">
              https://doi.org/10.1371/journal.pgen.1006616.g008</a></p></div><p><em>East Asian (N = 127)</em>: Comparison with the HapMap CHB (Han Chinese in Beijing, China), HapMap CHD (Chinese in Metropolitan Denver, Colorado), and HapMap JPT (Japanese in Tokyo, Japan) samples shows that cluster 4 represents genetic ancestry from East Asia (<a href="#pgen-1006616-g008">Fig 8</a>). We selected 166 individuals who had 90% or more ancestry from ADMIXTURE cluster 4 to be included in the ‘East Asian’ population group (<a href="#pgen-1006616-g008">Fig 8</a>). We further examined the distribution of the PCA plots and removed six individuals who clustered far from the CHB, CHD, and JPT samples. The clustering of the remaining 160 individuals around the CHB, CHD, and JPT samples is shown in <a href="#pgen.1006616.s002">S2 Fig</a>. Of these, we had high quality 3D images and complete covariate data for 127 individuals, who were then used in our analyses.</p>
<p><em>South Asian (N = 73)</em>: 98 individuals in our dataset had more than 60% ancestry from ADMIXTURE cluster 2, which seems to represent the South Asian cluster (<a href="#pgen-1006616-g008">Fig 8</a>). These 98 individuals were added to the ‘South Asian’ population group. While the cutoff here seems liberal compared to that used for the other populations, it is only meant to be a rough guide and was chosen by comparing with the ancestry proportions of the HapMap GIH (Gujarati Indians in Houston, Texas) samples (<a href="#pgen-1006616-g008">Fig 8</a>). However, we examined the distribution of the 98 individuals around the GIH (Gujarati Indians in Houston, Texas) samples using PCA plots (<a href="#pgen.1006616.s002">S2 Fig</a>), and noticed that six individuals appear to fall far from the main cluster, who were subsequently removed. Of the remaining, we had high quality 3D images and complete covariate data for 73 individuals, who were used for further analyses.</p>
<p><em>Northern European (N = 236)</em>: Our full dataset had a large number of individuals with European ancestry from different regions of Europe. In order to minimize structure within Europe, we only retained individuals with Northern European ancestry. Thus, we restricted the analysis to Europeans collected in Ireland (N = 151) and Poland (N = 85) who reported that the ancestry of all four of their grandparents was from the region of sampling. More details for these samples are described in Candille <em>et al</em>. [<a href="#pgen.1006616.ref065">65</a>]. Of this set, we had genotypes for 73 females (Irish = 37, Polish = 36) but none for males. This is the reason why only females were included in the phenotype-climate analyses, where genotypes were used to construct the genetic relationship matrix. The ADMIXTURE results for these women are shown in <a href="#pgen-1006616-g008">Fig 8</a> alongside the results of the CEU (Utah residents with Northern and Western European ancestry from the CEPH collection) and TSI (Tuscans from Italy) samples.</p>
</div>

<div id="section2"><h4>Qst calculation.</h4><p>Estimation of the variance components, <span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e015" loading="lazy"></span> and <span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e016" loading="lazy"></span>, was carried out using a linear model in which population group was treated as a fixed effect, with sex, age, and BMI as covariates. If <em>MSB</em> is the mean square among populations, <em>MSW</em> is the mean square within populations, <em>n</em><sub><em>i</em></sub> is the number of individuals in the i<sup>th</sup> populations, and <em>a</em> is the number of populations, <span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e017" loading="lazy"></span> and <span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e018" loading="lazy"></span> can be estimated as described as follows [<a href="#pgen.1006616.ref010">10</a>,<a href="#pgen.1006616.ref066">66</a>,<a href="#pgen.1006616.ref067">67</a>]:
<a name="pgen.1006616.e019" id="pgen.1006616.e019"></a><span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e019" loading="lazy"><span>(3)</span></span>
<a name="pgen.1006616.e020" id="pgen.1006616.e020"></a><span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e020" loading="lazy"><span>(4)</span></span>
<a name="pgen.1006616.e021" id="pgen.1006616.e021"></a><span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e021" loading="lazy"><span>(5)</span></span></p>
</div>

<div id="section3"><h4>Fst calculation.</h4><p>Wright’s Fst measures the genetic divergence between two or more populations. We estimated the Fst across the four populations for the LD-pruned set of 118,420 autosomal SNPs, described above, using Weir and Cockerham’s <em>θ</em> [<a href="#pgen.1006616.ref068">68</a>]. The genomic Fst distribution in shown in <a href="#pgen.1006616.s003">S3 Fig</a>.</p>
</div>

<div id="section4"><h4>Qst–Fst comparison.</h4><p>We used a non-parametric bootstrap approach to test whether the Qst of a phenotype is significantly greater than the Fst. This was done by computing Qst from subsamples generated by randomly selecting individuals with replacement, within population and sex such that the number of males and females in each population remains unchanged. Values of Fst were simulated by randomly sampling from the Fst of 118,420 SNPs (<a href="#pgen.1006616.s003">S3 Fig</a>). We generated 10,000 such pseudo-samples and, for each simulated value of Qst and Fst, computed the difference Qst–Fst. High values of Qst–Fst are indicative of divergent selection, and the empirical p-value for this test was determined by calculating the proportion of Qst–Fst values that are less than zero.</p>
</div>
</div>

<div id="section5"><h3>Estimation of <span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e022" loading="lazy"></span> and <em>c</em></h3>

<div id="section1"><h4>Estimation of <span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e023" loading="lazy"></span>.</h4><p>For the calculation of <span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e024" loading="lazy"></span>, we identified 1,825 unrelated individuals with primarily European ancestry, who derived more than 80% of their combined genetic ancestry from Cluster 3 and Cluster 6 based on the ADMIXTURE output (<a href="#pgen-1006616-g009">Fig 9</a>). This cutoff was chosen based on comparison with the genetic ancestries of CEU (Utah residents with Northern and Western European ancestry from the CEPH collection) and TSI (Toscans in Italy) individuals from the HapMap dataset (<a href="#pgen-1006616-g009">Fig 9</a>). We used GCTA to calculate the genome-wide kinship matrix for the 1,825 individuals from 118,420 autosomal SNPs, which overlapped across all four platforms. Of the 1,825 individuals with European ancestry, we had measured height for 1,825, measured melanin index for 1,231, and 3D photos for 1,718 individuals. From these data, we used GCTA to estimate the proportion of phenotypic variance explained by genotyped SNPs (<em>h</em><sub><em>g</em></sub><sup><em>2</em></sup>) for height, skin pigmentation, and nose shape aspects. The top four genetic PCs were included as covariates in the model to correct for population structure, which can inflate heritability estimates [<a href="#pgen.1006616.ref018">18</a>,<a href="#pgen.1006616.ref069">69</a>,<a href="#pgen.1006616.ref070">70</a>]. Also included in the model were sex, age, and BMI.</p>
<div data-doi="10.1371/journal.pgen.1006616.g009"><div><p><a title="Click for larger image" href="https://journals.plos.org/plosgenetics/article/figure/image?size=medium&amp;id=10.1371/journal.pgen.1006616.g009" data-doi="10.1371/journal.pgen.1006616" data-uri="10.1371/journal.pgen.1006616.g009"><img src="https://journals.plos.org/plosgenetics/article/figure/image?size=inline&amp;id=10.1371/journal.pgen.1006616.g009" alt="thumbnail" loading="lazy"></a></p></div><p><span>Fig 9. </span> ADMIXTURE ancestry proportions (k = 6) of 1,825 individuals with primarily European ancestry, who were used to estimate <em>h<sub>g</sub><sup>2</sup></em>.</p><p>Each bar represents an individual and the different colors represent the ADMIXTURE proportions from six clusters (k = 6). The ADMIXTURE proportions for HapMap European populations (CEU and TSI) are shown on the left for comparison.</p>
<p><a href="https://doi.org/10.1371/journal.pgen.1006616.g009">
              https://doi.org/10.1371/journal.pgen.1006616.g009</a></p></div></div>

<div id="section2"><h4>Estimation of c.</h4><p>We estimated the proportion of phenotypic variance between populations due to additive genetic effects (<em>c</em>) in an admixed population from Cape Verde, who derive their ancestry primarily from W. Africans and Europeans (<a href="#pgen-1006616-g010">Fig 10</a>), using the approach described in Zaitlen <em>et al</em>. [<a href="#pgen.1006616.ref040">40</a>]. 697 Cape Verdeans were genotyped on the Illumina Infinium HD Human 1M-Duo Beadarray (<em>Illumina</em>, <em>Inc</em>., <em>San Diego</em>, <em>CA</em>) for a total of 1,016,423 SNPs. This was reduced to 623,625 autosomal SNPs after removing palindromic SNPs, SNPs that did not overlap with the HapMap dataset, and other SNPs based on filters described in Beleza <em>et al</em>. [<a href="#pgen.1006616.ref017">17</a>]. We had high quality 3D images and complete covariate data for 409 of the 697 Cape Verdeans who were genotyped. For these individuals, we calculated local ancestry at 623,625 autosomal SNPs with LAMP-LD [<a href="#pgen.1006616.ref071">71</a>] using phased genotypes from the HapMap CEU and YRI samples as reference. Averaging local ancestry across all SNPs yields an estimate of global ancestry that is highly correlated with estimates obtained from ADMIXTURE using k = 2 (r<sup>2</sup> ∼ 1) (<a href="#pgen.1006616.s004">S4 Fig</a>). We used GCTA to calculate a kinship matrix based on local ancestry and to estimate the proportion of phenotypic variance explained by local ancestry (<em>h</em><sub><em>y</em></sub><sup><em>2</em></sup>). Global ancestry, calculated from the mean of local ancestry across all SNPs, was included as a covariate to correct for ancestry stratification and any environmental variables that might covary with ancestry [<a href="#pgen.1006616.ref016">16</a>,<a href="#pgen.1006616.ref072">72</a>]. Sex, age, and BMI were also included as covariates.</p>
<div data-doi="10.1371/journal.pgen.1006616.g010"><div><p><a title="Click for larger image" href="https://journals.plos.org/plosgenetics/article/figure/image?size=medium&amp;id=10.1371/journal.pgen.1006616.g010" data-doi="10.1371/journal.pgen.1006616" data-uri="10.1371/journal.pgen.1006616.g010"><img src="https://journals.plos.org/plosgenetics/article/figure/image?size=inline&amp;id=10.1371/journal.pgen.1006616.g010" alt="thumbnail" loading="lazy"></a></p></div><p><span>Fig 10. </span> ADMIXTURE ancestry proportions (k = 6) of 409 Cape Verdeans, who were used to estimate <em>c</em>.</p><p>Each bar represents an individual and the different colors represent the ADMIXTURE proportions from six clusters (k = 6).</p>
<p><a href="https://doi.org/10.1371/journal.pgen.1006616.g010">
              https://doi.org/10.1371/journal.pgen.1006616.g010</a></p></div></div>
</div>

<div id="section6"><h3>Phenotype-climate associations</h3>

<div id="section1"><h4>Testing for local adaptation.</h4><p>The primary focus here was to answer the question: Does nose shape variation between geographically distant populations show signals of local adaptation? To answer this, we needed to ascribe a climate value to every individual that can serve as a proxy of their ancestral environment. For this reason, we selected individuals used in the Qst–Fst analyses who reported at least one parent’s birthplace. These were further filtered to retain individuals whose parents’ birthplaces coincided with their continental ancestry, for example, East Asian individuals whose parents were born in the United States were removed. Finally, we restricted the analyses to females since genetic data were not available for males sampled in Ireland and Poland. This resulted in 140 females with genetic, phenotypic, and geographic data. We have plotted the geographic distribution of the parents of these individuals in <a href="#pgen-1006616-g006">Fig 6</a> with a line connecting the two parents of each individual. A single dot means that the parents were born in the same location. These locations were geocoded to obtain the latitude and longitude in decimal degrees using the ggmap package in R [<a href="#pgen.1006616.ref073">73</a>,<a href="#pgen.1006616.ref074">74</a>].</p>
<p>Gridded climate data were obtained in latitude/longitude coordinate system. Climate grids with mean monthly values for temperature (measured in degrees Celsius), and vapor pressure (measured in hectopascal or hPa) at 0.5° resolution for the period between 1901 and 2015 were obtained from The Climate Research Unit TS v3.24 Dataset (<a href="https://crudata.uea.ac.uk/cru/data/hrg/cru_ts_3.24/">https://crudata.uea.ac.uk/cru/data/hrg/cru_ts_3.24/</a>) [<a href="#pgen.1006616.ref075">75</a>]. Ground-level Ultraviolet-B (UVB) irradiance data (expressed in J/m<sup>2</sup>), for the period between 1996 and 2005, generated by the Earth Probe Total Ozone Mapping Spectrometer (EP/TOMS) Team at 0.5° resolution, were downloaded from the Goddard Earth Sciences Data and Information Services Center (GES DISC) website (<a href="https://disc.gsfc.nasa.gov/">https://disc.gsfc.nasa.gov/</a>) [<a href="#pgen.1006616.ref076">76</a>]. Relative humidity and absolute humidity grids were derived from temperature and vapor pressure as described in Maddux <em>et al</em>. (2016) [<a href="#pgen.1006616.ref043">43</a>]. For each of the four climate variables (UVB, temperature, absolute humidity, and relative humidity), a single grid was constructed by averaging across all months within the time period to represent long-term climate. R packages RnetCDF [<a href="#pgen.1006616.ref077">77</a>] and Raster [<a href="#pgen.1006616.ref078">78</a>] were used to read gridded data and to extract climate values at each location. We used climate values at the parents’ birthplace to represent the ancestral climate for each individual. Where the birthplace of both parents was known, the mean climate values extracted at the two locations was used. Both parents usually come from nearby regions, at least on a continental scale (<a href="#pgen-1006616-g006">Fig 6</a>), and the climate values for the parents are quite similar (<a href="#pgen.1006616.s005">S5 Fig</a>).</p>
<p>We used a linear mixed model to test the relationship between phenotype and climate variables, while correcting for age and BMI. We took non-independence among observations due to genetic similarity into account by allowing the observations to be correlated according to the SNP-based genome-wide kinship matrix. In matrix notation, the linear model takes the following form:
<a name="pgen.1006616.e025" id="pgen.1006616.e025"></a><span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e025" loading="lazy"><span>(6)</span></span>
Where <em>β</em> are fixed effects and <em>ε</em> is the random error term. Thus, <em>ε</em> ∼ <em>N</em>(0,<em>Gσ</em><sub><em>e</em></sub>), where <em>G</em> is the genome-wide kinship matrix, which was constructed from the genotypes at 118,420 SNPs in GCTA [<a href="#pgen.1006616.ref037">37</a>]. The slopes were estimated using maximum likelihood, implemented in the spaMM package in R [<a href="#pgen.1006616.ref074">74</a>,<a href="#pgen.1006616.ref079">79</a>]. P-values for each slope were generated from the likelihood ratio between full and reduced models (fit by removing each climate predictor).</p>
</div>
</div>

<div id="section7"><h3>Morphometric measurements</h3>

<div id="section1"><h4>Imaging processing.</h4><p>We captured high resolution 3D images of participants’ faces using the 3dMD Face system (<em>3dMD Atlanta</em>, <em>GA</em>). Five positioning landmarks (two on the inner corner of the eyes, two on the outer corners of the mouth, and one on the tip of the nose) were placed on each face in order to establish facial orientation. A spatially dense mesh of 7,150 quasi-landmarks (QLs) was mapped onto each image and its reflection. As a result of this mapping, the vertices and triangles are homologous across individuals [<a href="#pgen.1006616.ref080">80</a>]. Generalized Procrustes Superimposition [<a href="#pgen.1006616.ref024">24</a>] was carried out on both sets of images (original and reflected) to remove differences in position and orientation. The Procrustes coordinates of the original and reflected image for each participant were then averaged to remove effects of bilateral asymmetry. We selected the nose region (see outline in <a href="#pgen-1006616-g001">Fig 1</a>), which is comprised of 709 out of the 7,150 QLs making up the entire face, for further downstream analyses.</p>
</div>

<div id="section2"><h4>Linear distances.</h4><p>Linear distances were calculated using three mid-line landmarks; i. nasion (n), ii. pronasale (prn), iii. subnasale (sn), and two sets of paired landmarks; iv. left and right alar curvature (al<sub>l</sub> and al<sub>r</sub>), and v. left and right alar base (ac<sub>l</sub> and ac<sub>r</sub>). These landmarks have been defined previously [<a href="#pgen.1006616.ref001">1</a>,<a href="#pgen.1006616.ref026">26</a>] and their approximate positions are shown in <a href="#pgen-1006616-g001">Fig 1</a>. Two different observers placed each of these seven landmarks three different times on the spatially-dense QL template mesh that was mapped on all the faces. Because the QLs are homologous across individuals, we can automatically select corresponding landmarks for all individuals by indicating these on the QL template. This is possible because any point on the template surface can be expressed as a function of QLs using barycentric coordinates on triangles. Using this approach, we obtained the x, y, and z coordinates of the above-mentioned landmarks for each individual. The Euclidean distances (in mm) were then calculated separately for each of the six replicate placements, which were averaged across replicates and then averaged between the two observers, to obtain mean distances for each individual.</p>
</div>

<div id="section3"><h4>Nostril area.</h4><p>To measure area of the nostrils, the observers defined regions around the nostrils (approximate location shown in <a href="#pgen-1006616-g001">Fig 1</a>) three different times. The area of a region was calculated as the total area of the polygons within the region. Using this approach, we calculated the mean area of the right and left nostrils (in mm<sup>2</sup>) for each replicate. This was then averaged across replicates for the same observer, then across observers to obtain the mean nostril area for each individual.</p>
</div>

<div id="section4"><h4>External surface area.</h4><p>The external surface area of the nose (also in mm<sup>2</sup>) was calculated by summing the areas of polygons bounded by the 709 QLs, defining the entire nose region (outlined in <a href="#pgen-1006616-g001">Fig 1</a>).</p>
</div>

<div id="section5"><h4>Intra-observer error and inter-observer reliability.</h4><p>We calculated the standard deviation in the measurement of linear distances (in mm) and nostril area (in mm<sup>2</sup>) made by the two observers to estimate the degree of intra-observer error. The quantiles for these standard deviations are shown in <a href="#pgen-1006616-t006">Table 6</a>. The measurements from Observer2 are slightly more internally consistent than Observer1. For Observer1, nasal tip protrusion was the least precise distance measure with 95% of the standard deviations falling between 0.791mm and 1.165mm, with error for the remaining distances well under 1mm. The error distribution for nostril area was similar for both Observer1 (95% CI: 0.914–1.930) and Observer2 (95% CI: 0.888–2.243).</p>
<p>Inter-observer reliability was evaluated using intraclass correlation coefficient (ICC). ICC for each measure was calculated using linear mixed models where the observer was treated as fixed effect and subject was treated as random effect. This was carried out using the nlme package in R [<a href="#pgen.1006616.ref074">74</a>,<a href="#pgen.1006616.ref081">81</a>]. With the exception of nasal tip protrusion (ICC = 0.819), the ICCs for all other measures were quite high (ICC &gt; 0.95). This concordance is evident by comparing the mean measurements between Observer1 and Observer2 (<a href="#pgen.1006616.s006">S6 Fig</a>)</p>
</div>
</div>
</div>

<div xmlns:plos="http://plos.org" id="section5"><h2>Supporting information</h2><div><h3><a href="https://journals.plos.org/plosgenetics/article/file?type=supplementary&amp;id=10.1371/journal.pgen.1006616.s001">S1 Fig. </a></h3><p><strong>MDS plots (k = 2) showing clustering of genotyped individuals by A) genotyping platform and B) by ancestry.</strong> In B) the genotyped individuals from the study set are shown in black, while the HapMap samples are shown in color for reference. The plot shows that individuals cluster by ancestry and not by genotyping platform. Thus, batch effects due to genotyping platform are unlikely to lead to artifacts in ancestry inference.</p>
<p><a href="https://doi.org/10.1371/journal.pgen.1006616.s001">https://doi.org/10.1371/journal.pgen.1006616.s001</a></p><p>(TIF)</p>
</div><div><h3><a href="https://journals.plos.org/plosgenetics/article/file?type=supplementary&amp;id=10.1371/journal.pgen.1006616.s002">S2 Fig. </a></h3><p><strong>PCA plots showing clustering of genotyped individuals by A) South Asian and B) East Asian ancestry around HapMap reference samples.</strong> The colored points are reference samples from the HapMap dataset and the grey points are individuals from our dataset. A) South Asian individuals are shown with the HapMap GIH samples and B) East Asian individuals are shown with the combined East Asian sample (CHB+JPT+CHD) from the HapMap dataset. The red lines indicate the cutoff for removing individuals who appear to cluster far away from the main cluster. The individuals who were removed, based on clustering, are shown as triangles, whereas all other individuals are shown as circles.</p>
<p><a href="https://doi.org/10.1371/journal.pgen.1006616.s002">https://doi.org/10.1371/journal.pgen.1006616.s002</a></p><p>(TIF)</p>
</div><div><h3><a href="https://journals.plos.org/plosgenetics/article/file?type=supplementary&amp;id=10.1371/journal.pgen.1006616.s004">S4 Fig. </a>Global ancestry calculated using LAMP-LD is highly correlated with global ancestry calculated using ADMIXTURE.</h3><p>ADMIXTURE-based ancestry estimates were calculated using a k = 2 assumption. LAMP-LD-based ancestry estimates were calculating the mean local ancestry across 623,625 autosomal SNPs.</p>
<p><a href="https://doi.org/10.1371/journal.pgen.1006616.s004">https://doi.org/10.1371/journal.pgen.1006616.s004</a></p><p>(TIF)</p>
</div><div><h3><a href="https://journals.plos.org/plosgenetics/article/file?type=supplementary&amp;id=10.1371/journal.pgen.1006616.s007">S7 Fig. </a></h3><p><strong>QQplots showing that the residuals for A) the inverse of melanin index are more normally distributed than B) untransformed melanin index.</strong> The points are residuals from linear models where population, sex, age, and BMI were used as predictors.</p>
<p><a href="https://doi.org/10.1371/journal.pgen.1006616.s007">https://doi.org/10.1371/journal.pgen.1006616.s007</a></p><p>(TIF)</p>
</div></div>





<div xmlns:plos="http://plos.org"><h2>Acknowledgments</h2>
<p>The authors would first and foremost like to thank the study participants, without whom none of this would have been possible. We are grateful to the Shriver Lab team, Ripan Malhi, and the members of his lab for their efforts in collection and organization of the data. We are also grateful to Sandra Beleza and Isabel Inês Araújo for their efforts in collecting and sharing the Cape Verdean data. We acknowledge Ine Saye from Peter Claes’ group for helping with morphometric analyses. We also thank Nina Jablonski, George Chaplin, George Perry, Laurel Pearson, Julie White, and Matthew Reimherr for helpful discussions on the analytical aspects of the paper. Finally, we would like to thank two anonymous reviewers for providing helpful critique and comments on the first submission of this manuscript.</p>
</div><div xmlns:plos="http://plos.org"><h2>Author Contributions</h2><ol>

<li><strong>Conceptualization:</strong> AAZ MDS.</li>

<li><strong>Data curation:</strong> AAZ BCM MDS BM.</li>

<li><strong>Formal analysis:</strong> AAZ.</li>

<li><strong>Funding acquisition:</strong> MDS PC CH.</li>

<li><strong>Investigation:</strong> AAZ BCM CH MDS.</li>

<li><strong>Methodology:</strong> AAZ MDS.</li>

<li><strong>Project administration:</strong> MDS CH BCM BM.</li>

<li><strong>Resources:</strong> MDS CH PC BM.</li>

<li><strong>Software:</strong> PC AAZ.</li>

<li><strong>Supervision:</strong> MDS.</li>

<li><strong>Visualization:</strong> AAZ PC.</li>

<li><strong>Writing – original draft:</strong> AAZ.</li>

<li><strong>Writing – review &amp; editing:</strong> AAZ MDS PC CH BCM BM.</li>

</ol>
</div><div xmlns:plos="http://plos.org"><h2>References</h2><ol><li id="ref1"><span>1.
            </span><a name="pgen.1006616.ref001" id="pgen.1006616.ref001"></a>Farkas LG, Kolar JC, Munro IR. Geography of the nose: a morphometric study. Aesthetic Plast Surg. 1986;10: 191–223. pmid:3812136 <ul><li><a href="#" data-author="Farkas" data-cit="FarkasLG%2C%20KolarJC%2C%20MunroIR.%20Geography%20of%20the%20nose%3A%20a%20morphometric%20study.%20Aesthetic%20Plast%20Surg.%201986%3B10%3A%20191%E2%80%93223.%203812136" data-title="Geography%20of%20the%20nose%3A%20a%20morphometric%20study" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/3812136" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Geography+of+the+nose%3A+a+morphometric+study+Farkas+1986" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref2"><span>2.
            </span><a name="pgen.1006616.ref002" id="pgen.1006616.ref002"></a>Franciscus RG, Long JC. Variation in human nasal height and breadth. Am J Phys Anthropol. 1991;85: 419–427.  pmid:1928315 <ul data-doi="10.1002/ajpa.1330850406"><li><a href="https://doi.org/10.1002/ajpa.1330850406" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/1928315" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Variation+in+human+nasal+height+and+breadth+Franciscus+1991" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref3"><span>3.
            </span><a name="pgen.1006616.ref003" id="pgen.1006616.ref003"></a>Negus VE. Introduction to the comparative anatomy of the nose and paranasal sinuses. Ann R Coll Surg Engl. 1954;15: 141–171. pmid:13198060 <ul><li><a href="#" data-author="Negus" data-cit="NegusVE.%20Introduction%20to%20the%20comparative%20anatomy%20of%20the%20nose%20and%20paranasal%20sinuses.%20Ann%20R%20Coll%20Surg%20Engl.%201954%3B15%3A%20141%E2%80%93171.%2013198060" data-title="Introduction%20to%20the%20comparative%20anatomy%20of%20the%20nose%20and%20paranasal%20sinuses" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/13198060" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Introduction+to+the+comparative+anatomy+of+the+nose+and+paranasal+sinuses+Negus+1954" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref4"><span>4.
            </span><a name="pgen.1006616.ref004" id="pgen.1006616.ref004"></a>Hanna LM, Scherer PW. A Theoretical Model of Localized Heat and Water Vapor Transport in the Human Respiratory Tract. J Biomech Eng. American Society of Mechanical Engineers; 1986;108: 19. pmid:3959548 <ul><li><a href="#" data-author="Hanna" data-cit="HannaLM%2C%20SchererPW.%20A%20Theoretical%20Model%20of%20Localized%20Heat%20and%20Water%20Vapor%20Transport%20in%20the%20Human%20Respiratory%20Tract.%20J%20Biomech%20Eng.%20American%20Society%20of%20Mechanical%20Engineers%3B%201986%3B108%3A%2019.%203959548" data-title="A%20Theoretical%20Model%20of%20Localized%20Heat%20and%20Water%20Vapor%20Transport%20in%20the%20Human%20Respiratory%20Tract" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/3959548" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=A+Theoretical+Model+of+Localized+Heat+and+Water+Vapor+Transport+in+the+Human+Respiratory+Tract+Hanna+1986" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref5"><span>5.
            </span><a name="pgen.1006616.ref005" id="pgen.1006616.ref005"></a>Naftali S, Rosenfeld M, Wolf M, Elad D. The Air-Conditioning Capacity of the Human Nose. Ann Biomed Eng. 2005;33: 545–553. pmid:15909660 <ul><li><a href="#" data-author="Naftali" data-cit="NaftaliS%2C%20RosenfeldM%2C%20WolfM%2C%20EladD.%20The%20Air-Conditioning%20Capacity%20of%20the%20Human%20Nose.%20Ann%20Biomed%20Eng.%202005%3B33%3A%20545%E2%80%93553.%2015909660" data-title="The%20Air-Conditioning%20Capacity%20of%20the%20Human%20Nose" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/15909660" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=The+Air-Conditioning+Capacity+of+the+Human+Nose+Naftali+2005" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref6"><span>6.
            </span><a name="pgen.1006616.ref006" id="pgen.1006616.ref006"></a>Randell SH, Boucher RC. Effective mucus clearance is essential for respiratory health. Am J Respir Cell Mol Biol. 2006;35: 20–8.  pmid:16528010 <ul data-doi="10.1165/rcmb.2006-0082SF"><li><a href="https://doi.org/10.1165/rcmb.2006-0082SF" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/16528010" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Effective+mucus+clearance+is+essential+for+respiratory+health+Randell+2006" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref7"><span>7.
            </span><a name="pgen.1006616.ref007" id="pgen.1006616.ref007"></a>Churchill SE, Shackelford LL, Georgi JN, Black MT. Morphological variation and airflow dynamics in the human nose. Am J Hum Biol. 2004;16: 625–638.  pmid:15495233 <ul data-doi="10.1002/ajhb.20074"><li><a href="https://doi.org/10.1002/ajhb.20074" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/15495233" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Morphological+variation+and+airflow+dynamics+in+the+human+nose+Churchill+2004" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref8"><span>8.
            </span><a name="pgen.1006616.ref008" id="pgen.1006616.ref008"></a>Weiner JS. Nose Shape and Climate. Am J Phys Anthropol. 1954;12: 615–618. pmid:14350081 <ul><li><a href="#" data-author="Weiner" data-cit="WeinerJS.%20Nose%20Shape%20and%20Climate.%20Am%20J%20Phys%20Anthropol.%201954%3B12%3A%20615%E2%80%93618.%2014350081" data-title="Nose%20Shape%20and%20Climate" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/14350081" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Nose+Shape+and+Climate+Weiner+1954" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref9"><span>9.
            </span><a name="pgen.1006616.ref009" id="pgen.1006616.ref009"></a>Roseman CC, Weaver TD. Multivariate apportionment of global human craniometric diversity. Am J Phys Anthropol. 2004;125: 257–263.  pmid:15386236 <ul data-doi="10.1002/ajpa.10424"><li><a href="https://doi.org/10.1002/ajpa.10424" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/15386236" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Multivariate+apportionment+of+global+human+craniometric+diversity+Roseman+2004" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref10"><span>10.
            </span><a name="pgen.1006616.ref010" id="pgen.1006616.ref010"></a>Spitze K. Population structure in Daphnia obtusa: quantitative genetic and allozymic variation. Genetics. 1993;135: 367–374. Available: <a href="http://www.genetics.org/content/135/2/367">http://www.genetics.org/content/135/2/367</a> pmid:8244001 <ul><li><a href="#" data-author="Spitze" data-cit="SpitzeK.%20Population%20structure%20in%20Daphnia%20obtusa%3A%20quantitative%20genetic%20and%20allozymic%20variation.%20Genetics.%201993%3B135%3A%20367%E2%80%93374.%20Available%3A%20http%3A%2F%2Fwww.genetics.org%2Fcontent%2F135%2F2%2F367%208244001" data-title="Population%20structure%20in%20Daphnia%20obtusa%3A%20quantitative%20genetic%20and%20allozymic%20variation" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/8244001" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Population+structure+in+Daphnia+obtusa%3A+quantitative+genetic+and+allozymic+variation+Spitze+1993" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref11"><span>11.
            </span><a name="pgen.1006616.ref011" id="pgen.1006616.ref011"></a>Whitlock MC, Guillaume F. Testing for spatially divergent selection: comparing QST to FST. Genetics. 2009;183: 1055–63.  pmid:19687138 <ul data-doi="10.1534/genetics.108.099812"><li><a href="https://doi.org/10.1534/genetics.108.099812" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/19687138" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Testing+for+spatially+divergent+selection%3A+comparing+QST+to+FST+Whitlock+2009" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref12"><span>12.
            </span><a name="pgen.1006616.ref012" id="pgen.1006616.ref012"></a>Roseman CC. Detecting interregionally diversifying natural selection on modern human cranial form by using matched molecular and morphometric data. Proc Natl Acad Sci U S A. 2004;101: 12824–9.  pmid:15326305 <ul data-doi="10.1073/pnas.0402637101"><li><a href="https://doi.org/10.1073/pnas.0402637101" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/15326305" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Detecting+interregionally+diversifying+natural+selection+on+modern+human+cranial+form+by+using+matched+molecular+and+morphometric+data+Roseman+2004" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref13"><span>13.
            </span><a name="pgen.1006616.ref013" id="pgen.1006616.ref013"></a>Hubbe M, Hanihara T, Harvati K. Climate signatures in the morphological differentiation of worldwide modern human populations. Anat Rec. 2009;292: 1720–1733. <ul><li><a href="#" data-author="Hubbe" data-cit="HubbeM%2C%20HaniharaT%2C%20HarvatiK.%20Climate%20signatures%20in%20the%20morphological%20differentiation%20of%20worldwide%20modern%20human%20populations.%20Anat%20Rec.%202009%3B292%3A%201720%E2%80%931733." data-title="Climate%20signatures%20in%20the%20morphological%20differentiation%20of%20worldwide%20modern%20human%20populations" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Climate+signatures+in+the+morphological+differentiation+of+worldwide+modern+human+populations+Hubbe+2009" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref14"><span>14.
            </span><a name="pgen.1006616.ref014" id="pgen.1006616.ref014"></a>Guo J, Tan J, Yang Y, Zhou H, Hu S, Hashan A, et al. Variation and signatures of selection on the human face. J Hum Evol. Elsevier Ltd; 2014;75: 143–152.  pmid:25186351 <ul data-doi="10.1016/j.jhevol.2014.08.001"><li><a href="https://doi.org/10.1016/j.jhevol.2014.08.001" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/25186351" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Variation+and+signatures+of+selection+on+the+human+face+Guo+2014" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref15"><span>15.
            </span><a name="pgen.1006616.ref015" id="pgen.1006616.ref015"></a>Shriver MD, Parra EJ, Dios S, Bonilla C, Norton H, Jovel C, et al. Skin pigmentation, biogeographical ancestry and admixture mapping. Hum Genet. 2003;112: 387–99.  pmid:12579416 <ul data-doi="10.1007/s00439-002-0896-y"><li><a href="https://doi.org/10.1007/s00439-002-0896-y" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/12579416" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Skin+pigmentation%2C+biogeographical+ancestry+and+admixture+mapping+Shriver+2003" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref16"><span>16.
            </span><a name="pgen.1006616.ref016" id="pgen.1006616.ref016"></a>Parra EJ, Kittles R a, Shriver MD. Implications of correlations between skin color and genetic ancestry for biomedical research. Nat Genet. 2004;36: S54–S60.  pmid:15508005 <ul data-doi="10.1038/ng1440"><li><a href="https://doi.org/10.1038/ng1440" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/15508005" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Implications+of+correlations+between+skin+color+and+genetic+ancestry+for+biomedical+research+Parra+2004" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref17"><span>17.
            </span><a name="pgen.1006616.ref017" id="pgen.1006616.ref017"></a>Beleza S, Johnson NA, Candille SI, Absher DM, Coram MA, Lopes J, et al. Genetic architecture of skin and eye color in an African-European admixed population. Spritz RA, editor. PLoS Genet. Public Library of Science; 2013;9: e1003372.  pmid:23555287 <ul data-doi="10.1371/journal.pgen.1003372"><li><a href="https://doi.org/10.1371/journal.pgen.1003372" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/23555287" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Genetic+architecture+of+skin+and+eye+color+in+an+African-European+admixed+population.+Spritz+RA%2C+editor+Beleza+2013" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref18"><span>18.
            </span><a name="pgen.1006616.ref018" id="pgen.1006616.ref018"></a>Yang J, Benyamin B, McEvoy BP, Gordon S, Henders AK, Nyholt DR, et al. Common SNPs explain a large proportion of the heritability for human height. Nat Genet. Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.; 2010;42: 565–9.  pmid:20562875 <ul data-doi="10.1038/ng.608"><li><a href="https://doi.org/10.1038/ng.608" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/20562875" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Common+SNPs+explain+a+large+proportion+of+the+heritability+for+human+height+Yang+2010" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref19"><span>19.
            </span><a name="pgen.1006616.ref019" id="pgen.1006616.ref019"></a>Turchin MC, Chiang CWK, Palmer CD, Sankararaman S, Reich D, Hirschhorn JN. Evidence of widespread selection on standing variation in Europe at height-associated SNPs. Nat Genet. Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.; 2012;44: 1015–9.  pmid:22902787 <ul data-doi="10.1038/ng.2368"><li><a href="https://doi.org/10.1038/ng.2368" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/22902787" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Evidence+of+widespread+selection+on+standing+variation+in+Europe+at+height-associated+SNPs+Turchin+2012" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref20"><span>20.
            </span><a name="pgen.1006616.ref020" id="pgen.1006616.ref020"></a>Jablonski NG. the Evolution of Human Skin and Skin Color. Annu Rev Anthropol. 2004;33: 585–623. <ul><li><a href="#" data-author="Jablonski" data-cit="JablonskiNG.%20the%20Evolution%20of%20Human%20Skin%20and%20Skin%20Color.%20Annu%20Rev%20Anthropol.%202004%3B33%3A%20585%E2%80%93623." data-title="the%20Evolution%20of%20Human%20Skin%20and%20Skin%20Color" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=the+Evolution+of+Human+Skin+and+Skin+Color+Jablonski+2004" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref21"><span>21.
            </span><a name="pgen.1006616.ref021" id="pgen.1006616.ref021"></a>Lamason RL, Mohideen M-APK, Mest JR, Wong AC, Norton HL, Aros MC, et al. SLC24A5, a putative cation exchanger, affects pigmentation in zebrafish and humans. Science. 2005;310: 1782–6.  pmid:16357253 <ul data-doi="10.1126/science.1116238"><li><a href="https://doi.org/10.1126/science.1116238" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/16357253" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=SLC24A5%2C+a+putative+cation+exchanger%2C+affects+pigmentation+in+zebrafish+and+humans+Lamason+2005" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref22"><span>22.
            </span><a name="pgen.1006616.ref022" id="pgen.1006616.ref022"></a>Norton HL, Kittles RA, Parra E, McKeigue P, Mao X, Cheng K, et al. Genetic evidence for the convergent evolution of light skin in Europeans and East Asians. Mol Biol Evol. 2007;24: 710–22.  pmid:17182896 <ul data-doi="10.1093/molbev/msl203"><li><a href="https://doi.org/10.1093/molbev/msl203" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/17182896" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Genetic+evidence+for+the+convergent+evolution+of+light+skin+in+Europeans+and+East+Asians+Norton+2007" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref23"><span>23.
            </span><a name="pgen.1006616.ref023" id="pgen.1006616.ref023"></a>Jablonski NG, Chaplin G. The evolution of human skin coloration. J Hum Evol. 2000;39: 57–106.  pmid:10896812 <ul data-doi="10.1006/jhev.2000.0403"><li><a href="https://doi.org/10.1006/jhev.2000.0403" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/10896812" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=The+evolution+of+human+skin+coloration+Jablonski+2000" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref24"><span>24.
            </span><a name="pgen.1006616.ref024" id="pgen.1006616.ref024"></a>Gower JC. Generalized procrustes analysis. Psychometrika. 1975;40: 33–51. <ul><li><a href="#" data-author="Gower" data-cit="GowerJC.%20Generalized%20procrustes%20analysis.%20Psychometrika.%201975%3B40%3A%2033%E2%80%9351." data-title="Generalized%20procrustes%20analysis" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Generalized+procrustes+analysis+Gower+1975" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref25"><span>25.
            </span><a name="pgen.1006616.ref025" id="pgen.1006616.ref025"></a>Claes P, Liberton DK, Daniels K, Rosana KM, Quillen EE, Pearson LN, et al. Modeling 3D Facial Shape from DNA. Luquetti D, editor. PLoS Genet. Public Library of Science; 2014;10: e1004224.  pmid:24651127 <ul data-doi="10.1371/journal.pgen.1004224"><li><a href="https://doi.org/10.1371/journal.pgen.1004224" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/24651127" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Modeling+3D+Facial+Shape+from+DNA.+Luquetti+D%2C+editor+Claes+2014" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref26"><span>26.
            </span><a name="pgen.1006616.ref026" id="pgen.1006616.ref026"></a>Ferrario VF, Mian F, Peretta R, Rosati R, Sforza C. Three-dimensional computerized anthropometry of the nose: Landmark representation compared to surface analysis. Cleft Palate-Craniofacial J. 2007;44: 278–285. <ul><li><a href="#" data-author="Ferrario" data-cit="FerrarioVF%2C%20MianF%2C%20PerettaR%2C%20RosatiR%2C%20SforzaC.%20Three-dimensional%20computerized%20anthropometry%20of%20the%20nose%3A%20Landmark%20representation%20compared%20to%20surface%20analysis.%20Cleft%20Palate-Craniofacial%20J.%202007%3B44%3A%20278%E2%80%93285." data-title="Three-dimensional%20computerized%20anthropometry%20of%20the%20nose%3A%20Landmark%20representation%20compared%20to%20surface%20analysis" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Three-dimensional+computerized+anthropometry+of+the+nose%3A+Landmark+representation+compared+to+surface+analysis+Ferrario+2007" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref27"><span>27.
            </span><a name="pgen.1006616.ref027" id="pgen.1006616.ref027"></a>Nucara A, Pietrafesa M, Rizzo G, Scaccianoce G. Handbook of Anthropometry. Handb Anthr. 2012; 91–114. <ul><li><a href="#" data-author="Nucara" data-cit="NucaraA%2C%20PietrafesaM%2C%20RizzoG%2C%20ScaccianoceG.%20Handbook%20of%20Anthropometry.%20Handb%20Anthr.%202012%3B%2091%E2%80%93114." data-title="Handbook%20of%20Anthropometry" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Handbook+of+Anthropometry+Nucara+2012" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref28"><span>28.
            </span><a name="pgen.1006616.ref028" id="pgen.1006616.ref028"></a>Leinonen T, McCairns RJS, O’Hara RB, Merilä J. Q(ST)-F(ST) comparisons: evolutionary and ecological insights from genomic heterogeneity. Nat Rev Genet. Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.; 2013;14: 179–90.  pmid:23381120 <ul data-doi="10.1038/nrg3395"><li><a href="https://doi.org/10.1038/nrg3395" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/23381120" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Q%28ST%29-F%28ST%29+comparisons%3A+evolutionary+and+ecological+insights+from+genomic+heterogeneity+Leinonen+2013" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref29"><span>29.
            </span><a name="pgen.1006616.ref029" id="pgen.1006616.ref029"></a>Wright S. The Genetical Structure Of Populations. Ann Eugen. 1949;15: 323–354. <ul><li><a href="#" data-author="Wright" data-cit="WrightS.%20The%20Genetical%20Structure%20Of%20Populations.%20Ann%20Eugen.%201949%3B15%3A%20323%E2%80%93354." data-title="The%20Genetical%20Structure%20Of%20Populations" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=The+Genetical+Structure+Of+Populations+Wright+1949" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref30"><span>30.
            </span><a name="pgen.1006616.ref030" id="pgen.1006616.ref030"></a>Lande R. Neutral Theory of Quantitative Genetic Variance in an Island Model with Local Extinction and Colonization. Evolution (N Y). 1992;46: 381–389. <ul><li><a href="#" data-author="Lande" data-cit="LandeR.%20Neutral%20Theory%20of%20Quantitative%20Genetic%20Variance%20in%20an%20Island%20Model%20with%20Local%20Extinction%20and%20Colonization.%20Evolution%20%28N%20Y%29.%201992%3B46%3A%20381%E2%80%93389." data-title="Neutral%20Theory%20of%20Quantitative%20Genetic%20Variance%20in%20an%20Island%20Model%20with%20Local%20Extinction%20and%20Colonization" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Neutral+Theory+of+Quantitative+Genetic+Variance+in+an+Island+Model+with+Local+Extinction+and+Colonization+Lande+1992" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref31"><span>31.
            </span><a name="pgen.1006616.ref031" id="pgen.1006616.ref031"></a>Whitlock MC. Evolutionary inference from QST. Molecular Ecology. 2008. pp. 1885–1896.  pmid:18363667 <ul data-doi="10.1111/j.1365-294X.2008.03712.x"><li><a href="https://doi.org/10.1111/j.1365-294X.2008.03712.x" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/18363667" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Evolutionary+inference+from+QST+Whitlock+2008" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref32"><span>32.
            </span><a name="pgen.1006616.ref032" id="pgen.1006616.ref032"></a>Brommer JE. Whither Pst? The approximation of Qst by Pst in evolutionary and conservation biology. J Evol Biol. 2011;24: 1160–8.  pmid:21457173 <ul data-doi="10.1111/j.1420-9101.2011.02268.x"><li><a href="https://doi.org/10.1111/j.1420-9101.2011.02268.x" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/21457173" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Whither+Pst%3F+The+approximation+of+Qst+by+Pst+in+evolutionary+and+conservation+biology+Brommer+2011" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref33"><span>33.
            </span><a name="pgen.1006616.ref033" id="pgen.1006616.ref033"></a>Leinonen T, Cano JM, Mäkinen H, Merilä J. Contrasting patterns of body shape and neutral genetic divergence in marine and lake populations of threespine sticklebacks. J Evol Biol. 2006;19: 1803–12.  pmid:17040377 <ul data-doi="10.1111/j.1420-9101.2006.01182.x"><li><a href="https://doi.org/10.1111/j.1420-9101.2006.01182.x" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/17040377" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Contrasting+patterns+of+body+shape+and+neutral+genetic+divergence+in+marine+and+lake+populations+of+threespine+sticklebacks+Leinonen+2006" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref34"><span>34.
            </span><a name="pgen.1006616.ref034" id="pgen.1006616.ref034"></a>Perry GH, Foll M, Grenier J-C, Patin E, Nédélec Y, Pacis A, et al. Adaptive, convergent origins of the pygmy phenotype in African rainforest hunter-gatherers. Proc Natl Acad Sci U S A. 2014;111: E3596–603.  pmid:25136101 <ul data-doi="10.1073/pnas.1402875111"><li><a href="https://doi.org/10.1073/pnas.1402875111" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/25136101" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Adaptive%2C+convergent+origins+of+the+pygmy+phenotype+in+African+rainforest+hunter-gatherers+Perry+2014" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref35"><span>35.
            </span><a name="pgen.1006616.ref035" id="pgen.1006616.ref035"></a>Relethford JH. Craniometric variation among modern human populations. Am J Phys Anthropol. 1994;95: 53–62.  pmid:7527996 <ul data-doi="10.1002/ajpa.1330950105"><li><a href="https://doi.org/10.1002/ajpa.1330950105" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/7527996" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Craniometric+variation+among+modern+human+populations+Relethford+1994" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref36"><span>36.
            </span><a name="pgen.1006616.ref036" id="pgen.1006616.ref036"></a>Lynch M, Walsh B. Genetics and analysis of quantitative traits. Sunderland, MA: Sinauer Associates, Inc; 1998. <ul></ul></li><li id="ref37"><span>37.
            </span><a name="pgen.1006616.ref037" id="pgen.1006616.ref037"></a>Yang J, Lee SH, Goddard ME, Visscher PM. GCTA: a tool for genome-wide complex trait analysis. Am J Hum Genet. 2011;88: 76–82.  pmid:21167468 <ul data-doi="10.1016/j.ajhg.2010.11.011"><li><a href="https://doi.org/10.1016/j.ajhg.2010.11.011" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/21167468" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=GCTA%3A+a+tool+for+genome-wide+complex+trait+analysis+Yang+2011" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref38"><span>38.
            </span><a name="pgen.1006616.ref038" id="pgen.1006616.ref038"></a>Sabeti PC, Varilly P, Fry B, Lohmueller J, Hostetter E, Cotsapas C, et al. Genome-wide detection and characterization of positive selection in human populations. Nature. 2007;449: 913–918.  pmid:17943131 <ul data-doi="10.1038/nature06250"><li><a href="https://doi.org/10.1038/nature06250" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/17943131" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Genome-wide+detection+and+characterization+of+positive+selection+in+human+populations+Sabeti+2007" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref39"><span>39.
            </span><a name="pgen.1006616.ref039" id="pgen.1006616.ref039"></a>Darvasi A, Shifman S. The beauty of admixture. Nat Genet. 2005;37: 118–119.  pmid:15678141 <ul data-doi="10.1038/ng0205-118"><li><a href="https://doi.org/10.1038/ng0205-118" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/15678141" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=The+beauty+of+admixture+Darvasi+2005" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref40"><span>40.
            </span><a name="pgen.1006616.ref040" id="pgen.1006616.ref040"></a>Zaitlen N, Pasaniuc B, Sankararaman S, Bhatia G, Zhang J, Gusev A, et al. Leveraging population admixture to characterize the heritability of complex traits. Nat Genet. Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.; 2014;46: 1356–1362.  pmid:25383972 <ul data-doi="10.1038/ng.3139"><li><a href="https://doi.org/10.1038/ng.3139" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/25383972" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Leveraging+population+admixture+to+characterize+the+heritability+of+complex+traits+Zaitlen+2014" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref41"><span>41.
            </span><a name="pgen.1006616.ref041" id="pgen.1006616.ref041"></a>Yokley TR. Ecogeographic variation in human nasal passages. Am J Phys Anthropol. 2009;138: 11–22.  pmid:18623075 <ul data-doi="10.1002/ajpa.20893"><li><a href="https://doi.org/10.1002/ajpa.20893" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/18623075" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Ecogeographic+variation+in+human+nasal+passages+Yokley+2009" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref42"><span>42.
            </span><a name="pgen.1006616.ref042" id="pgen.1006616.ref042"></a>Noback ML, Harvati K, Spoor F. Climate-related variation of the human nasal cavity. Am J Phys Anthropol. 2011;145: 599–614.  pmid:21660932 <ul data-doi="10.1002/ajpa.21523"><li><a href="https://doi.org/10.1002/ajpa.21523" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/21660932" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Climate-related+variation+of+the+human+nasal+cavity+Noback+2011" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref43"><span>43.
            </span><a name="pgen.1006616.ref043" id="pgen.1006616.ref043"></a>Maddux SD, Yokley TR, Svoma BM, Franciscus RG. Absolute humidity and the human nose: A reanalysis of climate zones and their influence on nasal form and function. Am J Phys Anthropol. 2016;161: 309–320.  pmid:27374937 <ul data-doi="10.1002/ajpa.23032"><li><a href="https://doi.org/10.1002/ajpa.23032" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/27374937" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Absolute+humidity+and+the+human+nose%3A+A+reanalysis+of+climate+zones+and+their+influence+on+nasal+form+and+function+Maddux+2016" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref44"><span>44.
            </span><a name="pgen.1006616.ref044" id="pgen.1006616.ref044"></a>Keck T, Lindemann J. Numerical simulation and nasal air-conditioning. GMS Curr Top Otorhinolaryngol Head Neck Surg. 2010;9: Doc08.  pmid:22073112 <ul data-doi="10.3205/cto000072"><li><a href="https://doi.org/10.3205/cto000072" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/22073112" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Numerical+simulation+and+nasal+air-conditioning+Keck+2010" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref45"><span>45.
            </span><a name="pgen.1006616.ref045" id="pgen.1006616.ref045"></a>Inthavong K, Tian ZF, Tu JY. CFD Simulations on the Heating Capability in a Human Nasal Cavity. Rhinology. 2007; 842–847. <ul><li><a href="#" data-author="Inthavong" data-cit="InthavongK%2C%20TianZF%2C%20TuJY.%20CFD%20Simulations%20on%20the%20Heating%20Capability%20in%20a%20Human%20Nasal%20Cavity.%20Rhinology.%202007%3B%20842%E2%80%93847." data-title="CFD%20Simulations%20on%20the%20Heating%20Capability%20in%20a%20Human%20Nasal%20Cavity" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=CFD+Simulations+on+the+Heating+Capability+in+a+Human+Nasal+Cavity+Inthavong+2007" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref46"><span>46.
            </span><a name="pgen.1006616.ref046" id="pgen.1006616.ref046"></a>Irmak MK, Korkmaz A, Erogul O. Selective brain cooling seems to be a mechanism leading to human craniofacial diversity observed in different geographical regions. Med Hypotheses. 2004;63: 974–979.  pmid:15504564 <ul data-doi="10.1016/j.mehy.2004.05.003"><li><a href="https://doi.org/10.1016/j.mehy.2004.05.003" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/15504564" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Selective+brain+cooling+seems+to+be+a+mechanism+leading+to+human+craniofacial+diversity+observed+in+different+geographical+regions+Irmak+2004" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref47"><span>47.
            </span><a name="pgen.1006616.ref047" id="pgen.1006616.ref047"></a>Maloney SK, Mitchell D, Blache D. The contribution of carotid rete variability to brain temperature variability in sheep in a thermoneutral environment. Am J Physiol Regul Integr Comp Physiol. 2007;292: R1298–305.  pmid:17082355 <ul data-doi="10.1152/ajpregu.00275.2006"><li><a href="https://doi.org/10.1152/ajpregu.00275.2006" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/17082355" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=The+contribution+of+carotid+rete+variability+to+brain+temperature+variability+in+sheep+in+a+thermoneutral+environment+Maloney+2007" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref48"><span>48.
            </span><a name="pgen.1006616.ref048" id="pgen.1006616.ref048"></a>Mitchell D, Maloney SK, Jessen C, Laburn HP, Kamerman PR, Mitchell G, et al. Adaptive heterothermy and selective brain cooling in arid-zone mammals. Comp Biochem Physiol Part B Biochem Mol Biol. 2002;131: 571–585. <ul><li><a href="#" data-author="Mitchell" data-cit="MitchellD%2C%20MaloneySK%2C%20JessenC%2C%20LaburnHP%2C%20KamermanPR%2C%20MitchellG%2C%20et%20al.%20Adaptive%20heterothermy%20and%20selective%20brain%20cooling%20in%20arid-zone%20mammals.%20Comp%20Biochem%20Physiol%20Part%20B%20Biochem%20Mol%20Biol.%202002%3B131%3A%20571%E2%80%93585." data-title="Adaptive%20heterothermy%20and%20selective%20brain%20cooling%20in%20arid-zone%20mammals" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Adaptive+heterothermy+and+selective+brain+cooling+in+arid-zone+mammals+Mitchell+2002" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref49"><span>49.
            </span><a name="pgen.1006616.ref049" id="pgen.1006616.ref049"></a>Hetem RS, Strauss WM, Fick LG, Maloney SK, Meyer LCR, Fuller A, et al. Selective brain cooling in Arabian oryx (Oryx leucoryx): a physiological mechanism for coping with aridity? J Exp Biol. 2012;215: 3917–24.  pmid:22899527 <ul data-doi="10.1242/jeb.074666"><li><a href="https://doi.org/10.1242/jeb.074666" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/22899527" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Selective+brain+cooling+in+Arabian+oryx+%28Oryx+leucoryx%29%3A+a+physiological+mechanism+for+coping+with+aridity%3F+Hetem+2012" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref50"><span>50.
            </span><a name="pgen.1006616.ref050" id="pgen.1006616.ref050"></a>Blix AS, Walløe L, Folkow LP. Regulation of brain temperature in winter-acclimatized reindeer under heat stress. J Exp Biol. 2011;214: 3850–6.  pmid:22031750 <ul data-doi="10.1242/jeb.057455"><li><a href="https://doi.org/10.1242/jeb.057455" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/22031750" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Regulation+of+brain+temperature+in+winter-acclimatized+reindeer+under+heat+stress+Blix+2011" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref51"><span>51.
            </span><a name="pgen.1006616.ref051" id="pgen.1006616.ref051"></a>Crandall CG, Brothers RM, Zhang R, Brengelmann GL, Covaciu L, Jay O, et al. Comments on point:counterpoint: humans do/do not demonstrate selective brain cooling during hyperthermia. J Appl Physiol. American Physiological Society; 2011;110: 575–80.  pmid:21304015 <ul data-doi="10.1152/japplphysiol.01375.2010"><li><a href="https://doi.org/10.1152/japplphysiol.01375.2010" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/21304015" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Comments+on+point%3Acounterpoint%3A+humans+do%2Fdo+not+demonstrate+selective+brain+cooling+during+hyperthermia+Crandall+2011" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref52"><span>52.
            </span><a name="pgen.1006616.ref052" id="pgen.1006616.ref052"></a>Cabanac M. Selective brain cooling in humans: “fancy” or fact? FASEB J. 1993;7: 1143-1146-1147. <ul><li><a href="#" data-author="Cabanac" data-cit="CabanacM.%20Selective%20brain%20cooling%20in%20humans%3A%20%E2%80%9Cfancy%E2%80%9D%20or%20fact%3F%20FASEB%20J.%201993%3B7%3A%201143-1146-1147." data-title="Selective%20brain%20cooling%20in%20humans%3A%20%E2%80%9Cfancy%E2%80%9D%20or%20fact%3F" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Selective+brain+cooling+in+humans%3A+%E2%80%9Cfancy%E2%80%9D+or+fact%3F+Cabanac+1993" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref53"><span>53.
            </span><a name="pgen.1006616.ref053" id="pgen.1006616.ref053"></a>Puts DA. Beauty and the beast: mechanisms of sexual selection in humans. Evol Hum Behav. Elsevier; 2010;31: 157–175. <ul><li><a href="#" data-author="Puts" data-cit="PutsDA.%20Beauty%20and%20the%20beast%3A%20mechanisms%20of%20sexual%20selection%20in%20humans.%20Evol%20Hum%20Behav.%20Elsevier%3B%202010%3B31%3A%20157%E2%80%93175." data-title="Beauty%20and%20the%20beast%3A%20mechanisms%20of%20sexual%20selection%20in%20humans" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Beauty+and+the+beast%3A+mechanisms+of+sexual+selection+in+humans+Puts+2010" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref54"><span>54.
            </span><a name="pgen.1006616.ref054" id="pgen.1006616.ref054"></a>van Doorn GS, Edelaar P, Weissing FJ. On the origin of species by natural and sexual selection. Science. 2009;326: 1704–1707.  pmid:19965377 <ul data-doi="10.1126/science.1181661"><li><a href="https://doi.org/10.1126/science.1181661" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/19965377" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=On+the+origin+of+species+by+natural+and+sexual+selection+van+Doorn+2009" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref55"><span>55.
            </span><a name="pgen.1006616.ref055" id="pgen.1006616.ref055"></a>Bigham AW, Mao X, Mei R, Brutsaert T, Wilson MJ, Julian CG, et al. Identifying positive selection candidate loci for high-altitude adaptation in Andean populations. Hum Genomics. 2009;4: 79–90.  pmid:20038496 <ul data-doi="10.1186/1479-7364-4-2-79"><li><a href="https://doi.org/10.1186/1479-7364-4-2-79" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/20038496" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Identifying+positive+selection+candidate+loci+for+high-altitude+adaptation+in+Andean+populations+Bigham+2009" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref56"><span>56.
            </span><a name="pgen.1006616.ref056" id="pgen.1006616.ref056"></a>Wu T, Li S, Ward MP. Tibetans at extreme altitude. Wilderness Environ Med. 2005;16: 47–54. pmid:15813148 <ul><li><a href="#" data-author="Wu" data-cit="WuT%2C%20LiS%2C%20WardMP.%20Tibetans%20at%20extreme%20altitude.%20Wilderness%20Environ%20Med.%202005%3B16%3A%2047%E2%80%9354.%2015813148" data-title="Tibetans%20at%20extreme%20altitude" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/15813148" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Tibetans+at+extreme+altitude+Wu+2005" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref57"><span>57.
            </span><a name="pgen.1006616.ref057" id="pgen.1006616.ref057"></a>Paternoster L, Zhurov AI, Toma AM, Kemp JP, St Pourcain B, Timpson NJ, et al. Genome-wide association study of three-dimensional facial morphology identifies a variant in PAX3 associated with nasion position. Am J Hum Genet. Elsevier; 2012;90: 478–85.  pmid:22341974 <ul data-doi="10.1016/j.ajhg.2011.12.021"><li><a href="https://doi.org/10.1016/j.ajhg.2011.12.021" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/22341974" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Genome-wide+association+study+of+three-dimensional+facial+morphology+identifies+a+variant+in+PAX3+associated+with+nasion+position+Paternoster+2012" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref58"><span>58.
            </span><a name="pgen.1006616.ref058" id="pgen.1006616.ref058"></a>Liu F, van der Lijn F, Schurmann C, Zhu G, Chakravarty MM, Hysi PG, et al. A genome-wide association study identifies five loci influencing facial morphology in Europeans. Gibson G, editor. PLoS Genet. Public Library of Science; 2012;8: e1002932.  pmid:23028347 <ul data-doi="10.1371/journal.pgen.1002932"><li><a href="https://doi.org/10.1371/journal.pgen.1002932" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/23028347" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=A+genome-wide+association+study+identifies+five+loci+influencing+facial+morphology+in+Europeans+Liu+2012" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref59"><span>59.
            </span><a name="pgen.1006616.ref059" id="pgen.1006616.ref059"></a>Shaffer JR, Orlova E, Lee MK, Leslie EJ, Raffensperger ZD, Heike CL, et al. Genome-Wide Association Study Reveals Multiple Loci Influencing Normal Human Facial Morphology. Barsh GS, editor. PLOS Genet. Public Library of Science; 2016;12: e1006149.  pmid:27560520 <ul data-doi="10.1371/journal.pgen.1006149"><li><a href="https://doi.org/10.1371/journal.pgen.1006149" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/27560520" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Genome-Wide+Association+Study+Reveals+Multiple+Loci+Influencing+Normal+Human+Facial+Morphology+Shaffer+2016" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref60"><span>60.
            </span><a name="pgen.1006616.ref060" id="pgen.1006616.ref060"></a>Adhikari K, Fuentes-Guajardo M, Quinto-Sánchez M, Mendoza-Revilla J, Camilo Chacón-Duque J, Acuña-Alonzo V, et al. A genome-wide association scan implicates DCHS2, RUNX2, GLI3, PAX1 and EDAR in human facial variation. Nat Commun. Nature Research; 2016;7: 11616. <ul><li><a href="#" data-author="Adhikari" data-cit="AdhikariK%2C%20Fuentes-GuajardoM%2C%20Quinto-S%C3%A1nchezM%2C%20Mendoza-RevillaJ%2C%20Camilo%20Chac%C3%B3n-DuqueJ%2C%20Acu%C3%B1a-AlonzoV%2C%20et%20al.%20A%20genome-wide%20association%20scan%20implicates%20DCHS2%2C%20RUNX2%2C%20GLI3%2C%20PAX1%20and%20EDAR%20in%20human%20facial%20variation.%20Nat%20Commun.%20Nature%20Research%3B%202016%3B7%3A%2011616." data-title="A%20genome-wide%20association%20scan%20implicates%20DCHS2%2C%20RUNX2%2C%20GLI3%2C%20PAX1%20and%20EDAR%20in%20human%20facial%20variation" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=A+genome-wide+association+scan+implicates+DCHS2%2C+RUNX2%2C+GLI3%2C+PAX1+and+EDAR+in+human+facial+variation+Adhikari+2016" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref61"><span>61.
            </span><a name="pgen.1006616.ref061" id="pgen.1006616.ref061"></a>Beleza S, Campos J, Lopes J, Araújo II, Hoppfer Almada A, Correia e Silva A, et al. The Admixture Structure and Genetic Variation of the Archipelago of Cape Verde and Its Implications for Admixture Mapping Studies. PLoS One. 2012;7. <ul><li><a href="#" data-author="Beleza" data-cit="BelezaS%2C%20CamposJ%2C%20LopesJ%2C%20Ara%C3%BAjoII%2C%20Hoppfer%20AlmadaA%2C%20Correia%20e%20SilvaA%2C%20et%20al.%20The%20Admixture%20Structure%20and%20Genetic%20Variation%20of%20the%20Archipelago%20of%20Cape%20Verde%20and%20Its%20Implications%20for%20Admixture%20Mapping%20Studies.%20PLoS%20One.%202012%3B7." data-title="The%20Admixture%20Structure%20and%20Genetic%20Variation%20of%20the%20Archipelago%20of%20Cape%20Verde%20and%20Its%20Implications%20for%20Admixture%20Mapping%20Studies" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=The+Admixture+Structure+and+Genetic+Variation+of+the+Archipelago+of+Cape+Verde+and+Its+Implications+for+Admixture+Mapping+Studies+Beleza+2012" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref62"><span>62.
            </span><a name="pgen.1006616.ref062" id="pgen.1006616.ref062"></a>Illumina Inc. “TOP/BOT” strand and “A/B” allele—A guide to Illumina’s method for determining Strand and Allele for the GoldenGate and Infinium Assays. (Technical Note) [Internet]. 2006. Available: <a href="http://www.illumina.com/documents/products/technotes/technote_topbot.pdf">http://www.illumina.com/documents/products/technotes/technote_topbot.pdf</a> <ul></ul></li><li id="ref63"><span>63.
            </span><a name="pgen.1006616.ref063" id="pgen.1006616.ref063"></a>Purcell S, Neale B, Todd-Brown K, Thomas L, Ferreira MAR, Bender D, et al. PLINK: a tool set for whole-genome association and population-based linkage analyses. Am J Hum Genet. 2007;81: 559–75.  pmid:17701901 <ul data-doi="10.1086/519795"><li><a href="https://doi.org/10.1086/519795" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/17701901" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=PLINK%3A+a+tool+set+for+whole-genome+association+and+population-based+linkage+analyses+Purcell+2007" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref64"><span>64.
            </span><a name="pgen.1006616.ref064" id="pgen.1006616.ref064"></a>Chang CC, Chow CC, Tellier LC, Vattikuti S, Purcell SM, Lee JJ. Second-generation PLINK: rising to the challenge of larger and richer datasets. Gigascience. BioMed Central Ltd; 2015;4: 7.  pmid:25722852 <ul data-doi="10.1186/s13742-015-0047-8"><li><a href="https://doi.org/10.1186/s13742-015-0047-8" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/25722852" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Second-generation+PLINK%3A+rising+to+the+challenge+of+larger+and+richer+datasets+Chang+2015" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref65"><span>65.
            </span><a name="pgen.1006616.ref065" id="pgen.1006616.ref065"></a>Candille SI, Absher DM, Beleza S, Bauchet M, McEvoy B, Garrison NA, et al. Genome-wide association studies of quantitatively measured skin, hair, and eye pigmentation in four European populations. PLoS One. Public Library of Science; 2012;7: e48294.  pmid:23118974 <ul data-doi="10.1371/journal.pone.0048294"><li><a href="https://doi.org/10.1371/journal.pone.0048294" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/23118974" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Genome-wide+association+studies+of+quantitatively+measured+skin%2C+hair%2C+and+eye+pigmentation+in+four+European+populations+Candille+2012" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref66"><span>66.
            </span><a name="pgen.1006616.ref066" id="pgen.1006616.ref066"></a>Storz JF. Contrasting patterns of divergence in quantitative traits and neutral DNA markers: analysis of clinal variation. Mol Ecol. 2002;11: 2537–2551. pmid:12453238 <ul><li><a href="#" data-author="Storz" data-cit="StorzJF.%20Contrasting%20patterns%20of%20divergence%20in%20quantitative%20traits%20and%20neutral%20DNA%20markers%3A%20analysis%20of%20clinal%20variation.%20Mol%20Ecol.%202002%3B11%3A%202537%E2%80%932551.%2012453238" data-title="Contrasting%20patterns%20of%20divergence%20in%20quantitative%20traits%20and%20neutral%20DNA%20markers%3A%20analysis%20of%20clinal%20variation" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/12453238" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Contrasting+patterns+of+divergence+in+quantitative+traits+and+neutral+DNA+markers%3A+analysis+of+clinal+variation+Storz+2002" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref67"><span>67.
            </span><a name="pgen.1006616.ref067" id="pgen.1006616.ref067"></a>He Y, Li R, Wang J, Blanchet S, Lek S. Morphological Variation Among Wild Populations of Chinese Rare Minnow (Gobiocypris rarus): Deciphering the Role of Evolutionary Processes. Zoolog Sci. 2013;30: 475–83.  pmid:23721472 <ul data-doi="10.2108/zsj.30.475"><li><a href="https://doi.org/10.2108/zsj.30.475" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/23721472" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Morphological+Variation+Among+Wild+Populations+of+Chinese+Rare+Minnow+%28Gobiocypris+rarus%29%3A+Deciphering+the+Role+of+Evolutionary+Processes+He+2013" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref68"><span>68.
            </span><a name="pgen.1006616.ref068" id="pgen.1006616.ref068"></a>Weir BS, Cockerham CC. Estimating F-statistics for the analysis of population structure. Evolution (N Y). 1984;38: 1358–1370. <ul><li><a href="#" data-author="Weir" data-cit="WeirBS%2C%20CockerhamCC.%20Estimating%20F-statistics%20for%20the%20analysis%20of%20population%20structure.%20Evolution%20%28N%20Y%29.%201984%3B38%3A%201358%E2%80%931370." data-title="Estimating%20F-statistics%20for%20the%20analysis%20of%20population%20structure" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Estimating+F-statistics+for+the+analysis+of+population+structure+Weir+1984" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref69"><span>69.
            </span><a name="pgen.1006616.ref069" id="pgen.1006616.ref069"></a>Browning SR, Browning BL. Population structure can inflate SNP-based heritability estimates. Am J Hum Genet. 2011;89: 191–193.  pmid:21763486 <ul data-doi="10.1016/j.ajhg.2011.05.025"><li><a href="https://doi.org/10.1016/j.ajhg.2011.05.025" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/21763486" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Population+structure+can+inflate+SNP-based+heritability+estimates+Browning+2011" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref70"><span>70.
            </span><a name="pgen.1006616.ref070" id="pgen.1006616.ref070"></a>Goddard ME, Lee SH, Yang J, Wray NR, Visscher PM. Response to browning and browning. Am J Hum Genet. 2011;89: 193–195. <ul><li><a href="#" data-author="Goddard" data-cit="GoddardME%2C%20LeeSH%2C%20YangJ%2C%20WrayNR%2C%20VisscherPM.%20Response%20to%20browning%20and%20browning.%20Am%20J%20Hum%20Genet.%202011%3B89%3A%20193%E2%80%93195." data-title="Response%20to%20browning%20and%20browning" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Response+to+browning+and+browning+Goddard+2011" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref71"><span>71.
            </span><a name="pgen.1006616.ref071" id="pgen.1006616.ref071"></a>Baran Y, Pasaniuc B, Sankararaman S, Torgerson DG, Gignoux C, Eng C, et al. Fast and accurate inference of local ancestry in Latino populations. Bioinformatics. 2012;28: 1359–1367.  pmid:22495753 <ul data-doi="10.1093/bioinformatics/bts144"><li><a href="https://doi.org/10.1093/bioinformatics/bts144" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/22495753" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Fast+and+accurate+inference+of+local+ancestry+in+Latino+populations+Baran+2012" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref72"><span>72.
            </span><a name="pgen.1006616.ref072" id="pgen.1006616.ref072"></a>Halder I, Shriver MD. Measuring and using admixture to study the genetics of complex diseases. Hum Genomics. BioMed Central Ltd; 2003;1: 52.  pmid:15601533 <ul data-doi="10.1186/1479-7364-1-1-52"><li><a href="https://doi.org/10.1186/1479-7364-1-1-52" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/15601533" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Measuring+and+using+admixture+to+study+the+genetics+of+complex+diseases+Halder+2003" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref73"><span>73.
            </span><a name="pgen.1006616.ref073" id="pgen.1006616.ref073"></a>Kahle D, Wickham H. ggmap: Spatial Visualization with. R J. 2013;5: 144–161. Available: <a href="http://journal.r-project.org/archive/2013-1/kahle-wickham.pdf">http://journal.r-project.org/archive/2013-1/kahle-wickham.pdf</a> <ul><li><a href="#" data-author="Kahle" data-cit="KahleD%2C%20WickhamH.%20ggmap%3A%20Spatial%20Visualization%20with.%20R%20J.%202013%3B5%3A%20144%E2%80%93161.%20Available%3A%20http%3A%2F%2Fjournal.r-project.org%2Farchive%2F2013-1%2Fkahle-wickham.pdf" data-title="ggmap%3A%20Spatial%20Visualization%20with" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=ggmap%3A+Spatial+Visualization+with+Kahle+2013" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref74"><span>74.
            </span><a name="pgen.1006616.ref074" id="pgen.1006616.ref074"></a>R Core Team. R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. 2013. <ul></ul></li><li id="ref75"><span>75.
            </span><a name="pgen.1006616.ref075" id="pgen.1006616.ref075"></a>Harris I, Jones PD, Osborn TJ, Lister DH. Updated high-resolution grids of monthly climatic observations—the CRU TS3.10 Dataset. Int J Climatol. John Wiley &amp; Sons, Ltd; 2014;34: 623–642. <ul><li><a href="#" data-author="Harris" data-cit="HarrisI%2C%20JonesPD%2C%20OsbornTJ%2C%20ListerDH.%20Updated%20high-resolution%20grids%20of%20monthly%20climatic%20observations%E2%80%94the%20CRU%20TS3.10%20Dataset.%20Int%20J%20Climatol.%20John%20Wiley%20%26%20Sons%2C%20Ltd%3B%202014%3B34%3A%20623%E2%80%93642." data-title="Updated%20high-resolution%20grids%20of%20monthly%20climatic%20observations%E2%80%94the%20CRU%20TS3.10%20Dataset" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Updated+high-resolution+grids+of+monthly+climatic+observations%E2%80%94the+CRU+TS3.10+Dataset+Harris+2014" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref76"><span>76.
            </span><a name="pgen.1006616.ref076" id="pgen.1006616.ref076"></a>TOMS Science Team (1996). TOMS Earth Probe UV Reflectivity Monthly L3 Global 1 deg x 1.25 deg Lat/Lon Grid V008, version 008, Greenbelt, MD, Goddard Earth Sciences Data and Information Services Center (GES DISC) [Internet]. Available: <a href="http://disc.sci.gsfc.nasa.gov/datacollection/TOMSEPL3mref_008.html">http://disc.sci.gsfc.nasa.gov/datacollection/TOMSEPL3mref_008.html</a> <ul></ul></li><li id="ref77"><span>77.
            </span><a name="pgen.1006616.ref077" id="pgen.1006616.ref077"></a>Michna P, Woods M. RNetCDF—A Package for Reading and Writing NetCDF Datasets. R J. 2013;5: 29–35. <ul><li><a href="#" data-author="Michna" data-cit="MichnaP%2C%20WoodsM.%20RNetCDF%E2%80%94A%20Package%20for%20Reading%20and%20Writing%20NetCDF%20Datasets.%20R%20J.%202013%3B5%3A%2029%E2%80%9335." data-title="RNetCDF%E2%80%94A%20Package%20for%20Reading%20and%20Writing%20NetCDF%20Datasets" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=RNetCDF%E2%80%94A+Package+for+Reading+and+Writing+NetCDF+Datasets+Michna+2013" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref78"><span>78.
            </span><a name="pgen.1006616.ref078" id="pgen.1006616.ref078"></a>Hijmans RJ, Etten J Van. raster: Geographic analysis and modeling with raster data. R package version 2.0–12. [Internet]. 2012. Available: <a href="http://cran.r-project.org/package=raster">http://cran.r-project.org/package=raster</a> <ul></ul></li><li id="ref79"><span>79.
            </span><a name="pgen.1006616.ref079" id="pgen.1006616.ref079"></a>Rousset F, Ferdy J-B. Testing environmental and genetic effects in the presence of spatial autocorrelation. Ecography (Cop). 2014;37: 781–790. <ul><li><a href="#" data-author="Rousset" data-cit="RoussetF%2C%20FerdyJ-B.%20Testing%20environmental%20and%20genetic%20effects%20in%20the%20presence%20of%20spatial%20autocorrelation.%20Ecography%20%28Cop%29.%202014%3B37%3A%20781%E2%80%93790." data-title="Testing%20environmental%20and%20genetic%20effects%20in%20the%20presence%20of%20spatial%20autocorrelation" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Testing+environmental+and+genetic+effects+in+the+presence+of+spatial+autocorrelation+Rousset+2014" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref80"><span>80.
            </span><a name="pgen.1006616.ref080" id="pgen.1006616.ref080"></a>Claes P, Reijniers J, Shriver MD, Snyders J, Suetens P, Nielandt J, et al. An investigation of matching symmetry in the human pinnae with possible implications for 3D ear recognition and sound localization. J Anat. 2015;226: 60–72.  pmid:25382291 <ul data-doi="10.1111/joa.12252"><li><a href="https://doi.org/10.1111/joa.12252" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/25382291" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=An+investigation+of+matching+symmetry+in+the+human+pinnae+with+possible+implications+for+3D+ear+recognition+and+sound+localization+Claes+2015" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref81"><span>81.
            </span><a name="pgen.1006616.ref081" id="pgen.1006616.ref081"></a>Pinheiro J, Bates D, DebRoy S, Sarkar D, Team TRC. nlme: Linear and Nonlinear Mixed Effects Models. October. 2013. pp. 1–3. <ul><li><a href="#" data-author="Pinheiro" data-cit="PinheiroJ%2C%20BatesD%2C%20DebRoyS%2C%20SarkarD%2C%20TeamTRC.%20nlme%3A%20Linear%20and%20Nonlinear%20Mixed%20Effects%20Models.%20October.%202013.%20pp.%201%E2%80%933." data-title="nlme%3A%20Linear%20and%20Nonlinear%20Mixed%20Effects%20Models" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=nlme%3A+Linear+and+Nonlinear+Mixed+Effects+Models+Pinheiro+2013" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li></ol></div>



          

        </div>
      </div>
  
</div></div>]]></description>
        </item>
    </channel>
</rss>