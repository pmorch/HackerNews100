<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 30 Jul 2024 13:30:07 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[FTC Wins Round Two in Its Non-Compete Ban Defense (113 pts)]]></title>
            <link>https://www.jdsupra.com/legalnews/ftc-wins-round-two-in-its-non-compete-4112148/</link>
            <guid>41107845</guid>
            <pubDate>Tue, 30 Jul 2024 10:56:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jdsupra.com/legalnews/ftc-wins-round-two-in-its-non-compete-4112148/">https://www.jdsupra.com/legalnews/ftc-wins-round-two-in-its-non-compete-4112148/</a>, See on <a href="https://news.ycombinator.com/item?id=41107845">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="HTMLContentViewPanel">

<p>The Federal Trade Commission (FTC) is seeking to ban nearly all employee non-compete agreements in the United States [see <a href="https://www.mccarter.com/insights/yes-the-ftc-did-just-try-to-ban-non-compete-agreements-for-now/">April 25, 2024 Alert</a>]. Almost immediately after the FTC issued its final rule, lawsuits challenging that attempt were filed in several courts around the country. On July 3, 2024, the first court ruling in those lawsuits held, on a preliminary basis, that the FTC likely did not have the authority to issue such a sweeping rule [see <a href="https://www.mccarter.com/insights/the-ftcs-non-compete-ban-has-suffered-its-first-setback/">July 10, 2024 Alert</a>]. The United States District Court for the Northern District of Texas held that the FTC’s enabling statute did not authorize it to issue substantive rules to prevent “unfair methods of competition” but rather only allowed the FTC to make substantive rules about “unfair or deceptive acts or practices.” Because the FTC declared that non-competes fell into the former category rather than the latter, the Texas Court determined, the FTC lacked the authority to issue the regulation.</p>

<p>Less than three weeks later, however, another federal court held, again on a preliminary basis, that the FTC likely <strong>did</strong> have the authority to ban all employee non-compete agreements. The United States District Court for the Eastern District of Pennsylvania considered a preliminary injunction motion brought by a tree care company with a dozen employees, each of whom had signed a non-compete agreement. The Court denied the employer’s request for a preliminary injunction on two grounds. First, the Court held that the company had not demonstrated irreparable harm in the absence of a preliminary injunction. Second, and more interesting for purposes of discerning whether the ban will ever actually go into effect, the Court held that the FTC did indeed have the power under its statute to issue substantive rules regarding unfair methods of competition.</p>

<p>As the Pennsylvania Court explained, the core issue was “whether the FTC, under Sections 5 and 6 of the [Federal Trade Commission] Act, has the authority to promulgate substantive regulations like the [Non-Compete ban], or whether the FTC’s enforcement authority is limited to adjudication and strictly procedural rulemaking related to the adjudication process.” The Court began by noting that the FTC Act gave the FTC the power “to make rules and regulations for the purpose of carrying out the provisions of this subchapter,” 15 U.S.C. § 46(g). The Court observed that the statute did not distinguish between “procedural” and “substantive” regulations, and in fact mentioned neither term. The Pennsylvania Court stated that the plaintiff’s proposed interpretation of the FTC Act–the one that had been adopted three weeks earlier in the Texas case–“runs contrary to logic.” Further, the Pennsylvania Court determined that giving the FTC the power to “prevent” unfair methods of competition implies the power to issue substantive regulations in the area. Finally, the Pennsylvania Court declined to infer from the statute’s specific grant of authority to the FTC to issue rules for “unfair or deceptive acts” any limit to its ability to issue regulations related to “unfair methods of competition,” in large part because that section of the statute provides that it “shall not affect any authority of the<a href="https://www.law.cornell.edu/definitions/uscode.php?width=840&amp;height=800&amp;iframe=true&amp;def_id=15-USC-1283237621-1323160499&amp;term_occur=999&amp;term_src="> Commission </a>to prescribe rules (including interpretive rules), and general statements of policy, with respect to unfair methods of competition in or affecting commerce.”</p>

<p>So to switch from the boxing metaphor of our title to a baseball metaphor, after two innings the score is tied 1-1 with plenty of opportunities for both sides left in the game. We expect another preliminary ruling from a federal court in Florida soon, and the Texas Court has indicated that it will rule on the request for a final injunction by the end of August. Then, of course, the appeals will commence. Ultimately, it seems likely that the US Supreme Court will weigh in on the matter unless the political process stops the rule before it gets the chance. We expect several developments on this issue before the September 4, 2024, effective date of the non-compete rule, so stay tuned.</p>

<p>As we noted in our discussion of the Texas decision, the consequences of non-compliance with the FTC non-compete rule appear to be limited. The FTC has authority to seek civil penalties of up to $10,000 per violation in response to violations of its rules concerning <em>unfair or deceptive acts or practices, but not with regard to unfair methods of competition</em>. The non-compete rule is expressly based on the FTC’s finding that non-competes are an <em>unfair method of competition</em>. That (likely) means that even if the non-compete rule goes into effect, the FTC would not be able to impose civil penalties against an employer unless and until it first issues an administrative complaint, holds a hearing, issues a cease and desist order, and that order becomes final (meaning that the time to appeal expired or that all appeals have been exhausted). At that time, if the employer violated the final cease and desist order, the FTC could then file a lawsuit asking a court to award it civil penalties. Employers with non-compete agreements should discuss these issues with their attorneys in order to make the best decisions possible in this dynamic environment.</p>

<p>[<a href="https://www.mccarter.com/insights/ftc-wins-round-two-in-its-non-compete-ban-defense/" target="_blank">View source</a>.]</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to burn US$10M on an ArXiv preprint (108 pts)]]></title>
            <link>https://152334H.github.io/blog/scaling-exponents/</link>
            <guid>41107721</guid>
            <pubDate>Tue, 30 Jul 2024 10:26:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://152334H.github.io/blog/scaling-exponents/">https://152334H.github.io/blog/scaling-exponents/</a>, See on <a href="https://news.ycombinator.com/item?id=41107721">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content"><p>Recently, GDM released a great paper titled, <a href="https://arxiv.org/pdf/2407.05872" target="_blank" rel="noopener noreffer"><em>Scaling Exponents Across Parameterizations and Optimizers</em></a>, in which they conduct over 10,000 LLM training runs to obtain optimal hyperparameters under different regimes.</p><p>After reading it (it was great), I wanted to test my understanding of the paper by tallying up all experiments conducted within, calculating <strong>the total compute cost it would take to replicate the paper</strong>.</p><h2 id="headline-result">Headline result</h2><table><thead><tr><th>Subset</th><th>Sources of uncertainty</th><th>FLOPs</th><th>Costs @ $3/H100/hr</th></tr></thead><tbody><tr><td>Alignment</td><td>N/A</td><td>3.7e20</td><td>$888</td></tr><tr><td>LR variants (+default)</td><td>LR-sweeps, bayes search</td><td>7.99e23</td><td>$1.90M</td></tr><tr><td>LR variants (+optimal)</td><td>LR-sweeps</td><td>1.35e24</td><td>$3.22M</td></tr><tr><td>Epslion (Heatmaps)</td><td>LR-sweeps, $D$</td><td>1.34e24</td><td>$3.19M</td></tr><tr><td>Epslion (Full Sweeps)</td><td>LR-sweeps</td><td>7.99e23</td><td>$1.90M</td></tr><tr><td>Weight Decay</td><td>LR-sweeps</td><td>1.33e23</td><td>$317K</td></tr><tr><td>Adafactor vs Adam+PS</td><td>LR-sweeps, $D$</td><td>7.92e22</td><td>$188.5K</td></tr><tr><td>Compute Optimals</td><td>LR-sweeps, $D$</td><td>7.52e23</td><td>$1.79M</td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td><strong>Total</strong></td><td>too much</td><td><strong>5.42e24</strong></td><td><strong>$12.9M</strong></td></tr></tbody></table><p><strong>Any corrections on the numbers here will be appreciated.</strong></p><p>Although I have made significant efforts to vet these claims, <em>if I have made significant mistakes in mathematics, these results could be off by magnitudes.</em></p><div><p>Sidenote: What's an H100 worth?</p><div><p>Although it’s never stated, all experiments in the paper were almost certainly conducted with TPUs (because it’s from Google Deepmind). Furthermore, as there is no mention of int8 usage in their paper, it is most likely that all experiments were conducted with bfloat16 compute precision, per the <a href="https://github.com/google-deepmind/nanodo/blob/10aefdeed40a63293daf112b91a5538cd24fa3a4/nanodo/configs/default.py#L41" target="_blank" rel="noopener noreffer">nanodo default</a>.</p><p>However, as a GPU user, I prefer to calculate compute in terms of H100 hours. Some basic facts:</p><ul><li>The H100-SXM is <a href="https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-datasheet" target="_blank" rel="noopener noreffer">reported</a> as having 989.40TFLOP/s of 16-bit tensor core operations.<ul><li>Also, 66.9TFLOP/s fp32 non-tensor, but I won’t consider non-tensor operations (such as softmax or hadamard products) in my analysis.</li></ul></li><li>Recent pytorch <a href="https://pytorch.org/blog/maximizing-training/" target="_blank" rel="noopener noreffer">blogs</a> and <a href="https://github.com/pytorch/torchtitan/pull/165" target="_blank" rel="noopener noreffer">torchtitan</a> both report single-node FSDP’d bf16 H100 MFU for reasonably mid sized models at (optimistically) 40%.<ul><li>the smaller models ($D&lt;1024$) in the paper are unlikely to have MFU that high.</li><li>Although this is not hard to push higher with some manual tuning, the time spent tuning performance &amp; engineering required to heuristically adjust for efficiency depending on setting is unlikely to be worth it.</li></ul></li><li>The cost of a H100 node (at the time of writing) is $3.5/hr/gpu on <a href="https://cloud.lambdalabs.com/instances" target="_blank" rel="noopener noreffer">lambdalabs</a>, $2.85/hr/gpu from <a href="https://sfcompute.com/" target="_blank" rel="noopener noreffer">sfcompute</a>, and ballpark $2/hr/gpu if you get a <a href="https://gpulist.ai/" target="_blank" rel="noopener noreffer">long term bulk contract</a>.</li></ul><p>If we pessimistically estimate the true average tensor FLOP/s provided by a H100 GPU on an average run as 3.5e14 (aka slightly above 35% MFU), and the cost of a H100 GPU as $3/hr, we get:</p><div><table><tbody><tr><td><pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span></code></pre></td><td><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>cost_of_run</span><span>(</span><span>flops</span><span>:</span> <span>float</span><span>,</span> <span>pergpu_flops</span><span>=</span><span>3.5e14</span><span>):</span>
</span></span><span><span>  <span>gpu_hours</span> <span>=</span> <span>flops</span> <span>/</span> <span>3600</span> <span>/</span> <span>pergpu_flops</span>
</span></span><span><span>  <span>rental_cost</span> <span>=</span> <span>3</span> <span>*</span> <span>gpu_hours</span>
</span></span><span><span>  <span>single_node_duration</span> <span>=</span> <span>gpu_hours</span> <span>/</span> <span>8</span>
</span></span><span><span>  <span>return</span> <span>rental_cost</span><span>,</span> <span>single_node_duration</span>
</span></span></code></pre></td></tr></tbody></table></div><p><strong>These numbers are fungible</strong> and you can choose to mentally halve (or double) them if you find it appropriate.</p></div></div><hr><h2 id="a-summary-of-all-experiments-tried">A summary of all experiments tried</h2><figure><img src="https://152334h.github.io/blog/scaling-exponents/Pasted%20image%2020240722025146.png"></figure><p>There are a few different types of experiments done in the paper:</p><div><p>Experiment types</p><div><ul><li><strong>Alignment experiments</strong>, which use a single global close-to-optimal LR, while varying<ul><li>$D \in {1024, 2048, 4096}$</li><li>4x paramterizations</li><li>3x optimizers (Adam, SGD+momentum, Adafactor)</li></ul></li><li><strong>Learning rate</strong> experiments, which vary:<ul><li><p>3x optimizers (Adam, SGD+momentum, Adam+PS)</p></li><li><p>4x paramterizations</p></li><li><p><strong>14x model widths</strong> $D \in [128, 16384]$. but this is really best described as scaling numheads $H \in {1,2,4,6,8,12,16,20,24,32,48,64,96,128}$</p></li><li><p>Global LR vs Per-layer Beta LR vs Per-layer $\beta$ Gamma LR + Per-layer $\beta\ \gamma$ No align LR</p><ul><li>The $\gamma$ experiements are particularly complex to calculate, see <a href="#Problems" rel="">point 3</a></li></ul></li><li><p>LR by an <strong>indeterminate range</strong> – they sweep in intervals of $2^{0.25}\text{ or }2^{0.5}$ and terminate rightwards when</p><ol><li>the LR leads to NaNs OR</li><li>the eval loss for a given LR $\mathcal{L}^\eta \gt 1.2\times \text{argmin}_\eta(\mathcal{L^\eta})$</li></ol><p>i.e. the first (larger than optimal) LR to show either of those conditions is <strong>not plotted</strong>, and the LR $\sqrt{2}$ or $\surd\surd2$ is.</p><p>…or at least, that is what the paper says is supposed to be the case. <a href="#Problems" rel="">I explain my contentions later</a>.</p></li></ul></li><li><strong>Adam Epslion</strong> experiments, which vary<ul><li>over 4x parameterizations,<ul><li><em>at least</em> $D\in {3072, 4096, 6144, 8192, 12288, 16384}$ over Adam, where<ul><li><em>at least</em> 6x eps is tried</li><li><em>at least</em> constant vs per-layer $\epsilon$ is compared.</li><li><em>at least</em> 13x LR is tried. Appendix F: “<a href="https://arxiv.org/pdf/2407.05872#page=43" target="_blank" rel="noopener noreffer">learning rate sweep at each model dim for each value of epsilon or base epsilon</a>”</li></ul></li><li>according to Appendix J/K, over <strong>all 14 model dims</strong>,<ul><li>For Adam, 4x (base eps, small const, good per-layer, atan2)<ul><li>technically, we double-count base EPS from the LR experiments, but we also neglect the extra no-align per-layer eps experiments, so this cancels out</li></ul></li><li>For Adam+PS, 2x (base eps, good per-layer)<ul><li>the double-neglect accounting argument applies here too</li></ul></li></ul></li></ul></li></ul></li><li>extra <strong>weight decay</strong> experiments<ul><li>static: adam, per-layer, full alignment, decoupled 1e-4</li><li>4x parameterizations</li><li>LR experiment-like sweep across <strong>all 14 model widths</strong></li></ul></li><li>extra <strong>adafactor</strong> experiments<ul><li>2x optim (Adafactor vs adam+ps)</li><li>2x setting (globalLR+default vs perlayer+optimal)</li><li>4x parameterizations</li><li>LR experiment-like sweep across <strong>only 11x</strong> model widths up to $H=48$ due to FSDP.<ul><li>actually <a href="https://arxiv.org/pdf/2407.05872#page=52" target="_blank" rel="noopener noreffer">implemented as 12x</a> but <a href="https://arxiv.org/pdf/2407.05872#page=47" target="_blank" rel="noopener noreffer">final results are 11x</a> and I follow the latter.</li></ul></li></ul></li><li>extra fixed step vs <strong>compute optimal</strong><ul><li>the 50k fixed step experiments are <strong>not the same</strong> as any of the above; they use “default constant learning rate multipliers” and have different power laws.</li><li>3x optim (SGD+moment, adam, adafactor)</li><li>4x parameterizations</li><li>LR experiment-like sweep across model width &amp;&amp; LR.<ul><li>width <strong>only goes up to 11x</strong>, last 3 are missing on Compute Optimal.</li></ul></li><li>compute-optimal experiments use 20x tokens of non-embedding P as a heuristic.</li></ul></li></ul></div></div><p>However, there are many problems with the experimental summary as given above.</p><div><p>Problems</p><div><ol><li><p>It is <strong>not clear whether they re-executed</strong> the per-layerLR experiments for the two edge cases where per-layer constants lead to identical behavior to globalLR (where $c_1 = c_l = c_{L+1}$):</p><ul><li>muP + SGD + full alignment, or</li><li>Adafactor + any parameterization + no alignment</li></ul><p>My expectation is that <strong>their experiments were repeated</strong>, because if you look at Table E1, you’ll see that the muP+SGD+full columns actually have a single diverging value (presumably caused by precision differences):</p><figure><img src="https://152334h.github.io/blog/scaling-exponents/VXjZe27k.jpeg"></figure><p><strong>However</strong>, I was also given (private) notice that <strong>in some cases, the experiments with theoretically equivalent settings were merely executed once</strong>, with the eval losses copied twice. This makes the true extent of compute unknowable from the paper.</p></li><li><p>The LR experiments have indeterminate bounds, so I can’t directly figure out how many experiments were executed.</p><p>You can’t “just read the graphs” to figure out what the range of LRs used are either; they cut off the y/x axis:</p><figure><img src="https://152334h.github.io/blog/scaling-exponents/Pasted%20image%2020240722134014.png"></figure><p>Frankly, it doesn’t even look like the steps here are guaranteed to be split in intervals of $2^{0.25}\text{ or }2^{0.5}$.</p><figure><img src="https://152334h.github.io/blog/scaling-exponents/Pasted%20image%2020240730045534.png"></figure><p>After further inspection, it looks an awful lot like the runs have <strong>arbitrary LR ranges even for the same $D$, optim, parameterization, and alignment</strong>. Or I just don’t understand the selection process (what are the unshaded shapes?).</p></li><li><p>In <a href="https://arxiv.org/pdf/2407.05872#page=34" target="_blank" rel="noopener noreffer">C.4</a>., they state:</p><blockquote><p>When tuning the per-layer constant multiplicative factors defined in Section 4.2, we use <a href="https://github.com/google/vizier" target="_blank" rel="noopener noreffer">vizier</a> to perform 3D hparam search for $(γ_1, γ_h, γ_{L+1})$ at $b = 1024$. Recall that we define the learning rate in layer $l$ as $η_l = β_n·γ_l·\frac{n}{b}^{−cl}$ and sweep one dimension at all model sizes to determine $β_n$, so these values of $(γ_1, γ_h, γ_{L+1})$ define two ratios where any common factor can be absorbed by $β_n$.</p></blockquote><p>To be clear, that last segment means: “you can divide $(γ_1, γ_h, γ_{L+1})$ by any of the 3 values to obtain some $(\gamma_x, \gamma_y, 1)$ tuple, the sweep will bring $\beta_n$ back to the correct value”. And so they say:</p><blockquote><p>For each optimizer × parameterization, we run 800 trials with at most 100 trials in parallel with a range set to $[1\text{e−}2, 1e2]$ for each constant. If the optimal value for any of the constants is at or near the edge of the range after this first search, we extend the range of the sweep for that constant to 0.01 and 100x the optimal value found in the original sweep and repeat the same tuning procedure.</p></blockquote><p>Upside: this gives 800 experiments as a lower bound for the $\gamma$ experiments.
Downside: We otherwise have no plotted information about the 3D experiments that were conducted. The actual plotted graphs just show final eval loss against base LR, under the assumption that the $b=1024$ base line on the Optimal Constants graphs actually hide the extra work done to sweep $\gamma$ values.</p></li><li><p>It is deeply unclear to me what is actually implemented for the fixed-step vs compute optimal runs. If we look at the <a href="https://arxiv.org/pdf/2407.05872#page=51" target="_blank" rel="noopener noreffer">50k steps graph</a>:</p><figure><img src="https://152334h.github.io/blog/scaling-exponents/Pasted%20image%2020240730044030.png"></figure><p>It looks <em>extremely similar</em>, but <strong>not identical</strong> to the <a href="https://arxiv.org/pdf/2407.05872#page=56" target="_blank" rel="noopener noreffer">original Adam+GlobalLR+default graphs</a>:</p><figure><img src="https://152334h.github.io/blog/scaling-exponents/Pasted%20image%2020240730044047.png"></figure><p>I have no idea what the differences are supposed to be here. However, in the interest of sticking with the paper’s behaviour, I attempt to include the compute used for these psuedo-repeated experiments.</p></li></ol></div></div><p>For each of these issues, I do my best to pick an approximation that makes sense to me in the later sections.</p><hr><h2 id="transformer-information">Transformer information</h2><p>In Appendix C, the model is described as:</p><ul><li>decoder-only</li><li>no bias on weights (including layernorm, which only has learnable scale)</li><li>LPE, pre-LN, GeLU, no tied emb</li><li>T5 Sentencepiece 32k + 1BOS + 100extra, i.e. $V=32101$. This is never stated to be padded.</li><li>“Training inputs are sequence-packed, while evaluation inputs are padded”</li><li>$\text{batch size}=256$, $l_\text{seq}=512$, $L=8$, $D_\text{head}=128$</li><li>$D_\text{head}*H = D$, $R_\text{ffn} = 4$.</li></ul><p>with some extra details for later:</p><ul><li>no dropout</li><li>mostly FSDP</li><li>$P \approx L12D^2 + 2VD$ (this excludes the layernorm params ($2LD$) and the LPE ($Vl_\text{seq}$))</li><li>“The compute optimal experiments include models up to $H = 32$ or $H = 48$, and the fixed (50,000) step experiments include models up to $H = 128$.”</li></ul><h3 id="flops-per-token">FLOPs per token</h3><p>To start, we want to find $M$, the number of FLOPs required per token for a training run.</p><div><p>Basic transformer math</p><div><p>As a reminder for any noam-like transformer, the tensor FLOPs required per token $M$ is approx:</p><p>$$V - \text{vocab size}$$
$$D - \text{hidden dim}$$
$$L - \text{xf layer count}$$</p><p>$$R_{\text{ffn}} - \text{[ffn dim : outer dim] ratio, assuming no GLU}$$
$$R_{kv} - \text{[num k or v heads : num att heads] ratio}$$
$$l_{seq} - \text{assumed average sequence length}$$</p><p>$$M = 12D^2L(1 + R_{kv} + R_{\text{ffn}}) + 6DL\cdot l_{seq} + 6DV$$</p><p>In particular, $6DL\cdot l_\text{seq}$ assumes a causal mask halves the computation required (I assume flash-attn does this)</p></div></div><p>The paper does not describe the usage of any GQA/MQA, so I assume $R_\text{kv} = 1$. This gives us</p><blockquote><p>$M=72D^2L + 6DLl_\text{seq} + 6DV = 6D(12DL + Ll_\text{seq} + V) = 6D(L(12D+l_\text{seq}) + V)$</p></blockquote><p>We have additional constants of $L=8$, $l_\text{seq} = 512$, and $V=32101$, so we write:</p><div><table><tbody><tr><td><pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span></code></pre></td><td><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>M</span><span>(</span><span>d</span><span>:</span> <span>int</span><span>,</span> <span>L</span><span>=</span><span>8</span><span>,</span> <span>l_seq</span><span>=</span><span>512</span><span>,</span> <span>V</span><span>=</span><span>32101</span><span>)</span> <span>-&gt;</span> <span>int</span><span>:</span>
</span></span><span><span>    <span>return</span> <span>6</span><span>*</span><span>d</span> <span>*</span> <span>(</span><span>L</span><span>*</span><span>(</span><span>12</span><span>*</span><span>d</span> <span>+</span> <span>l_seq</span><span>)</span> <span>+</span> <span>V</span><span>)</span>
</span></span><span><span><span>TPE</span> <span>=</span> <span>50000</span> <span>*</span> <span>256</span> <span>*</span> <span>512</span>
</span></span></code></pre></td></tr></tbody></table></div><p>For all experiments <em>except the compute-optimal series in Appendix I</em>, we also have a hardcoded number of $steps=50000$ and global $BS=256$, making the total number of tokens seen per experiment $TPE=6.5536\text{e}9$ by default.</p><hr><h2 id="subproblem-alignment-experiments">Subproblem: Alignment experiments</h2><p>I <em>assume</em> the alignment experiments got their optimal LRs from the later experiments, and didn’t do their own sweeps, so that would make the cost simply,
$$
\sum_{d\in {1024,2048,4096}} 4\times\text{tokens per experiment}\times M(d)
$$</p><div><table><tbody><tr><td><pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span><span>6
</span></code></pre></td><td><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>alignment</span><span>()</span> <span>-&gt;</span> <span>int</span><span>:</span>
</span></span><span><span>    <span>return</span> <span>4</span> <span>*</span> <span>TPE</span> <span>*</span> <span>sum</span><span>(</span><span>M</span><span>(</span><span>d</span><span>)</span> <span>for</span> <span>d</span> <span>in</span> <span>[</span><span>1024</span><span>,</span><span>2048</span><span>,</span><span>4096</span><span>])</span>
</span></span><span><span><span># &gt;&gt;&gt; f'{alignment():.3E}'</span>
</span></span><span><span><span># '3.733E+20'</span>
</span></span><span><span><span># &gt;&gt;&gt; cost_of_run(alignment())[0]</span>
</span></span><span><span><span># 888.81395400704</span>
</span></span></code></pre></td></tr></tbody></table></div><p>These experiments would take &lt;US$1k to execute.</p><h2 id="subproblem-table-e1-experiments">Subproblem: Table E1 experiments</h2><p><a href="https://arxiv.org/pdf/2407.05872#page=40" target="_blank" rel="noopener noreffer">Table E1</a> has a neat collection of many of the runs done for obtaining the <em>best</em> eval losses under any given parameterization/optimizer/setting (some combination of global vs per-layer vs $\gamma$-optimal vs $\epsilon$-optimal).</p><p>This is an easier subproblem to tackle than the general issue of <em>all LR sweeps</em>, as the requirements are better known – though still not entirely determined, per the repetition ambiguity mentioned earlier. For that issue, I assume that all experiments were conducted, with no copied results, making the estimate here an upper bound.</p><p>We have the following schedule:</p><ul><li>$D\in {3072, 4096, 6144, 8192, 12288, 16384}$</li><li>4x parameterizations</li><li>3x optimizers, where<ul><li>SGD only receives 5 experimental settings</li><li>Adam &amp; Adam+PS receives 7</li></ul></li></ul><p>$$
\sum_{d\in {3072,4096,6144,8192,12288,16384}} 4\times(5+7*2)\times\text{tokens per experiment}\times M(d)
$$</p><div><table><tbody><tr><td><pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span><span>6
</span><span>7
</span><span>8
</span></code></pre></td><td><pre tabindex="0"><code data-lang="python"><span><span><span>H</span> <span>=</span> <span>[</span><span>1</span><span>,</span><span>2</span><span>,</span><span>4</span><span>,</span><span>6</span><span>,</span><span>8</span><span>,</span><span>12</span><span>,</span><span>16</span><span>,</span><span>20</span><span>,</span><span>24</span><span>,</span><span>32</span><span>,</span><span>48</span><span>,</span><span>64</span><span>,</span><span>96</span><span>,</span><span>128</span><span>]</span>
</span></span><span><span><span>D</span> <span>=</span> <span>[</span><span>h</span> <span>*</span> <span>128</span> <span>for</span> <span>h</span> <span>in</span> <span>H</span><span>]</span>
</span></span><span><span><span>def</span> <span>table_e1</span><span>()</span> <span>-&gt;</span> <span>int</span><span>:</span>
</span></span><span><span>  <span>sets_x_optims</span> <span>=</span> <span>5</span> <span>+</span> <span>7</span> <span>+</span> <span>7</span>
</span></span><span><span>  <span>return</span> <span>4</span> <span>*</span> <span>sets_x_optims</span> <span>*</span> <span>TPE</span> <span>*</span> <span>sum</span><span>(</span><span>M</span><span>(</span><span>d</span><span>)</span> <span>for</span> <span>d</span> <span>in</span> <span>D</span><span>[</span><span>-</span><span>6</span><span>:])</span>
</span></span><span><span><span># &gt;&gt;&gt; f'{table_e1():.3E}';cost_of_run(table_e1())</span>
</span></span><span><span><span># '1.634E+23'</span>
</span></span><span><span><span># (388955.9991064986, 16206.499962770775)</span>
</span></span></code></pre></td></tr></tbody></table></div><p>These would’ve taken slightly below $400k in H100 compute to execute. Reasonably speaking, this is within the bounds of SWE life savings / big academic budgets / TPU Research Cloud upper-class. Technically replicable, albeit not cheap.</p><p>But the bulk of the compute used in the paper comes from the LR sweeps, so we have to start working on that.</p><h2 id="estimating-lr-sweep-damage">Estimating LR sweep damage</h2><p>So, here’s a a graph:
<a href="https://152334h.github.io/blog/scaling-exponents/Pasted%20image%2020240730050758.png" title="/blog/scaling-exponents/Pasted%20image%2020240730050758.png" data-thumbnail="/blog/scaling-exponents/Pasted%20image%2020240730050758.png"></a></p><p><a href="https://152334h.github.io/blog/scaling-exponents/Pasted%20image%2020240730050758.png" title="/blog/scaling-exponents/Pasted%20image%2020240730050758.png" data-thumbnail="/blog/scaling-exponents/Pasted%20image%2020240730050758.png"><img src="https://152334h.github.io/blog/scaling-exponents/Pasted%20image%2020240730050758.png" data-src="/blog/scaling-exponents/Pasted%20image%2020240730050758.png" data-srcset="/blog/scaling-exponents/Pasted%20image%2020240730050758.png, /blog/scaling-exponents/Pasted%20image%2020240730050758.png 1.5x, /blog/scaling-exponents/Pasted%20image%2020240730050758.png 2x" data-sizes="auto" alt="/blog/scaling-exponents/Pasted%20image%2020240730050758.png" width="354" height="371" srcset="https://152334h.github.io/blog/scaling-exponents/Pasted%20image%2020240730050758.png, https://152334h.github.io/blog/scaling-exponents/Pasted%20image%2020240730050758.png 1.5x, https://152334h.github.io/blog/scaling-exponents/Pasted%20image%2020240730050758.png 2x"></a></p><p>Here’s another graph:
</p><a href="https://152334h.github.io/blog/scaling-exponents/Pasted%20image%2020240730050857.png" title="/blog/scaling-exponents/Pasted%20image%2020240730050857.png" data-thumbnail="/blog/scaling-exponents/Pasted%20image%2020240730050857.png"><p><img src="https://152334h.github.io/blog/scaling-exponents/Pasted%20image%2020240730050857.png" data-src="/blog/scaling-exponents/Pasted%20image%2020240730050857.png" data-srcset="/blog/scaling-exponents/Pasted%20image%2020240730050857.png, /blog/scaling-exponents/Pasted%20image%2020240730050857.png 1.5x, /blog/scaling-exponents/Pasted%20image%2020240730050857.png 2x" data-sizes="auto" alt="/blog/scaling-exponents/Pasted%20image%2020240730050857.png" width="367" height="347" srcset="https://152334h.github.io/blog/scaling-exponents/Pasted%20image%2020240730050857.png, https://152334h.github.io/blog/scaling-exponents/Pasted%20image%2020240730050857.png 1.5x, https://152334h.github.io/blog/scaling-exponents/Pasted%20image%2020240730050857.png 2x"></p></a><p>And here’s a third one:
</p><a href="https://152334h.github.io/blog/scaling-exponents/Pasted%20image%2020240730050917.png" title="/blog/scaling-exponents/Pasted%20image%2020240730050917.png" data-thumbnail="/blog/scaling-exponents/Pasted%20image%2020240730050917.png"><p><img src="https://152334h.github.io/blog/scaling-exponents/Pasted%20image%2020240730050917.png" data-src="/blog/scaling-exponents/Pasted%20image%2020240730050917.png" data-srcset="/blog/scaling-exponents/Pasted%20image%2020240730050917.png, /blog/scaling-exponents/Pasted%20image%2020240730050917.png 1.5x, /blog/scaling-exponents/Pasted%20image%2020240730050917.png 2x" data-sizes="auto" alt="/blog/scaling-exponents/Pasted%20image%2020240730050917.png" width="346" height="377" srcset="https://152334h.github.io/blog/scaling-exponents/Pasted%20image%2020240730050917.png, https://152334h.github.io/blog/scaling-exponents/Pasted%20image%2020240730050917.png 1.5x, https://152334h.github.io/blog/scaling-exponents/Pasted%20image%2020240730050917.png 2x"></p></a><p>Guess what?</p><ol><li>There isn’t a constant num. of LRs sweeped for a given $D$, or optim/parameterization/setting.<ul><li>Especially notable: number of runs seems inversely correlated with $D$; there are almost always less runs for the highest dim than the lowest.</li></ul></li><li>Neither is there an observable cutoff for when the runs stop – runs will spike up to 2x the optimal no problem.</li><li>You can’t get the <em>exact</em> correct number of runs by graph-reading; in many cases the points are out-of-bounds.</li></ol><p>The consistencies I <em>do</em> spot are that:</p><ul><li>there is typically a “starting LR” (smallest base) for any given line.</li><li>the hollowed points are typically to the right – but sometimes left – of the optimal point.</li></ul><p>so <strong>I <em>think</em> the mechanism worked this way</strong>:</p><ol><li>start a sweep with a starting LR and some expected jumpsizes of $\sqrt{2}$ or $\sqrt{\surd 2}$.</li><li>terminate it by the 20% / NaN heuristic.</li><li>if the graph looks weird (optimal point somewhere odd), rerun to fill many $2^{0.25}$ intervals around the current optimal. These result in the plotted hollow points</li></ol><p>I have no means of confirming this as the experimental procedure, as the authors of the paper stopped replying to me.</p><h3 id="an-arbitrary-decision">An arbitrary decision</h3><p>Due to my desire to finish this blog post in a reasonable amount of time, I made the unprincipled decision of approximating the number of experiments-per-line in any given Eval Loss vs Base Learning Rate graph as <strong>15</strong>.</p><p>Why 15? By eyeballing, the range of runs-per-line for the highest $D=16384$ hovers around 10~15. Although the lines with smaller D tend to have far more points on average, the amount of compute spent per run scales by $O(D^2)$, so I think this is fair enough.</p><p>Feel free to suggest a more principled approach if you have one.</p><hr><h2 id="main-problem-epslion">Main problem: Epslion</h2><p>Much of the compute used up by the paper comes from <a href="https://arxiv.org/pdf/2407.05872#page=15" target="_blank" rel="noopener noreffer">Section 4.3</a>, the Adam epslion experiments.</p><h3 id="optimal-eps-runs">Optimal eps runs</h3><p>Now that we have an estimate of LRs-per-line as 15, we can estimate the compute spent on the actual <a href="https://arxiv.org/pdf/2407.05872#page=44" target="_blank" rel="noopener noreffer">Adam epslion varying graphs</a>:</p><p>$$
\sum_{d} 4*(2+4) \times \text{points per line}\times\text{tokens per experiment}\times M(d)
$$</p><div><table><tbody><tr><td><pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span><span>6
</span><span>7
</span><span>8
</span></code></pre></td><td><pre tabindex="0"><code data-lang="python"><span><span><span>PpL</span> <span>=</span> <span>15</span> <span># unprincipled estimate</span>
</span></span><span><span><span>def</span> <span>eps_variants</span><span>()</span> <span>-&gt;</span> <span>int</span><span>:</span>
</span></span><span><span>  <span>return</span> <span>4</span> <span>*</span> <span>6</span> <span>*</span> <span>PpL</span> <span>*</span> <span>TPE</span> <span>*</span> <span>sum</span><span>(</span><span>M</span><span>(</span><span>d</span><span>)</span> <span>for</span> <span>d</span> <span>in</span> <span>D</span><span>)</span>
</span></span><span><span><span>'''
</span></span></span><span><span><span>&gt;&gt;&gt; f'{eps_variants():.3E}';cost_of_run(eps_variants())
</span></span></span><span><span><span>'7.988E+23'
</span></span></span><span><span><span>(1902022.3291813303, 79250.93038255542)
</span></span></span><span><span><span>'''</span>
</span></span></code></pre></td></tr></tbody></table></div><p>Simple enough, right? Ignoring the ~$2M bill.</p><h3 id="epslion-heatmaps">Epslion Heatmaps</h3><p>There are two ways you could approach the expected sweep range for this problem:</p><ol><li>assume the LR experiment sweep code was reused. All 14x $D$, LR swept by arcane unknown ruleset.</li><li>Limit to the graphs. Only the last 6 values of $D$ were shown – assume only those were used. Plus, if we look at <a href="https://arxiv.org/pdf/2407.05872#page=16" target="_blank" rel="noopener noreffer">Figure 6</a>:<figure><img src="https://152334h.github.io/blog/scaling-exponents/Pasted%20image%2020240730044906.png"></figure>Notice that the range of evaluated learning rates actually seems constant here, unlike in the normal Eval Loss vs Base LR plots.</li></ol><p>I’m picking the latter because it’s simpler. Would be happy to be shown evidence that this is wrong.</p><p>$$ \sum_{d\in {3072,4096,6144,8192,12288,16384}} 4\cdot 2\cdot 6\cdot 13\times \text{tokens per experiment}\times M(d) $$</p><div><table><tbody><tr><td><pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span><span>6
</span><span>7
</span><span>8
</span></code></pre></td><td><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>eps_heatmaps</span><span>()</span> <span>-&gt;</span> <span>int</span><span>:</span>
</span></span><span><span>  <span># eps-type * eps-val * parameterizations * LR range * ...</span>
</span></span><span><span>  <span>return</span> <span>2</span> <span>*</span> <span>6</span> <span>*</span> <span>4</span> <span>*</span> <span>13</span> <span>*</span> <span>TPE</span> <span>*</span> <span>sum</span><span>(</span><span>M</span><span>(</span><span>d</span><span>)</span> <span>for</span> <span>d</span> <span>in</span> <span>D</span><span>[</span><span>-</span><span>6</span><span>:])</span>
</span></span><span><span><span>'''
</span></span></span><span><span><span>&gt;&gt;&gt; f'{eps_heatmaps():.3E}';cost_of_run(eps_heatmaps())
</span></span></span><span><span><span>'1.341E+24'
</span></span></span><span><span><span>(3193533.466348094, 133063.89443117057)
</span></span></span><span><span><span>'''</span>
</span></span></code></pre></td></tr></tbody></table></div><h4 id="these-squares-are-worth-us32-million">These squares are worth US$3.2 Million</h4><figure><img src="https://152334h.github.io/blog/scaling-exponents/Pasted%20image%2020240730060102.png"></figure><p>To be clear, this is supposed to be an <strong>underestimate</strong> of the budget required, because we model the average number of unique LRs used per heatmap square as a constant $13$ instead of the (typically higher) value used in variable LR sweeps.</p><h2 id="main-problem-lr-sweep-strategies">Main problem: LR Sweep Strategies</h2><p>The other meat of the paper is in Section 4.2, the $\text{optimizer}\times\text{parameterization}\times D\times\text{LR setting}\times\text{alignment}\times\text{LR Sweeps}$ experiments.</p><h3 id="beta-only-experiments">$\beta$-only experiments</h3><p>“$\beta$” refers to the empirically obtained base LR constant under the equation $\eta_l = \beta_n\cdot\frac{n}{b}^{-c_l}$, also known as the <code>+default</code> experiments.</p><p>The paper sweeps this for 3x optimizers, 4x parameterizations, 14x widths, global vs per-layer $c_l$, and of course unknown LR sweep counts.</p><p>$$ \sum_{d} 3*4*2 \times \text{points per line}\times\text{tokens per experiment}\times M(d) $$</p><div><table><tbody><tr><td><pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span></code></pre></td><td><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>beta_only</span><span>()</span> <span>-&gt;</span> <span>int</span><span>:</span>
</span></span><span><span>  <span>return</span> <span>3</span><span>*</span><span>4</span><span>*</span><span>2</span><span>*</span><span>PpL</span> <span>*</span> <span>TPE</span> <span>*</span> <span>sum</span><span>(</span><span>M</span><span>(</span><span>d</span><span>)</span> <span>for</span> <span>d</span> <span>in</span> <span>D</span><span>)</span>
</span></span><span><span><span># 7.988E+23 (1902022.3291813303, 79250.93038255542)</span>
</span></span></code></pre></td></tr></tbody></table></div><p>Incidentally, this has an identical estimated cost to the <a href="#optimal-eps-runs" rel="">epslion variants</a>.</p><h3 id="gamma-experiments">$\gamma$ experiments</h3><p>So, two issues.</p><ol><li>These experiments are “like” the $\beta$-only experiments, but with 3x cases (GlobalLR, Perlayer-fullalign, Perlayer-nolign) instead of 2x (GlobalLR, Perlayer-fullalign).
$$ \sum_{d} 3*4*3 \times \text{points per line}\times\text{tokens per experiment}\times M(d) $$</li><li>Specifically for $d=1024=b$, we have <strong>at least</strong> 800 extra runs, due to the 3D hparam search for $(\gamma_1, \gamma_h, \gamma_{L+1})$.
$$ 3*4*3*800 \times\text{tokens per experiment}\times M(1024) $$</li></ol><p>We can combine those two as,
$$ 36\times\text{tokens per experiment}(800*M(1024) + \text{points per line}\sum_{d}\times M(d)) $$</p><div><table><tbody><tr><td><pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span></code></pre></td><td><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>gamma_expts</span><span>()</span> <span>-&gt;</span> <span>int</span><span>:</span>
</span></span><span><span>  <span>return</span> <span>36</span><span>*</span><span>TPE</span> <span>*</span> <span>(</span><span>800</span><span>*</span><span>M</span><span>(</span><span>1024</span><span>)</span> <span>+</span> <span>PpL</span><span>*</span><span>sum</span><span>(</span><span>M</span><span>(</span><span>d</span><span>)</span> <span>for</span> <span>d</span> <span>in</span> <span>D</span><span>))</span>
</span></span><span><span><span># gamma_expts 	 1.354E+24 (3224397.534237257, 134349.8972598857)</span>
</span></span></code></pre></td></tr></tbody></table></div><p>This is, once again, exceedingly close to that of the Adam $\epslion$ heatmap experiments.</p><p>Sidenote: I may be understanding the per-layer aspect of the paper incorrectly; I expected the compute expenditure of this section to be larger.</p><h3 id="weight-decay">Weight Decay</h3><p>The WD experiments are simple enough. We repeat 4x parameterizations &amp;&amp; do a single base-LR sweep on all $D$</p><p>$$
\sum_{d} 4*(2+4) \times \text{points per line}\times\text{tokens per experiment}\times M(d)
$$</p><div><table><tbody><tr><td><pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span><span>6
</span><span>7
</span></code></pre></td><td><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>weight_decay</span><span>()</span> <span>-&gt;</span> <span>int</span><span>:</span>
</span></span><span><span>  <span>return</span> <span>4</span> <span>*</span> <span>PpL</span> <span>*</span> <span>TPE</span> <span>*</span> <span>sum</span><span>(</span><span>M</span><span>(</span><span>d</span><span>)</span> <span>for</span> <span>d</span> <span>in</span> <span>D</span><span>)</span>
</span></span><span><span><span>'''
</span></span></span><span><span><span>&gt;&gt;&gt; f'{weight_decay():.3E}'; cost_of_run(weight_decay())
</span></span></span><span><span><span>'1.331E+23'
</span></span></span><span><span><span>(317003.7215302217, 13208.488397092571)
</span></span></span><span><span><span>'''</span>
</span></span></code></pre></td></tr></tbody></table></div><p>Incredibly cheap, I could afford that in some years.</p><h3 id="adafactor">Adafactor</h3><p>As a reminder, I only count the first 11 $D$, even though the report actually has 12 in one graph.</p><p>$$
\sum_{d\in D[:11]} 2 * 2* 4\times \text{points per line}\times\text{tokens per experiment}\times M(d)
$$</p><div><table><tbody><tr><td><pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span><span>6
</span><span>7
</span></code></pre></td><td><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>adafactor</span><span>()</span> <span>-&gt;</span> <span>int</span><span>:</span>
</span></span><span><span>  <span>return</span> <span>2</span><span>*</span><span>2</span><span>*</span><span>4</span><span>*</span><span>PpL</span><span>*</span><span>TPE</span><span>*</span><span>sum</span><span>(</span><span>M</span><span>(</span><span>d</span><span>)</span> <span>for</span> <span>d</span> <span>in</span> <span>D</span><span>[:</span><span>11</span><span>])</span>
</span></span><span><span><span>'''
</span></span></span><span><span><span>&gt;&gt;&gt; f'{adafactor():.3E}'; cost_of_run(adafactor())
</span></span></span><span><span><span>'7.918E+22'
</span></span></span><span><span><span>(188532.80765144504, 7855.533652143543)
</span></span></span><span><span><span>'''</span>
</span></span></code></pre></td></tr></tbody></table></div><h3 id="compute-optimal">Compute Optimal</h3><p>The paper states that,</p><blockquote><p>The compute optimal experiments include models up to $H = 32$ or $H = 48$, and the fixed (50,000) step experiments include models up to $H = 128$.</p></blockquote><p>If you read the graphs in <a href="https://arxiv.org/pdf/2407.05872#page=50" target="_blank" rel="noopener noreffer">Appendix I</a>, this is slightly wrong, because</p><ul><li><p>50k experiments go to $H=48$ on Adafactor, and $H=128$ otherwise</p></li><li><p>all compute optimal experiments go up to $H=32$ only.</p><p>Note that a 4B param run requires 80B tokens by chinchilla, and C4 is less than 200B tokens, so they couldn’t have gone higher without changing the dataset.</p></li></ul><p>This is honestly a bit complex, so let’s forgo the latex and just describe it in python:</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span></code></pre></td><td><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>P</span><span>(</span><span>d</span><span>:</span> <span>int</span><span>,</span> <span>L</span><span>=</span><span>8</span><span>,</span> <span>V</span><span>=</span><span>32101</span><span>)</span> <span>-&gt;</span> <span>int</span><span>:</span>
</span></span><span><span>    <span>return</span> <span>2</span> <span>*</span> <span>d</span> <span>*</span> <span>(</span><span>6</span><span>*</span><span>L</span><span>*</span><span>d</span> <span>+</span> <span>V</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>compute_optimal</span><span>():</span>
</span></span><span><span>  <span>indices_50k</span> <span>=</span> <span>(</span><span>14</span><span>,</span> <span>14</span><span>,</span> <span>12</span><span>)</span>
</span></span><span><span>  <span>return</span> <span>4</span><span>*</span><span>PpL</span><span>*</span><span>sum</span><span>([</span>
</span></span><span><span>    <span>TPE</span> <span>*</span> <span>sum</span><span>(</span><span>sum</span><span>(</span> <span>M</span><span>(</span><span>d</span><span>)</span> <span>for</span> <span>d</span> <span>in</span> <span>D</span><span>[:</span><span>i</span><span>]</span> <span>)</span> <span>for</span> <span>i</span> <span>in</span> <span>indices_50k</span><span>),</span>
</span></span><span><span>	<span>20</span>  <span>*</span> <span>sum</span><span>(</span><span>P</span><span>(</span><span>d</span><span>)</span><span>*</span><span>M</span><span>(</span><span>d</span><span>)</span> <span>for</span> <span>d</span> <span>in</span> <span>D</span><span>[:</span><span>11</span><span>])</span> <span>*</span><span>3</span><span>,</span>
</span></span><span><span>  <span>])</span>
</span></span><span><span><span># compute_optim 	 7.518E+23 (1790104.1799513847, 74587.67416464102)</span>
</span></span></code></pre></td></tr></tbody></table></div><h2 id="code-summary">Code summary</h2><p>Here is the full script to get the estimates I created:</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span><span>21
</span><span>22
</span><span>23
</span><span>24
</span><span>25
</span><span>26
</span><span>27
</span><span>28
</span><span>29
</span><span>30
</span><span>31
</span><span>32
</span><span>33
</span><span>34
</span><span>35
</span><span>36
</span><span>37
</span><span>38
</span><span>39
</span><span>40
</span><span>41
</span><span>42
</span><span>43
</span><span>44
</span><span>45
</span><span>46
</span><span>47
</span><span>48
</span><span>49
</span><span>50
</span><span>51
</span><span>52
</span><span>53
</span><span>54
</span><span>55
</span><span>56
</span><span>57
</span><span>58
</span><span>59
</span><span>60
</span><span>61
</span><span>62
</span><span>63
</span></code></pre></td><td><pre tabindex="0"><code data-lang="python"><span><span><span>TPE</span> <span>=</span> <span>50000</span> <span>*</span> <span>256</span> <span>*</span> <span>512</span>
</span></span><span><span><span>H</span> <span>=</span> <span>[</span><span>1</span><span>,</span><span>2</span><span>,</span><span>4</span><span>,</span><span>6</span><span>,</span><span>8</span><span>,</span><span>12</span><span>,</span><span>16</span><span>,</span><span>20</span><span>,</span><span>24</span><span>,</span><span>32</span><span>,</span><span>48</span><span>,</span><span>64</span><span>,</span><span>96</span><span>,</span><span>128</span><span>]</span>
</span></span><span><span><span>D</span> <span>=</span> <span>[</span><span>h</span> <span>*</span> <span>128</span> <span>for</span> <span>h</span> <span>in</span> <span>H</span><span>]</span>
</span></span><span><span><span>PpL</span> <span>=</span> <span>15</span> <span># unprincipled estimate</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>cost_of_run</span><span>(</span><span>flops</span><span>:</span> <span>float</span><span>,</span> <span>pergpu_flops</span><span>=</span><span>3.5e14</span><span>):</span>
</span></span><span><span>  <span>gpu_hours</span> <span>=</span> <span>flops</span> <span>/</span> <span>3600</span> <span>/</span> <span>pergpu_flops</span>
</span></span><span><span>  <span>rental_cost</span> <span>=</span> <span>3</span> <span>*</span> <span>gpu_hours</span>
</span></span><span><span>  <span>single_node_duration</span> <span>=</span> <span>gpu_hours</span> <span>/</span> <span>8</span>
</span></span><span><span>  <span>return</span> <span>rental_cost</span><span>,</span> <span>single_node_duration</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>M</span><span>(</span><span>d</span><span>:</span> <span>int</span><span>,</span> <span>L</span><span>=</span><span>8</span><span>,</span> <span>l_seq</span><span>=</span><span>512</span><span>,</span> <span>V</span><span>=</span><span>32101</span><span>)</span> <span>-&gt;</span> <span>int</span><span>:</span>
</span></span><span><span>    <span>return</span> <span>6</span><span>*</span><span>d</span> <span>*</span> <span>(</span><span>L</span><span>*</span><span>(</span><span>12</span><span>*</span><span>d</span> <span>+</span> <span>l_seq</span><span>)</span> <span>+</span> <span>V</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>P</span><span>(</span><span>d</span><span>:</span> <span>int</span><span>,</span> <span>L</span><span>=</span><span>8</span><span>,</span> <span>V</span><span>=</span><span>32101</span><span>)</span> <span>-&gt;</span> <span>int</span><span>:</span>
</span></span><span><span>    <span>return</span> <span>2</span> <span>*</span> <span>d</span> <span>*</span> <span>(</span><span>6</span><span>*</span><span>L</span><span>*</span><span>d</span> <span>+</span> <span>V</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>alignment</span><span>()</span> <span>-&gt;</span> <span>int</span><span>:</span>
</span></span><span><span>    <span>return</span> <span>4</span> <span>*</span> <span>TPE</span> <span>*</span> <span>sum</span><span>(</span><span>M</span><span>(</span><span>d</span><span>)</span> <span>for</span> <span>d</span> <span>in</span> <span>[</span><span>1024</span><span>,</span><span>2048</span><span>,</span><span>4096</span><span>])</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>table_e1</span><span>()</span> <span>-&gt;</span> <span>int</span><span>:</span>
</span></span><span><span>  <span>sets_x_optims</span> <span>=</span> <span>5</span> <span>+</span> <span>7</span> <span>+</span> <span>7</span>
</span></span><span><span>  <span>return</span> <span>4</span> <span>*</span> <span>sets_x_optims</span> <span>*</span> <span>TPE</span> <span>*</span> <span>sum</span><span>(</span><span>M</span><span>(</span><span>d</span><span>)</span> <span>for</span> <span>d</span> <span>in</span> <span>D</span><span>[</span><span>-</span><span>6</span><span>:])</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>eps_variants</span><span>()</span> <span>-&gt;</span> <span>int</span><span>:</span>
</span></span><span><span>  <span>return</span> <span>4</span> <span>*</span> <span>6</span> <span>*</span> <span>PpL</span> <span>*</span> <span>TPE</span> <span>*</span> <span>sum</span><span>(</span><span>M</span><span>(</span><span>d</span><span>)</span> <span>for</span> <span>d</span> <span>in</span> <span>D</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>eps_heatmaps</span><span>()</span> <span>-&gt;</span> <span>int</span><span>:</span>
</span></span><span><span>  <span>return</span> <span>2</span> <span>*</span> <span>6</span> <span>*</span> <span>4</span> <span>*</span> <span>13</span> <span>*</span> <span>TPE</span> <span>*</span> <span>sum</span><span>(</span><span>M</span><span>(</span><span>d</span><span>)</span> <span>for</span> <span>d</span> <span>in</span> <span>D</span><span>[</span><span>-</span><span>6</span><span>:])</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>beta_only</span><span>()</span> <span>-&gt;</span> <span>int</span><span>:</span>
</span></span><span><span>  <span>return</span> <span>3</span><span>*</span><span>4</span><span>*</span><span>2</span><span>*</span><span>PpL</span> <span>*</span> <span>TPE</span> <span>*</span> <span>sum</span><span>(</span><span>M</span><span>(</span><span>d</span><span>)</span> <span>for</span> <span>d</span> <span>in</span> <span>D</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>gamma_expts</span><span>()</span> <span>-&gt;</span> <span>int</span><span>:</span>
</span></span><span><span>  <span>return</span> <span>36</span><span>*</span><span>TPE</span> <span>*</span> <span>(</span><span>800</span><span>*</span><span>M</span><span>(</span><span>1024</span><span>)</span> <span>+</span> <span>PpL</span><span>*</span><span>sum</span><span>(</span><span>M</span><span>(</span><span>d</span><span>)</span> <span>for</span> <span>d</span> <span>in</span> <span>D</span><span>))</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>weight_decay</span><span>()</span> <span>-&gt;</span> <span>int</span><span>:</span>
</span></span><span><span>  <span>return</span> <span>4</span> <span>*</span> <span>PpL</span> <span>*</span> <span>TPE</span> <span>*</span> <span>sum</span><span>(</span><span>M</span><span>(</span><span>d</span><span>)</span> <span>for</span> <span>d</span> <span>in</span> <span>D</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>adafactor</span><span>()</span> <span>-&gt;</span> <span>int</span><span>:</span>
</span></span><span><span>  <span>return</span> <span>2</span><span>*</span><span>2</span><span>*</span><span>4</span><span>*</span><span>PpL</span><span>*</span><span>TPE</span><span>*</span><span>sum</span><span>(</span><span>M</span><span>(</span><span>d</span><span>)</span> <span>for</span> <span>d</span> <span>in</span> <span>D</span><span>[:</span><span>11</span><span>])</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>compute_optim</span><span>():</span>
</span></span><span><span>  <span>indices_50k</span> <span>=</span> <span>(</span><span>14</span><span>,</span> <span>14</span><span>,</span> <span>12</span><span>)</span>
</span></span><span><span>  <span>return</span> <span>4</span><span>*</span><span>PpL</span><span>*</span><span>sum</span><span>([</span>
</span></span><span><span>    <span>TPE</span> <span>*</span> <span>sum</span><span>(</span><span>sum</span><span>(</span> <span>M</span><span>(</span><span>d</span><span>)</span> <span>for</span> <span>d</span> <span>in</span> <span>D</span><span>[:</span><span>i</span><span>]</span> <span>)</span> <span>for</span> <span>i</span> <span>in</span> <span>indices_50k</span><span>),</span>
</span></span><span><span>	<span>20</span>  <span>*</span> <span>sum</span><span>(</span><span>P</span><span>(</span><span>d</span><span>)</span><span>*</span><span>M</span><span>(</span><span>d</span><span>)</span> <span>for</span> <span>d</span> <span>in</span> <span>D</span><span>[:</span><span>11</span><span>])</span> <span>*</span><span>3</span><span>,</span>
</span></span><span><span>  <span>])</span>
</span></span><span><span>
</span></span><span><span><span>total_flops</span><span>,</span> <span>total_price</span><span>,</span> <span>total_hours</span> <span>=</span> <span>0</span><span>,</span><span>0</span><span>,</span><span>0</span>
</span></span><span><span><span>for</span> <span>f</span> <span>in</span> <span>(</span><span>alignment</span><span>,</span><span>table_e1</span><span>,</span><span>eps_variants</span><span>,</span><span>eps_heatmaps</span><span>,</span><span>beta_only</span><span>,</span><span>gamma_expts</span><span>,</span><span>weight_decay</span><span>,</span><span>adafactor</span><span>,</span><span>compute_optim</span><span>):</span>
</span></span><span><span>    <span>flops</span> <span>=</span> <span>f</span><span>()</span>
</span></span><span><span>    <span>costs</span> <span>=</span> <span>cost_of_run</span><span>(</span><span>flops</span><span>)</span>
</span></span><span><span>    <span>print</span><span>(</span><span>f</span><span>'</span><span>{</span><span>f</span><span>.</span><span>__name__</span><span>:</span><span>15</span><span>}</span><span>'</span><span>,</span> <span>f</span><span>'</span><span>{</span><span>flops</span><span>:</span><span>.3E</span><span>}</span><span>'</span><span>,</span><span>costs</span><span>)</span>
</span></span><span><span>    <span>total_flops</span> <span>+=</span> <span>flops</span><span>;</span> <span>total_price</span> <span>+=</span> <span>costs</span><span>[</span><span>0</span><span>];</span> <span>total_hours</span> <span>+=</span> <span>costs</span><span>[</span><span>1</span><span>]</span>
</span></span><span><span>
</span></span><span><span><span>print</span><span>(</span><span>f</span><span>'</span><span>{</span><span>total_flops</span><span>=:</span><span>.3E</span><span>}</span><span>'</span><span>)</span>
</span></span><span><span><span>print</span><span>(</span><span>f</span><span>'rental price: US$</span><span>{</span><span>total_price</span><span>/</span><span>1e6</span><span>:</span><span>.3</span><span>}</span><span>M'</span><span>)</span>
</span></span><span><span><span>print</span><span>(</span><span>f</span><span>'h100 node months required: </span><span>{</span><span>total_hours</span><span>/</span><span>24</span><span>/</span><span>30</span><span>}</span><span>'</span><span>)</span>
</span></span><span><span><span>print</span><span>()</span>
</span></span><span><span><span>print</span><span>(</span><span>f</span><span>'(sanity check) </span><span>{</span><span>D</span><span>=}</span><span>'</span><span>)</span>
</span></span><span><span><span>print</span><span>(</span><span>'(sanity check) model sizes:'</span><span>,</span> <span>[</span><span>f</span><span>'</span><span>{</span><span>P</span><span>(</span><span>d</span><span>)</span><span>/</span><span>1e9</span><span>:</span><span>.3</span><span>}</span><span>B'</span> <span>for</span> <span>d</span> <span>in</span> <span>D</span><span>])</span>
</span></span><span><span><span>print</span><span>(</span><span>'(sanity check) M/6P:'</span><span>,</span> <span>[</span><span>f</span><span>'</span><span>{</span><span>100</span><span>*</span><span>M</span><span>(</span><span>d</span><span>)</span><span>/</span><span>P</span><span>(</span><span>d</span><span>)</span><span>/</span><span>6</span><span>:</span><span>.3</span><span>}</span><span>%'</span> <span>for</span> <span>d</span> <span>in</span> <span>D</span><span>])</span>
</span></span></code></pre></td></tr></tbody></table></div><p>This gives the following:</p><div><table><tbody><tr><td><pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span><span>6
</span><span>7
</span><span>8
</span><span>9
</span></code></pre></td><td><pre tabindex="0"><code data-lang="fallback"><span><span>alignment       3.733E+20 (888.81395400704, 37.033914750293334)
</span></span><span><span>table_e1        1.634E+23 (388955.9991064986, 16206.499962770775)
</span></span><span><span>eps_variants    7.988E+23 (1902022.3291813303, 79250.93038255542)
</span></span><span><span>eps_heatmaps    1.341E+24 (3193533.466348094, 133063.89443117057)
</span></span><span><span>beta_only       7.988E+23 (1902022.3291813303, 79250.93038255542)
</span></span><span><span>gamma_expts     1.354E+24 (3224397.534237257, 134349.8972598857)
</span></span><span><span>weight_decay    1.331E+23 (317003.7215302217, 13208.488397092571)
</span></span><span><span>adafactor       7.918E+22 (188532.80765144504, 7855.533652143543)
</span></span><span><span>compute_optim   7.518E+23 (1790104.1799513847, 74587.67416464102)
</span></span></code></pre></td></tr></tbody></table></div><div><table><tbody><tr><td><pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span><span>6
</span><span>7
</span></code></pre></td><td><pre tabindex="0"><code data-lang="fallback"><span><span>total_flops=5.421E+24
</span></span><span><span>rental price: US$12.9M
</span></span><span><span>h100 node months required: 746.9595590938408
</span></span><span><span>
</span></span><span><span>(sanity check) D=[128, 256, 512, 768, 1024, 1536, 2048, 2560, 3072, 4096, 6144, 8192, 12288, 16384]
</span></span><span><span>(sanity check) model sizes: ['0.00979B', '0.0227B', '0.058B', '0.106B', '0.166B', '0.325B', '0.534B', '0.794B', '1.1B', '1.87B', '4.02B', '6.97B', '15.3B', '26.8B']
</span></span><span><span>(sanity check) M/6P: ['63.4%', '68.5%', '75.3%', '79.7%', '82.8%', '86.8%', '89.3%', '91.0%', '92.2%', '93.9%', '95.7%', '96.7%', '97.7%', '98.3%']
</span></span></code></pre></td></tr></tbody></table></div><p>In the grand scheme of things, 5.42e24 is “not that big”. After all, that’s not even 15% of the <a href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models/" target="_blank" rel="noopener noreffer">compute used for Llama 3</a>; a <a href="https://www.semianalysis.com/p/100000-h100-clusters-power-network" target="_blank" rel="noopener noreffer">100k H100 cluster</a> could accomplish all of these experiments in just 2 days.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[C Macro Reflection in Zig – Zig Has Better C Interop Than C Itself (153 pts)]]></title>
            <link>https://jstrieb.github.io/posts/c-reflection-zig/</link>
            <guid>41106686</guid>
            <pubDate>Tue, 30 Jul 2024 06:57:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jstrieb.github.io/posts/c-reflection-zig/">https://jstrieb.github.io/posts/c-reflection-zig/</a>, See on <a href="https://news.ycombinator.com/item?id=41106686">Hacker News</a></p>
<div id="readability-page-1" class="page">



<h2 id="zig-has-better-c-interop-than-c-itself">Zig Has Better C Interop Than C Itself</h2>
<p>By <a href="https://jstrieb.github.io/">Jacob Strieb</a>.</p>
<p>Published on <a href="https://jstrieb.github.io/posts/c-reflection-zig/">July 30, 2024</a>.</p>
<hr>
<h2 id="zig">Zig</h2>
<p><a href="https://ziglang.org/learn/overview/">Zig</a> is a nascent programming language with an emphasis on low-level and systems programming that is positioned to be a C replacement.<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a> Despite being under active development (and having some rough edges as a result), Zig is extremely powerful, and is already used by a few substantial projects such as <a href="https://github.com/oven-sh/bun">Bun</a> and <a href="https://github.com/tigerbeetle/tigerbeetle">TigerBeetle</a>.</p>
<p>Zig has many interesting features, but its outstanding interoperability (“interop”) with C is especially impressive. It is easy to call an external library, as in this example from <a href="https://ziglang.org/learn/samples/">the Zig website</a>:</p>
<div id="cb1"><pre><code><span id="cb1-1"><a href="#cb1-1"></a><span>const</span> win <span>=</span> <span>@import</span>(<span>"std"</span>)<span>.</span>os<span>.</span>windows;</span>
<span id="cb1-2"><a href="#cb1-2"></a></span>
<span id="cb1-3"><a href="#cb1-3"></a><span>extern</span> <span>"user32"</span> <span>fn</span> MessageBoxA(</span>
<span id="cb1-4"><a href="#cb1-4"></a>  <span>?</span>win<span>.</span>HWND<span>,</span> </span>
<span id="cb1-5"><a href="#cb1-5"></a>  [<span>*:</span><span>0</span>]<span>const</span> <span>u8</span><span>,</span> </span>
<span id="cb1-6"><a href="#cb1-6"></a>  [<span>*:</span><span>0</span>]<span>const</span> <span>u8</span><span>,</span> </span>
<span id="cb1-7"><a href="#cb1-7"></a>  <span>u32</span><span>,</span></span>
<span id="cb1-8"><a href="#cb1-8"></a>) <span>callconv</span>(win<span>.</span>WINAPI) <span>i32</span>;</span>
<span id="cb1-9"><a href="#cb1-9"></a></span>
<span id="cb1-10"><a href="#cb1-10"></a><span>pub</span> <span>fn</span> main() <span>!</span><span>void</span> {</span>
<span id="cb1-11"><a href="#cb1-11"></a>    _ <span>=</span> MessageBoxA(<span>null</span><span>,</span> <span>"world!"</span><span>,</span> <span>"Hello"</span><span>,</span> <span>0</span>);</span>
<span id="cb1-12"><a href="#cb1-12"></a>}</span></code></pre></div>
<p>Calling external functions from C libraries is convenient, but lots of languages can do that. What is more impressive is that, in Zig, it is trivial to import C header files and use them as if they were regular Zig imports. We can rewrite the above to use the Windows header files, instead of manually forward-declaring <code>extern</code> functions:<a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<div id="cb2"><pre><code><span id="cb2-1"><a href="#cb2-1"></a><span>const</span> win32 <span>=</span> <span>@cImport</span>({</span>
<span id="cb2-2"><a href="#cb2-2"></a>    <span>@cInclude</span>(<span>"windows.h"</span>);</span>
<span id="cb2-3"><a href="#cb2-3"></a>    <span>@cInclude</span>(<span>"winuser.h"</span>);</span>
<span id="cb2-4"><a href="#cb2-4"></a>});</span>
<span id="cb2-5"><a href="#cb2-5"></a></span>
<span id="cb2-6"><a href="#cb2-6"></a><span>pub</span> <span>fn</span> main() <span>!</span><span>void</span> {</span>
<span id="cb2-7"><a href="#cb2-7"></a>    _ <span>=</span> win32<span>.</span>MessageBoxA(<span>null</span><span>,</span> <span>"world!"</span><span>,</span> <span>"Hello"</span><span>,</span> <span>0</span>);</span>
<span id="cb2-8"><a href="#cb2-8"></a>}</span></code></pre></div>
<p>The following command will compile both of the code examples above for Windows from any host operating system:</p>
<div id="cb3"><pre><code><span id="cb3-1"><a href="#cb3-1"></a><span># Using Zig 0.13.0</span></span>
<span id="cb3-2"><a href="#cb3-2"></a><span>zig</span> build-exe -lc -target x86_64-windows-gnu main.zig</span></code></pre></div>
<p>I continue to be astounded and delighted that that this code can both be written and cross-compiled so easily on any system.<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<h2 id="windows-programming">Windows Programming</h2>
<p>I have done my fair share of C programming, but until recently, I had never written a Win32 application,<a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a> nor had I ever written a program in Zig.<a href="#fn5" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<p>A typical Windows application has a <code>main</code> (or <code>wWinMain</code>) function and a “window procedure” (<code>WindowProc</code>) function. The main function initializes the application, and runs the loop in which messages are dispatched to the window procedure. The window procedure receives and handles the messages, typically taking a different action for each message type. To quote <a href="https://learn.microsoft.com/en-us/windows/win32/learnwin32/window-messages">the Microsoft website</a>:</p>
<blockquote>
<p>Windows uses a message-passing model. The operating system communicates with your application window by passing messages to it. A message is simply a numeric code that designates a particular event. For example, if the user presses the left mouse button, the window receives a message that has the following message code.</p>
<div id="cb4"><pre><code><span id="cb4-1"><a href="#cb4-1"></a><span>#define WM_LBUTTONDOWN 0x0201</span></span></code></pre></div>
<p>Some messages have data associated with them. For example, the <code>WM_LBUTTONDOWN</code> message includes the x-coordinate and y-coordinate of the mouse cursor.</p>
</blockquote>
<p>In practice, the window procedure becomes an enormous <code>switch</code> statement that matches the message code (<code>uMsg</code> in the example below) against macros defined in <code>winuser.h</code>. A minimal Zig example of a Win32 application with the standard structure (abridged from <a href="https://learn.microsoft.com/en-us/windows/win32/learnwin32/your-first-windows-program">the Microsoft Win32 tutorial sequence</a>) is as follows:</p>
<div id="cb5"><pre><code><span id="cb5-1"><a href="#cb5-1"></a><span>const</span> std <span>=</span> <span>@import</span>(<span>"std"</span>);</span>
<span id="cb5-2"><a href="#cb5-2"></a><span>const</span> windows <span>=</span> std<span>.</span>os<span>.</span>windows;</span>
<span id="cb5-3"><a href="#cb5-3"></a><span>const</span> win32 <span>=</span> <span>@cImport</span>({</span>
<span id="cb5-4"><a href="#cb5-4"></a>    <span>@cInclude</span>(<span>"windows.h"</span>);</span>
<span id="cb5-5"><a href="#cb5-5"></a>    <span>@cInclude</span>(<span>"winuser.h"</span>);</span>
<span id="cb5-6"><a href="#cb5-6"></a>});</span>
<span id="cb5-7"><a href="#cb5-7"></a></span>
<span id="cb5-8"><a href="#cb5-8"></a><span>var</span> stdout<span>:</span> std<span>.</span>fs<span>.</span>File<span>.</span>Writer <span>=</span> <span>undefined</span>;</span>
<span id="cb5-9"><a href="#cb5-9"></a></span>
<span id="cb5-10"><a href="#cb5-10"></a><span>pub</span> <span>export</span> <span>fn</span> WindowProc(hwnd<span>:</span> win32<span>.</span>HWND<span>,</span> uMsg<span>:</span> <span>c_uint</span><span>,</span> wParam<span>:</span> win32<span>.</span>WPARAM<span>,</span> lParam<span>:</span> win32<span>.</span>LPARAM) <span>callconv</span>(windows<span>.</span>WINAPI) win32<span>.</span>LRESULT {</span>
<span id="cb5-11"><a href="#cb5-11"></a>    <span>// Handle each type of window message we care about</span></span>
<span id="cb5-12"><a href="#cb5-12"></a>    _ <span>=</span> <span>switch</span> (uMsg) {</span>
<span id="cb5-13"><a href="#cb5-13"></a>        win32<span>.</span>WM_CLOSE <span>=&gt;</span> win32<span>.</span>DestroyWindow(hwnd)<span>,</span></span>
<span id="cb5-14"><a href="#cb5-14"></a>        win32<span>.</span>WM_DESTROY <span>=&gt;</span> win32<span>.</span>PostQuitMessage(<span>0</span>),</span>
<span id="cb5-15"><a href="#cb5-15"></a>        <span>else</span> <span>=&gt;</span> {</span>
<span id="cb5-16"><a href="#cb5-16"></a>            stdout<span>.</span>print(<span>"Unknown window message: 0x{x:0&gt;4}</span><span>\n</span><span>"</span><span>,</span> <span>.</span>{uMsg}) <span>catch</span> <span>undefined</span>;</span>
<span id="cb5-17"><a href="#cb5-17"></a>        }<span>,</span></span>
<span id="cb5-18"><a href="#cb5-18"></a>    };</span>
<span id="cb5-19"><a href="#cb5-19"></a>    <span>return</span> win32<span>.</span>DefWindowProcA(hwnd<span>,</span> uMsg<span>,</span> wParam<span>,</span> lParam);</span>
<span id="cb5-20"><a href="#cb5-20"></a>}</span>
<span id="cb5-21"><a href="#cb5-21"></a></span>
<span id="cb5-22"><a href="#cb5-22"></a><span>pub</span> <span>export</span> <span>fn</span> main(hInstance<span>:</span> win32<span>.</span>HINSTANCE) <span>c_int</span> {</span>
<span id="cb5-23"><a href="#cb5-23"></a>    stdout <span>=</span> std<span>.</span>io<span>.</span>getStdOut()<span>.</span>writer();</span>
<span id="cb5-24"><a href="#cb5-24"></a></span>
<span id="cb5-25"><a href="#cb5-25"></a>    <span>// Windows boilerplate to set up and draw a window</span></span>
<span id="cb5-26"><a href="#cb5-26"></a>    <span>var</span> class <span>=</span> std<span>.</span>mem<span>.</span>zeroes(win32<span>.</span>WNDCLASSEXA);</span>
<span id="cb5-27"><a href="#cb5-27"></a>    class<span>.</span>cbSize <span>=</span> <span>@sizeOf</span>(win32<span>.</span>WNDCLASSEXA);</span>
<span id="cb5-28"><a href="#cb5-28"></a>    class<span>.</span>style <span>=</span> win32<span>.</span>CS_VREDRAW <span>|</span> win32<span>.</span>CS_HREDRAW;</span>
<span id="cb5-29"><a href="#cb5-29"></a>    class<span>.</span>hInstance <span>=</span> hInstance;</span>
<span id="cb5-30"><a href="#cb5-30"></a>    class<span>.</span>lpszClassName <span>=</span> <span>"Class"</span>;</span>
<span id="cb5-31"><a href="#cb5-31"></a>    class<span>.</span>lpfnWndProc <span>=</span> WindowProc; <span>// Handle messages with this function</span></span>
<span id="cb5-32"><a href="#cb5-32"></a>    _ <span>=</span> win32<span>.</span>RegisterClassExA(<span>&amp;</span>class);</span>
<span id="cb5-33"><a href="#cb5-33"></a></span>
<span id="cb5-34"><a href="#cb5-34"></a>    <span>const</span> hwnd <span>=</span> win32<span>.</span>CreateWindowExA(win32<span>.</span>WS_EX_CLIENTEDGE<span>,</span> <span>"Class"</span><span>,</span> <span>"Window"</span><span>,</span> win32<span>.</span>WS_OVERLAPPEDWINDOW<span>,</span> win32<span>.</span>CW_USEDEFAULT<span>,</span> win32<span>.</span>CW_USEDEFAULT<span>,</span> win32<span>.</span>CW_USEDEFAULT<span>,</span> win32<span>.</span>CW_USEDEFAULT<span>,</span> <span>null</span><span>,</span> <span>null</span><span>,</span> hInstance<span>,</span> <span>null</span>);</span>
<span id="cb5-35"><a href="#cb5-35"></a>    _ <span>=</span> win32<span>.</span>ShowWindow(hwnd<span>,</span> win32<span>.</span>SW_NORMAL);</span>
<span id="cb5-36"><a href="#cb5-36"></a>    _ <span>=</span> win32<span>.</span>UpdateWindow(hwnd);</span>
<span id="cb5-37"><a href="#cb5-37"></a></span>
<span id="cb5-38"><a href="#cb5-38"></a>    <span>// Dispatch messages to WindowProc</span></span>
<span id="cb5-39"><a href="#cb5-39"></a>    <span>var</span> message<span>:</span> win32<span>.</span>MSG <span>=</span> std<span>.</span>mem<span>.</span>zeroes(win32<span>.</span>MSG);</span>
<span id="cb5-40"><a href="#cb5-40"></a>    <span>while</span> (win32<span>.</span>GetMessageA(<span>&amp;</span>message<span>,</span> <span>null</span><span>,</span> <span>0</span>, <span>0</span>) <span>&gt;</span> <span>0</span>) {</span>
<span id="cb5-41"><a href="#cb5-41"></a>        _ <span>=</span> win32<span>.</span>TranslateMessage(<span>&amp;</span>message);</span>
<span id="cb5-42"><a href="#cb5-42"></a>        _ <span>=</span> win32<span>.</span>DispatchMessageA(<span>&amp;</span>message);</span>
<span id="cb5-43"><a href="#cb5-43"></a>    }</span>
<span id="cb5-44"><a href="#cb5-44"></a></span>
<span id="cb5-45"><a href="#cb5-45"></a>    <span>return</span> <span>0</span>;</span>
<span id="cb5-46"><a href="#cb5-46"></a>}</span></code></pre></div>
<p>The output of the code above looks like the following when it is run:</p>
<pre><code>Unknown window message: 0x0024
Unknown window message: 0x0081
Unknown window message: 0x0083
Unknown window message: 0x0001
...
Unknown window message: 0x0008
Unknown window message: 0x0281
Unknown window message: 0x0282
Unknown window message: 0x0082</code></pre>
<h2 id="reflection">Reflection</h2>
<p>When extending the Windows code above to handle new message types, it is troublesome to determine which C macro corresponds to each message the window procedure receives. The numeric value of each message code is printed to the standard output, but mapping the numeric values back to C macro names involves either searching through documentation, or manually walking the header <code>#include</code> tree to find the right macro declaration.</p>
<p>The underlying cause of difficulty in mapping macro values back to macro names is that C does not have <a href="https://en.wikipedia.org/wiki/Reflective_programming">reflection</a> for preprocessor macros – there is no way to get a list of all defined macros, let alone all macros with a specific value, from within C code. The preprocessor runs before the code is actually compiled, so the compiler itself is unaware of macros.<a href="#fn6" id="fnref6" role="doc-noteref"><sup>6</sup></a> The separation between the preprocessor and the compiler enables the user to <a href="https://github.com/DosX-dev/obfus.h">make advanced changes to the code at compile time</a>, but in practice, that separation means compiled code cannot introspect macros.<a href="#fn7" id="fnref7" role="doc-noteref"><sup>7</sup></a></p>
<p>Though it may not be obvious from the code above, in Zig, references to macro and non-macro declarations from imported C header files are made in the same way. For example, <code>win32.TranslateMessage</code> is a function declared in the header file, and <code>win32.WM_CLOSE</code> is a macro declared using <code>#define</code>. Both are used in Zig by doing <code>imported_name.declared_value</code>. The Zig <code>@import</code> function returns a <code>struct</code>, so regular declarations and macros, alike, are represented as fields in the struct generated from importing the C header files.</p>
<p>It is significant that declarations are represented in imports as struct fields because, unlike C, Zig <em>does</em> have reflection. In particular, the <a href="https://ziglang.org/documentation/0.13.0/#typeInfo"><code>@typeInfo</code></a> function lists the fields and declarations of structs passed to it. This means that, though we cannot introspect C macros within C, we can introspect C macros within Zig. Consequently, we can create a mapping of macro values to macro names:</p>
<div id="cb7"><pre><code><span id="cb7-1"><a href="#cb7-1"></a><span>const</span> window_messages <span>=</span> get_window_messages();</span>
<span id="cb7-2"><a href="#cb7-2"></a></span>
<span id="cb7-3"><a href="#cb7-3"></a><span>// The WM_* macros have values less than 65536, so an array of that size can</span></span>
<span id="cb7-4"><a href="#cb7-4"></a><span>// represent all of them</span></span>
<span id="cb7-5"><a href="#cb7-5"></a><span>fn</span> get_window_messages() [<span>65536</span>][:<span>0</span>]<span>const</span> <span>u8</span> {</span>
<span id="cb7-6"><a href="#cb7-6"></a>    <span>var</span> result<span>:</span> [<span>65536</span>][:<span>0</span>]<span>const</span> <span>u8</span> <span>=</span> <span>undefined</span>;</span>
<span id="cb7-7"><a href="#cb7-7"></a>    <span>@setEvalBranchQuota</span>(<span>1000000</span>);</span>
<span id="cb7-8"><a href="#cb7-8"></a>    <span>// Loop over all struct fields and match against the expected prefix</span></span>
<span id="cb7-9"><a href="#cb7-9"></a>    <span>for</span> (<span>@typeInfo</span>(win32)<span>.</span>Struct<span>.</span>decls) <span>|</span>field<span>|</span> {</span>
<span id="cb7-10"><a href="#cb7-10"></a>        <span>if</span> (field<span>.</span>name<span>.</span>len <span>&gt;=</span> <span>3</span> <span>and</span> std<span>.</span>mem<span>.</span>eql(<span>u8</span><span>,</span> field<span>.</span>name[<span>0</span><span>..</span><span>3</span>], "<span>WM_</span><span>")) {</span></span>
<span id="cb7-11"><a href="#cb7-11"></a>            <span>const</span> value <span>=</span> <span>@field</span>(win32<span>,</span> field<span>.</span>name);</span>
<span id="cb7-12"><a href="#cb7-12"></a>            result[value] <span>=</span> field<span>.</span>name;</span>
<span id="cb7-13"><a href="#cb7-13"></a>        }</span>
<span id="cb7-14"><a href="#cb7-14"></a>    }</span>
<span id="cb7-15"><a href="#cb7-15"></a>    <span>// We return by value here, not by reference, so this is safe to do</span></span>
<span id="cb7-16"><a href="#cb7-16"></a>    <span>return</span> result;</span>
<span id="cb7-17"><a href="#cb7-17"></a>}</span></code></pre></div>
<p>Using the global constant <code>window_messages</code>, we can change our <code>WindowProc</code> function to print more helpful information about the messages it is receiving:</p>
<div id="cb8"><pre><code><span id="cb8-1"><a href="#cb8-1"></a><span>pub</span> <span>export</span> <span>fn</span> WindowProc(hwnd<span>:</span> win32<span>.</span>HWND<span>,</span> uMsg<span>:</span> <span>c_uint</span><span>,</span> wParam<span>:</span> win32<span>.</span>WPARAM<span>,</span> lParam<span>:</span> win32<span>.</span>LPARAM) <span>callconv</span>(windows<span>.</span>WINAPI) win32<span>.</span>LRESULT {</span>
<span id="cb8-2"><a href="#cb8-2"></a>    _ <span>=</span> <span>switch</span> (uMsg) {</span>
<span id="cb8-3"><a href="#cb8-3"></a>        win32<span>.</span>WM_CLOSE <span>=&gt;</span> win32<span>.</span>DestroyWindow(hwnd)<span>,</span></span>
<span id="cb8-4"><a href="#cb8-4"></a>        win32<span>.</span>WM_DESTROY <span>=&gt;</span> win32<span>.</span>PostQuitMessage(<span>0</span>),</span>
<span id="cb8-5"><a href="#cb8-5"></a>        <span>else</span> <span>=&gt;</span> {</span>
<span id="cb8-6"><a href="#cb8-6"></a>            <span>// New: print the macro for the current window message</span></span>
<span id="cb8-7"><a href="#cb8-7"></a>            stdout<span>.</span>print(</span>
<span id="cb8-8"><a href="#cb8-8"></a>                <span>"{s}: 0x{x:0&gt;4}</span><span>\n</span><span>"</span><span>,</span> </span>
<span id="cb8-9"><a href="#cb8-9"></a>                <span>.</span>{ window_messages[uMsg]<span>,</span> uMsg }<span>,</span></span>
<span id="cb8-10"><a href="#cb8-10"></a>            ) <span>catch</span> <span>undefined</span>;</span>
<span id="cb8-11"><a href="#cb8-11"></a>        }<span>,</span></span>
<span id="cb8-12"><a href="#cb8-12"></a>    };</span>
<span id="cb8-13"><a href="#cb8-13"></a>    <span>return</span> win32<span>.</span>DefWindowProcA(hwnd<span>,</span> uMsg<span>,</span> wParam<span>,</span> lParam);</span>
<span id="cb8-14"><a href="#cb8-14"></a>}</span></code></pre></div>
<p>Now, the output of the program looks much nicer when run:</p>
<pre><code>...
WM_NCHITTEST: 0x0084
WM_SETCURSOR: 0x0020
WM_MOUSEMOVE: 0x0200
WM_SYSKEYDOWN: 0x0104
WM_CHAR: 0x0102
WM_KEYUP: 0x0101
WM_SYSKEYUP: 0x0105
WM_WINDOWPOSCHANGING: 0x0046
WM_WINDOWPOSCHANGED: 0x0047
WM_NCACTIVATE: 0x0086
WM_ACTIVATE: 0x0006
WM_ACTIVATEAPP: 0x001c
WM_KILLFOCUS: 0x0008
WM_IME_SETCONTEXT: 0x0281
WM_NCDESTROY: 0x0082</code></pre>
<h2 id="conclusion">Conclusion</h2>
<p>Though this example is small, it illustrates that Zig can do what C does, but can do so more ergonomically by employing modern programming language constructs. One of Zig’s unique superpowers is that it bundles a C compiler toolchain – that is what enables it to transcend C <abbr title="Foreign Function Interface">FFI</abbr> and seamlessly include declarations from C header files, among other capabilities.</p>
<p>Incorporating C interoperability so deeply into the language highlights Zig’s prudent acknowledgment that C has been around for a long time, and is here to stay for a while longer. Integrating with C in this way means that Zig developers have had access to thousands of existing, battle-tested software libraries since the language’s first release. It also gives developers responsible for existing C or C++ codebases a path to transition them to Zig. Availability of high-quality libraries and transition paths for existing code are both critical obstacles to language adoption that Zig has cleverly bypassed by electing to subsume C in the course of replacing it.</p>
<p>Zig’s philosophy of pragmatism is apparent as soon as you begin learning the language. Within a few hours of getting started, I was able to come up with this C macro reflection trick, and also able to be generally productive. That is, to me, clear evidence of Zig’s intuitive, consistent design.<a href="#fn8" id="fnref8" role="doc-noteref"><sup>8</sup></a></p>
<p>Zig’s straightforward cross-compilation and C integration are what drew me to the language, but its philosophy and design are what will keep me here to stay.</p>
<h2 id="acknowledgments">Acknowledgments</h2>
<p>Thanks to <a href="https://github.com/lsnow99">Logan Snow</a> and <a href="https://www.linkedin.com/in/amyjl/">Amy Liu</a> for reviewing a draft of this post.</p>
<p>Shout out to <a href="https://andrewkelley.me/">Andrew Kelley</a> and the other Zig contributors.</p>
<section role="doc-endnotes">
<hr>
<ol>
<li id="fn1" role="doc-endnote"><p>Maybe also a C++ replacement, but there are more contenders vying for that role, such as Rust and Go.<a href="#fnref1" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>It’s not so bad when it’s just one external function. But when it’s tens or hundreds, importing the header file directly makes development a lot smoother.<a href="#fnref2" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>Zig also has <code>zig cc</code>, which is a drop-in replacement for GCC and Clang that enables easier-than-ever cross-compilation for C projects. If you ever do cross-compilation, I implore you to <a href="https://andrewkelley.me/post/zig-cc-powerful-drop-in-replacement-gcc-clang.html">read this awesome intro to <code>zig cc</code></a>, then try it for yourself.<a href="#fnref3" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>Mainly because getting the MSVC compiler set up for command-line use outside of Visual Studio is painful. Even figuring out what files to download and where to download them from is not straightforward. On the other hand, Zig cross-compilation has been painless.<a href="#fnref4" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p>As a result, I may not be writing idiomatic (or correct) Zig or Windows code. Everything included here should only be treated as “proof of concept” code for demonstrating an interesting technique.<a href="#fnref5" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p>Most <code>gcc</code> or <code>clang</code> invocations automatically invoke the preprocessor. When I talk about “the compiler” here, I specifically mean the C compiler proper, which runs after the preprocessor is done.<a href="#fnref6" role="doc-backlink">↩︎</a></p></li>
<li id="fn7" role="doc-endnote"><p>At least not without debug information or explicit macro name-value mappings being included in the binary. You could hack something together using <a href="https://en.wikipedia.org/wiki/X_macro">X macros</a> to achieve the latter. But those are a little gross (albeit kind of clever), and only apply if you control the header file where the macros are originally declared, which we don’t in the case of <code>windows.h</code>.<a href="#fnref7" role="doc-backlink">↩︎</a></p></li>
<li id="fn8" role="doc-endnote"><p>The design goals are best explained by Andrew Kelley, the creator of Zig, in his <a href="https://andrewkelley.me/post/intro-to-zig.html">post from 2016 introducing the language and its philosophy</a>.<a href="#fnref8" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[All I Know About Certificates – Certificate Authority (119 pts)]]></title>
            <link>https://www.pixelstech.net/article/1722045726-All-I-Know-About-Certificates----Certificate-Authority</link>
            <guid>41106205</guid>
            <pubDate>Tue, 30 Jul 2024 05:12:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pixelstech.net/article/1722045726-All-I-Know-About-Certificates----Certificate-Authority">https://www.pixelstech.net/article/1722045726-All-I-Know-About-Certificates----Certificate-Authority</a>, See on <a href="https://news.ycombinator.com/item?id=41106205">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article_content_panel">
		<article>
         	<p>One of the crucial steps in the TLS handshake is for the server to prove its identity to the client. While there is plenty of content explaining the principles of the handshake, there's less information about certificates, which are a critical component of TLS/SSL. This series of articles aim to explain what certificates are used for, how Google prevents others from impersonating Google, and why certificate issues frequently arise, among other topics.</p>
<p>(Postscript: It took me a full 10 hours to write these articles. It's quite straightforward, with no mathematical content, just a few OpenSSL commands. It's a long read, so feel free to grab a beer and take your time.)</p>
<p><strong>What Problem Do Certificates Solve?</strong></p>
<p>Imagine there is a bank called <code>super-bank.com</code> that holds billions of dollars in customer funds, allowing users to transfer money via its website.</p>
<p>Now, a hacker walks into a Starbucks, connects to the WiFi, and uses a fake DHCP service to tell other users on the Starbucks WiFi: "I am the gateway, let me forward your internet traffic if you want to go online!" This allows him to see all usernames and passwords of the users.</p>
<p>Some readers might say, "You can use TLS encryption!"</p>
<p>However, the hacker somehow obtains a certificate issued to <code>super-bank.com</code>, then sets up a fake server and tells users, "Trust me, I am the bank!"</p>
<p>Thus, he steals users' passwords again...</p>
<p>So, where does the problem lie? It lies in the certificate. The role of the certificate is: only the holder of the certificate is the real <code>super-bank.com</code>.</p>
<p>In other words, it proves "I am who I claim to be."</p>
<p>A common misconception is that only certificate authorities can issue certificates. In fact, anyone can issue certificates. I can create a Root certificate myself and issue certificates to any domain. There can be many certificates, but clients must only trust those that are <em>valid</em>.</p>
<p>So, which certificates are trustworthy? The ones issued by authoritative certificate authorities (CAs).</p>
<p>The complexity of this topic arises because these three participants—<strong>client</strong>, <strong>CA</strong>, and <strong>website</strong>—are interconnected yet each has its own responsibilities. During discussions, it’s easy to confuse their roles, leading to misunderstandings. Therefore, this series of articles will discuss the issues and actions of these participants separately, hoping to clarify the matter.</p>
<p>Their relationship is:</p>
<ul>
<li>The client trusts the CA.</li>
<li>The CA issues certificates to websites.</li>
<li>When a client visits a website, the website presents its certificate. Since the client trusts the CA, it also trusts the certificate issued by the CA.</li>
</ul>
<p>It's similar to dining at a restaurant: how do you know it's not a scam? You check if the restaurant has a business license issued by the regulatory authority. If the restaurant has a license issued by the authority, you trust the restaurant.</p>
<p>Now, the problem seems simple, but if we think about ways to break this chain of trust, we realize that solving this "trust" issue is not straightforward.</p>
<p>For example, can I apply to a CA for a certificate issued to <code>super-bank.com</code>? Why wouldn’t the CA issue this certificate to me?&nbsp;</p>
<h2>Certificate Authorities</h2>
<p>Certificate Authorities (CAs) have three main responsibilities:</p>
<ol>
<li>To Websites: When someone applies for a certificate, I must verify the applicant's identity. If they are not super-bank.com, I cannot issue the certificate to them.</li>
<li>To Themselves: Protect the private key of their root certificate.</li>
<li>To Clients: Be trusted by clients.</li>
</ol>
<h2>Verifying Website Identity</h2>
<p>We cannot issue certificates to the wrong person, as this would allow the certificate holder to impersonate others. Therefore, for all applicants, we must ensure they indeed have control over the domain before issuing a certificate. This means verifying the applicant's identity.</p>
<p>The industry standard for this is called the <a href="https://letsencrypt.org/docs/challenge-types/" target="_blank" rel="noopener">ACME Challenge</a>. The basic principle is: to prove to me (the CA) that you are <code>super-bank.com</code>, you need to make the URL <code>super-bank.com/.well-known/acme-challenge/foo</code> return the text <code>bar</code>. This demonstrates your control over the domain, and I will issue the certificate to you. (There are other methods supported as well, such as DNS TXT records.)</p>
<h2>Safeguarding the Private Key</h2>
<p>CAs must protect their private key because if it is leaked, they can no longer fulfill their role. The holder of the leaked key can issue certificates at will. If a private key is compromised, it would be catastrophic for the CA. All certificates issued by the CA would need to be revoked, websites would need to redeploy new certificates, and if clients do not promptly revoke the compromised CA, they could be at risk of accessing fraudulent servers, leading to potential account theft and fraud.</p>
<p>Given the numerous websites that need certificates (including reissuance upon expiration), it would be highly inefficient for the CA to access the private key from a secure vault for each certificate issuance.</p>
<h2>Use of Intermediate Certificates</h2>
<p>To address this, the CA Root certificate usually does not directly issue certificates to websites. Instead, it issues an intermediate certificate, which is then used to issue certificates to websites.</p>
<p>This is where <a href="https://en.wikipedia.org/wiki/X.509" target="_blank" rel="noopener">x509</a> comes in: if a client trusts the CA, then the client should trust certificates issued by the CA, and should trust certificates issued by those certificates, and so on. However, does this mean that a certificate issued to me by the CA can be used by me to issue other certificates? That would mean anyone could use the CA's credibility to issue certificates.</p>
<h2>Certificate Signing</h2>
<p>Of course not. CA-issued certificates include an <code>X509v3 Basic Constraints: critical</code> field, with the value <code>CA:FALSE</code>. By downloading the certificate for this blog and using the OpenSSL tool to parse it, you can see that in this certificate chain, both the Root and Intermediate certificates have CA:TRUE, while my certificate has CA:FALSE.</p>
<p><img src="https://www.pixelstech.net/article/images/site-cert-ca-false.png" width="750"></p>
<p>The CA:FALSE value on certificates issued to entities means that even if they issue a certificate, it won't be trusted. Can we modify this field to issue certificates?&nbsp;</p>
<p>The answer is yes, but if I modify my own certificate, the modified certificate will no longer be trusted.</p>
<h2>Trust Chain</h2>
<p>Client –trust–&gt; Root CA –trust–&gt; Intermediate CA –NOT trust –&gt; kawabangga.com — NOT trust –&gt; super-bank.com signed by kawabangga.com</p>
<p>Why is the modified certificate not trusted?&nbsp;</p>
<h2>Certificate Issuance Process</h2>
<p>The certificate issuance process is straightforward, utilizing the characteristics of private and public keys:</p>
<ul>
<li>Data signed with a private key can be verified with the public key.</li>
<li>Data encrypted with the public key can be decrypted with the private key.</li>
</ul>
<p>Simplified, it means data encrypted with one key can only be decrypted with the other key. One key is released as the public key, and the other is kept private.</p>
<p>The process of signing essentially involves encryption:</p>
<ol>
<li>Hash the entire certificate, then encrypt the hash value with the private key.</li>
<li>The verifier hashes the entire certificate, obtains the hash value, then uses the public key to decrypt the encrypted hash value. If the values match, it confirms the private key holder performed the encryption.</li>
</ol>
<h2>Client Verification Process</h2>
<p>The client verifies the certificate by comparing the hash value decrypted with the public key to the hash value it calculates.</p>
<p>This ensures:</p>
<ul>
<li>The signer cannot deny their actions, as only the private key holder can sign.</li>
<li>The signed content cannot be altered; any modification will cause the verification to fail.</li>
</ul>
<p>Thus, after the CA signs the certificate, no part of the certificate can be modified, including CA:FALSE and the validity period. If expired, the certificate must be re-signed. Simply changing the date to extend its use is not possible.</p>
<p>Many people mistakenly believe that certificate issuance involves the CA signing with its certificate. In reality, the CA only uses its private key to append an encrypted hash value to the original certificate.</p>
<h2>Certificate Authorities Must Be Trusted by Clients</h2>
<p>The foundation of everything is that clients must trust the CA. The trust in all other certificates stems from the trust in the CA Root certificate.</p>
<p>Building trust is a long process.</p>
<p>So how does a client trust a CA? The answer is that clients store the CA locally. The specific location depends on the client: for example, on Linux, it’s in <code>/etc/ssl/certs</code>, on Mac, it can be viewed using Keychain. Chrome’s trusted certificates are <a href="https://chromium.googlesource.com/chromium/src/+/main/net/data/ssl/chrome_root_store/root_store.md" target="_blank" rel="noopener">here</a>, Mozilla’s are <a href="https://wiki.mozilla.org/CA/Included_Certificates" target="_blank" rel="noopener">here</a>.</p>
<p>Given the vast number of clients in the world, adding a new CA seems difficult!</p>
<p>Actually, it’s not. a group of ambitious people created an organization called <a href="https://letsencrypt.org/" target="_blank" rel="noopener"><strong>Let’s Encrypt</strong></a> in 2013: "We want to issue free certificates to all websites needing TLS/SSL! In a friendly manner! Because we want a safer and more private internet."</p>
<p>Where does trust come from for a new CA? In fact, this CA doesn’t need to spend many years slowly gaining the trust of all clients. As mentioned earlier, it only needs a single Root CA’s trust. If a client trusts an old CA, and the old CA signs the new CA, the client will trust the new CA. In this case, the old CA is DST Root CA X3. Following this logic, clients trust DST Root CA X3, and thus trust certificates signed by DST Root CA X3 for Let’s Encrypt, and also trust intermediate certificates signed by Let’s Encrypt, but not further down. Remember CA:FALSE?</p>
<p>With so many certificates, how does a client verify them? It sounds troublesome, and it’s unclear who to trust.&nbsp;</p>
<p>Let's explore the role of clients next:</p>
<ul>
<li><a href="https://www.pixelstech.net/article/1722050582-All-I-Know-About-Certificates----Clients">All I Know About Certificates -- Clients</a></li>
<li><a href="https://www.pixelstech.net/article/1722050937-All-I-Know-About-Certificates----Websites">All I Know About Certificates -- Websites</a></li>
</ul>
<p>Reference:&nbsp;<a href="https://www.kawabangga.com/posts/5330" target="_blank" rel="noopener">https://www.kawabangga.com/posts/5330</a></p>		 </article>
     </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lewis Lapham has died (107 pts)]]></title>
            <link>https://www.nytimes.com/2024/07/24/business/media/lewis-h-lapham-dead.html</link>
            <guid>41106041</guid>
            <pubDate>Tue, 30 Jul 2024 04:26:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2024/07/24/business/media/lewis-h-lapham-dead.html">https://www.nytimes.com/2024/07/24/business/media/lewis-h-lapham-dead.html</a>, See on <a href="https://news.ycombinator.com/item?id=41106041">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2024/07/24/business/media/lewis-h-lapham-dead.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[A Visual Guide to LLM Quantization (154 pts)]]></title>
            <link>https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization</link>
            <guid>41105881</guid>
            <pubDate>Tue, 30 Jul 2024 03:42:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization">https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization</a>, See on <a href="https://news.ycombinator.com/item?id=41105881">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p>As their name suggests, Large Language Models (LLMs) are often too large to run on consumer hardware. These models may exceed billions of parameters and generally need GPUs with large amounts of VRAM to speed up inference.</p><p><span>As such, more and more research has been focused on making these models smaller through improved training, adapters, etc. One major technique in this field is called </span><em>quantization</em><span>.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9d17077-d9af-4b37-9b9b-57ef9aaa1ca9_680x486.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9d17077-d9af-4b37-9b9b-57ef9aaa1ca9_680x486.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9d17077-d9af-4b37-9b9b-57ef9aaa1ca9_680x486.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9d17077-d9af-4b37-9b9b-57ef9aaa1ca9_680x486.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9d17077-d9af-4b37-9b9b-57ef9aaa1ca9_680x486.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9d17077-d9af-4b37-9b9b-57ef9aaa1ca9_680x486.png" width="680" height="486" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e9d17077-d9af-4b37-9b9b-57ef9aaa1ca9_680x486.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:486,&quot;width&quot;:680,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:25448,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9d17077-d9af-4b37-9b9b-57ef9aaa1ca9_680x486.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9d17077-d9af-4b37-9b9b-57ef9aaa1ca9_680x486.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9d17077-d9af-4b37-9b9b-57ef9aaa1ca9_680x486.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9d17077-d9af-4b37-9b9b-57ef9aaa1ca9_680x486.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>In this post, I will introduce the field of quantization in the context of language modeling and explore concepts one by one to develop an intuition about the field. We will explore various methodologies, use cases, and the principles behind quantization. </p><p>As a visual guide, expect many visualizations to develop an intuition about quantization!</p><h5><strong>Table of Contents</strong></h5><ul><li><p><em><strong>Part 1: The “Problem” with Large Language Models</strong></em></p><ul><li><p><a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization#%C2%A7how-to-represent-numerical-values" rel="">How to Represent Numerical Values</a></p></li><li><p><a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization#%C2%A7memory-constraints" rel="">Memory Constraints</a></p></li></ul></li><li><p><em><strong>Part 2: Introduction to Quantization</strong></em></p><ul><li><p><a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization#%C2%A7common-data-types" rel="">Common Data Types</a></p><ul><li><p><a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization#%C2%A7fp16" rel="">FP16</a></p></li><li><p><a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization#%C2%A7bf16" rel="">BF16</a></p></li><li><p><a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization#%C2%A7int8" rel="">INT8</a></p></li></ul></li><li><p><a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization#%C2%A7symmetric-quantization" rel="">Symmetric Quantization</a></p></li><li><p><a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization#%C2%A7asymmetric-quantization" rel="">Asymmetric Quantization</a></p></li><li><p><a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization#%C2%A7range-mapping-and-clipping" rel="">Range Mapping and Clipping</a></p></li><li><p><a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization#%C2%A7calibration" rel="">Calibration</a></p><ul><li><p><a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization#%C2%A7weights-and-biases" rel="">Weights (and Biases)</a></p></li><li><p><a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization#%C2%A7activations" rel="">Activations</a></p></li></ul></li></ul></li><li><p><em><strong>Part 3: Post-Training Quantization (PTQ)</strong></em></p><ul><li><p><a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization#%C2%A7dynamic-quantization" rel="">Dynamic Quantization</a></p></li><li><p><a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization#%C2%A7static-quantization" rel="">Static Quantization</a></p></li><li><p><a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization#%C2%A7the-realm-of-bit-quantization" rel="">The Realm of 4-bit Quantization</a></p><ul><li><p><a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization#%C2%A7gptq" rel="">GPTQ</a></p></li><li><p><a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization#%C2%A7gguf" rel="">GGUF</a></p></li></ul></li></ul></li><li><p><em><strong>Part 4: Quantization-Aware Training (QAT)</strong></em></p><ul><li><p><a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization#%C2%A7the-era-of-bit-llms-bitnet" rel="">The Era of 1-bit LLMs: BitNet</a></p><ul><li><p><a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization#%C2%A7weight-quantization" rel="">Weight Quantization</a></p></li><li><p><a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization#%C2%A7activation-quantization" rel="">Activation Quantization</a></p></li><li><p><a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization#%C2%A7dequantization" rel="">Dequantization</a></p></li></ul></li><li><p><a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization#%C2%A7all-large-language-models-are-in-bits" rel="">All Large Language Models are in 1.58 Bits</a></p><ul><li><p><a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization#%C2%A7the-power-of" rel="">The Power of 0</a></p></li><li><p><a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization#%C2%A7quantization" rel="">Quantization</a></p></li></ul></li></ul></li></ul><p><span>LLMs get their name due to the number of parameters they contain. Nowadays, these models typically have billions of parameters (mostly </span><em>weights</em><span>) which can be quite expensive to store. </span></p><p>During inference, activations are created as a product of the input and the weights, which similarly can be quite large.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb99fe2ba-d4f4-4046-850c-e3f469add123_1368x708.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb99fe2ba-d4f4-4046-850c-e3f469add123_1368x708.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb99fe2ba-d4f4-4046-850c-e3f469add123_1368x708.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb99fe2ba-d4f4-4046-850c-e3f469add123_1368x708.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb99fe2ba-d4f4-4046-850c-e3f469add123_1368x708.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb99fe2ba-d4f4-4046-850c-e3f469add123_1368x708.png" width="1368" height="708" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b99fe2ba-d4f4-4046-850c-e3f469add123_1368x708.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:708,&quot;width&quot;:1368,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:82794,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb99fe2ba-d4f4-4046-850c-e3f469add123_1368x708.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb99fe2ba-d4f4-4046-850c-e3f469add123_1368x708.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb99fe2ba-d4f4-4046-850c-e3f469add123_1368x708.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb99fe2ba-d4f4-4046-850c-e3f469add123_1368x708.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>As a result, we would like to represent billions of values as efficiently as possible, minimizing the amount of space we need to store a given value.</p><p>Let’s start from the beginning and explore how numerical values are represented in the first place before optimizing them. </p><p><span>A given value is often represented as a floating point number (or </span><em>floats</em><span> in computer science): a positive or negative number with a decimal point. </span></p><p><span>These values are represented by “</span><em>bits</em><span>”, or binary digits. The </span><a href="https://en.wikipedia.org/wiki/IEEE_754" rel="">IEEE-754</a><span> standard describes how bits can represent one of three functions to represent the value: the </span><em>sign</em><span>, </span><em>exponent</em><span>, or </span><em>fraction (</em><span>or mantissa</span><em>)</em><span>.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8362c0e-0a77-4eda-80a8-8e5e1df4433f_1252x308.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8362c0e-0a77-4eda-80a8-8e5e1df4433f_1252x308.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8362c0e-0a77-4eda-80a8-8e5e1df4433f_1252x308.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8362c0e-0a77-4eda-80a8-8e5e1df4433f_1252x308.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8362c0e-0a77-4eda-80a8-8e5e1df4433f_1252x308.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8362c0e-0a77-4eda-80a8-8e5e1df4433f_1252x308.png" width="1252" height="308" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c8362c0e-0a77-4eda-80a8-8e5e1df4433f_1252x308.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:308,&quot;width&quot;:1252,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:27154,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8362c0e-0a77-4eda-80a8-8e5e1df4433f_1252x308.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8362c0e-0a77-4eda-80a8-8e5e1df4433f_1252x308.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8362c0e-0a77-4eda-80a8-8e5e1df4433f_1252x308.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8362c0e-0a77-4eda-80a8-8e5e1df4433f_1252x308.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Together, these three aspects can be used to calculate a value given a certain set of bit values:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4783fd02-a138-40c7-82c7-79dd05a179e4_1472x772.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4783fd02-a138-40c7-82c7-79dd05a179e4_1472x772.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4783fd02-a138-40c7-82c7-79dd05a179e4_1472x772.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4783fd02-a138-40c7-82c7-79dd05a179e4_1472x772.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4783fd02-a138-40c7-82c7-79dd05a179e4_1472x772.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4783fd02-a138-40c7-82c7-79dd05a179e4_1472x772.png" width="1456" height="764" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4783fd02-a138-40c7-82c7-79dd05a179e4_1472x772.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:764,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:68090,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4783fd02-a138-40c7-82c7-79dd05a179e4_1472x772.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4783fd02-a138-40c7-82c7-79dd05a179e4_1472x772.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4783fd02-a138-40c7-82c7-79dd05a179e4_1472x772.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4783fd02-a138-40c7-82c7-79dd05a179e4_1472x772.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The more bits we use to represent a value, the more precise it generally is:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1eafac2a-d027-4d66-95de-7030e0392b39_1796x940.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1eafac2a-d027-4d66-95de-7030e0392b39_1796x940.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1eafac2a-d027-4d66-95de-7030e0392b39_1796x940.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1eafac2a-d027-4d66-95de-7030e0392b39_1796x940.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1eafac2a-d027-4d66-95de-7030e0392b39_1796x940.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1eafac2a-d027-4d66-95de-7030e0392b39_1796x940.png" width="1456" height="762" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1eafac2a-d027-4d66-95de-7030e0392b39_1796x940.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:762,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:94299,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1eafac2a-d027-4d66-95de-7030e0392b39_1796x940.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1eafac2a-d027-4d66-95de-7030e0392b39_1796x940.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1eafac2a-d027-4d66-95de-7030e0392b39_1796x940.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1eafac2a-d027-4d66-95de-7030e0392b39_1796x940.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The more bits we have available, the larger the range of values that can be represented.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff306b7f1-dd3c-4001-91d3-bd61f22c5782_1128x452.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff306b7f1-dd3c-4001-91d3-bd61f22c5782_1128x452.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff306b7f1-dd3c-4001-91d3-bd61f22c5782_1128x452.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff306b7f1-dd3c-4001-91d3-bd61f22c5782_1128x452.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff306b7f1-dd3c-4001-91d3-bd61f22c5782_1128x452.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff306b7f1-dd3c-4001-91d3-bd61f22c5782_1128x452.png" width="1128" height="452" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f306b7f1-dd3c-4001-91d3-bd61f22c5782_1128x452.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:452,&quot;width&quot;:1128,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:35878,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff306b7f1-dd3c-4001-91d3-bd61f22c5782_1128x452.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff306b7f1-dd3c-4001-91d3-bd61f22c5782_1128x452.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff306b7f1-dd3c-4001-91d3-bd61f22c5782_1128x452.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff306b7f1-dd3c-4001-91d3-bd61f22c5782_1128x452.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>The interval of representable numbers a given representation can take is called the </span><em>dynamic range</em><span> whereas the distance between two neighboring values is called </span><em>precision</em><span>.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dbb8398-9f3f-4d9a-b63f-591cb37bdbdd_1144x856.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dbb8398-9f3f-4d9a-b63f-591cb37bdbdd_1144x856.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dbb8398-9f3f-4d9a-b63f-591cb37bdbdd_1144x856.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dbb8398-9f3f-4d9a-b63f-591cb37bdbdd_1144x856.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dbb8398-9f3f-4d9a-b63f-591cb37bdbdd_1144x856.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dbb8398-9f3f-4d9a-b63f-591cb37bdbdd_1144x856.png" width="1144" height="856" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7dbb8398-9f3f-4d9a-b63f-591cb37bdbdd_1144x856.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:856,&quot;width&quot;:1144,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:61500,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dbb8398-9f3f-4d9a-b63f-591cb37bdbdd_1144x856.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dbb8398-9f3f-4d9a-b63f-591cb37bdbdd_1144x856.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dbb8398-9f3f-4d9a-b63f-591cb37bdbdd_1144x856.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dbb8398-9f3f-4d9a-b63f-591cb37bdbdd_1144x856.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>A nifty feature of these bits is that we can calculate how much memory your device needs to store a given value. Since there are 8 bits in a byte of memory, we can create a basic formula for most forms of floating point representation.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe146740d-72e9-44dc-99e1-f7bc42737cec_1128x144.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe146740d-72e9-44dc-99e1-f7bc42737cec_1128x144.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe146740d-72e9-44dc-99e1-f7bc42737cec_1128x144.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe146740d-72e9-44dc-99e1-f7bc42737cec_1128x144.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe146740d-72e9-44dc-99e1-f7bc42737cec_1128x144.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe146740d-72e9-44dc-99e1-f7bc42737cec_1128x144.png" width="1128" height="144" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e146740d-72e9-44dc-99e1-f7bc42737cec_1128x144.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:144,&quot;width&quot;:1128,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:9756,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe146740d-72e9-44dc-99e1-f7bc42737cec_1128x144.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe146740d-72e9-44dc-99e1-f7bc42737cec_1128x144.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe146740d-72e9-44dc-99e1-f7bc42737cec_1128x144.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe146740d-72e9-44dc-99e1-f7bc42737cec_1128x144.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><strong>NOTE</strong><span>: In practice, more things relate to the amount of (V)RAM you need during inference, like the context size and architecture.</span></figcaption></figure></div><p><span>Now let’s assume that we have a model with 70 billion parameters. Most models are natively represented with float 32-bit (often called </span><em>full-precision</em><span>), which would require </span><strong>280GB</strong><span> of memory just to load the model.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c28e9b0-c002-4a49-9441-af24f261df40_1128x548.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c28e9b0-c002-4a49-9441-af24f261df40_1128x548.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c28e9b0-c002-4a49-9441-af24f261df40_1128x548.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c28e9b0-c002-4a49-9441-af24f261df40_1128x548.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c28e9b0-c002-4a49-9441-af24f261df40_1128x548.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c28e9b0-c002-4a49-9441-af24f261df40_1128x548.png" width="1128" height="548" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9c28e9b0-c002-4a49-9441-af24f261df40_1128x548.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:548,&quot;width&quot;:1128,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:32296,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c28e9b0-c002-4a49-9441-af24f261df40_1128x548.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c28e9b0-c002-4a49-9441-af24f261df40_1128x548.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c28e9b0-c002-4a49-9441-af24f261df40_1128x548.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c28e9b0-c002-4a49-9441-af24f261df40_1128x548.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>As such, it is very compelling to minimize the number of bits to represent the parameters of your model (as well as during training!). However, as the precision decreases the accuracy of the models generally does as well. </p><p><span>We want to reduce the number of bits representing values while maintaining accuracy… This is where </span><em>quantization</em><span> comes in!</span></p><p>Quantization aims to reduce the precision of a model’s parameter from higher bit-widths (like 32-bit floating point) to lower bit-widths (like 8-bit integers).</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82ac8f88-0cf5-4244-ba9f-cbffdb283947_1008x496.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82ac8f88-0cf5-4244-ba9f-cbffdb283947_1008x496.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82ac8f88-0cf5-4244-ba9f-cbffdb283947_1008x496.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82ac8f88-0cf5-4244-ba9f-cbffdb283947_1008x496.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82ac8f88-0cf5-4244-ba9f-cbffdb283947_1008x496.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82ac8f88-0cf5-4244-ba9f-cbffdb283947_1008x496.png" width="1008" height="496" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/82ac8f88-0cf5-4244-ba9f-cbffdb283947_1008x496.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:496,&quot;width&quot;:1008,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:39183,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82ac8f88-0cf5-4244-ba9f-cbffdb283947_1008x496.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82ac8f88-0cf5-4244-ba9f-cbffdb283947_1008x496.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82ac8f88-0cf5-4244-ba9f-cbffdb283947_1008x496.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82ac8f88-0cf5-4244-ba9f-cbffdb283947_1008x496.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>There is often some loss of precision (granularity) when reducing the number of bits to represent the original parameters. </p><p>To illustrate this effect, we can take any image and use only 8 colors to represent it:</p><p>Notice how the zoomed-in part seems more “grainy” than the original since we can use fewer colors to represent it.</p><p>The main goal of quantization is to reduce the number of bits (colors) needed to represent the original parameters while preserving the precision of the original parameters as best as possible. </p><p><span>First, let’s look at common data types and the impact of using them rather than 32-bit (called </span><em>full-precision</em><span> or </span><em>FP32</em><span>) representations.</span></p><p><span>Let’s look at an example of going from 32-bit to 16-bit (called </span><em>half precision </em><span>or </span><em>FP16</em><span>) floating point:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4ac888a-02b9-4153-915a-e103a12c33a4_1460x892.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4ac888a-02b9-4153-915a-e103a12c33a4_1460x892.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4ac888a-02b9-4153-915a-e103a12c33a4_1460x892.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4ac888a-02b9-4153-915a-e103a12c33a4_1460x892.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4ac888a-02b9-4153-915a-e103a12c33a4_1460x892.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4ac888a-02b9-4153-915a-e103a12c33a4_1460x892.png" width="1456" height="890" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f4ac888a-02b9-4153-915a-e103a12c33a4_1460x892.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:890,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:64559,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4ac888a-02b9-4153-915a-e103a12c33a4_1460x892.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4ac888a-02b9-4153-915a-e103a12c33a4_1460x892.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4ac888a-02b9-4153-915a-e103a12c33a4_1460x892.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4ac888a-02b9-4153-915a-e103a12c33a4_1460x892.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Notice how the range of values FP16 can take is quite a bit smaller than FP32. </p><p><span>To get a similar range of values as the original FP32, </span><em>bfloat 16</em><span> was introduced as a type of “truncated FP32”:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F172c93aa-58ae-4d11-8cb7-2917c265cb68_1460x936.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F172c93aa-58ae-4d11-8cb7-2917c265cb68_1460x936.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F172c93aa-58ae-4d11-8cb7-2917c265cb68_1460x936.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F172c93aa-58ae-4d11-8cb7-2917c265cb68_1460x936.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F172c93aa-58ae-4d11-8cb7-2917c265cb68_1460x936.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F172c93aa-58ae-4d11-8cb7-2917c265cb68_1460x936.png" width="1456" height="933" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/172c93aa-58ae-4d11-8cb7-2917c265cb68_1460x936.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:933,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:62352,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F172c93aa-58ae-4d11-8cb7-2917c265cb68_1460x936.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F172c93aa-58ae-4d11-8cb7-2917c265cb68_1460x936.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F172c93aa-58ae-4d11-8cb7-2917c265cb68_1460x936.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F172c93aa-58ae-4d11-8cb7-2917c265cb68_1460x936.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>BF16 uses the same amount of bits as FP16 but can take a wider range of values and is often used in deep learning applications. </p><p><span>When we reduce the number of bits even further, we approach the realm of </span><em>integer-based representations</em><span> rather than floating-point representations. To illustrate, going FP32 to INT8, which has only 8 bits, results in a fourth of the original number of bits:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa37a58d-1f5a-433c-b235-5b073596bbca_1460x848.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa37a58d-1f5a-433c-b235-5b073596bbca_1460x848.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa37a58d-1f5a-433c-b235-5b073596bbca_1460x848.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa37a58d-1f5a-433c-b235-5b073596bbca_1460x848.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa37a58d-1f5a-433c-b235-5b073596bbca_1460x848.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa37a58d-1f5a-433c-b235-5b073596bbca_1460x848.png" width="1456" height="846" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fa37a58d-1f5a-433c-b235-5b073596bbca_1460x848.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:846,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:59925,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa37a58d-1f5a-433c-b235-5b073596bbca_1460x848.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa37a58d-1f5a-433c-b235-5b073596bbca_1460x848.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa37a58d-1f5a-433c-b235-5b073596bbca_1460x848.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa37a58d-1f5a-433c-b235-5b073596bbca_1460x848.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Depending on the hardware, integer-based calculations might be faster than floating-point calculations but this isn’t always the case. However, computations are generally faster when using fewer bits. </figcaption></figure></div><p>For each reduction in bits, a mapping is performed to “squeeze” the initial FP32 representations into lower bits. </p><p>In practice, we do not need to map the entire FP32 range [-3.4e38, 3.4e38] into INT8. We merely need to find a way to map the range of our data (the model’s parameters) into IN8. </p><p><span>Common squeezing/mapping methods are </span><em>symmetric</em><span> and </span><em>asymmetric</em><span> quantization and are forms of </span><em>linear mapping</em><span>. </span></p><p>Let’s explore these methods to quantize from FP32 to INT8.</p><p>In symmetric quantization, the range of the original floating-point values is mapped to a symmetric range around zero in the quantized space. In the previous examples, notice how the ranges before and after quantization remain centered around zero.</p><p>This means that the quantized value for zero in the floating-point space is exactly zero in the quantized space.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F730bbb8a-3a44-47f6-aefe-f652b117ae22_1124x600.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F730bbb8a-3a44-47f6-aefe-f652b117ae22_1124x600.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F730bbb8a-3a44-47f6-aefe-f652b117ae22_1124x600.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F730bbb8a-3a44-47f6-aefe-f652b117ae22_1124x600.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F730bbb8a-3a44-47f6-aefe-f652b117ae22_1124x600.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F730bbb8a-3a44-47f6-aefe-f652b117ae22_1124x600.png" width="1124" height="600" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/730bbb8a-3a44-47f6-aefe-f652b117ae22_1124x600.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:600,&quot;width&quot;:1124,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:20896,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F730bbb8a-3a44-47f6-aefe-f652b117ae22_1124x600.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F730bbb8a-3a44-47f6-aefe-f652b117ae22_1124x600.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F730bbb8a-3a44-47f6-aefe-f652b117ae22_1124x600.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F730bbb8a-3a44-47f6-aefe-f652b117ae22_1124x600.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>A nice example of a form of symmetric quantization is called absolute maximum (</span><em>absmax</em><span>) quantization. </span></p><p><span>Given a list of values, we take the </span><em>highest </em><span>absolute value (</span><strong>α</strong><span>) as the range to perform the linear mapping.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F782beaa8-340f-45b8-ba7f-20491f66867a_1172x848.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F782beaa8-340f-45b8-ba7f-20491f66867a_1172x848.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F782beaa8-340f-45b8-ba7f-20491f66867a_1172x848.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F782beaa8-340f-45b8-ba7f-20491f66867a_1172x848.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F782beaa8-340f-45b8-ba7f-20491f66867a_1172x848.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F782beaa8-340f-45b8-ba7f-20491f66867a_1172x848.png" width="1172" height="848" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/782beaa8-340f-45b8-ba7f-20491f66867a_1172x848.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:848,&quot;width&quot;:1172,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:57351,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F782beaa8-340f-45b8-ba7f-20491f66867a_1172x848.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F782beaa8-340f-45b8-ba7f-20491f66867a_1172x848.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F782beaa8-340f-45b8-ba7f-20491f66867a_1172x848.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F782beaa8-340f-45b8-ba7f-20491f66867a_1172x848.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Note the [-127, 127] range of values represents the restricted range. The unrestricted range is [-128, 127] and depends on the quantization method.</figcaption></figure></div><p>Since it is a linear mapping centered around zero, the formula is straightforward. </p><p><span>We first calculate a scale factor (</span><em><strong>s</strong></em><span>) using:</span></p><ul><li><p><em><strong>b</strong></em><span> is the number of bytes that we want to quantize to (8), </span></p></li><li><p><strong>α</strong><em><strong> </strong></em><span>is the </span><em>highest </em><span>absolute value, </span></p></li></ul><p><span>Then, we use the </span><em><strong>s</strong></em><span> to quantize the input </span><em><strong>x</strong></em><span>:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cc76e35-13bf-4d6f-94bf-dbe4725c084f_1644x486.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cc76e35-13bf-4d6f-94bf-dbe4725c084f_1644x486.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cc76e35-13bf-4d6f-94bf-dbe4725c084f_1644x486.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cc76e35-13bf-4d6f-94bf-dbe4725c084f_1644x486.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cc76e35-13bf-4d6f-94bf-dbe4725c084f_1644x486.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cc76e35-13bf-4d6f-94bf-dbe4725c084f_1644x486.png" width="1456" height="430" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7cc76e35-13bf-4d6f-94bf-dbe4725c084f_1644x486.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:430,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:28246,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cc76e35-13bf-4d6f-94bf-dbe4725c084f_1644x486.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cc76e35-13bf-4d6f-94bf-dbe4725c084f_1644x486.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cc76e35-13bf-4d6f-94bf-dbe4725c084f_1644x486.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cc76e35-13bf-4d6f-94bf-dbe4725c084f_1644x486.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Filling in the values would then give us the following:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fd92531-447c-45de-af37-f33ffc446b0b_1644x486.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fd92531-447c-45de-af37-f33ffc446b0b_1644x486.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fd92531-447c-45de-af37-f33ffc446b0b_1644x486.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fd92531-447c-45de-af37-f33ffc446b0b_1644x486.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fd92531-447c-45de-af37-f33ffc446b0b_1644x486.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fd92531-447c-45de-af37-f33ffc446b0b_1644x486.png" width="1456" height="430" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3fd92531-447c-45de-af37-f33ffc446b0b_1644x486.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:430,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:32393,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fd92531-447c-45de-af37-f33ffc446b0b_1644x486.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fd92531-447c-45de-af37-f33ffc446b0b_1644x486.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fd92531-447c-45de-af37-f33ffc446b0b_1644x486.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fd92531-447c-45de-af37-f33ffc446b0b_1644x486.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>To retrieve the original FP32 values, we can use the previously calculated </span><em>scaling factor</em><span> (</span><em><strong>s</strong></em><span>) to </span><em>dequantize</em><span> the quantized values.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe708f283-3c74-4344-ae76-e96412098c0b_1644x246.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe708f283-3c74-4344-ae76-e96412098c0b_1644x246.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe708f283-3c74-4344-ae76-e96412098c0b_1644x246.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe708f283-3c74-4344-ae76-e96412098c0b_1644x246.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe708f283-3c74-4344-ae76-e96412098c0b_1644x246.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe708f283-3c74-4344-ae76-e96412098c0b_1644x246.png" width="1456" height="218" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e708f283-3c74-4344-ae76-e96412098c0b_1644x246.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:218,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:12077,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe708f283-3c74-4344-ae76-e96412098c0b_1644x246.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe708f283-3c74-4344-ae76-e96412098c0b_1644x246.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe708f283-3c74-4344-ae76-e96412098c0b_1644x246.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe708f283-3c74-4344-ae76-e96412098c0b_1644x246.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Applying the quantization and then dequantization process to retrieve the original looks as follows:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ea2d627-efc7-4a8a-9cf0-7a7020f1253d_1236x348.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ea2d627-efc7-4a8a-9cf0-7a7020f1253d_1236x348.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ea2d627-efc7-4a8a-9cf0-7a7020f1253d_1236x348.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ea2d627-efc7-4a8a-9cf0-7a7020f1253d_1236x348.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ea2d627-efc7-4a8a-9cf0-7a7020f1253d_1236x348.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ea2d627-efc7-4a8a-9cf0-7a7020f1253d_1236x348.png" width="1236" height="348" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5ea2d627-efc7-4a8a-9cf0-7a7020f1253d_1236x348.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:348,&quot;width&quot;:1236,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:26528,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ea2d627-efc7-4a8a-9cf0-7a7020f1253d_1236x348.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ea2d627-efc7-4a8a-9cf0-7a7020f1253d_1236x348.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ea2d627-efc7-4a8a-9cf0-7a7020f1253d_1236x348.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ea2d627-efc7-4a8a-9cf0-7a7020f1253d_1236x348.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>You can see certain values, such as </span><strong>3.08</strong><span> and </span><strong>3.02</strong><span> being assigned to the INT8, namely </span><strong>36</strong><span>. When you dequantize the values to return to FP32, they lose some precision and are not distinguishable anymore. </span></p><p><span>This is often referred to as the </span><em>quantization error</em><span> which we can calculate by finding the difference between the original and dequantized values.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe173b13b-ed99-4de0-a5e0-4b9114899b3f_1236x372.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe173b13b-ed99-4de0-a5e0-4b9114899b3f_1236x372.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe173b13b-ed99-4de0-a5e0-4b9114899b3f_1236x372.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe173b13b-ed99-4de0-a5e0-4b9114899b3f_1236x372.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe173b13b-ed99-4de0-a5e0-4b9114899b3f_1236x372.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe173b13b-ed99-4de0-a5e0-4b9114899b3f_1236x372.png" width="1236" height="372" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e173b13b-ed99-4de0-a5e0-4b9114899b3f_1236x372.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:372,&quot;width&quot;:1236,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:29159,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe173b13b-ed99-4de0-a5e0-4b9114899b3f_1236x372.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe173b13b-ed99-4de0-a5e0-4b9114899b3f_1236x372.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe173b13b-ed99-4de0-a5e0-4b9114899b3f_1236x372.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe173b13b-ed99-4de0-a5e0-4b9114899b3f_1236x372.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Generally, the lower the number of bits, the more quantization error we tend to have.</p><p><span>Asymmetric quantization, in contrast, is not symmetric around zero. Instead, it maps the minimum (</span><strong>β</strong><span>) and maximum (</span><strong>α</strong><span>) values from the float range to the minimum and maximum values of the quantized range. </span></p><p><span>The method we are going to explore is called </span><em>zero-point quantization</em><span>.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ffa0c54-88bf-45c1-8636-bdb097bb8e6b_1172x848.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ffa0c54-88bf-45c1-8636-bdb097bb8e6b_1172x848.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ffa0c54-88bf-45c1-8636-bdb097bb8e6b_1172x848.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ffa0c54-88bf-45c1-8636-bdb097bb8e6b_1172x848.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ffa0c54-88bf-45c1-8636-bdb097bb8e6b_1172x848.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ffa0c54-88bf-45c1-8636-bdb097bb8e6b_1172x848.png" width="1172" height="848" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8ffa0c54-88bf-45c1-8636-bdb097bb8e6b_1172x848.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:848,&quot;width&quot;:1172,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:57869,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ffa0c54-88bf-45c1-8636-bdb097bb8e6b_1172x848.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ffa0c54-88bf-45c1-8636-bdb097bb8e6b_1172x848.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ffa0c54-88bf-45c1-8636-bdb097bb8e6b_1172x848.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ffa0c54-88bf-45c1-8636-bdb097bb8e6b_1172x848.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Notice how the 0 has shifted positions? That’s why it’s called </span><em>asymmetric quantization</em><span>. The min/max values have different distances to 0 in the range [-7.59, 10.8].</span></p><p><span>Due to its shifted position, we have to calculate the zero-point for the INT8 range to perform the linear mapping. As before, we also have to calculate a </span><em>scale factor </em><span>(</span><em><strong>s</strong></em><span>) but use the difference of INT8’s range instead [-128, 127]</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16cde2f6-aeb5-44d8-b056-846a5f1a0448_1096x508.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16cde2f6-aeb5-44d8-b056-846a5f1a0448_1096x508.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16cde2f6-aeb5-44d8-b056-846a5f1a0448_1096x508.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16cde2f6-aeb5-44d8-b056-846a5f1a0448_1096x508.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16cde2f6-aeb5-44d8-b056-846a5f1a0448_1096x508.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16cde2f6-aeb5-44d8-b056-846a5f1a0448_1096x508.png" width="1096" height="508" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/16cde2f6-aeb5-44d8-b056-846a5f1a0448_1096x508.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:508,&quot;width&quot;:1096,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:31046,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16cde2f6-aeb5-44d8-b056-846a5f1a0448_1096x508.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16cde2f6-aeb5-44d8-b056-846a5f1a0448_1096x508.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16cde2f6-aeb5-44d8-b056-846a5f1a0448_1096x508.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16cde2f6-aeb5-44d8-b056-846a5f1a0448_1096x508.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Notice how this is a bit more involved due to the need to calculate the </span><em>zeropoint </em><span>(</span><em><strong>z</strong></em><span>) in the INT8 range to shift the weights.</span></p><p>As before, let’s fill in the formula:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92ad0583-277f-4168-bbc7-a5503b5e45c4_1096x468.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92ad0583-277f-4168-bbc7-a5503b5e45c4_1096x468.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92ad0583-277f-4168-bbc7-a5503b5e45c4_1096x468.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92ad0583-277f-4168-bbc7-a5503b5e45c4_1096x468.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92ad0583-277f-4168-bbc7-a5503b5e45c4_1096x468.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92ad0583-277f-4168-bbc7-a5503b5e45c4_1096x468.png" width="1096" height="468" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/92ad0583-277f-4168-bbc7-a5503b5e45c4_1096x468.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:468,&quot;width&quot;:1096,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:39990,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92ad0583-277f-4168-bbc7-a5503b5e45c4_1096x468.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92ad0583-277f-4168-bbc7-a5503b5e45c4_1096x468.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92ad0583-277f-4168-bbc7-a5503b5e45c4_1096x468.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92ad0583-277f-4168-bbc7-a5503b5e45c4_1096x468.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>To dequantize the quantized from INT8 back to FP32, we will need to use the previously calculated </span><em>scale factor</em><span> (</span><em><strong>s</strong></em><span>) and </span><em>zeropoint </em><span>(</span><em><strong>z</strong></em><span>).</span></p><p>Other than that, dequantization is straightforward:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0aee7fd2-c070-4710-9d8c-ccf598d5befe_1016x160.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0aee7fd2-c070-4710-9d8c-ccf598d5befe_1016x160.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0aee7fd2-c070-4710-9d8c-ccf598d5befe_1016x160.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0aee7fd2-c070-4710-9d8c-ccf598d5befe_1016x160.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0aee7fd2-c070-4710-9d8c-ccf598d5befe_1016x160.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0aee7fd2-c070-4710-9d8c-ccf598d5befe_1016x160.png" width="1016" height="160" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0aee7fd2-c070-4710-9d8c-ccf598d5befe_1016x160.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:160,&quot;width&quot;:1016,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:8232,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0aee7fd2-c070-4710-9d8c-ccf598d5befe_1016x160.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0aee7fd2-c070-4710-9d8c-ccf598d5befe_1016x160.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0aee7fd2-c070-4710-9d8c-ccf598d5befe_1016x160.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0aee7fd2-c070-4710-9d8c-ccf598d5befe_1016x160.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>When we put symmetric and asymmetric quantization side-by-side, we can quickly see the difference between methods:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01404566-e2ae-4e3f-9101-cafc68d92b40_1172x716.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01404566-e2ae-4e3f-9101-cafc68d92b40_1172x716.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01404566-e2ae-4e3f-9101-cafc68d92b40_1172x716.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01404566-e2ae-4e3f-9101-cafc68d92b40_1172x716.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01404566-e2ae-4e3f-9101-cafc68d92b40_1172x716.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01404566-e2ae-4e3f-9101-cafc68d92b40_1172x716.png" width="1172" height="716" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/01404566-e2ae-4e3f-9101-cafc68d92b40_1172x716.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:716,&quot;width&quot;:1172,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:48407,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01404566-e2ae-4e3f-9101-cafc68d92b40_1172x716.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01404566-e2ae-4e3f-9101-cafc68d92b40_1172x716.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01404566-e2ae-4e3f-9101-cafc68d92b40_1172x716.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01404566-e2ae-4e3f-9101-cafc68d92b40_1172x716.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Note the zero-centered nature of symmetric quantization versus the offset of asymmetric quantization.</p><p><span>In our previous examples, we explored how the range of values in a given vector could be mapped to a lower-bit representation. Although this allows for the full range of vector values to be mapped, it comes with a major downside, namely </span><em>outliers</em><span>.</span></p><p>Imagine that you have a vector with the following values:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce7fd7ab-3c4b-401d-893e-d417db946fd8_1172x184.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce7fd7ab-3c4b-401d-893e-d417db946fd8_1172x184.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce7fd7ab-3c4b-401d-893e-d417db946fd8_1172x184.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce7fd7ab-3c4b-401d-893e-d417db946fd8_1172x184.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce7fd7ab-3c4b-401d-893e-d417db946fd8_1172x184.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce7fd7ab-3c4b-401d-893e-d417db946fd8_1172x184.png" width="1172" height="184" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ce7fd7ab-3c4b-401d-893e-d417db946fd8_1172x184.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:184,&quot;width&quot;:1172,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:7881,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce7fd7ab-3c4b-401d-893e-d417db946fd8_1172x184.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce7fd7ab-3c4b-401d-893e-d417db946fd8_1172x184.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce7fd7ab-3c4b-401d-893e-d417db946fd8_1172x184.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce7fd7ab-3c4b-401d-893e-d417db946fd8_1172x184.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Note how one value is much larger than all others and could be considered an outlier. If we were to map the full range of this vector, all small values would get mapped to the same lower-bit representation and lose their differentiating factor:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72052ddb-1c54-45b3-9800-2c4335cc9581_1120x564.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72052ddb-1c54-45b3-9800-2c4335cc9581_1120x564.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72052ddb-1c54-45b3-9800-2c4335cc9581_1120x564.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72052ddb-1c54-45b3-9800-2c4335cc9581_1120x564.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72052ddb-1c54-45b3-9800-2c4335cc9581_1120x564.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72052ddb-1c54-45b3-9800-2c4335cc9581_1120x564.png" width="1120" height="564" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/72052ddb-1c54-45b3-9800-2c4335cc9581_1120x564.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:564,&quot;width&quot;:1120,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:34731,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72052ddb-1c54-45b3-9800-2c4335cc9581_1120x564.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72052ddb-1c54-45b3-9800-2c4335cc9581_1120x564.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72052ddb-1c54-45b3-9800-2c4335cc9581_1120x564.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72052ddb-1c54-45b3-9800-2c4335cc9581_1120x564.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>This is the absmax method we used earlier. Note that the same behavior happens with asymmetric quantization if we do not apply clipping.</figcaption></figure></div><p><span>Instead, we can choose to </span><em>clip</em><span> certain values. Clipping involves setting a different dynamic range of the original values such that all outliers get the same value.</span></p><p>In the example below, if we were to manually set the dynamic range to [-5, 5] all values outside that will either be mapped to -127 or to 127 regardless of their value:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52511453-ca48-42ca-9818-d1afa6dd7369_1120x408.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52511453-ca48-42ca-9818-d1afa6dd7369_1120x408.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52511453-ca48-42ca-9818-d1afa6dd7369_1120x408.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52511453-ca48-42ca-9818-d1afa6dd7369_1120x408.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52511453-ca48-42ca-9818-d1afa6dd7369_1120x408.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52511453-ca48-42ca-9818-d1afa6dd7369_1120x408.png" width="1120" height="408" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/52511453-ca48-42ca-9818-d1afa6dd7369_1120x408.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:408,&quot;width&quot;:1120,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:36292,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52511453-ca48-42ca-9818-d1afa6dd7369_1120x408.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52511453-ca48-42ca-9818-d1afa6dd7369_1120x408.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52511453-ca48-42ca-9818-d1afa6dd7369_1120x408.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52511453-ca48-42ca-9818-d1afa6dd7369_1120x408.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>The major advantage is that the quantization error of the </span><em>non-outliers</em><span> is reduced significantly. However, the quantization error of </span><em>outliers</em><span> increases.</span></p><p><span>In the example, I showed a naive method of choosing an arbitrary range of [-5, 5]. The process of selecting this range is known as </span><em>calibration</em><span> which aims to find a range that includes as many values as possible while minimizing the quantization error.</span></p><p>Performing this calibration step is not equal for all types of parameters. </p><p><span>We can view the weights and biases of an LLM as </span><em>static</em><span> values since they are known before running the model. For instance, the </span><a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B/tree/main" rel="">~20GB file of Llama 3</a><span> consists mostly of its weight and biases.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7d79e60e-92ea-4c91-bbdb-297c819cd821_1456x440.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7d79e60e-92ea-4c91-bbdb-297c819cd821_1456x440.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7d79e60e-92ea-4c91-bbdb-297c819cd821_1456x440.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7d79e60e-92ea-4c91-bbdb-297c819cd821_1456x440.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7d79e60e-92ea-4c91-bbdb-297c819cd821_1456x440.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7d79e60e-92ea-4c91-bbdb-297c819cd821_1456x440.png" width="1456" height="440" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7d79e60e-92ea-4c91-bbdb-297c819cd821_1456x440.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:440,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:19694,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7d79e60e-92ea-4c91-bbdb-297c819cd821_1456x440.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7d79e60e-92ea-4c91-bbdb-297c819cd821_1456x440.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7d79e60e-92ea-4c91-bbdb-297c819cd821_1456x440.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7d79e60e-92ea-4c91-bbdb-297c819cd821_1456x440.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Since there are significantly fewer biases (millions) than weights (billions), the biases are often kept in higher precision (such as INT16), and the main effort of quantization is put towards the weights.</p><p>For weights, which are static and known, calibration techniques for choosing the range include:</p><ul><li><p><span>Manually chosing a </span><em>percentile</em><span> of the input range</span></p></li><li><p><span>Optimize the </span><em>mean squared error</em><span> (MSE) between the original and quantized weights.</span></p></li><li><p><span>Minimizing </span><em>entropy</em><span> (KL-divergence) between the original and quantized values</span></p></li></ul><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff24238b2-de53-40c8-8869-9a7d83678544_772x312.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff24238b2-de53-40c8-8869-9a7d83678544_772x312.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff24238b2-de53-40c8-8869-9a7d83678544_772x312.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff24238b2-de53-40c8-8869-9a7d83678544_772x312.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff24238b2-de53-40c8-8869-9a7d83678544_772x312.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff24238b2-de53-40c8-8869-9a7d83678544_772x312.png" width="772" height="312" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f24238b2-de53-40c8-8869-9a7d83678544_772x312.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:312,&quot;width&quot;:772,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:27048,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff24238b2-de53-40c8-8869-9a7d83678544_772x312.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff24238b2-de53-40c8-8869-9a7d83678544_772x312.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff24238b2-de53-40c8-8869-9a7d83678544_772x312.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff24238b2-de53-40c8-8869-9a7d83678544_772x312.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Choosing a percentile, for instance, would lead to similar clipping behavior as we have seen before.</p><p><span>The input that is continuously updated throughout the LLM is typically referred to as “</span><em>activations</em><span>”.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6baaee7f-40dd-4f6a-9a8d-bd79a7b2abc7_1456x520.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6baaee7f-40dd-4f6a-9a8d-bd79a7b2abc7_1456x520.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6baaee7f-40dd-4f6a-9a8d-bd79a7b2abc7_1456x520.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6baaee7f-40dd-4f6a-9a8d-bd79a7b2abc7_1456x520.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6baaee7f-40dd-4f6a-9a8d-bd79a7b2abc7_1456x520.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6baaee7f-40dd-4f6a-9a8d-bd79a7b2abc7_1456x520.png" width="1456" height="520" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6baaee7f-40dd-4f6a-9a8d-bd79a7b2abc7_1456x520.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:520,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:26086,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6baaee7f-40dd-4f6a-9a8d-bd79a7b2abc7_1456x520.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6baaee7f-40dd-4f6a-9a8d-bd79a7b2abc7_1456x520.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6baaee7f-40dd-4f6a-9a8d-bd79a7b2abc7_1456x520.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6baaee7f-40dd-4f6a-9a8d-bd79a7b2abc7_1456x520.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Note that these values are called activations since they often go through some activation function, like sigmoid or relu.</figcaption></figure></div><p>Unlike weights, activations vary with each input data fed into the model during inference, making it challenging to quantize them accurately. </p><p>Since these values are updated after each hidden layer, we only know what they will be during inference as the input data passes through the model.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fbb4248-fc4f-4317-b13a-898976010536_1230x672.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fbb4248-fc4f-4317-b13a-898976010536_1230x672.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fbb4248-fc4f-4317-b13a-898976010536_1230x672.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fbb4248-fc4f-4317-b13a-898976010536_1230x672.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fbb4248-fc4f-4317-b13a-898976010536_1230x672.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fbb4248-fc4f-4317-b13a-898976010536_1230x672.png" width="1230" height="672" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6fbb4248-fc4f-4317-b13a-898976010536_1230x672.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:672,&quot;width&quot;:1230,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:83766,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fbb4248-fc4f-4317-b13a-898976010536_1230x672.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fbb4248-fc4f-4317-b13a-898976010536_1230x672.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fbb4248-fc4f-4317-b13a-898976010536_1230x672.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fbb4248-fc4f-4317-b13a-898976010536_1230x672.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Broadly, there are two methods for calibrating the quantization method of the weights and activations:</p><ul><li><p>Post-Training Quantization (PTQ)</p><ul><li><p><span>Quantization </span><em><strong>after</strong></em><span> training</span></p></li></ul></li><li><p>Quantization Aware Training (QAT)</p><ul><li><p><span>Quantization </span><em><strong>during</strong></em><span> training/fine-tuning</span></p></li></ul></li></ul><p><span>One of the most popular quantization techniques is post-training quantization (PTQ). It involves quantizing a model’s parameters (both weights and activations) </span><strong>after</strong><span> training the model. </span></p><p><span>Quantization of the </span><em>weights</em><span> is performed using either symmetric or asymmetric quantization. </span></p><p><span>Quantization of the </span><em>activations</em><span>, however, requires inference of the model to get their potential distribution since we do not know their range.</span></p><p>There are two forms of quantization of the activations:</p><ul><li><p><em>Dynamic </em><span>Quantization</span></p></li><li><p><em>Static</em><span> Quantization</span></p></li></ul><p>After data passes a hidden layer, its activations are collected:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fa3761f-0244-48f7-af56-5fb6c1cdd952_1476x756.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fa3761f-0244-48f7-af56-5fb6c1cdd952_1476x756.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fa3761f-0244-48f7-af56-5fb6c1cdd952_1476x756.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fa3761f-0244-48f7-af56-5fb6c1cdd952_1476x756.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fa3761f-0244-48f7-af56-5fb6c1cdd952_1476x756.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fa3761f-0244-48f7-af56-5fb6c1cdd952_1476x756.png" width="1456" height="746" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0fa3761f-0244-48f7-af56-5fb6c1cdd952_1476x756.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:746,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:86202,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fa3761f-0244-48f7-af56-5fb6c1cdd952_1476x756.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fa3761f-0244-48f7-af56-5fb6c1cdd952_1476x756.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fa3761f-0244-48f7-af56-5fb6c1cdd952_1476x756.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fa3761f-0244-48f7-af56-5fb6c1cdd952_1476x756.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>This distribution of activations is then used to calculate the </span><em>zeropoint</em><span> (</span><em><strong>z</strong></em><span>) and </span><em>scale factor</em><span> (</span><strong>s</strong><span>) values needed to quantize the output:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa593d70-c28a-43e3-b32c-5c7e46186408_1476x876.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa593d70-c28a-43e3-b32c-5c7e46186408_1476x876.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa593d70-c28a-43e3-b32c-5c7e46186408_1476x876.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa593d70-c28a-43e3-b32c-5c7e46186408_1476x876.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa593d70-c28a-43e3-b32c-5c7e46186408_1476x876.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa593d70-c28a-43e3-b32c-5c7e46186408_1476x876.png" width="1456" height="864" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fa593d70-c28a-43e3-b32c-5c7e46186408_1476x876.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:864,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:90712,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa593d70-c28a-43e3-b32c-5c7e46186408_1476x876.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa593d70-c28a-43e3-b32c-5c7e46186408_1476x876.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa593d70-c28a-43e3-b32c-5c7e46186408_1476x876.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa593d70-c28a-43e3-b32c-5c7e46186408_1476x876.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>The process is repeated each time data passes through a new layer. Therefore, each layer has its own separate </span><em><strong>z</strong></em><span> and </span><em><strong>s</strong></em><span> values and therefore different quantization schemes.</span></p><p><span>In contrast to dynamic quantization, static quantization does not calculate the </span><em>zeropoint</em><span> (</span><em><strong>z</strong></em><span>) and scale factor (</span><em><strong>s</strong></em><span>) during inference but beforehand.</span></p><p><span>To find those values, a </span><strong>calibration dataset</strong><span> is used and given to the model to collect these potential distributions.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46dd6825-2a1c-459e-88c0-022a01dcebf2_1194x636.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46dd6825-2a1c-459e-88c0-022a01dcebf2_1194x636.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46dd6825-2a1c-459e-88c0-022a01dcebf2_1194x636.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46dd6825-2a1c-459e-88c0-022a01dcebf2_1194x636.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46dd6825-2a1c-459e-88c0-022a01dcebf2_1194x636.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46dd6825-2a1c-459e-88c0-022a01dcebf2_1194x636.png" width="1194" height="636" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/46dd6825-2a1c-459e-88c0-022a01dcebf2_1194x636.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:636,&quot;width&quot;:1194,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:37692,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46dd6825-2a1c-459e-88c0-022a01dcebf2_1194x636.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46dd6825-2a1c-459e-88c0-022a01dcebf2_1194x636.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46dd6825-2a1c-459e-88c0-022a01dcebf2_1194x636.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46dd6825-2a1c-459e-88c0-022a01dcebf2_1194x636.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>After these values have been collected, we can calculate the necessary </span><em><strong>s</strong></em><span> and </span><em><strong>z</strong></em><span> values to perform quantization during inference.</span></p><p><span>When you are performing actual inference, the </span><em><strong>s</strong></em><span> and </span><em><strong>z</strong></em><span> values are not recalculated but are used globally over all activations to quantize them.</span></p><p><span>In general, dynamic quantization tends to be a bit more accurate since it only attempts to calculate the </span><em><strong>s</strong></em><span> and </span><em><strong>z</strong></em><span> values per hidden layer. However, it might increase compute time as these values need to be calculated.</span></p><p><span>In contrast, static quantization is less accurate but is faster as it already knows the </span><em><strong>s</strong></em><span> and </span><em><strong>z</strong></em><span> values used for quantization.</span></p><p>Going below 8-bit quantization has proved to be a difficult task as the quantization error increases with each loss of bit. Fortunately, there are several smart ways to reduce the bits to 6, 4, and even 2-bits (although going lower than 4-bits using these methods is typically not advised). </p><p>We will explore two methods that are commonly shared on HuggingFace:</p><ul><li><p><em>GPTQ</em><span> (full model on GPU)</span></p></li><li><p><em>GGUF</em><span> (potentially offload layers on the CPU)</span></p></li></ul><p><span>GPTQ is arguably one of the most well-known methods used in practice for quantization to 4-bits.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-145531349" href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization#footnote-1-145531349" target="_self" rel="">1</a></span></p><p>It uses asymmetric quantization and does so layer by layer such that each layer is processed independently before continuing to the next:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc260ef95-2dbf-4f7e-80ba-213ce6623fcd_1230x816.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc260ef95-2dbf-4f7e-80ba-213ce6623fcd_1230x816.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc260ef95-2dbf-4f7e-80ba-213ce6623fcd_1230x816.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc260ef95-2dbf-4f7e-80ba-213ce6623fcd_1230x816.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc260ef95-2dbf-4f7e-80ba-213ce6623fcd_1230x816.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc260ef95-2dbf-4f7e-80ba-213ce6623fcd_1230x816.png" width="1230" height="816" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c260ef95-2dbf-4f7e-80ba-213ce6623fcd_1230x816.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:816,&quot;width&quot;:1230,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:71396,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc260ef95-2dbf-4f7e-80ba-213ce6623fcd_1230x816.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc260ef95-2dbf-4f7e-80ba-213ce6623fcd_1230x816.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc260ef95-2dbf-4f7e-80ba-213ce6623fcd_1230x816.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc260ef95-2dbf-4f7e-80ba-213ce6623fcd_1230x816.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>During this layer-wise quantization process, it first converts the layer’s weights into the inverse-</span><strong>Hessian</strong><span>. It is a second-order derivative of the model’s loss function and tells us how sensitive the model's output is to changes in each weight.</span></p><p><span>Simplified, it essentially demonstrates the (</span><em>inverse</em><span>) </span><strong>importance of each weight</strong><span> in a layer.</span></p><p>Weights associated with smaller values in the Hessian matrix are more crucial because small changes in these weights can lead to significant changes in the model's performance.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad39a51b-e47f-44ec-af23-474292719be3_1440x696.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad39a51b-e47f-44ec-af23-474292719be3_1440x696.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad39a51b-e47f-44ec-af23-474292719be3_1440x696.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad39a51b-e47f-44ec-af23-474292719be3_1440x696.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad39a51b-e47f-44ec-af23-474292719be3_1440x696.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad39a51b-e47f-44ec-af23-474292719be3_1440x696.png" width="1440" height="696" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ad39a51b-e47f-44ec-af23-474292719be3_1440x696.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:696,&quot;width&quot;:1440,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:56672,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad39a51b-e47f-44ec-af23-474292719be3_1440x696.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad39a51b-e47f-44ec-af23-474292719be3_1440x696.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad39a51b-e47f-44ec-af23-474292719be3_1440x696.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad39a51b-e47f-44ec-af23-474292719be3_1440x696.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>In the inverse-Hessian, lower values indicate more “important” weights.</figcaption></figure></div><p>Next, we quantize and then dequantize the weight of the first row in our weight matrix:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3eac2072-f4a5-42ca-a251-f934a57d2df5_1146x438.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3eac2072-f4a5-42ca-a251-f934a57d2df5_1146x438.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3eac2072-f4a5-42ca-a251-f934a57d2df5_1146x438.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3eac2072-f4a5-42ca-a251-f934a57d2df5_1146x438.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3eac2072-f4a5-42ca-a251-f934a57d2df5_1146x438.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3eac2072-f4a5-42ca-a251-f934a57d2df5_1146x438.png" width="1146" height="438" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3eac2072-f4a5-42ca-a251-f934a57d2df5_1146x438.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:438,&quot;width&quot;:1146,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:19404,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3eac2072-f4a5-42ca-a251-f934a57d2df5_1146x438.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3eac2072-f4a5-42ca-a251-f934a57d2df5_1146x438.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3eac2072-f4a5-42ca-a251-f934a57d2df5_1146x438.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3eac2072-f4a5-42ca-a251-f934a57d2df5_1146x438.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>This process allows us to calculate the </span><strong>quantization error (</strong><em><strong>q</strong></em><strong>)</strong><span> which we can weigh using the inverse-Hessian (</span><em><strong>h_1</strong><span>)</span></em><span> that we calculated beforehand. </span></p><p>Essentially, we are creating a weighted-quantization error based on the importance of the weight:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd4b12b9-8b1b-4aa5-8d8c-ab8c1ab23671_1096x332.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd4b12b9-8b1b-4aa5-8d8c-ab8c1ab23671_1096x332.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd4b12b9-8b1b-4aa5-8d8c-ab8c1ab23671_1096x332.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd4b12b9-8b1b-4aa5-8d8c-ab8c1ab23671_1096x332.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd4b12b9-8b1b-4aa5-8d8c-ab8c1ab23671_1096x332.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd4b12b9-8b1b-4aa5-8d8c-ab8c1ab23671_1096x332.png" width="1096" height="332" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fd4b12b9-8b1b-4aa5-8d8c-ab8c1ab23671_1096x332.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:332,&quot;width&quot;:1096,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:16185,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd4b12b9-8b1b-4aa5-8d8c-ab8c1ab23671_1096x332.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd4b12b9-8b1b-4aa5-8d8c-ab8c1ab23671_1096x332.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd4b12b9-8b1b-4aa5-8d8c-ab8c1ab23671_1096x332.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd4b12b9-8b1b-4aa5-8d8c-ab8c1ab23671_1096x332.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Next, we redistribute this weighted quantization error over the other weights in the row. This allows for maintaining the overall function and output of the network.</p><p><span>For example, if we were to do this for the second weight, namely .3 (</span><em><strong>x_2</strong></em><span>), we would add the quantization error (</span><em><strong>q</strong></em><span>) multiplied by the inverse-Hessian of the second weight (</span><em><strong>h</strong></em><strong>_2</strong><span>)</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86456fdb-ba8f-4545-aa45-a0f4d4c59362_1096x244.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86456fdb-ba8f-4545-aa45-a0f4d4c59362_1096x244.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86456fdb-ba8f-4545-aa45-a0f4d4c59362_1096x244.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86456fdb-ba8f-4545-aa45-a0f4d4c59362_1096x244.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86456fdb-ba8f-4545-aa45-a0f4d4c59362_1096x244.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86456fdb-ba8f-4545-aa45-a0f4d4c59362_1096x244.png" width="1096" height="244" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/86456fdb-ba8f-4545-aa45-a0f4d4c59362_1096x244.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:244,&quot;width&quot;:1096,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:12974,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86456fdb-ba8f-4545-aa45-a0f4d4c59362_1096x244.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86456fdb-ba8f-4545-aa45-a0f4d4c59362_1096x244.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86456fdb-ba8f-4545-aa45-a0f4d4c59362_1096x244.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86456fdb-ba8f-4545-aa45-a0f4d4c59362_1096x244.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>We can do the same process over the third weight in the given row:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf47538b-4a3b-48df-9dbd-dded0ed09ce4_1284x438.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf47538b-4a3b-48df-9dbd-dded0ed09ce4_1284x438.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf47538b-4a3b-48df-9dbd-dded0ed09ce4_1284x438.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf47538b-4a3b-48df-9dbd-dded0ed09ce4_1284x438.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf47538b-4a3b-48df-9dbd-dded0ed09ce4_1284x438.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf47538b-4a3b-48df-9dbd-dded0ed09ce4_1284x438.png" width="1284" height="438" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/af47538b-4a3b-48df-9dbd-dded0ed09ce4_1284x438.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:438,&quot;width&quot;:1284,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:29608,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf47538b-4a3b-48df-9dbd-dded0ed09ce4_1284x438.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf47538b-4a3b-48df-9dbd-dded0ed09ce4_1284x438.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf47538b-4a3b-48df-9dbd-dded0ed09ce4_1284x438.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf47538b-4a3b-48df-9dbd-dded0ed09ce4_1284x438.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>We iterate over this process of redistributing the weighted quantization error until all values are quantized.</p><p>This works so well because weights are typically related to one another. So when one weight has a quantization error, related weights are updated accordingly (through the inverse-Hessian).</p><blockquote><p><strong>NOTE</strong><span>: </span><a href="https://arxiv.org/pdf/2210.17323" rel="">The authors</a><span> used several tricks to speed up computation and improve performance, such as adding a dampening factor to the Hessian, “lazy batching”, and precomputing information using the Cholesky method. I would highly advise checking out </span><a href="https://www.youtube.com/watch?v=mii-xFaPCrA" rel="">this YouTube video</a><span> on the subject.</span></p></blockquote><blockquote><p><strong>TIP</strong><span>: Check out </span><a href="https://github.com/turboderp/exllamav2" rel="">EXL2</a><span> if you want a quantization method aimed at performance optimizations and improving inference speed.</span></p></blockquote><p><span>While GPTQ is a great quantization method to run your full LLM on a GPU, you might not always have that capacity. Instead, we can use GGUF to offload any layer of the LLM to the CPU. </span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2-145531349" href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization#footnote-2-145531349" target="_self" rel="">2</a></span></p><p>This allows you to use both the CPU and GPU when you do not have enough VRAM.</p><p>The quantization method GGUF is updated frequently and might depend on the level of bit quantization. However, the general principle is as follows.</p><p><span>First, the weights of a given layer are split into “super” blocks each containing a set of “sub” blocks. From these blocks, we extract the scale factor (</span><em><strong>s</strong></em><span>) and alpha (</span><em><strong>α</strong></em><span>):</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98047d62-3925-4a29-a23b-c5bfa517f073_894x480.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98047d62-3925-4a29-a23b-c5bfa517f073_894x480.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98047d62-3925-4a29-a23b-c5bfa517f073_894x480.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98047d62-3925-4a29-a23b-c5bfa517f073_894x480.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98047d62-3925-4a29-a23b-c5bfa517f073_894x480.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98047d62-3925-4a29-a23b-c5bfa517f073_894x480.png" width="894" height="480" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/98047d62-3925-4a29-a23b-c5bfa517f073_894x480.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:480,&quot;width&quot;:894,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:15011,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98047d62-3925-4a29-a23b-c5bfa517f073_894x480.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98047d62-3925-4a29-a23b-c5bfa517f073_894x480.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98047d62-3925-4a29-a23b-c5bfa517f073_894x480.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98047d62-3925-4a29-a23b-c5bfa517f073_894x480.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>To quantize a given “sub” block, we can use the </span><em>absmax</em><span> quantization we used before. Remember that it multiplies a given weight by the scale factor </span><strong>(</strong><em><strong>s</strong></em><strong>)</strong><span>:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf159bb5-8158-43e3-bae1-8812fc0fa146_1096x120.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf159bb5-8158-43e3-bae1-8812fc0fa146_1096x120.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf159bb5-8158-43e3-bae1-8812fc0fa146_1096x120.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf159bb5-8158-43e3-bae1-8812fc0fa146_1096x120.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf159bb5-8158-43e3-bae1-8812fc0fa146_1096x120.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf159bb5-8158-43e3-bae1-8812fc0fa146_1096x120.png" width="1096" height="120" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bf159bb5-8158-43e3-bae1-8812fc0fa146_1096x120.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:120,&quot;width&quot;:1096,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:9257,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf159bb5-8158-43e3-bae1-8812fc0fa146_1096x120.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf159bb5-8158-43e3-bae1-8812fc0fa146_1096x120.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf159bb5-8158-43e3-bae1-8812fc0fa146_1096x120.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf159bb5-8158-43e3-bae1-8812fc0fa146_1096x120.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The scale factor is calculated using the information from the “sub” block but is quantized using the information from the “super” block which has its own scale factor:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3e7f68b-6ce8-45ba-a844-80b5eb0ab2d3_1096x196.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3e7f68b-6ce8-45ba-a844-80b5eb0ab2d3_1096x196.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3e7f68b-6ce8-45ba-a844-80b5eb0ab2d3_1096x196.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3e7f68b-6ce8-45ba-a844-80b5eb0ab2d3_1096x196.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3e7f68b-6ce8-45ba-a844-80b5eb0ab2d3_1096x196.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3e7f68b-6ce8-45ba-a844-80b5eb0ab2d3_1096x196.png" width="1096" height="196" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e3e7f68b-6ce8-45ba-a844-80b5eb0ab2d3_1096x196.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:196,&quot;width&quot;:1096,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:14531,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3e7f68b-6ce8-45ba-a844-80b5eb0ab2d3_1096x196.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3e7f68b-6ce8-45ba-a844-80b5eb0ab2d3_1096x196.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3e7f68b-6ce8-45ba-a844-80b5eb0ab2d3_1096x196.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3e7f68b-6ce8-45ba-a844-80b5eb0ab2d3_1096x196.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>This block-wise quantization uses the scale factor (</span><strong>s_super</strong><span>) from the “super” block to quantize the scale factor (</span><strong>s_sub</strong><span>) from the “sub” block.</span></p><p>The quantization level of each scale factor might differ with the “super” block generally having a higher precision than the scale factor of the “sub” block.</p><p>To illustrate, let’s explore a couple of quantization levels (2-bit, 4-bit, and 6-bit):</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43ca3393-869a-4be3-bf8b-0c52e42017d7_1984x692.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43ca3393-869a-4be3-bf8b-0c52e42017d7_1984x692.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43ca3393-869a-4be3-bf8b-0c52e42017d7_1984x692.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43ca3393-869a-4be3-bf8b-0c52e42017d7_1984x692.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43ca3393-869a-4be3-bf8b-0c52e42017d7_1984x692.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43ca3393-869a-4be3-bf8b-0c52e42017d7_1984x692.png" width="1456" height="508" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/43ca3393-869a-4be3-bf8b-0c52e42017d7_1984x692.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:508,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:68162,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43ca3393-869a-4be3-bf8b-0c52e42017d7_1984x692.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43ca3393-869a-4be3-bf8b-0c52e42017d7_1984x692.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43ca3393-869a-4be3-bf8b-0c52e42017d7_1984x692.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43ca3393-869a-4be3-bf8b-0c52e42017d7_1984x692.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><strong>NOTE</strong><span>: Depending on the quantization type, an additional minimum value (</span><em><strong>m</strong></em><span>) is needed to adjust the zero-point. These are quantized the same as the scale factor (</span><em><strong>s</strong></em><span>).</span></figcaption></figure></div><p><span>Check out </span><a href="https://github.com/ggerganov/llama.cpp/pull/1684" rel="">the original pull request</a><span> for an overview of all quantization levels. Also, see </span><a href="https://github.com/ggerganov/llama.cpp/pull/4861" rel="">this pull request</a><span> for more information on quantization using importance matrices.</span></p><p><span>In Part 3, we saw how we could quantize a model </span><em><strong>after</strong></em><span> training. A downside to this approach is that this quantization does not consider the actual training process. </span></p><p><span>This is where Quantization Aware Training (QAT) comes in. Instead of quantizing a model </span><em><strong>after</strong></em><span> it was trained with post-training quantization (PTQ), QAT aims to learn the quantization procedure </span><em><strong>during</strong></em><span> training.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad4fa3b-b440-4be3-90bd-b66e219f191e_1368x810.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad4fa3b-b440-4be3-90bd-b66e219f191e_1368x810.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad4fa3b-b440-4be3-90bd-b66e219f191e_1368x810.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad4fa3b-b440-4be3-90bd-b66e219f191e_1368x810.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad4fa3b-b440-4be3-90bd-b66e219f191e_1368x810.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad4fa3b-b440-4be3-90bd-b66e219f191e_1368x810.png" width="1368" height="810" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1ad4fa3b-b440-4be3-90bd-b66e219f191e_1368x810.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:810,&quot;width&quot;:1368,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:86444,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad4fa3b-b440-4be3-90bd-b66e219f191e_1368x810.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad4fa3b-b440-4be3-90bd-b66e219f191e_1368x810.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad4fa3b-b440-4be3-90bd-b66e219f191e_1368x810.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad4fa3b-b440-4be3-90bd-b66e219f191e_1368x810.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>QAT tends to be more accurate than PTQ since the quantization was already considered during training. It works as follows:</p><p><span>During training, so-called “</span><em>fake</em><span>” quants are introduced. This is the process of first quantizing the weights to, for example, INT4 and then dequantizing back to FP32:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3a17734-65f8-45d7-8e4e-f7bc1c592577_1824x360.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3a17734-65f8-45d7-8e4e-f7bc1c592577_1824x360.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3a17734-65f8-45d7-8e4e-f7bc1c592577_1824x360.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3a17734-65f8-45d7-8e4e-f7bc1c592577_1824x360.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3a17734-65f8-45d7-8e4e-f7bc1c592577_1824x360.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3a17734-65f8-45d7-8e4e-f7bc1c592577_1824x360.png" width="1456" height="287" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c3a17734-65f8-45d7-8e4e-f7bc1c592577_1824x360.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:287,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:33775,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3a17734-65f8-45d7-8e4e-f7bc1c592577_1824x360.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3a17734-65f8-45d7-8e4e-f7bc1c592577_1824x360.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3a17734-65f8-45d7-8e4e-f7bc1c592577_1824x360.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3a17734-65f8-45d7-8e4e-f7bc1c592577_1824x360.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>This process allows the model to consider the quantization process during training, the calculation of loss, and weight updates.</p><p><span>QAT attempts to explore the loss landscape for “</span><em>wide</em><span>” minima to minimize the quantization errors as “</span><em>narrow</em><span>” minima tend to result in larger quantization errors.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa70ee37e-3b4f-4598-8eef-2a9ab13658c1_1200x640.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa70ee37e-3b4f-4598-8eef-2a9ab13658c1_1200x640.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa70ee37e-3b4f-4598-8eef-2a9ab13658c1_1200x640.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa70ee37e-3b4f-4598-8eef-2a9ab13658c1_1200x640.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa70ee37e-3b4f-4598-8eef-2a9ab13658c1_1200x640.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa70ee37e-3b4f-4598-8eef-2a9ab13658c1_1200x640.png" width="1200" height="640" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a70ee37e-3b4f-4598-8eef-2a9ab13658c1_1200x640.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:640,&quot;width&quot;:1200,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:44538,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa70ee37e-3b4f-4598-8eef-2a9ab13658c1_1200x640.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa70ee37e-3b4f-4598-8eef-2a9ab13658c1_1200x640.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa70ee37e-3b4f-4598-8eef-2a9ab13658c1_1200x640.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa70ee37e-3b4f-4598-8eef-2a9ab13658c1_1200x640.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>For example, imagine if we did not consider quantization during the backward pass. We choose the weight with the smallest loss according to gradient descent. However, that would introduce a larger quantization error if it’s in a “</span><em>narrow</em><span>” minima.</span></p><p><span>In contrast, if we consider quantization, a different updated weight will be selected in a “</span><em>wide</em><span>” minima with a much lower quantization error.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb26d3f00-f599-4c75-beb4-21d87625b1d8_1200x640.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb26d3f00-f599-4c75-beb4-21d87625b1d8_1200x640.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb26d3f00-f599-4c75-beb4-21d87625b1d8_1200x640.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb26d3f00-f599-4c75-beb4-21d87625b1d8_1200x640.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb26d3f00-f599-4c75-beb4-21d87625b1d8_1200x640.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb26d3f00-f599-4c75-beb4-21d87625b1d8_1200x640.png" width="1200" height="640" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b26d3f00-f599-4c75-beb4-21d87625b1d8_1200x640.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:640,&quot;width&quot;:1200,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:64250,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb26d3f00-f599-4c75-beb4-21d87625b1d8_1200x640.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb26d3f00-f599-4c75-beb4-21d87625b1d8_1200x640.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb26d3f00-f599-4c75-beb4-21d87625b1d8_1200x640.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb26d3f00-f599-4c75-beb4-21d87625b1d8_1200x640.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>As such, although PTQ has a lower loss in high precision (e.g., FP32), QAT results in a lower loss in lower precision (e.g., INT4) which is what we aim for.</p><p>Going to 4-bits as we saw before is already quite small but what if we were to reduce it even further?</p><p><span>This is where </span><a href="https://arxiv.org/pdf/2310.11453" rel="">BitNet</a><span> comes in, representing the weights of a model single 1-bit, using either </span><strong>-1</strong><span> or </span><strong>1</strong><span> for a given weight.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-3-145531349" href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization#footnote-3-145531349" target="_self" rel="">3</a></span><span> </span></p><p>It does so by injecting the quantization process directly into the Transformer architecture.</p><p>Remember that the Transformer architecture is used as the foundation of most LLMs and is composed of computations that involve linear layers:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3587e75-d631-4ecd-8da9-26fedfc68c53_1364x768.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3587e75-d631-4ecd-8da9-26fedfc68c53_1364x768.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3587e75-d631-4ecd-8da9-26fedfc68c53_1364x768.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3587e75-d631-4ecd-8da9-26fedfc68c53_1364x768.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3587e75-d631-4ecd-8da9-26fedfc68c53_1364x768.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3587e75-d631-4ecd-8da9-26fedfc68c53_1364x768.png" width="1364" height="768" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e3587e75-d631-4ecd-8da9-26fedfc68c53_1364x768.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:768,&quot;width&quot;:1364,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:68464,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3587e75-d631-4ecd-8da9-26fedfc68c53_1364x768.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3587e75-d631-4ecd-8da9-26fedfc68c53_1364x768.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3587e75-d631-4ecd-8da9-26fedfc68c53_1364x768.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3587e75-d631-4ecd-8da9-26fedfc68c53_1364x768.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>These linear layers are generally represented with higher precision, like FP16, and are where most of the weights reside.</p><p><span>BitNet replaces these linear layers with something they call the </span><strong>BitLlinear</strong><span>:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6b8cbbf-057d-46c9-b275-dab262dd78d5_1364x768.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6b8cbbf-057d-46c9-b275-dab262dd78d5_1364x768.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6b8cbbf-057d-46c9-b275-dab262dd78d5_1364x768.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6b8cbbf-057d-46c9-b275-dab262dd78d5_1364x768.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6b8cbbf-057d-46c9-b275-dab262dd78d5_1364x768.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6b8cbbf-057d-46c9-b275-dab262dd78d5_1364x768.png" width="1364" height="768" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a6b8cbbf-057d-46c9-b275-dab262dd78d5_1364x768.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:768,&quot;width&quot;:1364,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:116998,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6b8cbbf-057d-46c9-b275-dab262dd78d5_1364x768.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6b8cbbf-057d-46c9-b275-dab262dd78d5_1364x768.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6b8cbbf-057d-46c9-b275-dab262dd78d5_1364x768.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6b8cbbf-057d-46c9-b275-dab262dd78d5_1364x768.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>A BitLinear layer works the same as a regular linear layer and calculates the output based on the weights multiplied by the activation. </p><p>In contrast, a BitLinear layer represents the weights of a model using 1-bit and activations using INT8:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9806feb5-2212-4fc3-af0b-ef42ae536787_1240x552.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9806feb5-2212-4fc3-af0b-ef42ae536787_1240x552.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9806feb5-2212-4fc3-af0b-ef42ae536787_1240x552.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9806feb5-2212-4fc3-af0b-ef42ae536787_1240x552.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9806feb5-2212-4fc3-af0b-ef42ae536787_1240x552.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9806feb5-2212-4fc3-af0b-ef42ae536787_1240x552.png" width="1240" height="552" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9806feb5-2212-4fc3-af0b-ef42ae536787_1240x552.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:552,&quot;width&quot;:1240,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:20000,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9806feb5-2212-4fc3-af0b-ef42ae536787_1240x552.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9806feb5-2212-4fc3-af0b-ef42ae536787_1240x552.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9806feb5-2212-4fc3-af0b-ef42ae536787_1240x552.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9806feb5-2212-4fc3-af0b-ef42ae536787_1240x552.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>A BitLinear layer, like Quantization-Aware Training (QAT) performs a form of “fake” quantization during training to analyze the effect of quantization of the weights and activations:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F25935e2a-7643-4705-8961-0b40506fe757_1296x832.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F25935e2a-7643-4705-8961-0b40506fe757_1296x832.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F25935e2a-7643-4705-8961-0b40506fe757_1296x832.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F25935e2a-7643-4705-8961-0b40506fe757_1296x832.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F25935e2a-7643-4705-8961-0b40506fe757_1296x832.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F25935e2a-7643-4705-8961-0b40506fe757_1296x832.png" width="1296" height="832" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/25935e2a-7643-4705-8961-0b40506fe757_1296x832.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:832,&quot;width&quot;:1296,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:84571,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F25935e2a-7643-4705-8961-0b40506fe757_1296x832.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F25935e2a-7643-4705-8961-0b40506fe757_1296x832.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F25935e2a-7643-4705-8961-0b40506fe757_1296x832.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F25935e2a-7643-4705-8961-0b40506fe757_1296x832.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><strong>NOTE</strong><span>: In the paper they used </span><strong>γ</strong><span> instead of </span><strong>α</strong><span> but since we used a throughout our examples, I’m using that. Also, note that </span><strong>β</strong><span> is not the same as we used in zero-point quantization but the average absolute value.</span></figcaption></figure></div><p>Let’s go through the BitLinear step-by-step.</p><p><span>While training, the weights are stored in INT8 and then quantized to 1-bit using a basic strategy, called the </span><em>signum function.</em></p><p>In essence, it moves the distribution of weights to be centered around 0 and then assigns everything left to 0 to be -1 and everything to the right to be 1:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1080b79-6d3c-4dde-a6f1-a354afae4f54_1152x508.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1080b79-6d3c-4dde-a6f1-a354afae4f54_1152x508.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1080b79-6d3c-4dde-a6f1-a354afae4f54_1152x508.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1080b79-6d3c-4dde-a6f1-a354afae4f54_1152x508.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1080b79-6d3c-4dde-a6f1-a354afae4f54_1152x508.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1080b79-6d3c-4dde-a6f1-a354afae4f54_1152x508.png" width="1152" height="508" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b1080b79-6d3c-4dde-a6f1-a354afae4f54_1152x508.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:508,&quot;width&quot;:1152,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:49127,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1080b79-6d3c-4dde-a6f1-a354afae4f54_1152x508.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1080b79-6d3c-4dde-a6f1-a354afae4f54_1152x508.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1080b79-6d3c-4dde-a6f1-a354afae4f54_1152x508.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1080b79-6d3c-4dde-a6f1-a354afae4f54_1152x508.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Additionally, it tracks a value </span><strong>β (</strong><em>average</em><span> absolute value</span><strong>)</strong><span> that we will use later on for dequantization. </span></p><p><span>To quantize the activations, BitLinear makes use of </span><em>absmax quantization</em><span> to convert the activations from FP16 to INT8 as they need to be in higher precision for the matrix multiplication (×). </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc69928b-3169-4c35-963b-c25ec218bc12_1260x552.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc69928b-3169-4c35-963b-c25ec218bc12_1260x552.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc69928b-3169-4c35-963b-c25ec218bc12_1260x552.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc69928b-3169-4c35-963b-c25ec218bc12_1260x552.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc69928b-3169-4c35-963b-c25ec218bc12_1260x552.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc69928b-3169-4c35-963b-c25ec218bc12_1260x552.png" width="1260" height="552" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bc69928b-3169-4c35-963b-c25ec218bc12_1260x552.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:552,&quot;width&quot;:1260,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:38296,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc69928b-3169-4c35-963b-c25ec218bc12_1260x552.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc69928b-3169-4c35-963b-c25ec218bc12_1260x552.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc69928b-3169-4c35-963b-c25ec218bc12_1260x552.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc69928b-3169-4c35-963b-c25ec218bc12_1260x552.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Additionally, it tracks </span><strong>α (</strong><em>highest</em><span> absolute value</span><strong>)</strong><span> that we will use later on for dequantization.</span></p><p><span>We tracked </span><strong>α (</strong><em>highest absolute value of activations</em><strong>) </strong><span>and </span><strong>β (</strong><em>average absolute value of weights</em><strong>) </strong><span>as those values will help us dequantize the activations  back to FP16.</span></p><p><span>The output activations are rescaled with {</span><strong>α</strong><span>, γ} to dequantize them to the original precision:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F330cb1bb-9a98-45c1-b140-0fa8038d521f_1296x404.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F330cb1bb-9a98-45c1-b140-0fa8038d521f_1296x404.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F330cb1bb-9a98-45c1-b140-0fa8038d521f_1296x404.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F330cb1bb-9a98-45c1-b140-0fa8038d521f_1296x404.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F330cb1bb-9a98-45c1-b140-0fa8038d521f_1296x404.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F330cb1bb-9a98-45c1-b140-0fa8038d521f_1296x404.png" width="1296" height="404" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/330cb1bb-9a98-45c1-b140-0fa8038d521f_1296x404.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:404,&quot;width&quot;:1296,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:22549,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F330cb1bb-9a98-45c1-b140-0fa8038d521f_1296x404.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F330cb1bb-9a98-45c1-b140-0fa8038d521f_1296x404.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F330cb1bb-9a98-45c1-b140-0fa8038d521f_1296x404.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F330cb1bb-9a98-45c1-b140-0fa8038d521f_1296x404.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>And that’s it! This procedure is relatively straightforward and allows models to be represented with only two values, either </span><strong>-1</strong><span> or </span><strong>1</strong><span>. </span></p><p>Using this procedure, the authors observed that as the model size grows, the smaller the performance gap between a 1-bit and FP16-trained becomes.</p><p>However, this is only for larger models (&gt;30B parameters) and the gab with smaller models is still quite large.</p><p><a href="https://arxiv.org/pdf/2402.17764" rel="">BitNet 1.58b</a><span> was introduced to improve upon the scaling issue previously mentioned.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-4-145531349" href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization#footnote-4-145531349" target="_self" rel="">4</a></span></p><p><span>In this new method, every single weight of the model is not just </span><strong>-1</strong><span> or </span><strong>1</strong><span>, but can now also take </span><strong>0</strong><span> as a value, making it </span><em>ternary</em><span>. Interestingly, adding just the </span><strong>0</strong><span> greatly improves upon BitNet and allows for much faster computation.</span></p><p>So why is adding 0 such a major improvement?</p><p><span>It has everything to do with </span><em>matrix multiplication</em><span>!</span></p><p>First, let’s explore how matrix multiplication in general works. When calculating the output, we multiply a weight matrix by an input vector. Below, the first multiplication of the first layer of a weight matrix is visualized:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f3a7393-5ad4-4375-8382-197c7a5aa442_1048x360.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f3a7393-5ad4-4375-8382-197c7a5aa442_1048x360.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f3a7393-5ad4-4375-8382-197c7a5aa442_1048x360.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f3a7393-5ad4-4375-8382-197c7a5aa442_1048x360.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f3a7393-5ad4-4375-8382-197c7a5aa442_1048x360.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f3a7393-5ad4-4375-8382-197c7a5aa442_1048x360.png" width="1048" height="360" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5f3a7393-5ad4-4375-8382-197c7a5aa442_1048x360.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:360,&quot;width&quot;:1048,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:24808,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f3a7393-5ad4-4375-8382-197c7a5aa442_1048x360.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f3a7393-5ad4-4375-8382-197c7a5aa442_1048x360.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f3a7393-5ad4-4375-8382-197c7a5aa442_1048x360.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f3a7393-5ad4-4375-8382-197c7a5aa442_1048x360.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Note that this multiplication involves two actions, </span><strong>multiplying</strong><span> individual weights with the input and then </span><strong>adding</strong><span> them all together.</span></p><p>BitNet 1.58b, in contrast, manages to forego the act of multiplication since ternary weights essentially tell you the following:</p><ul><li><p>1: I want to add this value</p></li><li><p>0: I do not want this value</p></li><li><p>-1: I want to subtract this value</p></li></ul><p>As a result, you only need to perform addition if your weights are quantized to 1.58 bit:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5fed2720-9aa3-4b83-8ba7-4347b2fe1f0d_1048x360.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5fed2720-9aa3-4b83-8ba7-4347b2fe1f0d_1048x360.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5fed2720-9aa3-4b83-8ba7-4347b2fe1f0d_1048x360.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5fed2720-9aa3-4b83-8ba7-4347b2fe1f0d_1048x360.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5fed2720-9aa3-4b83-8ba7-4347b2fe1f0d_1048x360.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5fed2720-9aa3-4b83-8ba7-4347b2fe1f0d_1048x360.png" width="1048" height="360" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5fed2720-9aa3-4b83-8ba7-4347b2fe1f0d_1048x360.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:360,&quot;width&quot;:1048,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:24345,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5fed2720-9aa3-4b83-8ba7-4347b2fe1f0d_1048x360.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5fed2720-9aa3-4b83-8ba7-4347b2fe1f0d_1048x360.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5fed2720-9aa3-4b83-8ba7-4347b2fe1f0d_1048x360.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5fed2720-9aa3-4b83-8ba7-4347b2fe1f0d_1048x360.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Not only can this speed up computation significantly, but it also allows for </span><strong>feature filtering</strong><span>.</span></p><p>By setting a given weight to 0 you can now ignore it instead of either adding or subtracting the weights as is the case with 1-bit representations.</p><p><span>To perform weight quantization BitNet 1.58b uses </span><em>absmean</em><span> quantization which is a variation of the absmax quantization that we saw before. </span></p><p><span>It simply compresses the distribution of weights and uses the absolute mean (</span><strong>α</strong><span>) to quantize values. They are then rounded to either -1, 0, or 1:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facda9425-8b3d-47fe-92de-16c33e57613b_1108x512.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facda9425-8b3d-47fe-92de-16c33e57613b_1108x512.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facda9425-8b3d-47fe-92de-16c33e57613b_1108x512.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facda9425-8b3d-47fe-92de-16c33e57613b_1108x512.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facda9425-8b3d-47fe-92de-16c33e57613b_1108x512.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facda9425-8b3d-47fe-92de-16c33e57613b_1108x512.png" width="1108" height="512" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/acda9425-8b3d-47fe-92de-16c33e57613b_1108x512.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:512,&quot;width&quot;:1108,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:46816,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facda9425-8b3d-47fe-92de-16c33e57613b_1108x512.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facda9425-8b3d-47fe-92de-16c33e57613b_1108x512.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facda9425-8b3d-47fe-92de-16c33e57613b_1108x512.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facda9425-8b3d-47fe-92de-16c33e57613b_1108x512.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Compared to BitNet the activation quantization is the same except for one thing. Instead of scaling the activations to range [</span><strong>0</strong><span>, </span><strong>2ᵇ⁻¹</strong><span>], they are now scaled to </span><br><span>[</span><strong>-2ᵇ⁻¹</strong><span>,</span><strong> 2ᵇ⁻¹</strong><span>] instead using </span><em>absmax quantization</em><span>.</span></p><p>And that’s it! 1.58-bit quantization required (mostly) two tricks:</p><ul><li><p><span>Adding </span><strong>0</strong><span> to create ternary representations [-1, 0, 1]</span></p></li><li><p><em>absmean quantization</em><span> for weights</span></p></li></ul><p><span>“</span><strong>13B</strong><span> BitNet b1.58 is more efficient, in terms of latency, memory usage, and energy consumption than a </span><strong>3B</strong><span> FP16 LLM”</span></p><p>As a result, we get lightweight models due to having only 1.58 computationally efficient bits!</p><p>This concludes our journey in quantization! Hopefully, this post gives you a better understanding of the potential of quantization, GPTQ, GGUF, and BitNet. Who knows how small the models will be in the future?!</p><p>To see more visualizations related to LLMs and to support this newsletter, check out the book I’m writing with Jay Alammar. It will be released soon!</p><p>Hopefully, this was an accessible introduction to quantization! If you want to go deeper, I would suggest the following resources:</p><ul><li><p><span>A HuggingFace blog about the </span><strong><a href="https://huggingface.co/blog/hf-bitsandbytes-integration" rel="">LLM.int8()</a></strong><span> quantization method: you can find the paper </span><a href="https://arxiv.org/pdf/2208.07339" rel="">here</a><span>. </span></p></li><li><p><span>Another great HuggingFace blog about </span><a href="https://huggingface.co/blog/embedding-quantization" rel="">quantization for </a><strong><a href="https://huggingface.co/blog/embedding-quantization" rel="">embeddings</a></strong><span>.</span></p></li><li><p><span>A blog about </span><a href="https://blog.eleuther.ai/transformer-math/" rel="">Transformer Math 101</a><span>, describing the basic math related to computation and memory usage for transformers.</span></p></li><li><p><a href="https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator" rel="">This</a><span> and </span><a href="https://vram.asmirnov.xyz/" rel="">this</a><span> are two nice resources to calculate the (</span><strong>V)RAM</strong><span> you need for a given model.</span></p></li><li><p><span>If you want to know more about </span><strong>QLoRA</strong><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-5-145531349" href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization#footnote-5-145531349" target="_self" rel="">5</a></span><span>, a quantization technique for fine-tuning, it is covered extensively in my upcoming book: </span><a href="https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961" rel="">Hands-On Large Language Models</a><span>.</span></p></li><li><p><span>A truly </span><a href="https://www.youtube.com/watch?v=mii-xFaPCrA" rel="">amazing YouTube video</a><span> about </span><strong>GPTQ</strong><span> explained incredibly intuitively. </span></p></li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Diffusion Training from Scratch on a Micro-Budget (152 pts)]]></title>
            <link>https://arxiv.org/abs/2407.15811</link>
            <guid>41105779</guid>
            <pubDate>Tue, 30 Jul 2024 03:19:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2407.15811">https://arxiv.org/abs/2407.15811</a>, See on <a href="https://news.ycombinator.com/item?id=41105779">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2407.15811">View PDF</a></p><blockquote>
            <span>Abstract:</span>As scaling laws in generative AI push performance, they also simultaneously concentrate the development of these models among actors with large computational resources. With a focus on text-to-image (T2I) generative models, we aim to address this bottleneck by demonstrating very low-cost training of large-scale T2I diffusion transformer models. As the computational cost of transformers increases with the number of patches in each image, we propose to randomly mask up to 75% of the image patches during training. We propose a deferred masking strategy that preprocesses all patches using a patch-mixer before masking, thus significantly reducing the performance degradation with masking, making it superior to model downscaling in reducing computational cost. We also incorporate the latest improvements in transformer architecture, such as the use of mixture-of-experts layers, to improve performance and further identify the critical benefit of using synthetic images in micro-budget training. Finally, using only 37M publicly available real and synthetic images, we train a 1.16 billion parameter sparse transformer with only \$1,890 economical cost and achieve a 12.7 FID in zero-shot generation on the COCO dataset. Notably, our model achieves competitive FID and high-quality generations while incurring 118$\times$ lower cost than stable diffusion models and 14$\times$ lower cost than the current state-of-the-art approach that costs \$28,400. We aim to release our end-to-end training pipeline to further democratize the training of large-scale diffusion models on micro-budgets.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Vikash Sehwag [<a href="https://arxiv.org/show-email/e522d0b4/2407.15811">view email</a>]      <br>    <strong>[v1]</strong>
        Mon, 22 Jul 2024 17:23:28 UTC (28,658 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microjs (129 pts)]]></title>
            <link>http://microjs.com/</link>
            <guid>41105261</guid>
            <pubDate>Tue, 30 Jul 2024 01:16:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://microjs.com/">http://microjs.com/</a>, See on <a href="https://news.ycombinator.com/item?id=41105261">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p><strong><span>Fantastic</span><span> Micro</span><span>-</span><span>Frameworks</span><span> and</span><br>
      <span>Micro</span><span>-</span><span>Libraries</span><span> for</span><span> Fun</span>
      <span>and</span><span> Profit</span><span>!</span></strong>
    </p>
    <p>How much library code do you really need — 50K? 100K? 150K? More? How
    much of that do you really use?</p>

    <p>Sure, we all love our favorite monolithic frameworks, and sometimes we
    even use them fully. But how often do we reach for the ride-on John
    Deere tractor with air conditioning and six-speaker sound system, when
    a judiciously applied pocketknife would do the trick better, faster,
    slicker?</p>

    <p>Micro-frameworks are definitely the pocketknives of the JavaScript
    library world: short, sweet, to the point. And at 5k and under,
    micro-frameworks are very very portable. A micro-framework does one
    thing and one thing only — and does it well. No cruft, no featuritis,
    no feature creep, no excess anywhere.</p>

    <p>Microjs.com helps you discover the most compact-but-powerful
    microframeworks, and makes it easy for you to pick one that’ll work
    for you.</p>

    <p>Want to add your own?
    <a href="https://github.com/microjs/microjs.com">Fork this site on GitHub</a>,
    add your framework to data.js and submit a pull request.</p>

    <p>
      <strong><span>Can't</span> <span>get</span> <span>enough</span><span>?</span></strong>
    </p>
    <p>
      Special thanks to <a href="https://everytimezone.com/">Time Zone Converter</a>.
    </p>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Four billion years in four minutes – Simulating worlds on the GPU (252 pts)]]></title>
            <link>https://davidar.io/post/sim-glsl</link>
            <guid>41104721</guid>
            <pubDate>Mon, 29 Jul 2024 23:33:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://davidar.io/post/sim-glsl">https://davidar.io/post/sim-glsl</a>, See on <a href="https://news.ycombinator.com/item?id=41104721">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<section id="abstract">
<h6>Abstract</h6>
<p>This post delves into the implementation of my <a href="https://www.shadertoy.com/view/XttcWn">procedural earth simulation</a>, written entirely in GLSL fragment shaders. It simulates the complete history of an earth-like planet in a few minutes, with the simulation updating at 60 frames per second.</p>
</section>
<figure>
<p><iframe src="https://player.vimeo.com/video/283607168" width="640" height="360" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen=""></iframe></p>
<figcaption>A video recording of the <a href="https://www.shadertoy.com/view/XttcWn">final shader</a>.</figcaption>
</figure>
<h2>Protoplanet</h2>
<blockquote>
<p>This story begins four and a half billion years ago, with a lump of molten rock...
</p></blockquote>
<figure>

</figure>
<p>The early earth was a <a href="https://en.wikipedia.org/wiki/Protoplanet">protoplanet</a>, red hot and heavily cratered by asteroid impacts. As my earth simulation is <em>entirely procedurally generated</em>, with no pre-rendered textures, the first task is to generate a map of this terrain. To calculate the <code>height</code> of the terrain at a given <code>lat</code>itude and <code>lon</code>gitude, first translate to 3D cartesian coordinates:
</p><pre><code>
vec3 p = 1.5 * vec3(
    sin(lon*PI/180.) * cos(lat*PI/180.),
    sin(lat*PI/180.),
    cos(lon*PI/180.) * cos(lat*PI/180.));
</code></pre>
<p>Now, as asteroids come in a variety of sizes, so do the resulting craters. To accommodate this, the shader iterates over five levels of detail, layering craters of decreasing size over each other. To make the craters have a realistic rugged appearance, this is mixed with some <a href="https://iquilezles.untergrund.net/www/articles/fbm/fbm.htm">fractional Brownian motion</a> noise, and scaled so that the largest craters have the most impact on the terrain.
</p><pre><code>
float height = 0.;
for (float i = 0.; i &lt; 5.; i++) {
    float c = craters(0.4 * pow(2.2, i) * p);
    float noise = 0.4 * exp(-3. * c) * FBM(10. * p);
    float w = clamp(3. * pow(0.4, i), 0., 1.);
    height += w * (c + noise);
}
height = pow(height, 3.);
</code></pre>
<p>The craters themselves are generated on a 3D grid, from which a sphere is carved out for the surface terrain. To avoid visible regularity, the crater centres are given a pseudo-random offset from the grid points, using a <a href="https://www.shadertoy.com/view/4djSRW">hash function</a>. To calculate influence of a crater at a given location, take a weighted average of the craters belonging to the nearby grid points, with weights exponentially decreasing with distance from the centre. The crater rims are generated by a simple sine curve.
</p><pre><code>
float craters(vec3 x) {
    vec3 p = floor(x);
    vec3 f = fract(x);
    float va = 0.;
    float wt = 0.;
    for (int i = -2; i &lt;= 2; i++)
     for (int j = -2; j &lt;= 2; j++)
      for (int k = -2; k &lt;= 2; k++) {
        vec3 g = vec3(i,j,k);
        vec3 o = 0.8 * hash33(p + g);
        float d = distance(f - g, o);
        float w = exp(-4. * d);
        va += w * sin(2.*PI * sqrt(d));
        wt += w;
    }
    return abs(va / wt);
}
</code></pre>
<p>The final procedurally generated heightmap looks like this:
</p><figure>
<img src="https://davidar.io/img/protoplanet.jpg">
</figure>
<p>Although relatively simple, after filling the low-lying regions with water, this procedural terrain resembles what scientists believe the early earth actually looked like:
</p><figure>
<img src="https://davidar.io/img/hadeanearth.jpg">
<figcaption>Artistic impression of the early earth, by <a href="https://sservi.nasa.gov/articles/new-nasa-research-shows-giant-asteroids-battered-early-earth/">NASA</a>.</figcaption>
</figure>
<blockquote>
<p>Water contained within was vaporised by the heat, which escaped and began circulating through the early atmosphere forming around the planet. As time progressed and the rock cooled, the water vapour began to condense into oceans. The flow of liquid water across the surface carved valleys in the terrain, leaving an accumulation of sediment in its wake.
</p></blockquote>
<h2>Tectonic plates</h2>
<p>The formation of mountains, ocean trenches, and familiar continental landforms requires a model of tectonic movement. The simulation randomly generates seed locations for plates, with an initial velocity. These plates grow in size over time with a simple aggregation model, which randomly selects neighbouring points and adds them to a plate if they have not already been assigned to another plate. All of the pixels within a plate store the velocity of the plate's movement. The aggregation model is similar to that of a diffusion-limited aggregation (but without the diffusion):
</p><figure>

</figure>
<p>Continuous movement of the plates is difficult, as it would require plate boundaries to account for movements measured in fractions of a pixel. To avoid this, the plates are instead moved at discrete time-steps, by a whole pixel either horizontally or vertically. These times are randomised for each plate such that the average velocity is maintained at the set speed and direction, and also so that it is unlikely that neighbouring plates will move simultaneously.
</p><p>Plate collisions occur when some boundary pixels of one plate move onto a location previously occupied by pixels belonging to another plate. This causes <a href="https://en.wikipedia.org/wiki/Subduction">subduction</a>, which is modelled by simply slightly increasing the elevation of the terrain at the locations of the collision. Although this only occurs at the pixels along the boundary of a plate, the impact is gradually spread to neighbouring pixels through a simple thermal erosion model, which pushes the elevation of a pixel in the direction of the average of its neighbours.
</p><p>Altogether this provides a decent simulation of the formation of continents with mountain ranges (which will be further improved with the introduction of hydraulic erosion in the next section):
</p><figure>

</figure>
<h2>Hydraulic erosion</h2>
<p>The rugged appearance of natural terrain is largely driven by the formation of river basins, which erode landscapes in a familiar branching pattern. A variety of water flow simulations are readily available for this task, but a difficulty here is that the resolution of the terrain map is quite low for an entire planet. Therefore, the model will have to be able to simulate rivers which are no more than a single pixel wide. <a href="https://arxiv.org/abs/1803.02977">Barnes (2018)</a> proposes a simple model which achieves just this.
</p><p>Simply put, each pixel examines its eight neighbours, to determine which direction has the greatest decrease in elevation (adjusted for the fact that the diagonal neighbours are further away). This direction of greatest slope is where water flowing out of this pixel will travel. Water is initially distributed amongst cells by rainfall, which is then transported between neighbouring pixels at each time-step.
</p><p>Erosion is driven by a <a href="https://en.wikipedia.org/wiki/Stream_power_law">stream power law</a>:
</p><pre><code>
elevation -= 0.05 * pow(water, 0.8) * pow(slope, 2.);
</code></pre>
<p>Here we have the <code>elevation</code> and amount of <code>water</code> located at the current cell, along with the <code>slope</code> in the direction the water is travelling. The decrease in elevation is capped so that it doesn't become lower than the location the water is flowing to.
</p><p>The interaction between the water flow and erosion results in the natural formation of river basins in the terrain:
</p><figure>

</figure>
<p>By colouring connected waterways (with the colour determined by the location of the river's mouth), it's possible to produce striking visualisations reminiscent of <a href="https://imgur.com/gallery/WaEbi">real river basin maps</a>:
</p><figure>
<img src="https://davidar.io/img/basin.png">
<figcaption>Simulated river basins. <a href="https://www.shadertoy.com/view/XsVBDz">Original shader</a>.</figcaption>
</figure>
<figure>
<img src="https://i.imgur.com/ZXLEvU3.jpg">
<figcaption>River basins of USA, by <a href="https://www.grasshoppergeography.com/"></a>Grasshopper Geography.</figcaption>
</figure>
<h2>Global climate</h2>
<p>Simulating the climate system of an entire planet is a daunting task, but luckily it turns out that it can be approximated relatively easily. The driving force behind everything in my climate simulation is a procedurally generated map of the <a href="https://en.wikipedia.org/wiki/Atmospheric_pressure#Mean_sea-level_pressure">mean sea-level pressure (MSLP)</a>.
</p><p>According to <a href="https://web.archive.org/web/20130619132254/http://jc.tech-galaxy.com/bricka/climate_cookbook.html">the Climate Cookbook</a>, the main ingredients in creating a MSLP map are where the landforms are located amidst the ocean, and the impact of latitude. In fact, if you take data from a real MSLP map of the Earth, separate out locations according to whether they are land or ocean, and plot the MSLP against latitude, you end up with two sinusoidal curves for the land and ocean with slightly different shapes.
</p><p>By fitting the parameters appropriately, I came up with a crude model of the annual mean pressure (here the <code>lat</code>itude is measured in degrees):
</p><pre><code>
if (land) {
    mslp = 1012.5 - 6. * cos(lat*PI/45.);
} else { // ocean
    mslp = 1014.5 - 20. * cos(lat*PI/30.);
}
</code></pre>
<p>Of course, this isn't quite enough to generate a realistic MSLP map, as generating values for the land and ocean separately results in sharp discontinuities at the boundaries between them. In reality, MSLP smoothly varies across the transition from ocean to land, due to the local diffusion of gas pressure. This diffusion process can be approximated quite well by simply applying a <a href="https://en.wikipedia.org/wiki/Gaussian_blur">Gaussian blur</a> to the MSLP map (with a standard deviation of 10--15 degrees).
</p><p>To allow for the climate to change along with the seasons, it's necessary to also model the difference in MSLP between January and July. Once again, terrestrial data suggests this follows a sinusoidal pattern. By fitting parameters and applying a Gaussian blur, this can be combined with the annual MSLP map to generate dynamic climate patterns which vary throughout the year.
</p><pre><code>
if (land) {
    delta = 15. * sin(lat*PI/90.);
} else { // ocean
    delta = 20. * sin(lat*PI/35.) * abs(lat)/90.;
}
</code></pre>
<p>Now, with the MSLP in hand, it is possible to generate wind currents and temperatures. In reality it's the temperate which generates the pressure, but correlation is correlation. This requires a little more fiddling to generate realistic values (<code>season</code> oscillates between -1 and 1 throughout the year):
</p><pre><code>
float temp = 40. * tanh(2.2 * exp(-0.5 * pow((lat + 5. * season)/30., 2.)))
             - 15. - (mslp - 1012.) / 1.8 + 1.5 * land - 4. * elevation;
</code></pre>
<p>Wind tends to move from high-pressure to low, but at a global scale we also need to account for the <a href="https://en.wikipedia.org/wiki/Coriolis_force">Coriolis force</a>, which is responsible for causing winds to circulate <em>around</em> pressure zones (<code>grad</code> is the MSLP gradient vector):
</p><pre><code>
vec2 coriolis = 15. * sin(lat*PI/180.) * vec2(-grad.y, grad.x);
vec2 velocity = coriolis - grad;
</code></pre>
<p>Although a relatively crude simulation, this generates remarkably <a href="https://gist.github.com/davidar/229193b04bdb0dd8cba20dc31592625a">realistic</a> wind circulation patterns. If you look closely, you may notice a number of natural phenomena being replicated, including the reversal of winds over India during the monsoon season:
</p><figure>

</figure>
<p>As a final detail, precipitation can be simulated by advecting water vapour from the ocean, through the wind vector field, and onto the land:
</p><figure>

</figure>
<p>The advection is implemented in a similar manner to fluid simulations:
</p><figure>

</figure>
<h2>Life</h2>
<p>The climate influences the distribution of life on a planet. Rainfall patterns and temperature variation dictate rates of plant growth. As the seasons change, herbivores migrate to regions with enough vegetation to sustain them. And, as they follow the vegetation, predators follow them. All of these dynamics can be captured by a <a href="https://en.wikipedia.org/wiki/Lotka-Volterra_equations">Lotka--Volterra</a> diffusion model:
</p><pre><code>
float dx = plant_growth - c.y;
float dy = reproduction * c.x - predation * c.z - 1.;
float dz = predation * c.y - 1.;
float dt = 0.1;
c.xyz += dt * c.xyz * vec3(dx, dy, dz);
</code></pre>
<p>The <code>xyz</code> elements of <code>c</code> represent the populations of vegetation, herbivores, and predators respectively. On a large scale, the dynamics of animal populations generate interesting patterns:
</p><figure>

</figure>
<p>In real life, these kinds of patterns are most easily seen with microbe populations in a petri dish, but the same laws govern large animal populations across the globe.
</p><figure>
<img src="https://davidar.io/img/spiralmold.jpg">
<figcaption><a href="http://www.evsc.net/projects/reaction-diffusion-2">Spiral waves in colonies of mold</a>.</figcaption>
</figure>
<h2>Humanity</h2>
<blockquote>
<p>Concluding the prelude on the early earth, the pace slows to a cycle between day and night, terrain becoming fixed as tectonic movements become imperceptible. Soon the night reveals unprecedented patterns of light, as humanity proceeds to colonise the surface of the planet.
</p><p>This rapid expansion brings its own set of changes, as humans begin to burn large amounts of fossil fuels to power their settlements. Carbon that had lain dormant for millions of years is released into the atmosphere, and dispersed around the planet.
</p><p>Over several hundred years, humans burn through all available fossil fuel resources, releasing five trillion tonnes of carbon into the atmosphere. This strengthens the greenhouse effect, <a href="https://www.nature.com/articles/nclimate3036">raising the global average temperature by almost 10 degrees Celsius</a>. Large regions of land around the equator are rendered uninhabitable by extreme temperatures, resulting in the disappearance of humanity from a significant portion of the planet.
</p></blockquote>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LG and Samsung are making TV screens disappear (154 pts)]]></title>
            <link>https://spectrum.ieee.org/transparent-tv</link>
            <guid>41104615</guid>
            <pubDate>Mon, 29 Jul 2024 23:13:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/transparent-tv">https://spectrum.ieee.org/transparent-tv</a>, See on <a href="https://news.ycombinator.com/item?id=41104615">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>Television giants LG [pictured] and Samsung both demonstrated see-through displays at CES 2024.    </p><p>
        Travis P. Ball/Sipa/AP 
    </p><div data-headline="How LG and Samsung Are Making TV Screens Disappear"><p><strong>A transparent television might </strong><span>seem like magic, but both </span><a href="https://www.lg.com/us/business/digital-signage/oled-signage/transparent-oled-displays" target="_blank">LG</a> and <a href="https://www.samsung.com/us/tvs/micro-led/highlights/" target="_blank">Samsung</a><span>demonstrated</span> such displays this past January in Las Vegas at <a href="https://spectrum.ieee.org/tag/ces">CES</a> 2024. And those large transparent TVs, which attracted countless spectators peeking through video images dancing on their screens, <span>were showstoppers.</span></p><p>
	Although they are indeed impressive, transparent TVs are not likely to appear—or disappear—in your living room any time soon. Samsung and LG have taken two very different approaches to achieve a similar end—LG is betting on OLED displays, while Samsung is pursuing microLED screens—and neither technology is quite ready for prime time. Understanding the hurdles that still need to be overcome, though, requires a deeper dive into each of these display technologies.
</p><h2>How does LG’s see-through OLED work?</h2><p>
	OLED stands for organic light-emitting diode, and that pretty much describes how it works. OLED materials are carbon-based compounds that emit light when energized with an electrical current. Different compounds produce different colors, which can be combined to create full-color images.
</p><p>
	To construct a display from these materials, manufacturers deposit them as thin films on some sort of substrate. The most common approach arranges red-, green-, and blue-emitting (RGB) materials in patterns to create a dense array of full-color pixels. A display with what is known as 4K resolution contains a matrix of 3,840 by 2,160 pixels—8.3 million pixels in all, formed from nearly 25 million red, green, and blue subpixels.
</p><p>
	The timing and amount of electrical current sent to each subpixel determines how much light it emits. So by controlling these currents properly, you can create the desired image on the screen. To accomplish this, each subpixel must be electrically connected to two or more transistors, which act as switches. Traditional wires wouldn’t do for this, though: They’d block the light. You need to use transparent (or largely transparent) conductive traces.
</p><p><img alt="An image of an array of 15 transparent TVs, shot with a fish-eye lens and displaying white trees with pink and green swaths of color above them.    " data-rm-shortcode-id="70ea364ea7a9bda570506de1bee303be" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/an-image-of-an-array-of-15-transparent-tvs-shot-with-a-fish-eye-lens-and-displaying-white-trees-with-pink-and-green-swaths-of-c.png?id=52961258&amp;width=980" height="1419" id="2fa5e" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/an-image-of-an-array-of-15-transparent-tvs-shot-with-a-fish-eye-lens-and-displaying-white-trees-with-pink-and-green-swaths-of-c.png?id=52961258&amp;width=980" width="2700"><small placeholder="Add Photo Caption...">LG’s demonstration of transparent OLED displays at CES 2024 seemed almost magical.  </small><small placeholder="Add Photo Credit...">Ethan Miller/Getty Images </small></p><p>
	A display has thousands of such traces arranged in a series of rows and columns to provide the necessary electrical connections to each subpixel. The <a href="https://spectrum.ieee.org/tag/transistor">transistor</a> switches are also fabricated on the same substrate. That all adds up to a lot of materials that must be part of each display. And those materials must be carefully chosen for the OLED display to appear transparent.
</p><p>
	The conductive traces are the easy part. The display industry has long used indium tin oxide as a thin-film conductor. A typical layer of this material is only 135 nanometers thick but allows about 80 percent of the light impinging on it to pass through.
</p><p>
	The transistors are more of a problem, because the materials used to fabricate them are inherently opaque. The solution is to make the transistors as small as you can, so that they block the least amount of light. The amorphous silicon layer used for transistors in most LCD displays is inexpensive, but its low electron mobility means that transistors composed of this material can only be made so small. This silicon layer can be annealed with <a href="https://spectrum.ieee.org/tag/lasers">lasers</a> to create low-temperature polysilicon, a crystallized form of silicon, which improves electron mobility, reducing the size of each transistor. But this process works only for small sheets of glass substrate.
</p><p>
	Faced with this challenge, designers of transparent OLED displays have turned to indium gallium zinc oxide (IGZO). This material has high enough electron mobility to allow for smaller transistors than is possible with amorphous silicon, meaning that IGZO transistors block less light.
</p><p>
	These tactics help solve the transparency problem, but OLEDs have some other challenges. For one, exposure to oxygen or water vapor destroys the light-emissive materials. So these displays need an encapsulating layer, something to cover their surfaces and edges. Because this layer creates a visible gap when two panels are placed edge to edge, you can’t tile a set of smaller displays to create a larger one. If you want a big OLED display, you need to fabricate a single large panel.
</p><p>
	The result of even the best engineering here is a “transparent” display that still blocks some light. You won’t mistake LG’s transparent TV for window glass: People and objects behind the screen appear noticeably darker than when viewed directly. According to one informed observer, the LG prototype appears to have 45 percent transparency.
</p><h2>How does Samsung’s magical MicroLED work?</h2><p>
	For its transparent displays, Samsung is using inorganic LEDs. These devices, which are very efficient at converting electricity into light, are commonplace today: in household lightbulbs, in automobile headlights and taillights, and in electronic gear, where they often show that the unit is turned on.
</p><p>
	In LED displays, each pixel contains three LEDs, one red, one green, and one blue. This works great for the giant digital displays used in highway billboards or in sports-stadium jumbotrons, whose images are meant to be viewed from a good distance. But up close, these LED pixel arrays are noticeable.
</p><p>
	TV displays, on the other hand, are meant to be viewed from modest distances and thus require far smaller LEDs than the chips used in, say, power-indicator lights. Two years ago, these “microLED” displays used chips that were just 30 by 50 micrometers. (A typical sheet of paper is 100 micrometers thick.) Today, such displays use chips less than half that size: 12 by 27 micrometers.
</p><p><img alt="A wooden frame surrounds a transparent display featuring an advertisement for a Black Friday Sale and a large image of a smartwatch. " data-rm-shortcode-id="da32250abbffa436cc0bab507c25ab6f" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/a-wooden-frame-surrounds-a-transparent-display-featuring-an-advertisement-for-a-black-friday-sale-and-a-large-image-of-a-smartwa.jpg?id=52961422&amp;width=980" height="1027" id="80878" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/a-wooden-frame-surrounds-a-transparent-display-featuring-an-advertisement-for-a-black-friday-sale-and-a-large-image-of-a-smartwa.jpg?id=52961422&amp;width=980" width="1800"><small placeholder="Add Photo Caption...">While transparent displays are stunning, they might not be practical for home use as televisions. Expect to see them adopted first as signage in retail settings.  </small><small placeholder="Add Photo Credit...">AUO </small></p><p>
	These tiny LED chips block very little light, making the display more transparent. The Taiwanese display maker 
	<a href="https://www.auo.com/en-global/products/index/Display_Panel_Products" target="_blank">AUO</a> recently demonstrated a microLED display with more than 60 percent transparency.
</p><p>
	Oxygen and moisture don’t affect microLEDs, so they don’t need to be encapsulated. This makes it possible to tile smaller panels to create a seamless larger display. And the silicon coating on such small panels can be annealed to create polysilicon, which performs better than IGZO, so the transistors can be even smaller and block less light.
</p><p>
	But the microLED approach has its own problems. Indeed, the technology is still in its infancy, with costing a great deal to manufacture and requiring some contortions to get uniform brightness and color across the entire display.
</p><p>
	For example, individual OLED materials emit a well-defined color, but that’s not the case for LEDs. Minute variations in the physical characteristics of an LED chip can alter the wavelength of light it emits by a measurable—and noticeable—amount. Manufacturers have typically addressed this challenge by using a binning process: They test thousands of chips and then group them into bins of similar wavelengths, discarding those that don’t fit the desired ranges. This explains in part why those large digital LED screens are so expensive: Many LEDs created for their construction must be discarded.
</p><p>
	But binning doesn’t really work when dealing with microLEDs. The tiny chips are difficult to test and are so expensive that costs would be astronomical if too many had to be rejected.
</p><p><img alt="A person wearing a white shirt with red text and a name badge is placing his hand behind a transparent display screen. The screen shows an image of splashing liquid and fire." data-rm-shortcode-id="264e05220f2d2f38c1e88cf9305f66c3" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/a-person-wearing-a-white-shirt-with-red-text-and-a-name-badge-is-placing-his-hand-behind-a-transparent-display-screen-the-scree.png?id=52961443&amp;width=980" height="1913" id="f741b" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/a-person-wearing-a-white-shirt-with-red-text-and-a-name-badge-is-placing-his-hand-behind-a-transparent-display-screen-the-scree.png?id=52961443&amp;width=980" width="2550"><small placeholder="Add Photo Caption...">Though you can see through today’s transparent displays, they do block a noticeable amount of light, making the background darker than when viewed directly. </small><small placeholder="Add Photo Credit...">Tekla S. Perry </small></p><p>
	Instead, manufacturers test microLED displays for uniformity after they’re assembled, then calibrate them to adjust the current applied to each subpixel so that color and brightness are uniform across the display. This calibration process, which involves scanning an image on the panel and then reprogramming the control circuitry, can sometimes require thousands of iterations.
</p><p>
	Then there’s the problem of assembling the panels. Remember those 25 million microLED chips that make up a 4K display? Each must be positioned precisely, and each must be connected to the correct electrical contacts.
</p><p>
	The LED chips are initially fabricated on sapphire wafers, each of which contains chips of only one color. These chips must be transferred from the wafer to a carrier to hold them temporarily before applying them to the panel backplane. The Taiwanese microLED company 
	<a href="https://www.playnitride.com/en/" target="_blank">PlayNitride</a> has developed a process for creating large tiles with chips spaced less than 2 micrometers apart. Its process for positioning these tiny chips has better than 99.9 percent yields. But even at a 99.9 percent yield, you can expect about 25,000 defective subpixels in a 4K display. They might be positioned incorrectly so that no electrical contact is made, or the wrong color chip is placed in the pattern, or a subpixel chip might be defective. While correcting these defects is sometimes possible, doing so just adds to the already high cost.
</p><p><img alt="A person looks at a transparent micro led screen displaying splashes of liquid in red, yellow, and green. " data-rm-shortcode-id="d4c2f8d71053ccb191750db6acaa2024" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/a-person-looks-at-a-transparent-micro-led-screen-displaying-splashes-of-liquid-in-red-yellow-and-green.png?id=52961477&amp;width=980" height="1800" id="1e653" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/a-person-looks-at-a-transparent-micro-led-screen-displaying-splashes-of-liquid-in-red-yellow-and-green.png?id=52961477&amp;width=980" width="2700"><small placeholder="Add Photo Caption...">Samsung’s microLED technology allows the image to extend right up to the edge of the glass panel, making it possible to create larger displays by tiling smaller panels together.  </small><small placeholder="Add Photo Credit...">Brendan Smialowski/AFP/Getty Images </small></p><p>
	Could MicroLEDs still be the future of flat-panel displays? “Every display analyst I know believes that microLEDs should be the ‘next big thing’ because of their brightness, efficiency, color, viewing angles, response times, and lifetime, “ says Bob Raikes, editor of the 
	<a href="https://8kassociation.com/8k-monitor-newsletters/" target="_blank"><em><em>8K Monitor</em></em></a> newsletter. “However, the practical hurdles of bringing them to market remain huge. That Apple, which has the deepest pockets of all, has abandoned microLEDs, at least for now, and after billions of dollars in investment, suggests that mass production for consumer markets is still a long way off.”
</p><p>
	At this juncture, even though microLED technology offers some clear advantages, OLED is more cost-effective and holds the early lead for practical applications of transparent displays.
</p><h2>But what is a transparent display good for?</h2><p>
	Samsung and LG aren’t the only companies to have demonstrated transparent panels recently.
</p><p>
	AUO’s 60-inch transparent display, made of tiled panels, won the 
	<a href="https://auo.com/en-global/New_Archive/detail/news_awards_20240517" target="_blank"><u>People’s Choice Award</u></a> for Best MicroLED-Based Technology at the Society for Information Display’s Display Week, held in May in San Jose, Calif. And the Chinese company BOE Technology Group demonstrated <a href="https://ces.vporoom.com/2024-01-12-BOE-Showcases-Cutting-Edge-Display-Technologies-at-the-CES-2024-and-Inspires-Innovative-Smart-Life-with-Partners" target="_blank">a 49-inch transparent OLED display</a> at CES 2024.
</p><p>
	These transparent displays all have one feature in common: They will be insanely expensive. Only LG’s transparent OLED display has been announced as a commercial product. It’s without a price or a ship date at this point, but it’s not hard to guess how costly it will be, given that nontransparent versions are expensive enough. For example, LG prices its top-end 77-inch OLED TV at US $4,500.
</p><p data-rm-resized-container="25%"><img alt="A diagram of the structure of a display pixel represented as a grey rectangle, which frames an open area labeled transmissive space, and three rectangular blocks labeled R, G, and B." data-rm-shortcode-id="3926328fa04e8ce10f5bce25ed40190b" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/a-diagram-of-the-structure-of-a-display-pixel-represented-as-a-grey-rectangle-which-frames-an-open-area-labeled-transmissive-sp.png?id=52961486&amp;width=980" height="1484" id="eea2a" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/a-diagram-of-the-structure-of-a-display-pixel-represented-as-a-grey-rectangle-which-frames-an-open-area-labeled-transmissive-sp.png?id=52961486&amp;width=980" width="1512"><small placeholder="Add Photo Caption...">Displays using both microLED technology [above] and OLED technology have some components in each pixel that block light coming from the background. These include the red, green, and blue emissive materials along with the transistors required to switch them on and off. Smaller components mean that you can have a larger transmissive space that will provide greater transparency.  </small><small placeholder="Add Photo Credit...">Illustration: Mark Montgomery; Source: Samsung</small></p><p>
	Thanks to seamless tiling, transparent microLED displays can be larger than their OLED counterparts. But their production costs are larger as well. Much larger. And that is reflected in prices. For example, Samsung’s nontransparent 114-inch microLED TV sells for $150,000. We can reasonably expect transparent models to cost even more.
</p><p>
	Seeing these prices, you really have to ask: What are the practical applications of transparent displays?
</p><p>
	Don’t expect these displays to show up in many living rooms as televisions. And high price is not the only reason. After all, who wants to see their bookshelves showing through in the background while they’re watching 
	<em><em>Dune</em></em>? That’s why the transparent OLED TV LG demonstrated at CES 2024 included a “contrast layer”—basically, a black cloth—that unrolls and covers the back of the display on demand.
</p><p>
	Transparent displays could have a place on the desktop—not so you can see through them, but so that a camera can sit behind the display, capturing your image while you’re looking directly at the screen. This would help you maintain eye contact during a Zoom call. One company—<a href="https://veeonow.com/" target="_blank">Veeo</a>—demonstrated a prototype of such a product <a href="https://www.newswire.com/news/veeo-joins-lg-display-in-partnership-to-create-the-first-behind-22057991" target="_blank">at CES 2024</a>, and it plans to release a 30-inch model for about $3,000 and a 55-inch model for about $8,500 later this year. Veeo’s products use LG’s transparent OLED technology.
</p><p>
	Transparent screens are already showing up as signage and other public-information displays. LG has 
	<a href="https://www.lgcorp.com/media/release/27627" target="_blank">installed transparent 55-inch OLED panels</a> in the windows of Seoul’s new high-speed underground rail cars, which are part of a system known as the Great Train eXpress. Riders can browse maps and other information on these displays, which can be made clear when needed for passengers to see what’s outside.
</p><p>
	LG transparent panels have also been featured in 
	<a href="https://news.lgdisplay.com/en/2022/01/rising-potential-of-transparent-oled/" target="_blank">an E35e excavator prototype</a> by Doosan Bobcat. This touchscreen display can act as the operator’s front or side window, showing important machine data or displaying real-time images from cameras mounted on the vehicle. Such transparent displays can serve a similar function as the head-up displays in some aircraft windshields.
</p><p>
	And so, while the large transparent displays are striking, you’ll be more likely to see them initially as displays for machinery operators, public entertainment, retail signage, and even car windshields. The early adopters might cover the costs of developing mass-production processes, which in turn could drive prices down. But even if costs eventually reach reasonable levels, whether the average consumer really want a transparent TV in their home is something that remains to be seen—unlike the device itself, whose whole point is not to be. 
	<span></span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SAM 2: Segment Anything in Images and Videos (602 pts)]]></title>
            <link>https://github.com/facebookresearch/segment-anything-2</link>
            <guid>41104523</guid>
            <pubDate>Mon, 29 Jul 2024 22:52:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/facebookresearch/segment-anything-2">https://github.com/facebookresearch/segment-anything-2</a>, See on <a href="https://news.ycombinator.com/item?id=41104523">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">SAM 2: Segment Anything in Images and Videos</h2><a id="user-content-sam-2-segment-anything-in-images-and-videos" aria-label="Permalink: SAM 2: Segment Anything in Images and Videos" href="#sam-2-segment-anything-in-images-and-videos"></a></p>
<p dir="auto"><strong><a href="https://ai.meta.com/research/" rel="nofollow">AI at Meta, FAIR</a></strong></p>
<p dir="auto"><a href="https://nikhilaravi.com/" rel="nofollow">Nikhila Ravi</a>, <a href="https://gabeur.github.io/" rel="nofollow">Valentin Gabeur</a>, <a href="https://scholar.google.com/citations?user=E8DVVYQAAAAJ&amp;hl=en" rel="nofollow">Yuan-Ting Hu</a>, <a href="https://ronghanghu.com/" rel="nofollow">Ronghang Hu</a>, <a href="https://scholar.google.com/citations?user=4LWx24UAAAAJ&amp;hl=en" rel="nofollow">Chaitanya Ryali</a>, <a href="https://scholar.google.com/citations?user=VeTSl0wAAAAJ&amp;hl=en" rel="nofollow">Tengyu Ma</a>, <a href="https://hkhedr.com/" rel="nofollow">Haitham Khedr</a>, <a href="https://scholar.google.de/citations?user=Tpt57v0AAAAJ&amp;hl=en" rel="nofollow">Roman Rädle</a>, <a href="https://scholar.google.com/citations?hl=fr&amp;user=n-SnMhoAAAAJ" rel="nofollow">Chloe Rolland</a>, <a href="https://scholar.google.com/citations?user=c8IpF9gAAAAJ&amp;hl=en" rel="nofollow">Laura Gustafson</a>, <a href="https://ericmintun.github.io/" rel="nofollow">Eric Mintun</a>, <a href="https://junting.github.io/" rel="nofollow">Junting Pan</a>, <a href="https://scholar.google.co.in/citations?user=m34oaWEAAAAJ&amp;hl=en" rel="nofollow">Kalyan Vasudev Alwala</a>, <a href="https://www.nicolascarion.com/" rel="nofollow">Nicolas Carion</a>, <a href="https://chaoyuan.org/" rel="nofollow">Chao-Yuan Wu</a>, <a href="https://www.rossgirshick.info/" rel="nofollow">Ross Girshick</a>, <a href="https://pdollar.github.io/" rel="nofollow">Piotr Dollár</a>, <a href="https://feichtenhofer.github.io/" rel="nofollow">Christoph Feichtenhofer</a></p>
<p dir="auto">[<a href="https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/" rel="nofollow"><code>Paper</code></a>] [<a href="https://ai.meta.com/sam2" rel="nofollow"><code>Project</code></a>] [<a href="https://sam2.metademolab.com/" rel="nofollow"><code>Demo</code></a>] [<a href="https://ai.meta.com/datasets/segment-anything-video" rel="nofollow"><code>Dataset</code></a>] [<a href="https://ai.meta.com/blog/segment-anything-2" rel="nofollow"><code>Blog</code></a>] [<a href="#citing-sam-2"><code>BibTeX</code></a>]</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/facebookresearch/segment-anything-2/blob/main/assets/model_diagram.png?raw=true"><img src="https://github.com/facebookresearch/segment-anything-2/raw/main/assets/model_diagram.png?raw=true" alt="SAM 2 architecture"></a></p>
<p dir="auto"><strong>Segment Anything Model 2 (SAM 2)</strong> is a foundation model towards solving promptable visual segmentation in images and videos. We extend SAM to video by considering images as a video with a single frame. The model design is a simple transformer architecture with streaming memory for real-time video processing. We build a model in the loop data engine, which improves model and data via user interaction, to collect <a href="https://ai.meta.com/datasets/segment-anything-video" rel="nofollow"><strong>our SA-V dataset</strong></a>, the largest video segmentation dataset to date. SAM 2 trained on our data provides strong performance across a wide range of tasks and visual domains.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/facebookresearch/segment-anything-2/blob/main/assets/sa_v_dataset.jpg?raw=true"><img src="https://github.com/facebookresearch/segment-anything-2/raw/main/assets/sa_v_dataset.jpg?raw=true" alt="SA-V dataset"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">Please install SAM 2 on a GPU machine using:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone git@github.com:facebookresearch/segment-anything-2.git

cd segment-anything-2; pip install -e ."><pre>git clone git@github.com:facebookresearch/segment-anything-2.git

<span>cd</span> segment-anything-2<span>;</span> pip install -e <span>.</span></pre></div>
<p dir="auto">To use the SAM 2 predictor and run the example notebooks, <code>jupyter</code> and <code>matplotlib</code> are required and can be installed by:</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Download Checkpoints</h3><a id="user-content-download-checkpoints" aria-label="Permalink: Download Checkpoints" href="#download-checkpoints"></a></p>
<p dir="auto">First, we need to download a model checkpoint. All the model checkpoints can be downloaded by running:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd checkpoints
./download_ckpts.sh"><pre><span>cd</span> checkpoints
./download_ckpts.sh</pre></div>
<p dir="auto">or individually from:</p>
<ul dir="auto">
<li><a href="https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_tiny.pt" rel="nofollow">sam2_hiera_tiny.pt</a></li>
<li><a href="https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_small.pt" rel="nofollow">sam2_hiera_small.pt</a></li>
<li><a href="https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_base_plus.pt" rel="nofollow">sam2_hiera_base_plus.pt</a></li>
<li><a href="https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt" rel="nofollow">sam2_hiera_large.pt</a></li>
</ul>
<p dir="auto">Then SAM 2 can be used in a few lines as follows for image and video prediction.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Image prediction</h3><a id="user-content-image-prediction" aria-label="Permalink: Image prediction" href="#image-prediction"></a></p>
<p dir="auto">SAM 2 has all the capabilities of <a href="https://github.com/facebookresearch/segment-anything">SAM</a> on static images, and we provide image prediction APIs that closely resemble SAM for image use cases. The <code>SAM2ImagePredictor</code> class has an easy interface for image prompting.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
from sam2.build_sam import build_sam2
from sam2.sam2_image_predictor import SAM2ImagePredictor

checkpoint = &quot;./checkpoints/sam2_hiera_large.pt&quot;
model_cfg = &quot;sam2_hiera_l.yaml&quot;
predictor = SAM2ImagePredictor(build_sam2(model_cfg, checkpoint))

with torch.inference_mode(), torch.autocast(&quot;cuda&quot;, dtype=torch.bfloat16):
    predictor.set_image(<your_image>)
    masks, _, _ = predictor.predict(<input_prompts>)"><pre><span>import</span> <span>torch</span>
<span>from</span> <span>sam2</span>.<span>build_sam</span> <span>import</span> <span>build_sam2</span>
<span>from</span> <span>sam2</span>.<span>sam2_image_predictor</span> <span>import</span> <span>SAM2ImagePredictor</span>

<span>checkpoint</span> <span>=</span> <span>"./checkpoints/sam2_hiera_large.pt"</span>
<span>model_cfg</span> <span>=</span> <span>"sam2_hiera_l.yaml"</span>
<span>predictor</span> <span>=</span> <span>SAM2ImagePredictor</span>(<span>build_sam2</span>(<span>model_cfg</span>, <span>checkpoint</span>))

<span>with</span> <span>torch</span>.<span>inference_mode</span>(), <span>torch</span>.<span>autocast</span>(<span>"cuda"</span>, <span>dtype</span><span>=</span><span>torch</span>.<span>bfloat16</span>):
    <span>predictor</span>.<span>set_image</span>(<span>&lt;</span><span>your_image</span><span>&gt;</span>)
    <span>masks</span>, <span>_</span>, <span>_</span> <span>=</span> <span>predictor</span>.<span>predict</span>(<span>&lt;</span><span>input_prompts</span><span>&gt;</span>)</pre></div>
<p dir="auto">Please refer to the examples in <a href="https://github.com/facebookresearch/segment-anything-2/blob/main/notebooks/image_predictor_example.ipynb">image_predictor_example.ipynb</a> for static image use cases.</p>
<p dir="auto">SAM 2 also supports automatic mask generation on images just like SAM. Please see <a href="https://github.com/facebookresearch/segment-anything-2/blob/main/notebooks/automatic_mask_generator_example.ipynb">automatic_mask_generator_example.ipynb</a> for automatic mask generation in images.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Video prediction</h3><a id="user-content-video-prediction" aria-label="Permalink: Video prediction" href="#video-prediction"></a></p>
<p dir="auto">For promptable segmentation and tracking in videos, we provide a video predictor with APIs for example to add prompts and propagate masklets throughout a video. SAM 2 supports video inference on multiple objects and uses an inference state to keep track of the interactions in each video.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
from sam2.build_sam import build_sam2_video_predictor

checkpoint = &quot;./checkpoints/sam2_hiera_large.pt&quot;
model_cfg = &quot;sam2_hiera_l.yaml&quot;
predictor = build_sam2_video_predictor(model_cfg, checkpoint)

with torch.inference_mode(), torch.autocast(&quot;cuda&quot;, dtype=torch.bfloat16):
    state = predictor.init_state(<your_video>)

    # add new prompts and instantly get the output on the same frame
    frame_idx, object_ids, masks = predictor.add_new_points(state, <your prompts>):

    # propagate the prompts to get masklets throughout the video
    for frame_idx, object_ids, masks in predictor.propagate_in_video(state):
        ..."><pre><span>import</span> <span>torch</span>
<span>from</span> <span>sam2</span>.<span>build_sam</span> <span>import</span> <span>build_sam2_video_predictor</span>

<span>checkpoint</span> <span>=</span> <span>"./checkpoints/sam2_hiera_large.pt"</span>
<span>model_cfg</span> <span>=</span> <span>"sam2_hiera_l.yaml"</span>
<span>predictor</span> <span>=</span> <span>build_sam2_video_predictor</span>(<span>model_cfg</span>, <span>checkpoint</span>)

<span>with</span> <span>torch</span>.<span>inference_mode</span>(), <span>torch</span>.<span>autocast</span>(<span>"cuda"</span>, <span>dtype</span><span>=</span><span>torch</span>.<span>bfloat16</span>):
    <span>state</span> <span>=</span> <span>predictor</span>.<span>init_state</span>(<span>&lt;</span><span>your_video</span><span>&gt;</span>)

    <span># add new prompts and instantly get the output on the same frame</span>
    <span>frame_idx</span>, <span>object_ids</span>, <span>masks</span> <span>=</span> <span>predictor</span>.<span>add_new_points</span>(<span>state</span>, <span>&lt;</span><span>your</span> <span>prompts</span><span>&gt;</span>):

    <span># propagate the prompts to get masklets throughout the video</span>
    <span>for</span> <span>frame_idx</span>, <span>object_ids</span>, <span>masks</span> <span>in</span> <span>predictor</span>.<span>propagate_in_video</span>(<span>state</span>):
        ...</pre></div>
<p dir="auto">Please refer to the examples in <a href="https://github.com/facebookresearch/segment-anything-2/blob/main/notebooks/video_predictor_example.ipynb">video_predictor_example.ipynb</a> for details on how to add prompts, make refinements, and track multiple objects in videos.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Model Description</h2><a id="user-content-model-description" aria-label="Permalink: Model Description" href="#model-description"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th><strong>Model</strong></th>
<th><strong>Size (M)</strong></th>
<th><strong>Speed (FPS)</strong></th>
<th><strong>SA-V test (J&amp;F)</strong></th>
<th><strong>MOSE val (J&amp;F)</strong></th>
<th><strong>LVOS v2 (J&amp;F)</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>sam2_hiera_tiny</td>
<td>38.9</td>
<td>47.2</td>
<td>75.0</td>
<td>70.9</td>
<td>75.3</td>
</tr>
<tr>
<td>sam2_hiera_small</td>
<td>46</td>
<td>43.3 (53.0 compiled*)</td>
<td>74.9</td>
<td>71.5</td>
<td>76.4</td>
</tr>
<tr>
<td>sam2_hiera_base_plus</td>
<td>80.8</td>
<td>34.8 (43.8 compiled*)</td>
<td>74.7</td>
<td>72.8</td>
<td>75.8</td>
</tr>
<tr>
<td>sam2_hiera_large</td>
<td>224.4</td>
<td>24.2 (30.2 compiled*)</td>
<td>76.0</td>
<td>74.6</td>
<td>79.8</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">* Compile the model by setting <code>compile_image_encoder: True</code> in the config.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Segment Aything Video Dataset</h2><a id="user-content-segment-aything-video-dataset" aria-label="Permalink: Segment Aything Video Dataset" href="#segment-aything-video-dataset"></a></p>
<p dir="auto">See <a href="https://github.com/facebookresearch/segment-anything-2/blob/main/sav_dataset/README.md">sav_dataset/README.md</a> for details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">The models are licensed under the <a href="https://github.com/facebookresearch/segment-anything-2/blob/main/LICENSE">Apache 2.0 license</a>. Please refer to our research paper for more details on the models.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">See <a href="https://github.com/facebookresearch/segment-anything-2/blob/main/CONTRIBUTING.md">contributing</a> and the <a href="https://github.com/facebookresearch/segment-anything-2/blob/main/CODE_OF_CONDUCT.md">code of conduct</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributors</h2><a id="user-content-contributors" aria-label="Permalink: Contributors" href="#contributors"></a></p>
<p dir="auto">The SAM 2 project was made possible with the help of many contributors (alphabetical):</p>
<p dir="auto">Karen Bergan, Daniel Bolya, Alex Bosenberg, Kai Brown, Vispi Cassod, Christopher Chedeau, Ida Cheng, Luc Dahlin, Shoubhik Debnath, Rene Martinez Doehner, Grant Gardner, Sahir Gomez, Rishi Godugu, Baishan Guo, Caleb Ho, Andrew Huang, Somya Jain, Bob Kamma, Amanda Kallet, Jake Kinney, Alexander Kirillov, Shiva Koduvayur, Devansh Kukreja, Robert Kuo, Aohan Lin, Parth Malani, Jitendra Malik, Mallika Malhotra, Miguel Martin, Alexander Miller, Sasha Mitts, William Ngan, George Orlin, Joelle Pineau, Kate Saenko, Rodrick Shepard, Azita Shokrpour, David Soofian, Jonathan Torres, Jenny Truong, Sagar Vaze, Meng Wang, Claudette Ward, Pengchuan Zhang.</p>
<p dir="auto">Third-party code: we use a GPU-based connected component algorithm adapted from <a href="https://github.com/zsef123/Connected_components_PyTorch"><code>cc_torch</code></a> (with its license in <a href="https://github.com/facebookresearch/segment-anything-2/blob/main/LICENSE_cctorch"><code>LICENSE_cctorch</code></a>) as an optional post-processing step for the mask predictions.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Citing SAM 2</h2><a id="user-content-citing-sam-2" aria-label="Permalink: Citing SAM 2" href="#citing-sam-2"></a></p>
<p dir="auto">If you use SAM 2 or the SA-V dataset in your research, please use the following BibTeX entry.</p>
<div dir="auto" data-snippet-clipboard-copy-content="@article{ravi2024sam2,
  title={SAM 2: Segment Anything in Images and Videos},
  author={Ravi, Nikhila and Gabeur, Valentin and Hu, Yuan-Ting and Hu, Ronghang and Ryali, Chaitanya and Ma, Tengyu and Khedr, Haitham and R{\&quot;a}dle, Roman and Rolland, Chloe and Gustafson, Laura and Mintun, Eric and Pan, Junting and Alwala, Kalyan Vasudev and Carion, Nicolas and Wu, Chao-Yuan and Girshick, Ross and Doll{\'a}r, Piotr and Feichtenhofer, Christoph},
  journal={arXiv preprint},
  year={2024}
}"><pre><span>@article</span>{<span>ravi2024sam2</span>,
  <span>title</span>=<span><span>{</span>SAM 2: Segment Anything in Images and Videos<span>}</span></span>,
  <span>author</span>=<span><span>{</span>Ravi, Nikhila and Gabeur, Valentin and Hu, Yuan-Ting and Hu, Ronghang and Ryali, Chaitanya and Ma, Tengyu and Khedr, Haitham and R{\"a}dle, Roman and Rolland, Chloe and Gustafson, Laura and Mintun, Eric and Pan, Junting and Alwala, Kalyan Vasudev and Carion, Nicolas and Wu, Chao-Yuan and Girshick, Ross and Doll{\'a}r, Piotr and Feichtenhofer, Christoph<span>}</span></span>,
  <span>journal</span>=<span><span>{</span>arXiv preprint<span>}</span></span>,
  <span>year</span>=<span><span>{</span>2024<span>}</span></span>
}</pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FastHTML – Modern web applications in pure Python (561 pts)]]></title>
            <link>https://fastht.ml/</link>
            <guid>41104305</guid>
            <pubDate>Mon, 29 Jul 2024 22:18:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fastht.ml/">https://fastht.ml/</a>, See on <a href="https://news.ycombinator.com/item?id=41104305">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<header>
  <nav>
    <a href="#">
      <img src="https://fastht.ml/assets/logo.svg" alt="FastHTML" width="105" height="24">
    </a>
    <a href="https://docs.fastht.ml/" target="_blank" rel="noopener noreferrer">Read docs</a>
  </nav>
</header>

<div>
      <h2>Modern web applications in pure Python</h2>
      <p>Built on solid web foundations, not the latest fads - with
FastHTML you can get started on anything from simple dashboards to
scalable web applications in minutes.</p>
    </div>
  <div>
      <div>
        <h2>This home page is a FastHTML app.</h2>
        <p>Click the buttons below to see four small, live components in action.</p>
      </div>
      
      
      
      
      <ul role="tablist" id="tab-list">
        <li role="tab" aria-selected="true">
          
        </li>
        <li role="tab" aria-selected="false">
          
        </li>
        <li role="tab" aria-selected="false">
          
        </li>
        <li role="tab" aria-selected="false">
          
        </li>
        
      </ul>
    </div>
  <section>
<div>
    <p>GET STARTED IN MINUTES</p>
    <h2>The fastest way to create a real web application.</h2>
    <p>With FastHTML you create good-looking modern web applications in pure Python and deploy them in minutes.</p>
  </div>

<div>
  <div>
    <h3>Get started fast</h3>
    <p>A single Python file is all that's needed to create any app you can think of. Or bring in any Python or JS library you like.</p>
  </div>
  <div>
    <h3>Flexibility</h3>
    <p>FastHTML provides full access to HTTP, HTML, JS, and CSS, bringing the foundations of the web to you. There's no limits to what you can build.</p>
  </div>
  <div>
    <h3>Speed &amp; scale</h3>
    <p>FastHTML applications are fast and scalable. They're also easy to deploy, since you can use any hosting service that supports Python.</p>
  </div>
</div>
  </section>
  <section id="stacked-cards-section">
    <div>
          <p>TECH STACK</p>
          <h2>FastHTML scales up and scales down.</h2>
          <p>
Read more about our 
            <a href="https://about.fastht.ml/vision" target="_blank" rel="noopener noreferrer">design philosophy here</a>
, or click a button below:
          </p>
        </div>
    <div id="stacked-cards">
        <div>
            <h3>Build on solid foundations</h3>
            <p>FastHTML stands on the shoulders of giants:</p>
            
          </div>
        <div>
            <h3>Use tools you already know</h3>
            <p>FastHTML embraces the familiar:</p>
            
          </div>
        <div>
            <h3>Deploy anywhere</h3>
            <p>FastHTML runs anywhere Python does, including 1-click deploy to:</p>
            
          </div>
      </div>
  </section>
  <section>
<div>
  <p>SAMPLES</p>
  <h2>See FastHTML in action</h2>
  <p>FastHTML can be used for everything from collaborative games to multi-modal UI. We've selected small self-contained examples for you to learn from.</p>
</div>



<a href="https://github.com/AnswerDotAI/fasthtml-example/tree/main">Discover all</a>
  </section>
  <div>
      <div>
        <p>FAQ</p>
        <h2>Questions? Answers.</h2>
        <p>Your top FastHTML questions clarified.</p>
      </div>
      <div>
        <div>
          <p>
          <label for="collapsible-3">
            <p>What kinds of applications can be written with this?</p>
            <img src="https://fastht.ml/assets/icons/plus-icon.svg" alt="Expand">
            <img src="https://fastht.ml/assets/icons/minus-icon.svg" alt="Collapse">
          </label></p><p>It's good for: general purpose web applications (i.e anything you'd build with React, Django, NexJS, etc); quick dashboards, prototypes, and in-company apps (e.g. like what you might use gradio/streamlit/etc for); Analytics/models/dashboards interactive reports; Custom blogs and content-heavy sites where you also want some interactive/dynamic content.</p>
        </div>
        <div>
          <p>
          <label for="collapsible-4">
            <p>Where can I deploy my FastHTML to? What's needed?</p>
            <img src="https://fastht.ml/assets/icons/plus-icon.svg" alt="Expand">
            <img src="https://fastht.ml/assets/icons/minus-icon.svg" alt="Collapse">
          </label></p><p>You can deploy a FastHTML app to any service or server that supports Python. We have guides and helpers for Railway.app, Vercel, Hugging Face Spaces, Replit, and PythonAnywhere. You can also use any VPS or server, or any on-premise machine with Python installed. All major operating systems are supported.</p>
        </div>
        <div>
          <p>
          <label for="collapsible-5">
            <p>How does FastHTML relate to FastAPI?</p>
            <img src="https://fastht.ml/assets/icons/plus-icon.svg" alt="Expand">
            <img src="https://fastht.ml/assets/icons/minus-icon.svg" alt="Collapse">
          </label></p><p>FastAPI is one of the inspirations for FastHTML. We are fans of its developer experience and tried to make FastHTML extremely familiar for FastAPI users. FastAPI is designed for creating APIs, whereas FastHTML is designed for creating HTML (i.e "Hypermedia applications"). Anything you could create with FastAPI (plus a JS frontend), you could also create with FastHTML, and vice versa -- if you prefer mainly writing JS, you might prefer FastAPI, since you can move a lot of client-side logic into the JS. If you prefer mainly writing Python, you'll probably want to use FastHTML, since you can often avoid using JS entirely.</p>
        </div>
        <div>
          <p>
          <label for="collapsible-6">
            <p>Is this only for multi-page "old style" web apps, or can FastHTML be used for modern SPA apps too?</p>
            <img src="https://fastht.ml/assets/icons/plus-icon.svg" alt="Expand">
            <img src="https://fastht.ml/assets/icons/minus-icon.svg" alt="Collapse">
          </label></p><p>FastHTML is specifically designed to make writing modern SPA apps as fast and easy as possible, whilst also ensuring the apps you write are scalable and performant. By default, FastHTML routes return lightweight "partials" that update the DOM directly, rather than doing a full page refresh.</p>
        </div>
        <div>
          <p>
          <label for="collapsible-7">
            <p>What is HTMX, and what's it go to do with FastHTML?</p>
            <img src="https://fastht.ml/assets/icons/plus-icon.svg" alt="Expand">
            <img src="https://fastht.ml/assets/icons/minus-icon.svg" alt="Collapse">
          </label></p><p>HTMX is best thought of as filling in the missing bits of a web browser -- in fact, web browser manufacturers are considering incorporating similar features directly into future browsers. It is a small javascript library that with a single line of HTML lets you respond to any event from any part of a web page by modifying the DOM in any way you like, all directly from Python. Whilst you don't have to use it with FastHTML, it will dramatically increase the amount of stuff you can do!</p>
        </div>
        <div>
          <p>
          <label for="collapsible-8">
            <p>Do I need to know JS? Can I use it if I want, with FastHTML?</p>
            <img src="https://fastht.ml/assets/icons/plus-icon.svg" alt="Expand">
            <img src="https://fastht.ml/assets/icons/minus-icon.svg" alt="Collapse">
          </label></p><p>No, and yes! You can write nearly any standard web app with just Python. However, using a bit of JS can be helpful -- for instance, nearly any existing JS lib can be incorporated into a FastHTML app, and you can sprinkle bits of JS into your pages anywhere you like.</p>
        </div>
        <div>
          <p>
          <label for="collapsible-9">
            <p>Are FastHTML apps slower than React, Next.JS, etc?</p>
            <img src="https://fastht.ml/assets/icons/plus-icon.svg" alt="Expand">
            <img src="https://fastht.ml/assets/icons/minus-icon.svg" alt="Collapse">
          </label></p><p>It depends. Apps using FastHTML and HTMX are often faster than JS-based approaches using big libraries, since they can be very lightweight.</p>
        </div>
      </div>
    </div>
  <div>
        <p>LOVE IS IN THE AIR</p>
        <h2>What the experts say</h2>
        <p>Top web programmers tell us that they love working with FastHTML.</p>
      </div>
  
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Running One-man SaaS for 9 Years (662 pts)]]></title>
            <link>https://blog.healthchecks.io/2024/07/running-one-man-saas-9-years-in/</link>
            <guid>41104293</guid>
            <pubDate>Mon, 29 Jul 2024 22:15:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.healthchecks.io/2024/07/running-one-man-saas-9-years-in/">https://blog.healthchecks.io/2024/07/running-one-man-saas-9-years-in/</a>, See on <a href="https://news.ycombinator.com/item?id=41104293">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
<main id="main">
<article id="post-1520" itemtype="https://schema.org/CreativeWork" itemscope="itemscope">
<div itemprop="text">
<p>Healthchecks.io launched in July 2015, which means this year we turn 9. Time flies!</p>
<p>Previous status updates:</p>
<ul>
<li>In 2018, <a href="https://blog.healthchecks.io/2018/08/my-one-person-saas-side-project-celebrates-its-third-birthday/">My One-person SaaS Side Project Celebrates its Third Birthday</a></li>
<li>In 2021, <a href="https://blog.healthchecks.io/2021/07/healthchecks-turns-6-status-update/">Healthchecks Turns 6, Status Update</a></li>
</ul>
<h3>Money</h3>
<p>Healthchecks.io currently has 652 paying customers, and the monthly recurring revenue is 14043 USD. MRR graph:</p>
<figure><img fetchpriority="high" decoding="async" width="1024" height="393" src="https://blog.healthchecks.io/wp-content/uploads/2024/07/mrr-1024x393.png" alt=""></figure>
<p>Side note: to minimize the number of data sub-processors, I am not using revenue analytics services. I used a script and a spreadsheet to make the MRR graph!</p>
<p>I’m happy to see MRR gradually go up, but I’m not optimizing for it. Healthchecks.io is sustainable as-is, and so I’m optimizing for enjoyment and life/work balance.</p>
<p>More stats (user count, check count, pings/day) are available on the <a href="https://healthchecks.io/about/">Healthchecks.io About page</a>.</p>
<h3>Still a one-man business?</h3>
<p>Yes, Healthchecks.io is still a one-man business. Until 2022, I was part-time contracting. Since January 2022 Healthchecks.io has been my only source of income, but I work on it part-time.</p>
<p>At least for the time being I’m not looking to expand the team. A large part of why I’m a “solopreneur” is because I do not want to manage or be managed. A cofounder or employee would mean regular meetings to discuss what’s done, and what’s to be done. It would be awesome to find someone who just magically does great work without needing any attention. Just brief monthly summaries of high-quality contributions, better than I could have done. But I don’t think I can find someone like that, and I also don’t think I could afford them.</p>
<h3>Growth Goals</h3>
<p>I’m not planning to tighten the limits of the free plans. I started Healthchecks in 2015 because I thought the existing services (Dead Man’s Snitch and Cronitor) were overpriced. I started with “I think this can be done better and cheaper”, and I’m sticking with it.</p>
<p>For the same reason, I’m also not planning to raise pricing for paid plans.</p>
<p>I’m choosing not to pursue enterprise customers who ask about PO billing, payments by wire transfer, custom agreements, and signing up to vendor portals. “But you are leaving money on the table!” – yes, it is a conscious decision. In my situation, the extra money will not make a meaningful difference, but the additional burden will make me more busy and grumpy.</p>
<p>Feature-wise, I am happy with the current scope and feature set of Healthchecks. I am <em>not</em> planning to expand the scope and add e.g. active uptime monitoring, hosted status pages, or APM features.</p>
<p>Healthchecks the product is <a href="https://hachyderm.io/@danderson/112766460393943288">hobbit software</a> and Healthchecks.io the business is a lifestyle business.</p>
<h3>Hosting Setup</h3>
<p>The hosting setup is mostly the same as in <a href="https://blog.healthchecks.io/2022/02/healthchecks-io-hosting-setup-2022-edition/">2022</a>. Just a few updates:</p>
<ul>
<li>Web servers upgraded to Hetzner’s AX42 (AMD 8700GE, 8 cores). On the old machines, saw a few nonsensical Python exceptions. A kernel update and a reboot didn’t fix it. Rather than messing with hardware troubleshooting, I upgraded to newer, faster, and more efficient machines.</li>
<li>Database servers upgraded to Hetzner’s EX101 (Intel 13900, 8+16 cores). I was setting up new database replicas after <a href="https://status.healthchecks.io/en/incidents/m7Qv7s8KCsVdMVjGvMbpJb/">an outage and failover event</a> and took the opportunity to upgrade hardware.</li>
<li>Healthchecks.io <a href="https://blog.healthchecks.io/2023/08/notes-on-self-hosted-transactional-email/">now sends its own email using maddy</a>.</li>
<li>Healthchecks.io <a href="https://blog.healthchecks.io/2022/04/we-moved-some-data-to-s3/">now stores ping body data in S3-compatible object storage</a>. This keeps the PostgreSQL database size down but adds reliance on an external service.</li>
</ul>
<p>That’s it for now, thank you for reading! Here’s to another 9 years, and in the closing here’s a complimentary picture of me trying to fit through pull-up bars, and my kids, Nora and Alberts, cheering:</p>
<figure><img decoding="async" width="1024" height="768" src="https://blog.healthchecks.io/wp-content/uploads/2024/07/pull_up_bars-1024x768.jpg" alt=""></figure>
<p>Happy monitoring,<br>Pēteris,<br>Healthchecks.io</p>
</div>
</article>
	</main>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to save $13.27 on your SaaS bill (163 pts)]]></title>
            <link>https://dgerrells.com/blog/how-to-save-13-27-on-your-saas-bill</link>
            <guid>41104243</guid>
            <pubDate>Mon, 29 Jul 2024 22:09:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dgerrells.com/blog/how-to-save-13-27-on-your-saas-bill">https://dgerrells.com/blog/how-to-save-13-27-on-your-saas-bill</a>, See on <a href="https://news.ycombinator.com/item?id=41104243">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><img src="https://dgerrells.com/images/vercelanal.jpg" alt="vercel example analytics dashboard showing great stats"></p>
<p>I decided to try out Vercel's analytics product on a newly minted pro plan, it included some 25k events.</p>
<p>You see, I had to start paying Vercel $ as more than 20 people visited my website. I had been using massive png images on a few high traffic pages which ate up my free outbound data. This is because the default format of taking a snippet on a Mac is a png. It is also because I didn't want to pay Vercel to make all 12 of my website's images go fast. I figured it wouldn't matter. I don't get much traffic.</p>
<blockquote>
<p>Fast forward 4 years.</p>
</blockquote>
<p>Well it did matter. And here I sit looking the fool as I humbly type my CC info into Vercel's payment form. How did I solve the issue? Did I integrate Vercel's images? Did I use an alternative cdn? No, I just converted the 2 worst offenders to jpgs and rewarded myself with another sip of coffee for a job well done. Clearly a decision the past me from four years ago would approve of.</p>
<p>I read Vercel's analytics marketing and pricing pages. 25k events are included with pro and $14 per 100k after. Seems pricey but I can cancel if I use up my quota. All good. Let's implement it.</p>
<p>I have used google, datadog, segment, and a few other client side offerings and came with expectations. It should be easy to implement and Vercel delivered. It took two lines of code since this is an older vercel project that used both the app and page routers.</p>
<pre><code>&lt;Analytics /&gt;
</code></pre>
<p>With a push to production it is live. Nice. I think it took all of 60 seconds. The dashboard view is decent. It has about what I am looking for. Popular urls, total visiters, browsers, country, all good stuff. There is some additional depth I'd like to see but it lives behind a prestigious super pro analytics tier that costs even more. That is ok though. The traffic is barely eating into the 25k quota though so I am happy. Good stuff.</p>
<h2>1 week later</h2>
<p><img src="https://dgerrells.com/images/usagewarningvercel.jpg" alt="vercel warning about too much usage"></p><p>You can guess where this went. No, not a big bill. Only $28. Surely though. Surely!!! There has to be a better way. And no, I am not thinking of the latest trending analytics sAAs vendor nor the resident OSS tool's managed cloud offering from the project's maintainers.</p>
<p>I live on the edge, the edge of the network, the browser, the bleeding edge. Everything must be serverless, multi-region, edge delivered, eventually consistent, strongly typed, ACID compliant, point in time recovery, buzzword buzzword, and buzzword bazzword. In the noise, if one listens closely, an echo can be heard. Old backend engineers from long long ago in the before time whisper of sacrilege. They use words like "htmx", "monolith", and "OOP". Usually I ignore the whispers like we do but one word kept coming up. It stayed with me. Day after day. Month after month. Taunting me. "sqlite".</p>
<p>We have been spoiled by the Vercel's of the world, the heroku's too, and even dare I say, the Salesforces. My infra game is weak. I thought it would be a fun challenge and good practice to try and save a few $ on my Vercy bill by building an analytics api from scratch using a new stack. A stack so bleeding edge that the edge lords have only just now heard of it.</p>
<h2>the squeeh stack</h2>
<p>The Squeeh stack is a new stack I just created 15 seconds ago. What is the Squeeh stack you ask? Well I am glad you asked. Any app which uses sqlite for data counts as a <code>Squeeh Stack</code><span>tm</span>.</p>
<ul>
<li>flask + sqlite + psql? <strong>squeeh stack!</strong></li>
<li>node + sqlite + hono + cloudflare? <strong>squeeh stack!!!</strong></li>
<li>unity + sqlite? <strong>squeeh snack!</strong></li>
<li>swift + tim apple + sqlite? <strong>yup also squeeh stack!</strong></li>
</ul>
<p>Sqlite may be the worst possible option for an analytics service but I keep hearing people saying it is fast. I have never used it though. People on the internet are generally a trustworthy bunch so I am going to trust them and use it. I am going to use bun and hono as the api layer. Bun because it has a delicious looking mascot and Hono because I saw this video where a guy said Hono and it made me laugh. I don't know why. I had never heard of Hono until then.</p>
<p>It didn't take long to get an api setup locally. A simple schema with a <code>db.ts</code> script creates the table. I am skipping migrations and other data best practices. No daily backups, snapshots, point in time recovery. Capturing the data is more important at this point.</p>
<pre><code>app.post(<span>"/analytics"</span>, <span>async</span> (c) =&gt; {
  <span>try</span> {
    <span>const</span> data = <span>await</span> c.req.json();
    insertLog(data);
    <span>return</span> c.json({ message: <span>"Event logged"</span> }, <span>201</span>);
  } <span>catch</span> (error) {
    <span>console</span>.error(<span>"Error logging analytics:"</span>, error);
    <span>return</span> c.json({ error: <span>"Internal Server Error"</span> }, <span>500</span>);
  }
});
</code></pre>
<p>It is time to get a gut check on how much sqlite could handle before continuing. It isn't that I don't trust the internet but you know, better to check now.</p>
<p>Gypity gave a pretty simple load test script using <code>hey</code>. I removed the useless comments and ran it.</p>
<pre><code>URL="http://localhost:3000/analytics"
DURATION="30s"
CONCURRENT_REQUESTS=10
TOTAL_REQUESTS=10000

DATA='{
  "time": "2024-07-23T15:12:20.53Z",
  "status_code": 200,
  "status_text": "OK",
  "host": "example.com",
  "request_path": "/some/path",
  "request_id": "abc123",
  "request_user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
  "level": "Info",
  "environment": "production",
  "location": "New York, USA",
  "ip_address": "203.0.113.1"
}'

hey -m POST -d "$DATA" -H "Content-Type: application/json" -c $CONCURRENT_REQUESTS -n $TOTAL_REQUESTS $URL
</code></pre>
<p>The first test ran fine.</p>
<pre><code>Summary:
  Total:        0.4716 secs
  Slowest:      0.0220 secs
  Fastest:      0.0000 secs
  Average:      0.0005 secs
  Requests/sec: 21204.8583

  Total data:   260000 bytes
  Size/request: 26 bytes
  ---------------------
</code></pre>
<p>I have no idea if that is a good result. Better bump it up to 1m requests and see how it does. I will also start another process running some reads against the same file to see what happens there.</p>
<p>And I get some locks and a few dozen failed requests. Adding the <code>WAL</code> pragma seems to fix the locking issue.</p>
<p><code>db.exec("PRAGMA journal_mode = WAL;");</code></p>
<p>Now that I am thoroughly distracted from the original goal time to fixate on making this number go up. I could buy a more powerful computer but batching the inserts would be cheaper. I wrote a function to do this for me.</p>
<pre><code><span>const</span> insertAnalytics = db.prepare(<span>`
  INSERT INTO analytics (
    data
  ) VALUES (many question marks)
`</span>);

<span>const</span> transact = db.transaction(<span>(<span>logs</span>) =&gt;</span> {
  <span>for</span> (<span>const</span> log <span>of</span> logs) {
    insertAnalytics.run(...orderSpecificLogFields);
  }
  <span>return</span> logs.length;
});
</code></pre>
<p>To gather the events before a batch I kept it stupid simple.</p>
<pre><code><span>let</span> activeLogBuffer: <span>any</span>[] = [];
<span>let</span> isActiveWrite = <span>false</span>;

<span><span>function</span> <span>backgroundPersist</span>(<span></span>) </span>{
  <span>if</span> (activeLogBuffer.length === <span>0</span> || isActiveWrite) <span>return</span>;
  <span>try</span> {
    <span>const</span> tempLogs = activeLogBuffer;
    activeLogBuffer = [];
    isActiveWrite = <span>true</span>;
    <span>const</span> count = transact(tempLogs);
    <span>console</span>.log(<span>`inserted <span>${count}</span> events`</span>);
  } <span>catch</span> (e) {
    <span>console</span>.error(<span>"batch insert error events dropped"</span>, e);
  }
  isActiveWrite = <span>false</span>;
}

<span>setInterval</span>(backgroundPersist, <span>20</span>);

app.post(<span>"/analytics"</span>, <span>async</span> (c) =&gt; {
  <span>try</span> {
    <span>const</span> data = <span>await</span> c.req.json();
    activeLogBuffer.push(data);
    <span>return</span> c.json({ message: <span>"Event logged"</span> }, <span>201</span>);
  } <span>catch</span> (error) {
    <span>console</span>.error(<span>"Error logging analytics:"</span>, error);
    <span>return</span> c.json({ error: <span>"Internal Server Error"</span> }, <span>500</span>);
  }
});
</code></pre>
<p>This is great as I can also return a response before the event persists which will prevent blocking until the write completes. I think it is a great idea to take a cue from frontend land and optimistically return an "Event logged" response even though the event has not yet been logged. Let's load test 100k with a few random read queries in another process.</p>
<pre><code>Summary:
  Total:        2.0621 secs
  Slowest:      0.0093 secs
  Fastest:      0.0000 secs
  Average:      0.0002 secs
  Requests/sec: 48495.3401

  Total data:   2600000 bytes
  Size/request: 26 bytes
</code></pre>
<p>And what about 1m with 20 concurrent requests.</p>
<pre><code>Summary:
  Total:        19.8167 secs
  Slowest:      0.0111 secs
  Fastest:      0.0000 secs
  Average:      0.0004 secs
  Requests/sec: 50462.3789

  Total data:   26000000 bytes
  Size/request: 26 bytes
</code></pre>
<p>There is a pragma to keep the db in-memory but it didn't seem to make a difference. I also read about how I could include more records per prepared statement which should help a bit more. I have been distracted long enough. This works fine.</p>
<p>Time to deploy it.</p>
<h2>how to get kicked off the ocean</h2>
<p>The api service is stupid simple, getting that api inside a docker container was not. I made the rookie mistake of having skill issues with docker. I tried to a get fancy docker compose file going and I did but it took way too long. I picked DigitalOcean for a VPS host and my expectations were high. While it is possible to have a docklet spin up based on an image pulled from a registry when an action is fired like a merge request, it is also involved. It is even more involved to get a zero downtime deployment going without dipping into more complicated orchestration.</p>
<p>I ended up ditching docker and running everything bare metal. I ssh'd into my VPS and got to work dusting off my admin skills. As I made config changes I built a bash script which should do everything needed to spin up the service on a new machine. Install all the dep, configure nginx with lets encrypt, etc. This took me a long time to do. It's not hard, just more skill issues. This made deploying changes much easier down the road.</p>
<p>After confirming I could access the remote api I figured I should load test it. I ran the same script and only hit some 250 req/s. I knew something was off though as the cpu and memory barely moved. I ran it again and it started to just hang. The VPS wasn't doing anything. The bun process was still running with no issues. I thought maybe I didn't provision enough compute so I bumped up to double the ram and a better processor. I ran the load test again and hit 2k req/s before hanging. The cpu and memory ticked up ever so slightly but then dropped down.</p>
<p>It turns out digital ocean blocked my ip. I can no longer directly ssh in. I have to use the console window from digital ocean's dashboard. To confirm this I had a friend run my same load test and he too was blocked from accessing that particular ip. Hilarious and it does work. I don't know how well but nothing like throwing some live traffic at it.</p>
<h2>a poor mans analytics</h2>
<p>The api will sit behind a function on Vercel. There isn't any auth on the endpoint so I'd rather obfuscate it a bit. I am also going to try and include a bit more information and implement some simple session tracking so I can get a better idea of unique users. Ip address could be used but I want something which will be more reliable. Cookies come to mind but I think an id in <code>localstorage</code> is better. This is the schema I needed to populate.</p>
<pre><code>db.exec(<span>`
  CREATE TABLE IF NOT EXISTS analytics (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    type TEXT,
    time TEXT,
    status_code INTEGER,
    status_text TEXT,
    host TEXT,
    request_path TEXT,
    request_id TEXT,
    request_user_agent TEXT,
    session_id TEXT,
    os TEXT,
    browser TEXT,
    country TEXT,
    level TEXT,
    environment TEXT,
    location TEXT,
    ip_address TEXT,
    content TEXT,
    referrer TEXT
  )
`</span>);
</code></pre>
<p>Storing a few derived fields from the user agent will make grouping by them much easier. Most of the fields are pretty simple to populate but location/country were trickier. I know that geo information can be included based on ip. To do this you have to setup a local ip lookup db which must be updated every month based on a vendor who kinda has a monopoly in the space. The lookup process can add some overhead. Vercel is suppose to populate the <code>geo</code> field on edge requests. I don't know why but my website doesn't run in the edge runtime. I decided to skip the geo lookup step. I can always add an ip lookup later on and run a backfill.</p>
<p>Here is the Vercel function.</p>
<pre><code><span>import</span> { UAParser } <span>from</span> <span>"ua-parser-js"</span>;

<span>const</span> url = process.env.ANALYTICS_API_URL || <span>"fallback"</span>;

<span>export</span> <span>async</span> <span><span>function</span> <span>POST</span>(<span>req</span>) </span>{
  <span>const</span> data = {
    ...requestData,
    <span>//set other data from headers etc</span>
  };

  <span>try</span> {
    <span>const</span> response = <span>await</span> fetch(url, {
      method: <span>"POST"</span>,
      headers: {
        <span>"Content-Type"</span>: <span>"application/json"</span>,
      },
      body: <span>JSON</span>.stringify(data),
    });

    <span>const</span> result = <span>await</span> response.json();
    <span>return</span> <span>new</span> Response(<span>JSON</span>.stringify(result), {
      status: <span>201</span>,
      headers: {
        <span>"Content-Type"</span>: <span>"application/json"</span>,
      },
    });
  } <span>catch</span> (error) {
    <span>// handle errors</span>
  }
}
</code></pre>
<p>I did want some idea of a country breakdown so I pulled it off the language settings in the browser.</p>
<p>And here is the react hook for that.</p>
<pre><code><span>import</span> { usePathname } <span>from</span> <span>"next/navigation"</span>;
<span>import</span> { useEffect } <span>from</span> <span>"react"</span>;

<span><span>function</span> <span>getSessionId</span>(<span></span>) </span>{
  <span>let</span> sessionId = <span>localStorage</span>.getItem(<span>"sessionId"</span>);
  <span>if</span> (!sessionId) {
    sessionId = <span>`session-<span>${crypto.randomUUID()}</span>`</span>;
    <span>localStorage</span>.setItem(<span>"sessionId"</span>, sessionId);
  }

  <span>return</span> sessionId;
}

<span>export</span> <span>const</span> useAnalytics = <span>() =&gt;</span> {
  <span>const</span> pathname = usePathname();

  useEffect(<span>() =&gt;</span> {
    <span>const</span> logAnalytics = <span>async</span> () =&gt; {
      <span>const</span> country = navigator.language.split(<span>"-"</span>)?.[<span>1</span>] || <span>"Unknown"</span>;
      <span>const</span> data = {
        status_code: <span>200</span>,
        status_text: <span>"OK"</span>,
        request_path: <span>window</span>.location.pathname,
        session_id: getSessionId(),
        referrer: <span>document</span>.referrer,
        <span>type</span>: <span>"page-view"</span>,
        country,
      };

      <span>try</span> {
        <span>await</span> fetch(<span>"/api/analytics"</span>, {
          method: <span>"POST"</span>,
          headers: {
            <span>"Content-Type"</span>: <span>"application/json"</span>,
          },
          body: <span>JSON</span>.stringify(data),
        });
      } <span>catch</span> (error) {
        <span>console</span>.error(<span>"Error logging analytics:"</span>, error);
      }
    };

    logAnalytics();
  }, [pathname]);

  <span>return</span> <span>null</span>;
};
</code></pre>
<p>While this does work and will get the job done. I added support for <code>navigator.sendBeacon</code>, <code>page-leave</code>, and <code>page-return</code> events. It was tricky to get cross browser support since I listen for multiple sources of a "session end" event and didn't want to double count. A <code>useRef</code> can solve this. If <code>navigator.sendBeacon</code> is not supported, a <code>fetch</code> request is used as a fallback.</p>
<pre><code><span>const</span> pathname = usePathname();
<span>const</span> hasFiredExitEventRef = useRef&lt;<span>boolean</span>&gt;(<span>false</span>);

useEffect(<span>() =&gt;</span> {
  logAnalytics(<span>"page-view"</span>);

  <span>const</span> handleVisibilityChange = <span>(<span>e: <span>any</span></span>) =&gt;</span> {
    <span>if</span> (<span>document</span>.visibilityState === <span>"visible"</span>) {
      logAnalytics(<span>"page-return"</span>);
      hasFiredExitEventRef.current = <span>false</span>;
      <span>return</span>;
    }

    <span>if</span> (hasFiredExitEventRef.current) <span>return</span>;

    <span>if</span> (<span>document</span>.visibilityState === <span>"hidden"</span>) {
      logAnalytics(<span>"page-leave"</span>);
      hasFiredExitEventRef.current = <span>true</span>;
      <span>return</span>;
    }

    <span>if</span> (e.type === <span>"pagehide"</span>) {
      logAnalytics(<span>"page-leave"</span>);
      hasFiredExitEventRef.current = <span>true</span>;
    }
  };

  <span>document</span>.addEventListener(<span>"visibilitychange"</span>, handleVisibilityChange);
  <span>window</span>.addEventListener(<span>"pagehide"</span>, handleVisibilityChange);

  <span>return</span> <span>() =&gt;</span> {
    <span>document</span>.removeEventListener(<span>"visibilitychange"</span>, handleVisibilityChange);
    <span>window</span>.removeEventListener(<span>"pagehide"</span>, handleVisibilityChange);
  };
}, [pathname]);
</code></pre>
<p>Naturally this hook must live only on the client so I will perform what I call "client component boxing" a common pattern in the new RSC world.</p>
<pre><code><span>"use client"</span>;
<span>import</span> { useAnalytics } <span>from</span> <span>"./useAnalytics"</span>;

<span>export</span> <span><span>function</span> <span>Analytics</span>(<span></span>) </span>{
  useAnalytics();
  <span>return</span> <span>null</span>;
}
</code></pre>
<p>Tell me this pattern isn't hilarious without it being hilarious. Adding it to the app is as easy as Vercel's so DX is the same.</p>
<pre><code><span>import</span> { Analytics } <span>from</span> <span>"./components/Analytics"</span>;
<span>import</span> { Analytics <span>as</span> VercelStyle } <span>from</span> <span>"@vercel/analytics/react"</span>;

<span>export</span> <span>default</span> <span>async</span> <span><span>function</span> <span>RootLayout</span>(<span>{
  children,
}: {
  children: React.ReactNode;
}</span>) </span>{
  <span>return</span> (
    &lt;html lang=<span>"en"</span>&gt;
      &lt;body&gt;
        {children}
        &lt;Analytics /&gt;
        &lt;VercelStyle /&gt;
      &lt;/body&gt;
    &lt;/html&gt;
  );
}
</code></pre>
<p>Vercel will stay running as I need a baseline to compare against. I almost pushed to main, as is the way, but decided to test it out in a branch instead. Usually everything I write works the first time as is tradition but I had a sneaky suspicion i didn't really know what I was doing. I deployed to a preview branch and started clicking around. I ran a query against the db file on my VPS and it was working. First try? Wow! That uhh...usually doesn't happen.</p>
<p>Rewarding myself with another sip of coffee I pushed it off to production.</p>
<h2>500 is the new green</h2>
<p>The next day I see a wall of red with sprinklings of green. 500s. Streams and streams of them. This is fine. I ssh into the vps and of course the bun process isn't running. There are no spikes in cpu, disk, memory, the service just stopped. But why?</p>
<p>I don't know but the solution was obvious. Find the root cause? No. Add orchestration with self healing hyper nano pods? Closer. It was <code>systemd</code>. I'd love to say I started at <code>systmed</code> but I actually noodled about with some node tooling first. The fact I forgot <code>systemd</code> existed is how I knew it was the right choice. It is even more embarrassing that gypity was the one who suggested it.</p>
<p>I settled on this config file. I updated the setup script to include registering this on the system.</p>
<pre><code>[Unit]
<span>Description</span>=Monolith<span> Server
</span><span>After</span>=network.target

[Service]
<span>ExecStart</span>=/root/.bun/bin/bun /root/squeeh-stack/app/src/index.ts
<span>WorkingDirectory</span>=/root/squeeh-stack/app
<span>StandardOutput</span>=append:/var/log/monolith-server/out.log
<span>StandardError</span>=append:/var/log/monolith-server/err.log
<span>Restart</span>=always
<span>User</span>=notRoot
<span>Environment</span>=NODE_ENV=production
<span>Type</span>=simple
<span>RestartSec</span>=3

[Install]
<span>WantedBy</span>=multi-user.target
</code></pre>
<p>I spun a bit trying to get this to work right. I thought I had a config wrong as the process kept crashing and restarting until it exhausted the default restart count. It turns out the db changed but I forgot to recreate it. Logs are great.</p>
<p>The red 500s are now all green. Overtime you can see when bun crashes and restarts. I am open to ideas on why this happens but my guess is because bun isn't written in rust.</p>
<p><img src="https://dgerrells.com/images/regularservicedeath.jpg" alt="digital ocean droplet chart with reg drops in usage"></p><p>You thought that was funny right? Because bun is written in zig and rust is clearly superior in every way. Well it wasn't bun, it was Hono the whole time. I looked in the <code>systemd</code> logs after a day and noticed that Hono's static router was crashing on some weird uri error.</p>
<pre><code><span>return</span> <span>async</span> (c, next) =&gt; {
<span>if</span> (c.finalized) {
<span>await</span> next (); <span>return</span>;
<span>let</span> filename = options.path ?? <span>decodeURI</span>(c.reg•path) ;
URIError: URI error
stack<span> -&gt;</span>&gt;&gt;
</code></pre>
<p>I don't know why I added a static router but when I removed it, not only did it stop crashing, it decreased the baseline cpu usage significantly. While it would be easy to say, "bad hono, no, that's a bad Hono!". It is possible I was doing something wrong, either way, this chart makes me happy.</p>
<p><img src="https://dgerrells.com/images/badhonobad.jpg" alt="chart showing better perf after fixing hono"></p><p>Ok, time for some analytics.</p>
<h2>analytics 101</h2>
<p>I wrote out the analytics features based on what Vercel has. I figured the bare minimum would be to match what they offer. I added a few more and send it off to gyptiy to write a bash script which would create a markdown file with this info. I wanted it to also email me but I knew I was already pushing it. It wasn't a usable result. Instead, I asked it to give me a js function which returns the query results.</p>
<pre><code>prompt

schema

metrics

unique visitors based on session id<span> group </span>by page, referrer, country, os, <span>and</span> browser
total unique visitors based on session id
total<span> page </span>views
unique visters change trend since last date range<span>
page </span>views change trend since last date range
average time spend on website
bounce rate <span>for</span> top 20 pages.
</code></pre>
<p>It got a little more than half right. A better ratio than the liveliness of my analytics service. I added an endpoint to return some json with metrics I could look at.</p>
<pre><code>app.get(<span>"/analytics/metrics"</span>, <span>async</span> (c) =&gt; {
  <span>try</span> {
    <span>const</span> metrics = <span>await</span> getAnalyticsMetrics(db);
    <span>return</span> c.json(metrics);
  } <span>catch</span> (error) {
    <span>console</span>.error(<span>"Error logging analytics:"</span>, error);
    <span>return</span> c.json({ error: <span>"Internal Server Error"</span> }, <span>500</span>);
  }
});
</code></pre>
<p>And it works.</p>
<p><img src="https://dgerrells.com/images/prettymetrics.jpg" alt="json metrics"></p><p>I keep reeding about how great gypity is at building UI products from the internet. I gave it my analytics json file and it spit out some react charts using <code>rechart</code>. I don't know rechart but the code looked simple enough. I plugged it in to nextjs and get an error I have never seen before.</p>
<p><img src="https://dgerrells.com/images/weirdoldreact.jpg" alt="old react error"></p><p>Research found that it is an error from back in the long ago times of class based react components. And sure enough the <code>rechart</code> library has class components. I "client component boxed" the <code>rechart</code> component and the error went away but the code didn't work either. Looks like rechart doesn't like RSC.</p>
<p>I asked gypity to try again and it picked <code>nivo</code> this time. I have heard of <code>nivo</code> it has pretty charts but I have never used it. Gyptiy wrote well over 1k lines of code for this one. I plugged the code in and got an error I was familiar with.</p>
<p><img src="https://dgerrells.com/images/nextjsclientcontextonly.jpg" alt="nextjs hates context in src"></p><p>It seems a context is used by the charts and RSC don't like those. Clearly <code>nivo</code> is an old and unsuitable library if it doesn't support RSC. I would add the latest <code>shaddy</code> chart library but I don't have tailwind setup. Instead I will drop the charts and opt for a simpler approach. More pure and soulful. Plain old html tables with css frosting.</p>
<p>This is the result.</p>
<p><img src="https://dgerrells.com/images/rigged-up-analytics.jpg" alt="super simple analytics dashboard"></p><p>I hate it but also find it endearing in an ugly duckling kind of way. I do have other data I could display like daily/weekly trends and could allow drilling down to individual sessions.</p>
<p>This is fine for now...</p>
<h3>dashboard round two</h3>
<p>It wasn't fine at all. That dashboard sucked. I changed some styles and flavor a bit and trimmed down superfluous information. I picked apart Vercel's dashboard design beyond the layout for inspiration. It is subtle in how simple it is to use. I like a bit more information thrown in my face personally but it got me thinking.</p>
<p>I tried to use ye'old gyptiy, sonnyte, and <code>v0</code> to make a chart component for me. None were up to the task. Everything either didn't work or looked terrible. No libraries allowed here.</p>
<p>I hacked together a chart component with the following api.</p>
<pre><code>&lt;BoxChart
  title=<span>"Your a wizard harry"</span>
  data={[
    {
      <span>label</span>: <span>"date"</span>,
      <span>value</span>: <span>42</span>,
    },
  ]}
  height={<span>300</span>}
/&gt;
</code></pre>
<p>It is put together with a bunch of divs and some flex box glue. It kinda works on mobile too but needs more polish.</p>
<p>Here is the new dashboard featuring the chart.</p>
<iframe src="https://www.youtube.com/embed/UQuLrQ7Sj_0" frameborder="false" credentialless="true" width="100%" height="430px" sandbox="allow-scripts allow-popups allow-top-navigation-by-user-activation allow-forms allow-same-origin allow-storage-access-by-user-activation allow-popups-to-escape-sandbox"></iframe>
<p>I like it. Here is a chart with live version with some data.</p>
<div><h6>big data energy</h6><div><p>Jul 29 9:20 PM</p><p>Jul 29 6:20 PM</p><p>Jul 29 4:20 PM</p><p>Jul 29 1:20 PM</p><p>Jul 29 10:20 AM</p><p>Jul 29 7:20 AM</p><p>Jul 29 5:20 AM</p><p>Jul 29 2:20 AM</p></div></div>
<p>With that out of the way it is time to look at the baseline.</p>
<h2>squeeh-stack vs Vercel</h2>
<p>The data when compared to Vercel is a pretty close match. My analytics seem to over count a bit compared to Vercel which could be how uniqueness is determined. I also don't filter out testing nor bot data. I did notice that Vercel's tracking gets blocked by default even with shields down on Brave where as mine is not. The data analytics people may bulk at the potential of over counting here but I just consider it a feature. Nothing helps juice up a company's valuation like inflated metrics.</p>
<p>Looking at language seems to give a good baseline when compared to Vercel's analytics which uses the ip. It is pretty close to accurate although someone in Dublin will show up as GB. I did find out that Vercel does populate the geo info. Some docs said to look at the <code>geo</code> object on the request where as in reality it is in a header.</p>
<pre><code><span>const</span> country = headers.get(<span>"x-vercel-ip-country"</span>) || <span>"Unknown"</span>;
<span>const</span> city = headers.get(<span>"x-vercel-ip-city"</span>) || <span>"Unknown"</span>;
<span>const</span> location = <span>`<span>${country}</span>, <span>${city}</span>`</span>;
</code></pre>
<p>With the current traffic this would run fine on a $6/m VPS. Data is enough to cover well over 100m events maybe even a billion depending on sqlite. I can add volumes for data backup for a few bucks more depending on size. I left the VPS over provisioned at a higher tier and came out to a $13.27 savings compared to my current Vercel Analytics spend. It took about 2 days to build this and reap those sweet sweet savings. CPU/Memory/etc is low. When load testing bun peaked at around 50mb. Pretty fat when compared to others but still significantly cheaper.</p>
<p>There is freedom to add additional analytics and queries since I have direct access to the service and data. For example I am able to get a bounce rate approximation. With a little more work I can get an average visit duration among others. I imagine Vercel has more features but behind a higher paywall.</p>
<p>An engineer who doesn't suffer from infra skill issues could spin up a much more robust and stable analytics service in a fraction of the time. However, for each additional "robustness" feature added, the cost and complexity will go up too. If I wanted zero downtime deployments, that means orchestration with additional provisioning. If I wanted data guarantees, that'd add even more.</p>
<p>I am going to keep running this along side Vercel to see how it does and will iterate on it overtime. Who knows, maybe I'll spin up a sAAs product which is nothing more than a droplet wrapper with a sqlite database slapped in. I better slap AI in the domain to make sure people know I mean business.</p>
<p>shush, I know of <a href="https://turso.tech/">turso</a>. They look amazing.</p>
<h2>fin.</h2>
<p>This was fun and outside my comfort zone. I want to do more to see what a squeeh stack can handle. I have ideas.</p>
<p>Cheers!</p>
<p><span><p>One final note. I know that Vercel is wrapping Tinybird behind the scenes.
Just imagine replacing all usages of "Vercel" with Tinybird.</p></span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CrowdStrike's impact on aviation (366 pts)]]></title>
            <link>https://heavymeta.org/2024/07/28/crowdstrikes-impact-on-aviation.html</link>
            <guid>41103101</guid>
            <pubDate>Mon, 29 Jul 2024 19:41:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://heavymeta.org/2024/07/28/crowdstrikes-impact-on-aviation.html">https://heavymeta.org/2024/07/28/crowdstrikes-impact-on-aviation.html</a>, See on <a href="https://news.ycombinator.com/item?id=41103101">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Just after midnight Eastern Time on July 19, 2024, the enterprise cybersecurity
company CrowdStrike YOLOed a software update to millions of Windows machines. Or
as they put it:</p>

<blockquote>
  <p>On July 19, 2024 at 04:09 UTC, as part of ongoing operations, CrowdStrike
released a sensor configuration update to Windows systems.</p>
</blockquote>

<p>That sensor configuration update caused the largest IT outage in history.</p>

<p><a href="https://twitter.com/Pinboard/status/1814361862890307692"><picture>
    <source media="(max-width: 640px)" srcset="https://heavymeta.org/images/pinboard-largest-it-outage-so-far-s.webp 1x,                     https://heavymeta.org/images/pinboard-largest-it-outage-so-far.webp 2x">
    <source media="(min-width: 641px)" srcset="https://heavymeta.org/images/pinboard-largest-it-outage-so-far.webp 1x,                     https://heavymeta.org/images/pinboard-largest-it-outage-so-far.webp 2x">
    <img src="https://heavymeta.org/images/pinboard-largest-it-outage-so-far-s.webp" alt="Screenshot of a @pinboard tweet: 'Largest IT outage in history *so far*!'">
  </picture></a></p>

<p>Overnight, about <a href="https://www.bbc.com/news/articles/cpe3zgznwjno">8.5 million
computers</a> blue screened,
affecting hospitals, banks, 911 systems–as the New York Times put it, “It is
more apt to ask what was not affected.” The answer is Linux, Macs, and phones.</p>

<p>The outage highlighted a different kind of digital divide. On one side, gmail,
Facebook, and Twitter kept running, letting us post photos of blue screens
located on the other side: the Windows machines responsible for actually doing
things in the world like making appointments, opening accounts, and dispatching
police.</p>

<p>They also run airlines.</p>

<p>Here’s a visualization of the chaos that CrowdStrike caused for airlines from
the <a href="https://www.nytimes.com/2024/07/19/technology/microsoft-crowdstrike-outage-what-happened.html">New York
Times</a>:</p>

<p><a href="https://heavymeta.org/images/nytimes-crowdstrike-airport-delays.webp"><picture>
    <source media="(max-width: 640px)" srcset="https://heavymeta.org/images/nytimes-crowdstrike-airport-delays-s.webp 1x,                     https://heavymeta.org/images/nytimes-crowdstrike-airport-delays.webp 2x">
    <source media="(min-width: 641px)" srcset="https://heavymeta.org/images/nytimes-crowdstrike-airport-delays.webp 1x,                     https://heavymeta.org/images/nytimes-crowdstrike-airport-delays.webp 2x">
    <img src="https://heavymeta.org/images/nytimes-crowdstrike-airport-delays-s.webp" alt="Chart from the New York Times: How the airlines cancellations rippled around the world (and across time zones). Share of canceled flights at 25 airports on Friday">
  </picture></a></p>

<p>Airline cancellations is a good metric, but I want to look directly at air
traffic: How many planes were in the air? How many planes should have been in
the air?</p>

<p>At about noon UTC, 8 hours after the CrowdStrike update hit, someone posted a
video to Twitter that they made with FlightRadar24 showing air traffic over the
United States. It was described as a 12-hour timelapse of American Airlines,
Delta, and United plane traffic that showed the nationwide ground stop of the
three airlines due to CrowdStrike.</p>

<p>Here’s the video:</p>

<video controls="">
  <source src="https://heavymeta.org/images/fr24-20240718.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>

<p>It’s not a good visualization of the impact because there’s no basis for
comparison. It clearly shows fewer planes flying at night, but that happens
every day. Was that night different from any other night? There’s no way to
tell. In Bellingcat’s <a href="https://www.bellingcat.com/resources/2024/04/25/oshit-seven-deadly-sins-of-bad-open-source-research/">“OSHIT: Seven Deadly Sins of Bad Open Source
Research”</a>,
sin #4 is “Lacking Context for Occurrences, Common or Otherwise”. In this post
I’ll show the effects CrowdStrike had on air traffic, with enough context to
make the significance clear.</p>

<h2 id="impact-on-us-aviation">Impact on U.S. Aviation</h2>

<p>CrowdStrike hit on July 19. This chart shows the number of aircraft that took
off in the United States, hour by hour, on that day. It also shows the same
numbers for July 12, the previous Friday. The same day one week previously seems
to be a good basis for comparison–both days are Fridays, and there aren’t any
major holidays on either day. I also plotted the stats for July 18, the day
before CrowdStrike, but it was very similar so I’ll continue to compare to the
previous week.</p>

<p>Note that the chart is for all of aviation in the United States, including fire
fighting aircraft, police, military, and general aviation as well as commercial
aviation.</p>

<p><a href="https://heavymeta.org/images/crowdstrike-us-all.webp"><picture>
    <source media="(max-width: 640px)" srcset="https://heavymeta.org/images/crowdstrike-us-all-s.webp 1x,                     https://heavymeta.org/images/crowdstrike-us-all.webp 2x">
    <source media="(min-width: 641px)" srcset="https://heavymeta.org/images/crowdstrike-us-all.webp 1x,                     https://heavymeta.org/images/crowdstrike-us-all.webp 2x">
    <img src="https://heavymeta.org/images/crowdstrike-us-all-s.webp" alt="CrowdStrike US all flights chart">
  </picture></a></p>

<p>From about 0600 to 1300 there seems to have been a small decrease in the number
of flights, and then a small increase in the rest of the day. Looking at the
cumulative statistics starting from 0400, when the CrowdStrike update was
pushed, flights were up 2.6% compared to the same period on the previous Friday.</p>

<p>This chart shows the percentage change in flights, comparing each hour on July
19 to the matching hour of the previous Friday as the baseline:</p>

<p><a href="https://heavymeta.org/images/crowdstrike-us-all-pct.webp"><picture>
    <source media="(max-width: 640px)" srcset="https://heavymeta.org/images/crowdstrike-us-all-pct-s.webp 1x,                     https://heavymeta.org/images/crowdstrike-us-all-pct.webp 2x">
    <source media="(min-width: 641px)" srcset="https://heavymeta.org/images/crowdstrike-us-all-pct.webp 1x,                     https://heavymeta.org/images/crowdstrike-us-all-pct.webp 2x">
    <img src="https://heavymeta.org/images/crowdstrike-us-all-pct-s.webp" alt="CrowdStrike US all flights percent change chart">
  </picture></a></p>

<p>This chart brings CrowdStrike’s effects into greater relief. The hour with the
largest percent decrease was from 0800 to 0900, which had only 261 flights
compared to the previous Friday’s 378 flights, a 31% reduction.</p>

<h2 id="airline-statistics">Airline Statistics</h2>

<p>Now let’s look at the statistics for the top 4 U.S. airlines: Delta, United,
American, and Southwest.</p>

<h3 id="delta-air-lines">Delta Air Lines</h3>

<p>Change during CrowdStrike: -1087 flights (-46%)</p>

<p><a href="https://heavymeta.org/images/crowdstrike-us-dal.webp"><picture>
    <source media="(max-width: 640px)" srcset="https://heavymeta.org/images/crowdstrike-us-dal-s.webp 1x,                     https://heavymeta.org/images/crowdstrike-us-dal.webp 2x">
    <source media="(min-width: 641px)" srcset="https://heavymeta.org/images/crowdstrike-us-dal.webp 1x,                     https://heavymeta.org/images/crowdstrike-us-dal.webp 2x">
    <img src="https://heavymeta.org/images/crowdstrike-us-dal-s.webp" alt="CrowdStrike US DAL chart">
  </picture></a></p>

<p><a href="https://heavymeta.org/images/crowdstrike-us-dal-pct.webp"><picture>
    <source media="(max-width: 640px)" srcset="https://heavymeta.org/images/crowdstrike-us-dal-pct-s.webp 1x,                     https://heavymeta.org/images/crowdstrike-us-dal-pct.webp 2x">
    <source media="(min-width: 641px)" srcset="https://heavymeta.org/images/crowdstrike-us-dal-pct.webp 1x,                     https://heavymeta.org/images/crowdstrike-us-dal-pct.webp 2x">
    <img src="https://heavymeta.org/images/crowdstrike-us-dal-pct-s.webp" alt="CrowdStrike US DAL percent chart"></picture></a></p>

<h3 id="united-airlines">United Airlines</h3>

<p>Change during CrowdStrike: -596 flights (-36%)</p>

<p><a href="https://heavymeta.org/images/crowdstrike-us-ual.webp"><picture>
    <source media="(max-width: 640px)" srcset="https://heavymeta.org/images/crowdstrike-us-ual-s.webp 1x,                     https://heavymeta.org/images/crowdstrike-us-ual.webp 2x">
    <source media="(min-width: 641px)" srcset="https://heavymeta.org/images/crowdstrike-us-ual.webp 1x,                     https://heavymeta.org/images/crowdstrike-us-ual.webp 2x">
    <img src="https://heavymeta.org/images/crowdstrike-us-ual-s.webp" alt="CrowdStrike US UAL chart">
  </picture></a></p>

<p><a href="https://heavymeta.org/images/crowdstrike-us-ual-pct.webp"><picture>
    <source media="(max-width: 640px)" srcset="https://heavymeta.org/images/crowdstrike-us-ual-pct-s.webp 1x,                     https://heavymeta.org/images/crowdstrike-us-ual-pct.webp 2x">
    <source media="(min-width: 641px)" srcset="https://heavymeta.org/images/crowdstrike-us-ual-pct.webp 1x,                     https://heavymeta.org/images/crowdstrike-us-ual-pct.webp 2x">
    <img src="https://heavymeta.org/images/crowdstrike-us-ual-pct-s.webp" alt="CrowdStrike US UAL percent chart">
  </picture></a></p>

<h3 id="american-airlines">American Airlines</h3>

<p>Change during CrowdStrike: -376 flights (-16%)</p>

<p><a href="https://heavymeta.org/images/crowdstrike-us-aal.webp"><picture>
    <source media="(max-width: 640px)" srcset="https://heavymeta.org/images/crowdstrike-us-aal-s.webp 1x,                     https://heavymeta.org/images/crowdstrike-us-aal.webp 2x">
    <source media="(min-width: 641px)" srcset="https://heavymeta.org/images/crowdstrike-us-aal.webp 1x,                     https://heavymeta.org/images/crowdstrike-us-aal.webp 2x">
    <img src="https://heavymeta.org/images/crowdstrike-us-aal-s.webp" alt="CrowdStrike US AAL chart">
  </picture></a></p>

<p><a href="https://heavymeta.org/images/crowdstrike-us-aal-pct.webp"><picture>
    <source media="(max-width: 640px)" srcset="https://heavymeta.org/images/crowdstrike-us-aal-pct-s.webp 1x,                     https://heavymeta.org/images/crowdstrike-us-aal-pct.webp 2x">
    <source media="(min-width: 641px)" srcset="https://heavymeta.org/images/crowdstrike-us-aal-pct.webp 1x,                     https://heavymeta.org/images/crowdstrike-us-aal-pct.webp 2x">
    <img src="https://heavymeta.org/images/crowdstrike-us-aal-pct-s.webp" alt="CrowdStrike US AAL percent chart"></picture></a></p>

<h3 id="southwest-airlines">Southwest Airlines</h3>

<p>Change during CrowdStrike: +101 flights (+3%)</p>

<p><a href="https://heavymeta.org/images/crowdstrike-us-swa.webp"><picture>
    <source media="(max-width: 640px)" srcset="https://heavymeta.org/images/crowdstrike-us-swa-s.webp 1x,                     https://heavymeta.org/images/crowdstrike-us-swa.webp 2x">
    <source media="(min-width: 641px)" srcset="https://heavymeta.org/images/crowdstrike-us-swa.webp 1x,                     https://heavymeta.org/images/crowdstrike-us-swa.webp 2x">
    <img src="https://heavymeta.org/images/crowdstrike-us-swa-s.webp" alt="CrowdStrike US SWA chart">
  </picture></a></p>

<p><a href="https://heavymeta.org/images/crowdstrike-us-swa-pct.webp"><picture>
    <source media="(max-width: 640px)" srcset="https://heavymeta.org/images/crowdstrike-us-swa-pct-s.webp 1x,                     https://heavymeta.org/images/crowdstrike-us-swa-pct.webp 2x">
    <source media="(min-width: 641px)" srcset="https://heavymeta.org/images/crowdstrike-us-swa-pct.webp 1x,                     https://heavymeta.org/images/crowdstrike-us-swa-pct.webp 2x">
    <img src="https://heavymeta.org/images/crowdstrike-us-swa-pct-s.webp" alt="CrowdStrike US SWA percent chart">
  </picture></a></p>

<h3 id="airlines-summary">Airlines Summary</h3>

<p>Delta was hardest hit, then United, and to a significantly smaller degree
American. Southwest didn’t seem to be affected at all.</p>

<!-- [![foo](/images/crowdstrike-us-airlines-pct-chg-s.webp)](/images/crowdstrike-us-airlines-pct-chg.webp) -->

<p><a href="https://heavymeta.org/images/crowdstrike-us-airlines-pct-chg.webp"><picture>
    <source media="(max-width: 640px)" srcset="https://heavymeta.org/images/crowdstrike-us-airlines-pct-chg-s.webp 1x,                     https://heavymeta.org/images/crowdstrike-us-airlines-pct-chg.webp 2x">
    <source media="(min-width: 641px)" srcset="https://heavymeta.org/images/crowdstrike-us-airlines-pct-chg.webp 1x,                     https://heavymeta.org/images/crowdstrike-us-airlines-pct-chg.webp 2x">
    <img src="https://heavymeta.org/images/crowdstrike-us-airlines-pct-chg-s.webp" alt="CrowdStrike US airlines percent change chart">
  </picture></a></p>

<p>Apparently <a href="https://www.techradar.com/pro/security/southwest-airlines-avoided-crowdstrike-microsoft-outage-because-its-still-running-windows-31-fourth-largest-us-airline-remained-free-of-bsod-errors-because-its-os-hasnt-been-updated-in-decades">Southwest Airlines’ ingenious strategy of never upgrading from
Windows
3.1</a>
allowed it to remain unscathed. <a href="https://www.osnews.com/story/140301/no-southwest-airlines-is-not-still-using-windows-3-1/">This seems to be
false</a>,
BTW. <a href="https://abcnews.go.com/Travel/wireStory/airlines-except-recovering-crowdstrike-tech-outage-feds-noticed-112167272">This ABC News
article</a>
says that Southwest wasn’t affected because they don’t use CrowdStrike.]</p>

<p>Delta Air Lines took an extended time to recover, canceling thousands of flights
in the days following the CrowdStrike update. Why were other airlines able to
get back to normal so much faster than Delta? A <a href="https://abcnews.go.com/Business/delta-days-restore-normal-service-after-crowdstrike-outage/story?id=112299966">terrible article from ABC
News</a>
said this:</p>

<blockquote>
  <p>The reason for the prolonged recovery from the outage was because the
CrowdStrike update disruption required a manual fix at each individual
computer system, experts told ABC News. While each fix can be completed in no
more than 10 minutes, the vast number of Delta’s digital terminals required
significant manpower to address, expert said.</p>
</blockquote>

<p>I’m reminded of sin #4 again–How is this different from any other airline? ABC
News has no idea. A <a href="https://www.reddit.com/r/delta/comments/1edtfbh/comment/lf9konn/">random redditor gave an unsourced
explanation</a>
that might be wrong but at least attempts to answer the question “Why Delta so
bad?” (DR = disaster recovery):</p>

<blockquote>
  <p>These “experts” are completely wrong. The core issue was Delta did NOT have a
proper DR plan ready and did NOT have a proper IT business continuity plan
ready. UA, AA, and F9 recovered so fast because they had plans on stand-by and
engaged them immediately. After the SWA IT problem, UA and AA put in robust DR
plans staged everywhere from the server farms, to cloud solutions, to end-user
stations at airports. They had plans on how to recover systems. DL outsources
a lot of their IT. UA and AA engaged those plans quickly. They did not hold
back paying OT for staff. UA and AA have just as much reliance on Windows as
Delta. AA was recovered by end of data Friday and resumed normal operations
Saturday. UA was about 12 hours behind them having it resolved by Saturday
morning resuming normal schedules Saturday afternoon. The ONUS is 100% on DL
C+ level in their IT decisions.</p>
</blockquote>

<h2 id="data-and-analysis">Data and Analysis</h2>

<p>I took raw ADS-B data from <a href="https://adsbexchange.com/">ADS-B Exchange</a> and
processed it through my custom code to detect aircraft takeoffs. I’m assuming
that a takeoff is roughly equivalent to a flight, which isn’t actually true but
is close enough for these purposes. It tends to undercount the number of
aircraft flying, e.g. in the case where an aircraft took off from a field
outside of ADS-B Exchange’s coverage, but it does so in a systematic way that
still allows for valid comparisons between time periods. That is, the absolute
numbers of flights may be too low, but the percent changes in numbers are
accurate.</p>

<p>I counted takeoffs instead of counting flying aircraft because I already had
code to detect takeoffs and didn’t want to write new code–this was just a quick
weekend project.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Attribution is dying, clicks are dying (224 pts)]]></title>
            <link>https://sparktoro.com/blog/attribution-is-dying-clicks-are-dying-marketing-is-going-back-to-the-20th-century/</link>
            <guid>41101948</guid>
            <pubDate>Mon, 29 Jul 2024 17:09:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sparktoro.com/blog/attribution-is-dying-clicks-are-dying-marketing-is-going-back-to-the-20th-century/">https://sparktoro.com/blog/attribution-is-dying-clicks-are-dying-marketing-is-going-back-to-the-20th-century/</a>, See on <a href="https://news.ycombinator.com/item?id=41101948">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Welcome to another edition of 5-Minute Whiteboards. And folks, we’ve got a doozy of a topic. Yes, I’m being intentionally provocative. But it’s because things really have changed in the last decade, yet too many of us are still asked to invest in marketing as though it’s 2014.</p>
<p>In just seven minutes, I’m going to try to change your mind about how marketing works in 2024 (and doesn’t), and the hard conversation we need to have with the C-suite, clients, team members, and those who believe every sale can be perfectly attributed to all the marketing channels and tactics that contributed. I’m also trying out a “digital” whiteboard format that’s likely to be my go to for the next few episodes (it’s easier to read, and the graphics can be embedded/shared with less friction). Let me know what you think of this format in the comments 😉</p>

<h2>The Way We’ve Done Digital Marketing for 20 Years is Ending</h2>
<p>Well, marketing friends, we gotta have a serious talk. Because the way we’ve done marketing for the last twenty years is ending. I’m serious. I believe that Rand in 2010 would have told you that digital marketing was all about being able to track every view and every click, so that when conversions happened, we could perfectly attribute them, is wrong today. Back then, we could say: <em>“Oh, this piece of content, this advertisement, this PR investment, this word-of-mouth effort is worthwhile because it turned into this trackable, perfectly attributable series of events in our analytics.”</em></p>
<p>It doesn’t work this way anymore.</p>
<p>That’s because clicks are dying and attribution is dying. There’s only one way forward.</p>
<h2>What’s Killing Clicks?</h2>
<p>“Tell me, Rand, what killed all these clicks?”</p>
<p>I’m going to tell you every one of the major search, social, and content platforms has an incentive to keep you there. LinkedIn wants to keep you on LinkedIn. Twitter wants to keep you on Twitter.</p>
<div>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/images.sparktoro.com\/blog\/wp-content\/uploads\/2022\/07\/zero-click-content-chart-pink-1024x731.png&quot;,&quot;figureClassNames&quot;:&quot;aligncenter size-large is-resized&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:null,&quot;imgStyles&quot;:&quot;width:700px&quot;,&quot;targetWidth&quot;:&quot;none&quot;,&quot;targetHeight&quot;:&quot;none&quot;,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img decoding="async" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://images.sparktoro.com/blog/wp-content/uploads/2022/07/zero-click-content-chart-pink-1024x731.png" alt=""></figure></div>
<p>So does Facebook, so does Reddit, so does YouTube, so does Instagram, so does TikTok, every one of these. So, they all bias to algorithms that penalize links and reward native content—zero-click content.</p>
<h2>What’s Killing Attribution?</h2>
<p>Attribution was killed by a variety of things: Apple’s cookie changes absolutely had a big effect when they pulled back on third party cookies inside safari that took a huge hit. Anti tracking and privacy laws in California, Canada, New York, and the EU, and many of those cookie permissions and do-not-track protocols have rolled out globally.</p>
<div>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/images.sparktoro.com\/blog\/wp-content\/uploads\/2024\/07\/what-killed-attribution.png&quot;,&quot;figureClassNames&quot;:&quot;aligncenter size-full is-resized&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-9185&quot;,&quot;imgStyles&quot;:&quot;width:600px&quot;,&quot;targetWidth&quot;:800,&quot;targetHeight&quot;:801,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img fetchpriority="high" decoding="async" width="800" height="801" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://images.sparktoro.com/blog/wp-content/uploads/2024/07/what-killed-attribution.png" alt="" srcset="https://images.sparktoro.com/blog/wp-content/uploads/2024/07/what-killed-attribution.png 800w, https://images.sparktoro.com/blog/wp-content/uploads/2024/07/what-killed-attribution-300x300.png 300w, https://images.sparktoro.com/blog/wp-content/uploads/2024/07/what-killed-attribution-150x150.png 150w, https://images.sparktoro.com/blog/wp-content/uploads/2024/07/what-killed-attribution-768x769.png 768w" sizes="(max-width: 800px) 100vw, 800px"></figure></div>
<p>Then there’s the massive adoption of ad blockers. We’re talking about <a href="https://www.ghostery.com/blog/privacy-report-advertisers-and-adblockers">a third to half of all Internet users using an ad blocker</a> on one or more of their devices. And and ad blockers don’t just block ads, they block all of the tracking that we do as well.</p>
<p>Multi device journeys mean that tracking someone, even with fancy browser fingerprinting (illegal in many parts of the world, but still technically allowed in the US), is rendered impossible unless they log in with the same credentials on all those devices.</p>
<p>The adoption of mobile and tablet devices, and the domination of apps that happen inside of them further hides attribution data. All of that in-app activity, those hours that people spend inside their apps on their phone/tablet cannot be perfectly attributed back to the same visits that they make on the web except when it is through the same ad/login tracking system. That means only a few big companies (mostly Google, Apple, and Meta) have access to that data.</p>
<p>And then there’s the zero click problem.</p>
<p>As an Internet user, I don’t click on things the way I used to. You don’t click on things. We, as a collective Internet, stopped clicking. Part of that was the platforms training us not to click. And part of that is our own predilection for consuming content in native formats on apps and websites.</p>
<div>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/images.sparktoro.com\/blog\/wp-content\/uploads\/2023\/04\/dark-traffic-major-social-networks-1024x635.png&quot;,&quot;figureClassNames&quot;:&quot;aligncenter size-large is-resized&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:null,&quot;imgStyles&quot;:&quot;width:750px&quot;,&quot;targetWidth&quot;:&quot;none&quot;,&quot;targetHeight&quot;:&quot;none&quot;,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img decoding="async" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://images.sparktoro.com/blog/wp-content/uploads/2023/04/dark-traffic-major-social-networks-1024x635.png" alt=""></figure></div>
<p>Last, but not least, there’s dark traffic. We studied this in-depth at SparkToro last year, noting that <a href="https://sparktoro.com/blog/new-research-dark-social-falsely-attributes-significant-percentages-of-web-traffic-as-direct/">more than half of the major social networks hide some or all</a> of their referral data.</p>
<h2>Show Me the Receipts</h2>
<p>We have the data to back up these assertions about the decline of both clicks and attribution. This isn’t just conjecture. I can prove it to you. We did a study in January of this year with Datos looking at <a href="https://sparktoro.com/blog/who-sends-traffic-on-the-web-and-how-much-new-research-from-datos-sparktoro/">who sends traffic on the web</a>, and it is overwhelmingly search.</p>
<div>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/images.sparktoro.com\/blog\/wp-content\/uploads\/2024\/03\/Datos-SparkToro-Referral-Study-Share-2024-1024x818.png&quot;,&quot;figureClassNames&quot;:&quot;aligncenter size-large is-resized&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:null,&quot;imgStyles&quot;:&quot;width:800px&quot;,&quot;targetWidth&quot;:&quot;none&quot;,&quot;targetHeight&quot;:&quot;none&quot;,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img decoding="async" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://images.sparktoro.com/blog/wp-content/uploads/2024/03/Datos-SparkToro-Referral-Study-Share-2024-1024x818.png" alt=""></figure></div>
<p>Google, Microsoft/Bing, and DuckDuckGo account for ~70% of all traffic referrals, every click that’s sent out. But, where people are spending time is NOT 70% search. That is not where 70% of visits go.</p>
<div>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/images.sparktoro.com\/blog\/wp-content\/uploads\/2024\/03\/Share-of-Visits-to-Categories-Datos-Jan-2023-24.png&quot;,&quot;figureClassNames&quot;:&quot;aligncenter size-large is-resized&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:null,&quot;imgStyles&quot;:&quot;width:800px&quot;,&quot;targetWidth&quot;:&quot;none&quot;,&quot;targetHeight&quot;:&quot;none&quot;,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img decoding="async" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://images.sparktoro.com/blog/wp-content/uploads/2024/03/Share-of-Visits-to-Categories-Datos-Jan-2023-24.png" alt=""></figure></div>
<p>In fact, search is only ~10% of all visits (and yes that includes AI search players like Perplexity). Ten percent. We spend time in social and productivity tools like our email, on news websites, buying things in e-commerce, on video and audio platforms.</p>
<p>If you try to prove acquisition via traffic referral data you will become Google’s fool. Here’s why: Everything we consume in social, news, productivity (like our emails), video and audio (YouTube and podcasts), and specific sectors like financial sites, travel sits, health sites, etc. leads us to potential searches. It exposes us to ideas, brands, problems… things we need to research.</p>
<p>If see something about a brand of risotto on a recipe website that looks interesting, it’s rare that I directly have a link to click. And often if I do, I still don’t bother! Because I’m used to zero-click consumption. Instead, later on, I might search in Google for that type of risotto.</p>
<p>When I go to Google and search… who gets the credit?</p>
<p>Google does. </p>
<div>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/images.sparktoro.com\/blog\/wp-content\/uploads\/2024\/07\/acquisition-via-referral-will-make-you-googles-fool.png&quot;,&quot;figureClassNames&quot;:&quot;aligncenter size-full is-resized&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-9187&quot;,&quot;imgStyles&quot;:&quot;width:800px&quot;,&quot;targetWidth&quot;:917,&quot;targetHeight&quot;:915,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img decoding="async" width="917" height="915" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://images.sparktoro.com/blog/wp-content/uploads/2024/07/acquisition-via-referral-will-make-you-googles-fool.png" alt="" srcset="https://images.sparktoro.com/blog/wp-content/uploads/2024/07/acquisition-via-referral-will-make-you-googles-fool.png 917w, https://images.sparktoro.com/blog/wp-content/uploads/2024/07/acquisition-via-referral-will-make-you-googles-fool-300x300.png 300w, https://images.sparktoro.com/blog/wp-content/uploads/2024/07/acquisition-via-referral-will-make-you-googles-fool-150x150.png 150w, https://images.sparktoro.com/blog/wp-content/uploads/2024/07/acquisition-via-referral-will-make-you-googles-fool-768x766.png 768w" sizes="(max-width: 917px) 100vw, 917px"></figure></div>
<h2>What’s the Solution to the Zero-Click, Impossible-to-Attribute World of Marketing in 2024?</h2>
<p>The way forward is to go where your audience is being influenced.</p>
<p>For example, let’s say I want to find the best rice for making risotto. I might search Google to solve that problem. But I might also watch videos on TikTok or Instagram. I could turn to my favorite recipe blog. I might ask the chefs at my favorite Italian restaurant.</p>
<p>Millions of people make risotto around the world every week. But, only 0.1% of them are searching Google for “best rice to make risotto.” If we wanna know how to influence those millions of people, we need to uncover ALL the sources that influence them. We need to know all the ways they’re nudged to buy one particular brand or kind of rice or another. We want the websites risotto searchers visit. We wanna know the social networks and social accounts that they use and follow. We want to know the podcasts and the Youtube channels and the subreddits that they visit.</p>
<p>Even if we’re exclusively thinking about a Google search for the “best rice for risotto,” I worry. I worry that an old school SEO looks at the potential click through rate and says this isn’t a keyword worth targeting.</p>
<p>Right? It’s not worth ranking for because Google’s showing an instant answer. There’s no clicks. But, there is a behavior-biasing result.</p>
<div>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/images.sparktoro.com\/blog\/wp-content\/uploads\/2024\/07\/go-where-audience-is.png&quot;,&quot;figureClassNames&quot;:&quot;aligncenter size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-9189&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:1400,&quot;targetHeight&quot;:692,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img decoding="async" width="1024" height="506" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://images.sparktoro.com/blog/wp-content/uploads/2024/07/go-where-audience-is-1024x506.png" alt="" srcset="https://images.sparktoro.com/blog/wp-content/uploads/2024/07/go-where-audience-is-1024x506.png 1024w, https://images.sparktoro.com/blog/wp-content/uploads/2024/07/go-where-audience-is-300x148.png 300w, https://images.sparktoro.com/blog/wp-content/uploads/2024/07/go-where-audience-is-768x380.png 768w, https://images.sparktoro.com/blog/wp-content/uploads/2024/07/go-where-audience-is.png 1400w" sizes="(max-width: 1024px) 100vw, 1024px"></figure></div>
<p>If you’rein the grocery store and search for “best kind of rice for risotto,” you’re probably going to buy arborio. Because Google’s big, bold answer at the top of the search result is Arborio. (Guess what: as an avid maker of risotto, and someone who’s been perfecting the dish for years, I can promise you that arborio is not the best kind of rice for risotto. You should be buying Carnaroli if you can).</p>
<p>The problem with doing marketing work to change this result and nudge Google to show Carnaroli (or even better, your particular brand of Carnaroli) is that even if you succeed, it lacks attribution. You can’t prove to your boss that by taking over that ranking you’ve provided any value to the company. Because not only are most of the searches going to end without a click, even those that DO send traffic won’t tell you what keyword referred them. Google killed that ability to see keyword data almost ten years ago.</p>
<h2>Success Comes By Reaching People Through the Right Sources of Influence</h2>
<p>There’s a brand of aged carnaroli rice, which which is excellent for risotto, and highly regarded by professional chefs and home cooks, too. It’s what I almost always use, and over the last few years, I’m obsessed with this stuff. It’s called <a href="https://acquerello.it/eng/">Acquerello</a>.</p>
<p>Acquerello was virtually unknown in the United States ten years ago. But in the last decade, they’ve massively grown their US sales by investing in tactics that cannot be perfectly attributed, no matter how sophisticated the AI-powered, econometrics/MMM software you’re using might be.</p>
<p>Acquerello recognized that to get into the major US publications, they needed an affiliate program, so they started selling through Amazon (even though it wasn’t a great brand positioning match). Then their PR team pitched folks like the editors at Bon Appetit (below).</p>
<div>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/images.sparktoro.com\/blog\/wp-content\/uploads\/2024\/07\/acquerello-for-risotto.png&quot;,&quot;figureClassNames&quot;:&quot;aligncenter size-full is-resized&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-9190&quot;,&quot;imgStyles&quot;:&quot;width:700px&quot;,&quot;targetWidth&quot;:917,&quot;targetHeight&quot;:917,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="917" height="917" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://images.sparktoro.com/blog/wp-content/uploads/2024/07/acquerello-for-risotto.png" alt="" srcset="https://images.sparktoro.com/blog/wp-content/uploads/2024/07/acquerello-for-risotto.png 917w, https://images.sparktoro.com/blog/wp-content/uploads/2024/07/acquerello-for-risotto-300x300.png 300w, https://images.sparktoro.com/blog/wp-content/uploads/2024/07/acquerello-for-risotto-150x150.png 150w, https://images.sparktoro.com/blog/wp-content/uploads/2024/07/acquerello-for-risotto-768x768.png 768w" sizes="(max-width: 917px) 100vw, 917px"></figure></div>
<p>Acquerello started nudging chefs all over Italy, where tourists often experienced great risotto for the first time, to include their brand name on their menus, and put their rice packaging on visible shelves in the restaurants. They started a huge word-of-mouth campaign with Italian restaurants and chefs in major US cities. They sponsored events in other European and North American countries. They worked with specialty retailers in the US to carry their product. They expanded their online sales distribution.</p>
<p>Every one of these things positively contributed to Acquerello’s success. But, almost none of them can be attributed to a particular sale. Tragically, most of the very smart, savvy, sophisticated American tech CEOs and CFOs I know, who grew up on attribution models to direct their spend, would veto every single thing Acquerello did because they couldn’t PROVE that it led to increased sales.</p>
<p><em>“Correlation, not causation means we might be throwing our money away,”</em> they’d say.</p>
<p>What I’m suggesting to you is that digital marketing in 2024 is a lot like marketing in 1964. It is getting the right message that appeals to the right people in the right places and at the right time to the right audience.</p>
<p>And I’m urging you to invest in it and to optimize for it using <a href="https://sparktoro.com/blog/how-to-measure-hard-to-measure-marketing-channels/">lift-based measurement and not attribution</a>.</p>
<p>Because attribution fundamentally don’t work anymore. When you make marketing investments based, instead, on knowing who your audience is and where/how you can influence them, you can massively improve your results.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Do Penguins Have Knees? (2019) (115 pts)]]></title>
            <link>https://www.penguinsinternational.org/do-penguins-have-knees-and-other-frequently-asked-questions/</link>
            <guid>41101502</guid>
            <pubDate>Mon, 29 Jul 2024 16:10:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.penguinsinternational.org/do-penguins-have-knees-and-other-frequently-asked-questions/">https://www.penguinsinternational.org/do-penguins-have-knees-and-other-frequently-asked-questions/</a>, See on <a href="https://news.ycombinator.com/item?id=41101502">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main">
<article id="post-2500">
<div data-hide-featured-media="1">
<div data-padding-pos="all" data-has-bg-color="false" data-bg-color="" data-bg-opacity="1" data-animation="" data-delay="0" id="fws_66a8328a72d1b" data-column-margin="default" data-midnight="dark">
<div>
<p>
<h2><strong>Do Penguins Have Knees?…and other frequently asked questions</strong></h2>
</p>
</div>
<div>
<p><span>by Autumn L. Syracuse, Educator I</span></p>
</div>
</div>
<div id="fws_66a8328a7358a" data-column-margin="default" data-midnight="dark">
<div data-padding-pos="all" data-has-bg-color="false" data-bg-color="" data-bg-opacity="1" data-animation="" data-delay="0">
<p><span>“Do penguins have knees?” and “So what is a penguin?” are two of the most common questions I hear regarding our penguin colony on display at the Aquarium of Niagara (</span><a href="http://www.aquariumofniagara.org/" target="_blank" rel="noopener"><span>www.aquariumofniagara.org</span></a><span>). Before we talk about penguin anatomy, let’s discuss what a penguin is exactly, first. It’s hard to imagine that these chunky bipeds that don’t fly — but swim — are indeed birds. Ostriches, emus, and rheas are pretty easy to identify as birds with their fluffy plumage of feathers. So why does it seem odd to include penguins in this group of flightless birds? Are all birds descended from one common ancestor? </span></p>
<p><span>Birds are: warm-blooded, air breathing, egg-laying, covered in feathers, and possess a bill. Now let’s take a look at penguins: They check all the boxes! “But </span><i><span>why</span></i><span> are they birds? They are so funny looking!” Although outwardly different from most other birds, penguins still possess many characteristics of other avian species. Let’s get down to the </span><i><span>bones</span></i><span> of it. </span></p>
</div>
<div data-max-width="100%" data-max-width-mobile="default" data-shadow="none" data-animation="" data-padding-pos="all" data-has-bg-color="false" data-bg-color="" data-bg-opacity="1" data-delay="0">
<p><img fetchpriority="high" decoding="async" data-delay="0" height="350" width="520" data-animation="none" data-nectar-img-src="https://www.penguinsinternational.org/wp-content/uploads/2019/07/pen1.jpg" src="https://www.penguinsinternational.org/wp-content/uploads/2019/07/pen1.jpg" alt="Penguin knees" data-nectar-img-srcset="https://www.penguinsinternational.org/wp-content/uploads/2019/07/pen1.jpg 520w, https://www.penguinsinternational.org/wp-content/uploads/2019/07/pen1-300x202.jpg 300w, https://www.penguinsinternational.org/wp-content/uploads/2019/07/pen1-400x269.jpg 400w" sizes="(max-width: 520px) 100vw, 520px" srcset="https://www.penguinsinternational.org/wp-content/uploads/2019/07/pen1.jpg 520w, https://www.penguinsinternational.org/wp-content/uploads/2019/07/pen1-300x202.jpg 300w, https://www.penguinsinternational.org/wp-content/uploads/2019/07/pen1-400x269.jpg 400w">
</p>
</div>
</div>
<div data-padding-pos="all" data-has-bg-color="false" data-bg-color="" data-bg-opacity="1" data-animation="" data-delay="0" id="fws_66a8328a740b0" data-column-margin="default" data-midnight="dark">
<h2><b>Okay, so do penguins have knees?</b></h2>
<p><span>A penguin’s skeletal structure is laid out in the same general pattern as other birds. One obvious characteristic of a bird skeleton is the keel, or sternum. This is designed to be very wide and flat, but lays perpendicular to the ribs. This large bone helps to attach the flight muscles and tendons, which is very important in both form and function to flight. And since penguins “fly” through the water, which is denser, they too need to rely on the keel and flight muscles for propulsion.&nbsp;</span></p>
<p><span>Another important adaptation that varies from other flighted (volant) birds is the density of their bones. Most birds we see flying in our yards and neighborhoods have skeletons with bones that are hollow. This creates a skeleton that is extremely lightweight, allowing the birds to be able to lift off into flight. Penguins would not benefit from bones of this same density. Penguins need to be able to dive underwater to hunt for their fish, and hollow bones would make them too buoyant. To help with this, penguin bones are solid and heavy, helping to give them more weight in order to dive deep.</span></p>
</div>
<div data-padding-pos="all" data-has-bg-color="false" data-bg-color="" data-bg-opacity="1" data-animation="" data-delay="0" id="fws_66a8328a743a8" data-column-margin="default" data-midnight="dark">
<h2><b>Penguin knees are tucked up inside their body</b></h2>
<p><span>Penguins are designed to be streamlined and hydrodynamic, so having long legs would add extra drag. Having short legs with webbed feet to act like rudders, helps to give them that torpedo-like figure. If we compare bird anatomy with humans, we would see something a bit peculiar. By taking a look at the side-by-side image in Figure 1, you can see how their leg bones compare to ours. What most people mistake for knees are actually the ankles of the birds. This gives the illusion that bird knees bend opposite of ours. The knees are actually tucked up inside the body cavity of the bird! So how does this look inside of a penguin? In the images below, you can see boxes surrounding the penguins’ knees.&nbsp;</span></p>
</div>
<div id="fws_66a8328a7463a" data-column-margin="default" data-midnight="dark">
<div data-max-width="100%" data-max-width-mobile="default" data-shadow="none" data-animation="" data-padding-pos="all" data-has-bg-color="false" data-bg-color="" data-bg-opacity="1" data-delay="0">
<p><img decoding="async" data-delay="0" height="419" width="512" data-animation="none" data-nectar-img-src="https://www.penguinsinternational.org/wp-content/uploads/2019/07/pen2.jpg" src="https://www.penguinsinternational.org/wp-content/uploads/2019/07/pen2.jpg" alt="Penguin knees" data-nectar-img-srcset="https://www.penguinsinternational.org/wp-content/uploads/2019/07/pen2.jpg 512w, https://www.penguinsinternational.org/wp-content/uploads/2019/07/pen2-300x246.jpg 300w" sizes="(max-width: 512px) 100vw, 512px" srcset="https://www.penguinsinternational.org/wp-content/uploads/2019/07/pen2.jpg 512w, https://www.penguinsinternational.org/wp-content/uploads/2019/07/pen2-300x246.jpg 300w">
</p>
</div>
<div data-padding-pos="all" data-has-bg-color="false" data-bg-color="" data-bg-opacity="1" data-animation="" data-delay="0">
<div data-max-width="100%" data-max-width-mobile="default" data-shadow="none" data-animation="none">
<p><img decoding="async" data-delay="0" height="295" width="242" data-animation="none" data-nectar-img-src="https://www.penguinsinternational.org/wp-content/uploads/2019/07/pen3.png" src="https://www.penguinsinternational.org/wp-content/uploads/2019/07/pen3.png" alt="Penguin knees">
</p>
</div>
<div>
<p>
<h5><b>Colored boxes highlighting the location of penguin </b><b>knees.https://www.neaq.org/blog/do-penguins-have-knees/</b></h5>
</p>
</div>
</div>
</div>
<div data-padding-pos="all" data-has-bg-color="false" data-bg-color="" data-bg-opacity="1" data-animation="" data-delay="0" id="fws_66a8328a75445" data-column-margin="default" data-midnight="dark">
<p><span>Imagine yourself wearing an oversized shirt, and pulling it over your knees so that only your ankles and feet are showing. Now imagine you’re trying to walk forward in this position. I bet you’d waddle too! This design gives the penguin an advantage in the water to help them swim quickly to catch food or avoid predators. On land, they tend to be slower and clumsy, which makes them more prone to predators. For Antarctic penguins, they rarely encounter predators on land, so having larger bodies isn’t detrimental. For other species in temperate or tropical climates, the water tends to be a bit safer place.</span></p>
<p><span>Originally, penguins were classified in the same group as other flightless birds (Ratites). After multiple studies, it was discovered that penguins evolved from flying birds, which were separate from the ancestors of other flightless birds. Mitochondrial DNA has further suggested their relationship to other seafaring flighted birds such as albatross, frigatebirds, and loons. In 2006, more genomic testing suggested that birds of the </span><i><span>Ciconiiformes</span></i><span> order (storks, gannets, plovers, and boobies) were their closest living relatives (Watanabe et al. 2006).&nbsp;</span></p>
</div>
<div id="fws_66a8328a75701" data-column-margin="default" data-midnight="dark">
<div data-padding-pos="all" data-has-bg-color="false" data-bg-color="" data-bg-opacity="1" data-animation="" data-delay="0">
<div data-max-width="100%" data-max-width-mobile="default" data-shadow="none" data-animation="none">
<p><img decoding="async" data-delay="0" height="660" width="660" data-animation="none" data-nectar-img-src="https://www.penguinsinternational.org/wp-content/uploads/2019/07/pen4.jpg" src="https://www.penguinsinternational.org/wp-content/uploads/2019/07/pen4.jpg" alt="Waimanu" data-nectar-img-srcset="https://www.penguinsinternational.org/wp-content/uploads/2019/07/pen4.jpg 660w, https://www.penguinsinternational.org/wp-content/uploads/2019/07/pen4-300x300.jpg 300w, https://www.penguinsinternational.org/wp-content/uploads/2019/07/pen4-150x150.jpg 150w, https://www.penguinsinternational.org/wp-content/uploads/2019/07/pen4-600x600.jpg 600w, https://www.penguinsinternational.org/wp-content/uploads/2019/07/pen4-100x100.jpg 100w, https://www.penguinsinternational.org/wp-content/uploads/2019/07/pen4-140x140.jpg 140w, https://www.penguinsinternational.org/wp-content/uploads/2019/07/pen4-500x500.jpg 500w, https://www.penguinsinternational.org/wp-content/uploads/2019/07/pen4-350x350.jpg 350w" sizes="(max-width: 660px) 100vw, 660px" srcset="https://www.penguinsinternational.org/wp-content/uploads/2019/07/pen4.jpg 660w, https://www.penguinsinternational.org/wp-content/uploads/2019/07/pen4-300x300.jpg 300w, https://www.penguinsinternational.org/wp-content/uploads/2019/07/pen4-150x150.jpg 150w, https://www.penguinsinternational.org/wp-content/uploads/2019/07/pen4-600x600.jpg 600w, https://www.penguinsinternational.org/wp-content/uploads/2019/07/pen4-100x100.jpg 100w, https://www.penguinsinternational.org/wp-content/uploads/2019/07/pen4-140x140.jpg 140w, https://www.penguinsinternational.org/wp-content/uploads/2019/07/pen4-500x500.jpg 500w, https://www.penguinsinternational.org/wp-content/uploads/2019/07/pen4-350x350.jpg 350w">
</p>
</div>
<div>
<p>
<h5><b>Paleontologist Ewan Fordyce with a model replica of a </b><b><i>Waimanu</i></b><b> penguin.&nbsp; </b><b>https://teara.govt.nz/en/photograph/9803/oldest-penguin</b></h5>
</p>
</div>
</div>
<div data-padding-pos="all" data-has-bg-color="false" data-bg-color="" data-bg-opacity="1" data-animation="" data-delay="0">
<p><span>It is still uncertain as to what other bird species may be related to penguins. But from finding and examining their fossilized bones, we have an idea of what they were like nearly 60 million years ago. Known by the genus name </span><i><span>Waimanu</span></i><span>, these ancient penguins may hold the record for the oldest evidence of bird lineage. Scientists believe that the extinction event that wiped out the dinosaurs during the Cretaceous period, also eliminated almost all bird species. After this catastrophe, it is believed that modern day penguins evolved from the few species that had survived, evolving quickly over a short amount of time—in relation to Earth’s history (Fordyce and Ksepka, 2012).</span></p>
<p><span>“So what kind of animal are they?” All of the evidence points to birds, but it still leaves questions unanswered. After learning of this fossil evidence, it leaves me with this question: “Are penguins birds? Or are birds penguins?” When we take a look at other body systems and explore their behavior, things become more “black and white.”</span></p>
</div>
</div>

<div id="fws_66a8328a762be" data-column-margin="default" data-midnight="dark">
<div data-padding-pos="all" data-has-bg-color="false" data-bg-color="" data-bg-opacity="1" data-animation="" data-delay="0">
<p>
<h3><span><b>Like our penguin blogs? Sign up for our newsletter to get them right in your inbox!</b></span></h3>
</p>
</div>
<div data-max-width="100%" data-max-width-mobile="default" data-shadow="none" data-animation="" data-padding-pos="top-bottom" data-has-bg-color="false" data-bg-color="" data-bg-opacity="1" data-delay="0">
<p><img decoding="async" data-delay="0" height="128" width="130" data-animation="fade-in" data-nectar-img-src="https://www.penguinsinternational.org/wp-content/uploads/2020/03/IMG_6441-2-1.jpg" src="https://www.penguinsinternational.org/wp-content/uploads/2020/03/IMG_6441-2-1.jpg" alt="King Penguins">
</p>
</div>



</div>

</div>
</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Is Cloudflare overcharging us for their images service? (181 pts)]]></title>
            <link>http://jpetazzo.github.io/2024/07/26/cloudflare-images-overcharge-billing/</link>
            <guid>41100958</guid>
            <pubDate>Mon, 29 Jul 2024 14:55:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://jpetazzo.github.io/2024/07/26/cloudflare-images-overcharge-billing/">http://jpetazzo.github.io/2024/07/26/cloudflare-images-overcharge-billing/</a>, See on <a href="https://news.ycombinator.com/item?id=41100958">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>I recently went down a very deep rabbit hole to understand why, some months, Cloudflare was charging us 3x what we were expecting for their Cloudflare Images service. I’m posting this write-up because back then, a quick search didn’t turn anything up; and Cloudflare support has totally ghosted us for more than 8 months now.</p>

<h2 id="context-and-scale">Context and scale</h2>

<p>Let’s get something out of the way first: this is not going to be a story about millions, or even thousands, of dollars. Merely hundreds. My partner <a href="https://www.linkedin.com/in/ajbowen/">AJ</a> runs a website called <a href="https://www.ephemerasearch.com/">EphemeraSearch</a> which is an archive of old mail, a treasure trove for folks doing history or genealogy research. It’s not making much money (yet!) but storing millions of postcards <em>does</em> incur significant hosting costs, so we’re trying to be thrifty, since these costs come directly out of our own pockets (we don’t have external investors, at least not yet).</p>

<p>The website itself was initially on Heroku, then moved to a self-hosted Kubernetes cluster (after a brief transition through AWS EKS, which turned to be awfully expensive, despite leveraging spot instances, very tight autoscaling, and the famously treacherous startup credits). Many third-party services are used whenever it makes sense; for instance, the search currently relies on Algolia.</p>

<p>Image hosting was initially using Cloudinary, but we knew from day one that it was only a temporary solution, as their pricing was prohibitive for us in the long run. We moved to Cloudflare Images because it seemed affordable enough at our scale (even though we’ll almost certainly replace it later, too) and there is no question that Cloudflare is an excellent CDN.</p>

<h2 id="the-problem">The problem</h2>

<p>The service was working well, but after a few months, we noticed something off with our Cloudflare Images bills. At that point, we had a couple of million images, and less than a million image views per month. According to their <a href="https://developers.cloudflare.com/images/pricing/">pricing</a> page, we should have been paying each month:</p>

<ul>
  <li>$100 for image storage ($5 per 100,000 images stored, x 20)</li>
  <li>$10 for image delivery ($1 per 100,000 images served, x 10)</li>
</ul>

<p>Instead, when summing our Cloudflare charges (as reflected on our credit card statements), we reached more than $400 some months.</p>

<p>What was going on?!?</p>

<h2 id="it-should-be-easy-right">It should be easy, right</h2>

<p>You might wonder, dear reader, “Why did you have to sum credit card charges to know your monthly bills? Don’t you get invoices that would basically give you that information?”</p>

<p>Of course, the first thing we did was look at the invoices that we were getting. Despite the relatively simple billing models for storage and delivery, the invoices are more confusing than they should be, because the two dimensions of billing work differently.</p>

<p><strong>Image delivery</strong> is a classic pay-for-what-you-use thing. It’s $1 per 100,000 images served, <em>post-paid</em>. In other words, at the end of the month, Cloudflare counts how many images they’ve served, divides by 100,000, rounds up, and that’s how much you pay in dollars.</p>

<p><strong>Image storage</strong>, however, is prepaid, and you decide how many increments of 100,000 images you’d like to purchase. When you’re close to running out, your account dashboard will show a warning message:</p>

<blockquote>
  <p>Your account has 2% of its storage capacity remaining. Please add storage capacity to your account.</p>
</blockquote>

<p>When you add storage capacity to your account, here is what happens.</p>

<p>First, you pre-pay immediately (and your credit card is charged) for the whole capacity that you’re using, prorated to the remaining number of days in your billing cycle. In other words, if your new storage capacity is 1 million images, and you have 10 days left in your billing cycle, you immediately pay $16.67:</p>

<ul>
  <li>1 million images = 10 increments of 100,000 images at $5 each = $50</li>
  <li>10 days remaining in a cycle of 30, so 50x10/30 = $16.67</li>
</ul>

<p>Then, you get credited on your next bill with your <em>previous</em> storage capacity, prorated by the same amount - that’s the time during which you will <em>not</em> use that capacity. In other words, if you upgraded from, say, 800,000 images when you have 10 days left to your current billing cycle, you get a credit of $13.33:</p>

<ul>
  <li>800,000 images = 8 increments of 100,000 images at $5 each = $40</li>
  <li>10 days remaining in a cycle of 30, so 40x10/30 = $13.33</li>
</ul>

<p>And finally, you receive a new invoice; meaning that in some months, instead of one invoice, you get multiple invoices with prorated charges. Fair enough.</p>

<p>At the end of the day (or rather, of the billing cycle), if we went from a capacity of say 800,000 to 900,000 and then again to 1,000,000, it looks like we should pay a prorated cost depending on how many days we provisioned each capacity. In any case, it should never cost more than 1,000,000 images, right?</p>

<p><em>Wrong.</em></p>

<p>As mentioned at the beginning of this post, in some months, instead of $110, our credit card charges were over $400, and we couldn’t understand why.</p>

<p><em>And neither could the Cloudflare support team.</em></p>

<h2 id="involving-support">Involving support</h2>

<p>We contacted Cloudflare support in November 2023:</p>

<blockquote>
  <p>I’m currently subscribed to Cloudflare Images with a capacity of 2,200,000 images. I’ve been adding many images in the last few months and am regularly adding capacity as needed. It’s my understanding that each upgrade should be prorated.
2.2m images should cost $110/mo. However, when I look at the charges for the month of October, they add up to almost $400!
September also exceeds $116 even though I had way less capacity then.</p>
</blockquote>

<p>Cloudflare replied:</p>

<blockquote>
  <p>[…] we’ve raised this issue with our Images team […]</p>
</blockquote>

<p>Then when we pinged them again some time later:</p>

<blockquote>
  <p>[…] We are experiencing an unprecedented demand for our service, which is causing delays for our customers.
I’ve submitted a request to our Engineering Team, so that we can thoroughly explain what happened, and if there was any mistake reagarding your Images service.</p>
</blockquote>

<p>And after pinging them one month later:</p>

<blockquote>
  <p>Please note that we are still working with the Engineering Team on this issue.</p>
</blockquote>

<p>Then after 3 more months without an answer:</p>

<blockquote>
  <p>Thank you for waiting. Please accept our apologies for the delay in responding to you. We are experiencing an unprecedented demand for our service, which is causing delays for our customers.
Our Engineering team continues to analyze your case and develop a solution for your issue. We have been conducting weekly reviews of it for the past eight weeks. Rest assured, we are diligently working to resolve this matter.</p>
</blockquote>

<p>After pinging them the Nth time, they pointed us to this <a href="https://www.cloudflarestatus.com/incidents/wsjmr28lwxw3">incident</a> which was indeed billing-related, but had absolutely nothing to do with our issue, alas.</p>

<p>We assume that our request was blindly lumped into the ongoing billing issue (even though our request dated from November 2023, and the billing issue ran through March-May 2024).</p>

<p>Last time we pinged support again, they had migrated support to Salesforce, so the original ticket seemed to be forgotten.</p>

<p><em>Great.</em></p>

<h2 id="re-analyzing-the-situation">Re-analyzing the situation</h2>

<p>Making sense of the invoices was not trivial, because each invoice will potentially mention:</p>

<ul>
  <li>itemized charges,</li>
  <li>an “available balance”,</li>
  <li>a “previous balance”,</li>
  <li>a “starting balance”,</li>
  <li>a list of payments and credits.</li>
</ul>

<p>That’s a lot of different balances, with quite confusing names. To make sense of it, we ended up painstakingly entering all the transactions (meaning charges, payments, credits) into a spreadsheet, to try and see which balance actually corresponded to what, and to try to understand if and how we had been overcharged. That took a few hours of data entry, but eventually, it gave us the following graph:</p>

<p><img src="http://jpetazzo.github.io/assets/cloudflare-images-charges.png" alt="Graph of our charges, payments, credits, and running balance"></p>

<p>And that’s finally what helped us to understand what was going on.</p>

<h2 id="the-explanation">The explanation</h2>

<p>When you change your provisioned image storage on Cloudflare Images, you pay for the new capacity upfront: your credit card gets charged immediately. Sure, it’s prorated by the time remaining on your billing cycle, but the money goes out <em>immediately</em>. You get a credit for the old capacity that you won’t use, but that credit will only show up on your next monthly bill.</p>

<p>Consider the extreme case where you would, at the beginning of your billing cycle, increase your capacity 5 times: each time, you pay for that capacity upfront; and you get a credit for the previous capacity but that credit only materializes the following month. On the next bill, you will see a very high negative balance (indicating that Cloudflare owes you a bunch of money) and your credit card will be charged less (or even not at all) that month, so things will eventually balance out. But in the meantime, you’re accruing these credit card charges.</p>

<p>In the end, this means that Cloudflare was indeed overcharging us, but only temporarily: if enough time passes during which we <em>do not</em> change our image storage capacity, the balance should eventually go down until Cloudflare doesn’t owe us money anymore.</p>

<p>The problem is that “changing capacity” is precisely the whole point of the hecking cloud. “Pay for what you use”. The first time I racked a machine in a datacenter in the 90s, I think we had at least a yearly commitment. In the 2000s it was fairly common to rent servers by the month, and by 2010 multiple cloud providers would let you rent machines by the hour, and then by the minute.</p>

<p>I don’t know why Cloudflare decided to have this extremely weird mix of post-paid, cloud-like billing (for image delivery) and prepaid, not-cloudlike-at-all billing (for image storage), but here we are.</p>

<h2 id="is-cloudflare-images-any-good">Is Cloudflare Images any good?</h2>

<p>We’re happy with the quality of the Cloudflare Images service, but our needs are very modest, and it’s definitely overpriced for our use-case.</p>

<p>If you need to store and serve <em>big</em> images (thousands of pixels in each dimension) and to resize them efficiently, Cloudflare Images might be interesting for you, because the pricing is exclusively based on the number of images, not their size. In our case, our images are typically in the 100KB-1MB range, and we only need a small number of variants for each of them.</p>

<p>It’s likely that we will replace Cloudflare Images in the long run. Looking at storage costs alone, S3 would be 4 times less expensive <em>for our use-case</em>. And when we scale our image collection 10x, other solutions (like a couple of replicated, dedicated servers with 20 TB SATA disks) become 20x cheaper.</p>

<h2 id="conclusions-and-thoughts">Conclusions and thoughts</h2>

<p>Many “indie” projects can easily fit on very cheap infrastructure and services, sometimes well within the free tier of some generous hosting providers.</p>

<p>In our case, however, the current scale of our collection (a few terabytes at the moment, and constantly growing) compared to the very low revenue (a trickle of sales commissions whenever someone ends up buying on eBay a postcard that they found through <a href="https://www.ephemerasearch.com/">EphemeraSearch</a>) means that we have to be very efficient with our (personal) funds.</p>

<p>In the IT industry, we often talk about “buy vs. build”. Over time, as we gain more experience and understand the complexity of the things we build, we often prefer to buy a quality service rather than cobble together a crappy version of our own, arguing that the time spent building it would better be invested somewhere else. In our situation, however, the bargain turns out a bit differently: now that this is <em>my</em> money, do I want to pay $1000/month for a service, or build it myself and run it on a $100/month server? How much time do I need to build and run that service; and can I reliably make $900/month by e.g. selling consulting services to cover that cost instead?</p>

<p>Preserving historical artifacts is not, unfortunately, something that investors or the capitalist system in general tend to favor. Let’s hope that this changes in the future, but in the meantime, follow us for more thrifty and scrappy hosting tips ! 😁💸</p>

<p><em>This post was reviewed by <a href="https://www.linkedin.com/in/ajbowen/">AJ Bowen</a>. Any remaining typo or mistake is mine. We want to clarify that we think that Cloudflare Images is a great service, but that its pricing model (specifically, the prepaid aspect that has to be manually adjusted) is utterly borked. We hope our findings will be helpful to others!</em></p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An Open Course on LLMs, Led by Practitioners (109 pts)]]></title>
            <link>https://hamel.dev/blog/posts/course/</link>
            <guid>41100951</guid>
            <pubDate>Mon, 29 Jul 2024 14:54:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hamel.dev/blog/posts/course/">https://hamel.dev/blog/posts/course/</a>, See on <a href="https://news.ycombinator.com/item?id=41100951">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="quarto-content">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main id="quarto-document-content">




<p>Today, we are releasing <a href="https://parlance-labs.com/education/">Mastering LLMs</a>, a set of workshops and talks from practitioners on topics like evals, retrieval-augmented-generation (RAG), fine-tuning and more. This course is unique because it is:</p>
<ul>
<li>Taught by 25+ industry veterans who are experts in information retrieval, machine learning, recommendation systems, MLOps and data science. We discuss how this prior art can be applied to LLMs to give you a meaningful advantage.</li>
<li>Focused on applied topics that are relevant to people building AI products.</li>
<li><ins>
<strong>Free and open to everyone</strong>
</ins>
.</li>
</ul>
<p>We have organized and annotated the talks from our popular paid course.<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a> This is a survey course for technical ICs (including engineers and data scientists) who have some experience with LLMs and need guidance on how to improve AI products.</p>
<div>
<figure>
<p><a href="https://parlance-labs.com/education/" target="_blank"><img src="https://hamel.dev/blog/posts/course/course.png" alt="Speakers include Jeremy Howard, Sophia Yang, Simon Willison, JJ Allaire, Wing Lian, Mark Saroufim, Jane Xu, Jason Liu, Emmanuel Ameisen, Hailey Schoelkopf, Johno Whitaker, Zach Mueller, John Berryman, Ben Clavié, Abhishek Thakur, Kyle Corbitt, Ankur Goyal, Freddy Boulton, Jo Bergum, Eugene Yan, Shreya Shankar, Charles Frye, Hamel Husain, Dan Becker and more"></a></p>
<figcaption><em>Speakers include Jeremy Howard, Sophia Yang, Simon Willison, JJ Allaire, Wing Lian, Mark Saroufim, Jane Xu, Jason Liu, Emmanuel Ameisen, Hailey Schoelkopf, Johno Whitaker, Zach Mueller, John Berryman, Ben Clavié, Abhishek Thakur, Kyle Corbitt, Ankur Goyal, Freddy Boulton, Jo Bergum, Eugene Yan, Shreya Shankar, Charles Frye, Hamel Husain, Dan Becker and more</em></figcaption>
</figure>
</div>
<section id="getting-the-most-value-from-the-course">
<h2 data-anchor-id="getting-the-most-value-from-the-course">Getting The Most Value From The Course</h2>
<section id="prerequisites">
<h3 data-anchor-id="prerequisites">Prerequisites</h3>
<p>The course assumes basic familiarity with LLMs. If you do not have any experience, we recommend watching <a href="https://www.youtube.com/watch?v=jkrNMKz9pWU">A Hacker’s Guide to LLMs</a>. We also recommend the tutorial <a href="https://www.philschmid.de/instruction-tune-llama-2">Instruction Tuning llama2</a> if you are interested in fine-tuning <a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
</section>
<section id="navigating-the-material">
<h3 data-anchor-id="navigating-the-material">Navigating The Material</h3>
<p>The course has over 40 hours of content. To help you navigate this, we provide:</p>
<ul>
<li><strong>Organization by subject area</strong>: evals, RAG, fine-tuning, building applications and prompt engineering.</li>
<li><strong>Chapter summaries:</strong> quickly peruse topics in each talk and skip ahead</li>
<li><strong>Notes, slides, and resources</strong>: these are resources used in the talk, as well as resources to learn more. Many times we have detailed notes as well!</li>
</ul>
<p>To get started, <a href="https://parlance-labs.com/education">navigate to this page</a> and explore topics that interest you. Feel free to skip sections that aren’t relevant to you. We’ve organized the talks within each subject to enhance your learning experience. Be sure to review the chapter summaries, notes, and resources, which are designed to help you focus on the most relevant content and dive deeper when needed. This is a survey course, which means we focus on introducing topics rather than diving deeply into code. To solidify your understanding, we recommend applying what you learn to a personal project.</p>
</section>
<section id="what-students-are-saying">
<h3>What Students Are Saying</h3>
<p>Here are some testimonials from students who have taken the course<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a>:</p>
<div>
<div>
<div>
<p><img src="https://hamel.dev/blog/posts/course/sanyam.jpeg"></p>
<section id="sanyam-bhutani-partner-engineer-meta">
<h2 data-anchor-id="sanyam-bhutani-partner-engineer-meta"><em>Sanyam Bhutani, Partner Engineer @ Meta</em></h2>
<section id="there-was-a-magical-time-in-2017-when-fastai-changed-the-deep-learning-world.-this-course-does-the-same-by-extending-very-applied-knowledge-to-llms-best-in-class-teachers-teach-you-their-knowledge-with-no-fluff">
<h3 data-anchor-id="there-was-a-magical-time-in-2017-when-fastai-changed-the-deep-learning-world.-this-course-does-the-same-by-extending-very-applied-knowledge-to-llms-best-in-class-teachers-teach-you-their-knowledge-with-no-fluff">There was a magical time in 2017 when fastai changed the deep learning world. This course does the same by extending very applied knowledge to LLMs Best in class teachers teach you their knowledge with no fluff</h3>
</section>
</section>
</div>
<div>
<p><img src="https://hamel.dev/blog/posts/course/laurian.jpeg"></p>
<section id="laurian-full-stack-computational-linguist">
<h2 data-anchor-id="laurian-full-stack-computational-linguist"><em>Laurian, Full Stack Computational Linguist</em></h2>
<section id="this-course-was-legendary-still-is-and-the-community-on-discord-is-amazing.-ive-been-through-these-lessons-twice-and-i-have-to-do-it-again-as-there-are-so-many-nuances-you-will-get-once-you-actually-have-those-problems-on-your-own-deployment.">
<h3 data-anchor-id="this-course-was-legendary-still-is-and-the-community-on-discord-is-amazing.-ive-been-through-these-lessons-twice-and-i-have-to-do-it-again-as-there-are-so-many-nuances-you-will-get-once-you-actually-have-those-problems-on-your-own-deployment.">This course was legendary, still is, and the community on Discord is amazing. I’ve been through these lessons twice and I have to do it again as there are so many nuances you will get once you actually have those problems on your own deployment.!</h3>
</section>
</section>
</div>
<div>
<p><img src="https://hamel.dev/blog/posts/course/andre.png"></p>
<section id="andre-cto">
<h2 data-anchor-id="andre-cto"><em>Andre, CTO</em></h2>
<section id="amazing-an-opinionated-view-of-llms-from-tools-to-fine-tuning.-excellent-speakers-giving-some-of-the-best-lectures-and-advice-out-there-a-lot-of-real-life-experiences-and-tips-you-cant-find-anywhere-on-the-web-packed-into-this-amazing-courseworkshopconference-thanks-dan-and-hamel-for-making-this-happen">
<h3 data-anchor-id="amazing-an-opinionated-view-of-llms-from-tools-to-fine-tuning.-excellent-speakers-giving-some-of-the-best-lectures-and-advice-out-there-a-lot-of-real-life-experiences-and-tips-you-cant-find-anywhere-on-the-web-packed-into-this-amazing-courseworkshopconference-thanks-dan-and-hamel-for-making-this-happen">Amazing! An opinionated view of LLMs, from tools to fine-tuning. Excellent speakers, giving some of the best lectures and advice out there! A lot of real-life experiences and tips you can’t find anywhere on the web packed into this amazing course/workshop/conference! Thanks Dan and Hamel for making this happen!</h3>
</section>
</section>
</div>
<div>
<p><img src="https://hamel.dev/blog/posts/course/marcus.png"></p>
<section id="marcus-software-engineer">
<h2 data-anchor-id="marcus-software-engineer"><em>Marcus, Software Engineer</em></h2>
<section id="the-mastering-llms-conference-answered-several-key-questions-i-had-about-when-to-fine-tune-base-models-building-evaluation-suits-and-when-to-use-rag.-the-sessions-provided-a-valuable-overview-of-the-technical-challenges-and-considerations-involved-in-building-and-deploying-custom-llms.">
<h3 data-anchor-id="the-mastering-llms-conference-answered-several-key-questions-i-had-about-when-to-fine-tune-base-models-building-evaluation-suits-and-when-to-use-rag.-the-sessions-provided-a-valuable-overview-of-the-technical-challenges-and-considerations-involved-in-building-and-deploying-custom-llms.">The Mastering LLMs conference answered several key questions I had about when to fine-tune base models, building evaluation suits and when to use RAG. The sessions provided a valuable overview of the technical challenges and considerations involved in building and deploying custom LLMs.</h3>
</section>
</section>
</div>
<div>
<p><img src="https://hamel.dev/blog/posts/course/ali.png"></p>
<section id="ali-principal-founder-scty">
<h2 data-anchor-id="ali-principal-founder-scty"><em>Ali, Principal &amp; Founder, SCTY</em></h2>
<section id="the-course-that-became-a-conference-filled-with-a-lineup-of-renowned-practitioners-whose-expertise-and-contributions-to-the-field-was-only-exceeded-by-their-generosity-of-spirit.">
<h3 data-anchor-id="the-course-that-became-a-conference-filled-with-a-lineup-of-renowned-practitioners-whose-expertise-and-contributions-to-the-field-was-only-exceeded-by-their-generosity-of-spirit.">The course that became a conference, filled with a lineup of renowned practitioners whose expertise (and contributions to the field) was only exceeded by their generosity of spirit.</h3>
</section>
</section>
</div>
<div>
<p><img src="https://hamel.dev/blog/posts/course/lukas.png"></p>
<section id="lukas-software-engineer">
<h2 data-anchor-id="lukas-software-engineer"><em>Lukas, Software Engineer</em></h2>
<section id="the-sheer-amount-of-diverse-speakers-that-cover-the-same-topics-from-different-approaches-both-praising-andor-degrading-certain-workflows-makes-this-extremely-valuable.-especially-when-a-lot-of-information-online-is-produced-by-those-who-are-building-a-commercial-product-behind-naturally-is-biased-towards-a-fine-tune-a-rag-an-open-source-llm-an-open-ai-llm-etc.-it-is-rather-extra-ordinary-to-have-a-variety-of-opinions-packed-like-this.-thank-you">
<h3 data-anchor-id="the-sheer-amount-of-diverse-speakers-that-cover-the-same-topics-from-different-approaches-both-praising-andor-degrading-certain-workflows-makes-this-extremely-valuable.-especially-when-a-lot-of-information-online-is-produced-by-those-who-are-building-a-commercial-product-behind-naturally-is-biased-towards-a-fine-tune-a-rag-an-open-source-llm-an-open-ai-llm-etc.-it-is-rather-extra-ordinary-to-have-a-variety-of-opinions-packed-like-this.-thank-you">The sheer amount of diverse speakers that cover the same topics from different approaches, both praising and/or degrading certain workflows makes this extremely valuable. Especially when a lot of information online, is produced by those, who are building a commercial product behind, naturally is biased towards a fine tune, a RAG, an open source LLM, an open ai LLM etc. It is rather extra ordinary to have a variety of opinions packed like this. Thank you!</h3>
</section>
</section>
</div>
</div>

<center>
<a href="https://parlance-labs.com/education" target="_blank">Course Website</a>
</center>
</div>
</section>
</section>
<section id="stay-connected">
<h2 data-anchor-id="stay-connected">Stay Connected</h2>
<p>I’m continuously learning about LLMs, and enjoy sharing my findings and thoughts. If you’re interested in this journey, consider subscribing.</p>
<p>What to expect:</p>
<ul>
<li>Occasional emails with my latest insights on LLMs</li>
<li>Early access to new content</li>
<li>No spam, just honest thoughts and discoveries</li>
</ul>



</section>


<section id="quarto-appendix" role="doc-endnotes"><h2>Footnotes</h2>

<ol>
<li id="fn1"><p>https://maven.com/parlance-labs/fine-tuning. We had more than 2,000 students in our first cohort. The students who paid for the original course had early access to the material, office hours, generous compute credits, and a lively Discord community.<a href="#fnref1" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>We find that instruction tuning a model to be a very useful educational experience even if you never intend to fine-tune, because it familiarizes you with topics such as (1) working with open weights models (2) generating synthetic data (3) managing prompts (4) fine-tuning (5) and generating predictions.<a href="#fnref2" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>These testimonials are taken from https://maven.com/parlance-labs/fine-tuning.<a href="#fnref3" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></main> <!-- /main -->

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Yark: YouTube Archiver with Offline UI (112 pts)]]></title>
            <link>https://github.com/Owez/yark</link>
            <guid>41100820</guid>
            <pubDate>Mon, 29 Jul 2024 14:37:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Owez/yark">https://github.com/Owez/yark</a>, See on <a href="https://news.ycombinator.com/item?id=41100820">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Yark</h2><a id="user-content-yark" aria-label="Permalink: Yark" href="#yark"></a></p>
<p dir="auto">YouTube archiving made simple.</p>




<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">To install Yark, simply download <a href="https://www.python.org/downloads/" rel="nofollow">Python 3.9+</a> and <a href="https://ffmpeg.org/" rel="nofollow">FFmpeg</a> (optional), then run the following:</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Managing your Archive</h2><a id="user-content-managing-your-archive" aria-label="Permalink: Managing your Archive" href="#managing-your-archive"></a></p>
<p dir="auto">Once you've installed Yark, think of a name for your archive (e.g., "foobar") and copy the target's url:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ yark new foobar https://www.youtube.com/channel/UCSMdm6bUYIBN0KfS2CVuEPA"><pre>$ yark new foobar https://www.youtube.com/channel/UCSMdm6bUYIBN0KfS2CVuEPA</pre></div>
<p dir="auto">Now that you've created the archive, you can tell Yark to download all videos and metadata using the refresh command:</p>

<p dir="auto">Once everything has been downloaded, Yark will automatically give you a status report of what's changed since the last refresh:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/Owez/yark/1.2-support/examples/images/cli_dark.png"><img src="https://raw.githubusercontent.com/Owez/yark/1.2-support/examples/images/cli_dark.png" alt="Report Demo" title="Report Demo" width="600"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Viewing your Archive</h2><a id="user-content-viewing-your-archive" aria-label="Permalink: Viewing your Archive" href="#viewing-your-archive"></a></p>
<p dir="auto">Viewing you archive is easy, just type <code>view</code> with your archives name:</p>

<p dir="auto">This will pop up an offline website in your browser letting you watch all videos 🚀</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/Owez/yark/1.2-support/examples/images/viewer_light.png"><img src="https://raw.githubusercontent.com/Owez/yark/1.2-support/examples/images/viewer_light.png" alt="Viewer Demo" title="Viewer Demo" width="650"></a></p>
<p dir="auto">Under each video is a rich history report filled with timelines and graphs, as well as a noting feature which lets you add timestamped and permalinked comments 👐</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/Owez/yark/1.2-support/examples/images/viewer_stats_light.png"><img src="https://raw.githubusercontent.com/Owez/yark/1.2-support/examples/images/viewer_stats_light.png" alt="Viewer Demo – Stats" title="Viewer Demo – Stats" width="650"></a></p>
<p dir="auto">Light and dark modes are both available and automatically apply based on the system's theme.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Details</h2><a id="user-content-details" aria-label="Permalink: Details" href="#details"></a></p>
<p dir="auto">Here are some things to keep in mind when using Yark; the good and the bad:</p>
<ul dir="auto">
<li>Don't create a new archive again if you just want to update it, Yark accumulates all new metadata for you via timestamps</li>
<li>Feel free to suggest new features via the issues tab on this repository</li>
<li>Scheduling isn't a feature just yet, please use <a href="https://en.wikipedia.org/wiki/Cron" rel="nofollow"><code>cron</code></a> or something similar!</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Archive Format</h2><a id="user-content-archive-format" aria-label="Permalink: Archive Format" href="#archive-format"></a></p>
<p dir="auto">The archive format itself is simple and consists of a directory-based structure with a core metadata file and all thumbnail/video data in their own directories as typical files:</p>
<ul dir="auto">
<li><code>[name]/</code> – Your self-contained archive
<ul dir="auto">
<li><code>yark.json</code> – Archive file with all metadata</li>
<li><code>yark.bak</code> – Backup archive file to protect against data damage</li>
<li><code>videos/</code> – Directory containing all known videos
<ul dir="auto">
<li><code>[id].*</code> – Files containing video data for YouTube videos</li>
</ul>
</li>
<li><code>thumbnails/</code> – Directory containing all known thumbnails
<ul dir="auto">
<li><code>[hash].png</code> – Files containing thumbnails with its hash</li>
</ul>
</li>
</ul>
</li>
</ul>
<p dir="auto">It's best to take a few minutes to familiarize yourself with your archive by looking at files which look interesting to you in it, everything is quite readable.</p>
</article></div></div>]]></description>
        </item>
    </channel>
</rss>